<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.0.36">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Hansen中高级计量体系 - 2&nbsp; Conditional Expectation and Projection</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
span.underline{text-decoration: underline;}
div.column{display: inline-block; vertical-align: top; width: 50%;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./chpt03-algebra.html" rel="next">
<link href="./part01-reg.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
    <div class="container-fluid d-flex justify-content-between">
      <h1 class="quarto-secondary-nav-title"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Conditional Expectation and Projection</span></h1>
      <button type="button" class="quarto-btn-toggle btn" aria-label="Show secondary navigation">
        <i class="bi bi-chevron-right"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Hansen中高级计量体系</a> 
        <div class="sidebar-tools-main">
    <a href="https://github.com/huhuaping/hansenEM/" title="Source Code" class="sidebar-tool px-1"><i class="bi bi-github"></i></a>
</div>
    </div>
      </div>
      <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
      </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">前言</a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./participate.html" class="sidebar-item-text sidebar-link">如何参与？</a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chpt01-intro.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./part01-reg.html" class="sidebar-item-text sidebar-link">回归</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chpt02-ce.html" class="sidebar-item-text sidebar-link active"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Conditional Expectation and Projection</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chpt03-algebra.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">The Algebra of Least Squares</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chpt04-lsr.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Least Squares Regression</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chpt05-normal-reg.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Normal Regression</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./part02-LSM.html" class="sidebar-item-text sidebar-link">大样本方法</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chpt06-review.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">A Review of Large Sample Asymptotics</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chpt07-asymptotic-ls.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Asymptotic Theory for Least Squares</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chpt08-restricted-est.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Restricted Estimation</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chpt09-hypothesit-test.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Hypothesis Testing</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chpt10-resample-method.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Resampling Methods</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./part03-MEQ.html" class="sidebar-item-text sidebar-link">多方程模型</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chpt11-multi-reg.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Multivariate Regression</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chpt12-iv.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Instrumental Variables</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chpt13-gmm.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Generalized Method of Moments</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./summary.html" class="sidebar-item-text sidebar-link">Summary</a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">References</a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" aria-expanded="true">Appendices</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./a1-notation.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">A</span>&nbsp; <span class="chapter-title">附录a1</span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction"> <span class="header-section-number">2.1</span> Introduction</a></li>
  <li><a href="#the-distribution-of-wages" id="toc-the-distribution-of-wages" class="nav-link" data-scroll-target="#the-distribution-of-wages"> <span class="header-section-number">2.2</span> The Distribution of Wages</a></li>
  <li><a href="#conditional-expectation" id="toc-conditional-expectation" class="nav-link" data-scroll-target="#conditional-expectation"> <span class="header-section-number">2.3</span> Conditional Expectation</a></li>
  <li><a href="#logs-and-percentages" id="toc-logs-and-percentages" class="nav-link" data-scroll-target="#logs-and-percentages"> <span class="header-section-number">2.4</span> Logs and Percentages</a></li>
  <li><a href="#conditional-expectation-function" id="toc-conditional-expectation-function" class="nav-link" data-scroll-target="#conditional-expectation-function"> <span class="header-section-number">2.5</span> Conditional Expectation Function</a></li>
  <li><a href="#continuous-variables" id="toc-continuous-variables" class="nav-link" data-scroll-target="#continuous-variables"> <span class="header-section-number">2.6</span> Continuous Variables</a></li>
  <li><a href="#law-of-iterated-expectations" id="toc-law-of-iterated-expectations" class="nav-link" data-scroll-target="#law-of-iterated-expectations"> <span class="header-section-number">2.7</span> Law of Iterated Expectations</a></li>
  <li><a href="#cef-error" id="toc-cef-error" class="nav-link" data-scroll-target="#cef-error"> <span class="header-section-number">2.8</span> CEF Error</a></li>
  <li><a href="#intercept-only-model" id="toc-intercept-only-model" class="nav-link" data-scroll-target="#intercept-only-model"> <span class="header-section-number">2.9</span> Intercept-Only Model</a></li>
  <li><a href="#regression-variance" id="toc-regression-variance" class="nav-link" data-scroll-target="#regression-variance"> <span class="header-section-number">2.10</span> Regression Variance</a></li>
  <li><a href="#best-predictor" id="toc-best-predictor" class="nav-link" data-scroll-target="#best-predictor"> <span class="header-section-number">2.11</span> Best Predictor</a></li>
  <li><a href="#conditional-variance" id="toc-conditional-variance" class="nav-link" data-scroll-target="#conditional-variance"> <span class="header-section-number">2.12</span> Conditional Variance</a></li>
  <li><a href="#homoskedasticity-and-heteroskedasticity" id="toc-homoskedasticity-and-heteroskedasticity" class="nav-link" data-scroll-target="#homoskedasticity-and-heteroskedasticity"> <span class="header-section-number">2.13</span> Homoskedasticity and Heteroskedasticity</a></li>
  <li><a href="#heteroskedastic-or-heteroscedastic" id="toc-heteroskedastic-or-heteroscedastic" class="nav-link" data-scroll-target="#heteroskedastic-or-heteroscedastic"> <span class="header-section-number">2.14</span> Heteroskedastic or Heteroscedastic?</a></li>
  <li><a href="#regression-derivative" id="toc-regression-derivative" class="nav-link" data-scroll-target="#regression-derivative"> <span class="header-section-number">2.15</span> Regression Derivative</a></li>
  <li><a href="#linear-cef" id="toc-linear-cef" class="nav-link" data-scroll-target="#linear-cef"> <span class="header-section-number">2.16</span> Linear CEF</a></li>
  <li><a href="#homoskedastic-linear-cef-model" id="toc-homoskedastic-linear-cef-model" class="nav-link" data-scroll-target="#homoskedastic-linear-cef-model"> <span class="header-section-number">2.17</span> Homoskedastic Linear CEF Model</a></li>
  <li><a href="#linear-cef-with-nonlinear-effects" id="toc-linear-cef-with-nonlinear-effects" class="nav-link" data-scroll-target="#linear-cef-with-nonlinear-effects"> <span class="header-section-number">2.18</span> Linear CEF with Nonlinear Effects</a></li>
  <li><a href="#linear-cef-with-dummy-variables" id="toc-linear-cef-with-dummy-variables" class="nav-link" data-scroll-target="#linear-cef-with-dummy-variables"> <span class="header-section-number">2.19</span> Linear CEF with Dummy Variables</a></li>
  <li><a href="#best-linear-predictor" id="toc-best-linear-predictor" class="nav-link" data-scroll-target="#best-linear-predictor"> <span class="header-section-number">2.20</span> Best Linear Predictor</a></li>
  <li><a href="#invertibility-and-identification" id="toc-invertibility-and-identification" class="nav-link" data-scroll-target="#invertibility-and-identification"> <span class="header-section-number">2.21</span> Invertibility and Identification</a></li>
  <li><a href="#minimization" id="toc-minimization" class="nav-link" data-scroll-target="#minimization"> <span class="header-section-number">2.22</span> Minimization</a></li>
  <li><a href="#illustrations-of-best-linear-predictor" id="toc-illustrations-of-best-linear-predictor" class="nav-link" data-scroll-target="#illustrations-of-best-linear-predictor"> <span class="header-section-number">2.23</span> Illustrations of Best Linear Predictor</a></li>
  <li><a href="#linear-predictor-error-variance" id="toc-linear-predictor-error-variance" class="nav-link" data-scroll-target="#linear-predictor-error-variance"> <span class="header-section-number">2.24</span> Linear Predictor Error Variance</a></li>
  <li><a href="#regression-coefficients" id="toc-regression-coefficients" class="nav-link" data-scroll-target="#regression-coefficients"> <span class="header-section-number">2.25</span> Regression Coefficients</a></li>
  <li><a href="#regression-sub-vectors" id="toc-regression-sub-vectors" class="nav-link" data-scroll-target="#regression-sub-vectors"> <span class="header-section-number">2.26</span> Regression Sub-Vectors</a></li>
  <li><a href="#coefficient-decomposition" id="toc-coefficient-decomposition" class="nav-link" data-scroll-target="#coefficient-decomposition"> <span class="header-section-number">2.27</span> Coefficient Decomposition</a></li>
  <li><a href="#omitted-variable-bias" id="toc-omitted-variable-bias" class="nav-link" data-scroll-target="#omitted-variable-bias"> <span class="header-section-number">2.28</span> Omitted Variable Bias</a></li>
  <li><a href="#best-linear-approximation" id="toc-best-linear-approximation" class="nav-link" data-scroll-target="#best-linear-approximation"> <span class="header-section-number">2.29</span> Best Linear Approximation</a></li>
  <li><a href="#regression-to-the-mean" id="toc-regression-to-the-mean" class="nav-link" data-scroll-target="#regression-to-the-mean"> <span class="header-section-number">2.30</span> Regression to the Mean</a></li>
  <li><a href="#reverse-regression" id="toc-reverse-regression" class="nav-link" data-scroll-target="#reverse-regression"> <span class="header-section-number">2.31</span> Reverse Regression</a></li>
  <li><a href="#limitations-of-the-best-linear-projection" id="toc-limitations-of-the-best-linear-projection" class="nav-link" data-scroll-target="#limitations-of-the-best-linear-projection"> <span class="header-section-number">2.32</span> Limitations of the Best Linear Projection</a></li>
  <li><a href="#random-coefficient-model" id="toc-random-coefficient-model" class="nav-link" data-scroll-target="#random-coefficient-model"> <span class="header-section-number">2.33</span> Random Coefficient Model</a></li>
  <li><a href="#causal-effects" id="toc-causal-effects" class="nav-link" data-scroll-target="#causal-effects"> <span class="header-section-number">2.34</span> Causal Effects</a></li>
  <li><a href="#existence-and-uniqueness-of-the-conditional-expectation" id="toc-existence-and-uniqueness-of-the-conditional-expectation" class="nav-link" data-scroll-target="#existence-and-uniqueness-of-the-conditional-expectation"> <span class="header-section-number">2.35</span> Existence and Uniqueness of the Conditional Expectation*</a></li>
  <li><a href="#identification" id="toc-identification" class="nav-link" data-scroll-target="#identification"> <span class="header-section-number">2.36</span> Identification*</a></li>
  <li><a href="#technical-proofs" id="toc-technical-proofs" class="nav-link" data-scroll-target="#technical-proofs"> <span class="header-section-number">2.37</span> Technical Proofs*</a></li>
  <li><a href="#exercises" id="toc-exercises" class="nav-link" data-scroll-target="#exercises"> <span class="header-section-number">2.38</span> Exercises</a></li>
  </ul>
<div class="toc-actions"><div><i class="bi bi-github"></i></div><div class="action-links"><p><a href="https://github.com/huhuaping/hansenEM/edit/master/chpt02-ce.qmd" class="toc-action">Edit this page</a></p></div></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title d-none d-lg-block"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Conditional Expectation and Projection</span></h1>
</div>



<div class="quarto-title-meta">

    
    
  </div>
  

</header>

<section id="introduction" class="level2" data-number="2.1">
<h2 data-number="2.1" class="anchored" data-anchor-id="introduction"><span class="header-section-number">2.1</span> Introduction</h2>
<p>The most commonly applied econometric tool is least squares estimation, also known as regression. Least squares is a tool to estimate the conditional mean of one variable (the dependent variable) given another set of variables (the regressors, conditioning variables, or covariates).</p>
<p>In this chapter we abstract from estimation and focus on the probabilistic foundation of the conditional expectation model and its projection approximation. This includes a review of probability theory. For a background in intermediate probability theory see Chapters 1-5 of Probability and Statistics for Economists.</p>
</section>
<section id="the-distribution-of-wages" class="level2" data-number="2.2">
<h2 data-number="2.2" class="anchored" data-anchor-id="the-distribution-of-wages"><span class="header-section-number">2.2</span> The Distribution of Wages</h2>
<p>Suppose that we are interested in wage rates in the United States. Since wage rates vary across workers we cannot describe wage rates by a single number. Instead, we can describe wages using a probability distribution. Formally, we view the wage of an individual worker as a random variable wage with the probability distribution</p>
<p><span class="math display">\[
F(y)=\mathbb{P}[\text { wage } \leq y] .
\]</span></p>
<p>When we say that a person’s wage is random we mean that we do not know their wage before it is measured, and we treat observed wage rates as realizations from the distribution <span class="math inline">\(F\)</span>. Treating unobserved wages as random variables and observed wages as realizations is a powerful mathematical abstraction which allows us to use the tools of mathematical probability.</p>
<p>A useful thought experiment is to imagine dialing a telephone number selected at random, and then asking the person who responds to tell us their wage rate. (Assume for simplicity that all workers have equal access to telephones and that the person who answers your call will answer honestly.) In this thought experiment, the wage of the person you have called is a single draw from the distribution <span class="math inline">\(F\)</span> of wages in the population. By making many such phone calls we can learn the full distribution.</p>
<p>When a distribution function <span class="math inline">\(F\)</span> is differentiable we define the probability density function</p>
<p><span class="math display">\[
f(y)=\frac{d}{d y} F(y) .
\]</span></p>
<p>The density contains the same information as the distribution function, but the density is typically easier to visually interpret.</p>
<p><img src="images//2022_09_17_efa0deee3441d06e0b66g-02.jpg" class="img-fluid"></p>
<ol type="a">
<li>Wage Density</li>
</ol>
<p><img src="images//2022_09_17_efa0deee3441d06e0b66g-02(1).jpg" class="img-fluid"></p>
<ol start="2" type="a">
<li>Log Wage Density</li>
</ol>
<p>Figure 2.1: Density of Wages and Log Wages</p>
<p>In Figure 2.1(a) we display an estimate <span class="math inline">\({ }^{1}\)</span> of the probability density function of U.S. wage rates in <span class="math inline">\(2009 .\)</span> We see that the density is peaked around <span class="math inline">\(\$ 15\)</span>, and most of the probability mass appears to lie between <span class="math inline">\(\$ 10\)</span> and <span class="math inline">\(\$ 40\)</span>. These are ranges for typical wage rates in the U.S. population.</p>
<p>Important measures of central tendency are the median and the mean. The median <span class="math inline">\(m\)</span> of a continuous distribution <span class="math inline">\(F\)</span> is the unique solution to</p>
<p><span class="math display">\[
F(m)=\frac{1}{2} .
\]</span></p>
<p>The median U.S. wage is <span class="math inline">\(\$ 19.23\)</span>. The median is a robust <span class="math inline">\({ }^{2}\)</span> measure of central tendency, but it is tricky to use for many calculations as it is not a linear operator.</p>
<p>The mean or expectation of a random variable <span class="math inline">\(Y\)</span> with discrete support is</p>
<p><span class="math display">\[
\mu=\mathbb{E}[Y]=\sum_{j=1}^{\infty} \tau_{j} \mathbb{P}\left[Y=\tau_{j}\right] .
\]</span></p>
<p>For a continuous random variable with density <span class="math inline">\(f(y)\)</span> the expectation is</p>
<p><span class="math display">\[
\mu=\mathbb{E}[Y]=\int_{-\infty}^{\infty} y f(y) d y .
\]</span></p>
<p>Here we have used the common and convenient convention of using the single character <span class="math inline">\(Y\)</span> to denote a random variable, rather than the more cumbersome label wage. An alternative notation which includes both discrete and continuous random variables as special cases is to write the integral as <span class="math inline">\(\int_{-\infty}^{\infty} y d F(y)\)</span>.</p>
<p>The expectation is a convenient measure of central tendency because it is a linear operator and arises naturally in many economic models. A disadvantage of the expectation is that it is not robust <span class="math inline">\({ }^{3}\)</span> especially</p>
<p><span class="math inline">\({ }^{1}\)</span> The distribution and density are estimated nonparametrically from the sample of 50,742 full-time non-military wageearners reported in the March 2009 Current Population Survey. The wage rate is constructed as annual individual wage and salary earnings divided by hours worked.</p>
<p><span class="math inline">\({ }^{2}\)</span> The median is not sensitive to pertubations in the tails of the distribution.</p>
<p><span class="math inline">\({ }^{3}\)</span> The expectation is sensitive to pertubations in the tails of the distribution. in the presence of substantial skewness or thick tails, both which are features of the wage distribution as can be seen in Figure 2.1(a). Another way of viewing this is that <span class="math inline">\(64 %\)</span> of workers earn less than the mean wage of <span class="math inline">\(\$ 23.90\)</span>, suggesting that it is incorrect to describe the mean <span class="math inline">\(\$ 23.90\)</span> as a “typical” wage rate.</p>
<p>In this context it is useful to transform the data by taking the natural logarithm” <span class="math inline">\({ }^{4}\)</span>. Figure <span class="math inline">\(2.1\)</span> (b) shows the density of <span class="math inline">\(\log\)</span> hourly wages <span class="math inline">\(\log (\)</span> wage <span class="math inline">\()\)</span> for the same population. The density of log wages is less skewed and fat-tailed than the density of the level of wages, so its mean</p>
<p><span class="math display">\[
\mathbb{E}[\log (\text { wage })]=2.95
\]</span></p>
<p>is a better (more robust) measure <span class="math inline">\({ }^{5}\)</span> of central tendency of the distribution. For this reason, wage regressions typically use log wages as a dependent variable rather than the level of wages.</p>
<p>Another useful way to summarize the probability distribution <span class="math inline">\(F(y)\)</span> is in terms of its quantiles. For any <span class="math inline">\(\alpha \in(0,1)\)</span>, the <span class="math inline">\(\alpha^{t h}\)</span> quantile of the continuous <span class="math inline">\({ }^{6}\)</span> distribution <span class="math inline">\(F\)</span> is the real number <span class="math inline">\(q_{\alpha}\)</span> which satisfies <span class="math inline">\(F\left(q_{\alpha}\right)=\alpha\)</span>. The quantile function <span class="math inline">\(q_{\alpha}\)</span>, viewed as a function of <span class="math inline">\(\alpha\)</span>, is the inverse of the distribution function <span class="math inline">\(F\)</span>. The most commonly used quantile is the median, that is, <span class="math inline">\(q_{0.5}=m\)</span>. We sometimes refer to quantiles by the percentile representation of <span class="math inline">\(\alpha\)</span> and in this case they are called percentiles. E.g. the median is the <span class="math inline">\(50^{t h}\)</span> percentile.</p>
</section>
<section id="conditional-expectation" class="level2" data-number="2.3">
<h2 data-number="2.3" class="anchored" data-anchor-id="conditional-expectation"><span class="header-section-number">2.3</span> Conditional Expectation</h2>
<p>We saw in Figure 2.1(b) the density of log wages. Is this distribution the same for all workers, or does the wage distribution vary across subpopulations? To answer this question, we can compare wage distributions for different groups - for example, men and women. To investigate, we plot in Figure <span class="math inline">\(2.2\)</span> (a) the densities of log wages for U.S. men and women. We can see that the two wage densities take similar shapes but the density for men is somewhat shifted to the right.</p>
<p>The values <span class="math inline">\(3.05\)</span> and <span class="math inline">\(2.81\)</span> are the mean log wages in the subpopulations of men and women workers. They are called the conditional expectation (or conditional mean) of log wages given gender. We can write their specific values as</p>
<p><span class="math display">\[
\begin{gathered}
\mathbb{E}[\log (\text { wage }) \mid \text { gender }=\text { man }]=3.05 \\
\mathbb{E}[\log (\text { wage }) \mid \text { gender }=\text { woman }]=2.81 .
\end{gathered}
\]</span></p>
<p>We call these expectations “conditional” as they are conditioning on a fixed value of the variable gender. While you might not think of a person’s gender as a random variable, it is random from the viewpoint of econometric analysis. If you randomly select an individual, the gender of the individual is unknown and thus random. (In the population of U.S. workers, the probability that a worker is a woman happens to be <span class="math inline">\(43 %\)</span>.) In observational data, it is most appropriate to view all measurements as random variables, and the means of subpopulations are then conditional means.</p>
<p>It is important to mention at this point that we in no way attribute causality or interpretation to the difference in the conditional expectation of log wages between men and women. There are multiple potential explanations.</p>
<p>As the two densities in Figure 2.2(a) appear similar, a hasty inference might be that there is not a meaningful difference between the wage distributions of men and women. Before jumping to this conclusion let us examine the differences in the distributions more carefully. As we mentioned above, the</p>
<p><span class="math inline">\({ }^{4}\)</span> Throughout the text, we will use <span class="math inline">\(\log (y)\)</span> or <span class="math inline">\(\log y\)</span> to denote the natural logarithm of <span class="math inline">\(y\)</span>.</p>
<p><span class="math inline">\({ }^{5}\)</span> More precisely, the geometric mean <span class="math inline">\(\exp (\mathbb{E}[\log W])=\$ 19.11\)</span> is a robust measure of central tendency.</p>
<p><span class="math inline">\({ }^{6}\)</span> If <span class="math inline">\(F\)</span> is not continuous the definition is <span class="math inline">\(q_{\alpha}=\inf \{y: F(y) \geq \alpha\}\)</span></p>
<p><img src="images//2022_09_17_efa0deee3441d06e0b66g-04.jpg" class="img-fluid"></p>
<ol type="a">
<li>Women and Men</li>
</ol>
<p><img src="images//2022_09_17_efa0deee3441d06e0b66g-04(1).jpg" class="img-fluid"></p>
<ol start="2" type="a">
<li>By Gender and Race</li>
</ol>
<p>Figure 2.2: Log Wage Density by Gender and Race</p>
<p>primary difference between the two densities appears to be their means. This difference equals</p>
<p><span class="math display">\[
\begin{aligned}
\mathbb{E}[\log (\text { wage }) \mid \text { gender }=\text { man }]-\mathbb{E}[\log (\text { wage }) \mid \text { gender }=\text { woman }] &amp;=3.05-2.81 \\
&amp;=0.24 .
\end{aligned}
\]</span></p>
<p>A difference in expected log wages of <span class="math inline">\(0.24\)</span> is often interpreted as an average <span class="math inline">\(24 %\)</span> difference between the wages of men and women, which is quite substantial. (For a more complete explanation see Section 2.4.)</p>
<p>Consider further splitting the male and female subpopulations by race, dividing the population into whites, Blacks, and other races. We display the log wage density functions of four of these groups in Figure <span class="math inline">\(2.2\)</span> (b). Again we see that the primary difference between the four density functions is their central tendency.</p>
<p>Focusing on the means of these distributions, Table <span class="math inline">\(2.1\)</span> reports the mean log wage for each of the six sub-populations.</p>
<p>Table 2.1: Mean Log Wages by Gender and Race</p>
<table class="table">
<thead>
<tr class="header">
<th style="text-align: left;"></th>
<th>men</th>
<th>women</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">white</td>
<td><span class="math inline">\(3.07\)</span></td>
<td><span class="math inline">\(2.82\)</span></td>
</tr>
<tr class="even">
<td style="text-align: left;">Black</td>
<td><span class="math inline">\(2.86\)</span></td>
<td><span class="math inline">\(2.73\)</span></td>
</tr>
<tr class="odd">
<td style="text-align: left;">other</td>
<td><span class="math inline">\(3.03\)</span></td>
<td><span class="math inline">\(2.86\)</span></td>
</tr>
</tbody>
</table>
<p>Once again we stress that we in no way attribute causality or interpretation to the differences across the entries of the table. The reason why we use these particular sub-populations to illustrate conditional expectation is because differences in economic outcomes between gender and racial groups in the United States (and elsewhere) are widely discussed; part of the role of social science is to carefully document such patterns, and part of its role is to craft models and explanations. Conditional expectations (by themselves) can help in the documentation and description; conditional expectations by themselves are neither a model nor an explanation.</p>
<p>The entries in Table <span class="math inline">\(2.1\)</span> are the conditional means of <span class="math inline">\(\log (\)</span> wage <span class="math inline">\()\)</span> given gender and race. For example</p>
<p><span class="math display">\[
\mathbb{E}[\log (\text { wage }) \mid \text { gender }=\text { man, race }=\text { white }]=3.07
\]</span></p>
<p>and</p>
<p><span class="math display">\[
\mathbb{E}[\log (\text { wage }) \mid \text { gender }=\text { woman, race }=\text { Black }]=2.73 \text {. }
\]</span></p>
<p>One benefit of focusing on conditional means is that they reduce complicated distributions to a single summary measure, and thereby facilitate comparisons across groups. Because of this simplifying property, conditional means are the primary interest of regression analysis and are a major focus in econometrics.</p>
<p>Table <span class="math inline">\(2.1\)</span> allows us to easily calculate average wage differences between groups. For example, we can see that the wage gap between men and women continues after disaggregation by race, as the average gap between white men and white women is <span class="math inline">\(25 %\)</span>, and that between Black men and Black women is <span class="math inline">\(13 %\)</span>. We also can see that there is a race gap, as the average wages of Blacks are substantially less than the other race categories. In particular, the average wage gap between white men and Black men is <span class="math inline">\(21 %\)</span>, and that between white women and Black women is <span class="math inline">\(9 %\)</span>.</p>
</section>
<section id="logs-and-percentages" class="level2" data-number="2.4">
<h2 data-number="2.4" class="anchored" data-anchor-id="logs-and-percentages"><span class="header-section-number">2.4</span> Logs and Percentages</h2>
<p>In this section we want to motivate and clarify the use of the logarithm in regression analysis by making two observations. First, when applied to numbers the difference of logarithms approximately equals the percentage difference. Second, when applied to averages the difference in logarithms approximately equals the percentage difference in the geometric mean. We now explain these ideas and the nature of the approximations involved.</p>
<p>Take two positive numbers <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span>. The percentage difference between <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> is</p>
<p><span class="math display">\[
p=100\left(\frac{a-b}{b}\right) .
\]</span></p>
<p>Rewriting,</p>
<p><span class="math display">\[
\frac{a}{b}=1+\frac{p}{100}
\]</span></p>
<p>Taking natural logarithms,</p>
<p><span class="math display">\[
\log a-\log b=\log \left(1+\frac{p}{100}\right) .
\]</span></p>
<p>A useful approximation for small <span class="math inline">\(x\)</span> is</p>
<p><span class="math display">\[
\log (1+x) \simeq x .
\]</span></p>
<p>This can be derived from the infinite series expansion of <span class="math inline">\(\log (1+x)\)</span> :</p>
<p><span class="math display">\[
\log (1+x)=x-\frac{x^{2}}{2}+\frac{x^{3}}{3}-\frac{x^{4}}{4}+\cdots=x+O\left(x^{2}\right) .
\]</span></p>
<p>The symbol <span class="math inline">\(O\left(x^{2}\right.\)</span> ) means that the remainder is bounded by <span class="math inline">\(A x^{2}\)</span> as <span class="math inline">\(x \rightarrow 0\)</span> for some <span class="math inline">\(A&lt;\infty\)</span>. Numerically, the approximation <span class="math inline">\(\log (1+x) \simeq x\)</span> is within <span class="math inline">\(0.001\)</span> for <span class="math inline">\(|x| \leq 0.1\)</span>, and the approximation error increases with <span class="math inline">\(|x|\)</span></p>
<p>Applying (2.3) to (2.2) and multiplying by 100 we find</p>
<p><span class="math display">\[
p \simeq 100(\log a)-\log b) .
\]</span></p>
<p>This shows that 100 multiplied by the difference in logarithms is approximately the percentage difference. Numerically, the approximation error is less than <span class="math inline">\(0.1\)</span> percentage points for <span class="math inline">\(|p| \leq 10\)</span>.</p>
<p>Now consider the difference in the expectation of log transformed random variables. Take two random variables <span class="math inline">\(X_{1}, X_{2}&gt;0\)</span>. Define their geometric means <span class="math inline">\(\theta_{1}=\exp \left(\mathbb{E}\left[\log X_{1}\right]\right)\)</span> and <span class="math inline">\(\theta_{2}=\exp \left(\mathbb{E}\left[\log X_{2}\right]\right)\)</span> and their percentage difference</p>
<p><span class="math display">\[
p=100\left(\frac{\theta_{2}-\theta_{1}}{\theta_{1}}\right) .
\]</span></p>
<p>The difference in the expectation of the log transforms (multiplied by 100) is</p>
<p><span class="math display">\[
100\left(\mathbb{E}\left[\log X_{2}\right]-\mathbb{E}\left[\log X_{1}\right]\right)=100\left(\log \theta_{2}-\log \theta_{1}\right) \simeq p
\]</span></p>
<p>the percentage difference between <span class="math inline">\(\theta_{2}\)</span> and <span class="math inline">\(\theta_{1}\)</span>. In words, the difference between the average of the log transformed variables is (approximately) the percentage difference in the geometric means.</p>
<p>The reason why this latter observation is important is because many econometric equations take the semi-log form</p>
<p><span class="math display">\[
\begin{aligned}
&amp;\mathbb{E}[\log Y \mid \operatorname{group}=1]=\mu_{1} \\
&amp;\mathbb{E}[\log Y \mid \operatorname{group}=2]=\mu_{2}
\end{aligned}
\]</span></p>
<p>and considerable attention is given to the difference <span class="math inline">\(\mu_{1}-\mu_{2}\)</span>. For example, in the previous section we compared the average log wages for men and women and found that the difference is <span class="math inline">\(0.24\)</span>. In that section we stated that this difference is often interpreted as the average percentage difference. This is not quite right, but is not quite wrong either. What the above calculation shows is that this difference is approximately the percentage difference in the geometric mean. So <span class="math inline">\(\mu_{1}-\mu_{2}\)</span> is an average percentage difference, where “average” refers to geometric rather than arithmetic mean.</p>
<p>To compare different measures of percentage difference see Table 2.2. In the first two columns we report average wages for men and women in the CPS population using four “averages”: arithmetic mean, median, geometric mean, and mean log. For both groups the arithmetic mean is higher than the median and geometric mean, and the latter two are similar to one another. This is a common feature of skewed distributions such as the wage distribution. The third column reports the percentage difference between the first two columns (using men’s wages as the base). For example, the first entry of <span class="math inline">\(34 %\)</span> states that the mean wage for men is <span class="math inline">\(34 %\)</span> higher than the mean wage for women. The next entries show that the median and geometric mean for men is <span class="math inline">\(26 %\)</span> higher than those for women. The final entry in this column is 100 times the simple difference between the mean log wage, which is <span class="math inline">\(24 %\)</span>. As shown above, the difference in the mean of the log transformation is approximately the percentage difference in the geometric mean, and this approximation is excellent for differences under <span class="math inline">\(10 %\)</span>.</p>
<p>Let’s summarize this analysis. It is common to take logarithms of variables and make comparisons between conditional means. We have shown that these differences are measures of the percentage difference in the geometric mean. Thus the common description that the difference between expected log transforms (such as the <span class="math inline">\(0.24\)</span> difference between those for men and women’s wages) is an approximate percentage difference (e.g.&nbsp;a 24% difference in men’s wages relative to women’s) is correct, so long as we realize that we are implicitly comparing geometric means.</p>
</section>
<section id="conditional-expectation-function" class="level2" data-number="2.5">
<h2 data-number="2.5" class="anchored" data-anchor-id="conditional-expectation-function"><span class="header-section-number">2.5</span> Conditional Expectation Function</h2>
<p>An important determinant of wages is education. In many empirical studies economists measure educational attainment by the number of years <span class="math inline">\({ }^{7}\)</span> of schooling. We will write this variable as education.</p>
<p><span class="math inline">\({ }^{7}\)</span> Here, education is defined as years of schooling beyond kindergarten. A high school graduate has education=12, a college graduate has education=16, a Master’s degree has education=18, and a professional degree (medical, law or PhD) has educa- Table 2.2: Average Wages and Percentage Differences</p>
<table class="table">
<thead>
<tr class="header">
<th style="text-align: left;"></th>
<th>men</th>
<th>women</th>
<th>% Difference</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Arithmetic Mean</td>
<td><span class="math inline">\(\$ 26.80\)</span></td>
<td><span class="math inline">\(\$ 20.00\)</span></td>
<td><span class="math inline">\(34 %\)</span></td>
</tr>
<tr class="even">
<td style="text-align: left;">Median</td>
<td><span class="math inline">\(\$ 21.14\)</span></td>
<td><span class="math inline">\(\$ 16.83\)</span></td>
<td><span class="math inline">\(26 %\)</span></td>
</tr>
<tr class="odd">
<td style="text-align: left;">Geometric Mean</td>
<td><span class="math inline">\(\$ 21.03\)</span></td>
<td><span class="math inline">\(\$ 16.64\)</span></td>
<td><span class="math inline">\(26 %\)</span></td>
</tr>
<tr class="even">
<td style="text-align: left;">Mean log Wage</td>
<td><span class="math inline">\(3.05\)</span></td>
<td><span class="math inline">\(2.81\)</span></td>
<td><span class="math inline">\(24 %\)</span></td>
</tr>
</tbody>
</table>
<p>The conditional expectation of <span class="math inline">\(\log (\)</span> wage <span class="math inline">\()\)</span> given gender, race, and education is a single number for each category. For example</p>
<p><span class="math display">\[
\mathbb{E}[\log (\text { wage }) \mid \text { gender }=\text { man, race }=\text { white, education }=12]=2.84 .
\]</span></p>
<p>We display in Figure <span class="math inline">\(2.3\)</span> the conditional expectation of <span class="math inline">\(\log\)</span> (wage) as a function of education, separately for (white) men and women. The plot is quite revealing. We see that the conditional expectation is increasing in years of education, but at a different rate for schooling levels above and below nine years. Another striking feature of Figure <span class="math inline">\(2.3\)</span> is that the gap between men and women is roughly constant for all education levels. As the variables are measured in logs this implies a constant average percentage gap between men and women regardless of educational attainment.</p>
<p><img src="images//2022_09_17_efa0deee3441d06e0b66g-07.jpg" class="img-fluid"></p>
<p>Figure 2.3: Expected Log Wage as a Function of Education tion=20. In many cases it is convenient to simplify the notation by writing variables using single characters, typically <span class="math inline">\(Y, X\)</span>, and/or <span class="math inline">\(Z\)</span>. It is conventional in econometrics to denote the dependent variable (e.g.&nbsp;<span class="math inline">\(\log (\)</span> wage <span class="math inline">\()\)</span> ) by the letter <span class="math inline">\(Y\)</span>, a conditioning variable (such as gender) by the letter <span class="math inline">\(X\)</span>, and multiple conditioning variables (such as race, education and gender) by the subscripted letters <span class="math inline">\(X_{1}, X_{2}, \ldots, X_{k}\)</span>.</p>
<p>Conditional expectations can be written with the generic notation</p>
<p><span class="math display">\[
\mathbb{E}\left[Y \mid X_{1}=x_{1}, X_{2}=x_{2}, \ldots, X_{k}=x_{k}\right]=m\left(x_{1}, x_{2}, \ldots, x_{k}\right) \text {. }
\]</span></p>
<p>We call this the conditional expectation function (CEF). The CEF is a function of <span class="math inline">\(\left(x_{1}, x_{2}, \ldots, x_{k}\right)\)</span> as it varies with the variables. For example, the conditional expectation of <span class="math inline">\(Y=\log (\)</span> wage <span class="math inline">\()\)</span> given <span class="math inline">\(\left(X_{1}, X_{2}\right)=(g e n d e r\)</span>, race) is given by the six entries of Table <span class="math inline">\(2.1 .\)</span></p>
<p>For greater compactness we typically write the conditioning variables as a vector in <span class="math inline">\(\mathbb{R}^{k}\)</span> :</p>
<p><span class="math display">\[
X=\left(\begin{array}{c}
X_{1} \\
X_{2} \\
\vdots \\
X_{k}
\end{array}\right)
\]</span></p>
<p>Given this notation, the CEF can be compactly written as</p>
<p><span class="math display">\[
\mathbb{E}[Y \mid X=x]=m(x) .
\]</span></p>
<p>The CEF <span class="math inline">\(m(x)=\mathbb{E}[Y \mid X=x]\)</span> is a function of <span class="math inline">\(x \in \mathbb{R}^{k}\)</span>. It says: “When <span class="math inline">\(X\)</span> takes the value <span class="math inline">\(x\)</span> then the average value of <span class="math inline">\(Y\)</span> is <span class="math inline">\(m(x)\)</span>.” Sometimes it is useful to view the CEF as a function of the random variable <span class="math inline">\(X\)</span>. In this case we evaluate the function <span class="math inline">\(m(x)\)</span> at <span class="math inline">\(X\)</span>, and write <span class="math inline">\(m(X)\)</span> or <span class="math inline">\(\mathbb{E}[Y \mid X]\)</span>. This is random as it is a function of the random variable <span class="math inline">\(X\)</span>.</p>
</section>
<section id="continuous-variables" class="level2" data-number="2.6">
<h2 data-number="2.6" class="anchored" data-anchor-id="continuous-variables"><span class="header-section-number">2.6</span> Continuous Variables</h2>
<p>In the previous sections we implicitly assumed that the conditioning variables are discrete. However, many conditioning variables are continuous. In this section, we take up this case and assume that the variables <span class="math inline">\((Y, X)\)</span> are continuously distributed with a joint density function <span class="math inline">\(f(y, x)\)</span>.</p>
<p>As an example, take <span class="math inline">\(Y=\log (\)</span> wage <span class="math inline">\()\)</span> and <span class="math inline">\(X=\)</span> experience, the latter the number of years of potential labor market experience <span class="math inline">\({ }^{8}\)</span>. The contours of their joint density are plotted in Figure <span class="math inline">\(2.4\)</span> (a) for the population of white men with 12 years of education.</p>
<p>Given the joint density <span class="math inline">\(f(y, x)\)</span> the variable <span class="math inline">\(x\)</span> has the marginal density</p>
<p><span class="math display">\[
f_{X}(x)=\int_{-\infty}^{\infty} f(y, x) d y .
\]</span></p>
<p>For any <span class="math inline">\(x\)</span> such that <span class="math inline">\(f_{X}(x)&gt;0\)</span> the conditional density of <span class="math inline">\(Y\)</span> given <span class="math inline">\(X\)</span> is defined as</p>
<p><span class="math display">\[
f_{Y \mid X}(y \mid x)=\frac{f(y, x)}{f_{X}(x)} .
\]</span></p>
<p>The conditional density is a renormalized slice of the joint density <span class="math inline">\(f(y, x)\)</span> holding <span class="math inline">\(x\)</span> fixed. The slice is renormalized (divided by <span class="math inline">\(f_{X}(x)\)</span> so that it integrates to one) and is thus a density. We can visualize this by slicing the joint density function at a specific value of <span class="math inline">\(x\)</span> parallel with the <span class="math inline">\(y\)</span>-axis. For example, take the density contours in Figure 2.4(a) and slice through the contour plot at a specific value of experience, and</p>
<p><span class="math inline">\({ }^{8}\)</span> As there is no direct measure for experience, we instead define experience as age-education-6</p>
<p><img src="images//2022_09_17_efa0deee3441d06e0b66g-09.jpg" class="img-fluid"></p>
<ol type="a">
<li>Joint Density of Log Wage and Experience</li>
</ol>
<p><img src="images//2022_09_17_efa0deee3441d06e0b66g-09(1).jpg" class="img-fluid"></p>
<ol start="2" type="a">
<li>Conditional Density of Log Wage given Experience</li>
</ol>
<p>Figure 2.4: Log Wage and Experience</p>
<p>then renormalize the slice so that it is a proper density. This gives us the conditional density of log(wage) for white men with 12 years of education and this level of experience. We do this for three levels of experience <span class="math inline">\((5,10\)</span>, and 25 years), and plot these densities in Figure <span class="math inline">\(2.4\)</span> (b). We can see that the distribution of wages shifts to the right and becomes more diffuse as experience increases.</p>
<p>The CEF of <span class="math inline">\(Y\)</span> given <span class="math inline">\(X=x\)</span> is the expectation of the conditional density (2.5)</p>
<p><span class="math display">\[
m(x)=\mathbb{E}[Y \mid X=x]=\int_{-\infty}^{\infty} y f_{Y \mid X}(y \mid x) d y .
\]</span></p>
<p>Intuitively, <span class="math inline">\(m(x)\)</span> is the expectation of <span class="math inline">\(Y\)</span> for the idealized subpopulation where the conditioning variables are fixed at <span class="math inline">\(x\)</span>. When <span class="math inline">\(X\)</span> is continuously distributed this subpopulation is infinitely small.</p>
<p>This definition (2.6) is appropriate when the conditional density (2.5) is well defined. However, Theorem <span class="math inline">\(2.13\)</span> in Section <span class="math inline">\(2.31\)</span> will show that <span class="math inline">\(m(x)\)</span> can be defined for any random variables <span class="math inline">\((Y, X)\)</span> so long as <span class="math inline">\(\mathbb{E}|Y|&lt;\infty\)</span></p>
<p>In Figure 2.4(a) the CEF of <span class="math inline">\(\log\)</span> (wage) given experience is plotted as the solid line. We can see that the CEF is a smooth but nonlinear function. The CEF is initially increasing in experience, flattens out around experience <span class="math inline">\(=30\)</span>, and then decreases for high levels of experience.</p>
</section>
<section id="law-of-iterated-expectations" class="level2" data-number="2.7">
<h2 data-number="2.7" class="anchored" data-anchor-id="law-of-iterated-expectations"><span class="header-section-number">2.7</span> Law of Iterated Expectations</h2>
<p>An extremely useful tool from probability theory is the law of iterated expectations. An important special case is known as the Simple Law. Theorem 2.1 Simple Law of Iterated Expectations</p>
<p>If <span class="math inline">\(\mathbb{E}|Y|&lt;\infty\)</span> then for any random vector <span class="math inline">\(X\)</span>,</p>
<p><span class="math display">\[
\mathbb{E}[\mathbb{E}[Y \mid X]]=\mathbb{E}[Y] .
\]</span></p>
<p>This states that the expectation of the conditional expectation is the unconditional expectation. In other words the average of the conditional averages is the unconditional average. For discrete <span class="math inline">\(X\)</span></p>
<p><span class="math display">\[
\mathbb{E}[\mathbb{E}[Y \mid X]]=\sum_{j=1}^{\infty} \mathbb{E}\left[Y \mid X=x_{j}\right] \mathbb{P}\left[X=x_{j}\right] .
\]</span></p>
<p>For continuous <span class="math inline">\(X\)</span></p>
<p><span class="math display">\[
\mathbb{E}[\mathbb{E}[Y \mid X]]=\int_{\mathbb{R}^{k}} \mathbb{E}[Y \mid X=x] f_{X}(x) d x .
\]</span></p>
<p>Going back to our investigation of average log wages for men and women, the simple law states that</p>
<p><span class="math display">\[
\begin{aligned}
&amp;\mathbb{E}[\log (\text { wage }) \mid \text { gender }=\text { man }] \mathbb{P}[\text { gender }=\text { man }] \\
&amp;+\mathbb{E}[\log (\text { wage }) \mid \text { gender }=\text { woman }] \mathbb{P}[\text { gender }=\text { woman }] \\
&amp;=\mathbb{E}[\log (\text { wage })]
\end{aligned}
\]</span></p>
<p>Or numerically,</p>
<p><span class="math display">\[
3.05 \times 0.57+2.81 \times 0.43=2.95 \text {. }
\]</span></p>
<p>The general law of iterated expectations allows two sets of conditioning variables.</p>
<p>Theorem 2.2 Law of Iterated Expectations If <span class="math inline">\(\mathbb{E}|Y|&lt;\infty\)</span> then for any random vectors <span class="math inline">\(X_{1}\)</span> and <span class="math inline">\(X_{2}\)</span>,</p>
<p><span class="math display">\[
\mathbb{E}\left[\mathbb{E}\left[Y \mid X_{1}, X_{2}\right] \mid X_{1}\right]=\mathbb{E}\left[Y \mid X_{1}\right] .
\]</span></p>
<p>Notice the way the law is applied. The inner expectation conditions on <span class="math inline">\(X_{1}\)</span> and <span class="math inline">\(X_{2}\)</span>, while the outer expectation conditions only on <span class="math inline">\(X_{1}\)</span>. The iterated expectation yields the simple answer <span class="math inline">\(\mathbb{E}\left[Y \mid X_{1}\right]\)</span>, the expectation conditional on <span class="math inline">\(X_{1}\)</span> alone. Sometimes we phrase this as: “The smaller information set wins.”</p>
<p>As an example</p>
<p><span class="math display">\[
\begin{aligned}
&amp;\mathbb{E}[\log (\text { wage }) \mid \text { gender }=\text { man, race }=\text { white }] \mathbb{P}[\text { race }=\text { white } \mid \text { gender }=\text { man }] \\
&amp;+\mathbb{E}[\log (\text { wage }) \mid \text { gender }=\text { man, race }=\text { Black }] \mathbb{P}[\text { race }=\text { Black } \mid \text { gender }=\text { man }] \\
&amp;+\mathbb{E}[\log (\text { wage }) \mid \text { gender }=\text { man, race }=\text { other }] \mathbb{P}[\text { race }=\text { other } \mid \text { gender }=\text { man }] \\
&amp;=\mathbb{E}[\log (\text { wage }) \mid \text { gender }=\text { man }]
\end{aligned}
\]</span></p>
<p>or numerically</p>
<p><span class="math display">\[
3.07 \times 0.84+2.86 \times 0.08+3.03 \times 0.08=3.05 \text {. }
\]</span></p>
<p>A property of conditional expectations is that when you condition on a random vector <span class="math inline">\(X\)</span> you can effectively treat it as if it is constant. For example, <span class="math inline">\(\mathbb{E}[X \mid X]=X\)</span> and <span class="math inline">\(\mathbb{E}[g(X) \mid X]=g(X)\)</span> for any function <span class="math inline">\(g(\cdot)\)</span>. The general property is known as the Conditioning Theorem.</p>
<p>Theorem 2.3 Conditioning Theorem If <span class="math inline">\(\mathbb{E}|Y|&lt;\infty\)</span> then</p>
<p><span class="math display">\[
\mathbb{E}[g(X) Y \mid X]=g(X) \mathbb{E}[Y \mid X] .
\]</span></p>
<p>If in addition <span class="math inline">\(\mathbb{E}|g(X)|&lt;\infty\)</span> then</p>
<p><span class="math display">\[
\mathbb{E}[g(X) Y]=\mathbb{E}[g(X) \mathbb{E}[Y \mid X]] .
\]</span></p>
<p>The proofs of Theorems 2.1, <span class="math inline">\(2.2\)</span> and <span class="math inline">\(2.3\)</span> are given in Section <span class="math inline">\(2.33 .\)</span></p>
</section>
<section id="cef-error" class="level2" data-number="2.8">
<h2 data-number="2.8" class="anchored" data-anchor-id="cef-error"><span class="header-section-number">2.8</span> CEF Error</h2>
<p>The CEF error <span class="math inline">\(e\)</span> is defined as the difference between <span class="math inline">\(Y\)</span> and the CEF evaluated at <span class="math inline">\(X\)</span> :</p>
<p><span class="math display">\[
e=Y-m(X) .
\]</span></p>
<p>By construction, this yields the formula</p>
<p><span class="math display">\[
Y=m(X)+e .
\]</span></p>
<p>In (2.9) it is useful to understand that the error <span class="math inline">\(e\)</span> is derived from the joint distribution of <span class="math inline">\((Y, X)\)</span>, and so its properties are derived from this construction.</p>
<p>Many authors in econometrics denote the CEF error using the Greek letter <span class="math inline">\(\varepsilon\)</span>. I do not follow this convention because the error <span class="math inline">\(e\)</span> is a random variable similar to <span class="math inline">\(Y\)</span> and <span class="math inline">\(X\)</span>, and it is typical to use Latin characters for random variables.</p>
<p>A key property of the CEF error is that it has a conditional expectation of zero. To see this, by the linearity of expectations, the definition <span class="math inline">\(m(X)=\mathbb{E}[Y \mid X]\)</span>, and the Conditioning Theorem</p>
<p><span class="math display">\[
\begin{aligned}
\mathbb{E}[e \mid X] &amp;=\mathbb{E}[(Y-m(X)) \mid X] \\
&amp;=\mathbb{E}[Y \mid X]-\mathbb{E}[m(X) \mid X] \\
&amp;=m(X)-m(X)=0 .
\end{aligned}
\]</span></p>
<p>This fact can be combined with the law of iterated expectations to show that the unconditional expectation is also zero.</p>
<p><span class="math display">\[
\mathbb{E}[e]=\mathbb{E}[\mathbb{E}[e \mid X]]=\mathbb{E}[0]=0 .
\]</span></p>
<p>We state this and some other results formally.</p>
<p>Theorem 2.4 Properties of the CEF error</p>
<p>If <span class="math inline">\(\mathbb{E}|Y|&lt;\infty\)</span> then</p>
<ol type="1">
<li><p><span class="math inline">\(\mathbb{E}[e \mid X]=0\)</span>.</p></li>
<li><p><span class="math inline">\(\mathbb{E}[e]=0\)</span>.</p></li>
<li><p>If <span class="math inline">\(\mathbb{E}|Y|^{r}&lt;\infty\)</span> for <span class="math inline">\(r \geq 1\)</span> then <span class="math inline">\(\mathbb{E}|e|^{r}&lt;\infty\)</span>.</p></li>
<li><p>For any function <span class="math inline">\(h(x)\)</span> such that <span class="math inline">\(\mathbb{E}|h(X) e|&lt;\infty\)</span> then <span class="math inline">\(\mathbb{E}[h(X) e]=0\)</span>. The proof of the third result is deferred to Section 2.33. The fourth result, whose proof is left to Exercise 2.3, implies that <span class="math inline">\(e\)</span> is uncorrelated with any function of the regressors.</p></li>
</ol>
<p>The equations</p>
<p><span class="math display">\[
\begin{aligned}
Y &amp;=m(X)+e \\
\mathbb{E}[e \mid X] &amp;=0
\end{aligned}
\]</span></p>
<p>together imply that <span class="math inline">\(m(X)\)</span> is the CEF of <span class="math inline">\(Y\)</span> given <span class="math inline">\(X\)</span>. It is important to understand that this is not a restriction. These equations hold true by definition.</p>
<p>The condition <span class="math inline">\(\mathbb{E}[e \mid X]=0\)</span> is implied by the definition of <span class="math inline">\(e\)</span> as the difference between <span class="math inline">\(Y\)</span> and the CEF <span class="math inline">\(m(X)\)</span>. The equation <span class="math inline">\(\mathbb{E}[e \mid X]=0\)</span> is sometimes called a conditional mean restriction, because the conditional mean of the error <span class="math inline">\(e\)</span> is restricted to equal zero. The property is also sometimes called mean independence, for the conditional mean of <span class="math inline">\(e\)</span> is 0 and thus independent of <span class="math inline">\(X\)</span>. However, it does not imply that the distribution of <span class="math inline">\(e\)</span> is independent of <span class="math inline">\(X\)</span>. Sometimes the assumption ” <span class="math inline">\(e\)</span> is independent of <span class="math inline">\(X\)</span> ” is added as a convenient simplification, but it is not generic feature of the conditional mean. Typically and generally, <span class="math inline">\(e\)</span> and <span class="math inline">\(X\)</span> are jointly dependent even though the conditional mean of <span class="math inline">\(e\)</span> is zero.</p>
<p>As an example, the contours of the joint density of the regression error <span class="math inline">\(e\)</span> and experience are plotted in Figure <span class="math inline">\(2.5\)</span> for the same population as Figure 2.4. Notice that the shape of the conditional distribution varies with the level of experience.</p>
<p><img src="images//2022_09_17_efa0deee3441d06e0b66g-12.jpg" class="img-fluid"></p>
<p>Labor Market Experience (Years)</p>
<p>Figure 2.5: Joint Density of Regression Error and Experience</p>
<p>As a simple example of a case where <span class="math inline">\(X\)</span> and <span class="math inline">\(e\)</span> are mean independent yet dependent let <span class="math inline">\(e=X u\)</span> where <span class="math inline">\(X\)</span> and <span class="math inline">\(u\)</span> are independent <span class="math inline">\(\mathrm{N}(0,1)\)</span>. Then conditional on <span class="math inline">\(X\)</span> the error <span class="math inline">\(e\)</span> has the distribution <span class="math inline">\(\mathrm{N}\left(0, X^{2}\right)\)</span>. Thus <span class="math inline">\(\mathbb{E}[e \mid X]=0\)</span> and <span class="math inline">\(e\)</span> is mean independent of <span class="math inline">\(X\)</span>, yet <span class="math inline">\(e\)</span> is not fully independent of <span class="math inline">\(X\)</span>. Mean independence does not imply full independence.</p>
</section>
<section id="intercept-only-model" class="level2" data-number="2.9">
<h2 data-number="2.9" class="anchored" data-anchor-id="intercept-only-model"><span class="header-section-number">2.9</span> Intercept-Only Model</h2>
<p>A special case of the regression model is when there are no regressors <span class="math inline">\(X\)</span>. In this case <span class="math inline">\(m(X)=\mathbb{E}[Y]=\mu\)</span>, the unconditional expectation of <span class="math inline">\(Y\)</span>. We can still write an equation for <span class="math inline">\(Y\)</span> in the regression format:</p>
<p><span class="math display">\[
\begin{aligned}
Y &amp;=\mu+e \\
\mathbb{E}[e] &amp;=0 .
\end{aligned}
\]</span></p>
<p>This is useful for it unifies the notation.</p>
</section>
<section id="regression-variance" class="level2" data-number="2.10">
<h2 data-number="2.10" class="anchored" data-anchor-id="regression-variance"><span class="header-section-number">2.10</span> Regression Variance</h2>
<p>An important measure of the dispersion about the CEF function is the unconditional variance of the CEF error <span class="math inline">\(e\)</span>. We write this as</p>
<p><span class="math display">\[
\sigma^{2}=\operatorname{var}[e]=\mathbb{E}\left[(e-\mathbb{E}[e])^{2}\right]=\mathbb{E}\left[e^{2}\right] .
\]</span></p>
<p>Theorem 2.4.3 implies the following simple but useful result.</p>
<p>Theorem 2.5 If <span class="math inline">\(\mathbb{E}\left[Y^{2}\right]&lt;\infty\)</span> then <span class="math inline">\(\sigma^{2}&lt;\infty\)</span>.</p>
<p>We can call <span class="math inline">\(\sigma^{2}\)</span> the regression variance or the variance of the regression error. The magnitude of <span class="math inline">\(\sigma^{2}\)</span> measures the amount of variation in <span class="math inline">\(Y\)</span> which is not “explained” or accounted for in the conditional expectation <span class="math inline">\(\mathbb{E}[Y \mid X]\)</span>.</p>
<p>The regression variance depends on the regressors <span class="math inline">\(X\)</span>. Consider two regressions</p>
<p><span class="math display">\[
\begin{aligned}
&amp;Y=\mathbb{E}\left[Y \mid X_{1}\right]+e_{1} \\
&amp;Y=\mathbb{E}\left[Y \mid X_{1}, X_{2}\right]+e_{2} .
\end{aligned}
\]</span></p>
<p>We write the two errors distinctly as <span class="math inline">\(e_{1}\)</span> and <span class="math inline">\(e_{2}\)</span> as they are different - changing the conditioning information changes the conditional expectation and therefore the regression error as well.</p>
<p>In our discussion of iterated expectations we have seen that by increasing the conditioning set the conditional expectation reveals greater detail about the distribution of <span class="math inline">\(Y\)</span>. What is the implication for the regression error?</p>
<p>It turns out that there is a simple relationship. We can think of the conditional expectation <span class="math inline">\(\mathbb{E}[Y \mid X]\)</span> as the “explained portion” of <span class="math inline">\(Y\)</span>. The remainder <span class="math inline">\(e=Y-\mathbb{E}[Y \mid X]\)</span> is the “unexplained portion”. The simple relationship we now derive shows that the variance of this unexplained portion decreases when we condition on more variables. This relationship is monotonic in the sense that increasing the amount of information always decreases the variance of the unexplained portion.</p>
<p>Theorem 2.6 If <span class="math inline">\(\mathbb{E}\left[Y^{2}\right]&lt;\infty\)</span> then</p>
<p><span class="math display">\[
\operatorname{var}[Y] \geq \operatorname{var}\left[Y-\mathbb{E}\left[Y \mid X_{1}\right]\right] \geq \operatorname{var}\left[Y-\mathbb{E}\left[Y \mid X_{1}, X_{2}\right]\right] .
\]</span></p>
<p>Theorem <span class="math inline">\(2.6\)</span> says that the variance of the difference between <span class="math inline">\(Y\)</span> and its conditional expectation (weakly) decreases whenever an additional variable is added to the conditioning information.</p>
<p>The proof of Theorem <span class="math inline">\(2.6\)</span> is given in Section 2.33.</p>
</section>
<section id="best-predictor" class="level2" data-number="2.11">
<h2 data-number="2.11" class="anchored" data-anchor-id="best-predictor"><span class="header-section-number">2.11</span> Best Predictor</h2>
<p>Suppose that given a random vector <span class="math inline">\(X\)</span> we want to predict or forecast <span class="math inline">\(Y\)</span>. We can write any predictor as a function <span class="math inline">\(g(X)\)</span> of <span class="math inline">\(X\)</span>. The (ex-post) prediction error is the realized difference <span class="math inline">\(Y-g(X)\)</span>. A non-stochastic measure of the magnitude of the prediction error is the expectation of its square</p>
<p><span class="math display">\[
\mathbb{E}\left[(Y-g(X))^{2}\right] .
\]</span></p>
<p>We can define the best predictor as the function <span class="math inline">\(g(X)\)</span> which minimizes (2.10). What function is the best predictor? It turns out that the answer is the CEF <span class="math inline">\(m(X)\)</span>. This holds regardless of the joint distribution of <span class="math inline">\((Y, X)\)</span>.</p>
<p>To see this, note that the mean squared error of a predictor <span class="math inline">\(g(X)\)</span> is</p>
<p><span class="math display">\[
\begin{aligned}
\mathbb{E}\left[(Y-g(X))^{2}\right] &amp;=\mathbb{E}\left[(e+m(X)-g(X))^{2}\right] \\
&amp;=\mathbb{E}\left[e^{2}\right]+2 \mathbb{E}[e(m(X)-g(X))]+\mathbb{E}\left[(m(X)-g(X))^{2}\right] \\
&amp;=\mathbb{E}\left[e^{2}\right]+\mathbb{E}\left[(m(X)-g(X))^{2}\right] \\
&amp; \geq \mathbb{E}\left[e^{2}\right] \\
&amp;=\mathbb{E}\left[(Y-m(X))^{2}\right] .
\end{aligned}
\]</span></p>
<p>The first equality makes the substitution <span class="math inline">\(Y=m(X)+e\)</span> and the third equality uses Theorem 2.4.4. The right-hand-side after the third equality is minimized by setting <span class="math inline">\(g(X)=m(X)\)</span>, yielding the inequality in the fourth line. The minimum is finite under the assumption <span class="math inline">\(\mathbb{E}\left[Y^{2}\right]&lt;\infty\)</span> as shown by Theorem <span class="math inline">\(2.5\)</span>.</p>
<p>We state this formally in the following result.</p>
<p>Theorem 2.7 Conditional Expectation as Best Predictor If <span class="math inline">\(\mathbb{E}\left[Y^{2}\right]&lt;\infty\)</span>, then for any predictor <span class="math inline">\(g(X)\)</span>,</p>
<p><span class="math display">\[
\mathbb{E}\left[(Y-g(X))^{2}\right] \geq \mathbb{E}\left[(Y-m(X))^{2}\right]
\]</span></p>
<p>where <span class="math inline">\(m(X)=\mathbb{E}[Y \mid X]\)</span></p>
<p>It may be helpful to consider this result in the context of the intercept-only model</p>
<p><span class="math display">\[
\begin{aligned}
Y &amp;=\mu+e \\
\mathbb{E}[e] &amp;=0 .
\end{aligned}
\]</span></p>
<p>Theorem <span class="math inline">\(2.7\)</span> shows that the best predictor for <span class="math inline">\(Y\)</span> (in the class of constants) is the unconditional mean <span class="math inline">\(\mu=\mathbb{E}[Y]\)</span> in the sense that the mean minimizes the mean squared prediction error.</p>
</section>
<section id="conditional-variance" class="level2" data-number="2.12">
<h2 data-number="2.12" class="anchored" data-anchor-id="conditional-variance"><span class="header-section-number">2.12</span> Conditional Variance</h2>
<p>While the conditional mean is a good measure of the location of a conditional distribution it does not provide information about the spread of the distribution. A common measure of the dispersion is the conditional variance. We first give the general definition of the conditional variance of a random variable <span class="math inline">\(Y\)</span>.</p>
<p>Definition 2.1 If <span class="math inline">\(\mathbb{E}\left[Y^{2}\right]&lt;\infty\)</span>, the conditional variance of <span class="math inline">\(Y\)</span> given <span class="math inline">\(X=x\)</span> is</p>
<p><span class="math display">\[
\sigma^{2}(x)=\operatorname{var}[Y \mid X=x]=\mathbb{E}\left[(Y-\mathbb{E}[Y \mid X=x])^{2} \mid X=x\right] .
\]</span></p>
<p>The conditional variance treated as a random variable is <span class="math inline">\(\operatorname{var}[Y \mid X]=\sigma^{2}(X)\)</span>.</p>
<p>The conditional variance is distinct from the unconditional variance var <span class="math inline">\([Y]\)</span>. The difference is that the conditional variance is a function of the conditioning variables. Notice that the conditional variance is the conditional second moment, centered around the conditional first moment.</p>
<p>Given this definition we define the conditional variance of the regression error.</p>
<p>Definition 2.2 If <span class="math inline">\(\mathbb{E}\left[e^{2}\right]&lt;\infty\)</span>, the conditional variance of the regression error <span class="math inline">\(e\)</span> given <span class="math inline">\(X=x\)</span> is</p>
<p><span class="math display">\[
\sigma^{2}(x)=\operatorname{var}[e \mid X=x]=\mathbb{E}\left[e^{2} \mid X=x\right] .
\]</span></p>
<p>The conditional variance of <span class="math inline">\(e\)</span> treated as a random variable is <span class="math inline">\(\operatorname{var}[e \mid X]=\sigma^{2}(X)\)</span>.</p>
<p>Again, the conditional variance <span class="math inline">\(\sigma^{2}(x)\)</span> is distinct from the unconditional variance <span class="math inline">\(\sigma^{2}\)</span>. The conditional variance is a function of the regressors, the unconditional variance is not. Generally, <span class="math inline">\(\sigma^{2}(x)\)</span> is a non-trivial function of <span class="math inline">\(x\)</span> and can take any form subject to the restriction that it is non-negative. One way to think about <span class="math inline">\(\sigma^{2}(x)\)</span> is that it is the conditional mean of <span class="math inline">\(e^{2}\)</span> given <span class="math inline">\(X\)</span>. Notice as well that <span class="math inline">\(\sigma^{2}(x)=\operatorname{var}[Y \mid X=x]\)</span> so it is equivalently the conditional variance of the dependent variable.</p>
<p>The variance of <span class="math inline">\(Y\)</span> is in a different unit of measurement than <span class="math inline">\(Y\)</span>. To convert the variance to the same unit of measure we define the conditional standard deviation as its square root <span class="math inline">\(\sigma(x)=\sqrt{\sigma^{2}(x)}\)</span>.</p>
<p>As an example of how the conditional variance depends on observables, compare the conditional log wage densities for men and women displayed in Figure 2.2. The difference between the densities is not purely a location shift but is also a difference in spread. Specifically, we can see that the density for men’s log wages is somewhat more spread out than that for women, while the density for women’s wages is somewhat more peaked. Indeed, the conditional standard deviation for men’s wages is <span class="math inline">\(3.05\)</span> and that for women is <span class="math inline">\(2.81\)</span>. So while men have higher average wages they are also somewhat more dispersed.</p>
<p>The unconditional variance is related to the conditional variance by the following identity.</p>
<p>Theorem 2.8 If <span class="math inline">\(\mathbb{E}\left[Y^{2}\right]&lt;\infty\)</span> then</p>
<p><span class="math display">\[
\operatorname{var}[Y]=\mathbb{E}[\operatorname{var}[Y \mid X]]+\operatorname{var}[\mathbb{E}[Y \mid X]] .
\]</span></p>
<p>See Theorem <span class="math inline">\(4.14\)</span> of Probability and Statistics for Economists. Theorem <span class="math inline">\(2.8\)</span> decomposes the unconditional variance into what are sometimes called the “within group variance” and the “across group variance”. For example, if <span class="math inline">\(X\)</span> is education level, then the first term is the expected variance of the conditional expectation by education level. The second term is the variance after controlling for education.</p>
<p>The regression error has a conditional mean of zero, so its unconditional error variance equals the expected conditional variance, or equivalently can be found by the law of iterated expectations.</p>
<p><span class="math display">\[
\sigma^{2}=\mathbb{E}\left[e^{2}\right]=\mathbb{E}\left[\mathbb{E}\left[e^{2} \mid X\right]\right]=\mathbb{E}\left[\sigma^{2}(X)\right] .
\]</span></p>
<p>That is, the unconditional error variance is the average conditional variance.</p>
<p>Given the conditional variance we can define a rescaled error</p>
<p><span class="math display">\[
u=\frac{e}{\sigma(X)} \text {. }
\]</span></p>
<p>We calculate that since <span class="math inline">\(\sigma(X)\)</span> is a function of <span class="math inline">\(X\)</span></p>
<p><span class="math display">\[
\mathbb{E}[u \mid X]=\mathbb{E}\left[\frac{e}{\sigma(X)} \mid X\right]=\frac{1}{\sigma(X)} \mathbb{E}[e \mid X]=0
\]</span></p>
<p>and</p>
<p><span class="math display">\[
\operatorname{var}[u \mid X]=\mathbb{E}\left[u^{2} \mid X\right]=\mathbb{E}\left[\frac{e^{2}}{\sigma^{2}(X)} \mid X\right]=\frac{1}{\sigma^{2}(X)} \mathbb{E}\left[e^{2} \mid X\right]=\frac{\sigma^{2}(X)}{\sigma^{2}(X)}=1 .
\]</span></p>
<p>Thus <span class="math inline">\(u\)</span> has a conditional expectation of zero and a conditional variance of 1 .</p>
<p>Notice that (2.11) can be rewritten as</p>
<p><span class="math display">\[
e=\sigma(X) u .
\]</span></p>
<p>and substituting this for <span class="math inline">\(e\)</span> in the CEF equation (2.9), we find that</p>
<p><span class="math display">\[
Y=m(X)+\sigma(X) u .
\]</span></p>
<p>This is an alternative (mean-variance) representation of the CEF equation.</p>
<p>Many econometric studies focus on the conditional expectation <span class="math inline">\(m(x)\)</span> and either ignore the conditional variance <span class="math inline">\(\sigma^{2}(x)\)</span>, treat it as a constant <span class="math inline">\(\sigma^{2}(x)=\sigma^{2}\)</span>, or treat it as a nuisance parameter (a parameter not of primary interest). This is appropriate when the primary variation in the conditional distribution is in the mean but can be short-sighted in other cases. Dispersion is relevant to many economic topics, including income and wealth distribution, economic inequality, and price dispersion. Conditional dispersion (variance) can be a fruitful subject for investigation.</p>
<p>The perverse consequences of a narrow-minded focus on the mean is parodied in a classic joke:</p>
<p>An economist was standing with one foot in a bucket of boiling water and the other foot in a bucket of ice. When asked how he felt, he replied, “On average I feel just fine.”</p>
<p>Clearly, the economist in question ignored variance!</p>
</section>
<section id="homoskedasticity-and-heteroskedasticity" class="level2" data-number="2.13">
<h2 data-number="2.13" class="anchored" data-anchor-id="homoskedasticity-and-heteroskedasticity"><span class="header-section-number">2.13</span> Homoskedasticity and Heteroskedasticity</h2>
<p>An important special case obtains when the conditional variance <span class="math inline">\(\sigma^{2}(x)\)</span> is a constant and independent of <span class="math inline">\(x\)</span>. This is called homoskedasticity.</p>
<p>Definition 2.3 The error is homoskedastic if <span class="math inline">\(\sigma^{2}(x)=\sigma^{2}\)</span> does not depend on <span class="math inline">\(x\)</span>.</p>
<p>In the general case where <span class="math inline">\(\sigma^{2}(x)\)</span> depends on <span class="math inline">\(x\)</span> we say that the error <span class="math inline">\(e\)</span> is heteroskedastic.</p>
<p>Definition 2.4 The error is heteroskedastic if <span class="math inline">\(\sigma^{2}(x)\)</span> depends on <span class="math inline">\(x\)</span>.</p>
<p>It is helpful to understand that the concepts homoskedasticity and heteroskedasticity concern the conditional variance, not the unconditional variance. By definition, the unconditional variance <span class="math inline">\(\sigma^{2}\)</span> is a constant and independent of the regressors <span class="math inline">\(X\)</span>. So when we talk about the variance as a function of the regressors we are talking about the conditional variance <span class="math inline">\(\sigma^{2}(x)\)</span>.</p>
<p>Some older or introductory textbooks describe heteroskedasticity as the case where “the variance of <span class="math inline">\(e\)</span> varies across observations”. This is a poor and confusing definition. It is more constructive to understand that heteroskedasticity means that the conditional variance <span class="math inline">\(\sigma^{2}(x)\)</span> depends on observables.</p>
<p>Older textbooks also tend to describe homoskedasticity as a component of a correct regression specification and describe heteroskedasticity as an exception or deviance. This description has influenced many generations of economists but it is unfortunately backwards. The correct view is that heteroskedasticity is generic and “standard”, while homoskedasticity is unusual and exceptional. The default in empirical work should be to assume that the errors are heteroskedastic, not the converse.</p>
<p>In apparent contradiction to the above statement we will still frequently impose the homoskedasticity assumption when making theoretical investigations into the properties of estimation and inference methods. The reason is that in many cases homoskedasticity greatly simplifies the theoretical calculations and it is therefore quite advantageous for teaching and learning. It should always be remembered, however, that homoskedasticity is never imposed because it is believed to be a correct feature of an empirical model but rather because of its simplicity.</p>
</section>
<section id="heteroskedastic-or-heteroscedastic" class="level2" data-number="2.14">
<h2 data-number="2.14" class="anchored" data-anchor-id="heteroskedastic-or-heteroscedastic"><span class="header-section-number">2.14</span> Heteroskedastic or Heteroscedastic?</h2>
<p>The spelling of the words homoskedastic and heteroskedastic have been somewhat controversial. Early econometrics textbooks were split, with some using a “c” as in heteroscedastic and some ” <span class="math inline">\(\mathrm{k}\)</span> ” as in heteroskedastic. McCulloch (1985) pointed out that the word is derived from Greek roots.</p>
<p><img src="images//2022_09_17_efa0deee3441d06e0b66g-17.jpg" class="img-fluid">\ means “to scatter”. Since the proper transliteration of the Greek letter <span class="math inline">\(\kappa\)</span> in <span class="math inline">\(\sigma \kappa \varepsilon \delta \alpha v v v \mu \iota\)</span> is ” <span class="math inline">\(\mathrm{k}\)</span> “, this implies that the correct English spelling of the two words is with a” <span class="math inline">\(\mathrm{k}\)</span> ” as in homoskedastic and heteroskedastic.</p>
</section>
<section id="regression-derivative" class="level2" data-number="2.15">
<h2 data-number="2.15" class="anchored" data-anchor-id="regression-derivative"><span class="header-section-number">2.15</span> Regression Derivative</h2>
<p>One way to interpret the CEF <span class="math inline">\(m(x)=\mathbb{E}[Y \mid X=x]\)</span> is in terms of how marginal changes in the regressors <span class="math inline">\(X\)</span> imply changes in the conditional expectation of the response variable <span class="math inline">\(Y\)</span>. It is typical to consider marginal changes in a single regressor, say <span class="math inline">\(X_{1}\)</span>, holding the remainder fixed. When a regressor <span class="math inline">\(X_{1}\)</span> is continuously distributed, we define the marginal effect of a change in <span class="math inline">\(X_{1}\)</span>, holding the variables <span class="math inline">\(X_{2}, \ldots, X_{k}\)</span> fixed, as the partial derivative of the CEF</p>
<p><span class="math display">\[
\frac{\partial}{\partial x_{1}} m\left(x_{1}, \ldots, x_{k}\right)
\]</span></p>
<p>When <span class="math inline">\(X_{1}\)</span> is discrete we define the marginal effect as a discrete difference. For example, if <span class="math inline">\(X_{1}\)</span> is binary, then the marginal effect of <span class="math inline">\(X_{1}\)</span> on the CEF is</p>
<p><span class="math display">\[
m\left(1, x_{2}, \ldots, x_{k}\right)-m\left(0, x_{2}, \ldots, x_{k}\right)
\]</span></p>
<p>We can unify the continuous and discrete cases with the notation</p>
<p><span class="math display">\[
\nabla_{1} m(x)=\left\{\begin{array}{cc}
\frac{\partial}{\partial x_{1}} m\left(x_{1}, \ldots, x_{k}\right), &amp; \text { if } X_{1} \text { is continuous } \\
m\left(1, x_{2}, \ldots, x_{k}\right)-m\left(0, x_{2}, \ldots, x_{k}\right), &amp; \text { if } X_{1} \text { is binary. }
\end{array}\right.
\]</span></p>
<p>Collecting the <span class="math inline">\(k\)</span> effects into one <span class="math inline">\(k \times 1\)</span> vector, we define the regression derivative with respect to <span class="math inline">\(X\)</span> :</p>
<p><span class="math display">\[
\nabla m(x)=\left[\begin{array}{c}
\nabla_{1} m(x) \\
\nabla_{2} m(x) \\
\vdots \\
\nabla_{k} m(x)
\end{array}\right]
\]</span></p>
<p>When all elements of <span class="math inline">\(X\)</span> are continuous, then we have the simplification <span class="math inline">\(\nabla m(x)=\frac{\partial}{\partial x} m(x)\)</span>, the vector of partial derivatives.</p>
<p>There are two important points to remember concerning our definition of the regression derivative. First, the effect of each variable is calculated holding the other variables constant. This is the ceteris paribus concept commonly used in economics. But in the case of a regression derivative, the conditional expectation does not literally hold all else constant. It only holds constant the variables included in the conditional expectation. This means that the regression derivative depends on which regressors are included. For example, in a regression of wages on education, experience, race and gender, the regression derivative with respect to education shows the marginal effect of education on expected wages, holding constant experience, race, and gender. But it does not hold constant an individual’s unobservable characteristics (such as ability), nor variables not included in the regression (such as the quality of education).</p>
<p>Second, the regression derivative is the change in the conditional expectation of <span class="math inline">\(Y\)</span>, not the change in the actual value of <span class="math inline">\(Y\)</span> for an individual. It is tempting to think of the regression derivative as the change in the actual value of <span class="math inline">\(Y\)</span>, but this is not a correct interpretation. The regression derivative <span class="math inline">\(\nabla m(x)\)</span> is the change in the actual value of <span class="math inline">\(Y\)</span> only if the error <span class="math inline">\(e\)</span> is unaffected by the change in the regressor <span class="math inline">\(X\)</span>. We return to a discussion of causal effects in Section 2.30.</p>
</section>
<section id="linear-cef" class="level2" data-number="2.16">
<h2 data-number="2.16" class="anchored" data-anchor-id="linear-cef"><span class="header-section-number">2.16</span> Linear CEF</h2>
<p>An important special case is when the CEF <span class="math inline">\(m(x)=\mathbb{E}[Y \mid X=x]\)</span> is linear in <span class="math inline">\(x\)</span>. In this case we can write the mean equation as</p>
<p><span class="math display">\[
m(x)=x_{1} \beta_{1}+x_{2} \beta_{2}+\cdots+x_{k} \beta_{k}+\beta_{k+1} .
\]</span></p>
<p>Notationally it is convenient to write this as a simple function of the vector <span class="math inline">\(x\)</span>. An easy way to do so is to augment the regressor vector <span class="math inline">\(X\)</span> by listing the number ” 1 ” as an element. We call this the “constant” and the corresponding coefficient is called the “intercept”. Equivalently, specify that the final element <span class="math inline">\({ }^{9}\)</span> of the vector <span class="math inline">\(x\)</span> is <span class="math inline">\(x_{k}=1\)</span>. Thus (2.4) has been redefined as the <span class="math inline">\(k \times 1\)</span> vector</p>
<p><span class="math display">\[
X=\left(\begin{array}{c}
X_{1} \\
X_{2} \\
\vdots \\
X_{k-1} \\
1
\end{array}\right)
\]</span></p>
<p>With this redefinition, the CEF is</p>
<p><span class="math display">\[
m(x)=x_{1} \beta_{1}+x_{2} \beta_{2}+\cdots+\beta_{k}=x^{\prime} \beta
\]</span></p>
<p>where</p>
<p><span class="math display">\[
\beta=\left(\begin{array}{c}
\beta_{1} \\
\vdots \\
\beta_{k}
\end{array}\right)
\]</span></p>
<p>is a <span class="math inline">\(k \times 1\)</span> coefficient vector. This is the linear CEF model. It is also often called the linear regression model, or the regression of <span class="math inline">\(Y\)</span> on <span class="math inline">\(X\)</span>.</p>
<p>In the linear CEF model the regression derivative is simply the coefficient vector. That is <span class="math inline">\(\nabla m(x)=\beta\)</span>. This is one of the appealing features of the linear CEF model. The coefficients have simple and natural interpretations as the marginal effects of changing one variable, holding the others constant.</p>
<p><span class="math display">\[
\begin{aligned}
&amp;\text { Linear CEF Model } \\
&amp;\begin{aligned}
Y &amp;=X^{\prime} \beta+e \\
\mathbb{E}[e \mid X] &amp;=0
\end{aligned}
\end{aligned}
\]</span></p>
<p>If in addition the error is homoskedastic we call this the homoskedastic linear CEF model.</p>
</section>
<section id="homoskedastic-linear-cef-model" class="level2" data-number="2.17">
<h2 data-number="2.17" class="anchored" data-anchor-id="homoskedastic-linear-cef-model"><span class="header-section-number">2.17</span> Homoskedastic Linear CEF Model</h2>
<p><span class="math display">\[
\begin{aligned}
Y &amp;=X^{\prime} \beta+e \\
\mathbb{E}[e \mid X] &amp;=0 \\
\mathbb{E}\left[e^{2} \mid X\right] &amp;=\sigma^{2}
\end{aligned}
\]</span></p>
<p><span class="math inline">\({ }^{9}\)</span> The order doesn’t matter. It could be any element.</p>
</section>
<section id="linear-cef-with-nonlinear-effects" class="level2" data-number="2.18">
<h2 data-number="2.18" class="anchored" data-anchor-id="linear-cef-with-nonlinear-effects"><span class="header-section-number">2.18</span> Linear CEF with Nonlinear Effects</h2>
<p>The linear CEF model of the previous section is less restrictive than it might appear, as we can include as regressors nonlinear transformations of the original variables. In this sense, the linear CEF framework is flexible and can capture many nonlinear effects.</p>
<p>For example, suppose we have two scalar variables <span class="math inline">\(X_{1}\)</span> and <span class="math inline">\(X_{2}\)</span>. The CEF could take the quadratic form</p>
<p><span class="math display">\[
m\left(x_{1}, x_{2}\right)=x_{1} \beta_{1}+x_{2} \beta_{2}+x_{1}^{2} \beta_{3}+x_{2}^{2} \beta_{4}+x_{1} x_{2} \beta_{5}+\beta_{6} .
\]</span></p>
<p>This equation is quadratic in the regressors <span class="math inline">\(\left(x_{1}, x_{2}\right)\)</span> yet linear in the coefficients <span class="math inline">\(\beta=\left(\beta_{1}, \ldots, \beta_{6}\right)^{\prime}\)</span>. We still call (2.14) a linear CEF because it is a linear function of the coefficients. At the same time, it has nonlinear effects because it is nonlinear in the underlying variables <span class="math inline">\(x_{1}\)</span> and <span class="math inline">\(x_{2}\)</span>. The key is to understand that (2.14) is quadratic in the variables <span class="math inline">\(\left(x_{1}, x_{2}\right)\)</span> yet linear in the coefficients <span class="math inline">\(\beta\)</span>.</p>
<p>To simplify the expression we define the transformations <span class="math inline">\(x_{3}=x_{1}^{2}, x_{4}=x_{2}^{2}, x_{5}=x_{1} x_{2}\)</span>, and <span class="math inline">\(x_{6}=1\)</span>, and redefine the regressor vector as <span class="math inline">\(x=\left(x_{1}, \ldots, x_{6}\right)^{\prime}\)</span>. With this redefinition, <span class="math inline">\(m\left(x_{1}, x_{2}\right)=x^{\prime} \beta\)</span> which is linear in <span class="math inline">\(\beta\)</span>. For most econometric purposes (estimation and inference on <span class="math inline">\(\beta\)</span> ) the linearity in <span class="math inline">\(\beta\)</span> is all that is important.</p>
<p>An exception is in the analysis of regression derivatives. In nonlinear equations such as (2.14) the regression derivative should be defined with respect to the original variables not with respect to the transformed variables. Thus</p>
<p><span class="math display">\[
\begin{aligned}
&amp;\frac{\partial}{\partial x_{1}} m\left(x_{1}, x_{2}\right)=\beta_{1}+2 x_{1} \beta_{3}+x_{2} \beta_{5} \\
&amp;\frac{\partial}{\partial x_{2}} m\left(x_{1}, x_{2}\right)=\beta_{2}+2 x_{2} \beta_{4}+x_{1} \beta_{5} .
\end{aligned}
\]</span></p>
<p>We see that in the model (2.14), the regression derivatives are not a simple coefficient, but are functions of several coefficients plus the levels of <span class="math inline">\(\left(x_{1}, x_{2}\right)\)</span>. Consequently it is difficult to interpret the coefficients individually. It is more useful to interpret them as a group.</p>
<p>We typically call <span class="math inline">\(\beta_{5}\)</span> the interaction effect. Notice that it appears in both regression derivative equations and has a symmetric interpretation in each. If <span class="math inline">\(\beta_{5}&gt;0\)</span> then the regression derivative with respect to <span class="math inline">\(x_{1}\)</span> is increasing in the level of <span class="math inline">\(x_{2}\)</span> (and the regression derivative with respect to <span class="math inline">\(x_{2}\)</span> is increasing in the level of <span class="math inline">\(x_{1}\)</span> ), while if <span class="math inline">\(\beta_{5}&lt;0\)</span> the reverse is true.</p>
</section>
<section id="linear-cef-with-dummy-variables" class="level2" data-number="2.19">
<h2 data-number="2.19" class="anchored" data-anchor-id="linear-cef-with-dummy-variables"><span class="header-section-number">2.19</span> Linear CEF with Dummy Variables</h2>
<p>When all regressors take a finite set of values it turns out the CEF can be written as a linear function of regressors.</p>
<p>This simplest example is a binary variable which takes only two distinct values. For example, in traditional data sets the variable gender takes only the values man and woman (or male and female). Binary variables are extremely common in econometric applications and are alternatively called dummy variables or indicator variables.</p>
<p>Consider the simple case of a single binary regressor. In this case the conditional expectation can only take two distinct values. For example,</p>
<p><span class="math display">\[
\mathbb{E}[Y \mid \text { gender }]=\left\{\begin{array}{llc}
\mu_{0} &amp; \text { if } \quad \text { gender }=\text { man } \\
\mu_{1} &amp; \text { if gender }=\text { woman. }
\end{array}\right.
\]</span></p>
<p>To facilitate a mathematical treatment we record dummy variables with the values <span class="math inline">\(\{0,1\}\)</span>. For example</p>
<p><span class="math display">\[
X_{1}=\left\{\begin{array}{llc}
0 &amp; \text { if } &amp; \text { gender }=\text { man } \\
1 &amp; \text { if } &amp; \text { gender }=\text { woman } .
\end{array}\right.
\]</span></p>
<p>Given this notation we write the conditional expectation as a linear function of the dummy variable <span class="math inline">\(X_{1}\)</span>. Thus <span class="math inline">\(\mathbb{E}\left[Y \mid X_{1}\right]=\beta_{1} X_{1}+\beta_{2}\)</span> where <span class="math inline">\(\beta_{1}=\mu_{1}-\mu_{0}\)</span> and <span class="math inline">\(\beta_{2}=\mu_{0}\)</span>. In this simple regression equation the intercept <span class="math inline">\(\beta_{2}\)</span> is equal to the conditional expectation of <span class="math inline">\(Y\)</span> for the <span class="math inline">\(X_{1}=0\)</span> subpopulation (men) and the slope <span class="math inline">\(\beta_{1}\)</span> is equal to the difference in the conditional expectations between the two subpopulations.</p>
<p>Alternatively, we could have defined <span class="math inline">\(X_{1}\)</span> as</p>
<p><span class="math display">\[
X_{1}= \begin{cases}1 &amp; \text { if } \quad \text { gender }=\text { man } \\ 0 &amp; \text { if } \quad \text { gender }=\text { woman } .\end{cases}
\]</span></p>
<p>In this case, the regression intercept is the expectation for women (rather than for men) and the regression slope has switched signs. The two regressions are equivalent but the interpretation of the coefficients has changed. Therefore it is always important to understand the precise definitions of the variables, and illuminating labels are helpful. For example, labelling <span class="math inline">\(X_{1}\)</span> as “gender” does not help distinguish between definitions (2.15) and (2.16). Instead, it is better to label <span class="math inline">\(X_{1}\)</span> as “women” or “female” if definition (2.15) is used, or as “men” or “male” if (2.16) is used.</p>
<p>Now suppose we have two dummy variables <span class="math inline">\(X_{1}\)</span> and <span class="math inline">\(X_{2}\)</span>. For example, <span class="math inline">\(X_{2}=1\)</span> if the person is married, else <span class="math inline">\(X_{2}=0\)</span>. The conditional expectation given <span class="math inline">\(X_{1}\)</span> and <span class="math inline">\(X_{2}\)</span> takes at most four possible values:</p>
<p><img src="images//2022_09_17_efa0deee3441d06e0b66g-21.jpg" class="img-fluid"></p>
<p>In this case we can write the conditional mean as a linear function of <span class="math inline">\(X, X_{2}\)</span> and their product <span class="math inline">\(X_{1} X_{2}\)</span> :</p>
<p><span class="math display">\[
\mathbb{E}\left[Y \mid X_{1}, X_{2}\right]=\beta_{1} X_{1}+\beta_{2} X_{2}+\beta_{3} X_{1} X_{2}+\beta_{4}
\]</span></p>
<p>where <span class="math inline">\(\beta_{1}=\mu_{10}-\mu_{00}, \beta_{2}=\mu_{01}-\mu_{00}, \beta_{3}=\mu_{11}-\mu_{10}-\mu_{01}+\mu_{00}\)</span>, and <span class="math inline">\(\beta_{4}=\mu_{00}\)</span>.</p>
<p>We can view the coefficient <span class="math inline">\(\beta_{1}\)</span> as the effect of gender on expected log wages for unmarried wage earners, the coefficient <span class="math inline">\(\beta_{2}\)</span> as the effect of marriage on expected log wages for men wage earners, and the coefficient <span class="math inline">\(\beta_{3}\)</span> as the difference between the effects of marriage on expected log wages among women and among men. Alternatively, it can also be interpreted as the difference between the effects of gender on expected log wages among married and non-married wage earners. Both interpretations are equally valid. We often describe <span class="math inline">\(\beta_{3}\)</span> as measuring the interaction between the two dummy variables, or the interaction effect, and describe <span class="math inline">\(\beta_{3}=0\)</span> as the case when the interaction effect is zero.</p>
<p>In this setting we can see that the CEF is linear in the three variables <span class="math inline">\(\left(X_{1}, X_{2}, X_{1} X_{2}\right)\)</span>. To put the model in the framework of Section <span class="math inline">\(2.15\)</span> we define the regressor <span class="math inline">\(X_{3}=X_{1} X_{2}\)</span> and the regressor vector as</p>
<p><span class="math display">\[
X=\left(\begin{array}{c}
X_{1} \\
X_{2} \\
X_{3} \\
1
\end{array}\right) .
\]</span></p>
<p>So while we started with two dummy variables, the number of regressors (including the intercept) is four.</p>
<p>If there are three dummy variables <span class="math inline">\(X_{1}, X_{2}, X_{3}\)</span>, then <span class="math inline">\(\mathbb{E}\left[Y \mid X_{1}, X_{2}, X_{3}\right]\)</span> takes at most <span class="math inline">\(2^{3}=8\)</span> distinct values and can be written as the linear function</p>
<p><span class="math display">\[
\mathbb{E}\left[Y \mid X_{1}, X_{2}, X_{3}\right]=\beta_{1} X_{1}+\beta_{2} X_{2}+\beta_{3} X_{3}+\beta_{4} X_{1} X_{2}+\beta_{5} X_{1} X_{3}+\beta_{6} X_{2} X_{3}+\beta_{7 X 1} X_{2} X_{3}+\beta_{8}
\]</span></p>
<p>which has eight regressors including the intercept. In general, if there are <span class="math inline">\(p\)</span> dummy variables <span class="math inline">\(X_{1}, \ldots, X_{p}\)</span> then the CEF <span class="math inline">\(\mathbb{E}\left[Y \mid X_{1}, X_{2}, \ldots, X_{p}\right]\)</span> takes at most <span class="math inline">\(2^{p}\)</span> distinct values and can be written as a linear function of the <span class="math inline">\(2^{p}\)</span> regressors including <span class="math inline">\(X_{1}, X_{2}, \ldots, X_{p}\)</span> and all cross-products. A linear regression model which includes all <span class="math inline">\(2^{p}\)</span> binary interactions is called a saturated dummy variable regression model. It is a complete model of the conditional expectation. In contrast, a model with no interactions equals</p>
<p><span class="math display">\[
\mathbb{E}\left[Y \mid X_{1}, X_{2}, \ldots, X_{p}\right]=\beta_{1} X_{1}+\beta_{2} X_{2}+\cdots+\beta_{p} X_{p}+\beta_{p} .
\]</span></p>
<p>This has <span class="math inline">\(p+1\)</span> coefficients instead of <span class="math inline">\(2^{p}\)</span>.</p>
<p>We started this section by saying that the conditional expectation is linear whenever all regressors take only a finite number of possible values. How can we see this? Take a categorical variable, such as race. For example, we earlier divided race into three categories. We can record categorical variables using numbers to indicate each category, for example</p>
<p><span class="math display">\[
X_{3}=\left\{\begin{array}{lll}
1 &amp; \text { if } &amp; \text { white } \\
2 &amp; \text { if } &amp; \text { Black } \\
3 &amp; \text { if } &amp; \text { other. }
\end{array}\right.
\]</span></p>
<p>When doing so, the values of <span class="math inline">\(X_{3}\)</span> have no meaning in terms of magnitude, they simply indicate the relevant category.</p>
<p>When the regressor is categorical the conditional expectation of <span class="math inline">\(Y\)</span> given <span class="math inline">\(X_{3}\)</span> takes a distinct value for each possibility:</p>
<p><span class="math display">\[
\mathbb{E}\left[Y \mid X_{3}\right]=\left\{\begin{array}{lll}
\mu_{1} &amp; \text { if } &amp; X_{3}=1 \\
\mu_{2} &amp; \text { if } &amp; X_{3}=2 \\
\mu_{3} &amp; \text { if } &amp; X_{3}=3 .
\end{array}\right.
\]</span></p>
<p>This is not a linear function of <span class="math inline">\(X_{3}\)</span> itself, but it can be made a linear function by constructing dummy variables for two of the three categories. For example</p>
<p><span class="math display">\[
\begin{aligned}
&amp;X_{4}=\left\{\begin{array}{llc}
1 &amp; \text { if } &amp; \text { Black } \\
0 &amp; \text { if } &amp; \text { not Black }
\end{array}\right. \\
&amp;X_{5}=\left\{\begin{array}{lll}
1 &amp; \text { if } &amp; \text { other } \\
0 &amp; \text { if } &amp; \text { not other. }
\end{array}\right.
\end{aligned}
\]</span></p>
<p>In this case, the categorical variable <span class="math inline">\(X_{3}\)</span> is equivalent to the pair of dummy variables <span class="math inline">\(\left(X_{4}, X_{5}\right)\)</span>. The explicit relationship is</p>
<p><span class="math display">\[
X_{3}=\left\{\begin{array}{lll}
1 &amp; \text { if } &amp; X_{4}=0 \text { and } X_{5}=0 \\
2 &amp; \text { if } &amp; X_{4}=1 \text { and } X_{5}=0 \\
3 &amp; \text { if } &amp; X_{4}=0 \text { and } X_{5}=1
\end{array}\right.
\]</span></p>
<p>Given these transformations, we can write the conditional expectation of <span class="math inline">\(Y\)</span> as a linear function of <span class="math inline">\(X_{4}\)</span> and <span class="math inline">\(X_{5}\)</span></p>
<p><span class="math display">\[
\mathbb{E}\left[Y \mid X_{3}\right]=\mathbb{E}\left[Y \mid X_{4}, X_{5}\right]=\beta_{1} X_{4}+\beta_{2} X_{5}+\beta_{3} .
\]</span></p>
<p>We can write the CEF as either <span class="math inline">\(\mathbb{E}\left[Y \mid X_{3}\right]\)</span> or <span class="math inline">\(\mathbb{E}\left[Y \mid X_{4}, X_{5}\right]\)</span> (they are equivalent), but it is only linear as a function of <span class="math inline">\(X_{4}\)</span> and <span class="math inline">\(X_{5}\)</span>.</p>
<p>This setting is similar to the case of two dummy variables, with the difference that we have not included the interaction term <span class="math inline">\(X_{4} X_{5}\)</span>. This is because the event <span class="math inline">\(\left\{X_{4}=1\right.\)</span> and <span class="math inline">\(\left.X_{5}=1\right\}\)</span> is empty by construction, so <span class="math inline">\(X_{4} X_{5}=0\)</span> by definition.</p>
</section>
<section id="best-linear-predictor" class="level2" data-number="2.20">
<h2 data-number="2.20" class="anchored" data-anchor-id="best-linear-predictor"><span class="header-section-number">2.20</span> Best Linear Predictor</h2>
<p>While the conditional expectation <span class="math inline">\(m(X)=\mathbb{E}[Y \mid X]\)</span> is the best predictor of <span class="math inline">\(Y\)</span> among all functions of <span class="math inline">\(X\)</span>, its functional form is typically unknown. In particular, the linear CEF model is empirically unlikely to be accurate unless <span class="math inline">\(X\)</span> is discrete and low-dimensional so all interactions are included. Consequently, in most cases it is more realistic to view the linear specification (2.13) as an approximation. In this section we derive a specific approximation with a simple interpretation.</p>
<p>Theorem <span class="math inline">\(2.7\)</span> showed that the conditional expectation <span class="math inline">\(m(X)\)</span> is the best predictor in the sense that it has the lowest mean squared error among all predictors. By extension, we can define an approximation to the CEF by the linear function with the lowest mean squared error among all linear predictors.</p>
<p>For this derivation we require the following regularity condition.</p>
<p>Assumption <span class="math inline">\(2.1\)</span></p>
<ol type="1">
<li><span class="math inline">\(\mathbb{E}\left[Y^{2}\right]&lt;\infty\)</span></li>
<li><span class="math inline">\(\mathbb{E}\|X\|^{2}&lt;\infty\)</span></li>
<li><span class="math inline">\(\boldsymbol{Q}_{X X}=\mathbb{E}\left[X X^{\prime}\right]\)</span> is positive definite.</li>
</ol>
<p>In Assumption 2.1.2 we use <span class="math inline">\(\|x\|=\left(x^{\prime} x\right)^{1 / 2}\)</span> to denote the Euclidean length of the vector <span class="math inline">\(x\)</span>.</p>
<p>The first two parts of Assumption <span class="math inline">\(2.1\)</span> imply that the variables <span class="math inline">\(Y\)</span> and <span class="math inline">\(X\)</span> have finite means, variances, and covariances. The third part of the assumption is more technical, and its role will become apparent shortly. It is equivalent to imposing that the columns of the matrix <span class="math inline">\(\boldsymbol{Q}_{X X}=\mathbb{E}\left[X X^{\prime}\right]\)</span> are linearly independent and that the matrix is invertible.</p>
<p>A linear predictor for <span class="math inline">\(Y\)</span> is a function <span class="math inline">\(X^{\prime} \beta\)</span> for some <span class="math inline">\(\beta \in \mathbb{R}^{k}\)</span>. The mean squared prediction error is</p>
<p><span class="math display">\[
S(\beta)=\mathbb{E}\left[\left(Y-X^{\prime} \beta\right)^{2}\right] .
\]</span></p>
<p>The best linear predictor of <span class="math inline">\(Y\)</span> given <span class="math inline">\(X\)</span>, written <span class="math inline">\(\mathscr{P}[Y \mid X]\)</span>, is found by selecting the <span class="math inline">\(\beta\)</span> which minimizes <span class="math inline">\(S(\beta)\)</span>.</p>
<p>Definition 2.5 The Best Linear Predictor of <span class="math inline">\(Y\)</span> given <span class="math inline">\(X\)</span> is</p>
<p><span class="math display">\[
\mathscr{P}[Y \mid X]=X^{\prime} \beta
\]</span></p>
<p>where <span class="math inline">\(\beta\)</span> minimizes the mean squared prediction error</p>
<p><span class="math display">\[
S(\beta)=\mathbb{E}\left[\left(Y-X^{\prime} \beta\right)^{2}\right] .
\]</span></p>
<p>The minimizer</p>
<p><span class="math display">\[
\beta=\underset{b \in \mathbb{R}^{k}}{\operatorname{argmin}} S(b)
\]</span></p>
<p>is called the Linear Projection Coefficient. We now calculate an explicit expression for its value. The mean squared prediction error (2.17) can be written out as a quadratic function of <span class="math inline">\(\beta\)</span> :</p>
<p><span class="math display">\[
S(\beta)=\mathbb{E}\left[Y^{2}\right]-2 \beta^{\prime} \mathbb{E}[X Y]+\beta^{\prime} \mathbb{E}\left[X X^{\prime}\right] \beta .
\]</span></p>
<p>The quadratic structure of <span class="math inline">\(S(\beta)\)</span> means that we can solve explicitly for the minimizer. The first-order condition for minimization (from Appendix A.20) is</p>
<p><span class="math display">\[
0=\frac{\partial}{\partial \beta} S(\beta)=-2 \mathbb{E}[X Y]+2 \mathbb{E}\left[X X^{\prime}\right] \beta .
\]</span></p>
<p>Rewriting <span class="math inline">\((2.20)\)</span> as</p>
<p><span class="math display">\[
2 \mathbb{E}[X Y]=2 \mathbb{E}\left[X X^{\prime}\right] \beta
\]</span></p>
<p>and dividing by 2 , this equation takes the form</p>
<p><span class="math display">\[
\boldsymbol{Q}_{X Y}=\boldsymbol{Q}_{X X} \beta
\]</span></p>
<p>where <span class="math inline">\(\boldsymbol{Q}_{X Y}=\mathbb{E}[X Y]\)</span> is <span class="math inline">\(k \times 1\)</span> and <span class="math inline">\(\boldsymbol{Q}_{X X}=\mathbb{E}\left[X X^{\prime}\right]\)</span> is <span class="math inline">\(k \times k\)</span>. The solution is found by inverting the matrix <span class="math inline">\(\boldsymbol{Q}_{X X}\)</span>, and is written</p>
<p><span class="math display">\[
\beta=\boldsymbol{Q}_{X X}^{-1} \boldsymbol{Q}_{X Y}
\]</span></p>
<p>or</p>
<p><span class="math display">\[
\beta=\left(\mathbb{E}\left[X X^{\prime}\right]\right)^{-1} \mathbb{E}[X Y] .
\]</span></p>
<p>It is worth taking the time to understand the notation involved in the expression (2.22). <span class="math inline">\(\boldsymbol{Q}_{X X}\)</span> is a <span class="math inline">\(k \times k\)</span> matrix and <span class="math inline">\(\boldsymbol{Q}_{X Y}\)</span> is a <span class="math inline">\(k \times 1\)</span> column vector. Therefore, alternative expressions such as <span class="math inline">\(\frac{\mathbb{E}[X Y]}{\mathbb{E}\left[X X^{\prime}\right]}\)</span> or <span class="math inline">\(\mathbb{E}[X Y]\left(\mathbb{E}\left[X X^{\prime}\right]\right)^{-1}\)</span> are incoherent and incorrect. We also can now see the role of Assumption 2.1.3. It is equivalent to assuming that <span class="math inline">\(\boldsymbol{Q}_{X X}\)</span> has an inverse <span class="math inline">\(\boldsymbol{Q}_{X X}^{-1}\)</span> which is necessary for the solution to the normal equations (2.21) to be unique, and equivalently for <span class="math inline">\((2.22)\)</span> to be uniquely defined. In the absence of Assumption <span class="math inline">\(2.1 .3\)</span> there could be multiple solutions to the equation (2.21).</p>
<p>We now have an explicit expression for the best linear predictor:</p>
<p><span class="math display">\[
\mathscr{P}[Y \mid X]=X^{\prime}\left(\mathbb{E}\left[X X^{\prime}\right]\right)^{-1} \mathbb{E}[X Y] .
\]</span></p>
<p>This expression is also referred to as the linear projection of <span class="math inline">\(Y\)</span> on <span class="math inline">\(X\)</span>.</p>
<p>The projection error is</p>
<p><span class="math display">\[
e=Y-X^{\prime} \beta .
\]</span></p>
<p>This equals the error (2.9) from the regression equation when (and only when) the conditional expectation is linear in <span class="math inline">\(X\)</span>, otherwise they are distinct.</p>
<p>Rewriting, we obtain a decomposition of <span class="math inline">\(Y\)</span> into linear predictor and error</p>
<p><span class="math display">\[
Y=X^{\prime} \beta+e .
\]</span></p>
<p>In general, we call equation (2.24) or <span class="math inline">\(X^{\prime} \beta\)</span> the best linear predictor of <span class="math inline">\(Y\)</span> given <span class="math inline">\(X\)</span>, or the linear projection of <span class="math inline">\(Y\)</span> on <span class="math inline">\(X\)</span>. Equation (2.24) is also often called the regression of <span class="math inline">\(Y\)</span> on <span class="math inline">\(X\)</span> but this can sometimes be confusing as economists use the term “regression” in many contexts. (Recall that we said in Section <span class="math inline">\(2.15\)</span> that the linear CEF model is also called the linear regression model.)</p>
<p>An important property of the projection error <span class="math inline">\(e\)</span> is</p>
<p><span class="math display">\[
\mathbb{E}[X e]=0 .
\]</span></p>
<p>To see this, using the definitions (2.23) and (2.22) and the matrix properties <span class="math inline">\(\boldsymbol{A} \boldsymbol{A}^{-1}=\boldsymbol{I}\)</span> and <span class="math inline">\(\boldsymbol{I} \boldsymbol{a}=\boldsymbol{a}\)</span>,</p>
<p><span class="math display">\[
\begin{aligned}
\mathbb{E}[X e] &amp;=\mathbb{E}\left[X\left(Y-X^{\prime} \beta\right)\right] \\
&amp;=\mathbb{E}[X Y]-\mathbb{E}\left[X X^{\prime}\right]\left(\mathbb{E}\left[X X^{\prime}\right]\right)^{-1} \mathbb{E}[X Y] \\
&amp;=0
\end{aligned}
\]</span></p>
<p>as claimed.</p>
<p>Equation (2.25) is a set of <span class="math inline">\(k\)</span> equations, one for each regressor. In other words, (2.25) is equivalent to</p>
<p><span class="math display">\[
\mathbb{E}\left[X_{j} e\right]=0
\]</span></p>
<p>for <span class="math inline">\(j=1, \ldots, k\)</span>. As in (2.12), the regressor vector <span class="math inline">\(X\)</span> typically contains a constant, e.g.&nbsp;<span class="math inline">\(X_{k}=1\)</span>. In this case (2.27) for <span class="math inline">\(j=k\)</span> is the same as</p>
<p><span class="math display">\[
\mathbb{E}[e]=0 .
\]</span></p>
<p>Thus the projection error has a mean of zero when the regressor vector contains a constant. (When <span class="math inline">\(X\)</span> does not have a constant (2.28) is not guaranteed. As it is desirable for <span class="math inline">\(e\)</span> to have a zero mean this is a good reason to always include a constant in any regression model.)</p>
<p>It is also useful to observe that because <span class="math inline">\(\operatorname{cov}\left(X_{j}, e\right)=\mathbb{E}\left[X_{j} e\right]-\mathbb{E}\left[X_{j}\right] \mathbb{E}[e]\)</span>, then (2.27)-(2.28) together imply that the variables <span class="math inline">\(X_{j}\)</span> and <span class="math inline">\(e\)</span> are uncorrelated.</p>
<p>This completes the derivation of the model. We summarize some of the most important properties.</p>
<p>Theorem 2.9 Properties of Linear Projection Model Under Assumption 2.1,</p>
<ol type="1">
<li><p>The moments <span class="math inline">\(\mathbb{E}\left[X X^{\prime}\right]\)</span> and <span class="math inline">\(\mathbb{E}[X Y]\)</span> exist with finite elements.</p></li>
<li><p>The linear projection coefficient (2.18) exists, is unique, and equals</p></li>
</ol>
<p><span class="math display">\[
\beta=\left(\mathbb{E}\left[X X^{\prime}\right]\right)^{-1} \mathbb{E}[X Y] .
\]</span></p>
<p> 1. The best linear predictor of <span class="math inline">\(Y\)</span> given <span class="math inline">\(X\)</span> is</p>
<p><span class="math display">\[
\mathscr{P}(Y \mid X)=X^{\prime}\left(\mathbb{E}\left[X X^{\prime}\right]\right)^{-1} \mathbb{E}[X Y] .
\]</span></p>
<p> 1. The projection error <span class="math inline">\(e=Y-X^{\prime} \beta\)</span> exists. It satisfies <span class="math inline">\(\mathbb{E}\left[e^{2}\right]&lt;\infty\)</span> and <span class="math inline">\(\mathbb{E}[X e]=0\)</span>.</p>
<ol start="2" type="1">
<li><p>If <span class="math inline">\(X\)</span> contains an constant, then <span class="math inline">\(\mathbb{E}[e]=0\)</span>.</p></li>
<li><p>If <span class="math inline">\(\mathbb{E}|Y|^{r}&lt;\infty\)</span> and <span class="math inline">\(\mathbb{E}\|X\|^{r}&lt;\infty\)</span> for <span class="math inline">\(r \geq 2\)</span> then <span class="math inline">\(\mathbb{E}|e|^{r}&lt;\infty\)</span>.</p></li>
</ol>
<p>A complete proof of Theorem <span class="math inline">\(2.9\)</span> is given in Section 2.33.</p>
<p>It is useful to reflect on the generality of Theorem 2.9. The only restriction is Assumption 2.1. Thus for any random variables <span class="math inline">\((Y, X)\)</span> with finite variances we can define a linear equation (2.24) with the properties listed in Theorem 2.9. Stronger assumptions (such as the linear CEF model) are not necessary. In this sense the linear model (2.24) exists quite generally. However, it is important not to misinterpret the generality of this statement. The linear equation (2.24) is defined as the best linear predictor. It is not necessarily a conditional mean, nor a parameter of a structural or causal economic model. Linear Projection Model</p>
<p><span class="math display">\[
\begin{aligned}
Y &amp;=X^{\prime} \beta+e \\
\mathbb{E}[X e] &amp;=0 \\
\beta &amp;=\left(\mathbb{E}\left[X X^{\prime}\right]\right)^{-1} \mathbb{E}[X Y]
\end{aligned}
\]</span></p>
</section>
<section id="invertibility-and-identification" class="level2" data-number="2.21">
<h2 data-number="2.21" class="anchored" data-anchor-id="invertibility-and-identification"><span class="header-section-number">2.21</span> Invertibility and Identification</h2>
<p>The linear projection coefficient <span class="math inline">\(\beta=\left(\mathbb{E}\left[X X^{\prime}\right]\right)^{-1} \mathbb{E}[X Y]\)</span> exists and is unique as long as the <span class="math inline">\(k \times k\)</span> matrix <span class="math inline">\(\boldsymbol{Q}_{X X}=\mathbb{E}\left[X X^{\prime}\right]\)</span> is invertible. The matrix <span class="math inline">\(\boldsymbol{Q}_{X X}\)</span> is often called the design matrix as in experimental settings the researcher is able to control <span class="math inline">\(\boldsymbol{Q}_{X X}\)</span> by manipulating the distribution of the regressors <span class="math inline">\(X\)</span>.</p>
<p>Observe that for any non-zero <span class="math inline">\(\alpha \in \mathbb{R}^{k}\)</span>,</p>
<p><span class="math display">\[
\alpha^{\prime} \boldsymbol{Q}_{X X} \alpha=\mathbb{E}\left[\alpha^{\prime} X X^{\prime} \alpha\right]=\mathbb{E}\left[\left(\alpha^{\prime} X\right)^{2}\right] \geq 0
\]</span></p>
<p>so <span class="math inline">\(\boldsymbol{Q}_{X X}\)</span> by construction is positive semi-definite, conventionally written as <span class="math inline">\(\boldsymbol{Q}_{X X} \geq 0\)</span>. The assumption that it is positive definite means that this is a strict inequality, <span class="math inline">\(\mathbb{E}\left[\left(\alpha^{\prime} X\right)^{2}\right]&gt;0\)</span>. This is conventionally written as <span class="math inline">\(\boldsymbol{Q}_{X X}&gt;0\)</span>. This condition means that there is no non-zero vector <span class="math inline">\(\alpha\)</span> such that <span class="math inline">\(\alpha^{\prime} X=0\)</span> identically. Positive definite matrices are invertible. Thus when <span class="math inline">\(\boldsymbol{Q}_{X X}&gt;0\)</span> then <span class="math inline">\(\beta=\left(\mathbb{E}\left[X X^{\prime}\right]\right)^{-1} \mathbb{E}[X Y]\)</span> exists and is uniquely defined. In other words, if we can exclude the possibility that a linear function of <span class="math inline">\(X\)</span> is degenerate, then <span class="math inline">\(\beta\)</span> is uniquely defined.</p>
<p>Theorem <span class="math inline">\(2.5\)</span> shows that the linear projection coefficient <span class="math inline">\(\beta\)</span> is identified (uniquely determined) under Assumption 2.1. The key is invertibility of <span class="math inline">\(\boldsymbol{Q}_{X X}\)</span>. Otherwise, there is no unique solution to the equation</p>
<p><span class="math display">\[
\boldsymbol{Q}_{X X} \beta=\boldsymbol{Q}_{X Y} .
\]</span></p>
<p>When <span class="math inline">\(\boldsymbol{Q}_{X X}\)</span> is not invertible there are multiple solutions to (2.29). In this case the coefficient <span class="math inline">\(\beta\)</span> is not identified as it does not have a unique value.</p>
</section>
<section id="minimization" class="level2" data-number="2.22">
<h2 data-number="2.22" class="anchored" data-anchor-id="minimization"><span class="header-section-number">2.22</span> Minimization</h2>
<p>The mean squared prediction error (2.19) is a function with vector argument of the form</p>
<p><span class="math display">\[
f(x)=a-2 b^{\prime} x+x^{\prime} \boldsymbol{C} x
\]</span></p>
<p>where <span class="math inline">\(\boldsymbol{C}&gt;0\)</span>. For any function of this form, the unique minimizer is</p>
<p><span class="math display">\[
x=\boldsymbol{C}^{-1} b .
\]</span></p>
<p>To see that this is the unique minimizer we present two proofs. The first uses matrix calculus. From Appendix A.20</p>
<p><span class="math display">\[
\begin{gathered}
\frac{\partial}{\partial x}\left(b^{\prime} x\right)=b \\
\frac{\partial}{\partial x}\left(x^{\prime} \boldsymbol{C} x\right)=2 \boldsymbol{C} x \\
\frac{\partial^{2}}{\partial x \partial x^{\prime}}\left(x^{\prime} \boldsymbol{C} x\right)=2 \boldsymbol{C} .
\end{gathered}
\]</span></p>
<p>Using (2.31) and (2.32), we find</p>
<p><span class="math display">\[
\frac{\partial}{\partial x} f(x)=-2 b+2 \boldsymbol{C} x .
\]</span></p>
<p>The first-order condition for minimization sets this derivative equal to zero. Thus the solution satisfies <span class="math inline">\(-2 b+2 \boldsymbol{C} x=0\)</span>. Solving for <span class="math inline">\(x\)</span> we find (2.30). Using (2.33) we also find</p>
<p><span class="math display">\[
\frac{\partial^{2}}{\partial x \partial x^{\prime}} f(x)=2 \boldsymbol{C}&gt;0
\]</span></p>
<p>which is the second-order condition for minimization. This shows that (2.30) is the unique minimizer of <span class="math inline">\(f(x)\)</span>.</p>
<p>Our second proof is algebraic. Re-write <span class="math inline">\(f(x)\)</span> as</p>
<p><span class="math display">\[
f(x)=\left(a-b^{\prime} \boldsymbol{C}^{-1} b\right)+\left(x-\boldsymbol{C}^{-1} b\right)^{\prime} \boldsymbol{C}\left(x-\boldsymbol{C}^{-1} b\right) .
\]</span></p>
<p>The first term does not depend on <span class="math inline">\(x\)</span> so does not affect the minimizer. The second term is a quadratic form in a positive definite matrix. This means that for any non-zero <span class="math inline">\(\alpha, \alpha^{\prime} \boldsymbol{C} \alpha&gt;0\)</span>. Thus for <span class="math inline">\(x \neq C^{-1} b\)</span>, the second-term is strictly positive, yet for <span class="math inline">\(x=C^{-1} b\)</span> this term equals zero. It is therefore minimized at <span class="math inline">\(x=C^{-1} b\)</span> as claimed.</p>
</section>
<section id="illustrations-of-best-linear-predictor" class="level2" data-number="2.23">
<h2 data-number="2.23" class="anchored" data-anchor-id="illustrations-of-best-linear-predictor"><span class="header-section-number">2.23</span> Illustrations of Best Linear Predictor</h2>
<p>We illustrate the best linear predictor (projection) using three log wage equations introduced in earlier sections.</p>
<p>For our first example, we consider a model with the two dummy variables for gender and race similar to Table 2.1. As we learned in Section 2.17, the entries in this table can be equivalently expressed by a linear CEF. For simplicity, let’s consider the CEF of <span class="math inline">\(\log (\)</span> wage <span class="math inline">\()\)</span> as a function of Black and female.</p>
<p><span class="math display">\[
\mathbb{E}[\log (\text { wage }) \mid \text { Black, female }]=-0.20 \text { Black }-0.24 \text { female }+0.10 \text { Black } \times \text { female }+3.06 \text {. }
\]</span></p>
<p>This is a CEF as the variables are binary and all interactions are included.</p>
<p>Now consider a simpler model omitting the interaction effect. This is the linear projection on the variables Black and female</p>
<p><span class="math display">\[
\mathscr{P}[\log (\text { wage }) \mid \text { Black, female }]=-0.15 \text { Black }-0.23 \text { female }+3.06 .
\]</span></p>
<p>What is the difference? The full CEF (2.34) shows that the race gap is differentiated by gender: it is <span class="math inline">\(20 %\)</span> for Black men (relative to non-Black men) and <span class="math inline">\(10 %\)</span> for Black women (relative to non-Black women). The projection model (2.35) simplifies this analysis, calculating an average <span class="math inline">\(15 %\)</span> wage gap for Black wage earners, ignoring the role of gender. Notice that this is despite the fact that gender is included in (2.35).</p>
<p><img src="images//2022_09_17_efa0deee3441d06e0b66g-28.jpg" class="img-fluid"></p>
<ol type="a">
<li>Projections onto Education</li>
</ol>
<p><img src="images//2022_09_17_efa0deee3441d06e0b66g-28(1).jpg" class="img-fluid"></p>
<ol start="2" type="a">
<li>Projections onto Experience</li>
</ol>
<p>Figure 2.6: Projections of Log Wage onto Education and Experience</p>
<p>For our second example we consider the CEF of log wages as a function of years of education for white men which was illustrated in Figure <span class="math inline">\(2.3\)</span> and is repeated in Figure 2.6(a). Superimposed on the figure are two projections. The first (given by the dashed line) is the linear projection of log wages on years of education</p>
<p><span class="math display">\[
\mathscr{P}[\log (\text { wage }) \mid \text { education }]=0.11 \text { education }+1.5 \text {. }
\]</span></p>
<p>This simple equation indicates an average <span class="math inline">\(11 %\)</span> increase in wages for every year of education. An inspection of the Figure shows that this approximation works well for education <span class="math inline">\(\geq 9\)</span>, but under-predicts for individuals with lower levels of education. To correct this imbalance we use a linear spline equation which allows different rates of return above and below 9 years of education:</p>
<p><span class="math display">\[
\begin{aligned}
&amp;\mathscr{P}[\log (\text { wage }) \mid \text { education, }(\text { education }-9) \times \mathbb{1} \text { education }&gt;9\}] \\
&amp;=0.02 \text { education }+0.10 \times(\text { education }-9) \times \mathbb{1} \text { education }&gt;9\}+2.3 .
\end{aligned}
\]</span></p>
<p>This equation is displayed in Figure 2.6(a) using the solid line, and appears to fit much better. It indicates a <span class="math inline">\(2 %\)</span> increase in mean wages for every year of education below 9 , and a <span class="math inline">\(12 %\)</span> increase in mean wages for every year of education above 9 . It is still an approximation to the conditional mean but it appears to be fairly reasonable.</p>
<p>For our third example we take the CEF of log wages as a function of years of experience for white men with 12 years of education, which was illustrated in Figure <span class="math inline">\(2.4\)</span> and is repeated as the solid line in Figure 2.6(b). Superimposed on the figure are two projections. The first (given by the dot-dashed line) is the linear projection on experience</p>
<p><span class="math display">\[
\mathscr{P}[\log (\text { wage }) \mid \text { experience }]=0.011 \text { experience }+2.5
\]</span></p>
<p>and the second (given by the dashed line) is the linear projection on experience and its square</p>
<p><span class="math display">\[
\mathscr{P}[\log (\text { wage }) \mid \text { experience }]=0.046 \text { experience }-0.0007 \text { experience }^{2}+2.3 \text {. }
\]</span></p>
<p>It is fairly clear from an examination of Figure <span class="math inline">\(2.6(\mathrm{~b})\)</span> that the first linear projection is a poor approximation. It over-predicts wages for young and old workers, under-predicts for the rest, and misses the strong downturn in expected wages for older wage-earners. The second projection fits much better. We can call this equation a quadratic projection because the function is quadratic in experience.</p>
</section>
<section id="linear-predictor-error-variance" class="level2" data-number="2.24">
<h2 data-number="2.24" class="anchored" data-anchor-id="linear-predictor-error-variance"><span class="header-section-number">2.24</span> Linear Predictor Error Variance</h2>
<p>As in the CEF model, we define the error variance as <span class="math inline">\(\sigma^{2}=\mathbb{E}\left[e^{2}\right]\)</span>. Setting <span class="math inline">\(Q_{Y Y}=\mathbb{E}\left[Y^{2}\right]\)</span> and <span class="math inline">\(\boldsymbol{Q}_{Y X}=\)</span> <span class="math inline">\(\mathbb{E}\left[Y X^{\prime}\right]\)</span> we can write <span class="math inline">\(\sigma^{2}\)</span> as</p>
<p><span class="math display">\[
\begin{aligned}
\sigma^{2} &amp;=\mathbb{E}\left[\left(Y-X^{\prime} \beta\right)^{2}\right] \\
&amp;=\mathbb{E}\left[Y^{2}\right]-2 \mathbb{E}\left[Y X^{\prime}\right] \beta+\beta^{\prime} \mathbb{E}\left[X X^{\prime}\right] \beta \\
&amp;=Q_{Y Y}-2 \boldsymbol{Q}_{Y X} \boldsymbol{Q}_{X X}^{-1} \boldsymbol{Q}_{X Y}+\boldsymbol{Q}_{Y X} \boldsymbol{Q}_{X X}^{-1} \boldsymbol{Q}_{X X} \boldsymbol{Q}_{X X}^{-1} \boldsymbol{Q}_{X Y} \\
&amp;=Q_{Y Y}-\boldsymbol{Q}_{Y X} \boldsymbol{Q}_{X X}^{-1} \boldsymbol{Q}_{X Y} \\
&amp; \stackrel{\text { def }}{=} Q_{Y Y \cdot X} .
\end{aligned}
\]</span></p>
<p>One useful feature of this formula is that it shows that <span class="math inline">\(Q_{Y Y \cdot X}=Q_{Y Y}-\boldsymbol{Q}_{Y X} \boldsymbol{Q}_{X X}^{-1} \boldsymbol{Q}_{X Y}\)</span> equals the variance of the error from the linear projection of <span class="math inline">\(Y\)</span> on <span class="math inline">\(X\)</span>.</p>
</section>
<section id="regression-coefficients" class="level2" data-number="2.25">
<h2 data-number="2.25" class="anchored" data-anchor-id="regression-coefficients"><span class="header-section-number">2.25</span> Regression Coefficients</h2>
<p>Sometimes it is useful to separate the constant from the other regressors and write the linear projection equation in the format</p>
<p><span class="math display">\[
Y=X^{\prime} \beta+\alpha+e
\]</span></p>
<p>where <span class="math inline">\(\alpha\)</span> is the intercept and <span class="math inline">\(X\)</span> does not contain a constant.</p>
<p>Taking expectations of this equation, we find</p>
<p><span class="math display">\[
\mathbb{E}[Y]=\mathbb{E}\left[X^{\prime} \beta\right]+\mathbb{E}[\alpha]+\mathbb{E}[e]
\]</span></p>
<p>or <span class="math inline">\(\mu_{Y}=\mu_{X}^{\prime} \beta+\alpha\)</span> where <span class="math inline">\(\mu_{Y}=\mathbb{E}[Y]\)</span> and <span class="math inline">\(\mu_{X}=\mathbb{E}[X]\)</span>, since <span class="math inline">\(\mathbb{E}[e]=0\)</span> from (2.28). (While <span class="math inline">\(X\)</span> does not contain a constant, the equation does so (2.28) still applies.) Rearranging, we find <span class="math inline">\(\alpha=\mu_{Y}-\mu_{X}^{\prime} \beta\)</span>. Subtracting this equation from (2.37) we find</p>
<p><span class="math display">\[
Y-\mu_{Y}=\left(X-\mu_{X}\right)^{\prime} \beta+e,
\]</span></p>
<p>a linear equation between the centered variables <span class="math inline">\(Y-\mu_{Y}\)</span> and <span class="math inline">\(X-\mu_{X}\)</span>. (They are centered at their means so are mean-zero random variables.) Because <span class="math inline">\(X-\mu_{X}\)</span> is uncorrelated with <span class="math inline">\(e\)</span>, (2.38) is also a linear projection. Thus by the formula for the linear projection model,</p>
<p><span class="math display">\[
\begin{aligned}
\beta &amp;=\left(\mathbb{E}\left[\left(X-\mu_{X}\right)\left(X-\mu_{X}\right)^{\prime}\right]\right)^{-1} \mathbb{E}\left[\left(X-\mu_{X}\right)\left(Y-\mu_{Y}\right)\right] \\
&amp;=\operatorname{var}[X]^{-1} \operatorname{cov}(X, Y)
\end{aligned}
\]</span></p>
<p>a function only of the covariances <span class="math inline">\({ }^{10}\)</span> of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>.</p>
<p>Theorem 2.10 In the linear projection model <span class="math inline">\(Y=X^{\prime} \beta+\alpha+e\)</span>,</p>
<p><span class="math display">\[
\alpha=\mu_{Y}-\mu_{X}^{\prime} \beta
\]</span></p>
<p>and</p>
<p><span class="math display">\[
\beta=\operatorname{var}[X]^{-1} \operatorname{cov}(X, Y) .
\]</span></p>
</section>
<section id="regression-sub-vectors" class="level2" data-number="2.26">
<h2 data-number="2.26" class="anchored" data-anchor-id="regression-sub-vectors"><span class="header-section-number">2.26</span> Regression Sub-Vectors</h2>
<p>Let the regressors be partitioned as</p>
<p><span class="math display">\[
X=\left(\begin{array}{l}
X_{1} \\
X_{2}
\end{array}\right)
\]</span></p>
<p>We can write the projection of <span class="math inline">\(Y\)</span> on <span class="math inline">\(X\)</span> as</p>
<p><span class="math display">\[
\begin{aligned}
Y &amp;=X^{\prime} \beta+e \\
&amp;=X_{1}^{\prime} \beta_{1}+X_{2}^{\prime} \beta_{2}+e \\
\mathbb{E}[X e] &amp;=0 .
\end{aligned}
\]</span></p>
<p>In this section we derive formulae for the sub-vectors <span class="math inline">\(\beta_{1}\)</span> and <span class="math inline">\(\beta_{2}\)</span>.</p>
<p>Partition <span class="math inline">\(\boldsymbol{Q}_{X X}\)</span> conformably with <span class="math inline">\(X\)</span></p>
<p><span class="math display">\[
\boldsymbol{Q}_{X X}=\left[\begin{array}{ll}
\boldsymbol{Q}_{11} &amp; \boldsymbol{Q}_{12} \\
\boldsymbol{Q}_{21} &amp; \boldsymbol{Q}_{22}
\end{array}\right]=\left[\begin{array}{ll}
\mathbb{E}\left[X_{1} X_{1}^{\prime}\right] &amp; \mathbb{E}\left[X_{1} X_{2}^{\prime}\right] \\
\mathbb{E}\left[X_{2} X_{1}^{\prime}\right] &amp; \mathbb{E}\left[X_{2} X_{2}^{\prime}\right]
\end{array}\right]
\]</span></p>
<p>and similarly</p>
<p><span class="math display">\[
\boldsymbol{Q}_{X Y}=\left[\begin{array}{l}
\boldsymbol{Q}_{1 Y} \\
\boldsymbol{Q}_{2 Y}
\end{array}\right]=\left[\begin{array}{c}
\mathbb{E}\left[X_{1} Y\right] \\
\mathbb{E}\left[X_{2} Y\right]
\end{array}\right] .
\]</span></p>
<p>By the partitioned matrix inversion formula (A.3)</p>
<p><span class="math display">\[
\boldsymbol{Q}_{X X}^{-1}=\left[\begin{array}{ll}
\boldsymbol{Q}_{11} &amp; \boldsymbol{Q}_{12} \\
\boldsymbol{Q}_{21} &amp; \boldsymbol{Q}_{22}
\end{array}\right]^{-1} \stackrel{\operatorname{def}}{=}\left[\begin{array}{ll}
\boldsymbol{Q}^{11} &amp; \boldsymbol{Q}^{12} \\
\boldsymbol{Q}^{21} &amp; \boldsymbol{Q}^{22}
\end{array}\right]=\left[\begin{array}{cc}
\boldsymbol{Q}_{11 \cdot 2}^{-1} &amp; -\boldsymbol{Q}_{11 \cdot 2}^{-1} \boldsymbol{Q}_{12} \boldsymbol{Q}_{22}^{-1} \\
-\boldsymbol{Q}_{22 \cdot 1}^{-1} \boldsymbol{Q}_{21} \boldsymbol{Q}_{11}^{-1} &amp; \boldsymbol{Q}_{22 \cdot 1}^{-1}
\end{array}\right]
\]</span></p>
<p><span class="math inline">\({ }^{10}\)</span> The covariance matrix between vectors <span class="math inline">\(X\)</span> and <span class="math inline">\(Z\)</span> is <span class="math inline">\(\operatorname{cov}(X, Z)=\mathbb{E}\left[(X-\mathbb{E}[X])(Z-\mathbb{E}[Z])^{\prime}\right]\)</span>. The covariance matrix of the <span class="math inline">\(\operatorname{vector} X\)</span> is <span class="math inline">\(\operatorname{var}[X]=\operatorname{cov}(X, X)=\mathbb{E}\left[(X-\mathbb{E}[X])(X-\mathbb{E}[X])^{\prime}\right]\)</span>. where <span class="math inline">\(\boldsymbol{Q}_{11 \cdot 2} \stackrel{\text { def }}{=} \boldsymbol{Q}_{11}-\boldsymbol{Q}_{12} \boldsymbol{Q}_{22}^{-1} \boldsymbol{Q}_{21}\)</span> and <span class="math inline">\(\boldsymbol{Q}_{22 \cdot 1} \stackrel{\text { def }}{=} \boldsymbol{Q}_{22}-\boldsymbol{Q}_{21} \boldsymbol{Q}_{11}^{-1} \boldsymbol{Q}_{12}\)</span>. Thus</p>
<p><span class="math display">\[
\begin{aligned}
\beta &amp;=\left(\begin{array}{l}
\beta_{1} \\
\beta_{2}
\end{array}\right) \\
&amp;=\left[\begin{array}{cc}
\boldsymbol{Q}_{11 \cdot 2}^{-1} &amp; -\boldsymbol{Q}_{11 \cdot 2}^{-1} \boldsymbol{Q}_{12} \boldsymbol{Q}_{22}^{-1} \\
-\boldsymbol{Q}_{22 \cdot 1}^{-1} \boldsymbol{Q}_{21} \boldsymbol{Q}_{11}^{-1} &amp; \boldsymbol{Q}_{22 \cdot 1}^{-1}
\end{array}\right]\left[\begin{array}{l}
\boldsymbol{Q}_{1 Y} \\
\boldsymbol{Q}_{2 Y}
\end{array}\right] \\
&amp;=\left(\begin{array}{c}
\boldsymbol{Q}_{11 \cdot 2}^{-1}\left(\boldsymbol{Q}_{1 y}-\boldsymbol{Q}_{12} \boldsymbol{Q}_{22}^{-1} \boldsymbol{Q}_{2 Y}\right) \\
\boldsymbol{Q}_{22 \cdot 1}^{-1}\left(\boldsymbol{Q}_{2 y}-\boldsymbol{Q}_{21} \boldsymbol{Q}_{11}^{-1} \boldsymbol{Q}_{1 Y}\right)
\end{array}\right) \\
&amp;=\left(\begin{array}{c}
\boldsymbol{Q}_{11 \cdot 2}^{-1} \boldsymbol{Q}_{1 Y \cdot 2} \\
\boldsymbol{Q}_{22 \cdot 1}^{-1} \boldsymbol{Q}_{2 Y \cdot 1}
\end{array}\right)
\end{aligned}
\]</span></p>
<p>We have shown that <span class="math inline">\(\beta_{1}=\boldsymbol{Q}_{11 \cdot 2}^{-1} \boldsymbol{Q}_{1 Y \cdot 2}\)</span> and <span class="math inline">\(\beta_{2}=\boldsymbol{Q}_{22 \cdot 1}^{-1} \boldsymbol{Q}_{2 Y \cdot 1}\)</span>.</p>
</section>
<section id="coefficient-decomposition" class="level2" data-number="2.27">
<h2 data-number="2.27" class="anchored" data-anchor-id="coefficient-decomposition"><span class="header-section-number">2.27</span> Coefficient Decomposition</h2>
<p>In the previous section we derived formulae for the coefficient sub-vectors <span class="math inline">\(\beta_{1}\)</span> and <span class="math inline">\(\beta_{2}\)</span>. We now use these formulae to give a useful interpretation of the coefficients in terms of an iterated projection.</p>
<p>Take equation (2.42) for the case <span class="math inline">\(\operatorname{dim}\left(X_{1}\right)=1\)</span> so that <span class="math inline">\(\beta_{1} \in \mathbb{R}\)</span>.</p>
<p><span class="math display">\[
Y=X_{1} \beta_{1}+X_{2}^{\prime} \beta_{2}+e .
\]</span></p>
<p>Now consider the projection of <span class="math inline">\(X_{1}\)</span> on <span class="math inline">\(X_{2}\)</span> :</p>
<p><span class="math display">\[
\begin{aligned}
X_{1} &amp;=X_{2}^{\prime} \gamma_{2}+u_{1} \\
\mathbb{E}\left[X_{2} u_{1}\right] &amp;=0 .
\end{aligned}
\]</span></p>
<p>From (2.22) and (2.36), <span class="math inline">\(\gamma_{2}=\boldsymbol{Q}_{22}^{-1} \boldsymbol{Q}_{21}\)</span> and <span class="math inline">\(\mathbb{E}\left[u_{1}^{2}\right]=\boldsymbol{Q}_{11 \cdot 2}=\boldsymbol{Q}_{11}-\boldsymbol{Q}_{12} \boldsymbol{Q}_{22}^{-1} \boldsymbol{Q}_{21}\)</span>. We can also calculate that</p>
<p><span class="math display">\[
\mathbb{E}\left[u_{1} Y\right]=\mathbb{E}\left[\left(X_{1}-\gamma_{2}^{\prime} X_{2}\right) Y\right]=\mathbb{E}\left[X_{1} Y\right]-\gamma_{2}^{\prime} \mathbb{E}\left[X_{2} Y\right]=\boldsymbol{Q}_{1 Y}-\boldsymbol{Q}_{12} \boldsymbol{Q}_{22}^{-1} \boldsymbol{Q}_{2 Y}=\boldsymbol{Q}_{1 Y \cdot 2} .
\]</span></p>
<p>We have found that</p>
<p><span class="math display">\[
\beta_{1}=\boldsymbol{Q}_{11 \cdot 2}^{-1} \boldsymbol{Q}_{1 Y \cdot 2}=\frac{\mathbb{E}\left[u_{1} Y\right]}{\mathbb{E}\left[u_{1}^{2}\right]}
\]</span></p>
<p>the coefficient from the simple regression of <span class="math inline">\(Y\)</span> on <span class="math inline">\(u_{1}\)</span>.</p>
<p>What this means is that in the multivariate projection equation (2.44), the coefficient <span class="math inline">\(\beta_{1}\)</span> equals the projection coefficient from a regression of <span class="math inline">\(Y\)</span> on <span class="math inline">\(u_{1}\)</span>, the error from a projection of <span class="math inline">\(X_{1}\)</span> on the other regressors <span class="math inline">\(X_{2}\)</span>. The error <span class="math inline">\(u_{1}\)</span> can be thought of as the component of <span class="math inline">\(X_{1}\)</span> which is not linearly explained by the other regressors. Thus the coefficient <span class="math inline">\(\beta_{1}\)</span> equals the linear effect of <span class="math inline">\(X_{1}\)</span> on <span class="math inline">\(Y\)</span> after stripping out the effects of the other variables.</p>
<p>There was nothing special in the choice of the variable <span class="math inline">\(X_{1}\)</span>. This derivation applies symmetrically to all coefficients in a linear projection. Each coefficient equals the simple regression of <span class="math inline">\(Y\)</span> on the error from a projection of that regressor on all the other regressors. Each coefficient equals the linear effect of that variable on <span class="math inline">\(Y\)</span> after linearly controlling for all the other regressors.</p>
</section>
<section id="omitted-variable-bias" class="level2" data-number="2.28">
<h2 data-number="2.28" class="anchored" data-anchor-id="omitted-variable-bias"><span class="header-section-number">2.28</span> Omitted Variable Bias</h2>
<p>Again, let the regressors be partitioned as in (2.41). Consider the projection of <span class="math inline">\(Y\)</span> on <span class="math inline">\(X_{1}\)</span> only. Perhaps this is done because the variables <span class="math inline">\(X_{2}\)</span> are not observed. This is the equation</p>
<p><span class="math display">\[
\begin{aligned}
Y &amp;=X_{1}^{\prime} \gamma_{1}+u \\
\mathbb{E}\left[X_{1} u\right] &amp;=0 .
\end{aligned}
\]</span></p>
<p>Notice that we have written the coefficient as <span class="math inline">\(\gamma_{1}\)</span> rather than <span class="math inline">\(\beta_{1}\)</span> and the error as <span class="math inline">\(u\)</span> rather than <span class="math inline">\(e\)</span>. This is because (2.45) is different than (2.42). Goldberger (1991) introduced the catchy labels long regression for (2.42) and short regression for (2.45) to emphasize the distinction.</p>
<p>Typically, <span class="math inline">\(\beta_{1} \neq \gamma_{1}\)</span>, except in special cases. To see this, we calculate</p>
<p><span class="math display">\[
\begin{aligned}
\gamma_{1} &amp;=\left(\mathbb{E}\left[X_{1} X_{1}^{\prime}\right]\right)^{-1} \mathbb{E}\left[X_{1} Y\right] \\
&amp;=\left(\mathbb{E}\left[X_{1} X_{1}^{\prime}\right]\right)^{-1} \mathbb{E}\left[X_{1}\left(X_{1}^{\prime} \beta_{1}+X_{2}^{\prime} \beta_{2}+e\right)\right] \\
&amp;=\beta_{1}+\left(\mathbb{E}\left[X_{1} X_{1}^{\prime}\right]\right)^{-1} \mathbb{E}\left[X_{1} X_{2}^{\prime}\right] \beta_{2} \\
&amp;=\beta_{1}+\Gamma_{12} \beta_{2}
\end{aligned}
\]</span></p>
<p>where <span class="math inline">\(\Gamma_{12}=\boldsymbol{Q}_{11}^{-1} \boldsymbol{Q}_{12}\)</span> is the coefficient matrix from a projection of <span class="math inline">\(X_{2}\)</span> on <span class="math inline">\(X_{1}\)</span> where we use the notation from Section <span class="math inline">\(2.22\)</span>.</p>
<p>Observe that <span class="math inline">\(\gamma_{1}=\beta_{1}+\Gamma_{12} \beta_{2} \neq \beta_{1}\)</span> unless <span class="math inline">\(\Gamma_{12}=0\)</span> or <span class="math inline">\(\beta_{2}=0\)</span>. Thus the short and long regressions have different coefficients. They are the same only under one of two conditions. First, if the projection of <span class="math inline">\(X_{2}\)</span> on <span class="math inline">\(X_{1}\)</span> yields a set of zero coefficients (they are uncorrelated), or second, if the coefficient on <span class="math inline">\(X_{2}\)</span> in (2.42) is zero. The difference <span class="math inline">\(\Gamma_{12} \beta_{2}\)</span> between <span class="math inline">\(\gamma_{1}\)</span> and <span class="math inline">\(\beta_{1}\)</span> is known as omitted variable bias. It is the consequence of omission of a relevant correlated variable.</p>
<p>To avoid omitted variables bias the standard advice is to include all potentially relevant variables in estimated models. By construction, the general model will be free of such bias. Unfortunately in many cases it is not feasible to completely follow this advice as many desired variables are not observed. In this case, the possibility of omitted variables bias should be acknowledged and discussed in the course of an empirical investigation.</p>
<p>For example, suppose <span class="math inline">\(Y\)</span> is log wages, <span class="math inline">\(X_{1}\)</span> is education, and <span class="math inline">\(X_{2}\)</span> is intellectual ability. It seems reasonable to suppose that education and intellectual ability are positively correlated (highly able individuals attain higher levels of education) which means <span class="math inline">\(\Gamma_{12}&gt;0\)</span>. It also seems reasonable to suppose that conditional on education, individuals with higher intelligence will earn higher wages on average, so that <span class="math inline">\(\beta_{2}&gt;0\)</span>. This implies that <span class="math inline">\(\Gamma_{12} \beta_{2}&gt;0\)</span> and <span class="math inline">\(\gamma_{1}=\beta_{1}+\Gamma_{12} \beta_{2}&gt;\beta_{1}\)</span>. Therefore, it seems reasonable to expect that in a regression of wages on education with intelligence omitted (as the latter is not measured), the coefficient on education is higher than in a regression where intelligence is included. In other words, in this context the omitted variable biases the regression coefficient upwards. It is possible, for example, that <span class="math inline">\(\beta_{1}=0\)</span> so that education has no direct effect on wages yet <span class="math inline">\(\gamma_{1}=\Gamma_{12} \beta_{2}&gt;0\)</span> meaning that the regression coefficient on education alone is positive, but is a consequence of the unmodeled correlation between education and intellectual ability.</p>
<p>Unfortunately, the above simple characterization of omitted variable bias does not immediately carry over to more complicated settings, as discovered by Luca, Magnus, and Peracchi (2018). For example, suppose we compare three nested projections</p>
<p><span class="math display">\[
\begin{aligned}
&amp;Y=X_{1}^{\prime} \gamma_{1}+u_{1} \\
&amp;Y=X_{1}^{\prime} \delta_{1}+X_{2}^{\prime} \delta_{2}+u_{2} \\
&amp;Y=X_{1}^{\prime} \beta_{1}+X_{2}^{\prime} \beta_{2}+X_{3}^{\prime} \beta_{3}+e .
\end{aligned}
\]</span></p>
<p>We can call them short, medium, and long regressions. Suppose that the parameter of interest is <span class="math inline">\(\beta_{1}\)</span> in the long regression. We are interested in the consequences of omitting <span class="math inline">\(X_{3}\)</span> when estimating the medium regression, and of omitting both <span class="math inline">\(X_{2}\)</span> and <span class="math inline">\(X_{3}\)</span> when estimating the short regression. In particular we are interested in the question: Is it better to estimate the short or medium regression, given that both omit <span class="math inline">\(X_{3}\)</span> ? Intuition suggests that the medium regression should be “less biased” but it is worth investigating in greater detail. By similar calculations to those above, we find that</p>
<p><span class="math display">\[
\begin{aligned}
&amp;\gamma_{1}=\beta_{1}+\Gamma_{12} \beta_{2}+\Gamma_{13} \beta_{3} \\
&amp;\delta_{1}=\beta_{1}+\Gamma_{13 \cdot 2} \beta_{3}
\end{aligned}
\]</span></p>
<p>where <span class="math inline">\(\Gamma_{13 \cdot 2}=\boldsymbol{Q}_{11 \cdot 2}^{-1} \boldsymbol{Q}_{13 \cdot 2}\)</span> using the notation from Section <span class="math inline">\(2.22\)</span>.</p>
<p>We see that the bias in the short regression coefficient is <span class="math inline">\(\Gamma_{12} \beta_{2}+\Gamma_{13} \beta_{3}\)</span> which depends on both <span class="math inline">\(\beta_{2}\)</span> and <span class="math inline">\(\beta_{3}\)</span>, while that for the medium regression coefficient is <span class="math inline">\(\Gamma_{13 \cdot 2} \beta_{3}\)</span> which only depends on <span class="math inline">\(\beta_{3}\)</span>. So the bias for the medium regression is less complicated and intuitively seems more likely to be smaller than that of the short regression. However it is impossible to strictly rank the two. It is quite possible that <span class="math inline">\(\gamma_{1}\)</span> is less biased than <span class="math inline">\(\delta_{1}\)</span>. Thus as a general rule it is unknown if estimation of the medium regression will be less biased than estimation of the short regression.</p>
</section>
<section id="best-linear-approximation" class="level2" data-number="2.29">
<h2 data-number="2.29" class="anchored" data-anchor-id="best-linear-approximation"><span class="header-section-number">2.29</span> Best Linear Approximation</h2>
<p>There are alternative ways we could construct a linear approximation <span class="math inline">\(X^{\prime} \beta\)</span> to the conditional expectation <span class="math inline">\(m(X)\)</span>. In this section we show that one alternative approach turns out to yield the same answer as the best linear predictor.</p>
<p>We start by defining the mean-square approximation error of <span class="math inline">\(X^{\prime} \beta\)</span> to <span class="math inline">\(m(X)\)</span> as the expected squared difference between <span class="math inline">\(X^{\prime} \beta\)</span> and the conditional expectation <span class="math inline">\(m(X)\)</span></p>
<p><span class="math display">\[
d(\beta)=\mathbb{E}\left[\left(m(X)-X^{\prime} \beta\right)^{2}\right] .
\]</span></p>
<p>The function <span class="math inline">\(d(\beta)\)</span> is a measure of the deviation of <span class="math inline">\(X^{\prime} \beta\)</span> from <span class="math inline">\(m(X)\)</span>. If the two functions are identical then <span class="math inline">\(d(\beta)=0\)</span>, otherwise <span class="math inline">\(d(\beta)&gt;0\)</span>. We can also view the mean-square difference <span class="math inline">\(d(\beta)\)</span> as a density-weighted average of the function <span class="math inline">\(\left(m(X)-X^{\prime} \beta\right)^{2}\)</span> since</p>
<p><span class="math display">\[
d(\beta)=\int_{\mathbb{R}^{k}}\left(m(x)-x^{\prime} \beta\right)^{2} f_{X}(x) d x
\]</span></p>
<p>where <span class="math inline">\(f_{X}(x)\)</span> is the marginal density of <span class="math inline">\(X\)</span>.</p>
<p>We can then define the best linear approximation to the conditional <span class="math inline">\(m(X)\)</span> as the function <span class="math inline">\(X^{\prime} \beta\)</span> obtained by selecting <span class="math inline">\(\beta\)</span> to minimize <span class="math inline">\(d(\beta)\)</span> :</p>
<p><span class="math display">\[
\beta=\underset{b \in \mathbb{R}^{k}}{\operatorname{argmin}} d(b) .
\]</span></p>
<p>Similar to the best linear predictor we are measuring accuracy by expected squared error. The difference is that the best linear predictor (2.18) selects <span class="math inline">\(\beta\)</span> to minimize the expected squared prediction error, while the best linear approximation (2.46) selects <span class="math inline">\(\beta\)</span> to minimize the expected squared approximation error.</p>
<p>Despite the different definitions, it turns out that the best linear predictor and the best linear approximation are identical. By the same steps as in (2.18) plus an application of conditional expectations we can find that</p>
<p><span class="math display">\[
\begin{aligned}
\beta &amp;=\left(\mathbb{E}\left[X X^{\prime}\right]\right)^{-1} \mathbb{E}[X m(X)] \\
&amp;=\left(\mathbb{E}\left[X X^{\prime}\right]\right)^{-1} \mathbb{E}[X Y]
\end{aligned}
\]</span></p>
<p>(see Exercise 2.19). Thus (2.46) equals (2.18). We conclude that the definition (2.46) can be viewed as an alternative motivation for the linear projection coefficient.</p>
</section>
<section id="regression-to-the-mean" class="level2" data-number="2.30">
<h2 data-number="2.30" class="anchored" data-anchor-id="regression-to-the-mean"><span class="header-section-number">2.30</span> Regression to the Mean</h2>
<p>The term regression originated in an influential paper by Francis Galton (1886) where he examined the joint distribution of the stature (height) of parents and children. Effectively, he was estimating the conditional expectation of children’s height given their parent’s height. Galton discovered that this conditional expectation was approximately linear with a slope of <span class="math inline">\(2 / 3\)</span>. This implies that on average a child’s height is more mediocre (average) than his or her parent’s height. Galton called this phenomenon regression to the mean, and the label regression has stuck to this day to describe most conditional relationships.</p>
<p>One of Galton’s fundamental insights was to recognize that if the marginal distributions of <span class="math inline">\(Y\)</span> and <span class="math inline">\(X\)</span> are the same (e.g.&nbsp;the heights of children and parents in a stable environment) then the regression slope in a linear projection is always less than one.</p>
<p>To be more precise, take the simple linear projection</p>
<p><span class="math display">\[
Y=X \beta+\alpha+e
\]</span></p>
<p>where <span class="math inline">\(Y\)</span> equals the height of the child and <span class="math inline">\(X\)</span> equals the height of the parent. Assume that <span class="math inline">\(Y\)</span> and <span class="math inline">\(X\)</span> have the same expectation so that <span class="math inline">\(\mu_{Y}=\mu_{X}=\mu\)</span>. Then from (2.39) <span class="math inline">\(\alpha=(1-\beta) \mu\)</span> so we can write the linear projection (2.49) as</p>
<p><span class="math display">\[
\mathscr{P}(Y \mid X)=(1-\beta) \mu+X \beta .
\]</span></p>
<p>This shows that the projected height of the child is a weighted average of the population expectation <span class="math inline">\(\mu\)</span> and the parent’s height <span class="math inline">\(X\)</span> with weights <span class="math inline">\(\beta\)</span> and <span class="math inline">\(1-\beta\)</span>. When the height distribution is stable across generations so that <span class="math inline">\(\operatorname{var}[Y]=\operatorname{var}[X]\)</span>, then this slope is the simple correlation of <span class="math inline">\(Y\)</span> and <span class="math inline">\(X\)</span>. Using (2.40)</p>
<p><span class="math display">\[
\beta=\frac{\operatorname{cov}(X, Y)}{\operatorname{var}[X]}=\operatorname{corr}(X, Y) .
\]</span></p>
<p>By the Cauchy-Schwarz inequality (B.32), <span class="math inline">\(-1 \leq \operatorname{corr}(X, Y) \leq 1\)</span>, with <span class="math inline">\(\operatorname{corr}(X, Y)=1\)</span> only in the degenerate case <span class="math inline">\(Y=X\)</span>. Thus if we exclude degeneracy, <span class="math inline">\(\beta\)</span> is strictly less than 1 .</p>
<p>This means that on average, a child’s height is more mediocre (closer to the population average) than the parent’s.</p>
<p>A common error - known as the regression fallacy - is to infer from <span class="math inline">\(\beta&lt;1\)</span> that the population is converging, meaning that its variance is declining towards zero. This is a fallacy because we derived the implication <span class="math inline">\(\beta&lt;1\)</span> under the assumption of constant means and variances. So certainly <span class="math inline">\(\beta&lt;1\)</span> does not imply that the variance <span class="math inline">\(Y\)</span> is less than than the variance of <span class="math inline">\(X\)</span>.</p>
<p>Another way of seeing this is to examine the conditions for convergence in the context of equation (2.49). Since <span class="math inline">\(X\)</span> and <span class="math inline">\(e\)</span> are uncorrelated, it follows that</p>
<p><span class="math display">\[
\operatorname{var}[Y]=\beta^{2} \operatorname{var}[X]+\operatorname{var}[e] .
\]</span></p>
<p>Then <span class="math inline">\(\operatorname{var}[Y]&lt;\operatorname{var}[X]\)</span> if and only if</p>
<p><span class="math display">\[
\beta^{2}&lt;1-\frac{\operatorname{var}[e]}{\operatorname{var}[X]}
\]</span></p>
<p>which is not implied by the simple condition <span class="math inline">\(|\beta|&lt;1\)</span>.</p>
<p>The regression fallacy arises in related empirical situations. Suppose you sort families into groups by the heights of the parents, and then plot the average heights of each subsequent generation over time. If the population is stable, the regression property implies that the plots lines will converge-children’s height will be more average than their parents. The regression fallacy is to incorrectly conclude that the population is converging. A message to be learned from this example is that such plots are misleading for inferences about convergence. The regression fallacy is subtle. It is easy for intelligent economists to succumb to its temptation. A famous example is The Triumph of Mediocrity in Business by Horace Secrist published in 1933. In this book, Secrist carefully and with great detail documented that in a sample of department stores over 19201930, when he divided the stores into groups based on 1920-1921 profits, and plotted the average profits of these groups for the subsequent 10 years, he found clear and persuasive evidence for convergence “toward mediocrity”. Of course, there was no discovery - regression to the mean is a necessary feature of stable distributions.</p>
</section>
<section id="reverse-regression" class="level2" data-number="2.31">
<h2 data-number="2.31" class="anchored" data-anchor-id="reverse-regression"><span class="header-section-number">2.31</span> Reverse Regression</h2>
<p>Galton noticed another interesting feature of the bivariate distribution. There is nothing special about a regression of <span class="math inline">\(Y\)</span> on <span class="math inline">\(X\)</span>. We can also regress <span class="math inline">\(X\)</span> on <span class="math inline">\(Y\)</span>. (In his heredity example this is the best linear predictor of the height of parents given the height of their children.) This regression takes the form</p>
<p><span class="math display">\[
X=Y \beta^{*}+\alpha^{*}+e^{*} .
\]</span></p>
<p>This is sometimes called the reverse regression. In this equation, the coefficients <span class="math inline">\(\alpha^{*}, \beta^{*}\)</span> and error <span class="math inline">\(e^{*}\)</span> are defined by linear projection. In a stable population we find that</p>
<p><span class="math display">\[
\begin{gathered}
\beta^{*}=\operatorname{corr}(X, Y)=\beta \\
\alpha^{*}=(1-\beta) \mu=\alpha
\end{gathered}
\]</span></p>
<p>which are exactly the same as in the projection of <span class="math inline">\(Y\)</span> on <span class="math inline">\(X\)</span> ! The intercept and slope have exactly the same values in the forward and reverse projections! [This equality is not particularly imporant; it is an artifact of the assumption that <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> have the same variances.]</p>
<p>While this algebraic discovery is quite simple, it is counter-intuitive. Instead, a common yet mistaken guess for the form of the reverse regression is to take the equation (2.49), divide through by <span class="math inline">\(\beta\)</span> and rewrite to find the equation</p>
<p><span class="math display">\[
X=Y \frac{1}{\beta}-\frac{\alpha}{\beta}-\frac{1}{\beta} e
\]</span></p>
<p>suggesting that the projection of <span class="math inline">\(X\)</span> on <span class="math inline">\(Y\)</span> should have a slope coefficient of <span class="math inline">\(1 / \beta\)</span> instead of <span class="math inline">\(\beta\)</span>, and intercept of <span class="math inline">\(-\alpha / \beta\)</span> rather than <span class="math inline">\(\alpha\)</span>. What went wrong? Equation (2.51) is perfectly valid because it is a simple manipulation of the valid equation (2.49). The trouble is that (2.51) is neither a CEF nor a linear projection. Inverting a projection (or CEF) does not yield a projection (or CEF). Instead, (2.50) is a valid projection, not (2.51).</p>
<p>In any event, Galton’s finding was that when the variables are standardized, the slope in both projections ( <span class="math inline">\(Y\)</span> on <span class="math inline">\(X\)</span>, and <span class="math inline">\(X\)</span> on <span class="math inline">\(Y\)</span> ) equals the correlation and both equations exhibit regression to the mean. It is not a causal relation, but a natural feature of joint distributions.</p>
</section>
<section id="limitations-of-the-best-linear-projection" class="level2" data-number="2.32">
<h2 data-number="2.32" class="anchored" data-anchor-id="limitations-of-the-best-linear-projection"><span class="header-section-number">2.32</span> Limitations of the Best Linear Projection</h2>
<p>Let’s compare the linear projection and linear CEF models.</p>
<p>From Theorem 2.4.4 we know that the CEF error has the property <span class="math inline">\(\mathbb{E}[X e]=0\)</span>. Thus a linear CEF is the best linear projection. However, the converse is not true as the projection error does not necessarily satisfy <span class="math inline">\(\mathbb{E}[e \mid X]=0\)</span>. Furthermore, the linear projection may be a poor approximation to the CEF.</p>
<p>To see these points in a simple example, suppose that the true process is <span class="math inline">\(Y=X+X^{2}\)</span> with <span class="math inline">\(X \sim \mathrm{N}(0,1)\)</span>. In this case the true CEF is <span class="math inline">\(m(x)=x+x^{2}\)</span> and there is no error. Now consider the linear projection of <span class="math inline">\(Y\)</span> on <span class="math inline">\(X\)</span> and a constant, namely the model <span class="math inline">\(Y=\beta X+\alpha+e\)</span>. Since <span class="math inline">\(X \sim \mathrm{N}(0,1)\)</span> then <span class="math inline">\(X\)</span> and <span class="math inline">\(X^{2}\)</span> are uncorrelated and the linear projection takes the form <span class="math inline">\(\mathscr{P}[Y \mid X]=X+1\)</span>. This is quite different from the true CEF <span class="math inline">\(m(X)=\)</span> <span class="math inline">\(X+X^{2}\)</span>. The projection error equals <span class="math inline">\(e=X^{2}-1\)</span> which is a deterministic function of <span class="math inline">\(X\)</span> yet is uncorrelated with <span class="math inline">\(X\)</span>. We see in this example that a projection error need not be a CEF error and a linear projection can be a poor approximation to the CEF.</p>
<p><img src="images//2022_09_17_efa0deee3441d06e0b66g-36.jpg" class="img-fluid"></p>
<p>Figure 2.7: Conditional Expectation and Two Linear Projections</p>
<p>Another defect of linear projection is that it is sensitive to the marginal distribution of the regressors when the conditional mean is nonlinear. We illustrate the issue in Figure <span class="math inline">\(2.7\)</span> for a constructed <span class="math inline">\({ }^{11}\)</span> joint distribution of <span class="math inline">\(Y\)</span> and <span class="math inline">\(X\)</span>. The thick line is the nonlinear CEF of <span class="math inline">\(Y\)</span> given <span class="math inline">\(X\)</span>. The data are divided in two groups - Group 1 and Group 2 - which have different marginal distributions for the regressor <span class="math inline">\(X\)</span>, and Group 1 has a lower mean value of <span class="math inline">\(X\)</span> than Group 2. The separate linear projections of <span class="math inline">\(Y\)</span> on <span class="math inline">\(X\)</span> for these two groups are displayed in the figure by the thin lines. These two projections are distinct approximations to the CEF. A defect with linear projection is that it leads to the incorrect conclusion that the effect of <span class="math inline">\(X\)</span> on <span class="math inline">\(Y\)</span> is different for individuals in the two groups. This conclusion is incorrect because in fact there is no difference in the conditional expectation function. The apparent difference is a by-product of linear approximations to a nonlinear expectation combined with different marginal distributions for the conditioning variables.</p>
<p><span class="math inline">\({ }^{11}\)</span> The <span class="math inline">\(X\)</span> in Group 1 are <span class="math inline">\(\mathrm{N}(2,1)\)</span>, those in Group 2 are <span class="math inline">\(\mathrm{N}(4,1)\)</span>, and the conditional distribution of <span class="math inline">\(Y\)</span> given <span class="math inline">\(X\)</span> is <span class="math inline">\(\mathrm{N}(m(X), 1)\)</span> where <span class="math inline">\(m(x)=2 x-x^{2} / 6\)</span>. The functions are plotted over <span class="math inline">\(0 \leq x \leq 6\)</span>.</p>
</section>
<section id="random-coefficient-model" class="level2" data-number="2.33">
<h2 data-number="2.33" class="anchored" data-anchor-id="random-coefficient-model"><span class="header-section-number">2.33</span> Random Coefficient Model</h2>
<p>A model which is notationally similar to but conceptually distinct from the linear CEF model is the linear random coefficient model. It takes the form <span class="math inline">\(Y=X^{\prime} \eta\)</span> where the individual-specific coefficient <span class="math inline">\(\eta\)</span> is random and independent of <span class="math inline">\(X\)</span>. For example, if <span class="math inline">\(X\)</span> is years of schooling and <span class="math inline">\(Y\)</span> is log wages, then <span class="math inline">\(\eta\)</span> is the individual-specific returns to schooling. If a person obtains an extra year of schooling, <span class="math inline">\(\eta\)</span> is the actual change in their wage. The random coefficient model allows the returns to schooling to vary in the population. Some individuals might have a high return to education (a high <span class="math inline">\(\eta\)</span> ) and others a low return, possibly 0 , or even negative.</p>
<p>In the linear CEF model the regressor coefficient equals the regression derivative - the change in the conditional expectation due to a change in the regressors, <span class="math inline">\(\beta=\nabla m(X)\)</span>. This is not the effect on a given individual, it is the effect on the population average. In contrast, in the random coefficient model the random vector <span class="math inline">\(\eta=\nabla\left(X^{\prime} \eta\right)\)</span> is the true causal effect - the change in the response variable <span class="math inline">\(Y\)</span> itself due to a change in the regressors.</p>
<p>It is interesting, however, to discover that the linear random coefficient model implies a linear CEF. To see this, let <span class="math inline">\(\beta=\mathbb{E}[\eta]\)</span> and <span class="math inline">\(\Sigma=\operatorname{var}[\eta]\)</span> denote the mean and covariance matrix of <span class="math inline">\(\eta\)</span> and then decompose the random coefficient as <span class="math inline">\(\eta=\beta+u\)</span> where <span class="math inline">\(u\)</span> is distributed independently of <span class="math inline">\(X\)</span> with mean zero and covariance matrix <span class="math inline">\(\Sigma\)</span>. Then we can write</p>
<p><span class="math display">\[
\mathbb{E}[Y \mid X]=X^{\prime} \mathbb{E}[\eta \mid X]=X^{\prime} \mathbb{E}[\eta]=X^{\prime} \beta
\]</span></p>
<p>so the CEF is linear in <span class="math inline">\(X\)</span>, and the coefficient <span class="math inline">\(\beta\)</span> equals the expectation of the random coefficient <span class="math inline">\(\eta\)</span>.</p>
<p>We can thus write the equation as a linear CEF <span class="math inline">\(Y=X^{\prime} \beta+e\)</span> where <span class="math inline">\(e=X^{\prime} u\)</span> and <span class="math inline">\(u=\eta-\beta\)</span>. The error is conditionally mean zero: <span class="math inline">\(\mathbb{E}[e \mid X]=0\)</span>. Furthermore</p>
<p><span class="math display">\[
\operatorname{var}[e \mid X]=X^{\prime} \operatorname{var}[\eta] X=X^{\prime} \Sigma X
\]</span></p>
<p>so the error is conditionally heteroskedastic with its variance a quadratic function of <span class="math inline">\(X\)</span>.</p>
<p>Theorem 2.11 In the linear random coefficient model <span class="math inline">\(Y=X^{\prime} \eta\)</span> with <span class="math inline">\(\eta\)</span> independent of <span class="math inline">\(X, \mathbb{E}\|X\|^{2}&lt;\infty\)</span>, and <span class="math inline">\(\mathbb{E}\|\eta\|^{2}&lt;\infty\)</span>, then</p>
<p><span class="math display">\[
\begin{aligned}
\mathbb{E}[Y \mid X] &amp;=X^{\prime} \beta \\
\operatorname{var}[Y \mid X] &amp;=X^{\prime} \Sigma X
\end{aligned}
\]</span></p>
<p>where <span class="math inline">\(\beta=\mathbb{E}[\eta]\)</span> and <span class="math inline">\(\Sigma=\operatorname{var}[\eta]\)</span></p>
</section>
<section id="causal-effects" class="level2" data-number="2.34">
<h2 data-number="2.34" class="anchored" data-anchor-id="causal-effects"><span class="header-section-number">2.34</span> Causal Effects</h2>
<p>So far we have avoided the concept of causality, yet often the underlying goal of an econometric analysis is to measure a causal relationship between variables. It is often of great interest to understand the causes and effects of decisions, actions, and policies. For example, we may be interested in the effect of class sizes on test scores, police expenditures on crime rates, climate change on economic activity, years of schooling on wages, institutional structure on growth, the effectiveness of rewards on behavior, the consequences of medical procedures for health outcomes, or any variety of possible causal relationships. In each case the goal is to understand what is the actual effect on the outcome due to a change in an input. We are not just interested in the conditional expectation or linear projection, we would like to know the actual change.</p>
<p>Two inherent barriers are: (1) the causal effect is typically specific to an individual; and (2) the causal effect is typically unobserved.</p>
<p>Consider the effect of schooling on wages. The causal effect is the actual difference a person would receive in wages if we could change their level of education holding all else constant. This is specific to each individual as their employment outcomes in these two distinct situations are individual. The causal effect is unobserved because the most we can observe is their actual level of education and their actual wage, but not the counterfactual wage if their education had been different.</p>
<p>To be concrete suppose that there are two individuals, Jennifer and George, and both have the possibility of being high-school graduates or college graduates, and both would have received different wages given their choices. For example, suppose that Jennifer would have earned <span class="math inline">\(\$ 10\)</span> an hour as a high-school graduate and <span class="math inline">\(\$ 20\)</span> an hour as a college graduate while George would have earned <span class="math inline">\(\$ 8\)</span> as a high-school graduate and <span class="math inline">\(\$ 12\)</span> as a college graduate. In this example the causal effect of schooling is <span class="math inline">\(\$ 10\)</span> a hour for Jennifer and <span class="math inline">\(\$ 4\)</span> an hour for George. The causal effects are specific to the individual and neither causal effect is observed.</p>
<p>Rubin (1974) developed the potential outcomes framework (also known as the Rubin causal model) to clarify the issues. Let <span class="math inline">\(Y\)</span> be a scalar outcome (for example, wages) and <span class="math inline">\(D\)</span> be a binary treatment (for example, college attendence). The specification of treatment as binary is not essential but simplifies the notation. A flexible model describing the impact of the treatment on the outcome is</p>
<p><span class="math display">\[
Y=h(D, U)
\]</span></p>
<p>where <span class="math inline">\(U\)</span> is an <span class="math inline">\(\ell \times 1\)</span> unobserved random factor and <span class="math inline">\(h\)</span> is a functional relationship. It is also common to use the simplified notation <span class="math inline">\(Y(0)=h(0, U)\)</span> and <span class="math inline">\(Y(1)=h(1, U)\)</span> for the potential outcomes associated with non-treatment and treatment, respectively. The notation implicitly holds <span class="math inline">\(U\)</span> fixed. The potential outcomes are specific to each individual as they depend on <span class="math inline">\(U\)</span>. For example, if <span class="math inline">\(Y\)</span> is an individual’s wage, the unobservables <span class="math inline">\(U\)</span> could include characteristics such as the individual’s abilities, skills, work ethic, interpersonal connections, and preferences, all of which potentially influence their wage. In our example these factors are summarized by the labels “Jennifer” and “George”.</p>
<p>Rubin described the effect as causal when we vary <span class="math inline">\(D\)</span> while holding <span class="math inline">\(U\)</span> constant. In our example this means changing an individual’s education while holding constant their other attributes.</p>
<p>Definition 2.6 In the model (2.52) the causal effect of <span class="math inline">\(D\)</span> on <span class="math inline">\(Y\)</span> is</p>
<p><span class="math display">\[
C(U)=Y(1)-Y(0)=h(1, U)-h(0, U),
\]</span></p>
<p>the change in <span class="math inline">\(Y\)</span> due to treatment while holding <span class="math inline">\(U\)</span> constant.</p>
<p>It may be helpful to understand that (2.53) is a definition and does not necessarily describe causality in a fundamental or experimental sense. Perhaps it would be more appropriate to label (2.53) as a structural effect (the effect within the structural model).</p>
<p>The causal effect of treatment <span class="math inline">\(C(U)\)</span> defined in (2.53) is heterogeneous and random as the potential outcomes <span class="math inline">\(Y(0)\)</span> and <span class="math inline">\(Y(1)\)</span> vary across individuals. Also, we do not observe both <span class="math inline">\(Y(0)\)</span> and <span class="math inline">\(Y(1)\)</span> for a given individual, but rather only the realized value</p>
<p><span class="math display">\[
Y=\left\{\begin{array}{lll}
Y(0) &amp; \text { if } &amp; D=0 \\
Y(1) &amp; \text { if } &amp; D=1 .
\end{array}\right.
\]</span></p>
<p>Table 2.3: Example Distribution</p>
<p>|College Graduate|0|0|6|10|<span class="math inline">\(\$ 17.00\)</span>| |:—————|:|:|:|-:|———:| |Difference | | | | | <span class="math inline">\(\$ 8.25\)</span>|</p>
<p>Consequently the causal effect <span class="math inline">\(C(U)\)</span> is unobserved.</p>
<p>Rubin’s goal was to learn features of the distribution of <span class="math inline">\(C(U)\)</span> including its expected value which he called the average causal effect. He defined it as follows.</p>
<p>Definition 2.7 In the model (2.52) the average causal effect of <span class="math inline">\(D\)</span> on <span class="math inline">\(Y\)</span> is</p>
<p><span class="math display">\[
\mathrm{ACE}=\mathbb{E}[C(U)]=\int_{\mathbb{R}^{\ell}} C(u) f(u) d u
\]</span></p>
<p>where <span class="math inline">\(f(u)\)</span> is the density of <span class="math inline">\(U\)</span>.</p>
<p>The ACE is the population average of the causal effect. Extending our Jennifer &amp; George example, suppose that half of the population are like Jennifer and the other half are like George. Then the average causal effect of college on wages is <span class="math inline">\((10+4) / 2=\$ 7\)</span> an hour.</p>
<p>To estimate the ACE a reasonable starting place is to compare average <span class="math inline">\(Y\)</span> for treated and untreated individuals. In our example this is the difference between the average wage among college graduates and high school graduates. This is the same as the coefficient in a regression of the outcome <span class="math inline">\(Y\)</span> on the treatment <span class="math inline">\(D\)</span>. Does this equal the ACE?</p>
<p>The answer depends on the relationship between treatment <span class="math inline">\(D\)</span> and the unobserved component <span class="math inline">\(U\)</span>. If <span class="math inline">\(D\)</span> is randomly assigned as in an experiment then <span class="math inline">\(D\)</span> and <span class="math inline">\(U\)</span> are independent and the regression coefficient equals the ACE. However, if <span class="math inline">\(D\)</span> and <span class="math inline">\(U\)</span> are dependent then the regression coefficient and ACE are different. To see this, observe that the difference between the average outcomes of the treated and untreated populations are</p>
<p><span class="math display">\[
\mathbb{E}[Y \mid D=1]-\mathbb{E}[Y \mid D=0]=\int_{\mathbb{R}^{\ell}} h(1, u) f(u \mid D=1) d u-\int_{\mathbb{R}^{\ell}} h(1, u) f(u \mid D=0) d u
\]</span></p>
<p>where <span class="math inline">\(f(u \mid D)\)</span> is the conditional density of <span class="math inline">\(U\)</span> given <span class="math inline">\(D\)</span>. If <span class="math inline">\(U\)</span> is independent of <span class="math inline">\(D\)</span> then <span class="math inline">\(f(u \mid D)=f(u)\)</span> and the above expression equals <span class="math inline">\(\int_{\mathbb{R}^{\ell}}(h(1, u)-h(0, u)) f(u) d u=\)</span> ACE. However, if <span class="math inline">\(U\)</span> and <span class="math inline">\(D\)</span> are dependent this equality fails.</p>
<p>To illustrate, let’s return to our example of Jennifer and George. Suppose that all high school students take an aptitude test. If a student gets a high <span class="math inline">\((\mathrm{H})\)</span> score they go to college with probability <span class="math inline">\(3 / 4\)</span>, and if a student gets a low (L) score they go to college with probability <span class="math inline">\(1 / 4\)</span>. Suppose further that Jennifer gets an aptitude score of <span class="math inline">\(\mathrm{H}\)</span> with probability 3/4, while George gets a score of <span class="math inline">\(\mathrm{H}\)</span> with probability <span class="math inline">\(1 / 4\)</span>. Given this situation, <span class="math inline">\(62.5 %\)</span> of Jennifer’s will go to college <span class="math inline">\({ }^{12}\)</span> while <span class="math inline">\(37.5 %\)</span> of George’s will go to college <span class="math inline">\({ }^{13}\)</span>.</p>
<p>An econometrician who randomly samples 32 individuals and collects data on educational attainment and wages will find the wage distribution displayed in Table 2.3.</p>
<p><span class="math inline">\(12 \mathbb{P}[\)</span> college <span class="math inline">\(\mid\)</span> Jennifer <span class="math inline">\(]=\mathbb{P}[\)</span> college <span class="math inline">\(\mid H] \mathbb{P}[H \mid\)</span> Jennifer <span class="math inline">\(]+\mathbb{P}[\)</span> college <span class="math inline">\(\mid L] \mathbb{P}[L \mid\)</span> Jennifer <span class="math inline">\(]=(3 / 4)^{2}+(1 / 4)^{2} .\)</span></p>
<p><span class="math inline">\(13 \mathbb{P}[\)</span> college <span class="math inline">\(\mid\)</span> George <span class="math inline">\(]=\mathbb{P}[\)</span> college <span class="math inline">\(\mid H] \mathbb{P}[H \mid\)</span> George <span class="math inline">\(]+\mathbb{P}[\)</span> college <span class="math inline">\(\mid L] \mathbb{P}[L \mid\)</span> George <span class="math inline">\(]=(3 / 4)(1 / 4)+(1 / 4)(3 / 4)\)</span>. Our econometrician finds that the average wage among high school graduates is <span class="math inline">\(\$ 8.75\)</span> while the average wage among college graduates is <span class="math inline">\(\$ 17.00\)</span>. The difference of <span class="math inline">\(\$ 8.25\)</span> is the econometrician’s regression coefficient for the effect of college on wages. But <span class="math inline">\(\$ 8.25\)</span> overstates the true ACE of <span class="math inline">\(\$ 7\)</span>. The reason is that college attendence is determined by an aptitude test which is correlated with an individual’s causal effect. Jennifer has both a high causal effect and is more likely to attend college, so the observed difference in wages overstates the causal effect of college.</p>
<p>To visualize Table <span class="math inline">\(2.3\)</span> examine Figure 2.8. The four points are the four education/wage pairs from the table, with the size of the points calibrated to the wage distribution. The two lines are the econometrician’s regression line and the average causal effect. The Jennifer’s in the population correspond to the points above the two lines, the George’s in the population correspond to the points below the two lines. Because most Jennifer’s go to College, and most George’s do not, the regression line is tilted away from the average causal effect towards the two large points.</p>
<p><img src="images//2022_09_17_efa0deee3441d06e0b66g-40.jpg" class="img-fluid"></p>
<p>Figure 2.8: Average Causal Effect vs Regression</p>
<p>Our first lesson from this analysis is that we need to be cautious about interpreting regression coefficients as causal effects. Unless the regressors (e.g.&nbsp;education attainment) can be interpreted as randomly assigned it is inappropriate to interpret the regression coefficients causally.</p>
<p>Our second lesson will be that a causal interpretation can be obtained if we condition on a sufficiently rich set of covariates. We now explore this issue.</p>
<p>Suppose that the observables include a set of covariates <span class="math inline">\(X\)</span> in addition to the outcome <span class="math inline">\(Y\)</span> and treatment <span class="math inline">\(D\)</span>. We extend the potential outcomes model (2.52) to include <span class="math inline">\(X\)</span> :</p>
<p><span class="math display">\[
Y=h(D, X, U) .
\]</span></p>
<p>We also extend the definition of a causal effect to allow conditioning on <span class="math inline">\(X\)</span>.</p>
<p>Definition <span class="math inline">\(2.8\)</span> In the model (2.54) the causal effect of <span class="math inline">\(D\)</span> on <span class="math inline">\(Y\)</span> is</p>
<p><span class="math display">\[
C(X, U)=h(1, X, U)-h(0, X, U),
\]</span></p>
<p>the change in <span class="math inline">\(Y\)</span> due to treatment holding <span class="math inline">\(X\)</span> and <span class="math inline">\(U\)</span> constant.</p>
<p>The conditional average causal effect of <span class="math inline">\(D\)</span> on <span class="math inline">\(Y\)</span> conditional on <span class="math inline">\(X=x\)</span> is</p>
<p><span class="math display">\[
\operatorname{ACE}(x)=\mathbb{E}[C(X, U) \mid X=x]=\int_{\mathbb{R}^{\ell}} C(x, u) f(u \mid x) d u
\]</span></p>
<p>where <span class="math inline">\(f(u \mid x)\)</span> is the conditional density of <span class="math inline">\(U\)</span> given <span class="math inline">\(X\)</span>.</p>
<p>The unconditional average causal effect of <span class="math inline">\(D\)</span> on <span class="math inline">\(Y\)</span> is</p>
<p><span class="math display">\[
\mathrm{ACE}=\mathbb{E}[C(X, U)]=\int \operatorname{ACE}(x) f(x) d x
\]</span></p>
<p>where <span class="math inline">\(f(x)\)</span> is the density of <span class="math inline">\(X\)</span>.</p>
<p>The conditional average causal effect <span class="math inline">\(\operatorname{ACE}(x)\)</span> is the ACE for the sub-population with characteristics <span class="math inline">\(X=x\)</span>. Given observations on <span class="math inline">\((Y, D, X)\)</span> we want to measure the causal effect of <span class="math inline">\(D\)</span> on <span class="math inline">\(Y\)</span>, and are interested if this can be obtained by a regression of <span class="math inline">\(Y\)</span> on <span class="math inline">\((D, X)\)</span>. We would like to interpret the coefficient on <span class="math inline">\(D\)</span> as a causal effect. Is this appropriate?</p>
<p>Our previous analysis showed that a causal interpretation obtains when <span class="math inline">\(U\)</span> is independent of the regressors. While this is sufficient it is stronger than necessary. Instead, the following is sufficient.</p>
<p>Definition 2.9 Conditional Independence Assumption (CIA). Conditional on <span class="math inline">\(X\)</span>, the random variables <span class="math inline">\(D\)</span> and <span class="math inline">\(U\)</span> are statistically independent.</p>
<p>The CIA implies that the conditional density of <span class="math inline">\(U\)</span> given <span class="math inline">\((D, X)\)</span> only depends on <span class="math inline">\(X\)</span>, thus <span class="math inline">\(f(u \mid D, X)=\)</span> <span class="math inline">\(f(u \mid X)\)</span>. This implies that the regression of <span class="math inline">\(Y\)</span> on <span class="math inline">\((D, X)\)</span> equals</p>
<p><span class="math display">\[
\begin{aligned}
m(d, x) &amp;=\mathbb{E}[Y \mid D=d, X=x] \\
&amp;=\mathbb{E}[h(d, x, U) \mid D=d, X=x] \\
&amp;=\int h(d, x, u) f(u \mid x) d u .
\end{aligned}
\]</span></p>
<p>Under the CIA the treatment effect measured by the regression is</p>
<p><span class="math display">\[
\begin{aligned}
\nabla m(d, x) &amp;=m(1, x)-m(0, x) \\
&amp;=\int h(1, x, u) f(u \mid x) d u-\int h(0, x, u) f(u \mid x) d u \\
&amp;=\int C(x, u) f(u \mid x) d u \\
&amp;=\operatorname{ACE}(x) .
\end{aligned}
\]</span></p>
<p>This is the conditional ACE. Thus under the CIA the regression coefficient equals the ACE.</p>
<p>We deduce that the regression of <span class="math inline">\(Y\)</span> on <span class="math inline">\((D, X)\)</span> reveals the causal impact of treatment when the CIA holds. This means that regression analysis can be interpreted causally when we can make the case that the regressors <span class="math inline">\(X\)</span> are sufficient to control for factors which are correlated with treatment.</p>
<p>Theorem 2.12 In the structural model (2.54), the Conditional Independence Assumption implies <span class="math inline">\(\nabla m(d, x)=\operatorname{ACE}(x)\)</span>, that the regression derivative with respect to treatment equals the conditional ACE.</p>
<p>This is a fascinating result. It shows that whenever the unobservable is independent of the treatment variable after conditioning on appropriate regressors, the regression derivative equals the conditional causal effect. This means the CEF has causal economic meaning, giving strong justification to estimation of the CEF.</p>
<p>It is important to understand the critical role of the CIA. If CIA fails then the equality (2.55) of the regression derivative and the ACE fails. The CIA states that conditional on <span class="math inline">\(X\)</span> the variables <span class="math inline">\(U\)</span> and <span class="math inline">\(D\)</span> are independent. This means that treatment <span class="math inline">\(D\)</span> is not affected by the unobserved individual factors <span class="math inline">\(U\)</span> and is effectively random. It is a strong assumption. In the wage/education example it means that education is not selected by individuals based on their unobserved characteristics.</p>
<p>However, it is also helpful to understand that the CIA is weaker than full independence of <span class="math inline">\(U\)</span> from the regressors <span class="math inline">\((D, X)\)</span>. What is required is only that <span class="math inline">\(U\)</span> and <span class="math inline">\(D\)</span> are independent after conditioning on <span class="math inline">\(X\)</span>. If <span class="math inline">\(X\)</span> is sufficiently rich this may not be restrictive.</p>
<p>Returning to our example, we require a variable <span class="math inline">\(X\)</span> which breaks the dependence between <span class="math inline">\(D\)</span> and <span class="math inline">\(U\)</span>. In our example, this variable is the aptitude test score, because the decision to attend college was based on the test score. It follows that educational attainment and type are independent once we condition on the test score.</p>
<p>To see this, observe that if a student’s test score is <span class="math inline">\(\mathrm{H}\)</span> the probability they go to college <span class="math inline">\((D=1)\)</span> is <span class="math inline">\(3 / 4\)</span> for both Jennifers and Georges. Similarly, if their test score is <span class="math inline">\(\mathrm{L}\)</span> the probability they go to college is <span class="math inline">\(1 / 4\)</span> for both types. This means that college attendence is independent of type, conditional on the aptitude test score.</p>
<p>The conditional ACE depends on the test score. Among students who receive a high test score, <span class="math inline">\(3 / 4\)</span> are Jennifers and <span class="math inline">\(1 / 4\)</span> are Georges. Thus the conditional ACE for students with a score of <span class="math inline">\(\mathrm{H}\)</span> is <span class="math inline">\((3 / 4) \times 10+\)</span> <span class="math inline">\((1 / 4) \times 4=\$ 8.50\)</span>. Among students who receive a low test score, <span class="math inline">\(1 / 4\)</span> are Jennifers and <span class="math inline">\(3 / 4\)</span> are Georges. Thus the ACE for students with a score of <span class="math inline">\(\mathrm{L}\)</span> is <span class="math inline">\((1 / 4) \times 10+(3 / 4) \times 4=\$ 5.50\)</span>. The unconditional ACE is the average, <span class="math inline">\(\mathrm{ACE}=(8.50+5.50) / 2=\$ 7\)</span>, because <span class="math inline">\(50 %\)</span> of students each receive scores of <span class="math inline">\(\mathrm{H}\)</span> and <span class="math inline">\(\mathrm{L}\)</span>.</p>
<p>Theorem <span class="math inline">\(2.12\)</span> shows that the conditional ACE is revealed by a regression which includes test scores. To see this in the wage distribution, suppose that the econometrician collects data on the aptitude test score as well as education and wages. Given a random sample of 32 individuals we would expect to find the wage distribution in Table <span class="math inline">\(2.4\)</span>.</p>
<p>Define a dummy highscore to indicate students who received a high test score. The regression of wages on college attendance and test scores with their interaction is</p>
<p><span class="math display">\[
\mathbb{E}[\text { wage } \mid \text { college, highscore }]=1.00 \text { highscore }+5.50 \text { college }+3.00 \text { highscore } \times \text { college }+8.50 \text {. }
\]</span></p>
<p>The coefficient on college, <span class="math inline">\(\$ 5.50\)</span>, is the regression derivative of college attendance for those with low test scores, and the sum of this coefficient with the interaction coefficient <span class="math inline">\(\$ 3.00\)</span> equals <span class="math inline">\(\$ 8.50\)</span> which is the Table 2.4: Example Distribution 2</p>
<table class="table">
<colgroup>
<col style="width: 50%">
<col style="width: 7%">
<col style="width: 9%">
<col style="width: 9%">
<col style="width: 9%">
<col style="width: 14%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;"></th>
<th style="text-align: right;"><span class="math inline">\(\$ 8\)</span></th>
<th style="text-align: right;"><span class="math inline">\(\$ 10\)</span></th>
<th style="text-align: right;"><span class="math inline">\(\$ 12\)</span></th>
<th style="text-align: right;"><span class="math inline">\(\$ 20\)</span></th>
<th style="text-align: right;">Mean</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">High-School Graduate + High Test Score</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">3</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;"><span class="math inline">\(\$ 9.50\)</span></td>
</tr>
<tr class="even">
<td style="text-align: left;">College Graduate + High Test Score</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">3</td>
<td style="text-align: right;">9</td>
<td style="text-align: right;"><span class="math inline">\(\$ 18.00\)</span></td>
</tr>
<tr class="odd">
<td style="text-align: left;">High-School Graduate + Low Test Score</td>
<td style="text-align: right;">9</td>
<td style="text-align: right;">3</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;"><span class="math inline">\(\$ 8.50\)</span></td>
</tr>
<tr class="even">
<td style="text-align: left;">College Graduate + Low Test Score</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">3</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;"><span class="math inline">\(\$ 14.00\)</span></td>
</tr>
</tbody>
</table>
<p>regression derivative for college attendance for those with high test scores. <span class="math inline">\(\$ 5.50\)</span> and <span class="math inline">\(\$ 8.50\)</span> equal the conditional causal effects as calculated above.</p>
<p>This shows that from the regression (2.56) an econometrician will find that the effect of college on wages is <span class="math inline">\(\$ 8.50\)</span> for those with high test scores and <span class="math inline">\(\$ 5.50\)</span> for those with low test scores with an average effect of <span class="math inline">\(\$ 7\)</span> (because <span class="math inline">\(50 %\)</span> of students receive high and low test scores). This is the true average causal effect of college on wages. Thus the regression coefficient on college in (2.56) can be interpreted causally, while a regression omitting the aptitude test score does not reveal the causal effect of education.</p>
<p>To summarize our findings, we have shown how it is possible that a simple regression will give a false measurement of a causal effect, but a more careful regression can reveal the true causal effect. The key is to condition on a suitably rich set of covariates such that the remaining unobserved factors affecting the outcome are independent of the treatment variable.</p>
</section>
<section id="existence-and-uniqueness-of-the-conditional-expectation" class="level2" data-number="2.35">
<h2 data-number="2.35" class="anchored" data-anchor-id="existence-and-uniqueness-of-the-conditional-expectation"><span class="header-section-number">2.35</span> Existence and Uniqueness of the Conditional Expectation*</h2>
<p>In Sections <span class="math inline">\(2.3\)</span> and <span class="math inline">\(2.6\)</span> we defined the conditional expectation when the conditioning variables <span class="math inline">\(X\)</span> are discrete and when the variables <span class="math inline">\((Y, X)\)</span> have a joint density. We have explored these cases because these are the situations where the conditional mean is easiest to describe and understand. However, the conditional mean exists quite generally without appealing to the properties of either discrete or continuous random variables.</p>
<p>To justify this claim we now present a deep result from probability theory. What it says is that the conditional mean exists for all joint distributions <span class="math inline">\((Y, X)\)</span> for which <span class="math inline">\(Y\)</span> has a finite mean.</p>
<p>Theorem 2.13 Existence of the Conditional Expectation If <span class="math inline">\(\mathbb{E}|Y|&lt;\infty\)</span> then there exists a function <span class="math inline">\(m(x)\)</span> such that for all sets <span class="math inline">\(\mathscr{X}\)</span> for which <span class="math inline">\(\mathbb{P}[X \in \mathscr{X}]\)</span> is defined,</p>
<p><span class="math display">\[
\mathbb{E}[\mathbb{1}\{X \in \mathscr{X}\} Y]=\mathbb{E}[\mathbb{1}\{X \in \mathscr{X}\} m(X)]
\]</span></p>
<p>The function <span class="math inline">\(m(X)\)</span> is almost everywhere unique, in the sense that if <span class="math inline">\(h(x)\)</span> satisfies (2.57), then there is a set <span class="math inline">\(S\)</span> such that <span class="math inline">\(\mathbb{P}[S]=1\)</span> and <span class="math inline">\(m(x)=h(x)\)</span> for <span class="math inline">\(x \in S\)</span>. The function <span class="math inline">\(m(x)\)</span> is called the conditional expectation and is written <span class="math inline">\(m(x)=\mathbb{E}[Y \mid X=x]\)</span></p>
<p>See, for example, Ash (1972), Theorem 6.3.3.</p>
<p>The conditional expectation <span class="math inline">\(m(x)\)</span> defined by (2.57) specializes to (2.6) when <span class="math inline">\((Y, X)\)</span> have a joint density. The usefulness of definition (2.57) is that Theorem <span class="math inline">\(2.13\)</span> shows that the conditional expectation <span class="math inline">\(m(X)\)</span> exists for all finite-mean distributions. This definition allows <span class="math inline">\(Y\)</span> to be discrete or continuous, for <span class="math inline">\(X\)</span> to be scalar or vector-valued, and for the components of <span class="math inline">\(X\)</span> to be discrete or continuously distributed.</p>
<p>You may have noticed that Theorem <span class="math inline">\(2.13\)</span> applies only to sets <span class="math inline">\(\mathscr{X}\)</span> for which <span class="math inline">\(\mathbb{P}[X \in \mathscr{X}]\)</span> is defined. This is a technical issue - measurability - which we largely side-step in this textbook. Formal probability theory only applies to sets which are measurable - for which probabilities are defined - as it turns out that not all sets satisfy measurability. This is not a practical concern for applications, so we defer such distinctions for formal theoretical treatments.</p>
</section>
<section id="identification" class="level2" data-number="2.36">
<h2 data-number="2.36" class="anchored" data-anchor-id="identification"><span class="header-section-number">2.36</span> Identification*</h2>
<p>A critical and important issue in structural econometric modeling is identification, meaning that a parameter is uniquely determined by the distribution of the observed variables. It is relatively straightforward in the context of the unconditional and conditional expectation, but it is worthwhile to introduce and explore the concept at this point for clarity.</p>
<p>Let <span class="math inline">\(F\)</span> denote the distribution of the observed data, for example the distribution of the pair <span class="math inline">\((Y, X)\)</span>. Let <span class="math inline">\(\mathscr{F}\)</span> be a collection of distributions <span class="math inline">\(F\)</span>. Let <span class="math inline">\(\theta\)</span> be a parameter of interest (for example, the expectation <span class="math inline">\(\mathbb{E}[Y])\)</span>.</p>
<p>Definition 2.10 A parameter <span class="math inline">\(\theta \in \mathbb{R}\)</span> is identified on <span class="math inline">\(\mathscr{F}\)</span> if for all <span class="math inline">\(F \in \mathscr{F}\)</span>, there is a uniquely determined value of <span class="math inline">\(\theta\)</span>.</p>
<p>Equivalently, <span class="math inline">\(\theta\)</span> is identified if we can write it as a mapping <span class="math inline">\(\theta=g(F)\)</span> on the set <span class="math inline">\(\mathscr{F}\)</span>. The restriction to the set <span class="math inline">\(\mathscr{F}\)</span> is important. Most parameters are identified only on a strict subset of the space of all distributions.</p>
<p>Take, for example, the expectation <span class="math inline">\(\mu=\mathbb{E}[Y]\)</span>. It is uniquely determined if <span class="math inline">\(\mathbb{E}|Y|&lt;\infty\)</span>, so <span class="math inline">\(\mu\)</span> is identified for the set <span class="math inline">\(\mathscr{F}=\{F: \mathbb{E}|Y|&lt;\infty\}\)</span>.</p>
<p>Next, consider the conditional expectation. Theorem <span class="math inline">\(2.13\)</span> demonstrates that <span class="math inline">\(\mathbb{E}|Y|&lt;\infty\)</span> is a sufficient condition for identification.</p>
<p>Theorem 2.14 Identification of the Conditional Expectation If <span class="math inline">\(\mathbb{E}|Y|&lt;\infty\)</span>, the conditional expectation <span class="math inline">\(m(x)=\mathbb{E}[Y \mid X=x]\)</span> is identified almost everywhere.</p>
<p>It might seem as if identification is a general property for parameters so long as we exclude degenerate cases. This is true for moments of observed data, but not necessarily for more complicated models. As a case in point, consider the context of censoring. Let <span class="math inline">\(Y\)</span> be a random variable with distribution <span class="math inline">\(F\)</span>. Instead of observing <span class="math inline">\(Y\)</span>, we observe <span class="math inline">\(Y^{*}\)</span> defined by the censoring rule</p>
<p><span class="math display">\[
Y^{*}=\left\{\begin{array}{cc}
Y &amp; \text { if } Y \leq \tau \\
\tau &amp; \text { if } Y&gt;\tau
\end{array}\right.
\]</span></p>
<p>That is, <span class="math inline">\(Y^{*}\)</span> is capped at the value <span class="math inline">\(\tau\)</span>. A common example is income surveys, where income responses are “top-coded” meaning that incomes above the top code <span class="math inline">\(\tau\)</span> are recorded as the top code. The observed variable <span class="math inline">\(Y^{*}\)</span> has distribution</p>
<p><span class="math display">\[
F^{*}(u)=\left\{\begin{array}{cc}
F(u) &amp; \text { for } u \leq \tau \\
1 &amp; \text { for } u \geq \tau .
\end{array}\right.
\]</span></p>
<p>We are interested in features of the distribution <span class="math inline">\(F\)</span> not the censored distribution <span class="math inline">\(F^{*}\)</span>. For example, we are interested in the expected wage <span class="math inline">\(\mu=\mathbb{E}[Y]\)</span>. The difficulty is that we cannot calculate <span class="math inline">\(\mu\)</span> from <span class="math inline">\(F^{*}\)</span> except in the trivial case where there is no censoring <span class="math inline">\(\mathbb{P}[Y \geq \tau]=0\)</span>. Thus the expectation <span class="math inline">\(\mu\)</span> is not generically identified from the censored distribution.</p>
<p>A typical solution to the identification problem is to assume a parametric distribution. For example, let <span class="math inline">\(\mathscr{F}\)</span> be the set of normal distributions <span class="math inline">\(Y \sim \mathrm{N}\left(\mu, \sigma^{2}\right)\)</span>. It is possible to show that the parameters <span class="math inline">\(\left(\mu, \sigma^{2}\right)\)</span> are identified for all <span class="math inline">\(F \in \mathscr{F}\)</span>. That is, if we know that the uncensored distribution is normal we can uniquely determine the parameters from the censored distribution. This is often called parametric identification as identification is restricted to a parametric class of distributions. In modern econometrics this is generally viewed as a second-best solution as identification has been achieved only through the use of an arbitrary and unverifiable parametric assumption.</p>
<p>A pessimistic conclusion might be that it is impossible to identify parameters of interest from censored data without parametric assumptions. Interestingly, this pessimism is unwarranted. It turns out that we can identify the quantiles <span class="math inline">\(q_{\alpha}\)</span> of <span class="math inline">\(F\)</span> for <span class="math inline">\(\alpha \leq \mathbb{P}[Y \leq \tau]\)</span>. For example, if <span class="math inline">\(20 %\)</span> of the distribution is censored we can identify all quantiles for <span class="math inline">\(\alpha \in(0,0.8)\)</span>. This is often called nonparametric identification as the parameters are identified without restriction to a parametric class.</p>
<p>What we have learned from this little exercise is that in the context of censored data moments can only be parametrically identified while non-censored quantiles are nonparametrically identified. Part of the message is that a study of identification can help focus attention on what can be learned from the data distributions available.</p>
</section>
<section id="technical-proofs" class="level2" data-number="2.37">
<h2 data-number="2.37" class="anchored" data-anchor-id="technical-proofs"><span class="header-section-number">2.37</span> Technical Proofs*</h2>
<p>Proof of Theorem 2.1 For convenience, assume that the variables have a joint density <span class="math inline">\(f(y, x)\)</span>. Since <span class="math inline">\(\mathbb{E}[Y \mid X]\)</span> is a function of the random vector <span class="math inline">\(X\)</span> only, to calculate its expectation we integrate with respect to the density <span class="math inline">\(f_{X}(x)\)</span> of <span class="math inline">\(X\)</span>, that is</p>
<p><span class="math display">\[
\mathbb{E}[\mathbb{E}[Y \mid X]]=\int_{\mathbb{R}^{k}} \mathbb{E}[Y \mid X] f_{X}(x) d x .
\]</span></p>
<p>Substituting in (2.6) and noting that <span class="math inline">\(f_{Y \mid X}(y \mid x) f_{X}(x)=f(y, x)\)</span>, we find that the above expression equals</p>
<p><span class="math display">\[
\int_{\mathbb{R}^{k}}\left(\int_{\mathbb{R}} y f_{Y \mid X}(y \mid x) d y\right) f_{X}(x) d x=\int_{\mathbb{R}^{k}} \int_{\mathbb{R}} y f(y, x) d y d x=\mathbb{E}[Y]
\]</span></p>
<p>the unconditional expectation of <span class="math inline">\(Y\)</span>.</p>
<p>Proof of Theorem 2.2 Again assume that the variables have a joint density. It is useful to observe that</p>
<p><span class="math display">\[
f\left(y \mid x_{1}, x_{2}\right) f\left(x_{2} \mid x_{1}\right)=\frac{f\left(y, x_{1}, x_{2}\right)}{f\left(x_{1}, x_{2}\right)} \frac{f\left(x_{1}, x_{2}\right)}{f\left(x_{1}\right)}=f\left(y, x_{2} \mid x_{1}\right)
\]</span></p>
<p>the density of <span class="math inline">\(\left(Y, X_{2}\right)\)</span> given <span class="math inline">\(X_{1}\)</span>. Here, we have abused notation and used a single symbol <span class="math inline">\(f\)</span> to denote the various unconditional and conditional densities to reduce notational clutter.</p>
<p>Note that</p>
<p><span class="math display">\[
\mathbb{E}\left[Y \mid X_{1}=x_{1}, X_{2}=x_{2}\right]=\int_{\mathbb{R}} y f\left(y \mid x_{1}, x_{2}\right) d y .
\]</span></p>
<p>Integrating (2.59) with respect to the conditional density of <span class="math inline">\(X_{2}\)</span> given <span class="math inline">\(X_{1}\)</span>, and applying (2.58) we find that</p>
<p><span class="math display">\[
\begin{aligned}
\mathbb{E}\left[\mathbb{E}\left[Y \mid X_{1}, X_{2}\right] \mid X_{1}=x_{1}\right] &amp;=\int_{\mathbb{R}^{k_{2}}} \mathbb{E}\left[Y \mid X_{1}=x_{1}, X_{2}=x_{2}\right] f\left(x_{2} \mid x_{1}\right) d x_{2} \\
&amp;=\int_{\mathbb{R}^{k_{2}}}\left(\int_{\mathbb{R}} y f\left(y \mid x_{1}, x_{2}\right) d y\right) f\left(x_{2} \mid x_{1}\right) d x_{2} \\
&amp;=\int_{\mathbb{R}^{k_{2}}} \int_{\mathbb{R}} y f\left(y \mid x_{1}, x_{2}\right) f\left(x_{2} \mid x_{1}\right) d y d x_{2} \\
&amp;=\int_{\mathbb{R}^{k_{2}}} \int_{\mathbb{R}} y f\left(y, x_{2} \mid x_{1}\right) d y d x_{2} \\
&amp;=\mathbb{E}\left[Y \mid X_{1}=x_{1}\right] .
\end{aligned}
\]</span></p>
<p>This implies <span class="math inline">\(\mathbb{E}\left[\mathbb{E}\left[Y \mid X_{1}, X_{2}\right] \mid X_{1}\right]=\mathbb{E}\left[Y \mid X_{1}\right]\)</span> as stated.</p>
<p>Proof of Theorem 2.3</p>
<p><span class="math display">\[
\mathbb{E}[g(X) Y \mid X=x]=\int_{\mathbb{R}} g(x) y f_{Y \mid X}(y \mid x) d y=g(x) \int_{\mathbb{R}} y f_{Y \mid X}(y \mid x) d y=g(x) \mathbb{E}[Y \mid X=x]
\]</span></p>
<p>This implies <span class="math inline">\(\mathbb{E}[g(X) Y \mid X]=g(X) \mathbb{E}[Y \mid X]\)</span> which is (2.7). Equation (2.8) follows by applying the simple law of iterated expectations (Theorem 2.1) to (2.7).</p>
<p>Proof of Theorem 2.4 Applying Minkowski’s inequality (B.34) to <span class="math inline">\(e=Y-m(X)\)</span>,</p>
<p><span class="math display">\[
\left(\mathbb{E}|e|^{r}\right)^{1 / r}=\left(\mathbb{E}|Y-m(X)|^{r}\right)^{1 / r} \leq\left(\mathbb{E}|Y|^{r}\right)^{1 / r}+\left(\mathbb{E}|m(X)|^{r}\right)^{1 / r}&lt;\infty,
\]</span></p>
<p>where the two parts on the right-hand-side are finite because <span class="math inline">\(\mathbb{E}|Y|^{r}&lt;\infty\)</span> by assumption and <span class="math inline">\(\mathbb{E}|m(X)|^{r}&lt;\)</span> <span class="math inline">\(\infty\)</span> by the conditional expectation inequality (B.29). The fact that <span class="math inline">\(\left(\mathbb{E}|e|^{r}\right)^{1 / r}&lt;\infty\)</span> implies <span class="math inline">\(\mathbb{E}|e|^{r}&lt;\infty\)</span>.</p>
<p>Proof of Theorem 2.6 The assumption that <span class="math inline">\(\mathbb{E}\left[Y^{2}\right]&lt;\infty\)</span> implies that all the conditional expectations below exist.</p>
<p>Using the law of iterated expectations (Theorem 2.2) <span class="math inline">\(\mathbb{E}\left[Y \mid X_{1}\right]=\mathbb{E}\left(\mathbb{E}\left[Y \mid X_{1}, X_{2}\right] \mid X_{1}\right)\)</span> and the conditional Jensen’s inequality (B.28),</p>
<p><span class="math display">\[
\left(\mathbb{E}\left[Y \mid X_{1}\right]\right)^{2}=\left(\mathbb{E}\left(\mathbb{E}\left[Y \mid X_{1}, X_{2}\right] \mid X_{1}\right)\right)^{2} \leq \mathbb{E}\left[\left(\mathbb{E}\left[Y \mid X_{1}, X_{2}\right]\right)^{2} \mid X_{1}\right] .
\]</span></p>
<p>Taking unconditional expectations, this implies</p>
<p><span class="math display">\[
\mathbb{E}\left[\left(\mathbb{E}\left[Y \mid X_{1}\right]\right)^{2}\right] \leq \mathbb{E}\left[\left(\mathbb{E}\left[Y \mid X_{1}, X_{2}\right]\right)^{2}\right] .
\]</span></p>
<p>Similarly,</p>
<p><span class="math display">\[
(\mathbb{E}[Y])^{2} \leq \mathbb{E}\left[\left(\mathbb{E}\left[Y \mid X_{1}\right]\right)^{2}\right] \leq \mathbb{E}\left[\left(\mathbb{E}\left[Y \mid X_{1}, X_{2}\right]\right)^{2}\right] .
\]</span></p>
<p>The variables <span class="math inline">\(Y, \mathbb{E}\left[Y \mid X_{1}\right]\)</span>, and <span class="math inline">\(\mathbb{E}\left[Y \mid X_{1}, X_{2}\right]\)</span> all have the same expectation <span class="math inline">\(\mathbb{E}[Y]\)</span>, so the inequality (2.60) implies that the variances are ranked monotonically:</p>
<p><span class="math display">\[
0 \leq \operatorname{var}\left(\mathbb{E}\left[Y \mid X_{1}\right]\right) \leq \operatorname{var}\left(\mathbb{E}\left[Y \mid X_{1}, X_{2}\right]\right) .
\]</span></p>
<p>Define <span class="math inline">\(e=Y-\mathbb{E}[Y \mid X]\)</span> and <span class="math inline">\(u=\mathbb{E}[Y \mid X]-\mu\)</span> so that we have the decomposition <span class="math inline">\(Y-\mu=e+u\)</span>. Notice <span class="math inline">\(\mathbb{E}[e \mid X]=0\)</span> and <span class="math inline">\(u\)</span> is a function of <span class="math inline">\(X\)</span>. Thus by the conditioning theorem (Theorem 2.3), <span class="math inline">\(\mathbb{E}[e u]=0\)</span> so <span class="math inline">\(e\)</span> and <span class="math inline">\(u\)</span> are uncorrelated. It follows that</p>
<p><span class="math display">\[
\operatorname{var}[Y]=\operatorname{var}[e]+\operatorname{var}[u]=\operatorname{var}[Y-\mathbb{E}[Y \mid X]]+\operatorname{var}[\mathbb{E}[Y \mid X]]
\]</span></p>
<p>The monotonicity of the variances of the conditional expectation (2.61) applied to the variance decomposition (2.62) implies the reverse monotonicity of the variances of the differences, completing the proof.</p>
<p>Proof of Theorem 2.9 For part 1, by the expectation inequality (B.30), (A.17) and Assumption 2.1,</p>
<p><span class="math display">\[
\left\|\mathbb{E}\left[X X^{\prime}\right]\right\| \leq \mathbb{E}\left\|X X^{\prime}\right\|=\mathbb{E}\|X\|^{2}&lt;\infty .
\]</span></p>
<p>Similarly, using the expectation inequality (B.30), the Cauchy-Schwarz inequality (B.32), and Assumption 2.1,</p>
<p><span class="math display">\[
\|\mathbb{E}[X Y]\| \leq \mathbb{E}\|X Y\| \leq\left(\mathbb{E}\|X\|^{2}\right)^{1 / 2}\left(\mathbb{E}\left[Y^{2}\right]\right)^{1 / 2}&lt;\infty .
\]</span></p>
<p>Thus the moments <span class="math inline">\(\mathbb{E}[X Y]\)</span> and <span class="math inline">\(\mathbb{E}\left[X X^{\prime}\right]\)</span> are finite and well defined.</p>
<p>For part 2, the coefficient <span class="math inline">\(\beta=\left(\mathbb{E}\left[X X^{\prime}\right]\right)^{-1} \mathbb{E}[X Y]\)</span> is well defined because <span class="math inline">\(\left(\mathbb{E}\left[X X^{\prime}\right]\right)^{-1}\)</span> exists under Assumption 2.1.</p>
<p>Part 3 follows from Definition <span class="math inline">\(2.5\)</span> and part <span class="math inline">\(2 .\)</span></p>
<p>For part 4, first note that</p>
<p><span class="math display">\[
\begin{aligned}
\mathbb{E}\left[e^{2}\right] &amp;=\mathbb{E}\left[\left(Y-X^{\prime} \beta\right)^{2}\right] \\
&amp;=\mathbb{E}\left[Y^{2}\right]-2 \mathbb{E}\left[Y X^{\prime}\right] \beta+\beta^{\prime} \mathbb{E}\left[X X^{\prime}\right] \beta \\
&amp;=\mathbb{E}\left[Y^{2}\right]-\mathbb{E}\left[Y X^{\prime}\right]\left(\mathbb{E}\left[X X^{\prime}\right]\right)^{-1} \mathbb{E}[X Y] \\
&amp; \leq \mathbb{E}\left[Y^{2}\right]&lt;\infty .
\end{aligned}
\]</span></p>
<p>The first inequality holds because <span class="math inline">\(\mathbb{E}\left[Y X^{\prime}\right]\left(\mathbb{E}\left[X X^{\prime}\right]\right)^{-1} \mathbb{E}[X Y]\)</span> is a quadratic form and therefore necessarily non-negative. Second, by the expectation inequality (B.30), the Cauchy-Schwarz inequality (B.32), and Assumption 2.1,</p>
<p><span class="math display">\[
\|\mathbb{E}[X e]\| \leq \mathbb{E}\|X e\|=\left(\mathbb{E}\|X\|^{2}\right)^{1 / 2}\left(\mathbb{E}\left[e^{2}\right]\right)^{1 / 2}&lt;\infty .
\]</span></p>
<p>It follows that the expectation <span class="math inline">\(\mathbb{E}[X e]\)</span> is finite, and is zero by the calculation (2.26).</p>
<p>For part 6, applying Minkowski’s inequality (B.34) to <span class="math inline">\(e=Y-X^{\prime} \beta\)</span>,</p>
<p><span class="math display">\[
\begin{aligned}
\left(\mathbb{E}|e|^{r}\right)^{1 / r} &amp;=\left(\mathbb{E}\left|Y-X^{\prime} \beta\right|^{r}\right)^{1 / r} \\
&amp; \leq\left(\mathbb{E}|Y|^{r}\right)^{1 / r}+\left(\mathbb{E}\left|X^{\prime} \beta\right|^{r}\right)^{1 / r} \\
&amp; \leq\left(\mathbb{E}|Y|^{r}\right)^{1 / r}+\left(\mathbb{E}\|X\|^{r}\right)^{1 / r}\|\beta\|&lt;\infty,
\end{aligned}
\]</span></p>
<p>the final inequality by assumption.</p>
</section>
<section id="exercises" class="level2" data-number="2.38">
<h2 data-number="2.38" class="anchored" data-anchor-id="exercises"><span class="header-section-number">2.38</span> Exercises</h2>
<p>Exercise 2.1 Find <span class="math inline">\(\mathbb{E}\left[\mathbb{E}\left[\mathbb{E}\left[Y \mid X_{1}, X_{2}, X_{3}\right] \mid X_{1}, X_{2}\right] \mid X_{1}\right]\)</span></p>
<p>Exercise 2.2 If <span class="math inline">\(\mathbb{E}[Y \mid X]=a+b X\)</span>, find <span class="math inline">\(\mathbb{E}[Y X]\)</span> as a function of moments of <span class="math inline">\(X\)</span>.</p>
<p>Exercise 2.3 Prove Theorem 2.4.4 using the law of iterated expectations. Exercise 2.4 Suppose that the random variables <span class="math inline">\(Y\)</span> and <span class="math inline">\(X\)</span> only take the values 0 and 1 , and have the following joint probability distribution</p>
<table class="table">
<thead>
<tr class="header">
<th></th>
<th><span class="math inline">\(X=0\)</span></th>
<th><span class="math inline">\(X=1\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(Y=0\)</span></td>
<td><span class="math inline">\(.1\)</span></td>
<td><span class="math inline">\(.2\)</span></td>
</tr>
<tr class="even">
<td><span class="math inline">\(Y=1\)</span></td>
<td><span class="math inline">\(.4\)</span></td>
<td><span class="math inline">\(.3\)</span></td>
</tr>
</tbody>
</table>
<p>Find <span class="math inline">\(\mathbb{E}[Y \mid X], \mathbb{E}\left[Y^{2} \mid X\right]\)</span>, and <span class="math inline">\(\operatorname{var}[Y \mid X]\)</span> for <span class="math inline">\(X=0\)</span> and <span class="math inline">\(X=1\)</span></p>
<p>Exercise 2.5 Show that <span class="math inline">\(\sigma^{2}(X)\)</span> is the best predictor of <span class="math inline">\(e^{2}\)</span> given <span class="math inline">\(X\)</span> :</p>
<ol type="a">
<li><p>Write down the mean-squared error of a predictor <span class="math inline">\(h(X)\)</span> for <span class="math inline">\(e^{2}\)</span>.</p></li>
<li><p>What does it mean to be predicting <span class="math inline">\(e^{2}\)</span> ?</p></li>
<li><p>Show that <span class="math inline">\(\sigma^{2}(X)\)</span> minimizes the mean-squared error and is thus the best predictor.</p></li>
</ol>
<p>Exercise 2.6 Use <span class="math inline">\(Y=m(X)+e\)</span> to show that <span class="math inline">\(\operatorname{var}[Y]=\operatorname{var}[m(X)]+\sigma^{2}\)</span></p>
<p>Exercise 2.7 Show that the conditional variance can be written as <span class="math inline">\(\sigma^{2}(X)=\mathbb{E}\left[Y^{2} \mid X\right]-(\mathbb{E}[Y \mid X])^{2}\)</span>.</p>
<p>Exercise 2.8 Suppose that <span class="math inline">\(Y\)</span> is discrete-valued, taking values only on the non-negative integers, and the conditional distribution of <span class="math inline">\(Y\)</span> given <span class="math inline">\(X=x\)</span> is Poisson:</p>
<p><span class="math display">\[
\mathbb{P}[Y=j \mid X=x]=\frac{\exp \left(-x^{\prime} \beta\right)\left(x^{\prime} \beta\right)^{j}}{j !}, \quad j=0,1,2, \ldots
\]</span></p>
<p>Compute <span class="math inline">\(\mathbb{E}[Y \mid X]\)</span> and <span class="math inline">\(\operatorname{var}[Y \mid X]\)</span>. Does this justify a linear regression model of the form <span class="math inline">\(Y=X^{\prime} \beta+e\)</span> ?</p>
<p><span class="math display">\[
\text { Hint: If } \mathbb{P}[Y=j]=\frac{\exp (-\lambda) \lambda^{j}}{j !} \text { then } \mathbb{E}[Y]=\lambda \text { and } \operatorname{var}[Y]=\lambda \text {. }
\]</span></p>
<p>Exercise 2.9 Suppose you have two regressors: <span class="math inline">\(X_{1}\)</span> is binary (takes values 0 and 1) and <span class="math inline">\(X_{2}\)</span> is categorical with 3 categories <span class="math inline">\((A, B, C)\)</span>. Write <span class="math inline">\(\mathbb{E}\left[Y \mid X_{1}, X_{2}\right]\)</span> as a linear regression.</p>
<p>Exercise 2.10 True or False. If <span class="math inline">\(Y=X \beta+e, X \in \mathbb{R}\)</span>, and <span class="math inline">\(\mathbb{E}[e \mid X]=0\)</span>, then <span class="math inline">\(\mathbb{E}\left[X^{2} e\right]=0\)</span>.</p>
<p>Exercise 2.11 True or False. If <span class="math inline">\(Y=X \beta+e, X \in \mathbb{R}\)</span>, and <span class="math inline">\(\mathbb{E}[X e]=0\)</span>, then <span class="math inline">\(\mathbb{E}\left[X^{2} e\right]=0\)</span>.</p>
<p>Exercise 2.12 True or False. If <span class="math inline">\(Y=X^{\prime} \beta+e\)</span> and <span class="math inline">\(\mathbb{E}[e \mid X]=0\)</span>, then <span class="math inline">\(e\)</span> is independent of <span class="math inline">\(X\)</span>.</p>
<p>Exercise 2.13 True or False. If <span class="math inline">\(Y=X^{\prime} \beta+e\)</span> and <span class="math inline">\(\mathbb{E}[X e]=0\)</span>, then <span class="math inline">\(\mathbb{E}[e \mid X]=0\)</span>.</p>
<p>Exercise 2.14 True or False. If <span class="math inline">\(Y=X^{\prime} \beta+e, \mathbb{E}[e \mid X]=0\)</span>, and <span class="math inline">\(\mathbb{E}\left[e^{2} \mid X\right]=\sigma^{2}\)</span>, then <span class="math inline">\(e\)</span> is independent of <span class="math inline">\(X\)</span>.</p>
<p>Exercise 2.15 Consider the intercept-only model <span class="math inline">\(Y=\alpha+e\)</span> with <span class="math inline">\(\alpha\)</span> the best linear predictor. Show that <span class="math inline">\(\alpha=\mathbb{E}[Y]\)</span></p>
<p>Exercise 2.16 Let <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> have the joint density <span class="math inline">\(f(x, y)=\frac{3}{2}\left(x^{2}+y^{2}\right)\)</span> on <span class="math inline">\(0 \leq x \leq 1,0 \leq y \leq 1\)</span>. Compute the coefficients of the best linear predictor <span class="math inline">\(Y=\alpha+\beta X+e\)</span>. Compute the conditional expectation <span class="math inline">\(m(x)=\)</span> <span class="math inline">\(\mathbb{E}[Y \mid X=x]\)</span>. Are the best linear predictor and conditional expectation different? Exercise 2.17 Let <span class="math inline">\(X\)</span> be a random variable with <span class="math inline">\(\mu=\mathbb{E}[X]\)</span> and <span class="math inline">\(\sigma^{2}=\operatorname{var}[X]\)</span>. Define</p>
<p><span class="math display">\[
g\left(x, \mu, \sigma^{2}\right)=\left(\begin{array}{c}
x-\mu \\
(x-\mu)^{2}-\sigma^{2}
\end{array}\right) .
\]</span></p>
<p>Show that <span class="math inline">\(\mathbb{E}[g(X, m, s)]=0\)</span> if and only if <span class="math inline">\(m=\mu\)</span> and <span class="math inline">\(s=\sigma^{2}\)</span>.</p>
<p>Exercise 2.18 Suppose that <span class="math inline">\(X=\left(1, X_{2}, X_{3}\right)\)</span> where <span class="math inline">\(X_{3}=\alpha_{1}+\alpha_{2} X_{2}\)</span> is a linear function of <span class="math inline">\(X_{2}\)</span>.</p>
<ol type="a">
<li><p>Show that <span class="math inline">\(\boldsymbol{Q}_{X X}=\mathbb{E}\left[X X^{\prime}\right]\)</span> is not invertible.</p></li>
<li><p>Use a linear transformation of <span class="math inline">\(X\)</span> to find an expression for the best linear predictor of <span class="math inline">\(Y\)</span> given <span class="math inline">\(X\)</span>. (Be explicit, do not just use the generalized inverse formula.)</p></li>
</ol>
<p>Exercise 2.19 Show (2.47)-(2.48), namely that for</p>
<p><span class="math display">\[
d(\beta)=\mathbb{E}\left[\left(m(X)-X^{\prime} \beta\right)^{2}\right]
\]</span></p>
<p>then</p>
<p><span class="math display">\[
\beta=\underset{b \in \mathbb{R}^{k}}{\operatorname{argmin}} d(b)=\left(\mathbb{E}\left[X X^{\prime}\right]\right)^{-1} \mathbb{E}[X m(X)]=\left(\mathbb{E}\left[X X^{\prime}\right]\right)^{-1} \mathbb{E}[X Y] .
\]</span></p>
<p>Hint: To show <span class="math inline">\(\mathbb{E}[X m(X)]=\mathbb{E}[X Y]\)</span> use the law of iterated expectations.</p>
<p>Exercise 2.20 Verify that (2.57) holds with <span class="math inline">\(m(X)\)</span> defined in (2.6) when <span class="math inline">\((Y, X)\)</span> have a joint density <span class="math inline">\(f(y, x)\)</span>.</p>
<p>Exercise 2.21 Consider the short and long projections</p>
<p><span class="math display">\[
\begin{gathered}
Y=X \gamma_{1}+e \\
Y=X \beta_{1}+X^{2} \beta_{2}+u
\end{gathered}
\]</span></p>
<ol type="a">
<li><p>Under what condition does <span class="math inline">\(\gamma_{1}=\beta_{1}\)</span> ?</p></li>
<li><p>Take the long projection is <span class="math inline">\(Y=X \theta_{1}+X^{3} \theta_{2}+v\)</span>. Is there a condition under which <span class="math inline">\(\gamma_{1}=\theta_{1}\)</span> ?</p></li>
</ol>
<p>Exercise 2.22 Take the homoskedastic model</p>
<p><span class="math display">\[
\begin{aligned}
Y &amp;=X_{1}^{\prime} \beta_{1}+X_{2}^{\prime} \beta_{2}+e \\
\mathbb{E}\left[e \mid X_{1}, X_{2}\right] &amp;=0 \\
\mathbb{E}\left[e^{2} \mid X_{1}, X_{2}\right] &amp;=\sigma^{2} \\
\mathbb{E}\left[X_{2} \mid X_{1}\right] &amp;=\Gamma X_{1} .
\end{aligned}
\]</span></p>
<p>Assume <span class="math inline">\(\Gamma \neq 0\)</span>. Suppose the parameter <span class="math inline">\(\beta_{1}\)</span> is of interest. We know that the exclusion of <span class="math inline">\(X_{2}\)</span> creates omited variable bias in the projection coefficient on <span class="math inline">\(X_{2}\)</span>. It also changes the equation error. Our question is: what is the effect on the homoskedasticity property of the induced equation error? Does the exclusion of <span class="math inline">\(X_{2}\)</span> induce heteroskedasticity or not? Be specific.</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    setTimeout(function() {
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      let href = ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const cites = ref.parentNode.getAttribute('data-cites').split(' ');
    tippyHover(ref, function() {
      var popup = window.document.createElement('div');
      cites.forEach(function(cite) {
        var citeDiv = window.document.createElement('div');
        citeDiv.classList.add('hanging-indent');
        citeDiv.classList.add('csl-entry');
        var biblioDiv = window.document.getElementById('ref-' + cite);
        if (biblioDiv) {
          citeDiv.innerHTML = biblioDiv.innerHTML;
        }
        popup.appendChild(citeDiv);
      });
      return popup.innerHTML;
    });
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./part01-reg.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text">回归</span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./chpt03-algebra.html" class="pagination-link">
        <span class="nav-page-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">The Algebra of Least Squares</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->



</body></html>
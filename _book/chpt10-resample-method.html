<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.0.36">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Hansen中高级计量体系 - 11&nbsp; Resampling Methods</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
span.underline{text-decoration: underline;}
div.column{display: inline-block; vertical-align: top; width: 50%;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./part03-MEQ.html" rel="next">
<link href="./chpt09-hypothesit-test.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
    <div class="container-fluid d-flex justify-content-between">
      <h1 class="quarto-secondary-nav-title"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Resampling Methods</span></h1>
      <button type="button" class="quarto-btn-toggle btn" aria-label="Show secondary navigation">
        <i class="bi bi-chevron-right"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Hansen中高级计量体系</a> 
        <div class="sidebar-tools-main">
    <a href="https://github.com/huhuaping/hansenEM/" title="Source Code" class="sidebar-tool px-1"><i class="bi bi-github"></i></a>
</div>
    </div>
      </div>
      <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
      </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">前言</a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./participate.html" class="sidebar-item-text sidebar-link">如何参与？</a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chpt01-intro.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./part01-reg.html" class="sidebar-item-text sidebar-link">回归</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chpt02-ce.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Conditional Expectation and Projection</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chpt02-ce-chn.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">条件预期和预测</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chpt03-algebra.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">The Algebra of Least Squares</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chpt04-lsr.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Least Squares Regression</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chpt05-normal-reg.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Normal Regression</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./part02-LSM.html" class="sidebar-item-text sidebar-link">大样本方法</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chpt06-review.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">A Review of Large Sample Asymptotics</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chpt07-asymptotic-ls.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Asymptotic Theory for Least Squares</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chpt08-restricted-est.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Restricted Estimation</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chpt09-hypothesit-test.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Hypothesis Testing</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chpt10-resample-method.html" class="sidebar-item-text sidebar-link active"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Resampling Methods</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./part03-MEQ.html" class="sidebar-item-text sidebar-link">多方程模型</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chpt11-multi-reg.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Multivariate Regression</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chpt12-iv.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Instrumental Variables</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chpt13-gmm.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Generalized Method of Moments</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./summary.html" class="sidebar-item-text sidebar-link">Summary</a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">References</a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" aria-expanded="true">Appendices</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./a1-notation.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">A</span>&nbsp; <span class="chapter-title">附录a1</span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction"> <span class="header-section-number">11.1</span> Introduction</a></li>
  <li><a href="#example" id="toc-example" class="nav-link" data-scroll-target="#example"> <span class="header-section-number">11.2</span> Example</a></li>
  <li><a href="#jackknife-estimation-of-variance" id="toc-jackknife-estimation-of-variance" class="nav-link" data-scroll-target="#jackknife-estimation-of-variance"> <span class="header-section-number">11.3</span> Jackknife Estimation of Variance</a></li>
  <li><a href="#example-1" id="toc-example-1" class="nav-link" data-scroll-target="#example-1"> <span class="header-section-number">11.4</span> Example</a></li>
  <li><a href="#jackknife-for-clustered-observations" id="toc-jackknife-for-clustered-observations" class="nav-link" data-scroll-target="#jackknife-for-clustered-observations"> <span class="header-section-number">11.5</span> Jackknife for Clustered Observations</a></li>
  <li><a href="#the-bootstrap-algorithm" id="toc-the-bootstrap-algorithm" class="nav-link" data-scroll-target="#the-bootstrap-algorithm"> <span class="header-section-number">11.6</span> The Bootstrap Algorithm</a></li>
  <li><a href="#bootstrap-variance-and-standard-errors" id="toc-bootstrap-variance-and-standard-errors" class="nav-link" data-scroll-target="#bootstrap-variance-and-standard-errors"> <span class="header-section-number">11.7</span> Bootstrap Variance and Standard Errors</a></li>
  <li><a href="#percentile-interval" id="toc-percentile-interval" class="nav-link" data-scroll-target="#percentile-interval"> <span class="header-section-number">11.8</span> Percentile Interval</a></li>
  <li><a href="#the-bootstrap-distribution" id="toc-the-bootstrap-distribution" class="nav-link" data-scroll-target="#the-bootstrap-distribution"> <span class="header-section-number">11.9</span> The Bootstrap Distribution</a></li>
  <li><a href="#the-distribution-of-the-bootstrap-observations" id="toc-the-distribution-of-the-bootstrap-observations" class="nav-link" data-scroll-target="#the-distribution-of-the-bootstrap-observations"> <span class="header-section-number">11.10</span> The Distribution of the Bootstrap Observations</a></li>
  <li><a href="#the-distribution-of-the-bootstrap-sample-mean" id="toc-the-distribution-of-the-bootstrap-sample-mean" class="nav-link" data-scroll-target="#the-distribution-of-the-bootstrap-sample-mean"> <span class="header-section-number">11.11</span> The Distribution of the Bootstrap Sample Mean</a></li>
  <li><a href="#bootstrap-asymptotics" id="toc-bootstrap-asymptotics" class="nav-link" data-scroll-target="#bootstrap-asymptotics"> <span class="header-section-number">11.12</span> Bootstrap Asymptotics</a></li>
  <li><a href="#consistency-of-the-bootstrap-estimate-of-variance" id="toc-consistency-of-the-bootstrap-estimate-of-variance" class="nav-link" data-scroll-target="#consistency-of-the-bootstrap-estimate-of-variance"> <span class="header-section-number">11.13</span> Consistency of the Bootstrap Estimate of Variance</a></li>
  <li><a href="#trimmed-estimator-of-bootstrap-variance" id="toc-trimmed-estimator-of-bootstrap-variance" class="nav-link" data-scroll-target="#trimmed-estimator-of-bootstrap-variance"> <span class="header-section-number">11.14</span> Trimmed Estimator of Bootstrap Variance</a></li>
  <li><a href="#unreliability-of-untrimmed-bootstrap-standard-errors" id="toc-unreliability-of-untrimmed-bootstrap-standard-errors" class="nav-link" data-scroll-target="#unreliability-of-untrimmed-bootstrap-standard-errors"> <span class="header-section-number">11.15</span> Unreliability of Untrimmed Bootstrap Standard Errors</a></li>
  <li><a href="#consistency-of-the-percentile-interval" id="toc-consistency-of-the-percentile-interval" class="nav-link" data-scroll-target="#consistency-of-the-percentile-interval"> <span class="header-section-number">11.16</span> Consistency of the Percentile Interval</a></li>
  <li><a href="#bias-corrected-percentile-interval" id="toc-bias-corrected-percentile-interval" class="nav-link" data-scroll-target="#bias-corrected-percentile-interval"> <span class="header-section-number">11.17</span> Bias-Corrected Percentile Interval</a></li>
  <li><a href="#mathrmbc_a-percentile-interval" id="toc-mathrmbc_a-percentile-interval" class="nav-link" data-scroll-target="#mathrmbc_a-percentile-interval"> <span class="header-section-number">11.18</span> <span class="math inline">\(\mathrm{BC}_{a}\)</span> Percentile Interval</a></li>
  <li><a href="#percentile-t-interval" id="toc-percentile-t-interval" class="nav-link" data-scroll-target="#percentile-t-interval"> <span class="header-section-number">11.19</span> Percentile-t Interval</a></li>
  <li><a href="#percentile-t-asymptotic-refinement" id="toc-percentile-t-asymptotic-refinement" class="nav-link" data-scroll-target="#percentile-t-asymptotic-refinement"> <span class="header-section-number">11.20</span> Percentile-t Asymptotic Refinement</a></li>
  <li><a href="#bootstrap-hypothesis-tests" id="toc-bootstrap-hypothesis-tests" class="nav-link" data-scroll-target="#bootstrap-hypothesis-tests"> <span class="header-section-number">11.21</span> Bootstrap Hypothesis Tests</a></li>
  <li><a href="#wald-type-bootstrap-tests" id="toc-wald-type-bootstrap-tests" class="nav-link" data-scroll-target="#wald-type-bootstrap-tests"> <span class="header-section-number">11.22</span> Wald-Type Bootstrap Tests</a></li>
  <li><a href="#criterion-based-bootstrap-tests" id="toc-criterion-based-bootstrap-tests" class="nav-link" data-scroll-target="#criterion-based-bootstrap-tests"> <span class="header-section-number">11.23</span> Criterion-Based Bootstrap Tests</a></li>
  <li><a href="#parametric-bootstrap" id="toc-parametric-bootstrap" class="nav-link" data-scroll-target="#parametric-bootstrap"> <span class="header-section-number">11.24</span> Parametric Bootstrap</a></li>
  <li><a href="#how-many-bootstrap-replications" id="toc-how-many-bootstrap-replications" class="nav-link" data-scroll-target="#how-many-bootstrap-replications"> <span class="header-section-number">11.25</span> How Many Bootstrap Replications?</a></li>
  <li><a href="#setting-the-bootstrap-seed" id="toc-setting-the-bootstrap-seed" class="nav-link" data-scroll-target="#setting-the-bootstrap-seed"> <span class="header-section-number">11.26</span> Setting the Bootstrap Seed</a></li>
  <li><a href="#bootstrap-regression" id="toc-bootstrap-regression" class="nav-link" data-scroll-target="#bootstrap-regression"> <span class="header-section-number">11.27</span> Bootstrap Regression</a></li>
  <li><a href="#bootstrap-regression-asymptotic-theory" id="toc-bootstrap-regression-asymptotic-theory" class="nav-link" data-scroll-target="#bootstrap-regression-asymptotic-theory"> <span class="header-section-number">11.28</span> Bootstrap Regression Asymptotic Theory</a></li>
  <li><a href="#wild-bootstrap" id="toc-wild-bootstrap" class="nav-link" data-scroll-target="#wild-bootstrap"> <span class="header-section-number">11.29</span> Wild Bootstrap</a></li>
  <li><a href="#bootstrap-for-clustered-observations" id="toc-bootstrap-for-clustered-observations" class="nav-link" data-scroll-target="#bootstrap-for-clustered-observations"> <span class="header-section-number">11.30</span> Bootstrap for Clustered Observations</a></li>
  <li><a href="#technical-proofs" id="toc-technical-proofs" class="nav-link" data-scroll-target="#technical-proofs"> <span class="header-section-number">11.31</span> Technical Proofs*</a></li>
  <li><a href="#exercises" id="toc-exercises" class="nav-link" data-scroll-target="#exercises"> <span class="header-section-number">11.32</span> Exercises</a></li>
  </ul>
<div class="toc-actions"><div><i class="bi bi-github"></i></div><div class="action-links"><p><a href="https://github.com/huhuaping/hansenEM/edit/master/chpt10-resample-method.qmd" class="toc-action">Edit this page</a></p></div></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title d-none d-lg-block"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Resampling Methods</span></h1>
</div>



<div class="quarto-title-meta">

    
    
  </div>
  

</header>

<section id="introduction" class="level2" data-number="11.1">
<h2 data-number="11.1" class="anchored" data-anchor-id="introduction"><span class="header-section-number">11.1</span> Introduction</h2>
<p>So far in this textbook we have discussed two approaches to inference: exact and asymptotic. Both have their strengths and weaknesses. Exact theory provides a useful benchmark but is based on the unrealistic and stringent assumption of the homoskedastic normal regression model. Asymptotic theory provides a more flexible distribution theory but is an approximation with uncertain accuracy.</p>
<p>In this chapter we introduce a set of alternative inference methods which are based around the concept of resampling - which means using sampling information extracted from the empirical distribution of the data. These are powerful methods, widely applicable, and often more accurate than exact methods and asymptotic approximations. Two disadvantages, however, are (1) resampling methods typically require more computation power; and (2) the theory is considerably more challenging. A consequence of the computation requirement is that most empirical researchers use asymptotic approximations for routine calculations while resampling approximations are used for final reporting.</p>
<p>We will discuss two categories of resampling methods used in statistical and econometric practice: jackknife and bootstrap. Most of our attention will be given to the bootstrap as it is the most commonly used resampling method in econometric practice.</p>
<p>The jackknife is the distribution obtained from the <span class="math inline">\(n\)</span> leave-one-out estimators (see Section 3.20). The jackknife is most commonly used for variance estimation.</p>
<p>The bootstrap is the distribution obtained by estimation on samples created by i.i.d. sampling with replacement from the dataset. (There are other variants of bootstrap sampling, including parametric sampling and residual sampling.) The bootstrap is commonly used for variance estimation, confidence interval construction, and hypothesis testing.</p>
<p>There is a third category of resampling methods known as sub-sampling which we will not cover in this textbook. Sub-sampling is the distribution obtained by estimation on sub-samples (sampling without replacement) of the dataset. Sub-sampling can be used for most of same purposes as the bootstrap. See the excellent monograph by Politis, Romano and Wolf (1999).</p>
</section>
<section id="example" class="level2" data-number="11.2">
<h2 data-number="11.2" class="anchored" data-anchor-id="example"><span class="header-section-number">11.2</span> Example</h2>
<p>To motivate our discussion we focus on the application presented in Section 3.7, which is a bivariate regression applied to the CPS subsample of married Black female wage earners with 12 years potential work experience and displayed in Table 3.1. The regression equation is</p>
<p><span class="math display">\[
\log (\text { wage })=\beta_{1} \text { education }+\beta_{2}+e .
\]</span></p>
<p>The estimates as reported in (4.44) are</p>
<p><span class="math display">\[
\begin{aligned}
&amp; \log (\text { wage })=0.155 \text { education }+0.698+\widehat{e} \\
&amp; \text { (0.031) } \quad(0.493) \\
&amp; \widehat{\sigma}^{2}=0.144 \\
&amp; \text { (0.043) } \\
&amp; n=20 \text {. }
\end{aligned}
\]</span></p>
<p>We focus on four estimates constructed from this regression. The first two are the coefficient estimates <span class="math inline">\(\widehat{\beta}_{1}\)</span> and <span class="math inline">\(\widehat{\beta}_{2}\)</span>. The third is the variance estimate <span class="math inline">\(\widehat{\sigma}^{2}\)</span>. The fourth is an estimate of the expected level of wages for an individual with 16 years of education (a college graduate), which turns out to be a nonlinear function of the parameters. Under the simplifying assumption that the error <span class="math inline">\(e\)</span> is independent of the level of education and normally distributed we find that the expected level of wages is</p>
<p><span class="math display">\[
\begin{aligned}
\mu &amp;=\mathbb{E}[\text { wage } \mid \text { education }=16] \\
&amp;=\mathbb{E}\left[\exp \left(16 \beta_{1}+\beta_{2}+e\right)\right] \\
&amp;=\exp \left(16 \beta_{1}+\beta_{2}\right) \mathbb{E}[\exp (e)] \\
&amp;=\exp \left(16 \beta_{1}+\beta_{2}+\sigma^{2} / 2\right) .
\end{aligned}
\]</span></p>
<p>The final equality is <span class="math inline">\(\mathbb{E}[\exp (e)]=\exp \left(\sigma^{2} / 2\right)\)</span> which can be obtained from the normal moment generating function. The parameter <span class="math inline">\(\mu\)</span> is a nonlinear function of the coefficients. The natural estimator of <span class="math inline">\(\mu\)</span> replaces the unknowns by the point estimators. Thus</p>
<p><span class="math display">\[
\widehat{\mu}=\exp \left(16 \widehat{\beta}_{1}+\widehat{\beta}_{2}+\widehat{\sigma}^{2} / 2\right)=25.80
\]</span></p>
<p>The standard error for <span class="math inline">\(\widehat{\mu}\)</span> can be found by extending Exercise <span class="math inline">\(7.8\)</span> to find the joint asymptotic distribution of <span class="math inline">\(\widehat{\sigma}^{2}\)</span> and the slope estimates, and then applying the delta method.</p>
<p>We are interested in calculating standard errors and confidence intervals for the four estimates described above.</p>
</section>
<section id="jackknife-estimation-of-variance" class="level2" data-number="11.3">
<h2 data-number="11.3" class="anchored" data-anchor-id="jackknife-estimation-of-variance"><span class="header-section-number">11.3</span> Jackknife Estimation of Variance</h2>
<p>The jackknife estimates moments of estimators using the distribution of the leave-one-out estimators. The jackknife estimators of bias and variance were introduced by Quenouille (1949) and Tukey (1958), respectively. The idea was expanded further in the monographs of Efron (1982) and Shao and Tu (1995).</p>
<p>Let <span class="math inline">\(\widehat{\theta}\)</span> be any estimator of a vector-valued parameter <span class="math inline">\(\theta\)</span> which is a function of a random sample of size <span class="math inline">\(n\)</span>. Let <span class="math inline">\(\boldsymbol{V}_{\widehat{\theta}}=\operatorname{var}[\widehat{\theta}]\)</span> be the variance of <span class="math inline">\(\widehat{\theta}\)</span>. Define the leave-one-out estimators <span class="math inline">\(\widehat{\theta}_{(-i)}\)</span> which are computed using the formula for <span class="math inline">\(\widehat{\theta}\)</span> except that observation <span class="math inline">\(i\)</span> is deleted. Tukey’s jackknife estimator for <span class="math inline">\(\boldsymbol{V}_{\widehat{\theta}}\)</span> is defined as a scale of the sample variance of the leave-one-out estimators:</p>
<p><span class="math display">\[
\widehat{\boldsymbol{V}}_{\widehat{\theta}}^{\text {jack }}=\frac{n-1}{n} \sum_{i=1}^{n}\left(\widehat{\theta}_{(-i)}-\bar{\theta}\right)\left(\widehat{\theta}_{(-i)}-\bar{\theta}\right)^{\prime}
\]</span></p>
<p>where <span class="math inline">\(\bar{\theta}\)</span> is the sample mean of the leave-one-out estimators <span class="math inline">\(\bar{\theta}=n^{-1} \sum_{i=1}^{n} \widehat{\theta}_{(-i)}\)</span>. For scalar estimators <span class="math inline">\(\widehat{\theta}\)</span> the jackknife standard error is the square root of (10.1): <span class="math inline">\(s_{\widehat{\theta}}^{\text {jack }}=\sqrt{\widehat{V}_{\widehat{\theta}}^{\text {jack }}}\)</span>.</p>
<p>A convenient feature of the jackknife estimator <span class="math inline">\(\widehat{V}_{\widehat{\theta}}^{\text {jack }}\)</span> is that the formula (10.1) is quite general and does not require any technical (exact or asymptotic) calculations. A downside is that can require <span class="math inline">\(n\)</span> separate estimations, which in some cases can be computationally costly.</p>
<p>In most cases <span class="math inline">\(\widehat{\boldsymbol{V}}_{\widehat{\theta}}^{\text {jack }}\)</span> will be similar to a robust asymptotic covariance matrix estimator. The main attractions of the jackknife estimator are that it can be used when an explicit asymptotic variance formula is not available and that it can be used as a check on the reliability of an asymptotic formula.</p>
<p>The formula (10.1) is not immediately intuitive so may benefit from some motivation. We start by examining the sample mean <span class="math inline">\(\bar{Y}=\frac{1}{n} \sum_{i=1}^{n} Y_{i}\)</span> for <span class="math inline">\(Y \in \mathbb{R}^{m}\)</span>. The leave-one-out estimator is</p>
<p><span class="math display">\[
\bar{Y}_{(-i)}=\frac{1}{n-1} \sum_{j \neq i} Y_{j}=\frac{n}{n-1} \bar{Y}-\frac{1}{n-1} Y_{i} .
\]</span></p>
<p>The sample mean of the leave-one-out estimators is</p>
<p><span class="math display">\[
\frac{1}{n} \sum_{i=1}^{n} \bar{Y}_{(-i)}=\frac{n}{n-1} \bar{Y}-\frac{1}{n-1} \bar{Y}=\bar{Y}
\]</span></p>
<p>The difference is</p>
<p><span class="math display">\[
\bar{Y}_{(-i)}-\bar{Y}=\frac{1}{n-1}\left(\bar{Y}-Y_{i}\right) .
\]</span></p>
<p>The jackknife estimate of variance (10.1) is then</p>
<p><span class="math display">\[
\begin{aligned}
\widehat{\boldsymbol{V}}_{\bar{Y}}^{\text {jack }} &amp;=\frac{n-1}{n} \sum_{i=1}^{n}\left(\frac{1}{n-1}\right)^{2}\left(\bar{Y}-Y_{i}\right)\left(\bar{Y}-Y_{i}\right)^{\prime} \\
&amp;=\frac{1}{n}\left(\frac{1}{n-1}\right) \sum_{i=1}^{n}\left(\bar{Y}-Y_{i}\right)\left(\bar{Y}-Y_{i}\right)^{\prime}
\end{aligned}
\]</span></p>
<p>This is identical to the conventional estimator for the variance of <span class="math inline">\(\bar{Y}\)</span>. Indeed, Tukey proposed the <span class="math inline">\((n-1) / n\)</span> scaling in (10.1) so that <span class="math inline">\(\widehat{V}_{\bar{Y}}^{\text {jack }}\)</span> precisely equals the conventional estimator.</p>
<p>We next examine the case of least squares regression coefficient estimator. Recall from (3.43) that the leave-one-out OLS estimator equals</p>
<p><span class="math display">\[
\widehat{\beta}_{(-i)}=\widehat{\beta}-\left(\boldsymbol{X}^{\prime} \boldsymbol{X}\right)^{-1} X_{i} \widetilde{e}_{i}
\]</span></p>
<p>where <span class="math inline">\(\widetilde{e}_{i}=\left(1-h_{i i}\right)^{-1} \widehat{e}_{i}\)</span> and <span class="math inline">\(h_{i i}=X_{i}^{\prime}\left(\boldsymbol{X}^{\prime} \boldsymbol{X}\right)^{-1} X_{i}\)</span>. The sample mean of the leave-one-out estimators is <span class="math inline">\(\bar{\beta}=\widehat{\beta}-\left(\boldsymbol{X}^{\prime} \boldsymbol{X}\right)^{-1} \widetilde{\mu}\)</span> where <span class="math inline">\(\widetilde{\mu}=n^{-1} \sum_{i=1}^{n} X_{i} \widetilde{e}_{i}\)</span>. Thus <span class="math inline">\(\widehat{\beta}_{(-i)}-\bar{\beta}=-\left(\boldsymbol{X}^{\prime} \boldsymbol{X}\right)^{-1}\left(X_{i} \widetilde{e}_{i}-\widetilde{\mu}\right)\)</span>. The jackknife estimate of variance for <span class="math inline">\(\widehat{\beta}\)</span> is</p>
<p><span class="math display">\[
\begin{aligned}
\widehat{\boldsymbol{V}}_{\widehat{\beta}}^{\text {jack }} &amp;=\frac{n-1}{n} \sum_{i=1}^{n}\left(\widehat{\beta}_{(-i)}-\bar{\beta}\right)\left(\widehat{\beta}_{(-i)}-\bar{\beta}\right)^{\prime} \\
&amp;=\frac{n-1}{n}\left(\boldsymbol{X}^{\prime} \boldsymbol{X}\right)^{-1}\left(\sum_{i=1}^{n} X_{i} X_{i}^{\prime} \tilde{e}_{i}^{2}-n \widetilde{\mu} \widetilde{\mu}^{\prime}\right)\left(\boldsymbol{X}^{\prime} \boldsymbol{X}\right)^{-1} \\
&amp;=\frac{n-1}{n} \widehat{\boldsymbol{V}}_{\widehat{\beta}}^{\mathrm{HC} 3}-(n-1)\left(\boldsymbol{X}^{\prime} \boldsymbol{X}\right)^{-1} \widetilde{\mu} \widetilde{\mu}^{\prime}\left(\boldsymbol{X}^{\prime} \boldsymbol{X}\right)^{-1}
\end{aligned}
\]</span></p>
<p>where <span class="math inline">\(\widehat{\boldsymbol{V}}_{\widehat{\beta}}^{\mathrm{HC}}\)</span> is the HC3 covariance estimator (4.39) based on prediction errors. The second term in (10.5) is typically quite small since <span class="math inline">\(\widetilde{\mu}\)</span> is typically small in magnitude. Thus <span class="math inline">\(\widehat{\boldsymbol{V}}_{\widehat{\beta}}^{\text {jack }} \simeq \widehat{\boldsymbol{V}}_{\widehat{\beta}}^{\mathrm{HC}}\)</span>. Indeed the HC3 estimator was originally motivated as a simplification of the jackknife estimator. This shows that for regression coefficients the jackknife estimator of variance is similar to a conventional robust estimator. This is accomplished without the user “knowing” the form of the asymptotic covariance matrix. This is further confirmation that the jackknife is making a reasonable calculation.</p>
<p>Third, we examine the jackknife estimator for a function <span class="math inline">\(\widehat{\theta}=r(\widehat{\beta})\)</span> of a least squares estimator. The leave-one-out estimator of <span class="math inline">\(\theta\)</span> is</p>
<p><span class="math display">\[
\begin{aligned}
\widehat{\theta}_{(-i)} &amp;=r\left(\widehat{\beta}_{(-i)}\right) \\
&amp;=r\left(\widehat{\beta}-\left(\boldsymbol{X}^{\prime} \boldsymbol{X}\right)^{-1} X_{i} \widetilde{e}_{i}\right) \\
&amp; \simeq \widehat{\theta}-\widehat{\boldsymbol{R}}^{\prime}\left(\boldsymbol{X}^{\prime} \boldsymbol{X}\right)^{-1} X_{i} \widetilde{e}_{i}
\end{aligned}
\]</span></p>
<p>The second equality is (10.4). The final approximation is obtained by a mean-value expansion, using <span class="math inline">\(r(\widehat{\beta})=\widehat{\theta}\)</span> and setting <span class="math inline">\(\widehat{\boldsymbol{R}}=(\partial / \partial \beta) r(\widehat{\beta})^{\prime}\)</span>. This approximation holds in large samples because <span class="math inline">\(\widehat{\beta}_{(-i)}\)</span> are uniformly consistent for <span class="math inline">\(\beta\)</span>. The jackknife variance estimator for <span class="math inline">\(\widehat{\theta}\)</span> thus equals</p>
<p><span class="math display">\[
\begin{aligned}
\widehat{\boldsymbol{V}}_{\widehat{\theta}}^{\mathrm{jack}} &amp;=\frac{n-1}{n} \sum_{i=1}^{n}\left(\widehat{\theta}_{(-i)}-\bar{\theta}\right)\left(\widehat{\theta}_{(-i)}-\bar{\theta}\right)^{\prime} \\
&amp; \simeq \frac{n-1}{n} \widehat{\boldsymbol{R}}^{\prime}\left(\boldsymbol{X}^{\prime} \boldsymbol{X}\right)^{-1}\left(\sum_{i=1}^{n} X_{i} X_{i}^{\prime} \widehat{e}_{i}^{2}-n \widetilde{\mu} \widetilde{\mu}^{\prime}\right)\left(\boldsymbol{X}^{\prime} \boldsymbol{X}\right)^{-1} \widehat{\boldsymbol{R}} \\
&amp;=\widehat{\boldsymbol{R}}^{\prime} \widehat{\boldsymbol{V}}_{\widehat{\beta}}^{\mathrm{jack}} \widehat{\boldsymbol{R}} \\
&amp; \simeq \widehat{\boldsymbol{R}}^{\prime} \widetilde{\boldsymbol{V}}_{\widehat{\beta}} \widehat{\boldsymbol{R}} .
\end{aligned}
\]</span></p>
<p>The final line equals a delta-method estimator for the variance of <span class="math inline">\(\widehat{\theta}\)</span> constructed with the covariance estimator (4.39). This shows that the jackknife estimator of variance for <span class="math inline">\(\widehat{\theta}\)</span> is approximately an asymptotic delta-method estimator. While this is an asymptotic approximation, it again shows that the jackknife produces an estimator which is asymptotically similar to one produced by asymptotic methods. This is despite the fact that the jackknife estimator is calculated without reference to asymptotic theory and does not require calculation of the derivatives of <span class="math inline">\(r(\beta)\)</span>.</p>
<p>This argument extends directly to any “smooth function” estimator. Most of the estimators discussed so far in this textbook take the form <span class="math inline">\(\widehat{\theta}=g(\bar{W})\)</span> where <span class="math inline">\(\bar{W}=n^{-1} \sum_{i=1}^{n} W_{i}\)</span> and <span class="math inline">\(W_{i}\)</span> is some vector-valued function of the data. For any such estimator <span class="math inline">\(\widehat{\theta}\)</span> the leave-one-out estimator equals <span class="math inline">\(\widehat{\theta}_{(-i)}=g\left(\bar{W}_{(-i)}\right)\)</span> and its jackknife estimator of variance is (10.1). Using (10.2) and a mean-value expansion we have the largesample approximation</p>
<p><span class="math display">\[
\begin{aligned}
\widehat{\theta}_{(-i)} &amp;=g\left(\bar{W}_{(-i)}\right) \\
&amp;=g\left(\frac{n}{n-1} \bar{W}-\frac{1}{n-1} W_{i}\right) \\
&amp; \simeq g(\bar{W})-\frac{1}{n-1} \boldsymbol{G}(\bar{W})^{\prime} W_{i}
\end{aligned}
\]</span></p>
<p>where <span class="math inline">\(\boldsymbol{G}(x)=(\partial / \partial x) g(x)^{\prime}\)</span>. Thus</p>
<p><span class="math display">\[
\widehat{\theta}_{(-i)}-\bar{\theta} \simeq-\frac{1}{n-1} \boldsymbol{G}(\bar{W})^{\prime}\left(W_{i}-\bar{W}\right)
\]</span></p>
<p>and the jackknife estimator of the variance of <span class="math inline">\(\widehat{\theta}\)</span> approximately equals</p>
<p><span class="math display">\[
\begin{aligned}
\widehat{\boldsymbol{V}}_{\widehat{\theta}}^{\mathrm{jack}} &amp;=\frac{n-1}{n} \sum_{i=1}^{n}\left(\widehat{\theta}_{(-i)}-\widehat{\theta}_{(\cdot)}\right)\left(\widehat{\theta}_{(-i)}-\widehat{\theta}_{(\cdot)}\right)^{\prime} \\
&amp; \simeq \frac{n-1}{n} \boldsymbol{G}(\bar{W})^{\prime}\left(\frac{1}{(n-1)^{2}} \sum_{i=1}^{n}\left(W_{i}-\bar{W}\right)\left(W_{i}-\bar{W}\right)^{\prime}\right) \boldsymbol{G}(\bar{W}) \\
&amp;=\boldsymbol{G}(\bar{W})^{\prime} \widehat{\boldsymbol{V}}_{\bar{W}}^{\mathrm{jack}} \boldsymbol{G}(\bar{W})
\end{aligned}
\]</span></p>
<p>where <span class="math inline">\(\widehat{V}_{\bar{W}}^{\text {jack }}\)</span> as defined in (10.3) is the conventional (and jackknife) estimator for the variance of <span class="math inline">\(\bar{W}\)</span>. Thus <span class="math inline">\(\widehat{\boldsymbol{V}}_{\widehat{\theta}}^{\text {jack }}\)</span> is approximately the delta-method estimator. Once again, we see that the jackknife estimator automatically calculates what is effectively the delta-method variance estimator, but without requiring the user to explicitly calculate the derivative of <span class="math inline">\(g(x)\)</span>.</p>
</section>
<section id="example-1" class="level2" data-number="11.4">
<h2 data-number="11.4" class="anchored" data-anchor-id="example-1"><span class="header-section-number">11.4</span> Example</h2>
<p>We illustrate by reporting the asymptotic and jackknife standard errors for the four parameter estimates given earlier. In Table <span class="math inline">\(10.1\)</span> we report the actual values of the leave-one-out estimates for each of the twenty observations in the sample. The jackknife standard errors are calculated as the scaled square roots of the sample variances of these leave-one-out estimates and are reported in the second-to-last row. For comparison the asymptotic standard errors are reported in the final row.</p>
<p>For all estimates the jackknife and asymptotic standard errors are quite similar. This reinforces the credibility of both standard error estimates. The largest differences arise for <span class="math inline">\(\widehat{\beta}_{2}\)</span> and <span class="math inline">\(\widehat{\mu}\)</span>, whose jackknife standard errors are about <span class="math inline">\(5 %\)</span> larger than the asymptotic standard errors.</p>
<p>The take-away from our presentation is that the jackknife is a simple and flexible method for variance and standard error calculation. Circumventing technical asymptotic and exact calculations, the jackknife produces estimates which in many cases are similar to asymptotic delta-method counterparts. The jackknife is especially appealing in cases where asymptotic standard errors are not available or are difficult to calculate. They can also be used as a double-check on the reasonableness of asymptotic delta-method calculations.</p>
<p>In Stata, jackknife standard errors for coefficient estimates in many models are obtained by the vce(jackknife) option. For nonlinear functions of the coefficients or other estimators the jackkn ife command can be combined with any other command to obtain jackknife standard errors.</p>
<p>To illustrate, below we list the Stata commands which calculate the jackknife standard errors listed above. The first line is least squares estimation with standard errors calculated by the jackknife. The second line calculates the error variance estimate <span class="math inline">\(\widehat{\sigma}^{2}\)</span> with a jackknife standard error. The third line does the same for the estimate <span class="math inline">\(\widehat{\mu}\)</span>.</p>
<p><img src="images//2022_09_17_704b7bd5000562ad7735g-05.jpg" class="img-fluid"></p>
<p>Table 10.1: Leave-one-out Estimators and Jackknife Standard Errors</p>
<table class="table">
<colgroup>
<col style="width: 16%">
<col style="width: 20%">
<col style="width: 20%">
<col style="width: 23%">
<col style="width: 18%">
</colgroup>
<thead>
<tr class="header">
<th>Observation</th>
<th><span class="math inline">\(\widehat{\beta}_{1(-i)}\)</span></th>
<th><span class="math inline">\(\widehat{\beta}_{2(-i)}\)</span></th>
<th><span class="math inline">\(\widehat{\sigma}_{(-i)}^{2}\)</span></th>
<th><span class="math inline">\(\widehat{\mu}_{(-i)}\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td><span class="math inline">\(0.150\)</span></td>
<td><span class="math inline">\(0.764\)</span></td>
<td><span class="math inline">\(0.150\)</span></td>
<td><span class="math inline">\(25.63\)</span></td>
</tr>
<tr class="even">
<td>2</td>
<td><span class="math inline">\(0.148\)</span></td>
<td><span class="math inline">\(0.798\)</span></td>
<td><span class="math inline">\(0.149\)</span></td>
<td><span class="math inline">\(25.48\)</span></td>
</tr>
<tr class="odd">
<td>3</td>
<td><span class="math inline">\(0.153\)</span></td>
<td><span class="math inline">\(0.739\)</span></td>
<td><span class="math inline">\(0.151\)</span></td>
<td><span class="math inline">\(25.97\)</span></td>
</tr>
<tr class="even">
<td>4</td>
<td><span class="math inline">\(0.156\)</span></td>
<td><span class="math inline">\(0.695\)</span></td>
<td><span class="math inline">\(0.144\)</span></td>
<td><span class="math inline">\(26.31\)</span></td>
</tr>
<tr class="odd">
<td>5</td>
<td><span class="math inline">\(0.154\)</span></td>
<td><span class="math inline">\(0.701\)</span></td>
<td><span class="math inline">\(0.146\)</span></td>
<td><span class="math inline">\(25.38\)</span></td>
</tr>
<tr class="even">
<td>6</td>
<td><span class="math inline">\(0.158\)</span></td>
<td><span class="math inline">\(0.655\)</span></td>
<td><span class="math inline">\(0.151\)</span></td>
<td><span class="math inline">\(26.05\)</span></td>
</tr>
<tr class="odd">
<td>7</td>
<td><span class="math inline">\(0.152\)</span></td>
<td><span class="math inline">\(0.705\)</span></td>
<td><span class="math inline">\(0.114\)</span></td>
<td><span class="math inline">\(24.32\)</span></td>
</tr>
<tr class="even">
<td>8</td>
<td><span class="math inline">\(0.146\)</span></td>
<td><span class="math inline">\(0.822\)</span></td>
<td><span class="math inline">\(0.147\)</span></td>
<td><span class="math inline">\(25.37\)</span></td>
</tr>
<tr class="odd">
<td>9</td>
<td><span class="math inline">\(0.162\)</span></td>
<td><span class="math inline">\(0.588\)</span></td>
<td><span class="math inline">\(0.151\)</span></td>
<td><span class="math inline">\(25.75\)</span></td>
</tr>
<tr class="even">
<td>10</td>
<td><span class="math inline">\(0.157\)</span></td>
<td><span class="math inline">\(0.693\)</span></td>
<td><span class="math inline">\(0.139\)</span></td>
<td><span class="math inline">\(26.40\)</span></td>
</tr>
<tr class="odd">
<td>11</td>
<td><span class="math inline">\(0.168\)</span></td>
<td><span class="math inline">\(0.510\)</span></td>
<td><span class="math inline">\(0.141\)</span></td>
<td><span class="math inline">\(26.40\)</span></td>
</tr>
<tr class="even">
<td>12</td>
<td><span class="math inline">\(0.158\)</span></td>
<td><span class="math inline">\(0.691\)</span></td>
<td><span class="math inline">\(0.118\)</span></td>
<td><span class="math inline">\(26.48\)</span></td>
</tr>
<tr class="odd">
<td>13</td>
<td><span class="math inline">\(0.139\)</span></td>
<td><span class="math inline">\(0.974\)</span></td>
<td><span class="math inline">\(0.141\)</span></td>
<td><span class="math inline">\(26.56\)</span></td>
</tr>
<tr class="even">
<td>14</td>
<td><span class="math inline">\(0.169\)</span></td>
<td><span class="math inline">\(0.451\)</span></td>
<td><span class="math inline">\(0.131\)</span></td>
<td><span class="math inline">\(26.26\)</span></td>
</tr>
<tr class="odd">
<td>15</td>
<td><span class="math inline">\(0.146\)</span></td>
<td><span class="math inline">\(0.852\)</span></td>
<td><span class="math inline">\(0.150\)</span></td>
<td><span class="math inline">\(24.93\)</span></td>
</tr>
<tr class="even">
<td>16</td>
<td><span class="math inline">\(0.156\)</span></td>
<td><span class="math inline">\(0.696\)</span></td>
<td><span class="math inline">\(0.148\)</span></td>
<td><span class="math inline">\(26.06\)</span></td>
</tr>
<tr class="odd">
<td>17</td>
<td><span class="math inline">\(0.165\)</span></td>
<td><span class="math inline">\(0.513\)</span></td>
<td><span class="math inline">\(0.140\)</span></td>
<td><span class="math inline">\(25.22\)</span></td>
</tr>
<tr class="even">
<td>18</td>
<td><span class="math inline">\(0.155\)</span></td>
<td><span class="math inline">\(0.698\)</span></td>
<td><span class="math inline">\(0.151\)</span></td>
<td><span class="math inline">\(25.90\)</span></td>
</tr>
<tr class="odd">
<td>19</td>
<td><span class="math inline">\(0.152\)</span></td>
<td><span class="math inline">\(0.742\)</span></td>
<td><span class="math inline">\(0.151\)</span></td>
<td><span class="math inline">\(25.73\)</span></td>
</tr>
<tr class="even">
<td>20</td>
<td><span class="math inline">\(0.155\)</span></td>
<td><span class="math inline">\(0.697\)</span></td>
<td><span class="math inline">\(0.151\)</span></td>
<td><span class="math inline">\(25.95\)</span></td>
</tr>
<tr class="odd">
<td><span class="math inline">\(s^{\text {jack }}\)</span></td>
<td><span class="math inline">\(0.032\)</span></td>
<td><span class="math inline">\(0.514\)</span></td>
<td><span class="math inline">\(0.046\)</span></td>
<td><span class="math inline">\(2.39\)</span></td>
</tr>
<tr class="even">
<td><span class="math inline">\(s^{\text {asy }}\)</span></td>
<td><span class="math inline">\(0.031\)</span></td>
<td><span class="math inline">\(0.493\)</span></td>
<td><span class="math inline">\(0.043\)</span></td>
<td><span class="math inline">\(2.29\)</span></td>
</tr>
</tbody>
</table>
</section>
<section id="jackknife-for-clustered-observations" class="level2" data-number="11.5">
<h2 data-number="11.5" class="anchored" data-anchor-id="jackknife-for-clustered-observations"><span class="header-section-number">11.5</span> Jackknife for Clustered Observations</h2>
<p>In Section <span class="math inline">\(4.21\)</span> we introduced the clustered regression model, cluster-robust variance estimators, and cluster-robust standard errors. Jackknife variance estimation can also be used for clustered samples but with some natural modifications. Recall that the least squares estimator in the clustered sample context can be written as</p>
<p><span class="math display">\[
\widehat{\beta}=\left(\sum_{g=1}^{G} \boldsymbol{X}_{g}^{\prime} \boldsymbol{X}_{g}\right)^{-1}\left(\sum_{g=1}^{G} \boldsymbol{X}_{g}^{\prime} \boldsymbol{Y}_{g}\right)
\]</span></p>
<p>where <span class="math inline">\(g=1, \ldots, G\)</span> indexes the cluster. Instead of leave-one-out estimators, it is natural to use deletecluster estimators, which delete one cluster at a time. They take the form (4.58):</p>
<p><span class="math display">\[
\widehat{\beta}_{(-g)}=\widehat{\beta}-\left(\boldsymbol{X}^{\prime} \boldsymbol{X}\right)^{-1} \boldsymbol{X}_{g}^{\prime} \widetilde{\boldsymbol{e}}_{g}
\]</span></p>
<p>where</p>
<p><span class="math display">\[
\begin{aligned}
&amp;\widetilde{\boldsymbol{e}}_{g}=\left(\boldsymbol{I}_{n_{g}}-\boldsymbol{X}_{g}\left(\boldsymbol{X}^{\prime} \boldsymbol{X}\right)^{-1} \boldsymbol{X}_{g}^{\prime}\right)^{-1} \widehat{\boldsymbol{e}}_{g} \\
&amp;\widehat{\boldsymbol{e}}_{g}=\boldsymbol{Y}_{g}-\boldsymbol{X}_{g} \widehat{\beta}
\end{aligned}
\]</span></p>
<p>The delete-cluster jackknife estimator of the variance of <span class="math inline">\(\widehat{\beta}\)</span> is</p>
<p><span class="math display">\[
\begin{aligned}
\widehat{\boldsymbol{V}}_{\widehat{\beta}}^{\mathrm{jack}} &amp;=\frac{G-1}{G} \sum_{g=1}^{G}\left(\widehat{\beta}_{(-g)}-\bar{\beta}\right)\left(\widehat{\beta}_{(-g)}-\bar{\beta}\right)^{\prime} \\
\bar{\beta} &amp;=\frac{1}{G} \sum_{g=1}^{G} \widehat{\beta}_{(-g)} .
\end{aligned}
\]</span></p>
<p>We call <span class="math inline">\(\widehat{V}_{\widehat{\beta}}^{\text {jack }}\)</span> a cluster-robust jackknife estimator of variance.</p>
<p>Using the same approximations as the previous section we can show that the delete-cluster jackknife estimator is asymptotically equivalent to the cluster-robust covariance matrix estimator (4.59) calculated with the delete-cluster prediction errors. This verifies that the delete-cluster jackknife is the appropriate jackknife approach for clustered dependence.</p>
<p>For parameters which are functions <span class="math inline">\(\widehat{\theta}=r(\widehat{\beta})\)</span> of the least squares estimator, the delete-cluster jackknife estimator of the variance of <span class="math inline">\(\widehat{\theta}\)</span> is</p>
<p><span class="math display">\[
\begin{aligned}
\widehat{\boldsymbol{V}}_{\widehat{\theta}}^{\text {jack }} &amp;=\frac{G-1}{G} \sum_{g=1}^{G}\left(\widehat{\theta}_{(-g)}-\bar{\theta}\right)\left(\widehat{\theta}_{(-g)}-\bar{\theta}\right)^{\prime} \\
\widehat{\theta}_{(-i)} &amp;=r\left(\widehat{\beta}_{(-g)}\right) \\
\bar{\theta} &amp;=\frac{1}{G} \sum_{g=1}^{G} \widehat{\theta}_{(-g)} .
\end{aligned}
\]</span></p>
<p>Using a mean-value expansion we can show that this estimator is asymptotically equivalent to the deltamethod cluster-robust covariance matrix estimator for <span class="math inline">\(\widehat{\theta}\)</span>. This shows that the jackknife estimator is appropriate for covariance matrix estimation.</p>
<p>As in the context of i.i.d. samples, one advantage of the jackknife covariance matrix estimators is that they do not require the user to make a technical calculation of the asymptotic distribution. A downside is an increase in computation cost, as <span class="math inline">\(G\)</span> separate regressions are effectively estimated.</p>
<p>In Stata, jackknife standard errors for coefficient estimates with clustered observations are obtained by using the options cluster (id) vce(jackkn ife) where id denotes the cluster variable.</p>
</section>
<section id="the-bootstrap-algorithm" class="level2" data-number="11.6">
<h2 data-number="11.6" class="anchored" data-anchor-id="the-bootstrap-algorithm"><span class="header-section-number">11.6</span> The Bootstrap Algorithm</h2>
<p>The bootstrap is a powerful approach to inference and is due to the pioneering work of Efron (1979). There are many textbook and monograph treatments of the bootstrap, including Efron (1982), Hall (1992), Efron and Tibshirani (1993), Shao and Tu (1995), and Davison and Hinkley (1997). Reviews for econometricians are provided by Hall (1994) and Horowitz (2001)</p>
<p>There are several ways to describe or define the bootstrap and there are several forms of the bootstrap. We start in this section by describing the basic nonparametric bootstrap algorithm. In subsequent sections we give more formal definitions of the bootstrap as well as theoretical justifications.</p>
<p>Briefly, the bootstrap distribution is obtained by estimation on independent samples created by i.i.d. sampling (sampling with replacement) from the original dataset.</p>
<p>To understand this it is useful to start with the concept of sampling with replacement from the dataset. To continue the empirical example used earlier in the chapter we focus on the dataset displayed in Table 3.1, which has <span class="math inline">\(n=20\)</span> observations. Sampling from this distribution means randomly selecting one row from this table. Mathematically this is the same as randomly selecting an integer from the set <span class="math inline">\(\{1,2, \ldots, 20\}\)</span>. To illustrate, MATLAB has a random integer generator (the function randi). Using the random number seed of 13 (an arbitrary choice) we obtain the random draw 16 . This means that we draw observation number 16 from Table 3.1. Examining the table we can see that this is an individual with wage <span class="math inline">\(\$ 18.75\)</span> and education of 16 years. We repeat by drawing another random integer on the set <span class="math inline">\(\{1,2, \ldots, 20\}\)</span> and this time obtain 5 . This means we take observation 5 from Table 3.1, which is an individual with wage <span class="math inline">\(\$ 33.17\)</span> and education of 16 years. We continue until we have <span class="math inline">\(n=20\)</span> such draws. This random set of observations are <span class="math inline">\(\{16,5,17,20,20,10,13,16,13,15,1,6,2,18,8,14,6,7,1,8\}\)</span>. We call this the bootstrap sample.</p>
<p>Notice that the observations <span class="math inline">\(1,6,8,13,16,20\)</span> each appear twice in the bootstrap sample, and the observations <span class="math inline">\(3,4,9,11,12,19\)</span> do not appear at all. That is okay. In fact, it is necessary for the bootstrap to work. This is because we are drawing with replacement. (If we instead made draws without replacement then the constructed dataset would have exactly the same observations as in Table 3.1, only in different order.) We can also ask the question “What is the probability that an individual observation will appear at least once in the bootstrap sample?” The answer is</p>
<p><span class="math display">\[
\begin{aligned}
\mathbb{P}[\text { Observation in Bootstrap Sample }] &amp;=1-\left(1-\frac{1}{n}\right)^{n} \\
&amp; \rightarrow 1-e^{-1} \simeq 0.632 .
\end{aligned}
\]</span></p>
<p>The limit holds as <span class="math inline">\(n \rightarrow \infty\)</span>. The approximation <span class="math inline">\(0.632\)</span> is excellent even for small <span class="math inline">\(n\)</span>. For example, when <span class="math inline">\(n=20\)</span> the probability (10.6) is <span class="math inline">\(0.641\)</span>. These calculations show that an individual observation is in the bootstrap sample with probability near <span class="math inline">\(2 / 3\)</span>.</p>
<p>Once again, the bootstrap sample is the constructed dataset with the 20 observations drawn randomly from the original sample. Notationally, we write the <span class="math inline">\(i^{\text {th }}\)</span> bootstrap observation as <span class="math inline">\(\left(Y_{i}^{*}, X_{i}^{*}\right)\)</span> and the bootstrap sample as <span class="math inline">\(\left\{\left(Y_{1}^{*}, X_{1}^{*}\right), \ldots,\left(Y_{n}^{*}, X_{n}^{*}\right)\right\}\)</span>. In our present example with <span class="math inline">\(Y\)</span> denoting the log wage the bootstrap sample is</p>
<p><span class="math display">\[
\left\{\left(Y_{1}^{*}, X_{1}^{*}\right), \ldots,\left(Y_{n}^{*}, X_{n}^{*}\right)\right\}=\{(2.93,16),(3.50,16) \ldots,(3.76,18)\}
\]</span></p>
<p>The bootstrap estimate <span class="math inline">\(\widehat{\beta}^{*}\)</span> is obtained by applying the least squares estimation formula to the bootstrap sample. Thus we regress <span class="math inline">\(Y^{*}\)</span> on <span class="math inline">\(X^{*}\)</span>. The other bootstrap estimates, in our example <span class="math inline">\(\widehat{\sigma}^{2 *}\)</span> and <span class="math inline">\(\widehat{\mu}^{*}\)</span>, are obtained by applying their estimation formulae to the bootstrap sample as well. Writing <span class="math inline">\(\widehat{\theta}^{*}=\)</span> <span class="math inline">\(\left(\widehat{\beta}_{1}^{*}, \widehat{\beta}_{2}^{*}, \widehat{\sigma}^{* 2}, \widehat{\mu}^{*}\right)^{\prime}\)</span> we have the bootstrap estimate of the parameter vector <span class="math inline">\(\theta=\left(\beta_{1}, \beta_{2}, \sigma^{2}, \mu\right)^{\prime}\)</span>. In our example (the bootstrap sample described above) <span class="math inline">\(\widehat{\theta}^{*}=(0.195,0.113,0.107,26.7)^{\prime}\)</span>. This is one draw from the bootstrap distribution of the estimates.</p>
<p>The estimate <span class="math inline">\(\widehat{\theta}^{*}\)</span> as described is one random draw from the distribution of estimates obtained by i.i.d. sampling from the original data. With one draw we can say relatively little. But we can repeat this exercise to obtain multiple draws from this bootstrap distribution. To distinguish between these draws we index the bootstrap samples by <span class="math inline">\(b=1, \ldots, B\)</span>, and write the bootstrap estimates as <span class="math inline">\(\widehat{\theta}_{b}^{*}\)</span> or <span class="math inline">\(\widehat{\theta}^{*}(b)\)</span>.</p>
<p>To continue our illustration we draw 20 more random integers <span class="math inline">\(\{19,5,7,19,1,2,13,18,1,15,17,2\)</span>, <span class="math inline">\(14,11,10,20,1,5,15,7\}\)</span> and construct a second bootstrap sample. On this sample we again estimate the parameters and obtain <span class="math inline">\(\widehat{\theta}^{*}(2)=(0.175,0.52,0.124,29.3)^{\prime}\)</span>. This is a second random draw from the distribution of <span class="math inline">\(\widehat{\theta}^{*}\)</span>. We repeat this <span class="math inline">\(B\)</span> times, storing the parameter estimates <span class="math inline">\(\widehat{\theta}^{*}(b)\)</span>. We have thus created a new dataset of bootstrap draws <span class="math inline">\(\left\{\widehat{\theta}^{*}(b): b=1, \ldots, B\right\}\)</span>. By construction the draws are independent across <span class="math inline">\(b\)</span> and identically distributed.</p>
<p>The number of bootstrap draws, <span class="math inline">\(B\)</span>, is often called the “number of bootstrap replications”. Typical choices for <span class="math inline">\(B\)</span> are 1000,5000 , and 10,000. We discuss selecting <span class="math inline">\(B\)</span> later, but roughly speaking, larger <span class="math inline">\(B\)</span> results in a more precise estimate at an increased computation cost. For our application we set <span class="math inline">\(B=\)</span> 10,000 . To illustrate, Figure <span class="math inline">\(13.1\)</span> displays the densities of the distributions of the bootstrap estimates <span class="math inline">\(\widehat{\beta}_{1}^{*}\)</span> and <span class="math inline">\(\widehat{\mu}^{*}\)</span> across 10,000 draws. The dashed lines show the point estimate. You can notice that the density for <span class="math inline">\(\widehat{\beta}_{1}^{*}\)</span> is slightly skewed to the left.\</p>
<p><img src="images//2022_09_17_704b7bd5000562ad7735g-09.jpg" class="img-fluid"></p>
<p>Figure 10.1: Bootstrap Distributions of <span class="math inline">\(\widehat{\beta}_{1}^{*}\)</span> and <span class="math inline">\(\widehat{\mu}^{*}\)</span></p>
</section>
<section id="bootstrap-variance-and-standard-errors" class="level2" data-number="11.7">
<h2 data-number="11.7" class="anchored" data-anchor-id="bootstrap-variance-and-standard-errors"><span class="header-section-number">11.7</span> Bootstrap Variance and Standard Errors</h2>
<p>Given the bootstrap draws we can estimate features of the bootstrap distribution. The bootstrap estimator of variance of an estimator <span class="math inline">\(\widehat{\theta}\)</span> is the sample variance across the bootstrap draws <span class="math inline">\(\widehat{\theta}^{*}(b)\)</span>. It equals</p>
<p><span class="math display">\[
\begin{aligned}
\widehat{\boldsymbol{V}}_{\widehat{\theta}}^{\text {boot }} &amp;=\frac{1}{B-1} \sum_{b=1}^{B}\left(\widehat{\theta}^{*}(b)-\bar{\theta}^{*}\right)\left(\widehat{\theta}^{*}(b)-\bar{\theta}^{*}\right)^{\prime} \\
\bar{\theta}^{*} &amp;=\frac{1}{B} \sum_{b=1}^{B} \widehat{\theta}^{*}(b)
\end{aligned}
\]</span></p>
<p>For a scalar estimator <span class="math inline">\(\hat{\theta}\)</span> the bootstrap standard error is the square root of the bootstrap estimator of variance:</p>
<p><span class="math display">\[
s_{\widehat{\widehat{\theta}}}^{\text {boot }}=\sqrt{\widehat{\boldsymbol{V}}_{\widehat{\theta}}^{\text {boot }}} .
\]</span></p>
<p>This is a very simple statistic to calculate and is the most common use of the bootstrap in applied econometric practice. A caveat (discussed in more detail in Section 10.15) is that in many cases it is better to use a trimmed estimator.</p>
<p>Standard errors are conventionally reported to convey the precision of the estimator. They are also commonly used to construct confidence intervals. Bootstrap standard errors can be used for this purpose. The normal-approximation bootstrap confidence interval is</p>
<p><span class="math display">\[
C^{\mathrm{nb}}=\left[\widehat{\theta}-z_{1-\alpha / 2} s_{\widehat{\theta}}^{\text {boot }}, \quad \widehat{\theta}+z_{1-\alpha / 2} s_{\widehat{\theta}}^{\text {boot }}\right]
\]</span></p>
<p>where <span class="math inline">\(z_{1-\alpha / 2}\)</span> is the <span class="math inline">\(1-\alpha / 2\)</span> quantile of the <span class="math inline">\(\mathrm{N}(0,1)\)</span> distribution. This interval <span class="math inline">\(C^{\mathrm{nb}}\)</span> is identical in format to an asymptotic confidence interval, but with the bootstrap standard error replacing the asymptotic standard error. <span class="math inline">\(C^{\mathrm{nb}}\)</span> is the default confidence interval reported by Stata when the bootstrap has been used to calculate standard errors. However, the normal-approximation interval is in general a poor choice for confidence interval construction as it relies on the normal approximation to the t-ratio which can be inaccurate in finite samples. There are other methods - such as the bias-corrected percentile method to be discussed in Section <span class="math inline">\(10.17\)</span> - which are just as simple to compute but have better performance. In general, bootstrap standard errors should be used as estimates of precision rather than as tools to construct confidence intervals.</p>
<p>Since <span class="math inline">\(B\)</span> is finite, all bootstrap statistics, such as <span class="math inline">\(\widehat{\boldsymbol{V}}_{\widehat{\theta}}^{\text {boot }}\)</span>, are estimates and hence random. Their values will vary across different choices for <span class="math inline">\(B\)</span> and simulation runs (depending on how the simulation seed is set). Thus you should not expect to obtain the exact same bootstrap standard errors as other researchers when replicating their results. They should be similar (up to simulation sampling error) but not precisely the same.</p>
<p>In Table <span class="math inline">\(10.2\)</span> we report the four parameter estimates introduced in Section <span class="math inline">\(10.2\)</span> along with asymptotic, jackknife and bootstrap standard errors. We also report four bootstrap confidence intervals which will be introduced in subsequent sections.</p>
<p>For these four estimators we can see that the bootstrap standard errors are quite similar to the asymptotic and jackknife standard errors. The most noticable difference arises for <span class="math inline">\(\widehat{\beta}_{2}\)</span>, where the bootstrap standard error is about <span class="math inline">\(10 %\)</span> larger than the asymptotic standard error.</p>
<p>Table 10.2: Comparison of Methods</p>
<table class="table">
<colgroup>
<col style="width: 26%">
<col style="width: 19%">
<col style="width: 19%">
<col style="width: 20%">
<col style="width: 13%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;"></th>
<th><span class="math inline">\(\widehat{\beta}_{1}\)</span></th>
<th><span class="math inline">\(\widehat{\beta}_{2}\)</span></th>
<th><span class="math inline">\(\widehat{\sigma}^{2}\)</span></th>
<th><span class="math inline">\(\widehat{\mu}\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Estimate</td>
<td><span class="math inline">\(0.155\)</span></td>
<td><span class="math inline">\(0.698\)</span></td>
<td><span class="math inline">\(0.144\)</span></td>
<td><span class="math inline">\(25.80\)</span></td>
</tr>
<tr class="even">
<td style="text-align: left;">Asymptotic s.e.</td>
<td><span class="math inline">\((0.031)\)</span></td>
<td><span class="math inline">\((0.493)\)</span></td>
<td><span class="math inline">\((0.043)\)</span></td>
<td><span class="math inline">\((2.29)\)</span></td>
</tr>
<tr class="odd">
<td style="text-align: left;">Jackknife s.e.</td>
<td><span class="math inline">\((0.032)\)</span></td>
<td><span class="math inline">\((0.514)\)</span></td>
<td><span class="math inline">\((0.046)\)</span></td>
<td><span class="math inline">\((2.39)\)</span></td>
</tr>
<tr class="even">
<td style="text-align: left;">Bootstrap s.e.</td>
<td><span class="math inline">\((0.034)\)</span></td>
<td><span class="math inline">\((0.548)\)</span></td>
<td><span class="math inline">\((0.041)\)</span></td>
<td><span class="math inline">\((2.38)\)</span></td>
</tr>
<tr class="odd">
<td style="text-align: left;"><span class="math inline">\(95 %\)</span> Percentile Interval</td>
<td><span class="math inline">\([0.08,0.21]\)</span></td>
<td><span class="math inline">\([-0.27,1.91]\)</span></td>
<td><span class="math inline">\([0.06,0.22]\)</span></td>
<td><span class="math inline">\([21.4,30.7]\)</span></td>
</tr>
<tr class="even">
<td style="text-align: left;"><span class="math inline">\(95 %\)</span> BC Percentile Interval</td>
<td><span class="math inline">\([0.08,0.21]\)</span></td>
<td><span class="math inline">\([-0.25,1.93]\)</span></td>
<td><span class="math inline">\([0.09,0.28]\)</span></td>
<td><span class="math inline">\([22.0,31.5]\)</span></td>
</tr>
<tr class="odd">
<td style="text-align: left;"><span class="math inline">\(95 %\)</span> BC</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p>In Stata, bootstrap standard errors for coefficient estimates in many models are obtained by the vce(bootstrap, reps(#)) option, where # is the number of bootstrap replications. For nonlinear functions of the coefficients or other estimators the bootstrap command can be combined with any other command to obtain bootstrap standard errors. Synonyms for bootstrap are bstrap and bs.</p>
<p>To illustrate, below we list the Stata commands which will calculate <span class="math inline">\({ }^{1}\)</span> the bootstrap standard errors listed above.</p>
<p><span class="math inline">\({ }^{1}\)</span> They will not precisely replicate the standard errors since those in Table <span class="math inline">\(10.2\)</span> were produced in Matlab which uses a different random number sequence.</p>
<p>Stata Commands reg wage education if <span class="math inline">\(\operatorname{mbf} 12==1\)</span>, vce(bootstrap, reps <span class="math inline">\((10000))\)</span></p>
<p>bs (e(rss)/e(N)), reps(10000): reg wage education if <span class="math inline">\(\mathrm{mbf} 12==1\)</span></p>
<p>bs ( <span class="math inline">\(\exp \left(16^{*}\right.\)</span> bb[education]+_b[_cons] <span class="math inline">\(\left.\left.+\mathrm{e}(\mathrm{rss}) / \mathrm{e}(\mathrm{N}) / 2\right)\right)\)</span>, reps(10000): ///</p>
<p>reg wage education if <span class="math inline">\(\operatorname{mbf} 12==1\)</span></p>
</section>
<section id="percentile-interval" class="level2" data-number="11.8">
<h2 data-number="11.8" class="anchored" data-anchor-id="percentile-interval"><span class="header-section-number">11.8</span> Percentile Interval</h2>
<p>The second most common use of bootstrap methods is for confidence intervals. There are multiple bootstrap methods to form confidence intervals. A popular and simple method is called the percentile interval. It is based on the quantiles of the bootstrap distribution.</p>
<p>In Section <span class="math inline">\(10.6\)</span> we described the bootstrap algorithm which creates an i.i.d. sample of bootstrap estimates <span class="math inline">\(\left\{\widehat{\theta}_{1}^{*}, \widehat{\theta}_{2}^{*}, \ldots, \widehat{\theta}_{B}^{*}\right\}\)</span> corresponding to an estimator <span class="math inline">\(\widehat{\theta}\)</span> of a parameter <span class="math inline">\(\theta\)</span>. We focus on the case of a scalar parameter <span class="math inline">\(\theta\)</span>.</p>
<p>For any <span class="math inline">\(0&lt;\alpha&lt;1\)</span> we can calculate the empirical quantile <span class="math inline">\(q_{\alpha}^{*}\)</span> of these bootstrap estimates. This is the number such that <span class="math inline">\(n \alpha\)</span> bootstrap estimates are smaller than <span class="math inline">\(q_{\alpha}^{*}\)</span>, and is typically calculated by taking the <span class="math inline">\(n \alpha^{t h}\)</span> order statistic of the <span class="math inline">\(\widehat{\theta}_{b}^{*}\)</span>. See Section <span class="math inline">\(11.13\)</span> of Probability and Statistics for Economists for a precise discussion of empirical quantiles and common quantile estimators.</p>
<p>The percentile bootstrap <span class="math inline">\(100(1-\alpha) %\)</span> confidence interval is</p>
<p><span class="math display">\[
C^{\mathrm{pc}}=\left[q_{\alpha / 2}^{*}, q_{1-\alpha / 2}^{*}\right] .
\]</span></p>
<p>For example, if <span class="math inline">\(B=1000, \alpha=0.05\)</span>, and the empirical quantile estimator is used, then <span class="math inline">\(C^{\mathrm{pc}}=\left[\widehat{\theta}_{(25)}^{*}, \widehat{\theta}_{(975)}^{*}\right]\)</span>.</p>
<p>To illustrate, the <span class="math inline">\(0.025\)</span> and <span class="math inline">\(0.975\)</span> quantiles of the bootstrap distributions of <span class="math inline">\(\widehat{\beta}_{1}^{*}\)</span> and <span class="math inline">\(\widehat{\mu}^{*}\)</span> are indicated in Figure <span class="math inline">\(13.1\)</span> by the arrows. The intervals between the arrows are the <span class="math inline">\(95 %\)</span> percentile intervals.</p>
<p>The percentile interval has the convenience that it does not require calculation of a standard error. This is particularly convenient in contexts where asymptotic standard error calculation is complicated, burdensome, or unknown. <span class="math inline">\(C^{\mathrm{pc}}\)</span> is a simple by-product of the bootstrap algorithm and does not require meaningful computational cost above that required to calculate the bootstrap standard error.</p>
<p>The percentile interval has the useful property that it is transformation-respecting. Take a monotone parameter transformation <span class="math inline">\(m(\theta)\)</span>. The percentile interval for <span class="math inline">\(m(\theta)\)</span> is simply the percentile interval for <span class="math inline">\(\theta\)</span> mapped by <span class="math inline">\(m(\theta)\)</span>. That is, if <span class="math inline">\(\left[q_{\alpha / 2}^{*}, q_{1-\alpha / 2}^{*}\right]\)</span> is the percentile interval for <span class="math inline">\(\theta\)</span>, then <span class="math inline">\(\left[m\left(q_{\alpha / 2}^{*}\right), m\left(q_{1-\alpha / 2}^{*}\right)\right]\)</span> is the percentile interval for <span class="math inline">\(m(\theta)\)</span>. This property follows directly from the equivariance property of sample quantiles. Many confidence-interval methods, such as the delta-method asymptotic interval and the normal-approximation interval <span class="math inline">\(C^{\mathrm{nb}}\)</span>, do not share this property.</p>
<p>To illustrate the usefulness of the transformation-respecting property consider the variance <span class="math inline">\(\sigma^{2}\)</span>. In some cases it is useful to report the variance <span class="math inline">\(\sigma^{2}\)</span> and in other cases it is useful to report the standard deviation <span class="math inline">\(\sigma\)</span>. Thus we may be interested in confidence intervals for <span class="math inline">\(\sigma^{2}\)</span> or <span class="math inline">\(\sigma\)</span>. To illustrate, the asymptotic <span class="math inline">\(95 %\)</span> normal confidence interval for <span class="math inline">\(\sigma^{2}\)</span> which we calculate from Table <span class="math inline">\(13.2\)</span> is <span class="math inline">\([0.060,0.228]\)</span>. Taking square roots we obtain an interval for <span class="math inline">\(\sigma\)</span> of [0.244,0.477]. Alternatively, the delta method standard error for <span class="math inline">\(\widehat{\sigma}=0.379\)</span> is <span class="math inline">\(0.057\)</span>, leading to an asymptotic <span class="math inline">\(95 %\)</span> confidence interval for <span class="math inline">\(\sigma\)</span> of <span class="math inline">\([0.265,0.493]\)</span> which is different. This shows that the delta method is not transformation-respecting. In contrast, the <span class="math inline">\(95 %\)</span> percentile interval for <span class="math inline">\(\sigma^{2}\)</span> is <span class="math inline">\([0.062,0.220]\)</span> and that for <span class="math inline">\(\sigma\)</span> is <span class="math inline">\([0.249,0.469]\)</span> which is identical to the square roots of the interval for <span class="math inline">\(\sigma^{2}\)</span>.</p>
<p>The bootstrap percentile intervals for the four estimators are reported in Table 13.2. In Stata, percentile confidence intervals can be obtained by using the command estat bootstrap, percentile or the command estat bootstrap, all after an estimation command which calculates standard errors via the bootstrap.</p>
</section>
<section id="the-bootstrap-distribution" class="level2" data-number="11.9">
<h2 data-number="11.9" class="anchored" data-anchor-id="the-bootstrap-distribution"><span class="header-section-number">11.9</span> The Bootstrap Distribution</h2>
<p>For applications it is often sufficient if one understands the bootstrap as an algorithm. However, for theory it is more useful to view the bootstrap as a specific estimator of the sampling distribution. For this it is useful to introduce some additional notation.</p>
<p>The key is that the distribution of any estimator or statistic is determined by the distribution of the data. While the latter is unknown it can be estimated by the empirical distribution of the data. This is what the bootstrap does.</p>
<p>To fix notation, let <span class="math inline">\(F\)</span> denote the distribution of an individual observation <span class="math inline">\(W\)</span>. (In regression, <span class="math inline">\(W\)</span> is the <span class="math inline">\(\operatorname{pair}(Y, X)\)</span>.) Let <span class="math inline">\(G_{n}(u, F)\)</span> denote the distribution of an estimator <span class="math inline">\(\widehat{\theta}\)</span>. That is,</p>
<p><span class="math display">\[
G_{n}(u, F)=\mathbb{P}[\widehat{\theta} \leq u \mid F] .
\]</span></p>
<p>We write the distribution <span class="math inline">\(G_{n}\)</span> as a function of <span class="math inline">\(n\)</span> and <span class="math inline">\(F\)</span> since the latter (generally) affect the distribution of <span class="math inline">\(\widehat{\theta}\)</span>. We are interested in the distribution <span class="math inline">\(G_{n}\)</span>. For example, we want to know its variance to calculate a standard error or its quantiles to calculate a percentile interval.</p>
<p>In principle, if we knew the distribution <span class="math inline">\(F\)</span> we should be able to determine the distribution <span class="math inline">\(G_{n}\)</span>. In practice there are two barriers to implementation. The first barrier is that the calculation of <span class="math inline">\(G_{n}(u, F)\)</span> is generally infeasible except in certain special cases such as the normal regression model. The second barrier is that in general we do not know <span class="math inline">\(F\)</span>.</p>
<p>The bootstrap simultaneously circumvents these two barriers by two clever ideas. First, the bootstrap proposes estimation of <span class="math inline">\(F\)</span> by the empirical distribution function (EDF) <span class="math inline">\(F_{n}\)</span>, which is the simplest nonparametric estimator of the joint distribution of the observations. The EDF is <span class="math inline">\(F_{n}(w)=n^{-1} \sum_{i=1}^{n} \mathbb{1}\left\{W_{i} \leq w\right\}\)</span>. (See Section <span class="math inline">\(11.2\)</span> of Probability and Statistics for Economists for details and properties.) Replacing <span class="math inline">\(F\)</span> with <span class="math inline">\(F_{n}\)</span> we obtain the idealized bootstrap estimator of the distribution of <span class="math inline">\(\widehat{\theta}\)</span></p>
<p><span class="math display">\[
G_{n}^{*}(u)=G_{n}\left(u, F_{n}\right) .
\]</span></p>
<p>The bootstrap’s second clever idea is to estimate <span class="math inline">\(G_{n}^{*}\)</span> by simulation. This is the bootstrap algorithm described in the previous sections. The essential idea is that simulation from <span class="math inline">\(F_{n}\)</span> is sampling with replacement from the original data, which is computationally simple. Applying the estimation formula for <span class="math inline">\(\hat{\theta}\)</span> we obtain i.i.d. draws from the distribution <span class="math inline">\(G_{n}^{*}(u)\)</span>. By making a large number <span class="math inline">\(B\)</span> of such draws we can estimate any feature of <span class="math inline">\(G_{n}^{*}\)</span> of interest. The bootstrap combines these two ideas: (1) estimate <span class="math inline">\(G_{n}(u, F)\)</span> by <span class="math inline">\(G_{n}\left(u, F_{n}\right)\)</span>; (2) estimate <span class="math inline">\(G_{n}\left(u, F_{n}\right)\)</span> by simulation. These ideas are intertwined. Only by considering these steps together do we obtain a feasible method.</p>
<p>The way to think about the connection between <span class="math inline">\(G_{n}\)</span> and <span class="math inline">\(G_{n}^{*}\)</span> is as follows. <span class="math inline">\(G_{n}\)</span> is the distribution of the estimator <span class="math inline">\(\widehat{\theta}\)</span> obtained when the observations are sampled i.i.d. from the population distribution <span class="math inline">\(F\)</span>. <span class="math inline">\(G_{n}^{*}\)</span> is the distribution of the same statistic, denoted <span class="math inline">\(\widehat{\theta}^{*}\)</span>, obtained when the observations are sampled i.i.d. from the empirical distribution <span class="math inline">\(F_{n}\)</span>. It is useful to conceptualize the “universe” which separately generates the dataset and the bootstrap sample. The “sampling universe” is the population distribution <span class="math inline">\(F\)</span>. In this universe the true parameter is <span class="math inline">\(\theta\)</span>. The “bootstrap universe” is the empircal distribution <span class="math inline">\(F_{n}\)</span>. When drawing from the bootstrap universe we are treating <span class="math inline">\(F_{n}\)</span> as if it is the true distribution. Thus anything which is true about <span class="math inline">\(F_{n}\)</span> should be treated as true in the bootstrap universe. In the bootstrap universe the “true” value of the parameter <span class="math inline">\(\theta\)</span> is the value determined by the EDF <span class="math inline">\(F_{n}\)</span>. In most cases this is the estimate <span class="math inline">\(\widehat{\theta}\)</span>. It is the true value of the coefficient when the true distribution is <span class="math inline">\(F_{n}\)</span>. We now carefully explain the connection with the bootstrap algorithm as previously described.</p>
<p>First, observe that sampling with replacement from the sample <span class="math inline">\(\left\{Y_{1}, \ldots, Y_{n}\right\}\)</span> is identical to sampling from the EDF <span class="math inline">\(F_{n}\)</span>. This is because the EDF is the probability distribution which puts probability mass <span class="math inline">\(1 / n\)</span> on each observation. Thus sampling from <span class="math inline">\(F_{n}\)</span> means sampling an observation with probability <span class="math inline">\(1 / n\)</span>, which is sampling with replacement.</p>
<p>Second, observe that the bootstrap estimator <span class="math inline">\(\widehat{\theta}^{*}\)</span> described here is identical to the bootstrap algorithm described in Section 10.6. That is, <span class="math inline">\(\widehat{\theta}^{*}\)</span> is the random vector generated by applying the estimator formula <span class="math inline">\(\widehat{\theta}\)</span> to samples obtained by random sampling from <span class="math inline">\(F_{n}\)</span>.</p>
<p>Third, observe that the distribution of these bootstrap estimators is the bootstrap distribution (10.9). This is a precise equality. That is, the bootstrap algorithm generates i.i.d. samples from <span class="math inline">\(F_{n}\)</span>, and when the estimators are applied we obtain random variables <span class="math inline">\(\widehat{\theta}^{*}\)</span> with the distribution <span class="math inline">\(G_{n}^{*}\)</span>.</p>
<p>Fourth, observe that the bootstrap statistics described earlier - bootstrap variance, standard error, and quantiles - are estimators of the corresponding features of the bootstrap distribution <span class="math inline">\(G_{n}^{*}\)</span>.</p>
<p>This discussion is meant to carefully describe why the notation <span class="math inline">\(G_{n}^{*}(u)\)</span> is useful to help understand the properties of the bootstrap algorithm. Since <span class="math inline">\(F_{n}\)</span> is the natural nonparametric estimator of the unknown distribution <span class="math inline">\(F, G_{n}^{*}(u)=G_{n}\left(u, F_{n}\right)\)</span> is the natural plug-in estimator of the unknown <span class="math inline">\(G_{n}(u, F)\)</span>. Furthermore, because <span class="math inline">\(F_{n}\)</span> is uniformly consistent for <span class="math inline">\(F\)</span> by the Glivenko-Cantelli Lemma (Theorem <span class="math inline">\(18.8\)</span> in Probability and Statistics for Economists) we also can expect <span class="math inline">\(G_{n}^{*}(u)\)</span> to be consistent for <span class="math inline">\(G_{n}(u)\)</span>. Making this precise is a bit challenging since <span class="math inline">\(F_{n}\)</span> and <span class="math inline">\(G_{n}\)</span> are functions. In the next several sections we develop an asymptotic distribution theory for the bootstrap distribution based on extending asymptotic theory to the case of conditional distributions.</p>
</section>
<section id="the-distribution-of-the-bootstrap-observations" class="level2" data-number="11.10">
<h2 data-number="11.10" class="anchored" data-anchor-id="the-distribution-of-the-bootstrap-observations"><span class="header-section-number">11.10</span> The Distribution of the Bootstrap Observations</h2>
<p>Let <span class="math inline">\(Y^{*}\)</span> be a random draw from the sample <span class="math inline">\(\left\{Y_{1}, \ldots, Y_{n}\right\}\)</span>. What is the distribution of <span class="math inline">\(Y^{*}\)</span> ?</p>
<p>Since we are fixing the observations, the correct question is: What is the conditional distribution of <span class="math inline">\(Y^{*}\)</span>, conditional on the observed data? The empirical distribution function <span class="math inline">\(F_{n}\)</span> summarizes the information in the sample, so equivalently we are talking about the distribution conditional on <span class="math inline">\(F_{n}\)</span>. Consequently we will write the bootstrap probability function and expectation as</p>
<p><span class="math display">\[
\begin{aligned}
\mathbb{P}^{*}\left[Y^{*} \leq x\right] &amp;=\mathbb{P}\left[Y^{*} \leq x \mid F_{n}\right] \\
\mathbb{E}^{*}\left[Y^{*}\right] &amp;=\mathbb{E}\left[Y^{*} \mid F_{n}\right] .
\end{aligned}
\]</span></p>
<p>Notationally, the starred distribution and expectation are conditional given the data.</p>
<p>The (conditional) distribution of <span class="math inline">\(Y^{*}\)</span> is the empirical distribution function <span class="math inline">\(F_{n}\)</span>, which is a discrete distribution with mass points <span class="math inline">\(1 / n\)</span> on each observation <span class="math inline">\(Y_{i}\)</span>. Thus even if the original data come from a continuous distribution, the bootstrap data distribution is discrete.</p>
<p>The (conditional) mean and variance of <span class="math inline">\(Y^{*}\)</span> are calculated from the EDF, and equal the sample mean and variance of the data. The mean is</p>
<p><span class="math display">\[
\mathbb{E}^{*}\left[Y^{*}\right]=\sum_{i=1}^{n} Y_{i} \mathbb{P}^{*}\left[Y^{*}=Y_{i}\right]=\sum_{i=1}^{n} Y_{i} \frac{1}{n}=\bar{Y}
\]</span></p>
<p>and the variance is</p>
<p><span class="math display">\[
\begin{aligned}
\operatorname{var}^{*}\left[Y^{*}\right] &amp;=\mathbb{E}^{*}\left[Y^{*} Y^{* \prime}\right]-\left(\mathbb{E}^{*}\left[Y^{*}\right]\right)\left(\mathbb{E}^{*}\left[Y^{*}\right]\right)^{\prime} \\
&amp;=\sum_{i=1}^{n} Y_{i} Y_{i}^{\prime} \mathbb{P}^{*}\left[Y^{*}=Y_{i}\right]-\bar{Y} \bar{Y}^{\prime} \\
&amp;=\sum_{i=1}^{n} Y_{i} Y_{i}^{\prime} \frac{1}{n}-\bar{Y} \bar{Y}^{\prime} \\
&amp;=\widehat{\Sigma}
\end{aligned}
\]</span></p>
<p>To summarize, the conditional distribution of <span class="math inline">\(Y^{*}\)</span>, given <span class="math inline">\(F_{n}\)</span>, is the discrete distribution on <span class="math inline">\(\left\{Y_{1}, \ldots, Y_{n}\right\}\)</span> with mean <span class="math inline">\(\bar{Y}\)</span> and covariance matrix <span class="math inline">\(\widehat{\Sigma}\)</span>.</p>
<p>We can extend this analysis to any integer moment <span class="math inline">\(r\)</span>. Assume <span class="math inline">\(Y\)</span> is scalar. The <span class="math inline">\(r^{t h}\)</span> moment of <span class="math inline">\(Y^{*}\)</span> is</p>
<p><span class="math display">\[
\mu_{r}^{* \prime}=\mathbb{E}^{*}\left[Y^{* r}\right]=\sum_{i=1}^{n} Y_{i}^{r} \mathbb{P}^{*}\left[Y^{*}=Y_{i}\right]=\frac{1}{n} \sum_{i=1}^{n} Y_{i}^{r}=\widehat{\mu}_{r}^{\prime},
\]</span></p>
<p>the <span class="math inline">\(r^{t h}\)</span> sample moment. The <span class="math inline">\(r^{t h}\)</span> central moment of <span class="math inline">\(Y^{*}\)</span> is</p>
<p><span class="math display">\[
\mu_{r}^{*}=\mathbb{E}^{*}\left[\left(Y^{*}-\bar{Y}\right)^{r}\right]=\frac{1}{n} \sum_{i=1}^{n}\left(Y_{i}-\bar{Y}\right)^{r}=\widehat{\mu}_{r},
\]</span></p>
<p>the <span class="math inline">\(r^{t h}\)</span> central sample moment. Similarly, the <span class="math inline">\(r^{t h}\)</span> cumulant of <span class="math inline">\(Y^{*}\)</span> is <span class="math inline">\(\kappa_{r}^{*}=\widehat{\kappa}_{r}\)</span>, the <span class="math inline">\(r^{t h}\)</span> sample cumulant.</p>
</section>
<section id="the-distribution-of-the-bootstrap-sample-mean" class="level2" data-number="11.11">
<h2 data-number="11.11" class="anchored" data-anchor-id="the-distribution-of-the-bootstrap-sample-mean"><span class="header-section-number">11.11</span> The Distribution of the Bootstrap Sample Mean</h2>
<p>The bootstrap sample mean is</p>
<p><span class="math display">\[
\bar{Y}^{*}=\frac{1}{n} \sum_{i=1}^{n} Y_{i}^{*} .
\]</span></p>
<p>We can calculate its (conditional) mean and variance. The mean is</p>
<p><span class="math display">\[
\mathbb{E}^{*}\left[\bar{Y}^{*}\right]=\mathbb{E}^{*}\left[\frac{1}{n} \sum_{i=1}^{n} Y_{i}^{*}\right]=\frac{1}{n} \sum_{i=1}^{n} \mathbb{E}^{*}\left[Y_{i}^{*}\right]=\frac{1}{n} \sum_{i=1}^{n} \bar{Y}=\bar{Y}
\]</span></p>
<p>using (10.10). Thus the bootstrap sample mean <span class="math inline">\(\bar{Y}^{*}\)</span> has a distribution centered at the sample mean <span class="math inline">\(\bar{Y}\)</span>. This is because the bootstrap observations <span class="math inline">\(Y_{i}^{*}\)</span> are drawn from the bootstrap universe, which treats the EDF as the truth, and the mean of the latter distribution is <span class="math inline">\(\bar{Y}\)</span>.</p>
<p>The (conditional) variance of the bootstrap sample mean is</p>
<p><span class="math display">\[
\operatorname{var}^{*}\left[\bar{Y}^{*}\right]=\operatorname{var}^{*}\left[\frac{1}{n} \sum_{i=1}^{n} Y_{i}^{*}\right]=\frac{1}{n^{2}} \sum_{i=1}^{n} \operatorname{var}^{*}\left[Y_{i}^{*}\right]=\frac{1}{n^{2}} \sum_{i=1}^{n} \widehat{\Sigma}=\frac{1}{n} \widehat{\Sigma}
\]</span></p>
<p>using (10.11). In the scalar case, <span class="math inline">\(\operatorname{var}^{*}\left[\bar{Y}^{*}\right]=\widehat{\sigma}^{2} / n\)</span>. This shows that the bootstrap variance of <span class="math inline">\(\bar{Y}^{*}\)</span> is precisely described by the sample variance of the original observations. Again, this is because the bootstrap observations <span class="math inline">\(Y_{i}^{*}\)</span> are drawn from the bootstrap universe.</p>
<p>We can extend this to any integer moment <span class="math inline">\(r\)</span>. Assume <span class="math inline">\(Y\)</span> is scalar. Define the normalized bootstrap sample mean <span class="math inline">\(Z_{n}^{*}=\sqrt{n}\left(\bar{Y}^{*}-\bar{Y}\right)\)</span>. Using expressions from Section <span class="math inline">\(6.17\)</span> of Probability and Statistics for Economists, the <span class="math inline">\(3^{r d}\)</span> through <span class="math inline">\(6^{\text {th }}\)</span> conditional moments of <span class="math inline">\(Z_{n}^{*}\)</span> are</p>
<p><span class="math display">\[
\begin{aligned}
&amp;\mathbb{E}^{*}\left[Z_{n}^{* 3}\right]=\widehat{\kappa}_{3} / n^{1 / 2} \\
&amp;\mathbb{E}^{*}\left[Z_{n}^{* 4}\right]=\widehat{\kappa}_{4} / n+3 \widehat{\kappa}_{2}^{2} \\
&amp;\mathbb{E}^{*}\left[Z_{n}^{* 5}\right]=\widehat{\kappa}_{5} / n^{3 / 2}+10 \widehat{\kappa}_{3} \widehat{\kappa}_{2} / n^{1 / 2} \\
&amp;\mathbb{E}^{*}\left[Z_{n}^{* 6}\right]=\widehat{\kappa}_{6} / n^{2}+\left(15 \widehat{\kappa}_{4} \kappa_{2}+10 \widehat{\kappa}_{3}^{2}\right) / n+15 \widehat{\kappa}_{2}^{3}
\end{aligned}
\]</span></p>
<p>where <span class="math inline">\(\widehat{\kappa}_{r}\)</span> is the <span class="math inline">\(r^{t h}\)</span> sample cumulant. Similar expressions can be derived for higher moments. The moments (10.14) are exact, not approximations.</p>
</section>
<section id="bootstrap-asymptotics" class="level2" data-number="11.12">
<h2 data-number="11.12" class="anchored" data-anchor-id="bootstrap-asymptotics"><span class="header-section-number">11.12</span> Bootstrap Asymptotics</h2>
<p>The bootstrap mean <span class="math inline">\(\bar{Y}^{*}\)</span> is a sample average over <span class="math inline">\(n\)</span> i.i.d. random variables, so we might expect it to converge in probability to its expectation. Indeed, this is the case, but we have to be a bit careful since the bootstrap mean has a conditional distribution (given the data) so we need to define convergence in probability for conditional distributions.</p>
<p>Definition <span class="math inline">\(10.1\)</span> We say that a random vector <span class="math inline">\(Z_{n}^{*}\)</span> converges in bootstrap probability to <span class="math inline">\(Z\)</span> as <span class="math inline">\(n \rightarrow \infty\)</span>, denoted <span class="math inline">\(Z_{n}^{*} \underset{p^{*}}{\longrightarrow} Z\)</span>, if for all <span class="math inline">\(\epsilon&gt;0\)</span></p>
<p><span class="math display">\[
\mathbb{P}^{*}\left[\left\|Z_{n}^{*}-Z\right\|&gt;\epsilon\right] \underset{p}{\longrightarrow} 0
\]</span></p>
<p>To understand this definition recall that conventional convergence in probability <span class="math inline">\(Z_{n} \underset{p}{\longrightarrow}\)</span> means that for a sufficiently large sample size <span class="math inline">\(n\)</span>, the probability is high that <span class="math inline">\(Z_{n}\)</span> is arbitrarily close to its limit <span class="math inline">\(Z\)</span>. In contrast, Definition <span class="math inline">\(10.1\)</span> says <span class="math inline">\(Z_{n}^{*} \underset{p^{*}}{ } Z\)</span> means that for a sufficiently large <span class="math inline">\(n\)</span>, the probability is high that the conditional probability that <span class="math inline">\(Z_{n}^{*}\)</span> is close to its limit <span class="math inline">\(Z\)</span> is high. Note that there are two uses of probability - both unconditional and conditional.</p>
<p>Our label “convergence in bootstrap probability” is a bit unusual. The label used in much of the statistical literature is “convergence in probability, in probability” but that seems like a mouthful. That literature more often focuses on the related concept of “convergence in probability, almost surely” which holds if we replace the ” <span class="math inline">\(\underset{p}{\text { " }}\)</span> convergence with almost sure convergence. We do not use this concept in this chapter as it is an unnecessary complication.</p>
<p>While we have stated Definition <span class="math inline">\(10.1\)</span> for the specific conditional probability distribution <span class="math inline">\(\mathbb{P}^{*}\)</span>, the idea is more general and can be used for any conditional distribution and any sequence of random vectors.</p>
<p>The following may seem obvious but it is useful to state for clarity. Its proof is given in Section <span class="math inline">\(10.31 .\)</span></p>
<p>Theorem <span class="math inline">\(10.1\)</span> If <span class="math inline">\(Z_{n} \underset{p}{\longrightarrow} Z\)</span> as <span class="math inline">\(n \rightarrow \infty\)</span> then <span class="math inline">\(Z_{n} \underset{p^{*}}{ } Z\)</span>.</p>
<p>Given Definition 10.1, we can establish a law of large numbers for the bootstrap sample mean. Theorem <span class="math inline">\(10.2\)</span> Bootstrap WLLN. If <span class="math inline">\(Y_{i}\)</span> are independent and uniformly integrable then <span class="math inline">\(\bar{Y}^{*}-\bar{Y} \underset{p^{*}}{\longrightarrow} 0\)</span> and <span class="math inline">\(\bar{Y}^{*} \underset{p^{*}}{\longrightarrow} \mu=\mathbb{E}[Y]\)</span> as <span class="math inline">\(n \rightarrow \infty\)</span>.</p>
<p>The proof (presented in Section 10.31) is somewhat different from the classical case as it is based on the Marcinkiewicz WLLN (Theorem 10.20, presented in Section 10.31).</p>
<p>Notice that the conditions for the bootstrap WLLN are the same for the conventional WLLN. Notice as well that we state two related but slightly different results. The first is that the difference between the bootstrap sample mean <span class="math inline">\(\bar{Y}^{*}\)</span> and the sample mean <span class="math inline">\(\bar{Y}\)</span> diminishes as the sample size diverges. The second result is that the bootstrap sample mean converges to the population mean <span class="math inline">\(\mu\)</span>. The latter is not surprising (since the sample mean <span class="math inline">\(\bar{Y}\)</span> converges in probability to <span class="math inline">\(\mu\)</span> ) but it is constructive to be precise since we are dealing with a new convergence concept.</p>
<p>Theorem 10.3 Bootstrap Continuous Mapping Theorem. If <span class="math inline">\(Z_{n}^{*} \underset{p^{*}}{ } c\)</span> as <span class="math inline">\(n \rightarrow\)</span> <span class="math inline">\(\infty\)</span> and <span class="math inline">\(g(\cdot)\)</span> is continuous at <span class="math inline">\(c\)</span>, then <span class="math inline">\(g\left(Z_{n}^{*}\right) \underset{p^{*}}{ } g(c)\)</span> as <span class="math inline">\(n \rightarrow \infty\)</span>.</p>
<p>The proof is essentially identical to that of Theorem <span class="math inline">\(6.6\)</span> so is omitted.</p>
<p>We next would like to show that the bootstrap sample mean is asymptotically normally distributed, but for that we need a definition of convergence for conditional distributions.</p>
<p>Definition <span class="math inline">\(10.2\)</span> Let <span class="math inline">\(Z_{n}^{*}\)</span> be a sequence of random vectors with conditional distributions <span class="math inline">\(G_{n}^{*}(x)=\mathbb{P}^{*}\left[Z_{n}^{*} \leq x\right]\)</span>. We say that <span class="math inline">\(Z_{n}^{*}\)</span> converges in bootstrap distribution to <span class="math inline">\(Z\)</span> as <span class="math inline">\(n \rightarrow \infty\)</span>, denoted <span class="math inline">\(Z_{n}^{*} \underset{d^{*}}{\longrightarrow}\)</span>, if for all <span class="math inline">\(x\)</span> at which <span class="math inline">\(G(x)=\mathbb{P}[Z \leq x]\)</span> is continuous, <span class="math inline">\(G_{n}^{*}(x) \underset{p}{\longrightarrow} G(x)\)</span> as <span class="math inline">\(n \rightarrow \infty\)</span>.</p>
<p>The difference with the conventional definition is that Definition <span class="math inline">\(10.2\)</span> treats the conditional distribution as random. An alternative label for Definition <span class="math inline">\(10.2\)</span> is “convergence in distribution, in probability”.</p>
<p>We now state a CLT for the bootstrap sample mean, with a proof given in Section 10.31.</p>
<p>Theorem 10.4 Bootstrap CLT. If <span class="math inline">\(Y_{i}\)</span> are i.i.d., <span class="math inline">\(\mathbb{E}\|Y\|^{2}&lt;\infty\)</span>, and <span class="math inline">\(\Sigma=\operatorname{var}[Y]&gt;0\)</span>, then as <span class="math inline">\(n \rightarrow \infty, \sqrt{n}\left(\bar{Y}^{*}-\bar{Y}\right) \underset{d^{*}}{\longrightarrow} \mathrm{N}(0, \Sigma)\)</span>.</p>
<p>Theorem <span class="math inline">\(10.4\)</span> shows that the normalized bootstrap sample mean has the same asymptotic distribution as the sample mean. Thus the bootstrap distribution is asymptotically the same as the sampling distribution. A notable difference, however, is that the bootstrap sample mean is normalized by centering at the sample mean, not at the population mean. This is because <span class="math inline">\(\bar{Y}\)</span> is the true mean in the bootstrap universe.</p>
<p>We next state the distributional form of the continuous mapping theorem for bootstrap distributions and the Bootstrap Delta Method. Theorem 10.5 Bootstrap Continuous Mapping Theorem</p>
<p>If <span class="math inline">\(Z_{n}^{*} \underset{d^{*}}{ } Z\)</span> as <span class="math inline">\(n \rightarrow \infty\)</span> and <span class="math inline">\(g: \mathbb{R}^{m} \rightarrow \mathbb{R}^{k}\)</span> has the set of discontinuity points <span class="math inline">\(D_{g}\)</span> such that <span class="math inline">\(\mathbb{P}^{*}\left[Z^{*} \in D_{g}\right]=0\)</span>, then <span class="math inline">\(g\left(Z_{n}^{*}\right) \underset{d^{*}}{\rightarrow} g(Z)\)</span> as <span class="math inline">\(n \rightarrow \infty\)</span>.</p>
<p>Theorem 10.6 Bootstrap Delta Method: If <span class="math inline">\(\widehat{\mu} \underset{p}{\longrightarrow} \mu, \sqrt{n}\left(\widehat{\mu}^{*}-\widehat{\mu}\right) \underset{d^{*}}{\longrightarrow} \xi\)</span>, and <span class="math inline">\(g(u)\)</span> is continuously differentiable in a neighborhood of <span class="math inline">\(\mu\)</span>, then as <span class="math inline">\(n \rightarrow \infty\)</span></p>
<p><span class="math display">\[
\sqrt{n}\left(g\left(\widehat{\mu}^{*}\right)-g(\widehat{\mu})\right) \underset{d^{*}}{\longrightarrow} \boldsymbol{G}^{\prime} \xi
\]</span></p>
<p>where <span class="math inline">\(\boldsymbol{G}(x)=\frac{\partial}{\partial x} g(x)^{\prime}\)</span> and <span class="math inline">\(\boldsymbol{G}=\boldsymbol{G}(\mu)\)</span>. In particular, if <span class="math inline">\(\xi \sim \mathrm{N}(0, \boldsymbol{V})\)</span> then as <span class="math inline">\(n \rightarrow \infty\)</span></p>
<p><span class="math display">\[
\sqrt{n}\left(g\left(\widehat{\mu}^{*}\right)-g(\widehat{\mu})\right) \underset{d^{*}}{\longrightarrow} \mathrm{N}\left(0, \boldsymbol{G}^{\prime} \boldsymbol{V} \boldsymbol{G}\right) .
\]</span></p>
<p>For a proof, see Exercise 10.7.</p>
<p>We state an analog of Theorem 6.10, which presented the asymptotic distribution for general smooth functions of sample means, which covers most econometric estimators.</p>
<p>Theorem 10.7 Under the assumptions of Theorem 6.10, that is, if <span class="math inline">\(Y_{i}\)</span> is i.i.d., <span class="math inline">\(\mu=\mathbb{E}[h(Y)], \theta=g(\mu), \mathbb{E}\|h(Y)\|^{2}&lt;\infty\)</span>, and <span class="math inline">\(\boldsymbol{G}(x)=\frac{\partial}{\partial x} g(x)^{\prime}\)</span> is continuous in a neighborhood of <span class="math inline">\(\mu\)</span>, for <span class="math inline">\(\widehat{\theta}=g(\widehat{\mu})\)</span> with <span class="math inline">\(\widehat{\mu}=\frac{1}{n} \sum_{i=1}^{n} h\left(Y_{i}\right)\)</span> and <span class="math inline">\(\widehat{\theta}^{*}=g\left(\widehat{\mu}^{*}\right)\)</span> with <span class="math inline">\(\widehat{\mu}^{*}=\frac{1}{n} \sum_{i=1}^{n} h\left(Y_{i}^{*}\right)\)</span>, as <span class="math inline">\(n \rightarrow \infty\)</span></p>
<p><span class="math display">\[
\sqrt{n}\left(\widehat{\theta}^{*}-\widehat{\theta}\right) \underset{d^{*}}{\longrightarrow} \mathrm{N}\left(0, \boldsymbol{V}_{\theta}\right)
\]</span></p>
<p>where <span class="math inline">\(\boldsymbol{V}_{\theta}=\boldsymbol{G}^{\prime} \boldsymbol{V} \boldsymbol{G}, \boldsymbol{V}=\mathbb{E}\left[(h(Y)-\mu)(h(Y)-\mu)^{\prime}\right]\)</span> and <span class="math inline">\(\boldsymbol{G}=\boldsymbol{G}(\mu)\)</span>.</p>
<p>For a proof, see Exercise 10.8.</p>
<p>Theorem <span class="math inline">\(10.7\)</span> shows that the asymptotic distribution of the bootstrap estimator <span class="math inline">\(\widehat{\theta}^{*}\)</span> is identical to that of the sample estimator <span class="math inline">\(\widehat{\theta}\)</span>. This means that we can learn the distribution of <span class="math inline">\(\widehat{\theta}\)</span> from the bootstrap distribution, and hence perform asymptotically correct inference.</p>
<p>For some bootstrap applications we use bootstrap estimates of variance. The plug-in estimator of <span class="math inline">\(\boldsymbol{V}_{\boldsymbol{\theta}}\)</span> is <span class="math inline">\(\widehat{\boldsymbol{V}}_{\theta}=\widehat{\boldsymbol{G}}^{\prime} \widehat{\boldsymbol{V}} \widehat{\boldsymbol{G}}\)</span> where <span class="math inline">\(\widehat{\boldsymbol{G}}=\boldsymbol{G}(\widehat{\mu})\)</span> and</p>
<p><span class="math display">\[
\widehat{\boldsymbol{V}}=\frac{1}{n} \sum_{i=1}^{n}\left(h\left(Y_{i}\right)-\widehat{\mu}\right)\left(h\left(Y_{i}\right)-\widehat{\mu}\right)^{\prime} .
\]</span></p>
<p>The bootstrap version is</p>
<p><span class="math display">\[
\begin{aligned}
&amp;\widehat{\boldsymbol{V}}_{\theta}^{*}=\widehat{\boldsymbol{G}}^{* \prime} \widehat{\boldsymbol{V}}^{*} \widehat{\boldsymbol{G}}^{*} \\
&amp;\widehat{\boldsymbol{G}}^{*}=\boldsymbol{G}\left(\widehat{\mu}^{*}\right) \\
&amp;\widehat{\boldsymbol{V}}^{*}=\frac{1}{n} \sum_{i=1}^{n}\left(h\left(Y_{i}^{*}\right)-\widehat{\mu}^{*}\right)\left(h\left(Y_{i}^{*}\right)-\widehat{\mu}^{*}\right)^{\prime} .
\end{aligned}
\]</span></p>
<p>Application of the bootstrap WLLN and bootstrap CMT show that <span class="math inline">\(\widehat{\boldsymbol{V}}_{\theta}^{*}\)</span> is consistent for <span class="math inline">\(\boldsymbol{V}_{\theta}\)</span>.</p>
<p>Theorem <span class="math inline">\(10.8\)</span> Under the assumptions of Theorem 10.7, <span class="math inline">\(\widehat{\boldsymbol{V}}_{\theta}^{*} \underset{p^{*}}{\longrightarrow} \boldsymbol{V}_{\theta}\)</span> as <span class="math inline">\(n \rightarrow \infty\)</span>.</p>
<p>For a proof, see Exercise 10.9.</p>
</section>
<section id="consistency-of-the-bootstrap-estimate-of-variance" class="level2" data-number="11.13">
<h2 data-number="11.13" class="anchored" data-anchor-id="consistency-of-the-bootstrap-estimate-of-variance"><span class="header-section-number">11.13</span> Consistency of the Bootstrap Estimate of Variance</h2>
<p>Recall the definition (10.7) of the bootstrap estimator of variance <span class="math inline">\(\widehat{\boldsymbol{V}}_{\widehat{\theta}}^{\text {boot }}\)</span> of an estimator <span class="math inline">\(\widehat{\theta}\)</span>. In this section we explore conditions under which <span class="math inline">\(\widehat{\boldsymbol{V}}_{\widehat{\theta}}^{\text {boot }}\)</span> is consistent for the asymptotic variance of <span class="math inline">\(\widehat{\theta}\)</span>.</p>
<p>To do so it is useful to focus on a normalized version of the estimator so that the asymptotic variance is not degenerate. Suppose that for some sequence <span class="math inline">\(a_{n}\)</span> we have</p>
<p><span class="math display">\[
Z_{n}=a_{n}(\widehat{\theta}-\theta) \underset{d}{\longrightarrow} \xi
\]</span></p>
<p>and</p>
<p><span class="math display">\[
Z_{n}^{*}=a_{n}\left(\widehat{\theta}^{*}-\widehat{\theta}\right) \underset{d^{*}}{\longrightarrow} \xi
\]</span></p>
<p>for some limit distribution <span class="math inline">\(\xi\)</span>. That is, for some normalization, both <span class="math inline">\(\hat{\theta}\)</span> and <span class="math inline">\(\widehat{\theta}^{*}\)</span> have the same asymptotic distribution. This is quite general as it includes the smooth function model. The conventional bootstrap estimator of the variance of <span class="math inline">\(Z_{n}\)</span> is the sample variance of the bootstrap draws <span class="math inline">\(\left\{Z_{n}^{*}(b): b=1, \ldots, B\right\}\)</span>. This equals the estimator (10.7) multiplied by <span class="math inline">\(a_{n}^{2}\)</span>. Thus it is equivalent (up to scale) whether we discuss estimating the variance of <span class="math inline">\(\widehat{\theta}\)</span> or <span class="math inline">\(Z_{n}\)</span>.</p>
<p>The bootstrap estimator of variance of <span class="math inline">\(Z_{n}\)</span> is</p>
<p><span class="math display">\[
\begin{aligned}
\widehat{\boldsymbol{V}}_{\theta}^{\text {boot,B }} &amp;=\frac{1}{B-1} \sum_{b=1}^{B}\left(Z_{n}^{*}(b)-Z_{n}^{*}\right)\left(Z_{n}^{*}(b)-Z_{n}^{*}\right)^{\prime} \\
\bar{Z}_{n}^{*} &amp;=\frac{1}{B} \sum_{b=1}^{B} Z_{n}^{*}(b)
\end{aligned}
\]</span></p>
<p>Notice that we index the estimator by the number of bootstrap replications <span class="math inline">\(B\)</span>.</p>
<p>Since <span class="math inline">\(Z_{n}^{*}\)</span> converges in bootstrap distribution to the same asymptotic distribution as <span class="math inline">\(Z_{n}\)</span>, it seems reasonable to guess that the variance of <span class="math inline">\(Z_{n}^{*}\)</span> will converge to that of <span class="math inline">\(\xi\)</span>. However, convergence in distribution is not sufficient for convergence in moments. For the variance to converge it is also necessary for the sequence <span class="math inline">\(Z_{n}^{*}\)</span> to be uniformly square integrable. Theorem <span class="math inline">\(10.9\)</span> If (10.15) and (10.16) hold for some sequence <span class="math inline">\(a_{n}\)</span> and <span class="math inline">\(\left\|Z_{n}^{*}\right\|^{2}\)</span> is uniformly integrable, then as <span class="math inline">\(B \rightarrow \infty\)</span></p>
<p><span class="math display">\[
\widehat{\boldsymbol{V}}_{\theta}^{\mathrm{boot}, \mathrm{B}} \underset{p^{*}}{\longrightarrow} \widehat{\boldsymbol{V}}_{\theta}^{\text {boot }}=\operatorname{var}\left[Z_{n}^{*}\right] \text {, }
\]</span></p>
<p>and as <span class="math inline">\(n \rightarrow \infty\)</span></p>
<p><span class="math display">\[
\widehat{\boldsymbol{V}}_{\theta}^{\text {boot }} \underset{p^{*}}{\longrightarrow} \boldsymbol{V}_{\theta}=\operatorname{var}[\xi] .
\]</span></p>
<p>This raises the question: Is the normalized sequence <span class="math inline">\(Z_{n}\)</span> uniformly integrable? We spend the remainder of this section exploring this question and turn in the next section to trimmed variance estimators which do not require uniform integrability.</p>
<p>This condition is reasonably straightforward to verify for the case of a scalar sample mean with a finite variance. That is, suppose <span class="math inline">\(Z_{n}^{*}=\sqrt{n}\left(\bar{Y}^{*}-\bar{Y}\right)\)</span> and <span class="math inline">\(\mathbb{E}\left[Y^{2}\right]&lt;\infty\)</span>. In (10.14) we calculated the exact fourth central moment of <span class="math inline">\(Z_{n}^{*}\)</span> :</p>
<p><span class="math display">\[
\mathbb{E}^{*}\left[Z_{n}^{* 4}\right]=\frac{\widehat{\kappa}_{4}}{n}+3 \widehat{\sigma}^{4}=\frac{\widehat{\mu}_{4}-3 \widehat{\sigma}^{4}}{n}+3 \widehat{\sigma}^{4}
\]</span></p>
<p>where <span class="math inline">\(\widehat{\sigma}^{2}=n^{-1} \sum_{i=1}^{n}\left(Y_{i}-\bar{Y}\right)^{2}\)</span> and <span class="math inline">\(\widehat{\mu}_{4}=n^{-1} \sum_{i=1}^{n}\left(Y_{i}-\bar{Y}\right)^{4}\)</span>. The assumption <span class="math inline">\(\mathbb{E}\left[Y^{2}\right]&lt;\infty\)</span> implies that <span class="math inline">\(\mathbb{E}\left[\widehat{\sigma}^{2}\right]=O(1)\)</span> so <span class="math inline">\(\widehat{\sigma}^{2}=O_{p}(1)\)</span>. Furthermore, <span class="math inline">\(n^{-1} \widehat{\mu}_{4}=n^{-2} \sum_{i=1}^{n}\left(Y_{i}-\bar{Y}\right)^{4}=o_{p}(1)\)</span> by the Marcinkiewicz WLLN (Theorem 10.20). It follows that</p>
<p><span class="math display">\[
\mathbb{E}^{*}\left[Z_{n}^{* 4}\right]=n^{2} \mathbb{E}^{*}\left[\left(\bar{Y}^{*}-\bar{Y}\right)^{4}\right]=O_{p}(1) .
\]</span></p>
<p>Theorem <span class="math inline">\(6.13\)</span> shows that this implies that <span class="math inline">\(Z_{n}^{* 2}\)</span> is uniformly integrable. Thus if <span class="math inline">\(Y\)</span> has a finite variance the normalized bootstrap sample mean is uniformly square integrable and the bootstrap estimate of variance is consistent by Theorem <span class="math inline">\(10.9\)</span>.</p>
<p>Now consider the smooth function model of Theorem 10.7. We can establish the following result.</p>
<p>Theorem 10.10 In the smooth function model of Theorem 10.7, if for some <span class="math inline">\(p \geq 1\)</span> the <span class="math inline">\(p^{t h}\)</span>-order derivatives of <span class="math inline">\(g(x)\)</span> are bounded, then <span class="math inline">\(Z_{n}^{*}=\sqrt{n}\left(\widehat{\theta}^{*}-\widehat{\theta}\right)\)</span> is uniformly square integrable and the bootstrap estimator of variance is consistent as in Theorem 10.9.</p>
<p>For a proof see Section <span class="math inline">\(10.31\)</span>.</p>
<p>This shows that the bootstrap estimate of variance is consistent for a reasonably broad class of estimators. The class of functions <span class="math inline">\(g(x)\)</span> covered by this result includes all <span class="math inline">\(p^{t h}\)</span>-order polynomials.</p>
</section>
<section id="trimmed-estimator-of-bootstrap-variance" class="level2" data-number="11.14">
<h2 data-number="11.14" class="anchored" data-anchor-id="trimmed-estimator-of-bootstrap-variance"><span class="header-section-number">11.14</span> Trimmed Estimator of Bootstrap Variance</h2>
<p>Theorem <span class="math inline">\(10.10\)</span> showed that the bootstrap estimator of variance is consistent for smooth functions with a bounded <span class="math inline">\(p^{t h}\)</span> order derivative. This is a fairly broad class but excludes many important applications. An example is <span class="math inline">\(\theta=\mu_{1} / \mu_{2}\)</span> where <span class="math inline">\(\mu_{1}=\mathbb{E}\left[Y_{1}\right]\)</span> and <span class="math inline">\(\mu_{2}=\mathbb{E}\left[Y_{2}\right]\)</span>. This function does not have a bounded derivative (unless <span class="math inline">\(\mu_{2}\)</span> is bounded away from zero) so is not covered by Theorem 10.10. This is more than a technical issue. When <span class="math inline">\(\left(Y_{1}, Y_{2}\right)\)</span> are jointly normally distributed then it is known that <span class="math inline">\(\widehat{\theta}=\bar{Y}_{1} / \bar{Y}_{2}\)</span> does not possess a finite variance. Consequently we cannot expect the bootstrap estimator of variance to perform well. (It is attempting to estimate the variance of <span class="math inline">\(\widehat{\theta}\)</span>, which is infinity.)</p>
<p>In these cases it is preferred to use a trimmed estimator of bootstrap variance. Let <span class="math inline">\(\tau_{n} \rightarrow \infty\)</span> be a sequence of positive trimming numbers satisfying <span class="math inline">\(\tau_{n}=O\left(e^{n / 8}\right)\)</span>. Define the trimmed statistic</p>
<p><span class="math display">\[
Z_{n}^{* *}=Z_{n}^{*} \mathbb{1}\left\{\left\|Z_{n}^{*}\right\| \leq \tau_{n}\right\} .
\]</span></p>
<p>The trimmed bootstrap estimator of variance is</p>
<p><span class="math display">\[
\begin{aligned}
\widehat{\boldsymbol{V}}_{\theta}^{\text {boot, }, \tau} &amp;=\frac{1}{B-1} \sum_{b=1}^{B}\left(Z_{n}^{* *}(b)-Z_{n}^{* *}\right)\left(Z_{n}^{* *}(b)-Z_{n}^{* *}\right)^{\prime} \\
Z_{n}^{* *} &amp;=\frac{1}{B} \sum_{b=1}^{B} Z_{n}^{* *}(b) .
\end{aligned}
\]</span></p>
<p>We first examine the behavior of <span class="math inline">\(\widehat{\boldsymbol{V}}_{\theta}^{\text {boot, } \mathrm{B}}\)</span> as the number of bootstrap replications <span class="math inline">\(B\)</span> grows to infinity. It is a sample variance of independent bounded random vectors. Thus by the bootstrap WLLN (Theorem 10.2) <span class="math inline">\(\widehat{\boldsymbol{V}}_{\theta}^{\mathrm{boot}, \mathrm{B}, \tau}\)</span> converges in bootstrap probability to the variance of <span class="math inline">\(Z_{n}^{* *}\)</span>.</p>
<p><img src="images//2022_09_17_704b7bd5000562ad7735g-20.jpg" class="img-fluid"></p>
<p>We next examine the behavior of the bootstrap estimator <span class="math inline">\(\widehat{\boldsymbol{V}}_{\theta}^{\text {boot, } \tau}\)</span> as <span class="math inline">\(n\)</span> grows to infinity. We focus on the smooth function model of Theorem 10.7, which showed that <span class="math inline">\(Z_{n}^{*}=\sqrt{n}\left(\widehat{\theta}^{*}-\widehat{\theta}\right) \underset{d^{*}}{\longrightarrow} \sim \mathrm{N}\left(0, \boldsymbol{V}_{\theta}\right)\)</span>. Since the trimming is asymptotically negligible, it follows that <span class="math inline">\(Z_{n}^{* *} \underset{d^{*}}{\longrightarrow}\)</span>. If we can show that <span class="math inline">\(Z_{n}^{* *}\)</span> is uniformly square integrable, Theorem <span class="math inline">\(10.9\)</span> shows that <span class="math inline">\(\operatorname{var}\left[Z_{n}^{* *}\right] \rightarrow \operatorname{var}[Z]=\boldsymbol{V}_{\theta}\)</span> as <span class="math inline">\(n \rightarrow \infty\)</span>. This is shown in the following result, whose proof is presented in Section 10.31.</p>
<p>Theorem <span class="math inline">\(10.12\)</span> Under the assumptions of Theorem 10.7, <span class="math inline">\(\widehat{\boldsymbol{V}}_{\theta}^{\mathrm{boot}, \tau} \underset{p^{*}}{\longrightarrow} \boldsymbol{V}_{\theta} .\)</span></p>
<p>Theorems <span class="math inline">\(10.11\)</span> and <span class="math inline">\(10.12\)</span> show that the trimmed bootstrap estimator of variance is consistent for the asymptotic variance in the smooth function model, which includes most econometric estimators. This justifies bootstrap standard errors as consistent estimators for the asymptotic distribution.</p>
<p>An important caveat is that these results critically rely on the trimmed variance estimator. This is a critical caveat as conventional statistical packages (e.g.&nbsp;Stata) calculate bootstrap standard errors using the untrimmed estimator (10.7). Thus there is no guarantee that the reported standard errors are consistent. The untrimmed variance estimator works in the context of Theorem <span class="math inline">\(10.10\)</span> and whenever the bootstrap statistic is uniformly square integrable, but not necessarily in general applications.</p>
<p>In practice, it may be difficult to know how to select the trimming sequence <span class="math inline">\(\tau_{n}\)</span>. The rule <span class="math inline">\(\tau_{n}=O\left(e^{n / 8}\right)\)</span> does not provide practical guidance. Instead, it may be useful to think about trimming in terms of percentages of the bootstrap draws. Thus we can set <span class="math inline">\(\tau_{n}\)</span> so that a given small percentage <span class="math inline">\(\gamma_{n}\)</span> is trimmed. For theoretical interpretation we would set <span class="math inline">\(\gamma_{n} \rightarrow 0\)</span> as <span class="math inline">\(n \rightarrow \infty\)</span>. In practice we might set <span class="math inline">\(\gamma_{n}=1 %\)</span>.</p>
</section>
<section id="unreliability-of-untrimmed-bootstrap-standard-errors" class="level2" data-number="11.15">
<h2 data-number="11.15" class="anchored" data-anchor-id="unreliability-of-untrimmed-bootstrap-standard-errors"><span class="header-section-number">11.15</span> Unreliability of Untrimmed Bootstrap Standard Errors</h2>
<p>In the previous section we presented a trimmed bootstrap variance estimator which should be used to form bootstrap standard errors for nonlinear estimators. Otherwise, the untrimmed estimator is potentially unreliable.</p>
<p>This is an unfortunate situation, because reporting of bootstrap standard errors is commonplace in contemporary applied econometric practice, and standard applications (including Stata) use the untrimmed estimator.</p>
<p>To illustrate the seriousness of the problem we use the simple wage regression (7.31) which we repeat here. This is the subsample of married Black women with 982 observations. The point estimates and standard errors are</p>
<p><img src="images//2022_09_17_704b7bd5000562ad7735g-21.jpg" class="img-fluid"></p>
<p>We are interested in the experience level which maximizes expected log wages <span class="math inline">\(\theta_{3}=-50 \beta_{2} / \beta_{3}\)</span>. The point estimate and standard errors calculated with different methods are reported in Table <span class="math inline">\(10.3\)</span> below.</p>
<p>The point estimate of the experience level with maximum earnings is <span class="math inline">\(\widehat{\theta}_{3}=35\)</span>. The asymptotic and jackknife standard errors are about 7 . The bootstrap standard error, however, is 825 ! Confused by this unusual value we rerun the bootstrap and obtain a standard error of 544 . Each was computed with 10,000 bootstrap replications. The fact that the two bootstrap standard errors are considerably different when recomputed (with different starting seeds) is indicative of moment failure. When there is an enormous discrepancy like this between the asymptotic and bootstrap standard error, and between bootstrap runs, it is a signal that there may be moment failure and consequently bootstrap standard errors are unreliable.</p>
<p>A trimmed bootstrap with <span class="math inline">\(\tau=25\)</span> (set to slightly exceed three asymptotic standard errors) produces a more reasonable standard error of <span class="math inline">\(10 .\)</span></p>
<p>One message from this application is that when different methods produce very different standard errors we should be cautious about trusting any single method. The large discrepancies indicate poor asymptotic approximations, rendering all methods inaccurate. Another message is to be cautious about reporting conventional bootstrap standard errors. Trimmed versions are preferred, especially for nonlinear functions of estimated coefficients.</p>
<p>Table 10.3: Experience Level Which Maximizes Expected log Wages</p>
<table class="table">
<thead>
<tr class="header">
<th style="text-align: left;">Estimate</th>
<th><span class="math inline">\(35.2\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Asymptotic s.e.</td>
<td><span class="math inline">\((7.0)\)</span></td>
</tr>
<tr class="even">
<td style="text-align: left;">Jackknife s.e.</td>
<td><span class="math inline">\((7.0)\)</span></td>
</tr>
<tr class="odd">
<td style="text-align: left;">Bootstrap s.e. (standard)</td>
<td><span class="math inline">\((825)\)</span></td>
</tr>
<tr class="even">
<td style="text-align: left;">Bootstrap s.e. (repeat)</td>
<td><span class="math inline">\((544)\)</span></td>
</tr>
<tr class="odd">
<td style="text-align: left;">Bootstrap s.e. (trimmed)</td>
<td><span class="math inline">\((10.1)\)</span></td>
</tr>
</tbody>
</table>
</section>
<section id="consistency-of-the-percentile-interval" class="level2" data-number="11.16">
<h2 data-number="11.16" class="anchored" data-anchor-id="consistency-of-the-percentile-interval"><span class="header-section-number">11.16</span> Consistency of the Percentile Interval</h2>
<p>Recall the percentile interval (10.8). We now provide conditions under which it has asymptotically correct coverage. Theorem <span class="math inline">\(10.13\)</span> Assume that for some sequence <span class="math inline">\(a_{n}\)</span></p>
<p><span class="math display">\[
a_{n}(\widehat{\theta}-\theta) \underset{d}{\longrightarrow} \xi
\]</span></p>
<p>and</p>
<p><span class="math display">\[
a_{n}\left(\widehat{\theta}^{*}-\widehat{\theta}\right) \underset{d^{*}}{\longrightarrow} \xi
\]</span></p>
<p>where <span class="math inline">\(\xi\)</span> is continuously distributed and symmetric about zero. Then <span class="math inline">\(\mathbb{P}\left[\theta \in C^{\mathrm{pc}}\right] \rightarrow 1-\alpha\)</span> as <span class="math inline">\(n \rightarrow \infty\)</span></p>
<p>The assumptions (10.18)-(10.19) hold for the smooth function model of Theorem 10.7, so this result incorporates many applications. The beauty of Theorem <span class="math inline">\(10.13\)</span> is that the simple confidence interval <span class="math inline">\(C^{\mathrm{pc}}\)</span> - which does not require technical calculation of asymptotic standard errors - has asymptotically valid coverage for any estimator which falls in the smooth function class, as well as any other estimator satisfying the convergence results (10.18)-(10.19) with <span class="math inline">\(\xi\)</span> symmetrically distributed. The conditions are weaker than those required for consistent bootstrap variance estimation (and normal-approximation confidence intervals) because it is not necessary to verify that <span class="math inline">\(\widehat{\theta}^{*}\)</span> is uniformly integrable, nor necessary to employ trimming.</p>
<p>The proof of Theorem <span class="math inline">\(10.7\)</span> is not difficult. The convergence assumption (10.19) implies that the <span class="math inline">\(\alpha^{t h}\)</span> quantile of <span class="math inline">\(a_{n}\left(\widehat{\theta}^{*}-\widehat{\theta}\right)\)</span>, which is <span class="math inline">\(a_{n}\left(q_{\alpha}^{*}-\widehat{\theta}\right)\)</span> by quantile equivariance, converges in probability to the <span class="math inline">\(\alpha^{t h}\)</span> quantile of <span class="math inline">\(\xi\)</span>, which we can denote as <span class="math inline">\(\bar{q}_{\alpha}\)</span>. Thus</p>
<p><span class="math display">\[
a_{n}\left(q_{\alpha}^{*}-\widehat{\theta}\right) \underset{p}{\longrightarrow} \bar{q}_{\alpha} .
\]</span></p>
<p>Let <span class="math inline">\(H(x)=\mathbb{P}[\xi \leq x]\)</span> be the distribution function of <span class="math inline">\(\xi\)</span>. The assumption of symmetry implies <span class="math inline">\(H(-x)=\)</span> <span class="math inline">\(1-H(x)\)</span>. Then the percentile interval has coverage</p>
<p><span class="math display">\[
\begin{aligned}
\mathbb{P}\left[\theta \in C^{\mathrm{pc}}\right] &amp;=\mathbb{P}\left[q_{\alpha / 2}^{*} \leq \theta \leq q_{1-\alpha / 2}^{*}\right] \\
&amp;=\mathbb{P}\left[-a_{n}\left(q_{\alpha / 2}^{*}-\widehat{\theta}\right) \geq a_{n}(\widehat{\theta}-\theta) \geq-a_{n}\left(q_{1-\alpha / 2}^{*}-\widehat{\theta}\right)\right] \\
&amp; \rightarrow \mathbb{P}\left[-\bar{q}_{\alpha / 2} \geq \xi \geq-\bar{q}_{1-\alpha / 2}\right] \\
&amp;=H\left(-\bar{q}_{\alpha / 2}\right)-H\left(-\bar{q}_{1-\alpha / 2}\right) \\
&amp;=H\left(\bar{q}_{1-\alpha / 2}\right)-H\left(\bar{q}_{\alpha / 2}\right) \\
&amp;=1-\alpha .
\end{aligned}
\]</span></p>
<p>The convergence holds by (10.18) and (10.20). The following equality uses the definition of <span class="math inline">\(H\)</span>, the nextto-last is the symmetry of <span class="math inline">\(H\)</span>, and the final equality is the definition of <span class="math inline">\(\bar{q}_{\alpha}\)</span>. This establishes Theorem <span class="math inline">\(10.13 .\)</span></p>
<p>Theorem <span class="math inline">\(10.13\)</span> seems quite general, but it critically rests on the assumption that the asymptotic distribution <span class="math inline">\(\xi\)</span> is symmetrically distributed about zero. This may seem innocuous since conventional asymptotic distributions are normal and hence symmetric, but it deserves further scrutiny. It is not merely a technical assumption - an examination of the steps in the preceeding argument isolate quite clearly that if the symmetry assumption is violated then the asymptotic coverage will not be <span class="math inline">\(1-\alpha\)</span>. While Theorem <span class="math inline">\(10.13\)</span> does show that the percentile interval is asymptotically valid for a conventional asymptotically normal estimator, the reliance on symmetry in the argument suggests that the percentile method will work poorly when the finite sample distribution is asymmetric. This turns out to be the case and leads us to consider alternative methods in the following sections. It is also worthwhile to investigate a finite sample justification for the percentile interval based on a heuristic analogy due to Efron.</p>
<p>Assume that there exists an unknown but strictly increasing transformation <span class="math inline">\(\psi(\theta)\)</span> such that <span class="math inline">\(\psi(\widehat{\theta})-\)</span> <span class="math inline">\(\psi(\theta)\)</span> has a pivotal distribution <span class="math inline">\(H(u)\)</span> (does not vary with <span class="math inline">\(\theta\)</span> ) which is symmetric about zero. For example, if <span class="math inline">\(\widehat{\theta} \sim \mathrm{N}\left(\theta, \sigma^{2}\right)\)</span> we can set <span class="math inline">\(\psi(\theta)=\theta / \sigma\)</span>. Alternatively, if <span class="math inline">\(\widehat{\theta}=\exp (\widehat{\mu})\)</span> and <span class="math inline">\(\widehat{\mu} \sim \mathrm{N}\left(\mu, \sigma^{2}\right)\)</span> then we can set <span class="math inline">\(\psi(\theta)=\)</span> <span class="math inline">\(\log (\theta) / \sigma\)</span></p>
<p>To assess the coverage of the percentile interval, observe that since the distribution <span class="math inline">\(H\)</span> is pivotal the bootstrap distribution <span class="math inline">\(\psi\left(\widehat{\theta}^{*}\right)-\psi(\widehat{\theta})\)</span> also has distribution <span class="math inline">\(H(u)\)</span>. Let <span class="math inline">\(\bar{q}_{\alpha}\)</span> be the <span class="math inline">\(\alpha^{\text {th }}\)</span> quantile of the distribution <span class="math inline">\(H\)</span>. Since <span class="math inline">\(q_{\alpha}^{*}\)</span> is the <span class="math inline">\(\alpha^{t h}\)</span> quantile of the distribution of <span class="math inline">\(\widehat{\theta}^{*}\)</span> and <span class="math inline">\(\psi\left(\widehat{\theta}^{*}\right)-\psi(\widehat{\theta})\)</span> is a monotonic transformation of <span class="math inline">\(\widehat{\theta}^{*}\)</span>, by the quantile equivariance property we deduce that <span class="math inline">\(\bar{q}_{\alpha}+\psi(\widehat{\theta})=\psi\left(q_{\alpha}^{*}\right)\)</span>. The percentile interval has coverage</p>
<p><span class="math display">\[
\begin{aligned}
\mathbb{P}\left[\theta \in C^{\mathrm{pc}}\right] &amp;=\mathbb{P}\left[q_{\alpha / 2}^{*} \leq \theta \leq q_{1-\alpha / 2}^{*}\right] \\
&amp;=\mathbb{P}\left[\psi\left(q_{\alpha / 2}^{*}\right) \leq \psi(\theta) \leq \psi\left(q_{1-\alpha / 2}^{*}\right)\right] \\
&amp;=\mathbb{P}\left[\psi(\widehat{\theta})-\psi\left(q_{\alpha / 2}^{*}\right) \geq \psi(\widehat{\theta})-\psi(\theta) \geq \psi(\widehat{\theta})-\psi\left(q_{1-\alpha / 2}^{*}\right)\right] \\
&amp;=\mathbb{P}\left[-\bar{q}_{\alpha / 2} \geq \psi(\widehat{\theta})-\psi(\theta) \geq-\bar{q}_{1-\alpha / 2}\right] \\
&amp;=H\left(-\bar{q}_{\alpha / 2}\right)-H\left(-\bar{q}_{1-\alpha / 2}\right) \\
&amp;=H\left(\bar{q}_{1-\alpha / 2}\right)-H\left(\bar{q}_{\alpha / 2}\right) \\
&amp;=1-\alpha .
\end{aligned}
\]</span></p>
<p>The second equality applies the monotonic transformation <span class="math inline">\(\psi(u)\)</span> to all elements. The fourth uses the relationship <span class="math inline">\(\bar{q}_{\alpha}+\psi(\widehat{\theta})=\psi\left(q_{\alpha}^{*}\right)\)</span>. The fifth uses the defintion of <span class="math inline">\(H\)</span>. The sixth uses the symmetry property of <span class="math inline">\(H\)</span>, and the final is by the definition of <span class="math inline">\(\bar{q}_{\alpha}\)</span> as the <span class="math inline">\(\alpha^{t h}\)</span> quantile of <span class="math inline">\(H\)</span>.</p>
<p>This calculation shows that under these assumptions the percentile interval has exact coverage <span class="math inline">\(1-\alpha\)</span>. The nice thing about this argument is the introduction of the unknown transformation <span class="math inline">\(\psi(u)\)</span> for which the percentile interval automatically adapts. The unpleasant feature is the assumption of symmetry. Similar to the asymptotic argument the calculation strongly relies on the symmetry of distribution <span class="math inline">\(H(x)\)</span>. Without symmetry the coverage will be incorrect.</p>
<p>Intuitively, we expect that when the assumptions are approximately true then the percentile interval will have approximately correct coverage. Thus so long as there is a transformation <span class="math inline">\(\psi(u)\)</span> such that <span class="math inline">\(\psi(\widehat{\theta})-\)</span> <span class="math inline">\(\psi(\theta)\)</span> is approximately pivotal and symmetric about zero, then the percentile interval should work well.</p>
<p>This argument has the following application. Suppose that the parameter of interest is <span class="math inline">\(\theta=\exp (\mu)\)</span> where <span class="math inline">\(\mu=\mathbb{E}[Y]\)</span> and suppose <span class="math inline">\(Y\)</span> has a pivotal symmetric distribution about <span class="math inline">\(\mu\)</span>. Then even though <span class="math inline">\(\widehat{\theta}=\)</span> <span class="math inline">\(\exp (\bar{Y})\)</span> does not have a symmetric distribution, the percentile interval applied to <span class="math inline">\(\widehat{\theta}\)</span> will have the correct coverage, because the monotonic transformation <span class="math inline">\(\log (\widehat{\theta})\)</span> has a pivotal symmetric distribution.</p>
</section>
<section id="bias-corrected-percentile-interval" class="level2" data-number="11.17">
<h2 data-number="11.17" class="anchored" data-anchor-id="bias-corrected-percentile-interval"><span class="header-section-number">11.17</span> Bias-Corrected Percentile Interval</h2>
<p>The accuracy of the percentile interval depends critically upon the assumption that the sampling distribution is approximately symmetrically distributed. This excludes finite sample bias, for an estimator which is biased cannot be symmetrically distributed. Many contexts in which we want to apply bootstrap methods (rather than asymptotic) are when the parameter of interest is a nonlinear function of the model parameters, and nonlinearity typically induces estimation bias. Consequently it is difficult to expect the percentile method to generally have accurate coverage.</p>
<p>To reduce the bias problem Efron (1982) introduced the bias-corrected (BC) percentile interval. The justification is heuristic but there is considerable evidence that the bias-corrected method is an important improvement on the percentile interval. The construction is based on the assumption is that there is a an unknown but strictly increasing transformation <span class="math inline">\(\psi(\theta)\)</span> and unknown constant <span class="math inline">\(z_{0}\)</span> such that</p>
<p><span class="math display">\[
Z=\psi(\widehat{\theta})-\psi(\theta)+z_{0} \sim \mathrm{N}(0,1) .
\]</span></p>
<p>(The assumption that <span class="math inline">\(Z\)</span> is normal is not critical. It could be replaced by any known symmetric and invertible distribution.) Let <span class="math inline">\(\Phi(x)\)</span> denote the normal distribution function, <span class="math inline">\(\Phi^{-1}(p)\)</span> its quantile function, and <span class="math inline">\(z_{\alpha}=\Phi^{-1}(\alpha)\)</span> the normal critical values. Then the BC interval can be constructed from the bootstrap estimators <span class="math inline">\(\widehat{\theta}_{b}^{*}\)</span> and bootstrap quantiles <span class="math inline">\(q_{\alpha}^{*}\)</span> as follows. Set</p>
<p><span class="math display">\[
p^{*}=\frac{1}{B} \sum_{b=1}^{B} \mathbb{1}\left\{\widehat{\theta}_{b}^{*} \leq \widehat{\theta}\right\}
\]</span></p>
<p>and</p>
<p><span class="math display">\[
z_{0}=\Phi^{-1}\left(p^{*}\right) .
\]</span></p>
<p><span class="math inline">\(p^{*}\)</span> is a measure of median bias, and <span class="math inline">\(z_{0}\)</span> is <span class="math inline">\(p^{*}\)</span> transformed into normal units. If the bias of <span class="math inline">\(\widehat{\theta}\)</span> is zero then <span class="math inline">\(p^{*}=0.5\)</span> and <span class="math inline">\(z_{0}=0\)</span>. If <span class="math inline">\(\widehat{\theta}\)</span> is upwards biased then <span class="math inline">\(p^{*}&lt;0.5\)</span> and <span class="math inline">\(z_{0}&lt;0\)</span>. Conversely if <span class="math inline">\(\widehat{\theta}\)</span> is dowward biased then <span class="math inline">\(p^{*}&gt;0.5\)</span> and <span class="math inline">\(z_{0}&gt;0\)</span>. Define for any <span class="math inline">\(\alpha\)</span> an adjusted version</p>
<p><span class="math display">\[
x(\alpha)=\Phi\left(z_{\alpha}+2 z_{0}\right) .
\]</span></p>
<p>If <span class="math inline">\(z_{0}=0\)</span> then <span class="math inline">\(x(\alpha)=\alpha\)</span>. If <span class="math inline">\(z_{0}&gt;0\)</span> then <span class="math inline">\(x(\alpha)&gt;\alpha\)</span>, and conversely when <span class="math inline">\(x(\alpha)&lt;0\)</span>. The BC interval is</p>
<p><span class="math display">\[
C^{\mathrm{bc}}=\left[q_{x(\alpha / 2)}^{*}, q_{x(1-\alpha / 2)}^{*}\right] .
\]</span></p>
<p>Essentially, rather than going from the <span class="math inline">\(2.5 %\)</span> to <span class="math inline">\(97.5 %\)</span> quantile, the BC interval uses adjusted quantiles, with the degree of adjustment depending on the extent of the bias.</p>
<p>The construction of the BC interval is not intuitive. We now show that assumption (10.21) implies that the BC interval has exact coverage. (10.21) implies that</p>
<p><span class="math display">\[
\mathbb{P}\left[\psi(\widehat{\theta})-\psi(\theta)+z_{0} \leq x\right]=\Phi(x) .
\]</span></p>
<p>Since the distribution is pivotal the result carries over to the bootstrap distribution</p>
<p><span class="math display">\[
\mathbb{P}^{*}\left[\psi\left(\widehat{\theta}^{*}\right)-\psi(\widehat{\theta})+z_{0} \leq x\right]=\Phi(x) .
\]</span></p>
<p>Evaluating (10.26) at <span class="math inline">\(x=z_{0}\)</span> we find <span class="math inline">\(\mathbb{P}^{*}\left[\psi\left(\widehat{\theta}^{*}\right)-\psi(\widehat{\theta}) \leq 0\right]=\Phi\left(z_{0}\right)\)</span> which implies <span class="math inline">\(\mathbb{P}^{*}\left[\widehat{\theta}^{*} \leq \widehat{\theta}\right]=\Phi\left(z_{0}\right)\)</span>. Inverting, we obtain</p>
<p><span class="math display">\[
z_{0}=\Phi^{-1}\left(\mathbb{P}^{*}\left[\widehat{\theta}^{*} \leq \widehat{\theta}\right]\right)
\]</span></p>
<p>which is the probability limit of (10.23) as <span class="math inline">\(B \rightarrow \infty\)</span>. Thus the unknown <span class="math inline">\(z_{0}\)</span> is recoved by (10.23), and we can treat <span class="math inline">\(z_{0}\)</span> as if it were known.</p>
<p>From (10.26) we deduce that</p>
<p><span class="math display">\[
\begin{aligned}
x(\alpha) &amp;=\Phi\left(z_{\alpha}+2 z_{0}\right) \\
&amp;\left.=\mathbb{P}^{*}\left[\psi\left(\widehat{\theta}^{*}\right)-\psi(\widehat{\theta}) \leq z_{\alpha}+z_{0}\right)\right] \\
&amp;=\mathbb{P}^{*}\left[\widehat{\theta}^{*} \leq \psi^{-1}\left(\psi(\widehat{\theta})+z_{0}+z_{\alpha}\right)\right] .
\end{aligned}
\]</span></p>
<p>This equation shows that <span class="math inline">\(\psi^{-1}\left(\psi(\widehat{\theta})+z_{0}+z_{\alpha}\right)\)</span> equals the <span class="math inline">\(x(\alpha)^{t h}\)</span> bootstrap quantile. That is, <span class="math inline">\(q_{x(\alpha)}^{*}=\)</span> <span class="math inline">\(\psi^{-1}\left(\psi(\widehat{\theta})+z_{0}+z_{\alpha}\right)\)</span>. Hence we can write (10.25) as</p>
<p><span class="math display">\[
C^{\mathrm{bc}}=\left[\psi^{-1}\left(\psi(\widehat{\theta})+z_{0}+z_{\alpha / 2}\right), \psi^{-1}\left(\psi(\widehat{\theta})+z_{0}+z_{1-\alpha / 2}\right)\right] .
\]</span></p>
<p>It has coverage probability</p>
<p><span class="math display">\[
\begin{aligned}
\mathbb{P}\left[\theta \in C^{\mathrm{bc}}\right] &amp;=\mathbb{P}\left[\psi^{-1}\left(\psi(\widehat{\theta})+z_{0}+z_{\alpha / 2}\right) \leq \theta \leq \psi^{-1}\left(\psi(\widehat{\theta})+z_{0}+z_{1-\alpha / 2}\right)\right] \\
&amp;=\mathbb{P}\left[\psi(\widehat{\theta})+z_{0}+z_{\alpha / 2} \leq \psi(\theta) \leq \psi(\widehat{\theta})+z_{0}+z_{1-\alpha / 2}\right] \\
&amp;=\mathbb{P}\left[-z_{\alpha / 2} \geq \psi(\widehat{\theta})-\psi(\theta)+z_{0} \geq-z_{1-\alpha / 2}\right] \\
&amp;=\mathbb{P}\left[z_{1-\alpha / 2} \geq Z \geq z_{\alpha / 2}\right] \\
&amp;=\Phi\left(z_{1-\alpha / 2}\right)-\Phi\left(z_{\alpha / 2}\right) \\
&amp;=1-\alpha .
\end{aligned}
\]</span></p>
<p>The second equality applies the transformation <span class="math inline">\(\psi(\theta)\)</span>. The fourth equality uses the model (10.21) and the fact <span class="math inline">\(z_{\alpha}=-z_{1-\alpha}\)</span>. This shows that the BC interval (10.25) has exact coverage under the assumption (10.21).</p>
<p>Furthermore, under the assumptions of Theorem 10.13, the <span class="math inline">\(\mathrm{BC}\)</span> interval has asymptotic coverage probability <span class="math inline">\(1-\alpha\)</span>, since the bias correction is asymptotically negligible.</p>
<p>An important property of the BC percentile interval is that it is transformation-respecting (like the percentile interval). To see this, observe that <span class="math inline">\(p^{*}\)</span> is invariant to transformations because it is a probability, and thus <span class="math inline">\(z_{0}^{*}\)</span> and <span class="math inline">\(x(\alpha)\)</span> are invariant. Since the interval is constructed from the <span class="math inline">\(x(\alpha / 2)\)</span> and <span class="math inline">\(x(1-\alpha / 2)\)</span> quantiles, the quantile equivariance property shows that the interval is transformation-respecting.</p>
<p>The bootstrap BC percentile intervals for the four estimators are reported in Table 13.2. They are generally similar to the percentile intervals, though the intervals for <span class="math inline">\(\sigma^{2}\)</span> and <span class="math inline">\(\mu\)</span> are somewhat shifted to the right.</p>
<p>In Stata, BC percentile confidence intervals can be obtained by using the command estat bootstrap after an estimation command which calculates standard errors via the bootstrap.</p>
</section>
<section id="mathrmbc_a-percentile-interval" class="level2" data-number="11.18">
<h2 data-number="11.18" class="anchored" data-anchor-id="mathrmbc_a-percentile-interval"><span class="header-section-number">11.18</span> <span class="math inline">\(\mathrm{BC}_{a}\)</span> Percentile Interval</h2>
<p>A further improvement on the BC interval was made by Efron (1987) to account for the skewness in the sampling distribution, which can be modeled by specifying that the variance of the estimator depends on the parameter. The resulting bootstrap accelerated bias-corrected percentile interval <span class="math inline">\(\left(\mathrm{BC}_{a}\right)\)</span> has improved performance on the BC interval, but requires a bit more computation and is less intuitive to understand.</p>
<p>The construction is a generalization of that for the BC intervals. The assumption is that there is an unknown but strictly increasing transformation <span class="math inline">\(\psi(\theta)\)</span> and unknown constants <span class="math inline">\(a\)</span> and <span class="math inline">\(z_{0}\)</span> such that</p>
<p><span class="math display">\[
Z=\frac{\psi(\widehat{\theta})-\psi(\theta)}{1+a \psi(\theta)}+z_{0} \sim \mathrm{N}(0,1) .
\]</span></p>
<p>(As before, the assumption that <span class="math inline">\(Z\)</span> is normal could be replaced by any known symmetric and invertible distribution.)</p>
<p>The constant <span class="math inline">\(z_{0}\)</span> is estimated by (10.23) just as for the BC interval. There are several possible estimators of <span class="math inline">\(a\)</span>. Efron’s suggestion is a scaled jackknife estimator of the skewness of <span class="math inline">\(\widehat{\theta}\)</span> :</p>
<p><span class="math display">\[
\begin{aligned}
&amp;\widehat{a}=\frac{\sum_{i=1}^{n}\left(\bar{\theta}-\widehat{\theta}_{(-i)}\right)^{3}}{6\left(\sum_{i=1}^{n}\left(\bar{\theta}-\widehat{\theta}_{(-i)}\right)^{2}\right)^{3 / 2}} \\
&amp;\bar{\theta}=\frac{1}{n} \sum_{i=1}^{n} \widehat{\theta}_{(-i)} .
\end{aligned}
\]</span></p>
<p>The jackknife estimator of <span class="math inline">\(\widehat{a}\)</span> makes the <span class="math inline">\(\mathrm{BC}_{a}\)</span> interval more computationally costly than other intervals.</p>
<p>Define for any <span class="math inline">\(\alpha\)</span> the adjusted version</p>
<p><span class="math display">\[
x(\alpha)=\Phi\left(z_{0}+\frac{z_{\alpha}+z_{0}}{1-a\left(z_{\alpha}+z_{0}\right)}\right) .
\]</span></p>
<p>The <span class="math inline">\(\mathrm{BC}_{a}\)</span> percentile interval is</p>
<p><span class="math display">\[
C^{\mathrm{bca}}=\left[q_{x(\alpha / 2)}^{*}, q_{x(1-\alpha / 2)}^{*}\right] .
\]</span></p>
<p>Note that <span class="math inline">\(x(\alpha)\)</span> simplifies to (10.24) and <span class="math inline">\(C^{\text {bca }}\)</span> simplies to <span class="math inline">\(C^{\text {bc }}\)</span> when <span class="math inline">\(a=0\)</span>. While <span class="math inline">\(C^{\text {bc }}\)</span> improves on <span class="math inline">\(C^{\text {pc }}\)</span> by correcting the median bias, <span class="math inline">\(C^{\text {bca }}\)</span> makes a further correction for skewness.</p>
<p>The <span class="math inline">\(\mathrm{BC}_{a}\)</span> interval is only well-defined for values of <span class="math inline">\(\alpha\)</span> such that <span class="math inline">\(a\left(z_{\alpha}+z_{0}\right)&lt;1\)</span>. (Or equivalently, if <span class="math inline">\(\alpha&lt;\Phi\left(a^{-1}-z_{0}\right)\)</span> for <span class="math inline">\(a&gt;0\)</span> and <span class="math inline">\(\alpha&gt;\Phi\left(a^{-1}-z_{0}\right)\)</span> for <span class="math inline">\(a&lt;0\)</span>.)</p>
<p>The <span class="math inline">\(\mathrm{BC}_{a}\)</span> interval, like the <span class="math inline">\(\mathrm{BC}\)</span> and percentile intervals, is transformation-respecting. Thus if <span class="math inline">\(\left[q_{x(\alpha / 2)}^{*}, q_{x(1-\alpha / 2)}^{*}\right]\)</span> is the <span class="math inline">\(\mathrm{BC}_{a}\)</span> interval for <span class="math inline">\(\theta\)</span>, then <span class="math inline">\(\left[m\left(q_{x(\alpha / 2)}^{*}\right), m\left(q_{x(1-\alpha / 2)}^{*}\right)\right]\)</span> is the <span class="math inline">\(\mathrm{BC}_{\alpha}\)</span> interval for <span class="math inline">\(\phi=m(\theta)\)</span> when <span class="math inline">\(m(\theta)\)</span> is monotone.</p>
<p>We now give a justification for the <span class="math inline">\(\mathrm{BC}_{a}\)</span> interval. The most difficult feature to understand is the estimator <span class="math inline">\(\widehat{a}\)</span> for <span class="math inline">\(a\)</span>. This involves higher-order approximations which are too advanced for our treatment, so we instead refer readers to Chapter <span class="math inline">\(4.1 .4\)</span> of Shao and Tu (1995) and simply assume that <span class="math inline">\(a\)</span> is known.</p>
<p>We now show that assumption (10.28) with <span class="math inline">\(a\)</span> known implies that <span class="math inline">\(C^{\text {bca }}\)</span> has exact coverage. The argument is essentially the same as that given in the previous section. Assumption (10.28) implies that the bootstrap distribution satisfies</p>
<p><span class="math display">\[
\mathbb{P}^{*}\left[\frac{\psi\left(\widehat{\theta}^{*}\right)-\psi(\widehat{\theta})}{1+a \psi(\widehat{\theta})}+z_{0} \leq x\right]=\Phi(x) .
\]</span></p>
<p>Evaluating at <span class="math inline">\(x=z_{0}\)</span> and inverting we obtain (10.27) which is the same as for the BC interval. Thus the estimator (10.23) is consistent as <span class="math inline">\(B \rightarrow \infty\)</span> and we can treat <span class="math inline">\(z_{0}\)</span> as if it were known.</p>
<p>From (10.29) we deduce that</p>
<p><span class="math display">\[
\begin{aligned}
x(\alpha) &amp;=\mathbb{P}^{*}\left[\frac{\psi\left(\widehat{\theta}^{*}\right)-\psi(\widehat{\theta})}{1+a \psi(\widehat{\theta})} \leq \frac{z_{\alpha}+z_{0}}{1-a\left(z_{\alpha}+z_{0}\right)}\right] \\
&amp;=\mathbb{P}^{*}\left[\widehat{\theta}^{*} \leq \psi^{-1}\left(\frac{\psi(\widehat{\theta})+z_{\alpha}+z_{0}}{1-a\left(z_{\alpha}+z_{0}\right)}\right)\right] .
\end{aligned}
\]</span></p>
<p>This shows that <span class="math inline">\(\psi^{-1}\left(\frac{\psi(\widehat{\theta})+z_{\alpha}+z_{0}}{1-a\left(z_{\alpha}+z_{0}\right)}\right)\)</span> equals the <span class="math inline">\(x(\alpha)^{t h}\)</span> bootstrap quantile. Hence we can write <span class="math inline">\(C^{\text {bca }}\)</span> as</p>
<p><span class="math display">\[
C^{\mathrm{bca}}=\left[\psi^{-1}\left(\frac{\psi(\widehat{\theta})+z_{\alpha / 2}+z_{0}}{1-a\left(z_{\alpha / 2}+z_{0}\right)}\right), \quad \psi^{-1}\left(\frac{\psi(\widehat{\theta})+z_{1-\alpha / 2}+z_{0}}{1-a\left(z_{1-\alpha / 2}+z_{0}\right)}\right)\right] .
\]</span></p>
<p>It has coverage probability</p>
<p><span class="math display">\[
\begin{aligned}
\mathbb{P}\left[\theta \in C^{\mathrm{bca}}\right] &amp;=\mathbb{P}\left[\psi^{-1}\left(\frac{\psi(\widehat{\theta})+z_{\alpha / 2}+z_{0}}{1-a\left(z_{\alpha / 2}+z_{0}\right)}\right) \leq \theta \leq \psi^{-1}\left(\frac{\psi(\widehat{\theta})+z_{1-\alpha / 2}+z_{0}}{1-a\left(z_{1-\alpha / 2}+z_{0}\right)}\right)\right] \\
&amp;=\mathbb{P}\left[\frac{\psi(\widehat{\theta})+z_{\alpha / 2}+z_{0}}{1-a\left(z_{\alpha / 2}+z_{0}\right)} \leq \psi(\theta) \leq \frac{\psi(\widehat{\theta})+z_{1-\alpha / 2}+z_{0}}{1-a\left(z_{1-\alpha / 2}+z_{0}\right)}\right] \\
&amp;=\mathbb{P}\left[-z_{\alpha / 2} \geq \frac{\psi(\widehat{\theta})-\psi(\theta)}{1+a \psi(\theta)}+z_{0} \geq-z_{1-\alpha / 2}\right] \\
&amp;=\mathbb{P}\left[z_{1-\alpha / 2} \geq Z \geq z_{\alpha / 2}\right] \\
&amp;=1-\alpha .
\end{aligned}
\]</span></p>
<p>The second equality applies the transformation <span class="math inline">\(\psi(\theta)\)</span>. The fourth equality uses the model (10.28) and the fact <span class="math inline">\(z_{\alpha}=-z_{1-\alpha}\)</span>. This shows that the <span class="math inline">\(\mathrm{BC}_{a}\)</span> interval <span class="math inline">\(C^{\text {bca }}\)</span> has exact coverage under the assumption (10.28) with <span class="math inline">\(a\)</span> known.</p>
<p>The bootstrap <span class="math inline">\(\mathrm{BC}_{a}\)</span> percentile intervals for the four estimators are reported in Table 13.2. They are generally similar to the BC intervals, though the intervals for <span class="math inline">\(\sigma^{2}\)</span> and <span class="math inline">\(\mu\)</span> are slightly shifted to the right.</p>
<p>In Stata, <span class="math inline">\(\mathrm{BC}_{a}\)</span> intervals can be obtained by using the command estat bootstrap, bca or the command estat bootstrap, all after an estimation command which calculates standard errors via the bootstrap using the bca option.</p>
</section>
<section id="percentile-t-interval" class="level2" data-number="11.19">
<h2 data-number="11.19" class="anchored" data-anchor-id="percentile-t-interval"><span class="header-section-number">11.19</span> Percentile-t Interval</h2>
<p>In many cases we can obtain improvement in accuracy by bootstrapping a studentized statistic such as a t-ratio. Let <span class="math inline">\(\widehat{\theta}\)</span> be an estimator of a scalar parameter <span class="math inline">\(\theta\)</span> and <span class="math inline">\(s(\widehat{\theta})\)</span> a standard error. The sample t-ratio is</p>
<p><span class="math display">\[
T=\frac{\widehat{\theta}-\theta}{s(\widehat{\theta})} .
\]</span></p>
<p>The bootstrap t-ratio is</p>
<p><span class="math display">\[
T^{*}=\frac{\widehat{\theta}^{*}-\widehat{\theta}}{s\left(\widehat{\theta}^{*}\right)}
\]</span></p>
<p>where <span class="math inline">\(s\left(\widehat{\theta}^{*}\right)\)</span> is the standard error calculated on the bootstrap sample. Notice that the bootstrap t-ratio is centered at the parameter estimator <span class="math inline">\(\widehat{\theta}\)</span>. This is because <span class="math inline">\(\widehat{\theta}\)</span> is the “true value” in the bootstrap universe.</p>
<p>The percentile-t interval is formed using the distribution of <span class="math inline">\(T^{*}\)</span>. This can be calculated via the bootstrap algorithm. On each bootstrap sample the estimator <span class="math inline">\(\widehat{\theta}^{*}\)</span> and its standard error <span class="math inline">\(s\left(\widehat{\theta}^{*}\right)\)</span> are calculated, and the t-ratio <span class="math inline">\(T^{*}=\left(\widehat{\theta}^{*}-\widehat{\theta}\right) / s\left(\widehat{\theta}^{*}\right)\)</span> calculated and stored. This is repeated <span class="math inline">\(B\)</span> times. The <span class="math inline">\(\alpha^{t h}\)</span> quantile <span class="math inline">\(q_{\alpha}^{*}\)</span> is estimated by the <span class="math inline">\(\alpha^{t h}\)</span> empirical quantile (or any quantile estimator) from the <span class="math inline">\(B\)</span> bootstrap draws of <span class="math inline">\(T^{*}\)</span>.</p>
<p>The bootstrap percentile-t confidence interval is defined as</p>
<p><span class="math display">\[
C^{\mathrm{pt}}=\left[\widehat{\theta}-s(\widehat{\theta}) q_{1-\alpha / 2}^{*}, \widehat{\theta}-s(\widehat{\theta}) q_{\alpha / 2}^{*}\right] .
\]</span></p>
<p>The form may appear unusual when compared with the percentile interval. The left endpoint is determined by the upper quantile of the distribution of <span class="math inline">\(T^{*}\)</span>, and the right endpoint is determined by the lower quantile. As we show below, this construction is important for the interval to have correct coverage when the distribution is not symmetric.</p>
<p>When the estimator is asymptotically normal and the standard error a reliable estimator of the standard deviation of the distribution we would expect the t-ratio <span class="math inline">\(T\)</span> to be roughly approximated by the normal distribution. In this case we would expect <span class="math inline">\(q_{0.975}^{*} \approx-q_{0.025}^{*} \approx 2\)</span>. Departures from this baseline occur as the distribution becomes skewed or fat-tailed. If the bootstrap quantiles depart substantially from this baseline it is evidence of substantial departure from normality. (It may also indicate a programming error, so in these cases it is wise to triple-check!)</p>
<p>The percentile-t has the following advantages. First, when the standard error <span class="math inline">\(s(\widehat{\theta})\)</span> is reasonably reliable, the percentile-t bootstrap makes use of the information in the standard error, thereby reducing the role of the bootstrap. This can improve the precision of the method relative to other methods. Second, as we show later, the percentile-t intervals achieve higher-order accuracy than the percentile and BC percentile intervals. Third, the percentile-t intervals correspond to the set of parameter values “not rejected” by one-sided t-tests using bootstrap critical values (bootstrap tests are presented in Section 10.21).</p>
<p>The percentile-t interval has the following disadvantages. First, they may be infeasible when standard error formula are unknown. Second, they may be practically infeasible when standard error calculations are computationally costly (since the standard error calculation needs to be performed on each bootstrap sample). Third, the percentile-t may be unreliable if the standard errors <span class="math inline">\(s(\widehat{\theta})\)</span> are unreliable and thus add more noise than clarity. Fourth, the percentile-t interval is not translation preserving, unlike the percentile, <span class="math inline">\(\mathrm{BC}\)</span> percentile, and <span class="math inline">\(\mathrm{BC}_{a}\)</span> percentile intervals.</p>
<p>It is typical to calculate percentile-t intervals with t-ratios constructed with conventional asymptotic standard errors. But this is not the only possible implementation. The percentile-t interval can be constructed with any data-dependent measure of scale. For example, if <span class="math inline">\(\widehat{\theta}\)</span> is a two-step estimator for which it is unclear how to construct a correct asymptotic standard error, but we know how to calculate a standard error <span class="math inline">\(s(\widehat{\theta})\)</span> appropriate for the second step alone, then <span class="math inline">\(s(\widehat{\theta})\)</span> can be used for a percentile-t-type interval as described above. It will not possess the higher-order accuracy properties of the following section, but it will satisfy the conditions for first-order validity.</p>
<p>Furthermore, percentile-t intervals can be constructed using bootstrap standard errors. That is, the statistics <span class="math inline">\(T\)</span> and <span class="math inline">\(T^{*}\)</span> can be computed using bootstrap standard errors <span class="math inline">\(s_{\widehat{\theta}}^{\text {boot }}\)</span>. This is computationally costly as it requires what we call a “nested bootstrap”. Specifically, for each bootstrap replication, a random sample is drawn, the bootstrap estimate <span class="math inline">\(\widehat{\theta}^{*}\)</span> computed, and then <span class="math inline">\(B\)</span> additional bootstrap sub-samples drawn from the bootstrap sample to compute the bootstrap standard error for the bootstrap estimate <span class="math inline">\(\widehat{\theta}^{*}\)</span>. Effectively <span class="math inline">\(B^{2}\)</span> bootstrap samples are drawn and estimated, which increases the computational requirement by an order of magnitude.</p>
<p>We now describe the distribution theory for first-order validity of the percentile-t bootstrap.</p>
<p>First, consider the smooth function model, where <span class="math inline">\(\widehat{\theta}=g(\widehat{\mu})\)</span> and <span class="math inline">\(s(\widehat{\theta})=\sqrt{\frac{1}{n} \widehat{\boldsymbol{G}}^{\prime} \widehat{\boldsymbol{V}} \widehat{\boldsymbol{G}}}\)</span> with bootstrap analogs <span class="math inline">\(\widehat{\theta}^{*}=g\left(\widehat{\mu}^{*}\right)\)</span> and <span class="math inline">\(s\left(\widehat{\theta}^{*}\right)=\sqrt{\frac{1}{n} \widehat{\boldsymbol{G}}^{* \prime} \widehat{\boldsymbol{V}}^{*} \widehat{\boldsymbol{G}}^{*}}\)</span>. From Theorems <span class="math inline">\(6.10,10.7\)</span>, and <span class="math inline">\(10.8\)</span></p>
<p><span class="math display">\[
T=\frac{\sqrt{n}(\widehat{\theta}-\theta)}{\sqrt{\widehat{\boldsymbol{G}}^{\prime} \widehat{\boldsymbol{V}} \widehat{\boldsymbol{G}}}} \underset{d}{\longrightarrow}
\]</span></p>
<p>and</p>
<p><span class="math display">\[
T^{*}=\frac{\sqrt{n}\left(\widehat{\theta}^{*}-\widehat{\theta}\right)}{\sqrt{\widehat{\boldsymbol{G}}^{* \prime} \widehat{\boldsymbol{V}}^{*} \widehat{\boldsymbol{G}}^{*}}} \overrightarrow{d^{*}} Z
\]</span></p>
<p>where <span class="math inline">\(Z \sim \mathrm{N}(0,1)\)</span>. This shows that the sample and bootstrap t-ratios have the same asymptotic distribution.</p>
<p>This motivates considering the broader situation where the sample and bootstrap t-ratios have the same asymptotic distribution but not necessarily normal. Thus assume that</p>
<p><span class="math display">\[
\begin{gathered}
T \underset{d}{\longrightarrow} \xi \\
T^{*} \underset{d^{*}}{\longrightarrow} \xi
\end{gathered}
\]</span></p>
<p>for some continuous distribution <span class="math inline">\(\xi\)</span>. (10.31) implies that the quantiles of <span class="math inline">\(T^{*}\)</span> converge in probability to those of <span class="math inline">\(\xi\)</span>, that is <span class="math inline">\(q_{\alpha}^{*} \longrightarrow \underset{p}{\longrightarrow} q_{\alpha}\)</span> where <span class="math inline">\(q_{\alpha}\)</span> is the <span class="math inline">\(\alpha^{t h}\)</span> quantile of <span class="math inline">\(\xi\)</span>. This and (10.30) imply</p>
<p><span class="math display">\[
\begin{aligned}
\mathbb{P}\left[\theta \in C^{\mathrm{pt}}\right] &amp;=\mathbb{P}\left[\widehat{\theta}-s(\widehat{\theta}) q_{1-\alpha / 2}^{*} \leq \theta \leq \hat{\theta}-s(\widehat{\theta}) q_{\alpha / 2}^{*}\right] \\
&amp;=\mathbb{P}\left[q_{\alpha / 2}^{*} \leq T \leq q_{1-\alpha / 2}^{*}\right] \\
&amp; \rightarrow \mathbb{P}\left[q_{\alpha / 2} \leq \xi \leq q_{1-\alpha / 2}\right] \\
&amp;=1-\alpha .
\end{aligned}
\]</span></p>
<p>Thus the percentile-t is asymptotically valid. Theorem <span class="math inline">\(10.14\)</span> If (10.30) and (10.31) hold where <span class="math inline">\(\xi\)</span> is continuously distributed, then <span class="math inline">\(\mathbb{P}\left[\theta \in C^{\mathrm{pt}}\right] \rightarrow 1-\alpha\)</span> as <span class="math inline">\(n \rightarrow \infty\)</span></p>
<p>The bootstrap percentile-t intervals for the four estimators are reported in Table 13.2. They are similar but somewhat different from the percentile-type intervals, and generally wider. The largest difference arises with the interval for <span class="math inline">\(\sigma^{2}\)</span> which is noticably wider than the other intervals.</p>
</section>
<section id="percentile-t-asymptotic-refinement" class="level2" data-number="11.20">
<h2 data-number="11.20" class="anchored" data-anchor-id="percentile-t-asymptotic-refinement"><span class="header-section-number">11.20</span> Percentile-t Asymptotic Refinement</h2>
<p>This section uses the theory of Edgeworth and Cornish-Fisher expansions introduced in Chapter 9.8-9.10 of Probability and Statistics for Economists. This theory will not be familiar to most students. If you are interested in the following refinement theory it is advisable to start by reading these sections of Probability and Statistics for Economists.</p>
<p>The percentile-t interval can be viewed as the intersection of two one-sided confidence intervals. In our discussion of Edgeworth expansions for the coverage probability of one-sided asymptotic confidence intervals (following Theorem <span class="math inline">\(7.15\)</span> in the context of functions of regression coefficients) we found that one-sided asymptotic confidence intervals have accuracy to order <span class="math inline">\(O\left(n^{-1 / 2}\right)\)</span>. We now show that the percentile-t interval has improved accuracy.</p>
<p>Theorem <span class="math inline">\(9.13\)</span> of Probability and Statistics for Economists showed that the Cornish-Fisher expansion for the quantile <span class="math inline">\(q_{\alpha}\)</span> of a t-ratio <span class="math inline">\(T\)</span> in the smooth function model takes the form</p>
<p><span class="math display">\[
q_{\alpha}=z_{\alpha}+n^{-1 / 2} p_{11}\left(z_{\alpha}\right)+O\left(n^{-1}\right)
\]</span></p>
<p>where <span class="math inline">\(p_{11}(x)\)</span> is an even polynomial of order 2 with coefficients depending on the moments up to order 8. The bootstrap quantile <span class="math inline">\(q_{\alpha}^{*}\)</span> has a similar Cornish-Fisher expansion</p>
<p><span class="math display">\[
q_{\alpha}^{*}=z_{\alpha}+n^{-1 / 2} p_{11}^{*}\left(z_{\alpha}\right)+O_{p}\left(n^{-1}\right)
\]</span></p>
<p>where <span class="math inline">\(p_{11}^{*}(x)\)</span> is the same as <span class="math inline">\(p_{11}(x)\)</span> except that the population moments are replaced by the corresponding sample moments. Sample moments are estimated at the rate <span class="math inline">\(n^{-1 / 2}\)</span>. Thus we can replace <span class="math inline">\(p_{11}^{*}\)</span> with <span class="math inline">\(p_{11}\)</span> without affecting the order of this expansion:</p>
<p><span class="math display">\[
q_{\alpha}^{*}=z_{\alpha}+n^{-1 / 2} p_{11}\left(z_{\alpha}\right)+O_{p}\left(n^{-1}\right)=q_{\alpha}+O_{p}\left(n^{-1}\right) .
\]</span></p>
<p>This shows that the bootstrap quantiles <span class="math inline">\(q_{\alpha}^{*}\)</span> of the studentized t-ratio are within <span class="math inline">\(O_{p}\left(n^{-1}\right)\)</span> of the exact quantiles <span class="math inline">\(q_{\alpha}\)</span>.</p>
<p>By the Edgeworth expansion Delta method (Theorem <span class="math inline">\(9.12\)</span> of Probability and Statistics for Economists), <span class="math inline">\(T\)</span> and <span class="math inline">\(T+\left(q_{\alpha}-q_{\alpha}^{*}\right)=T+O_{p}\left(n^{-1}\right)\)</span> have the same Edgeworth expansion to order <span class="math inline">\(O\left(n^{-1}\right)\)</span>. Thus</p>
<p><span class="math display">\[
\begin{aligned}
\mathbb{P}\left[T \leq q_{\alpha}^{*}\right] &amp;=\mathbb{P}\left[T+\left(q_{\alpha}-q_{\alpha}^{*}\right) \leq q_{\alpha}\right] \\
&amp;=\mathbb{P}\left[T \leq q_{\alpha}\right]+O\left(n^{-1}\right) \\
&amp;=\alpha+O\left(n^{-1}\right) .
\end{aligned}
\]</span></p>
<p>Thus the coverage of the percentile-t interval is</p>
<p><span class="math display">\[
\begin{aligned}
\mathbb{P}\left[\theta \in C^{\mathrm{pt}}\right] &amp;=\mathbb{P}\left[q_{\alpha / 2}^{*} \leq T \leq q_{1-\alpha / 2}^{*}\right] \\
&amp;=\mathbb{P}\left[q_{\alpha / 2} \leq T \leq q_{1-\alpha / 2}\right]+O\left(n^{-1}\right) \\
&amp;=1-\alpha+O\left(n^{-1}\right) .
\end{aligned}
\]</span></p>
<p>This is an improved rate of convergence relative to the one-sided asymptotic confidence interval. Theorem <span class="math inline">\(10.15\)</span> Under the assumptions of Theorem <span class="math inline">\(9.11\)</span> of Probability and Statistics for Economists, <span class="math inline">\(\mathbb{P}\left[\theta \in C^{\mathrm{pt}}\right]=1-\alpha+O\left(n^{-1}\right)\)</span>.</p>
<p>The following definition of the accuracy of a confidence interval is useful.</p>
<p>Definition 10.3 A confidence set <span class="math inline">\(C\)</span> for <span class="math inline">\(\theta\)</span> is <span class="math inline">\(k^{t h}\)</span>-order accurate if</p>
<p><span class="math display">\[
\mathbb{P}[\theta \in C]=1-\alpha+O\left(n^{-k / 2}\right) .
\]</span></p>
<p>Examining our results we find that one-sided asymptotic confidence intervals are first-order accurate but percentile-t intervals are second-order accurate. When a bootstrap confidence interval (or test) achieves high-order accuracy than the analogous asymptotic interval (or test), we say that the bootstrap method achieves an asymptotic refinement. Here, we have shown that the percentile-t interval achieves an asymptotic refinement.</p>
<p>In order to achieve this asymptotic refinement it is important that the t-ratio <span class="math inline">\(T\)</span> (and its bootstrap counter-part <span class="math inline">\(T^{*}\)</span> ) are constructed with asymptotically valid standard errors. This is because the first term in the Edgeworth expansion is the standard normal distribution and this requires that the t-ratio is asymptotically normal. This also has the practical finite-sample implication that the accuracy of the percentile-t interval in practice depends on the accuracy of the standard errors used to construct the t-ratio.</p>
<p>We do not go through the details, but normal-approximation bootstrap intervals, percentile bootstrap intervals, and bias-corrected percentile bootstrap intervals are all first-order accurate and do not achieve an asymptotic refinement.</p>
<p>The <span class="math inline">\(\mathrm{BC}_{a}\)</span> interval, however, can be shown to be asymptotically equivalent to the percentile-t interval, and thus achieves an asymptotic refinement. We do not make this demonstration here as it is advanced. See Section 3.10.4 of Hall (1992).</p>
<table class="table">
<colgroup>
<col style="width: 100%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Peter Hall</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Peter Gavin Hall (1951-2016) of Australia was one of the most influential and</td>
</tr>
<tr class="even">
<td style="text-align: left;">prolific theoretical statisticians in history. He made wide-ranging contributions.</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Some of the most relevant for econometrics are theoretical investigations of</td>
</tr>
<tr class="even">
<td style="text-align: left;">bootstrap methods and nonparametric kernel methods.</td>
</tr>
</tbody>
</table>
</section>
<section id="bootstrap-hypothesis-tests" class="level2" data-number="11.21">
<h2 data-number="11.21" class="anchored" data-anchor-id="bootstrap-hypothesis-tests"><span class="header-section-number">11.21</span> Bootstrap Hypothesis Tests</h2>
<p>To test the hypothesis <span class="math inline">\(\mathbb{M}_{0}: \theta=\theta_{0}\)</span> against <span class="math inline">\(\mathbb{M}_{1}: \theta \neq \theta_{0}\)</span> the most common approach is a t-test. We reject <span class="math inline">\(\mathbb{H}_{0}\)</span> in favor of <span class="math inline">\(\mathbb{H}_{1}\)</span> for large absolute values of the t-statistic <span class="math inline">\(T=\left(\widehat{\theta}-\theta_{0}\right) / s(\hat{\theta})\)</span> where <span class="math inline">\(\widehat{\theta}\)</span> is an estimator of <span class="math inline">\(\theta\)</span> and <span class="math inline">\(s(\widehat{\theta})\)</span> is a standard error for <span class="math inline">\(\widehat{\theta}\)</span>. For a bootstrap test we use the bootstrap algorithm to calculate the critical value. The bootstrap algorithm samples with replacement from the dataset. Given a bootstrap sample the bootstrap estimator <span class="math inline">\(\widehat{\theta}^{*}\)</span> and standard error <span class="math inline">\(s\left(\widehat{\theta}^{*}\right)\)</span> are calculated. Given these values the bootstrap <span class="math inline">\(\mathrm{t}\)</span>-statistic is <span class="math inline">\(T^{*}=\left(\widehat{\theta}^{*}-\widehat{\theta}\right) / s\left(\widehat{\theta}^{*}\right)\)</span>. There are two important features about the bootstrap t-statistic. First, <span class="math inline">\(T^{*}\)</span> is centered at the sample estimate <span class="math inline">\(\widehat{\theta}\)</span>, not at the hypothesized value <span class="math inline">\(\theta_{0}\)</span>. This is done because <span class="math inline">\(\widehat{\theta}\)</span> is the true value in the bootstrap universe, and the distribution of the t-statistic must be centered at the true value within the bootstrap sampling framework. Second, <span class="math inline">\(T^{*}\)</span> is calculated using the bootstrap standard error <span class="math inline">\(s\left(\widehat{\theta}^{*}\right)\)</span>. This allows the bootstrap to incorporate the randomness in standard error estimation.</p>
<p>The failure to properly center the bootstrap statistic at <span class="math inline">\(\widehat{\theta}\)</span> is a common error in applications. Often this is because the hypothesis to be tested is <span class="math inline">\(\mathbb{H}_{0}: \theta=0\)</span>, so the test statistic is <span class="math inline">\(T=\widehat{\theta} / s(\widehat{\theta})\)</span>. This intuitively suggests the bootstrap statistic <span class="math inline">\(T^{*}=\widehat{\theta}^{*} / s\left(\widehat{\theta}^{*}\right)\)</span>, but this is wrong. The correct bootstrap statistic is <span class="math inline">\(T^{*}=\)</span> <span class="math inline">\(\left(\widehat{\theta}^{*}-\widehat{\theta}\right) / s\left(\widehat{\theta}^{*}\right)\)</span></p>
<p>The bootstrap algorithm creates <span class="math inline">\(B\)</span> draws <span class="math inline">\(T^{*}(b)=\left(\widehat{\theta}^{*}(b)-\widehat{\theta}\right) / s\left(\widehat{\theta}^{*}(b)\right), b=1, \ldots, B\)</span>. The bootstrap <span class="math inline">\(100 \alpha %\)</span> critical value is <span class="math inline">\(q_{1-\alpha}^{*}\)</span>, where <span class="math inline">\(q_{\alpha}^{*}\)</span> is the <span class="math inline">\(\alpha^{\text {th }}\)</span> quantile of the absolute values of the bootstrap t-ratios <span class="math inline">\(\left|T^{*}(b)\right|\)</span>. For a <span class="math inline">\(100 \alpha %\)</span> test we reject <span class="math inline">\(\mathbb{M}_{0}: \theta=\theta_{0}\)</span> in favor of <span class="math inline">\(\mathbb{H}_{1}: \theta \neq \theta_{0}\)</span> if <span class="math inline">\(|T|&gt;q_{1-\alpha}^{*}\)</span> and fail to reject if <span class="math inline">\(|T| \leq q_{1-\alpha}^{*}\)</span>.</p>
<p>It is generally better to report p-values rather than critical values. Recall that a p-value is <span class="math inline">\(p=1-\)</span> <span class="math inline">\(G_{n}(|T|)\)</span> where <span class="math inline">\(G_{n}(u)\)</span> is the null distribution of the statistic <span class="math inline">\(|T|\)</span>. The bootstrap p-value is defined as <span class="math inline">\(p^{*}=1-G_{n}^{*}(|T|)\)</span>, where <span class="math inline">\(G_{n}^{*}(u)\)</span> is the bootstrap distribution of <span class="math inline">\(\left|T^{*}\right|\)</span>. This is estimated from the bootstrap algorithm as</p>
<p><span class="math display">\[
p^{*}=\frac{1}{B} \sum_{b=1}^{B} \mathbb{1}\left\{\left|T^{*}(b)\right|&gt;|T|\right\},
\]</span></p>
<p>the percentage of bootstrap t-statistics that are larger than the observed t-statistic. Intuitively, we want to know how “unusual” is the observed statistic <span class="math inline">\(T\)</span> when the null hypothesis is true. The bootstrap algorithm generates a large number of independent draws from the distribution <span class="math inline">\(T^{*}\)</span> (which is an approximation to the unknown distribution of <span class="math inline">\(T\)</span> ). If the percentage of the <span class="math inline">\(\left|T^{*}\right|\)</span> that exceed <span class="math inline">\(|T|\)</span> is very small (say <span class="math inline">\(1 %\)</span> ) this tells us that <span class="math inline">\(|T|\)</span> is an unusually large value. However, if the percentage is larger, say <span class="math inline">\(15 %\)</span>, then we cannot interpret <span class="math inline">\(|T|\)</span> as unusually large.</p>
<p>If desired, the bootstrap test can be implemented as a one-sided test. In this case the statistic is the signed version of the t-ratio, and bootstrap critical values are calculated from the upper tail of the distribution for the alternative <span class="math inline">\(\mathbb{H}_{1}: \theta&gt;\theta_{0}\)</span>, and from the lower tail for the alternative <span class="math inline">\(\mathbb{H}_{1}: \theta&lt;\theta_{0}\)</span>. There is a connection between the one-sided tests and the percentile-t confidence interval. The latter is the set of parameter values <span class="math inline">\(\theta\)</span> which are not rejected by either one-sided <span class="math inline">\(100 \alpha / 2 %\)</span> bootstrap t-test.</p>
<p>Bootstrap tests can also be conducted with other statistics. When standard errors are not available or are not reliable we can use the non-studentized statistic <span class="math inline">\(T=\widehat{\theta}-\theta_{0}\)</span>. The bootstrap version is <span class="math inline">\(T^{*}=\widehat{\theta}^{*}-\widehat{\theta}\)</span>. Let <span class="math inline">\(q_{\alpha}^{*}\)</span> be the <span class="math inline">\(\alpha^{\text {th }}\)</span> quantile of the bootstrap statistics <span class="math inline">\(\left|\widehat{\theta}^{*}(b)-\widehat{\theta}\right|\)</span>. A bootstrap <span class="math inline">\(100 \alpha %\)</span> test rejects <span class="math inline">\(\mathbb{H}_{0}: \theta=\theta_{0}\)</span> if <span class="math inline">\(\left|\widehat{\theta}-\theta_{0}\right|&gt;q_{1-\alpha}^{*}\)</span>. The bootstrap p-value is</p>
<p><span class="math display">\[
p^{*}=\frac{1}{B} \sum_{b=1}^{B} \mathbb{1}\left\{\left|\widehat{\theta}^{*}(b)-\widehat{\theta}\right|&gt;\left|\widehat{\theta}-\theta_{0}\right|\right\} .
\]</span></p>
<p>Theorem <span class="math inline">\(10.16\)</span> If (10.30) and (10.31) hold where <span class="math inline">\(\xi\)</span> is continuously distributed, then the bootstrap critical value satisfies <span class="math inline">\(q_{1-\alpha}^{*} \underset{p}{\longrightarrow} q_{1-\alpha}\)</span> where <span class="math inline">\(q_{1-\alpha}\)</span> is the <span class="math inline">\(1-\alpha^{t h}\)</span> quantile of <span class="math inline">\(|\xi|\)</span>. The bootstrap test “Reject <span class="math inline">\(\mathbb{M}_{0}\)</span> in favor of <span class="math inline">\(\mathbb{M}_{1}\)</span> if <span class="math inline">\(|T|&gt;q_{1-\alpha}^{*}\)</span>” has asymptotic size <span class="math inline">\(\alpha: \mathbb{P}\left[|T|&gt;q_{1-\alpha}^{*} \mid \mathbb{H}_{0}\right] \longrightarrow \alpha\)</span> as <span class="math inline">\(n \rightarrow \infty\)</span>. In the smooth function model the t-test (with correct standard errors) has the following performance.</p>
<p>Theorem <span class="math inline">\(10.17\)</span> Under the assumptions of Theorem <span class="math inline">\(9.11\)</span> of Probability and Statistics for Economists,</p>
<p><span class="math display">\[
q_{1-\alpha}^{*}=\bar{z}_{1-\alpha}+o_{p}\left(n^{-1}\right)
\]</span></p>
<p>where <span class="math inline">\(\bar{z}_{\alpha}=\Phi^{-1}((1+\alpha) / 2)\)</span> is the <span class="math inline">\(\alpha^{t h}\)</span> quantile of <span class="math inline">\(|Z|\)</span>. The asymptotic test “Reject <span class="math inline">\(\mathbb{M}_{0}\)</span> in favor of <span class="math inline">\(\mathbb{M}_{1}\)</span> if <span class="math inline">\(|T|&gt;\bar{z}_{1-\alpha}\)</span>” has accuracy</p>
<p><span class="math display">\[
\mathbb{P}\left[|T|&gt;\bar{z}_{1-\alpha} \mid \mathbb{H}_{0}\right]=1-\alpha+O\left(n^{-1}\right)
\]</span></p>
<p>and the bootstrap test “Reject <span class="math inline">\(\mathbb{M}_{0}\)</span> in favor of <span class="math inline">\(\mathbb{M}_{1}\)</span> if <span class="math inline">\(|T|&gt;q_{1-\alpha}^{*}\)</span>” has accuracy</p>
<p><span class="math display">\[
\mathbb{P}\left[|T|&gt;q_{1-\alpha}^{*} \mid \mathbb{M}_{0}\right]=1-\alpha+o\left(n^{-1}\right) .
\]</span></p>
<p>This shows that the bootstrap test achieves a refinement relative to the asymptotic test.</p>
<p>The reasoning is as follows. We have shown that the Edgeworth expansion for the absolute t-ratio takes the form</p>
<p><span class="math display">\[
\mathbb{P}[|T| \leq x]=2 \Phi(x)-1+n^{-1} 2 p_{2}(x)+o\left(n^{-1}\right) .
\]</span></p>
<p>This means the asymptotic test has accuracy of order <span class="math inline">\(O\left(n^{-1}\right)\)</span>.</p>
<p>Given the Edgeworth expansion, the Cornish-Fisher expansion for the <span class="math inline">\(\alpha^{t h}\)</span> quantile <span class="math inline">\(q_{\alpha}\)</span> of the distribution of <span class="math inline">\(|T|\)</span> takes the form</p>
<p><span class="math display">\[
q_{\alpha}=\bar{z}_{\alpha}+n^{-1} p_{21}\left(\bar{z}_{\alpha}\right)+o\left(n^{-1}\right) .
\]</span></p>
<p>The bootstrap quantile <span class="math inline">\(q_{\alpha}^{*}\)</span> has the Cornish-Fisher expansion</p>
<p><span class="math display">\[
\begin{aligned}
q_{\alpha}^{*} &amp;=\bar{z}_{\alpha}+n^{-1} p_{21}^{*}\left(\bar{z}_{\alpha}\right)+o\left(n^{-1}\right) \\
&amp;=\bar{z}_{\alpha}+n^{-1} p_{21}\left(\bar{z}_{\alpha}\right)+o_{p}\left(n^{-1}\right) \\
&amp;=q_{\alpha}+o_{p}\left(n^{-1}\right)
\end{aligned}
\]</span></p>
<p>where <span class="math inline">\(p_{21}^{*}(x)\)</span> is the same as <span class="math inline">\(p_{21}(x)\)</span> except that the population moments are replaced by the corresponding sample moments. The bootstrap test has rejection probability, using the Edgeworth expansion Delta method (Theorem <span class="math inline">\(11.12\)</span> of of Probability and Statistics for Economists)</p>
<p><span class="math display">\[
\begin{aligned}
\mathbb{P}\left[|T|&gt;q_{1-\alpha}^{*} \mid \mathbb{B}_{0}\right] &amp;=\mathbb{P}\left[|T|+\left(q_{1-\alpha}-q_{1-\alpha}^{*}\right)&gt;q_{1-\alpha}\right] \\
&amp;=\mathbb{P}\left[|T|&gt;q_{1-\alpha}\right]+o\left(n^{-1}\right) \\
&amp;=1-\alpha+o\left(n^{-1}\right)
\end{aligned}
\]</span></p>
<p>as claimed.</p>
</section>
<section id="wald-type-bootstrap-tests" class="level2" data-number="11.22">
<h2 data-number="11.22" class="anchored" data-anchor-id="wald-type-bootstrap-tests"><span class="header-section-number">11.22</span> Wald-Type Bootstrap Tests</h2>
<p>If <span class="math inline">\(\theta\)</span> is a vector then to test <span class="math inline">\(\mathbb{M}_{0}: \theta=\theta_{0}\)</span> against <span class="math inline">\(\mathbb{M}_{1}: \theta \neq \theta_{0}\)</span> at size <span class="math inline">\(\alpha\)</span>, a common test is based on the Wald statistic <span class="math inline">\(W=\left(\widehat{\theta}-\theta_{0}\right)^{\prime} \widehat{\boldsymbol{V}}_{\widehat{\theta}}^{-1}\left(\widehat{\theta}-\theta_{0}\right)\)</span> where <span class="math inline">\(\widehat{\theta}\)</span> is an estimator of <span class="math inline">\(\theta\)</span> and <span class="math inline">\(\widehat{\boldsymbol{V}}_{\widehat{\theta}}\)</span> is a covariance matrix estimator. For a bootstrap test we use the bootstrap algorithm to calculate the critical value. The bootstrap algorithm samples with replacement from the dataset. Given a bootstrap sample the bootstrap estimator <span class="math inline">\(\widehat{\theta}^{*}\)</span> and covariance matrix estimator <span class="math inline">\(\widehat{\boldsymbol{V}}_{\widehat{\theta}}^{*}\)</span> are calculated. Given these values the bootstrap Wald statistic is</p>
<p><span class="math display">\[
W^{*}=\left(\widehat{\theta}^{*}-\widehat{\theta}\right)^{\prime} \widehat{\boldsymbol{V}}_{\widehat{\theta}}^{*-1}\left(\widehat{\theta}^{*}-\widehat{\theta}\right) .
\]</span></p>
<p>As for the t-test it is essential that the bootstrap Wald statistic <span class="math inline">\(W^{*}\)</span> is centered at the sample estimator <span class="math inline">\(\widehat{\theta}\)</span> instead of the hypothesized value <span class="math inline">\(\theta_{0}\)</span>. This is because <span class="math inline">\(\widehat{\theta}\)</span> is the true value in the bootstrap universe.</p>
<p>Based on <span class="math inline">\(B\)</span> bootstrap replications we calculate the <span class="math inline">\(\alpha^{t h}\)</span> quantile <span class="math inline">\(q_{\alpha}^{*}\)</span> of the distribution of the bootstrap Wald statistics <span class="math inline">\(W^{*}\)</span>. The bootstrap test rejects <span class="math inline">\(\mathbb{M}_{0}\)</span> in favor of <span class="math inline">\(\mathbb{H}_{1}\)</span> if <span class="math inline">\(W&gt;q_{1-\alpha}^{*}\)</span>. More commonly, we calculate a bootstrap p-value. This is</p>
<p><span class="math display">\[
p^{*}=\frac{1}{B} \sum_{b=1}^{B} \mathbb{1}\left\{W^{*}(b)&gt;W\right\} .
\]</span></p>
<p>The asymptotic performance of the Wald test mimics that of the t-test. In general, the bootstrap Wald test is first-order correct (achieves the correct size asymptotically) and under conditions for which an Edgeworth expansion exists, has accuracy</p>
<p><span class="math display">\[
\mathbb{P}\left[W&gt;q_{1-\alpha}^{*} \mid \mathbb{H}_{0}\right]=1-\alpha+o\left(n^{-1}\right)
\]</span></p>
<p>and thus achieves a refinement relative to the asymptotic Wald test.</p>
<p>If a reliable covariance matrix estimator <span class="math inline">\(\widehat{\boldsymbol{V}}_{\widehat{\theta}}\)</span> is not available a Wald-type test can be implemented with any positive-definite weight matrix instead of <span class="math inline">\(\widehat{\boldsymbol{V}}_{\widehat{\theta}}\)</span>. This includes simple choices such as the identity matrix. The bootstrap algorithm can be used to calculate critical values and <span class="math inline">\(\mathrm{p}\)</span>-values for the test. So long as the estimator <span class="math inline">\(\hat{\theta}\)</span> has an asymptotic distribution this bootstrap test will be asymptotically firstorder valid. The test will not achieve an asymptotic refinement but provides a simple method to test hypotheses when covariance matrix estimates are not available.</p>
</section>
<section id="criterion-based-bootstrap-tests" class="level2" data-number="11.23">
<h2 data-number="11.23" class="anchored" data-anchor-id="criterion-based-bootstrap-tests"><span class="header-section-number">11.23</span> Criterion-Based Bootstrap Tests</h2>
<p>A criterion-based estimator takes the form</p>
<p><span class="math display">\[
\widehat{\beta}=\underset{\beta}{\operatorname{argmin}} J(\beta)
\]</span></p>
<p>for some criterion function <span class="math inline">\(J(\beta)\)</span>. This includes least squares, maximum likelihood, and minimum distance. Given a hypothesis <span class="math inline">\(\mathbb{M}_{0}: \theta=\theta_{0}\)</span> where <span class="math inline">\(\theta=r(\beta)\)</span>, the restricted estimator which satisfies <span class="math inline">\(\mathbb{H}_{0}\)</span> is</p>
<p><span class="math display">\[
\widetilde{\beta}=\underset{r(\beta)=\theta_{0}}{\operatorname{argmin}} J(\beta) .
\]</span></p>
<p>A criterion-based statistic to test <span class="math inline">\(\mathbb{H}_{0}\)</span> is</p>
<p><span class="math display">\[
J=\min _{r(\beta)=\theta_{0}} J(\beta)-\min _{\beta} J(\beta)=J(\widetilde{\beta})-J(\widehat{\beta}) .
\]</span></p>
<p>A criterion-based test rejects <span class="math inline">\(\mathbb{H}_{0}\)</span> for large values of <span class="math inline">\(J\)</span>. A bootstrap test uses the bootstrap algorithm to calculate the critical value.</p>
<p>In this context we need to be a bit thoughtful about how to construct bootstrap versions of <span class="math inline">\(J\)</span>. It might seem natural to construct the exact same statistic on the bootstrap samples as on the original sample, but this is incorrect. It makes the same error as calculating a t-ratio or Wald statistic centered at the hypothesized value. In the bootstrap universe, the true value of <span class="math inline">\(\theta\)</span> is not <span class="math inline">\(\theta_{0}\)</span>, rather it is <span class="math inline">\(\widehat{\theta}=r(\widehat{\beta})\)</span>. Thus when using the nonparametric bootstrap, we want to impose the constraint <span class="math inline">\(r(\beta)=r(\widehat{\beta})=\widehat{\theta}\)</span> to obtain the bootstrap version of <span class="math inline">\(J\)</span>.</p>
<p>Thus, the correct way to calculate a bootstrap version of <span class="math inline">\(J\)</span> is as follows. Generate a bootstrap sample by random sampling from the dataset. Let <span class="math inline">\(J^{*}(\beta)\)</span> be the the bootstrap version of the criterion. On a bootstrap sample calculate the unrestricted estimator <span class="math inline">\(\widehat{\beta}^{*}=\underset{\beta}{\operatorname{argmin}} J^{*}(\beta)\)</span> and the restricted version <span class="math inline">\(\widetilde{\beta}^{*}=\)</span> <span class="math inline">\(\operatorname{argmin} J^{*}(\beta)\)</span> where <span class="math inline">\(\widehat{\theta}=r(\widehat{\beta})\)</span>. The bootstrap statistic is <span class="math inline">\(r(\beta)=\hat{\theta}\)</span></p>
<p><span class="math display">\[
J^{*}=\min _{r(\beta)=\widehat{\theta}} J^{*}(\beta)-\min _{\beta} J^{*}(\beta)=J^{*}\left(\widetilde{\beta}^{*}\right)-J^{*}\left(\widehat{\beta}^{*}\right) .
\]</span></p>
<p>Calculate <span class="math inline">\(J^{*}\)</span> on each bootstrap sample. Take the <span class="math inline">\(1-\alpha^{\text {th }}\)</span> quantile <span class="math inline">\(q_{1-\alpha}^{*}\)</span>. The bootstrap test rejects <span class="math inline">\(\mathbb{H}_{0}\)</span> in favor of <span class="math inline">\(\mathbb{H}_{1}\)</span> if <span class="math inline">\(J&gt;q_{1-\alpha}^{*}\)</span>. The bootstrap p-value is</p>
<p><span class="math display">\[
p^{*}=\frac{1}{B} \sum_{b=1}^{B} \mathbb{1}\left\{J^{*}(b)&gt;J\right\} .
\]</span></p>
<p>Special cases of criterion-based tests are minimum distance tests, <span class="math inline">\(F\)</span> tests, and likelihood ratio tests. Take the F test for a linear hypothesis <span class="math inline">\(\boldsymbol{R}^{\prime} \beta=\theta_{0}\)</span>. The <span class="math inline">\(F\)</span> statistic is</p>
<p><span class="math display">\[
\mathrm{F}=\frac{\left(\widetilde{\sigma}^{2}-\widehat{\sigma}^{2}\right) / q}{\widehat{\sigma}^{2} /(n-k)}
\]</span></p>
<p>where <span class="math inline">\(\widehat{\sigma}^{2}\)</span> is the unrestricted estimator of the error variance, <span class="math inline">\(\widetilde{\sigma}^{2}\)</span> is the restricted estimator, <span class="math inline">\(q\)</span> is the number of restrictions and <span class="math inline">\(k\)</span> is the number of estimated coefficients. The bootstrap version of the <span class="math inline">\(F\)</span> statistic is</p>
<p><span class="math display">\[
\mathrm{F}^{*}=\frac{\left(\widetilde{\sigma}^{* 2}-\widehat{\sigma}^{* 2}\right) / q}{\widehat{\sigma}^{* 2} /(n-k)}
\]</span></p>
<p>where <span class="math inline">\(\widehat{\sigma}^{* 2}\)</span> is the unrestricted estimator on the bootstrap sample, and <span class="math inline">\(\widetilde{\sigma}^{* 2}\)</span> is the restricted estimator which imposes the restriction <span class="math inline">\(\widehat{\theta}=\boldsymbol{R}^{\prime} \widehat{\beta}\)</span>.</p>
<p>Take the likelihood ratio (LR) test for the hypothesis <span class="math inline">\(r(\beta)=\theta_{0}\)</span>. The LR test statistic is</p>
<p><span class="math display">\[
\mathrm{LR}=2\left(\ell_{n}(\widehat{\beta})-\ell_{n}(\widetilde{\beta})\right)
\]</span></p>
<p>where <span class="math inline">\(\widehat{\beta}\)</span> is the unrestricted MLE and <span class="math inline">\(\widetilde{\beta}\)</span> is the restricted MLE (imposing <span class="math inline">\(r(\beta)=\theta_{0}\)</span> ). The bootstrap version is</p>
<p><span class="math display">\[
\operatorname{LR}^{*}=2\left(\ell_{n}^{*}\left(\widehat{\beta}^{*}\right)-\ell_{n}^{*}\left(\widetilde{\beta}^{*}\right)\right)
\]</span></p>
<p>where <span class="math inline">\(\ell_{n}^{*}(\beta)\)</span> is the log-likelihood function calculated on the bootstrap sample, <span class="math inline">\(\widehat{\beta}^{*}\)</span> is the unrestricted maximizer, and <span class="math inline">\(\widetilde{\beta}^{*}\)</span> is the restricted maximizer imposing the restriction <span class="math inline">\(r(\beta)=r(\widehat{\beta})\)</span>.</p>
</section>
<section id="parametric-bootstrap" class="level2" data-number="11.24">
<h2 data-number="11.24" class="anchored" data-anchor-id="parametric-bootstrap"><span class="header-section-number">11.24</span> Parametric Bootstrap</h2>
<p>Throughout this chapter we have described the most popular form of the bootstrap known as the nonparametric bootstrap. However there are other forms of the bootstrap algorithm including the parametric bootstrap. This is appropriate when there is a full parametric model for the distribution as in likelihood estimation.</p>
<p>First, consider the context where the model specifies the full distribution of the random vector <span class="math inline">\(Y\)</span>, e.g.&nbsp;<span class="math inline">\(Y \sim F(y \mid \beta)\)</span> where the distribution function <span class="math inline">\(F\)</span> is known but the parameter <span class="math inline">\(\beta\)</span> is unknown. Let <span class="math inline">\(\widehat{\beta}\)</span> be an estimator of <span class="math inline">\(\beta\)</span> such as the maximum likelihood estimator. The parametric bootstrap algorithm generates bootstrap observations <span class="math inline">\(Y_{i}^{*}\)</span> by drawing random vectors from the distribution function <span class="math inline">\(F(y \mid \widehat{\beta})\)</span>. When this is done, the true value of <span class="math inline">\(\beta\)</span> in the bootstrap universe is <span class="math inline">\(\widehat{\beta}\)</span>. Everything which has been discussed in the chapter can be applied using this bootstrap algorithm.</p>
<p>Second, consider the context where the model specifies the conditional distribution of the random vector <span class="math inline">\(Y\)</span> given the random vector <span class="math inline">\(X\)</span>, e.g.&nbsp;<span class="math inline">\(Y \mid X \sim F(y \mid X, \beta)\)</span>. An example is the normal linear regression model, where <span class="math inline">\(Y \mid X \sim \mathrm{N}\left(X^{\prime} \beta, \sigma^{2}\right)\)</span>. In this context we can hold the regressors <span class="math inline">\(X_{i}\)</span> fixed and then draw the bootstrap observations <span class="math inline">\(Y_{i}^{*}\)</span> from the conditional distribution <span class="math inline">\(F\left(y \mid X_{i}, \widehat{\beta}\right)\)</span>. In the example of the normal regression model this is equivalent to drawing a normal error <span class="math inline">\(e_{i}^{*} \sim \mathrm{N}\left(0, \widehat{\sigma}^{2}\right)\)</span> and then setting <span class="math inline">\(Y_{i}^{*}=X_{i}^{\prime} \widehat{\beta}+\)</span> <span class="math inline">\(e_{i}^{*}\)</span>. Again, in this algorithm the true value of <span class="math inline">\(\beta\)</span> is <span class="math inline">\(\widehat{\beta}\)</span> and everything which is discussed in this chapter can be applied as before.</p>
<p>Third, consider tests of the hypothesis <span class="math inline">\(r(\beta)=\theta_{0}\)</span>. In this context we can also construct a restricted estimator <span class="math inline">\(\widetilde{\beta}\)</span> (for example the restricted MLE) which satisfies the hypothesis <span class="math inline">\(r(\widetilde{\beta})=\theta_{0}\)</span>. Then we can generate bootstrap samples by simulating from the distribution <span class="math inline">\(Y_{i}^{*} \sim F(y \mid \widetilde{\beta})\)</span>, or in the conditional context from <span class="math inline">\(Y_{i}^{*} \sim F\left(y \mid X_{i}, \widetilde{\beta}\right)\)</span>. When this is done the true value of <span class="math inline">\(\beta\)</span> in the bootstrap is <span class="math inline">\(\widetilde{\beta}\)</span> which satisfies the hypothesis. So in this context the correct values of the bootstrap statistics are</p>
<p><span class="math display">\[
\begin{gathered}
T^{*}=\frac{\widehat{\theta}^{*}-\theta_{0}}{s\left(\widehat{\theta}^{*}\right)} \\
W^{*}=\left(\widehat{\theta}^{*}-\theta_{0}\right)^{\prime} \widehat{\boldsymbol{V}}_{\widehat{\theta}}^{*-1}\left(\widehat{\theta}^{*}-\theta_{0}\right) \\
J^{*}=\min _{r(\beta)=\theta_{0}} J^{*}(\beta)-\min _{\beta} J^{*}(\beta) \\
\mathrm{LR}^{*}=2\left(\max _{\beta} \ell_{n}^{*}(\beta)-\max _{r(\beta)=\theta_{0}} \ell_{n}^{*}(\beta)\right)
\end{gathered}
\]</span></p>
<p>and</p>
<p><span class="math display">\[
\mathrm{F}^{*}=\frac{\left(\widetilde{\sigma}^{* 2}-\widehat{\sigma}^{* 2}\right) / q}{\widehat{\sigma}^{* 2} /(n-k)}
\]</span></p>
<p>where <span class="math inline">\(\widehat{\sigma}^{* 2}\)</span> is the unrestricted estimator on the bootstrap sample and <span class="math inline">\(\widetilde{\sigma}^{* 2}\)</span> is the restricted estimator which imposes the restriction <span class="math inline">\(\boldsymbol{R}^{\prime} \beta=\theta_{0}\)</span>.</p>
<p>The primary advantage of the parametric bootstrap (relative to the nonparametric bootstrap) is that it will be more accurate when the parametric model is correct. This may be quite important in small samples. The primary disadvantage of the parametric bootstrap is that it can be inaccurate when the parametric model is incorrect.</p>
</section>
<section id="how-many-bootstrap-replications" class="level2" data-number="11.25">
<h2 data-number="11.25" class="anchored" data-anchor-id="how-many-bootstrap-replications"><span class="header-section-number">11.25</span> How Many Bootstrap Replications?</h2>
<p>How many bootstrap replications should be used? There is no universally correct answer as there is a trade-off between accuracy and computation cost. Computation cost is essentially linear in <span class="math inline">\(B\)</span>. Accuracy (either standard errors or <span class="math inline">\(p\)</span>-values) is proportional to <span class="math inline">\(B^{-1 / 2}\)</span>. Improved accuracy can be obtained but only at a higher computational cost.</p>
<p>In most empirical research, most calculations are quick and investigatory, not requiring full accuracy. But final results (those going into the final version of the paper) should be accurate. Thus it seems reasonable to use asymptotic and/or bootstrap methods with a modest number of replications for daily calculations, but use a much larger <span class="math inline">\(B\)</span> for the final version.</p>
<p>In particular, for final calculations, <span class="math inline">\(B=10,000\)</span> is desired, with <span class="math inline">\(B=1000\)</span> a minimal choice. In contrast, for daily quick calculations values as low as <span class="math inline">\(B=100\)</span> may be sufficient for rough estimates. A useful way to think about the accuracy of bootstrap methods stems from the calculation of pvalues. The bootstrap p-value <span class="math inline">\(p^{*}\)</span> is an average of <span class="math inline">\(B\)</span> Bernoulli draws. The variance of the simulation estimator of <span class="math inline">\(p^{*}\)</span> is <span class="math inline">\(p^{*}\left(1-p^{*}\right) / B\)</span>, which is bounded above by <span class="math inline">\(1 / 4 B\)</span>. To calculate the <span class="math inline">\(\mathrm{p}\)</span>-value within, say, <span class="math inline">\(0.01\)</span> of the true value with <span class="math inline">\(95 %\)</span> probability requires a standard error below <span class="math inline">\(0.005\)</span>. This is ensured if <span class="math inline">\(B \geq 10,000\)</span>.</p>
<p>Stata by default sets <span class="math inline">\(B=50\)</span>. This is useful for verification that a program runs but is a poor choice for empirical reporting. Make sure that you set <span class="math inline">\(B\)</span> to the value you want.</p>
</section>
<section id="setting-the-bootstrap-seed" class="level2" data-number="11.26">
<h2 data-number="11.26" class="anchored" data-anchor-id="setting-the-bootstrap-seed"><span class="header-section-number">11.26</span> Setting the Bootstrap Seed</h2>
<p>Computers do not generate true random numbers but rather pseudo-random numbers generated by a deterministic algorithm. The algorithms generate sequences which are indistinguishable from random sequences so this is not a worry for bootstrap applications.</p>
<p>The methods, however, necessarily require a starting value known as a “seed”. Some packages (including Stata and MATLAB) implement this with a default seed which is reset each time the statistical package is started. This means if you start the package fresh, run a bootstrap program (e.g.&nbsp;a “do” file in Stata), exit the package, restart the package and then rerun the bootstrap program, you should obtain exactly the same results. If you instead run the bootstrap program (e.g.&nbsp;“do” file) twice sequentially without restarting the package, the seed is not reset so a different set of pseudo-random numbers will be generated and the results from the two runs will be different.</p>
<p>The R package has a different implementation. When <span class="math inline">\(\mathrm{R}\)</span> is loaded the random number seed is generated based on the computer’s clock (which results in an essentially random starting seed). Therefore if you run a bootstrap program in R, exit, restart, and rerun, you will obtain a different set of random draws and therefore a different bootstrap result.</p>
<p>Packages allow users to set their own seed. (In Stata, the command is set seed #. In MATLAB the command is <span class="math inline">\(r n g(\#)\)</span>. In <span class="math inline">\(R\)</span> the command is set. seed (#).) If the seed is set to a specific number at the start of a file then the exact same pseudo-random numbers will be generated each time the program is run. If this is the case, the results of a bootstrap calculation (standard error or test) will be identical across computer runs.</p>
<p>The fact that the bootstrap results can be fixed by setting the seed in the replication file has motivated many researchers to follow this choice. They set the seed at the start of the replication file so that repeated executions result in the same numerical findings.</p>
<p>Fixing seeds, however, should be done cautiously. It may be a wise choice for a final calculation (when a paper is finished) but is an unwise choice for daily calculations. If you use a small number of replications in your preliminary work, say <span class="math inline">\(B=100\)</span>, the bootstrap calculations will be inaccurate. But as you run your results again and again (as is typical in empirical projects) you will obtain the same numerical standard errors and test results, giving you a false sense of stability and accuracy. If instead a different seed is used each time the program is run then the bootstrap results will vary across runs, and you will observe that the results vary across these runs, giving you important and meaningful information about the (lack of) accuracy in your results. One way to ensure this is to set the seed according to the current clock. In MATLAB use the command rng(‘shuffle’). In R use set. seed (seed=NULL). Stata does not have this option.</p>
<p>These considerations lead to a recommended hybrid approach. For daily empirical investigations do not fix the bootstrap seed in your program unless you have it set by the clock. For your final calculations set the seed to a specific arbitrary choice, and set <span class="math inline">\(B=10,000\)</span> so that the results are insensitive to the seed.</p>
</section>
<section id="bootstrap-regression" class="level2" data-number="11.27">
<h2 data-number="11.27" class="anchored" data-anchor-id="bootstrap-regression"><span class="header-section-number">11.27</span> Bootstrap Regression</h2>
<p>A major focus of this textbook has been on the least squares estimator <span class="math inline">\(\widehat{\beta}\)</span> in the projection model. The bootstrap can be used to calculate standard errors and confidence intervals for smooth functions of the coefficient estimates.</p>
<p>The nonparametric bootstrap algorithm, as described before, samples observations randomly with replacement from the dataset, creating the bootstrap sample <span class="math inline">\(\left\{\left(Y_{1}^{*}, X_{1}^{*}\right), \ldots,\left(Y_{n}^{*}, X_{n}^{*}\right)\right\}\)</span>, or in matrix notation <span class="math inline">\(\left(\boldsymbol{Y}^{*}, \boldsymbol{X}^{*}\right)\)</span> It is important to recognize that entire observations (pairs of <span class="math inline">\(Y_{i}\)</span> and <span class="math inline">\(X_{i}\)</span> ) are sampled. This is often called the pairs bootstrap.</p>
<p>Given this bootstrap sample, we calculate the regression estimator</p>
<p><span class="math display">\[
\widehat{\beta}^{*}=\left(\boldsymbol{X}^{* \prime} \boldsymbol{X}^{*}\right)^{-1}\left(\boldsymbol{X}^{* \prime} \boldsymbol{Y}^{*}\right) .
\]</span></p>
<p>This is repeated <span class="math inline">\(B\)</span> times. The bootstrap standard errors are the standard deviations across the draws and confidence intervals are constructed from the empirical quantiles across the draws.</p>
<p>What is the nature of the bootstrap distribution of <span class="math inline">\(\widehat{\beta}^{*}\)</span> ? It is useful to start with the distribution of the bootstrap observations <span class="math inline">\(\left(Y_{i}^{*}, X_{i}^{*}\right)\)</span>, which is the discrete distribution which puts mass <span class="math inline">\(1 / n\)</span> on each observation pair <span class="math inline">\(\left(Y_{i}, X_{i}\right)\)</span>. The bootstrap universe can be thought of as the empirical scatter plot of the observations. The true value of the projection coefficient in this bootstrap universe is</p>
<p><span class="math display">\[
\left(\mathbb{E}^{*}\left[X_{i}^{*} X_{i}^{* \prime}\right]\right)^{-1}\left(\mathbb{E}^{*}\left[X_{i}^{*} Y_{i}^{*}\right]\right)=\left(\frac{1}{n} \sum_{i=1}^{n} X_{i} X_{i}^{\prime}\right)^{-1}\left(\frac{1}{n} \sum_{i=1}^{n} X_{i} Y_{i}\right)=\widehat{\beta}
\]</span></p>
<p>We see that the true value in the bootstrap distribution is the least squares estimator <span class="math inline">\(\widehat{\beta}\)</span>.</p>
<p>The bootstrap observations satisfy the projection equation</p>
<p><span class="math display">\[
\begin{aligned}
Y_{i}^{*} &amp;=X_{i}^{* \prime} \widehat{\beta}+e_{i}^{*} \\
\mathbb{E}^{*}\left[X_{i}^{*} e_{i}^{*}\right] &amp;=0 .
\end{aligned}
\]</span></p>
<p>For each bootstrap pair <span class="math inline">\(\left(Y_{i}^{*}, X_{i}^{*}\right)=\left(Y_{j}, X_{j}\right)\)</span> the true error <span class="math inline">\(e_{i}^{*}=\widehat{e}_{j}\)</span> equals the least squares residual from the original dataset. This is because each bootstrap pair corresponds to an actual observation.</p>
<p>A technical problem (which is typically ignored) is that it is possible for <span class="math inline">\(\boldsymbol{X}^{* \prime} \boldsymbol{X}^{*}\)</span> to be singular in a simulated bootstrap sample, in which case the least squares estimator <span class="math inline">\(\widehat{\beta}^{*}\)</span> is not uniquely defined. Indeed, the probability is positive that <span class="math inline">\(\boldsymbol{X}^{* \prime} \boldsymbol{X}^{*}\)</span> is singular. For example, the probability that a bootstrap sample consists entirely of one observation repeated <span class="math inline">\(n\)</span> times is <span class="math inline">\(n^{-(n-1)}\)</span>. This is a small probability, but positive. A more significant example is sparse dummy variable designs where it is possible to draw an entire sample with only one observed value for the dummy variable. For example, if a sample has <span class="math inline">\(n=20\)</span> observations with a dummy variable with treatment (equals 1) for only three of the 20 observations, the probability is <span class="math inline">\(4 %\)</span> that a bootstrap sample contains entirely non-treated values (all 0’s). <span class="math inline">\(4 %\)</span> is quite high!</p>
<p>The standard approach to circumvent this problem is to compute <span class="math inline">\(\widehat{\beta}^{*}\)</span> only if <span class="math inline">\(\boldsymbol{X}^{* \prime} \boldsymbol{X}^{*}\)</span> is non-singular as defined by a conventional numerical tolerance and treat it as missing otherwise. A better solution is to define a tolerance which bounds <span class="math inline">\(\boldsymbol{X}^{* \prime} \boldsymbol{X}^{*}\)</span> away from non-singularity. Define the ratio of the smallest eigenvalue of the bootstrap design matrix to that of the data design matrix</p>
<p><span class="math display">\[
\lambda^{*}=\frac{\lambda_{\min }\left(\boldsymbol{X}^{* \prime} \boldsymbol{X}^{*}\right)}{\lambda_{\min }\left(\boldsymbol{X}^{\prime} \boldsymbol{X}\right)} .
\]</span></p>
<p>If, in a given bootstrap replication, <span class="math inline">\(\lambda^{*}&lt;\tau\)</span> is smaller than a given tolerance (Shao and Tu <span class="math inline">\((1995, \mathrm{p} .291)\)</span> recommend <span class="math inline">\(\tau=1 / 2\)</span> ) then the estimator can be treated as missing, or we can define the trimming rule</p>
<p><span class="math display">\[
\widehat{\beta}^{*}=\left\{\begin{array}{cc}
\widehat{\beta}^{*} &amp; \text { if } \lambda^{*} \geq \tau \\
\widehat{\beta} &amp; \text { if } \lambda^{*}&lt;\tau
\end{array}\right.
\]</span></p>
<p>This ensures that the bootstrap estimator <span class="math inline">\(\widehat{\beta}^{*}\)</span> will be well behaved.</p>
</section>
<section id="bootstrap-regression-asymptotic-theory" class="level2" data-number="11.28">
<h2 data-number="11.28" class="anchored" data-anchor-id="bootstrap-regression-asymptotic-theory"><span class="header-section-number">11.28</span> Bootstrap Regression Asymptotic Theory</h2>
<p>Define the least squares estimator <span class="math inline">\(\widehat{\beta}\)</span>, its bootstrap version <span class="math inline">\(\widehat{\beta}^{*}\)</span> as in (10.32), and the transformations <span class="math inline">\(\widehat{\theta}=g(\widehat{\beta})\)</span> and <span class="math inline">\(\widehat{\theta}^{*}=r\left(\widehat{\beta}^{*}\right)\)</span> for some smooth transformation <span class="math inline">\(r\)</span>. Let <span class="math inline">\(\widehat{\boldsymbol{V}}_{\beta}\)</span> and <span class="math inline">\(\widehat{\boldsymbol{V}}_{\theta}\)</span> denote heteroskedasticityrobust covariance matrix estimators for <span class="math inline">\(\widehat{\beta}\)</span> and <span class="math inline">\(\widehat{\theta}\)</span>, and let <span class="math inline">\(\widehat{V}_{\beta}^{*}\)</span> and <span class="math inline">\(\widehat{\boldsymbol{V}}_{\theta}^{*}\)</span> be their bootstrap versions. When <span class="math inline">\(\theta\)</span> is scalar define the standard errors <span class="math inline">\(s(\widehat{\theta})=\sqrt{n^{-1} \widehat{\boldsymbol{V}}_{\theta}}\)</span> and <span class="math inline">\(s\left(\widehat{\theta}^{*}\right)=\sqrt{n^{-1} \widehat{\boldsymbol{V}}_{\theta^{*}}}\)</span>. Define the t-ratios <span class="math inline">\(T=\)</span> <span class="math inline">\((\widehat{\theta}-\theta) / s(\widehat{\theta})\)</span> and bootstrap version <span class="math inline">\(T^{*}=\left(\widehat{\theta}^{*}-\widehat{\theta}\right) / s\left(\widehat{\theta}^{*}\right)\)</span>. We are interested in the asymptotic distributions of <span class="math inline">\(\widehat{\beta}^{*}, \widehat{\theta}^{*}\)</span> and <span class="math inline">\(T^{*}\)</span></p>
<p>Since the bootstrap observations satisfy the model (10.33), we see by standard calculations that</p>
<p><span class="math display">\[
\sqrt{n}\left(\widehat{\beta}^{*}-\widehat{\beta}\right)=\left(\frac{1}{n} \sum_{i=1}^{n} X_{i}^{*} X_{i}^{* \prime}\right)^{-1}\left(\frac{1}{\sqrt{n}} \sum_{i=1}^{n} X_{i}^{*} e_{i}^{*}\right)
\]</span></p>
<p>By the bootstrap WLLN</p>
<p><span class="math display">\[
\frac{1}{n} \sum_{i=1}^{n} X_{i}^{*} X_{i}^{* \prime} \underset{p^{*}}{\longrightarrow} \mathbb{E}\left[X_{i} X_{i}^{\prime}\right]=\boldsymbol{Q}
\]</span></p>
<p>and by the bootstrap CLT</p>
<p><span class="math display">\[
\frac{1}{\sqrt{n}} \sum_{i=1}^{n} X_{i}^{*} e_{i}^{*} \underset{d^{*}}{\longrightarrow} \mathrm{N}(0, \Omega)
\]</span></p>
<p>where <span class="math inline">\(\Omega=\mathbb{E}\left[X X^{\prime} e^{2}\right]\)</span>. Again applying the bootstrap WLLN we obtain</p>
<p><span class="math display">\[
\widehat{\boldsymbol{V}}_{\beta} \underset{p^{*}}{ } \boldsymbol{V}_{\beta}=\boldsymbol{Q}^{-1} \Omega \boldsymbol{Q}^{-1}
\]</span></p>
<p>and</p>
<p><span class="math display">\[
\widehat{\boldsymbol{V}}_{\theta} \underset{p^{*}}{\longrightarrow} \boldsymbol{V}_{\theta}=\boldsymbol{R}^{\prime} \boldsymbol{V}_{\beta} \boldsymbol{R}
\]</span></p>
<p>where <span class="math inline">\(\boldsymbol{R}=\boldsymbol{R}(\beta)\)</span>.</p>
<p>Combining with the bootstrap CMT and delta method we establish the asymptotic distribution of the bootstrap regression estimator.</p>
<p>Theorem <span class="math inline">\(10.18\)</span> Under Assumption 7.2, as <span class="math inline">\(n \rightarrow \infty\)</span></p>
<p><span class="math display">\[
\sqrt{n}\left(\widehat{\theta}^{*}-\widehat{\theta}\right) \underset{d^{*}}{\longrightarrow} \mathrm{N}\left(0, \boldsymbol{V}_{\beta}\right) .
\]</span></p>
<p>If Assumption <span class="math inline">\(7.3\)</span> also holds then</p>
<p><span class="math display">\[
\sqrt{n}\left(\widehat{\theta}^{*}-\widehat{\theta}\right) \underset{d^{*}}{\longrightarrow} \mathrm{N}\left(0, \boldsymbol{V}_{\theta}\right) .
\]</span></p>
<p>If Assumption <span class="math inline">\(7.4\)</span> also holds then</p>
<p><span class="math display">\[
T^{*} \underset{d^{*}}{\longrightarrow} \mathrm{N}(0,1) .
\]</span></p>
<p>This means that the bootstrap confidence interval and testing methods all apply for inference on <span class="math inline">\(\beta\)</span> and <span class="math inline">\(\theta\)</span>. This includes the percentile, <span class="math inline">\(\mathrm{BC}\)</span> percentile, <span class="math inline">\(\mathrm{BC}_{a}\)</span>, and percentile-t intervals, and hypothesis tests based on t-tests, Wald tests, MD tests, LR tests and F tests.</p>
<p>To justify bootstrap standard errors we also need to verify the uniform square integrability of <span class="math inline">\(\widehat{\beta}^{*}\)</span> and <span class="math inline">\(\widehat{\theta}^{*}\)</span>. This is technically challenging because the least squares estimator involves matrix inversion which is not globally continuous. A partial solution is to use the trimmed estimator (10.34). This bounds the moments of <span class="math inline">\(\widehat{\beta}^{*}\)</span> by those of <span class="math inline">\(n^{-1} \sum_{i=1}^{n} X_{i}^{*} e_{i}^{*}\)</span>. Since this is a sample mean, Theorem <span class="math inline">\(10.10\)</span> applies and <span class="math inline">\(\widehat{\boldsymbol{V}}_{\beta}^{*}\)</span> is bootstrap consistent for <span class="math inline">\(\boldsymbol{V}_{\beta}\)</span>. However, this does not ensure that <span class="math inline">\(\widehat{\boldsymbol{V}}_{\theta}^{*}\)</span> will be consistent for <span class="math inline">\(\widehat{\boldsymbol{V}}_{\theta}\)</span> unless the function <span class="math inline">\(r(x)\)</span> satisfies the conditions of Theorem 10.10. For general applications use a trimmed estimator for the bootstrap variance. For some <span class="math inline">\(\tau_{n}=O\left(e^{n / 8}\right)\)</span> define</p>
<p><span class="math display">\[
\begin{aligned}
Z_{n}^{*} &amp;=\sqrt{n}\left(\widehat{\theta}^{*}-\widehat{\theta}\right) \\
Z^{* *} &amp;=z^{*} \mathbb{1}\left\{\left\|Z_{n}^{*}\right\| \leq \tau_{n}\right\} \\
Z^{* *} &amp;=\frac{1}{B} \sum_{b=1}^{B} Z^{* *}(b) \\
\widehat{\mathbf{V}}_{\theta}^{\text {boot }, \tau} &amp;=\frac{1}{B-1} \sum_{b=1}^{B}\left(Z^{* *}(b)-Z^{* *}\right)\left(Z^{* *}(b)-Z^{* *}\right)^{\prime} .
\end{aligned}
\]</span></p>
<p>The matrix <span class="math inline">\(\widehat{\boldsymbol{V}}_{\theta}^{\text {boot }}\)</span> is a trimmed bootstrap estimator of the variance of <span class="math inline">\(Z_{n}=\sqrt{n}(\widehat{\theta}-\theta)\)</span>. The associated bootstrap standard error for <span class="math inline">\(\widehat{\theta}\)</span> (in the scalar case) is <span class="math inline">\(s(\widehat{\theta})=\sqrt{n^{-1} \widehat{\boldsymbol{V}}_{\theta}^{\text {boot }}}\)</span>.</p>
<p>By an application of Theorems <span class="math inline">\(10.11\)</span> and 10.12, we find that this estimator <span class="math inline">\(\widehat{\boldsymbol{V}}_{\theta}^{\text {boot }}\)</span> is consistent for the asymptotic variance.</p>
<p>Theorem 10.19 Under Assumption <span class="math inline">\(7.2\)</span> and <span class="math inline">\(7.3\)</span>, as <span class="math inline">\(n \rightarrow \infty, \widehat{\boldsymbol{V}}_{\theta}^{\mathrm{boot}, \tau} \underset{p^{*}}{\longrightarrow} \boldsymbol{V}_{\theta}\)</span></p>
<p>Programs such as Stata use the untrimmed estimator <span class="math inline">\(\widehat{\boldsymbol{V}}_{\theta}^{\text {boot }}\)</span> rather than the trimmed estimator <span class="math inline">\(\widehat{\boldsymbol{V}}_{\theta}^{\text {boot }, \tau}\)</span>. This means that we should be cautious about interpreting reported bootstrap standard errors especially for nonlinear functions such as ratios.</p>
</section>
<section id="wild-bootstrap" class="level2" data-number="11.29">
<h2 data-number="11.29" class="anchored" data-anchor-id="wild-bootstrap"><span class="header-section-number">11.29</span> Wild Bootstrap</h2>
<p>Take the linear regression model</p>
<p><span class="math display">\[
\begin{aligned}
Y &amp;=X^{\prime} \beta+e \\
\mathbb{E}[e \mid X] &amp;=0 .
\end{aligned}
\]</span></p>
<p>What is special about this model is the conditional mean restriction. The nonparametric bootstrap (which samples the pairs <span class="math inline">\(\left(Y_{i}^{*}, X_{i}^{*}\right)\)</span> i.i.d. from the original observations) does not make use of this restriction. Consequently the bootstrap distribution for <span class="math inline">\(\left(Y^{*}, X^{*}\right)\)</span> does not satisfy the conditional mean restriction and therefore does not satisfy the linear regression assumption. To improve precision it seems reasonable to impose the conditional mean restriction on the bootstrap distribution.</p>
<p>A natural approach is to hold the regressors <span class="math inline">\(X_{i}\)</span> fixed and then draw the errors <span class="math inline">\(e_{i}^{*}\)</span> in some way which imposes a conditional mean of zero. The simplest approach is to draw the errors independent from the regressors, perhaps from the empirical distribution of the residuals. This procedure is known as the residual bootstrap. However, this imposes independence of the errors from the regressors which is much stronger than the conditional mean assumption. This is generally undesirable.</p>
<p>A method which imposes the conditional mean restriction while allowing general heteroskedasticity is the wild bootstrap. It was proposed by Liu (1988) and extended by Mammon (1993). The method uses auxiliary random variables <span class="math inline">\(\xi^{*}\)</span> which are i.i.d., mean zero, and variance 1 . The bootstrap observations are then generated as <span class="math inline">\(Y_{i}^{*}=X_{i}^{\prime} \widehat{\beta}+e_{i}^{*}\)</span> with <span class="math inline">\(e_{i}^{*}=\widehat{e}_{i} \xi_{i}^{*}\)</span>, where the regressors <span class="math inline">\(X_{i}\)</span> are held fixed at their sample values, <span class="math inline">\(\widehat{\beta}\)</span> is the sample least squares estimator, and <span class="math inline">\(\widehat{e}_{i}\)</span> are the least squares residuals, which are also held fixed at their sample values.</p>
<p>This algorithm generates bootstrap errors <span class="math inline">\(e_{i}^{*}\)</span> which are conditionally mean zero. Thus the bootstrap pairs <span class="math inline">\(\left(Y_{i}^{*}, X_{i}\right)\)</span> satisfy a linear regression with the “true” coefficient of <span class="math inline">\(\widehat{\beta}\)</span>. The conditional variance of the wild bootstrap errors <span class="math inline">\(e_{i}^{*}\)</span> are <span class="math inline">\(\mathbb{E}^{*}\left[e_{i}^{* 2} \mid X_{i}\right]=\widehat{e}_{i}^{2}\)</span>. This means that the conditional variance of the bootstrap estimator <span class="math inline">\(\widehat{\beta}^{*}\)</span> is</p>
<p><span class="math display">\[
\mathbb{E}^{*}\left[\left(\widehat{\beta}^{*}-\widehat{\beta}\right)\left(\widehat{\beta}^{*}-\widehat{\beta}\right)^{\prime} \mid \boldsymbol{X}\right]=\left(\boldsymbol{X}^{\prime} \boldsymbol{X}\right)^{-1}\left(\sum_{i=1}^{n} X_{i} X_{i}^{\prime} \widehat{e}_{i}^{2}\right)\left(\boldsymbol{X}^{\prime} \boldsymbol{X}\right)^{-1}
\]</span></p>
<p>which is the White estimator of the variance of <span class="math inline">\(\widehat{\beta}\)</span>. Thus the wild bootstrap replicates the appropriate first and second moments of the distribution.</p>
<p>Two distributions have been proposed for the auxilary variables <span class="math inline">\(\xi_{i}^{*}\)</span> both of which are two-point discrete distributions. The first are Rademacher random variables which satisfy <span class="math inline">\(\mathbb{P}\left[\xi^{*}=1\right]=\frac{1}{2}\)</span> and <span class="math inline">\(\mathbb{P}\left[\xi^{*}=-1\right]=\)</span> <span class="math inline">\(\frac{1}{2}\)</span>. The second is the Mammen (1993) two-point distribution</p>
<p><span class="math display">\[
\begin{aligned}
&amp;\mathbb{P}\left[\xi^{*}=\frac{1+\sqrt{5}}{2}\right]=\frac{\sqrt{5}-1}{2 \sqrt{5}} \\
&amp;\mathbb{P}\left[\xi^{*}=\frac{1-\sqrt{5}}{2}\right]=\frac{\sqrt{5}+1}{2 \sqrt{5}}
\end{aligned}
\]</span></p>
<p>The reasoning behind the Mammen distribution is that this choice implies <span class="math inline">\(\mathbb{E}\left[\xi^{* 3}\right]=1\)</span>, which implies that the third central moment of <span class="math inline">\(\widehat{\beta}^{*}\)</span> matches the natural nonparametric estimator of the third central moment of <span class="math inline">\(\widehat{\beta}\)</span>. Since the wild bootstrap matches the first three moments, the percentile-t interval and one-sided t-tests can be shown to achieve asymptotic refinements.</p>
<p>The reasoning behind the Rademacher distribution is that this choice implies <span class="math inline">\(\mathbb{E}\left[\xi^{* 4}\right]=1\)</span>, which implies that the fourth central moment of <span class="math inline">\(\widehat{\beta}^{*}\)</span> matches the natural nonparametric estimator of the fourth central moment of <span class="math inline">\(\widehat{\beta}\)</span>. If the regression errors <span class="math inline">\(e\)</span> are symmetrically distributed (so the third moment is zero) then the first four moments are matched. In this case the wild bootstrap should have even better performance, and additionally two-sided t-tests can be shown to achieve an asymptotic refinement. When the regression error is not symmetrically distributed these asymptotic refinements are not achieved. Limited simulation evidence for one-sided t-tests presented in Davidson and Flachaire (2008) suggests that the Rademacher distribution (used with the restricted wild bootstrap) has better performance and is their recommendation.</p>
<p>For hypothesis testing improved precision can be obtained by the restricted wild bootstrap. Consider tests of the hypothesis <span class="math inline">\(\mathbb{H}_{0}: r(\beta)=0\)</span>. Let <span class="math inline">\(\widetilde{\beta}\)</span> be a CLS or EMD estimator of <span class="math inline">\(\beta\)</span> subject to the restriction <span class="math inline">\(r(\widetilde{\beta})=0\)</span>. Let <span class="math inline">\(\widetilde{e}_{i}=Y_{i}-X_{i}^{\prime} \widetilde{\beta}\)</span> be the constrained residuals. The restricted wild bootstrap algorithm generates observations as <span class="math inline">\(Y_{i}^{*}=X_{i}^{\prime} \widetilde{\beta}+e_{i}^{*}\)</span> with <span class="math inline">\(e_{i}^{*}=\widetilde{e}_{i} \xi_{i}^{*}\)</span>. With this modification <span class="math inline">\(\widetilde{\beta}\)</span> is the true value in the bootstrap universe so the null hypothesis <span class="math inline">\(\mathbb{M}_{0}\)</span> holds. Thus bootstrap tests are constructed the same as for the parametric bootstrap using a restricted parameter estimator.</p>
</section>
<section id="bootstrap-for-clustered-observations" class="level2" data-number="11.30">
<h2 data-number="11.30" class="anchored" data-anchor-id="bootstrap-for-clustered-observations"><span class="header-section-number">11.30</span> Bootstrap for Clustered Observations</h2>
<p>Bootstrap methods can also be applied in to clustered samples though the methodological literature is relatively thin. Here we review methods discussed in Cameron, Gelbach and Miller (2008).</p>
<p>Let <span class="math inline">\(\boldsymbol{Y}_{g}=\left(Y_{1 g}, \ldots, Y_{n_{g} g}\right)^{\prime}\)</span> and <span class="math inline">\(\boldsymbol{X}_{g}=\left(X_{1 g}, \ldots, X_{n_{g} g}\right)^{\prime}\)</span> denote the <span class="math inline">\(n_{g} \times 1\)</span> vector of dependent variables and <span class="math inline">\(n_{g} \times k\)</span> matrix of regressors for the <span class="math inline">\(g^{t h}\)</span> cluster. A linear regression model using cluster notation is <span class="math inline">\(\boldsymbol{Y}_{g}=\)</span> <span class="math inline">\(\boldsymbol{X}_{g} \beta+\boldsymbol{e}_{g}\)</span> where <span class="math inline">\(\boldsymbol{e}_{g}=\left(e_{1 g}, \ldots, e_{n_{g} g}\right)^{\prime}\)</span> is an <span class="math inline">\(n_{g} \times 1\)</span> error vector. The sample has <span class="math inline">\(G\)</span> cluster pairs <span class="math inline">\(\left(\boldsymbol{Y}_{g}, \boldsymbol{X}_{g}\right)\)</span>.</p>
<p>The pairs cluster bootstrap samples <span class="math inline">\(G\)</span> cluster pairs <span class="math inline">\(\left(\boldsymbol{Y}_{g}, \boldsymbol{X}_{g}\right)\)</span> to create the bootstrap sample. Least squares is applied to the bootstrap sample to obtain the coefficient estimators. By repeating <span class="math inline">\(B\)</span> times bootstrap standard errors for coefficients estimates, or functions of the coefficient estimates, can be calculated. Percentile, <span class="math inline">\(\mathrm{BC}\)</span> percentile, and <span class="math inline">\(\mathrm{BC}_{a}\)</span> confidence intervals can be calculated.</p>
<p>The <span class="math inline">\(\mathrm{BC}_{a}\)</span> interval requires an estimator of the acceleration coefficient <span class="math inline">\(a\)</span> which is a scaled jackknife estimate of the third moment of the estimator. In the context of clustered observations the delete-cluster jackknife should be used for estimation of <span class="math inline">\(a\)</span>.</p>
<p>Furthermore, on each bootstrap sample the cluster-robust standard errors can be calculated and used to compute bootstrap t-ratios, from which percentile-t confidence intervals can be calculated. tions as</p>
<p>The wild cluster bootstrap fixes the clusters and regressors, and generates the bootstrap observa-</p>
<p><span class="math display">\[
\begin{aligned}
\boldsymbol{Y}_{g}^{*} &amp;=\boldsymbol{X}_{g} \widehat{\beta}+\boldsymbol{e}_{g}^{*} \\
\boldsymbol{e}_{g}^{*} &amp;=\widehat{\boldsymbol{e}}_{i} \xi_{g}^{*}
\end{aligned}
\]</span></p>
<p>where <span class="math inline">\(\xi_{g}^{*}\)</span> is a scalar auxilary random variable as described in the previous section. Notice that <span class="math inline">\(\xi_{g}^{*}\)</span> is interacted with the entire vector of residuals from cluster <span class="math inline">\(g\)</span>. Cameron, Gelbach and Miller (2008) follow the recommendation of Davidson and Flachaire (2008) and use Rademacher random variables for <span class="math inline">\(\xi_{g}^{*}\)</span>.</p>
<p>For hypothesis testing, Cameron, Gelbach and Miller (2008) recommend the restricted wild cluster bootstrap. For tests of <span class="math inline">\(\mathbb{M}_{0}: r(\beta)=0\)</span> let <span class="math inline">\(\widetilde{\beta}\)</span> be a CLS or EMD estimator of <span class="math inline">\(\beta\)</span> subject to the restriction <span class="math inline">\(r(\widetilde{\beta})=\)</span> 0. Let <span class="math inline">\(\widetilde{\boldsymbol{e}}_{g}=\boldsymbol{Y}_{g}-\boldsymbol{X}_{g} \widetilde{\beta}\)</span> be the constrained cluster-level residuals. The restricted wild cluster bootstrap algorithm generates observations as</p>
<p><span class="math display">\[
\begin{aligned}
\boldsymbol{Y}_{g}^{*} &amp;=\boldsymbol{X}_{g} \widetilde{\beta}+\boldsymbol{e}_{g}^{*} \\
\boldsymbol{e}_{g}^{*} &amp;=\widetilde{\boldsymbol{e}}_{i} \xi_{g}^{*}
\end{aligned}
\]</span></p>
<p>On each bootstrap sample the test statistic for <span class="math inline">\(\mathbb{M}_{0}\)</span> (t-ratio, Wald, LR, or F) is applied. Since the bootstrap algorithm satisfies <span class="math inline">\(\mathbb{M}_{0}\)</span> these statistics are centered at the hypothesized value. p-values are then calculated conventionally and used to assess the significance of the test statistic.</p>
<p>There are several reasons why conventional asymptotic approximations may work poorly with clustered observations. First, while the sample size <span class="math inline">\(n\)</span> may be large, the effective sample size is the number of clusters <span class="math inline">\(G\)</span>. This is because when the dependence structure within each cluster is unconstrained the central limit theorem effectively treats each cluster as a single observation. Thus, if <span class="math inline">\(G\)</span> is small we should treat inference as a small sample problem. Second, cluster-robust covariance matrix estimation explicitly treats each cluster as a single observation. Consequently the accuracy of normal approximations to tratios and Wald statistics is more accurately viewed as a small sample distribution problem. Third, when cluster sizes <span class="math inline">\(n_{g}\)</span> are heterogeneous this means that the estimation problems just described also involve heterogeneous variances. Specifically, heterogeneous cluster sizes induces a high degree of effective heteroskedasticity (since the variance of a within-cluster sum is proportional to <span class="math inline">\(n_{g}\)</span> ). When <span class="math inline">\(G\)</span> is small this means that cluster-robust inference is similar to finite-sample inference with a small heteroskedastic sample. Fourth, interest often concerns treatment which is applied at the level of a cluster (such as the effect of tracking discussed in Section 4.21). If the number of treated clusters is small this is equivalent to estimation with a highly sparse dummy variable design in which case cluster-robust covariance matrix estimation can be unreliable.</p>
<p>These concerns suggest that conventional normal approximations may be poor in the context of clustered observations with a small number of groups <span class="math inline">\(G\)</span>, motivating the use of bootstrap methods. However, these concerns also can cause challenges with the accuracy of bootstrap approximations. When the number of clusters <span class="math inline">\(G\)</span> is small, the cluster sizes <span class="math inline">\(n_{g}\)</span> heterogeneous, or the number of treated clusters small, bootstrap methods may be inaccurate. In such cases inference should proceed cautiously.</p>
<p>To illustrate the use of the pairs cluster bootstrap, Table <span class="math inline">\(10.4\)</span> reports the estimates of the example from Section <span class="math inline">\(4.21\)</span> of the effect of tracking on testscores from Duflo, Dupas, and Kremer (2011). In addition to the asymptotic cluster standard error we report the cluster jackknife and cluster bootstrap standard errors as well as three percentile-type confidence intervals. We use 10,000 bootstrap replications. In this example the asymptotic, jackknife, and cluster bootstrap standard errors are identical, which reflects the good balance of this particular regression design.</p>
<p>Table 10.4: Comparison of Methods for Estimate of Effect of Tracking</p>
<table class="table">
<thead>
<tr class="header">
<th style="text-align: left;">Coefficient on Tracking</th>
<th><span class="math inline">\(0.138\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Asymptotic cluster s.e.</td>
<td><span class="math inline">\((0.078)\)</span></td>
</tr>
<tr class="even">
<td style="text-align: left;">Jackknife cluster s.e.</td>
<td><span class="math inline">\((0.078)\)</span></td>
</tr>
<tr class="odd">
<td style="text-align: left;">Cluster Bootstrap s.e.</td>
<td><span class="math inline">\((0.078)\)</span></td>
</tr>
<tr class="even">
<td style="text-align: left;"><span class="math inline">\(95 %\)</span> Percentile Interval</td>
<td><span class="math inline">\([-0.013,0.291]\)</span></td>
</tr>
<tr class="odd">
<td style="text-align: left;"><span class="math inline">\(95 % \mathrm{BC}\)</span> Percentile Interval</td>
<td><span class="math inline">\([-0.015,0.289]\)</span></td>
</tr>
<tr class="even">
<td style="text-align: left;"><span class="math inline">\(95 % \mathrm{BC}_{a}\)</span> Percentile Interval</td>
<td><span class="math inline">\([-0.018,0.286]\)</span></td>
</tr>
</tbody>
</table>
<p>In Stata, to obtain cluster bootstrap standard errors and confidence intervals use the options cluster (id) vce(bootstrap, reps <span class="math inline">\(\#\)</span> )) , where id is the cluster variable and # is the number of replications.</p>
</section>
<section id="technical-proofs" class="level2" data-number="11.31">
<h2 data-number="11.31" class="anchored" data-anchor-id="technical-proofs"><span class="header-section-number">11.31</span> Technical Proofs*</h2>
<p>Some of the asymptotic results are facilitated by the following convergence result.</p>
<p>Theorem 10.20 Marcinkiewicz WLLN If <span class="math inline">\(u_{i}\)</span> are independent and uniformly integrable, then for any <span class="math inline">\(r&gt;\)</span> 1 , as <span class="math inline">\(n \rightarrow \infty, n^{-r} \sum_{i=1}^{n}\left|u_{i}\right|^{r} \underset{p}{\longrightarrow} 0\)</span>.</p>
<p>Proof of Theorem <span class="math inline">\(10.20\)</span></p>
<p><span class="math display">\[
n^{-r} \sum_{i=1}^{n}\left|u_{i}\right|^{r} \leq\left(n^{-1} \max _{1 \leq i \leq n}\left|u_{i}\right|\right)^{r-1} \frac{1}{n} \sum_{i=1}^{n}\left|u_{i}\right| \underset{p}{\longrightarrow} 0
\]</span></p>
<p>by the WLLN, Theorem <span class="math inline">\(6.15\)</span>, and <span class="math inline">\(r&gt;1\)</span>.</p>
<p>Proof of Theorem 10.1 Fix <span class="math inline">\(\epsilon&gt;0\)</span>. Since <span class="math inline">\(Z_{n} \underset{p}{\longrightarrow} Z\)</span> there is an <span class="math inline">\(n\)</span> sufficiently large such that</p>
<p><span class="math display">\[
\mathbb{P}\left[\left\|Z_{n}-Z\right\|&gt;\epsilon\right]&lt;\epsilon .
\]</span></p>
<p>Since the event <span class="math inline">\(\left\|Z_{n}-Z\right\|&gt;\epsilon\)</span> is non-random under the conditional probability <span class="math inline">\(\mathbb{P}^{*}\)</span>, for such <span class="math inline">\(n\)</span>,</p>
<p><span class="math display">\[
\mathbb{P}^{*}\left[\left\|Z_{n}-Z\right\|&gt;\epsilon\right]=\left\{\begin{array}{cc}
0 &amp; \text { with probability exceeding } 1-\epsilon \\
1 &amp; \text { with probability less than } \epsilon .
\end{array}\right.
\]</span></p>
<p>Since <span class="math inline">\(\varepsilon\)</span> is arbitrary we conclude <span class="math inline">\(\mathbb{P}^{*}\left[\left\|Z_{n}-Z\right\|&gt;\epsilon\right] \underset{p}{\longrightarrow} 0\)</span> as required.</p>
<p>Proof of Theorem 10.2 Fix <span class="math inline">\(\epsilon&gt;0\)</span>. By Markov’s inequality (B.36), the facts (10.12) and (10.13), and finally the Marcinkiewicz WLLN (Theorem 10.20) with <span class="math inline">\(r=2\)</span> and <span class="math inline">\(u_{i}=\left\|Y_{i}\right\|\)</span>,</p>
<p><span class="math display">\[
\begin{aligned}
\mathbb{P}^{*}\left[\left\|\bar{Y}^{*}-\bar{Y}\right\|&gt;\epsilon\right] &amp; \leq \epsilon^{-2} \mathbb{E}^{*}\left\|\bar{Y}^{*}-\bar{Y}\right\|^{2} \\
&amp;=\epsilon^{-2} \operatorname{tr}\left(\operatorname{var}^{*}\left[\bar{Y}^{*}\right]\right) \\
&amp;=\epsilon^{-2} \operatorname{tr}\left(\frac{1}{n} \widehat{\Sigma}\right) \\
&amp; \leq \epsilon^{-2} n^{-2} \sum_{i=1}^{n} Y_{i}^{\prime} Y_{i} \\
&amp; \underset{p}{ } 0
\end{aligned}
\]</span></p>
<p>This establishes that <span class="math inline">\(\bar{Y}^{*}-\bar{Y} \underset{p^{*}}{\longrightarrow} 0\)</span>.</p>
<p>Since <span class="math inline">\(\bar{Y}-\mu \underset{p}{\longrightarrow} 0\)</span> by the WLLN, <span class="math inline">\(\bar{Y}-\mu \underset{p^{*}}{\longrightarrow} 0\)</span> by Theorem 10.1. Since <span class="math inline">\(\bar{Y}^{*}-\mu=\bar{Y}^{*}-\bar{Y}+\bar{Y}-\mu\)</span>, we deduce that <span class="math inline">\(\bar{Y}^{*}-\mu \underset{p^{*}}{ } 0\)</span>.</p>
<p>Proof of Theorem 10.4 We verify conditions for the multivariate Lindeberg CLT (Theorem 6.4). (We cannot use the Lindeberg-Lévy CLT because the conditional distribution depends on <span class="math inline">\(n\)</span>.) Conditional on <span class="math inline">\(F_{n}\)</span>, the bootstrap draws <span class="math inline">\(Y_{i}^{*}-\bar{Y}\)</span> are i.i.d. with mean 0 and covariance matrix <span class="math inline">\(\widehat{\Sigma}\)</span>. Set <span class="math inline">\(v_{n}^{2}=\lambda_{\min }(\widehat{\Sigma})\)</span>. Note that by the WLLN, <span class="math inline">\(v_{n}^{2} \underset{p}{\rightarrow} v^{2}=\lambda_{\min }(\Sigma)&gt;0\)</span>. Thus for <span class="math inline">\(n\)</span> sufficiently large, <span class="math inline">\(v_{n}^{2}&gt;0\)</span> with high probability. Fix <span class="math inline">\(\epsilon&gt;0\)</span>. Equation (6.2) equals</p>
<p><span class="math display">\[
\begin{aligned}
\frac{1}{n v_{n}^{2}} \sum_{i=1}^{n} \mathbb{E}^{*}\left[\left\|Y_{i}^{*}-\bar{Y}\right\|^{2} \mathbb{1}\left\{\left\|Y_{i}^{*}-\bar{Y}\right\|^{2} \geq \epsilon n v_{n}^{2}\right\}\right] &amp;=\frac{1}{v_{n}^{2}} \mathbb{E}^{*}\left[\left\|Y_{i}^{*}-\bar{Y}\right\|^{2} \mathbb{1}\left\{\left\|Y_{i}^{*}-\bar{Y}\right\|^{2} \geq \epsilon n v_{n}^{2}\right\}\right] \\
&amp; \leq \frac{1}{\epsilon n v_{n}^{4}} \mathbb{E}^{*}\left\|Y_{i}^{*}-\bar{Y}\right\|^{4} \\
&amp; \leq \frac{2^{4}}{\epsilon n v_{n}^{4}} \mathbb{E}^{*}\left\|Y_{i}^{*}\right\|^{4} \\
&amp;=\frac{2^{4}}{\epsilon n^{2} v_{n}^{4}} \sum_{i=1}^{n}\left\|Y_{i}\right\|^{4} \\
&amp; \longrightarrow 0 .
\end{aligned}
\]</span></p>
<p>The second inequality uses Minkowski’s inequality (B.34), Liapunov’s inequality (B.35), and the <span class="math inline">\(c_{r}\)</span> inequality (B.6). The following equality is <span class="math inline">\(\mathbb{E}^{*}\left\|Y_{i}^{*}\right\|^{4}=n^{-1} \sum_{i=1}^{n}\left\|Y_{i}\right\|^{4}\)</span>, which is similar to (10.10). The final convergence holds by the Marcinkiewicz WLLN (Theorem 10.20) with <span class="math inline">\(r=2\)</span> and <span class="math inline">\(u_{i}=\left\|Y_{i}\right\|^{2}\)</span>. The conditions for Theorem <span class="math inline">\(6.4\)</span> hold and we conclude</p>
<p><span class="math display">\[
\widehat{\Sigma}^{-1 / 2} \sqrt{n}\left(\bar{Y}^{*}-\bar{Y}\right) \underset{d^{*}}{\longrightarrow} \mathrm{N}(0, \boldsymbol{I}) .
\]</span></p>
<p>Since <span class="math inline">\(\widehat{\Sigma} \underset{p^{*}}{\longrightarrow} \Sigma\)</span> we deduce that <span class="math inline">\(\sqrt{n}\left(\bar{Y}^{*}-\bar{Y}\right) \underset{d^{*}}{\longrightarrow} \mathrm{N}(0, \Sigma)\)</span> as claimed.</p>
<p>Proof of Theorem <span class="math inline">\(10.10\)</span> For notational simplicity assume <span class="math inline">\(\theta\)</span> and <span class="math inline">\(\mu\)</span> are scalar. Set <span class="math inline">\(h_{i}=h\left(Y_{i}\right)\)</span>. The assumption that the <span class="math inline">\(p^{t h}\)</span> derivative of <span class="math inline">\(g(u)\)</span> is bounded implies <span class="math inline">\(\left|g^{(p)}(u)\right| \leq C\)</span> for some <span class="math inline">\(C&lt;\infty\)</span>. Taking a <span class="math inline">\(p^{\text {th }}\)</span> order Taylor series expansion</p>
<p><span class="math display">\[
\widehat{\theta}^{*}-\widehat{\theta}=g\left(\bar{h}^{*}\right)-g(\bar{h})=\sum_{j=1}^{p-1} \frac{g^{(j)}(\bar{h})}{j !}\left(\bar{h}^{*}-\bar{h}\right)^{j}+\frac{g^{(p)}\left(\zeta_{n}^{*}\right)}{p !}\left(\bar{h}^{*}-\bar{h}\right)^{p}
\]</span></p>
<p>where <span class="math inline">\(\zeta_{n}^{*}\)</span> lies between <span class="math inline">\(\bar{h}^{*}\)</span> and <span class="math inline">\(\bar{h}\)</span>. This implies</p>
<p><span class="math display">\[
\left|z_{n}^{*}\right|=\sqrt{n}\left|\widehat{\theta}^{*}-\widehat{\theta}\right| \leq \sqrt{n} \sum_{j=1}^{p} c_{j} \mid \bar{h}^{*}-\bar{h}^{j}
\]</span></p>
<p>where <span class="math inline">\(c_{j}=\left|g^{(j)}(\bar{h})\right| / j\)</span> ! for <span class="math inline">\(j&lt;p\)</span> and <span class="math inline">\(c_{p}=C / p\)</span> !. We find that the fourth central moment of the normalized bootstrap estimator <span class="math inline">\(Z_{n}^{*}=\sqrt{n}\left(\widehat{\theta}^{*}-\widehat{\theta}\right)\)</span> satisfies the bound</p>
<p><span class="math display">\[
\mathbb{E}^{*}\left[Z_{n}^{* 4}\right] \leq \sum_{r=4}^{4 p} a_{r} n^{2} \mathbb{E}^{*}\left|\bar{h}^{*}-\bar{h}\right|^{r}
\]</span></p>
<p>where the coefficients <span class="math inline">\(a_{r}\)</span> are products of the coefficients <span class="math inline">\(c_{j}\)</span> and hence each <span class="math inline">\(O_{p}(1)\)</span>. We see that <span class="math inline">\(\mathbb{E}^{*}\left[Z_{n}^{* 4}\right]=\)</span> <span class="math inline">\(O_{p}(1)\)</span> if <span class="math inline">\(n^{2} \mathbb{E}^{*}\left|\bar{h}^{*}-\bar{h}\right|^{r}=O_{p}(1)\)</span> for <span class="math inline">\(r=4, \ldots, 4 p\)</span>.</p>
<p>We show this holds for any <span class="math inline">\(r \geq 4\)</span> using Rosenthal’s inequality (B.50), which states that for each <span class="math inline">\(r\)</span> there is a constant <span class="math inline">\(R_{r}&lt;\infty\)</span> such that</p>
<p><span class="math display">\[
\begin{aligned}
n^{2} \mathbb{E}^{*}\left|\bar{h}^{*}-\bar{h}\right|^{r} &amp;=n^{2-r_{\mathbb{E}}}\left|\sum_{i=1}^{n}\left(h_{i}^{*}-\bar{h}\right)\right|^{r} \\
&amp; \leq n^{2-r} R_{r}\left\{\left(n \mathbb{E}^{*}\left(h_{i}^{*}-\bar{h}\right)^{2}\right)^{r / 2}+n \mathbb{E}^{*}\left|h_{i}^{*}-\bar{h}\right|^{r}\right\} \\
&amp;=R_{r}\left\{n^{2-r / 2} \widehat{\sigma}^{r}+\frac{1}{n^{r-2}} \sum_{i=1}^{n}\left|h_{i}-\bar{h}\right|^{r}\right\}
\end{aligned}
\]</span></p>
<p>Since <span class="math inline">\(\mathbb{E}\left[h_{i}^{2}\right]&lt;\infty, \widehat{\sigma}^{2}=O_{p}(1)\)</span>, so the first term in (10.36) is <span class="math inline">\(O_{p}(1)\)</span>. Also, by the Marcinkiewicz WLLN (Theorem 10.20), <span class="math inline">\(n^{-r / 2} \sum_{i=1}^{n}\left|h_{i}-\bar{h}\right|^{r}=o_{p}\)</span> (1) for any <span class="math inline">\(r \geq 1\)</span>, so the second term in (10.36) is <span class="math inline">\(o_{p}(1)\)</span> for <span class="math inline">\(r \geq 4\)</span>. Thus for all <span class="math inline">\(r \geq 4,(10.36)\)</span> is <span class="math inline">\(O_{p}(1)\)</span> and thus (10.35) is <span class="math inline">\(O_{p}(1)\)</span>. We deduce that <span class="math inline">\(Z_{n}^{*}\)</span> is uniformly square integrable, and the bootstrap estimate of variance is consistent.</p>
<p>This argument can be extended to vector-valued means and estimates.</p>
<p>Proof of Theorem 10.12 We show that <span class="math inline">\(\mathbb{E}^{*}\left\|Z_{n}^{* *}\right\|^{4}=O_{p}(1)\)</span>. Theorem <span class="math inline">\(6.13\)</span> shows that <span class="math inline">\(Z_{n}^{* *}\)</span> is uniformly square integrable. Since <span class="math inline">\(Z_{n}^{* *} \underset{d^{*}}{\longrightarrow} Z\)</span>, Theorem <span class="math inline">\(6.14\)</span> implies that <span class="math inline">\(\operatorname{var}\left[Z_{n}^{* *}\right] \rightarrow \operatorname{var}[Z]=V_{\beta}\)</span> as stated.</p>
<p>Set <span class="math inline">\(h_{i}=h\left(Y_{i}\right)\)</span>. Since <span class="math inline">\(\boldsymbol{G}(x)=\frac{\partial}{\partial x} g(x)^{\prime}\)</span> is continuous in a neighborhood of <span class="math inline">\(\mu\)</span>, there exists <span class="math inline">\(\eta&gt;0\)</span> and <span class="math inline">\(M&lt;\infty\)</span> such that <span class="math inline">\(\|x-\mu\| \leq 2 \eta\)</span> implies <span class="math inline">\(\operatorname{tr}\left(\boldsymbol{G}(x)^{\prime} \boldsymbol{G}(x)\right) \leq M\)</span>. By the WLLN and bootstrap WLLN there is an <span class="math inline">\(n\)</span> sufficiently large such that <span class="math inline">\(\left\|\bar{h}_{n}-\mu\right\| \leq \eta\)</span> and <span class="math inline">\(\left\|\bar{h}_{n}^{*}-\bar{h}_{n}\right\| \leq \eta\)</span> with probability exceeding <span class="math inline">\(1-\eta\)</span>. On this event, <span class="math inline">\(\left\|x-\bar{h}_{n}\right\| \leq \eta\)</span> implies <span class="math inline">\(\operatorname{tr}\left(\boldsymbol{G}(x)^{\prime} \boldsymbol{G}(x)\right) \leq M\)</span>. Using the mean-value theorem at a point <span class="math inline">\(\zeta_{n}^{*}\)</span> intermediate between <span class="math inline">\(\bar{h}_{n}^{*}\)</span> and <span class="math inline">\(\bar{h}_{n}\)</span></p>
<p><span class="math display">\[
\begin{aligned}
\left\|Z_{n}^{* *}\right\|^{4} \mathbb{1}\left\{\left\|\bar{h}_{n}^{*}-\bar{h}_{n}\right\| \leq \eta\right\} &amp; \leq n^{2}\left\|g\left(\bar{h}_{n}^{*}\right)-g\left(\bar{h}_{n}\right)\right\|^{4} \mathbb{1}\left\{\left\|\bar{h}_{n}^{*}-\bar{h}_{n}\right\| \leq \eta\right\} \\
&amp; \leq n^{2}\left\|\boldsymbol{G}\left(\zeta_{n}^{*}\right)^{\prime}\left(\bar{h}_{n}^{*}-\bar{h}_{n}\right)\right\|^{4} \\
&amp; \leq M^{2} n^{2}\left\|\bar{h}_{n}^{*}-\bar{h}_{n}\right\|^{4} .
\end{aligned}
\]</span></p>
<p>Then</p>
<p><span class="math display">\[
\begin{aligned}
\mathbb{E}^{*}\left\|Z_{n}^{* *}\right\|^{4} &amp; \leq \mathbb{E}^{*}\left[\left\|Z_{n}^{* *}\right\|^{4} \mathbb{1}\left\{\left\|\bar{h}_{n}^{*}-\bar{h}_{n}\right\| \leq \eta\right\}\right]+\tau_{n}^{4} \mathbb{E}^{*}\left[\mathbb{1}\left\{\left\|\bar{h}_{n}^{*}-\bar{h}_{n}\right\|&gt;\eta\right\}\right] \\
&amp; \leq M^{2} n^{2} \mathbb{E}^{*}\left\|\bar{h}_{n}^{*}-\bar{h}_{n}\right\|^{4}+\tau_{n}^{4} \mathbb{P}^{*}\left(\left\|\bar{h}_{n}^{*}-\bar{h}_{n}\right\|&gt;\eta\right) .
\end{aligned}
\]</span></p>
<p>In (10.17) we showed that the first term in (10.37) is <span class="math inline">\(O_{p}(1)\)</span> in the scalar case. The vector case follows by element-by-element expansion.</p>
<p>Now take the second term in (10.37). We apply Bernstein’s inequality for vectors (B.41). Note that <span class="math inline">\(\bar{h}_{n}^{*}-\bar{h}_{n}=n^{-1} \sum_{i=1}^{n} u_{i}^{*}\)</span> with <span class="math inline">\(u_{i}^{*}=h_{i}^{*}-\bar{h}_{n}\)</span> and <span class="math inline">\(j^{t h}\)</span> element <span class="math inline">\(u_{j i}^{*}=h_{j i}^{*}-\bar{h}_{j n}\)</span>. The <span class="math inline">\(u_{i}^{*}\)</span> are i.i.d., mean zero, <span class="math inline">\(\mathbb{E}^{*}\left[u_{j i}^{* 2}\right]=\widehat{\sigma}_{j}^{2}=O_{p}(1)\)</span>, and satisfy the bound <span class="math inline">\(\left|u_{j i}^{*}\right| \leq 2 \max _{i, j}\left|h_{j i}\right|=B_{n}\)</span>, say. Bernstein’s inequality states that</p>
<p><span class="math display">\[
\mathbb{P}^{*}\left[\left\|\bar{h}_{n}^{*}-\bar{h}_{n}\right\|&gt;\eta\right] \leq 2 m \exp \left(-n^{1 / 2} \frac{\eta^{2}}{2 m^{2} n^{-1 / 2} \max _{j} \widehat{\sigma}_{j}^{2}+2 m n^{-1 / 2} B_{n} \eta / 3}\right) .
\]</span></p>
<p>Theorem <span class="math inline">\(6.15\)</span> shows that <span class="math inline">\(n^{-1 / 2} B_{n}=o_{p}(1)\)</span>. Thus the expression in the denominator of the parentheses in (10.38) is <span class="math inline">\(o_{p}\)</span> (1) as <span class="math inline">\(n \rightarrow \infty\)</span>, . It follows that for <span class="math inline">\(n\)</span> sufficiently large (10.38) is <span class="math inline">\(O_{p}\left(\exp \left(-n^{1 / 2}\right)\right)\)</span>. Hence the second term in (10.37) is <span class="math inline">\(O_{p}\left(\exp \left(-n^{1 / 2}\right)\right) o_{p}\left(\exp \left(-n^{1 / 2}\right)\right)=o_{p}(1)\)</span> by the assumption on <span class="math inline">\(\tau_{n}\)</span>.</p>
<p>We have shown that the two terms in (10.37) are each <span class="math inline">\(O_{p}(1)\)</span>. This completes the proof.</p>
</section>
<section id="exercises" class="level2" data-number="11.32">
<h2 data-number="11.32" class="anchored" data-anchor-id="exercises"><span class="header-section-number">11.32</span> Exercises</h2>
<p>Exercise 10.1 Find the jackknife estimator of variance of the estimator <span class="math inline">\(\widehat{\mu}_{r}=n^{-1} \sum_{i=1}^{n} Y_{i}^{r}\)</span> for <span class="math inline">\(\mu_{r}=\mathbb{E}\left[Y_{i}^{r}\right]\)</span>.</p>
<p>Exercise 10.2 Show that if the jackknife estimator of variance of <span class="math inline">\(\widehat{\beta}\)</span> is <span class="math inline">\(\widehat{V}_{\widehat{\beta}}^{\text {jack }}\)</span>, then the jackknife estimator of variance of <span class="math inline">\(\widehat{\theta}=\boldsymbol{a}+\boldsymbol{C} \widehat{\beta}\)</span> is <span class="math inline">\(\widehat{\boldsymbol{V}}_{\widehat{\theta}}^{\mathrm{jack}}=\boldsymbol{C} \widehat{\boldsymbol{V}}_{\widehat{\beta}}^{\mathrm{jack}} \boldsymbol{C}^{\prime}\)</span>.</p>
<p>Exercise 10.3 A two-step estimator such as (12.49) is <span class="math inline">\(\widehat{\beta}=\left(\sum_{i=1}^{n} \widehat{W}_{i} \widehat{W}_{i}^{\prime}\right)^{-1}\left(\sum_{i=1}^{n} \widehat{W}_{i} Y_{i}\right)\)</span> where <span class="math inline">\(\widehat{W}_{i}=\widehat{A}^{\prime} Z_{i}\)</span> and <span class="math inline">\(\widehat{\boldsymbol{A}}=\left(\boldsymbol{Z}^{\prime} \boldsymbol{Z}\right)^{-1} \boldsymbol{Z}^{\prime} \boldsymbol{X}\)</span>. Describe how to construct the jackknife estimator of variance of <span class="math inline">\(\widehat{\beta}\)</span>.</p>
<p>Exercise 10.4 Show that if the bootstrap estimator of variance of <span class="math inline">\(\widehat{\beta}\)</span> is <span class="math inline">\(\widehat{V}_{\widehat{\beta}}^{\text {boot }}\)</span>, then the bootstrap estimator of variance of <span class="math inline">\(\widehat{\theta}=\boldsymbol{a}+\boldsymbol{C} \widehat{\beta}\)</span> is <span class="math inline">\(\widehat{\boldsymbol{V}}_{\widehat{\theta}}^{\text {boot }}=\boldsymbol{C} \widehat{\boldsymbol{V}}_{\widehat{\beta}}^{\text {boot }} \boldsymbol{C}^{\prime}\)</span>.</p>
<p>Exercise <span class="math inline">\(10.5\)</span> Show that if the percentile interval for <span class="math inline">\(\beta\)</span> is <span class="math inline">\([L, U]\)</span> then the percentile interval for <span class="math inline">\(a+c \beta\)</span> is <span class="math inline">\([a+c L, a+c U]\)</span>.</p>
<p>Exercise <span class="math inline">\(10.6\)</span> Consider the following bootstrap procedure. Using the nonparametric bootstrap, generate bootstrap samples, calculate the estimate <span class="math inline">\(\widehat{\theta}^{*}\)</span> on these samples and then calculate</p>
<p><span class="math display">\[
T^{*}=\left(\widehat{\theta}^{*}-\widehat{\theta}\right) / s(\widehat{\theta}),
\]</span></p>
<p>where <span class="math inline">\(s(\hat{\theta})\)</span> is the standard error in the original data. Let <span class="math inline">\(q_{\alpha / 2}^{*}\)</span> and <span class="math inline">\(q_{1-\alpha / 2}^{*}\)</span> denote the <span class="math inline">\(\alpha / 2^{t h}\)</span> and <span class="math inline">\(1-\alpha / 2^{t h}\)</span> quantiles of <span class="math inline">\(T^{*}\)</span>, and define the bootstrap confidence interval</p>
<p><span class="math display">\[
C=\left[\widehat{\theta}+s(\widehat{\theta}) q_{\alpha / 2}^{*}, \quad \widehat{\theta}+s(\widehat{\theta}) q_{1-\alpha / 2}^{*}\right] .
\]</span></p>
<p>Show that <span class="math inline">\(C\)</span> exactly equals the percentile interval. Exercise <span class="math inline">\(10.7\)</span> Prove Theorem 10.6.</p>
<p>Exercise <span class="math inline">\(10.8\)</span> Prove Theorem 10.7.</p>
<p>Exercise <span class="math inline">\(10.9\)</span> Prove Theorem 10.8.</p>
<p>Exercise <span class="math inline">\(10.10\)</span> Let <span class="math inline">\(Y_{i}\)</span> be i.i.d., <span class="math inline">\(\mu=\mathbb{E}[Y]&gt;0\)</span>, and <span class="math inline">\(\theta=\mu^{-1}\)</span>. Let <span class="math inline">\(\widehat{\mu}=\bar{Y}_{n}\)</span> be the sample mean and <span class="math inline">\(\widehat{\theta}=\widehat{\mu}^{-1}\)</span>.</p>
<ol type="a">
<li><p>Is <span class="math inline">\(\hat{\theta}\)</span> unbiased for <span class="math inline">\(\theta\)</span> ?</p></li>
<li><p>If <span class="math inline">\(\widehat{\theta}\)</span> is biased, can you determine the direction of the bias <span class="math inline">\(\mathbb{E}[\widehat{\theta}-\theta]\)</span> (up or down)?</p></li>
<li><p>Is the percentile interval appropriate in this context for confidence interval construction?</p></li>
</ol>
<p>Exercise <span class="math inline">\(10.11\)</span> Consider the following bootstrap procedure for a regression of <span class="math inline">\(Y\)</span> on <span class="math inline">\(X\)</span>. Let <span class="math inline">\(\widehat{\beta}\)</span> denote the OLS estimator and <span class="math inline">\(\widehat{e}_{i}=Y_{i}-X_{i}^{\prime} \widehat{\beta}\)</span> the OLS residuals.</p>
<ol type="a">
<li><p>Draw a random vector <span class="math inline">\(\left(X^{*}, e^{*}\right)\)</span> from the pair <span class="math inline">\(\left\{\left(X_{i}, \widehat{e}_{i}\right): i=1, \ldots, n\right\}\)</span>. That is, draw a random integer <span class="math inline">\(i^{\prime}\)</span> from <span class="math inline">\([1,2, \ldots, n]\)</span>, and set <span class="math inline">\(X^{*}=X_{i^{\prime}}\)</span> and <span class="math inline">\(e^{*}=\widehat{e}_{i^{\prime}}\)</span>. Set <span class="math inline">\(Y^{*}=X^{* \prime} \widehat{\beta}+e^{*}\)</span>. Draw (with replacement) <span class="math inline">\(n\)</span> such vectors, creating a random bootstrap data set <span class="math inline">\(\left(\boldsymbol{Y}^{*}, \boldsymbol{X}^{*}\right)\)</span>.</p></li>
<li><p>Regress <span class="math inline">\(\boldsymbol{Y}^{*}\)</span> on <span class="math inline">\(\boldsymbol{X}^{*}\)</span>, yielding OLS estimator <span class="math inline">\(\widehat{\beta}^{*}\)</span> and any other statistic of interest.</p></li>
</ol>
<p>Show that this bootstrap procedure is (numerically) identical to the nonparametric bootstrap.</p>
<p>Exercise <span class="math inline">\(10.12\)</span> Take <span class="math inline">\(p^{*}\)</span> as defined in (10.22) for the BC percentile interval. Show that it is invariant to replacing <span class="math inline">\(\theta\)</span> with <span class="math inline">\(g(\theta)\)</span> for any strictly monotonically increasing transformation <span class="math inline">\(g(\theta)\)</span>. Does this extend to <span class="math inline">\(z_{0}^{*}\)</span> as defined in (10.23)?</p>
<p>Exercise <span class="math inline">\(10.13\)</span> Show that if the percentile-t interval for <span class="math inline">\(\beta\)</span> is <span class="math inline">\([L, U]\)</span> then the percentile-t interval for <span class="math inline">\(a+c \beta\)</span> is <span class="math inline">\([a+b L, a+b U]\)</span>.</p>
<p>Exercise 10.14 You want to test <span class="math inline">\(\mathbb{M}_{0}: \theta=0\)</span> against <span class="math inline">\(\mathbb{M}_{1}: \theta&gt;0\)</span>. The test for <span class="math inline">\(\mathbb{M}_{0}\)</span> is to reject if <span class="math inline">\(T_{n}=\widehat{\theta} / s(\widehat{\theta})&gt;c\)</span> where <span class="math inline">\(c\)</span> is picked so that Type I error is <span class="math inline">\(\alpha\)</span>. You do this as follows. Using the nonparametric bootstrap, you generate bootstrap samples, calculate the estimates <span class="math inline">\(\widehat{\theta}^{*}\)</span> on these samples and then calculate <span class="math inline">\(T^{*}=\)</span> <span class="math inline">\(\widehat{\theta}^{*} / s\left(\widehat{\theta}^{*}\right)\)</span>. Let <span class="math inline">\(q_{1-\alpha}^{*}\)</span> denote the <span class="math inline">\(1-\alpha^{t h}\)</span> quantile of <span class="math inline">\(T^{*}\)</span>. You replace <span class="math inline">\(c\)</span> with <span class="math inline">\(q_{1-\alpha}^{*}\)</span>, and thus reject <span class="math inline">\(\mathbb{H}_{0}\)</span> if <span class="math inline">\(T_{n}=\widehat{\theta} / s(\widehat{\theta})&gt;q_{1-\alpha}^{*}\)</span>. What is wrong with this procedure?</p>
<p>Exercise 10.15 Suppose that in an application, <span class="math inline">\(\widehat{\theta}=1.2\)</span> and <span class="math inline">\(s(\widehat{\theta})=0.2\)</span>. Using the nonparametric bootstrap, 1000 samples are generated from the bootstrap distribution, and <span class="math inline">\(\widehat{\theta}^{*}\)</span> is calculated on each sample. The <span class="math inline">\(\widehat{\theta}^{*}\)</span> are sorted, and the <span class="math inline">\(0.025^{t h}\)</span> and <span class="math inline">\(0.975^{t h}\)</span> quantiles of the <span class="math inline">\(\widehat{\theta}^{*}\)</span> are <span class="math inline">\(.75\)</span> and <span class="math inline">\(1.3\)</span>, respectively.</p>
<ol type="a">
<li><p>Report the <span class="math inline">\(95 %\)</span> percentile interval for <span class="math inline">\(\theta\)</span>.</p></li>
<li><p>With the given information, can you calculate the 95% BC percentile interval or percentile-t interval for <span class="math inline">\(\theta\)</span> ?</p></li>
</ol>
<p>Exercise 10.16 Take the normal regression model <span class="math inline">\(Y=X^{\prime} \beta+e\)</span> with <span class="math inline">\(e \mid X \sim \mathrm{N}\left(0, \sigma^{2}\right)\)</span> where we know the MLE equals the least squares estimators <span class="math inline">\(\widehat{\beta}\)</span> and <span class="math inline">\(\widehat{\sigma}^{2}\)</span>.</p>
<ol type="a">
<li><p>Describe the parametric regression bootstrap for this model. Show that the conditional distribution of the bootstrap observations is <span class="math inline">\(Y_{i}^{*} \mid F_{n} \sim \mathrm{N}\left(X_{i}^{\prime} \widehat{\beta}, \widehat{\sigma}^{2}\right)\)</span>. (b) Show that the distribution of the bootstrap least squares estimator is <span class="math inline">\(\widehat{\beta}^{*} \mid F_{n} \sim \mathrm{N}\left(\widehat{\beta},\left(\boldsymbol{X}^{\prime} \boldsymbol{X}\right)^{-1} \widehat{\sigma}^{2}\right)\)</span>.</p></li>
<li><p>Show that the distribution of the bootstrap t-ratio with a homoskedastic standard error is <span class="math inline">\(T^{*} \sim\)</span> <span class="math inline">\(t_{n-k}\)</span>.</p></li>
</ol>
<p>Exercise <span class="math inline">\(10.17\)</span> Consider the model <span class="math inline">\(Y=X^{\prime} \beta+e\)</span> with <span class="math inline">\(\mathbb{E}[e \mid X]=0, Y\)</span> scalar, and <span class="math inline">\(X\)</span> a <span class="math inline">\(k\)</span> vector. You have a random sample <span class="math inline">\(\left(Y_{i}, X_{i}: i=1, \ldots, n\right)\)</span>. You are interested in estimating the regression function <span class="math inline">\(m(x)=\)</span> <span class="math inline">\(\mathbb{E}[Y \mid X=x]\)</span> at a fixed vector <span class="math inline">\(x\)</span> and constructing a <span class="math inline">\(95 %\)</span> confidence interval.</p>
<ol type="a">
<li><p>Write down the standard estimator and asymptotic confidence interval for <span class="math inline">\(m(x)\)</span>.</p></li>
<li><p>Describe the percentile bootstrap confidence interval for <span class="math inline">\(m(x)\)</span>.</p></li>
<li><p>Describe the percentile-t bootstrap confidence interval for <span class="math inline">\(m(x)\)</span>.</p></li>
</ol>
<p>Exercise 10.18 The observed data is <span class="math inline">\(\left\{Y_{i}, X_{i}\right\} \in \mathbb{R} \times \mathbb{R}^{k}, k&gt;1, i=1, \ldots, n\)</span>. Take the model <span class="math inline">\(Y=X^{\prime} \beta+e\)</span> with <span class="math inline">\(\mathbb{E}[X e]=0 .\)</span></p>
<ol type="a">
<li><p>Write down an estimator for <span class="math inline">\(\mu_{3}=\mathbb{E}\left[e^{3}\right]\)</span>.</p></li>
<li><p>Explain how to use the percentile method to construct a 90% confidence interval for <span class="math inline">\(\mu_{3}\)</span> in this specific model.</p></li>
</ol>
<p>Exercise <span class="math inline">\(10.19\)</span> Take the model <span class="math inline">\(Y=X^{\prime} \beta+e\)</span> with <span class="math inline">\(\mathbb{E}[X e]=0\)</span>. Describe the bootstrap percentile confidence interval for <span class="math inline">\(\sigma^{2}=\mathbb{E}\left[e^{2}\right]\)</span>.</p>
<p>Exercise 10.20 The model is <span class="math inline">\(Y=X_{1}^{\prime} \beta_{1}+X_{2}^{\prime} \beta_{2}+e\)</span> with <span class="math inline">\(\mathbb{E}[X e]=0\)</span> and <span class="math inline">\(X_{2}\)</span> scalar. Describe how to test <span class="math inline">\(\mathbb{H}_{0}: \beta_{2}=0\)</span> against <span class="math inline">\(\mathbb{H}_{1}: \beta_{2} \neq 0\)</span> using the nonparametric bootstrap.</p>
<p>Exercise 10.21 The model is <span class="math inline">\(Y=X_{1}^{\prime} \beta_{1}+X_{2}^{\prime} \beta_{2}+e\)</span> with <span class="math inline">\(\mathbb{E}[X e]=0\)</span>, and both <span class="math inline">\(X_{1}\)</span> and <span class="math inline">\(X_{2} k \times 1\)</span>. Describe how to test <span class="math inline">\(\mathbb{M}_{0}: \beta_{1}=\beta_{2}\)</span> against <span class="math inline">\(\mathbb{M}_{1}: \beta_{1} \neq \beta_{2}\)</span> using the nonparametric bootstrap.</p>
<p>Exercise 10.22 Suppose a Ph.D.&nbsp;student has a sample <span class="math inline">\(\left(Y_{i}, X_{i}, Z_{i}: i=1, \ldots, n\right)\)</span> and estimates by OLS the equation <span class="math inline">\(Y=Z \alpha+X^{\prime} \beta+e\)</span> where <span class="math inline">\(\alpha\)</span> is the coefficient of interest. She is interested in testing <span class="math inline">\(\mathbb{H}_{0}: \alpha=0\)</span> against <span class="math inline">\(\mathbb{H}_{1}: \alpha \neq 0\)</span>. She obtains <span class="math inline">\(\widehat{\alpha}=2.0\)</span> with standard error <span class="math inline">\(s(\widehat{\alpha})=1.0\)</span> so the value of the t-ratio for <span class="math inline">\(\mathbb{H}_{0}\)</span> is <span class="math inline">\(T=\widehat{\alpha} / s(\widehat{\alpha})=2.0\)</span>. To assess significance, the student decides to use the bootstrap. She uses the following algorithm</p>
<ol type="1">
<li><p>Samples <span class="math inline">\(\left(Y_{i}^{*}, X_{i}^{*}, Z_{i}^{*}\right)\)</span> randomly from the observations. (Random sampling with replacement). Creates a random sample with <span class="math inline">\(n\)</span> observations.</p></li>
<li><p>On this pseudo-sample, estimates the equation <span class="math inline">\(Y_{i}^{*}=Z_{i}^{*} \alpha+X_{i}^{* \prime} \beta+e_{i}^{*}\)</span> by OLS and computes standard errors, including <span class="math inline">\(s\left(\widehat{\alpha}^{*}\right)\)</span>. The t-ratio for <span class="math inline">\(\mathbb{H}_{0}, T^{*}=\widehat{\alpha}^{*} / s\left(\widehat{\alpha}^{*}\right)\)</span> is computed and stored.</p></li>
<li><p>This is repeated <span class="math inline">\(B=10,000\)</span> times.</p></li>
<li><p>The <span class="math inline">\(0.95^{t h}\)</span> empirical quantile <span class="math inline">\(q_{.95}^{*}=3.5\)</span> of the bootstrap absolute t-ratios <span class="math inline">\(\left|T^{*}\right|\)</span> is computed.</p></li>
<li><p>The student notes that while <span class="math inline">\(|T|=2&gt;1.96\)</span> (and thus an asymptotic <span class="math inline">\(5 %\)</span> size test rejects <span class="math inline">\(\mathbb{M}_{0}\)</span> ), <span class="math inline">\(|T|=\)</span> <span class="math inline">\(2&lt;q_{.95}^{*}=3.5\)</span> and thus the bootstrap test does not reject <span class="math inline">\(\mathbb{M}_{0}\)</span>. As the bootstrap is more reliable, the student concludes that <span class="math inline">\(\mathbb{M}_{0}\)</span> cannot be rejected in favor of <span class="math inline">\(\mathbb{H}_{1}\)</span>. Question: Do you agree with the student’s method and reasoning? Do you see an error in her method?</p></li>
</ol>
<p>Exercise 10.23 Take the model <span class="math inline">\(Y=X_{1} \beta_{1}+X_{2} \beta_{2}+e\)</span> with <span class="math inline">\(\mathbb{E}[X e]=0\)</span> and scalar <span class="math inline">\(X_{1}\)</span> and <span class="math inline">\(X_{2}\)</span>. The parameter of interest is <span class="math inline">\(\theta=\beta_{1} \beta_{2}\)</span>. Show how to construct a confidence interval for <span class="math inline">\(\theta\)</span> using the following three methods.</p>
<ol type="a">
<li><p>Asymptotic Theory.</p></li>
<li><p>Percentile Bootstrap.</p></li>
<li><p>Percentile-t Bootstrap.</p></li>
</ol>
<p>Your answer should be specific to this problem, not general.</p>
<p>Exercise 10.24 Take the model <span class="math inline">\(Y=X_{1} \beta_{1}+X_{2} \beta_{2}+e\)</span> with i.i.d observations, <span class="math inline">\(\mathbb{E}[X e]=0\)</span> and scalar <span class="math inline">\(X_{1}\)</span> and <span class="math inline">\(X_{2}\)</span>. Describe how you would construct the percentile-t bootstrap confidence interval for <span class="math inline">\(\theta=\beta_{1} / \beta_{2}\)</span>.</p>
<p>Exercise 10.25 The model is i.i.d. data, <span class="math inline">\(i=1, \ldots, n, Y=X^{\prime} \beta+e\)</span> and <span class="math inline">\(\mathbb{E}[e \mid X]=0\)</span>. Does the presence of conditional heteroskedasticity invalidate the application of the nonparametric bootstrap? Explain.</p>
<p>Exercise 10.26 The RESET specification test for nonlinearity in a random sample (due to Ramsey (1969)) is the following. The null hypothesis is a linear regression <span class="math inline">\(Y=X^{\prime} \beta+e\)</span> with <span class="math inline">\(\mathbb{E}[e \mid X]=0\)</span>. The parameter <span class="math inline">\(\beta\)</span> is estimated by OLS yielding predicted values <span class="math inline">\(\widehat{Y}_{i}\)</span>. Then a second-stage least squares regression is estimated including both <span class="math inline">\(X_{i}\)</span> and <span class="math inline">\(\widehat{Y}_{i}\)</span></p>
<p><span class="math display">\[
Y_{i}=X_{i}^{\prime} \widetilde{\beta}+\left(\widehat{Y}_{i}\right)^{2} \widetilde{\gamma}+\widetilde{e}_{i}
\]</span></p>
<p>The RESET test statistic <span class="math inline">\(R\)</span> is the squared t-ratio on <span class="math inline">\(\widetilde{\gamma}\)</span>.</p>
<p>A colleague suggests obtaining the critical value for the test using the bootstrap. He proposes the following bootstrap implementation.</p>
<ul>
<li><p>Draw <span class="math inline">\(n\)</span> observations <span class="math inline">\(\left(Y_{i}^{*}, X_{i}^{*}\right)\)</span> randomly from the observed sample pairs <span class="math inline">\(\left(Y_{i}, X_{i}\right)\)</span> to create a bootstrap sample.</p></li>
<li><p>Compute the statistic <span class="math inline">\(R^{*}\)</span> on this bootstrap sample as described above.</p></li>
<li><p>Repeat this <span class="math inline">\(B\)</span> times. Sort the bootstrap statistics <span class="math inline">\(R^{*}\)</span>, take the <span class="math inline">\(0.95^{t h}\)</span> quantile and use this as the critical value.</p></li>
<li><p>Reject the null hypothesis if <span class="math inline">\(R\)</span> exceeds this critical value, otherwise do not reject.</p></li>
</ul>
<p>Is this procedure a correct implementation of the bootstrap in this context? If not, propose a modification.</p>
<p>Exercise 10.27 The model is <span class="math inline">\(Y=X^{\prime} \beta+e\)</span> with <span class="math inline">\(\mathbb{E}[X e] \neq 0\)</span>. We know that in this case, the least squares estimator may be biased for the parameter <span class="math inline">\(\beta\)</span>. We also know that the nonparametric BC percentile interval is (generally) a good method for confidence interval construction in the presence of bias. Explain whether or not you expect the BC percentile interval applied to the least squares estimator will have accurate coverage in this context.</p>
<p>Exercise 10.28 In Exercise 9.26 you estimated a cost function for 145 electric companies and tested the restriction <span class="math inline">\(\theta=\beta_{3}+\beta_{4}+\beta_{5}=1\)</span>. (a) Estimate the regression by unrestricted least squares and report standard errors calculated by asymptotic, jackknife and the bootstrap.</p>
<ol start="2" type="a">
<li><p>Estimate <span class="math inline">\(\theta=\beta_{3}+\beta_{4}+\beta_{5}\)</span> and report standard errors calculated by asymptotic, jackknife and the bootstrap.</p></li>
<li><p>Report confidence intervals for <span class="math inline">\(\theta\)</span> using the percentile and <span class="math inline">\(\mathrm{BC}_{a}\)</span> methods.</p></li>
</ol>
<p>Exercise 10.29 In Exercise 9.27 you estimated the Mankiw, Romer, and Weil (1992) unrestricted regression. Let <span class="math inline">\(\theta\)</span> be the sum of the second, third, and fourth coefficients.</p>
<ol type="a">
<li><p>Estimate the regression by unrestricted least squares and report standard errors calculated by asymptotic, jackknife and the bootstrap.</p></li>
<li><p>Estimate <span class="math inline">\(\theta\)</span> and report standard errors calculated by asymptotic, jackknife and the bootstrap.</p></li>
<li><p>Report confidence intervals for <span class="math inline">\(\theta\)</span> using the percentile and BC methods.</p></li>
</ol>
<p>Exercise 10.30 In Exercise <span class="math inline">\(7.28\)</span> you estimated a wage regression with the cps09mar dataset and the subsample of white Male Hispanics. Further restrict the sample to those never-married and live in the Midwest region. (This sample has 99 observations.) As in subquestion (b) let <span class="math inline">\(\theta\)</span> be the ratio of the return to one year of education to the return of one year of experience.</p>
<ol type="a">
<li><p>Estimate <span class="math inline">\(\theta\)</span> and report standard errors calculated by asymptotic, jackknife and the bootstrap.</p></li>
<li><p>Explain the discrepancy between the standard errors.</p></li>
<li><p>Report confidence intervals for <span class="math inline">\(\theta\)</span> using the BC percentile method.</p></li>
</ol>
<p>Exercise 10.31 In Exercise <span class="math inline">\(4.26\)</span> you extended the work from Duflo, Dupas, and Kremer (2011). Repeat that regression, now calculating the standard error by cluster bootstrap. Report a <span class="math inline">\(\mathrm{BC}_{a}\)</span> confidence interval for each coefficient.</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    setTimeout(function() {
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      let href = ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const cites = ref.parentNode.getAttribute('data-cites').split(' ');
    tippyHover(ref, function() {
      var popup = window.document.createElement('div');
      cites.forEach(function(cite) {
        var citeDiv = window.document.createElement('div');
        citeDiv.classList.add('hanging-indent');
        citeDiv.classList.add('csl-entry');
        var biblioDiv = window.document.getElementById('ref-' + cite);
        if (biblioDiv) {
          citeDiv.innerHTML = biblioDiv.innerHTML;
        }
        popup.appendChild(citeDiv);
      });
      return popup.innerHTML;
    });
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./chpt09-hypothesit-test.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Hypothesis Testing</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./part03-MEQ.html" class="pagination-link">
        <span class="nav-page-text">多方程模型</span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->



</body></html>
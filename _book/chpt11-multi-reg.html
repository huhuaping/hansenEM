<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.0.36">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Hansen中高级计量体系 - 11&nbsp; Multivariate Regression</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
span.underline{text-decoration: underline;}
div.column{display: inline-block; vertical-align: top; width: 50%;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./chpt12-iv.html" rel="next">
<link href="./part03-MEQ.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
    <div class="container-fluid d-flex justify-content-between">
      <h1 class="quarto-secondary-nav-title"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Multivariate Regression</span></h1>
      <button type="button" class="quarto-btn-toggle btn" aria-label="Show secondary navigation">
        <i class="bi bi-chevron-right"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Hansen中高级计量体系</a> 
        <div class="sidebar-tools-main">
    <a href="https://github.com/huhuaping/hansenEM/" title="Source Code" class="sidebar-tool px-1"><i class="bi bi-github"></i></a>
</div>
    </div>
      </div>
      <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
      </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">前言</a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./participate.html" class="sidebar-item-text sidebar-link">如何参与？</a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chpt01-intro.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./part01-reg.html" class="sidebar-item-text sidebar-link">回归</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chpt02-ce.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Conditional Expectation and Projection</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chpt03-algebra.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">The Algebra of Least Squares</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chpt04-lsr.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Least Squares Regression</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chpt05-normal-reg.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Normal Regression</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./part02-LSM.html" class="sidebar-item-text sidebar-link">大样本方法</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chpt06-review.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">A Review of Large Sample Asymptotics</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chpt07-asymptotic-ls.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Asymptotic Theory for Least Squares</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chpt08-restricted-est.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Restricted Estimation</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chpt09-hypothesit-test.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Hypothesis Testing</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chpt10-resample-method.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Resampling Methods</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./part03-MEQ.html" class="sidebar-item-text sidebar-link">多方程模型</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chpt11-multi-reg.html" class="sidebar-item-text sidebar-link active"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Multivariate Regression</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chpt12-iv.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Instrumental Variables</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chpt13-gmm.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Generalized Method of Moments</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./summary.html" class="sidebar-item-text sidebar-link">Summary</a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">References</a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" aria-expanded="true">Appendices</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./a1-notation.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">A</span>&nbsp; <span class="chapter-title">附录a1</span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction"> <span class="header-section-number">11.1</span> Introduction</a></li>
  <li><a href="#regression-systems" id="toc-regression-systems" class="nav-link" data-scroll-target="#regression-systems"> <span class="header-section-number">11.2</span> Regression Systems</a></li>
  <li><a href="#least-squares-estimator" id="toc-least-squares-estimator" class="nav-link" data-scroll-target="#least-squares-estimator"> <span class="header-section-number">11.3</span> Least Squares Estimator</a></li>
  <li><a href="#expectation-and-variance-of-systems-least-squares" id="toc-expectation-and-variance-of-systems-least-squares" class="nav-link" data-scroll-target="#expectation-and-variance-of-systems-least-squares"> <span class="header-section-number">11.4</span> Expectation and Variance of Systems Least Squares</a></li>
  <li><a href="#asymptotic-distribution" id="toc-asymptotic-distribution" class="nav-link" data-scroll-target="#asymptotic-distribution"> <span class="header-section-number">11.5</span> Asymptotic Distribution</a></li>
  <li><a href="#covariance-matrix-estimation" id="toc-covariance-matrix-estimation" class="nav-link" data-scroll-target="#covariance-matrix-estimation"> <span class="header-section-number">11.6</span> Covariance Matrix Estimation</a></li>
  <li><a href="#seemingly-unrelated-regression" id="toc-seemingly-unrelated-regression" class="nav-link" data-scroll-target="#seemingly-unrelated-regression"> <span class="header-section-number">11.7</span> Seemingly Unrelated Regression</a></li>
  <li><a href="#equivalence-of-sur-and-least-squares" id="toc-equivalence-of-sur-and-least-squares" class="nav-link" data-scroll-target="#equivalence-of-sur-and-least-squares"> <span class="header-section-number">11.8</span> Equivalence of SUR and Least Squares</a></li>
  <li><a href="#maximum-likelihood-estimator" id="toc-maximum-likelihood-estimator" class="nav-link" data-scroll-target="#maximum-likelihood-estimator"> <span class="header-section-number">11.9</span> Maximum Likelihood Estimator</a></li>
  <li><a href="#restricted-estimation" id="toc-restricted-estimation" class="nav-link" data-scroll-target="#restricted-estimation"> <span class="header-section-number">11.10</span> Restricted Estimation</a></li>
  <li><a href="#reduced-rank-regression" id="toc-reduced-rank-regression" class="nav-link" data-scroll-target="#reduced-rank-regression"> <span class="header-section-number">11.11</span> Reduced Rank Regression</a></li>
  <li><a href="#principal-component-analysis" id="toc-principal-component-analysis" class="nav-link" data-scroll-target="#principal-component-analysis"> <span class="header-section-number">11.12</span> Principal Component Analysis</a></li>
  <li><a href="#factor-models" id="toc-factor-models" class="nav-link" data-scroll-target="#factor-models"> <span class="header-section-number">11.13</span> Factor Models</a></li>
  <li><a href="#approximate-factor-models" id="toc-approximate-factor-models" class="nav-link" data-scroll-target="#approximate-factor-models"> <span class="header-section-number">11.14</span> Approximate Factor Models</a></li>
  <li><a href="#factor-models-with-additional-regressors" id="toc-factor-models-with-additional-regressors" class="nav-link" data-scroll-target="#factor-models-with-additional-regressors"> <span class="header-section-number">11.15</span> Factor Models with Additional Regressors</a></li>
  <li><a href="#factor-augmented-regression" id="toc-factor-augmented-regression" class="nav-link" data-scroll-target="#factor-augmented-regression"> <span class="header-section-number">11.16</span> Factor-Augmented Regression</a></li>
  <li><a href="#multivariate-normal" id="toc-multivariate-normal" class="nav-link" data-scroll-target="#multivariate-normal"> <span class="header-section-number">11.17</span> Multivariate Normal*</a></li>
  <li><a href="#exercises" id="toc-exercises" class="nav-link" data-scroll-target="#exercises"> <span class="header-section-number">11.18</span> Exercises</a></li>
  </ul>
<div class="toc-actions"><div><i class="bi bi-github"></i></div><div class="action-links"><p><a href="https://github.com/huhuaping/hansenEM/edit/master/chpt11-multi-reg.qmd" class="toc-action">Edit this page</a></p></div></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title d-none d-lg-block"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Multivariate Regression</span></h1>
</div>



<div class="quarto-title-meta">

    
    
  </div>
  

</header>

<section id="introduction" class="level2" data-number="11.1">
<h2 data-number="11.1" class="anchored" data-anchor-id="introduction"><span class="header-section-number">11.1</span> Introduction</h2>
<p>Multivariate regression is a system of regression equations. Multivariate regression is used as reduced form models for instrumental variable estimation (Chaper 12), vector autoregressions (Chapter 15), demand systems (demand for multiple goods), and other contexts.</p>
<p>Multivariate regression is also called by the name systems of regression equations. Closely related is the method of Seemingly Unrelated Regressions (SUR) introduced in Section 11.7.</p>
<p>Most of the tools of single equation regression generalize to multivariate regression. A major difference is a new set of notation to handle matrix estimators.</p>
</section>
<section id="regression-systems" class="level2" data-number="11.2">
<h2 data-number="11.2" class="anchored" data-anchor-id="regression-systems"><span class="header-section-number">11.2</span> Regression Systems</h2>
<p>A univariate linear regression equation equals <span class="math inline">\(Y=X^{\prime} \beta+e\)</span> where <span class="math inline">\(Y\)</span> is scalar and <span class="math inline">\(X\)</span> is a vector. Multivariate regression is a system of <span class="math inline">\(m\)</span> linear regressions, and equals</p>
<p><span class="math display">\[
Y_{j}=X_{j}^{\prime} \beta_{j}+e_{j}
\]</span></p>
<p>for <span class="math inline">\(j=1, \ldots, m\)</span>. Here we use the subscript <span class="math inline">\(j\)</span> to denote the <span class="math inline">\(j^{t h}\)</span> dependent variable, not the <span class="math inline">\(i^{t h}\)</span> individual. As an example, <span class="math inline">\(Y_{j}\)</span> could be expenditures by a household on good category <span class="math inline">\(j\)</span> (e.g., food, housing, transportation, clothing, recreation). The regressor vectors <span class="math inline">\(X_{j}\)</span> are <span class="math inline">\(k_{j} \times 1\)</span> and <span class="math inline">\(e_{j}\)</span> is an error. The coefficient vectors <span class="math inline">\(\beta_{j}\)</span> are <span class="math inline">\(k_{j} \times 1\)</span>. The total number of coefficients are <span class="math inline">\(\bar{k}=\sum_{j=1}^{m} k_{j}\)</span>. The regressors can be common across <span class="math inline">\(j\)</span> or can vary across <span class="math inline">\(j\)</span>. In the household expenditure example the regressors <span class="math inline">\(X_{j}\)</span> are typically common across <span class="math inline">\(j\)</span>, and include variables such as household income, number and ages of family members, and demographic characteristics. The regression system specializes to univariate regression when <span class="math inline">\(m=1\)</span>.</p>
<p>Define the <span class="math inline">\(m \times 1\)</span> error vector <span class="math inline">\(e=\left(e_{1}, \ldots, e_{m}\right)^{\prime}\)</span> and its <span class="math inline">\(m \times m\)</span> covariance matrix <span class="math inline">\(\Sigma=\mathbb{E}\left[e e^{\prime}\right]\)</span>. The diagonal elements are the variances of the errors <span class="math inline">\(e_{j}\)</span> and the off-diagonals are the covariances across variables.</p>
<p>We can group the <span class="math inline">\(m\)</span> equations (11.1) into a single equation as follows. Let <span class="math inline">\(Y=\left(Y_{1}, \ldots, Y_{m}\right)^{\prime}\)</span> be the <span class="math inline">\(m \times 1\)</span> vector of dependent variables. Define the <span class="math inline">\(m \times \bar{k}\)</span> matrix of regressors</p>
<p><span class="math display">\[
\bar{X}=\left(\begin{array}{cccc}
X_{1}^{\prime} &amp; 0 &amp; \cdots &amp; 0 \\
\vdots &amp; X_{2}^{\prime} &amp; &amp; \vdots \\
0 &amp; 0 &amp; \cdots &amp; X_{m}^{\prime}
\end{array}\right)
\]</span></p>
<p>and the <span class="math inline">\(\bar{k} \times 1\)</span> stacked coefficient vector</p>
<p><span class="math display">\[
\beta=\left(\begin{array}{c}
\beta_{1} \\
\vdots \\
\beta_{m}
\end{array}\right)
\]</span></p>
<p>The <span class="math inline">\(m\)</span> regression equations can be jointly written as</p>
<p><span class="math display">\[
Y=\bar{X} \beta+e .
\]</span></p>
<p>This is a system of <span class="math inline">\(m\)</span> equations.</p>
<p>For <span class="math inline">\(n\)</span> observations the joint system can be written in matrix notation by stacking. Define</p>
<p><span class="math display">\[
\boldsymbol{Y}=\left(\begin{array}{c}
Y_{1} \\
\vdots \\
Y_{n}
\end{array}\right), \quad \boldsymbol{e}=\left(\begin{array}{c}
e_{1} \\
\vdots \\
e_{n}
\end{array}\right), \quad \overline{\boldsymbol{X}}=\left(\begin{array}{c}
\bar{X}_{1} \\
\vdots \\
\bar{X}_{n}
\end{array}\right)
\]</span></p>
<p>which are <span class="math inline">\(m n \times 1, m n \times 1\)</span>, and <span class="math inline">\(m n \times \bar{k}\)</span>, respectively. The system can be written as <span class="math inline">\(\boldsymbol{Y}=\overline{\boldsymbol{X}} \beta+\boldsymbol{e}\)</span>.</p>
<p>In many applications the regressor vectors <span class="math inline">\(X_{j}\)</span> are common across the variables <span class="math inline">\(j\)</span>, so <span class="math inline">\(X_{j}=X\)</span> and <span class="math inline">\(k_{j}=k\)</span>. By this we mean that the same variables enter each equation with no exclusion restrictions. Several important simplifications occur in this context. One is that we can write (11.2) using the notation</p>
<p><span class="math display">\[
Y=\boldsymbol{B}^{\prime} X+e
\]</span></p>
<p>where <span class="math inline">\(\boldsymbol{B}=\left(\beta_{1}, \beta_{2}, \cdots, \beta_{m}\right)\)</span> is <span class="math inline">\(k \times m\)</span>. Another is that we can write the joint system of observations in the <span class="math inline">\(n \times m\)</span> matrix notation <span class="math inline">\(\boldsymbol{Y}=\boldsymbol{X} \boldsymbol{B}+\boldsymbol{E}\)</span> where</p>
<p><span class="math display">\[
\boldsymbol{Y}=\left(\begin{array}{c}
Y_{1}^{\prime} \\
\vdots \\
Y_{n}^{\prime}
\end{array}\right), \quad \boldsymbol{E}=\left(\begin{array}{c}
e_{1}^{\prime} \\
\vdots \\
e_{n}^{\prime}
\end{array}\right), \quad \boldsymbol{X}=\left(\begin{array}{c}
X_{1}^{\prime} \\
\vdots \\
X_{n}^{\prime}
\end{array}\right)
\]</span></p>
<p>Another convenient implication of common regressors is that we have the simplification</p>
<p><span class="math display">\[
\bar{X}=\left(\begin{array}{cccc}
X^{\prime} &amp; 0 &amp; \cdots &amp; 0 \\
0 &amp; X^{\prime} &amp; &amp; 0 \\
\vdots &amp; \vdots &amp; &amp; \vdots \\
0 &amp; 0 &amp; \cdots &amp; X^{\prime}
\end{array}\right)=\boldsymbol{I}_{m} \otimes X^{\prime}
\]</span></p>
<p>where <span class="math inline">\(\otimes\)</span> is the Kronecker product (see Appendix A.21).</p>
</section>
<section id="least-squares-estimator" class="level2" data-number="11.3">
<h2 data-number="11.3" class="anchored" data-anchor-id="least-squares-estimator"><span class="header-section-number">11.3</span> Least Squares Estimator</h2>
<p>The equations (11.1) can be estimated by least squares. This takes the form</p>
<p><span class="math display">\[
\widehat{\beta}_{j}=\left(\sum_{i=1}^{n} X_{j i} X_{j i}^{\prime}\right)^{-1}\left(\sum_{i=1}^{n} X_{j i} Y_{j i}\right) .
\]</span></p>
<p>An estimator of <span class="math inline">\(\beta\)</span> is the stacked vector</p>
<p><span class="math display">\[
\widehat{\beta}=\left(\begin{array}{c}
\widehat{\beta}_{1} \\
\vdots \\
\widehat{\beta}_{m}
\end{array}\right) .
\]</span></p>
<p>We can alternatively write this estimator using the systems notation</p>
<p><span class="math display">\[
\widehat{\beta}=\left(\overline{\boldsymbol{X}}^{\prime} \overline{\boldsymbol{X}}\right)^{-1}\left(\overline{\boldsymbol{X}}^{\prime} \boldsymbol{Y}\right)=\left(\sum_{i=1}^{n} \bar{X}_{i}^{\prime} \bar{X}_{i}\right)^{-1}\left(\sum_{i=1}^{n} \bar{X}_{i}^{\prime} Y_{i}\right)
\]</span></p>
<p>To see this, observe that</p>
<p><span class="math display">\[
\begin{aligned}
\overline{\boldsymbol{X}}^{\prime} \overline{\boldsymbol{X}} &amp;=\left(\begin{array}{ccc}
\bar{X}_{1}^{\prime} &amp; \cdots &amp; \bar{X}_{n}^{\prime}
\end{array}\right)\left(\begin{array}{c}
\bar{X}_{1} \\
\vdots \\
\bar{X}_{n}
\end{array}\right) \\
&amp;=\sum_{i=1}^{n} \bar{X}_{i}^{\prime} \bar{X}_{i} \\
&amp;=\sum_{i=1}^{n}\left(\begin{array}{cccc}
X_{1 i} &amp; 0 &amp; \cdots &amp; 0 \\
\vdots &amp; X_{2 i} &amp; &amp; \vdots \\
0 &amp; 0 &amp; \cdots &amp; X_{m i}
\end{array}\right)\left(\begin{array}{ccccc}
X_{1 i}^{\prime} &amp; 0 &amp; \cdots &amp; 0 \\
\vdots &amp; X_{2 i}^{\prime} &amp; &amp; \vdots \\
0 &amp; 0 &amp; \cdots &amp; X_{m i}^{\prime}
\end{array}\right) \\
&amp;=\left(\begin{array}{cccccc}
\sum_{i=1}^{n} X_{1 i} X_{1 i}^{\prime} &amp; &amp; 0 \\
\vdots &amp; &amp; \sum_{i=1}^{n} X_{2 i} X_{2 i}^{\prime} &amp; &amp; &amp; \\
0 &amp; &amp; 0 &amp; \cdots &amp; \sum_{i=1}^{n} X_{m i} X_{m i}^{\prime}
\end{array}\right)
\end{aligned}
\]</span></p>
<p>and</p>
<p><span class="math display">\[
\begin{aligned}
\overline{\boldsymbol{X}}^{\prime} \boldsymbol{Y} &amp;=\left(\begin{array}{ccc}
\bar{X}_{1}^{\prime} &amp; \cdots &amp; \bar{X}_{n}^{\prime}
\end{array}\right)\left(\begin{array}{c}
Y_{1} \\
\vdots \\
Y_{n}
\end{array}\right) \\
&amp;=\sum_{i=1}^{n} \bar{X}_{i}^{\prime} Y_{i} \\
&amp;=\sum_{i=1}^{n}\left(\begin{array}{cccc}
X_{1 i} &amp; 0 &amp; \cdots &amp; 0 \\
\vdots &amp; X_{2 i} &amp; &amp; \vdots \\
0 &amp; 0 &amp; \cdots &amp; X_{m i}
\end{array}\right)\left(\begin{array}{c}
Y_{1 i} \\
\vdots \\
Y_{m i}
\end{array}\right) \\
&amp;=\left(\begin{array}{c}
\sum_{i=1}^{n} X_{1 i} Y_{1 i} \\
\vdots \\
\sum_{i=1}^{n} X_{m i} Y_{m i}
\end{array}\right) .
\end{aligned}
\]</span></p>
<p>Hence</p>
<p><span class="math display">\[
\begin{aligned}
\left(\overline{\boldsymbol{X}}^{\prime} \overline{\boldsymbol{X}}\right)^{-1}\left(\overline{\boldsymbol{X}}^{\prime} \boldsymbol{Y}\right) &amp;=\left(\sum_{i=1}^{n} \bar{X}_{i} \bar{X}_{i}^{\prime}\right)^{-1}\left(\sum_{i=1}^{n} \bar{X}_{i} Y_{i}\right) \\
&amp;=\left(\begin{array}{c}
\left(\sum_{i=1}^{n} X_{1 i} X_{1 i}^{\prime}\right)^{-1}\left(\sum_{i=1}^{n} X_{1 i} Y_{1 i}\right) \\
\vdots \\
\left(\sum_{i=1}^{n} X_{m i} X_{m i}^{\prime}\right)^{-1}\left(\sum_{i=1}^{n} X_{m i} Y_{m i}\right)
\end{array}\right) \\
&amp;=\widehat{\beta}
\end{aligned}
\]</span></p>
<p>as claimed. The <span class="math inline">\(m \times 1\)</span> residual vector for the <span class="math inline">\(i^{t h}\)</span> observation is <span class="math inline">\(\widehat{e}_{i}=Y_{i}-\overline{\boldsymbol{X}}_{i}^{\prime} \widehat{\beta}\)</span>. The least squares estimator of the <span class="math inline">\(m \times m\)</span> error covariance matrix is</p>
<p><span class="math display">\[
\widehat{\Sigma}=\frac{1}{n} \sum_{i=1}^{n} \widehat{e}_{i} \widehat{e}_{i}^{\prime} .
\]</span></p>
<p>In the case of common regressors, the least squares coefficients can be written as</p>
<p><span class="math display">\[
\widehat{\beta}_{j}=\left(\sum_{i=1}^{n} X_{i} X_{i}^{\prime}\right)^{-1}\left(\sum_{i=1}^{n} X_{i} Y_{j i}\right)
\]</span></p>
<p>and</p>
<p><span class="math display">\[
\widehat{\boldsymbol{B}}=\left(\widehat{\beta}_{1}, \widehat{\beta}_{2}, \cdots, \widehat{\beta}_{m}\right)=\left(\boldsymbol{X}^{\prime} \boldsymbol{X}\right)^{-1}\left(\boldsymbol{X}^{\prime} \boldsymbol{Y}\right) .
\]</span></p>
<p>In Stata, multivariate regression can be implemented using the mvreg command.</p>
</section>
<section id="expectation-and-variance-of-systems-least-squares" class="level2" data-number="11.4">
<h2 data-number="11.4" class="anchored" data-anchor-id="expectation-and-variance-of-systems-least-squares"><span class="header-section-number">11.4</span> Expectation and Variance of Systems Least Squares</h2>
<p>We can calculate the finite-sample expectation and variance of <span class="math inline">\(\widehat{\beta}\)</span> under the conditional expectation assumption</p>
<p><span class="math display">\[
\mathbb{E}[e \mid X]=0
\]</span></p>
<p>where <span class="math inline">\(X\)</span> is the union of the regressors <span class="math inline">\(X_{j}\)</span>. Equation (11.7) is equivalent to <span class="math inline">\(\mathbb{E}\left\lfloor Y_{j} \mid X\right\rfloor=X_{j}^{\prime} \beta_{j}\)</span>, which means that the regression model is correctly specified.</p>
<p>We can center the estimator as</p>
<p><span class="math display">\[
\widehat{\beta}-\beta=\left(\overline{\boldsymbol{X}}^{\prime} \overline{\boldsymbol{X}}\right)^{-1}\left(\overline{\boldsymbol{X}}^{\prime} \boldsymbol{e}\right)=\left(\sum_{i=1}^{n} \bar{X}_{i}^{\prime} \bar{X}_{i}\right)^{-1}\left(\sum_{i=1}^{n} \bar{X}_{i}^{\prime} e_{i}\right)
\]</span></p>
<p>Taking conditional expectations we find <span class="math inline">\(\mathbb{E}[\widehat{\beta} \mid \boldsymbol{X}]=\beta\)</span>. Consequently, systems least squares is unbiased under correct specification.</p>
<p>To compute the variance of the estimator, define the conditional covariance matrix of the errors of the <span class="math inline">\(i^{t h}\)</span> observation <span class="math inline">\(\mathbb{E}\left[e_{i} e_{i}^{\prime} \mid X_{i}\right]=\Sigma_{i}\)</span> which in general is a function of <span class="math inline">\(X_{i}\)</span>. If the observations are mutually independent then</p>
<p><span class="math display">\[
\mathbb{E}\left[\boldsymbol{e} \boldsymbol{e}^{\prime} \mid \boldsymbol{X}\right]=\mathbb{E}\left[\left(\begin{array}{cccc}
e_{1} e_{1}^{\prime} &amp; e_{1} e_{2}^{\prime} &amp; \cdots &amp; e_{1} e_{n}^{\prime} \\
\vdots &amp; \ddots &amp; &amp; \vdots \\
e_{n} e_{1}^{\prime} &amp; e_{n} e_{2}^{\prime} &amp; \cdots &amp; e_{n} e_{n}^{\prime}
\end{array}\right) \mid \boldsymbol{X}\right]=\left(\begin{array}{cccc}
\Sigma_{1} &amp; 0 &amp; \cdots &amp; 0 \\
\vdots &amp; \ddots &amp; &amp; \vdots \\
0 &amp; 0 &amp; \cdots &amp; \Sigma_{n}
\end{array}\right) \text {. }
\]</span></p>
<p>Also, by independence across observations,</p>
<p><span class="math display">\[
\operatorname{var}\left[\sum_{i=1}^{n} \bar{X}_{i}^{\prime} e_{i} \mid \boldsymbol{X}\right]=\sum_{i=1}^{n} \operatorname{var}\left[\bar{X}_{i}^{\prime} e_{i} \mid X_{i}\right]=\sum_{i=1}^{n} \bar{X}_{i}^{\prime} \Sigma_{i} \bar{X}_{i} .
\]</span></p>
<p>It follows that</p>
<p><span class="math display">\[
\operatorname{var}[\widehat{\beta} \mid \boldsymbol{X}]=\left(\overline{\boldsymbol{X}}^{\prime} \overline{\boldsymbol{X}}\right)^{-1}\left(\sum_{i=1}^{n} \bar{X}_{i}^{\prime} \Sigma_{i} \bar{X}_{i}\right)\left(\overline{\boldsymbol{X}}^{\prime} \overline{\boldsymbol{X}}\right)^{-1} .
\]</span></p>
<p>When the regressors are common so that <span class="math inline">\(\bar{X}_{i}=\boldsymbol{I}_{m} \otimes X_{i}^{\prime}\)</span> then the covariance matrix can be written as</p>
<p><span class="math display">\[
\operatorname{var}[\widehat{\beta} \mid \boldsymbol{X}]=\left(\boldsymbol{I}_{m} \otimes\left(\boldsymbol{X}^{\prime} \boldsymbol{X}\right)^{-1}\right)\left(\sum_{i=1}^{n}\left(\Sigma_{i} \otimes X_{i} X_{i}^{\prime}\right)\right)\left(\boldsymbol{I}_{m} \otimes\left(\boldsymbol{X}^{\prime} \boldsymbol{X}\right)^{-1}\right)
\]</span></p>
<p>If the errors are conditionally homoskedastic</p>
<p><span class="math display">\[
\mathbb{E}\left[e e^{\prime} \mid X\right]=\Sigma
\]</span></p>
<p>then the covariance matrix simplifies to</p>
<p><span class="math display">\[
\operatorname{var}[\widehat{\beta} \mid \boldsymbol{X}]=\left(\overline{\boldsymbol{X}}^{\prime} \overline{\boldsymbol{X}}\right)^{-1}\left(\sum_{i=1}^{n} \bar{X}_{i}^{\prime} \Sigma \bar{X}_{i}\right)\left(\overline{\boldsymbol{X}}^{\prime} \overline{\boldsymbol{X}}\right)^{-1}
\]</span></p>
<p>If both simplifications (common regressors and conditional homoskedasticity) hold then we have the considerable simplication</p>
<p><span class="math display">\[
\operatorname{var}[\widehat{\beta} \mid \boldsymbol{X}]=\Sigma \otimes\left(\boldsymbol{X}^{\prime} \boldsymbol{X}\right)^{-1} .
\]</span></p>
</section>
<section id="asymptotic-distribution" class="level2" data-number="11.5">
<h2 data-number="11.5" class="anchored" data-anchor-id="asymptotic-distribution"><span class="header-section-number">11.5</span> Asymptotic Distribution</h2>
<p>For an asymptotic distribution it is sufficient to consider the equation-by-equation projection model in which case</p>
<p><span class="math display">\[
\mathbb{E}\left[X_{j} e_{j}\right]=0 .
\]</span></p>
<p>First, consider consistency. Since <span class="math inline">\(\widehat{\beta}_{j}\)</span> are the standard least squares estimators, they are consistent for the projection coefficients <span class="math inline">\(\beta_{j}\)</span>.</p>
<p>Second, consider the asymptotic distribution. Our single equation theory implies that the <span class="math inline">\(\widehat{\beta}_{j}\)</span> are asymptotically normal. But this theory does not provide a joint distribution of the <span class="math inline">\(\widehat{\beta}_{j}\)</span> across <span class="math inline">\(j\)</span>, which we now derive. Since the vector</p>
<p><span class="math display">\[
\bar{X}_{i}^{\prime} e_{i}=\left(\begin{array}{c}
X_{1 i} e_{1 i} \\
\vdots \\
X_{m i} e_{m i}
\end{array}\right)
\]</span></p>
<p>is i.i.d. across <span class="math inline">\(i\)</span> and mean zero under (11.9), the central limit theorem implies</p>
<p><span class="math display">\[
\frac{1}{\sqrt{n}} \sum_{i=1}^{n} \bar{X}_{i}^{\prime} e_{i} \underset{d}{\longrightarrow} \mathrm{N}(0, \Omega)
\]</span></p>
<p>where</p>
<p><span class="math display">\[
\Omega=\mathbb{E}\left[\bar{X}_{i}^{\prime} e_{i} e_{i}^{\prime} \bar{X}_{i}\right]=\mathbb{E}\left[\bar{X}_{i}^{\prime} \Sigma_{i} \bar{X}_{i}\right] .
\]</span></p>
<p>The matrix <span class="math inline">\(\Omega\)</span> is the covariance matrix of the variables <span class="math inline">\(X_{j i} e_{j i}\)</span> across equations. Under conditional homoskedasticity (11.8) the matrix <span class="math inline">\(\Omega\)</span> simplifies to</p>
<p><span class="math display">\[
\Omega=\mathbb{E}\left[\bar{X}_{i}^{\prime} \Sigma \bar{X}_{i}\right]
\]</span></p>
<p>(see Exercise 11.1). When the regressors are common it simplies to</p>
<p><span class="math display">\[
\Omega=\mathbb{E}\left[e e^{\prime} \otimes X X^{\prime}\right]
\]</span></p>
<p>(see Exercise 11.2). Under both conditions (homoskedasticity and common regressors) it simplifies to</p>
<p><span class="math display">\[
\Omega=\Sigma \otimes \mathbb{E}\left[X X^{\prime}\right]
\]</span></p>
<p>(see Exercise 11.3).</p>
<p>Applied to the centered and normalized estimator we obtain the asymptotic distribution. Theorem 11.1 Under Assumption 7.2, <span class="math inline">\(\sqrt{n}(\widehat{\beta}-\beta) \underset{d}{\longrightarrow} \mathrm{N}\left(0, \boldsymbol{V}_{\beta}\right)\)</span> where <span class="math inline">\(\boldsymbol{V}_{\beta}=\)</span> <span class="math inline">\(Q^{-1} \Omega Q^{-1}\)</span> and</p>
<p><span class="math display">\[
\boldsymbol{Q}=\mathbb{E}\left[\bar{X}^{\prime} \bar{X}\right]=\left(\begin{array}{cccc}
\mathbb{E}\left[X_{1} X_{1}^{\prime}\right] &amp; 0 &amp; \cdots &amp; 0 \\
\vdots &amp; \ddots &amp; &amp; \vdots \\
0 &amp; 0 &amp; \cdots &amp; \mathbb{E}\left[X_{m} X_{m}^{\prime}\right]
\end{array}\right)
\]</span></p>
<p>For a proof, see Exercise 11.4.</p>
<p>When the regressors are common the matrix <span class="math inline">\(\boldsymbol{Q}\)</span> simplifies as</p>
<p><span class="math display">\[
\boldsymbol{Q}=\boldsymbol{I}_{m} \otimes \mathbb{E}\left[X X^{\prime}\right]
\]</span></p>
<p>(See Exercise 11.5).</p>
<p>If both the regressors are common and the errors are conditionally homoskedastic (11.8) then we have the simplification</p>
<p><span class="math display">\[
\boldsymbol{V}_{\beta}=\Sigma \otimes\left(\mathbb{E}\left[X X^{\prime}\right]\right)^{-1}
\]</span></p>
<p>(see Exercise 11.6).</p>
<p>Sometimes we are interested in parameters <span class="math inline">\(\theta=r\left(\beta_{1}, \ldots, \beta_{m}\right)=r(\beta)\)</span> which are functions of the coefficients from multiple equations. In this case the least squares estimator of <span class="math inline">\(\theta\)</span> is <span class="math inline">\(\widehat{\theta}=r(\widehat{\beta})\)</span>. The asymptotic distribution of <span class="math inline">\(\widehat{\theta}\)</span> can be obtained from Theorem <span class="math inline">\(11.1\)</span> by the delta method.</p>
<p>Theorem 11.2 Under Assumptions <span class="math inline">\(7.2\)</span> and <span class="math inline">\(7.3, \sqrt{n}(\widehat{\theta}-\theta) \underset{d}{\longrightarrow} \mathrm{N}\left(0, \boldsymbol{V}_{\theta}\right)\)</span> where <span class="math inline">\(\boldsymbol{V}_{\theta}=\boldsymbol{R}^{\prime} \boldsymbol{V}_{\beta} \boldsymbol{R}\)</span> and <span class="math inline">\(\boldsymbol{R}=\frac{\partial}{\partial \beta} r(\beta)^{\prime} .\)</span></p>
<p>For a proof, see Exercise 11.7.</p>
<p>Theorem <span class="math inline">\(11.2\)</span> is an example where multivariate regression is fundamentally distinct from univariate regression. Only by treating least squares as a joint estimator can we obtain a distributional theory for a function of multiple equations. We can thereby construct standard errors, confidence intervals, and hypothesis tests.</p>
</section>
<section id="covariance-matrix-estimation" class="level2" data-number="11.6">
<h2 data-number="11.6" class="anchored" data-anchor-id="covariance-matrix-estimation"><span class="header-section-number">11.6</span> Covariance Matrix Estimation</h2>
<p>From the finite sample and asymptotic theory we can construct appropriate estimators for the variance of <span class="math inline">\(\widehat{\beta}\)</span>. In the general case we have</p>
<p><span class="math display">\[
\widehat{\boldsymbol{V}}_{\widehat{\beta}}=\left(\overline{\boldsymbol{X}}^{\prime} \overline{\boldsymbol{X}}\right)^{-1}\left(\sum_{i=1}^{n} \bar{X}_{i}^{\prime} \widehat{e}_{i} \widehat{e}_{i}^{\prime} \bar{X}_{i}\right)\left(\overline{\boldsymbol{X}}^{\prime} \overline{\boldsymbol{X}}\right)^{-1} .
\]</span></p>
<p>Under conditional homoskedasticity (11.8) an appropriate estimator is</p>
<p><span class="math display">\[
\widehat{\boldsymbol{V}}_{\widehat{\beta}}^{0}=\left(\overline{\boldsymbol{X}}^{\prime} \overline{\boldsymbol{X}}\right)^{-1}\left(\sum_{i=1}^{n} \bar{X}_{i}^{\prime} \widehat{\Sigma}_{i}\right)\left(\overline{\boldsymbol{X}}^{\prime} \overline{\boldsymbol{X}}\right)^{-1} .
\]</span></p>
<p>When the regressors are common then these estimators equal</p>
<p><span class="math display">\[
\widehat{\boldsymbol{V}}_{\widehat{\beta}}=\left(\boldsymbol{I}_{m} \otimes\left(\boldsymbol{X}^{\prime} \boldsymbol{X}\right)^{-1}\right)\left(\sum_{i=1}^{n}\left(\widehat{e}_{i} \widehat{e}_{i}^{\prime} \otimes X_{i} X_{i}^{\prime}\right)\right)\left(\boldsymbol{I}_{m} \otimes\left(\boldsymbol{X}^{\prime} \boldsymbol{X}\right)^{-1}\right)
\]</span></p>
<p>and <span class="math inline">\(\widehat{\boldsymbol{V}}_{\widehat{\beta}}^{0}=\widehat{\Sigma} \otimes\left(\boldsymbol{X}^{\prime} \boldsymbol{X}\right)^{-1}\)</span>, respectively.</p>
<p>Covariance matrix estimators for <span class="math inline">\(\widehat{\theta}\)</span> are found as</p>
<p><span class="math display">\[
\begin{aligned}
\widehat{\boldsymbol{V}}_{\widehat{\theta}} &amp;=\widehat{\boldsymbol{R}}^{\prime} \widehat{\boldsymbol{V}}_{\widehat{\beta}} \widehat{\boldsymbol{R}} \\
\widehat{\boldsymbol{V}}_{\widehat{\theta}}^{0} &amp;=\widehat{\boldsymbol{R}}^{\prime} \widehat{\boldsymbol{V}}_{\widehat{\beta}}^{0} \widehat{\boldsymbol{R}} \\
\widehat{\boldsymbol{R}} &amp;=\frac{\partial}{\partial \beta} r(\widehat{\beta})^{\prime} .
\end{aligned}
\]</span></p>
<p>Theorem 11.3 Under Assumption 7.2, <span class="math inline">\(n \widehat{\boldsymbol{V}}_{\widehat{\beta}} \underset{p}{\rightarrow} \boldsymbol{V}_{\beta}\)</span> and <span class="math inline">\(n \widehat{\boldsymbol{V}}_{\widehat{\beta}}^{0} \vec{p}^{0} \boldsymbol{V}_{\beta}^{0}\)</span></p>
<p>For a proof, see Exercise 11.8.</p>
</section>
<section id="seemingly-unrelated-regression" class="level2" data-number="11.7">
<h2 data-number="11.7" class="anchored" data-anchor-id="seemingly-unrelated-regression"><span class="header-section-number">11.7</span> Seemingly Unrelated Regression</h2>
<p>Consider the systems regression model under the conditional expectation and homoskedasticity assumptions</p>
<p><span class="math display">\[
\begin{aligned}
Y &amp;=\bar{X} \beta+e \\
\mathbb{E}[e \mid X] &amp;=0 \\
\mathbb{E}\left[e e^{\prime} \mid X\right] &amp;=\Sigma .
\end{aligned}
\]</span></p>
<p>Since the errors are correlated across equations we consider estimation by Generalized Least Squares (GLS). To derive the estimator, premultiply (11.15) by <span class="math inline">\(\Sigma^{-1 / 2}\)</span> so that the transformed error vector is i.i.d. with covariance matrix <span class="math inline">\(\boldsymbol{I}_{m}\)</span>. Then apply least squares and rearrange to find</p>
<p><span class="math display">\[
\widehat{\beta}_{\mathrm{gls}}=\left(\sum_{i=1}^{n} \bar{X}_{i}^{\prime} \Sigma^{-1} \bar{X}_{i}\right)^{-1}\left(\sum_{i=1}^{n} \bar{X}_{i}^{\prime} \Sigma^{-1} Y_{i}\right)
\]</span></p>
<p>(see Exercise 11.9). Another approach is to take the vector representation</p>
<p><span class="math display">\[
\boldsymbol{Y}=\overline{\boldsymbol{X}} \beta+\boldsymbol{e}
\]</span></p>
<p>and calculate that the equation error <span class="math inline">\(\boldsymbol{e}\)</span> has variance <span class="math inline">\(\mathbb{E}\left[\boldsymbol{e} \boldsymbol{e}^{\prime}\right]=\boldsymbol{I}_{n} \otimes \Sigma\)</span>. Premultiply the equation by <span class="math inline">\(\boldsymbol{I}_{n} \otimes\)</span> <span class="math inline">\(\Sigma^{-1 / 2}\)</span> so that the transformed error has covariance matrix <span class="math inline">\(\boldsymbol{I}_{n m}\)</span> and then apply least squares to find</p>
<p><span class="math display">\[
\widehat{\beta}_{\mathrm{gls}}=\left(\overline{\boldsymbol{X}}^{\prime}\left(\boldsymbol{I}_{n} \otimes \Sigma^{-1}\right) \overline{\boldsymbol{X}}\right)^{-1}\left(\overline{\boldsymbol{X}}^{\prime}\left(\boldsymbol{I}_{n} \otimes \Sigma^{-1}\right) \boldsymbol{Y}\right)
\]</span></p>
<p>(see Exercise 11.10). Expressions (11.16) and (11.17) are algebraically equivalent. To see the equivalence, observe that</p>
<p><span class="math display">\[
\begin{aligned}
\overline{\boldsymbol{X}}^{\prime}\left(\boldsymbol{I}_{n} \otimes \Sigma^{-1}\right) \overline{\boldsymbol{X}} &amp;=\left(\begin{array}{lll}
\bar{X}_{1}^{\prime} &amp; \cdots &amp; \bar{X}_{n}^{\prime}
\end{array}\right)\left(\begin{array}{cccc}
\Sigma^{-1} &amp; 0 &amp; \cdots &amp; 0 \\
\vdots &amp; \Sigma^{-1} &amp; &amp; \vdots \\
0 &amp; 0 &amp; \cdots &amp; \Sigma^{-1}
\end{array}\right)\left(\begin{array}{c}
\bar{X}_{1} \\
\vdots \\
\bar{X}_{n}
\end{array}\right) \\
&amp;=\sum_{i=1}^{n} \bar{X}_{i}^{\prime} \Sigma^{-1} \bar{X}_{i}
\end{aligned}
\]</span></p>
<p>and</p>
<p><span class="math display">\[
\begin{aligned}
\overline{\boldsymbol{X}}^{\prime}\left(\boldsymbol{I}_{n} \otimes \Sigma^{-1}\right) \boldsymbol{Y} &amp;=\left(\begin{array}{lll}
\bar{X}_{1}^{\prime} &amp; \cdots &amp; \bar{X}_{n}^{\prime}
\end{array}\right)\left(\begin{array}{cccc}
\Sigma^{-1} &amp; 0 &amp; \cdots &amp; 0 \\
\vdots &amp; \Sigma^{-1} &amp; &amp; \vdots \\
0 &amp; 0 &amp; \cdots &amp; 0^{-1}
\end{array}\right)\left(\begin{array}{c}
Y_{1} \\
\vdots \\
Y_{n}
\end{array}\right) \\
&amp;=\sum_{i=1}^{n} \bar{X}_{i}^{\prime} \Sigma^{-1} Y_{i} .
\end{aligned}
\]</span></p>
<p>Since <span class="math inline">\(\Sigma\)</span> is unknown it must be replaced by an estimator. Using <span class="math inline">\(\widehat{\Sigma}\)</span> from (11.5) we obtain a feasible GLS estimator.</p>
<p><span class="math display">\[
\begin{aligned}
\widehat{\beta}_{\mathrm{sur}} &amp;=\left(\sum_{i=1}^{n} \bar{X}_{i}^{\prime} \widehat{\Sigma}^{-1} \bar{X}_{i}\right)^{-1}\left(\sum_{i=1}^{n} \bar{X}_{i}^{\prime} \widehat{\Sigma}^{-1} Y_{i}\right) \\
&amp;=\left(\overline{\boldsymbol{X}}^{\prime}\left(\boldsymbol{I}_{n} \otimes \widehat{\Sigma}^{-1}\right) \overline{\boldsymbol{X}}\right)^{-1}\left(\overline{\boldsymbol{X}}^{\prime}\left(\boldsymbol{I}_{n} \otimes \widehat{\Sigma}^{-1}\right) \boldsymbol{Y}\right) .
\end{aligned}
\]</span></p>
<p>This is the Seemingly Unrelated Regression (SUR) estimator as introduced by Zellner (1962).</p>
<p>The estimator <span class="math inline">\(\widehat{\Sigma}\)</span> can be updated by calculating the SUR residuals <span class="math inline">\(\widehat{e}_{i}=Y_{i}-\bar{X}_{i}^{\prime} \widehat{\beta}_{\text {sur }}\)</span> and the covariance matrix estimator <span class="math inline">\(\widehat{\Sigma}=\frac{1}{n} \sum_{i=1}^{n} \widehat{e}_{i} \widehat{e}_{i}^{\prime}\)</span>. Substituted into (11.18) we obtain an iterated SUR estimator. This can be iterated until convergence.</p>
<p>Under conditional homoskedasticity (11.8) we can derive its asymptotic distribution.</p>
<p>Theorem 11.4 Under Assumption <span class="math inline">\(7.2\)</span> and (11.8)</p>
<p><span class="math display">\[
\sqrt{n}\left(\widehat{\beta}_{\text {sur }}-\beta\right) \underset{d}{\longrightarrow} \mathrm{N}\left(0, V_{\beta}^{*}\right)
\]</span></p>
<p>where <span class="math inline">\(\boldsymbol{V}_{\beta}^{*}=\left(\mathbb{E}\left[\bar{X}^{\prime} \Sigma^{-1} \bar{X}\right]\right)^{-1}\)</span>.</p>
<p>For a proof, see Exercise 11.11.</p>
<p>Under these assumptions, SUR is more efficient than least squares.</p>
<p>Theorem 11.5 Under Assumption <span class="math inline">\(7.2\)</span> and (11.8)</p>
<p><span class="math display">\[
\boldsymbol{V}_{\beta}^{*}=\left(\mathbb{E}\left[\bar{X}^{\prime} \Sigma^{-1} \bar{X}\right]\right)^{-1} \leq\left(\mathbb{E}\left[\bar{X}^{\prime} \bar{X}\right]\right)^{-1} \mathbb{E}\left[\bar{X}^{\prime} \Sigma \bar{X}\right]\left(\mathbb{E}\left[\bar{X}^{\prime} \bar{X}\right]\right)^{-1}=\boldsymbol{V}_{\beta}
\]</span></p>
<p>and thus <span class="math inline">\(\widehat{\beta}_{\text {sur }}\)</span> is asymptotically more efficient than <span class="math inline">\(\widehat{\beta}_{\text {ols. }}\)</span>. For a proof, see Exercise 11.12.</p>
<p>An appropriate estimator of the variance of <span class="math inline">\(\widehat{\beta}_{\text {sur }}\)</span> is</p>
<p><span class="math display">\[
\widehat{\boldsymbol{V}}_{\widehat{\beta}}=\left(\sum_{i=1}^{n} \bar{X}_{i}^{\prime} \widehat{\Sigma}^{-1} \bar{X}_{i}\right)^{-1}
\]</span></p>
<p>Theorem 11.6 Under Assumption <span class="math inline">\(7.2\)</span> and (11.8) <span class="math inline">\(n \widehat{\boldsymbol{V}}_{\widehat{\beta}} \underset{p}{\longrightarrow} \boldsymbol{V}_{\beta}\)</span>.</p>
<p>For a proof, see Exercise 11.13.</p>
<p>In Stata, the seemingly unrelated regressions estimator is implemented using the sureg command.</p>
<p><img src="images//2022_09_17_2536389c9af66f64c886g-09.jpg" class="img-fluid"></p>
</section>
<section id="equivalence-of-sur-and-least-squares" class="level2" data-number="11.8">
<h2 data-number="11.8" class="anchored" data-anchor-id="equivalence-of-sur-and-least-squares"><span class="header-section-number">11.8</span> Equivalence of SUR and Least Squares</h2>
<p>When the regressors are common across equations <span class="math inline">\(X_{j}=X\)</span> it turns out that the SUR estimator simplifies to least squares.</p>
<p>To see this, recall that when regressors are common this implies that <span class="math inline">\(\bar{X}=\boldsymbol{I}_{m} \otimes X^{\prime}\)</span>. Then</p>
<p><span class="math display">\[
\begin{aligned}
\bar{X}_{i}^{\prime} \widehat{\Sigma}^{-1} &amp;=\left(\boldsymbol{I}_{m} \otimes X_{i}\right) \widehat{\Sigma}^{-1} \\
&amp;=\widehat{\Sigma}^{-1} \otimes X_{i} \\
&amp;=\left(\widehat{\Sigma}^{-1} \otimes \boldsymbol{I}_{k}\right)\left(\boldsymbol{I}_{m} \otimes X_{i}\right) \\
&amp;=\left(\widehat{\Sigma}^{-1} \otimes \boldsymbol{I}_{k}\right) \bar{X}_{i}^{\prime} .
\end{aligned}
\]</span></p>
<p>Thus</p>
<p><span class="math display">\[
\begin{aligned}
\widehat{\beta}_{\mathrm{sur}} &amp;=\left(\sum_{i=1}^{n} \bar{X}_{i}^{\prime} \widehat{\Sigma}^{-1} \bar{X}_{i}\right)^{-1}\left(\sum_{i=1}^{n} \bar{X}_{i}^{\prime} \widehat{\Sigma}^{-1} Y_{i}\right) \\
&amp;=\left(\left(\widehat{\Sigma}^{-1} \otimes \boldsymbol{I}_{k}\right) \sum_{i=1}^{n} \bar{X}_{i}^{\prime} \bar{X}_{i}\right)^{-1}\left(\left(\widehat{\Sigma}^{-1} \otimes \boldsymbol{I}_{k}\right) \sum_{i=1}^{n} \bar{X}_{i}^{\prime} Y_{i}\right) \\
&amp;=\left(\sum_{i=1}^{n} \bar{X}_{i}^{\prime} \bar{X}_{i}\right)^{-1}\left(\sum_{i=1}^{n} \bar{X}_{i}^{\prime} Y_{i}\right)=\widehat{\beta}_{\mathrm{ols}}
\end{aligned}
\]</span></p>
<p>A model where regressors are not common across equations is nested within a model with the union of all regressors included in all equations. Thus the model with regressors common across equations is a fully unrestricted model, and a model where the regressors differ across equations is a restricted model. Thus the above result shows that the SUR estimator reduces to least squares in the absence of restrictions, but SUR can differ from least squares otherwise.</p>
<p>Another context where SUR=OLS is when the variance matrix is diagonal, <span class="math inline">\(\Sigma=\operatorname{diag}\left\{\sigma_{1}^{2}, \ldots, \sigma_{m}^{2}\right\}\)</span>. In this case <span class="math inline">\(\Sigma^{-1 / 2} \bar{X}_{i}=\bar{X}_{i} \operatorname{diag}\left\{\boldsymbol{I}_{k_{1}} \sigma_{1}^{-1 / 2}, \ldots, \boldsymbol{I}_{k_{m}} \sigma_{m}^{-1 / 2}\right\}\)</span> from which you can calculate that <span class="math inline">\(\widehat{\beta}_{\text {sur }}=\widehat{\beta}_{\text {ols }}\)</span>. The intuition is that there is no difference in systems estimation when the equations are uncorrelated, which occurs when <span class="math inline">\(\Sigma\)</span> is diagonal.</p>
</section>
<section id="maximum-likelihood-estimator" class="level2" data-number="11.9">
<h2 data-number="11.9" class="anchored" data-anchor-id="maximum-likelihood-estimator"><span class="header-section-number">11.9</span> Maximum Likelihood Estimator</h2>
<p>Take the linear model under the assumption that the error is independent of the regressors and multivariate normally distributed. Thus <span class="math inline">\(Y=\bar{X} \beta+e\)</span> with <span class="math inline">\(e \sim \mathrm{N}(0, \Sigma)\)</span>. In this case we can consider the maximum likelihood estimator (MLE) of the coefficients.</p>
<p>It is convenient to reparameterize the covariance matrix in terms of its inverse <span class="math inline">\(S=\Sigma^{-1}\)</span>. With this reparameterization the conditional density of <span class="math inline">\(Y\)</span> given <span class="math inline">\(X=x\)</span> equals</p>
<p><span class="math display">\[
f(y \mid x)=\frac{\operatorname{det}(\boldsymbol{S})^{1 / 2}}{(2 \pi)^{m / 2}} \exp \left(-\frac{1}{2}(y-x \beta)^{\prime} \boldsymbol{S}(y-x \beta)\right) .
\]</span></p>
<p>The log-likelihood function for the sample is</p>
<p><span class="math display">\[
\ell_{n}(\beta, \boldsymbol{S})=-\frac{n m}{2} \log (2 \pi)+\frac{n}{2} \log (\operatorname{det}(\boldsymbol{S}))-\frac{1}{2} \sum_{i=1}^{n}\left(Y_{i}-\bar{X}_{i} \beta\right)^{\prime} S\left(Y_{i}-\bar{X}_{i} \beta\right) .
\]</span></p>
<p>The maximum likelihood estimator <span class="math inline">\(\left(\widehat{\beta}_{\text {mle }}, \widehat{S}_{\text {mle }}\right)\)</span> maximizes the log-likelihood function. The first order conditions are</p>
<p><span class="math display">\[
0=\left.\frac{\partial}{\partial \beta} \ell_{n}(\beta, \boldsymbol{S})\right|_{\beta=\widehat{\beta}, \boldsymbol{S}=\widehat{\boldsymbol{S}}}=\sum_{i=1}^{n} \bar{X}_{i} \widehat{\boldsymbol{S}}\left(Y_{i}-\bar{X}_{i} \widehat{\beta}\right)
\]</span></p>
<p>and</p>
<p><span class="math display">\[
0=\left.\frac{\partial}{\partial \boldsymbol{S}} \ell_{n}(\beta, \Sigma)\right|_{\beta=\widehat{\beta}, \boldsymbol{S}=\widehat{\boldsymbol{S}}}=\frac{n}{2} \widehat{\boldsymbol{S}}^{-1}-\frac{1}{2} \operatorname{tr}\left(\sum_{i=1}^{n}\left(Y_{i}-\bar{X}_{i} \widehat{\beta}\right)\left(Y_{i}-\bar{X}_{i} \widehat{\beta}\right)^{\prime}\right)
\]</span></p>
<p>The second equation uses the matrix results <span class="math inline">\(\frac{\partial}{\partial S} \log (\operatorname{det}(\boldsymbol{S}))=\boldsymbol{S}^{-1}\)</span> and <span class="math inline">\(\frac{\partial}{\partial \boldsymbol{B}} \operatorname{tr}(\boldsymbol{A B})=\boldsymbol{A}^{\prime}\)</span> from Appendix A.20.</p>
<p>Solving and making the substitution <span class="math inline">\(\widehat{\Sigma}=\widehat{\boldsymbol{S}}^{-1}\)</span> we obtain</p>
<p><span class="math display">\[
\begin{gathered}
\widehat{\beta}_{\mathrm{mle}}=\left(\sum_{i=1}^{n} \bar{X}_{i}^{\prime} \widehat{\Sigma}^{-1} \bar{X}_{i}\right)^{-1}\left(\sum_{i=1}^{n} \bar{X}_{i}^{\prime} \widehat{\Sigma}^{-1} Y_{i}\right) \\
\widehat{\Sigma}_{\mathrm{mle}}=\frac{1}{n} \sum_{i=1}^{n}\left(Y_{i}-\bar{X}_{i} \widehat{\beta}\right)\left(Y_{i}-\bar{X}_{i} \widehat{\beta}\right)^{\prime} .
\end{gathered}
\]</span></p>
<p>Notice that each equation refers to the other. Hence these are not closed-form expressions but can be solved via iteration. The solution is identical to the iterated SUR estimator. Thus the iterated SUR estimator is identical to MLE under normality.</p>
<p>Recall that the SUR estimator simplifies to OLS when the regressors are common across equations. The same occurs for the MLE. Thus when <span class="math inline">\(\bar{X}_{i}=\boldsymbol{I}_{m} \otimes X_{i}^{\prime}\)</span> we find that <span class="math inline">\(\widehat{\beta}_{\mathrm{mle}}=\widehat{\beta}_{\text {ols }}\)</span> and <span class="math inline">\(\widehat{\Sigma}_{\text {mle }}=\widehat{\Sigma}_{\text {ols }}\)</span>.</p>
</section>
<section id="restricted-estimation" class="level2" data-number="11.10">
<h2 data-number="11.10" class="anchored" data-anchor-id="restricted-estimation"><span class="header-section-number">11.10</span> Restricted Estimation</h2>
<p>In many multivariate regression applications it is desired to impose restrictions on the coefficients. In particular, cross-equation restrictions (for example, imposing Slutsky symmetry on a demand system) can be quite important and can only be imposed by a multivariate estimation method. Estimation subject to restrictions can be done by minimum distance, maximum likelihood, or the generalized method of moments.</p>
<p>Minimum distance is a straightforward application of the methods of Chapter 8 to the estimators presented in this chapter, as such methods apply to any asymptotically normal estimator.</p>
<p>Imposing restrictions on maximum likelihood is also straightforward. The likelihood is maximized subject to the imposed restrictions. One important example is explored in detail in the following section.</p>
<p>Generalized method of moments estimation of multivariate regression subject to restrictions will be explored in Section 13.18. This is a particularly simple and straightforward way to estimate restricted multivariate regression models and is our generally preferred approach.</p>
</section>
<section id="reduced-rank-regression" class="level2" data-number="11.11">
<h2 data-number="11.11" class="anchored" data-anchor-id="reduced-rank-regression"><span class="header-section-number">11.11</span> Reduced Rank Regression</h2>
<p>One context where systems estimation is important is when it is desired to impose or test restrictions across equations. Restricted systems are commonly estimated by maximum likelihood under normality. In this section we explore one important special case of restricted multivariate regression known as reduced rank regression. The model was originally proposed by Anderson (1951) and extended by Johansen (1995).</p>
<p>The unrestricted model is</p>
<p><span class="math display">\[
\begin{aligned}
Y &amp;=\boldsymbol{B}^{\prime} X+\boldsymbol{C}^{\prime} Z+e \\
\mathbb{E}\left[e e^{\prime} \mid X, Z\right] &amp;=\Sigma
\end{aligned}
\]</span></p>
<p>where <span class="math inline">\(\boldsymbol{B}\)</span> is <span class="math inline">\(k \times m, \boldsymbol{C}\)</span> is <span class="math inline">\(\ell \times m, Y \in \mathbb{R}^{m}, X \in \mathbb{R}^{k}\)</span>, and <span class="math inline">\(Z \in \mathbb{R}^{\ell}\)</span>. We separate the regressors as <span class="math inline">\(X\)</span> and <span class="math inline">\(Z\)</span> because the coefficient matrix <span class="math inline">\(\boldsymbol{B}\)</span> will be restricted while <span class="math inline">\(\boldsymbol{C}\)</span> will be unrestricted.</p>
<p>The matrix <span class="math inline">\(\boldsymbol{B}\)</span> is full rank if</p>
<p><span class="math display">\[
\operatorname{rank}(\boldsymbol{B})=\min (k, m) .
\]</span></p>
<p>The reduced rank restriction is <span class="math inline">\(\operatorname{rank}(\boldsymbol{B})=r&lt;\min (k, m)\)</span> for some known <span class="math inline">\(r\)</span>.</p>
<p>The reduced rank restriction implies that we can write the coefficient matrix <span class="math inline">\(\boldsymbol{B}\)</span> in the factored form <span class="math inline">\(\boldsymbol{B}=\boldsymbol{G} \boldsymbol{A}^{\prime}\)</span> where <span class="math inline">\(\boldsymbol{A}\)</span> is <span class="math inline">\(m \times r\)</span> and <span class="math inline">\(\boldsymbol{G}\)</span> is <span class="math inline">\(k \times r\)</span>. This representation is not unique as we can replace <span class="math inline">\(\boldsymbol{G}\)</span> with <span class="math inline">\(\boldsymbol{G} \boldsymbol{Q}\)</span> and <span class="math inline">\(\boldsymbol{A}\)</span> with <span class="math inline">\(\boldsymbol{A} \boldsymbol{Q}^{-1 \prime}\)</span> for any invertible <span class="math inline">\(\boldsymbol{Q}\)</span> and the same relation holds. Identification therefore requires a normalization of the coefficients. A conventional normalization is <span class="math inline">\(\boldsymbol{G}^{\prime} \boldsymbol{D} \boldsymbol{G}=\boldsymbol{I}_{r}\)</span> for given <span class="math inline">\(\boldsymbol{D}\)</span>.</p>
<p>Equivalently, the reduced rank restriction can be imposed by requiring that <span class="math inline">\(\boldsymbol{B}\)</span> satisfy the restriction <span class="math inline">\(\boldsymbol{B} \boldsymbol{A}_{\perp}=\boldsymbol{G} \boldsymbol{A}^{\prime} \boldsymbol{A}_{\perp}=0\)</span> for some <span class="math inline">\(m \times(m-r)\)</span> coefficient matrix <span class="math inline">\(\boldsymbol{A}_{\perp}\)</span>. Since <span class="math inline">\(\boldsymbol{G}\)</span> is full rank this requires that <span class="math inline">\(\boldsymbol{A}^{\prime} \boldsymbol{A}_{\perp}=0\)</span>, hence <span class="math inline">\(\boldsymbol{A}_{\perp}\)</span> is the orthogonal complement of <span class="math inline">\(\boldsymbol{A}\)</span>. Note that <span class="math inline">\(\boldsymbol{A}_{\perp}\)</span> is not unique as it can be replaced by <span class="math inline">\(\boldsymbol{A}_{\perp} \boldsymbol{Q}\)</span> for any <span class="math inline">\((m-r) \times(m-r)\)</span> invertible <span class="math inline">\(\boldsymbol{Q}\)</span>. Thus if <span class="math inline">\(\boldsymbol{A}_{\perp}\)</span> is to be estimated it requires a normalization.</p>
<p>We discuss methods for estimation of <span class="math inline">\(\boldsymbol{G}, \boldsymbol{A}, \Sigma, \boldsymbol{C}\)</span>, and <span class="math inline">\(\boldsymbol{A}_{\perp}\)</span>. The standard approach is maximum likelihood under the assumption that <span class="math inline">\(e \sim \mathrm{N}(0, \Sigma)\)</span>. The log-likelihood function for the sample is</p>
<p><span class="math display">\[
\begin{aligned}
\ell_{n}(\boldsymbol{G}, \boldsymbol{A}, \boldsymbol{C}, \Sigma) &amp;=-\frac{n m}{2} \log (2 \pi)-\frac{n}{2} \log (\operatorname{det}(\Sigma)) \\
&amp;-\frac{1}{2} \sum_{i=1}^{n}\left(Y_{i}-\boldsymbol{A} \boldsymbol{G}^{\prime} X_{i}-\boldsymbol{C}^{\prime} Z_{i}\right)^{\prime} \Sigma^{-1}\left(Y_{i}-\boldsymbol{A} \boldsymbol{G}^{\prime} X_{i}-\boldsymbol{C}^{\prime} Z_{i}\right) .
\end{aligned}
\]</span></p>
<p>Anderson (1951) derived the MLE by imposing the constraint <span class="math inline">\(\boldsymbol{B} \boldsymbol{A}_{\perp}=0\)</span> via the method of Lagrange multipliers. This turns out to be algebraically cumbersome.</p>
<p>Johansen (1995) instead proposed the following straightforward concentration method. Treating <span class="math inline">\(\boldsymbol{G}\)</span> as if it is known, maximize the log-likelihood with respect to the other parameters. Resubstituting these estimators we obtain the concentrated log-likelihood function with respect to <span class="math inline">\(\boldsymbol{G}\)</span>. This can be maximized to find the MLE for <span class="math inline">\(\boldsymbol{G}\)</span>. The other parameter estimators are then obtain by substitution. We now describe these steps in detail.</p>
<p>Given <span class="math inline">\(\boldsymbol{G}\)</span> the likelihood is a normal multivariate regression in the variables <span class="math inline">\(\boldsymbol{G}^{\prime} X\)</span> and <span class="math inline">\(Z\)</span>, so the MLE for <span class="math inline">\(\boldsymbol{A}, \boldsymbol{C}\)</span> and <span class="math inline">\(\Sigma\)</span> are least squares. In particular, using the Frisch-Waugh-Lovell residual regression formula we can write the estimators for <span class="math inline">\(\boldsymbol{A}\)</span> and <span class="math inline">\(\Sigma\)</span> as</p>
<p><span class="math display">\[
\widehat{\boldsymbol{A}}(\boldsymbol{G})=\left(\widetilde{\boldsymbol{Y}}^{\prime} \widetilde{\boldsymbol{X}} \boldsymbol{G}\right)\left(\boldsymbol{G}^{\prime} \widetilde{\boldsymbol{X}}^{\prime} \widetilde{\boldsymbol{X}} \boldsymbol{G}\right)^{-1}
\]</span></p>
<p>and</p>
<p><span class="math display">\[
\widehat{\Sigma}(\boldsymbol{G})=\frac{1}{n}\left(\widetilde{\boldsymbol{Y}}^{\prime} \tilde{\boldsymbol{Y}}-\widetilde{\boldsymbol{Y}}^{\prime} \widetilde{\boldsymbol{X}} \boldsymbol{G}\left(\boldsymbol{G}^{\prime} \widetilde{\boldsymbol{X}}^{\prime} \widetilde{\boldsymbol{X}} \boldsymbol{G}\right)^{-1} \boldsymbol{G}^{\prime} \widetilde{\boldsymbol{X}}^{\prime} \widetilde{\boldsymbol{Y}}\right)
\]</span></p>
<p>where <span class="math inline">\(\tilde{\boldsymbol{Y}}=\boldsymbol{Y}-\boldsymbol{Z}\left(\boldsymbol{Z}^{\prime} \boldsymbol{Z}\right)^{-1} \boldsymbol{Z}^{\prime} \boldsymbol{Y}\)</span> and <span class="math inline">\(\widetilde{\boldsymbol{X}}=\boldsymbol{X}-\boldsymbol{Z}\left(\boldsymbol{Z}^{\prime} \boldsymbol{Z}\right)^{-1} \boldsymbol{Z}^{\prime} \boldsymbol{X}\)</span>.</p>
<p>Substituting these estimators into the log-likelihood function we obtain the concentrated likelihood function, which is a function of <span class="math inline">\(G\)</span> only.</p>
<p><span class="math display">\[
\begin{aligned}
\widetilde{\ell}_{n}(\boldsymbol{G}) &amp;=\ell_{n}(\boldsymbol{G}, \widehat{\boldsymbol{A}}(\boldsymbol{G}), \widehat{\boldsymbol{C}}(\boldsymbol{G}), \widehat{\Sigma}(\boldsymbol{G})) \\
&amp;=\frac{m}{2}(n \log (2 \pi)-1)-\frac{n}{2} \log \left[\operatorname{det}\left(\widetilde{\boldsymbol{Y}}^{\prime} \widetilde{\boldsymbol{Y}}-\widetilde{\boldsymbol{Y}}^{\prime} \widetilde{\boldsymbol{X}} \boldsymbol{G}\left(\boldsymbol{G}^{\prime} \widetilde{\boldsymbol{X}}^{\prime} \widetilde{\boldsymbol{X}} \boldsymbol{G}\right)^{-1} \boldsymbol{G}^{\prime} \widetilde{\boldsymbol{X}}^{\prime} \widetilde{\boldsymbol{Y}}\right)\right] \\
&amp;=\frac{m}{2}(n \log (2 \pi)-1)-\frac{n}{2} \log \left(\operatorname{det}\left(\widetilde{\boldsymbol{Y}}^{\prime} \widetilde{\boldsymbol{Y}}\right)\right)-\frac{n}{2} \log \left[\frac{\operatorname{det}\left(\boldsymbol{G}^{\prime}\left(\widetilde{\boldsymbol{X}}^{\prime} \widetilde{\boldsymbol{X}}-\widetilde{\boldsymbol{X}}^{\prime} \tilde{\boldsymbol{Y}}\left(\widetilde{\boldsymbol{Y}}^{\prime} \widetilde{\boldsymbol{Y}}\right)^{-1} \boldsymbol{Y}^{\prime} \widetilde{\boldsymbol{X}}\right) \boldsymbol{G}\right)}{\operatorname{det}\left(\boldsymbol{G}^{\prime} \widetilde{\boldsymbol{X}}^{\prime} \widetilde{\boldsymbol{X}} \boldsymbol{G}\right)} .\right.
\end{aligned}
\]</span></p>
<p>The third equality uses Theorem A.1.8. The MLE <span class="math inline">\(\widehat{\boldsymbol{G}}\)</span> for <span class="math inline">\(\boldsymbol{G}\)</span> is the maximizer of <span class="math inline">\(\widetilde{\ell}_{n}(\boldsymbol{G})\)</span>, or equivalently equals</p>
<p><span class="math display">\[
\begin{aligned}
\widehat{\boldsymbol{G}} &amp;=\underset{\boldsymbol{G}}{\operatorname{argmin}} \frac{\operatorname{det}\left(\boldsymbol{G}^{\prime}\left(\widetilde{\boldsymbol{X}}^{\prime} \widetilde{\boldsymbol{X}}-\widetilde{\boldsymbol{X}}^{\prime} \widetilde{\boldsymbol{Y}}\left(\tilde{\boldsymbol{Y}}^{\prime} \widetilde{\boldsymbol{Y}}\right)^{-1} \boldsymbol{Y}^{\prime} \tilde{\boldsymbol{X}}\right) \boldsymbol{G}\right)}{\operatorname{det}\left(\boldsymbol{G}^{\prime} \widetilde{\boldsymbol{X}}^{\prime} \widetilde{\boldsymbol{X}} \boldsymbol{G}\right)} \\
&amp;=\underset{\boldsymbol{G}}{\operatorname{argmax}} \frac{\operatorname{det}\left(\boldsymbol{G}^{\prime} \widetilde{\boldsymbol{X}}^{\prime} \widetilde{\boldsymbol{Y}}\left(\widetilde{\boldsymbol{Y}}^{\prime} \widetilde{\boldsymbol{Y}}\right)^{-1} \boldsymbol{Y}^{\prime} \widetilde{\boldsymbol{X}} \boldsymbol{G}\right)}{\operatorname{det}\left(\boldsymbol{G}^{\prime} \widetilde{\boldsymbol{X}}^{\prime} \widetilde{\boldsymbol{X}} \boldsymbol{G}\right)} \\
&amp;=\left\{v_{1}, \ldots, v_{r}\right\}
\end{aligned}
\]</span></p>
<p>which are the generalized eigenvectors of <span class="math inline">\(\widetilde{\boldsymbol{X}}^{\prime} \tilde{\boldsymbol{Y}}\left(\tilde{\boldsymbol{Y}}^{\prime} \tilde{\boldsymbol{Y}}\right)^{-1} \boldsymbol{Y}^{\prime} \tilde{\boldsymbol{X}}\)</span> with respect to <span class="math inline">\(\widetilde{\boldsymbol{X}}^{\prime} \widetilde{\boldsymbol{X}}\)</span> corresponding to the <span class="math inline">\(r\)</span> largest generalized eigenvalues. (Generalized eigenvalues and eigenvectors are discussed in Section A.14.) The estimator satisfies the normalization <span class="math inline">\(\widehat{\boldsymbol{G}}^{\prime} \widetilde{\boldsymbol{X}}^{\prime} \widetilde{\boldsymbol{X}} \widehat{\boldsymbol{G}}=\boldsymbol{I}_{r}\)</span>. Letting <span class="math inline">\(v_{j}^{*}\)</span> denote the eigenvectors of (11.20) we can also express <span class="math inline">\(\widehat{\boldsymbol{G}}=\left\{v_{m}^{*}, \ldots, v_{m-r+1}^{*}\right\}\)</span>.</p>
<p>This is computationally straightforward. In MATLAB, for example, the generalized eigenvalues and eigenvectors of a matrix <span class="math inline">\(\boldsymbol{A}\)</span> with respect to <span class="math inline">\(\boldsymbol{B}\)</span> are found using the command eig <span class="math inline">\((\mathrm{A}, \mathrm{B})\)</span>.</p>
<p>Given <span class="math inline">\(\widehat{\boldsymbol{G}}\)</span>, the MLE <span class="math inline">\(\widehat{\boldsymbol{A}}, \widehat{\boldsymbol{C}}, \widehat{\Sigma}\)</span> are found by least squares regression of <span class="math inline">\(Y\)</span> on <span class="math inline">\(\widehat{\boldsymbol{G}}^{\prime} X\)</span> and <span class="math inline">\(Z\)</span>. In particular, <span class="math inline">\(\widehat{\boldsymbol{A}}=\widehat{\boldsymbol{G}}^{\prime} \widetilde{\boldsymbol{X}}^{\prime} \tilde{\boldsymbol{Y}}\)</span> because <span class="math inline">\(\widehat{\boldsymbol{G}}^{\prime} \widetilde{\boldsymbol{X}}^{\prime} \widetilde{\boldsymbol{X}} \widehat{\boldsymbol{G}}=\boldsymbol{I}_{r}\)</span> We now discuss the estimator <span class="math inline">\(\widehat{\boldsymbol{A}}_{\perp}\)</span> of <span class="math inline">\(\boldsymbol{A}_{\perp}\)</span>. It turns out that</p>
<p><span class="math display">\[
\begin{aligned}
\widehat{\boldsymbol{A}}_{\perp} &amp;=\underset{\boldsymbol{A}}{\operatorname{argmax}} \frac{\operatorname{det}\left(\boldsymbol{A}^{\prime}\left(\tilde{\boldsymbol{Y}}^{\prime} \tilde{\boldsymbol{Y}}-\tilde{\boldsymbol{Y}}^{\prime} \tilde{\boldsymbol{X}}\left(\tilde{\boldsymbol{X}}^{\prime} \tilde{\boldsymbol{X}}\right)^{-1} \widetilde{\boldsymbol{X}}^{\prime} \tilde{\boldsymbol{Y}}\right) \boldsymbol{A}\right)}{\operatorname{det}\left(\boldsymbol{A}^{\prime} \tilde{\boldsymbol{Y}}^{\prime} \tilde{\boldsymbol{Y}} \boldsymbol{A}\right)} \\
&amp;=\left\{w_{1}, \ldots, w_{m-r}\right\}
\end{aligned}
\]</span></p>
<p>the eigenvectors of <span class="math inline">\(\tilde{\boldsymbol{Y}}^{\prime} \tilde{\boldsymbol{Y}}-\tilde{\boldsymbol{Y}}^{\prime} \tilde{\boldsymbol{X}}\left(\tilde{\boldsymbol{X}}^{\prime} \tilde{\boldsymbol{X}}\right)^{-1} \tilde{\boldsymbol{X}}^{\prime} \tilde{\boldsymbol{Y}}\)</span> with respect to <span class="math inline">\(\tilde{\boldsymbol{Y}}^{\prime} \tilde{\boldsymbol{Y}}\)</span> associated with the largest <span class="math inline">\(m-r\)</span> eigenvalues.</p>
<p>By the dual eigenvalue relation (Theorem A.5), equations (11.20) and (11.21) have the same non-zero eigenvalues <span class="math inline">\(\lambda_{j}\)</span> and the associated eigenvectors <span class="math inline">\(v_{j}^{*}\)</span> and <span class="math inline">\(w_{j}\)</span> satisfy the relationship</p>
<p><span class="math display">\[
w_{j}=\lambda_{j}^{-1 / 2}\left(\tilde{\boldsymbol{Y}}^{\prime} \tilde{\boldsymbol{Y}}\right)^{-1} \tilde{\boldsymbol{Y}}^{\prime} \widetilde{\boldsymbol{X}} v_{j}^{*}
\]</span></p>
<p>Letting <span class="math inline">\(\Lambda=\operatorname{diag}\left\{\lambda_{m}, \ldots, \lambda_{m-r+1}\right\}\)</span> this implies</p>
<p><span class="math display">\[
\left\{w_{m}, \ldots, w_{m-r+1}\right\}=\left(\widetilde{\boldsymbol{Y}}^{\prime} \widetilde{\boldsymbol{Y}}\right)^{-1} \tilde{\boldsymbol{Y}}^{\prime} \widetilde{\boldsymbol{X}}\left\{v_{m}^{*}, \ldots, v_{m-r+1}^{*}\right\} \Lambda=\left(\tilde{\boldsymbol{Y}}^{\prime} \widetilde{\boldsymbol{Y}}\right)^{-1} \widehat{\boldsymbol{A}} \Lambda .
\]</span></p>
<p>The second equality holds because <span class="math inline">\(\widehat{\boldsymbol{G}}=\left\{v_{m}^{*}, \ldots, v_{m-r+1}^{*}\right\}\)</span> and <span class="math inline">\(\widehat{\boldsymbol{A}}=\widetilde{\boldsymbol{Y}}^{\prime} \widetilde{\boldsymbol{X}} \widehat{\boldsymbol{G}}\)</span>. Since the eigenvectors <span class="math inline">\(w_{j}\)</span> satisfy the orthogonality property <span class="math inline">\(w_{j}^{\prime} \widetilde{\boldsymbol{Y}}^{\prime} \widetilde{\boldsymbol{Y}} w_{\ell}=0\)</span> for <span class="math inline">\(j \neq \ell\)</span>, it follows that</p>
<p><span class="math display">\[
0=\widehat{\boldsymbol{A}}_{\perp}^{\prime} \tilde{\boldsymbol{Y}}^{\prime} \tilde{\boldsymbol{Y}}\left\{w_{m}, \ldots, w_{m-r+1}\right\}=\widehat{\boldsymbol{A}}_{\perp}^{\prime} \widehat{\boldsymbol{A}} \Lambda .
\]</span></p>
<p>Since <span class="math inline">\(\Lambda&gt;0\)</span> we conclude that <span class="math inline">\(\widehat{A}_{\perp}^{\prime} \widehat{A}=0\)</span> as desired.</p>
<p>The solution <span class="math inline">\(\widehat{A}_{\perp}\)</span> in (11.21) can be represented several ways. One which is computationally convenient is to observe that</p>
<p><span class="math display">\[
\tilde{\boldsymbol{Y}}^{\prime} \widetilde{\boldsymbol{Y}}-\widetilde{\boldsymbol{Y}}^{\prime} \widetilde{\boldsymbol{X}}\left(\widetilde{\boldsymbol{X}}^{\prime} \widetilde{\boldsymbol{X}}\right)^{-1} \widetilde{\boldsymbol{Y}}^{\prime} \widetilde{\boldsymbol{X}}=\boldsymbol{Y}^{\prime} \boldsymbol{M}_{\boldsymbol{X}, \boldsymbol{Z}} \boldsymbol{Y}=\widetilde{\boldsymbol{E}}^{\prime} \widetilde{\boldsymbol{E}}
\]</span></p>
<p>where <span class="math inline">\(\boldsymbol{M}_{\boldsymbol{X}, \boldsymbol{Z}}=\boldsymbol{I}_{n}-(\boldsymbol{X}, \boldsymbol{Z})\left((\boldsymbol{X}, \boldsymbol{Z})^{\prime}(\boldsymbol{X}, \boldsymbol{Z})\right)^{-1}(\boldsymbol{X}, \boldsymbol{Z})^{\prime}\)</span> and <span class="math inline">\(\widetilde{\boldsymbol{E}}=\boldsymbol{M}_{\boldsymbol{X}, \boldsymbol{Z}} \boldsymbol{Y}\)</span> is the residual matrix from the unrestricted multivariate least squares regression of <span class="math inline">\(Y\)</span> on <span class="math inline">\(X\)</span> and <span class="math inline">\(\boldsymbol{Z}\)</span>. The first equality follows by the FrischWaugh-Lovell theorem. This shows that <span class="math inline">\(\widehat{\boldsymbol{A}}_{\perp}\)</span> are the generalized eigenvectors of <span class="math inline">\(\widetilde{\boldsymbol{E}}^{\prime} \widetilde{\boldsymbol{E}}\)</span> with respect to <span class="math inline">\(\widetilde{\boldsymbol{Y}}^{\prime} \tilde{\boldsymbol{Y}}\)</span> corresponding to the <span class="math inline">\(m-r\)</span> largest eigenvalues. In MATLAB, for example, these can be computed using the eig <span class="math inline">\((\mathrm{A}, \mathrm{B})\)</span> command.</p>
<p>Another representation is to write <span class="math inline">\(\boldsymbol{M}_{Z}=\boldsymbol{I}_{n}-\boldsymbol{Z}\left(\boldsymbol{Z}^{\prime} \boldsymbol{Z}\right)^{-1} \boldsymbol{Z}^{\prime}\)</span> so that</p>
<p><span class="math display">\[
\widehat{A}_{\perp}=\underset{A}{\operatorname{argmax}} \frac{\operatorname{det}\left(\boldsymbol{A}^{\prime} \boldsymbol{Y}^{\prime} \boldsymbol{M}_{\boldsymbol{X}, \boldsymbol{Z}} \boldsymbol{Y} \boldsymbol{A}\right)}{\operatorname{det}\left(\boldsymbol{A}^{\prime} \boldsymbol{Y}^{\prime} \boldsymbol{M}_{\boldsymbol{Z}} \boldsymbol{Y} \boldsymbol{A}\right)}=\underset{\boldsymbol{A}}{\operatorname{argmin}} \frac{\operatorname{det}\left(\boldsymbol{A}^{\prime} \boldsymbol{Y}^{\prime} \boldsymbol{M}_{\boldsymbol{Z}} \boldsymbol{Y} \boldsymbol{A}\right)}{\operatorname{det}\left(\boldsymbol{A}^{\prime} \boldsymbol{Y}^{\prime} \boldsymbol{M}_{\boldsymbol{X}, \boldsymbol{Z}} \boldsymbol{Y} \boldsymbol{A}\right)} .
\]</span></p>
<p>We summarize our findings. Theorem 11.7 The MLE for the reduced rank model (11.19) under <span class="math inline">\(e \sim \mathrm{N}(0, \Sigma)\)</span> is given as follows. Let <span class="math inline">\(\tilde{\boldsymbol{Y}}\)</span> and <span class="math inline">\(\widetilde{\boldsymbol{X}}\)</span> be the residual matrices from multivariate regression of <span class="math inline">\(\boldsymbol{Y}\)</span> and <span class="math inline">\(\boldsymbol{X}\)</span> on <span class="math inline">\(\boldsymbol{Z}\)</span>, respectively. Then <span class="math inline">\(\widehat{\boldsymbol{G}}_{\mathrm{mle}}=\left\{v_{1}, \ldots, v_{r}\right\}\)</span>, the generalized eigenvectors of <span class="math inline">\(\widetilde{\boldsymbol{X}}^{\prime} \tilde{\boldsymbol{Y}}\left(\widetilde{\boldsymbol{Y}}^{\prime} \tilde{\boldsymbol{Y}}\right)^{-1} \boldsymbol{Y}^{\prime} \widetilde{\boldsymbol{X}}\)</span> with respect to <span class="math inline">\(\widetilde{\boldsymbol{X}}^{\prime} \widetilde{\boldsymbol{X}}\)</span> corresponding to the <span class="math inline">\(r\)</span> largest eigenvalues <span class="math inline">\(\widehat{\lambda}_{j} . \widehat{\boldsymbol{A}}_{\text {mle }}, \widehat{\boldsymbol{C}}_{\text {mle }}\)</span> and <span class="math inline">\(\widehat{\Sigma}_{\text {mle }}\)</span> are obtained by the least squares regression</p>
<p><span class="math display">\[
\begin{aligned}
Y_{i} &amp;=\widehat{\boldsymbol{A}}_{\mathrm{mle}} \widehat{\boldsymbol{G}}_{\mathrm{mle}}^{\prime} X_{i}+\widehat{\boldsymbol{C}}_{\mathrm{mle}}^{\prime} Z_{i}+\widehat{e}_{i} \\
\widehat{\Sigma}_{\mathrm{mle}} &amp;=\frac{1}{n} \sum_{i=1}^{n} \widehat{e}_{i} \widehat{e}_{i}^{\prime}
\end{aligned}
\]</span></p>
<p>Let <span class="math inline">\(\widetilde{\boldsymbol{E}}\)</span> be the residual matrix from a multivariate regression of <span class="math inline">\(\boldsymbol{Y}\)</span> on <span class="math inline">\(\boldsymbol{X}\)</span> and <span class="math inline">\(\boldsymbol{Z}\)</span>. Then <span class="math inline">\(\widehat{\boldsymbol{A}}_{\perp}\)</span> equals the generalized eigenvectors of <span class="math inline">\(\widetilde{\boldsymbol{E}} \widetilde{\boldsymbol{E}}^{\prime}\)</span> with respect to <span class="math inline">\(\widetilde{\boldsymbol{Y}}^{\prime} \widetilde{\boldsymbol{Y}}\)</span> corresponding to the <span class="math inline">\(m-r\)</span> smallest eigenvalues. The maximized likelihood equals</p>
<p><span class="math display">\[
\ell_{n}=\frac{m}{2}(n \log (2 \pi)-1)-\frac{n}{2} \log \left(\operatorname{det}\left(\widetilde{\boldsymbol{Y}}^{\prime} \tilde{\boldsymbol{Y}}\right)\right)-\frac{n}{2} \sum_{j=1}^{r} \log \left(1-\widehat{\lambda}_{j}\right) .
\]</span></p>
<p>An R package for reduced rank regression is “RRR”. I am unaware of a Stata command.</p>
</section>
<section id="principal-component-analysis" class="level2" data-number="11.12">
<h2 data-number="11.12" class="anchored" data-anchor-id="principal-component-analysis"><span class="header-section-number">11.12</span> Principal Component Analysis</h2>
<p>In Section <span class="math inline">\(4.21\)</span> we described the Duflo, Dupas, and Kremer (2011) dataset which is a sample of Kenyan first grade test scores. Following the authors we focused on the variable totalscore which is each student’s composite test score. If you examine the data file you will find other pieces of information about the students’ performance, including each student’s score on separate sections of the test, with the labels wordscore (word recognition), sentscore (sentence recognition), letterscore (letter recognition), spellscore (spelling), additions_score (addition), substractions_score (subtraction), multiplications_score (multiplication). The “total” score sums the scores from the individual sections. Perhaps there is more information in the section scores. How can we learn about this from the data?</p>
<p>Principal component analysis (PCA) addresses this issue by ordering linear combinations by their contribution to variance. Definition <span class="math inline">\(11.1\)</span> Let <span class="math inline">\(X\)</span> be a <span class="math inline">\(k \times 1\)</span> random vector.</p>
<p>The first principal component is <span class="math inline">\(U_{1}=h_{1}^{\prime} X\)</span> where <span class="math inline">\(h_{1}\)</span> satisfies</p>
<p><span class="math display">\[
h_{1}=\underset{h^{\prime} h=1}{\operatorname{argmax}} \operatorname{var}\left[h^{\prime} X\right] .
\]</span></p>
<p>The second principal component is <span class="math inline">\(U_{2}=h_{2}^{\prime} X\)</span> where</p>
<p><span class="math display">\[
h_{2}=\underset{h^{\prime} h=1, h^{\prime} h_{1}=0}{\operatorname{argmax}} \operatorname{var}\left[h^{\prime} X\right] .
\]</span></p>
<p>In general, the <span class="math inline">\(j^{t h}\)</span> principal component is <span class="math inline">\(U_{j}=h_{j}^{\prime} X\)</span> where</p>
<p><span class="math display">\[
h_{j}=\underset{h^{\prime} h=1, h^{\prime} h_{1}=0, \ldots, h^{\prime} h_{j-1}=0}{\operatorname{argmax}} \operatorname{var}\left[h^{\prime} X\right] .
\]</span></p>
<p>The principal components of <span class="math inline">\(X\)</span> are linear combinations <span class="math inline">\(h^{\prime} X\)</span> ranked by contribution to variance. By the properties of quadratic forms (Section A.15) the weight vectors <span class="math inline">\(h_{j}\)</span> are the eigenvectors of <span class="math inline">\(\Sigma=\operatorname{var}[X]\)</span>.</p>
<p>Theorem 11.8 The principal components of <span class="math inline">\(X\)</span> are <span class="math inline">\(U_{j}=h_{j}^{\prime} X\)</span>, where <span class="math inline">\(h_{j}\)</span> is the eigenvector of <span class="math inline">\(\Sigma\)</span> associated with the <span class="math inline">\(j^{\text {th }}\)</span> ordered eigenvalue <span class="math inline">\(\lambda_{j}\)</span> of <span class="math inline">\(\Sigma\)</span>.</p>
<p>Another way to see the PCA construction is as follows. Since <span class="math inline">\(\Sigma\)</span> is symmetric the spectral decomposition (Theorem A.3) states that <span class="math inline">\(\Sigma=\boldsymbol{H} \boldsymbol{D} \boldsymbol{H}^{\prime}\)</span> where <span class="math inline">\(\boldsymbol{H}=\left[h_{1}, \ldots, h_{k}\right]\)</span> and <span class="math inline">\(\boldsymbol{D}=\operatorname{diag}\left(d_{1}, \ldots, d_{k}\right)\)</span> are the eigenvectors and eigenvalues of <span class="math inline">\(\Sigma\)</span>. Since <span class="math inline">\(\Sigma\)</span> is positive semi-definite the eigenvalues are real, non-negative, and ordered <span class="math inline">\(d_{1} \geq d_{2} \geq \cdots \geq d_{k}\)</span>. Let <span class="math inline">\(U=\left(U_{1}, \ldots, U_{k}\right)\)</span> be the principal components of <span class="math inline">\(X\)</span>. By Theorem 11.8, <span class="math inline">\(U=\boldsymbol{H}^{\prime} X\)</span>. The covariance matrix of <span class="math inline">\(U\)</span> is</p>
<p><span class="math display">\[
\operatorname{var}[U]=\operatorname{var}\left[\boldsymbol{H}^{\prime} X\right]=\boldsymbol{H}^{\prime} \Sigma \boldsymbol{H}=\boldsymbol{D}
\]</span></p>
<p>which is diagonal. This shows that <span class="math inline">\(\operatorname{var}\left[U_{j}\right]=d_{j}\)</span> and the principal components are mutually uncorrelated. The relative variance contribution of the <span class="math inline">\(j^{t h}\)</span> principal component is <span class="math inline">\(d_{j} / \operatorname{tr}(\Sigma)\)</span>.</p>
<p>Principal components are sensitive to the scaling of <span class="math inline">\(X\)</span>. Consequently, it is recommended to first scale each element of <span class="math inline">\(X\)</span> to have mean zero and unit variance. In this case <span class="math inline">\(\Sigma\)</span> is a correlation matrix.</p>
<p>The sample principal components are obtained by replacing the unknowns by sample estimators. Let <span class="math inline">\(\widehat{\Sigma}\)</span> be the sample covariance or correlation matrix and <span class="math inline">\(\widehat{h}_{1}, \widehat{h}_{2}, \ldots, \widehat{h}_{k}\)</span> its ordered eigenvectors. The sample principal components are <span class="math inline">\(\widehat{h}_{j}^{\prime} X_{i}\)</span>.</p>
<p>To illustrate we use the Duflo, Dupas, and Kremer (2011) dataset. In Table <span class="math inline">\(11.1\)</span> we display the seven eigenvalues of the sample correlation matrix for the seven test scores described above. The seven eigenvalues sum to seven because we have applied PCA to the correlation matrix. The first eigenvalue is <span class="math inline">\(4.0\)</span>, implying that the first principal component explains <span class="math inline">\(57 %\)</span> of the variance of the seven test scores. The second eigenvalue is <span class="math inline">\(1.0\)</span>, implying that the second principal component explains <span class="math inline">\(15 %\)</span> of the variance. Together the first two components explain <span class="math inline">\(72 %\)</span> of the variance of the seven test scores.</p>
<p>In Table <span class="math inline">\(11.2\)</span> we display the weight vectors (eigenvectors) for the first two principal components. The weights for the first component are all positive and similar in magnitude. This means that the first Table 11.1: Eigenvalue Decomposition of Sample Correlation Matrix</p>
<p>|Eigenvalue|Proportion|</p>
<p>|:|———-|———-| |1| <span class="math inline">\(4.02\)</span> | <span class="math inline">\(0.57\)</span> | |2| <span class="math inline">\(1.04\)</span> | <span class="math inline">\(0.15\)</span> | |3| <span class="math inline">\(0.57\)</span> | <span class="math inline">\(0.08\)</span> | |4| <span class="math inline">\(0.52\)</span> | <span class="math inline">\(0.08\)</span> | |5| <span class="math inline">\(0.37\)</span> | <span class="math inline">\(0.05\)</span> | |6| <span class="math inline">\(0.29\)</span> | <span class="math inline">\(0.04\)</span> | |7| <span class="math inline">\(0.19\)</span> | <span class="math inline">\(0.03\)</span> |</p>
<p>Table 11.2: Principal Component Weight Vectors</p>
<table class="table">
<thead>
<tr class="header">
<th style="text-align: left;"></th>
<th>First</th>
<th>Second</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">words</td>
<td><span class="math inline">\(0.41\)</span></td>
<td><span class="math inline">\(-0.32\)</span></td>
</tr>
<tr class="even">
<td style="text-align: left;">sentences</td>
<td><span class="math inline">\(0.32\)</span></td>
<td><span class="math inline">\(-0.49\)</span></td>
</tr>
<tr class="odd">
<td style="text-align: left;">letters</td>
<td><span class="math inline">\(0.40\)</span></td>
<td><span class="math inline">\(-0.13\)</span></td>
</tr>
<tr class="even">
<td style="text-align: left;">spelling</td>
<td><span class="math inline">\(0.43\)</span></td>
<td><span class="math inline">\(-0.28\)</span></td>
</tr>
<tr class="odd">
<td style="text-align: left;">addition</td>
<td><span class="math inline">\(0.38\)</span></td>
<td><span class="math inline">\(0.41\)</span></td>
</tr>
<tr class="even">
<td style="text-align: left;">subtraction</td>
<td><span class="math inline">\(0.35\)</span></td>
<td><span class="math inline">\(0.52\)</span></td>
</tr>
<tr class="odd">
<td style="text-align: left;">multiplication</td>
<td><span class="math inline">\(0.33\)</span></td>
<td><span class="math inline">\(0.36\)</span></td>
</tr>
</tbody>
</table>
<p>principal component is similar to a simple average of the seven test scores. This is quite fascinating. This is consistent with our intuition that a simple average (e.g.&nbsp;the variable totalscore) captures most of the information contained in the seven test scores. The weights for the second component have a different pattern. The four literacy scores receive negative weight and the three math scores receive positive weight with similar magnitudes. This means that the second principal component is similar to the difference between a student’s math and verbal test scores. Taken together, the information in the first two principal components is equivalent to “average verbal” and “average math” test scores. What this shows is that <span class="math inline">\(57 %\)</span> of the variation in the seven section test scores can be explained by a simple average (e.g.&nbsp;totalscore), and <span class="math inline">\(72 %\)</span> can be explained by averages for the verbal and math halves of the test.</p>
<p>In Stata, principal components analysis can be implemented with the pca command. In <span class="math inline">\(\mathrm{R}\)</span> use prcomp or princomp. All three can be applied to either covariance matrices (unscaled data) or correlation matrices (normalized data) but they have different default settings. The Stata pca command by default normalizes the observations. The R commands by default do not normalize the observations.</p>
</section>
<section id="factor-models" class="level2" data-number="11.13">
<h2 data-number="11.13" class="anchored" data-anchor-id="factor-models"><span class="header-section-number">11.13</span> Factor Models</h2>
<p>Closely related to principal components are factor models. These are statistical models which decompose random vectors into common factors and idiosyncratic errors. Factor models are popular throughout the social sciences. Consequently a variety of estimation methods have been developed. In the next few sections we focus on methods which are popular among economists.</p>
<p>Let <span class="math inline">\(X=\left(X_{1}, \ldots, X_{k}\right)^{\prime}\)</span> be a <span class="math inline">\(k \times 1\)</span> random vector (for example the seven test scores described in the previous section). Assume that the elements of <span class="math inline">\(X\)</span> are scaled to have mean zero and unit variance.</p>
<p>A single factor model for <span class="math inline">\(X\)</span> is</p>
<p><span class="math display">\[
X=\lambda F+u
\]</span></p>
<p>where <span class="math inline">\(\lambda \in \mathbb{R}^{k}\)</span> are factor loadings, <span class="math inline">\(F \in \mathbb{R}\)</span> is a common factor, and <span class="math inline">\(u \in \mathbb{R}^{k}\)</span> is a random error. The factor <span class="math inline">\(F\)</span> is individual-specific while the coefficient <span class="math inline">\(\lambda\)</span> is common across individuals. The model (11.22) specifies that correlation between the elements of <span class="math inline">\(X\)</span> is due to the common factor <span class="math inline">\(F\)</span>. In the student test score example it is intuitive to think of <span class="math inline">\(F\)</span> as a student’s scholastic “aptitude”; in this case the vector <span class="math inline">\(\lambda\)</span> describes how scholastic aptitude affects the seven subject scores.</p>
<p>A multiple factor model has <span class="math inline">\(r&lt;k\)</span> factors. We write the model as</p>
<p><span class="math display">\[
X=\Lambda F+u
\]</span></p>
<p>where <span class="math inline">\(\Lambda\)</span> is a <span class="math inline">\(k \times r\)</span> matrix of factor loadings and <span class="math inline">\(F=\left(F_{1}, \ldots, F_{r}\right)^{\prime}\)</span> is an <span class="math inline">\(r \times 1\)</span> vector of factors. In the student test score example possible factors could be “math aptitude”, “language skills”, “social skills”, “artistic ability”, “creativity”, etc. The factor loading matrix <span class="math inline">\(\Lambda\)</span> indicates the effect of each factor on each test score. The number of factors <span class="math inline">\(r\)</span> is taken as known. We discuss selection of <span class="math inline">\(r\)</span> later.</p>
<p>The error vector <span class="math inline">\(u\)</span> is assumed to be mean zero, uncorrelated with <span class="math inline">\(F\)</span>, and (under correct specification) to have mutually uncorrelated elements. We write its covariance matrix as <span class="math inline">\(\Psi=\mathbb{E}\left[u u^{\prime}\right]\)</span>. The factor vector <span class="math inline">\(F\)</span> can either be treated as random or as a regressor. In this section we treat <span class="math inline">\(F\)</span> as random; in the next we treat <span class="math inline">\(F\)</span> as regressors. The random factors <span class="math inline">\(F\)</span> are assumed mean zero and are normalized so that <span class="math inline">\(\mathbb{E}\left[F F^{\prime}\right]=\)</span> <span class="math inline">\(\boldsymbol{I}_{r}\)</span></p>
<p>The assumptions imply that the correlation matrix <span class="math inline">\(\Sigma=\mathbb{E}\left[X X^{\prime}\right]\)</span> equals</p>
<p><span class="math display">\[
\Sigma=\Lambda \Lambda^{\prime}+\Psi .
\]</span></p>
<p>The factor analysis literature often describes <span class="math inline">\(\Lambda \Lambda^{\prime}\)</span> as the communality and the idiosyncratic error matrix <span class="math inline">\(\Psi\)</span> as the uniqueness. The former is the portion of the variance which is explained by the factor model and the latter is the unexplained portion of the variance.</p>
<p>The model is often <span class="math inline">\({ }^{1}\)</span> estimated by maximum likelihood. Under joint normality of <span class="math inline">\((F, u)\)</span> the distribution of <span class="math inline">\(X\)</span> is <span class="math inline">\(\mathrm{N}\left(0, \Lambda \Lambda^{\prime}+\Psi\right)\)</span>. The parameters are <span class="math inline">\(\Lambda\)</span> and <span class="math inline">\(\Psi=\operatorname{diag}\left(\psi_{1}, \ldots, \psi_{k}\right)\)</span>. The log-likelihood function of a random sample <span class="math inline">\(\left(X_{1}, \ldots, X_{n}\right)\)</span> is</p>
<p><span class="math display">\[
\ell_{n}(\Lambda, \Psi)=-\frac{n k}{2} \log (2 \pi)-\frac{n}{2} \log \operatorname{det}\left(\Lambda \Lambda^{\prime}+\Psi\right)-\frac{n}{2} \operatorname{tr}\left(\left(\Lambda \Lambda^{\prime}+\Psi\right)^{-1} \widehat{\Sigma}\right) .
\]</span></p>
<p>The <span class="math inline">\(\operatorname{MLE}(\widehat{\Lambda}, \widehat{\Psi})\)</span> maximizes <span class="math inline">\(\ell_{n}(\Lambda, \Psi)\)</span>. There is not an algebraic solution so the estimator is found using numerical methods. Fortunately, computational algorithms are available in standard packages. A detailed description and analysis can be found in Anderson (2003, Chapter 14).</p>
<p>The form of the log-likelihood is intriguing. Notice that the log-likelihood is only a function of the observations through its correlation matrix <span class="math inline">\(\widehat{\Sigma}\)</span>, and only a function of the parameters through the population correlation matrix <span class="math inline">\(\Lambda \Lambda^{\prime}+\Psi\)</span>. The final term in (11.25) is a measure of the match between <span class="math inline">\(\widehat{\Sigma}\)</span> and <span class="math inline">\(\Lambda \Lambda^{\prime}+\Psi\)</span>. Together, we see that the Gaussian log-likelihood is essentially a measure of the fit of the model and sample correlation matrices. It is therefore not reliant on the normality assumption.</p>
<p>It is often of interest to estimate the factors <span class="math inline">\(F_{i}\)</span>. Given <span class="math inline">\(\Lambda\)</span> the equation <span class="math inline">\(X_{i}=\Lambda F_{i}+u_{i}\)</span> can be viewed as a regression with coefficient <span class="math inline">\(F_{i}\)</span>. Its least squares estimator is <span class="math inline">\(\widehat{F}_{i}=\left(\Lambda^{\prime} \Lambda\right)^{-1} \Lambda^{\prime} X_{i}\)</span>. The GLS estimator (taking into account the covariance matrix of <span class="math inline">\(\left.u_{i}\right)\)</span> is <span class="math inline">\(\widehat{F}_{i}=\left(\Lambda^{\prime} \Psi^{-1} \Lambda\right)^{-1} \Lambda^{\prime} \Psi^{-1} X_{i}\)</span>. This motivates the Bartlett scoring estimator</p>
<p><span class="math display">\[
\widetilde{F}_{i}=\left(\widehat{\Lambda}^{\prime} \widehat{\Psi}^{-1} \widehat{\Lambda}\right)^{-1} \widehat{\Lambda}^{\prime} \widehat{\Psi}^{-1} X_{i} .
\]</span></p>
<p>The idealized version satisfies</p>
<p><span class="math display">\[
\widehat{F}_{i}=\left(\Lambda^{\prime} \Psi^{-1} \Lambda\right)^{-1} \Lambda^{\prime} \Psi^{-1}\left(\Lambda F_{i}+u_{i}\right)=F_{i}+\left(\Lambda^{\prime} \Psi^{-1} \Lambda\right)^{-1} \Lambda^{\prime} \Psi^{-1} u_{i}
\]</span></p>
<p><span class="math inline">\({ }^{1}\)</span> There are other estimators used in applied factor analysis. However there is little reason to consider estimators beyond the MLE of this section and the principal components estimator of the next section. which is unbiased for <span class="math inline">\(F_{i}\)</span> and has variance <span class="math inline">\(\left(\Lambda^{\prime} \Psi^{-1} \Lambda\right)^{-1}\)</span>. Thus the Barlett scoring estimator is typically described as “unbiased” though this is actually a property of its idealized version <span class="math inline">\(\widehat{F}_{i}\)</span>.</p>
<p>A second estimator for the factors can be constructed from the multivariate linear projection of <span class="math inline">\(F\)</span> on <span class="math inline">\(X\)</span>. This is <span class="math inline">\(F=A X+\xi\)</span> where the coefficient matrix <span class="math inline">\(\boldsymbol{A}\)</span> is <span class="math inline">\(r \times k\)</span>. The coefficient matrix equals</p>
<p><span class="math display">\[
\boldsymbol{A}=\mathbb{E}\left[F X^{\prime}\right] \mathbb{E}\left[X X^{\prime}\right]^{-1}=\Lambda^{\prime} \Sigma^{-1},
\]</span></p>
<p>the second equation using <span class="math inline">\(\mathbb{E}\left[F X^{\prime}\right]=\mathbb{E}\left[F(\Lambda F+u)^{\prime}\right]=\mathbb{E}\left[F F^{\prime}\right] \Lambda^{\prime}+\mathbb{E}\left[F u^{\prime}\right]=\Lambda^{\prime}\)</span>. The predicted value of <span class="math inline">\(F_{i}\)</span> is <span class="math inline">\(F_{i}^{*}=\boldsymbol{A} X_{i}=\Lambda^{\prime} \Sigma^{-1} X_{i}\)</span>. This motivates the regression scoring estimator</p>
<p><span class="math display">\[
\bar{F}_{i}=\widehat{\Lambda}^{\prime} \widehat{\Sigma}^{-1} X_{i} .
\]</span></p>
<p>The idealized version <span class="math inline">\(F_{i}^{*}\)</span> has conditional expectation <span class="math inline">\(\Lambda^{\prime} \Sigma^{-1} \Lambda F_{i}\)</span> and is thus biased for <span class="math inline">\(F_{i}\)</span>. Hence the regression scoring estimator <span class="math inline">\(\bar{F}_{i}\)</span> is often described as “biased”. Some algebraic manipulations reveal that <span class="math inline">\(F_{i}^{*}\)</span> has MSE <span class="math inline">\(\boldsymbol{I}_{r}-\Lambda^{\prime}\left(\Lambda^{\prime} \Lambda+\Psi\right)^{-1} \Lambda\)</span> which is smaller (in a positive definite sense) than the MSE of the idealized Bartlett estimator <span class="math inline">\(\widehat{F}_{i}\)</span>.</p>
<p>Which estimator is preferred, Bartlett or regression scoring? The differences diminish when <span class="math inline">\(k\)</span> is large so the choice is most relevant for small to moderate <span class="math inline">\(k\)</span>. The regression scoring estimator has lower approximate MSE, meaning that it is a more precise estimator. Thus based on estimation precision this is our recommended choice.</p>
<p>The factor loadings <span class="math inline">\(\Lambda\)</span> and factors <span class="math inline">\(F\)</span> are not separately identified. To see this, notice that if you replace <span class="math inline">\((\Lambda, F)\)</span> with <span class="math inline">\(\Lambda^{*}=\Lambda \boldsymbol{G}\)</span> and <span class="math inline">\(F^{*}=\boldsymbol{G}^{\prime} F\)</span> where <span class="math inline">\(\boldsymbol{G}\)</span> is <span class="math inline">\(r \times r\)</span> and orthonormal then the regression model is identical. Such replacements are called “rotations” in the factor analysis literature. Any orthogonal rotation of the factor loadings is an equally valid representation. The default MLE outputs are one specific rotation; others can be obtained by a variety of algorithms (which we do not review here). Consequently it is unwise to attribute meaning to the individual factor loading estimates.</p>
<p>Another important and tricky issue is selection of the number of factors <span class="math inline">\(r\)</span>. There is no clear guideline. One approach is to examine the principal component decomposition, look for a division between the “large” and “small eigenvalues, and set <span class="math inline">\(r\)</span> to equal to the number of”large” eigenvalues. Another approach is based on testing. As a by-product of the MLE (and standard package implementations) we obtain the LR test for the null hypothesis of <span class="math inline">\(r\)</span> factors against the alternative hypothesis of <span class="math inline">\(k\)</span> factors. If the LR test rejects (has a small p-value) this is evidence that the given <span class="math inline">\(r\)</span> may be too small.</p>
<p>In Stata, the <span class="math inline">\(\operatorname{MLE}(\widehat{\Lambda}, \widehat{\Psi})\)</span> can be calculated with the factor, ml factors (r) command. The factor estimates <span class="math inline">\(\widetilde{F}_{i}\)</span> and <span class="math inline">\(\bar{F}_{i}\)</span> can be calculated by the predict command with either the barlett or regression option, respectively. In <span class="math inline">\(R\)</span>, the command factanal ( <span class="math inline">\(X\)</span>, factors=r, rotation=“none”) calculates the <span class="math inline">\(\operatorname{MLE}(\widehat{\Lambda}, \widehat{\Psi})\)</span> and also calculates the factor estimates <span class="math inline">\(\widetilde{F}_{i}\)</span> and/or <span class="math inline">\(\bar{F}_{i}\)</span> using the scores option.</p>
</section>
<section id="approximate-factor-models" class="level2" data-number="11.14">
<h2 data-number="11.14" class="anchored" data-anchor-id="approximate-factor-models"><span class="header-section-number">11.14</span> Approximate Factor Models</h2>
<p>The MLE of the previous section is a good choice for factor estimation when the number of variables <span class="math inline">\(k\)</span> is small and the factor model is believed to be correctly specified. In many economic applications of factor analysis, however, the number of variables is <span class="math inline">\(k\)</span> is large. In such contexts the MLE can be computationally costly and/or unstable. Furthermore it is typically not credible to believe that the model is correctly specified; rather it is more reasonable to view the factor model as a useful approximation. In this section we explore an approach known as the approximate factor model with estimation by principal components. The estimation method is justified by an asymptotic framework where the number of variables <span class="math inline">\(k \rightarrow \infty\)</span> The approximate factor model was introduced by Chamberlain and Rothschild (1983). It is the same as (11.23) but relaxes the assumption on the idiosyncratic error <span class="math inline">\(u\)</span> so that the covariance matrix <span class="math inline">\(\Psi=\)</span> <span class="math inline">\(\mathbb{E}\left[u u^{\prime}\right]\)</span> is left unrestricted. In this context the Gaussian MLE of the previous section is misspecified.</p>
<p>Chamberlain and Rothschild (and the literature which followed) proposed estimation by least squares. The idea is to treat the factors as unknown regressors and simultaneously estimate the factors <span class="math inline">\(F_{i}\)</span> and factor loadings <span class="math inline">\(\Lambda\)</span>. We first describe the estimation method.</p>
<p>Let <span class="math inline">\(\left(X_{1}, \ldots, X_{n}\right)\)</span> be a sample centered at sample means. The least squares criterion is</p>
<p><span class="math display">\[
\frac{1}{n} \sum_{i=1}^{n}\left(X_{i}-\Lambda F_{i}\right)^{\prime}\left(X_{i}-\Lambda F_{i}\right) .
\]</span></p>
<p>Let <span class="math inline">\(\left(\widehat{\Lambda}, \widehat{F}_{1}, \ldots, \widehat{F}_{n}\right)\)</span> be the joint minimizers. As <span class="math inline">\(\Lambda\)</span> and <span class="math inline">\(F_{i}\)</span> are not separately identified a normalization is needed. For compatibility with the notation of the previous section we use <span class="math inline">\(n^{-1} \sum_{i=1}^{n} \widehat{F}_{i} \widehat{F}_{i}^{\prime}=\boldsymbol{I}_{r}\)</span>.</p>
<p>We use a concentration argument to find the solution. As described in the previous section, each observation satisfies the multivariate equation <span class="math inline">\(X_{i}=\Lambda F_{i}+u_{i}\)</span>. For fixed <span class="math inline">\(\Lambda\)</span> this is a set of <span class="math inline">\(k\)</span> equations with <span class="math inline">\(r\)</span> unknowns <span class="math inline">\(F_{i}\)</span>. The least squares solution is <span class="math inline">\(\widehat{F}_{i}(\Lambda)=\left(\Lambda^{\prime} \Lambda\right)^{-1} \Lambda^{\prime} X_{i}\)</span>. Substituting this expression into the least squares criterion the concentrated least squares criterion for <span class="math inline">\(\Lambda\)</span> is</p>
<p><span class="math display">\[
\begin{aligned}
\frac{1}{n} \sum_{i=1}^{n}\left(X_{i}-\Lambda \widehat{F}_{i}(\Lambda)\right)^{\prime}\left(X_{i}-\Lambda \widehat{F}_{i}(\Lambda)\right) &amp;=\frac{1}{n} \sum_{i=1}^{n}\left(X_{i}-\Lambda\left(\Lambda^{\prime} \Lambda\right)^{-1} \Lambda^{\prime} X_{i}\right)^{\prime}\left(X_{i}-\Lambda\left(\Lambda^{\prime} \Lambda\right)^{-1} \Lambda^{\prime} X_{i}\right) \\
&amp;=\frac{1}{n} \sum_{i=1}^{n}\left(X_{i}^{\prime} X_{i}-X_{i}^{\prime} \Lambda\left(\Lambda^{\prime} \Lambda\right)^{-1} \Lambda^{\prime} X_{i}\right) \\
&amp;=\operatorname{tr}[\widehat{\Sigma}]-\operatorname{tr}\left[\left(\Lambda^{\prime} \Lambda\right)^{-1} \Lambda^{\prime} \widehat{\Sigma} \Lambda\right]
\end{aligned}
\]</span></p>
<p>where <span class="math inline">\(\widehat{\Sigma}=n^{-1} \sum_{i=1}^{n} X_{i} X_{i}^{\prime}\)</span> is the sample covariance matrix. The least squares estimator <span class="math inline">\(\widehat{\Lambda}\)</span> minimizes this criterion. Let <span class="math inline">\(\widehat{\boldsymbol{D}}\)</span> and <span class="math inline">\(\widehat{\boldsymbol{H}}\)</span> be first <span class="math inline">\(r\)</span> eigenvalues and eigenvectors of <span class="math inline">\(\widehat{\Sigma}\)</span>. Using the normalization <span class="math inline">\(\Lambda^{\prime} \Lambda=\boldsymbol{I}_{r}\)</span>, from the extrema results of Section A.15 the minimizer of the least squares criterion is <span class="math inline">\(\widehat{\Lambda}=\widehat{\boldsymbol{H}}\)</span>. More broadly any rotation of <span class="math inline">\(\widehat{\boldsymbol{H}}\)</span> is valid. Consider <span class="math inline">\(\widehat{\Lambda}=\widehat{\boldsymbol{H}} \widehat{\boldsymbol{D}}^{1 / 2}\)</span>. Recall the expression for the factors <span class="math inline">\(\widehat{F}_{i}(\Lambda)=\)</span> <span class="math inline">\(\left(\Lambda^{\prime} \Lambda\right)^{-1} \Lambda^{\prime} X_{i}\)</span>. We find that the estimated factors are</p>
<p><span class="math display">\[
\widehat{F}_{i}=\left(\widehat{\boldsymbol{D}}^{1 / 2} \widehat{\boldsymbol{H}}^{\prime} \widehat{\boldsymbol{H}} \widehat{\boldsymbol{D}}^{1 / 2}\right)^{-1} \widehat{\boldsymbol{D}}^{1 / 2} \widehat{\boldsymbol{H}}^{\prime} X_{i}=\widehat{\boldsymbol{D}}^{-1 / 2} \widehat{\boldsymbol{H}}^{\prime} X_{i}
\]</span></p>
<p>We calculate that</p>
<p><span class="math display">\[
n^{-1} \sum_{i=1}^{n} \widehat{F}_{i} \widehat{F}_{i}^{\prime}=\widehat{\boldsymbol{D}}^{-1 / 2} \widehat{\boldsymbol{H}}^{\prime} \widehat{\Sigma} \widehat{\boldsymbol{H}} \widehat{\boldsymbol{D}}^{-1 / 2 \prime}=\widehat{\boldsymbol{D}}^{-1 / 2} \widehat{\boldsymbol{D}} \widehat{\boldsymbol{D}}^{-1 / 2 \prime}=\boldsymbol{I}_{r}
\]</span></p>
<p>which is the desired normalization. This shows that the rotation <span class="math inline">\(\widehat{\Lambda}=\widehat{\boldsymbol{H}} \widehat{\boldsymbol{D}}^{1 / 2}\)</span> produces factor estimates satisfying this normalization.</p>
<p>We have proven the following result.</p>
<p>Theorem 11.9 The least squares estimator of the factor model (11.23) under the normalization <span class="math inline">\(n^{-1} \sum_{i=1}^{n} \widehat{F}_{i} \widehat{F}_{i}^{\prime}=\boldsymbol{I}_{r}\)</span> has the following solution:</p>
<ol type="1">
<li><p>Let <span class="math inline">\(\widehat{\boldsymbol{D}}=\operatorname{diag}\left[\widehat{d}_{1}, \ldots, \widehat{d}_{r}\right]\)</span> and <span class="math inline">\(\widehat{\boldsymbol{H}}=\left[\widehat{h}_{1}, \ldots, \widehat{h}_{r}\right]\)</span> be the first <span class="math inline">\(r\)</span> eigenvalues and eigenvectors of the sample covariance matrix <span class="math inline">\(\widehat{\Sigma}\)</span>.</p></li>
<li><p><span class="math inline">\(\widehat{\Lambda}=\widehat{\boldsymbol{H}} \widehat{\boldsymbol{D}}^{1 / 2}\)</span>.</p></li>
<li><p><span class="math inline">\(\widehat{F}_{i}=\widehat{\boldsymbol{D}}^{-1 / 2} \widehat{\boldsymbol{H}}^{\prime} X_{i}\)</span>. Theorem <span class="math inline">\(11.9\)</span> shows that the least squares estimator is based on an eigenvalue decomposition of the covariance matrix. This is computationally stable even in high dimensions.</p></li>
</ol>
<p>The factor estimates are the principal components scaled by the eigenvalues of <span class="math inline">\(\widehat{\Sigma}\)</span>. Specifically, the <span class="math inline">\(j^{t h}\)</span> factor estimate is <span class="math inline">\(\widehat{F}_{j i}=\widehat{d}_{j}^{-1 / 2} \widehat{h}_{j}^{\prime} X\)</span>. Consequently many authors call this estimator the “principalcomponent method”.</p>
<p>Unfortunately, <span class="math inline">\(\widehat{\Lambda}\)</span> is inconsistent for <span class="math inline">\(\Lambda\)</span> if <span class="math inline">\(k\)</span> is fixed, as we now show. By the WLLN and CMT, <span class="math inline">\(\widehat{\Sigma} \underset{p}{\longrightarrow}\)</span> and <span class="math inline">\(\widehat{\boldsymbol{H}} \underset{p}{\longrightarrow} \boldsymbol{H}\)</span>, the first <span class="math inline">\(r\)</span> eigenvectors of <span class="math inline">\(\Sigma\)</span>. When <span class="math inline">\(\Psi\)</span> is diagonal, the eigenvectors of <span class="math inline">\(\Sigma=\Lambda \Lambda^{\prime}+\Psi\)</span> do not lie in the range space of <span class="math inline">\(\Lambda\)</span> except in the special case <span class="math inline">\(\Psi=\sigma^{2} \boldsymbol{I}_{k}\)</span>. Consequently the estimator <span class="math inline">\(\widehat{\Lambda}\)</span> is inconsistent.</p>
<p>This inconsistency should not be viewed as surprising. The sample has a total of <span class="math inline">\(n k\)</span> observations and the model has a total of <span class="math inline">\(n r+k r-r(r+1) / 2\)</span> parameters. Since the number of estimated pararameters is proportional to sample size we should not expect estimator consistency.</p>
<p>As first recognized by Chamberlain and Rothschild, this deficiency diminishes as <span class="math inline">\(k\)</span> increases. Specifically, assume that <span class="math inline">\(k \rightarrow \infty\)</span> as <span class="math inline">\(n \rightarrow \infty\)</span>. One implication is that the number of observations <span class="math inline">\(n k\)</span> increase at a rate faster than <span class="math inline">\(n\)</span>, while the number of parameters increase at a rate proportional to <span class="math inline">\(n\)</span>. Another implication is that as <span class="math inline">\(k\)</span> increases there is increasing information about the factors.</p>
<p>To make this precise we add the following assumption. Let <span class="math inline">\(\lambda_{\min }(\boldsymbol{A})\)</span> and <span class="math inline">\(\lambda_{\max }(\boldsymbol{A})\)</span> denote the smallest and largest eigenvalues of a positive semi-definite matrix <span class="math inline">\(\boldsymbol{A}\)</span>.</p>
<p>Assumption <span class="math inline">\(11.1\)</span> As <span class="math inline">\(k \rightarrow \infty\)</span></p>
<ol type="1">
<li><p><span class="math inline">\(\lambda_{\max }(\Psi) \leq B&lt;\infty\)</span>.</p></li>
<li><p><span class="math inline">\(\lambda_{\min }\left(\Lambda^{\prime} \Lambda\right) \rightarrow \infty\)</span> as <span class="math inline">\(k \rightarrow \infty\)</span>.</p></li>
</ol>
<p>Assumption 11.1.1 bounds the covariance matrix of the idiosyncratic errors. When <span class="math inline">\(\Psi=\operatorname{diag}\left(\sigma_{1}^{2}, \ldots, \sigma_{k}^{2}\right)\)</span> this is the same as bounding the individual variances. Effectively Assumption 11.1.1 means that while the elements of <span class="math inline">\(u\)</span> can be correlated they cannot have a correlation structure similar to that of a factor model. Assumption 11.1.2 requires the factor loading matrix to increase in magnitude as the number of variables increases. This is a fairly mild requirement. When the factor loadings are of similar magnitude across variables, <span class="math inline">\(\lambda_{\min }\left(\Lambda^{\prime} \Lambda\right) \sim k \rightarrow \infty\)</span>. Conceptually, Assumption 11.1.2 requires additional variables to add information about the unobserved factors.</p>
<p>Assumption <span class="math inline">\(11.1\)</span> implies that in the covariance matrix factorization <span class="math inline">\(\Sigma=\Lambda \Lambda^{\prime}+\Psi\)</span> the component <span class="math inline">\(\Lambda \Lambda^{\prime}\)</span> dominates as <span class="math inline">\(k\)</span> increases. This means that for large <span class="math inline">\(k\)</span> the first <span class="math inline">\(r\)</span> eigenvectors of <span class="math inline">\(\Sigma\)</span> are equivalent to those of <span class="math inline">\(\Lambda \Lambda^{\prime}\)</span>, which are in the range space of <span class="math inline">\(\Lambda\)</span>. This observation led Chamberlain and Rothschild (1983) to deduce that the principal components estimator is an asymptotic (large <span class="math inline">\(k\)</span> ) analog estimator for the factor loadings and factors. Bai (2003) demonstrated that the estimator is consistent as <span class="math inline">\(n, k \rightarrow \infty\)</span> jointly. The conditions and proofs are technical so are not reviewed here.</p>
<p>Now consider the estimated factors</p>
<p><span class="math display">\[
\widehat{F}_{i}=\boldsymbol{D}^{-1 / 2} \boldsymbol{H}^{\prime} X_{i}=\boldsymbol{D}^{-1} \Lambda^{\prime} X_{i}
\]</span></p>
<p>where for simplicity we ignore estimation error. Since <span class="math inline">\(X_{i}=\Lambda F_{i}+u_{i}\)</span> and <span class="math inline">\(\Lambda^{\prime} \Lambda=\boldsymbol{D}\)</span> we can write this as</p>
<p><span class="math display">\[
\widehat{F}_{i}=F_{i}+\boldsymbol{D}^{-1} \Lambda^{\prime} u_{i} .
\]</span></p>
<p>This shows that <span class="math inline">\(\widehat{F}_{i}\)</span> is an unbiased estimator for <span class="math inline">\(F_{i}\)</span> and has variance <span class="math inline">\(\operatorname{var}\left[\widehat{F}_{i}\right]=\boldsymbol{D}^{-1} \Lambda^{\prime} \Psi \Lambda \boldsymbol{D}^{-1}\)</span>. Under Assumption 11.1, <span class="math inline">\(\left\|\operatorname{var}\left[\widehat{F}_{i}\right]\right\| \leq B / \lambda_{\min }\left(\Lambda^{\prime} \Lambda\right) \rightarrow 0\)</span>. Thus <span class="math inline">\(\widehat{F}_{i}\)</span> is consistent for <span class="math inline">\(F_{i}\)</span> as <span class="math inline">\(k \rightarrow \infty\)</span>. Bai (2003) shows that this extends to the feasible estimator as <span class="math inline">\(n, k \rightarrow \infty\)</span>.</p>
<p>In Stata, the least squares estimator <span class="math inline">\(\widehat{\Lambda}\)</span> and factors <span class="math inline">\(\widehat{F}_{i}\)</span> can be calculated with the factor, pcf factors (r) command followed by predict. In <span class="math inline">\(\mathrm{R}\)</span> a feasible estimation approach is to calculate the factors by eigenvalue decomposition.</p>
</section>
<section id="factor-models-with-additional-regressors" class="level2" data-number="11.15">
<h2 data-number="11.15" class="anchored" data-anchor-id="factor-models-with-additional-regressors"><span class="header-section-number">11.15</span> Factor Models with Additional Regressors</h2>
<p>Consider the model</p>
<p><span class="math display">\[
X=\Lambda F+B Z+e
\]</span></p>
<p>where <span class="math inline">\(X\)</span> and <span class="math inline">\(e\)</span> are <span class="math inline">\(k \times 1, \Lambda\)</span> is <span class="math inline">\(k \times r, F\)</span> is <span class="math inline">\(r \times 1, B\)</span> is <span class="math inline">\(k \times \ell\)</span>, and <span class="math inline">\(Z\)</span> is <span class="math inline">\(\ell \times 1\)</span>.</p>
<p>The coefficients <span class="math inline">\(\Lambda\)</span> and <span class="math inline">\(\boldsymbol{B}\)</span> can be estimated by a combination of factor regression (either MLE or principal components) and least squares. The key is the following two observations:</p>
<ol type="1">
<li><p>Given <span class="math inline">\(\boldsymbol{B}\)</span>, the coefficient <span class="math inline">\(\Lambda\)</span> can be estimated by factor regression applied to <span class="math inline">\(X-\boldsymbol{B} Z\)</span>.</p></li>
<li><p>Given the factors <span class="math inline">\(F\)</span>, the coefficients <span class="math inline">\(\Lambda\)</span> and <span class="math inline">\(\boldsymbol{B}\)</span> can be estimated by multivariate least squares of <span class="math inline">\(X\)</span> on <span class="math inline">\(F\)</span> and <span class="math inline">\(Z\)</span>.</p></li>
</ol>
<p>Estimation iterates between these two steps. Start with a preliminary estimator of <span class="math inline">\(\boldsymbol{B}\)</span> obtained by multivariate least squares of <span class="math inline">\(X\)</span> on <span class="math inline">\(Z\)</span>. Then apply the above two steps and iterate under convergence.</p>
</section>
<section id="factor-augmented-regression" class="level2" data-number="11.16">
<h2 data-number="11.16" class="anchored" data-anchor-id="factor-augmented-regression"><span class="header-section-number">11.16</span> Factor-Augmented Regression</h2>
<p>In the previous sections we considered factor models which decompose a set of variables into common factors and idiosyncratic errors. In this section we consider factor-augmented regression, which uses such common factors as regressors for dimension reduction.</p>
<p>Suppose we have the variables <span class="math inline">\((Y, Z, X)\)</span> where <span class="math inline">\(Y \in \mathbb{R}, Z \in \mathbb{R}^{\ell}\)</span>, and <span class="math inline">\(X \in \mathbb{R}^{k}\)</span>. In practice, <span class="math inline">\(k\)</span> may be large and the elements of <span class="math inline">\(X\)</span> may be highly correlated. The factor-augmented regression model is</p>
<p><span class="math display">\[
\begin{aligned}
Y &amp;=F^{\prime} \beta+Z^{\prime} \gamma+e \\
X &amp;=\Lambda F+u \\
\mathbb{E}[F e] &amp;=0 \\
\mathbb{E}[Z e] &amp;=0 \\
\mathbb{E}\left[F u^{\prime}\right] &amp;=0 \\
\mathbb{E}[u e] &amp;=0,
\end{aligned}
\]</span></p>
<p>The random variables are <span class="math inline">\(e \in \mathbb{R}, F \in \mathbb{R}^{r}\)</span>, and <span class="math inline">\(u \in \mathbb{R}^{k}\)</span>. The regression coefficients are <span class="math inline">\(\beta \in \mathbb{R}^{k}\)</span> and <span class="math inline">\(\gamma \in \mathbb{R}^{\ell}\)</span>. The matrix <span class="math inline">\(\Lambda\)</span> are the factor loadings.</p>
<p>This model specifies that the influence of <span class="math inline">\(X\)</span> on <span class="math inline">\(Y\)</span> is through the common factors <span class="math inline">\(F\)</span>. The idea is that the variation in the regressors is mostly captured by the variation in the factors, so the influence of the regressors can be captured through these factors. This can be viewed as a dimension-reduction technique as we have reduced the <span class="math inline">\(k\)</span>-dimensional <span class="math inline">\(X\)</span> to the <span class="math inline">\(r\)</span>-dimensional <span class="math inline">\(F\)</span>. Interest typically focuses on the regressors <span class="math inline">\(Z\)</span> and its coefficients <span class="math inline">\(\gamma\)</span>. The factors <span class="math inline">\(F\)</span> are included in the regression as “controls” and its coefficient <span class="math inline">\(\beta\)</span> is less typically of interest. Since it is difficult to interpret the factors <span class="math inline">\(F\)</span> only their range space is identified it is generally prudent to avoid intrepreting the coefficients <span class="math inline">\(\beta\)</span>. The model is typically estimated in multiple steps. First, the factor loadings <span class="math inline">\(\Lambda\)</span> and factors <span class="math inline">\(F_{i}\)</span> are estimated by factor regression. In the case of principal-components estimation the factor estimates are the scaled <span class="math inline">\({ }^{2}\)</span> principal components <span class="math inline">\(\widehat{F}_{i}=\widehat{\boldsymbol{D}}^{-1} \widehat{\Lambda}^{\prime} X_{i}\)</span>. Second, <span class="math inline">\(Y\)</span> is regressed on the estimated factors and the other regressors to obtain the estimator of <span class="math inline">\(\beta\)</span> and <span class="math inline">\(\gamma\)</span>. This second-step estimator equals (for simplicity assume there is no <span class="math inline">\(Z\)</span> )</p>
<p><span class="math display">\[
\begin{aligned}
\widehat{\beta} &amp;=\left(\sum_{i=1}^{n} \widehat{F}_{i} \widehat{F}_{i}^{\prime}\right)^{-1}\left(\sum_{i=1}^{n} \widehat{F}_{i} Y_{i}\right) \\
&amp;=\left(\widehat{\boldsymbol{D}}^{-1} \widehat{\Lambda}^{\prime} \frac{1}{n} \sum_{i=1}^{n} X_{i} X_{i}^{\prime} \widehat{\Lambda}^{-1}\right)^{-1}\left(\widehat{\boldsymbol{D}}^{-1} \widehat{\Lambda}^{\prime} \frac{1}{n} \sum_{i=1}^{n} X_{i} Y_{i}\right) .
\end{aligned}
\]</span></p>
<p>Now let’s investigate its asymptotic behavior. As <span class="math inline">\(n \rightarrow \infty, \widehat{\Lambda} \underset{p}{\rightarrow} \Lambda\)</span> and <span class="math inline">\(\widehat{\boldsymbol{D}} \underset{p}{\rightarrow} \boldsymbol{D}\)</span> so</p>
<p><span class="math display">\[
\widehat{\beta} \underset{p}{\longrightarrow} \beta^{*}=\left(\boldsymbol{D}^{-1} \Lambda^{\prime} \mathbb{E}\left[X X^{\prime}\right] \Lambda \boldsymbol{D}^{-1}\right)^{-1}\left(\boldsymbol{D}^{-1} \Lambda^{\prime} \mathbb{E}[X Y]\right) .
\]</span></p>
<p>Recall <span class="math inline">\(\mathbb{E}\left[X X^{\prime}\right]=\Lambda \Lambda^{\prime}+\Psi\)</span> and <span class="math inline">\(\Lambda^{\prime} \Lambda=\boldsymbol{D}\)</span>. We calculate that</p>
<p><span class="math display">\[
\mathbb{E}[X Y]=\mathbb{E}\left[(\Lambda F+u)\left(F^{\prime} \beta+e\right)\right]=\Lambda \beta .
\]</span></p>
<p>We find that the right-hand-side of (11.26) equals</p>
<p><span class="math display">\[
\beta^{*}=\left(D^{-1} \Lambda^{\prime}\left(\Lambda \Lambda^{\prime}+\Psi\right) \Lambda \boldsymbol{D}^{-1}\right)^{-1}\left(\boldsymbol{D}^{-1} \Lambda^{\prime} \Lambda \beta\right)=\left(\boldsymbol{I}_{r}+\boldsymbol{D}^{-1} \Lambda^{\prime} \Psi \Lambda \boldsymbol{D}^{-1}\right)^{-1} \beta
\]</span></p>
<p>which does not equal <span class="math inline">\(\beta\)</span>. Thus <span class="math inline">\(\widehat{\beta}\)</span> has a probability limit but is inconsistent for <span class="math inline">\(\beta\)</span> as <span class="math inline">\(n \rightarrow \infty\)</span>.</p>
<p>This deficiency diminishes as <span class="math inline">\(k \rightarrow \infty\)</span>. Indeed,</p>
<p><span class="math display">\[
\left\|\boldsymbol{D}^{-1} \Lambda^{\prime} \Psi \Lambda \boldsymbol{D}^{-1}\right\| \leq B\left\|\boldsymbol{D}^{-1}\right\| \rightarrow 0
\]</span></p>
<p>as <span class="math inline">\(k \rightarrow \infty\)</span>. This implies <span class="math inline">\(\beta^{*} \rightarrow \beta\)</span>. Hence, if we take the sequential asymptotic limit <span class="math inline">\(n \rightarrow \infty\)</span> followed by <span class="math inline">\(k \rightarrow\)</span> <span class="math inline">\(\infty\)</span>, we find <span class="math inline">\(\widehat{\beta} \underset{p}{\longrightarrow} \beta\)</span>. This implies that the estimator is consistent. Bai (2003) demonstrated consistency under the more rigorous but technically challenging setting where <span class="math inline">\(n, k \rightarrow \infty\)</span> jointly. The implication of this result is that factor augmented regression is consistent if both the sample size and dimension of <span class="math inline">\(X\)</span> are large.</p>
<p>For asymptotic normality of <span class="math inline">\(\widehat{\beta}\)</span> it turns out that we need to strengthen Assumption 11.1.2. The relevant condition is <span class="math inline">\(n^{-1 / 2} \lambda_{\min }\left(\Lambda^{\prime} \Lambda\right) \rightarrow \infty\)</span>. This is similar to the condition that <span class="math inline">\(k^{2} / n \rightarrow \infty\)</span>. This is technical but can be interpreted as meaning that <span class="math inline">\(k\)</span> is large relative to <span class="math inline">\(\sqrt{n}\)</span>. Intuitively, this requires that dimension of <span class="math inline">\(X\)</span> is larger than sample size <span class="math inline">\(n\)</span>.</p>
<p>In Stata, estimation takes the following steps. First, the factor command is used to estimate the factor model. Either MLE or principal components estimation can be used. Second, the predict command is used to estimate the factors, either by Barlett or regression scoring. Third, the factors are treated as regressors in an estimated regression.</p>
</section>
<section id="multivariate-normal" class="level2" data-number="11.17">
<h2 data-number="11.17" class="anchored" data-anchor-id="multivariate-normal"><span class="header-section-number">11.17</span> Multivariate Normal*</h2>
<p>Some interesting sampling results hold for matrix-valued normal variates. Let <span class="math inline">\(\boldsymbol{Y}\)</span> be an <span class="math inline">\(n \times m\)</span> matrix whose rows are independent and distributed <span class="math inline">\(\mathrm{N}(\mu, \Sigma)\)</span>. We say that <span class="math inline">\(\boldsymbol{Y}\)</span> is multivariate matrix normal, and</p>
<p><span class="math inline">\({ }^{2}\)</span> The unscaled principal components can equivalently be used if the coefficients <span class="math inline">\(\widehat{\beta}\)</span> are not reported. The coefficient estimates <span class="math inline">\(\hat{\gamma}\)</span> are unaffected by the choice of factor scaling. write <span class="math inline">\(Y \sim \mathrm{N}\left(\bar{\mu}, I_{n} \otimes \Sigma\right)\)</span>, where <span class="math inline">\(\bar{\mu}\)</span> is <span class="math inline">\(n \times m\)</span> with each row equal to <span class="math inline">\(\mu^{\prime}\)</span>. The notation is due to the fact that <span class="math inline">\(\operatorname{vec}\left((\boldsymbol{Y}-\mu)^{\prime}\right) \sim \mathrm{N}\left(0, \boldsymbol{I}_{n} \otimes \Sigma\right)\)</span></p>
<p>Definition 11.2 If <span class="math inline">\(n \times m Y \sim \mathrm{N}\left(\bar{\mu}, I_{n} \otimes \Sigma\right)\)</span> then <span class="math inline">\(W=Y^{\prime} Y\)</span> is distributed Wishart with <span class="math inline">\(n\)</span> degress of freedom and covariance matrix <span class="math inline">\(\Sigma\)</span>, and is written as <span class="math inline">\(W \sim\)</span> <span class="math inline">\(W_{m}(n, \Sigma)\)</span>.</p>
<p>The Wishart is a multivariate generalization of the chi-square. If <span class="math inline">\(W \sim W_{1}\left(n, \sigma^{2}\right)\)</span> then <span class="math inline">\(W \sim \sigma^{2} \chi_{n}^{2}\)</span>.</p>
<p>The Wishart arises as the exact distribution of a sample covariance matrix in the normal sampling model. The bias-corrected estimator of <span class="math inline">\(\Sigma\)</span> is</p>
<p><span class="math display">\[
\widehat{\Sigma}=\frac{1}{n-1} \sum_{i=1}^{n}\left(Y_{i}-\bar{Y}\right)\left(Y_{i}-\bar{Y}\right)^{\prime} .
\]</span></p>
<p>Theorem 11.10 If <span class="math inline">\(Y_{i} \sim \mathrm{N}(\mu, \Sigma)\)</span> are independent then <span class="math inline">\(\widehat{\Sigma} \sim W_{m}\left(n-1, \frac{1}{n-1} \Sigma\right)\)</span>.</p>
<p>The following manipulation is useful.</p>
<p>Theorem 11.11 If <span class="math inline">\(W \sim W_{m}(n, \Sigma)\)</span> then for <span class="math inline">\(m \times 1 \alpha,\left(\alpha^{\prime} W^{-1} \alpha\right)^{-1} \sim \frac{\chi_{n-m+1}^{2}}{\alpha^{\prime} \Sigma^{-1} \alpha}\)</span></p>
<p>To prove this, note that without loss of generality we can take <span class="math inline">\(\Sigma=\boldsymbol{I}_{m}\)</span> and <span class="math inline">\(\alpha^{\prime} \alpha=1\)</span>. Let <span class="math inline">\(\boldsymbol{H}\)</span> be <span class="math inline">\(m \times m\)</span> orthonormal with first row equal to <span class="math inline">\(\alpha\)</span>. so that <span class="math inline">\(\boldsymbol{H} \alpha=\left(\begin{array}{l}1 \\ 0\end{array}\right)\)</span>. Since the distribution of <span class="math inline">\(\boldsymbol{Y}\)</span> and <span class="math inline">\(\boldsymbol{Y} \boldsymbol{H}\)</span> are identical we can without loss of generality set <span class="math inline">\(\alpha=\left(\begin{array}{c}1 \\ 0\end{array}\right)\)</span>. Partition <span class="math inline">\(\boldsymbol{Y}=\left[\boldsymbol{Y}_{1}, \boldsymbol{Y}_{2}\right]\)</span> where <span class="math inline">\(\boldsymbol{Y}_{1}\)</span> is <span class="math inline">\(n \times 1, \boldsymbol{Y}_{2}\)</span> is <span class="math inline">\(n \times(m-1)\)</span>, and they are independent. Then</p>
<p><span class="math display">\[
\begin{aligned}
\left(\alpha^{\prime} W^{-1} \alpha\right)^{-1} &amp;=\left(\left(\begin{array}{ll}
1 &amp; 0
\end{array}\right)\left(\begin{array}{cc}
\boldsymbol{Y}_{1}^{\prime} \boldsymbol{Y}_{1} &amp; \boldsymbol{Y}_{1}^{\prime} \boldsymbol{Y}_{2} \\
\boldsymbol{Y}_{2}^{\prime} \boldsymbol{Y}_{1} &amp; \boldsymbol{Y}_{2}^{\prime} \boldsymbol{Y}_{2}
\end{array}\right)^{-1}\left(\begin{array}{l}
1 \\
0
\end{array}\right)\right)^{-1} \\
&amp;=\boldsymbol{Y}_{1}^{\prime} \boldsymbol{Y}_{1}-\boldsymbol{Y}_{1}^{\prime} \boldsymbol{Y}_{2}\left(\boldsymbol{Y}_{2}^{\prime} \boldsymbol{Y}_{2}\right)^{-1} \boldsymbol{Y}_{2}^{\prime} \boldsymbol{Y}_{1} \\
&amp;=\boldsymbol{Y}_{1}^{\prime} \boldsymbol{M}_{2} \boldsymbol{Y}_{1} \sim \chi_{n-(m-1)}^{2}
\end{aligned}
\]</span></p>
<p>where <span class="math inline">\(\boldsymbol{M}_{2}=\boldsymbol{I}_{m-1}-\boldsymbol{Y}_{2}\left(\boldsymbol{Y}_{2}^{\prime} \boldsymbol{Y}_{2}\right)^{-1} \boldsymbol{Y}_{2}^{\prime}\)</span>. The final distributional equality holds conditional on <span class="math inline">\(\boldsymbol{Y}_{2}\)</span> by the same argument in the proof of Theorem 5.7. Since this does not depend on <span class="math inline">\(\boldsymbol{Y}_{2}\)</span> it is the unconditional distribution as well. This establishes the stated result.</p>
<p>To test hypotheses about <span class="math inline">\(\mu\)</span> a classical statistic is known as Hotelling’s <span class="math inline">\(T^{2}\)</span> :</p>
<p><span class="math display">\[
T^{2}=n(\bar{Y}-\mu)^{\prime} \widehat{\Sigma}^{-1}(\bar{Y}-\mu) .
\]</span></p>
<p>Theorem 11.12 If <span class="math inline">\(Y \sim \mathrm{N}(\mu, \Sigma)\)</span> then</p>
<p><span class="math display">\[
T^{2} \sim \frac{m}{(n-m)(n-1)} F(m, n-m)
\]</span></p>
<p>a scaled F distribution.</p>
<p>To prove this recall that <span class="math inline">\(\bar{Y}\)</span> is independent of <span class="math inline">\(\widehat{\Sigma}\)</span>. Apply Theorem <span class="math inline">\(11.11\)</span> with <span class="math inline">\(\alpha=\bar{Y}-\mu\)</span>. Conditional on <span class="math inline">\(\bar{Y}\)</span> and using the fact that <span class="math inline">\(\widehat{\Sigma} \sim W_{m}\left(n-1, \frac{1}{n-1} \Sigma\right)\)</span>,</p>
<p><span class="math display">\[
\begin{aligned}
\frac{n}{T^{2}} &amp;=\left((\bar{Y}-\Sigma)^{\prime} \widehat{\Sigma}^{-1}(\bar{Y}-\Sigma)\right)^{-1} \\
&amp; \sim \frac{\chi_{n-1-m+1}^{2}}{(\bar{Y}-\mu)^{\prime}\left(\frac{1}{n-1} \Sigma\right)^{-1}(\bar{Y}-\mu)} \\
&amp; \sim n(n-1) \frac{\chi_{n-m}^{2}}{\chi_{m}^{2}}
\end{aligned}
\]</span></p>
<p>Since the two chi-square variables are independent, this is the stated result.</p>
<p>A very interesting property of this result is that the <span class="math inline">\(T^{2}\)</span> statistic is a multivariate quadratric form in normal random variables, yet it has the exact <span class="math inline">\(F\)</span> distribution.</p>
</section>
<section id="exercises" class="level2" data-number="11.18">
<h2 data-number="11.18" class="anchored" data-anchor-id="exercises"><span class="header-section-number">11.18</span> Exercises</h2>
<p>Exercise 11.1 Show (11.10) when the errors are conditionally homoskedastic (11.8).</p>
<p>Exercise 11.2 Show (11.11) when the regressors are common across equations <span class="math inline">\(X_{j}=X\)</span>.</p>
<p>Exercise 11.3 Show (11.12) when the regressors are common across equations <span class="math inline">\(X_{j}=X\)</span> and the errors are conditionally homoskedastic (11.8).</p>
<p>Exercise 11.4 Prove Theorem 11.1.</p>
<p>Exercise 11.5 Show (11.13) when the regressors are common across equations <span class="math inline">\(X_{j}=X\)</span>.</p>
<p>Exercise 11.6 Show (11.14) when the regressors are common across equations <span class="math inline">\(X_{j}=X\)</span> and the errors are conditionally homoskedastic (11.8).</p>
<p>Exercise 11.7 Prove Theorem 11.2.</p>
<p>Exercise 11.8 Prove Theorem 11.3.</p>
<p>Exercise <span class="math inline">\(11.9\)</span> Show that (11.16) follows from the steps described.</p>
<p>Exercise 11.10 Show that (11.17) follows from the steps described.</p>
<p>Exercise <span class="math inline">\(11.11\)</span> Prove Theorem 11.4. Exercise 11.12 Prove Theorem 11.5.</p>
<p>Hint: First, show that it is sufficient to show that</p>
<p><span class="math display">\[
\mathbb{E}\left[\bar{X}^{\prime} \bar{X}\right]\left(\mathbb{E}\left[\bar{X}^{\prime} \Sigma^{-1} \bar{X}\right]\right)^{-1} \mathbb{E}\left[\bar{X}^{\prime} \bar{X}\right] \leq \mathbb{E}\left[\bar{X}^{\prime} \Sigma \bar{X}\right] .
\]</span></p>
<p>Second, rewrite this equation using the transformations <span class="math inline">\(U=\Sigma^{1 / 2} \bar{X}\)</span> and <span class="math inline">\(V=\Sigma^{1 / 2} \bar{X}\)</span>, and then apply the matrix Cauchy-Schwarz inequality (B.33).</p>
<p>Exercise 11.13 Prove Theorem 11.6.</p>
<p>Exercise <span class="math inline">\(11.14\)</span> Take the model</p>
<p><span class="math display">\[
\begin{aligned}
Y &amp;=\pi^{\prime} \beta+e \\
\pi &amp;=\mathbb{E}[X \mid Z]=\Gamma^{\prime} Z \\
\mathbb{E}[e \mid Z] &amp;=0
\end{aligned}
\]</span></p>
<p>where <span class="math inline">\(Y\)</span> is scalar, <span class="math inline">\(X\)</span> is a <span class="math inline">\(k\)</span> vector and <span class="math inline">\(Z\)</span> is an <span class="math inline">\(\ell\)</span> vector. <span class="math inline">\(\beta\)</span> and <span class="math inline">\(\pi\)</span> are <span class="math inline">\(k \times 1\)</span> and <span class="math inline">\(\Gamma\)</span> is <span class="math inline">\(\ell \times k\)</span>. The sample is <span class="math inline">\(\left(Y_{i}, X_{i}, Z_{i}: i=1, \ldots, n\right)\)</span> with <span class="math inline">\(\pi_{i}\)</span> unobserved.</p>
<p>Consider the estimator <span class="math inline">\(\widehat{\beta}\)</span> for <span class="math inline">\(\beta\)</span> by OLS of <span class="math inline">\(Y\)</span> on <span class="math inline">\(\widehat{\pi}=\widehat{\Gamma}^{\prime} Z\)</span> where <span class="math inline">\(\widehat{\Gamma}\)</span> is the OLS coefficient from the multivariate regression of <span class="math inline">\(X\)</span> on <span class="math inline">\(Z\)</span>.</p>
<ol type="a">
<li><p>Show that <span class="math inline">\(\widehat{\beta}\)</span> is consistent for <span class="math inline">\(\beta\)</span>.</p></li>
<li><p>Find the asymptotic distribution <span class="math inline">\(\sqrt{n}(\widehat{\beta}-\beta)\)</span> as <span class="math inline">\(n \rightarrow \infty\)</span> assuming that <span class="math inline">\(\beta=0\)</span>.</p></li>
<li><p>Why is the assumption <span class="math inline">\(\beta=0\)</span> an important simplifying condition in part (b)?</p></li>
<li><p>Using the result in (c) construct an appropriate asymptotic test for the hypothesis <span class="math inline">\(\mathbb{H}_{0}: \beta=0\)</span>.</p></li>
</ol>
<p>Exercise <span class="math inline">\(11.15\)</span> The observations are i.i.d., <span class="math inline">\(\left(Y_{1 i}, Y_{2 i}, X_{i}: i=1, \ldots, n\right)\)</span>. The dependent variables <span class="math inline">\(Y_{1}\)</span> and <span class="math inline">\(Y_{2}\)</span> are real-valued. The regressor <span class="math inline">\(X\)</span> is a <span class="math inline">\(k\)</span>-vector. The model is the two-equation system</p>
<p><span class="math display">\[
\begin{aligned}
Y_{1} &amp;=X^{\prime} \beta_{1}+e_{1} \\
\mathbb{E}\left[X e_{1}\right] &amp;=0 \\
Y_{2} &amp;=X^{\prime} \beta_{2}+e_{2} \\
\mathbb{E}\left[X e_{2}\right] &amp;=0 .
\end{aligned}
\]</span></p>
<ol type="a">
<li><p>What are the appropriate estimators <span class="math inline">\(\widehat{\beta}_{1}\)</span> and <span class="math inline">\(\widehat{\beta}_{2}\)</span> for <span class="math inline">\(\beta_{1}\)</span> and <span class="math inline">\(\beta_{2}\)</span> ?</p></li>
<li><p>Find the joint asymptotic distribution of <span class="math inline">\(\widehat{\beta}_{1}\)</span> and <span class="math inline">\(\widehat{\beta}_{2}\)</span>.</p></li>
<li><p>Describe a test for <span class="math inline">\(\mathbb{H}_{0}: \beta_{1}=\beta_{2}\)</span>.</p></li>
</ol>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    setTimeout(function() {
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      let href = ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const cites = ref.parentNode.getAttribute('data-cites').split(' ');
    tippyHover(ref, function() {
      var popup = window.document.createElement('div');
      cites.forEach(function(cite) {
        var citeDiv = window.document.createElement('div');
        citeDiv.classList.add('hanging-indent');
        citeDiv.classList.add('csl-entry');
        var biblioDiv = window.document.getElementById('ref-' + cite);
        if (biblioDiv) {
          citeDiv.innerHTML = biblioDiv.innerHTML;
        }
        popup.appendChild(citeDiv);
      });
      return popup.innerHTML;
    });
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./part03-MEQ.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text">多方程模型</span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./chpt12-iv.html" class="pagination-link">
        <span class="nav-page-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Instrumental Variables</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->



</body></html>
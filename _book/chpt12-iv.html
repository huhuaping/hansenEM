<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.0.36">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Hansen中高级计量体系 - 12&nbsp; Instrumental Variables</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
span.underline{text-decoration: underline;}
div.column{display: inline-block; vertical-align: top; width: 50%;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./chpt13-gmm.html" rel="next">
<link href="./chpt11-multi-reg.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
    <div class="container-fluid d-flex justify-content-between">
      <h1 class="quarto-secondary-nav-title"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Instrumental Variables</span></h1>
      <button type="button" class="quarto-btn-toggle btn" aria-label="Show secondary navigation">
        <i class="bi bi-chevron-right"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Hansen中高级计量体系</a> 
        <div class="sidebar-tools-main">
    <a href="https://github.com/huhuaping/hansenEM/" title="Source Code" class="sidebar-tool px-1"><i class="bi bi-github"></i></a>
</div>
    </div>
      </div>
      <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
      </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">前言</a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./participate.html" class="sidebar-item-text sidebar-link">如何参与？</a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chpt01-intro.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./part01-reg.html" class="sidebar-item-text sidebar-link">回归</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chpt02-ce.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Conditional Expectation and Projection</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chpt03-algebra.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">The Algebra of Least Squares</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chpt04-lsr.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Least Squares Regression</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chpt05-normal-reg.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Normal Regression</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./part02-LSM.html" class="sidebar-item-text sidebar-link">大样本方法</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chpt06-review.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">A Review of Large Sample Asymptotics</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chpt07-asymptotic-ls.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Asymptotic Theory for Least Squares</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chpt08-restricted-est.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Restricted Estimation</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chpt09-hypothesit-test.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Hypothesis Testing</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chpt10-resample-method.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Resampling Methods</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./part03-MEQ.html" class="sidebar-item-text sidebar-link">多方程模型</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chpt11-multi-reg.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Multivariate Regression</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chpt12-iv.html" class="sidebar-item-text sidebar-link active"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Instrumental Variables</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chpt13-gmm.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Generalized Method of Moments</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./summary.html" class="sidebar-item-text sidebar-link">Summary</a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">References</a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" aria-expanded="true">Appendices</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./a1-notation.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">A</span>&nbsp; <span class="chapter-title">附录a1</span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction"> <span class="header-section-number">12.1</span> Introduction</a></li>
  <li><a href="#overview" id="toc-overview" class="nav-link" data-scroll-target="#overview"> <span class="header-section-number">12.2</span> Overview</a></li>
  <li><a href="#examples" id="toc-examples" class="nav-link" data-scroll-target="#examples"> <span class="header-section-number">12.3</span> Examples</a></li>
  <li><a href="#endogenous-regressors" id="toc-endogenous-regressors" class="nav-link" data-scroll-target="#endogenous-regressors"> <span class="header-section-number">12.4</span> Endogenous Regressors</a></li>
  <li><a href="#instruments" id="toc-instruments" class="nav-link" data-scroll-target="#instruments"> <span class="header-section-number">12.5</span> Instruments</a></li>
  <li><a href="#example-college-proximity" id="toc-example-college-proximity" class="nav-link" data-scroll-target="#example-college-proximity"> <span class="header-section-number">12.6</span> Example: College Proximity</a></li>
  <li><a href="#reduced-form" id="toc-reduced-form" class="nav-link" data-scroll-target="#reduced-form"> <span class="header-section-number">12.7</span> Reduced Form</a></li>
  <li><a href="#identification" id="toc-identification" class="nav-link" data-scroll-target="#identification"> <span class="header-section-number">12.8</span> Identification</a></li>
  <li><a href="#instrumental-variables-estimator" id="toc-instrumental-variables-estimator" class="nav-link" data-scroll-target="#instrumental-variables-estimator"> <span class="header-section-number">12.9</span> Instrumental Variables Estimator</a></li>
  <li><a href="#demeaned-representation" id="toc-demeaned-representation" class="nav-link" data-scroll-target="#demeaned-representation"> <span class="header-section-number">12.10</span> Demeaned Representation</a></li>
  <li><a href="#wald-estimator" id="toc-wald-estimator" class="nav-link" data-scroll-target="#wald-estimator"> <span class="header-section-number">12.11</span> Wald Estimator</a></li>
  <li><a href="#two-stage-least-squares" id="toc-two-stage-least-squares" class="nav-link" data-scroll-target="#two-stage-least-squares"> <span class="header-section-number">12.12</span> Two-Stage Least Squares</a></li>
  <li><a href="#limited-information-maximum-likelihood" id="toc-limited-information-maximum-likelihood" class="nav-link" data-scroll-target="#limited-information-maximum-likelihood"> <span class="header-section-number">12.13</span> Limited Information Maximum Likelihood</a></li>
  <li><a href="#split-sample-iv-and-jive" id="toc-split-sample-iv-and-jive" class="nav-link" data-scroll-target="#split-sample-iv-and-jive"> <span class="header-section-number">12.14</span> Split-Sample IV and JIVE</a></li>
  <li><a href="#consistency-of-2sls" id="toc-consistency-of-2sls" class="nav-link" data-scroll-target="#consistency-of-2sls"> <span class="header-section-number">12.15</span> Consistency of 2SLS</a></li>
  <li><a href="#asymptotic-distribution-of-2sls" id="toc-asymptotic-distribution-of-2sls" class="nav-link" data-scroll-target="#asymptotic-distribution-of-2sls"> <span class="header-section-number">12.16</span> Asymptotic Distribution of 2SLS</a></li>
  <li><a href="#determinants-of-2-sls-variance" id="toc-determinants-of-2-sls-variance" class="nav-link" data-scroll-target="#determinants-of-2-sls-variance"> <span class="header-section-number">12.17</span> Determinants of 2 SLS Variance</a></li>
  <li><a href="#covariance-matrix-estimation" id="toc-covariance-matrix-estimation" class="nav-link" data-scroll-target="#covariance-matrix-estimation"> <span class="header-section-number">12.18</span> Covariance Matrix Estimation</a></li>
  <li><a href="#liml-asymptotic-distribution" id="toc-liml-asymptotic-distribution" class="nav-link" data-scroll-target="#liml-asymptotic-distribution"> <span class="header-section-number">12.19</span> LIML Asymptotic Distribution</a></li>
  <li><a href="#functions-of-parameters" id="toc-functions-of-parameters" class="nav-link" data-scroll-target="#functions-of-parameters"> <span class="header-section-number">12.20</span> Functions of Parameters</a></li>
  <li><a href="#hypothesis-tests" id="toc-hypothesis-tests" class="nav-link" data-scroll-target="#hypothesis-tests"> <span class="header-section-number">12.21</span> Hypothesis Tests</a></li>
  <li><a href="#finite-sample-theory" id="toc-finite-sample-theory" class="nav-link" data-scroll-target="#finite-sample-theory"> <span class="header-section-number">12.22</span> Finite Sample Theory</a></li>
  <li><a href="#bootstrap-for-2sls" id="toc-bootstrap-for-2sls" class="nav-link" data-scroll-target="#bootstrap-for-2sls"> <span class="header-section-number">12.23</span> Bootstrap for 2SLS</a></li>
  <li><a href="#the-peril-of-bootstrap-2sls-standard-errors" id="toc-the-peril-of-bootstrap-2sls-standard-errors" class="nav-link" data-scroll-target="#the-peril-of-bootstrap-2sls-standard-errors"> <span class="header-section-number">12.24</span> The Peril of Bootstrap 2SLS Standard Errors</a></li>
  <li><a href="#clustered-dependence" id="toc-clustered-dependence" class="nav-link" data-scroll-target="#clustered-dependence"> <span class="header-section-number">12.25</span> Clustered Dependence</a></li>
  <li><a href="#generated-regressors" id="toc-generated-regressors" class="nav-link" data-scroll-target="#generated-regressors"> <span class="header-section-number">12.26</span> Generated Regressors</a></li>
  <li><a href="#regression-with-expectation-errors" id="toc-regression-with-expectation-errors" class="nav-link" data-scroll-target="#regression-with-expectation-errors"> <span class="header-section-number">12.27</span> Regression with Expectation Errors</a></li>
  <li><a href="#control-function-regression" id="toc-control-function-regression" class="nav-link" data-scroll-target="#control-function-regression"> <span class="header-section-number">12.28</span> Control Function Regression</a></li>
  <li><a href="#endogeneity-tests" id="toc-endogeneity-tests" class="nav-link" data-scroll-target="#endogeneity-tests"> <span class="header-section-number">12.29</span> Endogeneity Tests</a></li>
  <li><a href="#subset-endogeneity-tests" id="toc-subset-endogeneity-tests" class="nav-link" data-scroll-target="#subset-endogeneity-tests"> <span class="header-section-number">12.30</span> Subset Endogeneity Tests</a></li>
  <li><a href="#overidentification-tests" id="toc-overidentification-tests" class="nav-link" data-scroll-target="#overidentification-tests"> <span class="header-section-number">12.31</span> OverIdentification Tests</a></li>
  <li><a href="#subset-overidentification-tests" id="toc-subset-overidentification-tests" class="nav-link" data-scroll-target="#subset-overidentification-tests"> <span class="header-section-number">12.32</span> Subset OverIdentification Tests</a></li>
  <li><a href="#bootstrap-overidentification-tests" id="toc-bootstrap-overidentification-tests" class="nav-link" data-scroll-target="#bootstrap-overidentification-tests"> <span class="header-section-number">12.33</span> Bootstrap Overidentification Tests</a></li>
  <li><a href="#local-average-treatment-effects" id="toc-local-average-treatment-effects" class="nav-link" data-scroll-target="#local-average-treatment-effects"> <span class="header-section-number">12.34</span> Local Average Treatment Effects</a></li>
  <li><a href="#identification-failure" id="toc-identification-failure" class="nav-link" data-scroll-target="#identification-failure"> <span class="header-section-number">12.35</span> Identification Failure</a></li>
  <li><a href="#weak-instruments" id="toc-weak-instruments" class="nav-link" data-scroll-target="#weak-instruments"> <span class="header-section-number">12.36</span> Weak Instruments</a></li>
  <li><a href="#many-instruments" id="toc-many-instruments" class="nav-link" data-scroll-target="#many-instruments"> <span class="header-section-number">12.37</span> Many Instruments</a></li>
  <li><a href="#testing-for-weak-instruments" id="toc-testing-for-weak-instruments" class="nav-link" data-scroll-target="#testing-for-weak-instruments"> <span class="header-section-number">12.38</span> Testing for Weak Instruments</a></li>
  <li><a href="#weak-instruments-with-k_21" id="toc-weak-instruments-with-k_21" class="nav-link" data-scroll-target="#weak-instruments-with-k_21"> <span class="header-section-number">12.39</span> Weak Instruments with <span class="math inline">\(k_{2}&gt;1\)</span></a></li>
  <li><a href="#example-acemoglu-johnson-and-robinson-2001" id="toc-example-acemoglu-johnson-and-robinson-2001" class="nav-link" data-scroll-target="#example-acemoglu-johnson-and-robinson-2001"> <span class="header-section-number">12.40</span> Example: Acemoglu, Johnson, and Robinson (2001)</a></li>
  <li><a href="#example-angrist-and-krueger-1991" id="toc-example-angrist-and-krueger-1991" class="nav-link" data-scroll-target="#example-angrist-and-krueger-1991"> <span class="header-section-number">12.41</span> Example: Angrist and Krueger (1991)</a></li>
  <li><a href="#programming" id="toc-programming" class="nav-link" data-scroll-target="#programming"> <span class="header-section-number">12.42</span> Programming</a></li>
  <li><a href="#exercises" id="toc-exercises" class="nav-link" data-scroll-target="#exercises"> <span class="header-section-number">12.43</span> Exercises</a></li>
  </ul>
<div class="toc-actions"><div><i class="bi bi-github"></i></div><div class="action-links"><p><a href="https://github.com/huhuaping/hansenEM/edit/master/chpt12-iv.qmd" class="toc-action">Edit this page</a></p></div></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title d-none d-lg-block"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Instrumental Variables</span></h1>
</div>



<div class="quarto-title-meta">

    
    
  </div>
  

</header>

<section id="introduction" class="level2" data-number="12.1">
<h2 data-number="12.1" class="anchored" data-anchor-id="introduction"><span class="header-section-number">12.1</span> Introduction</h2>
<p>The concepts of endogeneity and instrumental variable are fundamental to econometrics, and mark a substantial departure from other branches of statistics. The ideas of endogeneity arise naturally in economics from models of simultaneous equations, most notably the classic supply/demand model of price determination.</p>
<p>The identification problem in simultaneous equations dates back to Philip Wright (1915) and Working (1927). The method of instrumental variables first appears in an Appendix of a 1928 book by Philip Wright, though the authorship is sometimes credited to his son Sewell Wright. The label “instrumental variables” was introduced by Reiersøl (1945). An excellent review of the history of instrumental variables is Stock and Trebbi (2003).</p>
</section>
<section id="overview" class="level2" data-number="12.2">
<h2 data-number="12.2" class="anchored" data-anchor-id="overview"><span class="header-section-number">12.2</span> Overview</h2>
<p>We say that there is endogeneity in the linear model</p>
<p><span class="math display">\[
Y=X^{\prime} \beta+e
\]</span></p>
<p>if <span class="math inline">\(\beta\)</span> is the parameter of interest and</p>
<p><span class="math display">\[
\mathbb{E}[X e] \neq 0 \text {. }
\]</span></p>
<p>This is a core problem in econometrics and largely differentiates the field from statistics. To distinguish (12.1) from the regression and projection models, we will call (12.1) a structural equation and <span class="math inline">\(\beta\)</span> a structural parameter. When (12.2) holds, it is typical to say that <span class="math inline">\(X\)</span> is endogenous for <span class="math inline">\(\beta\)</span>.</p>
<p>Endogeneity cannot happen if the coefficient is defined by linear projection. Indeed, we can define the linear projection coefficient <span class="math inline">\(\beta^{*}=\mathbb{E}\left[X X^{\prime}\right]^{-1} \mathbb{E}[X Y]\)</span> and linear projection equation</p>
<p><span class="math display">\[
\begin{aligned}
Y &amp;=X^{\prime} \beta^{*}+e^{*} \\
\mathbb{E}\left[X e^{*}\right] &amp;=0 .
\end{aligned}
\]</span></p>
<p>However, under endogeneity (12.2) the projection coefficient <span class="math inline">\(\beta^{*}\)</span> does not equal the structural parameter <span class="math inline">\(\beta\)</span>. Indeed,</p>
<p><span class="math display">\[
\begin{aligned}
\beta^{*} &amp;=\left(\mathbb{E}\left[X X^{\prime}\right]\right)^{-1} \mathbb{E}[X Y] \\
&amp;=\left(\mathbb{E}\left[X X^{\prime}\right]\right)^{-1} \mathbb{E}\left[X\left(X^{\prime} \beta+e\right)\right] \\
&amp;=\beta+\left(\mathbb{E}\left[X X^{\prime}\right]\right)^{-1} \mathbb{E}[X e] \neq \beta
\end{aligned}
\]</span></p>
<p>the final relation because <span class="math inline">\(\mathbb{E}[X e] \neq 0\)</span>.</p>
<p>Thus endogeneity requires that the coefficient be defined differently than projection. We describe such definitions as structural. We will present three examples in the following section.</p>
<p>Endogeneity implies that the least squares estimator is inconsistent for the structural parameter. Indeed, under i.i.d. sampling, least squares is consistent for the projection coefficient.</p>
<p><span class="math display">\[
\widehat{\beta} \underset{p}{\longrightarrow}\left(\mathbb{E}\left[X X^{\prime}\right]\right)^{-1} \mathbb{E}[X Y]=\beta^{*} \neq \beta .
\]</span></p>
<p>The inconsistency of least squares is typically referred to as endogeneity bias or estimation bias due to endogeneity. This is an imperfect label as the actual issue is inconsistency, not bias.</p>
<p>As the structural parameter <span class="math inline">\(\beta\)</span> is the parameter of interest, endogeneity requires the development of alternative estimation methods. We discuss those in later sections.</p>
</section>
<section id="examples" class="level2" data-number="12.3">
<h2 data-number="12.3" class="anchored" data-anchor-id="examples"><span class="header-section-number">12.3</span> Examples</h2>
<p>The concept of endogeneity may be easiest to understand by example. We discuss three. In each case it is important to see how the structural parameter <span class="math inline">\(\beta\)</span> is defined independently from the linear projection model.</p>
<p>Example: Measurement error in the regressor. Suppose that <span class="math inline">\((Y, Z)\)</span> are joint random variables, <span class="math inline">\(\mathbb{E}[Y \mid Z]=Z^{\prime} \beta\)</span> is linear, and <span class="math inline">\(\beta\)</span> is the structural parameter. <span class="math inline">\(Z\)</span> is not observed. Instead we observe <span class="math inline">\(X=Z+u\)</span> where <span class="math inline">\(u\)</span> is a <span class="math inline">\(k \times 1\)</span> measurement error, independent of <span class="math inline">\(e\)</span> and <span class="math inline">\(Z\)</span>. This is an example of a latent variable model, where “latent” refers to an unobserved structural variable.</p>
<p>The model <span class="math inline">\(X=Z+u\)</span> with <span class="math inline">\(Z\)</span> and <span class="math inline">\(u\)</span> independent and <span class="math inline">\(\mathbb{E}[u]=0\)</span> is known as classical measurement error. This means that <span class="math inline">\(X\)</span> is a noisy but unbiased measure of <span class="math inline">\(Z\)</span>.</p>
<p>By substitution we can express <span class="math inline">\(Y\)</span> as a function of the observed variable <span class="math inline">\(X\)</span>.</p>
<p><span class="math display">\[
Y=Z^{\prime} \beta+e=(X-u)^{\prime} \beta+e=X^{\prime} \beta+v
\]</span></p>
<p>where <span class="math inline">\(v=e-u^{\prime} \beta\)</span>. This means that <span class="math inline">\((Y, X)\)</span> satisfy the linear equation</p>
<p><span class="math display">\[
Y=X^{\prime} \beta+v
\]</span></p>
<p>with an error <span class="math inline">\(v\)</span>. But this error is not a projection error. Indeed,</p>
<p><span class="math display">\[
\mathbb{E}[X v]=\mathbb{E}\left[(Z+u)\left(e-u^{\prime} \beta\right)\right]=-\mathbb{E}\left[u u^{\prime}\right] \beta \neq 0
\]</span></p>
<p>if <span class="math inline">\(\beta \neq 0\)</span> and <span class="math inline">\(\mathbb{E}\left[u u^{\prime}\right] \neq 0\)</span>. As we learned in the previous section, if <span class="math inline">\(\mathbb{E}[X \nu] \neq 0\)</span> then least squares estimation will be inconsistent.</p>
<p>We can calculate the form of the projection coefficient (which is consistently estimated by least squares). For simplicity suppose that <span class="math inline">\(k=1\)</span>. We find</p>
<p><span class="math display">\[
\beta^{*}=\beta+\frac{\mathbb{E}[X \nu]}{\mathbb{E}\left[X^{2}\right]}=\beta\left(1-\frac{\mathbb{E}\left[u^{2}\right]}{\mathbb{E}\left[X^{2}\right]}\right) .
\]</span></p>
<p>Since <span class="math inline">\(\mathbb{E}\left[u^{2}\right] / \mathbb{E}\left[X^{2}\right]&lt;1\)</span> the projection coefficient shrinks the structural parameter <span class="math inline">\(\beta\)</span> towards zero. This is called measurement error bias or attenuation bias.</p>
<p>To illustrate, Figure 12.1(a) displays the impact of measurement error on the regression line. The three solid points are pairs <span class="math inline">\((Y, Z)\)</span> which are measured without error. The regression function drawn through these three points is marked as “No Measurement Error”. The six open circles mark pairs <span class="math inline">\((Y, X)\)</span> where <span class="math inline">\(X=Z+u\)</span> with <span class="math inline">\(u=\{+1,-1\}\)</span>. Thus <span class="math inline">\(X\)</span> is a mis-measured version of <span class="math inline">\(Z\)</span>. The six open circles spread the joint distribution along the <span class="math inline">\(\mathrm{x}\)</span>-axis, but not along the <span class="math inline">\(\mathrm{y}\)</span>-axis. The regression line drawn for these six points is marked as “With Measurement Error”. You can see that the latter regression line is flattened relative to the original regression function. This is the attenuation bias due to measurement error.</p>
<p><img src="images//2022_09_17_f9391324ededdbb7a34eg-03.jpg" class="img-fluid"></p>
<ol type="a">
<li>Measurement Error</li>
</ol>
<p><img src="images//2022_09_17_f9391324ededdbb7a34eg-03(1).jpg" class="img-fluid"></p>
<ol start="2" type="a">
<li>Supply and Demand</li>
</ol>
<p>Figure 12.1: Examples of Endogeneity</p>
<p>Example: Supply and Demand. The variables <span class="math inline">\(Q\)</span> and <span class="math inline">\(P\)</span> (quantity and price) are determined jointly by the demand equation</p>
<p><span class="math display">\[
Q=-\beta_{1} P+e_{1}
\]</span></p>
<p>and the supply equation</p>
<p><span class="math display">\[
Q=\beta_{2} P+e_{2} \text {. }
\]</span></p>
<p>Assume that <span class="math inline">\(e=\left(e_{1}, e_{2}\right)\)</span> satisfies <span class="math inline">\(\mathbb{E}[e]=0\)</span> and <span class="math inline">\(\mathbb{E}\left[e e^{\prime}\right]=\boldsymbol{I}_{2}\)</span> (the latter for simplicity). The question is: if we regress <span class="math inline">\(Q\)</span> on <span class="math inline">\(P\)</span>, what happens?</p>
<p>It is helpful to solve for <span class="math inline">\(Q\)</span> and <span class="math inline">\(P\)</span> in terms of the errors. In matrix notation,</p>
<p><span class="math display">\[
\left[\begin{array}{cc}
1 &amp; \beta_{1} \\
1 &amp; -\beta_{2}
\end{array}\right]\left(\begin{array}{l}
Q \\
P
\end{array}\right)=\left(\begin{array}{l}
e_{1} \\
e_{2}
\end{array}\right)
\]</span></p>
<p>so</p>
<p><span class="math display">\[
\begin{aligned}
\left(\begin{array}{l}
Q \\
P
\end{array}\right) &amp;=\left[\begin{array}{cc}
1 &amp; \beta_{1} \\
1 &amp; -\beta_{2}
\end{array}\right]^{-1}\left(\begin{array}{c}
e_{1} \\
e_{2}
\end{array}\right) \\
&amp;=\left[\begin{array}{cc}
\beta_{2} &amp; \beta_{1} \\
1 &amp; -1
\end{array}\right]\left(\begin{array}{l}
e_{1} \\
e_{2}
\end{array}\right)\left(\frac{1}{\beta_{1}+\beta_{2}}\right) \\
&amp;=\left(\begin{array}{c}
\left(\beta_{2} e_{1}+\beta_{1} e_{2}\right) /\left(\beta_{1}+\beta_{2}\right) \\
\left(e_{1}-e_{2}\right) /\left(\beta_{1}+\beta_{2}\right)
\end{array}\right) .
\end{aligned}
\]</span></p>
<p>The projection of <span class="math inline">\(Q\)</span> on <span class="math inline">\(P\)</span> yields <span class="math inline">\(Q=\beta^{*} P+e^{*}\)</span> with <span class="math inline">\(\mathbb{E}\left[P e^{*}\right]=0\)</span> and the projection coefficient is</p>
<p><span class="math display">\[
\beta^{*}=\frac{\mathbb{E}[P Q]}{\mathbb{E}\left[P^{2}\right]}=\frac{\beta_{2}-\beta_{1}}{2} .
\]</span></p>
<p>The projection coefficient <span class="math inline">\(\beta^{*}\)</span> equals neither the demand slope <span class="math inline">\(\beta_{1}\)</span> nor the supply slope <span class="math inline">\(\beta_{2}\)</span>, but equals an average of the two. (The fact that it is a simple average is an artifact of the covariance structure.)</p>
<p>The OLS estimator satisfies <span class="math inline">\(\widehat{\beta} \underset{p}{\rightarrow} \beta^{*}\)</span> and the limit does not equal either <span class="math inline">\(\beta_{1}\)</span> or <span class="math inline">\(\beta_{2}\)</span>. This is called simultaneous equations bias. This occurs generally when <span class="math inline">\(Y\)</span> and <span class="math inline">\(X\)</span> are jointly determined, as in a market equilibrium.</p>
<p>Generally, when both the dependent variable and a regressor are simultaneously determined then the regressor should be treated as endogenous.</p>
<p>To illustrate, Figure 12.1(b) draws a supply/demand model with Quantity on the y-axis and Price on the <span class="math inline">\(\mathrm{x}\)</span>-axis. The supply and demand equations are <span class="math inline">\(Q=P+\varepsilon_{1}\)</span> and <span class="math inline">\(Q=4-P-\varepsilon_{2}\)</span>, respectively. Suppose that the errors each have the Rademacher distribution <span class="math inline">\(\varepsilon \in\{-1,+1\}\)</span>. This model has four equilibrium outcomes, marked by the four points in the figure. The regression line through these four points has a slope of zero and is marked as “Regression”. This is what would be measured by a least squares regression of observed quantity on observed price. This is endogeneity bias due to simultaneity.</p>
<p>Example: Choice Variables as Regressors. Take the classic wage equation</p>
<p><span class="math display">\[
\log (\text { wage })=\beta \text { education }+e
\]</span></p>
<p>with <span class="math inline">\(\beta\)</span> the average causal effect of education on wages. If wages are affected by unobserved ability, and individuals with high ability self-select into higher education, then <span class="math inline">\(e\)</span> contains unobserved ability, so education and <span class="math inline">\(e\)</span> will be positively correlated. Hence education is endogenous. The positive correlation means that the linear projection coefficient <span class="math inline">\(\beta^{*}\)</span> will be upward biased relative to the structural coefficient <span class="math inline">\(\beta\)</span>. Thus least squares (which is estimating the projection coefficient) will tend to over-estimate the causal effect of education on wages.</p>
<p>This type of endogeneity occurs generally when <span class="math inline">\(Y\)</span> and <span class="math inline">\(X\)</span> are both choices made by an economic agent, even if they are made at different points in time.</p>
<p>Generally, when both the dependent variable and a regressor are choice variables made by the same agent, the variables should be treated as endogenous.</p>
<p>This example was illustrated back in Figure <span class="math inline">\(2.8\)</span> which displayed the joint distribution of wages and education of the population of Jennifers and Georges. In Figure 2.8, the plotted Average Causal Effect is the structural impact (on average in the population) of college education on wages. The plotted regression line has a larger slope, as it adds the endogeneity bias due to the fact that education is a choice variable.</p>
</section>
<section id="endogenous-regressors" class="level2" data-number="12.4">
<h2 data-number="12.4" class="anchored" data-anchor-id="endogenous-regressors"><span class="header-section-number">12.4</span> Endogenous Regressors</h2>
<p>We have defined endogeneity as the context where a regressor is correlated with the equation error. The converse of endogeneity is exogeneity. That is, we say a regressor <span class="math inline">\(X\)</span> is exogenous for <span class="math inline">\(\beta\)</span> if <span class="math inline">\(\mathbb{E}[X e]=\)</span> 0 . In general the distinction in an economic model is that a regressor <span class="math inline">\(X\)</span> is endogenous if it is jointly determined with <span class="math inline">\(Y\)</span>, while a regressor <span class="math inline">\(X\)</span> is exogenous if it is determined separately from <span class="math inline">\(Y\)</span>.</p>
<p>In most applications only a subset of the regressors are treated as endogenous. Partition <span class="math inline">\(X=\left(X_{1}, X_{2}\right)\)</span> with dimensions <span class="math inline">\(\left(k_{1}, k_{2}\right)\)</span> so that <span class="math inline">\(X_{1}\)</span> contains the exogenous regressors and <span class="math inline">\(X_{2}\)</span> contains the endogenous regressors. As the dependent variable <span class="math inline">\(Y\)</span> is also endogenous, we sometimes differentiate <span class="math inline">\(X_{2}\)</span> by calling it the endogenous right-hand-side variable. Similarly partition <span class="math inline">\(\beta=\left(\beta_{1}, \beta_{2}\right)\)</span>. With this notation the structural equation is</p>
<p><span class="math display">\[
Y=X_{1}^{\prime} \beta_{1}+X_{2}^{\prime} \beta_{2}+e .
\]</span></p>
<p>An alternative notation is as follows. Let <span class="math inline">\(Y_{2}=X_{2}\)</span> be the endogenous regressors and rename the dependent variable <span class="math inline">\(Y\)</span> as <span class="math inline">\(Y_{1}\)</span>. Then the structural equation is</p>
<p><span class="math display">\[
Y_{1}=X_{1}^{\prime} \beta_{1}+Y_{2}^{\prime} \beta_{2}+e .
\]</span></p>
<p>This is especially useful so that the notation clarifies which variables are endogenous and which exogenous. We also write <span class="math inline">\(\vec{Y}=\left(Y_{1}, Y_{2}\right)\)</span> as the set of endogenous variables. We use the notation <span class="math inline">\(\vec{Y}\)</span> so that there is no confusion with <span class="math inline">\(Y\)</span> as defined in (12.3).</p>
<p>The assumptions regarding the regressors and regression error are</p>
<p><span class="math display">\[
\begin{aligned}
&amp;\mathbb{E}\left[X_{1} e\right]=0 \\
&amp;\mathbb{E}\left[Y_{2} e\right] \neq 0 .
\end{aligned}
\]</span></p>
<p>The endogenous regressors <span class="math inline">\(Y_{2}\)</span> are the critical variables discussed in the examples of the previous section - simultaneous variables, choice variables, mis-measured regressors - that are potentially correlated with the equation error <span class="math inline">\(e\)</span>. In many applications <span class="math inline">\(k_{2}\)</span> is small (1 or 2 ). The exogenous variables <span class="math inline">\(X_{1}\)</span> are the remaining regressors (including the equation intercept) and can be low or high dimensional.</p>
</section>
<section id="instruments" class="level2" data-number="12.5">
<h2 data-number="12.5" class="anchored" data-anchor-id="instruments"><span class="header-section-number">12.5</span> Instruments</h2>
<p>To consistently estimate <span class="math inline">\(\beta\)</span> we require additional information. One type of information which is commonly used in economic applications are what we call instruments.</p>
<p>Definition <span class="math inline">\(12.1\)</span> The <span class="math inline">\(\ell \times 1\)</span> random vector <span class="math inline">\(Z\)</span> is an instrumental variable for (12.3) if</p>
<p><span class="math display">\[
\begin{aligned}
\mathbb{E}[Z e] &amp;=0 \\
\mathbb{E}\left[Z Z^{\prime}\right] &amp;&gt;0 \\
\operatorname{rank}\left(\mathbb{E}\left[Z X^{\prime}\right]\right) &amp;=k .
\end{aligned}
\]</span></p>
<p>There are three components to the definition as given. The first (12.5) is that the instruments are uncorrelated with the regression error. The second (12.6) is a normalization which excludes linearly redundant instruments. The third (12.7) is often called the relevance condition and is essential for the identification of the model, as we discuss later. A necessary condition for (12.7) is that <span class="math inline">\(\ell \geq k\)</span>.</p>
<p>Condition (12.5) - that the instruments are uncorrelated with the equation error - is often described as that they are exogenous in the sense that they are determined outside the model for <span class="math inline">\(Y\)</span>.</p>
<p>Notice that the regressors <span class="math inline">\(X_{1}\)</span> satisfy condition (12.5) and thus should be included as instrumental variables. They are therefore a subset of the variables <span class="math inline">\(Z\)</span>. Notationally we make the partition</p>
<p><span class="math display">\[
Z=\left(\begin{array}{l}
Z_{1} \\
Z_{2}
\end{array}\right)=\left(\begin{array}{c}
X_{1} \\
Z_{2}
\end{array}\right) \begin{aligned}
&amp;k_{1} \\
&amp;\ell_{2}
\end{aligned} .
\]</span></p>
<p>Here, <span class="math inline">\(X_{1}=Z_{1}\)</span> are the included exogenous variables and <span class="math inline">\(Z_{2}\)</span> are the excluded exogenous variables. That is, <span class="math inline">\(Z_{2}\)</span> are variables which could be included in the equation for <span class="math inline">\(Y\)</span> (in the sense that they are uncorrelated with <span class="math inline">\(e\)</span> ) yet can be excluded as they have true zero coefficients in the equation. With this notation we can also write the structural equation (12.4) as</p>
<p><span class="math display">\[
Y_{1}=Z_{1}^{\prime} \beta_{1}+Y_{2}^{\prime} \beta_{2}+e .
\]</span></p>
<p>This is useful notation as it clarifies that the variable <span class="math inline">\(Z_{1}\)</span> is exogenous and the variable <span class="math inline">\(Y_{2}\)</span> is endogenous.</p>
<p>Many authors describe <span class="math inline">\(Z_{1}\)</span> as the “exogenous variables”, <span class="math inline">\(Y_{2}\)</span> as the “endogenous variables”, and <span class="math inline">\(Z_{2}\)</span> as the “instrumental variables”.</p>
<p>We say that the model is just-identified if <span class="math inline">\(\ell=k\)</span> and over-identified if <span class="math inline">\(\ell&gt;k\)</span>.</p>
<p>What variables can be used as instrumental variables? From the definition <span class="math inline">\(\mathbb{E}[Z e]=0\)</span> the instrument must be uncorrelated with the equation error, meaning that it is excluded from the structural equation as mentioned above. From the rank condition (12.7) it is also important that the instrumental variables be correlated with the endogenous variables <span class="math inline">\(Y_{2}\)</span> after controlling for the other exogenous variables <span class="math inline">\(Z_{1}\)</span>. These two requirements are typically interpreted as requiring that the instruments be determined outside the system for <span class="math inline">\(\vec{Y}\)</span>, causally determine <span class="math inline">\(Y_{2}\)</span>, but do not causally determine <span class="math inline">\(Y_{1}\)</span> except through <span class="math inline">\(Y_{2}\)</span>.</p>
<p>Let’s take the three examples given above.</p>
<p>Measurement error in the regressor. When <span class="math inline">\(X\)</span> is a mis-measured version of <span class="math inline">\(Z\)</span> a common choice for an instrument <span class="math inline">\(Z_{2}\)</span> is an alternative measurement of <span class="math inline">\(Z\)</span>. For this <span class="math inline">\(Z_{2}\)</span> to satisfy the property of an instrumental variable the measurement error in <span class="math inline">\(Z_{2}\)</span> must be independent of that in <span class="math inline">\(X\)</span>.</p>
<p>Supply and Demand. An appropriate instrument for price <span class="math inline">\(P\)</span> in a demand equation is a variable <span class="math inline">\(Z_{2}\)</span> which influences supply but not demand. Such a variable affects the equilibrium values of <span class="math inline">\(P\)</span> and <span class="math inline">\(Q\)</span> but does not directly affect price except through quantity. Variables which affect supply but not demand are typically related to production costs.</p>
<p>An appropriate instrument for price in a supply equation is a variable which influences demand but not supply. Such a variable affects the equilibrium values of price and quantity but only affects price through quantity.</p>
<p>Choice Variable as Regressor. An ideal instrument affects the choice of the regressor (education) but does not directly influence the dependent variable (wages) except through the indirect effect on the regressor. We will discuss an example in the next section.</p>
</section>
<section id="example-college-proximity" class="level2" data-number="12.6">
<h2 data-number="12.6" class="anchored" data-anchor-id="example-college-proximity"><span class="header-section-number">12.6</span> Example: College Proximity</h2>
<p>In a influential paper David Card (1995) suggested if a potential student lives close to a college this reduces the cost of attendence and thereby raises the likelihood that the student will attend college. However, college proximity does not directly affect a student’s skills or abilities so should not have a direct effect on his or her market wage. These considerations suggest that college proximity can be used as an instrument for education in a wage regression. We use the simplest model reported in Card’s paper to illustrate the concepts of instrumental variables throughout the chapter.</p>
<p>Card used data from the National Longitudinal Survey of Young Men (NLSYM) for 1976. A baseline least squares wage regression for his data set is reported in the first column of Table 12.1. The dependent variable is the log of weekly earnings. The regressors are education (years of schooling), experience (years of work experience, calculated as age (years) less education <span class="math inline">\(+6\)</span> ), experience <span class="math inline">\({ }^{2} / 100\)</span>, Black, south (an indicator for residence in the southern region of the U.S.), and urban (an indicator for residence in a standard metropolitan statistical area). We drop observations for which wage is missing. The remaining sample has 3,010 observations. His data is the file Card1995 on the textbook website. The point estimate obtained by least squares suggests an <span class="math inline">\(7 %\)</span> increase in earnings for each year of education.</p>
<p>Table 12.1: Instrumental Variable Wage Regressions</p>
<table class="table">
<colgroup>
<col style="width: 26%">
<col style="width: 12%">
<col style="width: 12%">
<col style="width: 13%">
<col style="width: 12%">
<col style="width: 12%">
<col style="width: 12%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">education</th>
<th>OLS</th>
<th>IV(a)</th>
<th>IV(b)</th>
<th>2SLS(a)</th>
<th>2SLS(b)</th>
<th>LIML</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"></td>
<td><span class="math inline">\(0.074\)</span></td>
<td><span class="math inline">\(0.132\)</span></td>
<td><span class="math inline">\(0.133\)</span></td>
<td><span class="math inline">\(0.161\)</span></td>
<td><span class="math inline">\(0.160\)</span></td>
<td><span class="math inline">\(0.164\)</span></td>
</tr>
<tr class="even">
<td style="text-align: left;"></td>
<td><span class="math inline">\((0.004)\)</span></td>
<td><span class="math inline">\((0.049)\)</span></td>
<td><span class="math inline">\((0.051)\)</span></td>
<td><span class="math inline">\((0.040)\)</span></td>
<td><span class="math inline">\((0.041)\)</span></td>
<td><span class="math inline">\((0.042)\)</span></td>
</tr>
<tr class="odd">
<td style="text-align: left;"></td>
<td><span class="math inline">\(0.084\)</span></td>
<td><span class="math inline">\(0.107\)</span></td>
<td><span class="math inline">\(0.056\)</span></td>
<td><span class="math inline">\(0.119\)</span></td>
<td><span class="math inline">\(0.047\)</span></td>
<td><span class="math inline">\(0.120\)</span></td>
</tr>
<tr class="even">
<td style="text-align: left;">experience <span class="math inline">\(2 / 100\)</span></td>
<td><span class="math inline">\(-0.224\)</span></td>
<td><span class="math inline">\(-0.228\)</span></td>
<td><span class="math inline">\(-0.080\)</span></td>
<td><span class="math inline">\(-0.231\)</span></td>
<td><span class="math inline">\(-0.032\)</span></td>
<td><span class="math inline">\(-0.231\)</span></td>
</tr>
<tr class="odd">
<td style="text-align: left;"></td>
<td><span class="math inline">\((0.032)\)</span></td>
<td><span class="math inline">\((0.035)\)</span></td>
<td><span class="math inline">\((0.133)\)</span></td>
<td><span class="math inline">\((0.037)\)</span></td>
<td><span class="math inline">\((0.127)\)</span></td>
<td><span class="math inline">\((0.037)\)</span></td>
</tr>
<tr class="even">
<td style="text-align: left;">Black</td>
<td><span class="math inline">\(-0.190\)</span></td>
<td><span class="math inline">\(-0.131\)</span></td>
<td><span class="math inline">\(-0.103\)</span></td>
<td><span class="math inline">\(-0.102\)</span></td>
<td><span class="math inline">\(-0.064\)</span></td>
<td><span class="math inline">\(-0.099\)</span></td>
</tr>
<tr class="odd">
<td style="text-align: left;"></td>
<td><span class="math inline">\((0.017)\)</span></td>
<td><span class="math inline">\((0.051)\)</span></td>
<td><span class="math inline">\((0.075)\)</span></td>
<td><span class="math inline">\((0.044)\)</span></td>
<td><span class="math inline">\((0.061)\)</span></td>
<td><span class="math inline">\((0.045)\)</span></td>
</tr>
<tr class="even">
<td style="text-align: left;">south</td>
<td><span class="math inline">\(-0.125\)</span></td>
<td><span class="math inline">\(-0.105\)</span></td>
<td><span class="math inline">\(-0.098\)</span></td>
<td><span class="math inline">\(-0.095\)</span></td>
<td><span class="math inline">\(-0.086\)</span></td>
<td><span class="math inline">\(-0.094\)</span></td>
</tr>
<tr class="odd">
<td style="text-align: left;"></td>
<td><span class="math inline">\((0.015)\)</span></td>
<td><span class="math inline">\((0.023)\)</span></td>
<td><span class="math inline">\((0.0284)\)</span></td>
<td><span class="math inline">\((0.022)\)</span></td>
<td><span class="math inline">\((0.026)\)</span></td>
<td><span class="math inline">\((0.022)\)</span></td>
</tr>
<tr class="even">
<td style="text-align: left;">urban</td>
<td><span class="math inline">\(0.161\)</span></td>
<td><span class="math inline">\(0.131\)</span></td>
<td><span class="math inline">\(0.108\)</span></td>
<td><span class="math inline">\(0.116\)</span></td>
<td><span class="math inline">\(0.083\)</span></td>
<td><span class="math inline">\(0.115\)</span></td>
</tr>
<tr class="odd">
<td style="text-align: left;"></td>
<td><span class="math inline">\((0.015)\)</span></td>
<td><span class="math inline">\((0.030)\)</span></td>
<td><span class="math inline">\((0.049)\)</span></td>
<td><span class="math inline">\((0.026)\)</span></td>
<td><span class="math inline">\((0.041)\)</span></td>
<td><span class="math inline">\((0.027)\)</span></td>
</tr>
<tr class="even">
<td style="text-align: left;">Sargan</td>
<td></td>
<td></td>
<td></td>
<td><span class="math inline">\(0.82\)</span></td>
<td><span class="math inline">\(0.52\)</span></td>
<td><span class="math inline">\(0.82\)</span></td>
</tr>
<tr class="odd">
<td style="text-align: left;">p-value</td>
<td></td>
<td></td>
<td></td>
<td><span class="math inline">\(0.37\)</span></td>
<td><span class="math inline">\(0.47\)</span></td>
<td><span class="math inline">\(0.37\)</span></td>
</tr>
</tbody>
</table>
<p>Notes:</p>
<ol type="1">
<li><p>IV(a) uses college as an instrument for education.</p></li>
<li><p>IV(b) uses college, age, and age <span class="math inline">\(^{2} / 100\)</span> as instruments for education, experience, and experience <span class="math inline">\({ }^{2} / 100\)</span>.</p></li>
<li><p>2SLS(a) uses public and private as instruments for education.</p></li>
<li><p><span class="math inline">\(2 \mathrm{SLS}(\mathrm{b})\)</span> uses public, private, age, and age <span class="math inline">\({ }^{2}\)</span> as instruments for education, experience, and experience <span class="math inline">\(^{2} / 100\)</span>.</p></li>
<li><p>LIML uses public and private as instruments for education.</p></li>
</ol>
<p>As discussed in the previous sections it is reasonable to view years of education as a choice made by an individual and thus is likely endogenous for the structural return to education. This means that least squares is an estimate of a linear projection but is inconsistent for coefficient of a structural equation representing the causal impact of years of education on expected wages. Labor economics predicts that ability, education, and wages will be positively correlated. This suggests that the population projection coefficient estimated by least squares will be higher than the structural parameter (and hence upwards biased). However, the sign of the bias is uncertain because there are multiple regressors and there are other potential sources of endogeneity.</p>
<p>To instrument for the endogeneity of education, Card suggested that a reasonable instrument is a dummy variable indicating if the individual grew up near a college. We will consider three measures:</p>
<p>college Grew up in same county as a 4-year college</p>
<p>public Grew up in same county as a 4-year public college</p>
<p>private Grew up in same county as a 4-year private college.</p>
</section>
<section id="reduced-form" class="level2" data-number="12.7">
<h2 data-number="12.7" class="anchored" data-anchor-id="reduced-form"><span class="header-section-number">12.7</span> Reduced Form</h2>
<p>The reduced form is the relationship between the endogenous regressors <span class="math inline">\(Y_{2}\)</span> and the instruments <span class="math inline">\(Z\)</span>. A linear reduced form model for <span class="math inline">\(Y_{2}\)</span> is</p>
<p><span class="math display">\[
Y_{2}=\Gamma^{\prime} Z+u_{2}=\Gamma_{12}^{\prime} Z_{1}+\Gamma_{22}^{\prime} Z_{2}+u_{2}
\]</span></p>
<p>This is a multivariate regression as introduced in Chapter 11 . The <span class="math inline">\(\ell \times k_{2}\)</span> coefficient matrix <span class="math inline">\(\Gamma\)</span> is defined by linear projection:</p>
<p><span class="math display">\[
\Gamma=\mathbb{E}\left[Z Z^{\prime}\right]^{-1} \mathbb{E}\left[Z Y_{2}^{\prime}\right]
\]</span></p>
<p>This implies <span class="math inline">\(\mathbb{E}\left[Z u_{2}^{\prime}\right]=0\)</span>. The projection coefficient (12.11) is well defined and unique under (12.6).</p>
<p>We also construct the reduced form for <span class="math inline">\(Y_{1}\)</span>. Substitute (12.10) into (12.9) to obtain</p>
<p><span class="math display">\[
\begin{aligned}
Y_{1} &amp;=Z_{1}^{\prime} \beta_{1}+\left(\Gamma_{12}^{\prime} Z_{1}+\Gamma_{22}^{\prime} Z_{2}+u_{2}\right)^{\prime} \beta_{2}+e \\
&amp;=Z_{1}^{\prime} \lambda_{1}+Z_{2}^{\prime} \lambda_{2}+u_{1} \\
&amp;=Z^{\prime} \lambda+u_{1}
\end{aligned}
\]</span></p>
<p>where</p>
<p><span class="math display">\[
\begin{aligned}
&amp;\lambda_{1}=\beta_{1}+\Gamma_{12} \beta_{2} \\
&amp;\lambda_{2}=\Gamma_{22} \beta_{2} \\
&amp;u_{1}=u_{2}^{\prime} \beta_{2}+e .
\end{aligned}
\]</span></p>
<p>We can also write</p>
<p><span class="math display">\[
\lambda=\bar{\Gamma} \beta
\]</span></p>
<p>where</p>
<p><span class="math display">\[
\bar{\Gamma}=\left[\begin{array}{cc}
\boldsymbol{I}_{k_{1}} &amp; \Gamma_{12} \\
0 &amp; \Gamma_{22}
\end{array}\right]=\left[\begin{array}{cc}
\boldsymbol{I}_{k_{1}} &amp; \Gamma \\
0 &amp;
\end{array}\right] .
\]</span></p>
<p>Together, the reduced form equations for the system are</p>
<p><span class="math display">\[
\begin{aligned}
&amp;Y_{1}=\lambda^{\prime} Z+u_{1} \\
&amp;Y_{2}=\Gamma^{\prime} Z+u_{2} .
\end{aligned}
\]</span></p>
<p>or</p>
<p><span class="math display">\[
\vec{Y}=\left[\begin{array}{cc}
\lambda_{1}^{\prime} &amp; \lambda_{2}^{\prime} \\
\Gamma_{12}^{\prime} &amp; \Gamma_{22}^{\prime}
\end{array}\right] Z+u
\]</span></p>
<p>where <span class="math inline">\(u=\left(u_{1}, u_{2}\right)\)</span>.</p>
<p>The relationships (12.14)-(12.16) are critically important for understanding the identification of the structural parameters <span class="math inline">\(\beta_{1}\)</span> and <span class="math inline">\(\beta_{2}\)</span>, as we discuss below. These equations show the tight relationship between the structural parameters <span class="math inline">\(\left(\beta_{1}\right.\)</span> and <span class="math inline">\(\left.\beta_{2}\right)\)</span> and the reduced form parameters <span class="math inline">\((\Gamma\)</span> and <span class="math inline">\(\lambda)\)</span>.</p>
<p>The reduced form equations are projections so the coefficients may be estimated by least squares (see Chapter 11). The least squares estimators of (12.11) and (12.13) are</p>
<p><span class="math display">\[
\begin{aligned}
&amp;\widehat{\Gamma}=\left(\sum_{i=1}^{n} Z_{i} Z_{i}^{\prime}\right)^{-1}\left(\sum_{i=1}^{n} Z_{i} Y_{2 i}^{\prime}\right) \\
&amp;\widehat{\lambda}=\left(\sum_{i=1}^{n} Z_{i} Z_{i}^{\prime}\right)^{-1}\left(\sum_{i=1}^{n} Z_{i} Y_{1 i}\right) .
\end{aligned}
\]</span></p>
</section>
<section id="identification" class="level2" data-number="12.8">
<h2 data-number="12.8" class="anchored" data-anchor-id="identification"><span class="header-section-number">12.8</span> Identification</h2>
<p>A parameter is identified if it is a unique function of the probability distribution of the observables. One way to show that a parameter is identified is to write it as an explicit function of population moments. For example, the reduced form coefficient matrices <span class="math inline">\(\Gamma\)</span> and <span class="math inline">\(\lambda\)</span> are identified because they can be written as explicit functions of the moments of the variables <span class="math inline">\((Y, X, Z)\)</span>. That is,</p>
<p><span class="math display">\[
\begin{aligned}
&amp;\Gamma=\mathbb{E}\left[Z Z^{\prime}\right]^{-1} \mathbb{E}\left[Z Y_{2}^{\prime}\right] \\
&amp;\lambda=\mathbb{E}\left[Z Z^{\prime}\right]^{-1} \mathbb{E}\left[Z Y_{1}\right] .
\end{aligned}
\]</span></p>
<p>These are uniquely determined by the probability distribution of <span class="math inline">\(\left(Y_{1}, Y_{2}, Z\right)\)</span> if Definition <span class="math inline">\(12.1\)</span> holds, because this includes the requirement that <span class="math inline">\(\mathbb{E}\left[Z Z^{\prime}\right]\)</span> is invertible.</p>
<p>We are interested in the structural parameter <span class="math inline">\(\beta\)</span>. It relates to <span class="math inline">\((\lambda, \Gamma)\)</span> through (12.16). <span class="math inline">\(\beta\)</span> is identified if it uniquely determined by this relation. This is a set of <span class="math inline">\(\ell\)</span> equations with <span class="math inline">\(k\)</span> unknowns with <span class="math inline">\(\ell \geq k\)</span>. From linear algebra we know that there is a unique solution if and only if <span class="math inline">\(\bar{\Gamma}\)</span> has full rank <span class="math inline">\(k\)</span>.</p>
<p><span class="math display">\[
\operatorname{rank}(\bar{\Gamma})=k .
\]</span></p>
<p>Under (12.22) <span class="math inline">\(\beta\)</span> can be uniquely solved from (12.16). If (12.22) fails then (12.16) has fewer equations than coefficients so there is not a unique solution.</p>
<p>We can write <span class="math inline">\(\bar{\Gamma}=\mathbb{E}\left[Z Z^{\prime}\right]^{-1} \mathbb{E}\left[Z X^{\prime}\right]\)</span>. Combining this with (12.16) we obtain</p>
<p><span class="math display">\[
\mathbb{E}\left[Z Z^{\prime}\right]^{-1} \mathbb{E}\left[Z Y_{1}\right]=\mathbb{E}\left[Z Z^{\prime}\right]^{-1} \mathbb{E}\left[Z X^{\prime}\right] \beta
\]</span></p>
<p>or</p>
<p><span class="math display">\[
\mathbb{E}\left[Z Y_{1}\right]=\mathbb{E}\left[Z X^{\prime}\right] \beta
\]</span></p>
<p>which is a set of <span class="math inline">\(\ell\)</span> equations with <span class="math inline">\(k\)</span> unknowns. This has a unique solution if (and only if)</p>
<p><span class="math display">\[
\operatorname{rank}\left(\mathbb{E}\left[Z X^{\prime}\right]\right)=k
\]</span></p>
<p>which was listed in (12.7) as a condition of Definition 12.1. (Indeed, this is why it was listed as part of the definition.) We can also see that (12.22) and (12.23) are equivalent ways of expressing the same requirement. If this condition fails then <span class="math inline">\(\beta\)</span> will not be identified. The condition (12.22)-(12.23) is called the relevance condition.</p>
<p>It is useful to have explicit expressions for the solution <span class="math inline">\(\beta\)</span>. The easiest case is when <span class="math inline">\(\ell=k\)</span>. Then (12.22) implies <span class="math inline">\(\bar{\Gamma}\)</span> is invertible so the structural parameter equals <span class="math inline">\(\beta=\bar{\Gamma}^{-1} \lambda\)</span>. It is a unique solution because <span class="math inline">\(\bar{\Gamma}\)</span> and <span class="math inline">\(\lambda\)</span> are unique and <span class="math inline">\(\bar{\Gamma}\)</span> is invertible.</p>
<p>When <span class="math inline">\(\ell&gt;k\)</span> we can solve for <span class="math inline">\(\beta\)</span> by applying least squares to the system of equations <span class="math inline">\(\lambda=\bar{\Gamma} \beta\)</span>. This is <span class="math inline">\(\ell\)</span> equations with <span class="math inline">\(k\)</span> unknowns and no error. The least squares solution is <span class="math inline">\(\beta=\left(\bar{\Gamma}^{\prime} \bar{\Gamma}\right)^{-1} \bar{\Gamma}^{\prime} \lambda\)</span>. Under (12.22) the matrix <span class="math inline">\(\bar{\Gamma}^{\prime} \bar{\Gamma}\)</span> is invertible so the solution is unique.</p>
<p><span class="math inline">\(\beta\)</span> is identified if <span class="math inline">\(\operatorname{rank}(\bar{\Gamma})=k\)</span>, which is true if and only if <span class="math inline">\(\operatorname{rank}\left(\Gamma_{22}\right)=k_{2}\)</span> (by the upper-diagonal structure of <span class="math inline">\(\bar{\Gamma})\)</span>. Thus the key to identification of the model rests on the <span class="math inline">\(\ell_{2} \times k_{2}\)</span> matrix <span class="math inline">\(\Gamma_{22}\)</span> in (12.10). To see this, recall the reduced form relationships (12.14)-(12.15). We can see that <span class="math inline">\(\beta_{2}\)</span> is identified from (12.15) alone, and the necessary and sufficient condition is <span class="math inline">\(\operatorname{rank}\left(\Gamma_{22}\right)=k_{2}\)</span>. If this is satisfied then the solution equals <span class="math inline">\(\beta_{2}=\left(\Gamma_{22}^{\prime} \Gamma_{22}\right)^{-1} \Gamma_{22}^{\prime} \lambda_{2} \cdot \beta_{1}\)</span> is identified from this and (12.14), with the explicit solution <span class="math inline">\(\beta_{1}=\lambda_{1}-\Gamma_{12}\left(\Gamma_{22}^{\prime} \Gamma_{22}\right)^{-1} \Gamma_{22}^{\prime} \lambda_{2}\)</span>. In the just-identified case <span class="math inline">\(\left(\ell_{2}=k_{2}\right)\)</span> these equations simplify as <span class="math inline">\(\beta_{2}=\Gamma_{22}^{-1} \lambda_{2}\)</span> and <span class="math inline">\(\beta_{1}=\lambda_{1}-\Gamma_{12} \Gamma_{22}^{-1} \lambda_{2}\)</span></p>
</section>
<section id="instrumental-variables-estimator" class="level2" data-number="12.9">
<h2 data-number="12.9" class="anchored" data-anchor-id="instrumental-variables-estimator"><span class="header-section-number">12.9</span> Instrumental Variables Estimator</h2>
<p>In this section we consider the special case where the model is just-identified so that <span class="math inline">\(\ell=k\)</span>.</p>
<p>The assumption that <span class="math inline">\(Z\)</span> is an instrumental variable implies that <span class="math inline">\(\mathbb{E}[Z e]=0\)</span>. Making the substitution <span class="math inline">\(e=Y_{1}-X^{\prime} \beta\)</span> we find <span class="math inline">\(\mathbb{E}\left[Z\left(Y_{1}-X^{\prime} \beta\right)\right]=0\)</span>. Expanding,</p>
<p><span class="math display">\[
\mathbb{E}\left[Z Y_{1}\right]-\mathbb{E}\left[Z X^{\prime}\right] \beta=0 .
\]</span></p>
<p>This is a system of <span class="math inline">\(\ell=k\)</span> equations and <span class="math inline">\(k\)</span> unknowns. Solving for <span class="math inline">\(\beta\)</span> we find</p>
<p><span class="math display">\[
\beta=\left(\mathbb{E}\left[Z X^{\prime}\right]\right)^{-1} \mathbb{E}\left[Z Y_{1}\right] .
\]</span></p>
<p>This requires that the matrix <span class="math inline">\(\mathbb{E}\left[Z X^{\prime}\right]\)</span> is invertible, which holds under (12.7) or equivalently (12.23).</p>
<p>The instrumental variables (IV) estimator <span class="math inline">\(\beta\)</span> replaces population by sample moments. We find</p>
<p><span class="math display">\[
\begin{aligned}
\widehat{\beta}_{\mathrm{iv}} &amp;=\left(\frac{1}{n} \sum_{i=1}^{n} Z_{i} X_{i}^{\prime}\right)^{-1}\left(\frac{1}{n} \sum_{i=1}^{n} Z_{i} Y_{1 i}\right) \\
&amp;=\left(\sum_{i=1}^{n} Z_{i} X_{i}^{\prime}\right)^{-1}\left(\sum_{i=1}^{n} Z_{i} Y_{1 i}\right) .
\end{aligned}
\]</span></p>
<p>More generally, given any variable <span class="math inline">\(W \in \mathbb{R}^{k}\)</span> it is common to refer to the estimator</p>
<p><span class="math display">\[
\widehat{\beta}_{\mathrm{iv}}=\left(\sum_{i=1}^{n} W_{i} X_{i}^{\prime}\right)^{-1}\left(\sum_{i=1}^{n} W_{i} Y_{1 i}\right)
\]</span></p>
<p>as the IV estimator for <span class="math inline">\(\beta\)</span> using the instrument <span class="math inline">\(W\)</span>.</p>
<p>Alternatively, recall that when <span class="math inline">\(\ell=k\)</span> the structural parameter can be written as a function of the reduced form parameters as <span class="math inline">\(\beta=\bar{\Gamma}^{-1} \lambda\)</span>. Replacing <span class="math inline">\(\bar{\Gamma}\)</span> and <span class="math inline">\(\lambda\)</span> by their least squares estimators (12.18)-(12.19) we can construct what is called the Indirect Least Squares (ILS) estimator. Using the matrix algebra representations</p>
<p><span class="math display">\[
\begin{aligned}
\widehat{\beta}_{\mathrm{ils}} &amp;=\widehat{\bar{\Gamma}}^{-1} \widehat{\lambda} \\
&amp;=\left(\left(\boldsymbol{Z}^{\prime} \boldsymbol{Z}\right)^{-1}\left(\boldsymbol{Z}^{\prime} \boldsymbol{X}\right)\right)^{-1}\left(\left(\boldsymbol{Z}^{\prime} \boldsymbol{Z}\right)^{-1}\left(\boldsymbol{Z}^{\prime} \boldsymbol{Y}_{1}\right)\right) \\
&amp;=\left(\boldsymbol{Z}^{\prime} \boldsymbol{X}\right)^{-1}\left(\boldsymbol{Z}^{\prime} \boldsymbol{Z}\right)\left(\boldsymbol{Z}^{\prime} \boldsymbol{Z}\right)^{-1}\left(\boldsymbol{Z}^{\prime} \boldsymbol{Y}_{1}\right) \\
&amp;=\left(\boldsymbol{Z}^{\prime} \boldsymbol{X}\right)^{-1}\left(\boldsymbol{Z}^{\prime} \boldsymbol{Y}_{1}\right) .
\end{aligned}
\]</span></p>
<p>We see that this equals the IV estimator (12.24). Thus the ILS and IV estimators are identical.</p>
<p>Given the IV estimator we define the residual <span class="math inline">\(\widehat{e}_{i}=Y_{1 i}-X_{i}^{\prime} \widehat{\beta}_{\mathrm{iv}}\)</span>. It satisfies</p>
<p><span class="math display">\[
\boldsymbol{Z}^{\prime} \widehat{\boldsymbol{e}}=\boldsymbol{Z}^{\prime} \boldsymbol{Y}_{1}-\boldsymbol{Z}^{\prime} \boldsymbol{X}\left(\boldsymbol{Z}^{\prime} \boldsymbol{X}\right)^{-1}\left(\boldsymbol{Z}^{\prime} \boldsymbol{Y}_{1}\right)=0
\]</span></p>
<p>Since <span class="math inline">\(Z\)</span> includes an intercept this means that the residuals sum to zero and are uncorrelated with the included and excluded instruments.</p>
<p>To illustrate IV regression we estimate the reduced form equations, treating education as endogenous and using college as an instrumental variable. The reduced form equations for log(wage) and education are reported in the first and second columns of Table 12.2. Table 12.2: Reduced Form Regressions</p>
<p><img src="images//2022_09_17_f9391324ededdbb7a34eg-11.jpg" class="img-fluid"></p>
<p>Of particular interest is the equation for the endogenous regressor education, and the coefficients for the excluded instruments - in this case college. The estimated coefficient equals <span class="math inline">\(0.337\)</span> with a small standard error. This implies that growing up near a 4-year college increases average educational attainment by <span class="math inline">\(0.3\)</span> years. This seems to be a reasonable magnitude.</p>
<p>Since the structural equation is just-identified with one right-hand-side endogenous variable the ILS/IV estimate for the education coefficient is the ratio of the coefficient estimates for the instrument college in the two equations, e.g.&nbsp;<span class="math inline">\(0.045 / 0.337=0.13\)</span>, implying a <span class="math inline">\(13 %\)</span> return to each year of education. This is substantially greater than the <span class="math inline">\(7 %\)</span> least squares estimate from the first column of Table 12.1. The IV estimates of the full equation are reported in the second column of Table 12.1. One first reaction is surprise that the IV estimate is larger than the OLS estimate. The endogeneity of educational choice should lead to upward bias in the OLS estimator, which predicts that the IV estimate should have been smaller than the OLS estimator. An alternative explanation may be needed. One possibility is heterogeneous education effects (when the education coefficient <span class="math inline">\(\beta\)</span> is heterogenous across individuals). In Section <span class="math inline">\(12.34\)</span> we show that in this context the IV estimator picks up this treatment effect for a subset of the population, and this may explain why IV estimation results in a larger estimated coefficient.</p>
<p>Card (1995) also points out that if education is endogenous then so is our measure of experience as it is calculated by subtracting education from age. He suggests that we can use the variables age and age <span class="math inline">\({ }^{2}\)</span> as instruments for experience and experience <span class="math inline">\({ }^{2}\)</span>. The age variables are exogenous (not choice variables) yet highly correlated with experience and experience <span class="math inline">\({ }^{2}\)</span>. Notice that this approach treats experience <span class="math inline">\({ }^{2}\)</span> as a variable separate from experience. Indeed, this is the correct approach.</p>
<p>Following this recommendation we now have three endogenous regressors and three instruments. We present the three reduced form equations for the three endogenous regressors in the third through fifth columns of Table 12.2. It is interesting to compare the equations for education and experience. The two sets of coefficients are simply the sign change of the other with the exception of the coefficient on age. Indeed this must be the case because the three variables are linearly related. Does this cause a problem for 2SLS? Fortunately, no. The fact that the coefficient on age is not simply a sign change means that the equations are not linearly singular. Hence Assumption (12.22) is not violated.</p>
<p>The IV estimates using the three instruments college, age, and age <span class="math inline">\({ }^{2}\)</span> for the endogenous regressors education, experience, and experience <span class="math inline">\({ }^{2}\)</span> is presented in the third column of Table 12.1. The estimate of the returns to schooling is not affected by this change in the instrument set, but the estimated return to experience profile flattens (the quadratic effect diminishes).</p>
<p>The IV estimator may be calculated in Stata using the ivregress 2 sls command.</p>
</section>
<section id="demeaned-representation" class="level2" data-number="12.10">
<h2 data-number="12.10" class="anchored" data-anchor-id="demeaned-representation"><span class="header-section-number">12.10</span> Demeaned Representation</h2>
<p>Does the well-known demeaned representation for linear regression (3.18) carry over to the IV estimator? To see, write the linear projection equation in the format <span class="math inline">\(Y_{1}=X^{\prime} \beta+\alpha+e\)</span> where <span class="math inline">\(\alpha\)</span> is the intercept and <span class="math inline">\(X\)</span> does not contain a constant. Similarly, partition the instrument as <span class="math inline">\((1, Z)\)</span> where <span class="math inline">\(Z\)</span> does not contain a constant. We can write the IV estimator for the <span class="math inline">\(i^{t h}\)</span> equation as</p>
<p><span class="math display">\[
Y_{1 i}=X_{i}^{\prime} \widehat{\beta}_{\mathrm{iv}}+\widehat{\alpha}_{\mathrm{iv}}+\widehat{e}_{i} .
\]</span></p>
<p>The orthogonality (12.25) implies the two-equation system</p>
<p><span class="math display">\[
\begin{aligned}
&amp;\sum_{i=1}^{n}\left(Y_{1 i}-X_{i}^{\prime} \widehat{\beta}_{\mathrm{iv}}-\widehat{\alpha}_{\mathrm{iv}}\right)=0 \\
&amp;\sum_{i=1}^{n} Z_{i}\left(Y_{1 i}-X_{i}^{\prime} \widehat{\beta}_{\mathrm{iv}}-\widehat{\alpha}_{\mathrm{iv}}\right)=0 .
\end{aligned}
\]</span></p>
<p>The first equation implies <span class="math inline">\(\widehat{\alpha}_{\mathrm{iv}}=\overline{Y_{1}}-\bar{X}^{\prime} \widehat{\beta}_{\mathrm{iv}}\)</span>. Substituting into the second equation</p>
<p><span class="math display">\[
\sum_{i=1}^{n} Z_{i}\left(\left(Y_{1 i}-\overline{Y_{1}}\right)-\left(X_{i}-\bar{X}\right)^{\prime} \widehat{\beta}_{\mathrm{iv}}\right)
\]</span></p>
<p>and solving for <span class="math inline">\(\widehat{\beta}_{\text {iv }}\)</span> we find</p>
<p><span class="math display">\[
\begin{aligned}
\widehat{\beta}_{\mathrm{iv}} &amp;=\left(\sum_{i=1}^{n} Z_{i}\left(X_{i}-\bar{X}\right)^{\prime}\right)^{-1}\left(\sum_{i=1}^{n} Z_{i}\left(Y_{1 i}-\bar{Y}_{1}\right)\right) \\
&amp;=\left(\sum_{i=1}^{n}\left(Z_{i}-\bar{Z}\right)\left(X_{i}-\bar{X}\right)^{\prime}\right)^{-1}\left(\sum_{i=1}^{n}\left(Z_{i}-\bar{Z}\right)\left(Y_{1 i}-\bar{Y}_{1}\right)\right) .
\end{aligned}
\]</span></p>
<p>Thus the demeaning equations for least squares carry over to the IV estimator. The coefficient estimator <span class="math inline">\(\widehat{\beta}_{\text {iv }}\)</span> is a function only of the demeaned data.</p>
</section>
<section id="wald-estimator" class="level2" data-number="12.11">
<h2 data-number="12.11" class="anchored" data-anchor-id="wald-estimator"><span class="header-section-number">12.11</span> Wald Estimator</h2>
<p>In many cases including the Card proximity example the excluded instrument is a binary (dummy) variable. Let’s focus on that case and suppose that the model has just one endogenous regressor and no other regressors beyond the intercept. The model can be written as <span class="math inline">\(Y=X \beta+\alpha+e\)</span> with <span class="math inline">\(\mathbb{E}[e \mid Z]=0\)</span> and <span class="math inline">\(Z\)</span> binary. Take expectations of the structural equation given <span class="math inline">\(Z=1\)</span> and <span class="math inline">\(Z=0\)</span>, respectively. We obtain</p>
<p><span class="math display">\[
\begin{aligned}
&amp;\mathbb{E}[Y \mid Z=1]=\mathbb{E}[X \mid Z=1] \beta+\alpha \\
&amp;\mathbb{E}[Y \mid Z=0]=\mathbb{E}[X \mid Z=0] \beta+\alpha .
\end{aligned}
\]</span></p>
<p>Subtracting and dividing we obtain an expression for the slope coefficient</p>
<p><span class="math display">\[
\beta=\frac{\mathbb{E}[Y \mid Z=1]-\mathbb{E}[Y \mid Z=0]}{\mathbb{E}[X \mid Z=1]-\mathbb{E}[X \mid Z=0]} .
\]</span></p>
<p>The natural moment estimator replaces the expectations by the averages within the “grouped data” where <span class="math inline">\(Z_{i}=1\)</span> and <span class="math inline">\(Z_{i}=0\)</span>, respectively. That is, define the group means</p>
<p><span class="math display">\[
\begin{array}{ll}
\bar{Y}_{1}=\frac{\sum_{i=1}^{n} Z_{i} Y_{i}}{\sum_{i=1}^{n} Z_{i}}, &amp; \bar{Y}_{0}=\frac{\sum_{i=1}^{n}\left(1-Z_{i}\right) Y_{i}}{\sum_{i=1}^{n}\left(1-Z_{i}\right)} \\
\bar{X}_{1}=\frac{\sum_{i=1}^{n} Z_{i} X_{i}}{\sum_{i=1}^{n} Z_{i}}, &amp; \bar{X}_{0}=\frac{\sum_{i=1}^{n}\left(1-Z_{i}\right) X_{i}}{\sum_{i=1}^{n}\left(1-Z_{i}\right)}
\end{array}
\]</span></p>
<p>and the moment estimator</p>
<p><span class="math display">\[
\widehat{\beta}=\frac{\bar{Y}_{1}-\bar{Y}_{0}}{\bar{X}_{1}-\bar{X}_{0}} .
\]</span></p>
<p>This is the “Wald estimator” of Wald (1940).</p>
<p>These expressions are rather insightful. (12.27) shows that the structural slope coefficient is the expected change in <span class="math inline">\(Y\)</span> due to changing the instrument divided by the expected change in <span class="math inline">\(X\)</span> due to changing the instrument. Informally, it is the change in <span class="math inline">\(Y\)</span> (due to <span class="math inline">\(Z\)</span> ) over the change in <span class="math inline">\(X\)</span> (due to <span class="math inline">\(Z\)</span> ). Equation (12.28) shows that the slope coefficient can be estimated by the ratio of differences in means.</p>
<p>The expression (12.28) may appear like a distinct estimator from the IV estimator <span class="math inline">\(\widehat{\beta}_{\text {iv }}\)</span> but it turns out that they are the same. That is, <span class="math inline">\(\widehat{\beta}=\widehat{\beta}_{\mathrm{iv}}\)</span>. To see this, use (12.26) to find</p>
<p><span class="math display">\[
\widehat{\beta}_{\mathrm{iv}}=\frac{\sum_{i=1}^{n} Z_{i}\left(Y_{i}-\bar{Y}\right)}{\sum_{i=1}^{n} Z_{i}\left(X_{i}-\bar{X}\right)}=\frac{\bar{Y}_{1}-\bar{Y}}{\bar{X}_{1}-\bar{X}} .
\]</span></p>
<p>Then notice</p>
<p><span class="math display">\[
\bar{Y}_{1}-\bar{Y}=\bar{Y}_{1}-\left(\frac{1}{n} \sum_{i=1}^{n} Z_{i} \bar{Y}_{1}+\frac{1}{n} \sum_{i=1}^{n}\left(1-Z_{i}\right) \bar{Y}_{0}\right)=(1-\bar{Z})\left(\bar{Y}_{1}-\bar{Y}_{0}\right)
\]</span></p>
<p>and similarly</p>
<p><span class="math display">\[
\bar{X}_{1}-\bar{X}=(1-\bar{Z})\left(\bar{X}_{1}-\bar{X}_{0}\right)
\]</span></p>
<p>and hence</p>
<p><span class="math display">\[
\widehat{\beta}_{\mathrm{iv}}=\frac{(1-\bar{Z})\left(\bar{Y}_{1}-\bar{Y}_{0}\right)}{(1-\bar{Z})\left(\bar{X}_{1}-\bar{X}_{0}\right)}=\widehat{\beta}
\]</span></p>
<p>as defined in (12.28). Thus the Wald estimator equals the IV estimator.</p>
<p>We can illustrate using the Card proximity example. If we estimate a simple IV model with no covariates we obtain the estimate <span class="math inline">\(\widehat{\beta}_{\text {iv }}=0.19\)</span>. If we estimate the group-mean of log wages and education based on the instrument college we find</p>
<table class="table">
<thead>
<tr class="header">
<th style="text-align: left;"></th>
<th>near college</th>
<th>not near college</th>
<th>difference</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><span class="math inline">\(\log (\)</span> wage)</td>
<td><span class="math inline">\(6.311\)</span></td>
<td><span class="math inline">\(6.156\)</span></td>
<td><span class="math inline">\(0.155\)</span></td>
</tr>
<tr class="even">
<td style="text-align: left;">education</td>
<td><span class="math inline">\(13.527\)</span></td>
<td><span class="math inline">\(12.698\)</span></td>
<td><span class="math inline">\(0.829\)</span></td>
</tr>
<tr class="odd">
<td style="text-align: left;">ratio</td>
<td></td>
<td></td>
<td><span class="math inline">\(0.19\)</span></td>
</tr>
</tbody>
</table>
<p>Based on these estimates the Wald estimator of the slope coefficient is <span class="math inline">\((6.311-6.156) /(13.527-12.698)=\)</span> <span class="math inline">\(0.155 / 0.829=0.19\)</span>, the same as the IV estimator.</p>
</section>
<section id="two-stage-least-squares" class="level2" data-number="12.12">
<h2 data-number="12.12" class="anchored" data-anchor-id="two-stage-least-squares"><span class="header-section-number">12.12</span> Two-Stage Least Squares</h2>
<p>The IV estimator described in the previous section presumed <span class="math inline">\(\ell=k\)</span>. Now we allow the general case of <span class="math inline">\(\ell \geq k\)</span>. Examining the reduced-form equation (12.13) we see</p>
<p><span class="math display">\[
\begin{aligned}
Y_{1} &amp;=Z^{\prime} \bar{\Gamma} \beta+u_{1} \\
\mathbb{E}\left[Z u_{1}\right] &amp;=0 .
\end{aligned}
\]</span></p>
<p>Defining <span class="math inline">\(W=\bar{\Gamma}^{\prime} Z\)</span> we can write this as</p>
<p><span class="math display">\[
\begin{aligned}
Y_{1} &amp;=W^{\prime} \beta+u_{1} \\
\mathbb{E}\left[W u_{1}\right] &amp;=0 .
\end{aligned}
\]</span></p>
<p>One way of thinking about this is that <span class="math inline">\(Z\)</span> is set of candidate instruments. The instrument vector <span class="math inline">\(W=\bar{\Gamma}^{\prime} Z\)</span> is a <span class="math inline">\(k\)</span>-dimentional set of linear combinations.</p>
<p>Suppose that <span class="math inline">\(\bar{\Gamma}\)</span> were known. Then we would estimate <span class="math inline">\(\beta\)</span> by least squares of <span class="math inline">\(Y_{1}\)</span> on <span class="math inline">\(W=\bar{\Gamma}^{\prime} Z\)</span></p>
<p><span class="math display">\[
\widehat{\beta}=\left(\boldsymbol{W}^{\prime} \boldsymbol{W}\right)^{-1}\left(\boldsymbol{W}^{\prime} \boldsymbol{Y}\right)=\left(\bar{\Gamma}^{\prime} \boldsymbol{Z}^{\prime} \boldsymbol{Z} \bar{\Gamma}\right)^{-1}\left(\bar{\Gamma}^{\prime} \boldsymbol{Z}^{\prime} \boldsymbol{Y}_{1}\right) .
\]</span></p>
<p>While this is infeasible we can estimate <span class="math inline">\(\bar{\Gamma}\)</span> from the reduced form regression. Replacing <span class="math inline">\(\bar{\Gamma}\)</span> with its estimator <span class="math inline">\(\widehat{\Gamma}=\left(\boldsymbol{Z}^{\prime} \boldsymbol{Z}\right)^{-1}\left(\boldsymbol{Z}^{\prime} \boldsymbol{X}\right)\)</span> we obtain</p>
<p><span class="math display">\[
\begin{aligned}
\widehat{\beta}_{2 \text { sls }} &amp;=\left(\widehat{\Gamma}^{\prime} \boldsymbol{Z}^{\prime} \boldsymbol{Z} \widehat{\Gamma}\right)^{-1}\left(\widehat{\Gamma}^{\prime} \boldsymbol{Z}^{\prime} \boldsymbol{Y}_{1}\right) \\
&amp;=\left(\boldsymbol{X}^{\prime} \boldsymbol{Z}\left(\boldsymbol{Z}^{\prime} \boldsymbol{Z}\right)^{-1} \boldsymbol{Z}^{\prime} \boldsymbol{Z}\left(\boldsymbol{Z}^{\prime} \boldsymbol{Z}\right)^{-\mathbf{1}} \boldsymbol{Z}^{\prime} \boldsymbol{X}\right)^{-1} \boldsymbol{X}^{\prime} \boldsymbol{Z}\left(\boldsymbol{Z}^{\prime} \boldsymbol{Z}\right)^{-1} \boldsymbol{Z}^{\prime} \boldsymbol{Y}_{1} \\
&amp;=\left(\boldsymbol{X}^{\prime} \boldsymbol{Z}\left(\boldsymbol{Z}^{\prime} \boldsymbol{Z}\right)^{-1} \boldsymbol{Z}^{\prime} \boldsymbol{X}\right)^{-1} \boldsymbol{X}^{\prime} \boldsymbol{Z}\left(\boldsymbol{Z}^{\prime} \boldsymbol{Z}\right)^{-1} \boldsymbol{Z}^{\prime} \boldsymbol{Y}_{1}
\end{aligned}
\]</span></p>
<p>This is called the two-stage-least squares (2SLS) estimator. It was originally proposed by Theil (1953) and Basmann (1957) and is a standard estimator for linear equations with instruments.</p>
<p>If the model is just-identified, so that <span class="math inline">\(k=\ell\)</span>, then 2SLS simplifies to the IV estimator of the previous section. Since the matrices <span class="math inline">\(\boldsymbol{X}^{\prime} \boldsymbol{Z}\)</span> and <span class="math inline">\(\boldsymbol{Z}^{\prime} \boldsymbol{X}\)</span> are square we can factor</p>
<p><span class="math display">\[
\begin{aligned}
\left(\boldsymbol{X}^{\prime} \boldsymbol{Z}\left(\boldsymbol{Z}^{\prime} \boldsymbol{Z}\right)^{-1} \boldsymbol{Z}^{\prime} \boldsymbol{X}\right)^{-1} &amp;=\left(\boldsymbol{Z}^{\prime} \boldsymbol{X}\right)^{-1}\left(\left(\boldsymbol{Z}^{\prime} \boldsymbol{Z}\right)^{-1}\right)^{-1}\left(\boldsymbol{X}^{\prime} \boldsymbol{Z}\right)^{-1} \\
&amp;=\left(\boldsymbol{Z}^{\prime} \boldsymbol{X}\right)^{-1}\left(\boldsymbol{Z}^{\prime} \boldsymbol{Z}\right)\left(\boldsymbol{X}^{\prime} \boldsymbol{Z}\right)^{-1}
\end{aligned}
\]</span></p>
<p>(Once again, this only works when <span class="math inline">\(k=\ell\)</span>.) Then</p>
<p><span class="math display">\[
\begin{aligned}
\widehat{\beta}_{2 \text { sls }} &amp;=\left(\boldsymbol{X}^{\prime} \boldsymbol{Z}\left(\boldsymbol{Z}^{\prime} \boldsymbol{Z}\right)^{-1} \boldsymbol{Z}^{\prime} \boldsymbol{X}\right)^{-1} \boldsymbol{X}^{\prime} \boldsymbol{Z}\left(\boldsymbol{Z}^{\prime} \boldsymbol{Z}\right)^{-1} \boldsymbol{Z}^{\prime} \boldsymbol{Y}_{1} \\
&amp;=\left(\boldsymbol{Z}^{\prime} \boldsymbol{X}\right)^{-1}\left(\boldsymbol{Z}^{\prime} \boldsymbol{Z}\right)\left(\boldsymbol{X}^{\prime} \boldsymbol{Z}\right)^{-1} \boldsymbol{X}^{\prime} \boldsymbol{Z}\left(\boldsymbol{Z}^{\prime} \boldsymbol{Z}\right)^{-1} \boldsymbol{Z}^{\prime} \boldsymbol{Y}_{1} \\
&amp;=\left(\boldsymbol{Z}^{\prime} \boldsymbol{X}\right)^{-1}\left(\boldsymbol{Z}^{\prime} \boldsymbol{Z}\right)\left(\boldsymbol{Z}^{\prime} \boldsymbol{Z}\right)^{-1} \boldsymbol{Z}^{\prime} \boldsymbol{Y}_{1} \\
&amp;=\left(\boldsymbol{Z}^{\prime} \boldsymbol{X}\right)^{-1} \boldsymbol{Z}^{\prime} \boldsymbol{Y}_{1}=\widehat{\beta}_{\mathrm{iv}}
\end{aligned}
\]</span></p>
<p>as claimed. This shows that the 2SLS estimator as defined in (12.29) is a generalization of the IV estimator defined in (12.24).</p>
<p>There are several alternative representations of the 2SLS estimator which we now describe. First, defining the projection matrix</p>
<p><span class="math display">\[
\boldsymbol{P}_{\boldsymbol{Z}}=\boldsymbol{Z}\left(\boldsymbol{Z}^{\prime} \boldsymbol{Z}\right)^{-1} \boldsymbol{Z}^{\prime}
\]</span></p>
<p>we can write the 2SLS estimator more compactly as</p>
<p><span class="math display">\[
\widehat{\beta}_{2 \text { sls }}=\left(\boldsymbol{X}^{\prime} \boldsymbol{P}_{\boldsymbol{Z}} \boldsymbol{X}\right)^{-1} \boldsymbol{X}^{\prime} \boldsymbol{P}_{\boldsymbol{Z}} \boldsymbol{Y}_{1} .
\]</span></p>
<p>This is useful for representation and derivations but is not useful for computation as the <span class="math inline">\(n \times n\)</span> matrix <span class="math inline">\(\boldsymbol{P}_{\boldsymbol{Z}}\)</span> is too large to compute when <span class="math inline">\(n\)</span> is large.</p>
<p>Second, define the fitted values for <span class="math inline">\(\boldsymbol{X}\)</span> from the reduced form <span class="math inline">\(\widehat{\boldsymbol{X}}=\boldsymbol{P}_{\boldsymbol{Z}} \boldsymbol{X}=\boldsymbol{Z} \widehat{\Gamma}\)</span>. Then the 2SLS estimator can be written as</p>
<p><span class="math display">\[
\widehat{\beta}_{2 \text { sls }}=\left(\widehat{\boldsymbol{X}}^{\prime} \boldsymbol{X}\right)^{-1} \widehat{\boldsymbol{X}}^{\prime} \boldsymbol{Y}_{1}
\]</span></p>
<p>This is an IV estimator as defined in the previous section using <span class="math inline">\(\widehat{X}\)</span> as an instrument for <span class="math inline">\(X\)</span>.</p>
<p>Third, because <span class="math inline">\(\boldsymbol{P}_{Z}\)</span> is idempotent we can also write the 2SLS estimator as</p>
<p><span class="math display">\[
\widehat{\beta}_{2 \text { sls }}=\left(\boldsymbol{X}^{\prime} \boldsymbol{P}_{\boldsymbol{Z}} \boldsymbol{P}_{\boldsymbol{Z}} \boldsymbol{X}\right)^{-1} \boldsymbol{X}^{\prime} \boldsymbol{P}_{\boldsymbol{Z}} \boldsymbol{Y}_{1}=\left(\widehat{\boldsymbol{X}}^{\prime} \widehat{\boldsymbol{X}}\right)^{-1} \widehat{\boldsymbol{X}}^{\prime} \boldsymbol{Y}_{1}
\]</span></p>
<p>which is the least squares estimator obtained by regressing <span class="math inline">\(Y_{1}\)</span> on the fitted values <span class="math inline">\(\widehat{X}\)</span>.</p>
<p>This is the source of the “two-stage” name as it can be computed as follows.</p>
<ul>
<li><p>Regress <span class="math inline">\(X\)</span> on <span class="math inline">\(Z\)</span> to obtain the fitted <span class="math inline">\(\widehat{X}: \widehat{\Gamma}=\left(\boldsymbol{Z}^{\prime} \boldsymbol{Z}\right)^{-1}\left(\boldsymbol{Z}^{\prime} \boldsymbol{X}\right)\)</span> and <span class="math inline">\(\widehat{\boldsymbol{X}}=\boldsymbol{Z} \widehat{\Gamma}=\boldsymbol{P}_{\boldsymbol{Z}} \boldsymbol{X}\)</span>.</p></li>
<li><p>Regress <span class="math inline">\(Y_{1}\)</span> on <span class="math inline">\(\widehat{X}: \widehat{\beta}_{2 s l s}=\left(\widehat{\boldsymbol{X}}^{\prime} \widehat{\boldsymbol{X}}\right)^{-1} \widehat{\boldsymbol{X}}^{\prime} \boldsymbol{Y}_{1}\)</span></p></li>
</ul>
<p>It is useful to scrutinize the projection <span class="math inline">\(\widehat{\boldsymbol{X}}\)</span>. Recall, <span class="math inline">\(\boldsymbol{X}=\left[\boldsymbol{Z}_{1}, \boldsymbol{Y}_{2}\right]\)</span> and <span class="math inline">\(\boldsymbol{Z}=\left[\boldsymbol{Z}_{1}, \boldsymbol{Z}_{2}\right]\)</span>. Notice <span class="math inline">\(\widehat{\boldsymbol{X}}_{1}=\boldsymbol{P}_{\boldsymbol{Z}} \boldsymbol{Z}_{1}=\)</span> <span class="math inline">\(Z_{1}\)</span> because <span class="math inline">\(Z_{1}\)</span> lies in the span of <span class="math inline">\(\boldsymbol{Z}\)</span>. Then <span class="math inline">\(\widehat{\boldsymbol{X}}=\left[\widehat{\boldsymbol{X}}_{1}, \widehat{\boldsymbol{Y}}_{2}\right]=\left[\boldsymbol{Z}_{1}, \widehat{\boldsymbol{Y}}_{2}\right]\)</span>. This shows that in the second stage we regress <span class="math inline">\(Y_{1}\)</span> on <span class="math inline">\(Z_{1}\)</span> and <span class="math inline">\(\widehat{Y}_{2}\)</span>. This means that only the endogenous variables <span class="math inline">\(Y_{2}\)</span> are replaced by their fitted values, <span class="math inline">\(\widehat{Y}_{2}=\widehat{\Gamma}_{12}^{\prime} Z_{1}+\widehat{\Gamma}_{22}^{\prime} Z_{2}\)</span>.</p>
<p>A fourth representation of 2SLS can be obtained using the FWL Theorem. The third representation and following discussion showed that 2SLS is obtained as least squares of <span class="math inline">\(Y_{1}\)</span> on the fitted values <span class="math inline">\(\left(Z_{1}, \widehat{Y}_{2}\right)\)</span>. Hence the coefficient <span class="math inline">\(\widehat{\beta}_{2}\)</span> on the endogenous variables can be found by residual regression. Set <span class="math inline">\(\boldsymbol{P}_{1}=\)</span> <span class="math inline">\(Z_{1}\left(Z_{1}^{\prime} Z_{1}\right)^{-1} Z_{1}^{\prime}\)</span>. Applying the FWL theorem we obtain</p>
<p><span class="math display">\[
\begin{aligned}
\widehat{\beta}_{2} &amp;=\left(\widehat{\boldsymbol{Y}}_{2}^{\prime}\left(\boldsymbol{I}_{n}-\boldsymbol{P}_{1}\right) \widehat{\boldsymbol{Y}}_{2}\right)^{-1}\left(\widehat{\boldsymbol{Y}}_{2}^{\prime}\left(\boldsymbol{I}_{n}-\boldsymbol{P}_{1}\right) \boldsymbol{Y}_{1}\right) \\
&amp;=\left(\boldsymbol{Y}_{2}^{\prime} \boldsymbol{P}_{\boldsymbol{Z}}\left(\boldsymbol{I}_{n}-\boldsymbol{P}_{1}\right) \boldsymbol{P}_{\boldsymbol{Z}} \boldsymbol{Y}_{2}\right)^{-1}\left(\boldsymbol{Y}_{2}^{\prime} \boldsymbol{P}_{\boldsymbol{Z}}\left(\boldsymbol{I}_{n}-\boldsymbol{P}_{1}\right) \boldsymbol{Y}_{1}\right) \\
&amp;=\left(\boldsymbol{Y}_{2}^{\prime}\left(\boldsymbol{P}_{\boldsymbol{Z}}-\boldsymbol{P}_{1}\right) \boldsymbol{Y}_{2}\right)^{-1}\left(\boldsymbol{Y}_{2}^{\prime}\left(\boldsymbol{P}_{\boldsymbol{Z}}-\boldsymbol{P}_{1}\right) \boldsymbol{Y}_{1}\right)
\end{aligned}
\]</span></p>
<p>because <span class="math inline">\(\boldsymbol{P}_{Z} \boldsymbol{P}_{1}=\boldsymbol{P}_{1}\)</span>.</p>
<p>A fifth representation can be obtained by a further projection. The projection matrix <span class="math inline">\(\boldsymbol{P}_{\boldsymbol{Z}}\)</span> can be replaced by the projection onto the pair <span class="math inline">\(\left[\boldsymbol{Z}_{1}, \widetilde{\boldsymbol{Z}}_{2}\right.\)</span> ] where <span class="math inline">\(\widetilde{\boldsymbol{Z}}_{2}=\left(\boldsymbol{I}_{n}-\boldsymbol{P}_{1}\right) \boldsymbol{Z}_{2}\)</span> is <span class="math inline">\(\boldsymbol{Z}_{2}\)</span> projected orthogonal to <span class="math inline">\(\boldsymbol{Z}_{1}\)</span>. Since <span class="math inline">\(\boldsymbol{Z}_{1}\)</span> and <span class="math inline">\(\widetilde{\boldsymbol{Z}}_{2}\)</span> are orthogonal, <span class="math inline">\(\boldsymbol{P}_{\boldsymbol{Z}}=\boldsymbol{P}_{1}+\boldsymbol{P}_{2}\)</span> where <span class="math inline">\(\boldsymbol{P}_{2}=\widetilde{\boldsymbol{Z}}_{2}\left(\widetilde{\boldsymbol{Z}}_{2}^{\prime} \widetilde{\boldsymbol{Z}}_{2}\right)^{-1} \widetilde{\boldsymbol{Z}}_{2}^{\prime}\)</span>. Thus <span class="math inline">\(\boldsymbol{P}_{\boldsymbol{Z}}-\boldsymbol{P}_{1}=\boldsymbol{P}_{2}\)</span> and</p>
<p><span class="math display">\[
\begin{aligned}
\widehat{\beta}_{2} &amp;=\left(\boldsymbol{Y}_{2}^{\prime} \boldsymbol{P}_{2} \boldsymbol{Y}_{2}\right)^{-1}\left(\boldsymbol{Y}_{2}^{\prime} \boldsymbol{P}_{2} \boldsymbol{Y}_{1}\right) \\
&amp;=\left(\boldsymbol{Y}_{2}^{\prime} \widetilde{\boldsymbol{Z}}_{2}\left(\widetilde{\boldsymbol{Z}}_{2}^{\prime} \widetilde{\boldsymbol{Z}}_{2}\right)^{-1} \widetilde{\boldsymbol{Z}}_{2}^{\prime} \boldsymbol{Y}_{2}\right)^{-1}\left(\boldsymbol{Y}_{2}^{\prime} \widetilde{\boldsymbol{Z}}_{2}\left(\widetilde{\boldsymbol{Z}}_{2}^{\prime} \widetilde{\boldsymbol{Z}}_{2}\right)^{-1} \widetilde{\boldsymbol{Z}}_{2}^{\prime} \boldsymbol{Y}_{1}\right) .
\end{aligned}
\]</span></p>
<p>Given the 2SLS estimator we define the residual <span class="math inline">\(\widehat{e}_{i}=Y_{1 i}-X_{i}^{\prime} \widehat{\beta}_{2 s l s}\)</span>. When the model is overidentified the instruments and residuals are not orthogonal. That is, <span class="math inline">\(\boldsymbol{Z}^{\prime} \widehat{\boldsymbol{e}} \neq 0\)</span>. It does, however, satisfy</p>
<p><span class="math display">\[
\begin{aligned}
\widehat{\boldsymbol{X}}^{\prime} \widehat{\boldsymbol{e}} &amp;=\widehat{\boldsymbol{\Gamma}}^{\prime} \boldsymbol{Z}^{\prime} \widehat{\boldsymbol{e}} \\
&amp;=\boldsymbol{X}^{\prime} \boldsymbol{Z}\left(\boldsymbol{Z}^{\prime} \boldsymbol{Z}\right)^{-1} \boldsymbol{Z}^{\prime} \widehat{\boldsymbol{e}} \\
&amp;=\boldsymbol{X}^{\prime} \boldsymbol{Z}\left(\boldsymbol{Z}^{\prime} \boldsymbol{Z}\right)^{-1} \boldsymbol{Z}^{\prime} \boldsymbol{Y}_{1}-\boldsymbol{X}^{\prime} \boldsymbol{Z}\left(\boldsymbol{Z}^{\prime} \boldsymbol{Z}\right)^{-1} \boldsymbol{Z}^{\prime} \boldsymbol{X} \widehat{\beta}_{2 \text { sls }}=0 .
\end{aligned}
\]</span></p>
<p>Returning to Card’s college proximity example suppose that we treat experience as exogeneous but that instead of using the single instrument college (grew up near a 4-year college) we use the two instruments (public, private) (grew up near a public/private 4-year college, respectively). In this case we have one endogenous variable (education) and two instruments (public, private). The estimated reduced form equation for education is presented in the sixth column of Table 12.2. In this specification the coefficient on public - growing up near a public 4-year college - is somewhat larger than that found for the variable college in the previous specification (column 2). Furthermore, the coefficient on private-growing up near a private 4-year college - is much smaller. This indicates that the key impact of proximity on education is via public colleges rather than private colleges.</p>
<p>The 2SLS estimates obtained using these two instruments are presented in the fourth column of Table 12.1. The coefficient on education increases to <span class="math inline">\(0.161\)</span>, indicating a <span class="math inline">\(16 %\)</span> return to a year of education. This is roughly twice as large as the estimate obtained by least squares in the first column.</p>
<p>Additionally, if we follow Card and treat experience as endogenous and use age as an instrument we now have three endogenous variables (education, experience, experience <span class="math inline">\({ }^{2} / 100\)</span> ) and four instruments (public, private, age, <span class="math inline">\(a g e^{2}\)</span> ). We present the 2SLS estimates using this specification in the fifth column of Table 12.1. The estimate of the return to education remains <span class="math inline">\(16 %\)</span> and the return to experience flattens.</p>
<p>You might wonder if we could use all three instruments - college, public, and private. The answer is no. This is because college <span class="math inline">\(=\)</span> public <span class="math inline">\(+\)</span> private so the three variables are colinear. Since the instruments are linearly related the three together would violate the full-rank condition (12.6).</p>
<p>The 2SLS estimator may be calculated in Stata using the ivregress 2 sls command.</p>
</section>
<section id="limited-information-maximum-likelihood" class="level2" data-number="12.13">
<h2 data-number="12.13" class="anchored" data-anchor-id="limited-information-maximum-likelihood"><span class="header-section-number">12.13</span> Limited Information Maximum Likelihood</h2>
<p>An alternative method to estimate the parameters of the structural equation is by maximum likelihood. Anderson and Rubin (1949) derived the maximum likelihood estimator for the joint distribution of <span class="math inline">\(\vec{Y}=\left(Y_{1}, Y_{2}\right)\)</span>. The estimator is known as limited information maximum likelihood (LIML).</p>
<p>This estimator is called “limited information” because it is based on the structural equation for <span class="math inline">\(Y\)</span> combined with the reduced form equation for <span class="math inline">\(X_{2}\)</span>. If maximum likelihood is derived based on a structural equation for <span class="math inline">\(X_{2}\)</span> as well this leads to what is known as full information maximum likelihood (FIML). The advantage of LIML relative to FIML is that the former does not require a structural model for <span class="math inline">\(X_{2}\)</span> and thus allows the researcher to focus on the structural equation of interest - that for <span class="math inline">\(Y\)</span>. We do not describe the FIML estimator as it is not commonly used in applied econometrics.</p>
<p>While the LIML estimator is less widely used among economists than 2SLS it has received a resurgence of attention from econometric theorists.</p>
<p>To derive the LIML estimator recall the definition <span class="math inline">\(\vec{Y}=\left(Y_{1}, Y_{2}\right)\)</span> and the reduced form (12.17)</p>
<p><span class="math display">\[
\begin{aligned}
\vec{Y} &amp;=\left[\begin{array}{cc}
\lambda_{1}^{\prime} &amp; \lambda_{2} \\
\Gamma_{12}^{\prime} &amp; \Gamma_{22}^{\prime}
\end{array}\right]\left(\begin{array}{l}
Z_{1} \\
Z_{2}
\end{array}\right)+u \\
&amp;=\Pi_{1}^{\prime} Z_{1}+\Pi_{2}^{\prime} Z_{2}+u
\end{aligned}
\]</span></p>
<p>where <span class="math inline">\(\Pi_{1}=\left[\begin{array}{cc}\lambda_{1} &amp; \Gamma_{12}\end{array}\right]\)</span> and <span class="math inline">\(\Pi_{2}=\left[\begin{array}{cc}\lambda_{2} &amp; \Gamma_{22}\end{array}\right]\)</span>. The LIML estimator is derived under the assumption that <span class="math inline">\(u\)</span> is multivariate normal.</p>
<p>Define <span class="math inline">\(\gamma^{\prime}=\left[\begin{array}{ll}1 &amp; -\beta_{2}^{\prime}\end{array}\right]\)</span>. From (12.15) we find</p>
<p><span class="math display">\[
\Pi_{2} \gamma=\lambda_{2}-\Gamma_{22} \beta_{2}=0 .
\]</span></p>
<p>Thus the <span class="math inline">\(\ell_{2} \times\left(k_{2}+1\right)\)</span> coefficient matrix <span class="math inline">\(\Pi_{2}\)</span> in (12.33) has deficient rank. Indeed, its rank must be <span class="math inline">\(k_{2}\)</span> because <span class="math inline">\(\Gamma_{22}\)</span> has full rank.</p>
<p>This means that the model (12.33) is precisely the reduced rank regression model of Section <span class="math inline">\(11.11 .\)</span> Theorem <span class="math inline">\(11.7\)</span> presents the maximum likelihood estimators for the reduced rank parameters. In particular, the MLE for <span class="math inline">\(\gamma\)</span> is</p>
<p><span class="math display">\[
\widehat{\gamma}=\underset{\gamma}{\operatorname{argmin}} \frac{\gamma^{\prime} \overrightarrow{\boldsymbol{Y}}^{\prime} \boldsymbol{M}_{1} \overrightarrow{\boldsymbol{Y}} \gamma}{\gamma^{\prime} \overrightarrow{\boldsymbol{Y}}^{\prime} \boldsymbol{M}_{\boldsymbol{Z}} \overrightarrow{\boldsymbol{Y}} \gamma}
\]</span></p>
<p>where <span class="math inline">\(\boldsymbol{M}_{1}=\boldsymbol{I}_{n}-\boldsymbol{Z}_{1}\left(\boldsymbol{Z}_{1}^{\prime} \boldsymbol{Z}_{1}\right)^{-1} \boldsymbol{Z}_{1}^{\prime}\)</span> and <span class="math inline">\(\boldsymbol{M}_{\boldsymbol{Z}}=\boldsymbol{I}_{n}-\boldsymbol{Z}\left(\boldsymbol{Z}^{\prime} \boldsymbol{Z}\right)^{-1} \boldsymbol{Z}^{\prime}\)</span>. The minimization (12.34) is sometimes called the “least variance ratio” problem.</p>
<p>The minimization problem (12.34) is invariant to the scale of <span class="math inline">\(\gamma\)</span> (that is, <span class="math inline">\(\widehat{\gamma} c\)</span> is equivalently the argmin for any c) so normalization is required. A convenient choice is <span class="math inline">\(\gamma^{\prime} \overrightarrow{\boldsymbol{Y}}^{\prime} \boldsymbol{M}_{Z} \overrightarrow{\boldsymbol{Y}} \gamma=1\)</span>. Using this normalization and the theory of the minimum of quadratic forms (Section A.15) <span class="math inline">\(\widehat{\gamma}\)</span> is the generalized eigenvector of <span class="math inline">\(\overrightarrow{\boldsymbol{Y}}^{\prime} \boldsymbol{M}_{1} \overrightarrow{\boldsymbol{Y}}\)</span> with respect to <span class="math inline">\(\overrightarrow{\boldsymbol{Y}}^{\prime} \boldsymbol{M}_{Z} \overrightarrow{\boldsymbol{Y}}\)</span> associated with the smallest generalized eigenvalue. (See Section A.14 for the definition of generalized eigenvalues and eigenvectors.) Computationally this is straightforward. For example, in MATLAB the generalized eigenvalues and eigenvectors of the matrix <span class="math inline">\(\boldsymbol{A}\)</span> with respect to <span class="math inline">\(\boldsymbol{B}\)</span> is found by the command eig <span class="math inline">\((\boldsymbol{A}, \boldsymbol{B})\)</span>. Once this <span class="math inline">\(\widehat{\gamma}\)</span> is found any other normalization can be obtained by rescaling. For example, to obtain the MLE for <span class="math inline">\(\beta_{2}\)</span> make the partition <span class="math inline">\(\widehat{\gamma}^{\prime}=\left[\begin{array}{cc}\widehat{\gamma}_{1} &amp; \widehat{\gamma}_{2}^{\prime}\end{array}\right]\)</span> and set <span class="math inline">\(\widehat{\beta}_{2}=-\widehat{\gamma}_{2} / \widehat{\gamma}_{1}\)</span>.</p>
<p>To obtain the MLE for <span class="math inline">\(\beta_{1}\)</span> recall the structural equation <span class="math inline">\(Y_{1}=Z_{1}^{\prime} \beta_{1}+Y_{2}^{\prime} \beta_{2}+e\)</span>. Replace <span class="math inline">\(\beta_{2}\)</span> with the MLE <span class="math inline">\(\widehat{\beta}_{2}\)</span> and apply regression. This yields</p>
<p><span class="math display">\[
\widehat{\beta}_{1}=\left(Z_{1}^{\prime} Z_{1}\right)^{-1} Z_{1}^{\prime}\left(Y_{1}-Y_{2} \widehat{\beta}_{2}\right) .
\]</span></p>
<p>These solutions are the MLE for the structural parameters <span class="math inline">\(\beta_{1}\)</span> and <span class="math inline">\(\beta_{2}\)</span>.</p>
<p>Previous econometrics textbooks did not present a derivation of the LIML estimator as the original derivation by Anderson and Rubin (1949) is lengthy and not particularly insightful. In contrast the derivation given here based on reduced rank regression is simple.</p>
<p>There is an alternative (and traditional) expression for the LIML estimator. Define the minimum obtained in (12.34)</p>
<p><span class="math display">\[
\widehat{\boldsymbol{\kappa}}=\min _{\gamma} \frac{\gamma^{\prime} \overrightarrow{\boldsymbol{Y}}^{\prime} \boldsymbol{M}_{1} \overrightarrow{\boldsymbol{Y}} \gamma}{\gamma^{\prime} \overrightarrow{\boldsymbol{Y}}^{\prime} \boldsymbol{M}_{\boldsymbol{Z}} \overrightarrow{\boldsymbol{Y}} \gamma}
\]</span></p>
<p>which is the smallest generalized eigenvalue of <span class="math inline">\(\overrightarrow{\boldsymbol{Y}}^{\prime} \boldsymbol{M}_{1} \overrightarrow{\boldsymbol{Y}}\)</span> with respect to <span class="math inline">\(\overrightarrow{\boldsymbol{Y}}^{\prime} \boldsymbol{M}_{\boldsymbol{Z}} \overrightarrow{\boldsymbol{Y}}\)</span>. The LIML estimator can be written as</p>
<p><span class="math display">\[
\widehat{\beta}_{\text {liml }}=\left(\boldsymbol{X}^{\prime}\left(\boldsymbol{I}_{n}-\widehat{\kappa} \boldsymbol{M}_{\boldsymbol{Z}}\right) \boldsymbol{X}\right)^{-1}\left(\boldsymbol{X}^{\prime}\left(\boldsymbol{I}_{n}-\widehat{\kappa} \boldsymbol{M}_{\boldsymbol{Z}}\right) \boldsymbol{Y}_{1}\right) .
\]</span></p>
<p>We defer the derivation of (12.37) until the end of this section. Expression (12.37) does not simplify computation (because <span class="math inline">\(\widehat{\kappa}\)</span> requires solving the same eigenvector problem that yields <span class="math inline">\(\widehat{\beta}_{2}\)</span> ). However (12.37) is important for the distribution theory. It also helps reveal the algebraic connection between LIML, least squares, and 2SLS.</p>
<p>The estimator (12.37) with arbitrary <span class="math inline">\(\kappa\)</span> is known as a k-class estimator of <span class="math inline">\(\beta\)</span>. While the LIML estimator obtains by setting <span class="math inline">\(\kappa=\widehat{\kappa}\)</span>, the least squares estimator is obtained by setting <span class="math inline">\(\kappa=0\)</span> and 2SLS is obtained by setting <span class="math inline">\(\kappa=1\)</span>. It is worth observing that the LIML solution satisfies <span class="math inline">\(\widehat{\kappa} \geq 1\)</span>. When the model is just-identified the LIML estimator is identical to the IV and 2SLS estimators. They are only different in the over-identified setting. (One corollary is that under just-identification and normal errors the IV estimator is MLE.)</p>
<p>For inference it is useful to observe that (12.37) shows that <span class="math inline">\(\widehat{\beta}_{\mathrm{liml}}\)</span> can be written as an IV estimator</p>
<p><span class="math display">\[
\widehat{\beta}_{\mathrm{liml}}=\left(\widetilde{\boldsymbol{X}}^{\prime} \boldsymbol{X}\right)^{-1}\left(\widetilde{\boldsymbol{X}}^{\prime} \boldsymbol{Y}_{1}\right)
\]</span></p>
<p>using the instrument</p>
<p><span class="math display">\[
\widetilde{\boldsymbol{X}}=\left(\boldsymbol{I}_{n}-\widehat{\kappa} \boldsymbol{M}_{\boldsymbol{Z}}\right) \boldsymbol{X}=\left(\begin{array}{c}
\boldsymbol{X}_{1} \\
\boldsymbol{X}_{2}-\widehat{\kappa} \widehat{\boldsymbol{U}}_{2}
\end{array}\right)
\]</span></p>
<p>where <span class="math inline">\(\widehat{\boldsymbol{U}}_{2}=\boldsymbol{M}_{\boldsymbol{Z}} \boldsymbol{X}_{2}\)</span> are the reduced-form residuals from the multivariate regression of the endogenous regressors <span class="math inline">\(Y_{2}\)</span> on the instruments <span class="math inline">\(Z\)</span>. Expressing LIML using this IV formula is useful for variance estimation.</p>
<p>The LIML estimator has the same asymptotic distribution as 2SLS. However, they have quite different behaviors in finite samples. There is considerable evidence that the LIML estimator has reduced finite sample bias relative to 2 SLS when there are many instruments or the reduced form is weak. (We review these cases in the following sections.) However, on the other hand LIML has wider finite sample dispersion.</p>
<p>We now derive the expression (12.37). Use the normalization <span class="math inline">\(\gamma^{\prime}=\left[\begin{array}{ll}1 &amp; -\beta_{2}^{\prime}\end{array}\right]\)</span> to write (12.34) as</p>
<p><span class="math display">\[
\widehat{\beta}_{2}=\underset{\beta_{2}}{\operatorname{argmin}} \frac{\left(\boldsymbol{Y}_{1}-\boldsymbol{Y}_{2} \beta_{2}\right)^{\prime} \boldsymbol{M}_{1}\left(\boldsymbol{Y}_{1}-\boldsymbol{Y}_{2} \beta_{2}\right)}{\left(\boldsymbol{Y}_{1}-\boldsymbol{Y} \beta_{2}\right)^{\prime} \boldsymbol{M}_{\boldsymbol{Z}}\left(\boldsymbol{Y}_{1}-\boldsymbol{Y}_{2} \beta_{2}\right)} .
\]</span></p>
<p>The first-order-condition for minimization is <span class="math inline">\(2 /\left(\boldsymbol{Y}_{1}-\boldsymbol{Y}_{2} \widehat{\beta}_{2}\right)^{\prime} \boldsymbol{M}_{\boldsymbol{Z}}\left(\boldsymbol{Y}_{1}-\boldsymbol{Y}_{2} \widehat{\beta}_{2}\right)\)</span> times</p>
<p><span class="math display">\[
\begin{aligned}
0 &amp;=\boldsymbol{Y}_{2}^{\prime} \boldsymbol{M}_{1}\left(\boldsymbol{Y}_{1}-\boldsymbol{Y}_{2} \widehat{\beta}_{2}\right)-\frac{\left(\boldsymbol{Y}_{1}-\boldsymbol{Y}_{2} \widehat{\beta}_{2}\right)^{\prime} \boldsymbol{M}_{1}\left(\boldsymbol{Y}_{1}-\boldsymbol{Y}_{2} \widehat{\beta}_{2}\right)}{\left(\boldsymbol{Y}_{1}-\boldsymbol{Y}_{2} \widehat{\beta}_{2}\right)^{\prime} \boldsymbol{M}_{\boldsymbol{Z}}\left(\boldsymbol{Y}_{1}-\boldsymbol{Y}_{2} \widehat{\beta}_{2}\right)} \boldsymbol{X}_{2}^{\prime} \boldsymbol{M}_{\boldsymbol{Z}}\left(\boldsymbol{Y}_{1}-\boldsymbol{Y}_{2} \widehat{\beta}_{2}\right) \\
&amp;=\boldsymbol{Y}_{2}^{\prime} \boldsymbol{M}_{1}\left(\boldsymbol{Y}_{1}-\boldsymbol{Y}_{2} \widehat{\beta}_{2}\right)-\widehat{\kappa} \boldsymbol{X}_{2}^{\prime} \boldsymbol{M}_{\boldsymbol{Z}}\left(\boldsymbol{Y}_{1}-\boldsymbol{Y}_{2} \widehat{\beta}_{2}\right)
\end{aligned}
\]</span></p>
<p>using definition (12.36). Rewriting,</p>
<p><span class="math display">\[
\boldsymbol{Y}_{2}^{\prime}\left(\boldsymbol{M}_{1}-\widehat{\kappa} \boldsymbol{M}_{\boldsymbol{Z}}\right) \boldsymbol{X}_{2} \widehat{\beta}_{2}=\boldsymbol{X}_{2}^{\prime}\left(\boldsymbol{M}_{1}-\widehat{\kappa} \boldsymbol{M}_{\boldsymbol{Z}}\right) \boldsymbol{Y}_{1} .
\]</span></p>
<p>Equation (12.37) is the same as the two equation system</p>
<p><span class="math display">\[
\begin{aligned}
\boldsymbol{Z}_{1}^{\prime} \boldsymbol{Z}_{1} \widehat{\beta}_{1}+\boldsymbol{Z}_{1}^{\prime} \boldsymbol{Y}_{2} \widehat{\beta}_{2} &amp;=\boldsymbol{Z}_{1}^{\prime} \boldsymbol{Y}_{1} \\
\boldsymbol{Y}_{2}^{\prime} \boldsymbol{Z}_{1} \widehat{\beta}_{1}+\left(\boldsymbol{Y}_{2}^{\prime}\left(\boldsymbol{I}_{n}-\widehat{\kappa} \boldsymbol{M}_{\boldsymbol{Z}}\right) \boldsymbol{Y}_{2}\right) \widehat{\beta}_{2} &amp;=\boldsymbol{Y}_{2}^{\prime}\left(\boldsymbol{I}_{n}-\widehat{\kappa} \boldsymbol{M}_{\boldsymbol{Z}}\right) \boldsymbol{Y}_{1} .
\end{aligned}
\]</span></p>
<p>The first equation is (12.35). Using (12.35), the second is</p>
<p><span class="math display">\[
\boldsymbol{Y}_{2}^{\prime} \boldsymbol{Z}_{1}\left(\boldsymbol{Z}_{1}^{\prime} \boldsymbol{Z}_{1}\right)^{-1} \boldsymbol{Z}_{1}^{\prime}\left(\boldsymbol{Y}_{1}-\boldsymbol{Y}_{2} \widehat{\beta}_{2}\right)+\left(\boldsymbol{Y}_{2}^{\prime}\left(\boldsymbol{I}_{n}-\widehat{\kappa} \boldsymbol{M}_{\boldsymbol{Z}}\right) \boldsymbol{Y}_{2}\right) \widehat{\beta}_{2}=\boldsymbol{Y}_{2}^{\prime}\left(\boldsymbol{I}_{n}-\widehat{\kappa} \boldsymbol{M}_{\boldsymbol{Z}}\right) \boldsymbol{Y}_{1}
\]</span></p>
<p>which is (12.38) when rearranged. We have thus shown that (12.37) is equivalent to (12.35) and (12.38) and is thus a valid expression for the LIML estimator.</p>
<p>Returning to the Card college proximity example we now present the LIML estimates of the equation with the two instruments (public, private). They are reported in the final column of Table 12.1. They are quite similar to the 2SLS estimates.</p>
<p>The LIML estimator may be calculated in Stata using the ivregress liml command.</p>
<table class="table">
<colgroup>
<col style="width: 100%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Theodore Anderson</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Theodore (Ted) Anderson (1918-2016) was a American statistician and econo-</td>
</tr>
<tr class="even">
<td style="text-align: left;">metrician, who made fundamental contributions to multivariate statistical the-</td>
</tr>
<tr class="odd">
<td style="text-align: left;">ory. Important contributions include the Anderson-Darling distribution test, the</td>
</tr>
<tr class="even">
<td style="text-align: left;">Anderson-Rubin statistic, the method of reduced rank regression, and his most</td>
</tr>
<tr class="odd">
<td style="text-align: left;">famous econometrics contribution - the LIML estimator. He continued working</td>
</tr>
<tr class="even">
<td style="text-align: left;">throughout his long life, even publishing theoretical work at the age of 97 !</td>
</tr>
</tbody>
</table>
</section>
<section id="split-sample-iv-and-jive" class="level2" data-number="12.14">
<h2 data-number="12.14" class="anchored" data-anchor-id="split-sample-iv-and-jive"><span class="header-section-number">12.14</span> Split-Sample IV and JIVE</h2>
<p>The ideal instrument for estimation of <span class="math inline">\(\beta\)</span> is <span class="math inline">\(W=\Gamma^{\prime} Z\)</span>. We can write the ideal IV estimator as</p>
<p><span class="math display">\[
\widehat{\beta}_{\text {ideal }}=\left(\sum_{i=1}^{n} W_{i} X_{i}^{\prime}\right)^{-1}\left(\sum_{i=1}^{n} W_{i} Y_{i}\right) .
\]</span></p>
<p>This estimator is not feasible since <span class="math inline">\(\Gamma\)</span> is unknown. The 2SLS estimator replaces <span class="math inline">\(\Gamma\)</span> with the multivariate least squares estimator <span class="math inline">\(\widehat{\Gamma}\)</span> and <span class="math inline">\(W_{i}\)</span> with <span class="math inline">\(\widehat{W}_{i}=\widehat{\Gamma}^{\prime} Z_{i}\)</span> leading to the following representation for 2SLS</p>
<p><span class="math display">\[
\widehat{\beta}_{2 \text { sls }}=\left(\sum_{i=1}^{n} \widehat{W}_{i} X_{i}^{\prime}\right)^{-1}\left(\sum_{i=1}^{n} \widehat{W}_{i} Y_{i}\right) .
\]</span></p>
<p>Since <span class="math inline">\(\widehat{\Gamma}\)</span> is estimated on the full sample including observation <span class="math inline">\(i\)</span> it is a function of the reduced form error <span class="math inline">\(u\)</span> which is correlated with the structural error <span class="math inline">\(e\)</span>. It follows that <span class="math inline">\(\widehat{W}\)</span> and <span class="math inline">\(e\)</span> are correlated, which means that <span class="math inline">\(\widehat{\beta}_{2 s l s}\)</span> is biased for <span class="math inline">\(\beta\)</span>. This correlation and bias disappears asymptotically but it can be important in applications.</p>
<p>A possible solution to this problem is to replace <span class="math inline">\(\widehat{W}\)</span> with a predicted value which is uncorrelated with the error <span class="math inline">\(e\)</span>. One method is the split-sample IV (SSIV) estimator of Angrist and Krueger (1995). Divide the sample randomly into two independent halves <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span>. Use <span class="math inline">\(A\)</span> to estimate the reduced form and <span class="math inline">\(B\)</span> to estimate the structural coefficient. Specifically, use sample <span class="math inline">\(A\)</span> to construct <span class="math inline">\(\widehat{\Gamma}_{A}=\left(\boldsymbol{Z}_{A}^{\prime} \boldsymbol{Z}_{A}\right)^{-1}\left(\boldsymbol{Z}_{A}^{\prime} \boldsymbol{X}_{A}\right)\)</span>. Combine this with sample <span class="math inline">\(B\)</span> to create the predicted values <span class="math inline">\(\widehat{\boldsymbol{W}}_{B}=Z_{B} \widehat{\Gamma}_{A}\)</span>. The SSIV estimator is <span class="math inline">\(\widehat{\beta}_{\text {ssiv }}=\)</span> <span class="math inline">\(\left(\widehat{\boldsymbol{W}}_{B}^{\prime} \boldsymbol{X}_{B}\right)^{-1}\left(\widehat{\boldsymbol{W}}_{B}^{\prime} \boldsymbol{Y}_{B}\right)\)</span>. This has lower bias than <span class="math inline">\(\widehat{\beta}_{2 \text { sls. }}\)</span></p>
<p>A limitation of SSIV is that the results will be sensitive to the sample spliting. One split will produce one estimator; another split will produce a different estimator. Any specific split is arbitrary, so the estimator depends on the specific random sorting of the observations into the samples <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span>. A second limitation of SSIV is that it is unlikely to work well when the sample size <span class="math inline">\(n\)</span> is small.</p>
<p>A much better solution is obtained by a leave-one-out estimator for <span class="math inline">\(\Gamma\)</span>. Specifically, let</p>
<p><span class="math display">\[
\widehat{\Gamma}_{(-i)}=\left(\boldsymbol{Z}^{\prime} \boldsymbol{Z}-Z_{i} Z_{i}^{\prime}\right)^{-1}\left(\boldsymbol{Z}^{\prime} \boldsymbol{X}-Z_{i} X_{i}^{\prime}\right)
\]</span></p>
<p>be the least squares leave-one-out estimator of the reduced form matrix <span class="math inline">\(\Gamma\)</span>, and let <span class="math inline">\(\widehat{W}_{i}=\widehat{\Gamma}_{(-i)}^{\prime} Z_{i}\)</span> be the reduced form predicted values. Using <span class="math inline">\(\widehat{W}_{i}=\widehat{\Gamma}_{(-i)}^{\prime} Z_{i}\)</span> as an instrument we obtain the estimator</p>
<p><span class="math display">\[
\widehat{\beta}_{\mathrm{jive1}}=\left(\sum_{i=1}^{n} \widehat{W}_{i} X_{i}^{\prime}\right)^{-1}\left(\sum_{i=1}^{n} \widehat{W}_{i} Y_{i}\right)=\left(\sum_{i=1}^{n} \widehat{\Gamma}_{(-i)}^{\prime} Z_{i} X_{i}^{\prime}\right)^{-1}\left(\sum_{i=1}^{n} \widehat{\Gamma}_{(-i)}^{\prime} Z_{i} Y_{i}\right) .
\]</span></p>
<p>This was called the jackknife instrumental variables (JIVE1) estimator by Angrist, Imbens, and Krueger (1999). It first appeared in Phillips and Hale (1977).</p>
<p>Angrist, Imbens, and Krueger (1999) pointed out that a somewhat simpler adjustment also removes the correlation and bias. Define the estimator and predicted value</p>
<p><span class="math display">\[
\begin{aligned}
\widetilde{\Gamma}_{(-i)} &amp;=\left(\boldsymbol{Z}^{\prime} \boldsymbol{Z}\right)^{-1}\left(\boldsymbol{Z}^{\prime} \boldsymbol{X}-Z_{i} X_{i}^{\prime}\right) \\
\widetilde{W}_{i} &amp;=\widetilde{\Gamma}_{(-i)}^{\prime} Z_{i}
\end{aligned}
\]</span></p>
<p>which only adjusts the <span class="math inline">\(\boldsymbol{Z}^{\prime} \boldsymbol{X}\)</span> component. Their JIVE2 estimator is</p>
<p><span class="math display">\[
\widehat{\beta}_{\mathrm{jive} 2}=\left(\sum_{i=1}^{n} \widetilde{W}_{i} X_{i}^{\prime}\right)^{-1}\left(\sum_{i=1}^{n} \widetilde{W}_{i} Y_{i}\right)=\left(\sum_{i=1}^{n} \widetilde{\Gamma}_{(-i)}^{\prime} Z_{i} X_{i}^{\prime}\right)^{-1}\left(\sum_{i=1}^{n} \widetilde{\Gamma}_{(-i)}^{\prime} Z_{i} Y_{i}\right) .
\]</span></p>
<p>Using the formula for leave-one-out estimators (Theorem 3.7), the JIVE1 and JIVE2 estimators use two linear operations: the first to create the predicted values <span class="math inline">\(\widehat{W}_{i}\)</span> or <span class="math inline">\(\widetilde{W}_{i}\)</span>, and the second to calculate the IV estimator. Thus the estimators do not require significantly more computation than 2SLS.</p>
<p>An asymptotic distribution theory for JIVE1 and JIVE2 was developed by Chao, Swanson, Hausman, Newey, and Woutersen (2012).</p>
<p>The JIVE1 and JIVE2 estimators may be calculated in Stata using the <span class="math inline">\(j\)</span> ive command. It is not a part of the standard package but can be easily added.</p>
</section>
<section id="consistency-of-2sls" class="level2" data-number="12.15">
<h2 data-number="12.15" class="anchored" data-anchor-id="consistency-of-2sls"><span class="header-section-number">12.15</span> Consistency of 2SLS</h2>
<p>We now demonstrate the consistency of the 2SLS estimator for the structural parameter. The following is a set of regularity conditions.</p>
<p>Assumption $12.1</p>
<ol type="1">
<li><p>The variables <span class="math inline">\(\left(Y_{1 i}, X_{i}, Z_{i}\right), i=1, \ldots, n\)</span>, are independent and identically distributed.</p></li>
<li><p><span class="math inline">\(\mathbb{E}\left[Y_{1}^{2}\right]&lt;\infty\)</span>.</p></li>
<li><p><span class="math inline">\(\mathbb{E}\|X\|^{2}&lt;\infty\)</span>.</p></li>
<li><p><span class="math inline">\(\mathbb{E}\|Z\|^{2}&lt;\infty\)</span></p></li>
<li><p><span class="math inline">\(\mathbb{E}\left[Z Z^{\prime}\right]\)</span> is positive definite.</p></li>
<li><p><span class="math inline">\(\mathbb{E}\left[Z X^{\prime}\right]\)</span> has full rank <span class="math inline">\(k\)</span>.</p></li>
<li><p><span class="math inline">\(\mathbb{E}[Z e]=0\)</span>.</p></li>
</ol>
<p>Assumptions 12.1.2-4 state that all variables have finite variances. Assumption 12.1.5 states that the instrument vector has an invertible design matrix, which is identical to the core assumption about regressors in the linear regression model. This excludes linearly redundant instruments. Assumptions 12.1.6 and 12.1.7 are the key identification conditions for instrumental variables. Assumption 12.1.6 states that the instruments and regressors have a full-rank cross-moment matrix. This is often called the relevance condition. Assumption 12.1.7 states that the instrumental variables and structural error are uncorrelated. Assumptions 12.1.5-7 are identical to Definition 12.1.</p>
<p>Theorem 12.1 Under Assumption 12.1, <span class="math inline">\(\widehat{\beta}_{2 s l s} \underset{p}{\longrightarrow} \beta\)</span> as <span class="math inline">\(n \rightarrow \infty\)</span>.</p>
<p>The proof of the theorem is provided below.</p>
<p>This theorem shows that the 2SLS estimator is consistent for the structural coefficient <span class="math inline">\(\beta\)</span> under similar moment conditions as the least squares estimator. The key differences are the instrumental variables assumption <span class="math inline">\(\mathbb{E}[Z e]=0\)</span> and the relevance condition <span class="math inline">\(\operatorname{rank}\left(\mathbb{E}\left[Z X^{\prime}\right]\right)=k\)</span>.</p>
<p>The result includes the IV estimator (when <span class="math inline">\(\ell=k\)</span> ) as a special case.</p>
<p>The proof of this consistency result is similar to that for least squares. Take the structural equation <span class="math inline">\(\boldsymbol{Y}=\boldsymbol{X} \beta+\boldsymbol{e}\)</span> in matrix format and substitute it into the expression for the estimator. We obtain</p>
<p><span class="math display">\[
\begin{aligned}
\widehat{\beta}_{2 \text { sls }} &amp;=\left(\boldsymbol{X}^{\prime} \boldsymbol{Z}\left(\boldsymbol{Z}^{\prime} \boldsymbol{Z}\right)^{-1} \boldsymbol{Z}^{\prime} \boldsymbol{X}\right)^{-1} \boldsymbol{X}^{\prime} \boldsymbol{Z}\left(\boldsymbol{Z}^{\prime} \boldsymbol{Z}\right)^{-1} \boldsymbol{Z}^{\prime}(\boldsymbol{X} \beta+\boldsymbol{e}) \\
&amp;=\beta+\left(\boldsymbol{X}^{\prime} \boldsymbol{Z}\left(\boldsymbol{Z}^{\prime} \boldsymbol{Z}\right)^{-1} \boldsymbol{Z}^{\prime} \boldsymbol{X}\right)^{-1} \boldsymbol{X}^{\prime} \boldsymbol{Z}\left(\boldsymbol{Z}^{\prime} \boldsymbol{Z}\right)^{-1} \boldsymbol{Z}^{\prime} \boldsymbol{e} .
\end{aligned}
\]</span></p>
<p>This separates out the stochastic component. Re-writing and applying the WLLN and CMT</p>
<p><span class="math display">\[
\begin{aligned}
\widehat{\beta}_{2 s l s}-\beta &amp;=\left(\left(\frac{1}{n} \boldsymbol{X}^{\prime} \boldsymbol{Z}\right)\left(\frac{1}{n} \boldsymbol{Z}^{\prime} \boldsymbol{Z}\right)^{-1}\left(\frac{1}{n} \boldsymbol{Z}^{\prime} \boldsymbol{X}\right)\right)^{-1} \\
&amp; \times\left(\frac{1}{n} \boldsymbol{X}^{\prime} \boldsymbol{Z}\right)\left(\frac{1}{n} \boldsymbol{Z}^{\prime} \boldsymbol{Z}\right)^{-1}\left(\frac{1}{n} \boldsymbol{Z}^{\prime} \boldsymbol{e}\right) \\
\underset{p}{\rightarrow}\left(\boldsymbol{Q}_{X Z} \boldsymbol{Q}_{Z Z}^{-1} \boldsymbol{Q}_{Z X}\right)^{-1} \boldsymbol{Q}_{X Z} \boldsymbol{Q}_{Z Z}^{-1} \mathbb{E}[Z e]=0
\end{aligned}
\]</span></p>
<p>where</p>
<p><span class="math display">\[
\begin{aligned}
&amp;\boldsymbol{Q}_{X Z}=\mathbb{E}\left[X Z^{\prime}\right] \\
&amp;\boldsymbol{Q}_{Z Z}=\mathbb{E}\left[Z Z^{\prime}\right] \\
&amp;\boldsymbol{Q}_{Z X}=\mathbb{E}\left[Z X^{\prime}\right] .
\end{aligned}
\]</span></p>
<p>The WLLN holds under Assumptions 12.1.1 and 12.1.2-4. The continuous mapping theorem applies if the matrices <span class="math inline">\(\boldsymbol{Q}_{Z Z}\)</span> and <span class="math inline">\(\boldsymbol{Q}_{X Z} \boldsymbol{Q}_{Z Z}^{-1} \boldsymbol{Q}_{Z X}\)</span> are invertible, which hold under Assumptions 12.1.5 and 12.1.6. The final equality uses Assumption 12.1.7.</p>
</section>
<section id="asymptotic-distribution-of-2sls" class="level2" data-number="12.16">
<h2 data-number="12.16" class="anchored" data-anchor-id="asymptotic-distribution-of-2sls"><span class="header-section-number">12.16</span> Asymptotic Distribution of 2SLS</h2>
<p>We now show that the 2SLS estimator satisfies a central limit theorem. We first state a set of sufficient regularity conditions. Assumption 12.2 In addition to Assumption 12.1,</p>
<ol type="1">
<li><p><span class="math inline">\(\mathbb{E}\left[Y_{1}^{4}\right]&lt;\infty\)</span>.</p></li>
<li><p><span class="math inline">\(\mathbb{E}\|X\|^{4}&lt;\infty\)</span>.</p></li>
<li><p><span class="math inline">\(\mathbb{E}\|Z\|^{4}&lt;\infty\)</span>.</p></li>
<li><p><span class="math inline">\(\Omega=\mathbb{E}\left[Z Z^{\prime} e^{2}\right]\)</span> is positive definite.</p></li>
</ol>
<p>Assumption <span class="math inline">\(12.2\)</span> strengthens Assumption <span class="math inline">\(12.1\)</span> by requiring that the dependent variable and instruments have finite fourth moments. This is used to establish the central limit theorem.</p>
<p>Theorem 12.2 Under Assumption 12.2, as <span class="math inline">\(n \rightarrow \infty\)</span>.</p>
<p><span class="math display">\[
\sqrt{n}\left(\widehat{\beta}_{2 \text { sls }}-\beta\right) \underset{d}{\longrightarrow} \mathrm{N}\left(0, V_{\beta}\right)
\]</span></p>
<p>where</p>
<p><span class="math display">\[
\boldsymbol{V}_{\beta}=\left(\boldsymbol{Q}_{X Z} \boldsymbol{Q}_{Z Z}^{-1} \boldsymbol{Q}_{Z X}\right)^{-1}\left(\boldsymbol{Q}_{X Z} \boldsymbol{Q}_{Z Z}^{-1} \Omega \boldsymbol{Q}_{Z Z}^{-1} \boldsymbol{Q}_{Z X}\right)\left(\boldsymbol{Q}_{X Z} \boldsymbol{Q}_{Z Z}^{-1} \boldsymbol{Q}_{Z X}\right)^{-1}
\]</span></p>
<p>This shows that the 2 SLS estimator converges at a <span class="math inline">\(\sqrt{n}\)</span> rate to a normal random vector. It shows as well the form of the covariance matrix. The latter takes a substantially more complicated form than the least squares estimator.</p>
<p>As in the case of least squares estimation the asymptotic variance simplifies under a conditional homoskedasticity condition. For 2SLS the simplification occurs when <span class="math inline">\(\mathbb{E}\left[e^{2} \mid Z\right]=\sigma^{2}\)</span>. This holds when <span class="math inline">\(Z\)</span> and <span class="math inline">\(e\)</span> are independent. It may be reasonable in some contexts to conceive that the error <span class="math inline">\(e\)</span> is independent of the excluded instruments <span class="math inline">\(Z_{2}\)</span>, since by assumption the impact of <span class="math inline">\(Z_{2}\)</span> on <span class="math inline">\(Y\)</span> is only through <span class="math inline">\(X\)</span>, but there is no reason to expect <span class="math inline">\(e\)</span> to be independent of the included exogenous variables <span class="math inline">\(X_{1}\)</span>. Hence heteroskedasticity should be equally expected in 2SLS and least squares regression. Nevertheless, under homoskedasticity we have the simplifications <span class="math inline">\(\Omega=\boldsymbol{Q}_{Z Z} \sigma^{2}\)</span> and <span class="math inline">\(\boldsymbol{V}_{\beta}=\boldsymbol{V}_{\beta}^{0} \stackrel{\text { def }}{=}\left(\boldsymbol{Q}_{X Z} \boldsymbol{Q}_{Z Z}^{-1} \boldsymbol{Q}_{Z X}\right)^{-1} \sigma^{2}\)</span>.</p>
<p>The derivation of the asymptotic distribution builds on the proof of consistency. Using equation (12.39) we have</p>
<p><span class="math display">\[
\sqrt{n}\left(\widehat{\beta}_{2 \mathrm{sls}}-\beta\right)=\left(\left(\frac{1}{n} \boldsymbol{X}^{\prime} \boldsymbol{Z}\right)\left(\frac{1}{n} \boldsymbol{Z}^{\prime} \boldsymbol{Z}\right)^{-1}\left(\frac{1}{n} \boldsymbol{Z}^{\prime} \boldsymbol{X}\right)\right)^{-1}\left(\frac{1}{n} \boldsymbol{X}^{\prime} \boldsymbol{Z}\right)\left(\frac{1}{n} \boldsymbol{Z}^{\prime} \boldsymbol{Z}\right)^{-1}\left(\frac{1}{\sqrt{n}} \boldsymbol{Z}^{\prime} \boldsymbol{e}\right) \text {. }
\]</span></p>
<p>We apply the WLLN and CMT for the moment matrices involving <span class="math inline">\(X\)</span> and <span class="math inline">\(\boldsymbol{Z}\)</span> the same as in the proof of consistency. In addition, by the CLT for i.i.d. observations</p>
<p><span class="math display">\[
\frac{1}{\sqrt{n}} \boldsymbol{Z}^{\prime} \boldsymbol{e}=\frac{1}{\sqrt{n}} \sum_{i=1}^{n} Z_{i} e_{i} \underset{d}{\longrightarrow} \mathrm{N}(0, \Omega)
\]</span></p>
<p>because the vector <span class="math inline">\(Z_{i} e_{i}\)</span> is i.i.d. and mean zero under Assumptions 12.1.1 and 12.1.7, and has a finite second moment as we verify below. We obtain</p>
<p><span class="math display">\[
\begin{aligned}
\sqrt{n}\left(\widehat{\beta}_{2 \text { sls }}-\beta\right) &amp;=\left(\left(\frac{1}{n} \boldsymbol{X}^{\prime} \boldsymbol{Z}\right)\left(\frac{1}{n} \boldsymbol{Z}^{\prime} \boldsymbol{Z}\right)^{-1}\left(\frac{1}{n} \boldsymbol{Z}^{\prime} \boldsymbol{X}\right)\right)^{-1}\left(\frac{1}{n} \boldsymbol{X}^{\prime} \boldsymbol{Z}\right)\left(\frac{1}{n} \boldsymbol{Z}^{\prime} \boldsymbol{Z}\right)^{-1}\left(\frac{1}{\sqrt{n}} \boldsymbol{Z}^{\prime} \boldsymbol{e}\right) \\
&amp; \underset{d}{\rightarrow}\left(\boldsymbol{Q}_{X Z} \boldsymbol{Q}_{Z Z}^{-1} \boldsymbol{Q}_{Z X}\right)^{-1} \boldsymbol{Q}_{X Z} \boldsymbol{Q}_{Z Z}^{-1} \mathrm{~N}(0, \Omega)=\mathrm{N}\left(0, \boldsymbol{V}_{\beta}\right)
\end{aligned}
\]</span></p>
<p>as stated.</p>
<p>To complete the proof we demonstrate that <span class="math inline">\(Z e\)</span> has a finite second moment under Assumption 12.2. To see this, note that by Minkowski’s inequality (B.34)</p>
<p><span class="math display">\[
\left(\mathbb{E}\left[e^{4}\right]\right)^{1 / 4}=\left(\mathbb{E}\left[\left(Y_{1}-X^{\prime} \beta\right)^{4}\right]\right)^{1 / 4} \leq\left(\mathbb{E}\left[Y_{1}^{4}\right]\right)^{1 / 4}+\|\beta\|\left(\mathbb{E}\|X\|^{4}\right)^{1 / 4}&lt;\infty
\]</span></p>
<p>under Assumptions 12.2.1 and 12.2.2. Then by the Cauchy-Schwarz inequality (B.32)</p>
<p><span class="math display">\[
\mathbb{E}\|Z e\|^{2} \leq\left(\mathbb{E}\|Z\|^{4}\right)^{1 / 2}\left(\mathbb{E}\left[e^{4}\right]\right)^{1 / 2}&lt;\infty
\]</span></p>
<p>using Assumptions 12.2.3.</p>
</section>
<section id="determinants-of-2-sls-variance" class="level2" data-number="12.17">
<h2 data-number="12.17" class="anchored" data-anchor-id="determinants-of-2-sls-variance"><span class="header-section-number">12.17</span> Determinants of 2 SLS Variance</h2>
<p>It is instructive to examine the asymptotic variance of the 2SLS estimator to understand the factors which determine the precision (or lack thereof) of the estimator. As in the least squares case it is more transparent to examine the variance under the assumption of homoskedasticity. In this case the asymptotic variance takes the form</p>
<p><span class="math display">\[
\begin{aligned}
\boldsymbol{V}_{\beta}^{0} &amp;=\left(\boldsymbol{Q}_{X Z} \boldsymbol{Q}_{Z Z}^{-1} \boldsymbol{Q}_{Z X}\right)^{-1} \sigma^{2} \\
&amp;=\left(\mathbb{E}\left[X Z^{\prime}\right]\left(\mathbb{E}\left[Z Z^{\prime}\right]\right)^{-1} \mathbb{E}\left[Z X^{\prime}\right]\right)^{-1} \mathbb{E}\left[e^{2}\right] .
\end{aligned}
\]</span></p>
<p>As in the least squares case we can see that the variance of <span class="math inline">\(\widehat{\beta}_{2 \text { sls }}\)</span> is increasing in the variance of the error <span class="math inline">\(e\)</span> and decreasing in the variance of <span class="math inline">\(X\)</span>. What is different is that the variance is decreasing in the (matrixvalued) correlation between <span class="math inline">\(X\)</span> and <span class="math inline">\(Z\)</span>.</p>
<p>It is also useful to observe that the variance expression is not affected by the variance structure of <span class="math inline">\(Z\)</span>. Indeed, <span class="math inline">\(\boldsymbol{V}_{\beta}^{0}\)</span> is invariant to rotations of <span class="math inline">\(Z\)</span> (if you replace <span class="math inline">\(Z\)</span> with <span class="math inline">\(\boldsymbol{C Z}\)</span> for invertible <span class="math inline">\(\boldsymbol{C}\)</span> the expression does not change). This means that the variance expression is not affected by the scaling of <span class="math inline">\(Z\)</span> and is not directly affected by correlation among the <span class="math inline">\(Z\)</span>.</p>
<p>We can also use this expression to examine the impact of increasing the instrument set. Suppose we partition <span class="math inline">\(Z=\left(Z_{a}, Z_{b}\right)\)</span> where <span class="math inline">\(\operatorname{dim}\left(Z_{a}\right) \geq k\)</span> so we can construct a 2SLS estimator using <span class="math inline">\(Z_{a}\)</span> alone. Let <span class="math inline">\(\widehat{\beta}_{a}\)</span> and <span class="math inline">\(\widehat{\beta}\)</span> denote the 2SLS estimators constructed using the instrument sets <span class="math inline">\(Z_{a}\)</span> and <span class="math inline">\(\left(Z_{a}, Z_{b}\right)\)</span>, respectively. Without loss of generality we can assume that <span class="math inline">\(Z_{a}\)</span> and <span class="math inline">\(Z_{b}\)</span> are uncorrelated (if not, replace <span class="math inline">\(Z_{b}\)</span> with the projection error after projecting onto <span class="math inline">\(Z_{a}\)</span> ). In this case both <span class="math inline">\(\mathbb{E}\left[Z Z^{\prime}\right]\)</span> and <span class="math inline">\(\left(\mathbb{E}\left[Z Z^{\prime}\right]\right)^{-1}\)</span> are block diagonal so</p>
<p><span class="math display">\[
\begin{aligned}
\operatorname{avar}[\widehat{\beta}] &amp;=\left(\mathbb{E}\left[X Z^{\prime}\right]\left(\mathbb{E}\left[Z Z^{\prime}\right]\right)^{-1} \mathbb{E}\left[Z X^{\prime}\right]\right)^{-1} \sigma^{2} \\
&amp;=\left(\mathbb{E}\left[X Z_{a}^{\prime}\right]\left(\mathbb{E}\left[Z_{a} Z_{a}^{\prime}\right]\right)^{-1} \mathbb{E}\left[Z_{a} X^{\prime}\right]+\mathbb{E}\left[X Z_{b}^{\prime}\right]\left(\mathbb{E}\left[Z_{b} Z_{b}^{\prime}\right]\right)^{-1} \mathbb{E}\left[Z_{b} X^{\prime}\right]\right)^{-1} \sigma^{2} \\
&amp; \leq\left(\mathbb{E}\left[X Z_{a}^{\prime}\right]\left(\mathbb{E}\left[Z_{a} Z_{a}^{\prime}\right]\right)^{-1} \mathbb{E}\left[Z_{a} X^{\prime}\right]\right)^{-1} \sigma^{2} \\
&amp;=\operatorname{avar}\left[\widehat{\beta}_{a}\right]
\end{aligned}
\]</span></p>
<p>with strict inequality if <span class="math inline">\(\mathbb{E}\left[X Z_{b}^{\prime}\right] \neq 0\)</span>. Thus the 2SLS estimator with the full instrument set has a smaller asymptotic variance than the estimator with the smaller instrument set.</p>
<p>What we have shown is that the asymptotic variance of the 2SLS estimator is decreasing as the number of instruments increases. From the viewpoint of asymptotic efficiency this means that it is better to use more instruments (when they are available and are all known to be valid instruments).</p>
<p>Unfortunately there is a catch. It turns out that the finite sample bias of the 2SLS estimator (which cannot be calculated exactly but can be approximated using asymptotic expansions) is generically increasing linearily as the number of instruments increases. We will see some calculations illustrating this phenomenon in Section 12.37. Thus the choice of instruments in practice induces a trade-off between bias and variance.</p>
</section>
<section id="covariance-matrix-estimation" class="level2" data-number="12.18">
<h2 data-number="12.18" class="anchored" data-anchor-id="covariance-matrix-estimation"><span class="header-section-number">12.18</span> Covariance Matrix Estimation</h2>
<p>Estimation of the asymptotic covariance matrix <span class="math inline">\(\boldsymbol{V}_{\beta}\)</span> is done using similar techniques as for least squares estimation. The estimator is constructed by replacing the population moment matrices by sample counterparts. Thus</p>
<p><span class="math display">\[
\widehat{\boldsymbol{V}}_{\beta}=\left(\widehat{\mathbf{Q}}_{X Z} \widehat{\mathbf{Q}}_{Z Z}^{-1} \widehat{\mathbf{Q}}_{Z X}\right)^{-1}\left(\widehat{\mathbf{Q}}_{X Z} \widehat{\mathbf{Q}}_{Z Z}^{-1} \widehat{\Omega} \widehat{\mathbf{Q}}_{Z Z}^{-1} \widehat{\mathbf{Q}}_{Z X}\right)\left(\widehat{\mathbf{Q}}_{X Z} \widehat{\mathbf{Q}}_{Z Z}^{-1} \widehat{\mathbf{Q}}_{Z X}\right)^{-1}
\]</span></p>
<p>where</p>
<p><span class="math display">\[
\begin{aligned}
\widehat{\boldsymbol{Q}}_{Z Z} &amp;=\frac{1}{n} \sum_{i=1}^{n} Z_{i} Z_{i}^{\prime}=\frac{1}{n} \boldsymbol{Z}^{\prime} \boldsymbol{Z} \\
\widehat{\boldsymbol{Q}}_{X Z} &amp;=\frac{1}{n} \sum_{i=1}^{n} X_{i} Z_{i}^{\prime}=\frac{1}{n} \boldsymbol{X}^{\prime} \boldsymbol{Z} \\
\widehat{\Omega} &amp;=\frac{1}{n} \sum_{i=1}^{n} Z_{i} Z_{i}^{\prime} \widehat{e}_{i}^{2} \\
\widehat{e}_{i} &amp;=Y_{i}-X_{i}^{\prime} \widehat{\beta}_{2 s l s}
\end{aligned}
\]</span></p>
<p>The homoskedastic covariance matrix can be estimated by</p>
<p><span class="math display">\[
\begin{aligned}
\widehat{\boldsymbol{V}}_{\beta}^{0} &amp;=\left(\widehat{\boldsymbol{Q}}_{X Z} \widehat{\boldsymbol{Q}}_{Z Z}^{-1} \widehat{\boldsymbol{Q}}_{Z X}\right)^{-1} \widehat{\sigma}^{2} \\
\widehat{\sigma}^{2} &amp;=\frac{1}{n} \sum_{i=1}^{n} \widehat{e}_{i}^{2}
\end{aligned}
\]</span></p>
<p>Standard errors for the coefficients are obtained as the square roots of the diagonal elements of <span class="math inline">\(n^{-1} \widehat{\boldsymbol{V}}_{\beta}\)</span>. Confidence intervals, t-tests, and Wald tests may all be constructed from the coefficient and covariance matrix estimates exactly as for least squares regression.</p>
<p>In Stata the ivregress command by default calculates the covariance matrix estimator using the homoskedastic covariance matrix. To obtain covariance matrix estimation and standard errors with the robust estimator <span class="math inline">\(\widehat{\boldsymbol{V}}_{\beta}\)</span>, use the “,r” option.</p>
<p>Theorem 12.3 Under Assumption 12.2, as <span class="math inline">\(n \rightarrow \infty, \widehat{\boldsymbol{V}}_{\beta}^{0}{\underset{p}{\longrightarrow}}^{\boldsymbol{V}_{\beta}^{0}}\)</span> and <span class="math inline">\(\widehat{\boldsymbol{V}}_{\beta} \vec{p}^{\boldsymbol{V}_{\beta}}\)</span> To prove Theorem <span class="math inline">\(12.3\)</span> the key is to show <span class="math inline">\(\widehat{\Omega} \vec{p} \Omega\)</span> as the other convergence results were established in the proof of consistency. We defer this to Exercise 12.6.</p>
<p>It is important that the covariance matrix be constructed using the correct residual formula <span class="math inline">\(\widehat{e}_{i}=Y_{i}-\)</span> <span class="math inline">\(X_{i}^{\prime} \widehat{\beta}_{2 \text { sls }}\)</span>. This is different than what would be obtained if the “two-stage” computation method were used. To see this let’s walk through the two-stage method. First, we estimate the reduced form <span class="math inline">\(X_{i}=\widehat{\Gamma}^{\prime} Z_{i}+\widehat{u}_{i}\)</span> to obtain the predicted values <span class="math inline">\(\widehat{X}_{i}=\widehat{\Gamma}^{\prime} Z_{i}\)</span>. Second, we regress <span class="math inline">\(Y\)</span> on <span class="math inline">\(\widehat{X}\)</span> to obtain the 2SLS estimator <span class="math inline">\(\widehat{\beta}_{2 \text { sls }}\)</span>. This latter regression takes the form</p>
<p><span class="math display">\[
Y_{i}=\widehat{X}_{i}^{\prime} \widehat{\beta}_{2 \mathrm{sls}}+\widehat{v}_{i}
\]</span></p>
<p>where <span class="math inline">\(\widehat{v}_{i}\)</span> are least squares residuals. The covariance matrix (and standard errors) reported by this regression are constructed using the residual <span class="math inline">\(\widehat{v}_{i}\)</span>. For example, the homoskedastic formula is</p>
<p><span class="math display">\[
\begin{aligned}
\widehat{\boldsymbol{V}}_{\beta} &amp;=\left(\frac{1}{n} \widehat{\boldsymbol{X}}^{\prime} \widehat{\boldsymbol{X}}\right)^{-1} \widehat{\sigma}_{v}^{2}=\left(\widehat{\boldsymbol{Q}}_{X Z} \widehat{\boldsymbol{Q}}_{Z Z}^{-1} \widehat{\mathbf{Q}}_{Z X}\right)^{-1} \widehat{\sigma}_{v}^{2} \\
\widehat{\sigma}_{v}^{2} &amp;=\frac{1}{n} \sum_{i=1}^{n} \widehat{v}_{i}^{2}
\end{aligned}
\]</span></p>
<p>which is proportional to the variance estimator <span class="math inline">\(\widehat{\sigma}_{v}^{2}\)</span> rather than <span class="math inline">\(\widehat{\sigma}^{2}\)</span>. This is important because the residual <span class="math inline">\(\widehat{v}\)</span> differs from <span class="math inline">\(\widehat{e}\)</span>. We can see this because the regression (12.41) uses the regressor <span class="math inline">\(\widehat{X}\)</span> rather than <span class="math inline">\(X\)</span>. Indeed, we calculate that</p>
<p><span class="math display">\[
\widehat{v}_{i}=Y_{i}-X_{i}^{\prime} \widehat{\beta}_{2 \mathrm{sls}}+\left(X_{i}-\widehat{X}_{i}\right)^{\prime} \widehat{\beta}_{2 \mathrm{sls}}=\widehat{e}_{i}+\widehat{u}_{i}^{\prime} \widehat{\beta}_{2 \mathrm{sls}} \neq \widehat{e}_{i} \text {. }
\]</span></p>
<p>This means that standard errors reported by the regression (12.41) will be incorrect.</p>
<p>This problem is avoided if the 2SLS estimator is constructed directly and the standard errors calculated with the correct formula rather than taking the “two-step” shortcut.</p>
</section>
<section id="liml-asymptotic-distribution" class="level2" data-number="12.19">
<h2 data-number="12.19" class="anchored" data-anchor-id="liml-asymptotic-distribution"><span class="header-section-number">12.19</span> LIML Asymptotic Distribution</h2>
<p>In this section we show that the LIML estimator is asymptotically equivalent to the 2SLS estimator. We recommend, however, a different covariance matrix estimator based on the IV representation.</p>
<p>We start by deriving the asymptotic distribution. Recall that the LIML estimator has several representations including</p>
<p><span class="math display">\[
\widehat{\beta}_{\mathrm{liml}}=\left(\boldsymbol{X}^{\prime}\left(\boldsymbol{I}_{n}-\widehat{\kappa} \boldsymbol{M}_{\boldsymbol{Z}}\right) \boldsymbol{X}\right)^{-1}\left(\boldsymbol{X}^{\prime}\left(\boldsymbol{I}_{n}-\widehat{\kappa} \boldsymbol{M}_{\boldsymbol{Z}}\right) \boldsymbol{Y}_{1}\right)
\]</span></p>
<p>where</p>
<p><span class="math display">\[
\widehat{\boldsymbol{\kappa}}=\min _{\gamma} \frac{\gamma^{\prime} \overrightarrow{\boldsymbol{Y}}^{\prime} \boldsymbol{M}_{1} \overrightarrow{\boldsymbol{Y}} \gamma}{\gamma^{\prime} \overrightarrow{\boldsymbol{Y}}^{\prime} \boldsymbol{M}_{\boldsymbol{Z}} \overrightarrow{\boldsymbol{Y}} \gamma}
\]</span></p>
<p>and <span class="math inline">\(\gamma=\left(1,-\beta_{2}^{\prime}\right)^{\prime}\)</span>. For the distribution theory it is useful to rewrite the slope coefficient as</p>
<p><span class="math display">\[
\widehat{\beta}_{\mathrm{liml}}=\left(\boldsymbol{X}^{\prime} \boldsymbol{P}_{Z} \boldsymbol{X}-\widehat{\mu} \boldsymbol{X}^{\prime} \boldsymbol{M}_{Z} \boldsymbol{X}\right)^{-1}\left(\boldsymbol{X}^{\prime} \boldsymbol{P}_{\boldsymbol{Z}} \boldsymbol{Y}_{1}-\widehat{\mu} \boldsymbol{X}^{\prime} \boldsymbol{M}_{Z} \boldsymbol{Y}_{1}\right)
\]</span></p>
<p>where</p>
<p><span class="math display">\[
\widehat{\mu}=\widehat{\kappa}-1=\min _{\gamma} \frac{\gamma^{\prime} \overrightarrow{\boldsymbol{Y}}^{\prime} \boldsymbol{M}_{1} \boldsymbol{Z}_{2}\left(\boldsymbol{Z}_{2}^{\prime} \boldsymbol{M}_{1} \boldsymbol{Z}_{2}\right)^{-1} \boldsymbol{Z}_{2}^{\prime} \boldsymbol{M}_{1} \overrightarrow{\boldsymbol{Y}} \gamma}{\gamma^{\prime} \overrightarrow{\boldsymbol{Y}}^{\prime} \boldsymbol{M}_{\boldsymbol{Z}} \overrightarrow{\boldsymbol{Y}} \gamma} \text {. }
\]</span></p>
<p>This second equality holds because the span of <span class="math inline">\(\boldsymbol{Z}=\left[\boldsymbol{Z}_{1}, \boldsymbol{Z}_{2}\right]\)</span> equals the span of <span class="math inline">\(\left[\boldsymbol{Z}_{1}, \boldsymbol{M}_{1} \boldsymbol{Z}_{2}\right]\)</span>. This implies</p>
<p><span class="math display">\[
\boldsymbol{P}_{\boldsymbol{Z}}=\boldsymbol{Z}\left(\boldsymbol{Z}^{\prime} \boldsymbol{Z}\right)^{-1} \boldsymbol{Z}^{\prime}=\boldsymbol{Z}_{1}\left(\boldsymbol{Z}_{1}^{\prime} \boldsymbol{Z}_{1}\right)^{-1} \boldsymbol{Z}_{1}^{\prime}+\boldsymbol{M}_{1} \boldsymbol{Z}_{2}\left(\boldsymbol{Z}_{2}^{\prime} \boldsymbol{M}_{1} \boldsymbol{Z}_{2}\right)^{-1} \boldsymbol{Z}_{2}^{\prime} \boldsymbol{M}_{1}
\]</span></p>
<p>We now show that <span class="math inline">\(n \widehat{\mu}=O_{p}(1)\)</span>. The reduced form (12.33) implies that</p>
<p><span class="math display">\[
\boldsymbol{Y}=\boldsymbol{Z}_{1} \Pi_{1}+\boldsymbol{Z}_{2} \Pi_{2}+\boldsymbol{e} .
\]</span></p>
<p>It will be important to note that</p>
<p><span class="math display">\[
\Pi_{2}=\left[\lambda_{2}, \Gamma_{22}\right]=\left[\Gamma_{22} \beta_{2}, \Gamma_{22}\right]
\]</span></p>
<p>using (12.15). It follows that <span class="math inline">\(\Pi_{2} \gamma=0\)</span>. Note <span class="math inline">\(\boldsymbol{U} \gamma=\boldsymbol{e}\)</span>. Then <span class="math inline">\(\boldsymbol{M}_{\boldsymbol{Z}} \boldsymbol{Y} \gamma=\boldsymbol{M}_{\boldsymbol{Z}} \boldsymbol{e}\)</span> and <span class="math inline">\(\boldsymbol{M}_{1} \boldsymbol{Y} \gamma=\boldsymbol{M}_{1} \boldsymbol{e}\)</span>. Hence</p>
<p><span class="math display">\[
\begin{aligned}
n \widehat{\mu} &amp;=\min _{\gamma} \frac{\gamma^{\prime} \overrightarrow{\boldsymbol{Y}}^{\prime} \boldsymbol{M}_{1} \boldsymbol{Z}_{2}\left(\boldsymbol{Z}_{2}^{\prime} \boldsymbol{M}_{1} \boldsymbol{Z}_{2}\right)^{-1} \boldsymbol{Z}_{2}^{\prime} \boldsymbol{M}_{1} \overrightarrow{\boldsymbol{Y}} \gamma}{\gamma^{\prime} \frac{1}{n} \overrightarrow{\boldsymbol{Y}}^{\prime} \boldsymbol{M}_{\boldsymbol{Z}} \overrightarrow{\boldsymbol{Y}} \gamma} \\
&amp; \leq \frac{\left(\frac{1}{\sqrt{n}} \boldsymbol{e}^{\prime} \boldsymbol{M}_{1} \boldsymbol{Z}_{2}\right)\left(\frac{1}{n} \boldsymbol{Z}_{2}^{\prime} \boldsymbol{M}_{1} \boldsymbol{Z}_{2}\right)^{-1}\left(\frac{1}{\sqrt{n}} \boldsymbol{Z}_{2}^{\prime} \boldsymbol{M}_{1} \boldsymbol{e}\right)}{\frac{1}{n} \boldsymbol{e}^{\prime} \boldsymbol{M}_{\boldsymbol{Z}} \boldsymbol{e}} \\
&amp;=O_{p}(1) .
\end{aligned}
\]</span></p>
<p>It follows that</p>
<p><span class="math display">\[
\begin{aligned}
\sqrt{n}\left(\widehat{\beta}_{\mathrm{liml}}-\beta\right) &amp;=\left(\frac{1}{n} \boldsymbol{X}^{\prime} \boldsymbol{P}_{Z} \boldsymbol{X}-\widehat{\mu} \frac{1}{n} \boldsymbol{X}^{\prime} \boldsymbol{M}_{\boldsymbol{Z}} \boldsymbol{X}\right)^{-1}\left(\frac{1}{\sqrt{n}} \boldsymbol{X}^{\prime} \boldsymbol{P}_{Z} \boldsymbol{e}-\sqrt{n} \widehat{\mu} \frac{1}{n} \boldsymbol{X}^{\prime} \boldsymbol{M}_{Z} \boldsymbol{e}\right) \\
&amp;=\left(\frac{1}{n} \boldsymbol{X}^{\prime} \boldsymbol{P}_{Z} \boldsymbol{X}-o_{p}(1)\right)^{-1}\left(\frac{1}{\sqrt{n}} \boldsymbol{X}^{\prime} \boldsymbol{P}_{Z} \boldsymbol{e}-o_{p}(1)\right) \\
&amp;=\sqrt{n}\left(\widehat{\beta}_{2 \mathrm{sls}}-\beta\right)+o_{p}(1)
\end{aligned}
\]</span></p>
<p>which means that LIML and 2SLS have the same asymptotic distribution. This holds under the same assumptions as for 2SLS.</p>
<p>Consequently, one method to obtain an asymptotically valid covariance estimator for LIML is to use the 2SLS formula. However, this is not the best choice. Rather, consider the IV representation for LIML</p>
<p><span class="math display">\[
\widehat{\beta}_{\mathrm{liml}}=\left(\widetilde{\boldsymbol{X}}^{\prime} \boldsymbol{X}\right)^{-1}\left(\widetilde{\boldsymbol{X}}^{\prime} \boldsymbol{Y}_{1}\right)
\]</span></p>
<p>where</p>
<p><span class="math display">\[
\widetilde{\boldsymbol{X}}=\left(\begin{array}{c}
\boldsymbol{X}_{1} \\
\boldsymbol{X}_{2}-\widehat{\boldsymbol{K}}_{2}
\end{array}\right)
\]</span></p>
<p>and <span class="math inline">\(\widehat{\boldsymbol{U}}_{2}=\boldsymbol{M}_{\boldsymbol{Z}} \boldsymbol{X}_{2}\)</span>. The asymptotic covariance matrix formula for an IV estimator is</p>
<p><span class="math display">\[
\widehat{\boldsymbol{V}}_{\beta}=\left(\frac{1}{n} \widetilde{\boldsymbol{X}}^{\prime} \boldsymbol{X}\right)^{-1} \widehat{\Omega}\left(\frac{1}{n} \boldsymbol{X}^{\prime} \tilde{\boldsymbol{X}}\right)^{-1}
\]</span></p>
<p>where</p>
<p><span class="math display">\[
\begin{aligned}
\widehat{\Omega} &amp;=\frac{1}{n} \sum_{i=1}^{n} \widetilde{X}_{i} \widetilde{X}_{i} \widehat{e}_{i}^{2} \\
\widehat{e}_{i} &amp;=Y_{1 i}-X_{i}^{\prime} \widehat{\beta}_{\text {liml }} .
\end{aligned}
\]</span></p>
<p>This simplifies to the 2SLS formula when <span class="math inline">\(\widehat{\kappa}=1\)</span> but otherwise differs. The estimator (12.42) is a better choice than the 2SLS formula for covariance matrix estimation as it takes advantage of the LIML estimator structure.</p>
</section>
<section id="functions-of-parameters" class="level2" data-number="12.20">
<h2 data-number="12.20" class="anchored" data-anchor-id="functions-of-parameters"><span class="header-section-number">12.20</span> Functions of Parameters</h2>
<p>Given the distribution theory in Theorems <span class="math inline">\(12.2\)</span> and <span class="math inline">\(12.3\)</span> it is straightforward to derive the asymptotic distribution of smooth nonlinear functions of the coefficient estimators.</p>
<p>Specifically, given a function <span class="math inline">\(r(\beta): \mathbb{R}^{k} \rightarrow \Theta \subset \mathbb{R}^{q}\)</span> we define the parameter <span class="math inline">\(\theta=r(\beta)\)</span>. Given <span class="math inline">\(\widehat{\beta}_{2 \text { sls }}\)</span> a natural estimator of <span class="math inline">\(\theta\)</span> is <span class="math inline">\(\widehat{\theta}_{2 \text { sls }}=r\left(\widehat{\beta}_{2 \text { sls }}\right)\)</span>.</p>
<p>Consistency follows from Theorem <span class="math inline">\(12.1\)</span> and the continuous mapping theorem.</p>
<p>Theorem 12.4 Under Assumptions <span class="math inline">\(12.1\)</span> and 7.3, as <span class="math inline">\(n \rightarrow \infty, \widehat{\theta}_{2 s l s} \underset{p}{\longrightarrow} \theta\)</span></p>
<p>If <span class="math inline">\(r(\beta)\)</span> is differentiable then an estimator of the asymptotic covariance matrix for <span class="math inline">\(\widehat{\theta}_{2 \text { sls }}\)</span> is</p>
<p><span class="math display">\[
\begin{aligned}
\widehat{\boldsymbol{V}}_{\theta} &amp;=\widehat{\boldsymbol{R}}^{\prime} \widehat{\boldsymbol{V}}_{\beta} \widehat{\boldsymbol{R}} \\
\widehat{\boldsymbol{R}} &amp;=\frac{\partial}{\partial \beta} r\left(\widehat{\beta}_{2 s l s}\right)^{\prime} .
\end{aligned}
\]</span></p>
<p>We similarly define the homoskedastic variance estimator as <span class="math inline">\(\widehat{\boldsymbol{V}}_{\theta}^{0}=\widehat{\boldsymbol{R}}^{\prime} \widehat{\boldsymbol{V}}_{\beta}^{0} \widehat{\boldsymbol{R}}\)</span>.</p>
<p>The asymptotic distribution theory follows from Theorems <span class="math inline">\(12.2\)</span> and <span class="math inline">\(12.3\)</span> and the delta method.</p>
<p>Theorem 12.5 Under Assumptions <span class="math inline">\(12.2\)</span> and <span class="math inline">\(7.3\)</span>, as <span class="math inline">\(n \rightarrow \infty\)</span>,</p>
<p><span class="math display">\[
\sqrt{n}\left(\widehat{\theta}_{2 s l s}-\theta\right) \underset{d}{\longrightarrow} \mathrm{N}\left(0, \boldsymbol{V}_{\theta}\right)
\]</span></p>
<p>and <span class="math inline">\(\widehat{\boldsymbol{V}}_{\theta} \underset{p}{\longrightarrow} \boldsymbol{V}_{\theta}\)</span> where <span class="math inline">\(\boldsymbol{V}_{\theta}=\boldsymbol{R}^{\prime} \boldsymbol{V}_{\beta} \boldsymbol{R}\)</span> and <span class="math inline">\(\boldsymbol{R}=\frac{\partial}{\partial \beta} \boldsymbol{r}(\beta)^{\prime}\)</span></p>
<p>When <span class="math inline">\(q=1\)</span>, a standard error for <span class="math inline">\(\widehat{\theta}_{2 \text { sls }}\)</span> is <span class="math inline">\(s\left(\widehat{\theta}_{2 \text { sls }}\right)=\sqrt{n^{-1} \widehat{\boldsymbol{V}}_{\theta}}\)</span>.</p>
<p>For example, let’s take the parameter estimates from the fifth column of Table 12.1, which are the 2SLS estimates with three endogenous regressors and four excluded instruments. Suppose we are interested in the return to experience, which depends on the level of experience. The estimated return at experience <span class="math inline">\(=10\)</span> is <span class="math inline">\(0.047-0.032 \times 2 \times 10 / 100=0.041\)</span> and its standard error is <span class="math inline">\(0.003\)</span>. This implies a <span class="math inline">\(4 %\)</span> increase in wages per year of experience and is precisely estimated. Or suppose we are interested in the level of experience at which the function maximizes. The estimate is <span class="math inline">\(50 \times 0.047 / 0.032=73\)</span>. This has a standard error of 249 . The large standard error implies that the estimate (73 years of experience) is without precision and is thus uninformative.</p>
</section>
<section id="hypothesis-tests" class="level2" data-number="12.21">
<h2 data-number="12.21" class="anchored" data-anchor-id="hypothesis-tests"><span class="header-section-number">12.21</span> Hypothesis Tests</h2>
<p>As in the previous section, for a given function <span class="math inline">\(r(\beta): \mathbb{R}^{k} \rightarrow \Theta \subset \mathbb{R}^{q}\)</span> we define the parameter <span class="math inline">\(\theta=r(\beta)\)</span> and consider tests of hypotheses of the form <span class="math inline">\(\mathbb{M}_{0}: \theta=\theta_{0}\)</span> against <span class="math inline">\(\mathbb{H}_{1}: \theta \neq \theta_{0}\)</span>. The Wald statistic for <span class="math inline">\(\mathbb{M}_{0}\)</span> is</p>
<p><span class="math display">\[
W=n\left(\widehat{\theta}-\theta_{0}\right)^{\prime} \widehat{\boldsymbol{V}}_{\widehat{\theta}}^{-1}\left(\widehat{\theta}-\theta_{0}\right) .
\]</span></p>
<p>From Theorem <span class="math inline">\(12.5\)</span> we deduce that <span class="math inline">\(W\)</span> is asymptotically chi-square distributed. Let <span class="math inline">\(G_{q}(u)\)</span> denote the <span class="math inline">\(\chi_{q}^{2}\)</span> distribution function.</p>
<p>Theorem 12.6 Under Assumption 12.2, Assumption 7.3, and <span class="math inline">\(\mathbb{H}_{0}\)</span>, then as <span class="math inline">\(n \rightarrow\)</span> <span class="math inline">\(\infty, W \underset{d}{\rightarrow} \chi_{q}^{2}\)</span>. For <span class="math inline">\(c\)</span> satisfying <span class="math inline">\(\alpha=1-G_{q}(c), \mathbb{P}\left[W&gt;c \mid \mathbb{M}_{0}\right] \longrightarrow \alpha\)</span> so the test “Reject <span class="math inline">\(\mathbb{M}_{0}\)</span> if <span class="math inline">\(W&gt;c\)</span>” has asymptotic size <span class="math inline">\(\alpha\)</span>.</p>
<p>In linear regression we often report the <span class="math inline">\(F\)</span> version of the Wald statistic (by dividing by degrees of freedom) and use the <span class="math inline">\(F\)</span> distribution for inference as this is justified in the normal sampling model. For 2SLS estimation, however, this is not done as there is no finite sample <span class="math inline">\(F\)</span> justification for the <span class="math inline">\(F\)</span> version of the Wald statistic.</p>
<p>To illustrate, once again let’s take the parameter estimates from the fifth column of Table <span class="math inline">\(12.1\)</span> and again consider the return to experience which is determined by the coefficients on experience and experience <span class="math inline">\(^{2} / 100\)</span>. Neither coefficient is statisticially significant at the <span class="math inline">\(5 %\)</span> level and it is unclear if the overall effect is statistically significant. We can assess this by testing the joint hypothesis that both coefficients are zero. The Wald statistic for this hypothesis is <span class="math inline">\(W=244\)</span> which is highly significant with an asymptotic p-value of <span class="math inline">\(0.0000\)</span>. Thus by examining the joint test in contrast to the individual tests is quite clear that experience has a non-zero effect.</p>
</section>
<section id="finite-sample-theory" class="level2" data-number="12.22">
<h2 data-number="12.22" class="anchored" data-anchor-id="finite-sample-theory"><span class="header-section-number">12.22</span> Finite Sample Theory</h2>
<p>In Chapter 5 we reviewed the rich exact distribution available for the linear regression model under the assumption of normal innovations. There is a similarly rich literature in econometrics for IV, 2SLS and LIML estimators. An excellent review of the theory, mostly developed in the 1970s and early 1980s, is provided by Peter Phillips (1983).</p>
<p>This theory was developed under the assumption that the structural error vector <span class="math inline">\(e\)</span> and reduced form error <span class="math inline">\(u_{2}\)</span> are multivariate normally distributed. Even though the errors are normal, IV-type estimators are nonlinear functions of these errors and are thus non-normally distributed. Formulae for the exact distributions have been derived but are unfortunately functions of model parameters and hence are not directly useful for finite sample inference.</p>
<p>One important implication of this literature is that even in this optimal context of exact normal innovations the finite sample distributions of the IV estimators are non-normal and the finite sample distributions of test statistics are not chi-squared. The normal and chi-squared approximations hold asymptotically but there is no reason to expect these approximations to be accurate in finite samples.</p>
<p>A second important result is that under the assumption of normal errors most of the estimators do not have finite moments in any finite sample. A clean statement concerning the existence of moments for the 2SLS estimator was obtained by Kinal (1980) for the case of joint normality. Let <span class="math inline">\(\widehat{\beta}_{2 s l s, 2}\)</span> be the 2SLS estimators of the coefficients on the endogeneous regressors.</p>
<p>Theorem <span class="math inline">\(12.7\)</span> If <span class="math inline">\((Y, X, Z)\)</span> are jointly normal, then for any <span class="math inline">\(r, \mathbb{E}\left\|\widehat{\beta}_{2 s l s, 2}\right\|^{r}&lt;\infty\)</span> if and only if <span class="math inline">\(r&lt;\ell_{2}-k_{2}+1\)</span>. This result states that in the just-identified case the IV estimator does not have any finite order integer moments. In the over-identified case the number of finite moments corresponds to the number of overidentifying restrictions <span class="math inline">\(\left(\ell_{2}-k_{2}\right)\)</span>. Thus if there is one over-identifying restriction 2 SLS has a finite expectation and if there are two over-identifying restrictions then the 2SLS estimator has a finite variance.</p>
<p>The LIML estimator has a more severe moment problem as it has no finite integer moments (Mariano, 1982) regardless of the number of over-identifying restrictions. Due to this lack of moments Fuller (1977) proposed the following modification of LIML. His estimator is</p>
<p><span class="math display">\[
\begin{aligned}
\widehat{\beta}_{\text {Fuller }} &amp;=\left(\boldsymbol{X}^{\prime}\left(\boldsymbol{I}_{n}-K \boldsymbol{M}_{\boldsymbol{Z}}\right) \boldsymbol{X}\right)^{-1}\left(\boldsymbol{X}^{\prime}\left(\boldsymbol{I}_{n}-K \boldsymbol{M}_{\boldsymbol{Z}}\right) \boldsymbol{Y}_{1}\right) \\
K &amp;=\widehat{\kappa}-\frac{C}{n-k}
\end{aligned}
\]</span></p>
<p>for some <span class="math inline">\(C \geq 1\)</span>. Fuller showed that his estimator has all moments finite under suitable conditions.</p>
<p>Hausman, Newey, Woutersen, Chao and Swanson (2012) propose an estimator they call HFUL which combines the ideas of JIVE and Fuller which has excellent finite sample properties.</p>
</section>
<section id="bootstrap-for-2sls" class="level2" data-number="12.23">
<h2 data-number="12.23" class="anchored" data-anchor-id="bootstrap-for-2sls"><span class="header-section-number">12.23</span> Bootstrap for 2SLS</h2>
<p>The standard bootstrap algorithm for IV, 2SLS, and GMM generates bootstrap samples by sampling the triplets <span class="math inline">\(\left(Y_{1 i}^{*}, X_{i}^{*}, Z_{i}^{*}\right)\)</span> independently and with replacement from the original sample <span class="math inline">\(\left\{\left(Y_{1 i}, X_{i}, Z_{i}\right): i=\right.\)</span> <span class="math inline">\(1, \ldots, n\}\)</span>. Sampling <span class="math inline">\(n\)</span> such observations and stacking into observation matrices <span class="math inline">\(\left(\boldsymbol{Y}_{1}^{*}, \boldsymbol{X}^{*}, \boldsymbol{Z}^{*}\right)\)</span>, the bootstrap 2SLS estimator is</p>
<p><span class="math display">\[
\widehat{\beta}_{2 \mathrm{sls}}^{*}=\left(\boldsymbol{X}^{* \prime} \boldsymbol{Z}^{*}\left(\boldsymbol{Z}^{* \prime} \boldsymbol{Z}^{*}\right)^{-1} \boldsymbol{Z}^{* \prime} \boldsymbol{X}^{*}\right)^{-1} \boldsymbol{X}^{* \prime} \boldsymbol{Z}^{*}\left(\boldsymbol{Z}^{* \prime} \boldsymbol{Z}^{*}\right)^{-1} \boldsymbol{Z}^{* \prime} \boldsymbol{Y}_{1}^{*}
\]</span></p>
<p>This is repeated <span class="math inline">\(B\)</span> times to create a sample of <span class="math inline">\(B\)</span> bootstrap draws. Given these draws bootstrap statistics can be calculated. This includes the bootstrap estimate of variance, standard errors, and confidence intervals, including percentile, <span class="math inline">\(\mathrm{BC}\)</span> percentile, <span class="math inline">\(\mathrm{BC}_{a}\)</span> and percentile-t.</p>
<p>We now show that the bootstrap estimator has the same asymptotic distribution as the sample estimator. For overidentified cases this demonstration requires a bit of extra care. This was first shown by Hahn (1996).</p>
<p>The sample observations satisfy the model <span class="math inline">\(Y_{1}=X^{\prime} \beta+e\)</span> with <span class="math inline">\(\mathbb{E}[Z e]=0\)</span>. The true value of <span class="math inline">\(\beta\)</span> in the population can be written as</p>
<p><span class="math display">\[
\beta=\left(\mathbb{E}\left[X Z^{\prime}\right] \mathbb{E}\left[Z Z^{\prime}\right]^{-1} \mathbb{E}\left[Z X^{\prime}\right]\right)^{-1} \mathbb{E}\left[X Z^{\prime}\right] \mathbb{E}\left[Z Z^{\prime}\right]^{-1} \mathbb{E}\left[Z Y_{1}\right]
\]</span></p>
<p>The true value in the bootstrap universe is obtained by replacing the population moments by the sample moments, which equals the 2SLS estimator</p>
<p><span class="math display">\[
\begin{aligned}
&amp;\left(\mathbb{E}^{*}\left[X^{*} Z^{* \prime}\right] \mathbb{E}^{*}\left[Z^{*} Z^{* \prime}\right]^{-1} \mathbb{E}^{*}\left[Z^{*} X^{* \prime}\right]\right)^{-1} \mathbb{E}^{*}\left[X^{*} Z^{* \prime}\right] \mathbb{E}^{*}\left[Z^{*} Z^{* \prime}\right]^{-1} \mathbb{E}^{*}\left[Z^{*} Y_{1}^{*}\right] \\
&amp;=\left(\left(\frac{1}{n} \boldsymbol{X}^{\prime} \boldsymbol{Z}\right)\left(\frac{1}{n} \boldsymbol{Z}^{\prime} \boldsymbol{Z}\right)^{-1}\left(\frac{1}{n} \boldsymbol{Z}^{\prime} \boldsymbol{X}\right)\right)^{-1}\left(\frac{1}{n} \boldsymbol{X}^{\prime} \boldsymbol{Z}\right)\left(\frac{1}{n} \boldsymbol{Z}^{\prime} \boldsymbol{Z}\right)^{-1}\left[\frac{1}{n} \boldsymbol{Z}^{\prime} \boldsymbol{Y}_{1}\right] \\
&amp;=\widehat{\beta}_{2 \text { sls }} .
\end{aligned}
\]</span></p>
<p>The bootstrap observations thus satisfy the equation <span class="math inline">\(Y_{1 i}^{*}=X_{i}^{* \prime} \widehat{\beta}_{2 s l s}+e_{i}^{*}\)</span>. In matrix notation for the sample this is</p>
<p><span class="math display">\[
\boldsymbol{Y}_{1}^{*}=\boldsymbol{X}^{* \prime} \widehat{\beta}_{2 \mathrm{sls}}+\boldsymbol{e}^{*} .
\]</span></p>
<p>Given a bootstrap triple <span class="math inline">\(\left(Y_{1 i}^{*}, X_{i}^{*}, Z_{i}^{*}\right)=\left(Y_{1 j}, X_{j}, Z_{j}\right)\)</span> for some observation <span class="math inline">\(j\)</span> the true bootstrap error is</p>
<p><span class="math display">\[
e_{i}^{*}=Y_{1 j}-X_{j}^{\prime} \widehat{\beta}_{2 s l s}=\widehat{e}_{j} .
\]</span></p>
<p>It follows that</p>
<p><span class="math display">\[
\mathbb{E}^{*}\left[Z^{*} e^{*}\right]=n^{-1} \boldsymbol{Z}^{\prime} \widehat{\boldsymbol{e}} .
\]</span></p>
<p>This is generally not equal to zero in the over-identified case.</p>
<p>This an an important complication. In over-identified models the true observations satisfy the population condition <span class="math inline">\(\mathbb{E}[Z e]=0\)</span> but in the bootstrap sample <span class="math inline">\(\mathbb{E}^{*}\left[Z^{*} e^{*}\right] \neq 0\)</span>. This means that to apply the central limit theorem to the bootstrap estimator we first have to recenter the moment condition. That is, (12.44) and the bootstrap CLT imply</p>
<p><span class="math display">\[
\frac{1}{\sqrt{n}}\left(\boldsymbol{Z}^{* \prime} \boldsymbol{e}^{*}-\boldsymbol{Z}^{\prime} \widehat{\boldsymbol{e}}\right)=\frac{1}{\sqrt{n}} \sum_{i=1}^{n}\left(Z_{i}^{*} e_{i}^{*}-\mathbb{E}^{*}\left[Z^{*} e^{*}\right]\right) \underset{d^{*}}{\longrightarrow} \mathrm{N}(0, \Omega)
\]</span></p>
<p>where</p>
<p><span class="math display">\[
\Omega=\mathbb{E}\left[Z Z^{\prime} e^{2}\right] .
\]</span></p>
<p>Using (12.43) we can normalize the bootstrap estimator as</p>
<p><span class="math display">\[
\begin{aligned}
\sqrt{n}\left(\widehat{\boldsymbol{\beta}}_{2 \mathrm{sls}}^{*}-\widehat{\boldsymbol{\beta}}_{2 \mathrm{sls}}\right) &amp;=\sqrt{n}\left(\boldsymbol{X}^{* \prime} \boldsymbol{Z}^{*}\left(\boldsymbol{Z}^{* \prime} \boldsymbol{Z}^{*}\right)^{-1} \boldsymbol{Z}^{* \prime} \boldsymbol{X}^{*}\right)^{-1} \boldsymbol{X}^{* \prime} \boldsymbol{Z}^{*}\left(\boldsymbol{Z}^{* \prime} \boldsymbol{Z}^{*}\right)^{-1} \boldsymbol{Z}^{* \prime} \boldsymbol{e}^{*} \\
&amp;=\left(\left(\frac{1}{n} \boldsymbol{X}^{* \prime} \boldsymbol{Z}^{*}\right)\left(\frac{1}{n} \boldsymbol{Z}^{* \prime} \boldsymbol{Z}^{*}\right)^{-1}\left(\frac{1}{n} \boldsymbol{Z}^{* \prime} \boldsymbol{X}^{*}\right)\right)^{-1} \\
&amp; \times\left(\frac{1}{n} \boldsymbol{X}^{* \prime} \boldsymbol{Z}^{*}\right)\left(\frac{1}{n} \boldsymbol{Z}^{* \prime} \boldsymbol{Z}^{*}\right)^{-1} \frac{1}{\sqrt{n}}\left(\boldsymbol{Z}^{* \prime} \boldsymbol{e}^{*}-\boldsymbol{Z}^{\prime} \widehat{\boldsymbol{e}}\right) \\
&amp;+\left(\left(\frac{1}{n} \boldsymbol{X}^{* \prime} \boldsymbol{Z}^{*}\right)\left(\frac{1}{n} \boldsymbol{Z}^{* \prime} \boldsymbol{Z}^{*}\right)^{-1}\left(\frac{1}{n} \boldsymbol{Z}^{* \prime} \boldsymbol{X}^{*}\right)\right)^{-1} \\
&amp; \times\left(\frac{1}{n} \boldsymbol{X}^{* \prime} \boldsymbol{Z}^{*}\right)\left(\frac{1}{n} \boldsymbol{Z}^{* \prime} \boldsymbol{Z}^{*}\right)^{-1}\left(\frac{1}{\sqrt{n}} \boldsymbol{Z}^{\prime} \widehat{\boldsymbol{e}}\right) .
\end{aligned}
\]</span></p>
<p>Using the bootstrap WLLN,</p>
<p><span class="math display">\[
\begin{aligned}
\frac{1}{n} \boldsymbol{X}^{* \prime} \boldsymbol{Z}^{*} &amp;=\frac{1}{n} \boldsymbol{X}^{\prime} \boldsymbol{Z}+o_{p}(1) \\
\frac{1}{n} \boldsymbol{Z}^{* \prime} \boldsymbol{Z}^{*} &amp;=\frac{1}{n} \boldsymbol{Z}^{\prime} \boldsymbol{Z}+o_{p}(1) .
\end{aligned}
\]</span></p>
<p>This implies (12.47) is equal to</p>
<p><span class="math display">\[
\sqrt{n}\left(\boldsymbol{X}^{\prime} \boldsymbol{Z}\left(\boldsymbol{Z}^{\prime} \boldsymbol{Z}\right)^{-1}\left(\boldsymbol{Z}^{\prime} \boldsymbol{X}\right)\right)^{-1} \boldsymbol{X}^{\prime} \boldsymbol{Z}\left(\boldsymbol{Z}^{\prime} \boldsymbol{Z}\right)^{-1} \boldsymbol{Z}^{\prime} \widehat{\boldsymbol{e}}+o_{p}(1)=0+o_{p}(1)
\]</span></p>
<p>The equality holds because the 2SLS first-order condition implies <span class="math inline">\(\boldsymbol{X}^{\prime} \boldsymbol{Z}\left(\boldsymbol{Z}^{\prime} \boldsymbol{Z}\right)^{-1} \boldsymbol{Z}^{\prime} \widehat{\boldsymbol{e}}=0\)</span>. Also, combined with (12.45) we see that (12.46) converges in bootstrap distribution to</p>
<p><span class="math display">\[
\left(\boldsymbol{Q}_{X Z} \boldsymbol{Q}_{Z Z}^{-1} \boldsymbol{Q}_{Z X}\right)^{-1} \boldsymbol{Q}_{X Z} \boldsymbol{Q}_{Z Z}^{-1} \mathrm{~N}(0, \Omega)=\mathrm{N}\left(0, \boldsymbol{V}_{\beta}\right)
\]</span></p>
<p>where <span class="math inline">\(\boldsymbol{V}_{\beta}\)</span> is the 2SLS asymptotic variance from Theorem 12.2. This is the asymptotic distribution of <span class="math inline">\(\sqrt{n}\left(\widehat{\beta}_{2 s l s}^{*}-\widehat{\beta}_{2 s l s}\right)\)</span></p>
<p>By standard calculations we can also show that bootstrap t-ratios are asymptotically normal. Theorem 12.8 Under Assumption 12.2, as <span class="math inline">\(n \rightarrow \infty\)</span></p>
<p><span class="math display">\[
\sqrt{n}\left(\widehat{\beta}_{2 s l s}^{*}-\widehat{\beta}_{2 s l s}\right) \underset{d^{*}}{\longrightarrow} \mathrm{N}\left(0, \boldsymbol{V}_{\beta}\right)
\]</span></p>
<p>where <span class="math inline">\(\boldsymbol{V}_{\beta}\)</span> is the <span class="math inline">\(2 \mathrm{SLS}\)</span> asymptotic variance from Theorem 12.2. Furthermore,</p>
<p><span class="math display">\[
T^{*}=\frac{\sqrt{n}\left(\widehat{\beta}_{2 s l s}^{*}-\widehat{\beta}_{2 s l s}\right)}{s\left(\widehat{\beta}_{2 \text { sls }}^{*}\right)} \underset{d^{*}}{\longrightarrow} \mathrm{N}(0,1) .
\]</span></p>
<p>This shows that percentile-type and percentile-t confidence intervals are asymptotically valid.</p>
<p>One might expect that the asymptotic refinement arguments extend to the <span class="math inline">\(\mathrm{BC}_{a}\)</span> and percentile-t methods but this does not appear to be the case. While <span class="math inline">\(\sqrt{n}\left(\widehat{\beta}_{2 \text { sls }}^{*}-\widehat{\beta}_{2 s l s}\right)\)</span> and <span class="math inline">\(\sqrt{n}\left(\widehat{\beta}_{2 s l s}-\beta\right)\)</span> have the same asymptotic distribution they differ in finite samples by an <span class="math inline">\(O_{p}\left(n^{-1 / 2}\right)\)</span> term. This means that they have distinct Edgeworth expansions. Consequently, unadjusted bootstrap methods will not achieve an asymptotic refinement.</p>
<p>An alternative suggested by Hall and Horowitz (1996) is to recenter the bootstrap 2SLS estimator so that it satisfies the correct orthogonality condition. Define</p>
<p><span class="math display">\[
\widehat{\beta}_{2 \text { sls }}^{* *}=\left(\boldsymbol{X}^{* \prime} \boldsymbol{Z}^{*}\left(\boldsymbol{Z}^{* \prime} \boldsymbol{Z}^{*}\right)^{-1} \boldsymbol{Z}^{* \prime} \boldsymbol{X}^{*}\right)^{-1} \boldsymbol{X}^{* \prime} \boldsymbol{Z}^{*}\left(\boldsymbol{Z}^{* \prime} \boldsymbol{Z}^{*}\right)^{-1}\left(\boldsymbol{Z}^{* \prime} \boldsymbol{Y}_{1}^{*}-\boldsymbol{Z}^{\prime} \widehat{\boldsymbol{e}}\right) .
\]</span></p>
<p>We can see that</p>
<p><span class="math display">\[
\begin{aligned}
\sqrt{n}\left(\widehat{\beta}_{2 \text { sls }}^{* *}-\widehat{\beta}_{2 \mathrm{sls}}\right) &amp;=\left(\frac{1}{n} \boldsymbol{X}^{* \prime} \boldsymbol{Z}^{*}\left(\frac{1}{n} \boldsymbol{Z}^{* \prime} \boldsymbol{Z}^{*}\right)^{-1} \frac{1}{n} \boldsymbol{Z}^{* \prime} \boldsymbol{X}^{*}\right)^{-1} \\
&amp; \times\left(\frac{1}{n} \boldsymbol{X}^{* \prime} \boldsymbol{Z}^{*}\right)\left(\frac{1}{n} \boldsymbol{Z}^{* \prime} \boldsymbol{Z}^{*}\right)^{-1}\left(\frac{1}{\sqrt{n}} \sum_{i=1}^{n}\left(Z_{i}^{*} e_{i}^{*}-\mathbb{E}^{*}\left[Z^{*} e^{*}\right]\right)\right)
\end{aligned}
\]</span></p>
<p>which converges to the <span class="math inline">\(\mathrm{N}\left(0, \boldsymbol{V}_{\beta}\right)\)</span> distribution without special handling. Hall and Horowitz (1996) show that percentile-t methods applied to <span class="math inline">\(\widehat{\beta}_{2 \text { sls }}^{* *}\)</span> achieve an asymptotic refinement and are thus preferred to the unadjusted bootstrap estimator.</p>
<p>This recentered estimator, however, is not the standard implementation of the bootstrap for 2SLS as used in empirical practice.</p>
</section>
<section id="the-peril-of-bootstrap-2sls-standard-errors" class="level2" data-number="12.24">
<h2 data-number="12.24" class="anchored" data-anchor-id="the-peril-of-bootstrap-2sls-standard-errors"><span class="header-section-number">12.24</span> The Peril of Bootstrap 2SLS Standard Errors</h2>
<p>It is tempting to use the bootstrap algorithm to estimate variance matrices and standard errors for the 2SLS estimator. In fact this is one of the most common uses of bootstrap methods in current econometric practice. Unfortunately this is an unjustified and ill-conceived idea and should not be done. In finite samples the 2SLS estimator may not have a finite second moment, meaning that bootstrap variance estimates are unstable and unreliable.</p>
<p>Theorem <span class="math inline">\(12.7\)</span> shows that under joint normality the 2SLS estimator will have a finite variance if and only if the number of overidentifying restrictions is two or larger. Thus for just-identified IV, and 2SLS with one degree of overidentification, the finite sample variance is infinite. The bootstrap will be attempting to estimate this value - infinity - and will yield nonsensical answers. When the observations are not jointly normal there is no finite sample theory (so it is possible that the finite sample variance is actually finite) but this is unknown and unverifiable. In overidentified settings when the number of overidentifying restrictions is two or larger the bootstrap can be applied for standard error estimation. However this is not the most common application of IV methods in econometric practice and thus should be viewed as the exception rather than the norm.</p>
<p>To understand what is going on consider the simplest case of a just-identified model with a single endogenous regressor and no included exogenous regressors. In this case the estimator can be written as a ratio of means</p>
<p><span class="math display">\[
\widehat{\beta}_{\mathrm{iv}}-\beta=\frac{\sum_{i=1}^{n} Z_{i} e_{i}}{\sum_{i=1}^{n} Z_{i} X_{i}} .
\]</span></p>
<p>Under joint normality of <span class="math inline">\((e, X)\)</span> this has a Cauchy-like distribution which does not possess any finite integer moments. The trouble is that the denominator can be either positive or negative, and arbitrarily close to zero. This means that the ratio can take arbitrarily large values.</p>
<p>To illustrate let us return to the basic Card IV wage regression from column 2 of Table <span class="math inline">\(12.1\)</span> which uses college as an instrument for education. We estimate this equation for the subsample of Black men which has <span class="math inline">\(n=703\)</span> observations, and focus on the coefficient for the return to education. The coefficient estimate is reported in Table 12.3, along with asymptotic, jackknife, and two bootstrap standard errors each calculated with 10,000 bootstrap replications.</p>
<p>Table 12.3: Instrumental Variable Return to Education for Black Men</p>
<table class="table">
<thead>
<tr class="header">
<th style="text-align: left;">Estimate</th>
<th><span class="math inline">\(0.11\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Asymptotic s.e.</td>
<td><span class="math inline">\((0.11)\)</span></td>
</tr>
<tr class="even">
<td style="text-align: left;">Jackknife s.e.</td>
<td><span class="math inline">\((0.11)\)</span></td>
</tr>
<tr class="odd">
<td style="text-align: left;">Bootstrap s.e. (standard)</td>
<td><span class="math inline">\((1.42)\)</span></td>
</tr>
<tr class="even">
<td style="text-align: left;">Bootstrap s.e. (repeat)</td>
<td><span class="math inline">\((4.79)\)</span></td>
</tr>
</tbody>
</table>
<p>The bootstrap standard errors are an order of magnitude larger than the asymptotic standard errors, and vary substantially across the bootstrap runs despite using 10,000 bootstrap replications. This indicates moment failure and unreliability of the bootstrap standard errors.</p>
<p>This is a strong message that bootstrap standard errors should not be computed for IV estimators. Instead, report percentile-type confidence intervals.</p>
</section>
<section id="clustered-dependence" class="level2" data-number="12.25">
<h2 data-number="12.25" class="anchored" data-anchor-id="clustered-dependence"><span class="header-section-number">12.25</span> Clustered Dependence</h2>
<p>In Section <span class="math inline">\(4.21\)</span> we introduced clustered dependence. We can also use the methods of clustered dependence for 2SLS estimation. Recall, the <span class="math inline">\(g^{t h}\)</span> cluster has the observations <span class="math inline">\(\boldsymbol{Y}_{g}=\left(Y_{1 g}, \ldots, Y_{n_{g} g}\right)^{\prime}, \boldsymbol{X}_{g}=\)</span> <span class="math inline">\(\left(X_{1 g}, \ldots, X_{n_{g} g}\right)^{\prime}\)</span>, and <span class="math inline">\(Z_{g}=\left(Z_{1 g}, \ldots, Z_{n_{g} g}\right)^{\prime}\)</span>. The structural equation for the <span class="math inline">\(g^{t h}\)</span> cluster can be written as the matrix system <span class="math inline">\(\boldsymbol{Y}_{g}=\boldsymbol{X}_{g} \beta+\boldsymbol{e}_{g}\)</span>. Using this notation the centered 2SLS estimator can be written as</p>
<p><span class="math display">\[
\begin{aligned}
\widehat{\beta}_{2 \text { sls }}-\beta &amp;=\left(\boldsymbol{X}^{\prime} \boldsymbol{Z}\left(\boldsymbol{Z}^{\prime} \boldsymbol{Z}\right)^{-1} \boldsymbol{Z}^{\prime} \boldsymbol{X}\right)^{-1} \boldsymbol{X}^{\prime} \boldsymbol{Z}\left(\boldsymbol{Z}^{\prime} \boldsymbol{Z}\right)^{-1} \boldsymbol{Z}^{\prime} \boldsymbol{e} \\
&amp;=\left(\boldsymbol{X}^{\prime} \boldsymbol{Z}\left(\boldsymbol{Z}^{\prime} \boldsymbol{Z}\right)^{-1} \boldsymbol{Z}^{\prime} \boldsymbol{X}\right)^{-1} \boldsymbol{X}^{\prime} \boldsymbol{Z}\left(\boldsymbol{Z}^{\prime} \boldsymbol{Z}\right)^{-1}\left(\sum_{g=1}^{G} \boldsymbol{Z}_{g}^{\prime} \boldsymbol{e}_{g}\right)
\end{aligned}
\]</span></p>
<p>The cluster-robust covariance matrix estimator for <span class="math inline">\(\widehat{\beta}_{2 s l s}\)</span> thus takes the form</p>
<p><span class="math display">\[
\widehat{\boldsymbol{V}}_{\beta}=\left(\boldsymbol{X}^{\prime} \boldsymbol{Z}\left(\boldsymbol{Z}^{\prime} \boldsymbol{Z}\right)^{-1} \boldsymbol{Z}^{\prime} \boldsymbol{X}\right)^{-1} \boldsymbol{X}^{\prime} \boldsymbol{Z}\left(\boldsymbol{Z}^{\prime} \boldsymbol{Z}\right)^{-1} \widehat{\boldsymbol{S}}\left(\boldsymbol{Z}^{\prime} \boldsymbol{Z}\right)^{-1} \boldsymbol{Z}^{\prime} \boldsymbol{X}\left(\boldsymbol{X}^{\prime} \boldsymbol{Z}\left(\boldsymbol{Z}^{\prime} \boldsymbol{Z}\right)^{-1} \boldsymbol{Z}^{\prime} \boldsymbol{X}\right)^{-1}
\]</span></p>
<p>with</p>
<p><span class="math display">\[
\widehat{\boldsymbol{S}}=\sum_{g=1}^{G} \boldsymbol{Z}_{g}^{\prime} \widehat{\boldsymbol{e}}_{g} \widehat{\boldsymbol{e}}_{g}^{\prime} \boldsymbol{Z}_{g}
\]</span></p>
<p>and the clustered residuals <span class="math inline">\(\widehat{\boldsymbol{e}}_{g}=\boldsymbol{Y}_{g}-\boldsymbol{X}_{g} \widehat{\beta}_{2 \text { sls }}\)</span></p>
<p>The difference between the heteroskedasticity-robust estimator and the cluster-robust estimator is the covariance estimator <span class="math inline">\(\widehat{\boldsymbol{S}}\)</span>.</p>
</section>
<section id="generated-regressors" class="level2" data-number="12.26">
<h2 data-number="12.26" class="anchored" data-anchor-id="generated-regressors"><span class="header-section-number">12.26</span> Generated Regressors</h2>
<p>The “two-stage” form of the 2SLS estimator is an example of what is called “estimation with generated regressors”. We say a regressor is a generated if it is an estimate of an idealized regressor or if it is a function of estimated parameters. Typically, a generated regressor <span class="math inline">\(\widehat{W}\)</span> is an estimate of an unobserved ideal regressor <span class="math inline">\(W\)</span>. As an estimate, <span class="math inline">\(\widehat{W}_{i}\)</span> is a function of the full sample not just observation <span class="math inline">\(i\)</span>. Hence it is not “i.i.d.” as it is dependent across observations which invalidates the conventional regression assumptions. Consequently, the sampling distribution of regression estimates is affected. Unless this is incorporated into our inference methods, covariance matrix estimates and standard errors will be incorrect.</p>
<p>The econometric theory of generated regressors was developed by Pagan (1984) for linear models and extended to nonlinear models and more general two-step estimators by Pagan (1986). Independently, similar results were obtained by Murphy and Topel (1985). Here we focus on the linear model:</p>
<p><span class="math display">\[
\begin{aligned}
Y &amp;=W^{\prime} \beta+v \\
W &amp;=\boldsymbol{A}^{\prime} Z \\
\mathbb{E}[Z v] &amp;=0 .
\end{aligned}
\]</span></p>
<p>The observables are <span class="math inline">\((Y, Z)\)</span>. We also have an estimate <span class="math inline">\(\widehat{\boldsymbol{A}}\)</span> of <span class="math inline">\(\boldsymbol{A}\)</span>.</p>
<p>Given <span class="math inline">\(\widehat{A}\)</span> we construct the estimate <span class="math inline">\(\widehat{W}_{i}=\widehat{A}^{\prime} Z_{i}\)</span> of <span class="math inline">\(W_{i}\)</span>, replace <span class="math inline">\(W_{i}\)</span> in (12.48) with <span class="math inline">\(\widehat{W}_{i}\)</span>, and then estimate <span class="math inline">\(\beta\)</span> by least squares, resulting in the estimator</p>
<p><span class="math display">\[
\widehat{\beta}=\left(\sum_{i=1}^{n} \widehat{W}_{i} \widehat{W}_{i}^{\prime}\right)^{-1}\left(\sum_{i=1}^{n} \widehat{W}_{i} Y_{i}\right)
\]</span></p>
<p>The regressors <span class="math inline">\(\widehat{W}_{i}\)</span> are called generated regressors. The properties of <span class="math inline">\(\widehat{\beta}\)</span> are different than least squares with i.i.d. observations because the generated regressors are themselves estimates.</p>
<p>This framework includes 2SLS as well as other common estimators. The 2SLS model can be written as (12.48) by looking at the reduced form equation (12.13), with <span class="math inline">\(W=\Gamma^{\prime} Z, A=\Gamma\)</span>, and <span class="math inline">\(\widehat{A}=\widehat{\Gamma}\)</span>.</p>
<p>The examples which motivated Pagan (1984) and Murphy and Topel (1985) emerged from the macroeconomics literature, in particular the work of Barro (1977) which examined the impact of inflation expectations and expectation errors on economic output. Let <span class="math inline">\(\pi\)</span> denote realized inflation and <span class="math inline">\(Z\)</span> be variables available to economic agents. A model of inflation expectations sets <span class="math inline">\(W=\mathbb{E}[\pi \mid Z]=\gamma^{\prime} Z\)</span> and a model of expectation error sets <span class="math inline">\(W=\pi-\mathbb{E}[\pi \mid Z]=\pi-\gamma^{\prime} Z\)</span>. Since expectations and errors are not observed they are replaced in applications with the fitted values <span class="math inline">\(\widehat{W}_{i}=\widehat{\gamma}^{\prime} Z_{i}\)</span> and residuals <span class="math inline">\(\widehat{W}_{i}=\pi_{i}-\widehat{\gamma}^{\prime} Z_{i}\)</span> where <span class="math inline">\(\widehat{\gamma}\)</span> is the coefficient from a regression of <span class="math inline">\(\pi\)</span> on <span class="math inline">\(Z\)</span>.</p>
<p>The generated regressor framework includes all of these examples.</p>
<p>The goal is to obtain a distributional approximation for <span class="math inline">\(\widehat{\beta}\)</span> in order to construct standard errors, confidence intervals, and tests. Start by substituting equation (12.48) into (12.49). We obtain</p>
<p><span class="math display">\[
\widehat{\beta}=\left(\sum_{i=1}^{n} \widehat{W}_{i} \widehat{W}_{i}^{\prime}\right)^{-1}\left(\sum_{i=1}^{n} \widehat{W}_{i}\left(W_{i}^{\prime} \beta+v_{i}\right)\right) .
\]</span></p>
<p>Next, substitute <span class="math inline">\(W_{i}^{\prime} \beta=\widehat{W}_{i}^{\prime} \beta+\left(W_{i}-\widehat{W}_{i}\right)^{\prime} \beta\)</span>. We obtain</p>
<p><span class="math display">\[
\widehat{\beta}-\beta=\left(\sum_{i=1}^{n} \widehat{W}_{i} \widehat{W}_{i}^{\prime}\right)^{-1}\left(\sum_{i=1}^{n} \widehat{W}_{i}\left(\left(W_{i}-\widehat{W}_{i}\right)^{\prime} \beta+v_{i}\right)\right) .
\]</span></p>
<p>Effectively, this shows that the distribution of <span class="math inline">\(\widehat{\beta}-\beta\)</span> has two random components, one due to conventional regression component and the second due to the generated regressor. Conventional variance estimators do not address this second component and thus will be biased.</p>
<p>Interestingly, the distribution in (12.50) dramatically simplifies in the special case that the “generated regressor term” <span class="math inline">\(\left(W_{i}-\widehat{W}_{i}\right)^{\prime} \beta\)</span> disappears. This occurs when the slope coefficients on the generated regressors are zero. To be specific, partition <span class="math inline">\(W_{i}=\left(W_{1 i}, W_{2 i}\right), \widehat{W}_{i}=\left(W_{1 i}, \widehat{W}_{2 i}\right)\)</span>, and <span class="math inline">\(\beta=\left(\beta_{1}, \beta_{2}\right)\)</span> so that <span class="math inline">\(W_{1 i}\)</span> are the conventional observed regressors and <span class="math inline">\(\widehat{W}_{2 i}\)</span> are the generated regressors. Then <span class="math inline">\(\left(W_{i}-\widehat{W}_{i}\right)^{\prime} \beta=\)</span> <span class="math inline">\(\left(W_{2 i}-\widehat{W}_{2 i}\right)^{\prime} \beta_{2}\)</span>. Thus if <span class="math inline">\(\beta_{2}=0\)</span> this term disappears. In this case (12.50) equals</p>
<p><span class="math display">\[
\widehat{\beta}-\widehat{\beta}=\left(\sum_{i=1}^{n} \widehat{W}_{i} \widehat{W}_{i}^{\prime}\right)^{-1}\left(\sum_{i=1}^{n} \widehat{W}_{i} v_{i}\right) .
\]</span></p>
<p>This is a dramatic simplification.</p>
<p>Furthermore, since <span class="math inline">\(\widehat{W}_{i}=\widehat{A}^{\prime} Z_{i}\)</span> we can write the estimator as a function of sample moments:</p>
<p><span class="math display">\[
\sqrt{n}(\widehat{\beta}-\beta)=\left(\widehat{\boldsymbol{A}}^{\prime}\left(\frac{1}{n} \sum_{i=1}^{n} Z_{i} Z_{i}^{\prime}\right) \widehat{\boldsymbol{A}}\right)^{-1} \widehat{\boldsymbol{A}}^{\prime}\left(\frac{1}{\sqrt{n}} \sum_{i=1}^{n} Z_{i} v_{i}\right)
\]</span></p>
<p>If <span class="math inline">\(\widehat{\boldsymbol{A}} \underset{p}{\longrightarrow} \boldsymbol{A}\)</span> we find from standard manipulations that <span class="math inline">\(\sqrt{n}(\widehat{\beta}-\beta) \underset{d}{\longrightarrow} \mathrm{N}\left(0, \boldsymbol{V}_{\beta}\right)\)</span> where</p>
<p><span class="math display">\[
\boldsymbol{V}_{\boldsymbol{\beta}}=\left(\boldsymbol{A}^{\prime} \mathbb{E}\left[Z Z^{\prime}\right] \boldsymbol{A}\right)^{-1}\left(\boldsymbol{A}^{\prime} \mathbb{E}\left[Z Z^{\prime} v^{2}\right] \boldsymbol{A}\right)\left(\boldsymbol{A}^{\prime} \mathbb{E}\left[Z Z^{\prime}\right] \boldsymbol{A}\right)^{-1} .
\]</span></p>
<p>The conventional asymptotic covariance matrix estimator for <span class="math inline">\(\widehat{\beta}\)</span> takes the form</p>
<p><span class="math display">\[
\widehat{\boldsymbol{V}}_{\beta}=\left(\frac{1}{n} \sum_{i=1}^{n} \widehat{W}_{i} \widehat{W}_{i}^{\prime}\right)^{-1}\left(\frac{1}{n} \sum_{i=1}^{n} \widehat{W}_{i} \widehat{W}_{i}^{\prime} \widehat{v}_{i}^{2}\right)\left(\frac{1}{n} \sum_{i=1}^{n} \widehat{W}_{i} \widehat{W}_{i}^{\prime}\right)^{-1}
\]</span></p>
<p>where <span class="math inline">\(\widehat{v}_{i}=Y_{i}-\widehat{W}_{i}^{\prime} \widehat{\beta}\)</span>. Under the given assumptions, <span class="math inline">\(\widehat{\boldsymbol{V}}_{\beta} \underset{p}{\longrightarrow} \boldsymbol{V}_{\beta}\)</span>. Thus inference using <span class="math inline">\(\widehat{\boldsymbol{V}}_{\beta}\)</span> is asymptotically valid. This is useful when we are interested in tests of <span class="math inline">\(\beta_{2}=0\)</span>. Often this is of major interest in applications.</p>
<p>To test <span class="math inline">\(\mathbb{M}_{0}: \beta_{2}=0\)</span> we partition <span class="math inline">\(\widehat{\beta}=\left(\widehat{\beta}_{1}, \widehat{\beta}_{2}\right)\)</span> and construct a conventional Wald statistic</p>
<p><span class="math display">\[
W=n \widehat{\beta}_{2}^{\prime}\left(\left[\widehat{\boldsymbol{V}}_{\beta}\right]_{22}\right)^{-1} \widehat{\beta}_{2} .
\]</span></p>
<p>Theorem 12.9 Take model (12.48) with <span class="math inline">\(\mathbb{E}\left[Y^{4}\right]&lt;\infty, \mathbb{E}\|Z\|^{4}&lt;\infty, A^{\prime} \mathbb{E}\left[Z Z^{\prime}\right] \boldsymbol{A}&gt;\)</span> <span class="math inline">\(0, \widehat{\boldsymbol{A}} \underset{p}{\longrightarrow} \boldsymbol{A}\)</span>, and <span class="math inline">\(\widehat{W}_{i}=\left(W_{1 i}, \widehat{W}_{2 i}\right)\)</span>. Under <span class="math inline">\(\mathbb{H}_{0}: \beta_{2}=0\)</span>, as <span class="math inline">\(n \rightarrow \infty, \sqrt{n}(\widehat{\beta}-\beta) \underset{d}{\longrightarrow}\)</span> <span class="math inline">\(\mathrm{N}\left(0, \boldsymbol{V}_{\beta}\right)\)</span> where <span class="math inline">\(\boldsymbol{V}_{\beta}\)</span> is given in (12.51). For <span class="math inline">\(\widehat{\boldsymbol{V}}_{\beta}\)</span> given in (12.52), <span class="math inline">\(\widehat{\boldsymbol{V}}_{\beta} \underset{p}{\longrightarrow} \boldsymbol{V}_{\beta}\)</span>. Furthermore, <span class="math inline">\(W \underset{d}{\longrightarrow} \chi_{q}^{2}\)</span> where <span class="math inline">\(q=\operatorname{dim}\left(\beta_{2}\right)\)</span>. For <span class="math inline">\(c\)</span> satisfying <span class="math inline">\(\alpha=1-G_{q}(c)\)</span>, <span class="math inline">\(\mathbb{P}\left[W&gt;c \mid \mathbb{M}_{0}\right] \rightarrow \alpha\)</span>, so the test “Reject <span class="math inline">\(\mathbb{M}_{0}\)</span> if <span class="math inline">\(W&gt;c\)</span>” has asymptotic size <span class="math inline">\(\alpha\)</span>. In the special case that <span class="math inline">\(\widehat{\boldsymbol{A}}=\boldsymbol{A}(\boldsymbol{X}, \boldsymbol{Z})\)</span> and <span class="math inline">\(v \mid X, Z \sim \mathrm{N}\left(0, \sigma^{2}\right)\)</span> there is a finite sample version of the previous result. Let <span class="math inline">\(W^{0}\)</span> be the Wald statistic constructed with a homoskedastic covariance matrix estimator, and let</p>
<p><span class="math display">\[
F=W / q
\]</span></p>
<p>be the the <span class="math inline">\(F\)</span> statistic, where <span class="math inline">\(q=\operatorname{dim}\left(\beta_{2}\right)\)</span></p>
<p>Theorem 12.10 Take model (12.48) with <span class="math inline">\(\widehat{\boldsymbol{A}}=\boldsymbol{A}(\boldsymbol{X}, \boldsymbol{Z}), v \mid X, Z \sim \mathrm{N}\left(0, \sigma^{2}\right)\)</span> and <span class="math inline">\(\widehat{W}=\left(W_{1}, \widehat{W}_{2}\right)\)</span>. Under <span class="math inline">\(\mathbb{M}_{0}: \beta_{2}=0\)</span>, t-statistics have exact <span class="math inline">\(\mathrm{N}(0,1)\)</span> distributions, and the <span class="math inline">\(F\)</span> statistic (12.53) has an exact <span class="math inline">\(F_{q, n-k}\)</span> distribution where <span class="math inline">\(q=\operatorname{dim}\left(\beta_{2}\right)\)</span> and <span class="math inline">\(k=\operatorname{dim}(\beta)\)</span></p>
<p>To summarize, in the model <span class="math inline">\(Y=W_{1}^{\prime} \beta_{1}+W_{2}^{\prime} \beta_{2}+v\)</span> where <span class="math inline">\(W_{2}\)</span> is not observed but replaced with an estimate <span class="math inline">\(\widehat{W}_{2}\)</span>, conventional significance tests for <span class="math inline">\(\mathbb{M}_{0}: \beta_{2}=0\)</span> are asymptotically valid without adjustment.</p>
<p>While this theory allows tests of <span class="math inline">\(\mathbb{M}_{0}: \beta_{2}=0\)</span> it unfortunately does not justify conventional standard errors or confidence intervals. For this, we need to work out the distribution without imposing the simplification <span class="math inline">\(\beta_{2}=0\)</span>. This often needs to be worked out case-by-case or by using methods based on the generalized method of moments to be introduced in Chapter 13. However, in one important set of examples it is straightforward to work out the asymptotic distribution.</p>
<p>For the remainder of this section we examine the setting where the estimators <span class="math inline">\(\widehat{A}\)</span> take a least squares form so for some <span class="math inline">\(\boldsymbol{X}\)</span> can be written as <span class="math inline">\(\widehat{\boldsymbol{A}}=\left(\boldsymbol{Z}^{\prime} \boldsymbol{Z}\right)^{-1}\left(\boldsymbol{Z}^{\prime} \boldsymbol{X}\right)\)</span>. Such estimators correspond to the multivariate projection model</p>
<p><span class="math display">\[
\begin{aligned}
X &amp;=\boldsymbol{A}^{\prime} Z+u \\
\mathbb{E}\left[Z u^{\prime}\right] &amp;=0 .
\end{aligned}
\]</span></p>
<p>This class of estimators includes 2SLS and the expectation model described above. We can write the matrix of generated regressors as <span class="math inline">\(\widehat{W}=Z \widehat{A}\)</span> and then (12.50) as</p>
<p><span class="math display">\[
\begin{aligned}
\widehat{\beta}-\beta &amp;=\left(\widehat{\boldsymbol{W}}^{\prime} \widehat{\boldsymbol{W}}\right)^{-1}\left(\widehat{\boldsymbol{W}}^{\prime}((\boldsymbol{W}-\widehat{\boldsymbol{W}}) \beta+\boldsymbol{v})\right) \\
&amp;=\left(\widehat{\boldsymbol{A}}^{\prime} \boldsymbol{Z}^{\prime} \boldsymbol{Z} \widehat{\boldsymbol{A}}\right)^{-1}\left(\widehat{\boldsymbol{A}}^{\prime} \boldsymbol{Z}^{\prime}\left(-\boldsymbol{Z}\left(\boldsymbol{Z}^{\prime} \boldsymbol{Z}\right)^{-1}\left(\boldsymbol{Z}^{\prime} \boldsymbol{U}\right) \beta+\boldsymbol{v}\right)\right) \\
&amp;=\left(\widehat{\boldsymbol{A}}^{\prime} \boldsymbol{Z}^{\prime} \boldsymbol{Z} \widehat{\boldsymbol{A}}\right)^{-1}\left(\widehat{\boldsymbol{A}}^{\prime} \boldsymbol{Z}^{\prime}(-\boldsymbol{U} \beta+\boldsymbol{v})\right) \\
&amp;=\left(\widehat{\boldsymbol{A}}^{\prime} \boldsymbol{Z}^{\prime} \boldsymbol{Z} \widehat{\boldsymbol{A}}\right)^{-1}\left(\widehat{\boldsymbol{A}}^{\prime} \boldsymbol{Z}^{\prime} \boldsymbol{e}\right)
\end{aligned}
\]</span></p>
<p>where</p>
<p><span class="math display">\[
e=v-u^{\prime} \beta=Y-X^{\prime} \beta .
\]</span></p>
<p>This estimator has the asymptotic distribution <span class="math inline">\(\sqrt{n}(\widehat{\beta}-\beta) \underset{d}{\longrightarrow} \mathrm{N}\left(0, V_{\beta}\right)\)</span> where</p>
<p><span class="math display">\[
\boldsymbol{V}_{\beta}=\left(\boldsymbol{A}^{\prime} \mathbb{E}\left[Z Z^{\prime}\right] \boldsymbol{A}\right)^{-1}\left(\boldsymbol{A}^{\prime} \mathbb{E}\left[Z Z^{\prime} e^{2}\right] \boldsymbol{A}\right)\left(\boldsymbol{A}^{\prime} \mathbb{E}\left[Z Z^{\prime}\right] \boldsymbol{A}\right)^{-1} .
\]</span></p>
<p>Under conditional homoskedasticity the covariance matrix simplifies to</p>
<p><span class="math display">\[
\boldsymbol{V}_{\boldsymbol{\beta}}=\left(\boldsymbol{A}^{\prime} \mathbb{E}\left[Z Z^{\prime}\right] \boldsymbol{A}\right)^{-1} \mathbb{E}\left[e^{2}\right] .
\]</span></p>
<p>An appropriate estimator of <span class="math inline">\(\boldsymbol{V}_{\beta}\)</span> is</p>
<p><span class="math display">\[
\begin{aligned}
\widehat{\boldsymbol{V}}_{\beta} &amp;=\left(\frac{1}{n} \widehat{\boldsymbol{W}}^{\prime} \widehat{\boldsymbol{W}}\right)^{-1}\left(\frac{1}{n} \sum_{i=1}^{n} \widehat{W}_{i} \widehat{W}_{i}^{\prime} \widehat{e}_{i}^{2}\right)\left(\frac{1}{n} \widehat{\boldsymbol{W}}^{\prime} \widehat{\boldsymbol{W}}\right)^{-1} \\
\widehat{e}_{i} &amp;=Y_{i}-X_{i}^{\prime} \widehat{\beta}
\end{aligned}
\]</span></p>
<p>Under the assumption of conditional homoskedasticity this can be simplified as usual.</p>
<p>This appears to be the usual covariance matrix estimator, but it is not because the least squares residuals <span class="math inline">\(\widehat{v}_{i}=Y_{i}-\widehat{W_{i}^{\prime}} \widehat{\beta}\)</span> have been replaced with <span class="math inline">\(\widehat{e}_{i}\)</span>. This is exactly the substitution made by the 2SLS covariance matrix formula. Indeed, the covariance matrix estimator <span class="math inline">\(\widehat{\boldsymbol{V}}_{\beta}\)</span> precisely equals (12.40).</p>
<p>Theorem 12.11 Take model (12.48) and (12.54) with <span class="math inline">\(\mathbb{E}\left[Y^{4}\right]&lt;\infty, \mathbb{E}\|Z\|^{4}&lt;\infty\)</span>, <span class="math inline">\(\boldsymbol{A}^{\prime} \mathbb{E}\left[Z Z^{\prime}\right] \boldsymbol{A}&gt;0\)</span>, and <span class="math inline">\(\widehat{\boldsymbol{A}}=\left(\boldsymbol{Z}^{\prime} \boldsymbol{Z}\right)^{-1}\left(\boldsymbol{Z}^{\prime} \boldsymbol{X}\right)\)</span>. As <span class="math inline">\(n \rightarrow \infty, \sqrt{n}(\widehat{\beta}-\beta) \underset{d}{\rightarrow} \mathrm{N}\left(0, \boldsymbol{V}_{\beta}\right)\)</span> where <span class="math inline">\(\boldsymbol{V}_{\beta}\)</span> is given in (12.56) with <span class="math inline">\(e\)</span> defined in (12.55). For <span class="math inline">\(\widehat{\boldsymbol{V}}_{\beta}\)</span> given in (12.57), <span class="math inline">\(\widehat{\boldsymbol{V}}_{\beta} \underset{p}{\longrightarrow} \boldsymbol{V}_{\beta}\)</span>.</p>
<p>Since the parameter estimators are asymptotically normal and the covariance matrix is consistently estimated, standard errors and test statistics constructed from <span class="math inline">\(\widehat{\boldsymbol{V}}_{\beta}\)</span> are asymptotically valid with conventional interpretations.</p>
<p>We now summarize the results of this section. In general, care needs to be exercised when estimating models with generated regressors. As a general rule, generated regressors and two-step estimation affect sampling distributions and variance matrices. An important simplication occurs for tests that the generated regressors have zero slopes. In this case conventional tests have conventional distributions, both asymptotically and in finite samples. Another important special case occurs when the generated regressors are least squares fitted values. In this case the asymptotic distribution takes a conventional form but the conventional residual needs to be replaced by one constructed with the forecasted variable. With this one modification asymptotic inference using the generated regressors is conventional.</p>
</section>
<section id="regression-with-expectation-errors" class="level2" data-number="12.27">
<h2 data-number="12.27" class="anchored" data-anchor-id="regression-with-expectation-errors"><span class="header-section-number">12.27</span> Regression with Expectation Errors</h2>
<p>In this section we examine a generated regressor model which includes expectation errors in the regression. This is an important class of generated regressor models and is relatively straightforward to characterize. The model is</p>
<p><span class="math display">\[
\begin{aligned}
Y &amp;=X^{\prime} \beta+u^{\prime} \alpha+v \\
W &amp;=\boldsymbol{A}^{\prime} Z \\
X &amp;=W+u \\
\mathbb{E}[Z v] &amp;=0 \\
\mathbb{E}[u v] &amp;=0 \\
\mathbb{E}\left[Z u^{\prime}\right] &amp;=0 .
\end{aligned}
\]</span></p>
<p>The observables are <span class="math inline">\((Y, X, Z)\)</span>. This model states that <span class="math inline">\(W\)</span> is the expectation of <span class="math inline">\(X\)</span> (or more generally, the projection of <span class="math inline">\(X\)</span> on <span class="math inline">\(Z\)</span> ) and <span class="math inline">\(u\)</span> is its expectation error. The model allows for exogenous regressors as in the standard IV model if they are listed in <span class="math inline">\(W, X\)</span>, and <span class="math inline">\(Z\)</span>. This model is used, for example, to decompose the effect of expectations from expectation errors. In some cases it is desired to include only the expectation error <span class="math inline">\(u\)</span>, not the expectation <span class="math inline">\(W\)</span>. This does not change the results described here.</p>
<p>The model is estimated as follows. First, <span class="math inline">\(\boldsymbol{A}\)</span> is estimated by multivariate least squares of <span class="math inline">\(X\)</span> on <span class="math inline">\(Z\)</span>, <span class="math inline">\(\widehat{\boldsymbol{A}}=\left(\boldsymbol{Z}^{\prime} \boldsymbol{Z}\right)^{-1}\left(\boldsymbol{Z}^{\prime} \boldsymbol{X}\right)\)</span>, which yields as by-products the fitted values <span class="math inline">\(\widehat{W}_{i}=\widehat{\boldsymbol{A}}^{\prime} Z_{i}\)</span> and residuals <span class="math inline">\(\widehat{u}_{i}=\widehat{X}_{i}-\widehat{W}_{i}\)</span>. Second, the coefficients are estimated by least squares of <span class="math inline">\(Y\)</span> on the fitted values <span class="math inline">\(\widehat{W}\)</span> and residuals <span class="math inline">\(\widehat{u}\)</span></p>
<p><span class="math display">\[
Y_{i}=\widehat{W}_{i}^{\prime} \widehat{\beta}+\widehat{u}_{i}^{\prime} \widehat{\alpha}+\widehat{v}_{i} .
\]</span></p>
<p>We now examine the asymptotic distributions of these estimators.</p>
<p>By the first-step regression <span class="math inline">\(\boldsymbol{Z}^{\prime} \widehat{\boldsymbol{U}}=0, \widehat{\boldsymbol{W}}^{\prime} \widehat{\boldsymbol{U}}=0\)</span> and <span class="math inline">\(\boldsymbol{W}^{\prime} \widehat{\boldsymbol{U}}=0\)</span>. This means that <span class="math inline">\(\widehat{\beta}\)</span> and <span class="math inline">\(\widehat{\alpha}\)</span> can be computed separately. Notice that</p>
<p><span class="math display">\[
\widehat{\beta}=\left(\widehat{\boldsymbol{W}}^{\prime} \widehat{\boldsymbol{W}}\right)^{-1} \widehat{\boldsymbol{W}}^{\prime} \boldsymbol{Y}
\]</span></p>
<p>and</p>
<p><span class="math display">\[
\boldsymbol{Y}=\widehat{\boldsymbol{W}} \beta+\boldsymbol{U} \alpha+(\boldsymbol{W}-\widehat{\boldsymbol{W}}) \beta+\boldsymbol{v} .
\]</span></p>
<p>Substituting, using <span class="math inline">\(\widehat{\boldsymbol{W}}^{\prime} \widehat{\boldsymbol{U}}=0\)</span> and <span class="math inline">\(\boldsymbol{W}-\widehat{\boldsymbol{W}}=-\boldsymbol{Z}\left(\boldsymbol{Z}^{\prime} \boldsymbol{Z}\right)^{-1} \boldsymbol{Z}^{\prime} \boldsymbol{U}\)</span> we find</p>
<p><span class="math display">\[
\begin{aligned}
\widehat{\beta}-\beta &amp;=\left(\widehat{\boldsymbol{W}}^{\prime} \widehat{\boldsymbol{W}}\right)^{-1} \widehat{\boldsymbol{W}}^{\prime}(\boldsymbol{U} \alpha+(\boldsymbol{W}-\widehat{\boldsymbol{W}}) \beta+\boldsymbol{v}) \\
&amp;=\left(\widehat{\boldsymbol{A}}^{\prime} \boldsymbol{Z}^{\prime} \boldsymbol{Z} \widehat{\boldsymbol{A}}\right)^{-1} \widehat{\boldsymbol{A}}^{\prime} \boldsymbol{Z}^{\prime}(\boldsymbol{U} \alpha-\boldsymbol{U} \beta+\boldsymbol{v}) \\
&amp;=\left(\widehat{\boldsymbol{A}}^{\prime} \boldsymbol{Z}^{\prime} \boldsymbol{Z} \widehat{\boldsymbol{A}}\right)^{-1} \widehat{\boldsymbol{A}}^{\prime} \boldsymbol{Z}^{\prime} \boldsymbol{e}
\end{aligned}
\]</span></p>
<p>where</p>
<p><span class="math display">\[
e_{i}=v_{i}+u_{i}^{\prime}(\alpha-\beta)=Y_{i}-X_{i}^{\prime} \beta .
\]</span></p>
<p>We also find</p>
<p><span class="math display">\[
\widehat{\alpha}=\left(\widehat{\boldsymbol{U}}^{\prime} \widehat{\boldsymbol{U}}\right)^{-1} \widehat{\boldsymbol{U}}^{\prime} \boldsymbol{Y} .
\]</span></p>
<p>Since <span class="math inline">\(\widehat{\boldsymbol{U}}^{\prime} \boldsymbol{W}=0, \boldsymbol{U}-\widehat{\boldsymbol{U}}=\boldsymbol{Z}\left(\boldsymbol{Z}^{\prime} \boldsymbol{Z}\right)^{-1} \boldsymbol{Z}^{\prime} \boldsymbol{U}\)</span> and <span class="math inline">\(\widehat{\boldsymbol{U}}^{\prime} \boldsymbol{Z}=0\)</span> then</p>
<p><span class="math display">\[
\begin{aligned}
\widehat{\alpha}-\alpha &amp;=\left(\widehat{\boldsymbol{U}}^{\prime} \widehat{\boldsymbol{U}}\right)^{-1} \widehat{\boldsymbol{U}}^{\prime}(\boldsymbol{W} \beta+(\boldsymbol{U}-\widehat{\boldsymbol{U}}) \alpha+\boldsymbol{v}) \\
&amp;=\left(\widehat{\boldsymbol{U}}^{\prime} \widehat{\boldsymbol{U}}\right)^{-1} \widehat{\boldsymbol{U}}^{\prime} \boldsymbol{v}
\end{aligned}
\]</span></p>
<p>Together, we establish the following distributional result. Theorem 12.12 For the model and estimators described in this section, with <span class="math inline">\(\mathbb{E}\left[Y^{4}\right]&lt;\infty, \mathbb{E}\|Z\|^{4}&lt;\infty, \mathbb{E}\|X\|^{4}&lt;\infty, A^{\prime} \mathbb{E}\left[Z Z^{\prime}\right] A&gt;0\)</span>, and <span class="math inline">\(\mathbb{E}\left[u u^{\prime}\right]&gt;0\)</span>, as <span class="math inline">\(n \rightarrow \infty\)</span></p>
<p><span class="math display">\[
\sqrt{n}\left(\begin{array}{c}
\widehat{\beta}-\beta \\
\widehat{\alpha}-\alpha
\end{array}\right) \underset{d}{\longrightarrow} \mathrm{N}(0, \boldsymbol{V})
\]</span></p>
<p>where</p>
<p><span class="math display">\[
\boldsymbol{V}=\left(\begin{array}{ll}
\boldsymbol{V}_{\beta \beta} &amp; \boldsymbol{V}_{\beta \alpha} \\
\boldsymbol{V}_{\alpha \beta} &amp; \boldsymbol{V}_{\alpha \alpha}
\end{array}\right)
\]</span></p>
<p>and</p>
<p><span class="math display">\[
\begin{aligned}
\boldsymbol{V}_{\beta \beta} &amp;=\left(\boldsymbol{A}^{\prime} \mathbb{E}\left[Z Z^{\prime}\right] \boldsymbol{A}\right)^{-1}\left(\boldsymbol{A}^{\prime} \mathbb{E}\left[Z Z^{\prime} e^{2}\right] \boldsymbol{A}\right)\left(\boldsymbol{A}^{\prime} \mathbb{E}\left[Z Z^{\prime}\right] \boldsymbol{A}\right)^{-1} \\
\boldsymbol{V}_{\alpha \beta} &amp;=\left(\mathbb{E}\left[u u^{\prime}\right]\right)^{-1}\left(\mathbb{E}\left[u Z^{\prime} e v\right] \boldsymbol{A}\right)\left(\boldsymbol{A}^{\prime} \mathbb{E}\left[Z Z^{\prime}\right] \boldsymbol{A}\right)^{-1} \\
\boldsymbol{V}_{\alpha \alpha} &amp;=\left(\mathbb{E}\left[u u^{\prime}\right]\right)^{-1} \mathbb{E}\left[u u^{\prime} v^{2}\right]\left(\mathbb{E}\left[u u^{\prime}\right]\right)^{-1}
\end{aligned}
\]</span></p>
<p>The asymptotic covariance matrix is estimated by</p>
<p><span class="math display">\[
\begin{aligned}
&amp;\widehat{\boldsymbol{V}}_{\beta \beta}=\left(\frac{1}{n} \widehat{\boldsymbol{W}}^{\prime} \widehat{\boldsymbol{W}}\right)^{-1}\left(\frac{1}{n} \sum_{i=1}^{n} \widehat{W}_{i} \widehat{W}_{i}^{\prime} \widehat{e}_{i}^{2}\right)\left(\frac{1}{n} \widehat{\boldsymbol{W}}^{\prime} \widehat{\boldsymbol{W}}\right)^{-1} \\
&amp;\widehat{\boldsymbol{V}}_{\alpha \beta}=\left(\frac{1}{n} \widehat{\boldsymbol{U}}^{\prime} \widehat{\boldsymbol{U}}\right)^{-1}\left(\frac{1}{n} \sum_{i=1}^{n} \widehat{u}_{i} \widehat{W}_{i}^{\prime} \widehat{e}_{i} \widehat{v}_{i}\right)\left(\frac{1}{n} \widehat{\boldsymbol{W}}^{\prime} \widehat{\boldsymbol{W}}\right)^{-1} \\
&amp;\widehat{\boldsymbol{V}}_{\alpha \alpha}=\left(\frac{1}{n} \widehat{\boldsymbol{U}}^{\prime} \widehat{\boldsymbol{U}}\right)^{-1}\left(\frac{1}{n} \sum_{i=1}^{n} \widehat{U}_{i} \widehat{U}_{i}^{\prime} \widehat{v}_{i}^{2}\right)\left(\frac{1}{n} \widehat{\boldsymbol{U}}^{\prime} \widehat{\boldsymbol{U}}\right)^{-1}
\end{aligned}
\]</span></p>
<p>where</p>
<p><span class="math display">\[
\begin{aligned}
\widehat{W}_{i} &amp;=\widehat{A}^{\prime} Z_{i} \\
\widehat{u}_{i} &amp;=\widehat{X}_{i}-\widehat{W}_{i} \\
\widehat{e}_{i} &amp;=Y_{i}-X_{i}^{\prime} \widehat{\beta} \\
\widehat{v}_{i} &amp;=Y_{i}-\widehat{W}_{i}^{\prime} \widehat{\beta}-\widehat{u}_{i}^{\prime} \widehat{\alpha} .
\end{aligned}
\]</span></p>
<p>Under conditional homoskedasticity, specifically</p>
<p><span class="math display">\[
\mathbb{E}\left[\left(\begin{array}{cc}
e_{i}^{2} &amp; e_{i} v_{i} \\
e_{i} v_{i} &amp; v_{i}^{2}
\end{array}\right) \mid Z_{i}\right]=\boldsymbol{C}
\]</span></p>
<p>then <span class="math inline">\(\boldsymbol{V}_{\alpha \beta}=0\)</span> and the coefficient estimates <span class="math inline">\(\widehat{\beta}\)</span> and <span class="math inline">\(\widehat{\alpha}\)</span> are asymptotically independent. The variance components also simplify to</p>
<p><span class="math display">\[
\begin{aligned}
\boldsymbol{V}_{\beta \beta} &amp;=\left(\boldsymbol{A}^{\prime} \mathbb{E}\left[Z Z^{\prime}\right] \boldsymbol{A}\right)^{-1} \mathbb{E}\left[e_{i}^{2}\right] \\
\boldsymbol{V}_{\alpha \alpha} &amp;=\left(\mathbb{E}\left[u u^{\prime}\right]\right)^{-1} \mathbb{E}\left[v^{2}\right] .
\end{aligned}
\]</span></p>
<p>In this case we have the covariance matrix estimators</p>
<p><span class="math display">\[
\begin{aligned}
&amp;\widehat{\boldsymbol{V}}_{\beta \beta}^{0}=\left(\frac{1}{n} \widehat{\boldsymbol{W}}^{\prime} \widehat{\boldsymbol{W}}\right)^{-1}\left(\frac{1}{n} \sum_{i=1}^{n} \widehat{e}_{i}^{2}\right) \\
&amp;\widehat{\boldsymbol{V}}_{\alpha \alpha}^{0}=\left(\frac{1}{n} \widehat{\boldsymbol{U}}^{\prime} \widehat{\boldsymbol{U}}\right)^{-1}\left(\frac{1}{n} \sum_{i=1}^{n} \widehat{v}_{i}^{2}\right)
\end{aligned}
\]</span></p>
<p>and <span class="math inline">\(\widehat{\boldsymbol{V}}_{\alpha \beta}^{0}=0\)</span></p>
</section>
<section id="control-function-regression" class="level2" data-number="12.28">
<h2 data-number="12.28" class="anchored" data-anchor-id="control-function-regression"><span class="header-section-number">12.28</span> Control Function Regression</h2>
<p>In this section we present an alternative way of computing the 2SLS estimator by least squares. It is useful in nonlinear contexts, and also in the linear model to construct tests for endogeneity.</p>
<p>The structural and reduced form equations for the standard IV model are</p>
<p><span class="math display">\[
\begin{aligned}
Y &amp;=X_{1}^{\prime} \beta_{1}+X_{2}^{\prime} \beta_{2}+e \\
X_{2} &amp;=\Gamma_{12}^{\prime} Z_{1}+\Gamma_{22}^{\prime} Z_{2}+u_{2} .
\end{aligned}
\]</span></p>
<p>Since the instrumental variable assumption specifies that <span class="math inline">\(\mathbb{E}[Z e]=0, X_{2}\)</span> is endogenous (correlated with <span class="math inline">\(e)\)</span> if <span class="math inline">\(u_{2}\)</span> and <span class="math inline">\(e\)</span> are correlated. We can therefore consider the linear projection of <span class="math inline">\(e\)</span> on <span class="math inline">\(u_{2}\)</span></p>
<p><span class="math display">\[
\begin{aligned}
e &amp;=u_{2}^{\prime} \alpha+v \\
\alpha &amp;=\left(\mathbb{E}\left[u_{2} u_{2}^{\prime}\right]\right)^{-1} \mathbb{E}\left[u_{2} e\right] \\
\mathbb{E}\left[u_{2} v\right] &amp;=0 .
\end{aligned}
\]</span></p>
<p>Substituting this into the structural form equation we find</p>
<p><span class="math display">\[
\begin{aligned}
Y &amp;=X_{1}^{\prime} \beta_{1}+X_{2}^{\prime} \beta_{2}+u_{2}^{\prime} \alpha+v \\
\mathbb{E}\left[X_{1} v\right] &amp;=0 \\
\mathbb{E}\left[X_{2} v\right] &amp;=0 \\
\mathbb{E}\left[u_{2} v\right] &amp;=0 .
\end{aligned}
\]</span></p>
<p>Notice that <span class="math inline">\(X_{2}\)</span> is uncorrelated with <span class="math inline">\(v\)</span>. This is because <span class="math inline">\(X_{2}\)</span> is correlated with <span class="math inline">\(e\)</span> only through <span class="math inline">\(u_{2}\)</span>, and <span class="math inline">\(v\)</span> is the error after <span class="math inline">\(e\)</span> has been projected orthogonal to <span class="math inline">\(u_{2}\)</span>.</p>
<p>If <span class="math inline">\(u_{2}\)</span> were observed we could then estimate (12.59) by least squares. Since it is not observed we estimate it by the reduced-form residual <span class="math inline">\(\widehat{u}_{2 i}=X_{2 i}-\widehat{\Gamma}_{12}^{\prime} Z_{1 i}-\widehat{\Gamma}_{22}^{\prime} Z_{2 i}\)</span>. Then the coefficients <span class="math inline">\(\left(\beta_{1}, \beta_{2}, \alpha\right)\)</span> can be estimated by least squares of <span class="math inline">\(Y\)</span> on <span class="math inline">\(\left(X_{1}, X_{2}, \widehat{u}_{2}\right)\)</span>. We can write this as</p>
<p><span class="math display">\[
Y_{i}=X_{i}^{\prime} \widehat{\beta}+\widehat{u}_{2 i}^{\prime} \widehat{\alpha}+\widehat{v}_{i}
\]</span></p>
<p>or in matrix notation as</p>
<p><span class="math display">\[
\boldsymbol{Y}=\boldsymbol{X} \widehat{\beta}+\widehat{\boldsymbol{U}}_{2} \widehat{\alpha}+\widehat{\boldsymbol{v}} .
\]</span></p>
<p>This turns out to be an alternative algebraic expression for the 2SLS estimator.</p>
<p>Indeed, we now show that <span class="math inline">\(\widehat{\beta}=\widehat{\beta}_{2 s l s}\)</span>. First, note that the reduced form residual can be written as</p>
<p><span class="math display">\[
\widehat{\boldsymbol{U}}_{2}=\left(\boldsymbol{I}_{n}-\boldsymbol{P}_{\boldsymbol{Z}}\right) \boldsymbol{X}_{2}
\]</span></p>
<p>where <span class="math inline">\(\boldsymbol{P}_{\boldsymbol{Z}}\)</span> is defined in (12.30). By the FWL representation</p>
<p><span class="math display">\[
\widehat{\beta}=\left(\widetilde{\boldsymbol{X}}^{\prime} \widetilde{\boldsymbol{X}}\right)^{-1}\left(\widetilde{\boldsymbol{X}}^{\prime} \boldsymbol{Y}\right)
\]</span></p>
<p>where <span class="math inline">\(\widetilde{\boldsymbol{X}}=\left[\widetilde{\boldsymbol{X}}_{1}, \widetilde{\boldsymbol{X}}_{2}\right]\)</span> with</p>
<p><span class="math display">\[
\widetilde{\boldsymbol{X}}_{1}=\boldsymbol{X}_{1}-\widehat{\boldsymbol{U}}_{2}\left(\widehat{\boldsymbol{U}}_{2}^{\prime} \widehat{\boldsymbol{U}}_{2}\right)^{-1} \widehat{\boldsymbol{U}}_{2}^{\prime} \boldsymbol{X}_{1}=\boldsymbol{X}_{1}
\]</span></p>
<p>(since <span class="math inline">\(\left.\widehat{\boldsymbol{U}}_{2}^{\prime} \boldsymbol{X}_{1}=0\right)\)</span> and</p>
<p><span class="math display">\[
\begin{aligned}
\widetilde{\boldsymbol{X}}_{2} &amp;=\boldsymbol{X}_{2}-\widehat{\boldsymbol{U}}_{2}\left(\widehat{\boldsymbol{U}}_{2}^{\prime} \widehat{\boldsymbol{U}}_{2}\right)^{-1} \widehat{\boldsymbol{U}}_{2}^{\prime} \boldsymbol{X}_{2} \\
&amp;=\boldsymbol{X}_{2}-\widehat{\boldsymbol{U}}_{2}\left(\boldsymbol{X}_{2}^{\prime}\left(\boldsymbol{I}_{n}-\boldsymbol{P}_{\boldsymbol{Z}}\right) \boldsymbol{X}_{2}\right)^{-1} \boldsymbol{X}_{2}^{\prime}\left(\boldsymbol{I}_{n}-\boldsymbol{P}_{\boldsymbol{Z}}\right) \boldsymbol{X}_{2} \\
&amp;=\boldsymbol{X}_{2}-\widehat{\boldsymbol{U}}_{2} \\
&amp;=\boldsymbol{P}_{\boldsymbol{Z}} \boldsymbol{X}_{2} .
\end{aligned}
\]</span></p>
<p>Thus <span class="math inline">\(\tilde{\boldsymbol{X}}=\left[\boldsymbol{X}_{1}, \boldsymbol{P}_{\boldsymbol{Z}} \boldsymbol{X}_{2}\right]=\boldsymbol{P}_{\boldsymbol{Z}} \boldsymbol{X}\)</span>. Substituted into (12.61) we find</p>
<p><span class="math display">\[
\widehat{\beta}=\left(\boldsymbol{X}^{\prime} \boldsymbol{P}_{\boldsymbol{Z}} \boldsymbol{X}\right)^{-1}\left(\boldsymbol{X}^{\prime} \boldsymbol{P}_{\boldsymbol{Z}} \boldsymbol{Y}\right)=\widehat{\beta}_{2 \text { sls }}
\]</span></p>
<p>which is (12.31) as claimed.</p>
<p>Again, what we have found is that OLS estimation of equation (12.60) yields algebraically the 2SLS estimator <span class="math inline">\(\widehat{\beta}_{2 \text { sls }}\)</span>.</p>
<p>We now consider the distribution of the control function estimator <span class="math inline">\((\widehat{\beta}, \widehat{\alpha})\)</span>. It is a generated regression model, and in fact is covered by the model examined in Section <span class="math inline">\(12.27\)</span> after a slight reparametrization. Let <span class="math inline">\(W=\bar{\Gamma}^{\prime} Z\)</span>. Note <span class="math inline">\(u=X-W\)</span>. Then the main equation (12.59) can be written as <span class="math inline">\(Y=W^{\prime} \beta+u_{2}^{\prime} \gamma+v\)</span> where <span class="math inline">\(\gamma=\alpha+\beta_{2}\)</span>. This is the model in Section 12.27.</p>
<p>Set <span class="math inline">\(\widehat{\gamma}=\widehat{\alpha}+\widehat{\beta}_{2}\)</span>. It follows from (12.58) that as <span class="math inline">\(n \rightarrow \infty\)</span> we have the joint distribution</p>
<p><span class="math display">\[
\sqrt{n}\left(\begin{array}{c}
\widehat{\beta}_{2}-\beta_{2} \\
\widehat{\gamma}-\gamma
\end{array}\right) \vec{d} \mathrm{~N}(0, \boldsymbol{V})
\]</span></p>
<p>where</p>
<p><span class="math display">\[
\begin{aligned}
\boldsymbol{V}=\left(\begin{array}{ll}
\boldsymbol{V}_{22} &amp; \boldsymbol{V}_{2 \gamma} \\
\boldsymbol{V}_{\gamma 2} &amp; \boldsymbol{V}_{\gamma \gamma}
\end{array}\right) \\
\boldsymbol{V}_{22} &amp;=\left[\left(\bar{\Gamma}^{\prime} \mathbb{E}\left[Z Z^{\prime}\right] \bar{\Gamma}\right)^{-1} \bar{\Gamma}^{\prime} \mathbb{E}\left[Z Z^{\prime} e^{2}\right] \bar{\Gamma}\left(\bar{\Gamma}^{\prime} \mathbb{E}\left[Z Z^{\prime}\right] \bar{\Gamma}\right)^{-1}\right]_{22} \\
\boldsymbol{V}_{\gamma 2} &amp;=\left[\left(\mathbb{E}\left[u_{2} u_{2}^{\prime}\right]\right)^{-1} \mathbb{E}\left[u Z^{\prime} e v\right] \bar{\Gamma}\left(\bar{\Gamma}^{\prime} \mathbb{E}\left[Z Z^{\prime}\right] \bar{\Gamma}\right)^{-1}\right]_{\cdot 2} \\
\boldsymbol{V}_{\gamma \gamma} &amp;=\left(\mathbb{E}\left[u_{2} u_{2}^{\prime}\right]\right)^{-1} \mathbb{E}\left[u_{2} u_{2}^{\prime} v^{2}\right]\left(\mathbb{E}\left[u_{2} u_{2}^{\prime}\right]\right)^{-1} \\
e &amp;=Y-X^{\prime} \beta .
\end{aligned}
\]</span></p>
<p>The asymptotic distribution of <span class="math inline">\(\widehat{\gamma}=\widehat{\alpha}-\widehat{\beta}_{2}\)</span> can be deduced.</p>
<p>Theorem 12.13 If <span class="math inline">\(\mathbb{E}\left[Y^{4}\right]&lt;\infty, \mathbb{E}\|Z\|^{4}&lt;\infty, \mathbb{E}\|X\|^{4}&lt;\infty, A^{\prime} \mathbb{E}\left[Z Z^{\prime}\right] A&gt;0\)</span>, and <span class="math inline">\(\mathbb{E}\left[u u^{\prime}\right]&gt;0\)</span>, as <span class="math inline">\(n \rightarrow \infty\)</span></p>
<p><span class="math display">\[
\sqrt{n}(\widehat{\alpha}-\alpha) \underset{d}{\longrightarrow} \mathrm{N}\left(0, \boldsymbol{V}_{\alpha}\right)
\]</span></p>
<p>where</p>
<p><span class="math display">\[
\boldsymbol{V}_{\alpha}=\boldsymbol{V}_{22}+\boldsymbol{V}_{\gamma \gamma}-\boldsymbol{V}_{\gamma 2}-\boldsymbol{V}_{\gamma 2}^{\prime} .
\]</span></p>
<p>Under conditional homoskedasticity we have the important simplifications</p>
<p><span class="math display">\[
\begin{aligned}
\boldsymbol{V}_{22} &amp;=\left[\left(\bar{\Gamma}^{\prime} \mathbb{E}\left[Z Z^{\prime}\right] \bar{\Gamma}\right)^{-1}\right]_{22} \mathbb{E}\left[e^{2}\right] \\
\boldsymbol{V}_{\gamma \gamma} &amp;=\left(\mathbb{E}\left[u_{2} u_{2}^{\prime}\right]\right)^{-1} \mathbb{E}\left[v^{2}\right] \\
\boldsymbol{V}_{\gamma 2} &amp;=0 \\
\boldsymbol{V}_{\alpha} &amp;=\boldsymbol{V}_{22}+\boldsymbol{V}_{\gamma \gamma} .
\end{aligned}
\]</span></p>
<p>An estimator for <span class="math inline">\(\boldsymbol{V}_{\alpha}\)</span> in the general case is</p>
<p><span class="math display">\[
\widehat{\boldsymbol{V}}_{\alpha}=\widehat{\boldsymbol{V}}_{22}+\widehat{\boldsymbol{V}}_{\gamma \gamma}-\widehat{\boldsymbol{V}}_{\gamma 2}-\widehat{\boldsymbol{V}}_{\gamma 2}^{\prime}
\]</span></p>
<p>where</p>
<p><span class="math display">\[
\begin{aligned}
\widehat{\boldsymbol{V}}_{22} &amp;=\left[\frac{1}{n}\left(\boldsymbol{X}^{\prime} \boldsymbol{P}_{\boldsymbol{Z}} \boldsymbol{X}\right)^{-1} \boldsymbol{X}^{\prime} \boldsymbol{Z}\left(\boldsymbol{Z}^{\prime} \boldsymbol{Z}\right)^{-1}\left(\sum_{i=1}^{n} Z_{i} Z_{i}^{\prime} \widehat{e}_{i}^{2}\right)\left(\boldsymbol{Z}^{\prime} \boldsymbol{Z}\right)^{-1} \boldsymbol{Z}^{\prime} \boldsymbol{X}\left(\boldsymbol{X}^{\prime} \boldsymbol{P}_{\boldsymbol{Z}} \boldsymbol{X}\right)^{-1}\right]_{22} \\
\widehat{\boldsymbol{V}}_{\gamma 2} &amp;=\left[\frac{1}{n}\left(\widehat{\boldsymbol{U}}^{\prime} \widehat{\boldsymbol{U}}\right)^{-1}\left(\sum_{i=1}^{n} \widehat{u}_{i} \widehat{W}_{i}^{\prime} \widehat{e}_{i} \widehat{v}_{i}\right)\left(\boldsymbol{X}^{\prime} \boldsymbol{P}_{\boldsymbol{Z}} \boldsymbol{X}\right)^{-1}\right]_{-2} \\
\widehat{e}_{i} &amp;=Y_{i}-X_{i}^{\prime} \widehat{\beta} \\
\widehat{v}_{i} &amp;=Y_{i}-X_{i}^{\prime} \widehat{\beta}-\widehat{u}_{2 i}^{\prime} \widehat{\gamma}
\end{aligned}
\]</span></p>
<p>Under the assumption of conditional homoskedasticity we have the estimator</p>
<p><span class="math display">\[
\begin{aligned}
\widehat{\boldsymbol{V}}_{\alpha}^{0} &amp;=\widehat{\boldsymbol{V}}_{\beta \beta}^{0}+\widehat{\boldsymbol{V}}_{\gamma \gamma}^{0} \\
\widehat{\boldsymbol{V}}_{\beta \beta} &amp;=\left[\left(\boldsymbol{X}^{\prime} \boldsymbol{P}_{\boldsymbol{Z}} \boldsymbol{X}\right)^{-1}\right]_{22}\left(\sum_{i=1}^{n} \widehat{e}_{i}^{2}\right) \\
\widehat{\boldsymbol{V}}_{\gamma \gamma} &amp;=\left(\widehat{\boldsymbol{U}}^{\prime} \widehat{\boldsymbol{U}}\right)^{-1}\left(\sum_{i=1}^{n} \widehat{v}_{i}^{2}\right) .
\end{aligned}
\]</span></p>
</section>
<section id="endogeneity-tests" class="level2" data-number="12.29">
<h2 data-number="12.29" class="anchored" data-anchor-id="endogeneity-tests"><span class="header-section-number">12.29</span> Endogeneity Tests</h2>
<p>The 2SLS estimator allows the regressor <span class="math inline">\(X_{2}\)</span> to be endogenous, meaning that <span class="math inline">\(X_{2}\)</span> is correlated with the structural error <span class="math inline">\(e\)</span>. If this correlation is zero then <span class="math inline">\(X_{2}\)</span> is exogenous and the structural equation can be estimated by least squares. This is a testable restriction. Effectively, the null hypothesis is</p>
<p><span class="math display">\[
\mathbb{H}_{0}: \mathbb{E}\left[X_{2} e\right]=0
\]</span></p>
<p>with the alternative</p>
<p><span class="math display">\[
\mathbb{M}_{1}: \mathbb{E}\left[X_{2} e\right] \neq 0 .
\]</span></p>
<p>The maintained hypothesis is <span class="math inline">\(\mathbb{E}[Z e]=0\)</span>. Since <span class="math inline">\(X_{1}\)</span> is a component of <span class="math inline">\(Z\)</span> this implies <span class="math inline">\(\mathbb{E}\left[X_{1} e\right]=0\)</span>. Consequently we could alternatively write the null as <span class="math inline">\(\mathbb{H}_{0}: \mathbb{E}[X e]=0\)</span> (and some authors do so).</p>
<p>Recall the control function regression (12.59)</p>
<p><span class="math display">\[
\begin{aligned}
&amp;Y=X_{1}^{\prime} \beta_{1}+X_{2}^{\prime} \beta_{2}+u_{2}^{\prime} \alpha+v \\
&amp;\alpha=\left(\mathbb{E}\left[u_{2} u_{2}^{\prime}\right]\right)^{-1} \mathbb{E}\left[u_{2} e\right]
\end{aligned}
\]</span></p>
<p>Notice that <span class="math inline">\(\mathbb{E}\left[X_{2} e\right]=0\)</span> if and only if <span class="math inline">\(\mathbb{E}\left[u_{2} e\right]=0\)</span>, so the hypothesis can be restated as <span class="math inline">\(\mathbb{H}_{0}: \alpha=0\)</span> against <span class="math inline">\(\mathbb{H}_{1}: \alpha \neq 0\)</span>. Thus a natural test is based on the Wald statistic <span class="math inline">\(W\)</span> for <span class="math inline">\(\alpha=0\)</span> in the control function regression (12.28). Under Theorem 12.9, Theorem <span class="math inline">\(12.10\)</span>, and <span class="math inline">\(\mathbb{M}_{0}, W\)</span> is asymptotically chi-square with <span class="math inline">\(k_{2}\)</span> degrees of freedom. In addition, under the normal regression assumption the <span class="math inline">\(F\)</span> statistic has an exact <span class="math inline">\(F\left(k_{2}, n-\right.\)</span> <span class="math inline">\(k_{1}-2 k_{2}\)</span> ) distribution. We accept the null hypothesis that <span class="math inline">\(X_{2}\)</span> is exogenous if <span class="math inline">\(W\)</span> (or F) is smaller than the critical value, and reject in favor of the hypothesis that <span class="math inline">\(X_{2}\)</span> is endogenous if the statistic is larger than the critical value.</p>
<p>Specifically, estimate the reduced form by least squares</p>
<p><span class="math display">\[
X_{2 i}=\widehat{\Gamma}_{12}^{\prime} Z_{1 i}+\widehat{\Gamma}_{22}^{\prime} Z_{2 i}+\widehat{u}_{2 i}
\]</span></p>
<p>to obtain the residuals. Then estimate the control function by least squares</p>
<p><span class="math display">\[
Y_{i}=X_{i}^{\prime} \widehat{\beta}+\widehat{u}_{2 i}^{\prime} \widehat{\alpha}+\widehat{v}_{i} .
\]</span></p>
<p>Let <span class="math inline">\(W, W^{0}\)</span> and <span class="math inline">\(F=W^{0} / k_{2}\)</span> denote the Wald, homoskedastic Wald, and <span class="math inline">\(F\)</span> statistics for <span class="math inline">\(\alpha=0\)</span>.</p>
<p>Theorem 12.14 Under <span class="math inline">\(\mathbb{M}_{0}, W \underset{d}{\longrightarrow} \chi_{k_{2}}^{2}\)</span>. Let <span class="math inline">\(c_{1-\alpha}\)</span> solve <span class="math inline">\(\mathbb{P}\left[\chi_{k_{2}}^{2} \leq c_{1-\alpha}\right]=1-\alpha\)</span>. The test “Reject <span class="math inline">\(\mathbb{M}_{0}\)</span> if <span class="math inline">\(W&gt;c_{1-\alpha}\)</span>” has asymptotic size <span class="math inline">\(\alpha\)</span>.</p>
<p>Theorem 12.15 Suppose <span class="math inline">\(e \mid X, Z \sim \mathrm{N}\left(0, \sigma^{2}\right)\)</span>. Under <span class="math inline">\(\mathbb{H}_{0}, \mathrm{~F} \sim F\left(k_{2}, n-k_{1}-2 k_{2}\right)\)</span>. Let <span class="math inline">\(c_{1-\alpha}\)</span> solve <span class="math inline">\(\mathbb{P}\left[F\left(k_{2}, n-k_{1}-2 k_{2}\right) \leq c_{1-\alpha}\right]=1-\alpha\)</span>. The test “Reject <span class="math inline">\(\mathbb{H}_{0}\)</span> if <span class="math inline">\(\mathrm{F}&gt;\)</span> <span class="math inline">\(c_{1-\alpha}\)</span>” has exact size <span class="math inline">\(\alpha\)</span>.</p>
<p>Since in general we do not want to impose homoskedasticity these results suggest that the most appropriate test is the Wald statistic constructed with the robust heteroskedastic covariance matrix. This can be computed in Stata using the command estat endogenous after ivregress when the latter uses a robust covariance option. Stata reports the Wald statistic in <span class="math inline">\(F\)</span> form (and thus uses the <span class="math inline">\(F\)</span> distribution to calculate the p-value) as “Robust regression F”. Using the <span class="math inline">\(F\)</span> rather than the <span class="math inline">\(\chi^{2}\)</span> is not formally justified but is a reasonable finite sample adjustment. If the command estat endogenous is applied after ivregress without a robust covariance option Stata reports the <span class="math inline">\(F\)</span> statistic as “Wu-Hausman F”.</p>
<p>There is an alternative (and traditional) way to derive a test for endogeneity. Under <span class="math inline">\(\mathbb{M}_{0}\)</span>, both OLS and 2 SLS are consistent estimators. But under <span class="math inline">\(\mathbb{M}_{1}\)</span> they converge to different values. Thus the difference between the OLS and 2SLS estimators is a valid test statistic for endogeneity. It also measures what we often care most about - the impact of endogeneity on the parameter estimates. This literature was developed under the assumption of conditional homoskedasticity (and it is important for these results) so we assume this condition for the development of the statistic.</p>
<p>Let <span class="math inline">\(\widehat{\beta}=\left(\widehat{\beta}_{1}, \widehat{\beta}_{2}\right)\)</span> be the OLS estimator and let <span class="math inline">\(\widetilde{\beta}=\left(\widetilde{\beta}_{1}, \widetilde{\beta}_{2}\right)\)</span> be the 2SLS estimator. Under <span class="math inline">\(\mathbb{H}_{0}\)</span> and homoskedasticity the OLS estimator is Gauss-Markov efficient so by the Hausman equality</p>
<p><span class="math display">\[
\begin{aligned}
\operatorname{var}\left[\widehat{\beta}_{2}-\widetilde{\beta}_{2}\right] &amp;=\operatorname{var}\left[\widetilde{\beta}_{2}\right]-\operatorname{var}\left[\widehat{\beta}_{2}\right] \\
&amp;=\left(\left(\boldsymbol{X}_{2}^{\prime}\left(\boldsymbol{P}_{\boldsymbol{Z}}-\boldsymbol{P}_{1}\right) \boldsymbol{X}_{2}\right)^{-1}-\left(\boldsymbol{X}_{2}^{\prime} \boldsymbol{M}_{1} \boldsymbol{X}_{2}\right)^{-1}\right) \sigma^{2}
\end{aligned}
\]</span></p>
<p>where <span class="math inline">\(\boldsymbol{P}_{\boldsymbol{Z}}=\boldsymbol{Z}\left(\boldsymbol{Z}^{\prime} \boldsymbol{Z}\right)^{-1} \boldsymbol{Z}^{\prime}, \boldsymbol{P}_{1}=\boldsymbol{X}_{1}\left(\boldsymbol{X}_{1}^{\prime} \boldsymbol{X}_{1}\right)^{-1} \boldsymbol{X}_{1}^{\prime}\)</span>, and <span class="math inline">\(\boldsymbol{M}_{1}=\boldsymbol{I}_{n}-\boldsymbol{P}_{1}\)</span>. Thus a valid test statistic for <span class="math inline">\(\mathbb{H}_{0}\)</span> is</p>
<p><span class="math display">\[
T=\frac{\left(\widehat{\beta}_{2}-\widetilde{\beta}_{2}\right)^{\prime}\left(\left(\boldsymbol{X}_{2}^{\prime}\left(\boldsymbol{P}_{\boldsymbol{Z}}-\boldsymbol{P}_{1}\right) \boldsymbol{X}_{2}\right)^{-1}-\left(\boldsymbol{X}_{2}^{\prime} \boldsymbol{M}_{1} \boldsymbol{X}_{2}\right)^{-1}\right)^{-1}\left(\widehat{\beta}_{2}-\widetilde{\beta}_{2}\right)}{\widehat{\sigma}^{2}}
\]</span></p>
<p>for some estimator <span class="math inline">\(\widehat{\sigma}^{2}\)</span> of <span class="math inline">\(\sigma^{2}\)</span>. Durbin (1954) first proposed <span class="math inline">\(T\)</span> as a test for endogeneity in the context of IV estimation setting <span class="math inline">\(\widehat{\sigma}^{2}\)</span> to be the least squares estimator of <span class="math inline">\(\sigma^{2}\)</span>. Wu (1973) proposed <span class="math inline">\(T\)</span> as a test for endogeneity in the context of 2SLS estimation, considering a set of possible estimators <span class="math inline">\(\widehat{\sigma}^{2}\)</span> including the regression estimator from (12.63). Hausman (1978) proposed a version of <span class="math inline">\(T\)</span> based on the full contrast <span class="math inline">\(\widehat{\beta}-\widetilde{\beta}\)</span>, and observed that it equals the regression Wald statistic <span class="math inline">\(W^{0}\)</span> described earlier. In fact, when <span class="math inline">\(\widehat{\sigma}^{2}\)</span> is the regression estimator from (12.63) the statistic (12.64) algebraically equals both <span class="math inline">\(W^{0}\)</span> and the version of (12.64) based on the full contrast <span class="math inline">\(\widehat{\beta}-\widetilde{\beta}\)</span>. We show these equalities below. Thus these three approaches yield exactly the same statistic except for possible differences regarding the choice of <span class="math inline">\(\widehat{\sigma}^{2}\)</span>. Since the regression <span class="math inline">\(F\)</span> test described earlier has an exact <span class="math inline">\(F\)</span> distribution in the normal sampling model and thus can exactly control test size, this is the preferred version of the test. The general class of tests are called Durbin-Wu-Hausman tests, Wu-Hausman tests, or Hausman tests, depending on the author.</p>
<p>When <span class="math inline">\(k_{2}=1\)</span> (there is one right-hand-side endogenous variable), which is quite common in applications, the endogeneity test can be equivalently expressed at the t-statistic for <span class="math inline">\(\widehat{\alpha}\)</span> in the estimated control function. Thus it is sufficient to estimate the control function regression and check the t-statistic for <span class="math inline">\(\widehat{\alpha}\)</span>. If <span class="math inline">\(|\widehat{\alpha}|&gt;2\)</span> then we can reject the hypothesis that <span class="math inline">\(X_{2}\)</span> is exogenous for <span class="math inline">\(\beta\)</span>.</p>
<p>We illustrate using the Card proximity example using the two instruments public and private. We first estimate the reduced form for education, obtain the residual, and then estimate the control function regression. The residual has a coefficient <span class="math inline">\(-0.088\)</span> with a standard error of <span class="math inline">\(0.037\)</span> and a t-statistic of 2.4. Since the latter exceeds the <span class="math inline">\(5 %\)</span> critical value (its p-value is <span class="math inline">\(0.017\)</span> ) we reject exogeneity. This means that the 2SLS estimates are statistically different from the least squares estimates of the structural equation and supports our decision to treat education as an endogenous variable. (Alternatively, the <span class="math inline">\(F\)</span> statistic is <span class="math inline">\(2.4^{2}=5.7\)</span> with the same p-value).</p>
<p>We now show the equality of the various statistics.</p>
<p>We first show that the statistic (12.64) is not altered if based on the full contrast <span class="math inline">\(\widehat{\beta}-\widetilde{\beta}\)</span>. Indeed, <span class="math inline">\(\widehat{\beta}_{1}-\widetilde{\beta}_{1}\)</span> is a linear function of <span class="math inline">\(\widehat{\beta}_{2}-\widetilde{\beta}_{2}\)</span>, so there is no extra information in the full contrast. To see this, observe that given <span class="math inline">\(\widehat{\beta}_{2}\)</span> we can solve by least squares to find</p>
<p><span class="math display">\[
\widehat{\beta}_{1}=\left(\boldsymbol{X}_{1}^{\prime} \boldsymbol{X}_{1}\right)^{-1}\left(\boldsymbol{X}_{1}^{\prime}\left(\boldsymbol{Y}-\boldsymbol{X}_{2} \widehat{\beta}_{2}\right)\right)
\]</span></p>
<p>and similarly</p>
<p><span class="math display">\[
\widetilde{\beta}_{1}=\left(\boldsymbol{X}_{1}^{\prime} \boldsymbol{X}_{1}\right)^{-1}\left(\boldsymbol{X}_{1}^{\prime}\left(\boldsymbol{Y}-\boldsymbol{P}_{\boldsymbol{Z}} \boldsymbol{X}_{2} \widetilde{\beta}\right)\right)=\left(\boldsymbol{X}_{1}^{\prime} \boldsymbol{X}_{1}\right)^{-1}\left(\boldsymbol{X}_{1}^{\prime}\left(\boldsymbol{Y}-\boldsymbol{X}_{2} \widetilde{\beta}\right)\right)
\]</span></p>
<p>the second equality because <span class="math inline">\(\boldsymbol{P}_{\boldsymbol{Z}} \boldsymbol{X}_{1}=\boldsymbol{X}_{1}\)</span>. Thus</p>
<p><span class="math display">\[
\begin{aligned}
\widehat{\beta}_{1}-\widetilde{\beta}_{1} &amp;=\left(\boldsymbol{X}_{1}^{\prime} \boldsymbol{X}_{1}\right)^{-1} \boldsymbol{X}_{1}^{\prime}\left(\boldsymbol{Y}-\boldsymbol{X}_{2} \widehat{\beta}_{2}\right)-\left(\boldsymbol{X}_{1}^{\prime} \boldsymbol{X}_{1}\right)^{-1} \boldsymbol{X}_{1}^{\prime}\left(\boldsymbol{Y}-\boldsymbol{P}_{\boldsymbol{Z}} \boldsymbol{X}_{2} \widetilde{\beta}\right) \\
&amp;=\left(\boldsymbol{X}_{1}^{\prime} \boldsymbol{X}_{1}\right)^{-1} \boldsymbol{X}_{1}^{\prime} \boldsymbol{X}_{2}\left(\widetilde{\beta}_{2}-\widehat{\beta}_{2}\right)
\end{aligned}
\]</span></p>
<p>as claimed.</p>
<p>We next show that <span class="math inline">\(T\)</span> in (12.64) equals the homoskedastic Wald statistic <span class="math inline">\(W^{0}\)</span> for <span class="math inline">\(\widehat{\alpha}\)</span> from the regression (12.63). Consider the latter regression. Since <span class="math inline">\(\boldsymbol{X}_{2}\)</span> is contained in <span class="math inline">\(\boldsymbol{X}\)</span> the coefficient estimate <span class="math inline">\(\widehat{\alpha}\)</span> is invariant to replacing <span class="math inline">\(\widehat{\boldsymbol{U}}_{2}=\boldsymbol{X}_{2}-\widehat{\boldsymbol{X}}_{2}\)</span> with <span class="math inline">\(-\widehat{\boldsymbol{X}}_{2}=-\boldsymbol{P}_{\boldsymbol{Z}} \boldsymbol{X}_{2}\)</span>. By the FWL representation, setting <span class="math inline">\(\boldsymbol{M}_{\boldsymbol{X}}=\)</span> <span class="math inline">\(\boldsymbol{I}_{n}-\boldsymbol{X}\left(\boldsymbol{X}^{\prime} \boldsymbol{X}\right)^{-1} \boldsymbol{X}^{\prime}\)</span></p>
<p><span class="math display">\[
\widehat{\alpha}=-\left(\widehat{\boldsymbol{X}}_{2}^{\prime} \boldsymbol{M}_{\boldsymbol{X}} \widehat{\boldsymbol{X}}_{2}\right)^{-1} \widehat{\boldsymbol{X}}_{2}^{\prime} \boldsymbol{M}_{\boldsymbol{X}} \boldsymbol{Y}=-\left(\boldsymbol{X}_{2}^{\prime} \boldsymbol{P}_{\boldsymbol{Z}} \boldsymbol{M}_{\boldsymbol{X}} \boldsymbol{P}_{\boldsymbol{Z}} \boldsymbol{X}_{2}\right)^{-1} \boldsymbol{X}_{2}^{\prime} \boldsymbol{P}_{\boldsymbol{Z}} \boldsymbol{M}_{\boldsymbol{X}} \boldsymbol{Y}
\]</span></p>
<p>It follows that</p>
<p><span class="math display">\[
W^{0}=\frac{\boldsymbol{Y}^{\prime} \boldsymbol{M}_{\boldsymbol{X}} \boldsymbol{P}_{\boldsymbol{Z}} \boldsymbol{X}_{2}\left(\boldsymbol{X}_{2}^{\prime} \boldsymbol{P}_{\boldsymbol{Z}} \boldsymbol{M}_{\boldsymbol{X}} \boldsymbol{P}_{\boldsymbol{Z}} \boldsymbol{X}_{2}\right)^{-1} \boldsymbol{X}_{2}^{\prime} \boldsymbol{P}_{\boldsymbol{Z}} \boldsymbol{M}_{\boldsymbol{X}} \boldsymbol{Y}}{\widehat{\sigma}^{2}} .
\]</span></p>
<p>Our goal is to show that <span class="math inline">\(T=W^{0}\)</span>. Define <span class="math inline">\(\widetilde{\boldsymbol{X}}_{2}=\left(\boldsymbol{I}_{n}-\boldsymbol{P}_{1}\right) \boldsymbol{X}_{2}\)</span> so <span class="math inline">\(\widehat{\beta}_{2}=\left(\widetilde{\boldsymbol{X}}_{2}^{\prime} \widetilde{\boldsymbol{X}}_{2}\right)^{-1} \widetilde{\boldsymbol{X}}_{2}^{\prime} \boldsymbol{Y}\)</span>. Then using <span class="math inline">\(\left(\boldsymbol{P}_{\boldsymbol{Z}}-\boldsymbol{P}_{1}\right)\left(\boldsymbol{I}_{n}-\boldsymbol{P}_{1}\right)=\left(\boldsymbol{P}_{\boldsymbol{Z}}-\boldsymbol{P}_{1}\right)\)</span> and defining <span class="math inline">\(\boldsymbol{Q}=\widetilde{\boldsymbol{X}}_{2}\left(\widetilde{\boldsymbol{X}}_{2}^{\prime} \tilde{\boldsymbol{X}}_{2}\right)^{-1} \widetilde{\boldsymbol{X}}_{2}^{\prime}\)</span> we find</p>
<p><span class="math display">\[
\begin{aligned}
&amp;\boldsymbol{\Delta} \stackrel{\text { def }}{=}\left(\boldsymbol{X}_{2}^{\prime}\left(\boldsymbol{P}_{\boldsymbol{Z}}-\boldsymbol{P}_{1}\right) \boldsymbol{X}_{2}\right)\left(\widetilde{\beta}_{2}-\widehat{\beta}_{2}\right) \\
&amp;=\boldsymbol{X}_{2}^{\prime}\left(\boldsymbol{P}_{\boldsymbol{Z}}-\boldsymbol{P}_{1}\right) \boldsymbol{Y}-\left(\boldsymbol{X}_{2}^{\prime}\left(\boldsymbol{P}_{\boldsymbol{Z}}-\boldsymbol{P}_{1}\right) \boldsymbol{X}_{2}\right)\left(\widetilde{\boldsymbol{X}}_{2}^{\prime} \widetilde{\boldsymbol{X}}_{2}\right)^{-1} \widetilde{\boldsymbol{X}}_{2}^{\prime} \boldsymbol{Y} \\
&amp;=\boldsymbol{X}_{2}^{\prime}\left(\boldsymbol{P}_{\boldsymbol{Z}}-\boldsymbol{P}_{1}\right)\left(\boldsymbol{I}_{n}-\boldsymbol{Q}\right) \boldsymbol{Y} \\
&amp;=\boldsymbol{X}_{2}^{\prime}\left(\boldsymbol{P}_{\boldsymbol{Z}}-\boldsymbol{P}_{1}-\boldsymbol{P}_{\boldsymbol{Z}} \boldsymbol{Q}\right) \boldsymbol{Y} \\
&amp;=\boldsymbol{X}_{2}^{\prime} \boldsymbol{P}_{\boldsymbol{Z}}\left(\boldsymbol{I}_{n}-\boldsymbol{P}_{1}-\boldsymbol{Q}\right) \boldsymbol{Y} \\
&amp;=\boldsymbol{X}_{2}^{\prime} \boldsymbol{P}_{\boldsymbol{Z}} \boldsymbol{M}_{\boldsymbol{X}} \boldsymbol{Y} .
\end{aligned}
\]</span></p>
<p>The third-to-last equality is <span class="math inline">\(\boldsymbol{P}_{1} \boldsymbol{Q}=0\)</span> and the final uses <span class="math inline">\(\boldsymbol{M}_{\boldsymbol{X}}=\boldsymbol{I}_{n}-\boldsymbol{P}_{1}-\boldsymbol{Q}\)</span>. We also calculate that</p>
<p><span class="math display">\[
\begin{aligned}
&amp;\boldsymbol{Q}^{*} \stackrel{\text { def }}{=}\left(\boldsymbol{X}_{2}^{\prime}\left(\boldsymbol{P}_{\boldsymbol{Z}}-\boldsymbol{P}_{1}\right) \boldsymbol{X}_{2}\right)\left(\left(\boldsymbol{X}_{2}^{\prime}\left(\boldsymbol{P}_{\boldsymbol{Z}}-\boldsymbol{P}_{1}\right) \boldsymbol{X}_{2}\right)^{-1}-\left(\boldsymbol{X}_{2}^{\prime} \boldsymbol{M}_{1} \boldsymbol{X}_{2}\right)^{-1}\right)\left(\boldsymbol{X}_{2}^{\prime}\left(\boldsymbol{P}_{\boldsymbol{Z}}-\boldsymbol{P}_{1}\right) \boldsymbol{X}_{2}\right) \\
&amp;=\boldsymbol{X}_{2}^{\prime}\left(\boldsymbol{P}_{\boldsymbol{Z}}-\boldsymbol{P}_{1}-\left(\boldsymbol{P}_{\boldsymbol{Z}}-\boldsymbol{P}_{1}\right) \boldsymbol{Q}\left(\boldsymbol{P}_{\boldsymbol{Z}}-\boldsymbol{P}_{1}\right)\right) \boldsymbol{X}_{2} \\
&amp;=\boldsymbol{X}_{2}^{\prime}\left(\boldsymbol{P}_{\boldsymbol{Z}}-\boldsymbol{P}_{1}-\boldsymbol{P}_{\boldsymbol{Z}} \boldsymbol{Q} \boldsymbol{P}_{\boldsymbol{Z}}\right) \boldsymbol{X}_{2} \\
&amp;=\boldsymbol{X}_{2}^{\prime} \boldsymbol{P}_{\boldsymbol{Z}} \boldsymbol{M}_{\boldsymbol{X}} \boldsymbol{P}_{\boldsymbol{Z}} \boldsymbol{X}_{2} .
\end{aligned}
\]</span></p>
<p>Thus</p>
<p><span class="math display">\[
\begin{aligned}
T &amp;=\frac{\boldsymbol{\Delta}^{\prime} \boldsymbol{Q}^{*-1} \boldsymbol{\Delta}}{\widehat{\sigma}^{2}} \\
&amp;=\frac{\boldsymbol{Y}^{\prime} \boldsymbol{M}_{\boldsymbol{X}} \boldsymbol{P}_{\boldsymbol{Z}} \boldsymbol{X}_{2}\left(\boldsymbol{X}_{2}^{\prime} \boldsymbol{P}_{\boldsymbol{Z}} \boldsymbol{M}_{\boldsymbol{X}} \boldsymbol{P}_{\boldsymbol{Z}} \boldsymbol{X}_{2}\right)^{-1} \boldsymbol{X}_{2}^{\prime} \boldsymbol{P}_{\boldsymbol{Z}} \boldsymbol{M}_{\boldsymbol{X}} \boldsymbol{Y}}{\widehat{\sigma}^{2}} \\
&amp;=W^{0}
\end{aligned}
\]</span></p>
<p>as claimed.</p>
</section>
<section id="subset-endogeneity-tests" class="level2" data-number="12.30">
<h2 data-number="12.30" class="anchored" data-anchor-id="subset-endogeneity-tests"><span class="header-section-number">12.30</span> Subset Endogeneity Tests</h2>
<p>In some cases we may only wish to test the endogeneity of a subset of the variables. In the Card proximity example we may wish test the exogeneity of education separately from experience and its square. To execute a subset endogeneity test it is useful to partition the regressors into three groups so that the structural model is</p>
<p><span class="math display">\[
\begin{aligned}
Y &amp;=X_{1}^{\prime} \beta_{1}+X_{2}^{\prime} \beta_{2}+X_{3}^{\prime} \beta_{3}+e \\
\mathbb{E}[Z e] &amp;=0 .
\end{aligned}
\]</span></p>
<p>As before, the instrument vector <span class="math inline">\(Z\)</span> includes <span class="math inline">\(X_{1}\)</span>. The vector <span class="math inline">\(X_{3}\)</span> is treated as endogenous and <span class="math inline">\(X_{2}\)</span> is treated as potentially endogenous. The hypothesis to test is that <span class="math inline">\(X_{2}\)</span> is exogenous, or <span class="math inline">\(\mathbb{H}_{0}: \mathbb{E}\left[X_{2} e\right]=0\)</span> against <span class="math inline">\(\mathbb{H}_{1}: \mathbb{E}\left[X_{2} e\right] \neq 0\)</span></p>
<p>Under homoskedasticity a straightfoward test can be constructed by the Durbin-Wu-Hausman principle. Under <span class="math inline">\(\mathbb{M}_{0}\)</span> the appropriate estimator is <span class="math inline">\(2 \mathrm{SLS}\)</span> using the instruments <span class="math inline">\(\left(Z, X_{2}\right)\)</span>. Let this estimator of <span class="math inline">\(\beta_{2}\)</span> be denoted <span class="math inline">\(\widehat{\beta}_{2}\)</span>. Under <span class="math inline">\(\mathbb{H}_{1}\)</span> the appropriate estimator is 2SLS using the smaller instrument set <span class="math inline">\(Z\)</span>. Let this estimator of <span class="math inline">\(\beta_{2}\)</span> be denoted <span class="math inline">\(\widetilde{\beta}_{2}\)</span>. A Durbin-Wu-Hausman statistic for <span class="math inline">\(\mathbb{H}_{0}\)</span> against <span class="math inline">\(\mathbb{H}_{1}\)</span> is</p>
<p><span class="math display">\[
T=\left(\widehat{\beta}_{2}-\widetilde{\beta}_{2}\right)^{\prime}\left(\widehat{\operatorname{var}}\left[\widetilde{\beta}_{2}\right]-\widehat{\operatorname{var}}\left[\widehat{\beta}_{2}\right]\right)^{-1}\left(\widehat{\beta}_{2}-\widetilde{\beta}_{2}\right) .
\]</span></p>
<p>The asymptotic distribution under <span class="math inline">\(\mathbb{H}_{0}\)</span> is <span class="math inline">\(\chi_{k_{2}}^{2}\)</span> where <span class="math inline">\(k_{2}=\operatorname{dim}\left(X_{2}\right)\)</span>, so we reject the hypothesis that the variables <span class="math inline">\(X_{2}\)</span> are exogenous if <span class="math inline">\(T\)</span> exceeds an upper critical value from the <span class="math inline">\(\chi_{k_{2}}^{2}\)</span> distribution.</p>
<p>Instead of using the Wald statistic one could use the <span class="math inline">\(F\)</span> version of the test by dividing by <span class="math inline">\(k_{2}\)</span> and using the <span class="math inline">\(F\)</span> distribution for critical values. There is no finite sample justification for this modification, however, since <span class="math inline">\(X_{3}\)</span> is endogenous under the null hypothesis.</p>
<p>In Stata, the command estat endogenous (adding the variable name to specify which variable to test for exogeneity) after ivregress without a robust covariance option reports the <span class="math inline">\(F\)</span> version of this statistic as “Wu-Hausman F”. For example, in the Card proximity example using the four instruments public, private, age, and <span class="math inline">\(a g e^{2}\)</span>, if we estimate the equation by 2SLS with a non-robust covariance matrix and then compute the endogeneity test for education we find <span class="math inline">\(F=272\)</span> with a p-value of <span class="math inline">\(0.0000\)</span>, but if we compute the test for experience and its square we find <span class="math inline">\(F=2.98\)</span> with a p-value of <span class="math inline">\(0.051\)</span>. In this model, the assumption of exogeneity with homogenous coefficients is rejected for education but the result for experience is unclear.</p>
<p>A heteroskedasticity or cluster-robust test cannot be constructed easily by the Durbin-Wu-Hausman approach since the covariance matrix does not take a simple form. To allow for non-homoskedastic errors it is recommended to use GMM estimation. See Section 13.24.</p>
</section>
<section id="overidentification-tests" class="level2" data-number="12.31">
<h2 data-number="12.31" class="anchored" data-anchor-id="overidentification-tests"><span class="header-section-number">12.31</span> OverIdentification Tests</h2>
<p>When <span class="math inline">\(\ell&gt;k\)</span> the model is overidentified meaning that there are more moments than free parameters. This is a restriction and is testable. Such tests are called overidentification tests.</p>
<p>The instrumental variables model specifies <span class="math inline">\(\mathbb{E}[Z e]=0\)</span>. Equivalently, since <span class="math inline">\(e=Y-X^{\prime} \beta\)</span> this is</p>
<p><span class="math display">\[
\mathbb{E}[Z Y]-\mathbb{E}\left[Z X^{\prime}\right] \beta=0 .
\]</span></p>
<p>This is an <span class="math inline">\(\ell \times 1\)</span> vector of restrictions on the moment matrices <span class="math inline">\(\mathbb{E}[Z Y]\)</span> and <span class="math inline">\(\mathbb{E}\left[Z X^{\prime}\right]\)</span>. Yet since <span class="math inline">\(\beta\)</span> is of dimension <span class="math inline">\(k\)</span> which is less than <span class="math inline">\(\ell\)</span> it is not certain if indeed such a <span class="math inline">\(\beta\)</span> exists.</p>
<p>To make things a bit more concrete, suppose there is a single endogenous regressor <span class="math inline">\(X_{2}\)</span>, no <span class="math inline">\(X_{1}\)</span>, and two instruments <span class="math inline">\(Z_{1}\)</span> and <span class="math inline">\(Z_{2}\)</span>. Then the model specifies that</p>
<p><span class="math display">\[
\mathbb{E}\left(\left[Z_{1} Y\right]=\mathbb{E}\left[Z_{1} X_{2}\right] \beta\right.
\]</span></p>
<p>and</p>
<p><span class="math display">\[
\mathbb{E}\left[Z_{2} Y\right]=\mathbb{E}\left[Z_{2} X_{2}\right] \beta .
\]</span></p>
<p>Thus <span class="math inline">\(\beta\)</span> solves both equations. This is rather special.</p>
<p>Another way of thinking about this is we could solve for <span class="math inline">\(\beta\)</span> using either one equation or the other. In terms of estimation this is equivalent to estimating by IV using just the instrument <span class="math inline">\(Z_{1}\)</span> or instead just using the instrument <span class="math inline">\(Z_{2}\)</span>. These two estimators (in finite samples) are different. If the overidentification hypothesis is correct both are estimating the same parameter and both are consistent for <span class="math inline">\(\beta\)</span>. In contrast, if the overidentification hypothesis is false then the two estimators will converge to different probability limits and it is unclear if either probability limit is interesting.</p>
<p>For example, take the 2SLS estimates in the fourth column of Table <span class="math inline">\(12.1\)</span> which use public and private as instruments for education. Suppose we instead estimate by IV using just public as an instrument and then repeat using private. The IV coefficient for education in the first case is <span class="math inline">\(0.16\)</span> and in the second case 0.27. These appear to be quite different. However, the second estimate has a large standard error (0.16) so the difference may be sampling variation. An overidentification test addresses this question.</p>
<p>For a general overidentification test the null and alternative hypotheses are <span class="math inline">\(\mathbb{H}_{0}: \mathbb{E}[Z e]=0\)</span> against <span class="math inline">\(\mathbb{H}_{1}: \mathbb{E}[Z e] \neq 0\)</span>. We will also add the conditional homoskedasticity assumption</p>
<p><span class="math display">\[
\mathbb{E}\left[e^{2} \mid Z\right]=\sigma^{2} .
\]</span></p>
<p>To avoid (12.65) it is best to take a GMM approach which we defer until Chapter <span class="math inline">\(13 .\)</span></p>
<p>To implement a test of <span class="math inline">\(\mathbb{M}_{0}\)</span> consider a linear regression of the error <span class="math inline">\(e\)</span> on the instruments <span class="math inline">\(Z\)</span></p>
<p><span class="math display">\[
e=Z^{\prime} \alpha+v
\]</span></p>
<p>with <span class="math inline">\(\alpha=\left(\mathbb{E}\left[Z Z^{\prime}\right]\right)^{-1} \mathbb{E}[Z e]\)</span>. We can rewrite <span class="math inline">\(\mathbb{H}_{0}\)</span> as <span class="math inline">\(\alpha=0\)</span>. While <span class="math inline">\(e\)</span> is not observed we can replace it with the 2SLS residual <span class="math inline">\(\widehat{e}_{i}\)</span> and estimate <span class="math inline">\(\alpha\)</span> by least squares regression, e.g.&nbsp;<span class="math inline">\(\widehat{\alpha}=\left(Z^{\prime} \boldsymbol{Z}\right)^{-1} Z^{\prime} \widehat{\boldsymbol{e}}\)</span>. Sargan (1958) proposed testing <span class="math inline">\(\mathbb{M}_{0}\)</span> via a score test, which equals</p>
<p><span class="math display">\[
S=\widehat{\alpha}^{\prime}(\widehat{\operatorname{var}}[\widehat{\alpha}])^{-} \widehat{\alpha}=\frac{\widehat{\boldsymbol{e}}^{\prime} \boldsymbol{Z}\left(\boldsymbol{Z}^{\prime} \boldsymbol{Z}\right)^{-1} \boldsymbol{Z}^{\prime} \widehat{\boldsymbol{e}}}{\widehat{\sigma}^{2}} .
\]</span></p>
<p>where <span class="math inline">\(\widehat{\sigma}^{2}=\frac{1}{n} \widehat{\boldsymbol{e}} \widehat{\boldsymbol{e}}\)</span>. Basmann (1960) independently proposed a Wald statistic for <span class="math inline">\(\mathbb{H}_{0}\)</span>, which is <span class="math inline">\(S\)</span> with <span class="math inline">\(\widehat{\sigma}^{2}\)</span> replaced with <span class="math inline">\(\widetilde{\sigma}^{2}=n^{-1} \widehat{\boldsymbol{v}} ' \widehat{\boldsymbol{v}}\)</span> where <span class="math inline">\(\widehat{\boldsymbol{v}}=\widehat{\boldsymbol{e}}-\boldsymbol{Z} \widehat{\alpha}\)</span>. By the equivalence of homoskedastic score and Wald tests (see Section 9.16) Basmann’s statistic is a monotonic function of Sargan’s statistic and hence they yield equivalent tests. Sargan’s version is more typically reported.</p>
<p>The Sargan test rejects <span class="math inline">\(\mathbb{H}_{0}\)</span> in favor of <span class="math inline">\(\mathbb{M}_{1}\)</span> if <span class="math inline">\(S&gt;c\)</span> for some critical value <span class="math inline">\(c\)</span>. An asymptotic test sets <span class="math inline">\(c\)</span> as the <span class="math inline">\(1-\alpha\)</span> quantile of the <span class="math inline">\(\chi_{\ell-k}^{2}\)</span> distribution. This is justified by the asymptotic null distribution of <span class="math inline">\(S\)</span> which we now derive.</p>
<p>Theorem 12.16 Under Assumption <span class="math inline">\(12.2\)</span> and <span class="math inline">\(\mathbb{E}\left[e^{2} \mid Z\right]=\sigma^{2}\)</span>, then as <span class="math inline">\(n \rightarrow \infty\)</span>, <span class="math inline">\(S \underset{d}{\longrightarrow} \chi_{\ell-k}^{2}\)</span>. For <span class="math inline">\(c\)</span> satisfying <span class="math inline">\(\alpha=1-G_{\ell-k}(c), \mathbb{P}\left[S&gt;c \mid \mathbb{H}_{0}\right] \rightarrow \alpha\)</span> so the test “Reject</p>
<p><span class="math inline">\(\mathbb{M}_{0}\)</span> if <span class="math inline">\(S&gt;c\)</span> ” has asymptotic size <span class="math inline">\(\alpha\)</span>.</p>
<p>We prove Theorem <span class="math inline">\(12.16\)</span> below.</p>
<p>The Sargan statistic <span class="math inline">\(S\)</span> is an asymptotic test of the overidentifying restrictions under the assumption of conditional homoskedasticity. It has some limitations. First, it is an asymptotic test and does not have a finite sample (e.g.&nbsp;F) counterpart. Simulation evidence suggests that the test can be oversized (reject too frequently) in small and moderate sample sizes. Consequently, p-values should be interpreted cautiously. Second, the assumption of conditional homoskedasticity is unrealistic in applications. The best way to generalize the Sargan statistic to allow heteroskedasticity is to use the GMM overidentification statistic - which we will examine in Chapter 13. For 2SLS, Wooldrige (1995) suggested a robust score test, but Baum, Schaffer and Stillman (2003) point out that it is numerically equivalent to the GMM overidentification statistic. Hence the bottom line appears to be that to allow heteroskedasticity or clustering it is best to use a GMM approach.</p>
<p>In overidentified applications it is always prudent to report an overidentification test. If the test is insignificant it means that the overidentifying restrictions are not rejected, supporting the estimated model. If the overidentifying test statistic is highly significant (if the p-value is very small) this is evidence that the overidentifying restrictions are violated. In this case we should be concerned that the model is misspecified and interpreting the parameter estimates should be done cautiously.</p>
<p>When reporting the results of an overidentification test it seems reasonable to focus on very small significance levels such as <span class="math inline">\(1 %\)</span>. This means that we should only treat a model as “rejected” if the Sargan p-value is very small, e.g.&nbsp;less than <span class="math inline">\(0.01\)</span>. The reason to focus on very small significance levels is because it is very difficult to interpret the result “The model is rejected”. Stepping back a bit it does not seem credible that any overidentified model is literally true; rather what seems potentially credible is that an overidentified model is a reasonable approximation. A test is asking the question “Is there evidence that a model is not true” when we really want to know the answer to “Is there evidence that the model is a poor approximation”. Consequently it seems reasonable to require strong evidence to lead to the conclusion “Let’s reject this model”. The recommendation is that mild rejections ( <span class="math inline">\(\mathrm{p}\)</span>-values between <span class="math inline">\(1 %\)</span> and 5%) should be viewed as mildly worrisome but not critical evidence against a model. The results of an overidentification test should be integrated with other information before making a strong decision.</p>
<p>We illustrate the methods with the Card college proximity example. We have estimated two overidentified models by 2SLS in columns 4 &amp; 5 of Table 12.1. In each case the number of overidentifying restrictions is 1 . We report the Sargan statistic and its asymptotic <span class="math inline">\(p\)</span>-value (calculated using the <span class="math inline">\(\chi_{1}^{2}\)</span> distribution) in the table. Both p-values (0.37 and <span class="math inline">\(0.47)\)</span> are far from significant indicating that there is no evidence that the models are misspecified.</p>
<p>We now prove Theorem 12.16. The statistic <span class="math inline">\(S\)</span> is invariant to rotations of <span class="math inline">\(\boldsymbol{Z}\)</span> (replacing <span class="math inline">\(\boldsymbol{Z}\)</span> with <span class="math inline">\(\boldsymbol{Z} \boldsymbol{C}\)</span> ) so without loss of generality we assume <span class="math inline">\(\mathbb{E}\left[Z Z^{\prime}\right]=\boldsymbol{I}_{\ell}\)</span>. As <span class="math inline">\(n \rightarrow \infty, n^{-1 / 2} \boldsymbol{Z}^{\prime} \boldsymbol{e} \underset{d}{\rightarrow} Z\)</span> where <span class="math inline">\(Z \sim \mathrm{N}\left(0, \boldsymbol{I}_{\ell}\right)\)</span>. Also <span class="math inline">\(\frac{1}{n} \boldsymbol{Z}^{\prime} \boldsymbol{Z} \underset{p}{\longrightarrow} \boldsymbol{I}_{\ell}\)</span> and <span class="math inline">\(\frac{1}{n} \boldsymbol{Z}^{\prime} \boldsymbol{X} \underset{p}{\longrightarrow} \boldsymbol{Q}\)</span>, say. Then</p>
<p><span class="math display">\[
\begin{aligned}
n^{-1 / 2} \boldsymbol{Z}^{\prime} \widehat{\boldsymbol{e}} &amp;=\left(\boldsymbol{I}_{\ell}-\left(\frac{1}{n} \boldsymbol{Z}^{\prime} \boldsymbol{X}\right)\left(\frac{1}{n} \boldsymbol{X}^{\prime} \boldsymbol{P}_{\boldsymbol{Z}} \boldsymbol{X}\right)^{-1}\left(\frac{1}{n} \boldsymbol{X}^{\prime} \boldsymbol{Z}\right)\left(\frac{1}{n} \boldsymbol{Z}^{\prime} \boldsymbol{Z}\right)^{-1}\right) n^{-1 / 2} \boldsymbol{Z}^{\prime} \boldsymbol{e} \\
&amp; \underset{d}{\rightarrow} \sigma\left(\boldsymbol{I}_{\ell}-\boldsymbol{Q}\left(\boldsymbol{Q}^{\prime} \boldsymbol{Q}\right)^{-1} \boldsymbol{Q}^{\prime}\right) Z
\end{aligned}
\]</span></p>
<p>Since <span class="math inline">\(\widehat{\sigma}^{2} \underset{p}{\rightarrow} \sigma^{2}\)</span> it follows that</p>
<p><span class="math display">\[
S \underset{d}{\rightarrow} Z^{\prime}\left(\boldsymbol{I}_{\ell}-\boldsymbol{Q}\left(\boldsymbol{Q}^{\prime} \boldsymbol{Q}\right)^{-1} \boldsymbol{Q}^{\prime}\right) \mathrm{Z} \sim \chi_{\ell-k}^{2} .
\]</span></p>
<p>The distribution is <span class="math inline">\(\chi_{\ell-k}^{2}\)</span> because <span class="math inline">\(\boldsymbol{I}_{\ell}-\boldsymbol{Q}\left(\boldsymbol{Q}^{\prime} \boldsymbol{Q}\right)^{-1} \boldsymbol{Q}^{\prime}\)</span> is idempotent with rank <span class="math inline">\(\ell-k\)</span>.</p>
<p>The Sargan statistic test can be implemented in Stata using the command estat overid after ivregress 2sls or ivregres liml if a standard (non-robust) covariance matrix has been specified (that is, without the ‘, r’ option), or otherwise by the command estat overid, forcenonrobust.</p>
<p><img src="images//2022_09_17_f9391324ededdbb7a34eg-47.jpg" class="img-fluid"></p>
</section>
<section id="subset-overidentification-tests" class="level2" data-number="12.32">
<h2 data-number="12.32" class="anchored" data-anchor-id="subset-overidentification-tests"><span class="header-section-number">12.32</span> Subset OverIdentification Tests</h2>
<p>Tests of <span class="math inline">\(\mathbb{H}_{0}: \mathbb{E}[Z e]=0\)</span> are typically interpreted as tests of model specification. The alternative <span class="math inline">\(\mathbb{H}_{1}\)</span> : <span class="math inline">\(\mathbb{E}[Z e] \neq 0\)</span> means that at least one element of <span class="math inline">\(Z\)</span> is correlated with the error <span class="math inline">\(e\)</span> and is thus an invalid instrumental variable. In some cases it may be reasonable to test only a subset of the moment conditions.</p>
<p>As in the previous section we restrict attention to the homoskedastic case <span class="math inline">\(\mathbb{E}\left[e^{2} \mid Z\right]=\sigma^{2}\)</span>.</p>
<p>Partition <span class="math inline">\(Z=\left(Z_{a}, Z_{b}\right)\)</span> with dimensions <span class="math inline">\(\ell_{a}\)</span> and <span class="math inline">\(\ell_{b}\)</span>, respectively, where <span class="math inline">\(Z_{a}\)</span> contains the instruments which are believed to be uncorrelated with <span class="math inline">\(e\)</span> and <span class="math inline">\(Z_{b}\)</span> contains the instruments which may be correlated with <span class="math inline">\(e\)</span>. It is necessary to select this partition so that <span class="math inline">\(\ell_{a}&gt;k\)</span>, or equivalently <span class="math inline">\(\ell_{b}&lt;\ell-k\)</span>. This means that the model with just the instruments <span class="math inline">\(Z_{a}\)</span> is over-identified, or that <span class="math inline">\(\ell_{b}\)</span> is smaller than the number of overidentifying restrictions. (If <span class="math inline">\(\ell_{a}=k\)</span> then the tests described here exist but reduce to the Sargan test so are not interesting.) Hence the tests require that <span class="math inline">\(\ell-k&gt;1\)</span>, that the number of overidentifying restrictions exceeds one.</p>
<p>Given this partition the maintained hypothesis is <span class="math inline">\(\mathbb{E}\left[Z_{a} e\right]=0\)</span>. The null and alternative hypotheses are <span class="math inline">\(\mathbb{H}_{0}: \mathbb{E}\left[Z_{b} e\right]=0\)</span> against <span class="math inline">\(\mathbb{H}_{1}: \mathbb{E}\left[Z_{b} e\right] \neq 0\)</span>. That is, the null hypothesis is that the full set of moment conditions are valid while the alternative hypothesis is that the instrument subset <span class="math inline">\(Z_{b}\)</span> is correlated with <span class="math inline">\(e\)</span> and thus an invalid instrument. Rejection of <span class="math inline">\(\mathbb{H}_{0}\)</span> in favor of <span class="math inline">\(\mathbb{M}_{1}\)</span> is then interpreted as evidence that <span class="math inline">\(Z_{b}\)</span> is misspecified as an instrument.</p>
<p>Based on the same reasoning as described in the previous section, to test <span class="math inline">\(\mathbb{H}_{0}\)</span> against <span class="math inline">\(\mathbb{H}_{1}\)</span> we consider a partitioned version of the regression (12.66)</p>
<p><span class="math display">\[
e=Z_{a}^{\prime} \alpha_{a}+Z_{b}^{\prime} \alpha_{b}+v
\]</span></p>
<p>but now focus on the coefficient <span class="math inline">\(\alpha_{b}\)</span>. Given <span class="math inline">\(\mathbb{E}\left[Z_{a} e\right]=0, \mathbb{H}_{0}\)</span> is equivalent to <span class="math inline">\(\alpha_{b}=0\)</span>. The equation is estimated by least squares replacing the unobserved <span class="math inline">\(e_{i}\)</span> with the 2 SLS residual <span class="math inline">\(\widehat{e}_{i}\)</span>. The estimate of <span class="math inline">\(\alpha_{b}\)</span> is</p>
<p><span class="math display">\[
\widehat{\alpha}_{b}=\left(\boldsymbol{Z}_{b}^{\prime} \boldsymbol{M}_{a} \boldsymbol{Z}_{b}\right)^{-1} \boldsymbol{Z}_{b}^{\prime} \boldsymbol{M}_{a} \widehat{\boldsymbol{e}}
\]</span></p>
<p>where <span class="math inline">\(\boldsymbol{M}_{a}=\boldsymbol{I}_{n}-\boldsymbol{Z}_{a}\left(\boldsymbol{Z}_{a}^{\prime} \boldsymbol{Z}_{a}\right)^{-1} \boldsymbol{Z}_{a}^{\prime}\)</span>. Newey (1985) showed that an optimal (asymptotically most powerful) test of <span class="math inline">\(\mathbb{M}_{0}\)</span> against <span class="math inline">\(\mathbb{M}_{1}\)</span> is to reject for large values of the score statistic</p>
<p><span class="math display">\[
N=\widehat{\alpha}_{b}^{\prime}\left(\widehat{\operatorname{var}}\left[\widehat{\alpha}_{b}\right]\right)^{-} \widehat{\alpha}_{b}=\frac{\widehat{\boldsymbol{e}}^{\prime} \boldsymbol{R}\left(\boldsymbol{R}^{\prime} \boldsymbol{R}-\boldsymbol{R}^{\prime} \widehat{\boldsymbol{X}}\left(\widehat{\boldsymbol{X}}^{\prime} \widehat{\boldsymbol{X}}\right)^{-1} \widehat{\boldsymbol{X}}^{\prime} \boldsymbol{R}\right)^{-1} \boldsymbol{R}^{\prime} \widehat{\boldsymbol{e}}}{\widehat{\sigma}^{2}}
\]</span></p>
<p>where <span class="math inline">\(\widehat{\boldsymbol{X}}=\boldsymbol{P} \boldsymbol{X}, \boldsymbol{P}=\boldsymbol{Z}\left(\boldsymbol{Z}^{\prime} \boldsymbol{Z}\right)^{-1} \boldsymbol{Z}^{\prime}, \boldsymbol{R}=\boldsymbol{M}_{a} \boldsymbol{Z}_{b}\)</span>, and <span class="math inline">\(\widehat{\sigma}^{2}=\frac{1}{n} \widehat{\boldsymbol{e}}^{\prime} \widehat{\boldsymbol{e}}\)</span>.</p>
<p>Independently from Newey (1985), Eichenbaum, L. Hansen, and Singleton (1988) proposed a test based on the difference of Sargan statistics. Let <span class="math inline">\(S\)</span> be the Sargan test statistic (12.67) based on the full instrument set and <span class="math inline">\(S_{a}\)</span> be the Sargan statistic based on the instrument set <span class="math inline">\(Z_{a}\)</span>. The Sargan difference statistic is <span class="math inline">\(C=S-S_{a}\)</span>. Specifically, let <span class="math inline">\(\widetilde{\beta}_{2 \text { sls }}\)</span> be the 2SLS estimator using the instruments <span class="math inline">\(Z_{a}\)</span> only, set <span class="math inline">\(\widetilde{e}_{i}=Y_{i}-X_{i}^{\prime} \widetilde{\beta}_{2 s l s}\)</span>, and set <span class="math inline">\(\widetilde{\sigma}^{2}=\frac{1}{n} \widetilde{\boldsymbol{e}}^{\prime} \widetilde{\boldsymbol{e}}\)</span>. Then</p>
<p><span class="math display">\[
S_{a}=\frac{\widetilde{\boldsymbol{e}}^{\prime} \boldsymbol{Z}_{a}\left(\boldsymbol{Z}_{a}^{\prime} \boldsymbol{Z}_{a}\right)^{-1} \boldsymbol{Z}_{a}^{\prime} \widetilde{\boldsymbol{e}}}{\widetilde{\sigma}^{2}}
\]</span></p>
<p>An advantage of the <span class="math inline">\(C\)</span> statistic is that it is quite simple to calculate from the standard regression output.</p>
<p>At this point it is useful to reflect on our stated requirement that <span class="math inline">\(\ell_{a}&gt;k\)</span>. Indeed, if <span class="math inline">\(\ell_{a}&lt;k\)</span> then <span class="math inline">\(Z_{a}\)</span> fails the order condition for identification and <span class="math inline">\(\widetilde{\beta}_{2 \text { sls }}\)</span> cannot be calculated. Thus <span class="math inline">\(\ell_{a} \geq k\)</span> is necessary to compute <span class="math inline">\(S_{a}\)</span> and hence <span class="math inline">\(S\)</span>. Furthermore, if <span class="math inline">\(\ell_{a}=k\)</span> then model <span class="math inline">\(a\)</span> is just identified so while <span class="math inline">\(\widetilde{\beta}_{2 \text { sls }}\)</span> can be calculated, the statistic <span class="math inline">\(S_{a}=0\)</span> so <span class="math inline">\(C=S\)</span>. Thus when <span class="math inline">\(\ell_{a}=k\)</span> the subset test equals the full overidentification test so there is no gain from considering subset tests.</p>
<p>The <span class="math inline">\(C\)</span> statistic <span class="math inline">\(S_{a}\)</span> is asymptotically equivalent to replacing <span class="math inline">\(\widetilde{\sigma}^{2}\)</span> in <span class="math inline">\(S_{a}\)</span> with <span class="math inline">\(\widehat{\sigma}^{2}\)</span>, yielding the statistic</p>
<p><span class="math display">\[
C^{*}=\frac{\widehat{\boldsymbol{e}}^{\prime} \boldsymbol{Z}\left(\boldsymbol{Z}^{\prime} \boldsymbol{Z}\right)^{-1} \boldsymbol{Z}^{\prime} \widehat{\boldsymbol{e}}}{\widehat{\sigma}^{2}}-\frac{\widetilde{\boldsymbol{e}}^{\prime} \boldsymbol{Z}_{a}\left(\boldsymbol{Z}_{a}^{\prime} \boldsymbol{Z}_{a}\right)^{-1} \boldsymbol{Z}_{a}^{\prime} \widetilde{\boldsymbol{e}}}{\widehat{\sigma}^{2}} .
\]</span></p>
<p>It turns out that this is Newey’s statistic <span class="math inline">\(N\)</span>. These tests have chi-square asymptotic distributions.</p>
<p>Let <span class="math inline">\(c\)</span> satisfy <span class="math inline">\(\alpha=1-G_{\ell_{b}}(c)\)</span>.</p>
<p>Theorem 12.17 Algebraically, <span class="math inline">\(N=C^{*}\)</span>. Under Assumption <span class="math inline">\(12.2\)</span> and <span class="math inline">\(\mathbb{E}\left[e^{2} \mid Z\right]=\)</span> <span class="math inline">\(\sigma^{2}\)</span>, as <span class="math inline">\(n \rightarrow \infty, N \underset{d}{\longrightarrow} \chi_{\ell_{b}}^{2}\)</span> and <span class="math inline">\(C \underset{d}{\longrightarrow} \chi_{\ell_{b}}^{2}\)</span>. Thus the tests “Reject <span class="math inline">\(\mathbb{H}_{0}\)</span> if <span class="math inline">\(N&gt;c\)</span>” and</p>
<p>“Reject <span class="math inline">\(\mathbb{H}_{0}\)</span> if <span class="math inline">\(C&gt;c\)</span>” are asymptotically equivalent and have asymptotic size <span class="math inline">\(\alpha\)</span>.</p>
<p>Theorem <span class="math inline">\(12.17\)</span> shows that <span class="math inline">\(N\)</span> and <span class="math inline">\(C^{*}\)</span> are identical and are near equivalents to the convenient statistic C. The appropriate asymptotic distribution is <span class="math inline">\(\chi_{\ell_{b}}^{2}\)</span>. Computationally, the easiest method to implement a subset overidentification test is to estimate the model twice by 2SLS, first using the full instrument set <span class="math inline">\(Z\)</span> and the second using the partial instrument set <span class="math inline">\(Z_{a}\)</span>. Compute the Sargan statistics for both 2SLS regressions and compute <span class="math inline">\(C\)</span> as the difference in the Sargan statistics. In Stata, for example, this is simple to implement with a few lines of code.</p>
<p>We illustrate using the Card college proximity example. Our reported 2SLS estimates have <span class="math inline">\(\ell-k=1\)</span> so there is no role for a subset overidentification test. (Recall, the number of overidentifying restrictions must exceed one.) To illustrate we add extra instruments to the estimates in column 5 of Table <span class="math inline">\(12.1\)</span> (the 2SLS estimates using public, private, age, and age <span class="math inline">\({ }^{2}\)</span> as instruments for education, experience, and experience <span class="math inline">\(\left.{ }^{2} / 100\right)\)</span>. We add two instruments: the years of education of the father and the mother of the worker. These variables had been used in the earlier labor economics literature as instruments but Card did not. (He used them as regression controls in some specifications.) The motivation for using parent’s education as instruments is the hypothesis that parental education influences children’s educational attainment but does not directly influence their ability. The more modern labor economics literature has disputed this idea, arguing that children are educated in part at home and thus parent’s education has a direct impact on the skill attainment of children (and not just an indirect impact via educational attainment). The older view was that parent’s education is a valid instrument, the modern view is that it is not valid. We can test this dispute using a overidentification subset test.</p>
<p>We do this by estimating the wage equation by 2SLS using public, private, age, age <span class="math inline">\(^{2}\)</span>, father, and <span class="math inline">\(^{2}\)</span> mother, as instruments for education, experience, and experience <span class="math inline">\(\left.{ }^{2} / 100\right)\)</span>. We do not report the parameter estimates here but observe that this model is overidentified with 3 overidentifying restrictions. We calculate the Sargan overidentification statistic. It is <span class="math inline">\(7.9\)</span> with an asymptotic p-value (calculated using <span class="math inline">\(\chi_{3}^{2}\)</span> ) of <span class="math inline">\(0.048\)</span>. This is a mild rejection of the null hypothesis of correct specification. As we argued in the previous section this by itself is not reason to reject the model. Now we consider a subset overidentification test. We are interested in testing the validity of the two instruments father and mother, not the instruments public, private, age, <span class="math inline">\(a g e^{2}\)</span>. To test the hypothesis that these two instruments are uncorrelated with the structural error we compute the difference in Sargan statistic, <span class="math inline">\(C=7.9-0.5=7.4\)</span>, which has a p-value (calculated using <span class="math inline">\(\chi_{2}^{2}\)</span> ) of <span class="math inline">\(0.025\)</span>. This is marginally statistically significant, meaning that there is evidence that father and mother are not valid instruments for the wage equation. Since the <span class="math inline">\(\mathrm{p}\)</span>-value is not smaller than <span class="math inline">\(1 %\)</span> it is not overwhelming evidence but it still supports Card’s decision to not use parental education as instruments for the wage equation. We now prove the results in Theorem 12.17.</p>
<p>We first show that <span class="math inline">\(N=C^{*}\)</span>. Define <span class="math inline">\(\boldsymbol{P}_{a}=\boldsymbol{Z}_{a}\left(\boldsymbol{Z}_{a}^{\prime} \boldsymbol{Z}_{a}\right)^{-1} \boldsymbol{Z}_{a}^{\prime}\)</span> and <span class="math inline">\(\boldsymbol{P}_{\boldsymbol{R}}=\boldsymbol{R}\left(\boldsymbol{R}^{\prime} \boldsymbol{R}\right)^{-1} \boldsymbol{R}^{\prime}\)</span>. Since <span class="math inline">\(\left[\boldsymbol{Z}_{a}, \boldsymbol{R}\right]\)</span> span <span class="math inline">\(\boldsymbol{Z}\)</span> we find <span class="math inline">\(\boldsymbol{P}=\boldsymbol{P}_{\boldsymbol{R}}+\boldsymbol{P}_{a}\)</span> and <span class="math inline">\(\boldsymbol{P}_{\boldsymbol{R}} \boldsymbol{P}_{a}=0\)</span>. It will be useful to note that</p>
<p><span class="math display">\[
\begin{aligned}
\boldsymbol{P}_{R} \widehat{\boldsymbol{X}} &amp;=\boldsymbol{P}_{\boldsymbol{R}} \boldsymbol{P} \boldsymbol{X}=\boldsymbol{P}_{\boldsymbol{R}} \boldsymbol{X} \\
\widehat{\boldsymbol{X}}^{\prime} \widehat{\boldsymbol{X}}-\widehat{\boldsymbol{X}}^{\prime} \boldsymbol{P}_{\boldsymbol{R}} \widehat{\boldsymbol{X}} &amp;=\boldsymbol{X}^{\prime}\left(\boldsymbol{P}-\boldsymbol{P}_{\boldsymbol{R}}\right) \boldsymbol{X}=\boldsymbol{X}^{\prime} \boldsymbol{P}_{a} \boldsymbol{X}
\end{aligned}
\]</span></p>
<p>The fact that <span class="math inline">\(\boldsymbol{X}^{\prime} \boldsymbol{P} \widehat{\boldsymbol{e}}=\widehat{\boldsymbol{X}}^{\prime} \widehat{\boldsymbol{e}}=0\)</span> implies <span class="math inline">\(\boldsymbol{X}^{\prime} \boldsymbol{P}_{\boldsymbol{R}} \widehat{\boldsymbol{e}}=-\boldsymbol{X}^{\prime} \boldsymbol{P}_{a} \widehat{\boldsymbol{e}}\)</span>. Finally, since <span class="math inline">\(\boldsymbol{Y}=\boldsymbol{X} \widehat{\boldsymbol{\beta}}+\widehat{\boldsymbol{e}}\)</span>,</p>
<p><span class="math display">\[
\widetilde{\boldsymbol{e}}=\left(\boldsymbol{I}_{n}-\boldsymbol{X}\left(\boldsymbol{X}^{\prime} \boldsymbol{P}_{a} \boldsymbol{X}\right)^{-1} \boldsymbol{X}^{\prime} \boldsymbol{P}_{a}\right) \widehat{\boldsymbol{e}}
\]</span></p>
<p>so</p>
<p><span class="math display">\[
\widetilde{\boldsymbol{e}}^{\prime} \boldsymbol{P}_{a} \widetilde{\boldsymbol{e}}=\widehat{\boldsymbol{e}}^{\prime}\left(\boldsymbol{P}_{a}-\boldsymbol{P}_{a} \boldsymbol{X}\left(\boldsymbol{X}^{\prime} \boldsymbol{P}_{a} \boldsymbol{X}\right)^{-1} \boldsymbol{X}^{\prime} \boldsymbol{P}_{a}\right) \widehat{\boldsymbol{e}} .
\]</span></p>
<p>Applying the Woodbury matrix equality to the definition of <span class="math inline">\(N\)</span> and the above algebraic relationships,</p>
<p><span class="math display">\[
\begin{aligned}
N &amp;=\frac{\widehat{\boldsymbol{e}}^{\prime} \boldsymbol{P}_{\boldsymbol{R}} \widehat{\boldsymbol{e}}+\widehat{\boldsymbol{e}}^{\prime} \boldsymbol{P}_{\boldsymbol{R}} \widehat{\boldsymbol{X}}\left(\widehat{\boldsymbol{X}}^{\prime} \widehat{\boldsymbol{X}}-\widehat{\boldsymbol{X}}^{\prime} \boldsymbol{P}_{\boldsymbol{R}} \widehat{\boldsymbol{X}}\right)^{-1} \widehat{\boldsymbol{X}}^{\prime} \boldsymbol{P}_{\boldsymbol{R}} \widehat{\boldsymbol{e}}}{\widehat{\sigma}^{2}} \\
&amp;=\frac{\widehat{\boldsymbol{e}}^{\prime} \boldsymbol{P} \widehat{\boldsymbol{e}}-\widehat{\boldsymbol{e}}^{\prime} \boldsymbol{P}_{a} \widehat{\boldsymbol{e}}+\widehat{\boldsymbol{e}}^{\prime} \boldsymbol{P}_{a} \boldsymbol{X}\left(\boldsymbol{X}^{\prime} \boldsymbol{P}_{a} \boldsymbol{X}\right)^{-1} \boldsymbol{X}^{\prime} \boldsymbol{P}_{a} \widehat{\boldsymbol{e}}}{\widehat{\sigma}^{2}} \\
&amp;=\frac{\widehat{\boldsymbol{e}}^{\prime} \boldsymbol{P} \widehat{\boldsymbol{e}}-\widetilde{\boldsymbol{e}}^{\prime} \boldsymbol{P}_{a} \widetilde{\boldsymbol{e}}}{\widehat{\sigma}^{2}} \\
&amp;=C^{*}
\end{aligned}
\]</span></p>
<p>as claimed.</p>
<p>We next establish the asymptotic distribution. Since <span class="math inline">\(\boldsymbol{Z}_{a}\)</span> is a subset of <span class="math inline">\(\boldsymbol{Z}, \boldsymbol{P}_{a}=\boldsymbol{M}_{a} \boldsymbol{P}\)</span>, thus <span class="math inline">\(\boldsymbol{P} \boldsymbol{R}=\boldsymbol{R}\)</span> and <span class="math inline">\(\boldsymbol{R}^{\prime} \boldsymbol{X}=\boldsymbol{R}^{\prime} \widehat{\boldsymbol{X}}\)</span>. Consequently</p>
<p><span class="math display">\[
\begin{aligned}
\frac{1}{\sqrt{n}} \boldsymbol{R}^{\prime} \widehat{\boldsymbol{e}} &amp;=\frac{1}{\sqrt{n}} \boldsymbol{R}^{\prime}(\boldsymbol{Y}-\boldsymbol{X} \widehat{\boldsymbol{\beta}}) \\
&amp;=\frac{1}{\sqrt{n}} \boldsymbol{R}^{\prime}\left(\boldsymbol{I}_{n}-\boldsymbol{X}\left(\widehat{\boldsymbol{X}}^{\prime} \widehat{\boldsymbol{X}}\right)^{-1} \widehat{\boldsymbol{X}}^{\prime}\right) \boldsymbol{e} \\
&amp;=\frac{1}{\sqrt{n}} \boldsymbol{R}^{\prime}\left(\boldsymbol{I}_{n}-\widehat{\boldsymbol{X}}\left(\widehat{\boldsymbol{X}}^{\prime} \widehat{\boldsymbol{X}}\right)^{-1} \widehat{\boldsymbol{X}}^{\prime}\right) \boldsymbol{e} \\
&amp; \underset{d}{\longrightarrow} \mathrm{N}\left(0, \boldsymbol{V}_{2}\right)
\end{aligned}
\]</span></p>
<p>where</p>
<p><span class="math display">\[
\boldsymbol{V}_{2}=\operatorname{plim}_{n \rightarrow \infty}\left(\frac{1}{n} \boldsymbol{R}^{\prime} \boldsymbol{R}-\frac{1}{n} \boldsymbol{R}^{\prime} \widehat{\boldsymbol{X}}\left(\frac{1}{n} \widehat{\boldsymbol{X}}^{\prime} \widehat{\boldsymbol{X}}\right)^{-1} \frac{1}{n} \widehat{\boldsymbol{X}}^{\prime} \boldsymbol{R}\right) .
\]</span></p>
<p>It follows that <span class="math inline">\(N=C^{*} \underset{d}{\longrightarrow} \chi_{\ell_{b}}^{2}\)</span> as claimed. Since <span class="math inline">\(C=C^{*}+o_{p}(1)\)</span> it has the same limiting distribution.</p>
</section>
<section id="bootstrap-overidentification-tests" class="level2" data-number="12.33">
<h2 data-number="12.33" class="anchored" data-anchor-id="bootstrap-overidentification-tests"><span class="header-section-number">12.33</span> Bootstrap Overidentification Tests</h2>
<p>In small to moderate sample sizes the overidentification tests are not well approximated by the asymptotic chi-square distributions. For improved accuracy it is advised to use bootstrap critical values. The bootstrap for 2SLS (Section 12.23) can be used for this purpose but the bootstrap version of the overidentification statistic must be adjusted. This is because in the bootstrap universe the overidentified moment conditions are not satisfied. One solution is to center the moment conditions. For the 2SLS estimator the standard overidentification test is based on the Sargan statistic</p>
<p><span class="math display">\[
\begin{aligned}
&amp;S=n \frac{\widehat{\boldsymbol{e}}^{\prime} \boldsymbol{Z}\left(\boldsymbol{Z}^{\prime} \boldsymbol{Z}\right)^{-1} \boldsymbol{Z}^{\prime} \widehat{\boldsymbol{e}}}{\widehat{\boldsymbol{e}}^{\prime} \widehat{\boldsymbol{e}}} \\
&amp;\widehat{\boldsymbol{e}}=\boldsymbol{Y}-\boldsymbol{X} \widehat{\beta}_{2 s l s}
\end{aligned}
\]</span></p>
<p>The recentered bootstrap analog is</p>
<p><span class="math display">\[
\begin{aligned}
S^{* *} &amp;=n \frac{\left(\widehat{\boldsymbol{e}}^{* \prime} \boldsymbol{Z}^{*}-\boldsymbol{Z}^{\prime} \widehat{\boldsymbol{e}}\right)\left(\boldsymbol{Z}^{* \prime} \boldsymbol{Z}^{*}\right)^{-1}\left(\boldsymbol{Z}^{* \prime} \widehat{\boldsymbol{e}}^{*}-\boldsymbol{Z}^{\prime} \widehat{\boldsymbol{e}}\right)}{\widehat{\boldsymbol{e}}^{*} \widehat{\boldsymbol{e}}^{*}} \\
\widehat{\boldsymbol{e}}^{*} &amp;=\boldsymbol{Y}^{*}-\boldsymbol{X}^{*} \widehat{\beta}_{2 \mathrm{sls}}^{*}
\end{aligned}
\]</span></p>
<p>On each bootstrap sample <span class="math inline">\(S^{* *}(b)\)</span> is calculated and stored. The bootstrap p-value is</p>
<p><span class="math display">\[
p^{*}=\frac{1}{B} \sum_{b=1}^{B} \mathbb{1}\left\{S^{* *}(b)&gt;S\right\} .
\]</span></p>
<p>This bootstrap <span class="math inline">\(\mathrm{p}\)</span>-value is valid because the statistic <span class="math inline">\(S^{* *}\)</span> satisfies the overidentified moment conditions.</p>
</section>
<section id="local-average-treatment-effects" class="level2" data-number="12.34">
<h2 data-number="12.34" class="anchored" data-anchor-id="local-average-treatment-effects"><span class="header-section-number">12.34</span> Local Average Treatment Effects</h2>
<p>In a pair of influential papers, Imbens and Angrist (1994) and Angrist, Imbens and Rubin (1996) proposed an new interpretation of the instrumental variables estimator using the potential outcomes model introduced in Section 2.30.</p>
<p>We will restrict attention to the case that the endogenous regressor <span class="math inline">\(X\)</span> and excluded instrument <span class="math inline">\(Z\)</span> are binary variables. We write the model as a pair of potential outcome functions. The dependent variable <span class="math inline">\(Y\)</span> is a function of the regressor and an unobservable vector <span class="math inline">\(U, Y=h(X, U)\)</span>, and the endogenous regressor <span class="math inline">\(X\)</span> is a function of the instrument <span class="math inline">\(Z\)</span> and <span class="math inline">\(U, X=g(Z, U)\)</span>. By specifying <span class="math inline">\(U\)</span> as a vector there is no loss of generality in letting both equations depend on <span class="math inline">\(U\)</span>.</p>
<p>In this framework the outcomes are determined by the random vector <span class="math inline">\(U\)</span> and the exogenous instrument <span class="math inline">\(Z\)</span>. This determines <span class="math inline">\(X\)</span> which determines <span class="math inline">\(Y\)</span>. To put this in the context of the college proximity example the variable <span class="math inline">\(U\)</span> is everything specific about an individual. Given college proximity <span class="math inline">\(Z\)</span> the person decides to attend college or not. The person’s wage is determined by the individual attributes <span class="math inline">\(U\)</span> as well as college attendence <span class="math inline">\(X\)</span> but is not directly affected by college proximity <span class="math inline">\(Z\)</span>.</p>
<p>We can omit the random variable <span class="math inline">\(U\)</span> from the notation as follows. An individual has a realization <span class="math inline">\(U\)</span>. We then set <span class="math inline">\(Y(x)=h(x, U)\)</span> and <span class="math inline">\(X(z)=g(z, U)\)</span>. Also, given a realization <span class="math inline">\(Z\)</span> the observables are <span class="math inline">\(X=X(Z)\)</span> and <span class="math inline">\(Y=Y(X)\)</span>.</p>
<p>In this model the causal effect of college for an individual is <span class="math inline">\(C=Y(1)-Y(0)\)</span>. As discussed in Section <span class="math inline">\(2.30\)</span>, this is individual-specific and random.</p>
<p>We would like to learn about the distribution of the causal effects, or at least features of the distribution. A common feature of interest is the average treatment effect (ATE)</p>
<p><span class="math display">\[
\operatorname{ATE}=\mathbb{E}[C]=\mathbb{E}[Y(1)-Y(0)] .
\]</span></p>
<p>This, however, it typically not feasible to estimate allowing for endogenous <span class="math inline">\(X\)</span> without strong assumptions (such as that the causal effect <span class="math inline">\(C\)</span> is constant across individuals). The treatment effect literature has explored what features of the distribution of <span class="math inline">\(C\)</span> can be estimated. One particular feature of interest emphasized by Imbens and Angrist (1994) is the local average treatment effect (LATE). Roughly, this is the average effect upon those effected by the instrumental variable. To understand LATE, consider the college proximity example. In the potential outcomes framework each person is fully characterized by their individual unobservable <span class="math inline">\(U\)</span>. Given <span class="math inline">\(U\)</span>, their decision to attend college is a function of the proximity indicator <span class="math inline">\(Z\)</span>. For some students, proximity has no effect on their decision. For other students, it has an effect in the specific sense that given <span class="math inline">\(Z=1\)</span> they choose to attend college while if <span class="math inline">\(Z=0\)</span> they choose to not attend. We can summarize the possibilites with the following chart which is based on labels developed by Angrist, Imbens and Rubin (1996).</p>
<p><span class="math display">\[
\begin{array}{ccc}
&amp; X(0)=0 &amp; X(0)=1 \\
X(1)=0 &amp; \text { Never Takers } &amp; \text { Defiers } \\
X(1)=1 &amp; \text { Compliers } &amp; \text { Always Takers }
\end{array}
\]</span></p>
<p>The columns indicate the college attendence decision given <span class="math inline">\(Z=0\)</span> (not close to a college). The rows indicate the college attendence decision given <span class="math inline">\(Z=1\)</span> (close to a college). The four entries are labels for the four types of individuals based on these decisions. The upper-left entry are the individuals who do not attend college regardless of <span class="math inline">\(Z\)</span>. They are called “Never Takers”. The lower-right entry are the individuals who conversely attend college regardless of <span class="math inline">\(Z\)</span>. They are called “Always Takers”. The bottom left are the individuals who only attend college if they live close to one. They are called “Compliers”. The upper right entry is a bit of a challenge. These are individuals who attend college only if they do not live close to one. They are called “Dediers”. Imbens and Angrist discovered that to identify the parameters of interest we need to assume that there are no Dediers, or equivalently that <span class="math inline">\(X(1) \geq X(0)\)</span>. They call this a “monotonicity” condition - increasing the instrument does not decrease <span class="math inline">\(X\)</span> for any individual.</p>
<p>As another example, suppose we are interested in the effect of wearing a face mask <span class="math inline">\(X\)</span> on health <span class="math inline">\(Y\)</span> during a virus pandemic. Wearing a face mask is a choice made by the individual so should be viewed as endogenous. For an instrument <span class="math inline">\(Z\)</span> consider a government policy that requires face masks to be worn in public. The “Compliers” are those who wear a face mask if there is a policy but otherwise do not. The “Deniers” are those who do the converse. That is, these individuals would have worn a face mask based on the evidence of a pandemic but rebel against a government policy. Once again, identification requires that there are no Deniers.</p>
<p>We can distinguish the types in the table by the relative values of <span class="math inline">\(X(1)-X(0)\)</span>. For Never-Takers and Always-Takers <span class="math inline">\(X(1)-X(0)=0\)</span>, while for Compliers <span class="math inline">\(X(1)-X(0)=1\)</span>.</p>
<p>We are interested in the causal effect <span class="math inline">\(C=h(1, U)-h(0, U)\)</span> of college on wages. The average causal effect (ACE) is its expectation <span class="math inline">\(\mathbb{E}[Y(1)-Y(0)]\)</span>. To estimate the ACE we need observations of both <span class="math inline">\(Y(0)\)</span> and <span class="math inline">\(Y\)</span> (1) which means we need to observe some individuals who attend college and some who do not attend college. Consider the group “Never-Takers”. They never attend college so we only observe <span class="math inline">\(Y(0)\)</span>. It is thus impossible to estimate the ACE of college for this group. Similarly consider the group “Always-Takers”. They always attend college so we only observe <span class="math inline">\(Y(1)\)</span> and again we cannot estimate the ACE of college for this group. The group for which we can estimate the ACE are the “Compliers”. The ACE for this group is</p>
<p><span class="math display">\[
\text { LATE }=\mathbb{E}[Y(1)-Y(0) \mid X(1)&gt;X(0)] .
\]</span></p>
<p>Imbens and Angrist call this the local average treatment effect (LATE) as it is the average treatment effect for the sub-population whose endogenous regressor is affected by the instrument. Examining the definition, the LATE is the average causal effect of college attendence on wages for the sub-sample of individuals who choose to attend college if (and only if) they live close to one.</p>
<p>Interestingly, we show below that</p>
<p><span class="math display">\[
\text { LATE }=\frac{\mathbb{E}[Y \mid Z=1]-\mathbb{E}[Y \mid Z=0]}{\mathbb{E}[X \mid Z=1]-\mathbb{E}[X \mid Z=0]} .
\]</span></p>
<p>That is, LATE equals the Wald expression (12.27) for the slope coefficient in the IV regression model. This means that the standard IV estimator is an estimator of LATE. Thus when treatment effects are potentially heterogeneous we can interpret IV as an estimator of LATE. The equality (12.68) occurs under the following conditions.</p>
<p>Assumption 12.3 <span class="math inline">\(U\)</span> and <span class="math inline">\(Z\)</span> are independent and <span class="math inline">\(\mathbb{P}[X(1)-X(0)&lt;0]=0 .\)</span></p>
<p>One interesting feature about LATE is that its value can depend on the instrument <span class="math inline">\(Z\)</span> and the distribution of causal effects <span class="math inline">\(C\)</span> in the population. To make this concrete suppose that instead of the Card proximity instrument we consider an instrument based on the financial cost of local college attendence. It is reasonable to expect that while the set of students affected by these two instruments are similar the two sets of students will not be the same. That is, some students may be responsive to proximity but not finances, and conversely. If the causal effect <span class="math inline">\(C\)</span> has a different average in these two groups of students then LATE will be different when calculated with these two instruments. Thus LATE can vary by the choice of instrument.</p>
<p>How can that be? How can a well-defined parameter depend on the choice of instrument? Doesn’t this contradict the basic IV regression model? The answer is that the basic IV regression model is restrictive - it specifies that the causal effect <span class="math inline">\(\beta\)</span> is common across all individuals. Its value is the same regardless of the choice of specific instrument (so long as it satisfies the instrumental variables assumptions). In contrast, the potential outcomes framework is more general allowing for the causal effect to vary across individuals. What this analysis shows us is that in this context is quite possible for the LATE coefficient to vary by instrument. This occurs when causal effects are heterogeneous.</p>
<p>One implication of the LATE framework is that IV estimates should be interpreted as causal effects only for the population of compliers. Interpretation should focus on the population of potential compliers and extension to other populations should be done with caution. For example, in the Card proximity model the IV estimates of the causal return to schooling presented in Table <span class="math inline">\(12.1\)</span> should be interpreted as applying to the population of students who are incentivized to attend college by the presence of a college within their home county. The estimates should not be applied to other students.</p>
<p>Formally, the analysis of this section examined the case of a binary instrument and endogenous regressor. How does this generalize? Suppose that the regressor <span class="math inline">\(X\)</span> is discrete, taking <span class="math inline">\(J+1\)</span> discrete values. We can then rewrite the model as one with <span class="math inline">\(J\)</span> binary endogenous regressors. If we then have <span class="math inline">\(J\)</span> binary instruments we are back in the Imbens-Angrist framework (assuming the instruments have a monotonic impact on the endogenous regressors). A benefit is that with a larger set of instruments it is plausible that the set of compliers in the population is expanded.</p>
<p>We close this section by showing (12.68) under Assumption 12.3. The realized value of <span class="math inline">\(X\)</span> can be written as</p>
<p><span class="math display">\[
X=(1-Z) X(0)+Z X(1)=X(0)+Z(X(1)-X(0))
\]</span></p>
<p>Similarly</p>
<p><span class="math display">\[
Y=Y(0)+X(Y(1)-Y(0))=Y(0)+X C .
\]</span></p>
<p>Combining,</p>
<p><span class="math display">\[
Y=Y(0)+X(0) C+Z(X(1)-Y(0)) C .
\]</span></p>
<p>The independence of <span class="math inline">\(u\)</span> and <span class="math inline">\(Z\)</span> implies independence of <span class="math inline">\((Y(0), Y(1), X(0), X(1), C)\)</span> and <span class="math inline">\(Z\)</span>. Thus</p>
<p><span class="math display">\[
\mathbb{E}[Y \mid Z=1]=\mathbb{E}[Y(0)]+\mathbb{E}[X(0) C]+\mathbb{E}[(X(1)-X(0)) C]
\]</span></p>
<p>and</p>
<p><span class="math display">\[
\mathbb{E}[Y \mid Z=0]=\mathbb{E}[Y(0)]+\mathbb{E}[X(0) C] .
\]</span></p>
<p>Subtracting we obtain</p>
<p><span class="math display">\[
\begin{aligned}
\mathbb{E}[Y \mid Z=1]-\mathbb{E}[Y \mid Z=0] &amp;=\mathbb{E}[(X(1)-X(0)) C] \\
&amp;=1 \times \mathbb{E}[C \mid X(1)-X(0)=1] \mathbb{P}[X(1)-X(0)=1] \\
&amp;+0 \times \mathbb{E}[C \mid X(1)-X(0)=0] \mathbb{P}[X(1)-X(0)=0] \\
&amp;+(-1) \times \mathbb{E}[C \mid X(1)-X(0)=-1] \mathbb{P}[X(1)-X(0)=-1] \\
&amp;=\mathbb{E}[C \mid X(1)-X(0)=1](\mathbb{E}[X \mid X=1]-\mathbb{E}[X \mid Z=0])
\end{aligned}
\]</span></p>
<p>where the final equality uses <span class="math inline">\(\mathbb{P}[X(1)-X(0)&lt;0]=0\)</span> and</p>
<p><span class="math display">\[
\mathbb{P}[X(1)-X(0)=1]=\mathbb{E}[X(1)-X(0)]=\mathbb{E}[X \mid Z=1]-\mathbb{E}[X \mid Z=0] .
\]</span></p>
<p>Rearranging</p>
<p><span class="math display">\[
\mathrm{LATE}=\mathbb{E}[C \mid X(1)-X(0)=1]=\frac{\mathbb{E}[Y \mid Z=1]-\mathbb{E}[Y \mid Z=0]}{\mathbb{E}[X \mid Z=1]-\mathbb{E}[X \mid Z=0]}
\]</span></p>
<p>as claimed.</p>
</section>
<section id="identification-failure" class="level2" data-number="12.35">
<h2 data-number="12.35" class="anchored" data-anchor-id="identification-failure"><span class="header-section-number">12.35</span> Identification Failure</h2>
<p>Recall the reduced form equation</p>
<p><span class="math display">\[
X_{2}=\Gamma_{12}^{\prime} Z_{1}+\Gamma_{22}^{\prime} Z_{2}+u_{2} .
\]</span></p>
<p>The parameter <span class="math inline">\(\beta\)</span> fails to be identified if <span class="math inline">\(\Gamma_{22}\)</span> has deficient rank. The consequences of identification failure for inference are quite severe.</p>
<p>Take the simplest case where <span class="math inline">\(k_{1}=0\)</span> and <span class="math inline">\(k_{2}=\ell_{2}=1\)</span>. Then the model may be written as</p>
<p><span class="math display">\[
\begin{aligned}
&amp;Y=X \beta+e \\
&amp;X=Z \gamma+u
\end{aligned}
\]</span></p>
<p>and <span class="math inline">\(\Gamma_{22}=\gamma=\mathbb{E}[Z X] / \mathbb{E}\left[Z^{2}\right]\)</span>. We see that <span class="math inline">\(\beta\)</span> is identified if and only if <span class="math inline">\(\gamma \neq 0\)</span>, which occurs when <span class="math inline">\(\mathbb{E}[X Z] \neq 0\)</span>. Thus identification hinges on the existence of correlation between the excluded exogenous variable and the included endogenous variable.</p>
<p>Suppose this condition fails. In this case <span class="math inline">\(\gamma=0\)</span> and <span class="math inline">\(\mathbb{E}[X Z]=0\)</span>. We now analyze the distribution of the least squares and IV estimators of <span class="math inline">\(\beta\)</span>. For simplicity we assume conditional homoskedasticity and normalize the variances of <span class="math inline">\(e, u\)</span>, and <span class="math inline">\(Z\)</span> to unity. Thus</p>
<p><span class="math display">\[
\operatorname{var}\left[\left(\begin{array}{c}
e \\
u
\end{array}\right) \mid Z\right]=\left(\begin{array}{ll}
1 &amp; \rho \\
\rho &amp; 1
\end{array}\right) .
\]</span></p>
<p>The errors have non-zero correlation <span class="math inline">\(\rho \neq 0\)</span> when the variables are endogenous.</p>
<p>By the CLT we have the joint convergence</p>
<p><span class="math display">\[
\frac{1}{\sqrt{n}} \sum_{i=1}^{n}\left(\begin{array}{c}
Z_{i} e_{i} \\
Z_{i} u_{i}
\end{array}\right) \underset{d}{ }\left(\begin{array}{l}
\xi_{1} \\
\xi_{2}
\end{array}\right) \sim \mathrm{N}\left(0,\left(\begin{array}{cc}
1 &amp; \rho \\
\rho &amp; 1
\end{array}\right)\right) .
\]</span></p>
<p>It is convenient to define <span class="math inline">\(\xi_{0}=\xi_{1}-\rho \xi_{2}\)</span> which is normal and independent of <span class="math inline">\(\xi_{2}\)</span>. As a benchmark it is useful to observe that the least squares estimator of <span class="math inline">\(\beta\)</span> satisfies</p>
<p><span class="math display">\[
\widehat{\beta}_{\mathrm{ols}}-\beta=\frac{n^{-1} \sum_{i=1}^{n} u_{i} e_{i}}{n^{-1} \sum_{i=1}^{n} u_{i}^{2}} \underset{p}{\longrightarrow} \rho \neq 0
\]</span></p>
<p>so endogeneity causes <span class="math inline">\(\widehat{\beta}_{\text {ols }}\)</span> to be inconsistent for <span class="math inline">\(\beta\)</span>.</p>
<p>Under identification failure <span class="math inline">\(\gamma=0\)</span> the asymptotic distribution of the IV estimator is</p>
<p><span class="math display">\[
\widehat{\beta}_{\mathrm{iv}}-\beta=\frac{\frac{1}{\sqrt{n}} \sum_{i=1}^{n} Z_{i} e_{i}}{\frac{1}{\sqrt{n}} \sum_{i=1}^{n} Z_{i} X_{i}} \underset{\mathrm{d}}{\xi_{2}}=\rho+\frac{\xi_{0}}{\xi_{2}} .
\]</span></p>
<p>This asymptotic convergence result uses the continuous mapping theorem which applies since the function <span class="math inline">\(\xi_{1} / \xi_{2}\)</span> is continuous everywhere except at <span class="math inline">\(\xi_{2}=0\)</span>, which occurs with probability equal to zero.</p>
<p>This limiting distribution has several notable features.</p>
<p>First, <span class="math inline">\(\widehat{\beta}_{\mathrm{iv}}\)</span> does not converge in probability to a limit, rather it converges in distribution to a random variable. Thus the IV estimator is inconsistent. Indeed, it is not possible to consistently estimate an unidentified parameter and <span class="math inline">\(\beta\)</span> is not identified when <span class="math inline">\(\gamma=0\)</span>.</p>
<p>Second, the ratio <span class="math inline">\(\xi_{0} / \xi_{2}\)</span> is symmetrically distributed about zero so the median of the limiting distribution of <span class="math inline">\(\widehat{\beta}_{\text {iv }}\)</span> is <span class="math inline">\(\beta+\rho\)</span>. This means that the IV estimator is median biased under endogeneity. Thus under identification failure the IV estimator does not correct the centering (median bias) of least squares.</p>
<p>Third, the ratio <span class="math inline">\(\xi_{0} / \xi_{2}\)</span> of two independent normal random variables is Cauchy distributed. This is particularly nasty as the Cauchy distribution does not have a finite mean. The distribution has thick tails meaning that extreme values occur with higher frequency than the normal. Inferences based on the normal distribution can be quite incorrect.</p>
<p>Together, these results show that <span class="math inline">\(\gamma=0\)</span> renders the IV estimator particularly poorly behaved - it is inconsistent, median biased, and non-normally distributed.</p>
<p>We can also examine the behavior of the t-statistic. For simplicity consider the classical (homoskedastic) t-statistic. The error variance estimate has the asymptotic distribution</p>
<p><span class="math display">\[
\begin{aligned}
&amp; \widehat{\sigma}^{2}=\frac{1}{n} \sum_{i=1}^{n}\left(Y_{i}-X_{i} \widehat{\beta}_{\mathrm{iv}}\right)^{2} \\
&amp; =\frac{1}{n} \sum_{i=1}^{n} e_{i}^{2}-\frac{2}{n} \sum_{i=1}^{n} e_{i} X_{i}\left(\widehat{\beta}_{\mathrm{iv}}-\beta\right)+\frac{1}{n} \sum_{i=1}^{n} X_{i}^{2}\left(\widehat{\beta}_{\mathrm{iv}}-\beta\right)^{2} \\
&amp; \underset{d}{\longrightarrow} 1-2 \rho \frac{\xi_{1}}{\xi_{2}}+\left(\frac{\xi_{1}}{\xi_{2}}\right)^{2} \text {. }
\end{aligned}
\]</span></p>
<p>Thus the t-statistic has the asymptotic distribution</p>
<p><img src="images//2022_09_17_f9391324ededdbb7a34eg-55.jpg" class="img-fluid"></p>
<p>The limiting distribution is non-normal, meaning that inference using the normal distribution will be (considerably) incorrect. This distribution depends on the correlation <span class="math inline">\(\rho\)</span>. The distortion is increasing in <span class="math inline">\(\rho\)</span>. Indeed as <span class="math inline">\(\rho \rightarrow 1\)</span> we have <span class="math inline">\(\xi_{1} / \xi_{2} \rightarrow p 1\)</span> and the unexpected finding <span class="math inline">\(\widehat{\sigma}^{2} \rightarrow{ }_{p} 0\)</span>. The latter means that the conventional standard error <span class="math inline">\(s\left(\widehat{\beta}_{\text {iv }}\right)\)</span> for <span class="math inline">\(\widehat{\beta}_{\text {iv }}\)</span> also converges in probability to zero. This implies that the t-statistic diverges in the sense <span class="math inline">\(|T| \rightarrow p \infty\)</span>. In this situations users may incorrectly interpret estimates as precise despite the fact that they are highly imprecise.</p>
</section>
<section id="weak-instruments" class="level2" data-number="12.36">
<h2 data-number="12.36" class="anchored" data-anchor-id="weak-instruments"><span class="header-section-number">12.36</span> Weak Instruments</h2>
<p>In the previous section we examined the extreme consequences of full identification failure. Similar problems occur when identification is weak in the sense that the reduced form coefficients are of small magnitude. In this section we derive the asymptotic distribution of the OLS, 2SLS, and LIML estimators when the reduced form coefficients are treated as weak. We show that the estimators are inconsistent and the 2SLS and LIML estimators remain random in large samples.</p>
<p>To simplify the exposition we assume that there are no included exogenous variables (no <span class="math inline">\(X_{1}\)</span> ) so we write <span class="math inline">\(X_{2}, Z_{2}\)</span>, and <span class="math inline">\(\beta_{2}\)</span> simply as <span class="math inline">\(X, Z\)</span>, and <span class="math inline">\(\beta\)</span>. The model is</p>
<p><span class="math display">\[
\begin{aligned}
&amp;Y=X^{\prime} \beta+e \\
&amp;X=\Gamma^{\prime} Z+u_{2} .
\end{aligned}
\]</span></p>
<p>Recall the reduced form error vector <span class="math inline">\(u=\left(u_{1}, u_{2}\right)\)</span> and its covariance matrix</p>
<p><span class="math display">\[
\mathbb{E}\left[u u^{\prime}\right]=\Sigma=\left[\begin{array}{cc}
\Sigma_{11} &amp; \Sigma_{12} \\
\Sigma_{21} &amp; \Sigma_{22}
\end{array}\right] .
\]</span></p>
<p>Recall that the structural error is <span class="math inline">\(e=u_{1}-\beta^{\prime} u_{2}=\gamma^{\prime} u\)</span> where <span class="math inline">\(\gamma=(1,-\beta)\)</span> which has variance <span class="math inline">\(\mathbb{E}\left[e^{2} \mid Z\right]=\)</span> <span class="math inline">\(\gamma^{\prime} \Sigma \gamma\)</span>. Also define the covariance <span class="math inline">\(\Sigma_{2 e}=\mathbb{E}\left[u_{2} e \mid Z\right]=\Sigma_{21}-\Sigma_{22} \beta\)</span>.</p>
<p>In Section <span class="math inline">\(12.35\)</span> we assumed complete identification failure in the sense that <span class="math inline">\(\Gamma=0\)</span>. We now want to assume that identification does not completely fail but is weak in the sense that <span class="math inline">\(\Gamma\)</span> is small. A rich asymptotic distribution theory has been developed to understand this setting by modeling <span class="math inline">\(\Gamma\)</span> as “localto-zero”. The seminal contribution is Staiger and Stock (1997). The theory was extended to nonlinear GMM estimation by Stock and Wright (2000).</p>
<p>The technical device introduced by Staiger and Stock (1997) is to assume that the reduced form parameter is local-to-zero, specifically</p>
<p><span class="math display">\[
\Gamma=n^{-1 / 2} \boldsymbol{C}
\]</span></p>
<p>where <span class="math inline">\(\boldsymbol{C}\)</span> is a free matrix. The <span class="math inline">\(n^{-1 / 2}\)</span> scaling is picked because it provides just the right balance to allow a useful distribution theory. The local-to-zero assumption (12.71) is not meant to be taken literally but rather is meant to be a useful distributional approximation. The parameter <span class="math inline">\(\boldsymbol{C}\)</span> indexes the degree of identification. Larger <span class="math inline">\(\|\boldsymbol{C}\|\)</span> implies stronger identification; smaller <span class="math inline">\(\|\boldsymbol{C}\|\)</span> implies weaker identification.</p>
<p>We now derive the asymptotic distribution of the least squares, 2SLS, and LIML estimators under the local-to-unity assumption (12.71).</p>
<p>The least squares estimator satisfies</p>
<p><span class="math display">\[
\begin{aligned}
\widehat{\beta}_{\mathrm{ols}}-\beta &amp;=\left(n^{-1} \boldsymbol{X}^{\prime} \boldsymbol{X}\right)^{-1}\left(n^{-1} \boldsymbol{X}^{\prime} \boldsymbol{e}\right) \\
&amp;=\left(n^{-1} \boldsymbol{U}_{2}^{\prime} \boldsymbol{U}_{2}\right)^{-1}\left(n^{-1} \boldsymbol{U}_{2}^{\prime} \boldsymbol{e}\right)+o_{p}(1) \\
&amp; \longrightarrow \underset{22}{-1} \Sigma_{2 e} .
\end{aligned}
\]</span></p>
<p>Thus the least squares estimator is inconsistent for <span class="math inline">\(\beta\)</span>.</p>
<p>To examine the 2SLS estimator, by the central limit theorem</p>
<p><span class="math display">\[
\frac{1}{\sqrt{n}} \sum_{i=1}^{n} Z_{i} u_{i}^{\prime} \underset{d}{\longrightarrow} \xi=\left[\xi_{1}, \xi_{2}\right]
\]</span></p>
<p>where</p>
<p><span class="math display">\[
\operatorname{vec}(\xi) \sim \mathrm{N}\left(0, \mathbb{E}\left[u u^{\prime} \otimes Z Z^{\prime}\right]\right)
\]</span></p>
<p>This implies</p>
<p><span class="math display">\[
\frac{1}{\sqrt{n}} \boldsymbol{Z}^{\prime} \boldsymbol{e} \longrightarrow \underset{d}{\xi_{e}}=\xi \gamma
\]</span></p>
<p>We also find that</p>
<p><span class="math display">\[
\frac{1}{\sqrt{n}} \boldsymbol{Z}^{\prime} \boldsymbol{X}=\frac{1}{n} \boldsymbol{Z}^{\prime} \boldsymbol{Z} \boldsymbol{C}+\frac{1}{\sqrt{n}} \boldsymbol{Z}^{\prime} \boldsymbol{U}_{2} \underset{d}{\longrightarrow} \boldsymbol{Q}_{Z} \boldsymbol{C}+\xi_{2} .
\]</span></p>
<p>Thus</p>
<p><span class="math display">\[
\boldsymbol{X}^{\prime} \boldsymbol{P}_{\boldsymbol{Z}} \boldsymbol{X}=\left(\frac{1}{\sqrt{n}} \boldsymbol{X}^{\prime} \boldsymbol{Z}\right)\left(\frac{1}{n} \boldsymbol{Z}^{\prime} \boldsymbol{Z}\right)^{-1}\left(\frac{1}{\sqrt{n}} \boldsymbol{Z}^{\prime} \boldsymbol{X}\right) \underset{d}{\longrightarrow}\left(\boldsymbol{Q}_{Z} \boldsymbol{C}+\xi_{2}\right)^{\prime} \boldsymbol{Q}_{Z}^{-1}\left(\boldsymbol{Q}_{Z} \boldsymbol{C}+\xi_{2}\right)
\]</span></p>
<p>and</p>
<p><span class="math display">\[
\boldsymbol{X}^{\prime} \boldsymbol{P}_{\boldsymbol{Z}} \boldsymbol{e}=\left(\frac{1}{\sqrt{n}} \boldsymbol{X}^{\prime} \boldsymbol{Z}\right)\left(\frac{1}{n} \boldsymbol{Z}^{\prime} \boldsymbol{Z}\right)^{-1}\left(\frac{1}{\sqrt{n}} \boldsymbol{Z}^{\prime} \boldsymbol{e}\right) \underset{d}{\longrightarrow}\left(\boldsymbol{Q}_{Z} \boldsymbol{C}+\xi_{2}\right)^{\prime} \boldsymbol{Q}_{Z}^{-1} \xi_{e}
\]</span></p>
<p>We find that the 2SLS estimator has the asymptotic distribution</p>
<p><span class="math display">\[
\begin{aligned}
\widehat{\beta}_{2 \text { sls }}-\beta &amp;=\left(\boldsymbol{X}^{\prime} \boldsymbol{P}_{\boldsymbol{Z}} \boldsymbol{X}\right)^{-1}\left(\boldsymbol{X}^{\prime} \boldsymbol{P}_{\boldsymbol{Z}} \boldsymbol{e}\right) \\
&amp; \longrightarrow\left(\left(\boldsymbol{Q}_{Z} \boldsymbol{C}+\xi_{2}\right)^{\prime} \boldsymbol{Q}_{Z}^{-1}\left(\boldsymbol{Q}_{Z} \boldsymbol{C}+\xi_{2}\right)\right)^{-1}\left(\boldsymbol{Q}_{Z} \boldsymbol{C}+\xi_{2}\right)^{\prime} \boldsymbol{Q}_{Z}^{-1} \xi_{e} .
\end{aligned}
\]</span></p>
<p>As in the case of complete identification failure we find that <span class="math inline">\(\widehat{\beta}_{2 s l s}\)</span> is inconsistent for <span class="math inline">\(\beta\)</span>, it is asymptotically random, and its asymptotic distribution is non-normal. The distortion is affected by the coefficient <span class="math inline">\(\boldsymbol{C}\)</span>. As <span class="math inline">\(\|\boldsymbol{C}\| \rightarrow \infty\)</span> the distribution in (12.72) converges in probability to zero suggesting that <span class="math inline">\(\widehat{\beta}_{2 \text { sls }}\)</span> is consistent for <span class="math inline">\(\beta\)</span>. This corresponds to the classic “strong identification” context.</p>
<p>Now consider the LIML estimator. The reduced form is <span class="math inline">\(\overrightarrow{\boldsymbol{Y}}=\boldsymbol{Z \Pi}+\boldsymbol{U}\)</span>. This implies <span class="math inline">\(\boldsymbol{M}_{Z} \overrightarrow{\boldsymbol{Y}}=\boldsymbol{M}_{Z} \boldsymbol{U}\)</span> and by standard asymptotic theory</p>
<p><span class="math display">\[
\frac{1}{n} \overrightarrow{\boldsymbol{Y}}^{\prime} \boldsymbol{M}_{\boldsymbol{Z}} \overrightarrow{\boldsymbol{Y}}=\frac{1}{n} \boldsymbol{U}^{\prime} \boldsymbol{M}_{\boldsymbol{Z}} \boldsymbol{U} \underset{p}{\longrightarrow}=\mathbb{E}\left[u u^{\prime}\right] .
\]</span></p>
<p>Define <span class="math inline">\(\bar{\beta}=\left[\beta, \boldsymbol{I}_{k}\right]\)</span> so that the reduced form coefficients equal <span class="math inline">\(\Pi=[\boldsymbol{\Gamma} \beta, \boldsymbol{\Gamma}]=n^{-1 / 2} \boldsymbol{C} \bar{\beta}\)</span>. Then</p>
<p><span class="math display">\[
\frac{1}{\sqrt{n}} \boldsymbol{Z}^{\prime} \overrightarrow{\boldsymbol{Y}}=\frac{1}{n} \boldsymbol{Z}^{\prime} \boldsymbol{Z} \boldsymbol{C} \bar{\beta}+\frac{1}{\sqrt{n}} \boldsymbol{Z}^{\prime} \boldsymbol{U} \underset{d}{\longrightarrow} \boldsymbol{Q}_{Z} \boldsymbol{C} \bar{\beta}+\xi
\]</span></p>
<p>and</p>
<p><span class="math display">\[
\overrightarrow{\boldsymbol{Y}}^{\prime} \boldsymbol{Z}\left(\boldsymbol{Z}^{\prime} \boldsymbol{Z}\right)^{-1} \boldsymbol{Z}^{\prime} \overrightarrow{\boldsymbol{Y}} \underset{d}{\longrightarrow}\left(\boldsymbol{Q}_{Z} \boldsymbol{C} \bar{\beta}+\xi\right)^{\prime} \boldsymbol{Q}_{Z}^{-1}\left(\boldsymbol{Q}_{Z} \boldsymbol{C} \bar{\beta}+\xi\right) .
\]</span></p>
<p>This allows us to calculate that by the continuous mapping theorem</p>
<p><span class="math display">\[
\begin{aligned}
n \widehat{\mu} &amp;=\min _{\gamma} \frac{\gamma^{\prime} \overrightarrow{\boldsymbol{Y}}^{\prime} \boldsymbol{Z}\left(\boldsymbol{Z}^{\prime} \boldsymbol{Z}\right)^{-1} \boldsymbol{Z}^{\prime} \overrightarrow{\boldsymbol{Y}} \gamma}{\gamma^{\prime} \frac{1}{n} \overrightarrow{\boldsymbol{Y}}^{\prime} \boldsymbol{M}_{\boldsymbol{Z}} \overrightarrow{\boldsymbol{Y}} \gamma} \\
&amp; \underset{d}{\longrightarrow} \min _{\gamma} \frac{\gamma^{\prime}\left(\boldsymbol{Q}_{Z} \boldsymbol{C} \bar{\beta}+\xi\right)^{\prime} \boldsymbol{Q}_{Z}^{-1}\left(\boldsymbol{Q}_{Z} \boldsymbol{C} \bar{\beta}+\xi\right) \gamma}{\gamma^{\prime} \Sigma \gamma} \\
&amp;=\mu^{*}
\end{aligned}
\]</span></p>
<p>say, which is a function of <span class="math inline">\(\xi\)</span> and thus random. We deduce that the asymptotic distribution of the LIML estimator is</p>
<p><span class="math display">\[
\begin{aligned}
\widehat{\beta}_{\mathrm{liml}}-\beta=&amp;\left(\boldsymbol{X}^{\prime} \boldsymbol{P}_{\boldsymbol{Z}} \boldsymbol{X}-n \widehat{\mu} \frac{1}{n} \boldsymbol{X}^{\prime} \boldsymbol{M}_{\boldsymbol{Z}} \boldsymbol{X}\right)^{-1}\left(\boldsymbol{X}^{\prime} \boldsymbol{P}_{\boldsymbol{Z}} \boldsymbol{e}-n \widehat{\mu} \frac{1}{n} \boldsymbol{X}^{\prime} \boldsymbol{M}_{\boldsymbol{Z}} \boldsymbol{e}\right) \\
\underset{d}{\longrightarrow}\left(\left(\boldsymbol{Q}_{Z} \boldsymbol{C}+\xi_{2}\right)^{\prime} \boldsymbol{Q}_{Z}^{-1}\left(\boldsymbol{Q}_{Z} \boldsymbol{C}+\xi_{2}\right)-\mu^{*} \Sigma_{22}\right)^{-1}\left(\left(\boldsymbol{Q}_{Z} \boldsymbol{C}+\xi_{2}\right)^{\prime} \boldsymbol{Q}_{Z}^{-1} \xi_{e}-\mu^{*} \Sigma_{2 e}\right) .
\end{aligned}
\]</span></p>
<p>Similarly to 2SLS, the LIML estimator is inconsistent for <span class="math inline">\(\beta\)</span>, is asymptotically random, and non-normally distributed.</p>
<p>We summarize.</p>
<p>Theorem 12.18 Under (12.71),</p>
<p><span class="math display">\[
\begin{gathered}
\widehat{\beta}_{\mathrm{ols}}-\beta \underset{p}{\longrightarrow} \Sigma_{22}^{-1} \Sigma_{2 e} \\
\widehat{\beta}_{2 \mathrm{sls}}-\beta \underset{d}{\longrightarrow}\left(\left(\boldsymbol{Q}_{Z} \boldsymbol{C}+\xi_{2}\right)^{\prime} \boldsymbol{Q}_{Z}^{-1}\left(\boldsymbol{Q}_{Z} \boldsymbol{C}+\xi_{2}\right)\right)^{-1}\left(\boldsymbol{Q}_{Z} \boldsymbol{C}+\xi_{2}\right)^{\prime} \boldsymbol{Q}_{Z}^{-1} \xi_{e}
\end{gathered}
\]</span></p>
<p>and</p>
<p><span class="math display">\[
\begin{aligned}
&amp;\widehat{\beta}_{\mathrm{liml}}-\beta \underset{d}{\longrightarrow}\left(\left(\boldsymbol{Q}_{Z} \boldsymbol{C}+\xi_{2}\right)^{\prime} \boldsymbol{Q}_{Z}^{-1}\left(\boldsymbol{Q}_{Z} \boldsymbol{C}+\xi_{2}\right)-\mu^{*} \Sigma_{22}\right)^{-1} \\
&amp;\times\left(\left(\boldsymbol{Q}_{Z} \boldsymbol{C}+\xi_{2}\right)^{\prime} \boldsymbol{Q}_{Z}^{-1} \xi_{e}-\mu^{*} \boldsymbol{\Sigma}_{2 e}\right)
\end{aligned}
\]</span></p>
<p>where</p>
<p><span class="math display">\[
\mu^{*}=\min _{\gamma} \frac{\gamma^{\prime}\left(\boldsymbol{Q}_{Z} \boldsymbol{C} \bar{\beta}+\xi\right)^{\prime} \boldsymbol{Q}_{Z}^{-1}\left(\boldsymbol{Q}_{Z} \boldsymbol{C} \bar{\beta}+\xi\right) \gamma}{\gamma^{\prime} \Sigma \gamma}
\]</span></p>
<p>and <span class="math inline">\(\bar{\beta}=\left[\beta, I_{k}\right]\)</span></p>
<p>All three estimators are inconsistent. The 2SLS and LIML estimators are asymptotically random with non-standard distributions, similar to the asymptotic distribution of the IV estimator under complete identification failure explored in the previous section. The difference under weak identification is the presence of the coefficient matrix <span class="math inline">\(\boldsymbol{C}\)</span>.</p>
</section>
<section id="many-instruments" class="level2" data-number="12.37">
<h2 data-number="12.37" class="anchored" data-anchor-id="many-instruments"><span class="header-section-number">12.37</span> Many Instruments</h2>
<p>Some applications have available a large number <span class="math inline">\(\ell\)</span> of instruments. If they are all valid, using a large number should reduce the asymptotic variance relative to estimation with a smaller number of instruments. Is it then good practice to use many instruments? Or is there a cost to this practice? Bekker (1994) initiated a large literature investigating this question by formalizing the idea of “many instruments”. Bekker proposed an asymptotic approximation which treats the number of instruments <span class="math inline">\(\ell\)</span> as proportional to the sample size, that is <span class="math inline">\(\ell=\alpha n\)</span>, or equivalently that <span class="math inline">\(\ell / n \rightarrow \alpha \in[0,1)\)</span>. The distributional theory obtained is similar in many respects to the weak instrument theory outlined in the previous section. Consequently the impact of “weak” and “many” instruments is similar.</p>
<p>Again for simplicity we assume that there are no included exogenous regressors so that the model is</p>
<p><span class="math display">\[
\begin{aligned}
&amp;Y=X^{\prime} \beta+e \\
&amp;X=\Gamma^{\prime} Z+u_{2}
\end{aligned}
\]</span></p>
<p>with <span class="math inline">\(Z \ell \times 1\)</span>. We also make the simplifying assumption that the reduced form errors are conditionally homoskedastic. Specifically,</p>
<p><span class="math display">\[
\mathbb{E}\left[u u^{\prime} \mid Z\right]=\Sigma=\left[\begin{array}{cc}
\Sigma_{11} &amp; \Sigma_{12} \\
\Sigma_{21} &amp; \Sigma_{22}
\end{array}\right] .
\]</span></p>
<p>In addition we assume that the conditional fourth moments are bounded</p>
<p><span class="math display">\[
\mathbb{E}\left[\|u\|^{4} \mid Z\right] \leq B&lt;\infty .
\]</span></p>
<p>The idea that there are “many instruments” is formalized by the assumption that the number of instruments is increasing proportionately with the sample size</p>
<p><span class="math display">\[
\frac{\ell}{n} \longrightarrow \alpha .
\]</span></p>
<p>The best way to think about this is to view <span class="math inline">\(\alpha\)</span> as the ratio of <span class="math inline">\(\ell\)</span> to <span class="math inline">\(n\)</span> in a given sample. Thus if an application has <span class="math inline">\(n=100\)</span> observations and <span class="math inline">\(\ell=10\)</span> instruments, then we should treat <span class="math inline">\(\alpha=0.10\)</span>.</p>
<p>Suppose that there is a single endogenous regressor <span class="math inline">\(X\)</span>. Calculate its variance using the reduced form: <span class="math inline">\(\operatorname{var}[X]=\operatorname{var}\left[Z^{\prime} \Gamma\right]+\operatorname{var}[u]\)</span>. Suppose as well that <span class="math inline">\(\operatorname{var}[X]\)</span> and <span class="math inline">\(\operatorname{var}[u]\)</span> are unchanging as <span class="math inline">\(\ell\)</span> increases. This implies that <span class="math inline">\(\operatorname{var}\left[Z^{\prime} \Gamma\right]\)</span> is unchanging even though the dimension <span class="math inline">\(\ell\)</span> is increasing. This is a useful assumption as it implies that the population <span class="math inline">\(R^{2}\)</span> of the reduced form is not changing with <span class="math inline">\(\ell\)</span>. We don’t need this exact condition, rather we simply assume that the sample version converges in probability to a fixed constant. Specifically, we assume that</p>
<p><span class="math display">\[
\frac{1}{n} \sum_{i=1}^{n} \Gamma^{\prime} Z_{i} Z_{i}^{\prime} \Gamma \underset{p}{\longrightarrow} \boldsymbol{H}
\]</span></p>
<p>for some matrix <span class="math inline">\(\boldsymbol{H}&gt;0\)</span>. Again, this essentially implies that the <span class="math inline">\(R^{2}\)</span> of the reduced form regressions for each component of <span class="math inline">\(X\)</span> converge to constants.</p>
<p>As a baseline it is useful to examine the behavior of the least squares estimator of <span class="math inline">\(\beta\)</span>. First, observe that the variance of <span class="math inline">\(\operatorname{vec}\left(n^{-1} \sum_{i=1}^{n} \Gamma^{\prime} Z_{i} u_{i}^{\prime}\right)\)</span>, conditional on <span class="math inline">\(Z\)</span>, is</p>
<p><span class="math display">\[
\Sigma \otimes n^{-2} \sum_{i=1}^{n} \Gamma^{\prime} Z_{i} Z_{i}^{\prime} \Gamma \underset{p}{\longrightarrow} 0
\]</span></p>
<p>by (12.77). Thus it converges in probability to zero:</p>
<p><span class="math display">\[
n^{-1} \sum_{i=1}^{n} \Gamma^{\prime} Z_{i} u_{i}^{\prime} \underset{p}{\longrightarrow} 0 .
\]</span></p>
<p>Combined with (12.77) and the WLLN we find</p>
<p><span class="math display">\[
\frac{1}{n} \sum_{i=1}^{n} X_{i} e_{i}=\frac{1}{n} \sum_{i=1}^{n} \Gamma^{\prime} Z_{i} e_{i}+\frac{1}{n} \sum_{i=1}^{n} u_{2 i} e_{i} \underset{p}{\longrightarrow} \Sigma_{2 e}
\]</span></p>
<p>and</p>
<p><span class="math display">\[
\frac{1}{n} \sum_{i=1}^{n} X_{i} X_{i}^{\prime}=\frac{1}{n} \sum_{i=1}^{n} \Gamma^{\prime} Z_{i} Z_{i}^{\prime} \Gamma+\frac{1}{n} \sum_{i=1}^{n} \Gamma^{\prime} Z_{i} u_{2 i}^{\prime}+\frac{1}{n} \sum_{i=1}^{n} u_{2 i} Z_{i}^{\prime} \Gamma+\frac{1}{n} \sum_{i=1}^{n} u_{2 i} u_{2 i}^{\prime} \underset{p}{\rightarrow} \boldsymbol{H}+\Sigma_{22}
\]</span></p>
<p>Hence</p>
<p><span class="math display">\[
\widehat{\beta}_{\mathrm{ols}}=\beta+\left(\frac{1}{n} \sum_{i=1}^{n} X_{i} X_{i}^{\prime}\right)^{-1}\left(\frac{1}{n} \sum_{i=1}^{n} X_{i} e_{i}\right) \underset{p}{\longrightarrow} \beta+\left(\boldsymbol{H}+\Sigma_{22}\right)^{-1} \Sigma_{2 e}
\]</span></p>
<p>Thus least squares is inconsistent for <span class="math inline">\(\beta\)</span>.</p>
<p>Now consider the 2SLS estimator. In matrix notation, setting <span class="math inline">\(\boldsymbol{P}_{Z}=\boldsymbol{Z}\left(\boldsymbol{Z}^{\prime} \boldsymbol{Z}\right)^{-1} \boldsymbol{Z}^{\prime}\)</span>,</p>
<p><span class="math display">\[
\begin{aligned}
\widehat{\beta}_{2 \mathrm{sls}}-\beta &amp;=\left(\frac{1}{n} \boldsymbol{X}^{\prime} \boldsymbol{P}_{\boldsymbol{Z}} \boldsymbol{X}\right)^{-1}\left(\frac{1}{n} \boldsymbol{X}^{\prime} \boldsymbol{P}_{\boldsymbol{Z}} \boldsymbol{e}\right) \\
&amp;=\left(\frac{1}{n} \bar{\Gamma}^{\prime} \boldsymbol{Z}^{\prime} \boldsymbol{Z} \bar{\Gamma}+\frac{1}{n} \bar{\Gamma}^{\prime} \boldsymbol{Z}^{\prime} \boldsymbol{u}_{2}+\frac{1}{n} \boldsymbol{u}_{2}^{\prime} \boldsymbol{Z} \bar{\Gamma}+\frac{1}{n} \boldsymbol{u}_{2}^{\prime} \boldsymbol{P}_{Z} \boldsymbol{u}_{2}\right)^{-1}\left(\frac{1}{n} \Gamma^{\prime} \boldsymbol{Z}^{\prime} \boldsymbol{e}+\frac{1}{n} \boldsymbol{u}_{2}^{\prime} \boldsymbol{P}_{Z} \boldsymbol{e}\right)
\end{aligned}
\]</span></p>
<p>In the expression on the right-side of (12.79) several of the components have been examined in (12.77) and (12.78). We now examine the remaining components <span class="math inline">\(\frac{1}{n} \boldsymbol{u}_{2}^{\prime} \boldsymbol{P}_{\boldsymbol{Z}} \boldsymbol{e}\)</span> and <span class="math inline">\(\frac{1}{n} \boldsymbol{u}_{2}^{\prime} \boldsymbol{P}_{\boldsymbol{Z}} \boldsymbol{u}_{2}\)</span> which are sub-components of the matrix <span class="math inline">\(\frac{1}{n} \boldsymbol{u}^{\prime} \boldsymbol{P}_{\boldsymbol{Z}} \boldsymbol{u}\)</span>. Take the <span class="math inline">\(j k^{t h}\)</span> element <span class="math inline">\(\frac{1}{n} \boldsymbol{u}_{j}^{\prime} \boldsymbol{P}_{\boldsymbol{Z}} \boldsymbol{u}_{k}\)</span>.</p>
<p>First, take its expectation. We have (given under the conditional homoskedasticity assumption (12.74))</p>
<p><span class="math display">\[
\mathbb{E}\left[\frac{1}{n} \boldsymbol{u}_{j}^{\prime} \boldsymbol{P}_{\boldsymbol{Z}} \boldsymbol{u}_{k} \mid \boldsymbol{Z}\right]=\frac{1}{n} \operatorname{tr}\left(\mathbb{E}\left[\boldsymbol{P}_{\boldsymbol{Z}} \boldsymbol{u}_{k} \boldsymbol{u}_{j}^{\prime} \mid \boldsymbol{Z}\right]\right)=\frac{1}{n} \operatorname{tr}\left(\boldsymbol{P}_{\boldsymbol{Z}}\right) \Sigma_{j k}=\frac{\ell}{n} \Sigma_{j k} \rightarrow \alpha \Sigma_{j k}
\]</span></p>
<p>using <span class="math inline">\(\operatorname{tr}\left(\boldsymbol{P}_{Z}\right)=\ell\)</span>.</p>
<p>Second, we calculate its variance which is a more cumbersome exercise. Let <span class="math inline">\(P_{i m}=Z_{i}^{\prime}\left(\boldsymbol{Z}^{\prime} \boldsymbol{Z}\right)^{-1} Z_{m}\)</span> be the <span class="math inline">\(i m^{t h}\)</span> element of <span class="math inline">\(\boldsymbol{P}_{\boldsymbol{Z}}\)</span>. Then <span class="math inline">\(\boldsymbol{u}_{j}^{\prime} \boldsymbol{P}_{\boldsymbol{Z}} \boldsymbol{u}_{k}=\sum_{i=1}^{n} \sum_{m=1}^{n} u_{j i} u_{k m} P_{i m}\)</span>. The matrix <span class="math inline">\(\boldsymbol{P}_{\boldsymbol{Z}}\)</span> is idempotent. It therefore has the properties <span class="math inline">\(\sum_{i=1}^{n} P_{i i}=\operatorname{tr}\left(\boldsymbol{P}_{Z}\right)=\ell\)</span> and <span class="math inline">\(0 \leq P_{i i} \leq 1\)</span>. The property <span class="math inline">\(\boldsymbol{P}_{Z} \boldsymbol{P}_{Z}=\boldsymbol{P}_{Z}\)</span> also implies <span class="math inline">\(\sum_{m=1}^{n} P_{i m}^{2}=P_{i i}\)</span>. Then</p>
<p><span class="math display">\[
\begin{aligned}
\operatorname{var}\left[\frac{1}{n} \boldsymbol{u}_{j}^{\prime} \boldsymbol{P}_{\boldsymbol{Z}} \boldsymbol{u}_{k} \mid \boldsymbol{Z}\right] &amp;=\frac{1}{n^{2}} \mathbb{E}\left[\sum_{i=1}^{n} \sum_{m=1}^{n}\left(u_{j i} u_{k m}-\mathbb{E}\left[u_{j i} u_{k m}\right] \mathbb{1}\{i=m\}\right) P_{i m} \mid \boldsymbol{Z}\right]^{2} \\
&amp;=\frac{1}{n^{2}} \mathbb{E}\left[\sum_{i=1}^{n} \sum_{m=1}^{n} \sum_{q=1}^{n} \sum_{r=1}^{n}\left(u_{j i} u_{k m}-\Sigma_{j k} \mathbb{1}\{i=m\}\right) P_{i m}\left(u_{j q} u_{k r}-\Sigma_{j k} \mathbb{1}\{q=r\}\right) P_{q r}\right] \\
&amp;=\frac{1}{n^{2}} \sum_{i=1}^{n} \mathbb{E}\left[\left(u_{j i} u_{k i}-\Sigma_{j k}\right)^{2}\right] P_{i i}^{2} \\
&amp;+\frac{1}{n^{2}} \sum_{i=1}^{n} \sum_{m \neq i} \mathbb{E}\left[u_{j i}^{2} u_{k m}^{2}\right] P_{i m}^{2}+\frac{1}{n^{2}} \sum_{i=1}^{n} \sum_{m \neq i} \mathbb{E}\left[u_{j i} u_{k m} u_{j m} u_{k i}\right] P_{i m}^{2} \\
&amp; \leq \frac{B}{n^{2}}\left(\sum_{i=1}^{n} P_{i i}^{2}+2 \sum_{i=1}^{n} \sum_{m=1}^{n} P_{i m}^{2}\right) \\
&amp; \leq \frac{3 B}{n^{2}} \sum_{i=1}^{n} P_{i i} \\
&amp;=3 B \frac{\ell}{n^{2}} \rightarrow 0 .
\end{aligned}
\]</span></p>
<p>The third equality holds because the remaining cross-products have zero expectation as the observations are independent and the errors have zero mean. The first inequality is (12.75). The second uses <span class="math inline">\(P_{i i}^{2} \leq P_{i i}\)</span> and <span class="math inline">\(\sum_{m=1}^{n} P_{i m}^{2}=P_{i i}\)</span>. The final equality is <span class="math inline">\(\sum_{i=1}^{n} P_{i i}=\ell\)</span>.</p>
<p>Using (12.76), (12.80), Markov’s inequality (B.36), and combining across all <span class="math inline">\(j\)</span> and <span class="math inline">\(k\)</span> we deduce that</p>
<p><span class="math display">\[
\frac{1}{n} \boldsymbol{u}^{\prime} \boldsymbol{P}_{\boldsymbol{Z}} \boldsymbol{u} \underset{p}{\longrightarrow} \alpha \Sigma .
\]</span></p>
<p>Returning to the 2SLS estimator (12.79) and combining (12.77), (12.78), and (12.81), we find</p>
<p><span class="math display">\[
\widehat{\beta}_{2 \text { sls }}-\beta \underset{p}{\longrightarrow}\left(\boldsymbol{H}+\alpha \Sigma_{22}\right)^{-1} \alpha \Sigma_{2 e} .
\]</span></p>
<p>Thus 2SLS is also inconsistent for <span class="math inline">\(\beta\)</span>. The limit, however, depends on the magnitude of <span class="math inline">\(\alpha\)</span>.</p>
<p>We finally examine the LIML estimator. (12.81) implies</p>
<p><span class="math display">\[
\frac{1}{n} \boldsymbol{Y}^{\prime} \boldsymbol{M}_{Z} \boldsymbol{Y}=\frac{1}{n} \boldsymbol{u}^{\prime} \boldsymbol{u}-\frac{1}{n} \boldsymbol{u}^{\prime} \boldsymbol{P}_{\boldsymbol{Z}} \boldsymbol{u} \underset{p}{\longrightarrow}(1-\alpha) \Sigma .
\]</span></p>
<p>Similarly</p>
<p><span class="math display">\[
\begin{aligned}
\frac{1}{n} \boldsymbol{Y}^{\prime} \boldsymbol{Z}\left(\boldsymbol{Z}^{\prime} \boldsymbol{Z}\right)^{-1} \boldsymbol{Z}^{\prime} \boldsymbol{Y} &amp;=\bar{\beta}^{\prime} \Gamma^{\prime}\left(\frac{1}{n} \boldsymbol{Z}^{\prime} \boldsymbol{Z}\right) \Gamma \bar{\beta}+\bar{\beta}^{\prime} \Gamma^{\prime}\left(\frac{1}{n} \boldsymbol{Z}^{\prime} \boldsymbol{u}\right)+\left(\frac{1}{n} \boldsymbol{u}^{\prime} \boldsymbol{Z}\right) \Gamma \bar{\beta}+\frac{1}{n} \boldsymbol{u}^{\prime} \boldsymbol{P}_{\boldsymbol{Z}} \boldsymbol{u} \\
&amp; \underset{d}{\longrightarrow} \bar{\beta}^{\prime} \boldsymbol{H} \bar{\beta}+\alpha \Sigma .
\end{aligned}
\]</span></p>
<p>Hence</p>
<p><span class="math display">\[
\widehat{\mu}=\min _{\gamma} \frac{\gamma^{\prime} \boldsymbol{Y}^{\prime} \boldsymbol{Z}\left(\boldsymbol{Z}^{\prime} \boldsymbol{Z}\right)^{-1} \boldsymbol{Z}^{\prime} \boldsymbol{Y} \gamma}{\gamma^{\prime} \boldsymbol{Y}^{\prime} \boldsymbol{M}_{\boldsymbol{Z}} \boldsymbol{Y} \gamma} \underset{d}{\longrightarrow} \min _{\gamma} \frac{\gamma^{\prime}\left(\bar{\beta}^{\prime} \boldsymbol{H} \bar{\beta}+\alpha \Sigma\right) \gamma}{\gamma^{\prime}(1-\alpha) \Sigma \gamma}=\frac{\alpha}{1-\alpha}
\]</span></p>
<p>and</p>
<p><span class="math display">\[
\begin{aligned}
\widehat{\beta}_{\mathrm{liml}}-\beta &amp;=\left(\frac{1}{n} \boldsymbol{X}^{\prime} \boldsymbol{P}_{\boldsymbol{Z}} \boldsymbol{X}-\widehat{\mu} \frac{1}{n} \boldsymbol{X}^{\prime} \boldsymbol{M}_{\boldsymbol{Z}} \boldsymbol{X}\right)^{-1}\left(\frac{1}{n} \boldsymbol{X}^{\prime} \boldsymbol{P}_{Z} \boldsymbol{e}-\widehat{\mu} \frac{1}{n} \boldsymbol{X}^{\prime} \boldsymbol{M}_{\boldsymbol{Z}} \boldsymbol{e}\right) \\
&amp; \underset{d}{\longrightarrow}\left(\boldsymbol{H}+\alpha \Sigma_{22}-\frac{\alpha}{1-\alpha}(1-\alpha) \Sigma_{22}\right)^{-1}\left(\alpha \Sigma_{2 e}-\frac{\alpha}{1-\alpha}(1-\alpha) \Sigma_{2 e}\right) \\
&amp;=\boldsymbol{H}^{-1} 0 \\
&amp;=0 .
\end{aligned}
\]</span></p>
<p>Thus LIML is consistent for <span class="math inline">\(\beta\)</span>, unlike 2SLS.</p>
<p>We state these results formally.</p>
<p>Theorem 12.19 In model (12.73), under assumptions (12.74), (12.75) and (12.76), then as <span class="math inline">\(n \rightarrow \infty\)</span>.</p>
<p><span class="math display">\[
\begin{aligned}
&amp;\widehat{\beta}_{\text {ols }} \underset{p}{\longrightarrow} \beta+\left(\boldsymbol{H}+\Sigma_{22}\right)^{-1} \Sigma_{2 e} \\
&amp;\widehat{\beta}_{2 \text { sls }} \underset{p}{\longrightarrow} \beta+\left(\boldsymbol{H}+\alpha \Sigma_{22}\right)^{-1} \alpha \Sigma_{2 e} \\
&amp;\widehat{\beta}_{\text {liml }} \underset{p}{\longrightarrow} \beta .
\end{aligned}
\]</span></p>
<p>This result is quite insightful. It shows that while endogeneity <span class="math inline">\(\left(\Sigma_{2 e} \neq 0\right)\)</span> renders the least squares estimator inconsistent, the 2SLS estimator is also inconsistent if the number of instruments diverges proportionately with <span class="math inline">\(n\)</span>. The limit in Theorem <span class="math inline">\(12.19\)</span> shows a continuity between least squares and 2 SLS. The probability limit of the 2SLS estimator is continuous in <span class="math inline">\(\alpha\)</span>, with the extreme case <span class="math inline">\((\alpha=1)\)</span> implying that 2SLS and least squares have the same probability limit. The general implication is that the inconsistency of 2 SLS is increasing in <span class="math inline">\(\alpha\)</span>.</p>
<p>The theorem also shows that unlike 2SLS the LIML estimator is consistent under the many instruments assumption. Effectively, LIML makes a bias-correction.</p>
<p>Theorems <span class="math inline">\(12.18\)</span> (weak instruments) and <span class="math inline">\(12.19\)</span> (many instruments) tell a cautionary tale. They show that when instruments are weak and/or many the 2SLS estimator is inconsistent. The degree of inconsistency depends on the weakness of the instruments (the magnitude of the matrix <span class="math inline">\(\boldsymbol{C}\)</span> in Theorem 12.18) and the degree of overidentification (the ratio <span class="math inline">\(\alpha\)</span> in Theorem 12.19). The Theorems also show that the LIML estimator is inconsistent under the weak instrument assumption but with a bias-correction, and is consistent under the many instrument assumption. This suggests that LIML is more robust than 2SLS to weak and many instruments.</p>
<p>An important limitation of the results in Theorem <span class="math inline">\(12.19\)</span> is the assumption of conditional homoskedasticity. It appears likely that the consistency of LIML fails in the many instrument setting if the errors are heteroskedastic.</p>
<p>In applications users should be aware of the potential consequences of the many instrument framework. It is useful to calculate the “many instrument ratio” <span class="math inline">\(\alpha=\ell / n\)</span>. While there is no specific rule-ofthumb for <span class="math inline">\(\alpha\)</span> which leads to acceptable inference a minimum criterion is that if <span class="math inline">\(\alpha \geq 0.05\)</span> you should be seriously concerned about the many-instrument problem. In general, when <span class="math inline">\(\alpha\)</span> is large it seems preferable to use LIML instead of 2SLS.</p>
</section>
<section id="testing-for-weak-instruments" class="level2" data-number="12.38">
<h2 data-number="12.38" class="anchored" data-anchor-id="testing-for-weak-instruments"><span class="header-section-number">12.38</span> Testing for Weak Instruments</h2>
<p>In the previous sections we found that weak instruments results in non-standard asymptotic distributions for the 2SLS and LIML estimators. In practice how do we know if this is a problem? Is there a way to check if the instruments are weak?</p>
<p>This question was addressed in an influential paper by Stock and Yogo (2005) as an extension of Staiger and Stock (1997). Stock-Yogo focus on two implications of weak instruments: (1) estimation bias and (2) inference distortion. They show how to test the hypothesis that these distortions are not “too big”. They propose <span class="math inline">\(F\)</span> tests for the excluded instruments in the reduced form regressions with non-standard critical values. In particular, when there is one endogenous regressor and a single instrument the StockYogo test rejects the null of weak instruments when this <span class="math inline">\(F\)</span> statistic exceeds 10 . While Stock and Yogo explore two types of distortions, we focus exclusively on inference as that is the more challenging problem. In this section we describe the Stock-Yogo theory and tests for the case of a single endogenous regressor <span class="math inline">\(\left(k_{2}=1\right)\)</span>. In the following section we describe their method for the case of multiple endogeneous regressors.</p>
<p>While the theory in Stock and Yogo allows for an arbitrary number of exogenous regressors and instruments, for the sake of clear exposition we will focus on the very simple case of no included exogenous variables <span class="math inline">\(\left(k_{1}=0\right)\)</span> and just one exogenous instrument <span class="math inline">\(\left(\ell_{2}=1\right)\)</span> which is model (12.69) from Section 12.35.</p>
<p><span class="math display">\[
\begin{aligned}
&amp;Y=X \beta+e \\
&amp;X=Z \Gamma+u .
\end{aligned}
\]</span></p>
<p>Furthermore, as in Section <span class="math inline">\(12.35\)</span> we assume conditional homoskedasticity and normalize the variances as in (12.70). Since the model is just-identified the 2SLS, LIML, and IV estimators are all equivalent.</p>
<p>The question of primary interest is to determine conditions on the reduced form under which the IV estimator of the structural equation is well behaved, and secondly, what statistical tests can be used to learn if these conditions are satisfied. As in Section <span class="math inline">\(12.36\)</span> we assume that the reduced form coefficient <span class="math inline">\(\Gamma\)</span> is local-to-zero, specifically <span class="math inline">\(\Gamma=n^{-1 / 2} \mu\)</span>. The asymptotic distribution of the IV estimator is presented in Theorem 12.18. Given the simplifying assumptions the result is</p>
<p><span class="math display">\[
\widehat{\beta}_{\mathrm{iv}}-\beta \underset{d}{\longrightarrow} \frac{\xi_{e}}{\mu+\xi_{2}}
\]</span></p>
<p>where <span class="math inline">\(\left(\xi_{e}, \xi_{2}\right)\)</span> are bivariate normal. For inference we also examine the behavior of the classical (ho- moskedastic) t-statistic for the IV estimator. Note</p>
<p><span class="math display">\[
\begin{aligned}
\widehat{\sigma}^{2} &amp;=\frac{1}{n} \sum_{i=1}^{n}\left(Y_{i}-X_{i} \widehat{\beta}_{\mathrm{iv}}\right)^{2} \\
&amp;=\frac{1}{n} \sum_{i=1}^{n} e_{i}^{2}-\frac{2}{n} \sum_{i=1}^{n} e_{i} X_{i}\left(\widehat{\beta}_{\mathrm{iv}}-\beta\right)+\frac{1}{n} \sum_{i=1}^{n} X_{i}^{2}\left(\widehat{\beta}_{\mathrm{iv}}-\beta\right)^{2} \\
&amp; \underset{d}{\longrightarrow} 1-2 \rho \frac{\xi_{e}}{\mu+\xi_{2}}+\left(\frac{\xi_{e}}{\mu+\xi_{2}}\right)^{2} .
\end{aligned}
\]</span></p>
<p>Thus</p>
<p><img src="images//2022_09_17_f9391324ededdbb7a34eg-63.jpg" class="img-fluid"></p>
<p>In general, <span class="math inline">\(S\)</span> is non-normal and its distribution depends on the parameters <span class="math inline">\(\rho\)</span> and <span class="math inline">\(\mu\)</span>.</p>
<p>Can we use the distribution <span class="math inline">\(S\)</span> for inference on <span class="math inline">\(\beta\)</span> ? The distribution depends on two unknown parameters and neither is consistently estimable. This means we cannot use the distribution in (12.82) with <span class="math inline">\(\rho\)</span> and <span class="math inline">\(\mu\)</span> replaced with estimates. To eliminate the dependence on <span class="math inline">\(\rho\)</span> one possibility is to use the “worst case” value which turns out to be <span class="math inline">\(\rho=1\)</span>. By worst-case we mean the value which causes the greatest distortion away from normal critical values. Setting <span class="math inline">\(\rho=1\)</span> we have the considerable simplification</p>
<p><span class="math display">\[
S=S_{1}=\xi\left|1+\frac{\xi}{\mu}\right|
\]</span></p>
<p>where <span class="math inline">\(\xi \sim \mathrm{N}(0,1)\)</span>. When the model is strongly identified (so <span class="math inline">\(|\mu|\)</span> is very large) then <span class="math inline">\(S_{1} \approx \xi\)</span> is standard normal, consistent with classical theory. However when <span class="math inline">\(|\mu|\)</span> is very small (but non-zero) <span class="math inline">\(\left|S_{1}\right| \approx \xi^{2} / \mu\)</span> (in the sense that this term dominates), which is a scaled <span class="math inline">\(\chi_{1}^{2}\)</span> and quite far from normal. As <span class="math inline">\(|\mu| \rightarrow 0\)</span> we find the extreme case <span class="math inline">\(\left|S_{1}\right| \rightarrow p \infty\)</span>.</p>
<p>While (12.83) is a convenient simplification it does not yield a useful approximation for inference as the distribution in (12.83) is highly dependent on the unknown <span class="math inline">\(\mu\)</span>. If we take the worst-case value of <span class="math inline">\(\mu\)</span>, which is <span class="math inline">\(\mu=0\)</span>, we find that <span class="math inline">\(\left|S_{1}\right|\)</span> diverges and all distributional approximations fail.</p>
<p>To break this impasse Stock and Yogo (2005) recommended a constructive alternative. Rather than using the worst-case <span class="math inline">\(\mu\)</span> they suggested finding a threshold such that if <span class="math inline">\(\mu\)</span> exceeds this threshold then the distribution (12.83) is not “too badly” distorted from the normal distribution.</p>
<p>Specifically, the Stock-Yogo recommendation can be summarized by two steps. First, the distribution result (12.83) can be used to find a threshold value <span class="math inline">\(\tau^{2}\)</span> such that if <span class="math inline">\(\mu^{2} \geq \tau^{2}\)</span> then the size of the nominal <span class="math inline">\({ }^{1}\)</span> 5% test “Reject if <span class="math inline">\(|T| \geq 1.96\)</span>” has asymptotic size <span class="math inline">\(\mathbb{P}\left[\left|S_{1}\right| \geq 1.96\right] \leq 0.15\)</span>. This means that while the goal is to obtain a test with size <span class="math inline">\(5 %\)</span>, we recognize that there may be size distortion due to weak instruments and are willing to tolerate a specific distortion. For example, a <span class="math inline">\(10 %\)</span> distortion means we allow the actual size to be up to <span class="math inline">\(15 %\)</span>. Second, they use the asymptotic distribution of the reduced-form (first stage) <span class="math inline">\(F\)</span> statistic to test if the actual unknown value of <span class="math inline">\(\mu^{2}\)</span> exceeds the threshold <span class="math inline">\(\tau^{2}\)</span>. These two steps together give rise to the rule-of-thumb that the first-stage <span class="math inline">\(F\)</span> statistic should exceed 10 in order to achieve reliable IV inference. (This is for the case of one instrumental variable. If there is more than one instrument then the rule-of-thumb changes.) We now describe the steps behind this reasoning in more detail.</p>
<p>The first step is to use the distribution (12.82) to determine the threshold <span class="math inline">\(\tau^{2}\)</span>. Formally, the goal is to find the value of <span class="math inline">\(\tau^{2}=\mu^{2}\)</span> at which the asymptotic size of a nominal <span class="math inline">\(5 %\)</span> test is actually a given <span class="math inline">\(r\)</span> (e.g.</p>
<p><span class="math inline">\({ }^{1}\)</span> The term “nominal size” of a test is the official intended size - the size which would obtain under ideal circumstances. In this context the test “Reject if <span class="math inline">\(|T| \geq 1.96\)</span>” has nominal size <span class="math inline">\(0.05\)</span> as this would be the asymptotic rejection probability in the ideal context of strong instruments. <span class="math inline">\(r=0.15)\)</span>, thus <span class="math inline">\(\mathbb{P}\left[\left|S_{1}\right| \geq 1.96\right] \leq r\)</span>. By some algebra and the quadratic formula the event <span class="math inline">\(|\xi(1+\xi / \mu)|&lt;x\)</span> is the same as</p>
<p><span class="math display">\[
\frac{\mu^{2}}{4}-x \mu&lt;\left(\xi+\frac{\mu}{2}\right)^{2}&lt;\frac{\mu^{2}}{4}+x \mu .
\]</span></p>
<p>The random variable between the inequalities is distributed <span class="math inline">\(\chi_{1}^{2}\left(\mu^{2} / 4\right)\)</span>, a noncentral chi-square with one degree of freedom and noncentrality parameter <span class="math inline">\(\mu^{2} / 4\)</span>. Thus</p>
<p><span class="math display">\[
\begin{aligned}
\mathbb{P}\left[\left|S_{1}\right| \geq x\right] &amp;=\mathbb{P}\left[\chi_{1}^{2}\left(\frac{\mu^{2}}{4}\right) \geq \frac{\mu^{2}}{4}+x \mu\right]+\mathbb{P}\left[\chi_{1}^{2}\left(\frac{\mu^{2}}{4}\right) \leq \frac{\mu^{2}}{4}-x \mu\right] \\
&amp;=1-G\left(\frac{\mu^{2}}{4}+x \mu, \frac{\mu^{2}}{4}\right)+G\left(\frac{\mu^{2}}{4}-x \mu, \frac{\mu^{2}}{4}\right)
\end{aligned}
\]</span></p>
<p>where <span class="math inline">\(G(u, \lambda)\)</span> is the distribution function of <span class="math inline">\(\chi_{1}^{2}(\lambda)\)</span>. Hence the desired threshold <span class="math inline">\(\tau^{2}\)</span> solves</p>
<p><span class="math display">\[
1-G\left(\frac{\tau^{2}}{4}+1.96 \tau, \frac{\tau^{2}}{4}\right)+G\left(\frac{\tau^{2}}{4}-1.96 \tau, \frac{\tau^{2}}{4}\right)=r
\]</span></p>
<p>or effectively</p>
<p><span class="math display">\[
G\left(\frac{\tau^{2}}{4}+1.96 \tau, \frac{\tau^{2}}{4}\right)=1-r
\]</span></p>
<p>because <span class="math inline">\(\tau^{2} / 4-1.96 \tau&lt;0\)</span> for relevant values of <span class="math inline">\(\tau\)</span>. The numerical solution (computed with the non-central chi-square distribution function, e.g.&nbsp;ncx <span class="math inline">\(2 c d f\)</span> in MATLAB) is <span class="math inline">\(\tau^{2}=1.70\)</span> when <span class="math inline">\(r=0.15\)</span>. (That is, the command</p>
<p><span class="math display">\[
\operatorname{ncx} 2 \mathrm{cdf}(1.7 / 4+1.96 * \operatorname{sqrt}(1.7), 1,1.7 / 4)
\]</span></p>
<p>yields the answer <span class="math inline">\(0.8500\)</span>. Stock and Yogo (2005) approximate the same calculation using simulation methods and report <span class="math inline">\(\tau^{2}=1.82\)</span>.)</p>
<p>This calculation means that if the reduced form satisfies <span class="math inline">\(\mu^{2} \geq 1.7\)</span>, or equivalently if <span class="math inline">\(\Gamma^{2} \geq 1.7 / n\)</span>, then the asymptotic size of a nominal <span class="math inline">\(5 %\)</span> test on the structural parameter is no larger than <span class="math inline">\(15 %\)</span>.</p>
<p>To summarize the Stock-Yogo first step, we calculate the minimum value <span class="math inline">\(\tau^{2}\)</span> for <span class="math inline">\(\mu^{2}\)</span> sufficient to ensure that the asymptotic size of a nominal 5% t-test does not exceed <span class="math inline">\(r\)</span>, and find that <span class="math inline">\(\tau^{2}=1.70\)</span> for <span class="math inline">\(r=0.15\)</span>.</p>
<p>The Stock-Yogo second step is to find a critical value for the first-stage <span class="math inline">\(F\)</span> statistic sufficient to reject the hypothesis that <span class="math inline">\(\mathbb{M}_{0}: \mu^{2}=\tau^{2}\)</span> against <span class="math inline">\(\mathbb{M}_{1}: \mu^{2}&gt;\tau^{2}\)</span>. We now describe this procedure.</p>
<p>They suggest testing <span class="math inline">\(\mathbb{M}_{0}: \mu^{2}=\tau^{2}\)</span> at the <span class="math inline">\(5 %\)</span> size using the first stage <span class="math inline">\(F\)</span> statistic. If the <span class="math inline">\(F\)</span> statistic is small so that the test does not reject then we should be worried that the true value of <span class="math inline">\(\mu^{2}\)</span> is small and there is a weak instrument problem. On the other hand if the <span class="math inline">\(F\)</span> statistic is large so that the test rejects then we can have some confidence that the true value of <span class="math inline">\(\mu^{2}\)</span> is sufficiently large that the weak instrument problem is not too severe.</p>
<p>To implement the test we need to calculate an appropriate critical value. It should be calculated under the null hypothesis <span class="math inline">\(\mathbb{H}_{0}: \mu^{2}=\tau^{2}\)</span>. This is different from a conventional <span class="math inline">\(F\)</span> test which is calculated under <span class="math inline">\(\mathbb{M}_{0}: \mu^{2}=0\)</span>.</p>
<p>We start by calculating the asymptotic distribution of <span class="math inline">\(\mathrm{F}\)</span>. Since there is one regressor and one instrument in our simplified setting the first-stage <span class="math inline">\(F\)</span> statistic is the squared t-statistic from the reduced form. Given our previous calculations it has the asymptotic distribution</p>
<p><span class="math display">\[
\mathrm{F}=\frac{\widehat{\gamma}^{2}}{s(\widehat{\gamma})^{2}}=\frac{\left(\sum_{i=1}^{n} Z_{i} X_{i}\right)^{2}}{\left(\sum_{i=1}^{n} X_{i}^{2}\right) \widehat{\sigma}_{u}^{2}} \underset{d}{ }\left(\mu+\xi_{2}\right)^{2} \sim \chi_{1}^{2}\left(\mu^{2}\right) .
\]</span></p>
<p>This is a non-central chi-square distribution <span class="math inline">\(G\left(u, \mu^{2}\right)\)</span> with one degree of freedom and non-centrality parameter <span class="math inline">\(\mu^{2}\)</span>. To test <span class="math inline">\(\mathbb{M}_{0}: \mu^{2}=\tau^{2}\)</span> against <span class="math inline">\(\mathbb{M}_{1}: \mu^{2}&gt;\tau^{2}\)</span> we reject for <span class="math inline">\(\mathrm{F} \geq c\)</span> where <span class="math inline">\(c\)</span> is selected so that the asymptotic rejection probability satisfies</p>
<p><span class="math display">\[
\mathbb{P}\left[\mathrm{F} \geq c \mid \mu^{2}=\tau^{2}\right] \rightarrow \mathbb{P}\left[\chi_{1}^{2}\left(\tau^{2}\right) \geq c\right]=1-G\left(c, \tau^{2}\right)=0.05
\]</span></p>
<p>for <span class="math inline">\(\tau^{2}=1.70\)</span>, or equivalently <span class="math inline">\(G(c, 1.7)=0.95\)</span>. This is found by inverting the non-central chi-square quantile function, e.g.&nbsp;the function <span class="math inline">\(Q(p, d)\)</span> which solves <span class="math inline">\(G(Q(p, d), d)=p\)</span>. We find that <span class="math inline">\(c=Q(0.95,1.7)=8.7\)</span>. In MATLAB, this can be computed by ncx2inv <span class="math inline">\((.95,1.7\)</span> ). Stock and Yogo (2005) report <span class="math inline">\(c=9.0\)</span> because they used <span class="math inline">\(\tau^{2}=1.82\)</span>.</p>
<p>This means that if <span class="math inline">\(\mathrm{F}&gt;8.7\)</span> we can reject <span class="math inline">\(\mathbb{M}_{0}: \mu^{2}=1.7\)</span> against <span class="math inline">\(\mathbb{H}_{1}: \mu^{2}&gt;1.7\)</span> with an asymptotic <span class="math inline">\(5 %\)</span> test. In this context we should expect the IV estimator and tests to be reasonably well behaved. However, if <span class="math inline">\(\mathrm{F}&lt;8.7\)</span> then we should be cautious about the IV estimator, confidence intervals, and tests. This finding led Staiger and Stock (1997) to propose the informal “rule of thumb” that the first stage <span class="math inline">\(F\)</span> statistic should exceed 10. Notice that <span class="math inline">\(\mathrm{F}\)</span> exceeding <span class="math inline">\(8.7\)</span> (or 10) is equivalent to the reduced form t-statistic exceeding <span class="math inline">\(2.94\)</span> (or 3.16), which is considerably larger than a conventional check if the t-statistic is “significant”. Equivalently, the recommended rule-of-thumb for the case of a single instrument is to estimate the reduced form and verify that the t-statistic for exclusion of the instrumental variable exceeds 3 in absolute value.</p>
<p>Does the proposed procedure control the asymptotic size of a 2SLS test? The first step has asymptotic size bounded below <span class="math inline">\(r\)</span> (e.g.&nbsp;15%). The second step has asymptotic size 5%. By the Bonferroni bound (see Section 9.20) the two steps together have asymptotic size bounded below <span class="math inline">\(r+0.05\)</span> (e.g.&nbsp;20%). We can thus call the Stock-Yogo procedure a rigorous test with asymptotic size <span class="math inline">\(r+0.05\)</span> (or 20%).</p>
<p>Our analysis has been confined to the case <span class="math inline">\(k_{2}=\ell_{2}=1\)</span>. Stock and Yogo (2005) also examine the case <span class="math inline">\(\ell_{2}&gt;1\)</span> (which requires numerical simulation to solve) and both the 2SLS and LIML estimators. They show that the <span class="math inline">\(F\)</span> statistic critical values depend on the number of instruments <span class="math inline">\(\ell_{2}\)</span> as well as the estimator. Their critical values (calculated by simulation) are in their paper and posted on Motohiro Yogo’s webpage. We report a subset in Table 12.4.</p>
<p>Table 12.4: 5% Critical Value for Weak Instruments, <span class="math inline">\(k_{2}=1\)</span></p>
<p>|<span class="math inline">\(\ell_{2}\)</span>|<span class="math inline">\(0.10\)</span>|<span class="math inline">\(0.15\)</span>|<span class="math inline">\(0.20\)</span>|<span class="math inline">\(0.25\)</span>||<span class="math inline">\(0.10\)</span>|<span class="math inline">\(0.15\)</span>|<span class="math inline">\(0.20\)</span>|<span class="math inline">\(0.25\)</span>| |———:|—–:|—–:|—–:|—–:||—–:|—–:|—–:|—–:| | 1|<span class="math inline">\(16.4\)</span>| <span class="math inline">\(9.0\)</span>| <span class="math inline">\(6.7\)</span>| <span class="math inline">\(5.5\)</span>||<span class="math inline">\(16.4\)</span>| <span class="math inline">\(9.0\)</span>| <span class="math inline">\(6.7\)</span>| <span class="math inline">\(5.5\)</span>| | 2|<span class="math inline">\(19.9\)</span>|<span class="math inline">\(11.6\)</span>| <span class="math inline">\(8.7\)</span>| <span class="math inline">\(7.2\)</span>|| <span class="math inline">\(8.7\)</span>| <span class="math inline">\(5.3\)</span>| <span class="math inline">\(4.4\)</span>| <span class="math inline">\(3.9\)</span>| | 3|<span class="math inline">\(22.3\)</span>|<span class="math inline">\(12.8\)</span>| <span class="math inline">\(9.5\)</span>| <span class="math inline">\(7.8\)</span>|| <span class="math inline">\(6.5\)</span>| <span class="math inline">\(4.4\)</span>| <span class="math inline">\(3.7\)</span>| <span class="math inline">\(3.3\)</span>| | 4|<span class="math inline">\(24.6\)</span>|<span class="math inline">\(14.0\)</span>|<span class="math inline">\(10.3\)</span>| <span class="math inline">\(8.3\)</span>|| <span class="math inline">\(5.4\)</span>| <span class="math inline">\(3.9\)</span>| <span class="math inline">\(3.3\)</span>| <span class="math inline">\(3.0\)</span>| | 5|<span class="math inline">\(26.9\)</span>|<span class="math inline">\(15.1\)</span>|<span class="math inline">\(11.0\)</span>| <span class="math inline">\(8.8\)</span>|| <span class="math inline">\(4.8\)</span>| <span class="math inline">\(3.6\)</span>| <span class="math inline">\(3.0\)</span>| <span class="math inline">\(2.8\)</span>| | 6|<span class="math inline">\(29.2\)</span>|<span class="math inline">\(16.2\)</span>|<span class="math inline">\(11.7\)</span>| <span class="math inline">\(9.4\)</span>|| <span class="math inline">\(4.4\)</span>| <span class="math inline">\(3.3\)</span>| <span class="math inline">\(2.9\)</span>| <span class="math inline">\(2.6\)</span>| | 7|<span class="math inline">\(31.5\)</span>|<span class="math inline">\(17.4\)</span>|<span class="math inline">\(12.5\)</span>| <span class="math inline">\(9.9\)</span>|| <span class="math inline">\(4.2\)</span>| <span class="math inline">\(3.2\)</span>| <span class="math inline">\(2.7\)</span>| <span class="math inline">\(2.5\)</span>| | 8|<span class="math inline">\(33.8\)</span>|<span class="math inline">\(18.5\)</span>|<span class="math inline">\(13.2\)</span>|<span class="math inline">\(10.5\)</span>|| <span class="math inline">\(4.0\)</span>| <span class="math inline">\(3.0\)</span>| <span class="math inline">\(2.6\)</span>| <span class="math inline">\(2.4\)</span>| | 9|<span class="math inline">\(36.2\)</span>|<span class="math inline">\(19.7\)</span>|<span class="math inline">\(14.0\)</span>|<span class="math inline">\(11.1\)</span>|| <span class="math inline">\(3.8\)</span>| <span class="math inline">\(2.9\)</span>| <span class="math inline">\(2.5\)</span>| <span class="math inline">\(2.3\)</span>| | 10|<span class="math inline">\(38.5\)</span>|<span class="math inline">\(20.9\)</span>|<span class="math inline">\(14.8\)</span>|<span class="math inline">\(11.6\)</span>|| <span class="math inline">\(3.7\)</span>| <span class="math inline">\(2.8\)</span>| <span class="math inline">\(2.5\)</span>| <span class="math inline">\(2.2\)</span>| | 15|<span class="math inline">\(50.4\)</span>|<span class="math inline">\(26.8\)</span>|<span class="math inline">\(18.7\)</span>|<span class="math inline">\(12.2\)</span>|| <span class="math inline">\(3.3\)</span>| <span class="math inline">\(2.5\)</span>| <span class="math inline">\(2.2\)</span>| <span class="math inline">\(2.0\)</span>| | 20|<span class="math inline">\(62.3\)</span>|<span class="math inline">\(32.8\)</span>|<span class="math inline">\(22.7\)</span>|<span class="math inline">\(17.6\)</span>|| <span class="math inline">\(3.2\)</span>| <span class="math inline">\(2.3\)</span>| <span class="math inline">\(2.1\)</span>| <span class="math inline">\(1.9\)</span>| | 25|<span class="math inline">\(74.2\)</span>|<span class="math inline">\(38.8\)</span>|<span class="math inline">\(26.7\)</span>|<span class="math inline">\(20.6\)</span>|| <span class="math inline">\(3.8\)</span>| <span class="math inline">\(2.2\)</span>| <span class="math inline">\(2.0\)</span>| <span class="math inline">\(1.8\)</span>| | 30|<span class="math inline">\(86.2\)</span>|<span class="math inline">\(44.8\)</span>|<span class="math inline">\(30.7\)</span>|<span class="math inline">\(23.6\)</span>|| <span class="math inline">\(3.9\)</span>| <span class="math inline">\(2.2\)</span>| <span class="math inline">\(1.9\)</span>| <span class="math inline">\(1.7\)</span>|</p>
<p>Source: . One striking feature about these critical values is that those for the 2SLS estimator are strongly increasing in <span class="math inline">\(\ell_{2}\)</span> while those for the LIML estimator are decreasing in <span class="math inline">\(\ell_{2}\)</span>. This means that when the number of instruments <span class="math inline">\(\ell_{2}\)</span> is large, 2SLS requires a much stronger reduced form (larger <span class="math inline">\(\mu^{2}\)</span> ) in order for inference to be reliable, but this is not the case for LIML. This is direct evidence that LIML inference is less sensitive to weak instruments than 2SLS. This makes a strong case for LIML over 2SLS, especially when <span class="math inline">\(\ell_{2}\)</span> is large or the instruments are potentially weak.</p>
<p>We now summarize the recommended Staiger-Stock/Stock-Yogo procedure for <span class="math inline">\(k_{1} \geq 1, k_{2}=1\)</span>, and <span class="math inline">\(\ell_{2} \geq 1\)</span>. The structural equation and reduced form equations are</p>
<p><span class="math display">\[
\begin{aligned}
&amp;Y_{1}=Z_{1}^{\prime} \beta_{1}+Y_{2} \beta_{2}+e \\
&amp;Y_{2}=Z_{1}^{\prime} \gamma_{1}+Z_{2}^{\prime} \gamma_{2}+u .
\end{aligned}
\]</span></p>
<p>The structural equation is estimated by either 2 SLS or LIML. Let <span class="math inline">\(\mathrm{F}\)</span> be the <span class="math inline">\(F\)</span> statistic for <span class="math inline">\(\mathbb{H}_{0}: \gamma_{2}=0\)</span> in the reduced form equation. Let <span class="math inline">\(s\left(\widehat{\beta}_{2}\right)\)</span> be a standard error for <span class="math inline">\(\beta_{2}\)</span> in the structural equation. The procedure is:</p>
<ol type="1">
<li><p>Compare <span class="math inline">\(F\)</span> with the critical values <span class="math inline">\(c\)</span> in Table <span class="math inline">\(12.4\)</span> with the row selected to match the number of excluded instruments <span class="math inline">\(\ell_{2}\)</span> and the columns to match the estimation method (2SLS or LIML) and the desired size <span class="math inline">\(r\)</span>.</p></li>
<li><p>If <span class="math inline">\(F&gt;c\)</span> then report the 2 SLS or LIML estimates with conventional inference.</p></li>
</ol>
<p>The Stock-Yogo test can be implemented in Stata using the command estat firststage after ivregress 2 sls or ivregres liml if a standard (non-robust) covariance matrix has been specified (that is, without the ‘, r’ option).</p>
<p>There are possible extensions to the Stock-Yogo procedure.</p>
<p>One modest extension is to use the information to convey the degree of confidence in the accuracy of a confidence interval. Suppose in an application you have <span class="math inline">\(\ell_{2}=5\)</span> excluded instruments and have estimated your equation by 2SLS. Now suppose that your reduced form <span class="math inline">\(F\)</span> statistic equals 12 . You check Table <span class="math inline">\(12.4\)</span> and find that <span class="math inline">\(\mathrm{F}=12\)</span> is significant with <span class="math inline">\(r=0.20\)</span>. Thus we can interpret the conventional 2SLS confidence interval as having coverage of <span class="math inline">\(80 %\)</span> (or <span class="math inline">\(75 %\)</span> if we make the Bonferroni correction). On the other hand if <span class="math inline">\(\mathrm{F}=27\)</span> we would conclude that the test for weak instruments is significant with <span class="math inline">\(r=0.10\)</span>, meaning that the conventional 2SLS confidence interval can be interpreted as having coverage of <span class="math inline">\(90 %\)</span> (or <span class="math inline">\(85 %\)</span> after Bonferroni correction). Thus the value of the <span class="math inline">\(\mathrm{F}\)</span> statistic can be used to calibrate the coverage accuracy.</p>
<p>A more substantive extension, which we now discuss, reverses the steps. Unfortunately this discussion will be limited to the case <span class="math inline">\(\ell_{2}=1\)</span>. First, use the reduced form <span class="math inline">\(F\)</span> statistic to find a one-sided confidence interval for <span class="math inline">\(\mu^{2}\)</span> of the form <span class="math inline">\(\left[\mu_{L}^{2}, \infty\right)\)</span>. Second, use the lower bound <span class="math inline">\(\mu_{L}^{2}\)</span> to calculate a critical value <span class="math inline">\(c\)</span> for <span class="math inline">\(S_{1}\)</span> such that the 2SLS test has asymptotic size bounded below <span class="math inline">\(0.05\)</span>. This produces better size control than the Stock-Yogo procedure and produces more informative confidence intervals for <span class="math inline">\(\beta_{2}\)</span>. We now describe the steps in detail.</p>
<p>The first goal is to find a one-sided confidence interval for <span class="math inline">\(\mu^{2}\)</span>. This is found by test inversion. As we described earlier, for any <span class="math inline">\(\tau^{2}\)</span> we reject <span class="math inline">\(\mathbb{M}_{0}: \mu^{2}=\tau^{2}\)</span> in favor of <span class="math inline">\(\mathbb{H}_{1}: \mu^{2}&gt;\tau^{2}\)</span> if <span class="math inline">\(\mathrm{F}&gt;c\)</span> where <span class="math inline">\(G\left(c, \tau^{2}\right)=0.95\)</span>. Equivalently, we reject if <span class="math inline">\(G\left(\mathrm{~F}, \tau^{2}\right)&gt;0.95\)</span>. By the test inversion principle an asymptotic <span class="math inline">\(95 %\)</span> confidence interval <span class="math inline">\(\left[\mu_{L}^{2}, \infty\right)\)</span> is the set of all values of <span class="math inline">\(\tau^{2}\)</span> which are not rejected. Since <span class="math inline">\(G\left(\mathrm{~F}, \tau^{2}\right) \geq 0.95\)</span> for all <span class="math inline">\(\tau^{2}\)</span> in this set, the lower bound <span class="math inline">\(\mu_{L}^{2}\)</span> satisfies <span class="math inline">\(G\left(\mathrm{~F}, \mu_{L}^{2}\right)=0.95\)</span>, and is found numerically. In MATLAB, the solution is mu2 when <span class="math inline">\(n c x 2 c d f(F, 1, m u 2)\)</span> returns <span class="math inline">\(0.95 .\)</span></p>
<p>The second goal is to find the critical value <span class="math inline">\(c\)</span> such that <span class="math inline">\(\mathbb{P}\left(\left|S_{1}\right| \geq c\right)=0.05\)</span> when <span class="math inline">\(\mu^{2}=\mu_{L}^{2}\)</span>. From (12.84) this is achieved when</p>
<p><span class="math display">\[
1-G\left(\frac{\mu_{L}^{2}}{4}+c \mu_{L}, \frac{\mu_{L}^{2}}{4}\right)+G\left(\frac{\mu_{L}^{2}}{4}-c \mu_{L}, \frac{\mu_{L}^{2}}{4}\right)=0.05 .
\]</span></p>
<p>This can be solved as</p>
<p><span class="math display">\[
G\left(\frac{\mu_{L}^{2}}{4}+c \mu_{L}, \frac{\mu_{L}^{2}}{4}\right)=0.95 \text {. }
\]</span></p>
<p>(The third term on the left-hand-side of (12.85) is zero for all solutions so can be ignored.) Using the non-central chi-square quantile function <span class="math inline">\(Q(p, d)\)</span>, this <span class="math inline">\(C\)</span> equals</p>
<p><span class="math display">\[
c=\frac{Q\left(0.95, \frac{\mu_{L}^{2}}{4}\right)-\frac{\mu_{L}^{2}}{4}}{\mu_{L}} .
\]</span></p>
<p>For example, in MATLAB this is found as <span class="math inline">\(c=(n c x 2 i n v ~(.95,1, \mathrm{mu} 2 / 4)-\mathrm{mu} 2 / 4) / \mathrm{sqrt}(\mathrm{mu} 2) .95 %\)</span> confidence intervals for <span class="math inline">\(\beta_{2}\)</span> are then calculated as <span class="math inline">\(\widehat{\beta}_{\mathrm{iv}} \pm c s\left(\widehat{\beta}_{\mathrm{iv}}\right)\)</span>.</p>
<p>We can also calculate a p-value for the t-statistic <span class="math inline">\(T\)</span> for <span class="math inline">\(\beta_{2}\)</span>. This is</p>
<p><span class="math display">\[
p=1-G\left(\frac{\mu_{L}^{2}}{4}+|T| \mu_{L}, \frac{\mu_{L}^{2}}{4}\right)+G\left(\frac{\mu_{L}^{2}}{4}-|T| \mu_{L}, \frac{\mu_{L}^{2}}{4}\right)
\]</span></p>
<p>where the third term equals zero if <span class="math inline">\(|T| \geq \mu_{L} / 4\)</span>. In MATLAB, for example, this can be calculated by the commands</p>
<p><span class="math inline">\(\mathrm{T} 1=\mathrm{mu} 2 / 4+\operatorname{abs}(\mathrm{T}) * \operatorname{sqrt}(\mathrm{mu} 2)\)</span></p>
<p><span class="math inline">\(\mathrm{T} 2=\mathrm{mu} 2 / 4-\operatorname{abs}(\mathrm{T}) * \operatorname{sqrt}(\mathrm{mu} 2) ;\)</span></p>
<p><span class="math inline">\(\mathrm{p}=-\mathrm{ncx} 2 \mathrm{cdf}(\mathrm{T} 1,1, \mathrm{mu} 2 / 4)+\mathrm{ncx} 2 \mathrm{cdf}(\mathrm{T} 2,1, \mathrm{mu} 2 / 4)\)</span>;</p>
<p>These confidence intervals and p-values will be larger than the conventional intervals and p-values, reflecting the incorporation of information about the strength of the instruments through the first-stage <span class="math inline">\(F\)</span> statistic. Also, by the Bonferroni bound these tests have asymptotic size bounded below <span class="math inline">\(10 %\)</span> and the confidence intervals have asymptotic converage exceeding <span class="math inline">\(90 %\)</span>, unlike the Stock-Yogo method which has size of <span class="math inline">\(20 %\)</span> and coverage of <span class="math inline">\(80 %\)</span>.</p>
<p>The augmented procedure suggested here, only for the <span class="math inline">\(\ell_{2}=1\)</span> case, is</p>
<ol type="1">
<li><p>Find <span class="math inline">\(\mu_{L}^{2}\)</span> which solves <span class="math inline">\(G\left(\mathrm{~F}, \mu_{L}^{2}\right)=0.95\)</span>. In MATLAB, the solution is mu2 when <span class="math inline">\(\operatorname{cx} 2 \operatorname{cdf}(\mathrm{F}, 1, \operatorname{mu} 2)\)</span> returns <span class="math inline">\(0.95 .\)</span></p></li>
<li><p>Find <span class="math inline">\(c\)</span> which solves <span class="math inline">\(G\left(\mu_{L}^{2} / 4+c \mu_{L}, \mu_{L}^{2} / 4\right)=0.95\)</span>. In MATLAB, the command is <span class="math inline">\(c=(n c x 2 \operatorname{inv}(.95,1, \mathrm{mu} 2 / 4)-\mathrm{mu} 2 / 4) / \mathrm{sqrt}(\mathrm{mu} 2)\)</span></p></li>
<li><p>Report the confidence interval <span class="math inline">\(\widehat{\beta}_{2} \pm c s\left(\widehat{\beta}_{2}\right)\)</span> for <span class="math inline">\(\beta_{2}\)</span>.</p></li>
<li><p>For the <span class="math inline">\(\mathrm{t}\)</span> statistic <span class="math inline">\(T=\left(\widehat{\beta}_{2}-\beta_{2}\right) / s\left(\widehat{\beta}_{2}\right)\)</span> the asymptotic <span class="math inline">\(\mathrm{p}\)</span>-value is</p></li>
</ol>
<p><span class="math display">\[
p=1-G\left(\frac{\mu_{L}^{2}}{4}+|T| \mu_{L}, \frac{\mu_{L}^{2}}{4}\right)+G\left(\frac{\mu_{L}^{2}}{4}-|T| \mu_{L}, \frac{\mu_{L}^{2}}{4}\right)
\]</span></p>
<p>which is computed in MATLAB by <span class="math inline">\(\mathrm{T} 1=\mathrm{mu} 2 / 4+\mathrm{abs}(\mathrm{T}) * \operatorname{sqrt}(\mathrm{mu} 2) ; \mathrm{T} 2=\mathrm{mu} 2 / 4-\mathrm{abs}(\mathrm{T}) * \mathrm{sqrt}(\mathrm{mu} 2)\)</span>; and <span class="math inline">\(\mathrm{p}=1-\mathrm{ncx} 2 \mathrm{cdf}(\mathrm{T} 1,1, \mathrm{mu} 2 / 4)+\mathrm{ncx} 2 \mathrm{cdf}(\mathrm{T} 2,1, \mathrm{mu} 2 / 4)\)</span>.</p>
<p>We have described an extension to the Stock-Yogo procedure for the case of one instrumental variable <span class="math inline">\(\ell_{2}=1\)</span>. This restriction was due to the use of the analytic formula (12.85) for the asymptotic distribution which is only available when <span class="math inline">\(\ell_{2}=1\)</span>. In principle the procedure could be extended using simulation or bootstrap methods but this has not been done to my knowledge.</p>
<p>To illustrate the Stock-Yogo and extended procedures let us return to the Card proximity example. Take the IV estimates reported in the second column of Table <span class="math inline">\(12.1\)</span> which used college proximity as a single instrument. The reduced form estimates for the endogenous variable education are reported in the second column of Table 12.2. The excluded instrument college has a t-ratio of <span class="math inline">\(4.2\)</span> which implies an <span class="math inline">\(F\)</span> statistic of 17.8. The <span class="math inline">\(F\)</span> statistic exceeds the rule-of thumb of 10 so the structural estimates pass the Stock-Yogo threshold. Based on their recommendation this means that we can interpret the estimates conventionally. However, the conventional confidence interval, e.g.&nbsp;for the returns to education <span class="math inline">\(0.132 \pm\)</span> <span class="math inline">\(0.049 \times 1.96=[0.04,0.23]\)</span>, has an asymptotic coverage of <span class="math inline">\(80 %\)</span> rather than the nominal <span class="math inline">\(95 %\)</span> rate.</p>
<p>Now consider the extended procedure. Given <span class="math inline">\(\mathrm{F}=17.8\)</span> we calculate the lower bound <span class="math inline">\(\mu_{L}^{2}=6.6\)</span>. This implies a critical value of <span class="math inline">\(C=2.7\)</span>. Hence an improved confidence interval for the returns to education in this equation is <span class="math inline">\(0.132 \pm 0.049 \times 2.7=[0.01,0.26]\)</span>. This is a wider confidence interval but has improved asymptotic coverage of <span class="math inline">\(90 %\)</span>. The p-value for <span class="math inline">\(\beta_{2}=0\)</span> is <span class="math inline">\(p=0.012\)</span>.</p>
<p>Next, take the 2SLS estimates reported in the fourth column of Table <span class="math inline">\(11.1\)</span> which use the two instruments public and private. The reduced form equation is reported in column six of Table 12.2. An <span class="math inline">\(F\)</span> statistic for exclusion of the two instruments is <span class="math inline">\(F=13.9\)</span> which exceeds the <span class="math inline">\(15 %\)</span> size threshold for 2SLS and all thresholds for LIML, indicating that the structural estimates pass the Stock-Yogo threshold test and can be interpreted conventionally.</p>
<p>The weak instrument methods described here are important for applied econometrics as they discipline researchers to assess the quality of their reduced form relationships before reporting structural estimates. The theory, however, has limitations and shortcomings, in particular the strong assumption of conditional homoskedasticity. Despite this limitation, in practice researchers apply the Stock-Yogo recommendations to estimates computed with heteroskedasticity-robust standard errors. This is an active area of research so the recommended methods may change in the years ahead.</p>
</section>
<section id="weak-instruments-with-k_21" class="level2" data-number="12.39">
<h2 data-number="12.39" class="anchored" data-anchor-id="weak-instruments-with-k_21"><span class="header-section-number">12.39</span> Weak Instruments with <span class="math inline">\(k_{2}&gt;1\)</span></h2>
<p>When there is more than one endogenous regressor <span class="math inline">\(\left(k_{2}&gt;1\right)\)</span> it is better to examine the reduced form as a system. Staiger and Stock (1997) and Stock and Yogo (2005) provided an analysis of this case and constructed a test for weak instruments. The theory is considerably more involved than the <span class="math inline">\(k_{2}=1\)</span> case so we briefly summarize it here excluding many details, emphasizing their suggested methods.</p>
<p>The structural equation and reduced form equations are</p>
<p><span class="math display">\[
\begin{aligned}
&amp;Y_{1}=Z_{1}^{\prime} \beta_{1}+Y_{2}^{\prime} \beta_{2}+e \\
&amp;Y_{2}=\Gamma_{12}^{\prime} Z_{1}+\Gamma_{22}^{\prime} Z_{2}+u_{2} .
\end{aligned}
\]</span></p>
<p>As in the previous section we assume that the errors are conditionally homoskedastic.</p>
<p>Identification of <span class="math inline">\(\beta_{2}\)</span> requires the matrix <span class="math inline">\(\Gamma_{22}\)</span> to be full rank. A necessary condition is that each row of <span class="math inline">\(\Gamma_{22}^{\prime}\)</span> is non-zero but this is not sufficient.</p>
<p>We focus on the size performance of the homoskedastic Wald statistic for the 2SLS estimator of <span class="math inline">\(\beta_{2}\)</span>. For simplicity assume that the variance of <span class="math inline">\(e\)</span> is known and normalized to one. Using representation (12.32), the Wald statistic can be written as</p>
<p><span class="math display">\[
W=\boldsymbol{e}^{\prime} \widetilde{\boldsymbol{Z}}_{2}\left(\widetilde{\boldsymbol{Z}}_{2}^{\prime} \widetilde{\boldsymbol{Z}}_{2}\right)^{-1} \widetilde{\boldsymbol{Z}}_{2}^{\prime} \boldsymbol{Y}_{2}\left(\boldsymbol{Y}_{2}^{\prime} \widetilde{\boldsymbol{Z}}_{2}\left(\widetilde{\boldsymbol{Z}}_{2}^{\prime} \widetilde{\boldsymbol{Z}}_{2}\right)^{-1} \widetilde{\boldsymbol{Z}}_{2}^{\prime} \boldsymbol{Y}_{2}\right)^{-1}\left(\boldsymbol{Y}_{2}^{\prime} \widetilde{\boldsymbol{Z}}_{2}\left(\widetilde{\boldsymbol{Z}}_{2}^{\prime} \widetilde{\boldsymbol{Z}}_{2}\right)^{-1} \widetilde{\boldsymbol{Z}}_{2}^{\prime} \boldsymbol{e}\right)
\]</span></p>
<p>where <span class="math inline">\(\widetilde{\boldsymbol{Z}}_{2}=\left(\boldsymbol{I}_{n}-\boldsymbol{P}_{1}\right) \boldsymbol{Z}_{2}\)</span> and <span class="math inline">\(\boldsymbol{P}_{1}=\boldsymbol{Z}_{1}\left(\boldsymbol{Z}_{1}^{\prime} \boldsymbol{Z}_{1}\right)^{-1} \boldsymbol{Z}_{1}^{\prime}\)</span>.</p>
<p>Recall from Section <span class="math inline">\(12.36\)</span> that Stock and Staiger model the excluded instruments <span class="math inline">\(Z_{2}\)</span> as weak by setting <span class="math inline">\(\Gamma_{22}=n^{-1 / 2} \boldsymbol{C}\)</span> for some matrix <span class="math inline">\(\boldsymbol{C}\)</span>. In this framework we have the asymptotic distribution results</p>
<p><span class="math display">\[
\frac{1}{n} \widetilde{\boldsymbol{Z}}_{2}^{\prime} \widetilde{\boldsymbol{Z}}_{2} \underset{p}{\longrightarrow} \boldsymbol{Q}=\mathbb{E}\left[Z_{2} Z_{2}^{\prime}\right]-\mathbb{E}\left[Z_{2} Z_{1}^{\prime}\right]\left(\mathbb{E}\left[Z_{1} Z_{1}^{\prime}\right]\right)^{-1} \mathbb{E}\left[Z_{1} Z_{2}^{\prime}\right]
\]</span></p>
<p>and</p>
<p><span class="math display">\[
\frac{1}{\sqrt{n}} \widetilde{\boldsymbol{Z}}_{2}^{\prime} \boldsymbol{e} \underset{d}{\longrightarrow} \boldsymbol{Q}^{1 / 2} \xi_{0}
\]</span></p>
<p>where <span class="math inline">\(\xi_{0}\)</span> is a matrix normal variate whose columns are independent <span class="math inline">\(\mathrm{N}(0, \boldsymbol{I})\)</span>. Furthermore, setting <span class="math inline">\(\Sigma=\)</span> <span class="math inline">\(\mathbb{E}\left[u_{2} u_{2}^{\prime}\right]\)</span> and <span class="math inline">\(\overline{\boldsymbol{C}}=\boldsymbol{Q}^{1 / 2} \boldsymbol{C} \Sigma^{-1 / 2}\)</span>,</p>
<p><span class="math display">\[
\frac{1}{\sqrt{n}} \widetilde{\boldsymbol{Z}}_{2}^{\prime} \boldsymbol{Y}_{2}=\frac{1}{n} \widetilde{\boldsymbol{Z}}_{2}^{\prime} \widetilde{\boldsymbol{Z}}_{2} \boldsymbol{C}+\frac{1}{\sqrt{n}} \widetilde{\boldsymbol{Z}}_{2}^{\prime} \boldsymbol{U}_{2} \underset{d}{\longrightarrow} \boldsymbol{Q}^{1 / 2} \overline{\boldsymbol{C}} \Sigma^{1 / 2}+\boldsymbol{Q}^{1 / 2} \xi_{2} \Sigma^{1 / 2}
\]</span></p>
<p>where <span class="math inline">\(\xi_{2}\)</span> is a matrix normal variate whose columns are independent <span class="math inline">\(\mathrm{N}(0, \boldsymbol{I})\)</span>. The variables <span class="math inline">\(\xi_{0}\)</span> and <span class="math inline">\(\xi_{2}\)</span> are correlated. Together we obtain the asymptotic distribution of the Wald statistic</p>
<p><span class="math display">\[
W \underset{d}{\longrightarrow} S=\xi_{0}^{\prime}\left(\overline{\boldsymbol{C}}+\xi_{2}\right)\left(\overline{\boldsymbol{C}}^{\prime} \overline{\boldsymbol{C}}\right)^{-1}\left(\overline{\boldsymbol{C}}+\xi_{2}\right)^{\prime} \xi_{0}
\]</span></p>
<p>Using the spectral decomposition, <span class="math inline">\(\overline{\boldsymbol{C}}^{\prime} \overline{\boldsymbol{C}}=\boldsymbol{H}^{\prime} \Lambda \boldsymbol{H}\)</span> where <span class="math inline">\(\boldsymbol{H}^{\prime} \boldsymbol{H}=\boldsymbol{I}\)</span> and <span class="math inline">\(\Lambda\)</span> is diagonal. Thus we can write <span class="math inline">\(S=\xi_{0}^{\prime} \bar{\xi}_{2} \Lambda^{-1} \bar{\xi}_{2}^{\prime} \xi_{0}\)</span> where <span class="math inline">\(\bar{\xi}_{2}=\overline{\boldsymbol{C}} \boldsymbol{H}^{\prime}+\xi_{2} \boldsymbol{H}^{\prime}\)</span>. The matrix <span class="math inline">\(\xi^{*}=\left(\xi_{0}, \bar{\xi}_{2}\right)\)</span> is multivariate normal, so <span class="math inline">\(\xi^{* \prime} \xi^{*}\)</span> has what is called a non-central Wishart distribution. It only depends on the matrix <span class="math inline">\(\overline{\boldsymbol{C}}\)</span> through <span class="math inline">\(\boldsymbol{H} \overline{\boldsymbol{C}}^{\prime} \overline{\boldsymbol{C}} \boldsymbol{H}^{\prime}=\Lambda\)</span> which are the eigenvalues of <span class="math inline">\(\overline{\boldsymbol{C}}^{\prime} \overline{\boldsymbol{C}}\)</span>. Since <span class="math inline">\(S\)</span> is a function of <span class="math inline">\(\xi^{*}\)</span> only through <span class="math inline">\(\bar{\xi}_{2}^{\prime} \xi_{0}\)</span> we conclude that <span class="math inline">\(S\)</span> is a function of <span class="math inline">\(\overline{\boldsymbol{C}}\)</span> only through these eigenvalues.</p>
<p>This is a very quick derivation of a rather involved derivation but the conclusion drawn by Stock and Yogo is that the asymptotic distribution of the Wald statistic is non-standard and a function of the model parameters only through the eigenvalues of <span class="math inline">\(\overline{\boldsymbol{C}} \overline{\bar{C}}\)</span> and the correlations between the normal variates <span class="math inline">\(\xi_{0}\)</span> and <span class="math inline">\(\bar{\xi}_{2}\)</span>. The worst-case can be summarized by the maximal correlation between <span class="math inline">\(\xi_{0}\)</span> and <span class="math inline">\(\bar{\xi}_{2}\)</span> and the smallest eigenvalue of <span class="math inline">\(\overline{\boldsymbol{C}}^{\prime} \overline{\boldsymbol{C}}\)</span>. For convenience they rescale the latter by dividing by the number of endogenous variables. Define</p>
<p><span class="math display">\[
\boldsymbol{G}=\overline{\boldsymbol{C}}^{\prime} \overline{\boldsymbol{C}} / k_{2}=\Sigma^{-1 / 2} \boldsymbol{C}^{\prime} \boldsymbol{Q} \boldsymbol{C} \Sigma^{-1 / 2} / k_{2}
\]</span></p>
<p>and</p>
<p><span class="math display">\[
g=\lambda_{\min }(\boldsymbol{G})=\lambda_{\min }\left(\Sigma^{-1 / 2} \boldsymbol{C}^{\prime} \boldsymbol{Q} \boldsymbol{C} \Sigma^{-1 / 2}\right) / k_{2} .
\]</span></p>
<p>This can be estimated from the reduced-form regression</p>
<p><span class="math display">\[
X_{2 i}=\widehat{\Gamma}_{12}^{\prime} Z_{1 i}+\widehat{\Gamma}_{22}^{\prime} Z_{2 i}+\widehat{u}_{2 i} .
\]</span></p>
<p>The estimator is</p>
<p><span class="math display">\[
\begin{aligned}
\widehat{\boldsymbol{G}} &amp;=\widehat{\Sigma}^{-1 / 2} \widehat{\Gamma}_{22}^{\prime}\left(\widetilde{\boldsymbol{Z}}_{2}^{\prime} \widetilde{\boldsymbol{Z}}_{2}\right) \widehat{\Gamma}_{22} \widehat{\Sigma}^{-1 / 2} / k_{2}=\widehat{\Sigma}^{-1 / 2}\left(\boldsymbol{X}_{2}^{\prime} \widetilde{\boldsymbol{Z}}_{2}\left(\widetilde{\boldsymbol{Z}}_{2}^{\prime} \widetilde{\boldsymbol{Z}}_{2}\right)^{-1} \widetilde{\boldsymbol{Z}}_{2}^{\prime} \boldsymbol{X}_{2}\right) \widehat{\Sigma}^{-1 / 2} / k_{2} \\
\widehat{\Sigma} &amp;=\frac{1}{n-k} \sum_{i=1}^{n} \widehat{u}_{2 i} \widehat{u}_{2 i}^{\prime} \\
\widehat{g} &amp;=\lambda_{\min }(\widehat{\boldsymbol{G}})
\end{aligned}
\]</span></p>
<p><span class="math inline">\(\widehat{\boldsymbol{G}}\)</span> is a matrix <span class="math inline">\(F\)</span>-type statistic for the coefficient matrix <span class="math inline">\(\widehat{\Gamma}_{22}\)</span>.</p>
<p>The statistic <span class="math inline">\(\widehat{g}\)</span> was proposed by Cragg and Donald (1993) as a test for underidentification. Stock and Yogo (2005) use it as a test for weak instruments. Using simulation methods they determined critical values for <span class="math inline">\(\widehat{g}\)</span> similar to those for <span class="math inline">\(k_{2}=1\)</span>. For given size <span class="math inline">\(r&gt;0.05\)</span> there is a critical value <span class="math inline">\(c\)</span> (reported in the table below) such that if <span class="math inline">\(\widehat{g}&gt;c\)</span> then the 2SLS (or LIML) Wald statistic <span class="math inline">\(W\)</span> for <span class="math inline">\(\widehat{\beta}_{2}\)</span> has asymptotic size bounded below <span class="math inline">\(r\)</span>. On the other hand, if <span class="math inline">\(\widehat{g} \leq c\)</span> then we cannot bound the asymptotic size below <span class="math inline">\(r\)</span> and we cannot reject the hypothesis of weak instruments. Critical values (calculated by simulation) are reported in their paper and posted on Motohiro Yogo’s webpage. We report a subset for the case <span class="math inline">\(k_{2}=2\)</span> in Table 12.5. The methods and theory applies to the cases <span class="math inline">\(k_{2}&gt;2\)</span> as well but those critical values have not been calculated. As for the <span class="math inline">\(k_{2}=1\)</span> case the critical values for 2 SLS are dramatically increasing in <span class="math inline">\(\ell_{2}\)</span>. Thus when the model is over-identified, we need a large value of <span class="math inline">\(\widehat{g}\)</span> to reject the hypothesis of weak instruments. This is a strong cautionary message to check the <span class="math inline">\(\widehat{g}\)</span> statistic in applications. Furthermore, the critical values for LIML are generally decreasing in <span class="math inline">\(\ell_{2}\)</span> (except for <span class="math inline">\(r=0.10\)</span> where the critical values are increasing for large <span class="math inline">\(\ell_{2}\)</span> ). This means that for over-identified models LIML inference is less sensitive to weak instruments than 2SLS and may be the preferred estimation method.</p>
<p>The Stock-Yogo test can be implemented in Stata using the command estat firststage after ivregress 2sls or ivregres <span class="math inline">\(\operatorname{liml}\)</span> if a standard (non-robust) covariance matrix has been specified (that is, without the ‘, r’ option). Critical values which control for size are only available for <span class="math inline">\(k_{2} \leq 2\)</span>. For for <span class="math inline">\(k_{2}&gt;2\)</span> critical values which control for relative bias are reported.</p>
<p>Robust versions of the test have been proposed by Kleibergen and Paap (2006). These can be implemented in Stata using the downloadable command ivreg2.</p>
<p>Table 12.5: 5% Critical Value for Weak Instruments, <span class="math inline">\(k_{2}=2\)</span></p>
<p>|<span class="math inline">\(\ell_{2}\)</span>|<span class="math inline">\(0.10\)</span>|<span class="math inline">\(0.15\)</span>|<span class="math inline">\(0.20\)</span>|<span class="math inline">\(0.25\)</span>||<span class="math inline">\(0.10\)</span>|<span class="math inline">\(0.15\)</span>|<span class="math inline">\(0.20\)</span>|<span class="math inline">\(0.25\)</span>| |———:|—–:|—–:|—–:|—–:||—–:|—–:|—–:|—–:| | 2| <span class="math inline">\(7.0\)</span>| <span class="math inline">\(4.6\)</span>| <span class="math inline">\(3.9\)</span>| <span class="math inline">\(3.6\)</span>|| <span class="math inline">\(7.0\)</span>| <span class="math inline">\(4.6\)</span>| <span class="math inline">\(3.9\)</span>| <span class="math inline">\(3.6\)</span>| | 3|<span class="math inline">\(13.4\)</span>| <span class="math inline">\(8.2\)</span>| <span class="math inline">\(6.4\)</span>| <span class="math inline">\(5.4\)</span>|| <span class="math inline">\(5.4\)</span>| <span class="math inline">\(3.8\)</span>| <span class="math inline">\(3.3\)</span>| <span class="math inline">\(3.1\)</span>| | 4|<span class="math inline">\(16.9\)</span>| <span class="math inline">\(9.9\)</span>| <span class="math inline">\(7.5\)</span>| <span class="math inline">\(6.3\)</span>|| <span class="math inline">\(4.7\)</span>| <span class="math inline">\(3.4\)</span>| <span class="math inline">\(3.0\)</span>| <span class="math inline">\(2.8\)</span>| | 5|<span class="math inline">\(19.4\)</span>|<span class="math inline">\(11.2\)</span>| <span class="math inline">\(8.4\)</span>| <span class="math inline">\(6.9\)</span>|| <span class="math inline">\(4.3\)</span>| <span class="math inline">\(3.1\)</span>| <span class="math inline">\(2.8\)</span>| <span class="math inline">\(2.6\)</span>| | 6|<span class="math inline">\(21.7\)</span>|<span class="math inline">\(12.3\)</span>| <span class="math inline">\(9.1\)</span>| <span class="math inline">\(7.4\)</span>|| <span class="math inline">\(4.1\)</span>| <span class="math inline">\(2.9\)</span>| <span class="math inline">\(2.6\)</span>| <span class="math inline">\(2.5\)</span>| | 7|<span class="math inline">\(23.7\)</span>|<span class="math inline">\(13.3\)</span>| <span class="math inline">\(9.8\)</span>| <span class="math inline">\(7.9\)</span>|| <span class="math inline">\(3.9\)</span>| <span class="math inline">\(2.8\)</span>| <span class="math inline">\(2.5\)</span>| <span class="math inline">\(2.4\)</span>| | 8|<span class="math inline">\(25.6\)</span>|<span class="math inline">\(14.3\)</span>|<span class="math inline">\(10.4\)</span>| <span class="math inline">\(8.4\)</span>|| <span class="math inline">\(3.8\)</span>| <span class="math inline">\(2.7\)</span>| <span class="math inline">\(2.4\)</span>| <span class="math inline">\(2.3\)</span>| | 9|<span class="math inline">\(27.5\)</span>|<span class="math inline">\(15.2\)</span>|<span class="math inline">\(11.0\)</span>| <span class="math inline">\(8.8\)</span>|| <span class="math inline">\(3.7\)</span>| <span class="math inline">\(2.7\)</span>| <span class="math inline">\(2.4\)</span>| <span class="math inline">\(2.2\)</span>| | 10|<span class="math inline">\(29.3\)</span>|<span class="math inline">\(16.2\)</span>|<span class="math inline">\(11.6\)</span>| <span class="math inline">\(9.3\)</span>|| <span class="math inline">\(3.6\)</span>| <span class="math inline">\(2.6\)</span>| <span class="math inline">\(2.3\)</span>| <span class="math inline">\(2.1\)</span>| | 15|<span class="math inline">\(38.0\)</span>|<span class="math inline">\(20.6\)</span>|<span class="math inline">\(14.6\)</span>|<span class="math inline">\(11.6\)</span>|| <span class="math inline">\(3.5\)</span>| <span class="math inline">\(2.4\)</span>| <span class="math inline">\(2.1\)</span>| <span class="math inline">\(2.0\)</span>| | 20|<span class="math inline">\(46.6\)</span>|<span class="math inline">\(25.0\)</span>|<span class="math inline">\(17.6\)</span>|<span class="math inline">\(13.8\)</span>|| <span class="math inline">\(3.6\)</span>| <span class="math inline">\(2.4\)</span>| <span class="math inline">\(2.0\)</span>| <span class="math inline">\(1.9\)</span>| | 25|<span class="math inline">\(55.1\)</span>|<span class="math inline">\(29.3\)</span>|<span class="math inline">\(20.6\)</span>|<span class="math inline">\(16.1\)</span>|| <span class="math inline">\(3.6\)</span>| <span class="math inline">\(2.4\)</span>|<span class="math inline">\(1.97\)</span>| <span class="math inline">\(1.8\)</span>| | 30|<span class="math inline">\(63.5\)</span>|<span class="math inline">\(33.6\)</span>|<span class="math inline">\(23.5\)</span>|<span class="math inline">\(18.3\)</span>|| <span class="math inline">\(4.1\)</span>| <span class="math inline">\(2.4\)</span>|<span class="math inline">\(1.95\)</span>| <span class="math inline">\(1.7\)</span>|</p>
<p>Source: .</p>
</section>
<section id="example-acemoglu-johnson-and-robinson-2001" class="level2" data-number="12.40">
<h2 data-number="12.40" class="anchored" data-anchor-id="example-acemoglu-johnson-and-robinson-2001"><span class="header-section-number">12.40</span> Example: Acemoglu, Johnson, and Robinson (2001)</h2>
<p>One particularly well-cited instrumental variable regression is in Acemoglu, Johnson, and Robinson (2001) with additional details published in (2012). They are interested in the effect of political institutions on economic performance. The theory is that good institutions (rule-of-law, property rights) should result in a country having higher long-term economic output than if the same country had poor institutions. To investigate this question they focus on a sample of 64 former European colonies. Their data is in the file AJR2001 on the textbook website.</p>
<p>The authors’ premise is that modern political institutions have been influenced by colonization. In particular they argue that colonizing countries tended to set up colonies as either an “extractive state” or as a “migrant colony”. An extractive state was used by the colonizer to extract resources for the colonizing country but was not largely settled by the European colonists. In this case the colonists had no incentive to set up good political institutions. In contrast, if a colony was set up as a “migrant colony” then large numbers of European settlers migrated to the colony to live. These settlers desired institutions similar to those in their home country and hence had an incentive to set up good political institutions. The nature of institutions is quite persistent over time so these <span class="math inline">\(19^{t h}\)</span>-century foundations affect the nature of modern institutions. The authors conclude that the <span class="math inline">\(19^{t h}\)</span>-century nature of the colony is predictive of the nature of modern institutions and hence modern economic growth.</p>
<p>To start the investigation they report an OLS regression of log GDP per capita in 1995 on a measure of political institutions they call risk which is a measure of legal protection against expropriation. This variable ranges from 0 to 10 , with 0 the lowest protection against appropriation and 10 the highest. For each country the authors take the average value of the index over 1985 to 1995 (the mean is <span class="math inline">\(6.5\)</span> with a standard deviation of 1.5). Their reported OLS estimates (intercept omitted) are</p>
<p><img src="images//2022_09_17_f9391324ededdbb7a34eg-71.jpg" class="img-fluid"></p>
<p>These estimates imply a <span class="math inline">\(52 %\)</span> difference in GDP between countries with a 1 -unit difference in risk.</p>
<p>The authors argue that the risk is endogenous since economic output influences political institutions and because the variable risk is undoubtedly measured with error. These issues induce least-square bias in different directions and thus the overall bias effect is unclear.</p>
<p>To correct for endogeneity bias the authors argue the need for an instrumental variable which does not directly affect economic performance yet is associated with political institutions. Their innovative suggestion was to use the mortality rate which faced potential European settlers in the <span class="math inline">\(19^{t h}\)</span> century. Colonies with high expected mortality were less attractive to European settlers resulting in lower levels of European migrants. As a consequence the authors expect such colonies to be more likely structured as an extractive state rather than a migrant colony. To measure the expected mortality rate the authors use estimates provided by historical research of the annualized deaths per 1000 soldiers, labeled mortality. (They used military mortality rates as the military maintained high-quality records.) The first-stage regression is</p>
<p><span class="math display">\[
\text { risk }=\underset{(0.13)}{-0.61} \log (\text { mortality })+\widehat{u} .
\]</span></p>
<p>These estimates confirm that <span class="math inline">\(19^{t h}\)</span>-century high mortality rates are associated with lower quality modern institutions. Using <span class="math inline">\(\log\)</span> (mortality) as an instrument for risk, they estimate the structural equation using 2SLS and report</p>
<p><span class="math display">\[
\log (\text { GDP per Capita })=\begin{gathered}
0.94 \text { risk. } \\
(0.16)
\end{gathered}
\]</span></p>
<p>This estimate is much higher than the OLS estimate from (12.86). The estimate is consistent with a near doubling of GDP due to a 1-unit difference in the risk index.</p>
<p>These are simple regressions involving just one right-hand-side variable. The authors considered a range of other models. Included in these results are a reversal of a traditional finding. In a conventional least squares regression two relevant variables for output are latitude (distance from the equator) and africa (a dummy variable for countries from Africa) both of which are difficult to interpret causally. But in the proposed instrumental variables regression the variables latitude and africa have much smaller and statistically insignificant - coefficients. To assess the specification we can use the Stock-Yogo and endogeneity tests. The Stock-Yogo test is from the reduced form (12.87). The instrument has a t-ratio of <span class="math inline">\(4.8\)</span> (or <span class="math inline">\(F=23\)</span> ) which exceeds the StockYogo critical value and hence can be treated as strong. For an endogeneity test we take the least squares residual <span class="math inline">\(\widehat{u}\)</span> from this equation and include it in the structural equation and estimate by least squares. We find a coefficient on <span class="math inline">\(\widehat{u}\)</span> of <span class="math inline">\(-0.57\)</span> with a t-ratio of <span class="math inline">\(4.7\)</span> which is highly significant. We conclude that the least squares and 2SLS estimates are statistically different and reject the hypothesis that the variable risk is exogenous for the GDP structural equation.</p>
<p>In Exercise <span class="math inline">\(12.22\)</span> you will replicate and extend these results using the authors’ data.</p>
<p>This paper is a creative and careful use of instrumental variables. The creativity stems from the historical analysis which lead to the focus on mortality as a potential predictor of migration choices. The care comes in the implementation as the authors needed to gather country-level data on political institutions and mortality from distinct sources. Putting these pieces together is the art of the project.</p>
</section>
<section id="example-angrist-and-krueger-1991" class="level2" data-number="12.41">
<h2 data-number="12.41" class="anchored" data-anchor-id="example-angrist-and-krueger-1991"><span class="header-section-number">12.41</span> Example: Angrist and Krueger (1991)</h2>
<p>Another influential instrument variable regression is Angrist and Krueger (1991). Their concern, similar to Card (1995), is estimation of the structural returns to education while treating educational attainment as endogenous. Like Card, their goal is to find an instrument which is exogenous for wages yet has an impact on education. A subset of their data in the file AK1991 on the textbook website.</p>
<p>Their creative suggestion was to focus on compulsory school attendance policies and their interaction with birthdates. Compulsory schooling laws vary across states in the United States, but typically require that youth remain in school until their sixteenth or seventeenth birthday. Angrist and Krueger argue that compulsory schooling has a causal effect on wages - youth who would have chosen to drop out of school stay in school for more years - and thus have more education which causally impacts their earnings as adults.</p>
<p>Angrist and Krueger observe that these policies have differential impact on youth who are born early or late in the school year. Students who are born early in the calendar year are typically older when they enter school. Consequently when they attain the legal dropout age they have attended less school than those born near the end of the year. This means that birthdate (early in the calendar year versus late) exogenously impacts educational attainment and thus wages through education. Yet birthdate must be exogenous for the structural wage equation as there is no reason to believe that birthdate itself has a causal impact on a person’s ability or wages. These considerations together suggest that birthdate is a valid instrumental variable for education in a causal wage equation.</p>
<p>Typical wage datasets include age but not birthdates. To obtain information on birthdate, Angrist and Krueger used U.S. Census data which includes an individual’s quarter of birth (January-March, AprilJune, etc.). They use this variable to construct 2SLS estimates of the return to education.</p>
<p>Their paper carefully documents that educational attainment varies by quarter of birth (as predicted by the above discussion), and reports a large set of least squares and 2SLS estimates. We focus on two estimates at the core of their analysis, reported in column (6) of their Tables <span class="math inline">\(\mathrm{V}\)</span> and VII. This involves data from the 1980 census with men born in 1930-1939, with 329,509 observations. The first equation is</p>
<p><img src="images//2022_09_17_f9391324ededdbb7a34eg-72.jpg" class="img-fluid"></p>
<p>where <span class="math inline">\(e d u\)</span> is years of education and Black, urban, and married are dummy variables indicating race (1 if Black, 0 otherwise), lives in a metropolitan area, and if married. In addition to the reported coefficients the equation also includes as regressors nine year-of-birth dummies and eight region-of-residence dummies. The equation is estimated by 2 SLS. The instrumental variables are the 30 interactions of three quarter-of-birth times ten year-of-birth dummy variables.</p>
<p>This equation indicates an <span class="math inline">\(8 %\)</span> increase in wages due to each year of education.</p>
<p>Angrist and Krueger observe that the effect of compulsory education laws are likely to vary across states, so expand the instrument set to include interactions with state-of-birth. They estimate the following equation by 2 SLS</p>
<p><img src="images//2022_09_17_f9391324ededdbb7a34eg-73.jpg" class="img-fluid"></p>
<p>This equation also adds fifty state-of-birth dummy variables as regressors. The instrumental variables are the 180 interactions of quarter-of-birth times year-of-birth dummy variables, plus quarter-of-birth times state-of-birth interactions.</p>
<p>This equation shows a similar estimated causal effect of education on wages as in (12.89). More notably, the standard error is smaller in (12.90) suggesting improved precision by the expanded instrumental variable set.</p>
<p>However, these estimates seem excellent candidates for weak instruments and many instruments. Indeed, this paper (published in 1991) helped spark these two literatures. We can use the Stock-Yogo tools to explore the instrument strength and the implications for the Angrist-Krueger estimates.</p>
<p>We first take equation (12.89). Using the original Angrist-Krueger data we estimate the corresponding reduced form and calculate the <span class="math inline">\(F\)</span> statistic for the 30 excluded instruments. We find <span class="math inline">\(F=4.8\)</span>. It has an asymptotic p-value of <span class="math inline">\(0.000\)</span> suggesting that we can reject (at any significance level) the hypothesis that the coefficients on the excluded instruments are zero. Thus Angrist and Krueger appear to be correct that quarter of birth helps to explain educational attainment and are thus a valid instrumental variable set. However, using the Stock-Yogo test, <span class="math inline">\(F=4.8\)</span> is not high enough to reject the hypothesis that the instruments are weak. Specifically, for <span class="math inline">\(\ell_{2}=30\)</span> and <span class="math inline">\(15 %\)</span> size the critical value for the <span class="math inline">\(F\)</span> statistic is 45 . The actual value of <span class="math inline">\(4.8\)</span> is far below 45. Since we cannot reject that the instruments are weak this indicates that we cannot interpret the 2SLS estimates and test statistics in (12.89) as reliable.</p>
<p>Second, take (12.90) with the expanded regressor and instrument set. Estimating the corresponding reduced form we find the <span class="math inline">\(F\)</span> statistic for the 180 excluded instruments is <span class="math inline">\(\mathrm{F}=2.43\)</span> which also has an asymptotic p-value of <span class="math inline">\(0.000\)</span> indicating that we can reject at any significance level the hypothesis that the excluded instruments have no effect on educational attainment. However, using the Stock-Yogo test we also cannot reject the hypothesis that the instruments are weak. While Stock and Yogo did not calculate the critical values for <span class="math inline">\(\ell_{2}=180\)</span>, the 2 SLS critical values are increasing in <span class="math inline">\(\ell_{2}\)</span> so we can use those for <span class="math inline">\(\ell_{2}=30\)</span> as a lower bound. The observed value of <span class="math inline">\(\mathrm{F}=2.43\)</span> is far below the level needed for significance. Consequently the results in (12.90) cannot be viewed as reliable. In particular, the observation that the standard errors in (12.90) are smaller than those in (12.89) should not be interpreted as evidence of greater precision. Rather, they should be viewed as evidence of unreliability due to weak instruments.</p>
<p>When instruments are weak one constructive suggestion is to use LIML estimation rather than 2SLS. Another constructive suggestion is to alter the instrument set. While Angrist and Krueger used a large number of instrumental variables we can consider a smaller set. Take equation (12.89). Rather than estimating it using the 30 interaction instruments consider using only the three quarter-of-birth dummy variables. We report the reduced form estimates here:</p>
<p><img src="images//2022_09_17_f9391324ededdbb7a34eg-73(1).jpg" class="img-fluid"></p>
<p>where <span class="math inline">\(Q_{2}, Q_{3}\)</span>, and <span class="math inline">\(Q_{4}\)</span> are dummy variables for birth in the <span class="math inline">\(2^{n d}, 3^{r d}\)</span>, and <span class="math inline">\(4^{t h}\)</span> quarter. The regression also includes nine year-of-birth and eight region-of-residence dummy variables.</p>
<p>The reduced form coefficients in (12.91) on the quarter-of-birth dummies are instructive. The coefficients are positive and increasing, consistent with the Angrist-Krueger hypothesis that individuals born later in the year achieve higher average education. Focusing on the weak instrument problem the <span class="math inline">\(F\)</span> test for exclusion of these three variables is <span class="math inline">\(\mathrm{F}=31\)</span>. The Stock-Yogo critical value is <span class="math inline">\(12.8\)</span> for <span class="math inline">\(\ell_{2}=3\)</span> and a size of <span class="math inline">\(15 %\)</span>, and is <span class="math inline">\(22.3\)</span> for a size of <span class="math inline">\(10 %\)</span>. Since <span class="math inline">\(F=31\)</span> exceeds both these thresholds we can reject the hypothesis that this reduced form is weak. Estimating the model by 2SLS with these three instruments we find</p>
<p><img src="images//2022_09_17_f9391324ededdbb7a34eg-74.jpg" class="img-fluid"></p>
<p>These estimates indicate a slightly larger (10%) causal impact of education on wages but with a larger standard error. The Stock-Yogo analysis indicates that we can interpret the confidence intervals from these estimates as having asymptotic coverage <span class="math inline">\(85 %\)</span>.</p>
<p>While the original Angrist-Krueger estimates suffer due to weak instruments their paper is a very creative and thoughtful application of the natural experiment methodology. They discovered a completely exogenous variation present in the world - birthdate - and showed how this has a small but measurable effect on educational attainment and thereby on earnings. Their crafting of this natural experiment regression is clever and demonstrates a style of analysis which can successfully underlie an effective instrumental variables empirical analysis.</p>
</section>
<section id="programming" class="level2" data-number="12.42">
<h2 data-number="12.42" class="anchored" data-anchor-id="programming"><span class="header-section-number">12.42</span> Programming</h2>
<p>We now present Stata code for some of the empirical work reported in this chapter.</p>
<p>Stata do File for Card Example use Card1995.dta, clear</p>
<p>set more off</p>
<p>gen exp = age <span class="math inline">\(76-\)</span> ed <span class="math inline">\(76-6\)</span></p>
<p>gen <span class="math inline">\(\exp 2=\left(\exp ^{\wedge} 2\right) / 100\)</span></p>
<ul>
<li>Drop observations with missing wage</li>
</ul>
<p>drop if lwage <span class="math inline">\(76==.\)</span></p>
<ul>
<li>Table <span class="math inline">\(12.1\)</span> regressions</li>
</ul>
<p>reg lwage76 ed76 exp exp2 black reg76r smsa76r, <span class="math inline">\(r\)</span></p>
<p>ivregress 2 sls lwage76 exp exp2 black reg76r smsa76r (ed76=nearc4), r</p>
<p>ivregress 2 sls lwage76 black reg76r smsa76r (ed76 exp exp2 <span class="math inline">\(=\)</span> nearc4 age76 age2), r perfect</p>
<p>ivregress 2sls lwage76 exp exp2 black reg76r smsa76r (ed76=nearc4a nearc4b), <span class="math inline">\(\mathrm{r}\)</span></p>
<p>ivregress 2 sls lwage76 black reg76r smsa76r (ed76 exp exp2 <span class="math inline">\(=\)</span> nearc4a nearc4b age76 age2), <span class="math inline">\(r\)</span> perfect</p>
<p>ivregress liml lwage76 exp exp2 black reg76r smsa76r (ed76=nearc4a nearc4b), <span class="math inline">\(r\)</span></p>
<ul>
<li>Table <span class="math inline">\(12.2\)</span> regressions</li>
</ul>
<p>reg lwage76 exp exp2 black reg76r smsa76r nearc4, <span class="math inline">\(r\)</span></p>
<p>reg ed76 exp exp2 black reg76r smsa76r nearc4, <span class="math inline">\(r\)</span></p>
<p>reg ed76 black reg76r smsa76r nearc4 age76 age2, <span class="math inline">\(r\)</span></p>
<p>reg exp black reg76r smsa76r nearc4 age76 age2, <span class="math inline">\(r\)</span></p>
<p>reg exp2 black reg76r smsa76r nearc4 age76 age2, <span class="math inline">\(r\)</span></p>
<p>reg ed76 exp exp2 black reg76r smsa76r nearc4a nearc4b, <span class="math inline">\(r\)</span></p>
<p>reg lwage76 ed76 exp exp2 smsa76r reg76r, <span class="math inline">\(r\)</span></p>
<p>reg lwage76 nearc4 exp exp2 smsa76r reg76r, <span class="math inline">\(r\)</span></p>
<p>reg ed76 nearc4 exp exp2 smsa76r reg76r, <span class="math inline">\(r\)</span></p>
<p>Stata do File for Acemoglu-Johnson-Robinson Example use AJR2001.dta, clear</p>
<p>reg loggdp risk</p>
<p>reg risk logmort0</p>
<p>predict <span class="math inline">\(u\)</span>, residual</p>
<p>ivregress 2sls loggdp (risk=logmort0)</p>
<p>reg loggdp risk <span class="math inline">\(u\)</span></p>
<table class="table">
<colgroup>
<col style="width: 100%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Stata do File for Angrist-Krueger Example</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">use AK1991.dta, clear</td>
</tr>
<tr class="even">
<td style="text-align: left;">ivregress 2sls logwage black smsa married i.yob i.region (edu = i.qob#i.yob)</td>
</tr>
<tr class="odd">
<td style="text-align: left;">ivregress 2sls logwage black smsa married i.yob i.region i.state (edu <span class="math inline">\(=\)</span></td>
</tr>
<tr class="even">
<td style="text-align: left;">i.qob#i.yob i.qob#i.state)</td>
</tr>
<tr class="odd">
<td style="text-align: left;">reg edu black smsa married i.yob i.region i.qob#i.yob</td>
</tr>
<tr class="even">
<td style="text-align: left;">testparm i.qob#i.yob</td>
</tr>
<tr class="odd">
<td style="text-align: left;">reg edu black smsa married i.yob i.region i.state i.qob#i.yob i.qob#i.state</td>
</tr>
<tr class="even">
<td style="text-align: left;">testparm i.qob#i.yob i.qob#i.state</td>
</tr>
<tr class="odd">
<td style="text-align: left;">reg edu black smsa married i.yob i.region i.qob</td>
</tr>
<tr class="even">
<td style="text-align: left;">testparm i.qob</td>
</tr>
<tr class="odd">
<td style="text-align: left;">ivregress 2sls logwage black smsa married i.yob i.region (edu = i.qob)</td>
</tr>
</tbody>
</table>
</section>
<section id="exercises" class="level2" data-number="12.43">
<h2 data-number="12.43" class="anchored" data-anchor-id="exercises"><span class="header-section-number">12.43</span> Exercises</h2>
<p>Exercise 12.1 Consider the single equation model <span class="math inline">\(Y=Z \beta+e\)</span> where <span class="math inline">\(Y\)</span> and <span class="math inline">\(Z\)</span> are both real-valued <span class="math inline">\((1 \times 1)\)</span>. Let <span class="math inline">\(\widehat{\beta}\)</span> denote the IV estimator of <span class="math inline">\(\beta\)</span> using as an instrument a dummy variable <span class="math inline">\(D\)</span> (takes only the values 0 and 1). Find a simple expression for the IV estimator in this context.</p>
<p>Exercise 12.2 Take the linear model <span class="math inline">\(Y=X^{\prime} \beta+e\)</span> with <span class="math inline">\(\mathbb{E}[e \mid X]=0\)</span>. Suppose <span class="math inline">\(\sigma^{2}(x)=\mathbb{E}\left[e^{2} \mid X=x\right]\)</span> is known. Show that the GLS estimator of <span class="math inline">\(\beta\)</span> can be written as an IV estimator using some instrument <span class="math inline">\(Z\)</span>. (Find an expression for <span class="math inline">\(Z\)</span>.)</p>
<p>Exercise 12.3 Take the linear model <span class="math inline">\(Y=X^{\prime} \beta+e\)</span>. Let the OLS estimator for <span class="math inline">\(\beta\)</span> be <span class="math inline">\(\widehat{\beta}\)</span> with OLS residual <span class="math inline">\(\widehat{e}_{i}\)</span>. Let the IV estimator for <span class="math inline">\(\beta\)</span> using some instrument <span class="math inline">\(Z\)</span> be <span class="math inline">\(\widetilde{\beta}\)</span> with IV residual <span class="math inline">\(\widetilde{e}_{i}=Y_{i}-X_{i}^{\prime} \widetilde{\beta}\)</span>. If <span class="math inline">\(X\)</span> is indeed endogenous, will IV “fit” better than OLS in the sense that <span class="math inline">\(\sum_{i=1}^{n} \widetilde{e}_{i}^{2}&lt;\sum_{i=1}^{n} \widehat{e}_{i}^{2}\)</span>, at least in large samples?</p>
<p>Exercise 12.4 The reduced form between the regressors <span class="math inline">\(X\)</span> and instruments <span class="math inline">\(Z\)</span> takes the form <span class="math inline">\(X=\Gamma^{\prime} Z+u\)</span> where <span class="math inline">\(X\)</span> is <span class="math inline">\(k \times 1, Z\)</span> is <span class="math inline">\(\ell \times 1\)</span>, and <span class="math inline">\(\Gamma\)</span> is <span class="math inline">\(\ell \times k\)</span>. The parameter <span class="math inline">\(\Gamma\)</span> is defined by the population moment condition <span class="math inline">\(\mathbb{E}\left[Z u^{\prime}\right]=0\)</span>. Show that the method of moments estimator for <span class="math inline">\(\Gamma\)</span> is <span class="math inline">\(\widehat{\Gamma}=\left(\boldsymbol{Z}^{\prime} \boldsymbol{Z}\right)^{-1}\left(\boldsymbol{Z}^{\prime} \boldsymbol{X}\right)\)</span>.</p>
<p>Exercise 12.5 In the structural model <span class="math inline">\(Y=X^{\prime} \beta+e\)</span> with <span class="math inline">\(X=\Gamma^{\prime} Z+u\)</span> and <span class="math inline">\(\Gamma \ell \times k, \ell \geq k\)</span>, we claim that a necessary condition for <span class="math inline">\(\beta\)</span> to be identified (can be recovered from the reduced form) is <span class="math inline">\(\operatorname{rank}(\Gamma)=k\)</span>. Explain why this is true. That is, show that if <span class="math inline">\(\operatorname{rank}(\Gamma)&lt;k\)</span> then <span class="math inline">\(\beta\)</span> is not identified.</p>
<p>Exercise 12.6 For Theorem <span class="math inline">\(12.3\)</span> establish that <span class="math inline">\(\widehat{\boldsymbol{V}}_{\beta} \underset{p}{\longrightarrow} \boldsymbol{V}_{\beta}\)</span></p>
<p>Exercise 12.7 Take the linear model <span class="math inline">\(Y=X^{\prime} \beta+e\)</span> with <span class="math inline">\(\mathbb{E}[e \mid X]=0\)</span> where <span class="math inline">\(X\)</span> and <span class="math inline">\(\beta\)</span> are <span class="math inline">\(1 \times 1\)</span>.</p>
<ol type="a">
<li><p>Show that <span class="math inline">\(\mathbb{E}[X e]=0\)</span> and <span class="math inline">\(\mathbb{E}\left[X^{2} e\right]=0\)</span>. Is <span class="math inline">\(Z=\left(\begin{array}{ll}X &amp; X^{2}\end{array}\right)^{\prime}\)</span> a valid instrument for estimation of <span class="math inline">\(\beta\)</span> ?</p></li>
<li><p>Define the 2SLS estimator of <span class="math inline">\(\beta\)</span> using <span class="math inline">\(Z\)</span> as an instrument for <span class="math inline">\(X\)</span>. How does this differ from OLS? Exercise 12.8 Suppose that price and quantity are determined by the intersection of the linear demand and supply curves</p></li>
</ol>
<p><span class="math display">\[
\begin{aligned}
\text { Demand: } &amp; Q=a_{0}+a_{1} P+a_{2} Y+e_{1} \\
\text { Supply: } &amp; Q=b_{0}+b_{1} P+b_{2} W+e_{2}
\end{aligned}
\]</span></p>
<p>where income <span class="math inline">\((Y)\)</span> and wage <span class="math inline">\((W)\)</span> are determined outside the market. In this model are the parameters identified?</p>
<p>Exercise 12.9 Consider the model <span class="math inline">\(Y=X^{\prime} \beta+e\)</span> with <span class="math inline">\(\mathbb{E}[e \mid Z]=0\)</span> with <span class="math inline">\(Y\)</span> scalar and <span class="math inline">\(X\)</span> and <span class="math inline">\(Z\)</span> each a <span class="math inline">\(k\)</span> vector. You have a random sample <span class="math inline">\(\left(Y_{i}, X_{i}, Z_{i}: i=1, \ldots, n\right)\)</span>.</p>
<ol type="a">
<li><p>Assume that <span class="math inline">\(X\)</span> is exogenous in the sense that <span class="math inline">\(\mathbb{E}[e \mid Z, X]=0\)</span>. Is the IV estimator <span class="math inline">\(\widehat{\beta}_{\mathrm{iv}}\)</span> unbiased?</p></li>
<li><p>Continuing to assume that <span class="math inline">\(X\)</span> is exogenous, find the conditional covariance matrix <span class="math inline">\(\operatorname{var}\left[\widehat{\beta}_{\text {iv }} \mid \boldsymbol{X}, \boldsymbol{Z}\right]\)</span>.</p></li>
</ol>
<p>Exercise 12.10 Consider the model</p>
<p><span class="math display">\[
\begin{aligned}
Y &amp;=X^{\prime} \beta+e \\
X &amp;=\Gamma^{\prime} Z+u \\
\mathbb{E}[Z e] &amp;=0 \\
\mathbb{E}\left[Z u^{\prime}\right] &amp;=0
\end{aligned}
\]</span></p>
<p>with <span class="math inline">\(Y\)</span> scalar and <span class="math inline">\(X\)</span> and <span class="math inline">\(Z\)</span> each a <span class="math inline">\(k\)</span> vector. You have a random sample <span class="math inline">\(\left(Y_{i}, X_{i}, Z_{i}: i=1, \ldots, n\right)\)</span>. Take the control function equation <span class="math inline">\(e=u^{\prime} \gamma+v\)</span> with <span class="math inline">\(\mathbb{E}[u v]=0\)</span> and assume for simplicity that <span class="math inline">\(u\)</span> is observed. Inserting into the structural equation we find <span class="math inline">\(Y=Z^{\prime} \beta+u^{\prime} \gamma+v\)</span>. The control function estimator <span class="math inline">\((\widehat{\beta}, \widehat{\gamma})\)</span> is OLS estimation of this equation.</p>
<ol type="a">
<li><p>Show that <span class="math inline">\(\mathbb{E}[X v]=0\)</span> (algebraically).</p></li>
<li><p>Derive the asymptotic distribution of <span class="math inline">\((\widehat{\beta}, \widehat{\gamma})\)</span>.</p></li>
</ol>
<p>Exercise 12.11 Consider the structural equation</p>
<p><span class="math display">\[
Y=\beta_{0}+\beta_{1} X+\beta_{2} X^{2}+e
\]</span></p>
<p>with <span class="math inline">\(X \in \mathbb{R}\)</span> treated as endogenous so that <span class="math inline">\(\mathbb{E}[X e] \neq 0\)</span>. We have an instrument <span class="math inline">\(Z \in \mathbb{R}\)</span> which satisfies <span class="math inline">\(\mathbb{E}[e \mid Z]=0\)</span> so in particular <span class="math inline">\(\mathbb{E}[e]=0, \mathbb{E}[Z e]=0\)</span> and <span class="math inline">\(\mathbb{E}\left[Z^{2} e\right]=0\)</span>.</p>
<ol type="a">
<li><p>Should <span class="math inline">\(X^{2}\)</span> be treated as endogenous or exogenous?</p></li>
<li><p>Suppose we have a scalar instrument <span class="math inline">\(Z\)</span> which satisfies</p></li>
</ol>
<p><span class="math display">\[
X=\gamma_{0}+\gamma_{1} Z+u
\]</span></p>
<p>with <span class="math inline">\(u\)</span> independent of <span class="math inline">\(Z\)</span> and mean zero.</p>
<p>Consider using <span class="math inline">\(\left(1, Z, Z^{2}\right.\)</span> ) as instruments. Is this a sufficient number of instruments? Is (12.93) just-identified, over-identified, or under-identified?</p>
<ol start="3" type="a">
<li>Write out the reduced form equation for <span class="math inline">\(X^{2}\)</span>. Under what condition on the reduced form parameters (12.94) are the parameters in (12.93) identified? Exercise 12.12 Consider the structural equation and reduced form</li>
</ol>
<p><span class="math display">\[
\begin{aligned}
Y &amp;=\beta X^{2}+e \\
X &amp;=\gamma Z+u \\
\mathbb{E}[Z e] &amp;=0 \\
\mathbb{E}[Z u] &amp;=0
\end{aligned}
\]</span></p>
<p>with <span class="math inline">\(X^{2}\)</span> treated as endogenous so that <span class="math inline">\(\mathbb{E}\left[X^{2} e\right] \neq 0\)</span>. For simplicity assume no intercepts. <span class="math inline">\(Y, Z\)</span>, and <span class="math inline">\(X\)</span> are scalar. Assume <span class="math inline">\(\gamma \neq 0\)</span>. Consider the following estimator. First, estimate <span class="math inline">\(\gamma\)</span> by OLS of <span class="math inline">\(X\)</span> on <span class="math inline">\(Z\)</span> and construct the fitted values <span class="math inline">\(\widehat{X}_{i}=\widehat{\gamma} Z_{i}\)</span>. Second, estimate <span class="math inline">\(\beta\)</span> by OLS of <span class="math inline">\(Y_{i}\)</span> on <span class="math inline">\(\left(\widehat{X}_{i}\right)^{2}\)</span>.</p>
<ol type="a">
<li><p>Write out this estimator <span class="math inline">\(\widehat{\beta}\)</span> explicitly as a function of the sample.</p></li>
<li><p>Find its probability limit as <span class="math inline">\(n \rightarrow \infty\)</span>.</p></li>
<li><p>In general, is <span class="math inline">\(\widehat{\beta}\)</span> consistent for <span class="math inline">\(\beta\)</span> ? Is there a reasonable condition under which <span class="math inline">\(\widehat{\beta}\)</span> is consistent?</p></li>
</ol>
<p>Exercise 12.13 Consider the structural equation <span class="math inline">\(Y_{1}=Z_{1}^{\prime} \beta_{1}+Y_{2}^{\prime} \beta_{2}+e\)</span> with <span class="math inline">\(\mathbb{E}[Z e]=0\)</span> where <span class="math inline">\(Y_{2}\)</span> is <span class="math inline">\(k_{2} \times 1\)</span> and treated as endogenous. The variables <span class="math inline">\(Z=\left(Z_{1}, Z_{2}\right)\)</span> are treated as exogenous where <span class="math inline">\(Z_{2}\)</span> is <span class="math inline">\(\ell_{2} \times 1\)</span> and <span class="math inline">\(\ell_{2} \geq k_{2}\)</span>. You are interested in testing the hypothesis <span class="math inline">\(\mathbb{H}_{0}: \beta_{2}=0\)</span>.</p>
<p>Consider the reduced form equation for <span class="math inline">\(Y_{1}\)</span></p>
<p><span class="math display">\[
Y_{1}=Z_{1}^{\prime} \lambda_{1}+Z_{2}^{\prime} \lambda_{2}+u_{1} .
\]</span></p>
<p>Show how to test <span class="math inline">\(\mathbb{M}_{0}\)</span> using only the OLS estimates of (12.95).</p>
<p>Hint: This will require an analysis of the reduced form equations and their relation to the structural equation.</p>
<p>Exercise 12.14 Take the linear instrumental variables equation <span class="math inline">\(Y_{1}=Z_{1}^{\prime} \beta_{1}+Y_{2}^{\prime} \beta_{2}+e\)</span> with <span class="math inline">\(\mathbb{E}[Z e]=0\)</span> where <span class="math inline">\(Z_{1}\)</span> is <span class="math inline">\(k_{1} \times 1, Y_{2}\)</span> is <span class="math inline">\(k_{2} \times 1\)</span>, and <span class="math inline">\(Z\)</span> is <span class="math inline">\(\ell \times 1\)</span>, with <span class="math inline">\(\ell \geq k=k_{1}+k_{2}\)</span>. The sample size is <span class="math inline">\(n\)</span>. Assume that <span class="math inline">\(\boldsymbol{Q}_{Z Z}=\)</span> <span class="math inline">\(\mathbb{E}\left[Z Z^{\prime}\right]&gt;0\)</span> and <span class="math inline">\(Q_{Z X}=\mathbb{E}\left[Z X^{\prime}\right]\)</span> has full rank <span class="math inline">\(k\)</span>.</p>
<p>Suppose that only <span class="math inline">\(\left(Y_{1}, Z_{1}, Z_{2}\right)\)</span> are available and <span class="math inline">\(Y_{2}\)</span> is missing from the dataset.</p>
<p>Consider the 2SLS estimator <span class="math inline">\(\widehat{\beta}_{1}\)</span> of <span class="math inline">\(\beta_{1}\)</span> obtained from the misspecified IV regression of <span class="math inline">\(Y_{1}\)</span> on <span class="math inline">\(Z_{1}\)</span> only, using <span class="math inline">\(Z_{2}\)</span> as an instrument for <span class="math inline">\(Z_{1}\)</span>.</p>
<ol type="a">
<li><p>Find a stochastic decomposition <span class="math inline">\(\widehat{\beta}_{1}=\beta_{1}+b_{1 n}+r_{1 n}\)</span> where <span class="math inline">\(r_{1 n}\)</span> depends on the error <span class="math inline">\(e\)</span> and <span class="math inline">\(b_{1 n}\)</span> does not depend on the error <span class="math inline">\(e\)</span>.</p></li>
<li><p>Show that <span class="math inline">\(r_{1 n} \rightarrow p 0\)</span> as <span class="math inline">\(n \rightarrow \infty\)</span>.</p></li>
<li><p>Find the probability limit of <span class="math inline">\(b_{1 n}\)</span> and <span class="math inline">\(\widehat{\beta}_{1}\)</span> as <span class="math inline">\(n \rightarrow \infty\)</span>.</p></li>
<li><p>Does <span class="math inline">\(\widehat{\beta}_{1}\)</span> suffer from “omitted variables bias”? Explain. Under what conditions is there no omitted variables bias?</p></li>
<li><p>Find the asymptotic distribution as <span class="math inline">\(n \rightarrow \infty\)</span> of <span class="math inline">\(\sqrt{n}\left(\widehat{\beta}_{1}-\beta_{1}-b_{1 n}\right)\)</span>.</p></li>
</ol>
<p>Exercise 12.15 Take the linear instrumental variables equation <span class="math inline">\(Y_{1}=Z \beta_{1}+Y_{2} \beta_{2}+e\)</span> with <span class="math inline">\(\mathbb{E}[e \mid Z]=0\)</span> where both <span class="math inline">\(X\)</span> and <span class="math inline">\(Z\)</span> are scalar <span class="math inline">\(1 \times 1\)</span>.</p>
<ol type="a">
<li>Can the coefficients <span class="math inline">\(\left(\beta_{1}, \beta_{2}\right)\)</span> be estimated by 2 SLS using <span class="math inline">\(Z\)</span> as an instrument for <span class="math inline">\(Y_{2}\)</span> ?</li>
</ol>
<p>Why or why not? (b) Can the coefficients <span class="math inline">\(\left(\beta_{1}, \beta_{2}\right)\)</span> be estimated by 2SLS using <span class="math inline">\(Z\)</span> and <span class="math inline">\(Z^{2}\)</span> as instruments?</p>
<ol start="3" type="a">
<li><p>For the 2SLS estimator suggested in (b), what is the implicit exclusion restriction?</p></li>
<li><p>In (b) what is the implicit assumption about instrument relevance?</p></li>
</ol>
<p>[Hint: Write down the implied reduced form equation for <span class="math inline">\(Y_{2}\)</span>.]</p>
<ol start="5" type="a">
<li>In a generic application would you be comfortable with the assumptions in (c) and (d)?</li>
</ol>
<p>Exercise 12.16 Take a linear equation with endogeneity and a just-identified linear reduced form <span class="math inline">\(Y=\)</span> <span class="math inline">\(X \beta+e\)</span> with <span class="math inline">\(X=\gamma Z+u_{2}\)</span> where both <span class="math inline">\(X\)</span> and <span class="math inline">\(Z\)</span> are scalar <span class="math inline">\(1 \times 1\)</span>. Assume that <span class="math inline">\(\mathbb{E}[Z e]=0\)</span> and <span class="math inline">\(\mathbb{E}\left[Z u_{2}\right]=0\)</span>.</p>
<ol type="a">
<li><p>Derive the reduced form equation <span class="math inline">\(Y=Z \lambda+u_{1}\)</span>. Show that <span class="math inline">\(\beta=\lambda / \gamma\)</span> if <span class="math inline">\(\gamma \neq 0\)</span>, and that <span class="math inline">\(\mathbb{E}[Z u]=0\)</span>.</p></li>
<li><p>Let <span class="math inline">\(\widehat{\lambda}\)</span> denote the OLS estimate from linear regression of <span class="math inline">\(Y\)</span> on <span class="math inline">\(Z\)</span>, and let <span class="math inline">\(\widehat{\gamma}\)</span> denote the OLS estimate from linear regression of <span class="math inline">\(X\)</span> on <span class="math inline">\(Z\)</span>. Write <span class="math inline">\(\theta=(\lambda, \gamma)^{\prime}\)</span> and let <span class="math inline">\(\widehat{\theta}=(\widehat{\lambda}, \widehat{\gamma})^{\prime}\)</span>. Define <span class="math inline">\(u=\left(u_{1}, u_{2}\right)\)</span>. Write <span class="math inline">\(\sqrt{n}(\widehat{\theta}-\theta)\)</span> using a single expression as a function of the error <span class="math inline">\(u\)</span>.</p></li>
<li><p>Show that <span class="math inline">\(\mathbb{E}[Z u]=0\)</span>.</p></li>
<li><p>Derive the joint asymptotic distribution of <span class="math inline">\(\sqrt{n}(\widehat{\theta}-\theta)\)</span> as <span class="math inline">\(n \rightarrow \infty\)</span>. Hint: Define <span class="math inline">\(\Omega_{u}=\mathbb{E}\left[Z^{2} u u^{\prime}\right]\)</span>.</p></li>
<li><p>Using the previous result and the Delta Method find the asymptotic distribution of the Indirect Least Squares estimator <span class="math inline">\(\widehat{\beta}=\widehat{\lambda} / \widehat{\gamma}\)</span>.</p></li>
<li><p>Is the answer in (e) the same as the asymptotic distribution of the 2SLS estimator in Theorem 12.2? Hint: Show that <span class="math inline">\(\left(\begin{array}{ll}1 &amp; -\beta\end{array}\right) u=e\)</span> and <span class="math inline">\(\left(\begin{array}{cc}1 &amp; -\beta\end{array}\right) \Omega_{u}\left(\begin{array}{c}1 \\ -\beta\end{array}\right)=\mathbb{E}\left[Z^{2} e^{2}\right]\)</span>.</p></li>
</ol>
<p>Exercise 12.17 Take the model <span class="math inline">\(Y=X^{\prime} \beta+e\)</span> with <span class="math inline">\(\mathbb{E}[Z e]=0\)</span> and consider the two-stage least squares estimator. The first-stage estimate is least squares of <span class="math inline">\(X\)</span> on <span class="math inline">\(Z\)</span> with least squares fitted values <span class="math inline">\(\widehat{X}\)</span>. The second-stage is least squares of <span class="math inline">\(Y\)</span> on <span class="math inline">\(\widehat{X}\)</span> with coefficient estimator <span class="math inline">\(\widehat{\beta}\)</span> and least squares residuals <span class="math inline">\(\widehat{e}_{i}=\)</span> <span class="math inline">\(Y_{i}-\widehat{X}_{i} \widehat{\beta}\)</span>. Consider <span class="math inline">\(\widehat{\sigma}^{2}=\frac{1}{n} \sum_{i=1}^{n} \widehat{e}_{i}^{2}\)</span> as an estimator for <span class="math inline">\(\sigma^{2}=\mathbb{E}\left[e_{i}^{2}\right]\)</span>. Is this appropriate? If not, propose an alternative estimator.</p>
<p>Exercise 12.18 You have two independent i.i.d. samples <span class="math inline">\(\left(Y_{1 i}, X_{1 i}, Z_{1 i}: i=1, \ldots, n\right)\)</span> and <span class="math inline">\(\left(Y_{2 i}, X_{2 i}, Z_{2 i}: i=\right.\)</span> <span class="math inline">\(1, \ldots, n\)</span> ). The dependent variables <span class="math inline">\(Y_{1}\)</span> and <span class="math inline">\(Y_{2}\)</span> are real-valued. The regressors <span class="math inline">\(X_{1}\)</span> and <span class="math inline">\(X_{2}\)</span> and instruments <span class="math inline">\(Z_{1}\)</span> and <span class="math inline">\(Z_{2}\)</span> are <span class="math inline">\(k\)</span>-vectors. The model is standard just-identified linear instrumental variables</p>
<p><span class="math display">\[
\begin{aligned}
Y_{1} &amp;=X_{1}^{\prime} \beta_{1}+e_{1} \\
\mathbb{E}\left[Z_{1} e_{1}\right] &amp;=0 \\
Y_{2} &amp;=X_{2}^{\prime} \beta_{2}+e_{2} \\
\mathbb{E}\left[Z_{2} e_{2}\right] &amp;=0 .
\end{aligned}
\]</span></p>
<p>For concreteness, sample 1 are women and sample 2 are men. You want to test <span class="math inline">\(\mathbb{M}_{0}: \beta_{1}=\beta_{2}\)</span>, that the two samples have the same coefficients.</p>
<ol type="a">
<li><p>Develop a test statistic for <span class="math inline">\(\mathbb{H}_{0}\)</span>.</p></li>
<li><p>Derive the asymptotic distribution of the test statistic. (c) Describe (in brief) the testing procedure.</p></li>
</ol>
<p>Exercise 12.19 You want to use household data to estimate <span class="math inline">\(\beta\)</span> in the model <span class="math inline">\(Y=X \beta+e\)</span> with <span class="math inline">\(X\)</span> scalar and endogenous, using as an instrument the state of residence.</p>
<ol type="a">
<li><p>What are the assumptions needed to justify this choice of instrument?</p></li>
<li><p>Is the model just identified or overidentified?</p></li>
</ol>
<p>Exercise 12.20 The model is <span class="math inline">\(Y=X^{\prime} \beta+e\)</span> with <span class="math inline">\(\mathbb{E}[Z e]=0\)</span>. An economist wants to obtain the 2 SLS estimates and standard errors for <span class="math inline">\(\beta\)</span>. He uses the following steps</p>
<ul>
<li><p>Regresses <span class="math inline">\(X\)</span> on <span class="math inline">\(Z\)</span>, obtains the predicted values <span class="math inline">\(\widehat{X}\)</span>.</p></li>
<li><p>Regresses <span class="math inline">\(Y\)</span> on <span class="math inline">\(\widehat{X}\)</span>, obtains the coefficient estimate <span class="math inline">\(\widehat{\beta}\)</span> and standard error <span class="math inline">\(s(\widehat{\beta})\)</span> from this regression. Is this correct? Does this produce the 2SLS estimates and standard errors?</p></li>
</ul>
<p>Exercise 12.21 In the linear model <span class="math inline">\(Y=X \beta+e\)</span> with <span class="math inline">\(X \in \mathbb{R}\)</span> suppose <span class="math inline">\(\sigma^{2}(x)=\mathbb{E}\left[e^{2} \mid X=x\right]\)</span> is known. Show that the GLS estimator of <span class="math inline">\(\beta\)</span> can be written as an instrumental variables estimator using some instrument <span class="math inline">\(Z\)</span>. (Find an expression for <span class="math inline">\(Z\)</span>.)</p>
<p>Exercise 12.22 You will replicate and extend the work reported in Acemoglu, Johnson, and Robinson (2001). The authors provided an expanded set of controls when they published their 2012 extension and posted the data on the AER website. This dataset is A JR2001 on the textbook website.</p>
<ol type="a">
<li><p>Estimate the OLS regression (12.86), the reduced form regression (12.87), and the 2SLS regression (12.88). (Which point estimate is different by <span class="math inline">\(0.01\)</span> from the reported values? This is a common phenomenon in empirical replication).</p></li>
<li><p>For the above estimates calculate both homoskedastic and heteroskedastic-robust standard errors. Which were used by the authors (as reported in (12.86)-(12.87)-(12.88)?)</p></li>
<li><p>Calculate the 2SLS estimates by the Indirect Least Squares formula. Are they the same?</p></li>
<li><p>Calculate the 2SLS estimates by the two-stage approach. Are they the same?</p></li>
<li><p>Calculate the 2SLS estimates by the control variable approach. Are they the same?</p></li>
<li><p>Acemoglu, Johnson, and Robinson (2001) reported many specifications including alternative regressor controls, for example latitude and africa. Estimate by least squares the equation for logGDP adding latitude and africa as regressors. Does this regression suggest that latitude and africa are predictive of the level of GDP?</p></li>
<li><p>Now estimate the same equation as in (f) but by 2SLS using log(mortality) as an instrument for risk. How does the interpretation of the effect of latitude and africa change?</p></li>
<li><p>Return to our baseline model (without including latitude and africa). The authors’ reduced form equation uses <span class="math inline">\(\log\)</span> (mortality) as the instrument, rather than, say, the level of mortality. Estimate the reduced form for risk with mortality as the instrument. (This variable is not provided in the dataset so you need to take the exponential of <span class="math inline">\(\log\)</span> (mortality).) Can you explain why the authors preferred the equation with <span class="math inline">\(\log (\)</span> mortality) ? (i) Try an alternative reduced form including both <span class="math inline">\(\log\)</span> (mortality) and the square of <span class="math inline">\(\log (\)</span> mortality). Interpret the results. Re-estimate the structural equation by 2 SLS using both <span class="math inline">\(\log (\)</span> mortality) and its square as instruments. How do the results change?</p></li>
<li><p>For the estimates in (i) are the instruments strong or weak using the Stock-Yogo test?</p></li>
<li><p>Calculate and interpret a test for exogeneity of the instruments.</p></li>
<li><p>Estimate the equation by LIML using the instruments <span class="math inline">\(\log (\)</span> mortality) and the square of <span class="math inline">\(\log (\)</span> mortality).</p></li>
</ol>
<p>Exercise 12.23 In Exercise 12.22 you extended the work reported in Acemoglu, Johnson, and Robinson (2001). Consider the 2SLS regression (12.88). Compute the standard errors both by the asymptotic formula and by the bootstrap using a large number <span class="math inline">\((10,000)\)</span> of bootstrap replications. Re-calculate the bootstrap standard errors. Comment on the reliability of bootstrap standard errors for IV regression.</p>
<p>Exercise 12.24 You will replicate and extend the work reported in the chapter relating to Card (1995). The data is from the author’s website and is posted as Card1995. The model we focus on is labeled 2SLS(a) in Table <span class="math inline">\(12.1\)</span> which uses public and private as instruments for edu. The variables you will need for this exercise include lwage76, ed76, age76, smsa76r, reg76r, nearc2, nearc4, nearc4a, nearc4b. See the description file for definitions. Experience is not in the dataset, so needs to be generated as age-edu-6.</p>
<ol type="a">
<li><p>First, replicate the reduced form regression presented in the final column of Table 12.2, and the 2SLS regression described above (using public and private as instruments for <span class="math inline">\(e d u\)</span> ) to verify that you have the same variable defintions.</p></li>
<li><p>Try a different reduced form model. The variable nearc2 means “grew up near a 2-year college”. See if adding it to the reduced form equation is useful.</p></li>
<li><p>Try more interactions in the reduced form. Create the interactions nearc <span class="math inline">\(4 a^{*}\)</span> age 76 and nearc <span class="math inline">\(4 a^{*}\)</span> age <span class="math inline">\(76^{2} / 100\)</span>, and add them to the reduced form equation. Estimate this by least squares. Interpret the coefficients on the two new variables.</p></li>
<li><p>Estimate the structural equation by 2SLS using the expanded instrument set <span class="math inline">\(\left\{\right.\)</span> nearc <span class="math inline">\(4 a\)</span>, nearc <span class="math inline">\(4 b\)</span>, nearc <span class="math inline">\(4 a^{*}\)</span> age 76 , nearc <span class="math inline">\(4 a^{*}\)</span> age <span class="math inline">\(\left.76^{2} / 100\right\}\)</span>.</p></li>
</ol>
<p>What is the impact on the structural estimate of the return to schooling?</p>
<ol start="5" type="a">
<li><p>Using the Stock-Yogo test are the instruments strong or weak?</p></li>
<li><p>Test the hypothesis that <span class="math inline">\(e d u\)</span> is exogenous for the structural return to schooling.</p></li>
<li><p>Re-estimate the last equation by LIML. Do the results change meaningfully?</p></li>
</ol>
<p>Exercise 12.25 In Exercise 12.24 you extended the work reported in Card (1995). Now, estimate the IV equation corresponding to the IV(a) column of Table 12.1 which is the baseline specification considered in Card. Use the bootstrap to calculate a BC percentile confidence interval. In this example should we also report the bootstrap standard error?</p>
<p>Exercise 12.26 You will extend Angrist and Krueger (1991) using the data file AK1991 on the textbook website.. Their Table VIII reports estimates of an analog of (12.90) for the subsample of 26,913 Black men. Use this sub-sample for the following analysis. (a) Estimate an equation which is identical in form to (12.90) with the same additional regressors (year-of-birth, region-of-residence, and state-of-birth dummy variables) and 180 excluded instrumental variables (the interactions of quarter-of-birth times year-of-birth dummy variables and quarter-of-birth times state-of-birth interactions) but use the subsample of Black men. One regressor must be omitted to achieve identification. Which variable is this?</p>
<ol start="2" type="a">
<li><p>Estimate the reduced form for the above equation by least squares. Calculate the <span class="math inline">\(F\)</span> statistic for the excluded instruments. What do you conclude about the strength of the instruments?</p></li>
<li><p>Repeat, estimating the reduced form for the analog of (12.89) which has 30 excluded instrumental variables and does not include the state-of-birth dummy variables in the regression. What do you conclude about the strength of the instruments?</p></li>
<li><p>Repeat, estimating the reduced form for the analog of (12.92) which has only 3 excluded instrumental variables. Are the instruments sufficiently strong for 2SLS estimation? For LIML estimation?</p></li>
<li><p>Estimate the structural wage equation using what you believe is the most appropriate set of regressors, instruments, and the most appropriate estimation method. What is the estimated return to education (for the subsample of Black men) and its standard error? Without doing a formal hypothesis test, do these results (or in which way?) appear meaningfully different from the results for the full sample?</p></li>
</ol>
<p>Exercise 12.27 In Exercise 12.26 you extended the work reported in Angrist and Krueger (1991) by estimating wage equations for the subsample of Black men. Re-estimate equation (12.92) for this group using as instruments only the three quarter-of-birth dummy variables. Calculate the standard error for the return to education by asymptotic and bootstrap methods. Calculate a BC percentile interval. In this application of 2SLS is it appropriate to report the bootstrap standard error?</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    setTimeout(function() {
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      let href = ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const cites = ref.parentNode.getAttribute('data-cites').split(' ');
    tippyHover(ref, function() {
      var popup = window.document.createElement('div');
      cites.forEach(function(cite) {
        var citeDiv = window.document.createElement('div');
        citeDiv.classList.add('hanging-indent');
        citeDiv.classList.add('csl-entry');
        var biblioDiv = window.document.getElementById('ref-' + cite);
        if (biblioDiv) {
          citeDiv.innerHTML = biblioDiv.innerHTML;
        }
        popup.appendChild(citeDiv);
      });
      return popup.innerHTML;
    });
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./chpt11-multi-reg.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Multivariate Regression</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./chpt13-gmm.html" class="pagination-link">
        <span class="nav-page-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Generalized Method of Moments</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->



</body></html>
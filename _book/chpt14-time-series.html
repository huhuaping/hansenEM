<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.253">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Hansen中高级计量体系 - 14&nbsp; Time Series</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./chpt15-multiple-time-series.html" rel="next">
<link href="./part04-pannel.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
    <div class="container-fluid d-flex justify-content-between">
      <h1 class="quarto-secondary-nav-title"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Time Series</span></h1>
      <button type="button" class="quarto-btn-toggle btn" aria-label="Show secondary navigation">
        <i class="bi bi-chevron-right"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Hansen中高级计量体系</a> 
        <div class="sidebar-tools-main">
    <a href="https://github.com/huhuaping/hansenEM/" title="Source Code" class="sidebar-tool px-1"><i class="bi bi-github"></i></a>
</div>
    </div>
      </div>
      <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
      </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">前言</a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./participate.html" class="sidebar-item-text sidebar-link">如何参与？</a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chpt01-intro.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chpt01-intro-chn.html" class="sidebar-item-text sidebar-link">介绍</a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./part01-reg.html" class="sidebar-item-text sidebar-link">回归</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chpt02-ce.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Conditional Expectation and Projection</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chpt02-ce-chn.html" class="sidebar-item-text sidebar-link">条件预期和预测</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chpt03-algebra.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">The Algebra of Least Squares</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chpt03-algebra-chn.html" class="sidebar-item-text sidebar-link">最小二乘代数</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chpt04-lsr.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Least Squares Regression</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chpt05-normal-reg.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Normal Regression</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./part02-LSM.html" class="sidebar-item-text sidebar-link">大样本方法</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chpt06-review.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">A Review of Large Sample Asymptotics</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chpt07-asymptotic-ls.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Asymptotic Theory for Least Squares</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chpt08-restricted-est.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Restricted Estimation</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chpt09-hypothesit-test.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Hypothesis Testing</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chpt10-resample-method.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Resampling Methods</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./part03-MEQ.html" class="sidebar-item-text sidebar-link">多方程模型</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chpt11-multi-reg.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Multivariate Regression</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chpt12-iv.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Instrumental Variables</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chpt13-gmm.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Generalized Method of Moments</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./part04-pannel.html" class="sidebar-item-text sidebar-link">面板数据</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chpt14-time-series.html" class="sidebar-item-text sidebar-link active"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Time Series</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chpt15-multiple-time-series.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Multivariate Time Series</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chpt17-panel-data.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Panel Data</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chpt18-did.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Difference in Differences</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./part05-nonpar.html" class="sidebar-item-text sidebar-link">非参方法</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chpt19-nonparameter.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Nonparametric Regression</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chpt20-series-reg.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">Series Regression</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chpt21-rdd.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">20</span>&nbsp; <span class="chapter-title">Regression Discontinuity</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chpt22-m-est.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">21</span>&nbsp; <span class="chapter-title">M-Estimators</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./part06-nonlinear.html" class="sidebar-item-text sidebar-link">非线性方法</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-6" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chpt23-nonliear-ls.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">22</span>&nbsp; <span class="chapter-title">Nonlinear Least Squares</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chpt24-quantile-reg.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">23</span>&nbsp; <span class="chapter-title">Quantile Regression</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chpt26-multiple-choice.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">24</span>&nbsp; <span class="chapter-title">Multiple Choice</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chpt27-censor-selection.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">25</span>&nbsp; <span class="chapter-title">Censoring and Selection</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chpt28-model-selection.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">26</span>&nbsp; <span class="chapter-title">Model Selection, Stein Shrinkage, and Model Averaging</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chpt29-ML.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">27</span>&nbsp; <span class="chapter-title">Machine Learning</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./summary.html" class="sidebar-item-text sidebar-link">Summary</a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">References</a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" aria-expanded="true">Appendices</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-7" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./a1-notation.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">A</span>&nbsp; <span class="chapter-title">附录a1</span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
    <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">目录</h2>
   
  <ul>
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction"><span class="toc-section-number">14.1</span>  Introduction</a></li>
  <li><a href="#examples" id="toc-examples" class="nav-link" data-scroll-target="#examples"><span class="toc-section-number">14.2</span>  Examples</a></li>
  <li><a href="#differences-and-growth-rates" id="toc-differences-and-growth-rates" class="nav-link" data-scroll-target="#differences-and-growth-rates"><span class="toc-section-number">14.3</span>  Differences and Growth Rates</a></li>
  <li><a href="#stationarity" id="toc-stationarity" class="nav-link" data-scroll-target="#stationarity"><span class="toc-section-number">14.4</span>  Stationarity</a></li>
  <li><a href="#transformations-of-stationary-processes" id="toc-transformations-of-stationary-processes" class="nav-link" data-scroll-target="#transformations-of-stationary-processes"><span class="toc-section-number">14.5</span>  Transformations of Stationary Processes</a></li>
  <li><a href="#convergent-series" id="toc-convergent-series" class="nav-link" data-scroll-target="#convergent-series"><span class="toc-section-number">14.6</span>  Convergent Series</a></li>
  <li><a href="#ergodicity" id="toc-ergodicity" class="nav-link" data-scroll-target="#ergodicity"><span class="toc-section-number">14.7</span>  Ergodicity</a></li>
  <li><a href="#ergodic-theorem" id="toc-ergodic-theorem" class="nav-link" data-scroll-target="#ergodic-theorem"><span class="toc-section-number">14.8</span>  Ergodic Theorem</a></li>
  <li><a href="#conditioning-on-information-sets" id="toc-conditioning-on-information-sets" class="nav-link" data-scroll-target="#conditioning-on-information-sets"><span class="toc-section-number">14.9</span>  Conditioning on Information Sets</a></li>
  <li><a href="#martingale-difference-sequences" id="toc-martingale-difference-sequences" class="nav-link" data-scroll-target="#martingale-difference-sequences"><span class="toc-section-number">14.10</span>  Martingale Difference Sequences</a></li>
  <li><a href="#clt-for-martingale-differences" id="toc-clt-for-martingale-differences" class="nav-link" data-scroll-target="#clt-for-martingale-differences"><span class="toc-section-number">14.11</span>  CLT for Martingale Differences</a></li>
  <li><a href="#mixing" id="toc-mixing" class="nav-link" data-scroll-target="#mixing"><span class="toc-section-number">14.12</span>  Mixing</a></li>
  <li><a href="#clt-for-correlated-observations" id="toc-clt-for-correlated-observations" class="nav-link" data-scroll-target="#clt-for-correlated-observations"><span class="toc-section-number">14.13</span>  CLT for Correlated Observations</a></li>
  <li><a href="#linear-projection" id="toc-linear-projection" class="nav-link" data-scroll-target="#linear-projection"><span class="toc-section-number">14.14</span>  Linear Projection</a></li>
  <li><a href="#white-noise" id="toc-white-noise" class="nav-link" data-scroll-target="#white-noise"><span class="toc-section-number">14.15</span>  White Noise</a></li>
  <li><a href="#the-wold-decomposition" id="toc-the-wold-decomposition" class="nav-link" data-scroll-target="#the-wold-decomposition"><span class="toc-section-number">14.16</span>  The Wold Decomposition</a></li>
  <li><a href="#lag-operator" id="toc-lag-operator" class="nav-link" data-scroll-target="#lag-operator"><span class="toc-section-number">14.17</span>  Lag Operator</a></li>
  <li><a href="#autoregressive-wold-representation" id="toc-autoregressive-wold-representation" class="nav-link" data-scroll-target="#autoregressive-wold-representation"><span class="toc-section-number">14.18</span>  Autoregressive Wold Representation</a></li>
  <li><a href="#linear-models" id="toc-linear-models" class="nav-link" data-scroll-target="#linear-models"><span class="toc-section-number">14.19</span>  Linear Models</a></li>
  <li><a href="#moving-average-processes" id="toc-moving-average-processes" class="nav-link" data-scroll-target="#moving-average-processes"><span class="toc-section-number">14.20</span>  Moving Average Processes</a></li>
  <li><a href="#infinite-order-moving-average-process" id="toc-infinite-order-moving-average-process" class="nav-link" data-scroll-target="#infinite-order-moving-average-process"><span class="toc-section-number">14.21</span>  Infinite-Order Moving Average Process</a></li>
  <li><a href="#first-order-autoregressive-process" id="toc-first-order-autoregressive-process" class="nav-link" data-scroll-target="#first-order-autoregressive-process"><span class="toc-section-number">14.22</span>  First-Order Autoregressive Process</a></li>
  <li><a href="#unit-root-and-explosive-ar1-processes" id="toc-unit-root-and-explosive-ar1-processes" class="nav-link" data-scroll-target="#unit-root-and-explosive-ar1-processes"><span class="toc-section-number">14.23</span>  Unit Root and Explosive AR(1) Processes</a></li>
  <li><a href="#second-order-autoregressive-process" id="toc-second-order-autoregressive-process" class="nav-link" data-scroll-target="#second-order-autoregressive-process"><span class="toc-section-number">14.24</span>  Second-Order Autoregressive Process</a></li>
  <li><a href="#arp-processes" id="toc-arp-processes" class="nav-link" data-scroll-target="#arp-processes"><span class="toc-section-number">14.25</span>  AR(p) Processes</a></li>
  <li><a href="#impulse-response-function" id="toc-impulse-response-function" class="nav-link" data-scroll-target="#impulse-response-function"><span class="toc-section-number">14.26</span>  Impulse Response Function</a></li>
  <li><a href="#arma-and-arima-processes" id="toc-arma-and-arima-processes" class="nav-link" data-scroll-target="#arma-and-arima-processes"><span class="toc-section-number">14.27</span>  ARMA and ARIMA Processes</a></li>
  <li><a href="#mixing-properties-of-linear-processes" id="toc-mixing-properties-of-linear-processes" class="nav-link" data-scroll-target="#mixing-properties-of-linear-processes"><span class="toc-section-number">14.28</span>  Mixing Properties of Linear Processes</a></li>
  <li><a href="#identification" id="toc-identification" class="nav-link" data-scroll-target="#identification"><span class="toc-section-number">14.29</span>  Identification</a></li>
  <li><a href="#estimation-of-autoregressive-models" id="toc-estimation-of-autoregressive-models" class="nav-link" data-scroll-target="#estimation-of-autoregressive-models"><span class="toc-section-number">14.30</span>  Estimation of Autoregressive Models</a></li>
  <li><a href="#asymptotic-distribution-of-least-squares-estimator" id="toc-asymptotic-distribution-of-least-squares-estimator" class="nav-link" data-scroll-target="#asymptotic-distribution-of-least-squares-estimator"><span class="toc-section-number">14.31</span>  Asymptotic Distribution of Least Squares Estimator</a></li>
  <li><a href="#distribution-under-homoskedasticity" id="toc-distribution-under-homoskedasticity" class="nav-link" data-scroll-target="#distribution-under-homoskedasticity"><span class="toc-section-number">14.32</span>  Distribution Under Homoskedasticity</a></li>
  <li><a href="#asymptotic-distribution-under-general-dependence" id="toc-asymptotic-distribution-under-general-dependence" class="nav-link" data-scroll-target="#asymptotic-distribution-under-general-dependence"><span class="toc-section-number">14.33</span>  Asymptotic Distribution Under General Dependence</a></li>
  <li><a href="#covariance-matrix-estimation" id="toc-covariance-matrix-estimation" class="nav-link" data-scroll-target="#covariance-matrix-estimation"><span class="toc-section-number">14.34</span>  Covariance Matrix Estimation</a></li>
  <li><a href="#covariance-matrix-estimation-under-general-dependence" id="toc-covariance-matrix-estimation-under-general-dependence" class="nav-link" data-scroll-target="#covariance-matrix-estimation-under-general-dependence"><span class="toc-section-number">14.35</span>  Covariance Matrix Estimation Under General Dependence</a></li>
  <li><a href="#testing-the-hypothesis-of-no-serial-correlation" id="toc-testing-the-hypothesis-of-no-serial-correlation" class="nav-link" data-scroll-target="#testing-the-hypothesis-of-no-serial-correlation"><span class="toc-section-number">14.36</span>  Testing the Hypothesis of No Serial Correlation</a></li>
  <li><a href="#testing-for-omitted-serial-correlation" id="toc-testing-for-omitted-serial-correlation" class="nav-link" data-scroll-target="#testing-for-omitted-serial-correlation"><span class="toc-section-number">14.37</span>  Testing for Omitted Serial Correlation</a></li>
  <li><a href="#model-selection" id="toc-model-selection" class="nav-link" data-scroll-target="#model-selection"><span class="toc-section-number">14.38</span>  Model Selection</a></li>
  <li><a href="#illustrations" id="toc-illustrations" class="nav-link" data-scroll-target="#illustrations"><span class="toc-section-number">14.39</span>  Illustrations</a></li>
  <li><a href="#time-series-regression-models" id="toc-time-series-regression-models" class="nav-link" data-scroll-target="#time-series-regression-models"><span class="toc-section-number">14.40</span>  Time Series Regression Models</a></li>
  <li><a href="#static-distributed-lag-and-autoregressive-distributed-lag-models" id="toc-static-distributed-lag-and-autoregressive-distributed-lag-models" class="nav-link" data-scroll-target="#static-distributed-lag-and-autoregressive-distributed-lag-models"><span class="toc-section-number">14.41</span>  Static, Distributed Lag, and Autoregressive Distributed Lag Models</a></li>
  <li><a href="#time-trends" id="toc-time-trends" class="nav-link" data-scroll-target="#time-trends"><span class="toc-section-number">14.42</span>  Time Trends</a></li>
  <li><a href="#illustration" id="toc-illustration" class="nav-link" data-scroll-target="#illustration"><span class="toc-section-number">14.43</span>  Illustration</a></li>
  <li><a href="#granger-causality" id="toc-granger-causality" class="nav-link" data-scroll-target="#granger-causality"><span class="toc-section-number">14.44</span>  Granger Causality</a></li>
  <li><a href="#testing-for-serial-correlation-in-regression-models" id="toc-testing-for-serial-correlation-in-regression-models" class="nav-link" data-scroll-target="#testing-for-serial-correlation-in-regression-models"><span class="toc-section-number">14.45</span>  Testing for Serial Correlation in Regression Models</a></li>
  <li><a href="#bootstrap-for-time-series" id="toc-bootstrap-for-time-series" class="nav-link" data-scroll-target="#bootstrap-for-time-series"><span class="toc-section-number">14.46</span>  Bootstrap for Time Series</a></li>
  <li><a href="#recursive-bootstrap" id="toc-recursive-bootstrap" class="nav-link" data-scroll-target="#recursive-bootstrap"><span class="toc-section-number">14.47</span>  Recursive Bootstrap</a></li>
  <li><a href="#pairwise-bootstrap" id="toc-pairwise-bootstrap" class="nav-link" data-scroll-target="#pairwise-bootstrap"><span class="toc-section-number">14.48</span>  Pairwise Bootstrap</a></li>
  <li><a href="#fixed-design-residual-bootstrap" id="toc-fixed-design-residual-bootstrap" class="nav-link" data-scroll-target="#fixed-design-residual-bootstrap"><span class="toc-section-number">14.49</span>  Fixed Design Residual Bootstrap</a></li>
  <li><a href="#fixed-design-wild-bootstrap" id="toc-fixed-design-wild-bootstrap" class="nav-link" data-scroll-target="#fixed-design-wild-bootstrap"><span class="toc-section-number">14.50</span>  Fixed Design Wild Bootstrap</a></li>
  <li><a href="#block-bootstrap" id="toc-block-bootstrap" class="nav-link" data-scroll-target="#block-bootstrap"><span class="toc-section-number">14.51</span>  Block Bootstrap</a></li>
  <li><a href="#technical-proofs" id="toc-technical-proofs" class="nav-link" data-scroll-target="#technical-proofs"><span class="toc-section-number">14.52</span>  Technical Proofs*</a></li>
  <li><a href="#exercises" id="toc-exercises" class="nav-link" data-scroll-target="#exercises"><span class="toc-section-number">14.53</span>  Exercises</a></li>
  </ul>
<div class="toc-actions"><div><i class="bi bi-github"></i></div><div class="action-links"><p><a href="https://github.com/huhuaping/hansenEM/edit/master/chpt14-time-series.qmd" class="toc-action">Edit this page</a></p></div></div></nav>
</nav>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title d-none d-lg-block"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Time Series</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<section id="introduction" class="level2" data-number="14.1">
<h2 data-number="14.1" class="anchored" data-anchor-id="introduction"><span class="header-section-number">14.1</span> Introduction</h2>
<p>A time series <span class="math inline">\(Y_{t} \in \mathbb{R}^{m}\)</span> is a process which is sequentially ordered over time. In this textbook we focus on discrete time series where <span class="math inline">\(t\)</span> is an integer, though there is also a considerable literature on continuoustime processes. To denote the time period it is typical to use the subscript <span class="math inline">\(t\)</span>. The time series is univariate if <span class="math inline">\(m=1\)</span> and multivariate if <span class="math inline">\(m&gt;1\)</span>. This chapter is primarily focused on univariate time series models, though we describe the concepts for the multivariate case when the added generality does not add extra complication.</p>
<p>Most economic time series are recorded at discrete intervals such as annual, quarterly, monthly, weekly, or daily. The number of observaed periods <span class="math inline">\(s\)</span> per year is called the frequency. In most cases we will denote the observed sample by the periods <span class="math inline">\(t=1, \ldots, n\)</span>.</p>
<p>Because of the sequential nature of time series we expect that observations close in calender time, e.g.&nbsp;<span class="math inline">\(Y_{t}\)</span> and its lagged value <span class="math inline">\(Y_{t-1}\)</span>, will be dependent. This type of dependence structure requires a different distributional theory than for cross-sectional and clustered observations since we cannot divide the sample into independent groups. Many of the issues which distinguish time series from cross-section econometrics concern the modeling of these dependence relationships.</p>
<p>There are many excellent textbooks for time series analysis. The encyclopedic standard is Hamilton (1994). Others include Harvey (1990), Tong (1990), Brockwell and Davis (1991), Fan and Yao (2003), Lütkepohl (2005), Enders (2014), and Kilian and Lütkepohl (2017). For textbooks on the related subject of forecasting see Granger and Newbold (1986), Granger (1989), and Elliott and Timmermann (2016).</p>
</section>
<section id="examples" class="level2" data-number="14.2">
<h2 data-number="14.2" class="anchored" data-anchor-id="examples"><span class="header-section-number">14.2</span> Examples</h2>
<p>Many economic time series are macroeconomic variables. An excellent resource for U.S. macroeconomic data are the FRED-MD and FRED-QD databases which contain a wide set of monthly and quarterly variables, assembled and maintained by the St.&nbsp;Louis Federal Reserve Bank. See McCracken and Ng (2016, 2021). The datasets FRED-MD and FRED-QD for 1959-2017 are posted on the textbook website. FRED-MD has 129 variables over 708 months. FRED-QD has 248 variables over 236 quarters.</p>
<p>When working with time series data one of the first tasks is to plot the series against time. In Figures 14.1-14.2 we plot eight example time series from FRED-QD and FRED-MD. As is conventional, the x-axis displays calendar dates (in this case years) and the y-axis displays the level of the series. The series plotted are: (1a) Real U.S. GDP ( <span class="math inline">\(g d p c 1)\)</span>; (1b) U.S.-Canada exchange rate (excausx); (1c) Interest rate on U.S. 10-year Treasury bond (gs10); (1d) Real crude oil price (oilpricex); (2a) U.S. unemployment rate (unrate); (2b) U.S. real non-durables consumption growth rate (growth rate of <span class="math inline">\(p c n d x\)</span> ); (2c) U.S. CPI inflation rate</p>
<p><img src="images//2022_10_23_6047885e7d154c9f28afg-02.jpg" class="img-fluid"></p>
<ol type="a">
<li>U.S. Real GDP</li>
</ol>
<p><img src="images//2022_10_23_6047885e7d154c9f28afg-02(1).jpg" class="img-fluid"></p>
<ol start="3" type="a">
<li>Interest Rate on 10-Year Treasury</li>
</ol>
<p><img src="images//2022_10_23_6047885e7d154c9f28afg-02(2).jpg" class="img-fluid"></p>
<ol start="2" type="a">
<li>U.S.-Canada Exchange Rate</li>
</ol>
<p><img src="images//2022_10_23_6047885e7d154c9f28afg-02(3).jpg" class="img-fluid"></p>
<ol start="4" type="a">
<li>Real Crude Oil Price</li>
</ol>
<p>Figure 14.1: GDP, Exchange Rate, Interest Rate, Oil Price</p>
<p>(growth rate of cpiaucsl); (2d) S&amp;P 500 return (growth rate of <span class="math inline">\(s p 500\)</span> ). (1a) and (2b) are quarterly series, the rest are monthly.</p>
<p>Many of the plots are smooth, meaning that the neighboring values (in calendar time) are similar to one another and hence are serially correlated. Some of the plots are non-smooth, meaning that the neighboring values are less similar and hence less correlated. At least one plot (real GDP) displays an upward trend.</p>
<p><img src="images//2022_10_23_6047885e7d154c9f28afg-03.jpg" class="img-fluid"></p>
<ol type="a">
<li>U.S. Unemployment Rate</li>
</ol>
<p><img src="images//2022_10_23_6047885e7d154c9f28afg-03(1).jpg" class="img-fluid"></p>
<ol start="3" type="a">
<li>U.S. Inflation Rate</li>
</ol>
<p><img src="images//2022_10_23_6047885e7d154c9f28afg-03(2).jpg" class="img-fluid"></p>
<ol start="2" type="a">
<li>Consumption Growth Rate</li>
</ol>
<p><img src="images//2022_10_23_6047885e7d154c9f28afg-03(3).jpg" class="img-fluid"></p>
<ol start="4" type="a">
<li>S&amp;P 500 Return</li>
</ol>
<p>Figure 14.2: Unemployment Rate, Consumption Growth Rate, Inflation Rate, and S&amp;P 500 Return</p>
</section>
<section id="differences-and-growth-rates" class="level2" data-number="14.3">
<h2 data-number="14.3" class="anchored" data-anchor-id="differences-and-growth-rates"><span class="header-section-number">14.3</span> Differences and Growth Rates</h2>
<p>It is common to transform series by taking logarithms, differences, and/or growth rates. Three of the series in Figure <span class="math inline">\(14.2\)</span> (consumption growth, inflation [growth rate of CPI index], and S&amp;P 500 return) are displayed as growth rates. This may be done for a number of reasons. The most credible is that this is the suitable transformation for the desired analysis.</p>
<p>Many aggregate series such as real GDP are transformed by taking natural logarithms. This flattens the apparent exponential growth and makes fluctuations proportionate.</p>
<p>The first difference of a series <span class="math inline">\(Y_{t}\)</span> is</p>
<p><span class="math display">\[
\Delta Y_{t}=Y_{t}-Y_{t-1}
\]</span></p>
<p>The second difference is</p>
<p><span class="math display">\[
\Delta^{2} Y_{t}=\Delta Y_{t}-\Delta Y_{t-1} .
\]</span></p>
<p>Higher-order differences can be defined similarly but are not used in practice. The annual, or year-onyear, change of a series <span class="math inline">\(Y_{t}\)</span> with frequency <span class="math inline">\(s\)</span> is</p>
<p><span class="math display">\[
\Delta_{s} Y_{t}=Y_{t}-Y_{t-s} .
\]</span></p>
<p>There are several methods to calculate growth rates. The one-period growth rate is the percentage change from period <span class="math inline">\(t-1\)</span> to period <span class="math inline">\(t\)</span> :</p>
<p><span class="math display">\[
Q_{t}=100\left(\frac{\Delta Y_{t}}{Y_{t-1}}\right)=100\left(\frac{Y_{t}}{Y_{t-1}}-1\right) .
\]</span></p>
<p>The multiplication by 100 is not essential but scales <span class="math inline">\(Q_{t}\)</span> so that it is a percentage. This is the transformation used for the plots in Figures <span class="math inline">\(14.2\)</span> (b)-(d). For quarterly data, <span class="math inline">\(Q_{t}\)</span> is the quarterly growth rate. For monthly data, <span class="math inline">\(Q_{t}\)</span> is the monthly growth rate.</p>
<p>For non-annual data the one-period growth rate (14.1) may be unappealing for interpretation. Consequently, statistical agencies commonly report “annualized” growth rates which is the annual growth which would occur if the one-period growth rate is compounded for a full year. For a series with frequency <span class="math inline">\(s\)</span> the annualized growth rate is</p>
<p><span class="math display">\[
A_{t}=100\left(\left(\frac{Y_{t}}{Y_{t-1}}\right)^{s}-1\right) .
\]</span></p>
<p>Notice that <span class="math inline">\(A_{t}\)</span> is a nonlinear function of <span class="math inline">\(Q_{t}\)</span>.</p>
<p>Year-on-year growth rates are</p>
<p><span class="math display">\[
G_{t}=100\left(\frac{\Delta_{s} Y_{t}}{Y_{t-s}}\right)=100\left(\frac{Y_{t}}{Y_{t-s}}-1\right) .
\]</span></p>
<p>These do not need annualization.</p>
<p>Growth rates are closely related to logarithmic transformations. For small growth rates, <span class="math inline">\(Q_{t}, A_{t}\)</span> and <span class="math inline">\(G_{t}\)</span> are approximately first differences in logarithms:</p>
<p><span class="math display">\[
\begin{aligned}
Q_{t} &amp; \simeq 100 \Delta \log Y_{t} \\
A_{t} &amp; \simeq s \times 100 \Delta \log Y_{t} \\
G_{t} &amp; \simeq 100 \Delta_{s} \log Y_{t} .
\end{aligned}
\]</span></p>
<p>For analysis using growth rates I recommend the one-period growth rates (14.1) or differenced logarithms rather than the annualized growth rates (14.2). While annualized growth rates are preferred for reporting, they are a highly nonlinear transformation which is unnatural for statistical analysis. Differenced logarithms are the most common choice and are recommended for models which combine log-levels and growth rates for then the models are linear in all variables.</p>
</section>
<section id="stationarity" class="level2" data-number="14.4">
<h2 data-number="14.4" class="anchored" data-anchor-id="stationarity"><span class="header-section-number">14.4</span> Stationarity</h2>
<p>Recall that cross-sectional observations are conventionally treated as random draws from an underlying population. This is not an appropriate model for time series processes due to serial dependence. Instead, we treat the observed sample <span class="math inline">\(\left\{Y_{1}, \ldots, Y_{n}\right\}\)</span> as a realization of a dependent stochastic process. It is often useful to view <span class="math inline">\(\left\{Y_{1}, \ldots, Y_{n}\right\}\)</span> as a subset of an underlying doubly-infinite sequence <span class="math inline">\(\left\{\ldots, Y_{t-1}, Y_{t}, Y_{t+1}, \ldots\right\}\)</span>.</p>
<p>A random vector <span class="math inline">\(Y_{t}\)</span> can be characterized by its distribution. A set such as <span class="math inline">\(\left(Y_{t}, Y_{t+1}, \ldots, Y_{t+\ell}\right)\)</span> can be characterized by its joint distribution. Important features of these distributions are their means, variances, and covariances. Since there is only one observed time series sample, in order to learn about these distributions there needs to be some sort of constancy. This may only hold after a suitable transformation such as growth rates (as discussed in the previous section).</p>
<p>The most commonly assumed form of constancy is stationarity. There are two definitions. The first is sufficient for construction of linear models.</p>
<p>Definition <span class="math inline">\(14.1\left\{Y_{t}\right\}\)</span> is covariance or weakly stationary if the expectation <span class="math inline">\(\mu=\)</span> <span class="math inline">\(\mathbb{E}\left[Y_{t}\right]\)</span> and covariance matrix <span class="math inline">\(\Sigma=\operatorname{var}\left[Y_{t}\right]=\mathbb{E}\left[\left(Y_{t}-\mu\right)\left(Y_{t}-\mu\right)^{\prime}\right]\)</span> are finite and are independent of <span class="math inline">\(t\)</span>, and the autocovariances</p>
<p><span class="math display">\[
\Gamma(k)=\operatorname{cov}\left(Y_{t}, Y_{t-k}\right)=\mathbb{E}\left[\left(Y_{t}-\mu\right)\left(Y_{t-k}-\mu\right)^{\prime}\right]
\]</span></p>
<p>are independent of <span class="math inline">\(t\)</span> for all <span class="math inline">\(k\)</span></p>
<p>In the univariate case we typically write the variance as <span class="math inline">\(\sigma^{2}\)</span> and autocovariances as <span class="math inline">\(\gamma(k)\)</span>.</p>
<p>The expectation <span class="math inline">\(\mu\)</span> and variance <span class="math inline">\(\Sigma\)</span> are features of the marginal distribution of <span class="math inline">\(Y_{t}\)</span> (the distribution of <span class="math inline">\(Y_{t}\)</span> at a specific time period <span class="math inline">\(t\)</span> ). Their constancy as stated in the above definition means that these features of the distribution are stable over time.</p>
<p>The autocovariances <span class="math inline">\(\Gamma(k)\)</span> are features of the bivariate distributions of <span class="math inline">\(\left(Y_{t}, Y_{t-k}\right)\)</span>. Their constancy as stated in the definition means that the correlation patterns between adjacent <span class="math inline">\(Y_{t}\)</span> are stable over time and only depend on the number of time periods <span class="math inline">\(k\)</span> separating the variables. By symmetry we have <span class="math inline">\(\Gamma(-k)=\)</span> <span class="math inline">\(\Gamma(k)^{\prime}\)</span>. In the univariate case this simplifies to <span class="math inline">\(\gamma(-k)=\gamma(k)\)</span>. The autocovariances <span class="math inline">\(\Gamma(k)\)</span> are finite under the assumption that the covariance matrix <span class="math inline">\(\Sigma\)</span> is finite by the Cauchy-Schwarz inequality.</p>
<p>The autocovariances summarize the linear dependence between <span class="math inline">\(Y_{t}\)</span> and its lags. A scale-free measure of linear dependence in the univariate case are the autocorrelations</p>
<p><span class="math display">\[
\rho(k)=\operatorname{corr}\left(Y_{t}, Y_{t-k}\right)=\frac{\operatorname{cov}\left(Y_{t}, Y_{t-k}\right)}{\sqrt{\operatorname{var}\left[Y_{t}\right] \operatorname{var}\left[Y_{t-1}\right]}}=\frac{\gamma(k)}{\sigma^{2}}=\frac{\gamma(k)}{\gamma(0)} .
\]</span></p>
<p>Notice by symmetry that <span class="math inline">\(\rho(-k)=\rho(k)\)</span>.</p>
<p>The second definition of stationarity concerns the entire joint distribution.</p>
<p>Definition 14.2 <span class="math inline">\(\left\{Y_{t}\right\}\)</span> is strictly stationary if the joint distribution of <span class="math inline">\(\left(Y_{t}, \ldots, Y_{t+\ell}\right)\)</span> is independent of <span class="math inline">\(t\)</span> for all <span class="math inline">\(\ell\)</span>. This is the natural generalization of the cross-section definition of identical distributions. Strict stationarity implies that the (marginal) distribution of <span class="math inline">\(Y_{t}\)</span> does not vary over time. It also implies that the bivariate distributions of <span class="math inline">\(\left(Y_{t}, Y_{t+1}\right)\)</span> and multivariate distributions of <span class="math inline">\(\left(Y_{t}, \ldots, Y_{t+\ell}\right)\)</span> are stable over time. Under the assumption of a bounded variance a strictly stationary process is covariance stationary <span class="math inline">\({ }^{1}\)</span>.</p>
<p>For formal statistical theory we generally require the stronger assumption of strict stationarity. Therefore if we label a process as “stationary” you should interpret it as meaning “strictly stationary”.</p>
<p>The core meaning of both weak and strict stationarity is the same - that the distribution of <span class="math inline">\(Y_{t}\)</span> is stable over time. To understand the concept it may be useful to review the plots in Figures 14.1-14.2. Are these stationary processes? If so, we would expect that the expectation and variance to be stable over time. This seems unlikely to apply to the series in Figure 14.1, as in each case it is difficult to describe what is the “typical” value of the series. Stationarity may be appropriate for the series in Figure <span class="math inline">\(14.2\)</span> as each oscillates with a fairly regular pattern. It is difficult, however, to know whether or not a given time series is stationary simply by examining a time series plot.</p>
<p>A straightforward but essential relationship is that an i.i.d. process is strictly stationary.</p>
<p>Theorem 14.1 If <span class="math inline">\(Y_{t}\)</span> is i.i.d., then it strictly stationary.</p>
<p>Here are some examples of strictly stationary scalar processes. In each, <span class="math inline">\(e_{t}\)</span> is i.i.d. and <span class="math inline">\(\mathbb{E}\left[e_{t}\right]=0\)</span>.</p>
<p>Example 14.1 <span class="math inline">\(Y_{t}=e_{t}+\theta e_{t-1}\)</span>.</p>
<p>Example 14.2 <span class="math inline">\(Y_{t}=Z\)</span> for some random variable <span class="math inline">\(Z\)</span>.</p>
<p>Example 14.3 <span class="math inline">\(Y_{t}=(-1)^{t} Z\)</span> for a random variable <span class="math inline">\(Z\)</span> which is symmetrically distributed about 0 .</p>
<p>Here are some examples of processes which are not stationary.</p>
<p>Example 14.4 <span class="math inline">\(Y_{t}=t\)</span>.</p>
<p>Example 14.5 <span class="math inline">\(Y_{t}=(-1)^{t}\)</span>.</p>
<p>Example 14.6 <span class="math inline">\(Y_{t}=\cos (\theta t)\)</span>.</p>
<p>Example 14.7 <span class="math inline">\(Y_{t}=\sqrt{t} e_{t}\)</span>.</p>
<p>Example 14.8 <span class="math inline">\(Y_{t}=e_{t}+t^{-1 / 2} e_{t-1}\)</span>.</p>
<p>Example 14.9 <span class="math inline">\(Y_{t}=Y_{t-1}+e_{t}\)</span> with <span class="math inline">\(Y_{0}=0\)</span>.</p>
<p>From the examples we can see that stationarity means that the distribution is constant over time. It does not mean, however, that the process has some sort of limited dependence, nor that there is an absence of periodic patterns. These restrictions are associated with the concepts of ergodicity and mixing which we shall introduce in subsequent sections.</p>
<p><span class="math inline">\({ }^{1}\)</span> More generally, the two classes are non-nested since strictly stationary infinite variance processes are not covariance stationary.</p>
</section>
<section id="transformations-of-stationary-processes" class="level2" data-number="14.5">
<h2 data-number="14.5" class="anchored" data-anchor-id="transformations-of-stationary-processes"><span class="header-section-number">14.5</span> Transformations of Stationary Processes</h2>
<p>One of the important properties of strict stationarity is that it is preserved by transformation. That is, transformations of strictly stationary processes are also strictly stationary. This includes transformations which include the full history of <span class="math inline">\(Y_{t}\)</span>.</p>
<p>Theorem 14.2 If <span class="math inline">\(Y_{t}\)</span> is strictly stationary and <span class="math inline">\(X_{t}=\phi\left(Y_{t}, Y_{t-1}, Y_{t-2}, \ldots\right) \in \mathbb{R}^{q}\)</span> is a random vector then <span class="math inline">\(X_{t}\)</span> is strictly stationary.</p>
<p>Theorem <span class="math inline">\(14.2\)</span> is extremely useful both for the study of stochastic processes which are constructed from underlying errors and for the study of sample statistics such as linear regression estimators which are functions of sample averages of squares and cross-products of the original data.</p>
<p>We give the proof of Theorem <span class="math inline">\(14.2\)</span> in Section 14.47.</p>
</section>
<section id="convergent-series" class="level2" data-number="14.6">
<h2 data-number="14.6" class="anchored" data-anchor-id="convergent-series"><span class="header-section-number">14.6</span> Convergent Series</h2>
<p>A transformation which includes the full past history is an infinite-order moving average. For scalar <span class="math inline">\(Y\)</span> and coefficients <span class="math inline">\(a_{j}\)</span> define the vector process</p>
<p><span class="math display">\[
X_{t}=\sum_{j=0}^{\infty} a_{j} Y_{t-j} .
\]</span></p>
<p>Many time-series models involve representations and transformations of the form (14.3).</p>
<p>The infinite series (14.3) exists if it is convergent, meaning that the sequence <span class="math inline">\(\sum_{j=0}^{N} a_{j} Y_{t-j}\)</span> has a finite limit as <span class="math inline">\(N \rightarrow \infty\)</span>. Since the inputs <span class="math inline">\(Y_{t}\)</span> are random we define this as a probability limit.</p>
<p>Definition 14.3 The infinite series (14.3) converges almost surely if <span class="math inline">\(\sum_{j=0}^{N} a_{j} Y_{t-j}\)</span> has a finite limit as <span class="math inline">\(N \rightarrow \infty\)</span> with probability one. In this case we describe <span class="math inline">\(X_{t}\)</span> as convergent.</p>
<p>Theorem 14.3 If <span class="math inline">\(Y_{t}\)</span> is strictly stationary, <span class="math inline">\(\mathbb{E}|Y|&lt;\infty\)</span>, and <span class="math inline">\(\sum_{j=0}^{\infty}\left|a_{j}\right|&lt;\infty\)</span>, then (14.3) converges almost surely. Furthermore, <span class="math inline">\(X_{t}\)</span> is strictly stationary.</p>
<p>The proof of Theorem <span class="math inline">\(14.3\)</span> is provided in Section <span class="math inline">\(14.47\)</span>.</p>
</section>
<section id="ergodicity" class="level2" data-number="14.7">
<h2 data-number="14.7" class="anchored" data-anchor-id="ergodicity"><span class="header-section-number">14.7</span> Ergodicity</h2>
<p>Stationarity alone is not sufficient for the weak law of large numbers as there are strictly stationary processes with no time series variation. As we described earlier, an example of a stationary process is <span class="math inline">\(Y_{t}=Z\)</span> for some random variable <span class="math inline">\(Z\)</span>. This is random but constant over all time. An implication is that the sample mean of <span class="math inline">\(Y_{t}=Z\)</span> will be inconsistent for the population expectation.</p>
<p>What is a minimal assumption beyond stationarity so that the law of large numbers applies? This topic is called ergodicity. It is sufficiently important that it is treated as a separate area of study. We mention only a few highlights here. For a rigorous treatment see a standard textbook such as Walters (1982).</p>
<p>A time series <span class="math inline">\(Y_{t}\)</span> is ergodic if all invariant events are trivial, meaning that any event which is unaffected by time-shifts has probability either zero or one. This definition is rather abstract and difficult to grasp but fortunately it is not needed by most economists.</p>
<p>A useful intuition is that if <span class="math inline">\(Y_{t}\)</span> is ergodic then its sample paths will pass through all parts of the sample space never getting “stuck” in a subregion.</p>
<p>We will first describe the properties of ergodic series which are relevant for our needs and follow with the more rigorous technical definitions. For proofs of the results see Section 14.47.</p>
<p>First, many standard time series processes can be shown to be ergodic. A useful starting point is the observation that an i.i.d. sequence is ergodic.</p>
<p>Theorem 14.4 If <span class="math inline">\(Y_{t} \in \mathbb{R}^{m}\)</span> is i.i.d. then it strictly stationary and ergodic.</p>
<p>Second, ergodicity, like stationarity, is preserved by transformation.</p>
<p>Theorem 14.5 If <span class="math inline">\(Y_{t} \in \mathbb{R}^{m}\)</span> is strictly stationary and ergodic and <span class="math inline">\(X_{t}=\)</span> <span class="math inline">\(\phi\left(Y_{t}, Y_{t-1}, Y_{t-2}, \ldots\right)\)</span> is a random vector, then <span class="math inline">\(X_{t}\)</span> is strictly stationary and ergodic.</p>
<p>As an example, the infinite-order moving average transformation (14.3) is ergodic if the input is ergodic and the coefficients are absolutely convergent.</p>
<p>Theorem 14.6 If <span class="math inline">\(Y_{t}\)</span> is strictly stationary, ergodic, <span class="math inline">\(\mathbb{E}|Y|&lt;\infty\)</span>, and <span class="math inline">\(\sum_{j=0}^{\infty}\left|a_{j}\right|&lt;\infty\)</span> then <span class="math inline">\(X_{t}=\sum_{j=0}^{\infty} a_{j} Y_{t-j}\)</span> is strictly stationary and ergodic.</p>
<p>We now present a useful property. It is that the Cesàro sum of the autocovariances limits to zero.</p>
<p>Theorem 14.7 If <span class="math inline">\(Y_{t} \in \mathbb{R}\)</span> is strictly stationary, ergodic, and <span class="math inline">\(\mathbb{E}\left[Y^{2}\right]&lt;\infty\)</span>, then</p>
<p><span class="math display">\[
\lim _{n \rightarrow \infty} \frac{1}{n} \sum_{\ell=1}^{n} \operatorname{cov}\left(Y_{t}, Y_{t+\ell}\right)=0 .
\]</span></p>
<p>The result (14.4) can be interpreted as that the autocovariances “on average” tend to zero. Some authors have mis-stated ergodicity as implying that the covariances tend to zero but this is not correct, as (14.4) allows, for example, the non-convergent sequence <span class="math inline">\(\operatorname{cov}\left(Y_{t}, Y_{t+\ell}\right)=(-1)^{\ell}\)</span>. The reason why (14.4) is particularly useful is because it is sufficient for the WLLN as we discover later in Theorem 14.9.</p>
<p>We now give the formal definition of ergodicity for interested readers. As the concepts will not be used again most readers can safely skip this discussion.</p>
<p>As we stated above, by definition the series <span class="math inline">\(Y_{t} \in \mathbb{R}^{m}\)</span> is ergodic if all invariant events are trivial. To understand this we introduce some technical definitions. First, we can write an event as <span class="math inline">\(A=\left\{\widetilde{Y}_{t} \in G\right\}\)</span> where <span class="math inline">\(\widetilde{Y}_{t}=\left(\ldots, Y_{t-1}, Y_{t}, Y_{t+1}, \ldots\right)\)</span> is an infinite history and <span class="math inline">\(G \subset \mathbb{R}^{m \infty}\)</span>. Second, the <span class="math inline">\(\ell^{t h}\)</span> time-shift of <span class="math inline">\(\widetilde{Y}_{t}\)</span> is defined as <span class="math inline">\(\widetilde{Y}_{t+\ell}=\left(\ldots, Y_{t-1+\ell}, Y_{t+\ell}, Y_{t+1+\ell}, \ldots\right)\)</span>. Thus <span class="math inline">\(\widetilde{Y}_{t+\ell}\)</span> replaces each observation in <span class="math inline">\(\widetilde{Y}_{t}\)</span> by its <span class="math inline">\(\ell^{t h}\)</span> shifted value <span class="math inline">\(Y_{t+\ell}\)</span>. A time-shift of the event <span class="math inline">\(A=\left\{\widetilde{Y}_{t} \in G\right\}\)</span> is <span class="math inline">\(A_{\ell}=\left\{\widetilde{Y}_{t+\ell} \in G\right\}\)</span>. Third, an event <span class="math inline">\(A\)</span> is called invariant if it is unaffected by a time-shift, so that <span class="math inline">\(A_{\ell}=A\)</span>. Thus replacing any history <span class="math inline">\(\widetilde{Y}_{t}\)</span> with its shifted history <span class="math inline">\(\widetilde{Y}_{t+\ell}\)</span> doesn’t change the event. Invariant events are rather special. An example of an invariant event is <span class="math inline">\(A=\left\{\max _{-\infty&lt;t&lt;\infty} Y_{t} \leq 0\right\}\)</span>. Fourth, an event <span class="math inline">\(A\)</span> is called trivial if either <span class="math inline">\(\mathbb{P}[A]=0\)</span> or <span class="math inline">\(\mathbb{P}[A]=1\)</span>. You can think of trivial events as essentially non-random. Recall, by definition <span class="math inline">\(Y_{t}\)</span> is ergodic if all invariant events are trivial. This means that any event which is unaffected by a time shift is trivial-is essentially non-random. For example, again consider the invariant event <span class="math inline">\(A=\left\{\max _{-\infty&lt;t&lt;\infty} Y_{t} \leq 0\right\}\)</span>. If <span class="math inline">\(Y_{t}=Z \sim \mathrm{N}(0,1)\)</span> for all <span class="math inline">\(t\)</span> then <span class="math inline">\(\mathbb{P}[A]=\mathbb{P}[Z \leq 0]=0.5\)</span>. Since this does not equal 0 or 1 then <span class="math inline">\(Y_{t}=Z\)</span> is not ergodic. However, if <span class="math inline">\(Y_{t}\)</span> is i.i.d. <span class="math inline">\(\mathrm{N}(0,1)\)</span> then <span class="math inline">\(\mathbb{P}\left[\max _{-\infty&lt;t&lt;\infty} Y_{t} \leq 0\right]=0\)</span>. This is a trivial event. For <span class="math inline">\(Y_{t}\)</span> to be ergodic (it is in this case) all such invariant events must be trivial.</p>
<p>An important technical result is that ergodicity is equivalent to the following property.</p>
<p>Theorem 14.8 A stationary series <span class="math inline">\(Y_{t} \in \mathbb{R}^{m}\)</span> is ergodic iff for all events <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span></p>
<p><span class="math display">\[
\lim _{n \rightarrow \infty} \frac{1}{n} \sum_{\ell=1}^{n} \mathbb{P}\left[A_{\ell} \cap B\right]=\mathbb{P}[A] \mathbb{P}[B] .
\]</span></p>
<p>This result is rather deep so we do not prove it here. See Walters (1982), Corollary 1.14.2, or Davidson (1994), Theorem 14.7. The limit in (14.5) is the Cesàro sum of <span class="math inline">\(\mathbb{P}\left[A_{\ell} \cap B\right]\)</span>. The Theorem of Cesàro Means (Theorem A.4 of Probability and Statistics for Economists) shows that a sufficient condition for (14.5) is that <span class="math inline">\(\mathbb{P}\left[A_{\ell} \cap B\right] \rightarrow \mathbb{P}[A] \mathbb{P}[B]\)</span> which is known as mixing. Thus mixing implies ergodicity. Mixing, roughly, means that separated events are asymptotically independent. Ergodicity is weaker, only requiring that the events are asymptotically independent “on average”. We discuss mixing in Section 14.12.</p>
</section>
<section id="ergodic-theorem" class="level2" data-number="14.8">
<h2 data-number="14.8" class="anchored" data-anchor-id="ergodic-theorem"><span class="header-section-number">14.8</span> Ergodic Theorem</h2>
<p>The ergodic theorem is one of the most famous results in time series theory. There are actually several forms of the theorem, most of which concern almost sure convergence. For simplicity we state the theorem in terms of convergence in probability. Theorem 14.9 Ergodic Theorem.</p>
<p>If <span class="math inline">\(Y_{t} \in \mathbb{R}^{m}\)</span> is strictly stationary, ergodic, and <span class="math inline">\(\mathbb{E}\|Y\|&lt;\infty\)</span>, then as <span class="math inline">\(n \rightarrow \infty\)</span>,</p>
<p><span class="math display">\[
\mathbb{E}\|\bar{Y}-\mu\| \longrightarrow 0
\]</span></p>
<p>and</p>
<p><span class="math display">\[
\bar{Y} \underset{p}{\longrightarrow} \mu
\]</span></p>
<p>where <span class="math inline">\(\mu=\mathbb{E}[Y]\)</span>.</p>
<p>The ergodic theorem shows that ergodicity is sufficient for consistent estimation. The moment condition <span class="math inline">\(\mathbb{E}\|Y\|&lt;\infty\)</span> is the same as in the WLLN for i.i.d. observations.</p>
<p>We now provide a proof of the ergodic theorem for the scalar case under the additional assumption that <span class="math inline">\(\operatorname{var}[Y]=\sigma^{2}&lt;\infty\)</span>. A proof which relaxes this assumption is provided in Section 14.47.</p>
<p>By direct calculation</p>
<p><span class="math display">\[
\operatorname{var}[\bar{Y}]=\frac{1}{n^{2}} \sum_{t=1}^{n} \sum_{j=1}^{n} \gamma(t-j)
\]</span></p>
<p>where <span class="math inline">\(\gamma(\ell)=\operatorname{cov}\left(Y_{t}, Y_{t+\ell}\right)\)</span>. The double sum is over all elements of an <span class="math inline">\(n \times n\)</span> matrix whose <span class="math inline">\(t j^{t h}\)</span> element is <span class="math inline">\(\gamma(t-j)\)</span>. The diagonal elements are <span class="math inline">\(\gamma(0)=\sigma^{2}\)</span>, the first off-diagonal elements are <span class="math inline">\(\gamma(1)\)</span>, the second offdiagonal elements are <span class="math inline">\(\gamma(2)\)</span> and so on. This means that there are precisely <span class="math inline">\(n\)</span> diagonal elements equalling <span class="math inline">\(\sigma^{2}, 2(n-1)\)</span> equalling <span class="math inline">\(\gamma(1)\)</span>, etc. Thus the above equals</p>
<p><span class="math display">\[
\begin{aligned}
\operatorname{var}[\bar{Y}] &amp;=\frac{1}{n^{2}}\left(n \sigma^{2}+2(n-1) \gamma(1)+2(n-2) \gamma(2)+\cdots+2 \gamma(n-1)\right) \\
&amp;=\frac{\sigma^{2}}{n}+\frac{2}{n} \sum_{\ell=1}^{n}\left(1-\frac{\ell}{n}\right) \gamma(\ell) .
\end{aligned}
\]</span></p>
<p>This is a rather intruiging expression. It shows that the variance of the sample mean precisely equals <span class="math inline">\(\sigma^{2} / n\)</span> (which is the variance of the sample mean under i.i.d. sampling) plus a weighted Cesàro mean of the autocovariances. The latter is zero under i.i.d. sampling but is non-zero otherwise. Theorem <span class="math inline">\(14.7\)</span> shows that the Cesàro mean of the autocovariances converges to zero. Let <span class="math inline">\(w_{n \ell}=2\left(\ell / n^{2}\right)\)</span>, which satisfy the conditions of the Toeplitz Lemma (Theorem A.5 of Probability and Statistics for Economists). Then</p>
<p><span class="math display">\[
\frac{2}{n} \sum_{\ell=1}^{n}\left(1-\frac{\ell}{n}\right) \gamma(\ell)=\frac{2}{n^{2}} \sum_{\ell=1}^{n-1} \sum_{j=1}^{\ell} \gamma(j)=\sum_{\ell=1}^{n-1} w_{n \ell}\left(\frac{1}{\ell} \sum_{j=1}^{\ell} \gamma(j)\right) \longrightarrow 0
\]</span></p>
<p>Together, we have shown that (14.8) is <span class="math inline">\(o(1)\)</span> under ergodicity. Hence <span class="math inline">\(\operatorname{var}[\bar{Y}] \rightarrow 0\)</span>. Markov’s inequality establishes that <span class="math inline">\(\bar{Y} \underset{p}{\longrightarrow} \mu\)</span>.</p>
</section>
<section id="conditioning-on-information-sets" class="level2" data-number="14.9">
<h2 data-number="14.9" class="anchored" data-anchor-id="conditioning-on-information-sets"><span class="header-section-number">14.9</span> Conditioning on Information Sets</h2>
<p>In the past few sections we have introduced the concept of the infinite histories. We now consider conditional expectations given infinite histories.</p>
<p>First, some basics. Recall from probability theory that an outcome is an element of a sample space. An event is a set of outcomes. A probability law is a rule which assigns non-negative real numbers to events. When outcomes are infinite histories then events are collections of such histories and a probability law is a rule which assigns numbers to collections of infinite histories.</p>
<p>Now we wish to define a conditional expectation given an infinite past history. Specifically, we wish to define</p>
<p><span class="math display">\[
\mathbb{E}_{t-1}\left[Y_{t}\right]=\mathbb{E}\left[Y_{t} \mid Y_{t-1}, Y_{t-2}, \ldots\right] \text {. }
\]</span></p>
<p>the expected value of <span class="math inline">\(Y_{t}\)</span> given the history <span class="math inline">\(\widetilde{Y}_{t-1}=\left(Y_{t-1}, Y_{t-2}, \ldots\right)\)</span> up to time <span class="math inline">\(t\)</span>. Intuitively, <span class="math inline">\(\mathbb{E}_{t-1}\left[Y_{t}\right]\)</span> is the mean of the conditional distribution, the latter reflecting the information in the history. Mathematically this cannot be defined using (2.6) as the latter requires a joint density for <span class="math inline">\(\left(Y_{t}, Y_{t-1}, Y_{t-2}, \ldots\right)\)</span> which does not make much sense. Instead, we can appeal to Theorem <span class="math inline">\(2.13\)</span> which states that the conditional expectation (14.10) exists if <span class="math inline">\(\mathbb{E}\left|Y_{t}\right|&lt;\infty\)</span> and the probabilities <span class="math inline">\(\mathbb{P}\left[\widetilde{Y}_{t-1} \in A\right]\)</span> are defined. The latter events are discussed in the previous paragraph. Thus the conditional expectation is well defined.</p>
<p>In this textbook we have avoided measure-theoretic terminology to keep the presentation accessible, and because it is my belief that measure theory is more distracting than helpful. However, it is standard in the time series literature to follow the measure-theoretic convention of writing (14.10) as the conditional expectation given a <span class="math inline">\(\sigma\)</span>-field. So at the risk of being overly-technical we will follow this convention and write the expectation (14.10) as <span class="math inline">\(\mathbb{E}\left[Y_{t} \mid \mathscr{F}_{t-1}\right]\)</span> where <span class="math inline">\(\mathscr{F}_{t-1}=\sigma\left(\widetilde{Y}_{t-1}\right)\)</span> is the <span class="math inline">\(\sigma\)</span>-field generated by the history <span class="math inline">\(\widetilde{Y}_{t-1}\)</span>. A <span class="math inline">\(\sigma\)</span>-field (also known as a <span class="math inline">\(\sigma\)</span>-algebra) is a collection of sets satisfying certain regularity conditions <span class="math inline">\({ }^{2}\)</span>. See Probability and Statistics for Economists, Section 1.14. The <span class="math inline">\(\sigma\)</span>-field generated by a random variable <span class="math inline">\(Y\)</span> is the collection of measurable events involving <span class="math inline">\(Y\)</span>. Similarly, the <span class="math inline">\(\sigma\)</span>-field generated by an infinite history is the collection of measurable events involving this history. Intuitively, <span class="math inline">\(\mathscr{F}_{t-1}\)</span> contains all the information available in the history <span class="math inline">\(\widetilde{Y}_{t-1}\)</span>. Consequently, economists typically call <span class="math inline">\(\mathscr{F}_{t-1}\)</span> an information set rather than a <span class="math inline">\(\sigma\)</span>-field. As I said, in this textbook we endeavor to avoid measure theoretic complications so will follow the economists’ label rather than the probabilists’, but use the latter’s notation as is conventional. To summarize, we will write <span class="math inline">\(\mathscr{F}_{t}=\sigma\left(Y_{t}, Y_{t-1}, \ldots\right)\)</span> to indicate the information set generated by an infinite history <span class="math inline">\(\left(Y_{t}, Y_{t-1}, \ldots\right)\)</span>, and will write <span class="math inline">\((14.10)\)</span> as <span class="math inline">\(\mathbb{E}\left[Y_{t} \mid \mathscr{F}_{t-1}\right]\)</span>.</p>
<p>We now describe some properties about information sets <span class="math inline">\(\mathscr{F}_{t}\)</span>.</p>
<p>First, they are nested: <span class="math inline">\(\mathscr{F}_{t-1} \subset \mathscr{F}\)</span>. This means that information accumulates over time. Information is not lost.</p>
<p>Second, it is important to be precise about which variables are contained in the information set. Some economists are sloppy and refer to “the information set at time <span class="math inline">\(t\)</span>” without specifying which variables are in the information set. It is better to be specific. For example, the information sets <span class="math inline">\(\mathscr{F}_{1 t}=\)</span> <span class="math inline">\(\sigma\left(Y_{t}, Y_{t-1}, \ldots\right)\)</span> and <span class="math inline">\(\mathscr{F}_{2 t}=\sigma\left(Y_{t}, X_{t}, Y_{t-1}, X_{t-1} \ldots\right)\)</span> are distinct even though they are both dated at time <span class="math inline">\(t\)</span>.</p>
<p>Third, the conditional expectations (14.10) follow the law of iterated expectations and the conditioning theorem, thus</p>
<p><span class="math display">\[
\begin{aligned}
\mathbb{E}\left[\mathbb{E}\left[Y_{t} \mid \mathscr{F}_{t-1}\right] \mid \mathscr{F}_{t-2}\right] &amp;=\mathbb{E}\left[Y_{t} \mid \mathscr{F}_{t-2}\right] \\
\mathbb{E}\left[\mathbb{E}\left[Y_{t} \mid \mathscr{F}_{t-1}\right]\right] &amp;=\mathbb{E}\left[Y_{t}\right]
\end{aligned}
\]</span></p>
<p>and</p>
<p><span class="math display">\[
\mathbb{E}\left[Y_{t-1} Y_{t} \mid \mathscr{F}_{t-1}\right]=Y_{t-1} \mathbb{E}\left[Y_{t} \mid \mathscr{F}_{t-1}\right]
\]</span></p>
</section>
<section id="martingale-difference-sequences" class="level2" data-number="14.10">
<h2 data-number="14.10" class="anchored" data-anchor-id="martingale-difference-sequences"><span class="header-section-number">14.10</span> Martingale Difference Sequences</h2>
<p>An important concept in economics is unforecastability, meaning that the conditional expectation is the unconditional expectation. This is similar to the properties of a regression error. An unforecastable process is called a martingale difference sequence (MDS).</p>
<p><span class="math inline">\({ }^{2} \mathrm{~A} \sigma\)</span>-field contains the universal set, is closed under complementation, and closed under countable unions. A MDS <span class="math inline">\(e_{t}\)</span> is defined with respect to a specific sequence of information sets <span class="math inline">\(\mathscr{F}_{t}\)</span>. Most commonly the latter are the natural filtration <span class="math inline">\(\mathscr{F}_{t}=\sigma\left(e_{t}, e_{t-1}, \ldots\right)\)</span> (the past history of <span class="math inline">\(\left.e_{t}\right)\)</span> but it could be a larger information set. The only requirement is that <span class="math inline">\(e_{t}\)</span> is adapted to <span class="math inline">\(\mathscr{F}_{t}\)</span>, meaning that <span class="math inline">\(\mathbb{E}\left[e_{t} \mid \mathscr{F}_{t}\right]=e_{t}\)</span>.</p>
<p>Definition 14.4 The process <span class="math inline">\(\left(e_{t}, \mathscr{F}_{t}\right)\)</span> is a Martingale Difference Sequence (MDS) if <span class="math inline">\(e_{t}\)</span> is adapted to <span class="math inline">\(\mathscr{F}_{t}\)</span>, EE <span class="math inline">\(\left|e_{t}\right|&lt;\infty\)</span>, and <span class="math inline">\(\mathbb{E}\left[e_{t} \mid \mathscr{F}_{t-1}\right]=0\)</span>.</p>
<p>In words, a MDS <span class="math inline">\(e_{t}\)</span> is unforecastable in the mean. It is useful to notice that if we apply iterated expectations <span class="math inline">\(\mathbb{E}\left[e_{t}\right]=\mathbb{E}\left[\mathbb{E}\left[e_{t} \mid \mathscr{F}_{t-1}\right]\right]=0\)</span>. Thus a MDS is mean zero.</p>
<p>The definition of a MDS requires the information sets <span class="math inline">\(\mathscr{F}_{t}\)</span> to contain the information in <span class="math inline">\(e_{t}\)</span>, but is broader in the sense that it can contain more information. When no explicit definition is given it is standard to assume that <span class="math inline">\(\mathscr{F}_{t}\)</span> is the natural filtration. However, it is best to explicitly specify the information sets so there is no confusion.</p>
<p>The term “martingale difference sequence” refers to the fact that the summed process <span class="math inline">\(S_{t}=\sum_{j=1}^{t} e_{j}\)</span> is a martingale and <span class="math inline">\(e_{t}\)</span> is its first-difference. A martingale <span class="math inline">\(S_{t}\)</span> is a process which has a finite mean and <span class="math inline">\(\mathbb{E}\left[S_{t} \mid \mathscr{F}_{t-1}\right]=S_{t-1}\)</span></p>
<p>If <span class="math inline">\(e_{t}\)</span> is i.i.d. and mean zero it is a MDS but the reverse is not the case. To see this, first suppose that <span class="math inline">\(e_{t}\)</span> is i.i.d. and mean zero. It is then independent of <span class="math inline">\(\mathscr{F}_{t-1}=\sigma\left(e_{t-1}, e_{t-2}, \ldots\right)\)</span> so <span class="math inline">\(\mathbb{E}\left[e_{t} \mid \mathscr{F}_{t-1}\right]=\mathbb{E}\left[e_{t}\right]=0\)</span>. Thus an i.i.d. shock is a MDS as claimed.</p>
<p>To show that the reverse is not true let <span class="math inline">\(u_{t}\)</span> be i.i.d. <span class="math inline">\(\mathrm{N}(0,1)\)</span> and set</p>
<p><span class="math display">\[
e_{t}=u_{t} u_{t-1}
\]</span></p>
<p>By the conditioning theorem</p>
<p><span class="math display">\[
\mathbb{E}\left[e_{t} \mid \mathscr{F}_{t-1}\right]=u_{t-1} \mathbb{E}\left[u_{t} \mid \mathscr{F}_{t-1}\right]=0
\]</span></p>
<p>so <span class="math inline">\(e_{t}\)</span> is a MDS. The process (14.11) is not, however, i.i.d. One way to see this is to calculate the first autocovariance of <span class="math inline">\(e_{t}^{2}\)</span>, which is</p>
<p><span class="math display">\[
\begin{aligned}
\operatorname{cov}\left(e_{t}^{2}, e_{t-1}^{2}\right) &amp;=\mathbb{E}\left[e_{t}^{2} e_{t-1}^{2}\right]-\mathbb{E}\left[e_{t}^{2}\right] \mathbb{E}\left[e_{t-1}^{2}\right] \\
&amp;=\mathbb{E}\left[u_{t}^{2}\right] \mathbb{E}\left[u_{t-1}^{4}\right] \mathbb{E}\left[u_{t-2}^{2}\right]-1 \\
&amp;=2 \neq 0 .
\end{aligned}
\]</span></p>
<p>Since the covariance is non-zero, <span class="math inline">\(e_{t}\)</span> is not an independent sequence. Thus <span class="math inline">\(e_{t}\)</span> is a MDS but not i.i.d.</p>
<p>An important property of a square integrable MDS is that it is serially uncorrelated. To see this, observe that by iterated expectations, the conditioning theorem, and the definition of a MDS, for <span class="math inline">\(k&gt;0\)</span>,</p>
<p><span class="math display">\[
\begin{aligned}
\operatorname{cov}\left(e_{t}, e_{t-k}\right) &amp;=\mathbb{E}\left[e_{t} e_{t-k}\right] \\
&amp;=\mathbb{E}\left[\mathbb{E}\left[e_{t} e_{t-k} \mid \mathscr{F}_{t-1}\right]\right] \\
&amp;=\mathbb{E}\left[\mathbb{E}\left[e_{t} \mid \mathscr{F}_{t-1}\right] e_{t-k}\right] \\
&amp;=\mathbb{E}\left[0 e_{t-k}\right] \\
&amp;=0 .
\end{aligned}
\]</span></p>
<p>Thus the autocovariances and autocorrelations are zero. A process that is serially uncorrelated, however, is not necessarily a MDS. Take the process <span class="math inline">\(e_{t}=u_{t}+\)</span> <span class="math inline">\(u_{t-1} u_{t-2}\)</span> with <span class="math inline">\(u_{t}\)</span> i.i.d. <span class="math inline">\(\mathrm{N}(0,1)\)</span>. The process <span class="math inline">\(e_{t}\)</span> is not a MDS because <span class="math inline">\(\mathbb{E}\left[e_{t} \mid \mathscr{F}_{t-1}\right]=u_{t-1} u_{t-2} \neq 0\)</span>. However,</p>
<p><span class="math display">\[
\begin{aligned}
\operatorname{cov}\left(e_{t}, e_{t-1}\right) &amp;=\mathbb{E}\left[e_{t} e_{t-1}\right] \\
&amp;=\mathbb{E}\left[\left(u_{t}+u_{t-1} u_{t-2}\right)\left(u_{t-1}+u_{t-2} u_{t-3}\right)\right] \\
&amp;=\mathbb{E}\left[u_{t} u_{t-1}+u_{t} u_{t-2} u_{t-3}+u_{t-1}^{2} u_{t-2}+u_{t-1} u_{t-2}^{2} u_{t-3}\right] \\
&amp;=\mathbb{E}\left[u_{t}\right] \mathbb{E}\left[u_{t-1}\right]+\mathbb{E}\left[u_{t}\right] \mathbb{E}\left[u_{t-2}\right] \mathbb{E}\left[u_{t-3}\right] \\
&amp;+\mathbb{E}\left[u_{t-1}^{2}\right] \mathbb{E}\left[u_{t-2}\right]+\mathbb{E}\left[u_{t-1}\right] \mathbb{E}\left[u_{t-2}^{2}\right] \mathbb{E}\left[u_{t-3}\right] \\
&amp;=0 .
\end{aligned}
\]</span></p>
<p>Similarly, <span class="math inline">\(\operatorname{cov}\left(e_{t}, e_{t-k}\right)=0\)</span> for <span class="math inline">\(k \neq 0\)</span>. Thus <span class="math inline">\(e_{t}\)</span> is serially uncorrelated. We have proved the following.</p>
<p>Theorem 14.10 If <span class="math inline">\(\left(e_{t}, \mathscr{F}_{t}\right)\)</span> is a MDS and <span class="math inline">\(\mathbb{E}\left[e_{t}^{2}\right]&lt;\infty\)</span> then <span class="math inline">\(e_{t}\)</span> is serially uncorrelated.</p>
<p>Another important special case is a homoskedastic martingale difference sequence.</p>
<p>Definition 14.5 The MDS <span class="math inline">\(\left(e_{t}, \mathscr{F}_{t}\right)\)</span> is a Homoskedastic Martingale Difference Sequence if <span class="math inline">\(\mathbb{E}\left[e_{t}^{2} \mid \mathscr{F}_{t-1}\right]=\sigma^{2}\)</span>.</p>
<p>A homoskedastic MDS should more properly be called a conditionally homoskedastic MDS because the property concerns the conditional distribution rather than the unconditional. That is, any strictly stationary MDS satisfies a constant variance <span class="math inline">\(\mathbb{E}\left[e_{t}^{2}\right]\)</span> but only a homoskedastic MDS has a constant conditional variance <span class="math inline">\(\mathbb{E}\left[e_{t}^{2} \mid \mathscr{F}_{t-1}\right]\)</span></p>
<p>A homoskedatic MDS is analogous to a conditionally homoskedastic regression error. It is intermediate between a MDS and an i.i.d. sequence. Specifically, a square integrable and mean zero i.i.d. sequence is a homoskedastic MDS and the latter is a MDS.</p>
<p>The reverse is not the case. First, a MDS is not necessarily conditionally homoskedastic. Consider the example <span class="math inline">\(e_{t}=u_{t} u_{t-1}\)</span> given previously which we showed is a MDS. It is not conditionally homoskedastic, however, because</p>
<p><span class="math display">\[
\mathbb{E}\left[e_{t}^{2} \mid \mathscr{F}_{t-1}\right]=u_{t-1}^{2} \mathbb{E}\left[u_{t}^{2} \mid \mathscr{F}_{t-1}\right]=u_{t-1}^{2}
\]</span></p>
<p>which is time-varying. Thus this MDS <span class="math inline">\(e_{t}\)</span> is conditionally heteroskedastic. Second, a homoskedastic MDS is not necessarily i.i.d. Consider the following example. Set <span class="math inline">\(e_{t}=\sqrt{1-2 / \eta_{t-1}} T_{t}\)</span>, where <span class="math inline">\(T_{t}\)</span> is distributed as student <span class="math inline">\(t\)</span> with degree of freedom parameter <span class="math inline">\(\eta_{t-1}=2+e_{t-1}^{2}\)</span>. This is scaled so that <span class="math inline">\(\mathbb{E}\left[e_{t} \mid \mathscr{F}_{t-1}\right]=0\)</span> and <span class="math inline">\(\mathbb{E}\left[e_{t}^{2} \mid \mathscr{F}_{t-1}\right]=1\)</span>, and is thus a homoskedastic MDS. The conditional distribution of <span class="math inline">\(e_{t}\)</span> depends on <span class="math inline">\(e_{t-1}\)</span> through the degree of freedom parameter. Hence <span class="math inline">\(e_{t}\)</span> is not an independent sequence.</p>
<p>One way to think about the difference between MDS and i.i.d. shocks is in terms of forecastability. An i.i.d. process is fully unforecastable in that no function of an i.i.d. process is forecastable. A MDS is unforecastable in the mean but other moments may be forecastable.</p>
<p>As we mentioned above, the definition of a MDS <span class="math inline">\(e_{t}\)</span> allows for conditional heteroskedasticity, meaning that the conditional variance <span class="math inline">\(\sigma_{t}^{2}=\mathbb{E}\left[e_{t}^{2} \mid \mathscr{F}_{t-1}\right]\)</span> may be time-varying. In financial econometrics there are many models for conditional heteroskedasticity, including autoregressive conditional heteroskedasticity (ARCH), generalized ARCH (GARCH), and stochastic volatility. A good reference for this class of models is Campbell, Lo, and MacKinlay (1997).</p>
</section>
<section id="clt-for-martingale-differences" class="level2" data-number="14.11">
<h2 data-number="14.11" class="anchored" data-anchor-id="clt-for-martingale-differences"><span class="header-section-number">14.11</span> CLT for Martingale Differences</h2>
<p>We are interested in an asymptotic approximation for the distribution of the normalized sample mean</p>
<p><span class="math display">\[
S_{n}=\frac{1}{\sqrt{n}} \sum_{t=1}^{n} u_{t}
\]</span></p>
<p>where <span class="math inline">\(u_{t}\)</span> is mean zero with variance <span class="math inline">\(\mathbb{E}\left[u_{t} u_{t}^{\prime}\right]=\Sigma&lt;\infty\)</span>. In this section we present a CLT for the case where <span class="math inline">\(u_{t}\)</span> is a martingale difference sequence.</p>
<p>Theorem 14.11 MDS CLT If <span class="math inline">\(u_{t}\)</span> is a strictly stationary and ergodic martingale difference sequence and <span class="math inline">\(\mathbb{E}\left[u_{t} u_{t}^{\prime}\right]=\Sigma&lt;\infty\)</span>, then as <span class="math inline">\(n \rightarrow \infty\)</span>,</p>
<p><span class="math display">\[
S_{n}=\frac{1}{\sqrt{n}} \sum_{t=1}^{n} u_{t} \underset{d}{\longrightarrow} \mathrm{N}(0, \Sigma) \text {. }
\]</span></p>
<p>The conditions for Theorem <span class="math inline">\(14.11\)</span> are similar to the Lindeberg-Lévy CLT. The only difference is that the i.i.d. assumption has been replaced by the assumption of a strictly stationarity and ergodic MDS.</p>
<p>The proof of Theorem <span class="math inline">\(14.11\)</span> is technically advanced so we do not present the full details, but instead refer readers to Theorem <span class="math inline">\(3.2\)</span> of Hall and Heyde (1980) or Theorem <span class="math inline">\(25.3\)</span> of Davidson (1994) (which are more general than Theorem 14.11, not requiring strict stationarity). To illustrate the role of the MDS assumption we give a sketch of the proof in Section 14.47.</p>
</section>
<section id="mixing" class="level2" data-number="14.12">
<h2 data-number="14.12" class="anchored" data-anchor-id="mixing"><span class="header-section-number">14.12</span> Mixing</h2>
<p>For many results, including a CLT for correlated (non-MDS) series, we need a stronger restriction on the dependence between observations than ergodicity.</p>
<p>Recalling the property (14.5) of ergodic sequences we can measure the dependence between two events <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> by the discrepancy</p>
<p><span class="math display">\[
\alpha(A, B)=|\mathbb{P}[A \cap B]-\mathbb{P}[A] \mathbb{P}[B]| .
\]</span></p>
<p>This equals 0 when <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are independent and is positive otherwise. In general, <span class="math inline">\(\alpha(A, B)\)</span> can be used to measure the degree of dependence between the events <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span>.</p>
<p>Now consider the two information sets ( <span class="math inline">\(\sigma\)</span>-fields)</p>
<p><span class="math display">\[
\begin{aligned}
\mathscr{F}_{-\infty}^{t} &amp;=\sigma\left(\ldots, Y_{t-1}, Y_{t}\right) \\
\mathscr{F}_{t}^{\infty} &amp;=\sigma\left(Y_{t}, Y_{t+1}, \ldots\right) .
\end{aligned}
\]</span></p>
<p>The first is the history of the series up until period <span class="math inline">\(t\)</span> and the second is the history of the series starting in period <span class="math inline">\(t\)</span> and going forward. We then separate the information sets by <span class="math inline">\(\ell\)</span> periods, that is, take <span class="math inline">\(\mathscr{F}_{-\infty}^{t-\ell}\)</span> and <span class="math inline">\(\mathscr{F}_{t}^{\infty}\)</span>. We can measure the degree of dependence between the information sets by taking all events in each and then taking the largest discrepancy (14.13). This is</p>
<p><span class="math display">\[
\alpha(\ell)=\sup _{A \in \mathscr{F}_{-\infty}^{t-\ell}, B \in \mathscr{F}_{t}^{\infty}} \alpha(A, B) .
\]</span></p>
<p>The constants <span class="math inline">\(\alpha(\ell)\)</span> are known as the strong mixing coefficients. We say that <span class="math inline">\(Y_{t}\)</span> is strong mixing if <span class="math inline">\(\alpha(\ell) \rightarrow 0\)</span> as <span class="math inline">\(\ell \rightarrow \infty\)</span>. This means that as the time separation increases between the information sets, the degree of dependence decreases, eventually reaching independence.</p>
<p>From the Theorem of Cesàro Means (Theorem A.4 of Probability and Statistics for Economists), strong mixing implies (14.5) which is equivalent to ergodicity. Thus a mixing process is ergodic.</p>
<p>An intuition concerning mixing can be colorfully illustrated by the following example due to Halmos (1956). A martini is a drink consisting of a large portion of gin and a small part of vermouth. Suppose that you pour a serving of gin into a martini glass, pour a small amount of vermouth on top, and then stir the drink with a swizzle stick. If your stirring process is mixing, with each turn of the stick the vermouth will become more evenly distributed throughout the gin, and asymptotically (as the number of stirs tends to infinity) the vermouth and gin distributions will become independent <span class="math inline">\({ }^{3}\)</span>. If so, this is a mixing process.</p>
<p>For applications, mixing is often useful when we can characterize the rate at which the coefficients <span class="math inline">\(\alpha(\ell)\)</span> decline to zero. There are two types of conditions which are seen in asymptotic theory: rates and summation. Rate conditions take the form <span class="math inline">\(\alpha(\ell)=O\left(\ell^{-r}\right)\)</span> or <span class="math inline">\(\alpha(\ell)=o\left(\ell^{-r}\right)\)</span>. Summation conditions take the form <span class="math inline">\(\sum_{\ell=0}^{\infty} \alpha(\ell)^{r}&lt;\infty\)</span> or <span class="math inline">\(\sum_{\ell=0}^{\infty} \ell^{s} \alpha(\ell)^{r}&lt;\infty\)</span>.</p>
<p>There are alternative measures of dependence beyond (14.13) and many have been proposed. Strong mixing is one of the weakest (and thus embraces a wide set of time series processes) but is insufficiently strong for some applications. Another popular dependence measure is known as absolute regularity or <span class="math inline">\(\beta\)</span>-mixing. The <span class="math inline">\(\beta\)</span>-mixing coefficients are</p>
<p><span class="math display">\[
\beta(\ell)=\sup _{A \in \mathscr{F}_{t}^{\infty}} \mathbb{E}\left|\mathbb{P}\left[A \mid \mathscr{F}_{-\infty}^{t-\ell}\right]-\mathbb{P}[A]\right| .
\]</span></p>
<p>Absolute regularity is stronger than strong mixing in the sense that <span class="math inline">\(\beta(\ell) \rightarrow 0\)</span> implies <span class="math inline">\(\alpha(\ell) \rightarrow 0\)</span>, and rate conditions for the <span class="math inline">\(\beta\)</span>-mixing coefficients imply the same rates for the strong mixing coefficients.</p>
<p>One reason why mixing is useful for applications is that it is preserved by transformations.</p>
<p>Theorem 14.12 If <span class="math inline">\(Y_{t}\)</span> has mixing coefficients <span class="math inline">\(\alpha_{Y}(\ell)\)</span> and <span class="math inline">\(X_{t}=\)</span> <span class="math inline">\(\phi\left(Y_{t}, Y_{t-1}, Y_{t-2}, \ldots, Y_{t-q}\right)\)</span> then <span class="math inline">\(X_{t}\)</span> has mixing coefficients <span class="math inline">\(\alpha_{X}(\ell) \leq \alpha_{Y}(\ell-q)\)</span> (for <span class="math inline">\(\ell \geq q)\)</span>. The coefficients <span class="math inline">\(\alpha_{X}(\ell)\)</span> satisfy the same summation and rate conditions as <span class="math inline">\(\alpha_{Y}(\ell)\)</span>.</p>
<p>A limitation of the above result is that it is confined to a finite number of lags unlike the transformation results for stationarity and ergodicity.</p>
<p>Mixing can be a useful tool because of the following inequalities.</p>
<p><span class="math inline">\({ }^{3}\)</span> Of course, if you really make an asymptotic number of stirs you will never finish stirring and you won’t be able to enjoy the martini. Hence in practice it is advised to stop stirring before the number of stirs reaches infinity. Theorem 14.13 Let <span class="math inline">\(\mathscr{F}_{-\infty}^{t}\)</span> and <span class="math inline">\(\mathscr{F}_{t}^{\infty}\)</span> be constructed from the pair <span class="math inline">\(\left(X_{t}, Z_{t}\right)\)</span>.</p>
<ol type="1">
<li>If <span class="math inline">\(\left|X_{t}\right| \leq C_{1}\)</span> and <span class="math inline">\(\left|Z_{t}\right| \leq C_{2}\)</span> then</li>
</ol>
<p><span class="math display">\[
\left|\operatorname{cov}\left(X_{t-\ell}, Z_{t}\right)\right| \leq 4 C_{1} C_{2} \alpha(\ell) .
\]</span></p>
<p> 1. If <span class="math inline">\(\mathbb{E}\left|X_{t}\right|^{r}&lt;\infty\)</span> and <span class="math inline">\(\mathbb{E}\left|Z_{t}\right|^{q}&lt;\infty\)</span> for <span class="math inline">\(1 / r+1 / q&lt;1\)</span> then</p>
<p><span class="math display">\[
\left|\operatorname{cov}\left(X_{t-\ell}, Z_{t}\right)\right| \leq 8\left(\mathbb{E}\left|X_{t}\right|^{r}\right)^{1 / r}\left(\mathbb{E}\left|Z_{t}\right|^{q}\right)^{1 / q} \alpha(\ell)^{1-1 / r-1 / q} .
\]</span></p>
<p> 1. If <span class="math inline">\(\mathbb{E}\left[Z_{t}\right]=0\)</span> and <span class="math inline">\(\mathbb{E}\left|Z_{t}\right|^{r}&lt;\infty\)</span> for <span class="math inline">\(r \geq 1\)</span> then</p>
<p><span class="math display">\[
\mathbb{E}\left|\mathbb{E}\left[Z_{t} \mid \mathscr{F}_{-\infty}^{t-\ell}\right]\right| \leq 6\left(\mathbb{E}\left|Z_{t}\right|^{r}\right)^{1 / r} \alpha(\ell)^{1-1 / r} .
\]</span></p>
<p>The proof is given in Section 14.47. Our next result follows fairly directly from the definition of mixing.</p>
<p>Theorem 14.14 If <span class="math inline">\(Y_{t}\)</span> is i.i.d. then it is strong mixing and ergodic.</p>
</section>
<section id="clt-for-correlated-observations" class="level2" data-number="14.13">
<h2 data-number="14.13" class="anchored" data-anchor-id="clt-for-correlated-observations"><span class="header-section-number">14.13</span> CLT for Correlated Observations</h2>
<p>In this section we develop a CLT for the normalized mean <span class="math inline">\(S_{n}\)</span> defined in (14.12) allowing the variables <span class="math inline">\(u_{t}\)</span> to be serially correlated.</p>
<p>In (14.8) we found that in the scalar case</p>
<p><span class="math display">\[
\operatorname{var}\left[S_{n}\right]=\sigma^{2}+2 \sum_{\ell=1}^{n}\left(1-\frac{\ell}{n}\right) \gamma(\ell)
\]</span></p>
<p>where <span class="math inline">\(\sigma^{2}=\operatorname{var}\left[u_{t}\right]\)</span> and <span class="math inline">\(\gamma(\ell)=\operatorname{cov}\left(u_{t}, u_{t-\ell}\right)\)</span>. Since <span class="math inline">\(\gamma(-\ell)=\gamma(\ell)\)</span> this can be written as</p>
<p><span class="math display">\[
\operatorname{var}\left[S_{n}\right]=\sum_{\ell=-n}^{n}\left(1-\frac{|\ell|}{n}\right) \gamma(\ell) .
\]</span></p>
<p>In the vector case define the variance <span class="math inline">\(\Sigma=\mathbb{E}\left[u_{t} u_{t}^{\prime}\right]\)</span> and the matrix covariance <span class="math inline">\(\Gamma(\ell)=\mathbb{E}\left[u_{t} u_{t-\ell}^{\prime}\right]\)</span> which satisfies <span class="math inline">\(\Gamma(-\ell)=\Gamma(\ell)^{\prime}\)</span>. We obtain by a calculation analogous to (14.14)</p>
<p><span class="math display">\[
\operatorname{var}\left[S_{n}\right]=\Sigma+\sum_{\ell=1}^{n}\left(1-\frac{\ell}{n}\right)\left(\Gamma(\ell)+\Gamma(\ell)^{\prime}\right)=\sum_{\ell=-n}^{n}\left(1-\frac{|\ell|}{n}\right) \Gamma(\ell) .
\]</span></p>
<p>A necessary condition for <span class="math inline">\(S_{n}\)</span> to converge to a normal distribution is that the variance (14.15) converges to a limit. Indeed, as <span class="math inline">\(n \rightarrow \infty\)</span></p>
<p><span class="math display">\[
\sum_{\ell=1}^{n}\left(1-\frac{\ell}{n}\right) \Gamma(\ell)=\frac{1}{n} \sum_{\ell=1}^{n-1} \sum_{j=1}^{\ell} \Gamma(j) \rightarrow \sum_{\ell=0}^{\infty} \Gamma(\ell)
\]</span></p>
<p>where the convergence holds by the Theorem of Cesàro Means if the limit in (14.16) is convergent. A necessary condition for this to hold is that the covariances <span class="math inline">\(\Gamma(\ell)\)</span> decline to zero as <span class="math inline">\(\ell \rightarrow \infty\)</span>. A sufficient condition is that the covariances are absolutely summable which can be verified using a mixing inequality. Using the triangle inequality (B.16) and Theorem 14.13.2, for any <span class="math inline">\(r&gt;2\)</span></p>
<p><span class="math display">\[
\sum_{\ell=0}^{\infty}\|\Gamma(\ell)\| \leq 8\left(\mathbb{E}\left\|u_{t}\right\|^{r}\right)^{2 / r} \sum_{\ell=0}^{\infty} \alpha(\ell)^{1-2 / r} .
\]</span></p>
<p>This implies that (14.15) converges if <span class="math inline">\(\mathbb{E}\left\|u_{t}\right\|^{r}&lt;\infty\)</span> and <span class="math inline">\(\sum_{\ell=0}^{\infty} \alpha(\ell)^{1-2 / r}&lt;\infty\)</span>. We conclude that under these assumptions</p>
<p><span class="math display">\[
\operatorname{var}\left[S_{n}\right] \rightarrow \sum_{\ell=-\infty}^{\infty} \Gamma(\ell) \stackrel{\text { def }}{=} \Omega
\]</span></p>
<p>The matrix <span class="math inline">\(\Omega\)</span> plays a special role in the inference theory for tme series. It is often called the long-run variance of <span class="math inline">\(u_{t}\)</span> as it is the variance of sample means in large samples.</p>
<p>It turns out that these conditions are sufficient for the CLT.</p>
<p>Theorem 14.15 If <span class="math inline">\(u_{t}\)</span> is strictly stationary with mixing coefficients <span class="math inline">\(\alpha(\ell), \mathbb{E}\left[u_{t}\right]=\)</span> 0 , for some <span class="math inline">\(r&gt;2\)</span>, <span class="math inline">\(\mathbb{E}\left\|u_{t}\right\|^{r}&lt;\infty\)</span> and <span class="math inline">\(\sum_{\ell=0}^{\infty} \alpha(\ell)^{1-2 / r}&lt;\infty\)</span>, then (14.17) is convergent and <span class="math inline">\(S_{n}=n^{-1 / 2} \sum_{t=1}^{n} u_{t} \underset{d}{\longrightarrow} \mathrm{N}(0, \Omega)\)</span></p>
<p>The proof is in Section <span class="math inline">\(14.47\)</span>.</p>
<p>The theorem requires <span class="math inline">\(r&gt;2\)</span> finite moments which is stronger than the MDS CLT. This <span class="math inline">\(r\)</span> does not need to be an integer, meaning that the theorem holds under slightly more than two finite moments. The summability condition on the mixing coefficients in Theorem <span class="math inline">\(14.15\)</span> is considerably stronger than ergodicity. There is a trade-off involving the choice of <span class="math inline">\(r\)</span>. A larger <span class="math inline">\(r\)</span> means more moments are required finite but a slower decay in the coefficients <span class="math inline">\(\alpha(\ell)\)</span> is allowed. Smaller <span class="math inline">\(r\)</span> is less restrictive regarding moments but requires a faster decay rate in the mixing coefficients.</p>
</section>
<section id="linear-projection" class="level2" data-number="14.14">
<h2 data-number="14.14" class="anchored" data-anchor-id="linear-projection"><span class="header-section-number">14.14</span> Linear Projection</h2>
<p>In Chapter 2 we extensively studied the properties of linear projection models. In the context of stationary time series we can use similar tools. An important extension is to allow for projections onto infinite dimensional random vectors. For this analysis we assume that <span class="math inline">\(Y_{t}\)</span> is covariance stationary.</p>
<p>Recall that when <span class="math inline">\((Y, X)\)</span> have a joint distribution with bounded variances the linear projection of <span class="math inline">\(Y\)</span> onto <span class="math inline">\(X\)</span> (the best linear predictor) is the minimizer of <span class="math inline">\(S(\beta)=\mathbb{E}\left[\left(Y-\beta^{\prime} X\right)^{2}\right]\)</span> and has the solution</p>
<p><span class="math display">\[
\mathscr{P}[Y \mid X]=X^{\prime}\left(\mathbb{E}\left[X X^{\prime}\right]\right)^{-1} \mathbb{E}[X Y] \text {. }
\]</span></p>
<p>This projection is unique and has a unique projection error <span class="math inline">\(e=Y-\mathscr{P}[Y \mid X]\)</span>.</p>
<p>This idea extends to any Hilbert space including the infinite past history <span class="math inline">\(\widetilde{Y}_{t-1}=\left(\ldots, Y_{t-2}, Y_{t-1}\right)\)</span>. From the projection theorem for Hilbert spaces (see Theorem 2.3.1 of Brockwell and Davis (1991)) the projection <span class="math inline">\(\mathscr{P}_{t-1}\left[Y_{t}\right]=\mathscr{P}\left[Y_{t} \mid \tilde{Y}_{t-1}\right]\)</span> of <span class="math inline">\(Y_{t}\)</span> onto <span class="math inline">\(\widetilde{Y}_{t-1}\)</span> is unique and has a unique projection error</p>
<p><span class="math display">\[
e_{t}=Y_{t}-\mathscr{P}_{t-1}\left[Y_{t}\right] .
\]</span></p>
<p>The projection error is mean zero, has finite variance <span class="math inline">\(\sigma^{2}=\mathbb{E}\left[e_{t}^{2}\right] \leq \mathbb{E}\left[Y_{t}^{2}\right]&lt;\infty\)</span>, and is serially uncorrelated. By Theorem 14.2, if <span class="math inline">\(Y_{t}\)</span> is strictly stationary then <span class="math inline">\(\mathscr{P}_{t-1}\left[Y_{t}\right]\)</span> and <span class="math inline">\(e_{t}\)</span> are strictly stationary.</p>
<p>The property (14.18) implies that the projection errors are serially uncorrelated. We state these results formally.</p>
<p>Theorem 14.16 If <span class="math inline">\(Y_{t} \in \mathbb{R}\)</span> is covariance stationary it has the projection equation</p>
<p><span class="math display">\[
Y_{t}=\mathscr{P}_{t-1}\left[Y_{t}\right]+e_{t} .
\]</span></p>
<p>The projection error <span class="math inline">\(e_{t}\)</span> satisfies</p>
<p><span class="math display">\[
\begin{aligned}
\mathbb{E}\left[e_{t}\right] &amp;=0 \\
\mathbb{E}\left[e_{t-j} e_{t}\right] &amp;=0 \quad j \geq 1
\end{aligned}
\]</span></p>
<p>and</p>
<p><span class="math display">\[
\sigma^{2}=\mathbb{E}\left[e_{t}^{2}\right] \leq \mathbb{E}\left[Y_{t}^{2}\right]&lt;\infty .
\]</span></p>
<p>If <span class="math inline">\(Y_{t}\)</span> is strictly stationary then <span class="math inline">\(e_{t}\)</span> is strictly stationary.</p>
</section>
<section id="white-noise" class="level2" data-number="14.15">
<h2 data-number="14.15" class="anchored" data-anchor-id="white-noise"><span class="header-section-number">14.15</span> White Noise</h2>
<p>The projection error <span class="math inline">\(e_{t}\)</span> is mean zero, has a finite variance, and is serially uncorrelated. This describes what is known as a white noise process.</p>
<p>Definition 14.6 The process <span class="math inline">\(e_{t}\)</span> is white noise if <span class="math inline">\(\mathbb{E}\left[e_{t}\right]=0, \mathbb{E}\left[e_{t}^{2}\right]=\sigma^{2}&lt;\infty\)</span>, and <span class="math inline">\(\operatorname{cov}\left(e_{t}, e_{t-k}\right)=0\)</span> for <span class="math inline">\(k \neq 0\)</span>.</p>
<p>A MDS is white noise (Theorem 14.10) but the reverse is not true as shown by the example <span class="math inline">\(e_{t}=\)</span> <span class="math inline">\(u_{t}+u_{t-1} u_{t-2}\)</span> given in Section 14.10, which is white noise but not a MDS. Therefore, the following types of shocks are nested: i.i.d., MDS, and white noise, with i.i.d. being the most narrow class and white noise the broadest. It is helpful to observe that a white noise process can be conditionally heteroskedastic as the conditional variance is unrestricted.</p>
</section>
<section id="the-wold-decomposition" class="level2" data-number="14.16">
<h2 data-number="14.16" class="anchored" data-anchor-id="the-wold-decomposition"><span class="header-section-number">14.16</span> The Wold Decomposition</h2>
<p>In Section <span class="math inline">\(14.14\)</span> we showed that a covariance stationary process has a white noise projection error. This result can be used to express the series as an infinite linear function of the projection errors. This is a famous result known as the Wold decomposition. Theorem 14.17 The Wold Decomposition If <span class="math inline">\(Y_{t}\)</span> is covariance stationary and <span class="math inline">\(\sigma^{2}&gt;0\)</span> where <span class="math inline">\(\sigma^{2}\)</span> is the projection error variance (14.19), then <span class="math inline">\(Y_{t}\)</span> has the linear representation</p>
<p><span class="math display">\[
Y_{t}=\mu_{t}+\sum_{j=0}^{\infty} b_{j} e_{t-j}
\]</span></p>
<p>where <span class="math inline">\(e_{t}\)</span> are the white noise projection errors (14.18), <span class="math inline">\(b_{0}=1\)</span>,</p>
<p><span class="math display">\[
\sum_{j=1}^{\infty} b_{j}^{2}&lt;\infty,
\]</span></p>
<p>and</p>
<p><span class="math display">\[
\mu_{t}=\lim _{m \rightarrow \infty} \mathscr{P}_{t-m}\left[Y_{t}\right]
\]</span></p>
<p>The Wold decomposition shows that <span class="math inline">\(Y_{t}\)</span> can be written as a linear function of the white noise projection errors plus <span class="math inline">\(\mu_{t}\)</span>. The infinite sum in (14.20) is also known as a linear process. The Wold decomposition is a foundational result for linear time series analysis. Since any covariance stationary process can be written in this format this justifies linear models as approximations.</p>
<p>The series <span class="math inline">\(\mu_{t}\)</span> is the projection of <span class="math inline">\(Y_{t}\)</span> on the history from the infinite past. It is the part of <span class="math inline">\(Y_{t}\)</span> which is perfectly predictable from its past values and is called the deterministic component. In most cases <span class="math inline">\(\mu_{t}=\mu\)</span>, the unconditional mean of <span class="math inline">\(Y_{t}\)</span>. However, it is possible for stationary processes to have more substantive deterministic components. An example is</p>
<p><span class="math display">\[
\mu_{t}=\left\{\begin{array}{cc}
(-1)^{t} &amp; \text { with probability } 1 / 2 \\
(-1)^{t+1} &amp; \text { with probability } 1 / 2 .
\end{array}\right.
\]</span></p>
<p>This series is strictly stationary, mean zero, and variance one. However, it is perfectly predictable given the previous history as it simply oscillates between <span class="math inline">\(-1\)</span> and 1 .</p>
<p>In practical applied time series analysis, deterministic components are typically excluded by assumption. We call a stationary time series non-deterministic <span class="math inline">\({ }^{4}\)</span> if <span class="math inline">\(\mu_{t}=\mu\)</span>, a constant. In this case the Wold decomposition has a simpler form.</p>
<p>Theorem 14.18 If <span class="math inline">\(Y_{t}\)</span> is covariance stationary and non-deterministic then <span class="math inline">\(Y_{t}\)</span> has the linear representation</p>
<p><span class="math display">\[
Y_{t}=\mu+\sum_{j=0}^{\infty} b_{j} e_{t-j},
\]</span></p>
<p>where <span class="math inline">\(b_{j}\)</span> satisfy (14.21) and <span class="math inline">\(e_{t}\)</span> are the white noise projection errors (14.18).</p>
<p>A limitation of the Wold decomposition is the restriction to linearity. While it shows that there is a valid linear approximation, it may be that a nonlinear model provides a better approximation.</p>
<p>For a proof of Theorem <span class="math inline">\(14.17\)</span> see Section 14.47.</p>
<p><span class="math inline">\({ }^{4}\)</span> Most authors define purely non-deterministic as the case <span class="math inline">\(\mu_{t}=0\)</span>. We allow for a non-zero mean so to accomodate practical time series applications.</p>
</section>
<section id="lag-operator" class="level2" data-number="14.17">
<h2 data-number="14.17" class="anchored" data-anchor-id="lag-operator"><span class="header-section-number">14.17</span> Lag Operator</h2>
<p>An algebraic construct which is useful for the analysis of time series models is the lag operator.</p>
<p>Definition 14.7 The lag operator L satisfies L <span class="math inline">\(Y_{t}=Y_{t-1}\)</span>.</p>
<p>Defining <span class="math inline">\(\mathrm{L}^{2}=\mathrm{LL}\)</span>, we see that <span class="math inline">\(\mathrm{L}^{2} Y_{t}=\mathrm{L} Y_{t-1}=Y_{t-2}\)</span>. In general, <span class="math inline">\(\mathrm{L}^{k} Y_{t}=Y_{t-k}\)</span>.</p>
<p>Using the lag operator the Wold decomposition can be written in the format</p>
<p><span class="math display">\[
\begin{aligned}
Y_{t} &amp;=\mu+b_{0} e_{t}+b_{1} \mathrm{~L} e_{t}+b_{2} \mathrm{~L}^{2} e_{t}+\cdots \\
&amp;=\mu+\left(b_{0}+b_{1} \mathrm{~L}+b_{2} \mathrm{~L}^{2}+\cdots\right) e_{t} \\
&amp;=\mu+b(\mathrm{~L}) e_{t}
\end{aligned}
\]</span></p>
<p>where <span class="math inline">\(b(z)=b_{0}+b_{1} z+b_{2} z^{2}+\cdots\)</span> is an infinite-order polynomial. The expression <span class="math inline">\(Y_{t}=\mu+b(\mathrm{~L}) e_{t}\)</span> is compact way to write the Wold representation.</p>
</section>
<section id="autoregressive-wold-representation" class="level2" data-number="14.18">
<h2 data-number="14.18" class="anchored" data-anchor-id="autoregressive-wold-representation"><span class="header-section-number">14.18</span> Autoregressive Wold Representation</h2>
<p>From Theorem 14.16, <span class="math inline">\(Y_{t}\)</span> satisfies a projection onto its infinite past. Theorem <span class="math inline">\(14.18\)</span> shows that this projection equals a linear function of the lagged projection errors. An alternative is to write the projection as a linear function of the lagged <span class="math inline">\(Y_{t}\)</span>. It turns out that to obtain a unique and convergent representation we need a strengthening of the conditions.</p>
<p>Theorem 14.19 If <span class="math inline">\(Y_{t}\)</span> is covariance stationary, non-deterministic, with Wold representation <span class="math inline">\(Y_{t}=b(\mathrm{~L}) e_{t}\)</span>, such that <span class="math inline">\(|b(z)| \geq \delta&gt;0\)</span> for all complex <span class="math inline">\(|z| \leq 1\)</span>, and for some integer <span class="math inline">\(s \geq 0\)</span> the Wold coefficients satisfy <span class="math inline">\(\sum_{j=0}^{\infty}\left(\sum_{k=0}^{\infty} k^{s} b_{j+k}\right)^{2}&lt;\infty\)</span>, then <span class="math inline">\(Y_{t}\)</span> has the representation</p>
<p><span class="math display">\[
Y_{t}=\mu+\sum_{j=1}^{\infty} a_{j} Y_{t-j}+e_{t}
\]</span></p>
<p>for some coefficients <span class="math inline">\(\mu\)</span> and <span class="math inline">\(a_{j}\)</span>. The coefficients satisfy <span class="math inline">\(\sum_{k=0}^{\infty} k^{s}\left|a_{k}\right|&lt;\infty\)</span> so (14.23) is convergent.</p>
<p>Equation (14.23) is known as an infinite-order autoregressive representation with autoregressive coefficients <span class="math inline">\(a_{j}\)</span>.</p>
<p>A solution to the equation <span class="math inline">\(b(z)=0\)</span> is a root of the polynomial <span class="math inline">\(b(z)\)</span>. The assumption <span class="math inline">\(|b(z)|&gt;0\)</span> for <span class="math inline">\(|z| \leq 1\)</span> means that the roots of <span class="math inline">\(b(z)\)</span> lie outside the unit circle <span class="math inline">\(|z|=1\)</span> (the circle in the complex plane with radius one). Theorem <span class="math inline">\(14.19\)</span> makes the stronger restriction that <span class="math inline">\(|b(z)|\)</span> is bounded away from 0 for <span class="math inline">\(z\)</span> on or within the unit circle. The need for this strengthening is less intuitive but essentially excludes the possibility of an infinite number of roots outside but arbitrarily close to the unit circle. The summability assumption on the Wold coefficients ensures convergence of the autoregressive coefficients <span class="math inline">\(a_{j}\)</span>. To understand the restriction on the roots of <span class="math inline">\(b(z)\)</span> consider the simple case <span class="math inline">\(b(z)=1-b_{1} z\)</span>. (Below we call this a MA(1) model.) The requirement <span class="math inline">\(|b(z)| \geq \delta\)</span> for <span class="math inline">\(|z| \leq 1\)</span> means <span class="math inline">\({ }^{5}\left|b_{1}\right| \leq 1-\delta\)</span>. Thus the assumption in Theorem <span class="math inline">\(14.19\)</span> bounds the coefficient strictly below 1 . Now consider an infinite polynomial case <span class="math inline">\(b(z)=\prod_{j=1}^{\infty}\left(1-b_{j} z\right)\)</span>. The assumption in Theorem <span class="math inline">\(14.19\)</span> requires <span class="math inline">\(\sup _{j}\left|b_{j}\right|&lt;1\)</span>.</p>
<p>Theorem <span class="math inline">\(14.19\)</span> is attributed to Wiener and Masani (1958). For a recent treatment and proof see Corollary 6.1.17 of Politis and McElroy (2020). These authors (as is common in the literature) state their assumptions differently than we do in Theorem 14.19. First, instead of the condition on <span class="math inline">\(b(z)\)</span> they bound from below the spectral density function <span class="math inline">\(f(\lambda)\)</span> of <span class="math inline">\(Y_{t}\)</span>. We do not define the spectral density in this text so we restate their condition in terms of the linear process polynomial <span class="math inline">\(b(z)\)</span>. Second, instead of the condition on the Wold coefficients they require that the autocovariances satisfy <span class="math inline">\(\sum_{k=0}^{\infty} k^{s}|\gamma(k)|&lt;\infty\)</span>. This is implied by our stated summability condition on the <span class="math inline">\(b_{j}\)</span> (using the expression for <span class="math inline">\(\gamma(k)\)</span> in Section <span class="math inline">\(14.21\)</span> below and simplifying).</p>
</section>
<section id="linear-models" class="level2" data-number="14.19">
<h2 data-number="14.19" class="anchored" data-anchor-id="linear-models"><span class="header-section-number">14.19</span> Linear Models</h2>
<p>In the previous two sections we showed that any non-deterministic covariance stationary time series has the projection representation</p>
<p><span class="math display">\[
Y_{t}=\mu+\sum_{j=0}^{\infty} b_{j} e_{t-j}
\]</span></p>
<p>and under a restriction on the projection coefficients satisfies the autoregressive representation</p>
<p><span class="math display">\[
Y_{t}=\mu+\sum_{j=1}^{\infty} a_{j} Y_{t-j}+e_{t} .
\]</span></p>
<p>In both equations the errors <span class="math inline">\(e_{t}\)</span> are white noise projection errors. These representations help us understand that linear models can be used as approximations for stationary time series.</p>
<p>For the next several sections we reverse the analysis. We will assume a specific linear model and then study the properties of the resulting time series. In particular we will be seeking conditions under which the stated process is stationary. This helps us understand the properties of linear models. Throughout, we assume that the error <span class="math inline">\(e_{t}\)</span> is a strictly stationary and ergodic white noise process. This allows as a special case the stronger assumption that <span class="math inline">\(e_{t}\)</span> is i.i.d. but is less restrictive. In particular, it allows for conditional heteroskedasticity.</p>
</section>
<section id="moving-average-processes" class="level2" data-number="14.20">
<h2 data-number="14.20" class="anchored" data-anchor-id="moving-average-processes"><span class="header-section-number">14.20</span> Moving Average Processes</h2>
<p>The first-order moving average process, denoted MA(1), is</p>
<p><span class="math display">\[
Y_{t}=\mu+e_{t}+\theta e_{t-1}
\]</span></p>
<p>where <span class="math inline">\(e_{t}\)</span> is a strictly stationary and ergodic white noise process with var <span class="math inline">\(\left[e_{t}\right]=\sigma^{2}\)</span>. The model is called a “moving average” because <span class="math inline">\(Y_{t}\)</span> is a weighted average of the shocks <span class="math inline">\(e_{t}\)</span> and <span class="math inline">\(e_{t-1}\)</span>.</p>
<p><span class="math inline">\({ }^{5}\)</span> To see this, focus on the case <span class="math inline">\(b_{1} \geq 0\)</span>. The requirement <span class="math inline">\(\left|1-b_{1} z\right| \geq \delta\)</span> for <span class="math inline">\(|z| \leq 1\)</span> means <span class="math inline">\(\min _{|z| \leq 1}\left|1-b_{1} z\right|=1-b_{1} \geq \delta\)</span> or <span class="math inline">\(b_{1} \leq 1-\delta\)</span>. It is straightforward to calculate that a MA(1) has the following moments.</p>
<p><span class="math display">\[
\begin{aligned}
\mathbb{E}\left[Y_{t}\right] &amp;=\mu \\
\operatorname{var}\left[Y_{t}\right] &amp;=\left(1+\theta^{2}\right) \sigma^{2} \\
\gamma(1) &amp;=\theta \sigma^{2} \\
\rho(1) &amp;=\frac{\theta}{1+\theta^{2}} \\
\gamma(k) &amp;=\rho(k)=0, \quad k \geq 2 .
\end{aligned}
\]</span></p>
<p>Thus the MA(1) process has a non-zero first autocorrelation with the remainder zero.</p>
<p>A MA(1) process with <span class="math inline">\(\theta \neq 0\)</span> is serially correlated with each pair of adjacent observations <span class="math inline">\(\left(Y_{t-1}, Y_{t}\right)\)</span> correlated. If <span class="math inline">\(\theta&gt;0\)</span> the pair are positively correlated, while if <span class="math inline">\(\theta&lt;0\)</span> they are negatively correlated. The serial correlation is limited in that observations separated by multiple periods are mutually independent.</p>
<p>The <span class="math inline">\(\mathbf{q}^{t h}\)</span>-order moving average process, denoted <span class="math inline">\(\mathbf{M A}(\mathbf{q})\)</span>, is</p>
<p><span class="math display">\[
Y_{t}=\mu+\theta_{0} e_{t}+\theta_{1} e_{t-1}+\theta_{2} e_{t-2}+\cdots+\theta_{q} e_{t-q}
\]</span></p>
<p>where <span class="math inline">\(\theta_{0}=1\)</span>. It is straightforward to calculate that a MA(q) has the following moments.</p>
<p><span class="math display">\[
\begin{aligned}
\mathbb{E}\left[Y_{t}\right] &amp;=\mu \\
\operatorname{var}\left[Y_{t}\right] &amp;=\left(\sum_{j=0}^{q} \theta_{j}^{2}\right) \sigma^{2} \\
\gamma(k) &amp;=\left(\sum_{j=0}^{q-k} \theta_{j+k} \theta_{j}\right) \sigma^{2}, \quad k \leq q \\
\rho(k) &amp;=\frac{\sum_{j=0}^{q-k} \theta_{j+k} \theta_{j}}{\sum_{j=0}^{q} \theta_{j}^{2}} \\
\gamma(k) &amp;=\rho(k)=0, \quad k&gt;q .
\end{aligned}
\]</span></p>
<p>In particular, a MA(q) has <span class="math inline">\(q\)</span> non-zero autocorrelations with the remainder zero.</p>
<p>A MA(q) process <span class="math inline">\(Y_{t}\)</span> is strictly stationary and ergodic.</p>
<p>A MA(q) process with moderately large <span class="math inline">\(q\)</span> can have considerably more complicated dependence relations than a MA(1) process. One specific pattern which can be induced by a MA process is smoothing. Suppose that the coefficients <span class="math inline">\(\theta_{j}\)</span> all equal 1. Then <span class="math inline">\(Y_{t}\)</span> is a smoothed version of the shocks <span class="math inline">\(e_{t}\)</span>.</p>
<p>To illustrate, Figure <span class="math inline">\(14.3(\)</span> a) displays a plot of a simulated white noise (i.i.d. <span class="math inline">\(\mathrm{N}(0,1)\)</span> ) process with <span class="math inline">\(n=120\)</span> observations. Figure 14.3(b) displays a plot of a MA(8) process constructed with the same innovations, with <span class="math inline">\(\theta_{j}=1, j=1, \ldots, 8\)</span>. You can see that the white noise has no predictable behavior while the <span class="math inline">\(\mathrm{MA}(8)\)</span> is smooth.</p>
</section>
<section id="infinite-order-moving-average-process" class="level2" data-number="14.21">
<h2 data-number="14.21" class="anchored" data-anchor-id="infinite-order-moving-average-process"><span class="header-section-number">14.21</span> Infinite-Order Moving Average Process</h2>
<p>An infinite-order moving average process, denoted MA( <span class="math inline">\(\infty\)</span> ), also known as a linear process, is</p>
<p><span class="math display">\[
Y_{t}=\mu+\sum_{j=0}^{\infty} \theta_{j} e_{t-j}
\]</span></p>
<p><img src="images//2022_10_23_6047885e7d154c9f28afg-23.jpg" class="img-fluid"></p>
<ol type="a">
<li>White Noise</li>
</ol>
<p><img src="images//2022_10_23_6047885e7d154c9f28afg-23(1).jpg" class="img-fluid"></p>
<ol start="2" type="a">
<li>MA(8)</li>
</ol>
<p>Figure 14.3: White Noise and MA(8)</p>
<p>where <span class="math inline">\(e_{t}\)</span> is a strictly stationary and ergodic white noise process, <span class="math inline">\(\operatorname{var}\left[e_{t}\right]=\sigma^{2}\)</span>, and <span class="math inline">\(\sum_{j=0}^{\infty}\left|\theta_{j}\right|&lt;\infty\)</span>. From Theorem 14.6, <span class="math inline">\(Y_{t}\)</span> is strictly stationary and ergodic. A linear process has the following moments:</p>
<p><span class="math display">\[
\begin{aligned}
\mathbb{E}\left[Y_{t}\right] &amp;=\mu \\
\operatorname{var}\left[Y_{t}\right] &amp;=\left(\sum_{j=0}^{\infty} \theta_{j}^{2}\right) \sigma^{2} \\
\gamma(k) &amp;=\left(\sum_{j=0}^{\infty} \theta_{j+k} \theta_{j}\right) \sigma^{2} \\
\rho(k) &amp;=\frac{\sum_{j=0}^{\infty} \theta_{j+k} \theta_{j}}{\sum_{j=0}^{\infty} \theta_{j}^{2}} .
\end{aligned}
\]</span></p>
</section>
<section id="first-order-autoregressive-process" class="level2" data-number="14.22">
<h2 data-number="14.22" class="anchored" data-anchor-id="first-order-autoregressive-process"><span class="header-section-number">14.22</span> First-Order Autoregressive Process</h2>
<p>The first-order autoregressive process, denoted AR(1), is</p>
<p><span class="math display">\[
Y_{t}=\alpha_{0}+\alpha_{1} Y_{t-1}+e_{t}
\]</span></p>
<p>where <span class="math inline">\(e_{t}\)</span> is a strictly stationary and ergodic white noise process with var <span class="math inline">\(\left[e_{t}\right]=\sigma^{2}\)</span>. The AR(1) model is probably the single most important model in econometric time series analysis.</p>
<p>As a simple motivating example let <span class="math inline">\(Y_{t}\)</span> be is the employment level (number of jobs) in an economy. Suppose that a fixed fraction <span class="math inline">\(1-\alpha_{1}\)</span> of employees lose their job and a random number <span class="math inline">\(u_{t}\)</span> of new employees are hired each period. Setting <span class="math inline">\(\alpha_{0}=\mathbb{E}\left[u_{t}\right]\)</span> and <span class="math inline">\(e_{t}=u_{t}-\alpha_{0}\)</span>, this implies the law of motion (14.25).</p>
<p>To illustrate the behavior of the AR(1) process, Figure <span class="math inline">\(14.4\)</span> plots two simulated AR(1) processes. Each is generated using the white noise process <span class="math inline">\(e_{t}\)</span> displayed in Figure 14.3(a). The plot in Figure 14.4(a) sets</p>
<p><img src="images//2022_10_23_6047885e7d154c9f28afg-24.jpg" class="img-fluid"></p>
<ol type="a">
<li><span class="math inline">\(\operatorname{AR}(1)\)</span> with <span class="math inline">\(\alpha_{1}=0.5\)</span></li>
</ol>
<p><img src="images//2022_10_23_6047885e7d154c9f28afg-24(1).jpg" class="img-fluid"></p>
<ol start="2" type="a">
<li><span class="math inline">\(\operatorname{AR}(1)\)</span> with <span class="math inline">\(\alpha_{1}=0.95\)</span></li>
</ol>
<p>Figure 14.4: AR(1) Processes</p>
<p><span class="math inline">\(\alpha_{1}=0.5\)</span> and the plot in Figure 14.4(b) sets <span class="math inline">\(\alpha_{1}=0.95\)</span>. You can see how both are more smooth than the white noise process and that the smoothing increases with <span class="math inline">\(\alpha\)</span>.</p>
<p>Our first goal is to obtain conditions under which (14.25) is stationary. We can do so by showing that <span class="math inline">\(Y_{t}\)</span> can be written as a convergent linear process and then appealing to Theorem 14.5. To find a linear process representation for <span class="math inline">\(Y_{t}\)</span> we can use backward recursion. Notice that <span class="math inline">\(Y_{t}\)</span> in (14.25) depends on its previous value <span class="math inline">\(Y_{t-1}\)</span>. If we take (14.25) and lag it one period we find <span class="math inline">\(Y_{t-1}=\alpha_{0}+\alpha_{1} Y_{t-2}+e_{t-1}\)</span>. Substituting this into (14.25) we find</p>
<p><span class="math display">\[
\begin{aligned}
Y_{t} &amp;=\alpha_{0}+\alpha_{1}\left(\alpha_{0}+\alpha_{1} Y_{t-2}+e_{t-1}\right)+e_{t} \\
&amp;=\alpha_{0}+\alpha_{1} \alpha_{0}+\alpha_{1}^{2} Y_{t-2}+\alpha_{1} e_{t-1}+e_{t} .
\end{aligned}
\]</span></p>
<p>Similarly we can lag (14.31) twice to find <span class="math inline">\(Y_{t-2}=\alpha_{0}+\alpha_{1} Y_{t-3}+e_{t-2}\)</span> and can be used to substitute out <span class="math inline">\(Y_{t-2}\)</span>. Continuing recursively <span class="math inline">\(t\)</span> times, we find</p>
<p><span class="math display">\[
\begin{aligned}
Y_{t} &amp;=\alpha_{0}\left(1+\alpha_{1}+\alpha_{1}^{2}+\cdots+\alpha_{1}^{t-1}\right)+\alpha_{1}^{t} Y_{0}+\alpha_{1}^{t-1} e_{1}+\alpha_{1}^{t-2} e_{2}+\cdots+e_{t} \\
&amp;=\alpha_{0} \sum_{j=0}^{t-1} \alpha_{1}^{j}+\alpha_{1}^{t} Y_{0}+\sum_{j=0}^{t-1} \alpha_{1}^{j} e_{t-j} .
\end{aligned}
\]</span></p>
<p>Thus <span class="math inline">\(Y_{t}\)</span> equals an intercept plus the scaled initial condition <span class="math inline">\(\alpha_{1}^{t} Y_{0}\)</span> and the moving average <span class="math inline">\(\sum_{j=0}^{t-1} \alpha_{1}^{j} e_{t-j}\)</span>.</p>
<p>Now suppose we continue this recursion into the infinite past. By Theorem <span class="math inline">\(14.3\)</span> this converges if <span class="math inline">\(\sum_{j=0}^{\infty}\left|\alpha_{1}\right|^{j}&lt;\infty\)</span>. The limit is provided by the following well-known result.</p>
<p>Theorem <span class="math inline">\(14.20 \sum_{k=0}^{\infty} \beta^{k}=\frac{1}{1-\beta}\)</span> is absolutely convergent if <span class="math inline">\(|\beta|&lt;1\)</span> The series converges by the ratio test (see Theorem A.3 of Probability and Statistics for Economists). To find the limit,</p>
<p><span class="math display">\[
A=\sum_{k=0}^{\infty} \beta^{k}=1+\sum_{k=1}^{\infty} \beta^{k}=1+\beta \sum_{k=0}^{\infty} \beta^{k}=1+\beta A .
\]</span></p>
<p>Solving, we find <span class="math inline">\(A=1 /(1-\beta)\)</span>.</p>
<p>Thus the intercept in (14.26) converges to <span class="math inline">\(\alpha_{0} /\left(1-\alpha_{1}\right)\)</span>. We deduce the following:</p>
<p>Theorem 14.21 If <span class="math inline">\(\mathbb{E}\left|e_{t}\right|&lt;\infty\)</span> and <span class="math inline">\(\left|\alpha_{1}\right|&lt;1\)</span> then the AR(1) process (14.25) has the convergent representation</p>
<p><span class="math display">\[
Y_{t}=\mu+\sum_{j=0}^{\infty} \alpha_{1}^{j} e_{t-j}
\]</span></p>
<p>where <span class="math inline">\(\mu=\alpha_{0} /\left(1-\alpha_{1}\right)\)</span>. The AR(1) process <span class="math inline">\(Y_{t}\)</span> is strictly stationary and ergodic.</p>
<p>We can compute the moments of <span class="math inline">\(Y_{t}\)</span> from (14.27)</p>
<p><span class="math display">\[
\begin{gathered}
\mathbb{E}\left[Y_{t}\right]=\mu+\sum_{k=0}^{\infty} \alpha_{1}^{k} \mathbb{E}\left[e_{t-k}\right]=\mu \\
\operatorname{var}\left[Y_{t}\right]=\sum_{k=0}^{\infty} \alpha_{1}^{2 k} \operatorname{var}\left[e_{t-k}\right]=\frac{\sigma^{2}}{1-\alpha_{1}^{2}} .
\end{gathered}
\]</span></p>
<p>One way to calculate the moments is as follows. Apply expectations to both sides of (14.25)</p>
<p><span class="math display">\[
\mathbb{E}\left[Y_{t}\right]=\alpha_{0}+\alpha_{1} \mathbb{E}\left[Y_{t-1}\right]+\mathbb{E}\left[e_{t}\right]=\alpha_{0}+\alpha_{1} \mathbb{E}\left[Y_{t-1}\right] .
\]</span></p>
<p>Stationarity implies <span class="math inline">\(\mathbb{E}\left[Y_{t-1}\right]=\mathbb{E}\left[Y_{t}\right]\)</span>. Solving we find <span class="math inline">\(\mathbb{E}\left[Y_{t}\right]=\alpha_{0} /\left(1-\alpha_{1}\right)\)</span>. Similarly,</p>
<p><span class="math display">\[
\operatorname{var}\left[Y_{t}\right]=\operatorname{var}\left[\alpha Y_{t-1}+e_{t}\right]=\alpha_{1}^{2} \operatorname{var}\left[Y_{t-1}\right]+\operatorname{var}\left[e_{t}\right]=\alpha_{1}^{2} \operatorname{var}\left[Y_{t-1}\right]+\sigma^{2} .
\]</span></p>
<p>Stationarity implies <span class="math inline">\(\operatorname{var}\left[Y_{t-1}\right]=\operatorname{var}\left[Y_{t}\right]\)</span>. Solving we find <span class="math inline">\(\operatorname{var}\left[Y_{t}\right]=\sigma^{2} /\left(1-\alpha_{1}^{2}\right)\)</span>. This method is useful for calculation of autocovariances and autocorrelations. For simplicity set <span class="math inline">\(\alpha_{0}=0\)</span> so that <span class="math inline">\(\mathbb{E}\left[Y_{t}\right]=0\)</span> and <span class="math inline">\(\mathbb{E}\left[Y_{t}^{2}\right]=\operatorname{var}\left[Y_{t}\right]\)</span>. We find</p>
<p><span class="math display">\[
\gamma(1)=\mathbb{E}\left[Y_{t-1} Y_{t}\right]=\mathbb{E}\left[Y_{t-1}\left(\alpha_{1} Y_{t-1}+e_{t}\right)\right]=\alpha_{1} \operatorname{var}\left[Y_{t}\right]
\]</span></p>
<p>so</p>
<p><span class="math display">\[
\rho(1)=\gamma(1) / \operatorname{var}\left[Y_{t}\right]=\alpha_{1} .
\]</span></p>
<p>Furthermore,</p>
<p><span class="math display">\[
\gamma(k)=\mathbb{E}\left[Y_{t-k} Y_{t}\right]=\mathbb{E}\left[Y_{t-k}\left(\alpha_{1} Y_{t-1}+e_{t}\right)\right]=\alpha_{1} \gamma(k-1)
\]</span></p>
<p>By recursion we obtain</p>
<p><span class="math display">\[
\begin{aligned}
&amp;\gamma(k)=\alpha_{1}^{k} \operatorname{var}\left[Y_{t}\right] \\
&amp;\rho(k)=\alpha_{1}^{k} .
\end{aligned}
\]</span></p>
<p>Thus the AR(1) process with <span class="math inline">\(\alpha_{1} \neq 0\)</span> has non-zero autocorrelations of all orders which decay to zero geometrically as <span class="math inline">\(k\)</span> increases. For <span class="math inline">\(\alpha_{1}&gt;0\)</span> the autocorrelations are all positive. For <span class="math inline">\(\alpha_{1}&lt;0\)</span> the autocorrelations alternate in sign.</p>
<p>We can also express the AR(1) process using the lag operator notation:</p>
<p><span class="math display">\[
\left(1-\alpha_{1} \mathrm{~L}\right) Y_{t}=\alpha_{0}+e_{t}
\]</span></p>
<p>We can write this as <span class="math inline">\(\alpha(\mathrm{L}) Y_{t}=\alpha_{0}+e_{t}\)</span> where <span class="math inline">\(\alpha(\mathrm{L})=1-\alpha_{1} \mathrm{~L}\)</span>. We call <span class="math inline">\(\alpha(z)=1-\alpha_{1} z\)</span> the autoregressive polynomial of <span class="math inline">\(Y_{t}\)</span>.</p>
<p>This suggests an alternative way of obtaining the representation (14.27). We can invert the operator (1- <span class="math inline">\(\left.\alpha_{1} \mathrm{~L}\right)\)</span> to write <span class="math inline">\(Y_{t}\)</span> as a function of lagged <span class="math inline">\(e_{t}\)</span>. That is, suppose that the inverse operator <span class="math inline">\(\left(1-\alpha_{1} \mathrm{~L}\right)^{-1}\)</span> exists. Then we can use this operator on (14.28) to find</p>
<p><span class="math display">\[
Y_{t}=\left(1-\alpha_{1} \mathrm{~L}\right)^{-1}\left(1-\alpha_{1} \mathrm{~L}\right) Y_{t}=\left(1-\alpha_{1} \mathrm{~L}\right)^{-1}\left(\alpha_{0}+e_{t}\right) .
\]</span></p>
<p>What is the operator <span class="math inline">\(\left(1-\alpha_{1} \mathrm{~L}\right)^{-1}\)</span> ? Recall from Theorem <span class="math inline">\(14.20\)</span> that for <span class="math inline">\(|x|&lt;1\)</span>,</p>
<p><span class="math display">\[
\sum_{j=0}^{\infty} x^{j}=\frac{1}{1-x}=(1-x)^{-1} .
\]</span></p>
<p>Evaluate this expression at <span class="math inline">\(x=\alpha_{1} z\)</span>. We find</p>
<p><span class="math display">\[
\left(1-\alpha_{1} z\right)^{-1}=\sum_{j=0}^{\infty} \alpha_{1}^{j} z^{j} .
\]</span></p>
<p>Setting <span class="math inline">\(z=\mathrm{L}\)</span> this is</p>
<p><span class="math display">\[
\left(1-\alpha_{1} \mathrm{~L}\right)^{-1}=\sum_{j=0}^{\infty} \alpha_{1}^{j} \mathrm{~L}^{j} .
\]</span></p>
<p>Substituted into (14.29) we obtain</p>
<p><span class="math display">\[
\begin{aligned}
Y_{t} &amp;=\left(1-\alpha_{1} \mathrm{~L}\right)^{-1}\left(\alpha_{0}+e_{t}\right) \\
&amp;=\left(\sum_{j=0}^{\infty} \alpha^{j} \mathrm{~L}^{j}\right)\left(\alpha_{0}+e_{t}\right) \\
&amp;=\sum_{j=0}^{\infty} \alpha_{1}^{j} \mathrm{~L}^{j}\left(\alpha_{0}+e_{t}\right) \\
&amp;=\sum_{j=0}^{\infty} \alpha_{1}^{j}\left(\alpha_{0}+e_{t-j}\right) \\
&amp;=\frac{\alpha_{0}}{1-\alpha_{1}}+\sum_{j=0}^{\infty} \alpha_{1}^{j} e_{t-j}
\end{aligned}
\]</span></p>
<p>which is (14.27). This is valid for <span class="math inline">\(\left|\alpha_{1}\right|&lt;1\)</span>.</p>
<p>This illustrates another important concept. We say that a polynomial <span class="math inline">\(\alpha(z)\)</span> is invertible if</p>
<p><span class="math display">\[
\alpha(z)^{-1}=\sum_{j=0}^{\infty} a_{j} z^{j}
\]</span></p>
<p>is absolutely convergent. In particular, the <span class="math inline">\(\operatorname{AR}(1)\)</span> autoregressive polynomial <span class="math inline">\(\alpha(z)=1-\alpha_{1} z\)</span> is invertible if <span class="math inline">\(\left|\alpha_{1}\right|&lt;1\)</span>. This is the same condition as for stationarity of the AR(1) process. Invertibility turns out to be a useful property.</p>
</section>
<section id="unit-root-and-explosive-ar1-processes" class="level2" data-number="14.23">
<h2 data-number="14.23" class="anchored" data-anchor-id="unit-root-and-explosive-ar1-processes"><span class="header-section-number">14.23</span> Unit Root and Explosive AR(1) Processes</h2>
<p>The AR(1) process (14.25) is stationary if <span class="math inline">\(\left|\alpha_{1}\right|&lt;1\)</span>. What happens otherwise?</p>
<p>If <span class="math inline">\(\alpha_{0}=0\)</span> and <span class="math inline">\(\alpha_{1}=1\)</span> the model is known as a random walk.</p>
<p><span class="math display">\[
Y_{t}=Y_{t-1}+e_{t} .
\]</span></p>
<p>This is also called a unit root process, a martingale, or an integrated process. By back-substitution</p>
<p><span class="math display">\[
Y_{t}=Y_{0}+\sum_{j=1}^{t} e_{j} .
\]</span></p>
<p>Thus the initial condition does not disappear for large <span class="math inline">\(t\)</span>. Consequently the series is non-stationary. The autoregressive polynomial <span class="math inline">\(\alpha(z)=1-z\)</span> is not invertible, meaning that <span class="math inline">\(Y_{t}\)</span> cannot be written as a convergent function of the infinite past history of <span class="math inline">\(e_{t}\)</span>.</p>
<p>The stochastic behavior of a random walk is noticably different from a stationary AR(1) process. It wanders up and down with equal likelihood and is not mean-reverting. While it has no tendency to return to its previous values the wandering nature of a random walk can give the illusion of mean reversion. The difference is that a random walk will take a very large number of time periods to “revert”.</p>
<p><img src="images//2022_10_23_6047885e7d154c9f28afg-27.jpg" class="img-fluid"></p>
<ol type="a">
<li>Example 1</li>
</ol>
<p><img src="images//2022_10_23_6047885e7d154c9f28afg-27(1).jpg" class="img-fluid"></p>
<ol start="2" type="a">
<li>Example 2</li>
</ol>
<p>Figure 14.5: Random Walk Processes</p>
<p>To illustrate, Figure <span class="math inline">\(14.5\)</span> plots two independent random walk processes. The plot in panel (a) uses the innovations from Figure 14.3(a). The plot in panel (b) uses an independent set of i.i.d. <span class="math inline">\(N(0,1)\)</span> errors. You can see that the plot in panel (a) appears similar to the MA(8) and AR(1) plots in the sense that the series is smooth with long swings, but the difference is that the series does not return to a longterm mean. It appears to have drifted down over time. The plot in panel (b) appears to have quite different behavior, falling dramatically over a 5-year period, and then appearing to stabilize. These are both common behaviors of random walk processes. If <span class="math inline">\(\alpha_{1}&gt;1\)</span> the process is explosive. The model (14.25) with <span class="math inline">\(\alpha_{1}&gt;1\)</span> exhibits exponential growth and high sensitivity to initial conditions. Explosive autoregressive processes do not seem to be good descriptions for most economic time series. While aggregate time series such as the GDP process displayed in Figure 14.1 (a) exhibit a similar exponential growth pattern, the exponential growth can typically be removed by taking logarithms.</p>
<p>The case <span class="math inline">\(\alpha_{1}&lt;-1\)</span> induces explosive oscillating growth and does not appear to be empirically relevant for economic applications.</p>
</section>
<section id="second-order-autoregressive-process" class="level2" data-number="14.24">
<h2 data-number="14.24" class="anchored" data-anchor-id="second-order-autoregressive-process"><span class="header-section-number">14.24</span> Second-Order Autoregressive Process</h2>
<p>The second-order autoregressive process, denoted <span class="math inline">\(\mathbf{A R}(2)\)</span>, is</p>
<p><span class="math display">\[
Y_{t}=\alpha_{0}+\alpha_{1} Y_{t-1}+\alpha_{2} Y_{t-2}+e_{t}
\]</span></p>
<p>where <span class="math inline">\(e_{t}\)</span> is a strictly stationary and ergodic white noise process. The dynamic patterns of an AR(2) process are more complicated than an AR(1) process.</p>
<p>As a motivating example consider the multiplier-accelerator model of Samuelson (1939). It might be a bit dated as a model but it is simple so hopefully makes the point. Aggregate output (in an economy with no trade) is defined as <span class="math inline">\(Y_{t}=\)</span> Consumption <span class="math inline">\(_{t}+\)</span> Investment <span class="math inline">\(_{t}+\)</span> Gov <span class="math inline">\(_{t}\)</span>. Suppose that individuals make their consumption decisions on the previous period’s income Consumption <span class="math inline">\(t=b Y_{t-1}\)</span>, firms make their investment decisions on the change in consumption Investment <span class="math inline">\(t_{t}=d \Delta C_{t}\)</span>, and government spending is random <span class="math inline">\(G o v_{t}=a+e_{t}\)</span>. Then aggregate output follows</p>
<p><span class="math display">\[
Y_{t}=a+b(1+d) Y_{t-1}-b d Y_{t-2}+e_{t}
\]</span></p>
<p>which is an <span class="math inline">\(\operatorname{AR}(2)\)</span> process.</p>
<p>Using the lag operator we can write (14.31) as</p>
<p><span class="math display">\[
Y_{t}-\alpha_{1} \mathrm{~L} Y_{t}-\alpha_{2} \mathrm{~L}^{2} Y_{t}=\alpha_{0}+e_{t}
\]</span></p>
<p>or <span class="math inline">\(\alpha(\mathrm{L}) Y_{t}=\alpha_{0}+e_{t}\)</span> where <span class="math inline">\(\alpha(\mathrm{L})=1-\alpha_{1} \mathrm{~L}-\alpha_{2} \mathrm{~L}^{2}\)</span>. We call <span class="math inline">\(\alpha(z)\)</span> the autoregressive polynomial of <span class="math inline">\(Y_{t}\)</span>.</p>
<p>We would like to find the conditions for the stationarity of <span class="math inline">\(Y_{t}\)</span>. It turns out that it is convenient to transform the process (14.31) into a VAR(1) process (to be studied in the next chapter). Set <span class="math inline">\(\widetilde{Y}_{t}=\left(Y_{t}, Y_{t-1}\right)^{\prime}\)</span>, which is stationary if and only if <span class="math inline">\(Y_{t}\)</span> is stationary. Equation (14.31) implies that <span class="math inline">\(\widetilde{Y}_{t}\)</span> satisfies</p>
<p><span class="math display">\[
\left(\begin{array}{c}
Y_{t} \\
Y_{t-1}
\end{array}\right)=\left(\begin{array}{cc}
\alpha_{1} &amp; \alpha_{2} \\
1 &amp; 0
\end{array}\right)\left(\begin{array}{c}
Y_{t-1} \\
Y_{t-2}
\end{array}\right)+\left(\begin{array}{c}
a_{0}+e_{t} \\
0
\end{array}\right)
\]</span></p>
<p>or</p>
<p><span class="math display">\[
\widetilde{Y}_{t}=\boldsymbol{A} \widetilde{Y}_{t-1}+\widetilde{e}_{t}
\]</span></p>
<p>where <span class="math inline">\(\boldsymbol{A}=\left(\begin{array}{cc}\alpha_{1} &amp; \alpha_{2} \\ 1 &amp; 0\end{array}\right)\)</span> and <span class="math inline">\(\widetilde{e}_{t}=\left(a_{0}+e_{t}, 0\right)^{\prime}\)</span>. Equation (14.33) falls in the class of VAR(1) models studied in Section 15.6. Theorem <span class="math inline">\(15.6\)</span> shows that the <span class="math inline">\(\operatorname{VAR}(1)\)</span> process is strictly stationary and ergodic if the innovations satisfy <span class="math inline">\(\mathbb{E}\left\|\widetilde{e}_{t}\right\|&lt;\infty\)</span> and all eigenvalues <span class="math inline">\(\lambda\)</span> of <span class="math inline">\(\boldsymbol{A}\)</span> are less than one in absolute value. The eigenvalues satisfy <span class="math inline">\(\operatorname{det}\left(\boldsymbol{A}-\boldsymbol{I}_{2} \lambda\right)=0\)</span>, where</p>
<p><span class="math display">\[
\operatorname{det}\left(\boldsymbol{A}-\boldsymbol{I}_{2} \lambda\right)=\operatorname{det}\left(\begin{array}{cc}
\alpha_{1}-\lambda &amp; \alpha_{2} \\
1 &amp; -\lambda
\end{array}\right)=\lambda^{2}-\lambda \alpha_{1}-\alpha_{2}=\lambda^{2} \alpha(1 / \lambda)
\]</span></p>
<p>and <span class="math inline">\(\alpha(z)=1-\alpha_{1} z-\alpha_{2} z^{2}\)</span> is the autoregressive polynomial. Thus the eigenvalues satisfy <span class="math inline">\(\alpha(1 / \lambda)=0\)</span>. Factoring the autoregressive polynomial as <span class="math inline">\(\alpha(z)=\left(1-\lambda_{1} z\right)\left(1-\lambda_{2} z\right)\)</span> the solutions <span class="math inline">\(\alpha(1 / \lambda)=0\)</span> must equal <span class="math inline">\(\lambda_{1}\)</span> and <span class="math inline">\(\lambda_{2}\)</span>. The quadratic formula shows that these equal</p>
<p><span class="math display">\[
\lambda_{j}=\frac{\alpha_{1} \pm \sqrt{\alpha_{1}^{2}+4 \alpha_{2}}}{2} .
\]</span></p>
<p>These eigenvalues are real if <span class="math inline">\(\alpha_{1}^{2}+4 \alpha_{2} \geq 0\)</span> and are complex conjugates otherwise. The AR(2) process is stationary if the solutions (14.34) satisfy <span class="math inline">\(\left|\lambda_{j}\right|&lt;1\)</span>.</p>
<p><img src="images//2022_10_23_6047885e7d154c9f28afg-29.jpg" class="img-fluid"></p>
<p>Figure 14.6: Stationarity Region for <span class="math inline">\(\operatorname{AR}(2)\)</span></p>
<p>Using (14.34) to solve for the AR coefficients in terms of the eigenvalues we find <span class="math inline">\(\alpha_{1}=\lambda_{1}+\lambda_{2}\)</span> and <span class="math inline">\(\alpha_{2}=-\lambda_{1} \lambda_{2}\)</span>. With some algebra (the details are deferred to Section 14.47) we can show that <span class="math inline">\(\left|\lambda_{1}\right|&lt;1\)</span> and <span class="math inline">\(\left|\lambda_{2}\right|&lt;1\)</span> iff the following restrictions hold on the autoregressive coefficients:</p>
<p><span class="math display">\[
\begin{aligned}
\alpha_{1}+\alpha_{2} &amp;&lt;1 \\
\alpha_{2}-\alpha_{1}&lt;1 \\
\alpha_{2} &amp;&gt;-1 .
\end{aligned}
\]</span></p>
<p>These restrictions describe a triangle in <span class="math inline">\(\left(\alpha_{1}, \alpha_{2}\right)\)</span> space which is shown in Figure 14.6. Coefficients within this triangle correspond to a stationary <span class="math inline">\(\operatorname{AR}(2)\)</span> process.</p>
<p>Take the Samuelson multiplier-accelerator model (14.32). You can calculate that (14.35)-(14.37) are satisfied (and thus the process is strictly stationary) if <span class="math inline">\(0 \leq b&lt;1\)</span> and <span class="math inline">\(0 \leq d \leq 1\)</span>, which are reasonable restrictions on the model parameters. The most important restriction is <span class="math inline">\(b&lt;1\)</span>, which in the language of old-school macroeconomics is that the marginal propensity to consume out of income is less than one.</p>
<p>Furthermore, the triangle is divided into two regions as marked in Figure 14.6: the region above the parabola <span class="math inline">\(\alpha_{1}^{2}+4 \alpha_{2}=0\)</span> producing real eigenvalues <span class="math inline">\(\lambda_{j}\)</span>, and the region below the parabola producing complex eigenvalues <span class="math inline">\(\lambda_{j}\)</span>. This is interesting because when the eigenvalues are complex the autocorrelations of <span class="math inline">\(Y_{t}\)</span> display damped oscillations. For this reason the dynamic patterns of an AR(2) can be much more complicated than those of an AR(1).</p>
<p>Again, take the Samuelson multiplier-accelerator model (14.32). You can calculate that if <span class="math inline">\(b \geq 0\)</span>, the model has real eigenvalues iff <span class="math inline">\(b \geq 4 d /(1+d)^{2}\)</span>, which holds for <span class="math inline">\(b\)</span> large and <span class="math inline">\(d\)</span> small, which are “stable” parameterizations. On the other hand, the model has complex eigenvalues (and thus oscillations) for sufficiently small <span class="math inline">\(b\)</span> and large <span class="math inline">\(d\)</span>.</p>
<p>Theorem 14.22 If <span class="math inline">\(\mathbb{E}\left|e_{t}\right|&lt;\infty\)</span> and <span class="math inline">\(\left|\lambda_{j}\right|&lt;1\)</span> for <span class="math inline">\(\lambda_{j}\)</span> defined in (14.34), or equivalently if the inequalities (14.35)-(14.37) hold, then the <span class="math inline">\(\mathrm{AR}(2)\)</span> process (14.31) is absolutely convergent, strictly stationary, and ergodic.</p>
<p>The proof is presented in Section <span class="math inline">\(14.47\)</span>.</p>
<p><img src="images//2022_10_23_6047885e7d154c9f28afg-30.jpg" class="img-fluid"></p>
<ol type="a">
<li><span class="math inline">\(\operatorname{AR}(2)\)</span></li>
</ol>
<p><img src="images//2022_10_23_6047885e7d154c9f28afg-30(1).jpg" class="img-fluid"></p>
<ol start="2" type="a">
<li><span class="math inline">\(\operatorname{AR}(2)\)</span> with Complex Roots</li>
</ol>
<p>Figure 14.7: AR(2) Processes</p>
<p>To illustrate, Figure <span class="math inline">\(14.7\)</span> displays two simulated AR(2) processes. The plot in panel (a) sets <span class="math inline">\(\alpha_{1}=\alpha_{2}=\)</span> 0.4. These coefficients produce real factors so the process displays behavior similar to that of the AR(1) processes. The plot in panel (b) sets <span class="math inline">\(\alpha_{1}=1.3\)</span> and <span class="math inline">\(\alpha_{2}=-0.8\)</span>. These coefficients produce complex factors so the process displays oscillations.</p>
</section>
<section id="arp-processes" class="level2" data-number="14.25">
<h2 data-number="14.25" class="anchored" data-anchor-id="arp-processes"><span class="header-section-number">14.25</span> AR(p) Processes</h2>
<p>The <span class="math inline">\(\mathbf{p}^{\text {th }}\)</span>-order autoregressive process, denoted <span class="math inline">\(\mathbf{A R}(\mathbf{p})\)</span>, is</p>
<p><span class="math display">\[
Y_{t}=\alpha_{0}+\alpha_{1} Y_{t-1}+\alpha_{2} Y_{t-2}+\cdots+\alpha_{p} Y_{t-p}+e_{t}
\]</span></p>
<p>where <span class="math inline">\(e_{t}\)</span> is a strictly stationary and ergodic white noise process.</p>
<p>Using the lag operator,</p>
<p><span class="math display">\[
Y_{t}-\alpha_{1} \mathrm{~L} Y_{t}-\alpha_{2} \mathrm{~L}^{2} Y_{t}-\cdots-\alpha_{p} \mathrm{~L}^{p} Y_{t}=\alpha_{0}+e_{t}
\]</span></p>
<p>or <span class="math inline">\(\alpha(\mathrm{L}) Y_{t}=\alpha_{0}+e_{t}\)</span> where</p>
<p><span class="math display">\[
\alpha(\mathrm{L})=1-\alpha_{1} \mathrm{~L}-\alpha_{2} \mathrm{~L}^{2}-\cdots-\alpha_{p} \mathrm{~L}^{p} .
\]</span></p>
<p>We call <span class="math inline">\(\alpha(z)\)</span> the autoregressive polynomial of <span class="math inline">\(Y_{t}\)</span>.</p>
<p>We find conditions for the stationarity of <span class="math inline">\(Y_{t}\)</span> by a technique similar to that used for the AR(2) process. Set <span class="math inline">\(\widetilde{Y}_{t}=\left(Y_{t}, Y_{t-1}, \ldots, Y_{t-p+1}\right)^{\prime}\)</span> and <span class="math inline">\(\widetilde{e}_{t}=\left(a_{0}+e_{t}, 0, \ldots, 0\right)^{\prime}\)</span>. Equation (14.38) implies that <span class="math inline">\(\widetilde{Y}_{t}\)</span> satisfies the VAR(1) equation (14.33) with</p>
<p><span class="math display">\[
\boldsymbol{A}=\left(\begin{array}{ccccc}
\alpha_{1} &amp; \alpha_{2} &amp; \cdots &amp; \alpha_{p-1} &amp; \alpha_{p} \\
1 &amp; 0 &amp; \cdots &amp; 0 &amp; 0 \\
0 &amp; 1 &amp; \cdots &amp; 0 &amp; 0 \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots &amp; \vdots \\
0 &amp; 0 &amp; \cdots &amp; 1 &amp; 0
\end{array}\right)
\]</span></p>
<p>As shown in the proof of Theorem <span class="math inline">\(14.23\)</span> below, the eigenvalues <span class="math inline">\(\lambda_{j}\)</span> of <span class="math inline">\(\boldsymbol{A}\)</span> are the reciprocals of the roots <span class="math inline">\(r_{j}\)</span> of the autoregressive polynomial (14.39). The roots <span class="math inline">\(r_{j}\)</span> are the solutions to <span class="math inline">\(\alpha\left(r_{j}\right)=0\)</span>. Theorem <span class="math inline">\(15.6\)</span> shows that stationarity of <span class="math inline">\(\widetilde{Y}_{t}\)</span> holds if the eigenvalues <span class="math inline">\(\lambda_{j}\)</span> are less than one in absolute value, or equivalently when the roots <span class="math inline">\(r_{j}\)</span> are greater than one in absolute value. For complex numbers the equation <span class="math inline">\(|z|=1\)</span> defines the unit circle (the circle with radius of unity). We therefore say that ” <span class="math inline">\(z\)</span> lies outside the unit circle” if <span class="math inline">\(|z|&gt;1\)</span>.</p>
<p>Theorem 14.23 If <span class="math inline">\(\mathbb{E}\left|e_{t}\right|&lt;\infty\)</span> and all roots of <span class="math inline">\(\alpha(z)\)</span> lie outside the unit circle then the AR(p) process (14.38) is absolutely convergent, strictly stationary, and ergodic.</p>
<p>When the roots of <span class="math inline">\(\alpha(z)\)</span> lie outside the unit circle then the polynomial <span class="math inline">\(\alpha(z)\)</span> is invertible. Inverting the autoregressive representation <span class="math inline">\(\alpha(\mathrm{L}) Y_{t}=\alpha_{0}+e_{t}\)</span> we obtain an infinite-order moving average representation</p>
<p><span class="math display">\[
Y_{t}=\mu+b(\mathrm{~L}) e_{t}
\]</span></p>
<p>where</p>
<p><span class="math display">\[
b(z)=\alpha(z)^{-1}=\sum_{j=0}^{\infty} b_{j} z^{j}
\]</span></p>
<p>and <span class="math inline">\(\mu=\alpha(1)^{-1} a_{0}\)</span>.</p>
<p>We have the following characterization of the moving average coefficients. Theorem 14.24 If all roots <span class="math inline">\(r_{j}\)</span> of the autoregressive polynomial <span class="math inline">\(\alpha(z)\)</span> satisfy <span class="math inline">\(\left|r_{j}\right|&gt;1\)</span> then (14.41) holds with <span class="math inline">\(\left|b_{j}\right| \leq(j+1)^{p} \lambda^{j}\)</span> and <span class="math inline">\(\sum_{j=0}^{\infty}\left|b_{j}\right|&lt;\infty\)</span> where <span class="math inline">\(\lambda=\max _{1 \leq j \leq p}\left|r_{j}^{-1}\right|&lt;1\)</span></p>
<p>The proof is presented in Section <span class="math inline">\(14.47\)</span>.</p>
</section>
<section id="impulse-response-function" class="level2" data-number="14.26">
<h2 data-number="14.26" class="anchored" data-anchor-id="impulse-response-function"><span class="header-section-number">14.26</span> Impulse Response Function</h2>
<p>The coefficients of the moving average representation</p>
<p><span class="math display">\[
\begin{aligned}
Y_{t} &amp;=b(\mathrm{~L}) e_{t} \\
&amp;=\sum_{j=0}^{\infty} b_{j} e_{t-j} \\
&amp;=b_{0} e_{t}+b_{1} e_{t-1}+b_{2} e_{t-2}+\cdots
\end{aligned}
\]</span></p>
<p>are known among economists as the impulse response function (IRF). Often the IRF is scaled by the standard deviation of <span class="math inline">\(e_{t}\)</span>. We discuss this scaling at the end of the section. In linear models the impulse response function is defined as the change in <span class="math inline">\(Y_{t+j}\)</span> due to a shock at time <span class="math inline">\(t\)</span>. This is</p>
<p><span class="math display">\[
\frac{\partial}{\partial e_{t}} Y_{t+j}=b_{j} .
\]</span></p>
<p>This means that the coefficient <span class="math inline">\(b_{j}\)</span> can be interpreted as the magnitude of the impact of a time <span class="math inline">\(t\)</span> shock on the time <span class="math inline">\(t+j\)</span> variable. Plots of <span class="math inline">\(b_{j}\)</span> can be used to assess the time-propagation of shocks.</p>
<p>It is desirable to have a convenient method to calculate the impulse responses <span class="math inline">\(b_{j}\)</span> from the coefficients of an autoregressive model (14.38). There are two methods which we now describe.</p>
<p>The first uses a simple recursion. In the linear <span class="math inline">\(\operatorname{AR}(\mathrm{p})\)</span> model, we can see that the coefficient <span class="math inline">\(b_{j}\)</span> is the simple derivative</p>
<p><span class="math display">\[
b_{j}=\frac{\partial}{\partial e_{t}} Y_{t+j}=\frac{\partial}{\partial e_{0}} Y_{j}
\]</span></p>
<p>We can calculate <span class="math inline">\(b_{j}\)</span> by generating a history and perturbing the shock <span class="math inline">\(e_{0}\)</span>. Since this calculation is unaffected by all other shocks we can simply set <span class="math inline">\(e_{t}=0\)</span> for <span class="math inline">\(t \neq 0\)</span> and set <span class="math inline">\(e_{0}=1\)</span>. This implies the recursion</p>
<p><span class="math display">\[
\begin{aligned}
b_{0} &amp;=1 \\
b_{1} &amp;=\alpha_{1} b_{0} \\
b_{2} &amp;=\alpha_{1} b_{1}+\alpha_{2} b_{0} \\
&amp; \vdots \\
b_{j} &amp;=\alpha_{1} b_{j-1}+\alpha_{2} b_{j-2}+\cdots+\alpha_{p} b_{j-p} .
\end{aligned}
\]</span></p>
<p>This recursion is conveniently calculated by the following simulation. Set <span class="math inline">\(Y_{t}=0\)</span> for <span class="math inline">\(t \leq 0\)</span>. Set <span class="math inline">\(e_{0}=1\)</span> and <span class="math inline">\(e_{t}=0\)</span> for <span class="math inline">\(t \geq 1\)</span>. Generate <span class="math inline">\(Y_{t}\)</span> for <span class="math inline">\(t \geq 0\)</span> by <span class="math inline">\(Y_{t}=\alpha_{1} Y_{t-1}+\alpha_{2} Y_{t-2}+\cdots+\alpha_{p} Y_{t-p}+e_{t}\)</span>. Then <span class="math inline">\(Y_{j}=b_{j}\)</span>.</p>
<p>A second method uses the vector representation (14.33) of the AR(p) model with coefficient matrix (14.40). By recursion</p>
<p><span class="math display">\[
\widetilde{Y}_{t}=\sum_{j=0}^{\infty} \boldsymbol{A}^{j} \widetilde{e}_{t-j}
\]</span></p>
<p>Here, <span class="math inline">\(\boldsymbol{A}^{j}=\boldsymbol{A} \cdots \boldsymbol{A}\)</span> means the <span class="math inline">\(\boldsymbol{j}^{t h}\)</span> matrix product of <span class="math inline">\(\boldsymbol{A}\)</span> with itself. Setting <span class="math inline">\(S=(1,0, \ldots 0)^{\prime}\)</span> we find</p>
<p><span class="math display">\[
Y_{t}=\sum_{j=0}^{\infty} S^{\prime} A^{j} S e_{t-j} .
\]</span></p>
<p>By linearity</p>
<p><span class="math display">\[
b_{j}=\frac{\partial}{\partial e_{t}} Y_{t+j}=S^{\prime} A^{j} S .
\]</span></p>
<p>Thus the coefficient <span class="math inline">\(b_{j}\)</span> can be calculated by forming the matrix <span class="math inline">\(\boldsymbol{A}\)</span>, its <span class="math inline">\(j\)</span>-fold product <span class="math inline">\(\boldsymbol{A}^{j}\)</span>, and then taking the upper-left element.</p>
<p>As mentioned at the beginning of the section it is often desirable to scale the IRF so that it is the response to a one-deviation shock. Let <span class="math inline">\(\sigma^{2}=\operatorname{var}\left[e_{t}\right]\)</span> and define <span class="math inline">\(\varepsilon_{t}=e_{t} / \sigma\)</span> which has unit variance. Then the IRF at lag <span class="math inline">\(j\)</span> is</p>
<p><span class="math display">\[
\operatorname{IRF}_{j}=\frac{\partial}{\partial \varepsilon_{t}} Y_{t+j}=\sigma b_{j} .
\]</span></p>
</section>
<section id="arma-and-arima-processes" class="level2" data-number="14.27">
<h2 data-number="14.27" class="anchored" data-anchor-id="arma-and-arima-processes"><span class="header-section-number">14.27</span> ARMA and ARIMA Processes</h2>
<p>The autoregressive-moving-average process, denoted ARMA(p,q), is</p>
<p><span class="math display">\[
Y_{t}=\alpha_{0}+\alpha_{1} Y_{t-1}+\alpha_{2} Y_{t-2}+\cdots+\alpha_{p} Y_{t-p}+\theta_{0} e_{t}+\theta_{1} e_{t-1}+\theta_{2} e_{t-2}+\cdots+\theta_{q} e_{t-q}
\]</span></p>
<p>where <span class="math inline">\(e_{t}\)</span> is a strictly stationary and erogodic white noise process. It can be written using lag operator notation as <span class="math inline">\(\alpha(\mathrm{L}) Y_{t}=\alpha_{0}+\theta(\mathrm{L}) e_{t}\)</span>.</p>
<p>Theorem 14.25 The ARMA(p,q) process (14.43) is strictly stationary and ergodic if all roots of <span class="math inline">\(\alpha(z)\)</span> lie outside the unit circle. In this case we can write</p>
<p><span class="math display">\[
Y_{t}=\mu+b(\mathrm{~L}) e_{t}
\]</span></p>
<p>where <span class="math inline">\(b_{j}=O\left(j^{p} \beta^{j}\right)\)</span> and <span class="math inline">\(\sum_{j=0}^{\infty}\left|b_{j}\right|&lt;\infty\)</span>.</p>
<p>The process <span class="math inline">\(Y_{t}\)</span> follows an autoregressive-integrated moving-average process, denoted ARIMA(p,d,q), if <span class="math inline">\(\Delta^{d} Y_{t}\)</span> is ARMA(p,q). It can be written using lag operator notation as <span class="math inline">\(\alpha(\mathrm{L})(1-\mathrm{L})^{d} Y_{t}=\alpha_{0}+\theta(\mathrm{L}) e_{t}\)</span>.</p>
</section>
<section id="mixing-properties-of-linear-processes" class="level2" data-number="14.28">
<h2 data-number="14.28" class="anchored" data-anchor-id="mixing-properties-of-linear-processes"><span class="header-section-number">14.28</span> Mixing Properties of Linear Processes</h2>
<p>There is a considerable probability literature investigating the mixing properties of time series processes. One challenge is that as autoregressive processes depend on the infinite past sequence of innovations <span class="math inline">\(e_{t}\)</span> it is not immediately obvious if they satisfy the mixing conditions.</p>
<p>In fact, a simple AR(1) is not necessarily mixing. A counter-example was developed by Andrews (1984). He showed that if the error <span class="math inline">\(e_{t}\)</span> has a two-point discrete distribution then an AR(1) is not strong mixing. The reason is that a discrete innovation combined with the autoregressive structure means that by observing <span class="math inline">\(Y_{t}\)</span> you can deduce with near certainty the past history of the shocks <span class="math inline">\(e_{t}\)</span>. The example seems rather special but shows the need to be careful with the theory. The intuition stemming from Andrews’ example is that for an autoregressive process to be mixing it is necessary for the errors <span class="math inline">\(e_{t}\)</span> to be continuous.</p>
<p>A useful characterization was provided by Pham and Tran (1985).</p>
<p>Theorem 14.26 Suppose that <span class="math inline">\(Y_{t}=\mu+\sum_{j=0}^{\infty} \theta_{j} e_{t-j}\)</span> satisfies the following conditions:</p>
<ol type="1">
<li><span class="math inline">\(e_{t}\)</span> is i.i.d. with <span class="math inline">\(\mathbb{E}\left|e_{t}\right|^{r}&lt;\infty\)</span> for some <span class="math inline">\(r&gt;0\)</span> and density <span class="math inline">\(f(x)\)</span> which satisfies</li>
</ol>
<p><span class="math display">\[
\int_{-\infty}^{\infty}|f(x-u)-f(x)| d x \leq C|u|
\]</span></p>
<p>for some <span class="math inline">\(C&lt;\infty\)</span>.</p>
<p> 1. All roots of <span class="math inline">\(\theta(z)=0\)</span> lie outside the unit circle and <span class="math inline">\(\sum_{j=0}^{\infty}\left|\theta_{j}\right|&lt;\infty\)</span>.</p>
<ol start="2" type="1">
<li><span class="math inline">\(\sum_{k=1}^{\infty}\left(\sum_{j=k}^{\infty}\left|\theta_{j}\right|\right)^{r /(1+r)}&lt;\infty\)</span>.</li>
</ol>
<p>Then for some <span class="math inline">\(B&lt;\infty\)</span></p>
<p><span class="math display">\[
\alpha(\ell) \leq 4 \beta(\ell) \leq B \sum_{k=\ell}^{\infty}\left(\sum_{j=k}^{\infty}\left|\theta_{j}\right|\right)^{r /(1+r)}
\]</span></p>
<p>and <span class="math inline">\(Y_{t}\)</span> is absolutely regular and strong mixing.</p>
<p>The condition (14.44) is rather unusual, but specifies that <span class="math inline">\(e_{t}\)</span> has a smooth density. This rules out Andrews’ counter-example.</p>
<p>The summability condition on the coefficients in part 3 involves a trade-off with the number of moments <span class="math inline">\(r\)</span>. If <span class="math inline">\(e_{t}\)</span> has all moments finite (e.g.&nbsp;normal errors) then we can set <span class="math inline">\(r=\infty\)</span> and this condition simplifies to <span class="math inline">\(\sum_{k=1}^{\infty} k\left|\theta_{k}\right|&lt;\infty\)</span>. For any finite <span class="math inline">\(r\)</span> the summability condition holds if <span class="math inline">\(\theta_{j}\)</span> has geometric decay.</p>
<p>It is instructive to deduce how the decay in the coefficients <span class="math inline">\(\theta_{j}\)</span> affects the rate for the mixing coefficients <span class="math inline">\(\alpha(\ell)\)</span>. If <span class="math inline">\(\left|\theta_{j}\right| \leq O\left(j^{-\eta}\right)\)</span> then <span class="math inline">\(\sum_{j=k}^{\infty}\left|\theta_{j}\right| \leq O\left(k^{-(\eta-1)}\right)\)</span> so the rate is <span class="math inline">\(\alpha(\ell) \leq 4 \beta(\ell) \leq O\left(\ell^{-s}\right)\)</span> for <span class="math inline">\(s=(\eta-1) r /(1+r)-1\)</span>. Mixing requires <span class="math inline">\(s&gt;0\)</span>, which holds for sufficiently large <span class="math inline">\(\eta\)</span>. For example, if <span class="math inline">\(r=4\)</span> it holds for <span class="math inline">\(\eta&gt;9 / 4\)</span>.</p>
<p>The primary message from this section is that linear processes, including autoregressive and ARMA processes, are mixing if the innovations satisfy suitable conditions. The mixing coefficients decay at rates related to the decay rates of the moving average coefficients.</p>
</section>
<section id="identification" class="level2" data-number="14.29">
<h2 data-number="14.29" class="anchored" data-anchor-id="identification"><span class="header-section-number">14.29</span> Identification</h2>
<p>The parameters of a model are identified if the parameters are uniquely determined by the probability distribution of the observations. In the case of linear time series analysis we typically focus on the first two moments of the observations (means, variances, covariances). We therefore say that the coefficients of a stationary MA, AR, or ARMA model are identified if they are uniquely determined by the autocorrelation function. That is, given the autocorrelation function <span class="math inline">\(\rho(k)\)</span>, are the coefficients unique? It turns out that the answer is that MA and ARMA models are generally not identified. Identification is achieved by restricting the class of polynomial operators. In contrast, AR models are generally identified.</p>
<p>Let us start with the MA(1) model</p>
<p><span class="math display">\[
Y_{t}=e_{t}+\theta e_{t-1} .
\]</span></p>
<p>It has first-order autocorrelation</p>
<p><span class="math display">\[
\rho(1)=\frac{\theta}{1+\theta^{2}} .
\]</span></p>
<p>Set <span class="math inline">\(\omega=1 / \theta\)</span>. Then</p>
<p><span class="math display">\[
\frac{\omega}{1+\omega^{2}}=\frac{1 / \omega}{1+(1 / \omega)^{2}}=\frac{\theta}{1+\theta^{2}}=\rho(1) .
\]</span></p>
<p>Thus the MA(1) model with coefficient <span class="math inline">\(\omega=1 / \theta\)</span> produces the same autocorrelations as the MA(1) model with coefficient <span class="math inline">\(\theta\)</span>. For example, <span class="math inline">\(\theta=1 / 2\)</span> and <span class="math inline">\(\omega=2\)</span> each yield <span class="math inline">\(\rho(1)=2 / 5\)</span>. There is no empirical way to distinguish between the models <span class="math inline">\(Y_{t}=e_{t}+\theta e_{t-1}\)</span> and <span class="math inline">\(Y_{t}=e_{t}+\omega e_{t-1}\)</span>. Thus the coefficient <span class="math inline">\(\theta\)</span> is not identified.</p>
<p>The standard solution is to select the parameter which produces an invertible moving average polynomial. Since there is only one such choice this yields a unique solution. This may be sensible when there is reason to believe that shocks have their primary impact in the contemporaneous period and secondary (lesser) impact in the second period.</p>
<p>Now consider the MA(2) model</p>
<p><span class="math display">\[
Y_{t}=e_{t}+\theta_{1} e_{t-1}+\theta_{2} e_{t-2} .
\]</span></p>
<p>The moving average polynomial can be factored as</p>
<p><span class="math display">\[
\theta(z)=\left(1-\beta_{1} z\right)\left(1-\beta_{2} z\right)
\]</span></p>
<p>so that <span class="math inline">\(\beta_{1} \beta_{2}=\theta_{2}\)</span> and <span class="math inline">\(\beta_{1}+\beta_{2}=-\theta_{1}\)</span>. The process has first- and second-order autocorrelations</p>
<p><span class="math display">\[
\begin{aligned}
&amp;\rho(1)=\frac{\theta_{1}+\theta_{1} \theta_{2}}{1+\theta_{1}^{2}+\theta_{2}^{2}}=\frac{-\beta_{1}-\beta_{2}-\beta_{1}^{2} \beta_{2}-\beta_{1} \beta_{2}^{2}}{1+\beta_{1}^{2}+\beta_{2}^{2}+2 \beta_{1} \beta_{2}+\beta_{1}^{2} \beta_{2}^{2}} \\
&amp;\rho(2)=\frac{\theta_{2}}{1+\theta_{1}^{2}+\theta_{2}^{2}}=\frac{\beta_{1} \beta_{2}}{1+\beta_{1}^{2}+\beta_{2}^{2}+2 \beta_{1} \beta_{2}+\beta_{1}^{2} \beta_{2}^{2}} .
\end{aligned}
\]</span></p>
<p>If we replace <span class="math inline">\(\beta_{1}\)</span> with <span class="math inline">\(\omega_{1}=1 / \beta_{1}\)</span> we obtain</p>
<p><span class="math display">\[
\begin{aligned}
&amp;\rho(1)=\frac{-1 / \beta_{1}-\beta_{2}-\beta_{2} / \beta_{1}^{2}-\beta_{2}^{2} / \beta_{1}}{1+1 / \beta_{1}^{2}+\beta_{2}^{2}+2 \beta_{2} / \beta_{1}+\beta_{2}^{2} / \beta_{1}^{2}}=\frac{-\beta_{1}-\beta_{2} \beta_{1}^{2}-\beta_{2}-\beta_{2}^{2} \beta_{1}}{\beta_{1}^{2}+1+\beta_{2}^{2} \beta_{1}^{2}+2 \beta_{2} \beta_{1}+\beta_{2}^{2}} \\
&amp;\rho(2)=\frac{\beta_{2} / \beta_{1}}{1+1 / \beta_{1}^{2}+\beta_{2}^{2}+2 \beta_{2} / \beta_{1}+\beta_{2}^{2} / \beta_{1}^{2}}=\frac{\beta_{1} \beta_{2}}{\beta_{1}^{2}+1+\beta_{1}^{2} \beta_{2}^{2}+2 \beta_{1} \beta_{2}+\beta_{2}^{2}}
\end{aligned}
\]</span></p>
<p>which is unchanged. Similarly if we replace <span class="math inline">\(\beta_{2}\)</span> with <span class="math inline">\(\omega_{2}=1 / \beta_{2}\)</span> we obtain unchanged first- and secondorder autocorrelations. It follows that in the MA(2) model the factors <span class="math inline">\(\beta_{1}\)</span> and <span class="math inline">\(\beta_{2}\)</span> nor the coefficients <span class="math inline">\(\theta_{1}\)</span> and <span class="math inline">\(\theta_{2}\)</span> are identified. Consequently there are four distinct <span class="math inline">\(\mathrm{MA}(2)\)</span> models which are identifiably indistinguishable.</p>
<p>This analysis extends to the MA(q) model. The factors of the MA polynomial can be replaced by their inverses and consequently the coefficients are not identified.</p>
<p>The standard solution is to confine attention to MA(q) models with invertible roots. This technically solves the identification dilemma. This solution corresponds to the Wold decomposition, as it is defined in terms of the projection errors which correspond to the invertible representation.</p>
<p>A deeper identification failure occurs in ARMA models. Consider an ARMA(1,1) model</p>
<p><span class="math display">\[
Y_{t}=\alpha Y_{t-1}+e_{t}+\theta e_{t-1} .
\]</span></p>
<p>Written in lag operator notation</p>
<p><span class="math display">\[
(1-\alpha \mathrm{L}) Y_{t}=(1+\theta \mathrm{L}) e_{t} .
\]</span></p>
<p>The identification failure is that when <span class="math inline">\(\alpha=-\theta\)</span> then the model simplifies to <span class="math inline">\(Y_{t}=e_{t}\)</span>. This means that the continuum of models with <span class="math inline">\(\alpha=-\theta\)</span> are all identical and the coefficients are not identified.</p>
<p>This extends to higher order ARMA models. Take the ARMA <span class="math inline">\((2,2)\)</span> model written in factored lag operator notation</p>
<p><span class="math display">\[
\left(1-\alpha_{1} \mathrm{~L}\right)\left(1-\alpha_{2} \mathrm{~L}\right) Y_{t}=\left(1+\theta_{1} \mathrm{~L}\right)\left(1+\theta_{2} \mathrm{~L}\right) e_{t} .
\]</span></p>
<p>The models with <span class="math inline">\(\alpha_{1}=-\theta_{1}, \alpha_{1}=-\theta_{2}, \alpha_{2}=-\theta_{1}\)</span>, or <span class="math inline">\(\alpha_{2}=-\theta_{2}\)</span> all simplify to an ARMA(1,1). Thus all these models are identical and hence the coefficients are not identified.</p>
<p>The problem is called “cancelling roots” due to the fact that it arises when there are two identical lag polynomial factors in the AR and MA polynomials.</p>
<p>The standard solution in the ARMA literature is to assume that there are no cancelling roots. The trouble with this solution is that this is an assumption about the true process which is unknown. Thus it is not really a solution to the identification problem. One recommendation is to be careful when using ARMA models and be aware that highly parameterized models may not have unique coefficients.</p>
<p>Now consider the <span class="math inline">\(\operatorname{AR}(\mathrm{p})\)</span> model (14.38). It can be written as</p>
<p><span class="math display">\[
Y_{t}=X_{t}^{\prime} \alpha+e_{t}
\]</span></p>
<p>where <span class="math inline">\(\alpha=\left(\alpha_{0}, \alpha_{1}, \ldots \alpha_{p}\right)^{\prime}\)</span> and <span class="math inline">\(X_{t}=\left(1, Y_{t-1}, \ldots, Y_{t-p}\right)^{\prime}\)</span>. The MDS assumption implies that <span class="math inline">\(\mathbb{E}\left[e_{t}\right]=0\)</span> and <span class="math inline">\(\mathbb{E}\left[X_{t} e_{t}\right]=0\)</span>. This means that the coefficient <span class="math inline">\(\alpha\)</span> satisfies</p>
<p><span class="math display">\[
\alpha=\left(\mathbb{E}\left[X_{t} X_{t}^{\prime}\right]\right)^{-1}\left(\mathbb{E}\left[X_{t} Y_{t}\right]\right) .
\]</span></p>
<p>This equation is unique if <span class="math inline">\(\boldsymbol{Q}=\mathbb{E}\left[X_{t} X_{t}^{\prime}\right]\)</span> is positive definite. It turns out that this is generically true so <span class="math inline">\(\alpha\)</span> is unique and identified.</p>
<p>Theorem 14.27 In the AR(p) model (14.38), if <span class="math inline">\(0&lt;\sigma^{2}&lt;\infty\)</span> then <span class="math inline">\(\boldsymbol{Q}&gt;0\)</span> and <span class="math inline">\(\alpha\)</span> is unique and identified.</p>
<p>The assumption <span class="math inline">\(\sigma^{2}&gt;0\)</span> means that <span class="math inline">\(Y_{t}\)</span> is not purely deterministic.</p>
<p>We can extend this result to approximating <span class="math inline">\(\operatorname{AR}(\mathrm{p})\)</span> models. That is, consider the equation (14.45) without the assumption that <span class="math inline">\(Y_{t}\)</span> is necessarily a true AR(p) with a MDS error. Instead, suppose that <span class="math inline">\(Y_{t}\)</span> is a non-deterministic stationary process. (Recall, non-deterministic means that <span class="math inline">\(\sigma^{2}&gt;0\)</span> where <span class="math inline">\(\sigma^{2}\)</span> is the projection error variance (14.19).) We then define the coefficient <span class="math inline">\(\alpha\)</span> as the best linear predictor, which is (14.46). The error <span class="math inline">\(e_{t}\)</span> is defined by the equation (14.45). This is a linear projection model.</p>
<p>As in the case of any linear projection, the error <span class="math inline">\(e_{t}\)</span> satisfies <span class="math inline">\(\mathbb{E}\left[X_{t} e_{t}\right]=0\)</span>. This means that <span class="math inline">\(\mathbb{E}\left[e_{t}\right]=0\)</span> and <span class="math inline">\(\mathbb{E}\left[Y_{t-j} e_{t}\right]=0\)</span> for <span class="math inline">\(j=1, \ldots, p\)</span>. However, the error <span class="math inline">\(e_{t}\)</span> is not necessarily a MDS nor white noise.</p>
<p>The coefficient <span class="math inline">\(\alpha\)</span> is identified if <span class="math inline">\(\boldsymbol{Q}&gt;0\)</span>. The proof of Theorem <span class="math inline">\(14.27\)</span> (presented in Section 14.47) does not make use of the assumption that <span class="math inline">\(Y_{t}\)</span> is an <span class="math inline">\(\operatorname{AR}(\mathrm{p})\)</span> with a MDS error. Rather, it only uses the assumption that <span class="math inline">\(\sigma^{2}&gt;0\)</span>. This holds in the approximate <span class="math inline">\(\operatorname{AR}(\mathrm{p})\)</span> model as well under the assumption that <span class="math inline">\(Y_{t}\)</span> is nondeterministic. We conclude that any approximating AR(p) is identified.</p>
<p>Theorem 14.28 If <span class="math inline">\(Y_{t}\)</span> is strictly stationary, not purely deterministic, and <span class="math inline">\(\mathbb{E}\left[Y_{t}^{2}\right]&lt;\infty\)</span>, then for any <span class="math inline">\(p, \boldsymbol{Q}=\mathbb{E}\left[X_{t} X_{t}^{\prime}\right]&gt;0\)</span> and thus the coefficient vector (14.46) is identified.</p>
</section>
<section id="estimation-of-autoregressive-models" class="level2" data-number="14.30">
<h2 data-number="14.30" class="anchored" data-anchor-id="estimation-of-autoregressive-models"><span class="header-section-number">14.30</span> Estimation of Autoregressive Models</h2>
<p>We consider estimation of an <span class="math inline">\(\mathrm{AR}(\mathrm{p})\)</span> model for stationary, ergodic, and non-deterministic <span class="math inline">\(Y_{t}\)</span>. The model is (14.45) where <span class="math inline">\(X_{t}=\left(1, Y_{t-1}, \ldots, Y_{t-p}\right)^{\prime}\)</span>. The coefficient <span class="math inline">\(\alpha\)</span> is defined by projection in (14.46). The error is defined by (14.45) and has variance <span class="math inline">\(\sigma^{2}=\mathbb{E}\left[e_{t}^{2}\right]\)</span>. This allows <span class="math inline">\(Y_{t}\)</span> to follow a true AR(p) process but it is not necessary.</p>
<p>The least squares estimator is</p>
<p><span class="math display">\[
\widehat{\alpha}=\left(\sum_{t=1}^{n} X_{t} X_{t}^{\prime}\right)^{-1}\left(\sum_{t=1}^{n} X_{t} Y_{t}\right) .
\]</span></p>
<p>This notation presumes that there are <span class="math inline">\(n+p\)</span> total observations on <span class="math inline">\(Y_{t}\)</span> from which the first <span class="math inline">\(p\)</span> are used as initial conditions so that <span class="math inline">\(X_{1}=\left(1, Y_{0}, Y_{-1}, \ldots, Y_{-p+1}\right)\)</span> is defined. Effectively, this redefines the sample period. (An alternative notational choice is to define the periods so the sums range from observations <span class="math inline">\(p+1\)</span> to <span class="math inline">\(n\)</span>.)</p>
<p>The least squares residuals are <span class="math inline">\(\widehat{e}_{t}=Y_{t}-X_{t}^{\prime} \widehat{\alpha}\)</span>. The error variance can be estimated by <span class="math inline">\(\widehat{\sigma}^{2}=n^{-1} \sum_{t=1}^{n} \widehat{e}_{t}^{2}\)</span> or <span class="math inline">\(s^{2}=(n-p-1)^{-1} \sum_{t=1}^{n} \widehat{e}_{t}^{2}\)</span>.</p>
<p>If <span class="math inline">\(Y_{t}\)</span> is strictly stationary and ergodic then so are <span class="math inline">\(X_{t} X_{t}^{\prime}\)</span> and <span class="math inline">\(X_{t} Y_{t}\)</span>. They have finite means if <span class="math inline">\(\mathbb{E}\left[Y_{t}^{2}\right]&lt;\infty\)</span>. Under these assumptions the Ergodic Theorem implies that</p>
<p><span class="math display">\[
\frac{1}{n} \sum_{t=1}^{n} X_{t} Y_{t} \underset{p}{\longrightarrow} \mathbb{E}\left[X_{t} Y_{t}\right]
\]</span></p>
<p>and</p>
<p><span class="math display">\[
\frac{1}{n} \sum_{t=1}^{n} X_{t} X_{t}^{\prime} \underset{p}{\longrightarrow} \mathbb{E}\left[X_{t} X_{t}^{\prime}\right]=\boldsymbol{Q} .
\]</span></p>
<p>Theorem <span class="math inline">\(14.28\)</span> shows that <span class="math inline">\(\boldsymbol{Q}&gt;0\)</span>. Combined with the continuous mapping theorem we see that</p>
<p><span class="math display">\[
\widehat{\alpha}=\left(\frac{1}{n} \sum_{t=1}^{n} X_{t} X_{t}^{\prime}\right)^{-1}\left(\frac{1}{n} \sum_{t=1}^{n} X_{t} Y_{t}\right) \underset{p}{\longrightarrow}\left(\mathbb{E}\left[X_{t} X_{t}^{\prime}\right]\right)^{-1} \mathbb{E}\left[X_{t} Y_{t}\right]=\alpha .
\]</span></p>
<p>It is straightforward to show that <span class="math inline">\(\widehat{\sigma}^{2}\)</span> is consistent as well.</p>
<p>Theorem 14.29 If <span class="math inline">\(Y_{t}\)</span> is strictly stationary, ergodic, not purely deterministic, and <span class="math inline">\(\mathbb{E}\left[Y_{t}^{2}\right]&lt;\infty\)</span>, then for any <span class="math inline">\(p, \widehat{\alpha} \underset{p}{\longrightarrow} \alpha\)</span> and <span class="math inline">\(\widehat{\sigma}^{2} \underset{p}{\longrightarrow} \sigma^{2}\)</span> as <span class="math inline">\(n \rightarrow \infty\)</span>.</p>
<p>This shows that under very mild conditions the coefficients of an AR(p) model can be consistently estimated by least squares. Once again, this does not require that the series <span class="math inline">\(Y_{t}\)</span> is actually an <span class="math inline">\(\mathrm{AR}(\mathrm{p})\)</span> process. It holds for any stationary process with the coefficient defined by projection.</p>
</section>
<section id="asymptotic-distribution-of-least-squares-estimator" class="level2" data-number="14.31">
<h2 data-number="14.31" class="anchored" data-anchor-id="asymptotic-distribution-of-least-squares-estimator"><span class="header-section-number">14.31</span> Asymptotic Distribution of Least Squares Estimator</h2>
<p>The asymptotic distribution of the least squares estimator <span class="math inline">\(\widehat{\alpha}\)</span> depends on the stochastic assumptions. In this section we derive the asymptotic distribution under the assumption of correct specification.</p>
<p>Specifically, we assume that the error <span class="math inline">\(e_{t}\)</span> is a MDS. An important implication of the MDS assumption is that since <span class="math inline">\(X_{t}=\left(1, Y_{t-1}, \ldots, Y_{t-p}\right)^{\prime}\)</span> is part of the information set <span class="math inline">\(\mathscr{F}_{t-1}\)</span>, by the conditioning theorem,</p>
<p><span class="math display">\[
\mathbb{E}\left[X_{t} e_{t} \mid \mathscr{F}_{t-1}\right]=X_{t} \mathbb{E}\left[e_{t} \mid \mathscr{F}_{t-1}\right]=0 .
\]</span></p>
<p>Thus <span class="math inline">\(X_{t} e_{t}\)</span> is a MDS. It has a finite variance if <span class="math inline">\(e_{t}\)</span> has a finite fourth moment. To see this, by Theorem <span class="math inline">\(14.24, Y_{t}=\mu+\sum_{j=0}^{\infty} b_{j} e_{t-j}\)</span> with <span class="math inline">\(\sum_{j=0}^{\infty}\left|b_{j}\right|&lt;\infty\)</span>. Using Minkowski’s Inequality,</p>
<p><span class="math display">\[
\left(\mathbb{E}\left|Y_{t}\right|^{4}\right)^{1 / 4} \leq \sum_{j=0}^{\infty}\left|b_{j}\right|\left(\mathbb{E}\left|e_{t-j}\right|^{4}\right)^{1 / 4}&lt;\infty .
\]</span></p>
<p>Thus <span class="math inline">\(\mathbb{E}\left[Y_{t}^{4}\right]&lt;\infty\)</span>. The Cauchy-Schwarz inequality then shows that <span class="math inline">\(\mathbb{E}\left\|X_{t} e_{t}\right\|^{2}&lt;\infty\)</span>. We can then apply the martingale difference CLT (Theorem 14.11) to see that</p>
<p><span class="math display">\[
\frac{1}{\sqrt{n}} \sum_{t=1}^{n} X_{t} e_{t} \underset{d}{\longrightarrow} \mathrm{N}(0, \Sigma)
\]</span></p>
<p>where <span class="math inline">\(\Sigma=\mathbb{E}\left[X_{t} X_{t}^{\prime} e_{t}^{2}\right]\)</span></p>
<p>Theorem 14.30 If <span class="math inline">\(Y_{t}\)</span> follows the AR(p) model (14.38), all roots of <span class="math inline">\(a(z)\)</span> lie outside the unit circle, <span class="math inline">\(\mathbb{E}\left[e_{t} \mid \mathscr{F}_{t-1}\right]=0, \mathbb{E}\left[e_{t}^{4}\right]&lt;\infty\)</span>, and <span class="math inline">\(\mathbb{E}\left[e_{t}^{2}\right]&gt;0\)</span>, then as <span class="math inline">\(n \rightarrow \infty\)</span>, <span class="math inline">\(\sqrt{n}(\widehat{\alpha}-\alpha) \underset{d}{\rightarrow} \mathrm{N}(0, \boldsymbol{V})\)</span> where <span class="math inline">\(\boldsymbol{V}=\boldsymbol{Q}^{-1} \Sigma \boldsymbol{Q}^{-1}\)</span>.</p>
<p>This is identical in form to the asymptotic distribution of least squares in cross-section regression. The implication is that asymptotic inference is the same. In particular, the asymptotic covariance matrix is estimated just as in the cross-section case.</p>
</section>
<section id="distribution-under-homoskedasticity" class="level2" data-number="14.32">
<h2 data-number="14.32" class="anchored" data-anchor-id="distribution-under-homoskedasticity"><span class="header-section-number">14.32</span> Distribution Under Homoskedasticity</h2>
<p>In cross-section regression we found that the covariance matrix simplifies under the assumption of conditional homoskedasticity. The same occurs in the time series context. Assume that the error is a homoskedastic MDS:</p>
<p><span class="math display">\[
\begin{aligned}
\mathbb{E}\left[e_{t} \mid \mathscr{F}_{t-1}\right] &amp;=0 \\
\mathbb{E}\left[e_{t}^{2} \mid \mathscr{F}_{t-1}\right] &amp;=\sigma^{2} .
\end{aligned}
\]</span></p>
<p>In this case</p>
<p><span class="math display">\[
\Sigma=\mathbb{E}\left[X_{t} X_{t}^{\prime} \mathbb{E}\left[e_{t}^{2} \mid \mathscr{F}_{t-1}\right]\right]=\boldsymbol{Q} \sigma^{2}
\]</span></p>
<p>and the asymptotic distribution simplifies.</p>
<p>Theorem 14.31 Under the assumptions of Theorem 14.30, if in addition <span class="math inline">\(\mathbb{E}\left[e_{t}^{2} \mid \mathscr{F}_{t-1}\right]=\sigma^{2}\)</span>, then as <span class="math inline">\(n \rightarrow \infty, \sqrt{n}(\widehat{\alpha}-\alpha) \underset{d}{\longrightarrow} \mathrm{N}\left(0, \boldsymbol{V}^{0}\right)\)</span> where <span class="math inline">\(\boldsymbol{V}^{0}=\sigma^{2} \boldsymbol{Q}^{-1}\)</span>.</p>
<p>These results show that under correct specification (a MDS error) the format of the asymptotic distribution of the least squares estimator exactly parallels the cross-section case. In general the covariance matrix takes a sandwich form with components exactly equal to the cross-section case. Under conditional homoskedasticity the covariance matrix simplies exactly as in the cross-section case. A particularly useful insight which can be derived from Theorem <span class="math inline">\(14.31\)</span> is to focus on the simple AR(1) with no intercept. In this case <span class="math inline">\(Q=\mathbb{E}\left[Y_{t}^{2}\right]=\sigma^{2} /\left(1-\alpha_{1}^{2}\right)\)</span> so the asymptotic distribution simplifies to</p>
<p><span class="math display">\[
\sqrt{n}\left(\widehat{\alpha}_{1}-\alpha_{1}\right) \underset{d}{\longrightarrow} \mathrm{N}\left(0,1-\alpha_{1}^{2}\right) .
\]</span></p>
<p>Thus the asymptotic variance depends only on <span class="math inline">\(\alpha_{1}\)</span> and is decreasing with <span class="math inline">\(\alpha_{1}^{2}\)</span>. An intuition is that larger <span class="math inline">\(\alpha_{1}^{2}\)</span> means greater signal and hence greater estimation precision. This result also shows that the asymptotic distribution is non-similar: the variance is a function of the parameter of interest. This means that we can expect (from advanced statistical theory) asymptotic inference to be less accurate than indicated by nominal levels.</p>
<p>In the context of cross-section data we argued that the homoskedasticity assumption was dubious except for occassional theoretical insight. For practical applications it is recommended to use heteroskedasticityrobust theory and methods when possible. The same argument applies to the time series case. While the distribution theory simplifies under conditional homoskedasticity there is no reason to expect homoskedasticity to hold in practice. Therefore in applications it is better to use the heteroskedasticityrobust distributional theory when possible.</p>
<p>Unfortunately, many existing time series textbooks report the distribution theory from (14.31). This has influenced computer software packages many of which also by default (or exclusively) use the homoskedastic distribution theory. This is unfortunate.</p>
</section>
<section id="asymptotic-distribution-under-general-dependence" class="level2" data-number="14.33">
<h2 data-number="14.33" class="anchored" data-anchor-id="asymptotic-distribution-under-general-dependence"><span class="header-section-number">14.33</span> Asymptotic Distribution Under General Dependence</h2>
<p>If the <span class="math inline">\(\mathrm{AR}(\mathrm{p})\)</span> model (14.38) holds with white noise errors or if the <span class="math inline">\(\mathrm{AR}(\mathrm{p})\)</span> is an approximation with <span class="math inline">\(\alpha\)</span> defined as the best linear predictor then the MDS central limit theory does not apply. Instead, if <span class="math inline">\(Y_{t}\)</span> is strong mixing we can use the central limit theory for mixing processes (Theorem 14.15).</p>
<p>Theorem 14.32 Assume that <span class="math inline">\(Y_{t}\)</span> is strictly stationary, ergodic, and for some <span class="math inline">\(r&gt;\)</span> <span class="math inline">\(4, \mathbb{E}\left|Y_{t}\right|^{r}&lt;\infty\)</span> and the mixing coefficients satisfy <span class="math inline">\(\sum_{\ell=1}^{\infty} \alpha(\ell)^{1-4 / r}&lt;\infty\)</span>. Let <span class="math inline">\(\alpha\)</span> be defined as the best linear projection coefficients (14.46) from an AR(p) model with projection errors <span class="math inline">\(e_{t}\)</span>. Let <span class="math inline">\(\widehat{\alpha}\)</span> be the least squares estimator of <span class="math inline">\(\alpha\)</span>. Then</p>
<p><span class="math display">\[
\Omega=\sum_{\ell=-\infty}^{\infty} \mathbb{E}\left[X_{t-\ell} X_{t}^{\prime} e_{t} e_{t-\ell}\right]
\]</span></p>
<p>is convergent and <span class="math inline">\(\sqrt{n}(\widehat{\alpha}-\alpha) \underset{d}{\longrightarrow} \mathrm{N}(0, \boldsymbol{V})\)</span> as <span class="math inline">\(n \rightarrow \infty\)</span>, where <span class="math inline">\(\boldsymbol{V}=\boldsymbol{Q}^{-1} \Omega \boldsymbol{Q}^{-1}\)</span>.</p>
<p>This result is substantially different from the cross-section case. It shows that model misspecification (including misspecifying the order of the autoregression) renders invalid the conventional “heteroskedasticityrobust” covariance matrix formula. Misspecified models do not have unforecastable (martingale difference) errors so the regression scores <span class="math inline">\(X_{t} e_{t}\)</span> are potentially serially correlated. The asymptotic variance takes a sandwich form with the central component <span class="math inline">\(\Omega\)</span> the long-run variance (recall Section 14.13) of the regression scores <span class="math inline">\(X_{t} e_{t}\)</span>.</p>
</section>
<section id="covariance-matrix-estimation" class="level2" data-number="14.34">
<h2 data-number="14.34" class="anchored" data-anchor-id="covariance-matrix-estimation"><span class="header-section-number">14.34</span> Covariance Matrix Estimation</h2>
<p>Under the assumption of correct specification covariance matrix estimation is identical to the crosssection case. The asymptotic covariance matrix estimator under homoskedasticity is</p>
<p><span class="math display">\[
\begin{aligned}
\widehat{\boldsymbol{V}}^{0} &amp;=\widehat{\sigma}^{2} \widehat{\boldsymbol{Q}}^{-1} \\
\widehat{\boldsymbol{Q}} &amp;=\frac{1}{n} \sum_{t=1}^{n} X_{t} X_{t}^{\prime}
\end{aligned}
\]</span></p>
<p>The estimator <span class="math inline">\(s^{2}\)</span> may be used instead of <span class="math inline">\(\widehat{\sigma}^{2}\)</span>.</p>
<p>The heteroskedasticity-robust asymptotic covariance matrix estimator is</p>
<p><span class="math display">\[
\widehat{\boldsymbol{V}}=\widehat{\boldsymbol{Q}}^{-1} \widehat{\Sigma} \widehat{\boldsymbol{Q}}^{-1}
\]</span></p>
<p>where</p>
<p><span class="math display">\[
\widehat{\Sigma}=\frac{1}{n} \sum_{t=1}^{n} X_{t} X_{t}^{\prime} \widehat{e}_{t}^{2} .
\]</span></p>
<p>Degree-of-freedom adjustments may be made as in the cross-section case though a theoretical justification has not been developed.</p>
<p>Standard errors <span class="math inline">\(s\left(\widehat{\alpha}_{j}\right)\)</span> for individual coefficient estimates can be formed by taking the scaled diagonal elements of <span class="math inline">\(\widehat{\boldsymbol{V}}\)</span></p>
<p>Theorem 14.33 Under the assumptions of Theorem 14.32, as <span class="math inline">\(n \rightarrow \infty, \widehat{\boldsymbol{V}} \underset{p}{\rightarrow} \boldsymbol{V}\)</span> and <span class="math inline">\(\left(\widehat{\alpha}_{j}-\alpha_{j}\right) / s\left(\widehat{\alpha}_{j}\right) \longrightarrow \underset{d}{\longrightarrow} \mathrm{N}(0,1)\)</span></p>
<p>Theorem <span class="math inline">\(14.33\)</span> shows that standard covariance matrix estimation is consistent and the resulting tratios are asymptotically normal. This means that for stationary autoregressions, inference can proceed using conventional regression methods.</p>
</section>
<section id="covariance-matrix-estimation-under-general-dependence" class="level2" data-number="14.35">
<h2 data-number="14.35" class="anchored" data-anchor-id="covariance-matrix-estimation-under-general-dependence"><span class="header-section-number">14.35</span> Covariance Matrix Estimation Under General Dependence</h2>
<p>Under the assumptions of Theorem <span class="math inline">\(14.32\)</span> the conventional covariance matrix estimators are inconsistent as they do not capture the serial dependence in the regression scores <span class="math inline">\(X_{t} e_{t}\)</span>. To consistently estimate the covariance matrix we need an estimator of the long-run variance <span class="math inline">\(\Omega\)</span>. The appropriate class of estimators are called Heteroskedasticity and Autocorrelation Consistent (HAC) or Heteroskedasticity and Autocorrelation Robust (HAR) covariance matrix estimators.</p>
<p>To understand the methods it is helpful to define the vector series <span class="math inline">\(u_{t}=X_{t} e_{t}\)</span> and autocovariance matrices <span class="math inline">\(\Gamma(\ell)=\mathbb{E}\left[u_{t-\ell} u_{t}^{\prime}\right]\)</span> so that</p>
<p><span class="math display">\[
\Omega=\sum_{\ell=-\infty}^{\infty} \Gamma(\ell) .
\]</span></p>
<p>Since this sum is convergent the autocovariance matrices converge to zero as <span class="math inline">\(\ell \rightarrow \infty\)</span>. Therefore <span class="math inline">\(\Omega\)</span> can be approximated by taking a finite sum of autocovariances such as</p>
<p><span class="math display">\[
\Omega_{M}=\sum_{\ell=-M}^{M} \Gamma(\ell) .
\]</span></p>
<p>The number <span class="math inline">\(M\)</span> is sometimes called the lag truncation number. Other authors call it the bandwidth. An estimator of <span class="math inline">\(\Gamma(\ell)\)</span> is</p>
<p><span class="math display">\[
\widehat{\Gamma}(\ell)=\frac{1}{n} \sum_{1 \leq t-\ell \leq n} \widehat{u}_{t-\ell} \widehat{u}_{t}^{\prime}
\]</span></p>
<p>where <span class="math inline">\(\widehat{u}_{t}=X_{t} \widehat{e}_{t}\)</span>. By the ergodic theorem we can show that for any <span class="math inline">\(\ell, \widehat{\Gamma}(\ell) \underset{p}{\longrightarrow} \Gamma(\ell)\)</span>. Thus for any fixed <span class="math inline">\(M\)</span>, the estimator</p>
<p><span class="math display">\[
\widehat{\Omega}_{M}=\sum_{\ell=-M}^{M} \widehat{\Gamma}(\ell)
\]</span></p>
<p>is consistent for <span class="math inline">\(\Omega_{M}\)</span>.</p>
<p>If the serial correlation in <span class="math inline">\(X_{t} e_{t}\)</span> is known to be zero after <span class="math inline">\(M\)</span> lags, then <span class="math inline">\(\Omega_{M}=\Omega\)</span> and the estimator (14.49) is consistent for <span class="math inline">\(\Omega\)</span>. This estimator was proposed by L. Hansen and Hodrick (1980) in the context of multiperiod forecasts and by L. Hansen (1982) for the generalized method of moments.</p>
<p>In the general case we can select <span class="math inline">\(M\)</span> to increase with sample size <span class="math inline">\(n\)</span>. If the rate at which <span class="math inline">\(M\)</span> increases is sufficiently slow then <span class="math inline">\(\widehat{\Omega}_{M}\)</span> will be consistent for <span class="math inline">\(\Omega\)</span>, as first shown by White and Domowitz (1984).</p>
<p>Once we view the lag truncation number <span class="math inline">\(M\)</span> as a choice the estimator (14.49) has two potential deficiencies. One is that <span class="math inline">\(\widehat{\Omega}_{M}\)</span> can change non-smoothly with <span class="math inline">\(M\)</span> which makes estimation results sensitive to the choice of <span class="math inline">\(M\)</span>. The other is that <span class="math inline">\(\widehat{\Omega}_{M}\)</span> may not be positive semi-definite and is therefore not a valid covariance matrix estimator. We can see this in the simple case of scalar <span class="math inline">\(u_{t}\)</span> and <span class="math inline">\(M=1\)</span>. In this case <span class="math inline">\(\widehat{\Omega}_{1}=\widehat{\gamma}(0)(1+2 \widehat{\rho}(1))\)</span> which is negative when <span class="math inline">\(\widehat{\rho}(1)&lt;-1 / 2\)</span>. Thus if the data are strongly negatively autocorrelated the variance estimator can be negative. A negative variance estimator means that standard errors are ill-defined (a naïve computation will produce a complex standard error which makes no sense <span class="math inline">\({ }^{6}\)</span> ).</p>
<p>These two deficiencies can be resolved if we amend (14.49) by a weighted sum of autocovariances. Newey and West (1987b) proposed</p>
<p><span class="math display">\[
\widehat{\Omega}_{\mathrm{nW}}=\sum_{\ell=-M}^{M}\left(1-\frac{|\ell|}{M+1}\right) \widehat{\Gamma}(\ell)
\]</span></p>
<p>This is a weighted sum of the autocovariances. Other weight functions can be used; the one in (14.50) is known as the Bartlett kernel <span class="math inline">\({ }^{7}\)</span>. Newey and West (1987b) showed that this estimator has the algebraic property that <span class="math inline">\(\widehat{\Omega}_{\mathrm{nw}} \geq 0\)</span> (it is positive semi-definite), solving the negative variance problem, and it is also a smooth function of <span class="math inline">\(M\)</span>. Thus this estimator solves the two problems described above.</p>
<p>For <span class="math inline">\(\widehat{\Omega}_{n w}\)</span> to be consistent for <span class="math inline">\(\Omega\)</span> the lag trunction number <span class="math inline">\(M\)</span> must increase to infinity with <span class="math inline">\(n\)</span>. Sufficient conditions were established by B. E. Hansen (1992).</p>
<p>Theorem 14.34 Under the assumptions of Theorem <span class="math inline">\(14.32\)</span> plus <span class="math inline">\(\sum_{\ell=1}^{\infty} \alpha(\ell)^{1 / 2-4 / r}&lt;\infty\)</span>, if <span class="math inline">\(M \rightarrow \infty\)</span> yet <span class="math inline">\(M^{3} / n=O(1)\)</span>, then as <span class="math inline">\(n \rightarrow \infty, \widehat{\Omega}_{\mathrm{nw}} \underset{p}{\rightarrow} \Omega\)</span></p>
<p>The assumption <span class="math inline">\(M^{3} / n=O(1)\)</span> technically means that <span class="math inline">\(M\)</span> grows no faster than <span class="math inline">\(n^{1 / 3}\)</span> but this does not have a practical counterpart other than the implication that ” <span class="math inline">\(M\)</span> should be much smaller than <span class="math inline">\(n\)</span> “. The assumption on the mixing coefficients is slightly stronger than in Theorem 14.32, due to the technical nature of the derivation.</p>
<p><span class="math inline">\({ }^{6}\)</span> A common computational mishap is a complex standard error. This occurs when a covariance matrix estimator has negative elements on the diagonal.</p>
<p><span class="math inline">\({ }^{7}\)</span> See Andrews (1991b) for a description of popular options. In practice, the choice of weight function is much less important than the choice of lag truncation number <span class="math inline">\(M\)</span>. A important practical issue is how to select <span class="math inline">\(M\)</span>. One way to think about it is that <span class="math inline">\(M\)</span> impacts the precision of the estimator <span class="math inline">\(\widehat{\Omega}_{\mathrm{nw}}\)</span> through its bias and variance. Since <span class="math inline">\(\widehat{\Gamma}(\ell)\)</span> is a sample average its variance is <span class="math inline">\(O(1 / n)\)</span> so we expect the variance of <span class="math inline">\(\widehat{\Omega}_{M}\)</span> to be of order <span class="math inline">\(O(M / n)\)</span>. The bias of <span class="math inline">\(\widehat{\Omega}_{\mathrm{nw}}\)</span> for <span class="math inline">\(\Omega\)</span> is harder to calculate but depends on the rate at which the covariances <span class="math inline">\(\Gamma(\ell)\)</span> decay to zero. Andrews (1991b) found that the <span class="math inline">\(M\)</span> which minimizes the mean squared error of <span class="math inline">\(\widehat{\Omega}_{\mathrm{nw}}\)</span> satisfies the rate <span class="math inline">\(M=C n^{1 / 3}\)</span> where the constant <span class="math inline">\(C\)</span> depends on the autocovariances. Practical rules to estimate and implement this optimal lag truncation parameter have been proposed by Andrews (1991b) and Newey and West (1994). Andrews’ rule for the Newey-West estimator (14.50) can be written as</p>
<p><span class="math display">\[
M=\left(6 \frac{\rho^{2}}{\left(1-\rho^{2}\right)^{2}}\right)^{1 / 3} n^{1 / 3}
\]</span></p>
<p>where <span class="math inline">\(\rho\)</span> is a serial correlation parameter. When <span class="math inline">\(u_{t}\)</span> is scalar, <span class="math inline">\(\rho\)</span> is the first autocorrelation of <span class="math inline">\(u_{t}\)</span>. Andrews suggested using an estimator of <span class="math inline">\(\rho\)</span> to plug into this formula to find <span class="math inline">\(M\)</span>. An alternative is to use a default value of <span class="math inline">\(\rho\)</span>. For example, if we set <span class="math inline">\(\rho=0.5\)</span> then the Andrews rule is <span class="math inline">\(M=1.4 n^{1 / 3}\)</span>, which is a useful benchmark.</p>
</section>
<section id="testing-the-hypothesis-of-no-serial-correlation" class="level2" data-number="14.36">
<h2 data-number="14.36" class="anchored" data-anchor-id="testing-the-hypothesis-of-no-serial-correlation"><span class="header-section-number">14.36</span> Testing the Hypothesis of No Serial Correlation</h2>
<p>In some cases it may be of interest to test the hypothesis that the series <span class="math inline">\(Y_{t}\)</span> is serially uncorrelated against the alternative that it is serially correlated. There have been many proposed tests of this hypothesis. The most appropriate is based on the least squares regression of an AR(p) model. Take the model</p>
<p><span class="math display">\[
Y_{t}=\alpha_{0}+\alpha_{1} Y_{t-1}+\alpha_{2} Y_{t-2}+\cdots+\alpha_{p} Y_{t-p}+e_{t}
\]</span></p>
<p>with <span class="math inline">\(e_{t}\)</span> a MDS. In this model the series <span class="math inline">\(Y_{t}\)</span> is serially uncorrelated if the slope coefficients are all zero. Thus the hypothesis of interest is</p>
<p><span class="math display">\[
\begin{aligned}
&amp;\mathbb{H}_{0}: \alpha_{1}=\cdots=\alpha_{p}=0 \\
&amp;\mathbb{H}_{1}: \alpha_{j} \neq 0 \text { for some } j \geq 1 .
\end{aligned}
\]</span></p>
<p>The test can be implemented by a Wald or F test. Estimate the AR(p) model by least squares. Form the Wald or F statistic using the variance estimator (14.48). (The Newey-West estimator should not be used as there is no serial correlation under the null hypothesis.) Accept the hypothesis if the test statistic is smaller than a conventional critical value (or if the p-value exceeds the significance level) and reject the hypothesis otherwise.</p>
<p>Implementation of this test requires a choice of autoregressive order <span class="math inline">\(p\)</span>. This choice affects the power of the test. A sufficient number of lags should be included so to pick up potential serial correlation patterns but not so many that the power of the test is diluted. A reasonable choice in many applications is to set <span class="math inline">\(p\)</span> to equals <span class="math inline">\(s\)</span>, the seasonal periodicity. Thus include four lags for quarterly data or twelve lags for monthly data.</p>
</section>
<section id="testing-for-omitted-serial-correlation" class="level2" data-number="14.37">
<h2 data-number="14.37" class="anchored" data-anchor-id="testing-for-omitted-serial-correlation"><span class="header-section-number">14.37</span> Testing for Omitted Serial Correlation</h2>
<p>When using an AR(p) model it may be of interest to know if there is any remaining serial correlation. This can be expressed as a test for serial correlation in the error or equivalently as a test for a higher-order autogressive model. Take the <span class="math inline">\(\operatorname{AR}(\mathrm{p})\)</span> model</p>
<p><span class="math display">\[
Y_{t}=\alpha_{0}+\alpha_{1} Y_{t-1}+\alpha_{2} Y_{t-2}+\cdots+\alpha_{p} Y_{t-p}+u_{t} .
\]</span></p>
<p>The null hypothesis is that <span class="math inline">\(u_{t}\)</span> is serially uncorrelated and the alternative hypothesis is that it is serially correlated. We can model the latter as a mean-zero autoregressive process</p>
<p><span class="math display">\[
u_{t}=\theta_{1} u_{t-1}+\cdots+\theta_{q} u_{t-q}+e_{t} .
\]</span></p>
<p>The hypothesis is</p>
<p><span class="math display">\[
\begin{aligned}
&amp;\mathbb{H}_{0}: \theta_{1}=\cdots=\theta_{q}=0 \\
&amp;\mathbb{H}_{1}: \theta_{j} \neq 0 \text { for some } j \geq 1 .
\end{aligned}
\]</span></p>
<p>A seemingly natural test for <span class="math inline">\(\mathbb{H}_{0}\)</span> uses a two-step method. First estimate (14.52) by least squares and obtain the residuals <span class="math inline">\(\widehat{u}_{t}\)</span>. Second, estimate (14.53) by least squares by regressing <span class="math inline">\(\widehat{u}_{t}\)</span> on its lagged values and obtain the Wald or <span class="math inline">\(F\)</span> test for <span class="math inline">\(\mathbb{M}_{0}\)</span>. This seems like a natural approach but it is muddled by the fact that the distribution of the Wald statistic is distorted by the two-step procedure. The Wald statistic is not asymptotically chi-square so it is inappropriate to make a decision based on the conventional critical values. One approach to obtain the correct asymptotic distribution is to use the generalized method of moments, treating (14.52)-(14.53) as a two-equation just-identified system.</p>
<p>An easier solution is to re-write (14.52)-(14.53) as a higher-order autoregression so that we can use a standard test statistic. To illustrate how this works take the case <span class="math inline">\(q=1\)</span>. Take (14.52) and lag the equation once:</p>
<p><span class="math display">\[
Y_{t-1}=\alpha_{0}+\alpha_{1} Y_{t-2}+\alpha_{2} Y_{t-3}+\cdots+\alpha_{p} Y_{t-p-1}+u_{t-1} .
\]</span></p>
<p>Multiply this by <span class="math inline">\(\theta_{1}\)</span> and subtract from (14.52) to find</p>
<p><span class="math display">\[
\begin{aligned}
Y_{t}-\theta_{1} Y_{t-1} &amp;=\alpha_{0}+\alpha_{1} Y_{t-1}+\alpha_{2} Y_{t-2}+\cdots+\alpha_{p} Y_{t-p}+u_{t} \\
&amp;-\theta_{1} \alpha_{0}-\theta_{1} \alpha_{1} Y_{t-2}-\theta_{1} \alpha_{2} Y_{t-3}-\cdots-\theta_{1} \alpha_{p} Y_{t-p-1}-\theta_{1} u_{t-1}
\end{aligned}
\]</span></p>
<p>or</p>
<p><span class="math display">\[
Y_{t}=\alpha_{0}\left(1-\theta_{1}\right)+\left(\alpha_{1}+\theta_{1}\right) Y_{t-1}+\left(\alpha_{2}-\theta_{1} \alpha_{1}\right) Y_{t-2}+\cdots-\theta_{1} \alpha_{p} Y_{t-p-1}+e_{t} .
\]</span></p>
<p>This is an <span class="math inline">\(\operatorname{AR}(\mathrm{p}+1)\)</span>. It simplifies to an <span class="math inline">\(\operatorname{AR}(\mathrm{p})\)</span> when <span class="math inline">\(\theta_{1}=0\)</span>. Thus <span class="math inline">\(\mathbb{H}_{0}\)</span> is equivalent to the restriction that the coefficient on <span class="math inline">\(Y_{t-p-1}\)</span> is zero.</p>
<p>Thus testing the null hypothesis of an <span class="math inline">\(\operatorname{AR}(\mathrm{p})\)</span> (14.52) against the alternative that the error is an <span class="math inline">\(\operatorname{AR}(1)\)</span> is equivalent to testing an <span class="math inline">\(\operatorname{AR}(\mathrm{p})\)</span> against an <span class="math inline">\(\operatorname{AR}(\mathrm{p}+1)\)</span>. The latter test is implemented as a t test on the coefficient on <span class="math inline">\(Y_{t-p-1}\)</span>.</p>
<p>More generally, testing the null hypothesis of an <span class="math inline">\(\operatorname{AR}(\mathrm{p})\)</span> (14.52) against the alternative that the error is an <span class="math inline">\(\operatorname{AR}(\mathrm{q})\)</span> is equivalent to testing that <span class="math inline">\(Y_{t}\)</span> is an <span class="math inline">\(\mathrm{AR}(\mathrm{p})\)</span> against the alternative that <span class="math inline">\(Y_{t}\)</span> is an <span class="math inline">\(\mathrm{AR}(\mathrm{p}+\mathrm{q})\)</span>. The latter test is implemented as a Wald (or F) test on the coefficients on <span class="math inline">\(Y_{t-p-1}, \ldots, Y_{t-p-q}\)</span>. If the statistic is smaller than the critical values (or the p-value is larger than the significance level) then we reject the hypothesis that the <span class="math inline">\(\operatorname{AR}(\mathrm{p})\)</span> is correctly specified in favor of the alternative that there is omitted serial correlation. Otherwise we accept the hypothesis that the AR(p) model is correctly specified.</p>
<p>Another way of deriving the test is as follows. Write (14.52) and (14.53) using lag operator notation <span class="math inline">\(\alpha(\mathrm{L}) Y_{t}=\alpha_{0}+u_{t}\)</span> with <span class="math inline">\(\theta(\mathrm{L}) u_{t}=e_{t}\)</span>. Applying the operator <span class="math inline">\(\theta(\mathrm{L})\)</span> to the first equation we obtain <span class="math inline">\(\theta(\mathrm{L}) \alpha(\mathrm{L}) Y_{t}=\)</span> <span class="math inline">\(\alpha_{0}^{*}+e_{t}\)</span> where <span class="math inline">\(\alpha_{0}^{*}=\theta(1) \alpha_{0}\)</span>. The product <span class="math inline">\(\theta(\mathrm{L}) \alpha(\mathrm{L})\)</span> is a polynomial of order <span class="math inline">\(p+q\)</span> so <span class="math inline">\(Y_{t}\)</span> is an AR(p+q).</p>
<p>While this discussion is all good fun, it is unclear if there is good reason to use the test described in this section. Economic theory does not typically produce hypotheses concerning the autoregressive order. Consequently there is rarely a case where there is scientific interest in testing, say, the hypothesis that a series is an AR(4) or any other specific autoregressive order. Instead, practitioners tend to use hypothesis tests for another purpose - model selection. That is, in practice users want to know “What autoregressive model should be used” in a specific application and resort to hypothesis tests to aid in this decision. This is an inappropriate use of hypothesis tests because tests are designed to provide answers to scientific questions rather than being designed to select models with good approximation properties. Instead, model selection should be based on model selection tools. One is described in the following section.</p>
</section>
<section id="model-selection" class="level2" data-number="14.38">
<h2 data-number="14.38" class="anchored" data-anchor-id="model-selection"><span class="header-section-number">14.38</span> Model Selection</h2>
<p>What is an appropriate choice of autoregressive order <span class="math inline">\(p\)</span> ? This is the problem of model selection. A good choice is to minimize the Akaike information criterion (AIC)</p>
<p><span class="math display">\[
\operatorname{AIC}(p)=n \log \widehat{\sigma}^{2}(p)+2 p
\]</span></p>
<p>where <span class="math inline">\(\widehat{\sigma}^{2}(p)\)</span> is the estimated residual variance from an <span class="math inline">\(\operatorname{AR}(\mathrm{p})\)</span>. The AIC is a penalized version of the Gaussian log-likelihood function for the estimated regression model. It is an estimator of the divergence between the fitted model and the true conditional density (see Section 28.4). By selecting the model with the smallest value of the AIC you select the model with the smallest estimated divergence - the highest estimated fit between the estimated and true densities.</p>
<p>The AIC is also a monotonic transformation of an estimator of the one-step-ahead forecast mean squared error. Thus selecting the model with the smallest value of the AIC you are selecting the model with the smallest estimated forecast error.</p>
<p>One possible hiccup in computing the AIC criterion for multiple models is that the sample size available for estimation changes as <span class="math inline">\(p\)</span> changes. (If you increase <span class="math inline">\(p\)</span>, you need more initial conditions.) This renders AIC comparisons inappropriate. The same sample - the same number of observations - should be used for estimation of all models. This is because AIC is a penalized likelihood, and if the samples are different then the likelihoods are not the same. The appropriate remedy is to fix a upper value <span class="math inline">\(\bar{p}\)</span>, and then reserve the first <span class="math inline">\(\bar{p}\)</span> as initial conditions. Then estimate the models <span class="math inline">\(\operatorname{AR}(1), \operatorname{AR}(2), \ldots, \operatorname{AR}(\bar{p})\)</span> on this (unified) sample.</p>
<p>The AIC of an estimated regression model can be displayed in Stata by using the estimates stats command.</p>
</section>
<section id="illustrations" class="level2" data-number="14.39">
<h2 data-number="14.39" class="anchored" data-anchor-id="illustrations"><span class="header-section-number">14.39</span> Illustrations</h2>
<p>We illustrate autoregressive estimation with three empirical examples using U.S. quarterly time series from the FRED-QD data file.</p>
<p>The first example is real GDP growth rates (growth rate of <span class="math inline">\(g d p c 1\)</span> ). We estimate autoregressive models of order 0 through 4 using the sample from <span class="math inline">\(1980-2017^{8}\)</span>. This is a commonly estimated model in applied macroeconomic practice and is the empirical version of the Samuelson multiplier-accelerator model discussed in Section 14.24. The coefficient estimates, conventional (heteroskedasticity-robust) standard errors, Newey-West (with <span class="math inline">\(M=5\)</span> ) standard errors, and AIC, are displayed in Table 14.1. This sample has 152 observations. The model selected by the AIC criterion is the AR(2). The estimated model has positive and small values for the first two autoregressive coefficients. This means that quarterly output growth</p>
<p><span class="math inline">\({ }^{8}\)</span> This sub-sample was used for estimation as it has been argued that the growth rate of U.S. GDP slowed around this period. The goal was to estimate the model over a period of time when the series is plausibly stationary. Table 14.1: U.S. GDP AR Models</p>
<table class="table">
<thead>
<tr class="header">
<th><span class="math inline">\(\alpha_{0}\)</span></th>
<th>AR(0)</th>
<th>AR(1)</th>
<th>AR(2)</th>
<th>AR(3)</th>
<th>AR(4)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td></td>
<td><span class="math inline">\(0.65\)</span></td>
<td><span class="math inline">\(0.40\)</span></td>
<td><span class="math inline">\(0.34\)</span></td>
<td><span class="math inline">\(0.34\)</span></td>
<td><span class="math inline">\(0.34\)</span></td>
</tr>
<tr class="even">
<td></td>
<td><span class="math inline">\((0.06)\)</span></td>
<td><span class="math inline">\((0.08)\)</span></td>
<td><span class="math inline">\((0.10)\)</span></td>
<td><span class="math inline">\((0.10)\)</span></td>
<td><span class="math inline">\((0.11)\)</span></td>
</tr>
<tr class="odd">
<td></td>
<td><span class="math inline">\([0.09]\)</span></td>
<td><span class="math inline">\([0.08]\)</span></td>
<td><span class="math inline">\([0.09]\)</span></td>
<td><span class="math inline">\([0.09]\)</span></td>
<td><span class="math inline">\([0.09]\)</span></td>
</tr>
<tr class="even">
<td></td>
<td></td>
<td><span class="math inline">\(0.39\)</span></td>
<td><span class="math inline">\(0.34\)</span></td>
<td><span class="math inline">\(0.33\)</span></td>
<td><span class="math inline">\(0.34\)</span></td>
</tr>
<tr class="odd">
<td></td>
<td></td>
<td><span class="math inline">\((0.09)\)</span></td>
<td><span class="math inline">\((0.10)\)</span></td>
<td><span class="math inline">\((0.10)\)</span></td>
<td><span class="math inline">\((0.10)\)</span></td>
</tr>
<tr class="even">
<td><span class="math inline">\(\alpha_{2}\)</span></td>
<td></td>
<td><span class="math inline">\([0.10]\)</span></td>
<td><span class="math inline">\([0.10]\)</span></td>
<td><span class="math inline">\([0.10]\)</span></td>
<td><span class="math inline">\([0.10]\)</span></td>
</tr>
<tr class="odd">
<td></td>
<td></td>
<td></td>
<td><span class="math inline">\(0.14\)</span></td>
<td><span class="math inline">\(0.13\)</span></td>
<td><span class="math inline">\(0.13\)</span></td>
</tr>
<tr class="even">
<td></td>
<td></td>
<td></td>
<td><span class="math inline">\((0.11)\)</span></td>
<td><span class="math inline">\((0.13)\)</span></td>
<td><span class="math inline">\((0.14)\)</span></td>
</tr>
<tr class="odd">
<td><span class="math inline">\(\alpha_{3}\)</span></td>
<td></td>
<td></td>
<td><span class="math inline">\([0.10]\)</span></td>
<td><span class="math inline">\([0.10]\)</span></td>
<td><span class="math inline">\([0.11]\)</span></td>
</tr>
<tr class="even">
<td></td>
<td></td>
<td></td>
<td></td>
<td><span class="math inline">\(0.02\)</span></td>
<td><span class="math inline">\(0.03\)</span></td>
</tr>
<tr class="odd">
<td></td>
<td></td>
<td></td>
<td></td>
<td><span class="math inline">\((0.11)\)</span></td>
<td><span class="math inline">\((0.12)\)</span></td>
</tr>
<tr class="even">
<td><span class="math inline">\(\alpha_{4}\)</span></td>
<td></td>
<td></td>
<td></td>
<td><span class="math inline">\([0.07]\)</span></td>
<td><span class="math inline">\([0.09]\)</span></td>
</tr>
<tr class="odd">
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td><span class="math inline">\(-0.02\)</span></td>
</tr>
<tr class="even">
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td><span class="math inline">\((0.12)\)</span></td>
</tr>
<tr class="odd">
<td>AIC</td>
<td>329</td>
<td>306</td>
<td>305</td>
<td>307</td>
<td>309</td>
</tr>
</tbody>
</table>
<ol type="1">
<li><p>Standard errors robust to heteroskedasticity in parenthesis.</p></li>
<li><p>Newey-West standard errors in square brackets, with <span class="math inline">\(M=5\)</span>.</p></li>
</ol>
<p>rates are positively correlated from quarter to quarter, but only mildly so, and most of the correlation is captured by the first lag. The coefficients of this model are in the real section of Figure 14.6, meaning that the dynamics of the estimated model do not display oscillations. The coefficients of the estimated AR(4) model are nearly identical to the AR(2) model. The conventional and Newey-West standard errors are somewhat different from one another for the AR(0) and AR(4) models, but are nearly identical to one another for the <span class="math inline">\(\operatorname{AR}(1)\)</span> and <span class="math inline">\(\operatorname{AR}(2)\)</span> models</p>
<p>Our second example is real non-durables consumption growth rates <span class="math inline">\(C_{t}\)</span> (growth rate of <span class="math inline">\(p c n d x\)</span> ). This is motivated by an influential paper by Robert Hall (1978) who argued that the permanent income hypothesis implies that changes in consumption should be unpredictable (martingale differences). To test this model Hall (1978) estimated an AR(4) model. Our estimated regression using the full sample <span class="math inline">\((n=231)\)</span> is reported in the following equation.</p>
<p><img src="images//2022_10_23_6047885e7d154c9f28afg-45.jpg" class="img-fluid"></p>
<p>Here, we report heteroskedasticity-robust standard errors. Hall’s hypothesis is that all autoregressive coefficients should be zero. We test this joint hypothesis with an <span class="math inline">\(F\)</span> statistic and find <span class="math inline">\(\mathrm{F}=3.32\)</span> with a p-value of <span class="math inline">\(p=0.012\)</span>. This is significant at the <span class="math inline">\(5 %\)</span> level and close to the <span class="math inline">\(1 %\)</span> level. The first three autoregressive coefficients appear to be positive, but small, indicating positive serial correlation. This evidence is (mildly) inconsistent with Hall’s hypothesis. We report heteroskedasticity-robust standard errors (not Newey-West standard errors) since the purpose was to test the hypothesis of no serial correlation. Table 14.2: U.S. Inflation AR Models</p>
<table class="table">
<thead>
<tr class="header">
<th style="text-align: left;"><span class="math inline">\(\alpha_{0}\)</span></th>
<th>AR(1)</th>
<th>AR(2)</th>
<th>AR(3)</th>
<th>AR(4)</th>
<th>AR(5)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"></td>
<td><span class="math inline">\(0.004\)</span></td>
<td><span class="math inline">\(0.003\)</span></td>
<td><span class="math inline">\(0.003\)</span></td>
<td><span class="math inline">\(0.003\)</span></td>
<td><span class="math inline">\(0.003\)</span></td>
</tr>
<tr class="even">
<td style="text-align: left;"></td>
<td><span class="math inline">\((0.034)\)</span></td>
<td><span class="math inline">\((0.032)\)</span></td>
<td><span class="math inline">\((0.032)\)</span></td>
<td><span class="math inline">\((0.032)\)</span></td>
<td><span class="math inline">\((0.032)\)</span></td>
</tr>
<tr class="odd">
<td style="text-align: left;"></td>
<td><span class="math inline">\([0.023]\)</span></td>
<td><span class="math inline">\([0.028]\)</span></td>
<td><span class="math inline">\([0.029]\)</span></td>
<td><span class="math inline">\([0.031]\)</span></td>
<td><span class="math inline">\([0.032]\)</span></td>
</tr>
<tr class="even">
<td style="text-align: left;"><span class="math inline">\(\alpha_{1}\)</span></td>
<td><span class="math inline">\(-0.26\)</span></td>
<td><span class="math inline">\(-0.36\)</span></td>
<td><span class="math inline">\(-0.36\)</span></td>
<td><span class="math inline">\(-0.36\)</span></td>
<td><span class="math inline">\(-0.37\)</span></td>
</tr>
<tr class="odd">
<td style="text-align: left;"></td>
<td><span class="math inline">\((0.08)\)</span></td>
<td><span class="math inline">\((0.07)\)</span></td>
<td><span class="math inline">\((0.07)\)</span></td>
<td><span class="math inline">\((0.07)\)</span></td>
<td><span class="math inline">\((0.07)\)</span></td>
</tr>
<tr class="even">
<td style="text-align: left;"></td>
<td><span class="math inline">\([0.05]\)</span></td>
<td><span class="math inline">\([0.07]\)</span></td>
<td><span class="math inline">\([0.07]\)</span></td>
<td><span class="math inline">\([0.07]\)</span></td>
<td><span class="math inline">\([0.07]\)</span></td>
</tr>
<tr class="odd">
<td style="text-align: left;"><span class="math inline">\(\alpha_{2}\)</span></td>
<td></td>
<td><span class="math inline">\(-0.36\)</span></td>
<td><span class="math inline">\(-0.37\)</span></td>
<td><span class="math inline">\(-0.42\)</span></td>
<td><span class="math inline">\(-0.43\)</span></td>
</tr>
<tr class="even">
<td style="text-align: left;"></td>
<td></td>
<td><span class="math inline">\((0.07)\)</span></td>
<td><span class="math inline">\((0.06)\)</span></td>
<td><span class="math inline">\((0.06)\)</span></td>
<td><span class="math inline">\((0.06)\)</span></td>
</tr>
<tr class="odd">
<td style="text-align: left;"></td>
<td></td>
<td><span class="math inline">\([0.06]\)</span></td>
<td><span class="math inline">\([0.05]\)</span></td>
<td><span class="math inline">\([0.07]\)</span></td>
<td><span class="math inline">\([0.07]\)</span></td>
</tr>
<tr class="even">
<td style="text-align: left;"><span class="math inline">\(\alpha_{3}\)</span></td>
<td></td>
<td></td>
<td><span class="math inline">\(-0.00\)</span></td>
<td><span class="math inline">\(-0.06\)</span></td>
<td><span class="math inline">\(-0.08\)</span></td>
</tr>
<tr class="odd">
<td style="text-align: left;"></td>
<td></td>
<td></td>
<td><span class="math inline">\((0.09)\)</span></td>
<td><span class="math inline">\((0.10)\)</span></td>
<td><span class="math inline">\((0.11)\)</span></td>
</tr>
<tr class="even">
<td style="text-align: left;"></td>
<td></td>
<td></td>
<td><span class="math inline">\([0.09]\)</span></td>
<td><span class="math inline">\([0.12]\)</span></td>
<td><span class="math inline">\([0.13]\)</span></td>
</tr>
<tr class="odd">
<td style="text-align: left;"><span class="math inline">\(\alpha_{4}\)</span></td>
<td></td>
<td></td>
<td></td>
<td><span class="math inline">\(-0.16\)</span></td>
<td><span class="math inline">\(-0.18\)</span></td>
</tr>
<tr class="even">
<td style="text-align: left;"></td>
<td></td>
<td></td>
<td></td>
<td><span class="math inline">\((0.08)\)</span></td>
<td><span class="math inline">\((0.08)\)</span></td>
</tr>
<tr class="odd">
<td style="text-align: left;"></td>
<td></td>
<td></td>
<td></td>
<td><span class="math inline">\([0.09]\)</span></td>
<td><span class="math inline">\([0.09]\)</span></td>
</tr>
<tr class="even">
<td style="text-align: left;"><span class="math inline">\(\alpha_{5}\)</span></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td><span class="math inline">\(-0.04\)</span></td>
</tr>
<tr class="odd">
<td style="text-align: left;"></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td><span class="math inline">\((0.07)\)</span></td>
</tr>
<tr class="even">
<td style="text-align: left;"></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td><span class="math inline">\([0.06]\)</span></td>
</tr>
<tr class="odd">
<td style="text-align: left;">AIC</td>
<td>342</td>
<td>312</td>
<td>314</td>
<td>310</td>
<td>312</td>
</tr>
</tbody>
</table>
<ol type="1">
<li><p>Standard errors robust to heteroskedasticity in parenthesis.</p></li>
<li><p>Newey-West standard errors in square brackets, with <span class="math inline">\(M=5\)</span>.</p></li>
</ol>
<p>The third example is the first difference of CPI inflation (first difference of growth rate of cpiaucsl). This is motivated by Stock and Watson (2007) who examined forecasting models for inflation rates. We estimate autoregressive models of order 1 through 8 using the full sample ( <span class="math inline">\(n=226)\)</span>; we report models 1 through 5 in Table 14.2. The model with the lowest AIC is the AR(4). All four estimated autoregressive coefficients are negative, most particularly the first two. The two sets of standard errors are quite similar for the AR(4) model. There are meaningful differences only for the lower order AR models.</p>
</section>
<section id="time-series-regression-models" class="level2" data-number="14.40">
<h2 data-number="14.40" class="anchored" data-anchor-id="time-series-regression-models"><span class="header-section-number">14.40</span> Time Series Regression Models</h2>
<p>Least squares regression methods can be used broadly with stationary time series. Interpretation and usefulness can depend, however, on constructive dynamic specifications. Furthermore, it is necessary to be aware of the serial correlation properties of the series involved, and to use the appropriate covariance matrix estimator when the dynamics have not been explicitly modeled.</p>
<p>Let <span class="math inline">\(\left(Y_{t}, X_{t}\right)\)</span> be paired observations with <span class="math inline">\(Y_{t}\)</span> the dependent variable and <span class="math inline">\(X_{t}\)</span> a vector of regressors including an intercept. The regressors can contain lagged <span class="math inline">\(Y_{t}\)</span> so this framework includes the autoregressive model as a special case. A linear regression model takes the form</p>
<p><span class="math display">\[
Y_{t}=X_{t}^{\prime} \beta+e_{t} .
\]</span></p>
<p>The coefficient vector is defined by projection and therefore equals</p>
<p><span class="math display">\[
\beta=\left(\mathbb{E}\left[X_{t} X_{t}^{\prime}\right]\right)^{-1} \mathbb{E}\left[X_{t} Y_{t}\right] .
\]</span></p>
<p>The error <span class="math inline">\(e_{t}\)</span> is defined by (14.54) and thus its properties are determined by that relationship. Implicitly the model assumes that the variables have finite second moments and <span class="math inline">\(\mathbb{E}\left[X_{t} X_{t}^{\prime}\right]&gt;0\)</span>, otherwise the model is not uniquely defined and a regressor could be eliminated. By the property of projection the error is uncorrelated with the regressors <span class="math inline">\(\mathbb{E}\left[X_{t} e_{t}\right]=0\)</span>.</p>
<p>The least squares estimator of <span class="math inline">\(\beta\)</span> is</p>
<p><span class="math display">\[
\widehat{\beta}=\left(\sum_{t=1}^{n} X_{t} X_{t}^{\prime}\right)^{-1}\left(\sum_{t=1}^{n} X_{t} Y_{t}\right) .
\]</span></p>
<p>Under the assumption that the joint series <span class="math inline">\(\left(Y_{t}, X_{t}\right)\)</span> is strictly stationary and ergodic the estimator is consistent. Under the mixing and moment conditions of Theorem <span class="math inline">\(14.32\)</span> the estimator is asymptotically normal with a general covariance matrix</p>
<p>However, under the stronger assumption that the error is a MDS the asymptotic covariance matrix simplifies. It is worthwhile investigating this condition further. The necessary condition is <span class="math inline">\(\mathbb{E}\left[e_{t} \mid \mathscr{F}_{t-1}\right]=\)</span> 0 where <span class="math inline">\(\mathscr{F}_{t-1}\)</span> is an information set to which <span class="math inline">\(\left(e_{t-1}, X_{t}\right)\)</span> is adapted. This notation may appear somewhat odd but recall in the autoregessive context that <span class="math inline">\(X_{t}=\left(1, Y_{t-1}, \ldots, Y_{t-p}\right)\)</span> contains variables dated time <span class="math inline">\(t-1\)</span> and previously, thus <span class="math inline">\(X_{t}\)</span> in this context is a “time <span class="math inline">\(t-1\)</span>” variable. The reason why we need <span class="math inline">\(\left(e_{t-1}, X_{t}\right)\)</span> to be adapted to <span class="math inline">\(\mathscr{F}_{t-1}\)</span> is that for the regression function <span class="math inline">\(X_{t}^{\prime} \beta\)</span> to be the conditional mean of <span class="math inline">\(Y_{t}\)</span> given <span class="math inline">\(\mathscr{F}_{t-1}, X_{t}\)</span> must be part of the information set <span class="math inline">\(\mathscr{F}_{t-1}\)</span>. Under this assumption</p>
<p><span class="math display">\[
\mathbb{E}\left[X_{t} e_{t} \mid \mathscr{F}_{t-1}\right]=X_{t} \mathbb{E}\left[e_{t} \mid \mathscr{F}_{t-1}\right]=0
\]</span></p>
<p>so <span class="math inline">\(\left(X_{t} e_{t}, \mathscr{F}_{t}\right)\)</span> is a MDS. This means we can apply the MDS CLT to obtain the asymptotic distribution.</p>
<p>We summarize this discussion with the following formal statement.</p>
<p>Theorem 14.35 If <span class="math inline">\(\left(Y_{t}, X_{t}\right)\)</span> is strictly stationary, ergodic, with finite second moments, and <span class="math inline">\(\boldsymbol{Q}=\mathbb{E}\left[X_{t} X_{t}^{\prime}\right]&gt;0\)</span>, then <span class="math inline">\(\beta\)</span> in (14.55) is uniquely defined and the least squares estimator is consistent, <span class="math inline">\(\widehat{\beta} \underset{p}{\longrightarrow} \beta\)</span>.</p>
<p>If in addition, <span class="math inline">\(\mathbb{E}\left[e_{t} \mid \mathscr{F}_{t-1}\right]=0\)</span>, where <span class="math inline">\(\mathscr{F}_{t-1}\)</span> is an information set to which <span class="math inline">\(\left(e_{t-1}, X_{t}\right)\)</span> is adapted, <span class="math inline">\(\mathbb{E}\left|Y_{t}\right|^{4}&lt;\infty\)</span>, and <span class="math inline">\(\mathbb{E}\left\|X_{t}\right\|^{4}&lt;\infty\)</span>, then</p>
<p><span class="math display">\[
\sqrt{n}(\widehat{\beta}-\beta) \underset{d}{\longrightarrow} \mathrm{N}\left(0, \boldsymbol{Q}^{-1} \Omega \boldsymbol{Q}^{-1}\right)
\]</span></p>
<p>as <span class="math inline">\(n \rightarrow \infty\)</span>, where <span class="math inline">\(\Omega=\mathbb{E}\left[X_{t} X_{t}^{\prime} e_{t}^{2}\right]\)</span></p>
<p>Alternatively, if for some <span class="math inline">\(r&gt;4\)</span>, <span class="math inline">\(\mathbb{E}\left|Y_{t}\right|^{r}&lt;\infty\)</span>, <span class="math inline">\(\mathbb{E}\left\|X_{t}\right\|^{r}&lt;\infty\)</span>, and the mixing coefficients for <span class="math inline">\(\left(Y_{t}, X_{t}\right)\)</span> satisfy <span class="math inline">\(\sum_{\ell=1}^{\infty} \alpha(\ell)^{1-4 / r}&lt;\infty\)</span>, then (14.56) holds with</p>
<p><span class="math display">\[
\Omega=\sum_{\ell=-\infty}^{\infty} \mathbb{E}\left[X_{t-\ell} X_{t}^{\prime} e_{t} e_{t-\ell}\right] .
\]</span></p>
</section>
<section id="static-distributed-lag-and-autoregressive-distributed-lag-models" class="level2" data-number="14.41">
<h2 data-number="14.41" class="anchored" data-anchor-id="static-distributed-lag-and-autoregressive-distributed-lag-models"><span class="header-section-number">14.41</span> Static, Distributed Lag, and Autoregressive Distributed Lag Models</h2>
<p>In this section we describe standard linear time series regression models.</p>
<p>Let <span class="math inline">\(\left(Y_{t}, Z_{t}\right)\)</span> be paired observations with <span class="math inline">\(Y_{t}\)</span> the dependent variable and <span class="math inline">\(Z_{t}\)</span> an observed regressor vector which does not include lagged <span class="math inline">\(Y_{t}\)</span>.</p>
<p>The simplest regression model is the static equation</p>
<p><span class="math display">\[
Y_{t}=\alpha+Z_{t}^{\prime} \beta+e_{t} .
\]</span></p>
<p>This is (14.54) by setting <span class="math inline">\(X_{t}=\left(1, Z_{t}^{\prime}\right)^{\prime}\)</span>. Static models are motivated to describe how <span class="math inline">\(Y_{t}\)</span> and <span class="math inline">\(Z_{t}\)</span> co-move. Their advantage is their simplicity. The disadvantage is that they are difficult to interpret. The coefficient is the best linear predictor (14.55) but almost certainly is dynamically misspecified. The regression of <span class="math inline">\(Y_{t}\)</span> on contemporeneous <span class="math inline">\(Z_{t}\)</span> is difficult to interpret without a causal framework since the two may be simultaneous. If this regression is estimated it is important that the standard errors be calculated using the Newey-West method to account for serial correlation in the error.</p>
<p>A model which allows the regressor to have impact over several periods is called a distributed lag (DL) model. It takes the form</p>
<p><span class="math display">\[
Y_{t}=\alpha+Z_{t-1}^{\prime} \beta_{1}+Z_{t-2}^{\prime} \beta_{2}+\cdots+Z_{t-q}^{\prime} \beta_{q}+e_{t} .
\]</span></p>
<p>It is also possible to include the contemporenous regressor <span class="math inline">\(Z_{t}\)</span>. In this model the leading coefficient <span class="math inline">\(\beta_{1}\)</span> represents the initial impact of <span class="math inline">\(Z_{t}\)</span> on <span class="math inline">\(Y_{t}, \beta_{2}\)</span> represents the impact in the second period, and so on. The cumulative impact is the sum of the coefficients <span class="math inline">\(\beta_{1}+\cdots+\beta_{q}\)</span> which is called the long-run multiplier.</p>
<p>The distributed lag model falls in the class (14.54) by setting <span class="math inline">\(X_{t}=\left(1, Z_{t-1}^{\prime}, Z_{t-2}^{\prime}, \ldots, Z_{t-q}^{\prime}\right)^{\prime}\)</span>. While it allows for a lagged impact of <span class="math inline">\(Z_{t}\)</span> on <span class="math inline">\(Y_{t}\)</span>, the model does not incorporate serial correlation so the error <span class="math inline">\(e_{t}\)</span> should be expected to be serially correlated. Thus the model is (typically) dynamically misspecified which can make interpretation difficult. It is also necessary to use Newey-West standard errors to account for the serial correlation.</p>
<p>A more complete model combines autoregressive and distributed lags. It takes the form</p>
<p><span class="math display">\[
Y_{t}=\alpha_{0}+\alpha_{1} Y_{t-1}+\cdots+\alpha_{p} Y_{t-p}+Z_{t-1}^{\prime} \beta_{1}+\cdots+Z_{t-q}^{\prime} \beta_{q}+e_{t} .
\]</span></p>
<p>This is called an autoregressive distributed lag (AR-DL) model. It nests both the autoregressive and distributed lag models thereby combining serial correlation and dynamic impact. The AR-DL model falls in the class (14.54) by setting <span class="math inline">\(X_{t}=\left(1, Y_{t-1}, \ldots, Y_{t-p}, Z_{t-1}^{\prime}, \ldots, Z_{t-q}^{\prime}\right)^{\prime}\)</span>.</p>
<p>If the lag orders <span class="math inline">\(p\)</span> and <span class="math inline">\(q\)</span> are selected sufficiently large the AR-DL model will have an error which is approximately white noise in which case the model can be interpreted as dynamically well-specified and conventional standard error methods can be used.</p>
<p>In an AR-DL specification the long-run multiplier is</p>
<p><span class="math display">\[
\frac{\beta_{1}+\cdots+\beta_{q}}{1-\alpha_{1}-\cdots-\alpha_{p}}
\]</span></p>
<p>which is a nonlinear function of the coefficients.</p>
</section>
<section id="time-trends" class="level2" data-number="14.42">
<h2 data-number="14.42" class="anchored" data-anchor-id="time-trends"><span class="header-section-number">14.42</span> Time Trends</h2>
<p>Many economic time series have means which change over time. A useful way to think about this is the components model</p>
<p><span class="math display">\[
Y_{t}=T_{t}+u_{t}
\]</span></p>
<p>where <span class="math inline">\(T_{t}\)</span> is the trend component and <span class="math inline">\(u_{t}\)</span> is the stochastic component. The latter can be modeled by a linear process or autoregression</p>
<p><span class="math display">\[
\alpha(\mathrm{L}) u_{t}=e_{t}
\]</span></p>
<p>The trend component is often modeled as a linear function in the time index</p>
<p><span class="math display">\[
T_{t}=\beta_{0}+\beta_{1} t
\]</span></p>
<p>or a quadratic function in time</p>
<p><span class="math display">\[
T_{t}=\beta_{0}+\beta_{1} t+\beta_{2} t^{2} .
\]</span></p>
<p>These models are typically not thought of as being literally true but rather as useful approximations.</p>
<p>When we write down time series models we write the index as <span class="math inline">\(t=1, \ldots, n\)</span>. But in practical applications the time index corresponds to a date, e.g.&nbsp;<span class="math inline">\(t=1960,1961, \ldots, 2017\)</span>. Furthermore, if the data is at a higher frequency than annual then it is incremented in fractional units. This is not of fundamental importance; it merely changes the meaning of the intercept <span class="math inline">\(\beta_{0}\)</span> and slope <span class="math inline">\(\beta_{1}\)</span>. Consequently these should not be interpreted outside of how the time index is defined.</p>
<p>One traditional way of dealing with time trends is to “detrend” the data. This means using an estimation method to estimate the trend and subtract it off. The simplest method is least squares linear detrending. Given the linear model</p>
<p><span class="math display">\[
Y_{t}=\beta_{0}+\beta_{1} t+u_{t}
\]</span></p>
<p>the coefficients are estimated by least squares. The detrended series is the residual <span class="math inline">\(\widehat{u}_{t}\)</span>. More intricate methods can be used but they have a similar flavor.</p>
<p>To understand the properties of the detrending method we can apply an asymptotic approximation. A time trend is not a stationary process so we should be thoughtful before applying standard theory. We will study asymptotics for non-stationary processes in more detail in Chapter 16 so our treatment here will be brief. It turns out that most of our conventional procedures work just fine with time trends (and quadratics in time) as regressors. The rates of convergence change but this does not affect anything of practical importance.</p>
<p>Let us demonstrate that the least squares estimator of the coefficients in (14.57) is consistent. We can write the estimator as</p>
<p><span class="math display">\[
\left(\begin{array}{c}
\widehat{\beta}_{0}-\beta_{0} \\
\widehat{\beta}_{1}-\beta_{1}
\end{array}\right)=\left(\begin{array}{cc}
n &amp; \sum_{t=1}^{n} t \\
\sum_{t=1}^{n} t &amp; \sum_{t=1}^{n} t^{2}
\end{array}\right)^{-1}\left(\begin{array}{c}
\sum_{t=1}^{n} u_{t} \\
\sum_{t=1}^{n} t u_{t}
\end{array}\right) .
\]</span></p>
<p>We need to study the behavior of the sums in the design matrix. For this the following result is useful, which follows by taking the limit of the Riemann sum for the integral <span class="math inline">\(\int_{0}^{1} x^{r} d x=1 /(1+r)\)</span>.</p>
<p>Theorem 14.36 For any <span class="math inline">\(r&gt;0\)</span>, as <span class="math inline">\(n \rightarrow \infty, n^{-1-r} \sum_{t=1}^{n} t^{r} \longrightarrow 1 /(1+r)\)</span>.</p>
<p>Theorem <span class="math inline">\(14.36\)</span> implies that</p>
<p><span class="math display">\[
\frac{1}{n^{2}} \sum_{t=1}^{n} t \rightarrow \frac{1}{2}
\]</span></p>
<p>and</p>
<p><span class="math display">\[
\frac{1}{n^{3}} \sum_{t=1}^{n} t^{2} \rightarrow \frac{1}{3} .
\]</span></p>
<p>What is interesting about these results is that the sums require normalizations other than <span class="math inline">\(n^{-1}\)</span> ! To handle this in multiple regression it is convenient to define a scaling matrix which normalizes each element in the regression by its convergence rate. Define the matrix <span class="math inline">\(D_{n}=\left[\begin{array}{ll}1 &amp; 0 \\ 0 &amp; n\end{array}\right]\)</span>. The first diagonal element is the intercept and second for the time trend. Then</p>
<p><span class="math display">\[
\begin{aligned}
D_{n}\left(\begin{array}{c}
\widehat{\beta}_{0}-\beta_{0} \\
\widehat{\beta}_{1}-\beta_{1}
\end{array}\right) &amp;=D_{n}\left(\begin{array}{cc}
n &amp; \sum_{t=1}^{n} t \\
\sum_{t=1}^{n} t &amp; \sum_{t=1}^{n} t^{2}
\end{array}\right)^{-1} D_{n} D_{n}^{-1}\left(\begin{array}{c}
\sum_{t=1}^{n} u_{t} \\
\sum_{t=1}^{n} t u_{t}
\end{array}\right) \\
&amp;=\left(D_{n}^{-1}\left(\begin{array}{cc}
n &amp; \sum_{t=1}^{n} t \\
\sum_{t=1}^{n} t &amp; \sum_{t=1}^{n} t^{2}
\end{array}\right)_{n}^{-1}\right)^{-1}\left(\begin{array}{c}
\sum_{t=1}^{n} u_{t} \\
\frac{1}{n} \sum_{t=1}^{n} t u_{t}
\end{array}\right) \\
&amp;=\left(\begin{array}{cc}
n &amp; \frac{1}{n} \sum_{t=1}^{n} t \\
\frac{1}{n} \sum_{t=1}^{n} t &amp; \frac{1}{n^{2}} \sum_{t=1}^{n} t^{2}
\end{array}\right)^{-1}\left(\begin{array}{c}
\sum_{i=1}^{n} u_{t} \\
\frac{1}{n} \sum_{i=1}^{n} t u_{t}
\end{array}\right)
\end{aligned}
\]</span></p>
<p>Multiplying by <span class="math inline">\(n^{1 / 2}\)</span> we obtain</p>
<p><span class="math display">\[
\left(\begin{array}{c}
n^{1 / 2}\left(\widehat{\beta}_{0}-\beta_{0}\right) \\
n^{3 / 2}\left(\widehat{\beta}_{1}-\beta_{1}\right)
\end{array}\right)=\left(\begin{array}{cc}
1 &amp; \frac{1}{n^{2}} \sum_{t=1}^{n} t \\
\frac{1}{n^{2}} \sum_{t=1}^{n} t &amp; \frac{1}{n^{3}} \sum_{t=1}^{n} t^{2}
\end{array}\right)^{-1}\left(\begin{array}{c}
\frac{1}{n_{1}^{1 / 2}} \sum_{t=1}^{n} u_{t} \\
\frac{n^{3 / 2}}{n} \sum_{t=1}^{n} t u_{t}
\end{array}\right)
\]</span></p>
<p>The denominator matrix satisfies</p>
<p><span class="math display">\[
\left(\begin{array}{cc}
1 &amp; \frac{1}{n^{2}} \sum_{t=1}^{n} t \\
\frac{1}{n^{2}} \sum_{t=1}^{n} t &amp; \frac{1}{n^{3}} \sum_{t=1}^{n} t^{2}
\end{array}\right) \rightarrow\left(\begin{array}{cc}
1 &amp; \frac{1}{2} \\
\frac{1}{2} &amp; \frac{1}{3}
\end{array}\right)
\]</span></p>
<p>which is invertible. Setting <span class="math inline">\(X_{n t}=(t / n, 1)\)</span>, the numerator vector can be written as <span class="math inline">\(n^{-1 / 2} \sum_{t=1}^{n} X_{n t} u_{t}\)</span>. It has variance</p>
<p><span class="math display">\[
\begin{aligned}
\left\|\operatorname{var}\left[\frac{1}{n^{1 / 2}} \sum_{t=1}^{n} X_{n t} u_{t}\right]\right\| &amp;=\left\|\frac{1}{n} \sum_{t=1}^{n} \sum_{j=1}^{n} X_{n t} X_{n j}^{\prime} \mathbb{E}\left[u_{t} u_{j}\right]\right\| \\
&amp; \leq \sqrt{2} \sum_{\ell=-\infty}^{\infty}\left\|\mathbb{E}\left[u_{t} u_{j}\right]\right\|&lt;\infty
\end{aligned}
\]</span></p>
<p>by Theorem <span class="math inline">\(14.15\)</span> if <span class="math inline">\(u_{t}\)</span> satisfies the mixing and moment conditions for the central limit theorem. This means that the numerator vector is <span class="math inline">\(O_{p}(1)\)</span>. (It is also asymptotically normal but we defer this demonstration for now.) We conclude that</p>
<p><span class="math display">\[
\left(\begin{array}{c}
n^{1 / 2}\left(\widehat{\beta}_{0}-\beta_{0}\right) \\
n^{3 / 2}\left(\widehat{\beta}_{1}-\beta_{1}\right)
\end{array}\right)=O_{p}(1)
\]</span></p>
<p>This shows that both coefficients are consistent, <span class="math inline">\(\widehat{\beta}_{0}\)</span> converges at the standard <span class="math inline">\(n^{1 / 2}\)</span> rate, and <span class="math inline">\(\widehat{\beta}_{1}\)</span> converges at the faster <span class="math inline">\(n^{3 / 2}\)</span> rate.</p>
<p>The consistency of the coefficient estimators (and their rates of convergence) can be used to show that linear detrending (regression of <span class="math inline">\(Y_{t}\)</span> on an intercept and time trend to obtain a residual <span class="math inline">\(\widehat{u}_{t}\)</span> ) is consistent for the error <span class="math inline">\(u_{t}\)</span> in (14.57).</p>
<p>An alternative is to include a time trend in the estimated regression. If we have an autoregression, a distributed lag, or an AL-DL model, we add a time index to obtain a model of the form</p>
<p><span class="math display">\[
Y_{t}=\alpha_{0}+\alpha_{1} Y_{t-1}+\cdots+\alpha_{p} Y_{t-p}+Z_{t-1}^{\prime} \beta_{1}+\cdots+Z_{t-q}^{\prime} \beta_{q}+\gamma t+e_{t} .
\]</span></p>
<p>Estimation by least squares is equivalent to estimation after linear detrending by the FWL theorem. Inclusion of a linear (and possibly quadratic) time trend in a regression model is typically the easiest method to incorporate time trends.</p>
</section>
<section id="illustration" class="level2" data-number="14.43">
<h2 data-number="14.43" class="anchored" data-anchor-id="illustration"><span class="header-section-number">14.43</span> Illustration</h2>
<p>We illustrate the models described in the previous section using a classical Phillips curve for inflation prediction. A. W. Phillips (1958) famously observed that the unemployment rate and the wage inflation rate are negatively correlated over time. Equations relating the inflation rate, or the change in the inflation rate, to macroeconomic indicators such as the unemployment rate are typically described as “Phillips curves”. A simple Phillips curve takes the form</p>
<p><span class="math display">\[
\Delta \pi_{t}=\alpha+\beta U_{t}+e_{t}
\]</span></p>
<p>where <span class="math inline">\(\pi_{t}\)</span> is price inflation and <span class="math inline">\(U_{t}\)</span> is the unemployment rate. This specification relates the change in inflation in a given period to the level of the unemployment rate in the previous period.</p>
<p>The least squares estimate of (14.58) using U.S. quarterly series from FRED-QD is reported in the first column of Table 14.3. Both heteroskedasticity-robust and Newey-West standard errors are reported. The Newey-West standard errors are the appropriate choice since the estimated equation is static - no modeling of the serial correlation. In this example the measured impact of the unemployment rate on inflation appears minimal. The estimate is consistent with a small effect of the unemployment rate on the inflation rate but it is not precisely estimated.</p>
<p>A distributed lag (DL) model takes the form</p>
<p><span class="math display">\[
\Delta \pi_{t}=\alpha+\beta_{1} U_{t-1}+\beta_{2} U_{t-2}+\cdots+\beta_{q} U_{t-q}+e_{t} .
\]</span></p>
<p>The least squares estimate of (14.59) is reported in the second column of Table 14.3. The estimates are quite different from the static model. We see large negative impacts in the first and third periods, countered by a large positive impact in the second period. The model suggests that the unemployment rate has a strong impact on the inflation rate but the long-run impact is mitigated. The long-run multiplier is reported at the bottom of the column. The point estimate of <span class="math inline">\(-0.022\)</span> is quite small and similar to the static estimate. It implies that an increase in the unemployment rate by 5 percentage points (a typical recession) decreases the long-run annual inflation rate by about a half of a percentage point.</p>
<p>An AR-DL takes the form</p>
<p><span class="math display">\[
\Delta \pi_{t}=\alpha_{0}+\alpha_{1} \Delta \pi_{t-1}+\cdots+\alpha_{p} \Delta \pi_{t-p}+\beta_{1} U_{t-1}+\cdots+\beta_{q} U_{t-q}+e_{t} .
\]</span></p>
<p>The least squares estimate of <span class="math inline">\((14.60)\)</span> is reported in the third column of Table 14.3. The coefficient estimates are similar to those from the distributed lag model. The point estimate of the long-run multiplier is also nearly identical but with a smaller standard error.</p>
</section>
<section id="granger-causality" class="level2" data-number="14.44">
<h2 data-number="14.44" class="anchored" data-anchor-id="granger-causality"><span class="header-section-number">14.44</span> Granger Causality</h2>
<p>In the AR-DL model (14.60) the unemployment rate has no predictive impact on the inflation rate under the coefficient restriction <span class="math inline">\(\beta_{1}=\cdots=\beta_{q}=0\)</span>. This restriction is called Granger non-causality. When the coefficients are non-zero we say that the unemployment rate “Granger causes” the inflation rate. This definition of causality was developed by Granger (1969) and Sims (1972).</p>
<p>The reason why we call this “Granger causality” rather than “causality” is because this is not a structural definition. An alternative label is “predictive causality”.</p>
<p>To be precise, assume that we have two series <span class="math inline">\(\left(Y_{t}, Z_{t}\right)\)</span>. Consider the projection of <span class="math inline">\(Y_{t}\)</span> onto the lagged history of both series</p>
<p><span class="math display">\[
\begin{aligned}
Y_{t} &amp;=\mathscr{P}_{t-1}\left(Y_{t}\right)+e_{t} \\
&amp;=\alpha_{0}+\sum_{j=1}^{\infty} \alpha_{j} Y_{t-j}+\sum_{j=1}^{\infty} \beta_{j} Z_{t-j}+e_{t}
\end{aligned}
\]</span></p>
<p>Table 14.3: Phillips Curve Regressions</p>
<p><img src="images//2022_10_23_6047885e7d154c9f28afg-52.jpg" class="img-fluid"></p>
<ol type="1">
<li><p>Standard errors robust to heteroskedasticity in parenthesis.</p></li>
<li><p>Newey-West standard errors in square brackets with <span class="math inline">\(M=5\)</span>. We say that <span class="math inline">\(Z_{t}\)</span> does not Granger-cause <span class="math inline">\(Y_{t}\)</span> if <span class="math inline">\(\beta_{j}=0\)</span> for all <span class="math inline">\(j\)</span>. If <span class="math inline">\(\beta_{j} \neq 0\)</span> for some <span class="math inline">\(j\)</span> then we say that <span class="math inline">\(Z_{t}\)</span> Granger-causes <span class="math inline">\(Y_{t}\)</span>.</p></li>
</ol>
<p>It is important that the definition includes the projection on the past history of <span class="math inline">\(Y_{t}\)</span>. Granger causality means that <span class="math inline">\(Z_{t}\)</span> helps to predict <span class="math inline">\(Y_{t}\)</span> even after the past history of <span class="math inline">\(Y_{t}\)</span> has been accounted for.</p>
<p>The definition can alternatively be written in terms of conditional expectations rather than projections. We can say that <span class="math inline">\(Z_{t}\)</span> does not Granger-cause <span class="math inline">\(Y_{t}\)</span> if</p>
<p><span class="math display">\[
\mathbb{E}\left[Y_{t} \mid Y_{t-1}, Y_{t-2} \ldots ; Z_{t-1}, Z_{t-2}, \ldots\right]=\mathbb{E}\left[Y_{t} \mid Y_{t-1}, Y_{t-2}, \ldots\right] .
\]</span></p>
<p>Granger causality can be tested in AR-DL models using a standard Wald or F test. In the context of model (14.60) we report the F statistic for <span class="math inline">\(\beta_{1}=\cdots=\beta_{q}=0\)</span>. The test rejects the hypothesis (and thus finds evidence of Granger causality) if the statistic is larger than the critical value (if the p-value is small) and fails to reject the hypothesis (and thus finds no evidence of causality) if the statistic is smaller than the critical value.</p>
<p>For example, in the results presented in Table <span class="math inline">\(14.3\)</span> the F statistic for the hypothesis <span class="math inline">\(\beta_{1}=\cdots=\beta_{4}=0\)</span> using the Newey-West covariance matrix is <span class="math inline">\(\mathrm{F}=6.98\)</span> with a p-value of <span class="math inline">\(0.000\)</span>. This is statistically significant at any conventional level so we can conclude that the unemployment rate has a predictively causal impact on inflation.</p>
<p>Granger causality should not be interpreted structurally outside the context of an economic model. For example consider the regression of GDP growth rates <span class="math inline">\(Y_{t}\)</span> on stock price growth rates <span class="math inline">\(R_{t}\)</span>. We use the quarterly series from FRED-QD, estimating an AR-DL specification with two lags</p>
<p><img src="images//2022_10_23_6047885e7d154c9f28afg-53.jpg" class="img-fluid"></p>
<p>The coefficients on the lagged stock price growth rates are small in magnitude but the first lag appears statistically significant. The <span class="math inline">\(\mathrm{F}\)</span> statistic for exclusion of <span class="math inline">\(\left(R_{t-1}, R_{t-2}\right)\)</span> is <span class="math inline">\(F=9.3\)</span> with a <span class="math inline">\(\mathrm{p}\)</span>-value of <span class="math inline">\(0.0002\)</span>, which is highly significant. We can therefore reject the hypothesis of no Granger causality and deduce that stock prices Granger-cause GDP growth. We should be wary of concluding that this is structurally causal - that stock market movements cause output fluctuations. A more reasonable explanation from economic theory is that stock prices are forward-looking measures of expected future profits. When corporate profits are forecasted to rise the value of corporate stock rises, bidding up stock prices. Thus stock prices move in advance of actual economic activity but are not necessarily structurally causal.</p>
<p><img src="images//2022_10_23_6047885e7d154c9f28afg-53(1).jpg" class="img-fluid"></p>
</section>
<section id="testing-for-serial-correlation-in-regression-models" class="level2" data-number="14.45">
<h2 data-number="14.45" class="anchored" data-anchor-id="testing-for-serial-correlation-in-regression-models"><span class="header-section-number">14.45</span> Testing for Serial Correlation in Regression Models</h2>
<p>Consider the problem of testing for omitted serial correlation in an AR-DL model such as</p>
<p><span class="math display">\[
Y_{t}=\alpha_{0}+\alpha_{1} Y_{t-1}+\cdots+\alpha_{p} Y_{t-p}+\beta_{1} Z_{t-1}+\cdots+\beta_{q} Z_{t-q}+u_{t} .
\]</span></p>
<p>The null hypothesis is that <span class="math inline">\(u_{t}\)</span> is serially uncorrelated and the alternative hypothesis is that it is serially correlated. We can model the latter as a mean-zero autoregressive process</p>
<p><span class="math display">\[
u_{t}=\theta_{1} u_{t-1}+\cdots+\theta_{r} u_{t-r}+e_{t} .
\]</span></p>
<p>The hypothesis is</p>
<p><span class="math display">\[
\begin{aligned}
&amp;\mathbb{H}_{0}: \theta_{1}=\cdots=\theta_{r}=0 \\
&amp;\mathbb{H}_{1}: \theta_{j} \neq 0 \text { for some } j \geq 1
\end{aligned}
\]</span></p>
<p>There are two ways to implement a test of <span class="math inline">\(\mathbb{H}_{0}\)</span> against <span class="math inline">\(\mathbb{H}_{1}\)</span>. The first is to estimate equations (14.61)(14.62) sequentially by least squares and construct a test for <span class="math inline">\(\mathbb{H}_{0}\)</span> on the second equation. This test is complicated by the two-step estimation. Therefore this approach is not recommended.</p>
<p>The second approach is to combine equations (14.61)-(14.62) into a single model and execute the test as a restriction within this model. One way to make this combination is by using lag operator notation. Write (14.61)-(14.62) as</p>
<p><span class="math display">\[
\begin{aligned}
&amp;\alpha(\mathrm{L}) Y_{t}=\alpha_{0}+\beta(\mathrm{L}) Z_{t-1}+u_{t} \\
&amp;\theta(\mathrm{L}) u_{t}=e_{t}
\end{aligned}
\]</span></p>
<p>Applying the operator <span class="math inline">\(\theta(\mathrm{L})\)</span> to the first equation we obtain</p>
<p><span class="math display">\[
\theta(\mathrm{L}) \alpha(\mathrm{L}) Y_{t}=\theta(\mathrm{L}) \alpha_{0}+\theta(\mathrm{L}) \beta(\mathrm{L}) Z_{t-1}+\theta(\mathrm{L}) u_{t}
\]</span></p>
<p>or</p>
<p><span class="math display">\[
\alpha^{*}(\mathrm{~L}) Y_{t}=\alpha_{0}^{*}+\beta^{*}(\mathrm{~L}) Z_{t-1}+e_{t}
\]</span></p>
<p>where <span class="math inline">\(\alpha^{*}(\mathrm{~L})\)</span> is a <span class="math inline">\(p+r\)</span> order polynomial and <span class="math inline">\(\beta^{*}(\mathrm{~L})\)</span> is a <span class="math inline">\(q+r\)</span> order polynomial. The restriction <span class="math inline">\(\mathbb{H}_{0}\)</span> is that these are <span class="math inline">\(p\)</span> and <span class="math inline">\(q\)</span> order polynomials. Thus we can implement a test of <span class="math inline">\(\mathbb{H}_{0}\)</span> against <span class="math inline">\(\mathbb{H}_{1}\)</span> by estimating an AR-DL model with <span class="math inline">\(p+r\)</span> and <span class="math inline">\(q+r\)</span> lags, and testing the exclusion of the final <span class="math inline">\(r\)</span> lags of <span class="math inline">\(Y_{t}\)</span> and <span class="math inline">\(Z_{t}\)</span>. This test has a conventional asymptotic distribution so is simple to implement.</p>
<p>The basic message is that testing for omitted serial correlation can be implement in regression models by estimating and contrasting different dynamic specifications.</p>
</section>
<section id="bootstrap-for-time-series" class="level2" data-number="14.46">
<h2 data-number="14.46" class="anchored" data-anchor-id="bootstrap-for-time-series"><span class="header-section-number">14.46</span> Bootstrap for Time Series</h2>
<p>Recall that the bootstrap approximates the sampling distribution of estimators and test statistics by the empirical distribution of the observations. The traditional nonparametric bootstrap is appropriate for independent observations. For dependent observations alternative methods should be used.</p>
<p>Bootstrapping for time series is considerably more complicated than the cross section case. Many methods have been proposed. One of the challenges is that theoretical justifications are more difficult to establish than in the independent observation case.</p>
<p>In this section we describe the most popular methods to implement bootstrap resampling for time series data.</p>
</section>
<section id="recursive-bootstrap" class="level2" data-number="14.47">
<h2 data-number="14.47" class="anchored" data-anchor-id="recursive-bootstrap"><span class="header-section-number">14.47</span> Recursive Bootstrap</h2>
<ol type="1">
<li><p>Estimate a complete model such as an <span class="math inline">\(\mathrm{AR}(\mathrm{p})\)</span> producing coefficient estimates <span class="math inline">\(\widehat{\alpha}\)</span> and residuals <span class="math inline">\(\widehat{e}_{t}\)</span>.</p></li>
<li><p>Fix the initial condition <span class="math inline">\(\left(Y_{-p+1}, Y_{-p+2}, \ldots, Y_{0}\right)\)</span>.</p></li>
<li><p>Simulate i.i.d. draws <span class="math inline">\(e_{t}^{*}\)</span> from the empirical distribution of the residuals <span class="math inline">\(\left\{\widehat{e}_{1}, \ldots, \widehat{e}_{n}\right\}\)</span>.</p></li>
<li><p>Create the bootstrap series <span class="math inline">\(Y_{t}^{*}\)</span> by the recursive formula</p></li>
</ol>
<p><span class="math display">\[
Y_{t}^{*}=\widehat{\alpha}_{0}+\widehat{\alpha}_{1} Y_{t-1}^{*}+\widehat{\alpha}_{2} Y_{t-2}^{*}+\cdots+\widehat{\alpha}_{p} Y_{t-p}^{*}+e_{t}^{*} .
\]</span></p>
<p>This construction creates bootstrap samples <span class="math inline">\(Y_{t}^{*}\)</span> with the stochastic properties of the estimated AR(p) model including the auxiliary assumption that the errors are i.i.d. This method can work well if the true process is an <span class="math inline">\(\mathrm{AR}(\mathrm{p})\)</span>. One flaw is that it imposes homoskedasticity on the errors <span class="math inline">\(e_{t}^{*}\)</span> which may be different than the properties of the actual <span class="math inline">\(e_{t}\)</span>. Another limitation is that it is inappropriate for AR-DL models unless the conditioning variables are strictly exogenous.</p>
<p>There are alternative versions of this basic method. First, instead of fixing the initial conditions at the sample values a random block can be drawn from the sample. The difference is that this produces an unconditional distribution rather than a conditional one. Second, instead of drawing the errors from the residuals a parametric (typically normal) distribution can be used. This can improve precision when sample sizes are small but otherwise is not recommended.</p>
</section>
<section id="pairwise-bootstrap" class="level2" data-number="14.48">
<h2 data-number="14.48" class="anchored" data-anchor-id="pairwise-bootstrap"><span class="header-section-number">14.48</span> Pairwise Bootstrap</h2>
<ol type="1">
<li><p>Write the sample as <span class="math inline">\(\left\{Y_{t}, X_{t}\right\}\)</span> where <span class="math inline">\(X_{t}=\left(Y_{t-1}, \ldots, Y_{t-p}\right)^{\prime}\)</span> contains the lagged values used in estimation.</p></li>
<li><p>Apply the traditional nonparametric bootstrap which samples pairs <span class="math inline">\(\left(Y_{t}^{*}, X_{t}^{*}\right)\)</span> i.i.d. from <span class="math inline">\(\left\{Y_{t}, X_{t}\right\}\)</span> with replacement to create the bootstrap sample.</p></li>
<li><p>Create the bootstrap estimates on this bootstrap sample, e.g.&nbsp;regress <span class="math inline">\(Y_{t}^{*}\)</span> on <span class="math inline">\(X_{t}^{*}\)</span>.</p></li>
</ol>
<p>This construction is essentially the traditional nonparametric bootstrap but applied to the paired sample <span class="math inline">\(\left\{Y_{t}, X_{t}\right\}\)</span>. It does not mimic the time series correlations across observations. However, it does produce bootstrap statistics with the correct first-order asymptotic distribution under MDS errors. This method may be useful when we are interested in the distribution of nonlinear functions of the coefficient estimates and therefore desire an improvement on the Delta Method approximation.</p>
</section>
<section id="fixed-design-residual-bootstrap" class="level2" data-number="14.49">
<h2 data-number="14.49" class="anchored" data-anchor-id="fixed-design-residual-bootstrap"><span class="header-section-number">14.49</span> Fixed Design Residual Bootstrap</h2>
<ol type="1">
<li><p>Write the sample as <span class="math inline">\(\left\{Y_{t}, X_{t}, \widehat{e}_{t}\right\}\)</span> where <span class="math inline">\(X_{t}=\left(Y_{t-1}, \ldots, Y_{t-p}\right)^{\prime}\)</span> contains the lagged values used in estimation and <span class="math inline">\(\widehat{e}_{t}\)</span> are the residuals.</p></li>
<li><p>Fix the regressors <span class="math inline">\(X_{t}\)</span> at their sample values.</p></li>
<li><p>Simulate i.i.d. draws <span class="math inline">\(e_{t}^{*}\)</span> from the empirical distribution of the residuals <span class="math inline">\(\left\{\widehat{e}_{1}, \ldots, \widehat{e}_{n}\right\}\)</span>.</p></li>
<li><p>Set <span class="math inline">\(Y_{t}^{*}=X_{t}^{\prime} \widehat{\beta}+e_{t}^{*}\)</span>.</p></li>
</ol>
<p>This construction is similar to the pairwise bootstrap but imposes an i.i.d. error. It is therefore only valid when the errors are i.i.d. (and thus excludes heteroskedasticity).</p>
</section>
<section id="fixed-design-wild-bootstrap" class="level2" data-number="14.50">
<h2 data-number="14.50" class="anchored" data-anchor-id="fixed-design-wild-bootstrap"><span class="header-section-number">14.50</span> Fixed Design Wild Bootstrap</h2>
<ol type="1">
<li><p>Write the sample as <span class="math inline">\(\left\{Y_{t}, X_{t}, \widehat{e}_{t}\right\}\)</span> where <span class="math inline">\(X_{t}=\left(Y_{t-1}, \ldots, Y_{t-p}\right)^{\prime}\)</span> contains the lagged values used in estimation and <span class="math inline">\(\widehat{e}_{t}\)</span> are the residuals.</p></li>
<li><p>Fix the regressors <span class="math inline">\(X_{t}\)</span> and residuals <span class="math inline">\(\widehat{e}_{t}\)</span> at their sample values.</p></li>
<li><p>Simulate i.i.d. auxiliary random variables <span class="math inline">\(\xi_{t}^{*}\)</span> with mean zero and variance one. See Section <span class="math inline">\(10.29\)</span> for a discussion of choices.</p></li>
<li><p>Set <span class="math inline">\(e_{t}^{*}=\xi_{t}^{*} \widehat{e}_{t}\)</span> and <span class="math inline">\(Y_{t}^{*}=X_{t}^{\prime} \widehat{\beta}+e_{t}^{*}\)</span></p></li>
</ol>
<p>This construction is similar to the pairwise and fixed design bootstrap combined with the wild bootstrap. This imposes the conditional mean assumption on the error but allows heteroskedasticity.</p>
</section>
<section id="block-bootstrap" class="level2" data-number="14.51">
<h2 data-number="14.51" class="anchored" data-anchor-id="block-bootstrap"><span class="header-section-number">14.51</span> Block Bootstrap</h2>
<ol type="1">
<li><p>Write the sample as <span class="math inline">\(\left\{Y_{t}, X_{t}\right\}\)</span> where <span class="math inline">\(X_{t}=\left(Y_{t-1}, \ldots, Y_{t-p}\right)^{\prime}\)</span> contains the lagged values used in estimation.</p></li>
<li><p>Divide the sample of paired observations <span class="math inline">\(\left\{Y_{t}, X_{t}\right\}\)</span> into <span class="math inline">\(n / m\)</span> blocks of length <span class="math inline">\(m\)</span>.</p></li>
<li><p>Resample complete blocks. For each simulated sample draw <span class="math inline">\(n / m\)</span> blocks.</p></li>
<li><p>Paste the blocks together to create the bootstrap time series <span class="math inline">\(\left\{Y_{t}^{*}, X_{t}^{*}\right\}\)</span>.</p></li>
</ol>
<p>This construction allows for arbitrary stationary serial correlation, heteroskedasticity, and modelmisspecification. One challenge is that the block bootstrap is sensitive to the block length and the way that the data are partitioned into blocks. The method may also work less well in small samples. Notice that the block bootstrap with <span class="math inline">\(m=1\)</span> is equal to the pairwise bootstrap and the latter is the traditional nonparametric bootstrap. Thus the block bootstrap is a natural generalization of the nonparametric bootstrap.</p>
</section>
<section id="technical-proofs" class="level2" data-number="14.52">
<h2 data-number="14.52" class="anchored" data-anchor-id="technical-proofs"><span class="header-section-number">14.52</span> Technical Proofs*</h2>
<p>Proof of Theorem 14.2 Define <span class="math inline">\(\tilde{Y}_{t}=\left(Y_{t}, Y_{t-1}, Y_{t-2}, \ldots\right) \in \mathbb{R}^{m \times \infty}\)</span> as the history of <span class="math inline">\(Y_{t}\)</span> up to time <span class="math inline">\(t\)</span>. Write <span class="math inline">\(X_{t}=\phi\left(\widetilde{Y}_{t}\right)\)</span>. Let <span class="math inline">\(B\)</span> be the pre-image of <span class="math inline">\(\left\{X_{t} \leq x\right\}\)</span> (the vectors <span class="math inline">\(\widetilde{Y} \in \mathbb{R}^{m \times \infty}\)</span> such that <span class="math inline">\(\left.\phi(\widetilde{Y}) \leq x\right)\)</span>. Then</p>
<p><span class="math display">\[
\mathbb{P}\left[X_{t} \leq x\right]=\mathbb{P}\left[\phi\left(\widetilde{Y}_{t}\right) \leq x\right]=\mathbb{P}\left[\tilde{Y}_{t} \in B\right] .
\]</span></p>
<p>Since <span class="math inline">\(Y_{t}\)</span> is strictly stationary, <span class="math inline">\(\mathbb{P}\left[\tilde{Y}_{t} \in B\right]\)</span> is independent <span class="math inline">\({ }^{9}\)</span> of <span class="math inline">\(t\)</span>. This means that the distribution of <span class="math inline">\(X_{t}\)</span> is independent of <span class="math inline">\(t\)</span>. This argument can be extended to show that the distribution of <span class="math inline">\(\left(X_{t}, \ldots, X_{t+\ell}\right)\)</span> is independent of <span class="math inline">\(t\)</span>. This means that <span class="math inline">\(X_{t}\)</span> is strictly stationary as claimed.</p>
<p>Proof of Theorem 14.3 By the Cauchy criterion for convergence (see Theorem A.2 of Probability and Statistics for Economists), <span class="math inline">\(S_{N}=\sum_{j=0}^{N} a_{j} Y_{t-j}\)</span> converges almost surely if for all <span class="math inline">\(\epsilon&gt;0\)</span>,</p>
<p><span class="math display">\[
\inf _{N} \sup _{j&gt;N}\left|S_{N+j}-S_{N}\right| \leq \epsilon .
\]</span></p>
<p><span class="math inline">\({ }^{9}\)</span> An astute reader may notice that the independence of <span class="math inline">\(\mathbb{P}\left[\widetilde{Y}_{t} \in B\right]\)</span> from <span class="math inline">\(t\)</span> does not follow directly from the definition of strict stationarity. Indeed, a full derivation requires a measure-theoretic treatment. See Section 1.2.B of Petersen (1983) or Section <span class="math inline">\(3.5\)</span> of Stout (1974). Let <span class="math inline">\(A_{\epsilon}\)</span> be this event. Its complement is</p>
<p><span class="math display">\[
A_{\epsilon}^{c}=\bigcap_{N=1}^{\infty}\left\{\sup _{j&gt;N}\left|\sum_{i=N+1}^{N+j} a_{i} Y_{t-i}\right|&gt;\epsilon\right\} .
\]</span></p>
<p>This has probability</p>
<p><span class="math display">\[
\mathbb{P}\left[A_{\epsilon}^{c}\right] \leq \lim _{N \rightarrow \infty} \mathbb{P}\left[\sup _{j&gt;N}\left|\sum_{i=N+1}^{N+j} a_{i} Y_{t-i}\right|&gt;\epsilon\right] \leq \lim _{N \rightarrow \infty} \frac{1}{\epsilon} \mathbb{E}\left[\sup _{j&gt;N}\left|\sum_{i=N+1}^{N+j} a_{i} Y_{t-i}\right|\right] \leq \frac{1}{\epsilon} \lim _{N \rightarrow \infty} \sum_{i=N+1}^{\infty}\left|a_{i}\right| \mathbb{E}\left|Y_{t-i}\right|=0 .
\]</span></p>
<p>The second equality is Markov’s inequality (B.36) and the following is the triangle inequality (B.1). The limit is zero because <span class="math inline">\(\sum_{i=0}^{\infty}\left|a_{i}\right|&lt;\infty\)</span> and <span class="math inline">\(\mathbb{E}\left|Y_{t}\right|&lt;\infty\)</span>. Hence for all <span class="math inline">\(\epsilon&gt;0, \mathbb{P}\left[A_{\varepsilon}^{c}\right]=0\)</span> and <span class="math inline">\(\mathbb{P}\left[A_{\epsilon}\right]=1\)</span>. This means that <span class="math inline">\(S_{N}\)</span> converges with probability one, as claimed.</p>
<p>Since <span class="math inline">\(Y_{t}\)</span> is strictly stationary then <span class="math inline">\(X_{t}\)</span> is as well by Theorem <span class="math inline">\(14.2\)</span>.</p>
<p>Proof of Theorem 14.4 See Theorem 14.14.</p>
<p>Proof of Theorem 14.5 Strict stationarity follows from Theorem 14.2. Let <span class="math inline">\(\widetilde{Y}_{t}\)</span> and <span class="math inline">\(\widetilde{X}_{t}\)</span> be the histories of <span class="math inline">\(Y_{t}\)</span> and <span class="math inline">\(X_{t}\)</span>. Write <span class="math inline">\(X_{t}=\phi\left(\widetilde{Y}_{t}\right)\)</span>. Let <span class="math inline">\(A\)</span> be an invariant event for <span class="math inline">\(X_{t}\)</span>. We want to show <span class="math inline">\(\mathbb{P}[A]=0\)</span> or 1 . The event <span class="math inline">\(A\)</span> is a collection of <span class="math inline">\(\widetilde{X}_{t}\)</span> histories, and occurs if and and only if an associated collection of <span class="math inline">\(\widetilde{Y}_{t}\)</span> histories occur. That is, for some sets <span class="math inline">\(G\)</span> and <span class="math inline">\(H\)</span>,</p>
<p><span class="math display">\[
A=\left\{\widetilde{X}_{t} \in G\right\}=\left\{\phi\left(\widetilde{Y}_{t}\right) \in G\right\}=\left\{\widetilde{Y}_{t} \in H\right\} .
\]</span></p>
<p>The assumption that <span class="math inline">\(A\)</span> is invariant means it is unaffected by the time shift, thus can be written as</p>
<p><span class="math display">\[
A=\left\{\widetilde{X}_{t+\ell} \in G\right\}=\left\{\widetilde{Y}_{t+\ell} \in H\right\} .
\]</span></p>
<p>This means the event <span class="math inline">\(\left\{\widetilde{Y}_{t+\ell} \in H\right\}\)</span> is invariant. Since <span class="math inline">\(Y_{t}\)</span> is ergodic the event has probability 0 or 1. Hence <span class="math inline">\(\mathbb{P}[A]=0\)</span> or 1 , as desired.</p>
<p>Proof of Theorem 14.7 Suppose <span class="math inline">\(Y_{t}\)</span> is discrete with support on <span class="math inline">\(\left(\tau_{1}, \ldots, \tau_{N}\right)\)</span> and without loss of generality assume <span class="math inline">\(\mathbb{E}\left[Y_{t}\right]=0\)</span>. Then by Theorem <span class="math inline">\(14.8\)</span></p>
<p><span class="math display">\[
\begin{aligned}
\lim _{n \rightarrow \infty} \frac{1}{n} \sum_{\ell=1}^{n} \operatorname{cov}\left(Y_{t}, Y_{t+\ell}\right) &amp;=\lim _{n \rightarrow \infty} \frac{1}{n} \sum_{\ell=1}^{n} \mathbb{E}\left[Y_{t} Y_{t+\ell}\right] \\
&amp;=\lim _{n \rightarrow \infty} \frac{1}{n} \sum_{\ell=1}^{n} \sum_{j=1}^{N} \sum_{k=1}^{N} \tau_{j} \tau_{k} \mathbb{P}\left[Y_{t}=\tau_{j}, Y_{t+\ell}=\tau_{k}\right] \\
&amp;=\sum_{j=1}^{N} \sum_{k=1}^{N} \tau_{j} \tau_{k} \lim _{n \rightarrow \infty} \frac{1}{n} \sum_{\ell=1}^{n} \mathbb{P}\left[Y_{t}=\tau_{j}, Y_{t+\ell}=\tau_{k}\right] \\
&amp;=\sum_{j=1}^{N} \sum_{k=1}^{N} \tau_{j} \tau_{k} \mathbb{P}\left[y_{t}=\tau_{j}\right] \mathbb{P}\left[Y_{t+\ell}=\tau_{k}\right] \\
&amp;=\mathbb{E}\left[Y_{t}\right] \mathbb{E}\left[Y_{t+\ell}\right] \\
&amp;=0 .
\end{aligned}
\]</span></p>
<p>which is (14.4). This can be extended to the case of continuous distributions using the monotone convergence theorem. See Corollary <span class="math inline">\(14.8\)</span> of Davidson (1994).</p>
<p>Proof of Theorem 14.9 We show (14.6). (14.7) follows by Markov’s inequality (B.36). Without loss of generality we focus on the scalar case and assume <span class="math inline">\(\mathbb{E}\left[Y_{t}\right]=0\)</span>. Fix <span class="math inline">\(\epsilon&gt;0\)</span>. Pick <span class="math inline">\(B\)</span> large enough such that</p>
<p><span class="math display">\[
\mathbb{E}\left|Y_{t} \mathbb{1}\left\{\left|Y_{t}\right|&gt;B\right\}\right| \leq \frac{\epsilon}{4}
\]</span></p>
<p>which is feasible because <span class="math inline">\(\mathbb{E}\left|Y_{t}\right|&lt;\infty\)</span>. Define</p>
<p><span class="math display">\[
\begin{aligned}
W_{t} &amp;=Y_{t} \mathbb{1}\left\{\left|Y_{t}\right| \leq B\right\}-\mathbb{E}\left[Y_{t} \mathbb{1}\left\{\left|Y_{t}\right| \leq B\right\}\right] \\
Z_{t} &amp;=Y_{t} \mathbb{1}\left\{\left|Y_{t}\right|&gt;B\right\}-\mathbb{E}\left[Y_{t} \mathbb{1}\left\{\left|Y_{t}\right|&gt;B\right\}\right] .
\end{aligned}
\]</span></p>
<p>Notice that <span class="math inline">\(W_{t}\)</span> is a bounded transformation of the ergodic series <span class="math inline">\(Y_{t}\)</span>. Thus by (14.4) and (14.9) there is an <span class="math inline">\(n\)</span> sufficiently large so that</p>
<p><span class="math display">\[
\frac{\operatorname{var}\left[W_{t}\right]}{n}+\frac{2}{n} \sum_{m=1}^{n}\left(1-\frac{m}{n}\right) \operatorname{cov}\left(W_{t}, W_{j}\right) \leq \frac{\epsilon^{2}}{4}
\]</span></p>
<p>By the triangle inequality (B.1)</p>
<p><span class="math display">\[
\mathbb{E}|\bar{Y}|=\mathbb{E}|\bar{W}+\bar{Z}| \leq \mathbb{E}|\bar{W}|+\mathbb{E}|\bar{Z}| .
\]</span></p>
<p>By another application of the triangle inequality and (14.63)</p>
<p><span class="math display">\[
\mathbb{E}|\bar{Z}| \leq \mathbb{E}\left|Z_{t}\right| \leq 2 \mathbb{E}\left|Y_{t} \mathbb{1}\left(\left|Y_{t}\right|&gt;B\right)\right| \leq \frac{\epsilon}{2} .
\]</span></p>
<p>By Jensen’s inequality (B.27), direct calculation, and (14.64)</p>
<p><span class="math display">\[
\begin{aligned}
(\mathbb{E}|\bar{W}|)^{2} &amp; \leq \mathbb{E}\left[|\bar{W}|^{2}\right] \\
&amp;=\frac{1}{n^{2}} \sum_{t=1}^{n} \sum_{j=1}^{n} \mathbb{E}\left[W_{t} W_{j}\right] \\
&amp;=\frac{\operatorname{var}\left[W_{t}\right]}{n}+\frac{2}{n} \sum_{m=1}^{n}\left(1-\frac{m}{n}\right) \operatorname{cov}\left(W_{t}, W_{j}\right) \\
&amp; \leq \frac{\epsilon^{2}}{4} .
\end{aligned}
\]</span></p>
<p>Thus</p>
<p><span class="math display">\[
\mathbb{E}|\bar{W}| \leq \frac{\epsilon}{2} .
\]</span></p>
<p>Together, (14.65), (14.66) and (14.67) show that <span class="math inline">\(\mathbb{E}|\bar{Y}| \leq \epsilon\)</span>. Since <span class="math inline">\(\varepsilon\)</span> is arbitrary, this establishes (14.6) as claimed.</p>
<p>Proof of Theorem 14.11 (sketch) By the Cramér-Wold device (Theorem <span class="math inline">\(8.4\)</span> from Probability and Statistics for Economists) it is sufficient to establish the result for scalar <span class="math inline">\(u_{t}\)</span>. Let <span class="math inline">\(\sigma^{2}=\mathbb{E}\left[u_{t}^{2}\right]\)</span>. By a Taylor series expansion, for <span class="math inline">\(x\)</span> small <span class="math inline">\(\log (1+x) \simeq x-x^{2} / 2\)</span>. Taking exponentials and rearranging we obtain the approximation</p>
<p>Fix <span class="math inline">\(\lambda\)</span>. Define</p>
<p><span class="math display">\[
\exp (x) \simeq(1+x) \exp \left(\frac{x^{2}}{2}\right) .
\]</span></p>
<p><span class="math display">\[
\begin{aligned}
T_{j} &amp;=\prod_{i=1}^{j}\left(1+\frac{\lambda}{\sqrt{n}} u_{t}\right) \\
V_{n} &amp;=\frac{1}{n} \sum_{t=1}^{n} u_{t}^{2} .
\end{aligned}
\]</span></p>
<p>Since <span class="math inline">\(u_{t}\)</span> is strictly stationary and ergodic, <span class="math inline">\(V_{n} \stackrel{p}{\rightarrow} \sigma^{2}\)</span> by the Ergodic Theorem (Theorem 14.9). Since <span class="math inline">\(u_{t}\)</span> is a MDS</p>
<p><span class="math display">\[
\mathbb{E}\left[T_{n}\right]=1 .
\]</span></p>
<p>To see this, define <span class="math inline">\(\mathscr{F}_{t}=\sigma\left(\ldots, u_{t-1}, u_{t}\right)\)</span>. Note <span class="math inline">\(T_{j}=T_{j-1}\left(1+\frac{\lambda}{\sqrt{n}} u_{j}\right)\)</span>. By iterated expectations</p>
<p><span class="math display">\[
\begin{aligned}
\mathbb{E}\left[T_{n}\right] &amp;=\mathbb{E}\left[\mathbb{E}\left[T_{n} \mid \mathscr{F}_{n-1}\right]\right] \\
&amp;=\mathbb{E}\left[T_{n-1} \mathbb{E}\left[1+\frac{\lambda}{\sqrt{n}} u_{n} \mid \mathscr{F}_{n-1}\right]\right] \\
&amp;=\mathbb{E}\left[T_{n-1}\right]=\cdots=\mathbb{E}\left[T_{1}\right] \\
&amp;=1 .
\end{aligned}
\]</span></p>
<p>This is (14.69).</p>
<p>The moment generating function of <span class="math inline">\(S_{n}\)</span> is</p>
<p><span class="math display">\[
\begin{aligned}
\mathbb{E}\left[\exp \left(\frac{\lambda}{\sqrt{n}} \sum_{t=1}^{n} u_{t}\right)\right] &amp;=\mathbb{E}\left[\prod_{i=1}^{n} \exp \left(\frac{\lambda}{\sqrt{n}} u_{t}\right)\right] \\
&amp; \simeq \mathbb{E}\left[\prod_{i=1}^{n}\left[1+\frac{\lambda}{\sqrt{n}} u_{t}\right] \exp \left(\frac{\lambda^{2}}{2 n} u_{t}^{2}\right)\right] \\
&amp;=\mathbb{E}\left[T_{n} \exp \left(\frac{\lambda^{2} V_{n}}{2}\right)\right] \\
&amp; \simeq \mathbb{E}\left[T_{n} \exp \left(\frac{\lambda^{2} \sigma^{2}}{2}\right)\right] \\
&amp;=\exp \left(\frac{\lambda^{2} \sigma^{2}}{2}\right) .
\end{aligned}
\]</span></p>
<p>The approximation in (14.70) is (14.68). The approximation (14.71) is <span class="math inline">\(V_{n} \vec{p} \sigma^{2}\)</span>. (A rigorous justification which allows this substitution in the expectation is technical.) The final equality is (14.69). This shows that the moment generating function of <span class="math inline">\(S_{n}\)</span> is approximately that of <span class="math inline">\(\mathrm{N}\left(0, \sigma^{2}\right)\)</span>, as claimed.</p>
<p>The assumption that <span class="math inline">\(u_{t}\)</span> is a MDS is critical for (14.69). <span class="math inline">\(T_{n}\)</span> is a nonlinear function of the errors <span class="math inline">\(u_{t}\)</span> so a white noise assumption cannot be used instead. The MDS assumption is exactly the minimal condition needed to obtain (14.69). This is why the MDS assumption cannot be easily replaced by a milder assumption such as white noise.</p>
<p>Proof of Theorem 14.13.1 Without loss of generality suppose <span class="math inline">\(\mathbb{E}\left[X_{t}\right]=0\)</span> and <span class="math inline">\(\mathbb{E}\left[Z_{t}\right]=0\)</span>. Set <span class="math inline">\(\eta_{t-m}=\)</span> <span class="math inline">\(\operatorname{sgn}\left(\mathbb{E}\left[Z_{t} \mid \mathscr{F}_{-\infty}^{t-m}\right]\right)\)</span>. By iterated expectations, <span class="math inline">\(\left|X_{t}\right| \leq C_{1},\left|\mathbb{E}\left[Z_{t} \mid \mathscr{F}_{-\infty}^{t-m}\right]\right|=\eta_{t-m} \mathbb{E}\left[Z_{t} \mid \mathscr{F}_{-\infty}^{t-m}\right]\)</span>, and again using iterated expectations</p>
<p><span class="math display">\[
\begin{aligned}
\left|\operatorname{cov}\left(X_{t-m}, Z_{t}\right)\right| &amp;=\left|\mathbb{E}\left[\mathbb{E}\left[X_{t-m} Z_{t} \mid \mathscr{F}_{-\infty}^{t-m}\right]\right]\right| \\
&amp;=\left|\mathbb{E}\left(X_{t-m} \mathbb{E}\left[Z_{t} \mid \mathscr{F}_{-\infty}^{t-m}\right]\right)\right| \\
&amp; \leq C_{1} \mathbb{E}\left|\mathbb{E}\left[Z_{t} \mid \mathscr{F}_{-\infty}^{t-m}\right]\right| \\
&amp;=C_{1} \mathbb{E}\left[\eta_{t-m} \mathbb{E}\left[Z_{t} \mid \mathscr{F}_{-\infty}^{t-m}\right]\right] \\
&amp;=C_{1} \mathbb{E}\left[\mathbb{E}\left[\eta_{t-m} Z_{t} \mid \mathscr{F}_{-\infty}^{t-m}\right]\right] \\
&amp;=C_{1} \mathbb{E}\left[\eta_{t-m} Z_{t}\right] \\
&amp;=C_{1} \operatorname{cov}\left(\eta_{t-m}, Z_{t}\right) .
\end{aligned}
\]</span></p>
<p>Setting <span class="math inline">\(\xi_{t}=\operatorname{sgn}\left(\mathbb{E}\left[X_{t-m} \mid \mathscr{F}_{t}^{\infty}\right]\right)\)</span>, by a similar argument (14.72) is bounded by <span class="math inline">\(C_{1} C_{2} \operatorname{cov}\left(\eta_{t-m}, \xi_{t}\right)\)</span>. Set <span class="math inline">\(A_{1}=\mathbb{1}\left\{\eta_{t-m}=1\right\}, A_{2}=\mathbb{1}\left\{\eta_{t-m}=-1\right\}, B_{1}=\mathbb{1}\left\{\xi_{t}=1\right\}, B_{2}=\mathbb{1}\left\{\xi_{t}=-1\right\}\)</span>. We calculate</p>
<p><span class="math display">\[
\begin{aligned}
\left|\operatorname{cov}\left(\eta_{t-m}, \xi_{t}\right)\right| &amp;=\mid \mathbb{P}\left[A_{1} \cap B_{1}\right]+\mathbb{P}\left[A_{2} \cap B_{2}\right]-\mathbb{P}\left[A_{2} \cap B_{1}\right]-\mathbb{P}\left[A_{1} \cap B_{2}\right] \\
&amp;-\mathbb{P}\left[A_{1}\right] \mathbb{P}\left[B_{1}\right]-\mathbb{P}\left[A_{2}\right] \mathbb{P}\left[B_{2}\right]+\mathbb{P}\left[A_{2}\right] \mathbb{P}\left[B_{1}\right]+\mathbb{P}\left[A_{1}\right] \mathbb{P}\left[B_{2}\right] \mid \\
&amp; \leq 4 \alpha(m) .
\end{aligned}
\]</span></p>
<p>Together, <span class="math inline">\(\left|\operatorname{cov}\left(X_{t-m}, z_{t}\right)\right| \leq 4 C_{1} C_{2} \alpha(m)\)</span> as claimed.</p>
<p>Proof of Theorem 14.13.2 Assume <span class="math inline">\(\mathbb{E}\left[X_{t}\right]=0\)</span> and <span class="math inline">\(\mathbb{E}\left[Z_{t}\right]=0\)</span>. We first show that if <span class="math inline">\(\left|X_{t}\right| \leq C\)</span> then</p>
<p><span class="math display">\[
\left|\operatorname{cov}\left(X_{t-\ell}, Z_{t}\right)\right| \leq 6 C\left(\mathbb{E}\left|Z_{t}\right|^{r}\right)^{1 / r} \alpha(\ell)^{1-1 / r} .
\]</span></p>
<p>Indeed, if <span class="math inline">\(\alpha(\ell)=0\)</span> the result is immediate so assume <span class="math inline">\(\alpha(\ell)&gt;0\)</span>. Set <span class="math inline">\(D=\alpha(\ell)^{-1 / r}\left(\mathbb{E}\left|Z_{t}\right|^{r}\right)^{1 / r}, V_{t}=Z_{t} \mathbb{1}\left\{\left|Z_{t}\right| \leq D\right\}\)</span> and <span class="math inline">\(W_{t}=Z_{t} \mathbb{1}\left\{\left|Z_{t}\right|&gt;D\right\}\)</span>. Using the triangle inequality (B.1) and then part 1, because <span class="math inline">\(\left|X_{t}\right| \leq C\)</span> and <span class="math inline">\(\left|V_{t}\right| \leq D\)</span>,</p>
<p><span class="math display">\[
\left|\operatorname{cov}\left(X_{t-\ell}, Z_{t}\right)\right| \leq\left|\operatorname{cov}\left(X_{t-\ell}, V_{t}\right)\right|+\left|\operatorname{cov}\left(X_{t-\ell}, W_{t}\right)\right| \leq 4 C D \alpha(\ell)+2 C \mathbb{E}\left|w_{t}\right| .
\]</span></p>
<p>Also,</p>
<p><span class="math display">\[
\mathbb{E}\left|W_{t}\right|=\mathbb{E}\left|Z_{t} \mathbb{1}\left\{\left|Z_{t}\right|&gt;D\right\}\right|=\mathbb{E}\left|\frac{\left|Z_{t}\right|^{r}}{\left|Z_{t}\right|^{r-1}} \mathbb{1}\left\{\left|Z_{t}\right|&gt;D\right\}\right| \leq \frac{\mathbb{E}\left|Z_{t}\right|^{r}}{D^{r-1}}=\alpha(\ell)^{(r-1) / r}\left(\mathbb{E}\left|Z_{t}\right|^{r}\right)^{1 / r}
\]</span></p>
<p>using the definition of <span class="math inline">\(D\)</span>. Together we have</p>
<p><span class="math display">\[
\left|\operatorname{cov}\left(X_{t-\ell}, Z_{t}\right)\right| \leq 6 C\left(\mathbb{E}\left|X_{t}\right|^{r}\right)^{1 / r} \alpha(\ell)^{1-1 / r} .
\]</span></p>
<p>which is (14.73) as claimed.</p>
<p>Now set <span class="math inline">\(C=\alpha(\ell)^{-1 / r}\left(\mathbb{E}\left|X_{t}\right|^{r}\right)^{1 / r}, V_{t}=X_{t} \mathbb{1}\left\{\left|X_{t}\right| \leq C\right\}\)</span> and <span class="math inline">\(W_{t}=X_{t} \mathbb{1}\left\{\left|X_{t}\right|&gt;C\right\}\)</span>. Using the triangle inequality and (14.73)</p>
<p><span class="math display">\[
\left|\operatorname{cov}\left(X_{t-\ell}, Z_{t}\right)\right| \leq\left|\operatorname{cov}\left(V_{t-\ell}, Z_{t}\right)\right|+\left|\operatorname{cov}\left(W_{t-\ell}, Z_{t}\right)\right| .
\]</span></p>
<p>Since <span class="math inline">\(\left|V_{t}\right| \leq C\)</span>, using (14.73) and the definition of <span class="math inline">\(C\)</span></p>
<p><span class="math display">\[
\left|\operatorname{cov}\left(V_{t-\ell}, Z_{t}\right)\right| \leq 6 C\left(\mathbb{E}\left|Z_{t}\right|^{q}\right)^{1 / q} \alpha(\ell)^{1-1 / q}=6\left(\mathbb{E}\left|X_{t}\right|^{r}\right)^{1 / r}\left(\mathbb{E}\left|Z_{t}\right|^{q}\right)^{1 / q} \alpha(\ell)^{1-1 / q-1 / r} .
\]</span></p>
<p>Using Hölder’s inequality (B.31) and the definition of <span class="math inline">\(C\)</span></p>
<p><span class="math display">\[
\begin{aligned}
\left|\operatorname{cov}\left(W_{t-\ell}, Z_{t}\right)\right| &amp; \leq 2\left(\mathbb{E}\left|W_{t}\right|^{q /(q-1)}\right)^{(q-1) / q}\left(\mathbb{E}\left|Z_{t}\right|^{q}\right)^{1 / q} \\
&amp;=2\left(\mathbb{E}\left[\left|X_{t}\right|^{q /(q-1)} \mathbb{1}\left\{\left|X_{t}\right|&gt;C\right\}\right]\right)^{(q-1) / q}\left(\mathbb{E}\left|Z_{t}\right|^{q}\right)^{1 / q} \\
&amp;=2\left(\mathbb{E}\left[\frac{\left|X_{t}\right|^{r}}{\left|X_{t}\right|^{r-q /(q-1)}} \mathbb{1}\left\{\left|X_{t}\right|&gt;C\right\}\right]\right)^{(q-1) / q}\left(\mathbb{E}\left|Z_{t}\right|^{q}\right)^{1 / q} \\
&amp; \leq \frac{2}{C^{r(q-1) / q-1}}\left(\mathbb{E}\left|X_{t}\right|^{r}\right)^{(q-1) / q}\left(\mathbb{E}\left|Z_{t}\right|^{q}\right)^{1 / q} \\
&amp;=2\left(\mathbb{E}\left|X_{t}\right|^{r}\right)^{1 / r}\left(\mathbb{E}\left|Z_{t}\right|^{q}\right)^{1 / q} \alpha(\ell)^{1-1 / q-1 / r} .
\end{aligned}
\]</span></p>
<p>Together we have</p>
<p><span class="math display">\[
\left|\operatorname{cov}\left(X_{t-\ell}, Z_{t}\right)\right| \leq 8\left(\mathbb{E}\left|X_{t}\right|^{r}\right)^{1 / r}\left(\mathbb{E}\left|Z_{t}\right|^{q}\right)^{1 / q} \alpha(\ell)^{1-1 / r-1 / q}
\]</span></p>
<p>as claimed. Proof of Theorem 14.13.3 Set <span class="math inline">\(\eta_{t-\ell}=\operatorname{sgn}\left(\mathbb{E}\left[Z_{t} \mid \mathscr{F}_{-\infty}^{t-\ell}\right]\right)\)</span> which satisfies <span class="math inline">\(\left|\eta_{t-\ell}\right| \leq 1\)</span>. Since <span class="math inline">\(\eta_{t-\ell}\)</span> is <span class="math inline">\(\mathscr{F}_{-\infty}^{t-\ell}-\)</span> measurable, iterated expectations, using (14.73) with <span class="math inline">\(C=1\)</span>, the conditional Jensen’s inequality (B.28), and iterated expectations,</p>
<p><span class="math display">\[
\begin{aligned}
\mathbb{E}\left|\mathbb{E}\left[Z_{t} \mid \mathscr{F}_{-\infty}^{t-\ell}\right]\right| &amp;=\mathbb{E}\left[\eta_{t-\ell} \mathbb{E}\left[Z_{t} \mid \mathscr{F}_{-\infty}^{t-\ell}\right]\right] \\
&amp;=\mathbb{E}\left[\mathbb{E}\left[\eta_{t-\ell} Z_{t} \mid \mathscr{F}_{-\infty}^{t-\ell}\right]\right] \\
&amp;=\mathbb{E}\left[\eta_{t-\ell} Z_{t}\right] \\
&amp; \leq 6\left(\mathbb{E}\left|\mathbb{E}\left[Z_{t} \mid \mathscr{F}_{-\infty}^{t-\ell}\right]\right|^{r}\right)^{1 / r} \alpha(\ell)^{1-1 / r} \\
&amp; \leq 6\left(\mathbb{E}\left(\mathbb{E}\left[\left|Z_{t}\right|^{r} \mid \mathscr{F}_{-\infty}^{t-\ell}\right]\right)\right)^{1 / r} \alpha(\ell)^{1-1 / r} \\
&amp;=6\left(\mathbb{E}\left|Z_{t}\right|^{r} \mid\right)^{1 / r} \alpha(\ell)^{1-1 / r}
\end{aligned}
\]</span></p>
<p>as claimed.</p>
<p>Proof of Theorem 14.15 By the Cramér-Wold device (Theorem <span class="math inline">\(8.4\)</span> of Probability and Statistics for Economists) it is sufficient to prove the result for the scalar case. Our proof method is based on a MDS approximation. The trick is to establish the relationship</p>
<p><span class="math display">\[
u_{t}=e_{t}+Z_{t}-Z_{t+1}
\]</span></p>
<p>where <span class="math inline">\(e_{t}\)</span> is a strictly stationary and ergodic MDS with <span class="math inline">\(\mathbb{E}\left[e_{t}^{2}\right]=\Omega\)</span> and <span class="math inline">\(\mathbb{E}\left|Z_{t}\right|&lt;\infty\)</span>. Defining <span class="math inline">\(S_{n}^{e}=\frac{1}{\sqrt{n}} \sum_{t=1}^{n} e_{t}\)</span>, we have</p>
<p><span class="math display">\[
S_{n}=\frac{1}{\sqrt{n}} \sum_{t=1}^{n}\left(e_{t}+Z_{t}-Z_{t+1}\right)=S_{n}^{e}+\frac{Z_{1}}{\sqrt{n}}-\frac{Z_{n+1}}{\sqrt{n}} .
\]</span></p>
<p>The first component on the right side is asymptotically <span class="math inline">\(\mathrm{N}(0, \Omega)\)</span> by the MDS CLT (Theorem 14.11). The second and third terms are <span class="math inline">\(o_{p}(1)\)</span> by Markov’s inequality (B.36).</p>
<p>The desired relationship (14.74) holds as follows. Set <span class="math inline">\(\mathscr{F}_{t}=\sigma\left(\ldots, u_{t-1}, u_{t}\right)\)</span>,</p>
<p><span class="math display">\[
e_{t}=\sum_{\ell=0}^{\infty}\left(\mathbb{E}\left[u_{t+\ell} \mid \mathscr{F}_{t}\right]-\mathbb{E}\left[u_{t+\ell} \mid \mathscr{F}_{t-1}\right]\right)
\]</span></p>
<p>and</p>
<p><span class="math display">\[
Z_{t}=\sum_{\ell=0}^{\infty} \mathbb{E}\left[u_{t+\ell} \mid \mathscr{F}_{t-1}\right] .
\]</span></p>
<p>You can verify that these definitions satisfy (14.74) given <span class="math inline">\(\mathbb{E}\left[u_{t} \mid \mathscr{F}_{t}\right]=u_{t}\)</span>. The variable <span class="math inline">\(Z_{t}\)</span> has a finite expectation because by the triangle inequality (B.1), Theorem 14.13.3, and the assumptions</p>
<p><span class="math display">\[
\mathbb{E}\left|Z_{t}\right|=\mathbb{E}\left|\sum_{\ell=0}^{\infty} \mathbb{E}\left[u_{t+\ell} \mid \mathscr{F}_{t-1}\right]\right| \leq 6\left(\mathbb{E}\left|u_{t}\right|^{r}\right)^{1 / r} \sum_{\ell=0}^{\infty} \alpha(\ell)^{1-1 / r}&lt;\infty
\]</span></p>
<p>the final inequality because <span class="math inline">\(\sum_{\ell=0}^{\infty} \alpha(\ell)^{1-2 / r}&lt;\infty\)</span> implies <span class="math inline">\(\sum_{\ell=0}^{\infty} \alpha(\ell)^{1-1 / r}&lt;\infty\)</span>.</p>
<p>The series <span class="math inline">\(e_{t}\)</span> in (14.76) has a finite expectation by the same calculation as for <span class="math inline">\(Z_{t}\)</span>. It is a MDS since by iterated expectations</p>
<p><span class="math display">\[
\begin{aligned}
\mathbb{E}\left[e_{t} \mid \mathscr{F}_{t-1}\right] &amp;=\mathbb{E}\left[\sum_{\ell=0}^{\infty}\left(\mathbb{E}\left[u_{t+\ell} \mid \mathscr{F}_{t}\right]-\mathbb{E}\left[u_{t+\ell} \mid \mathscr{F}_{t-1}\right]\right) \mid \mathscr{F}_{t-1}\right] \\
&amp;=\sum_{\ell=0}^{\infty}\left(\mathbb{E}\left[\mathbb{E}\left[u_{t+\ell} \mid \mathscr{F}_{t}\right] \mid \mathscr{F}_{t-1}\right]-\mathbb{E}\left[\mathbb{E}\left[u_{t+\ell} \mid \mathscr{F}_{t-1}\right] \mid \mathscr{F}_{t-1}\right]\right) \\
&amp;=\sum_{\ell=0}^{\infty}\left(\mathbb{E}\left[u_{t+\ell} \mid \mathscr{F}_{t-1}\right]-\mathbb{E}\left[u_{t+\ell} \mid \mathscr{F}_{t-1}\right]\right) \\
&amp;=0 .
\end{aligned}
\]</span></p>
<p>It is strictly stationary and ergodic by Theorem <span class="math inline">\(14.2\)</span> because it is a function of the history <span class="math inline">\(\left(\ldots, u_{t-1}, u_{t}\right)\)</span>.</p>
<p>The proof is completed by showing that <span class="math inline">\(e_{t}\)</span> has a finite variance which equals <span class="math inline">\(\Omega\)</span>. The trickiest step is to show that <span class="math inline">\(\operatorname{var}\left[e_{t}\right]&lt;\infty\)</span>. Since</p>
<p><span class="math display">\[
\mathbb{E}\left|S_{n}\right| \leq \sqrt{\operatorname{var}\left[S_{n}\right]} \rightarrow \sqrt{\Omega}
\]</span></p>
<p>(as shown in (14.17)) it follows that <span class="math inline">\(\mathbb{E}\left|S_{n}\right| \leq 2 \sqrt{\Omega}\)</span> for <span class="math inline">\(n\)</span> sufficiently large. Using (14.75) and <span class="math inline">\(\mathbb{E}\left|Z_{t}\right|&lt;\infty\)</span>, for <span class="math inline">\(n\)</span> sufficiently large,</p>
<p><span class="math display">\[
\mathbb{E}\left|S_{n}^{e}\right| \leq \mathbb{E}\left|S_{n}\right|+\frac{\mathbb{E}\left|Z_{1}\right|}{\sqrt{n}}+\frac{\mathbb{E}\left|Z_{n+1}\right|}{\sqrt{n}} \leq 3 \sqrt{\Omega} .
\]</span></p>
<p>Now define <span class="math inline">\(e_{B t}=e_{t} \mathbb{1}\left\{\left|e_{t}\right| \leq B\right\}-\mathbb{E}\left[e_{t} \mathbb{1}\left\{\left|e_{t}\right| \leq B\right\} \mid \mathscr{F}_{t-1}\right]\)</span> which is a bounded MDS. By Theorem 14.11, <span class="math inline">\(\frac{1}{\sqrt{n}} \sum_{t=1}^{n} e_{B t} \stackrel{d}{\longrightarrow} \mathrm{N}\left(0, \sigma_{B}^{2}\right)\)</span> where <span class="math inline">\(\sigma_{B}^{2}=\mathbb{E}\left[e_{B t}^{2}\right]\)</span>. Since the sequence is uniformly integrable this implies</p>
<p><span class="math display">\[
\mathbb{E}\left|\frac{1}{\sqrt{n}} \sum_{t=1}^{n} e_{B t}\right| \longrightarrow \mathbb{E}\left|\mathrm{N}\left(0, \sigma_{B}^{2}\right)\right|=\sqrt{\frac{2}{\pi}} \sigma_{B}
\]</span></p>
<p>using <span class="math inline">\(\mathbb{E}|\mathrm{N}(0,1)|=2 / \pi\)</span>. We want to show that <span class="math inline">\(\operatorname{var}\left[e_{t}\right]&lt;\infty\)</span>. Suppose not. Then <span class="math inline">\(\sigma_{B} \rightarrow \infty\)</span> as <span class="math inline">\(B \rightarrow \infty\)</span>, so there will be some <span class="math inline">\(B\)</span> sufficiently large such that the right-side of (14.78) exceeds the right-side of (14.77). This is a contradiction. We deduce that <span class="math inline">\(\operatorname{var}\left[e_{t}\right]&lt;\infty\)</span>.</p>
<p>Examining (14.75), we see that since var <span class="math inline">\(\left[S_{n}\right] \rightarrow \Omega&lt;\infty\)</span> and <span class="math inline">\(\operatorname{var}\left[S_{n}^{e}\right]=\operatorname{var}\left[e_{t}\right]&lt;\infty\)</span> then <span class="math inline">\(\operatorname{var}\left[Z_{1}-Z_{n+1}\right] / n&lt;\)</span> <span class="math inline">\(\infty\)</span>. Since <span class="math inline">\(Z_{t}\)</span> is stationary, we deduce that <span class="math inline">\(\operatorname{var}\left[Z_{1}-Z_{n+1}\right]&lt;\infty\)</span>. Equation (14.75) implies var <span class="math inline">\(\left[e_{t}\right]=\operatorname{var}\left[S_{n}^{e}\right]=\)</span> <span class="math inline">\(\operatorname{var}\left[S_{n}\right]+o(1) \rightarrow \Omega\)</span>. We deduce that <span class="math inline">\(\operatorname{var}\left[e_{t}\right]=\Omega\)</span> as claimed.</p>
<p>Proof of Theorem 14.17 (Sketch) Consider the projection of <span class="math inline">\(Y_{t}\)</span> onto <span class="math inline">\(\left(\ldots, e_{t-1}, e_{t}\right)\)</span>. Since the projection errors <span class="math inline">\(e_{t}\)</span> are uncorrelated, the coefficients of this projection are the bivariate projection coefficients <span class="math inline">\(b_{j}=\)</span> <span class="math inline">\(\mathbb{E}\left[Y_{t} e_{t-j}\right] / \mathbb{E}\left[e_{t-j}^{2}\right]\)</span>. The leading coefficient is</p>
<p><span class="math display">\[
b_{0}=\frac{\mathbb{E}\left[Y_{t} e_{t}\right]}{\sigma^{2}}=\frac{\sum_{j=1}^{\infty} \alpha_{j} \mathbb{E}\left[Y_{t-j} e_{t}\right]+\mathbb{E}\left[e_{t}^{2}\right]}{\sigma^{2}}=1
\]</span></p>
<p>using Theorem 14.16. By Bessel’s Inequality (Brockwell and Davis, 1991, Corollary 2.4.1),</p>
<p><span class="math display">\[
\sum_{j=1}^{\infty} b_{j}^{2}=\sigma^{-4} \sum_{j=1}^{\infty}\left(\mathbb{E}\left[Y_{t} e_{t}\right]\right)^{2} \leq \sigma^{-4}\left(\mathbb{E}\left[Y_{t}^{2}\right]\right)^{2}&lt;\infty
\]</span></p>
<p>because <span class="math inline">\(\mathbb{E}\left[Y_{t}^{2}\right]&lt;\infty\)</span> by the assumption of covariance stationarity.</p>
<p>The error from the projection of <span class="math inline">\(Y_{t}\)</span> onto <span class="math inline">\(\left(\ldots, e_{t-1}, e_{t}\right)\)</span> is <span class="math inline">\(\mu_{t}=Y_{t}-\sum_{j=0}^{\infty} b_{j} e_{t-j}\)</span>. The fact that this can be written as (14.22) is technical. See Theorem 5.7.1 of Brockwell and Davis (1991). Proof of Theorem 14.22 In the text we showed that <span class="math inline">\(\left|\lambda_{j}\right|&lt;1\)</span> is sufficient for <span class="math inline">\(Y_{t}\)</span> to be strictly stationary and ergodic. We now verify that <span class="math inline">\(\left|\lambda_{j}\right|&lt;1\)</span> is equivalent to (14.35)-(14.37). The roots <span class="math inline">\(\lambda_{j}\)</span> are defined in (14.34). Consider separately the cases of real roots and complex roots.</p>
<p>Suppose that the roots are real, which occurs when <span class="math inline">\(\alpha_{1}^{2}+4 \alpha_{2} \geq 0\)</span>. Then <span class="math inline">\(\left|\lambda_{j}\right|&lt;1\)</span> iff <span class="math inline">\(\left|\alpha_{1}\right|&lt;2\)</span> and</p>
<p><span class="math display">\[
\frac{\alpha_{1}+\sqrt{\alpha_{1}^{2}+4 \alpha_{2}}}{2}&lt;1 \quad \text { and } \quad-1&lt;\frac{\alpha_{1}-\sqrt{\alpha_{1}^{2}+4 \alpha_{2}}}{2} .
\]</span></p>
<p>Equivalently, this holds iff</p>
<p><span class="math display">\[
\alpha_{1}^{2}+4 \alpha_{2}&lt;\left(2-\alpha_{1}\right)^{2}=4-4 \alpha_{1}+\alpha_{1}^{2} \quad \text { and } \quad \alpha_{1}^{2}+4 \alpha_{2}&lt;\left(2+\alpha_{1}\right)^{2}=4+4 \alpha_{1}+\alpha_{1}^{2}
\]</span></p>
<p>or equivalently iff</p>
<p><span class="math display">\[
\alpha_{2}&lt;1-\alpha_{1} \quad \text { and } \quad \alpha_{2}&lt;1+\alpha_{1}
\]</span></p>
<p>which are (14.35) and (14.36). <span class="math inline">\(\alpha_{1}^{2}+4 \alpha_{2} \geq 0\)</span> and <span class="math inline">\(\left|\alpha_{1}\right|&lt;2\)</span> imply <span class="math inline">\(\alpha_{2} \geq-\alpha_{1}^{2} / 4 \geq-1\)</span>, which is (14.37).</p>
<p>Now suppose the roots are complex, which occurs when <span class="math inline">\(\alpha_{1}^{2}+4 \alpha_{2}&lt;0\)</span>. The squared modulus of the roots <span class="math inline">\(\lambda_{j}=\left(\alpha_{1} \pm \sqrt{\alpha_{1}^{2}+4 \alpha_{2}}\right) / 2\)</span> are</p>
<p><span class="math display">\[
\left|\lambda_{j}\right|^{2}=\left(\frac{\alpha_{1}}{2}\right)^{2}-\left(\frac{\sqrt{\alpha_{1}^{2}+4 \alpha_{2}}}{2}\right)^{2}=-\alpha_{2} .
\]</span></p>
<p>Thus the requirement <span class="math inline">\(\left|\lambda_{j}\right|&lt;1\)</span> is satisfied iff <span class="math inline">\(\alpha_{2}&gt;-1\)</span>, which is (14.37). <span class="math inline">\(\alpha_{1}^{2}+4 \alpha_{2}&lt;0\)</span> and <span class="math inline">\(\alpha_{2}&gt;-1\)</span> imply <span class="math inline">\(\alpha_{1}^{2}&lt;\)</span> <span class="math inline">\(-4 \alpha_{2}&lt;4\)</span>, so <span class="math inline">\(\left|\alpha_{1}\right|&lt;2\)</span>. <span class="math inline">\(\alpha_{1}^{2}+4 \alpha_{2}&lt;0\)</span> and <span class="math inline">\(\left|\alpha_{1}\right|&lt;2\)</span> imply <span class="math inline">\(\alpha_{1}+\alpha_{2}&lt;\alpha_{1}-\alpha_{1}^{2} / 4&lt;1\)</span> and <span class="math inline">\(\alpha_{2}-\alpha_{1}&lt;-\alpha_{1}^{2} / 4-\alpha_{1}&lt;1\)</span> which are (14.35) and (14.36).</p>
<p>Proof of Theorem 14.23 To complete the proof we need to establish that the eigenvalues <span class="math inline">\(\lambda_{j}\)</span> of <span class="math inline">\(\boldsymbol{A}\)</span> defined in (14.40) equal the reciprocals of the roots <span class="math inline">\(r_{j}\)</span> of the autoregressive polynomial <span class="math inline">\(\alpha(z)\)</span> of (14.39). Our goal is therefore to show that if <span class="math inline">\(\lambda\)</span> satisfies <span class="math inline">\(\operatorname{det}\left(\boldsymbol{A}-\boldsymbol{I}_{p} \lambda\right)=0\)</span> then it satisfies <span class="math inline">\(\alpha(1 / \lambda)=0\)</span>.</p>
<p>Notice that</p>
<p><span class="math display">\[
\boldsymbol{A}-\boldsymbol{I}_{p} \lambda=\left(\begin{array}{cc}
-\lambda+\alpha_{1} &amp; \widetilde{\alpha}^{\prime} \\
a &amp; \boldsymbol{B}
\end{array}\right)
\]</span></p>
<p>where <span class="math inline">\(\widetilde{\alpha}^{\prime}=\left(\alpha_{2}, \ldots, \alpha_{p}\right), a^{\prime}=(1,0, \ldots, 0)\)</span>, and <span class="math inline">\(\boldsymbol{B}\)</span> is a lower-diagonal matrix with <span class="math inline">\(-\lambda\)</span> on the diagonal and 1 immediately below the diagonal. Notice that <span class="math inline">\(\operatorname{det}(\boldsymbol{B})=(-\lambda)^{p-1}\)</span> and by direct calculation</p>
<p><span class="math display">\[
\boldsymbol{B}^{-1}=-\left(\begin{array}{ccccc}
\lambda^{-1} &amp; 0 &amp; \cdots &amp; 0 &amp; 0 \\
\lambda^{-2} &amp; \lambda^{-1} &amp; \cdots &amp; 0 &amp; 0 \\
\lambda^{-3} &amp; \lambda^{-2} &amp; \cdots &amp; 0 &amp; 0 \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots &amp; \vdots \\
\lambda^{-p+1} &amp; \lambda^{-p+2} &amp; \cdots &amp; \lambda^{-2} &amp; \lambda^{-1}
\end{array}\right) .
\]</span></p>
<p>Using the properties of the determinant (Theorem A.1.5)</p>
<p><span class="math display">\[
\begin{aligned}
\operatorname{det}\left(\boldsymbol{A}-\boldsymbol{I}_{p} \lambda\right) &amp;=\operatorname{det}\left(\begin{array}{cc}
-\lambda+\alpha_{1} &amp; \widetilde{\alpha}^{\prime} \\
a &amp; \boldsymbol{B}
\end{array}\right) \\
&amp;=\operatorname{det}(\boldsymbol{B})\left(-\lambda+\alpha_{1}-\widetilde{\alpha}^{\prime} \boldsymbol{B}^{-1} a\right) \\
&amp;=(-\lambda)^{p}\left(1-\alpha_{1} \lambda^{-1}-\alpha_{2} \lambda^{-2}-\alpha_{3} \lambda^{-3}-\cdots-\alpha_{p} \lambda^{-p}\right) \\
&amp;=(-\lambda)^{p} \alpha(1 / \lambda) .
\end{aligned}
\]</span></p>
<p>Thus if <span class="math inline">\(\lambda\)</span> satisfies <span class="math inline">\(\operatorname{det}\left(\boldsymbol{A}-\boldsymbol{I}_{p} \lambda\right)=0\)</span> then <span class="math inline">\(\alpha(1 / \lambda)=0\)</span> as required.</p>
<p>Proof of Theorem 14.24 By the Fundamental Theorem of Algebra we can factor the autoregressive polynomial as <span class="math inline">\(\alpha(z)=\prod_{\ell=1}^{p}\left(1-\lambda_{\ell} z\right)\)</span> where <span class="math inline">\(\lambda_{\ell}=r_{\ell}^{-1}\)</span>. By assumption <span class="math inline">\(\left|\lambda_{\ell}\right|&lt;1\)</span>. Inverting the autoregressive polynomial we obtain</p>
<p><span class="math display">\[
\begin{aligned}
\alpha(z)^{-1} &amp;=\prod_{\ell=1}^{p}\left(1-\lambda_{\ell} z\right)^{-1} \\
&amp;=\prod_{\ell=1}^{p}\left(\sum_{j=0}^{\infty} \lambda_{\ell}^{j} z^{j}\right) \\
&amp;=\sum_{j=0}^{\infty}\left(\sum_{i_{1}+\cdots+i_{p}=j} \lambda_{1}^{i_{1}} \cdots \lambda_{p}^{i_{p}}\right) z^{j} \\
&amp;=\sum_{j=0}^{\infty} b_{j} z^{j}
\end{aligned}
\]</span></p>
<p>with <span class="math inline">\(b_{j}=\sum_{i_{1}+\cdots+i_{p}=j} \lambda_{1}^{i_{1}} \cdots \lambda_{p}^{i_{p}}\)</span></p>
<p>Using the triangle inequality and the stars and bars theorem (Theorem <span class="math inline">\(1.10\)</span> of Probability and Statistics for Economists)</p>
<p><span class="math display">\[
\begin{aligned}
\left|b_{j}\right| &amp; \leq \sum_{i_{1}+\cdots+i_{p}=j}\left|\lambda_{1}\right|^{i_{1}} \cdots\left|\lambda_{p}\right|^{i_{p}} \\
&amp; \leq \sum_{i_{1}+\cdots+i_{p}=j} \lambda^{j} \\
&amp; \leq\left(\begin{array}{c}
p+j-1 \\
j
\end{array}\right) \lambda^{j} \\
&amp;=\frac{(p+j-1) !}{(p-1) ! j !} \lambda^{j} \\
&amp; \leq(j+1)^{p} \lambda^{j}
\end{aligned}
\]</span></p>
<p>as claimed. We next verify the convergence of <span class="math inline">\(\sum_{j=0}^{\infty}\left|b_{j}\right| \leq \sum_{j=0}^{\infty}(j+1)^{p} \lambda^{j}\)</span>. Note that</p>
<p><span class="math display">\[
\lim _{j \rightarrow \infty} \frac{(j+1)^{p} \lambda^{j}}{(j)^{p} \lambda^{j-1}}=\lambda&lt;1
\]</span></p>
<p>By the ratio test (Theorem A.3.2 of Probability and Statistics for Economists) <span class="math inline">\(\sum_{j=0}^{\infty}(j+1)^{p} \lambda^{j}\)</span> is convergent.</p>
<p>Proof of Theorem 14.27 If <span class="math inline">\(\boldsymbol{Q}\)</span> is singular then there is some <span class="math inline">\(\gamma\)</span> such that <span class="math inline">\(\gamma^{\prime} \boldsymbol{Q} \gamma=0\)</span>. We can normalize <span class="math inline">\(\gamma\)</span> to have a unit coefficient on <span class="math inline">\(Y_{t-1}\)</span> (or the first non-zero coefficient other than the intercept). We then have that <span class="math inline">\(\mathbb{E}\left[\left(Y_{t-1}-\left(1, Y_{t-2}, \ldots, Y_{t-p)}\right)^{\prime} \phi\right)^{2}\right]=0\)</span> for some <span class="math inline">\(\phi\)</span>, or equivalently <span class="math inline">\(\mathbb{E}\left[\left(Y_{t}-\left(1, Y_{t-1}, \ldots, Y_{t-p+1)}\right)^{\prime} \phi\right)^{2}\right]=\)</span> 0. Setting <span class="math inline">\(\beta=\left(\phi^{\prime}, 0\right)^{\prime}\)</span> this implies <span class="math inline">\(\mathbb{E}\left[\left(Y_{t}-\beta^{\prime} X_{t}\right)^{2}\right]=0\)</span>. Since <span class="math inline">\(\alpha\)</span> is the best linear predictor we must have <span class="math inline">\(\beta=\alpha\)</span>. This implies <span class="math inline">\(\sigma^{2}=\mathbb{E}\left[\left(Y_{t}-\alpha^{\prime} X_{t}\right)^{2}\right]=0\)</span>. This contradicts the assumption <span class="math inline">\(\sigma^{2}&gt;0\)</span>. We conclude that <span class="math inline">\(\boldsymbol{Q}\)</span> is not singular.</p>
</section>
<section id="exercises" class="level2" data-number="14.53">
<h2 data-number="14.53" class="anchored" data-anchor-id="exercises"><span class="header-section-number">14.53</span> Exercises</h2>
<p>Exercise 14.1 For a scalar time series <span class="math inline">\(Y_{t}\)</span> define the sample autocovariance and autocorrelation</p>
<p><span class="math display">\[
\begin{aligned}
&amp;\widehat{\gamma}(k)=n^{-1} \sum_{t=k+1}^{n}\left(Y_{t}-\bar{Y}\right)\left(Y_{t-k}-\bar{Y}\right) \\
&amp;\widehat{\rho}(k)=\frac{\widehat{\gamma}(k)}{\widehat{\gamma}(0)}=\frac{\sum_{t=k+1}^{n}\left(Y_{t}-\bar{Y}\right)\left(Y_{t-k}-\bar{Y}\right)}{\sum_{t=1}^{n}\left(Y_{t}-\bar{Y}\right)^{2}} .
\end{aligned}
\]</span></p>
<p>Assume the series is strictly stationary, ergodic, strictly stationary, and <span class="math inline">\(\mathbb{E}\left[Y_{t}^{2}\right]&lt;\infty\)</span>.</p>
<p>Show that <span class="math inline">\(\widehat{\gamma}(k) \underset{p}{\longrightarrow} \gamma(k)\)</span> and <span class="math inline">\(\widehat{\rho}(k) \underset{p}{\longrightarrow} \gamma(k)\)</span> as <span class="math inline">\(n \rightarrow \infty\)</span>. (Use the Ergodic Theorem.)</p>
<p>Exercise 14.2 Show that if <span class="math inline">\(\left(e_{t}, \mathscr{F}_{t}\right)\)</span> is a MDS and <span class="math inline">\(X_{t}\)</span> is <span class="math inline">\(\mathscr{F}_{t}\)</span>-measurable then <span class="math inline">\(u_{t}=X_{t-1} e_{t}\)</span> is a MDS.</p>
<p>Exercise 14.3 Let <span class="math inline">\(\sigma_{t}^{2}=\mathbb{E}\left[e_{t}^{2} \mid \mathscr{F}_{t-1}\right]\)</span>. Show that <span class="math inline">\(u_{t}=e_{t}^{2}-\sigma_{t}^{2}\)</span> is a MDS.</p>
<p>Exercise 14.4 Continuing the previous exercise, show that if <span class="math inline">\(\mathbb{E}\left[e_{t}^{4}\right]&lt;\infty\)</span> then</p>
<p><span class="math display">\[
n^{-1 / 2} \sum_{t=1}^{n}\left(e_{t}^{2}-\sigma_{t}^{2}\right) \underset{d}{\longrightarrow} \mathrm{N}\left(0, v^{2}\right) \text {. }
\]</span></p>
<p>Express <span class="math inline">\(v^{2}\)</span> in terms of the moments of <span class="math inline">\(e_{t}\)</span>.</p>
<p>Exercise 14.5 A stochastic volatility model is</p>
<p><span class="math display">\[
\begin{aligned}
Y_{t} &amp;=\sigma_{t} e_{t} \\
\log \sigma_{t}^{2} &amp;=\omega+\beta \log \sigma_{t-1}^{2}+u_{t}
\end{aligned}
\]</span></p>
<p>where <span class="math inline">\(e_{t}\)</span> and <span class="math inline">\(u_{t}\)</span> are independent i.i.d. <span class="math inline">\(\mathrm{N}(0,1)\)</span> shocks.</p>
<ol type="a">
<li><p>Write down an information set for which <span class="math inline">\(Y_{t}\)</span> is a MDS.</p></li>
<li><p>Show that if <span class="math inline">\(|\beta|&lt;1\)</span> then <span class="math inline">\(Y_{t}\)</span> is strictly stationary and ergodic.</p></li>
</ol>
<p>Exercise 14.6 Verify the formula <span class="math inline">\(\rho(1)=\theta /\left(1+\theta^{2}\right)\)</span> for a MA(1) process.</p>
<p>Exercise 14.7 Verify the formula <span class="math inline">\(\rho(k)=\left(\sum_{j=0}^{\infty} \theta_{j+k} \theta_{j}\right) /\left(\sum_{j=0}^{q} \theta_{j}^{2}\right)\)</span> for a <span class="math inline">\(\mathrm{MA}(\infty)\)</span> process.</p>
<p>Exercise 14.8 Suppose <span class="math inline">\(Y_{t}=Y_{t-1}+e_{t}\)</span> with <span class="math inline">\(e_{t}\)</span> i.i.d. <span class="math inline">\((0,1)\)</span> and <span class="math inline">\(Y_{0}=0\)</span>. Find var <span class="math inline">\(\left[Y_{t}\right]\)</span>. Is <span class="math inline">\(Y_{t}\)</span> stationary?</p>
<p>Exercise 14.9 Take the AR(1) model with no intercept <span class="math inline">\(Y_{t}=\alpha_{1} Y_{t-1}+e_{t}\)</span>.</p>
<ol type="a">
<li><p>Find the impulse response function <span class="math inline">\(b_{j}=\frac{\partial}{\partial e_{t}} Y_{t+j}\)</span>.</p></li>
<li><p>Let <span class="math inline">\(\widehat{\alpha}_{1}\)</span> be the least squares estimator of <span class="math inline">\(\alpha_{1}\)</span>. Find an estimator of <span class="math inline">\(b_{j}\)</span>.</p></li>
<li><p>Let <span class="math inline">\(s\left(\widehat{\alpha}_{1}\right)\)</span> be a standard error for <span class="math inline">\(\widehat{\alpha}_{1}\)</span>. Use the delta method to find a 95% asymptotic confidence interval for <span class="math inline">\(b_{j}\)</span></p></li>
</ol>
<p>Exercise 14.10 Take the AR(2) model <span class="math inline">\(Y_{t}=\alpha_{1} Y_{t-1}+\alpha_{2} Y_{t-1}+e_{t}\)</span>. (a) Find expressions for the impulse responses <span class="math inline">\(b_{1}, b_{2}, b_{3}\)</span> and <span class="math inline">\(b_{4}\)</span>.</p>
<ol start="2" type="a">
<li><p>Let <span class="math inline">\(\left(\widehat{\alpha}_{1}, \widehat{\alpha}_{2}\right)\)</span> be the least squares estimator. Find an estimator of <span class="math inline">\(b_{2}\)</span>.</p></li>
<li><p>Let <span class="math inline">\(\widehat{\boldsymbol{V}}\)</span> be the estimated covariance matrix for the coefficients. Use the delta method to find a <span class="math inline">\(95 %\)</span> asymptotic confidence interval for <span class="math inline">\(b_{2}\)</span>.</p></li>
</ol>
<p>Exercise 14.11 Show that the models</p>
<p><span class="math display">\[
\alpha(\mathrm{L}) Y_{t}=\alpha_{0}+e_{t}
\]</span></p>
<p>and</p>
<p><span class="math display">\[
\begin{aligned}
&amp;\alpha(\mathrm{L}) Y_{t}=\mu+u_{t} \\
&amp;\alpha(\mathrm{L}) u_{t}=e_{t}
\end{aligned}
\]</span></p>
<p>are identical. Find an expression for <span class="math inline">\(\mu\)</span> in terms of <span class="math inline">\(\alpha_{0}\)</span> and <span class="math inline">\(\alpha(\mathrm{L})\)</span>.</p>
<p>Exercise 14.12 Take the model</p>
<p><span class="math display">\[
\begin{aligned}
\alpha(\mathrm{L}) Y_{t} &amp;=u_{t} \\
\beta(\mathrm{L}) u_{t} &amp;=e_{t}
\end{aligned}
\]</span></p>
<p>where <span class="math inline">\(\alpha(\mathrm{L})\)</span> and <span class="math inline">\(\beta(\mathrm{L})\)</span> are <span class="math inline">\(p\)</span> and <span class="math inline">\(q\)</span> order lag polynomials. Show that these equations imply that</p>
<p><span class="math display">\[
\gamma(\mathrm{L}) Y_{t}=e_{t}
\]</span></p>
<p>for some lag polynomial <span class="math inline">\(\gamma(\mathrm{L})\)</span>. What is the order of <span class="math inline">\(\gamma(\mathrm{L})\)</span> ?</p>
<p>Exercise 14.13 Suppose that <span class="math inline">\(Y_{t}=e_{t}+u_{t}+\theta u_{t-1}\)</span> where <span class="math inline">\(u_{t}\)</span> and <span class="math inline">\(e_{t}\)</span> are mutually independent i.i.d. <span class="math inline">\((0,1)\)</span> processes.</p>
<ol type="a">
<li>Show that <span class="math inline">\(Y_{t}\)</span> is a MA(1) process <span class="math inline">\(Y_{t}=\eta_{t}+\psi \eta_{t-1}\)</span> for a white noise error <span class="math inline">\(\eta_{t}\)</span>.</li>
</ol>
<p>Hint: Calculate the autocorrelation function of <span class="math inline">\(Y_{t}\)</span>.</p>
<ol start="2" type="a">
<li><p>Find an expression for <span class="math inline">\(\psi\)</span> in terms of <span class="math inline">\(\theta\)</span>.</p></li>
<li><p><span class="math inline">\(\operatorname{Suppose} \theta=1\)</span>. Find <span class="math inline">\(\psi\)</span>.</p></li>
</ol>
<p>Exercise 14.14 Suppose that</p>
<p><span class="math display">\[
\begin{aligned}
Y_{t} &amp;=X_{t}+e_{t} \\
X_{t} &amp;=\alpha X_{t-1}+u_{t}
\end{aligned}
\]</span></p>
<p>where the errors <span class="math inline">\(e_{t}\)</span> and <span class="math inline">\(u_{t}\)</span> are mutually independent i.i.d. processes. Show that <span class="math inline">\(Y_{t}\)</span> is an ARMA(1,1) process.</p>
<p>Exercise 14.15 A Gaussian AR model is an autoregression with i.i.d. <span class="math inline">\(\mathrm{N}\left(0, \sigma^{2}\right)\)</span> errors. Consider the Gaussian AR(1) model</p>
<p><span class="math display">\[
\begin{aligned}
Y_{t} &amp;=\alpha_{0}+\alpha_{1} Y_{t-1}+e_{t} \\
e_{t} \sim \mathrm{N}\left(0, \sigma^{2}\right)
\end{aligned}
\]</span></p>
<p>with <span class="math inline">\(\left|\alpha_{1}\right|&lt;1\)</span>. Show that the marginal distribution of <span class="math inline">\(Y_{t}\)</span> is also normal:</p>
<p><span class="math display">\[
Y_{t} \sim \mathrm{N}\left(\frac{\alpha_{0}}{1-\alpha_{1}}, \frac{\sigma^{2}}{1-\alpha_{1}^{2}}\right) .
\]</span></p>
<p>Hint: Use the MA representation of <span class="math inline">\(Y_{t}\)</span>. Exercise 14.16 Assume that <span class="math inline">\(Y_{t}\)</span> is a Gaussian <span class="math inline">\(\operatorname{AR}(1)\)</span> as in the previous exercise. Calculate the moments</p>
<p><span class="math display">\[
\begin{aligned}
\mu &amp;=\mathbb{E}\left[Y_{t}\right] \\
\sigma_{Y}^{2} &amp;=\mathbb{E}\left[\left(Y_{t}-\mu\right)^{2}\right] \\
\kappa &amp;=\mathbb{E}\left[\left(Y_{t}-\mu\right)^{4}\right]
\end{aligned}
\]</span></p>
<p>A colleague suggests estimating the parameters <span class="math inline">\(\left(\alpha_{0}, \alpha_{1}, \sigma^{2}\right)\)</span> of the Gaussian AR(1) model by GMM applied to the corresponding sample moments. He points out that there are three moments and three parameters, so it should be identified. Can you find a flaw in his approach?</p>
<p>Hint: This is subtle.</p>
<p>Exercise 14.17 Take the nonlinear process</p>
<p><span class="math display">\[
Y_{t}=Y_{t-1}^{\alpha} u_{t}^{1-\alpha}
\]</span></p>
<p>where <span class="math inline">\(u_{t}\)</span> is i.i.d. with strictly positive support.</p>
<ol type="a">
<li><p>Find the condition under which <span class="math inline">\(Y_{t}\)</span> is strictly stationary and ergodic.</p></li>
<li><p>Find an explicit expression for <span class="math inline">\(Y_{t}\)</span> as a function of <span class="math inline">\(\left(u_{t}, u_{t-1}, \ldots\right)\)</span>.</p></li>
</ol>
<p>Exercise 14.18 Take the quarterly series pnfix (nonresidential real private fixed investment) from FRED-QD.</p>
<ol type="a">
<li><p>Transform the series into quarterly growth rates.</p></li>
<li><p>Estimate an AR(4) model. Report using heteroskedastic-consistent standard errors.</p></li>
<li><p>Repeat using the Newey-West standard errors, using <span class="math inline">\(M=5\)</span>.</p></li>
<li><p>Comment on the magnitude and interpretation of the coefficients.</p></li>
<li><p>Calculate (numerically) the impulse responses for <span class="math inline">\(j=1, \ldots, 10\)</span>.</p></li>
</ol>
<p>Exercise 14.19 Take the quarterly series oilpricex (real price of crude oil) from FRED-QD.</p>
<ol type="a">
<li><p>Transform the series by taking first differences.</p></li>
<li><p>Estimate an AR(4) model. Report using heteroskedastic-consistent standard errors.</p></li>
<li><p>Test the hypothesis that the real oil prices is a random walk by testing that the four AR coefficients jointly equal zero.</p></li>
<li><p>Interpret the coefficient estimates and test result.</p></li>
</ol>
<p>Exercise 14.20 Take the monthly series unrate (unemployment rate) from FRED-MD.</p>
<ol type="a">
<li><p>Estimate AR(1) through AR(8) models, using the sample starting in <span class="math inline">\(1960 \mathrm{~m} 1\)</span> so that all models use the same observations.</p></li>
<li><p>Compute the AIC for each AR model and report.</p></li>
<li><p>Which AR model has the lowest AIC? (d) Report the coefficient estimates and standard errors for the selected model.</p></li>
</ol>
<p>Exercise 14.21 Take the quarterly series unrate (unemployment rate) and claimsx (initial claims) from FRED-QD. “Initial claims” are the number of individuals who file for unemployment insurance.</p>
<ol type="a">
<li><p>Estimate a distributed lag regression of the unemployment rate on initial claims. Use lags 1 through 4. Which standard error method is appropriate?</p></li>
<li><p>Estimate an autoregressive distributed lag regression of the unemployment rate on initial claims. Use lags 1 through 4 for both variables.</p></li>
<li><p>Test the hypothesis that initial claims does not Granger cause the unemployment rate.</p></li>
<li><p>Interpret your results.</p></li>
</ol>
<p>Exercise 14.22 Take the quarterly series gdpcl (real GDP) and houst (housing starts) from FRED-QD. “Housing starts” are the number of new houses on which construction is started.</p>
<ol type="a">
<li><p>Transform the real GDP series into its one quarter growth rate.</p></li>
<li><p>Estimate a distributed lag regression of GDP growth on housing starts. Use lags 1 through 4. Which standard error method is appropriate?</p></li>
<li><p>Estimate an autoregressive distributed lag regression of GDP growth on housing starts. Use lags 1 through 2 for GDP growth and 1 through 4 for housing starts.</p></li>
<li><p>Test the hypothesis that housing starts does not Granger cause GDP growth.</p></li>
<li><p>Interpret your results.</p></li>
</ol>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./part04-pannel.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text">面板数据</span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./chpt15-multiple-time-series.html" class="pagination-link">
        <span class="nav-page-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Multivariate Time Series</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->



</body></html>
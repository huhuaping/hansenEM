<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.253">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Hansen中高级计量体系 - 16&nbsp; Panel Data</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./chpt18-did.html" rel="next">
<link href="./chpt15-multiple-time-series.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
    <div class="container-fluid d-flex justify-content-between">
      <h1 class="quarto-secondary-nav-title"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Panel Data</span></h1>
      <button type="button" class="quarto-btn-toggle btn" aria-label="Show secondary navigation">
        <i class="bi bi-chevron-right"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Hansen中高级计量体系</a> 
        <div class="sidebar-tools-main">
    <a href="https://github.com/huhuaping/hansenEM/" title="Source Code" class="sidebar-tool px-1"><i class="bi bi-github"></i></a>
</div>
    </div>
      </div>
      <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
      </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">前言</a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./participate.html" class="sidebar-item-text sidebar-link">如何参与？</a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chpt01-intro.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chpt01-intro-chn.html" class="sidebar-item-text sidebar-link">介绍</a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./part01-reg.html" class="sidebar-item-text sidebar-link">回归</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chpt02-ce.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Conditional Expectation and Projection</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chpt02-ce-chn.html" class="sidebar-item-text sidebar-link">条件预期和预测</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chpt03-algebra.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">The Algebra of Least Squares</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chpt03-algebra-chn.html" class="sidebar-item-text sidebar-link">最小二乘代数</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chpt04-lsr.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Least Squares Regression</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chpt05-normal-reg.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Normal Regression</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./part02-LSM.html" class="sidebar-item-text sidebar-link">大样本方法</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chpt06-review.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">A Review of Large Sample Asymptotics</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chpt07-asymptotic-ls.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Asymptotic Theory for Least Squares</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chpt08-restricted-est.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Restricted Estimation</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chpt09-hypothesit-test.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Hypothesis Testing</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chpt10-resample-method.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Resampling Methods</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./part03-MEQ.html" class="sidebar-item-text sidebar-link">多方程模型</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chpt11-multi-reg.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Multivariate Regression</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chpt12-iv.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Instrumental Variables</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chpt13-gmm.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Generalized Method of Moments</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./part04-pannel.html" class="sidebar-item-text sidebar-link">面板数据</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chpt14-time-series.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Time Series</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chpt15-multiple-time-series.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Multivariate Time Series</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chpt17-panel-data.html" class="sidebar-item-text sidebar-link active"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Panel Data</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chpt18-did.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Difference in Differences</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./part05-nonpar.html" class="sidebar-item-text sidebar-link">非参方法</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chpt19-nonparameter.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Nonparametric Regression</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chpt20-series-reg.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">Series Regression</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chpt21-rdd.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">20</span>&nbsp; <span class="chapter-title">Regression Discontinuity</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chpt22-m-est.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">21</span>&nbsp; <span class="chapter-title">M-Estimators</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./part06-nonlinear.html" class="sidebar-item-text sidebar-link">非线性方法</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-6" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chpt23-nonliear-ls.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">22</span>&nbsp; <span class="chapter-title">Nonlinear Least Squares</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chpt24-quantile-reg.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">23</span>&nbsp; <span class="chapter-title">Quantile Regression</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chpt26-multiple-choice.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">24</span>&nbsp; <span class="chapter-title">Multiple Choice</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chpt27-censor-selection.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">25</span>&nbsp; <span class="chapter-title">Censoring and Selection</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chpt28-model-selection.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">26</span>&nbsp; <span class="chapter-title">Model Selection, Stein Shrinkage, and Model Averaging</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chpt29-ML.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">27</span>&nbsp; <span class="chapter-title">Machine Learning</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./summary.html" class="sidebar-item-text sidebar-link">Summary</a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">References</a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" aria-expanded="true">Appendices</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-7" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./a1-notation.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">A</span>&nbsp; <span class="chapter-title">附录a1</span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
    <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">目录</h2>
   
  <ul>
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction"><span class="toc-section-number">16.1</span>  Introduction</a></li>
  <li><a href="#time-indexing-and-unbalanced-panels" id="toc-time-indexing-and-unbalanced-panels" class="nav-link" data-scroll-target="#time-indexing-and-unbalanced-panels"><span class="toc-section-number">16.2</span>  Time Indexing and Unbalanced Panels</a></li>
  <li><a href="#notation" id="toc-notation" class="nav-link" data-scroll-target="#notation"><span class="toc-section-number">16.3</span>  Notation</a></li>
  <li><a href="#pooled-regression" id="toc-pooled-regression" class="nav-link" data-scroll-target="#pooled-regression"><span class="toc-section-number">16.4</span>  Pooled Regression</a></li>
  <li><a href="#one-way-error-component-model" id="toc-one-way-error-component-model" class="nav-link" data-scroll-target="#one-way-error-component-model"><span class="toc-section-number">16.5</span>  One-Way Error Component Model</a></li>
  <li><a href="#random-effects" id="toc-random-effects" class="nav-link" data-scroll-target="#random-effects"><span class="toc-section-number">16.6</span>  Random Effects</a></li>
  <li><a href="#fixed-effect-model" id="toc-fixed-effect-model" class="nav-link" data-scroll-target="#fixed-effect-model"><span class="toc-section-number">16.7</span>  Fixed Effect Model</a></li>
  <li><a href="#within-transformation" id="toc-within-transformation" class="nav-link" data-scroll-target="#within-transformation"><span class="toc-section-number">16.8</span>  Within Transformation</a></li>
  <li><a href="#fixed-effects-estimator" id="toc-fixed-effects-estimator" class="nav-link" data-scroll-target="#fixed-effects-estimator"><span class="toc-section-number">16.9</span>  Fixed Effects Estimator</a></li>
  <li><a href="#differenced-estimator" id="toc-differenced-estimator" class="nav-link" data-scroll-target="#differenced-estimator"><span class="toc-section-number">16.10</span>  Differenced Estimator</a></li>
  <li><a href="#dummy-variables-regression" id="toc-dummy-variables-regression" class="nav-link" data-scroll-target="#dummy-variables-regression"><span class="toc-section-number">16.11</span>  Dummy Variables Regression</a></li>
  <li><a href="#fixed-effects-covariance-matrix-estimation" id="toc-fixed-effects-covariance-matrix-estimation" class="nav-link" data-scroll-target="#fixed-effects-covariance-matrix-estimation"><span class="toc-section-number">16.12</span>  Fixed Effects Covariance Matrix Estimation</a></li>
  <li><a href="#fixed-effects-estimation-in-stata" id="toc-fixed-effects-estimation-in-stata" class="nav-link" data-scroll-target="#fixed-effects-estimation-in-stata"><span class="toc-section-number">16.13</span>  Fixed Effects Estimation in Stata</a></li>
  <li><a href="#between-estimator" id="toc-between-estimator" class="nav-link" data-scroll-target="#between-estimator"><span class="toc-section-number">16.14</span>  Between Estimator</a></li>
  <li><a href="#feasible-gls" id="toc-feasible-gls" class="nav-link" data-scroll-target="#feasible-gls"><span class="toc-section-number">16.15</span>  Feasible GLS</a></li>
  <li><a href="#intercept-in-fixed-effects-regression" id="toc-intercept-in-fixed-effects-regression" class="nav-link" data-scroll-target="#intercept-in-fixed-effects-regression"><span class="toc-section-number">16.16</span>  Intercept in Fixed Effects Regression</a></li>
  <li><a href="#estimation-of-fixed-effects" id="toc-estimation-of-fixed-effects" class="nav-link" data-scroll-target="#estimation-of-fixed-effects"><span class="toc-section-number">16.17</span>  Estimation of Fixed Effects</a></li>
  <li><a href="#gmm-interpretation-of-fixed-effects" id="toc-gmm-interpretation-of-fixed-effects" class="nav-link" data-scroll-target="#gmm-interpretation-of-fixed-effects"><span class="toc-section-number">16.18</span>  GMM Interpretation of Fixed Effects</a></li>
  <li><a href="#identification-in-the-fixed-effects-model" id="toc-identification-in-the-fixed-effects-model" class="nav-link" data-scroll-target="#identification-in-the-fixed-effects-model"><span class="toc-section-number">16.19</span>  Identification in the Fixed Effects Model</a></li>
  <li><a href="#asymptotic-distribution-of-fixed-effects-estimator" id="toc-asymptotic-distribution-of-fixed-effects-estimator" class="nav-link" data-scroll-target="#asymptotic-distribution-of-fixed-effects-estimator"><span class="toc-section-number">16.20</span>  Asymptotic Distribution of Fixed Effects Estimator</a></li>
  <li><a href="#asymptotic-distribution-for-unbalanced-panels" id="toc-asymptotic-distribution-for-unbalanced-panels" class="nav-link" data-scroll-target="#asymptotic-distribution-for-unbalanced-panels"><span class="toc-section-number">16.21</span>  Asymptotic Distribution for Unbalanced Panels</a></li>
  <li><a href="#heteroskedasticity-robust-covariance-matrix-estimation" id="toc-heteroskedasticity-robust-covariance-matrix-estimation" class="nav-link" data-scroll-target="#heteroskedasticity-robust-covariance-matrix-estimation"><span class="toc-section-number">16.22</span>  Heteroskedasticity-Robust Covariance Matrix Estimation</a></li>
  <li><a href="#heteroskedasticity-robust-estimation---unbalanced-case" id="toc-heteroskedasticity-robust-estimation---unbalanced-case" class="nav-link" data-scroll-target="#heteroskedasticity-robust-estimation---unbalanced-case"><span class="toc-section-number">16.23</span>  Heteroskedasticity-Robust Estimation - Unbalanced Case</a></li>
  <li><a href="#hausman-test-for-random-vs-fixed-effects" id="toc-hausman-test-for-random-vs-fixed-effects" class="nav-link" data-scroll-target="#hausman-test-for-random-vs-fixed-effects"><span class="toc-section-number">16.24</span>  Hausman Test for Random vs Fixed Effects</a></li>
  <li><a href="#random-effects-or-fixed-effects" id="toc-random-effects-or-fixed-effects" class="nav-link" data-scroll-target="#random-effects-or-fixed-effects"><span class="toc-section-number">16.25</span>  Random Effects or Fixed Effects?</a></li>
  <li><a href="#time-trends" id="toc-time-trends" class="nav-link" data-scroll-target="#time-trends"><span class="toc-section-number">16.26</span>  Time Trends</a></li>
  <li><a href="#two-way-error-components" id="toc-two-way-error-components" class="nav-link" data-scroll-target="#two-way-error-components"><span class="toc-section-number">16.27</span>  Two-Way Error Components</a></li>
  <li><a href="#instrumental-variables" id="toc-instrumental-variables" class="nav-link" data-scroll-target="#instrumental-variables"><span class="toc-section-number">16.28</span>  Instrumental Variables</a></li>
  <li><a href="#identification-with-instrumental-variables" id="toc-identification-with-instrumental-variables" class="nav-link" data-scroll-target="#identification-with-instrumental-variables"><span class="toc-section-number">16.29</span>  Identification with Instrumental Variables</a></li>
  <li><a href="#asymptotic-distribution-of-fixed-effects-2sls-estimator" id="toc-asymptotic-distribution-of-fixed-effects-2sls-estimator" class="nav-link" data-scroll-target="#asymptotic-distribution-of-fixed-effects-2sls-estimator"><span class="toc-section-number">16.30</span>  Asymptotic Distribution of Fixed Effects 2SLS Estimator</a></li>
  <li><a href="#linear-gmm" id="toc-linear-gmm" class="nav-link" data-scroll-target="#linear-gmm"><span class="toc-section-number">16.31</span>  Linear GMM</a></li>
  <li><a href="#estimation-with-time-invariant-regressors" id="toc-estimation-with-time-invariant-regressors" class="nav-link" data-scroll-target="#estimation-with-time-invariant-regressors"><span class="toc-section-number">16.32</span>  Estimation with Time-Invariant Regressors</a></li>
  <li><a href="#hausman-taylor-model" id="toc-hausman-taylor-model" class="nav-link" data-scroll-target="#hausman-taylor-model"><span class="toc-section-number">16.33</span>  Hausman-Taylor Model</a></li>
  <li><a href="#jackknife-covariance-matrix-estimation" id="toc-jackknife-covariance-matrix-estimation" class="nav-link" data-scroll-target="#jackknife-covariance-matrix-estimation"><span class="toc-section-number">16.34</span>  Jackknife Covariance Matrix Estimation</a></li>
  <li><a href="#panel-bootstrap" id="toc-panel-bootstrap" class="nav-link" data-scroll-target="#panel-bootstrap"><span class="toc-section-number">16.35</span>  Panel Bootstrap</a></li>
  <li><a href="#dynamic-panel-models" id="toc-dynamic-panel-models" class="nav-link" data-scroll-target="#dynamic-panel-models"><span class="toc-section-number">16.36</span>  Dynamic Panel Models</a></li>
  <li><a href="#the-bias-of-fixed-effects-estimation" id="toc-the-bias-of-fixed-effects-estimation" class="nav-link" data-scroll-target="#the-bias-of-fixed-effects-estimation"><span class="toc-section-number">16.37</span>  The Bias of Fixed Effects Estimation</a></li>
  <li><a href="#anderson-hsiao-estimator" id="toc-anderson-hsiao-estimator" class="nav-link" data-scroll-target="#anderson-hsiao-estimator"><span class="toc-section-number">16.38</span>  Anderson-Hsiao Estimator</a></li>
  <li><a href="#arellano-bond-estimator" id="toc-arellano-bond-estimator" class="nav-link" data-scroll-target="#arellano-bond-estimator"><span class="toc-section-number">16.39</span>  Arellano-Bond Estimator</a></li>
  <li><a href="#weak-instruments" id="toc-weak-instruments" class="nav-link" data-scroll-target="#weak-instruments"><span class="toc-section-number">16.40</span>  Weak Instruments</a></li>
  <li><a href="#dynamic-panels-with-predetermined-regressors" id="toc-dynamic-panels-with-predetermined-regressors" class="nav-link" data-scroll-target="#dynamic-panels-with-predetermined-regressors"><span class="toc-section-number">16.41</span>  Dynamic Panels with Predetermined Regressors</a></li>
  <li><a href="#blundell-bond-estimator" id="toc-blundell-bond-estimator" class="nav-link" data-scroll-target="#blundell-bond-estimator"><span class="toc-section-number">16.42</span>  Blundell-Bond Estimator</a></li>
  <li><a href="#forward-orthogonal-transformation" id="toc-forward-orthogonal-transformation" class="nav-link" data-scroll-target="#forward-orthogonal-transformation"><span class="toc-section-number">16.43</span>  Forward Orthogonal Transformation</a></li>
  <li><a href="#empirical-illustration" id="toc-empirical-illustration" class="nav-link" data-scroll-target="#empirical-illustration"><span class="toc-section-number">16.44</span>  Empirical Illustration</a></li>
  <li><a href="#exercises" id="toc-exercises" class="nav-link" data-scroll-target="#exercises"><span class="toc-section-number">16.45</span>  Exercises</a></li>
  <li><a href="#exercise-17.1" id="toc-exercise-17.1" class="nav-link" data-scroll-target="#exercise-17.1"><span class="toc-section-number">16.46</span>  Exercise 17.1</a></li>
  <li><a href="#exercise-17.11" id="toc-exercise-17.11" class="nav-link" data-scroll-target="#exercise-17.11"><span class="toc-section-number">16.47</span>  Exercise <span class="math inline">\(17.11\)</span></a></li>
  <li><a href="#exercise-17.12" id="toc-exercise-17.12" class="nav-link" data-scroll-target="#exercise-17.12"><span class="toc-section-number">16.48</span>  Exercise <span class="math inline">\(17.12\)</span></a></li>
  </ul>
<div class="toc-actions"><div><i class="bi bi-github"></i></div><div class="action-links"><p><a href="https://github.com/huhuaping/hansenEM/edit/master/chpt17-panel-data.qmd" class="toc-action">Edit this page</a></p></div></div></nav>
</nav>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title d-none d-lg-block"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Panel Data</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<section id="introduction" class="level2" data-number="16.1">
<h2 data-number="16.1" class="anchored" data-anchor-id="introduction"><span class="header-section-number">16.1</span> Introduction</h2>
<p>Economists traditionally use the term panel data to refer to data structures consisting of observations on individuals for multiple time periods. Other fields such as statistics typically call this structure longitudinal data. The observed “individuals” can be, for example, people, households, workers, firms, schools, production plants, industries, regions, states, or countries. The distinguishing feature relative to cross-sectional data sets is the presence of multiple observations for each individual. More broadly, panel data methods can be applied to any context with cluster-type dependence.</p>
<p>There are several distinct advantages of panel data relative to cross-section data. One is the possibility of controlling for unobserved time-invariant endogeneity without the use of instrumental variables. A second is the possibility of allowing for broader forms of heterogeneity. A third is modeling dynamic relationships and effects.</p>
<p>There are two broad categories of panel data sets in economic applications: micro panels and macro panels. Micro panels are typically surveys or administrative records on individuals and are characterized by a large number of individuals (often in the 1000’s or higher) and a relatively small number of time periods (often 2 to 20 years). Macro panels are typically national or regional macroeconomic variables and are characterized by a moderate number of individuals (e.g.&nbsp;7-20) and a moderate number of time periods (20-60 years).</p>
<p>Panel data was once relatively esoteric in applied economic practice. Now, it is a dominant feature of applied research.</p>
<p>A typical maintained assumption for micro panels (which we follow in this chapter) is that the individuals are mutually independent while the observations for a given individual are correlated across time periods. This means that the observations follow a clustered dependence structure. Because of this, current econometric practice is to use cluster-robust covariance matrix estimators when possible. Similar assumptions are often used for macro panels though the assumption of independence across individuals (e.g.&nbsp;countries) is much less compelling.</p>
<p>The application of panel data methods in econometrics started with the pioneering work of Mundlak (1961) and Balestra and Nerlove (1966).</p>
<p>Several excellent monographs and textbooks have been written on panel econometrics, including Arellano (2003), Hsiao (2003), Wooldridge (2010), and Baltagi (2013). This chapter will summarize some of the main themes but for a more in-depth treatment see these references.</p>
<p>One challenge arising in panel data applications is that the computational methods can require meticulous attention to detail. It is therefore advised to use established packages for routine applications. For most panel data applications in economics Stata is the standard package.</p>
</section>
<section id="time-indexing-and-unbalanced-panels" class="level2" data-number="16.2">
<h2 data-number="16.2" class="anchored" data-anchor-id="time-indexing-and-unbalanced-panels"><span class="header-section-number">16.2</span> Time Indexing and Unbalanced Panels</h2>
<p>It is typical to index observations by both the individual <span class="math inline">\(i\)</span> and the time period <span class="math inline">\(t\)</span>, thus <span class="math inline">\(Y_{i t}\)</span> denotes a variable for individual <span class="math inline">\(i\)</span> in period <span class="math inline">\(t\)</span>. We index individuals as <span class="math inline">\(i=1, \ldots, N\)</span> and time periods as <span class="math inline">\(t=1, \ldots T\)</span>. Thus <span class="math inline">\(N\)</span> is the number of individuals in the panel and <span class="math inline">\(T\)</span> is the number of time series periods.</p>
<p>Panel data sets can involve data at any time series frequency though the typical application involves annual data. The observations in a data set will be indexed by calendar time which for the case of annual observations is the year. For notational convenience it is customary to denote the time periods as <span class="math inline">\(t=\)</span> <span class="math inline">\(1, \ldots, T\)</span>, so that <span class="math inline">\(t=1\)</span> is the first time period observed and <span class="math inline">\(T\)</span> is the final time period.</p>
<p>When observations are available on all individuals for the same time periods we say that the panel is balanced. In this case there are an equal number <span class="math inline">\(T\)</span> of observations for each individual and the total number of observations is <span class="math inline">\(n=N T\)</span>.</p>
<p>When different time periods are available for the individuals in the sample we say that the panel is unbalanced. This is the most common type of panel data set. It does not pose a problem for applications but does make the notation cumbersome and also complicates computer programming.</p>
<p>To illustrate, consider the data set Invest 1993 on the textbook webpage. This is a sample of 1962 U.S. firms extracted from Compustat, assembled by Bronwyn Hall, and used in the empirical work in Hall and Hall (1993). In Table 17.1 we display a set of variables from the data set for the first 13 observations. The first variable is the firm code number. The second variable is the year of the observation. These two variables are essential for any panel data analysis. In Table <span class="math inline">\(17.1\)</span> you can see that the first firm (#32) is observed for the years 1970 through 1977. The second firm (#209) is observed for 1987 through 1991. You can see that the years vary considerably across the firms so this is an unbalanced panel.</p>
<p>For unbalanced panels the time index <span class="math inline">\(t=1, \ldots, T\)</span> denotes the full set of time periods. For example, in the data set Invest 1993 there are observations for the years 1960 through 1991, so the total number of time periods is <span class="math inline">\(T=32\)</span>. Each individual is observed for a subset of <span class="math inline">\(T_{i}\)</span> periods. The set of time periods for individual <span class="math inline">\(i\)</span> is denoted as <span class="math inline">\(S_{i}\)</span> so that individual-specific sums (over time periods) are written as <span class="math inline">\(\sum_{t \in S_{i}}\)</span>.</p>
<p>The observed time periods for a given individual are typically contiguous (for example, in Table 17.1, firm #32 is observed for each year from 1970 through 1977) but in some cases are non-continguous (if, for example, 1973 was missing for firm #32). The total number of observations in the sample is <span class="math inline">\(n=\sum_{i=1}^{N} T_{i}\)</span>.</p>
<p>Table 17.1: Observations from Investment Data Set</p>
<table class="table">
<colgroup>
<col style="width: 14%">
<col style="width: 3%">
<col style="width: 7%">
<col style="width: 11%">
<col style="width: 13%">
<col style="width: 7%">
<col style="width: 11%">
<col style="width: 13%">
<col style="width: 16%">
</colgroup>
<thead>
<tr class="header">
<th>Firm Code Number</th>
<th style="text-align: right;">Year</th>
<th style="text-align: right;"><span class="math inline">\(I_{i t}\)</span></th>
<th style="text-align: right;"><span class="math inline">\(\bar{I}_{i}\)</span></th>
<th style="text-align: right;"><span class="math inline">\(\dot{I}_{i t}\)</span></th>
<th style="text-align: right;"><span class="math inline">\(Q_{i t}\)</span></th>
<th style="text-align: right;"><span class="math inline">\(\bar{Q}_{i}\)</span></th>
<th style="text-align: right;"><span class="math inline">\(\dot{Q}_{i t}\)</span></th>
<th style="text-align: right;"><span class="math inline">\(\widehat{e}_{i t}\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>32</td>
<td style="text-align: right;">1970</td>
<td style="text-align: right;"><span class="math inline">\(0.122\)</span></td>
<td style="text-align: right;"><span class="math inline">\(0.155\)</span></td>
<td style="text-align: right;"><span class="math inline">\(-0.033\)</span></td>
<td style="text-align: right;"><span class="math inline">\(1.17\)</span></td>
<td style="text-align: right;"><span class="math inline">\(0.62\)</span></td>
<td style="text-align: right;"><span class="math inline">\(0.55\)</span></td>
<td style="text-align: right;">.</td>
</tr>
<tr class="even">
<td>32</td>
<td style="text-align: right;">1971</td>
<td style="text-align: right;"><span class="math inline">\(0.092\)</span></td>
<td style="text-align: right;"><span class="math inline">\(0.155\)</span></td>
<td style="text-align: right;"><span class="math inline">\(-0.063\)</span></td>
<td style="text-align: right;"><span class="math inline">\(0.79\)</span></td>
<td style="text-align: right;"><span class="math inline">\(0.62\)</span></td>
<td style="text-align: right;"><span class="math inline">\(0.17\)</span></td>
<td style="text-align: right;"><span class="math inline">\(-0.005\)</span></td>
</tr>
<tr class="odd">
<td>32</td>
<td style="text-align: right;">1972</td>
<td style="text-align: right;"><span class="math inline">\(0.094\)</span></td>
<td style="text-align: right;"><span class="math inline">\(0.155\)</span></td>
<td style="text-align: right;"><span class="math inline">\(-0.061\)</span></td>
<td style="text-align: right;"><span class="math inline">\(0.91\)</span></td>
<td style="text-align: right;"><span class="math inline">\(0.62\)</span></td>
<td style="text-align: right;"><span class="math inline">\(0.29\)</span></td>
<td style="text-align: right;"><span class="math inline">\(-0.005\)</span></td>
</tr>
<tr class="even">
<td>32</td>
<td style="text-align: right;">1973</td>
<td style="text-align: right;"><span class="math inline">\(0.116\)</span></td>
<td style="text-align: right;"><span class="math inline">\(0.155\)</span></td>
<td style="text-align: right;"><span class="math inline">\(-0.039\)</span></td>
<td style="text-align: right;"><span class="math inline">\(0.29\)</span></td>
<td style="text-align: right;"><span class="math inline">\(0.62\)</span></td>
<td style="text-align: right;"><span class="math inline">\(-0.33\)</span></td>
<td style="text-align: right;"><span class="math inline">\(0.014\)</span></td>
</tr>
<tr class="odd">
<td>32</td>
<td style="text-align: right;">1974</td>
<td style="text-align: right;"><span class="math inline">\(0.099\)</span></td>
<td style="text-align: right;"><span class="math inline">\(0.155\)</span></td>
<td style="text-align: right;"><span class="math inline">\(-0.057\)</span></td>
<td style="text-align: right;"><span class="math inline">\(0.30\)</span></td>
<td style="text-align: right;"><span class="math inline">\(0.62\)</span></td>
<td style="text-align: right;"><span class="math inline">\(-0.32\)</span></td>
<td style="text-align: right;"><span class="math inline">\(-0.002\)</span></td>
</tr>
<tr class="even">
<td>32</td>
<td style="text-align: right;">1975</td>
<td style="text-align: right;"><span class="math inline">\(0.187\)</span></td>
<td style="text-align: right;"><span class="math inline">\(0.155\)</span></td>
<td style="text-align: right;"><span class="math inline">\(0.032\)</span></td>
<td style="text-align: right;"><span class="math inline">\(0.56\)</span></td>
<td style="text-align: right;"><span class="math inline">\(0.62\)</span></td>
<td style="text-align: right;"><span class="math inline">\(-0.06\)</span></td>
<td style="text-align: right;"><span class="math inline">\(0.086\)</span></td>
</tr>
<tr class="odd">
<td>32</td>
<td style="text-align: right;">1976</td>
<td style="text-align: right;"><span class="math inline">\(0.349\)</span></td>
<td style="text-align: right;"><span class="math inline">\(0.155\)</span></td>
<td style="text-align: right;"><span class="math inline">\(0.194\)</span></td>
<td style="text-align: right;"><span class="math inline">\(0.38\)</span></td>
<td style="text-align: right;"><span class="math inline">\(0.62\)</span></td>
<td style="text-align: right;"><span class="math inline">\(-0.24\)</span></td>
<td style="text-align: right;"><span class="math inline">\(0.248\)</span></td>
</tr>
<tr class="even">
<td>32</td>
<td style="text-align: right;">1977</td>
<td style="text-align: right;"><span class="math inline">\(0.182\)</span></td>
<td style="text-align: right;"><span class="math inline">\(0.155\)</span></td>
<td style="text-align: right;"><span class="math inline">\(0.027\)</span></td>
<td style="text-align: right;"><span class="math inline">\(0.57\)</span></td>
<td style="text-align: right;"><span class="math inline">\(0.62\)</span></td>
<td style="text-align: right;"><span class="math inline">\(-0.05\)</span></td>
<td style="text-align: right;"><span class="math inline">\(0.081\)</span></td>
</tr>
<tr class="odd">
<td>209</td>
<td style="text-align: right;">1987</td>
<td style="text-align: right;"><span class="math inline">\(0.095\)</span></td>
<td style="text-align: right;"><span class="math inline">\(0.071\)</span></td>
<td style="text-align: right;"><span class="math inline">\(0.024\)</span></td>
<td style="text-align: right;"><span class="math inline">\(9.06\)</span></td>
<td style="text-align: right;"><span class="math inline">\(21.57\)</span></td>
<td style="text-align: right;"><span class="math inline">\(-12.51\)</span></td>
<td style="text-align: right;">.</td>
</tr>
<tr class="even">
<td>209</td>
<td style="text-align: right;">1988</td>
<td style="text-align: right;"><span class="math inline">\(0.044\)</span></td>
<td style="text-align: right;"><span class="math inline">\(0.071\)</span></td>
<td style="text-align: right;"><span class="math inline">\(-0.027\)</span></td>
<td style="text-align: right;"><span class="math inline">\(16.90\)</span></td>
<td style="text-align: right;"><span class="math inline">\(21.57\)</span></td>
<td style="text-align: right;"><span class="math inline">\(-4.67\)</span></td>
<td style="text-align: right;"><span class="math inline">\(-0.244\)</span></td>
</tr>
<tr class="odd">
<td>209</td>
<td style="text-align: right;">1989</td>
<td style="text-align: right;"><span class="math inline">\(0.069\)</span></td>
<td style="text-align: right;"><span class="math inline">\(0.071\)</span></td>
<td style="text-align: right;"><span class="math inline">\(-0.002\)</span></td>
<td style="text-align: right;"><span class="math inline">\(25.14\)</span></td>
<td style="text-align: right;"><span class="math inline">\(21.57\)</span></td>
<td style="text-align: right;"><span class="math inline">\(3.57\)</span></td>
<td style="text-align: right;"><span class="math inline">\(-0.257\)</span></td>
</tr>
<tr class="even">
<td>209</td>
<td style="text-align: right;">1990</td>
<td style="text-align: right;"><span class="math inline">\(0.113\)</span></td>
<td style="text-align: right;"><span class="math inline">\(0.071\)</span></td>
<td style="text-align: right;"><span class="math inline">\(0.042\)</span></td>
<td style="text-align: right;"><span class="math inline">\(25.60\)</span></td>
<td style="text-align: right;"><span class="math inline">\(21.57\)</span></td>
<td style="text-align: right;"><span class="math inline">\(4.03\)</span></td>
<td style="text-align: right;"><span class="math inline">\(-0.226\)</span></td>
</tr>
<tr class="odd">
<td>209</td>
<td style="text-align: right;">1991</td>
<td style="text-align: right;"><span class="math inline">\(0.034\)</span></td>
<td style="text-align: right;"><span class="math inline">\(0.071\)</span></td>
<td style="text-align: right;"><span class="math inline">\(-0.037\)</span></td>
<td style="text-align: right;"><span class="math inline">\(31.14\)</span></td>
<td style="text-align: right;"><span class="math inline">\(21.57\)</span></td>
<td style="text-align: right;"><span class="math inline">\(9.57\)</span></td>
<td style="text-align: right;"><span class="math inline">\(-0.283\)</span></td>
</tr>
</tbody>
</table>
</section>
<section id="notation" class="level2" data-number="16.3">
<h2 data-number="16.3" class="anchored" data-anchor-id="notation"><span class="header-section-number">16.3</span> Notation</h2>
<p>This chapter focuses on panel data regression models whose observations are pairs <span class="math inline">\(\left(Y_{i t}, X_{i t}\right)\)</span> where <span class="math inline">\(Y_{i t}\)</span> is the dependent variable and <span class="math inline">\(X_{i t}\)</span> is a <span class="math inline">\(k\)</span>-vector of regressors. These are the observations on individual <span class="math inline">\(i\)</span> for time period <span class="math inline">\(t\)</span>.</p>
<p>It will be useful to cluster the observations at the level of the individual. We borrow the notation from Section <span class="math inline">\(4.21\)</span> to write <span class="math inline">\(\boldsymbol{Y}_{i}\)</span> as the <span class="math inline">\(T_{i} \times 1\)</span> stacked observations on <span class="math inline">\(Y_{i t}\)</span> for <span class="math inline">\(t \in S_{i}\)</span>, stacked in chronological order. Similarly, we write <span class="math inline">\(\boldsymbol{X}_{i}\)</span> as the <span class="math inline">\(T_{i} \times k\)</span> matrix of stacked <span class="math inline">\(X_{i t}^{\prime}\)</span> for <span class="math inline">\(t \in S_{i}\)</span>, stacked in chronological order.</p>
<p>We will also sometimes use matrix notation for the full sample. To do so, let <span class="math inline">\(\boldsymbol{Y}=\left(\boldsymbol{Y}_{1}^{\prime}, \ldots, \boldsymbol{Y}_{N}^{\prime}\right)^{\prime}\)</span> denote the <span class="math inline">\(n \times 1\)</span> vector of stacked <span class="math inline">\(\boldsymbol{Y}_{i}\)</span>, and set <span class="math inline">\(\boldsymbol{X}=\left(\boldsymbol{X}_{1}^{\prime}, \ldots, \boldsymbol{X}_{N}^{\prime}\right)^{\prime}\)</span> similarly.</p>
</section>
<section id="pooled-regression" class="level2" data-number="16.4">
<h2 data-number="16.4" class="anchored" data-anchor-id="pooled-regression"><span class="header-section-number">16.4</span> Pooled Regression</h2>
<p>The simplest model in panel regresion is pooled regresssion</p>
<p><span class="math display">\[
\begin{aligned}
Y_{i t} &amp;=X_{i t}^{\prime} \beta+e_{i t} \\
\mathbb{E}\left[X_{i t} e_{i t}\right] &amp;=0 .
\end{aligned}
\]</span></p>
<p>where <span class="math inline">\(\beta\)</span> is a <span class="math inline">\(k \times 1\)</span> coefficient vector and <span class="math inline">\(e_{i t}\)</span> is an error. The model can be written at the level of the individual as</p>
<p><span class="math display">\[
\begin{aligned}
\boldsymbol{Y}_{i} &amp;=\boldsymbol{X}_{i} \beta+\boldsymbol{e}_{i} \\
\mathbb{E}\left[\boldsymbol{X}_{i}^{\prime} \boldsymbol{e}_{i}\right] &amp;=0
\end{aligned}
\]</span></p>
<p>where <span class="math inline">\(\boldsymbol{e}_{i}\)</span> is <span class="math inline">\(T_{i} \times 1\)</span>. The equation for the full sample is <span class="math inline">\(\boldsymbol{Y}=\boldsymbol{X} \beta+\boldsymbol{e}\)</span> where <span class="math inline">\(\boldsymbol{e}\)</span> is <span class="math inline">\(n \times 1\)</span>.</p>
<p>The standard estimator of <span class="math inline">\(\beta\)</span> in the pooled regression model is least squares, which can be written as</p>
<p><span class="math display">\[
\begin{aligned}
\widehat{\beta}_{\text {pool }} &amp;=\left(\sum_{i=1}^{N} \sum_{t \in S_{i}} X_{i t} X_{i t}^{\prime}\right)^{-1}\left(\sum_{i=1}^{N} \sum_{t \in S_{i}} X_{i t} Y_{i t}\right) \\
&amp;=\left(\sum_{i=1}^{N} \boldsymbol{X}_{i}^{\prime} \boldsymbol{X}_{i}\right)^{-1}\left(\sum_{i=1}^{N} \boldsymbol{X}_{i}^{\prime} \boldsymbol{Y}_{i}\right) \\
&amp;=\left(\boldsymbol{X}^{\prime} \boldsymbol{X}\right)^{-1}\left(\boldsymbol{X}^{\prime} \boldsymbol{Y}\right) .
\end{aligned}
\]</span></p>
<p>In the context of panel data <span class="math inline">\(\widehat{\beta}_{\text {pool }}\)</span> is called the pooled regression estimator. The vector of residuals for the <span class="math inline">\(i^{t h}\)</span> individual is <span class="math inline">\(\widehat{\boldsymbol{e}}_{i}=\boldsymbol{Y}_{i}-\boldsymbol{X}_{i} \widehat{\beta}_{\text {pool }}\)</span>.</p>
<p>The pooled regression model is ideally suited for the context where the errors <span class="math inline">\(e_{i t}\)</span> satisfy strict mean independence:</p>
<p><span class="math display">\[
\mathbb{E}\left[e_{i t} \mid \boldsymbol{X}_{i}\right]=0 .
\]</span></p>
<p>This occurs when the errors <span class="math inline">\(e_{i t}\)</span> are mean independent of all regressors <span class="math inline">\(X_{i j}\)</span> for all time periods <span class="math inline">\(j=1, \ldots, T\)</span>. Strict mean independence is stronger than pairwise mean independence <span class="math inline">\(\mathbb{E}\left[e_{i t} \mid X_{i t}\right]=0\)</span> as well as projection (17.1). Strict mean independence requires that neither lagged nor future values of <span class="math inline">\(X_{i t}\)</span> help to forecast <span class="math inline">\(e_{i t}\)</span>. It excludes lagged dependent variables (such as <span class="math inline">\(Y_{i t-1}\)</span> ) from <span class="math inline">\(X_{i t}\)</span> (otherwise <span class="math inline">\(e_{i t}\)</span> would be predictable given <span class="math inline">\(X_{i t+1}\)</span> ). It also requires that <span class="math inline">\(X_{i t}\)</span> is exogenous in the sense discussed in Chapter 12.</p>
<p>We now describe some statistical properties of <span class="math inline">\(\widehat{\beta}_{\text {pool }}\)</span> under (17.2). First, notice that by linearity and the cluster-level notation we can write the estimator as</p>
<p><span class="math display">\[
\widehat{\beta}_{\mathrm{pool}}=\left(\sum_{i=1}^{N} \boldsymbol{X}_{i}^{\prime} \boldsymbol{X}_{i}\right)^{-1}\left(\sum_{i=1}^{N} \boldsymbol{X}_{i}^{\prime}\left(\boldsymbol{X}_{i} \beta+\boldsymbol{e}_{i}\right)\right)=\beta+\left(\sum_{i=1}^{N} \boldsymbol{X}_{i}^{\prime} \boldsymbol{X}_{i}\right)^{-1}\left(\sum_{i=1}^{N} \boldsymbol{X}_{i}^{\prime} \boldsymbol{e}_{i}\right) .
\]</span></p>
<p>Using (17.2)</p>
<p><span class="math display">\[
\mathbb{E}\left[\widehat{\beta}_{\text {pool }} \mid \boldsymbol{X}\right]=\beta+\left(\sum_{i=1}^{N} \boldsymbol{X}_{i}^{\prime} \boldsymbol{X}_{i}\right)^{-1}\left(\sum_{i=1}^{N} \boldsymbol{X}_{i}^{\prime} \mathbb{E}\left[\boldsymbol{e}_{i} \mid \boldsymbol{X}_{i}\right]\right)=\beta
\]</span></p>
<p>so <span class="math inline">\(\widehat{\beta}_{\text {pool }}\)</span> is unbiased for <span class="math inline">\(\beta\)</span>.</p>
<p>Under the additional assumption that the error <span class="math inline">\(e_{i t}\)</span> is serially uncorrelated and homoskedastic the covariance estimator takes a classical form and the classical homoskedastic variance estimator can be used. If the error <span class="math inline">\(e_{i t}\)</span> is heteroskedastic but serially uncorrelated then a heteroskedasticity-robust covariance matrix estimator can be used.</p>
<p>In general, however, we expect the errors <span class="math inline">\(e_{i t}\)</span> to be correlated across time <span class="math inline">\(t\)</span> for a given individual. This does not necessarily violate (17.2) but invalidates classical covariance matrix estimation. The conventional solution is to use a cluster-robust covariance matrix estimator which allows arbitrary withincluster dependence. Cluster-robust covariance matrix estimators for pooled regression equal</p>
<p><span class="math display">\[
\widehat{\boldsymbol{V}}_{\text {pool }}=\left(\boldsymbol{X}^{\prime} \boldsymbol{X}\right)^{-1}\left(\sum_{i=1}^{N} \boldsymbol{X}_{i}^{\prime} \widehat{\boldsymbol{e}}_{i} \widehat{\boldsymbol{e}}_{i}^{\prime} \boldsymbol{X}_{i}\right)\left(\boldsymbol{X}^{\prime} \boldsymbol{X}\right)^{-1} .
\]</span></p>
<p>As in (4.55) this can be multiplied by a degree-of-freedom adjustment. The adjustment used by the Stata regress command is</p>
<p><span class="math display">\[
\widehat{\boldsymbol{V}}_{\text {pool }}=\left(\frac{n-1}{n-k}\right)\left(\frac{N}{N-1}\right)\left(\boldsymbol{X}^{\prime} \boldsymbol{X}\right)^{-1}\left(\sum_{i=1}^{N} \boldsymbol{X}_{i}^{\prime} \widehat{\boldsymbol{e}}_{i} \widehat{\boldsymbol{e}}_{i}^{\prime} \boldsymbol{X}_{i}\right)\left(\boldsymbol{X}^{\prime} \boldsymbol{X}\right)^{-1}
\]</span></p>
<p>The pooled regression estimator with cluster-robust standard errors can be obtained using the Stata command regress cluster(id) where id indicates the individual.</p>
<p>When strict mean independence (17.2) fails the pooled least squares estimator <span class="math inline">\(\widehat{\beta}_{\text {pool }}\)</span> is not necessarily consistent for <span class="math inline">\(\beta\)</span>. Since strict mean independence is a strong and undesirable restriction it is typically preferred to adopt one of the alternative estimators described in the following sections.</p>
<p>To illustrate the pooled regression estimator consider the data set Invest1993 described earlier. We consider a simple investment model</p>
<p><span class="math display">\[
I_{i t}=\beta_{1} Q_{i t-1}+\beta_{2} D_{i t-1}+\beta_{3} C F_{i t-1}+\beta_{4} T_{i}+e_{i t}
\]</span></p>
<p>where <span class="math inline">\(I\)</span> is investment/assets, <span class="math inline">\(Q\)</span> is market value/assets, <span class="math inline">\(D\)</span> is long term debt/assets, <span class="math inline">\(C F\)</span> is cash flow/assets, and <span class="math inline">\(T\)</span> is a dummy variable indicating if the corporation’s stock is traded on the NYSE or AMEX. The regression also includes 19 dummy variables indicating an industry code. The <span class="math inline">\(Q\)</span> theory of investment suggests that <span class="math inline">\(\beta_{1}&gt;0\)</span> while <span class="math inline">\(\beta_{2}=\beta_{3}=0\)</span>. Theories of liquidity constraints suggest that <span class="math inline">\(\beta_{2}&lt;0\)</span> and <span class="math inline">\(\beta_{3}&gt;0\)</span>. We will be using this example throughout this chapter. The values of <span class="math inline">\(I\)</span> and <span class="math inline">\(Q\)</span> for the first 13 observations are also displayed in Table 17.1.</p>
<p>In Table <span class="math inline">\(17.2\)</span> we present the pooled regression estimates of (17.3) in the first column with clusterrobust standard errors.</p>
</section>
<section id="one-way-error-component-model" class="level2" data-number="16.5">
<h2 data-number="16.5" class="anchored" data-anchor-id="one-way-error-component-model"><span class="header-section-number">16.5</span> One-Way Error Component Model</h2>
<p>One approach to panel data regression is to model the correlation structure of the regression error <span class="math inline">\(e_{i t}\)</span>. The most common choice is an error-components structure. The simplest takes the form</p>
<p><span class="math display">\[
e_{i t}=u_{i}+\varepsilon_{i t}
\]</span></p>
<p>Table 17.2: Estimates of Investment Equation</p>
<p><img src="images//2022_10_23_acbfcce1ea7ce1901e2dg-05.jpg" class="img-fluid"></p>
<p>Cluster-robust standard errors in parenthesis.</p>
<p>where <span class="math inline">\(u_{i}\)</span> is an individual-specific effect and <span class="math inline">\(\varepsilon_{i t}\)</span> are idiosyncratic (i.i.d.) errors. This is known as a oneway error component model.</p>
<p>In vector notation we can write <span class="math inline">\(\boldsymbol{e}_{i}=\mathbf{1}_{i} u_{i}+\boldsymbol{\varepsilon}_{i}\)</span> where <span class="math inline">\(\mathbf{1}_{i}\)</span> is a <span class="math inline">\(T_{i} \times 1\)</span> vector of 1’s.</p>
<p>The one-way error component regression model is</p>
<p><span class="math display">\[
Y_{i t}=X_{i t}^{\prime} \beta+u_{i}+\varepsilon_{i t}
\]</span></p>
<p>written at the level of the observation, or <span class="math inline">\(\boldsymbol{Y}_{i}=\boldsymbol{X}_{i} \beta+\mathbf{1}_{i} u_{i}+\boldsymbol{\varepsilon}_{i}\)</span> written at the level of the individual.</p>
<p>To illustrate why an error-component structure such as (17.4) might be appropriate, examine Table 17.1. In the final column we have included the pooled regression residuals <span class="math inline">\(\widehat{e}_{i t}\)</span> for these observations. (There is no residual for the first year for each firm due to the lack of lagged regressors for this observation.) What is quite striking is that the residuals for the second firm (#209) are all negative, clustering around <span class="math inline">\(-0.25\)</span>. While informal, this suggests that it may be appropriate to model these errors using (17.4), expecting that firm #209 has a large negative value for its individual effect <span class="math inline">\(u\)</span>.</p>
</section>
<section id="random-effects" class="level2" data-number="16.6">
<h2 data-number="16.6" class="anchored" data-anchor-id="random-effects"><span class="header-section-number">16.6</span> Random Effects</h2>
<p>The random effects model assumes that the errors <span class="math inline">\(u_{i}\)</span> and <span class="math inline">\(\varepsilon_{i t}\)</span> in (17.4) are conditionally mean zero, uncorrelated, and homoskedastic.</p>
<p>Assumption 17.1 Random Effects. Model (17.4) holds with</p>
<p><span class="math display">\[
\begin{aligned}
\mathbb{E}\left[\varepsilon_{i t} \mid \boldsymbol{X}_{i}\right] &amp;=0 \\
\mathbb{E}\left[\varepsilon_{i t}^{2} \mid \boldsymbol{X}_{i}\right] &amp;=\sigma_{\varepsilon}^{2} \\
\mathbb{E}\left[\varepsilon_{i t} \varepsilon_{j s} \mid \boldsymbol{X}_{i}\right] &amp;=0 \\
\mathbb{E}\left[u_{i} \mid \boldsymbol{X}_{i}\right] &amp;=0 \\
\mathbb{E}\left[u_{i}^{2} \mid \boldsymbol{X}_{i}\right] &amp;=\sigma_{u}^{2} \\
\mathbb{E}\left[u_{i} \varepsilon_{i t} \mid \boldsymbol{X}_{i}\right] &amp;=0
\end{aligned}
\]</span></p>
<p>where (17.7) holds for all <span class="math inline">\(s \neq t\)</span>. Assumption <span class="math inline">\(17.1\)</span> is known as a random effects specification. It implies that the vector of errors <span class="math inline">\(\boldsymbol{e}_{i}\)</span> for individual <span class="math inline">\(i\)</span> has the covariance structure</p>
<p><span class="math display">\[
\begin{aligned}
\mathbb{E}\left[\boldsymbol{e}_{i} \mid \boldsymbol{X}_{i}\right] &amp;=0 \\
\mathbb{E}\left[\boldsymbol{e}_{i} \boldsymbol{e}_{i}^{\prime} \mid \boldsymbol{X}_{i}\right] &amp;=\mathbf{1}_{i} \mathbf{1}_{i}^{\prime} \sigma_{u}^{2}+\boldsymbol{I}_{i} \sigma_{\varepsilon}^{2} \\
&amp;=\left(\begin{array}{cccc}
\sigma_{u}^{2}+\sigma_{\varepsilon}^{2} &amp; \sigma_{u}^{2} &amp; \cdots &amp; \sigma_{u}^{2} \\
\sigma_{u}^{2} &amp; \sigma_{u}^{2}+\sigma_{\varepsilon}^{2} &amp; \cdots &amp; \sigma_{u}^{2} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
\sigma_{u}^{2} &amp; \sigma_{u}^{2} &amp; \cdots &amp; \sigma_{u}^{2}+\sigma_{\varepsilon}^{2}
\end{array}\right) \\
&amp;=\sigma_{\varepsilon}^{2} \Omega_{i},
\end{aligned}
\]</span></p>
<p>say, where <span class="math inline">\(\boldsymbol{I}_{i}\)</span> is an identity matrix of dimension <span class="math inline">\(T_{i}\)</span>. The matrix <span class="math inline">\(\Omega_{i}\)</span> depends on <span class="math inline">\(i\)</span> since its dimension depends on the number of observed time periods <span class="math inline">\(T_{i}\)</span>.</p>
<p>Assumptions 17.1.1 and 17.1.4 state that the idiosyncratic error <span class="math inline">\(\varepsilon_{i t}\)</span> and individual-specific error <span class="math inline">\(u_{i}\)</span> are strictly mean independent so the combined error <span class="math inline">\(e_{i t}\)</span> is strictly mean independent as well.</p>
<p>The random effects model is equivalent to an equi-correlation model. That is, suppose that the error <span class="math inline">\(e_{i t}\)</span> satisfies</p>
<p><span class="math display">\[
\begin{aligned}
\mathbb{E}\left[e_{i t} \mid \boldsymbol{X}_{i}\right] &amp;=0 \\
\mathbb{E}\left[e_{i t}^{2} \mid \boldsymbol{X}_{i}\right] &amp;=\sigma^{2}
\end{aligned}
\]</span></p>
<p>and</p>
<p><span class="math display">\[
\mathbb{E}\left[e_{i s} e_{i t} \mid \boldsymbol{X}_{i}\right]=\rho \sigma^{2}
\]</span></p>
<p>for <span class="math inline">\(s \neq t\)</span>. These conditions imply that <span class="math inline">\(e_{i t}\)</span> can be written as (17.4) with the components satisfying Assumption <span class="math inline">\(17.1\)</span> with <span class="math inline">\(\sigma_{u}^{2}=\rho \sigma^{2}\)</span> and <span class="math inline">\(\sigma_{\varepsilon}^{2}=(1-\rho) \sigma^{2}\)</span>. Thus random effects and equi-correlation are identical.</p>
<p>The random effects regression model is</p>
<p><span class="math display">\[
Y_{i t}=X_{i t}^{\prime} \beta+u_{i}+\varepsilon_{i t}
\]</span></p>
<p>or <span class="math inline">\(\boldsymbol{Y}_{i}=\boldsymbol{X}_{i} \beta+\mathbf{1}_{i} u_{i}+\boldsymbol{\varepsilon}_{i}\)</span> where the errors satisfy Assumption 17.1.</p>
<p>Given the error structure the natural estimator for <span class="math inline">\(\beta\)</span> is GLS. Suppose <span class="math inline">\(\sigma_{u}^{2}\)</span> and <span class="math inline">\(\sigma_{\varepsilon}^{2}\)</span> are known. The GLS estimator of <span class="math inline">\(\beta\)</span> is</p>
<p><span class="math display">\[
\widehat{\beta}_{\mathrm{gls}}=\left(\sum_{i=1}^{N} \boldsymbol{X}_{i}^{\prime} \Omega_{i}^{-1} \boldsymbol{X}_{i}\right)^{-1}\left(\sum_{i=1}^{N} \boldsymbol{X}_{i}^{\prime} \Omega_{i}^{-1} \boldsymbol{Y}_{i}\right) .
\]</span></p>
<p>A feasible GLS estimator replaces the unknown <span class="math inline">\(\sigma_{u}^{2}\)</span> and <span class="math inline">\(\sigma_{\varepsilon}^{2}\)</span> with estimators. See Section <span class="math inline">\(17.15\)</span>.</p>
<p>We now describe some statistical properties of the estimator under Assumption 17.1. By linearity</p>
<p><span class="math display">\[
\widehat{\beta}_{\mathrm{gls}}-\beta=\left(\sum_{i=1}^{N} \boldsymbol{X}_{i}^{\prime} \Omega_{i}^{-1} \boldsymbol{X}_{i}\right)^{-1}\left(\sum_{i=1}^{N} \boldsymbol{X}_{i}^{\prime} \Omega_{i}^{-1} \boldsymbol{e}_{i}\right) .
\]</span></p>
<p>Thus</p>
<p><span class="math display">\[
\mathbb{E}\left[\widehat{\beta}_{\mathrm{gls}}-\beta \mid \boldsymbol{X}\right]=\left(\sum_{i=1}^{N} \boldsymbol{X}_{i}^{\prime} \Omega_{i}^{-1} \boldsymbol{X}_{i}\right)^{-1}\left(\sum_{i=1}^{N} \boldsymbol{X}_{i}^{\prime} \Omega_{i}^{-1} \mathbb{E}\left[\boldsymbol{e}_{i} \mid \boldsymbol{X}_{i}\right]\right)=0 .
\]</span></p>
<p>Thus <span class="math inline">\(\widehat{\beta}_{\text {gls }}\)</span> is conditionally unbiased for <span class="math inline">\(\beta\)</span>. The conditional variance of <span class="math inline">\(\widehat{\beta}_{\text {gls }}\)</span> is</p>
<p><span class="math display">\[
\boldsymbol{V}_{\mathrm{gls}}=\left(\sum_{i=1}^{n} \boldsymbol{X}_{i}^{\prime} \Omega_{i}^{-1} \boldsymbol{X}_{i}\right)^{-1} \sigma_{\varepsilon}^{2}
\]</span></p>
<p>Now let’s compare <span class="math inline">\(\widehat{\beta}_{\text {gls }}\)</span> with the pooled estimator <span class="math inline">\(\widehat{\beta}_{\text {pool. }}\)</span>. Under Assumption <span class="math inline">\(17.1\)</span> the latter is also conditionally unbiased for <span class="math inline">\(\beta\)</span> and has conditional variance</p>
<p><span class="math display">\[
\boldsymbol{V}_{\text {pool }}=\left(\sum_{i=1}^{n} \boldsymbol{X}_{i}^{\prime} \boldsymbol{X}_{i}\right)^{-1}\left(\sum_{i=1}^{n} \boldsymbol{X}_{i}^{\prime} \Omega_{i} \boldsymbol{X}_{i}\right)^{-1}\left(\sum_{i=1}^{n} \boldsymbol{X}_{i}^{\prime} \boldsymbol{X}_{i}\right)^{-1} .
\]</span></p>
<p>Using the algebra of the Gauss-Markov Theorem we deduce that</p>
<p><span class="math display">\[
\boldsymbol{V}_{\text {gls }} \leq \boldsymbol{V}_{\text {pool }}
\]</span></p>
<p>and thus the random effects estimator <span class="math inline">\(\widehat{\beta}_{\text {gls }}\)</span> is more efficient than the pooled estimator <span class="math inline">\(\widehat{\beta}_{\text {pool }}\)</span> under Assumption 17.1. (See Exercise 17.1.) The two variance matrices are identical when there is no individualspecific effect (when <span class="math inline">\(\sigma_{u}^{2}=0\)</span> ) for then <span class="math inline">\(\boldsymbol{V}_{\text {gls }}=\boldsymbol{V}_{\text {pool }}=\left(\boldsymbol{X}^{\prime} \boldsymbol{X}\right)^{-1} \sigma_{\varepsilon}^{2}\)</span>.</p>
<p>Under the assumption that the random effects model is a useful approximation but not literally true then we may consider a cluster-robust covariance matrix estimator such as</p>
<p><span class="math display">\[
\widehat{\boldsymbol{V}}_{\mathrm{gls}}=\left(\sum_{i=1}^{N} \boldsymbol{X}_{i}^{\prime} \Omega_{i}^{-1} \boldsymbol{X}_{i}\right)^{-1}\left(\sum_{i=1}^{N} \boldsymbol{X}_{i}^{\prime} \Omega_{i}^{-1} \widehat{\boldsymbol{e}}_{i} \widehat{\boldsymbol{e}}_{i}^{\prime} \Omega_{i}^{-1} \boldsymbol{X}_{i}\right)\left(\sum_{i=1}^{n} \boldsymbol{X}_{i}^{\prime} \Omega_{i}^{-1} \boldsymbol{X}_{i}\right)^{-1}
\]</span></p>
<p>where <span class="math inline">\(\widehat{\boldsymbol{e}}_{i}=\boldsymbol{Y}_{i}-\boldsymbol{X}_{i} \widehat{\beta}_{\mathrm{gls}}\)</span>. This may be re-scaled by a degree of freedom adjustment if desired.</p>
<p>The random effects estimator <span class="math inline">\(\widehat{\beta}_{\text {gls }}\)</span> can be obtained using the Stata command xtreg. The default covariance matrix estimator is (17.11). For the cluster-robust covariance matrix estimator (17.14) use the command xtreg vce(robust). (The xtset command must be used first to declare the group identifier. For example, cusip is the group identifier in Table 17.1.)</p>
<p>To illustrate, in the second column of Table <span class="math inline">\(17.2\)</span> we present the random effect regression estimates of the investment model (17.3) with cluster-robust standard errors (17.14). The point estimates are reasonably different from the pooled regression estimator. The coefficient on debt switches from positive to negative (the latter consistent with theories of liquidity constraints) and the coefficient on cash flow increases significantly in magnitude. These changes appear to be greater in magnitude than would be expected if Assumption <span class="math inline">\(17.1\)</span> were correct. In the next section we consider a less restrictive specification.</p>
</section>
<section id="fixed-effect-model" class="level2" data-number="16.7">
<h2 data-number="16.7" class="anchored" data-anchor-id="fixed-effect-model"><span class="header-section-number">16.7</span> Fixed Effect Model</h2>
<p>Consider the one-way error component regression model</p>
<p><span class="math display">\[
Y_{i t}=X_{i t}^{\prime} \beta+u_{i}+\varepsilon_{i t}
\]</span></p>
<p>or</p>
<p><span class="math display">\[
\boldsymbol{Y}_{i}=\boldsymbol{X}_{i} \beta+\mathbf{l}_{i} u_{i}+\boldsymbol{\varepsilon}_{i} .
\]</span></p>
<p>In many applications it is useful to interpret the individual-specific effect <span class="math inline">\(u_{i}\)</span> as a time-invariant unobserved missing variable. For example, in a wage regression <span class="math inline">\(u_{i}\)</span> may be the unobserved ability of individual <span class="math inline">\(i\)</span>. In the investment model (17.3) <span class="math inline">\(u_{i}\)</span> may be a firm-specific productivity factor.</p>
<p>When <span class="math inline">\(u_{i}\)</span> is interpreted as an omitted variable it is natural to expect it to be correlated with the regressors <span class="math inline">\(X_{i t}\)</span>. This is especially the case when <span class="math inline">\(X_{i t}\)</span> includes choice variables.</p>
<p>To illustrate, consider the entries in Table 17.1. The final column displays the pooled regression residuals <span class="math inline">\(\widehat{e}_{i t}\)</span> for the first 13 observations which we interpret as estimates of the error <span class="math inline">\(e_{i t}=u_{i}+\varepsilon_{i t}\)</span>. As described before, what is particularly striking about the residuals is that they are all strongly negative for firm #209, clustering around <span class="math inline">\(-0.25\)</span>. We can interpret this as an estimate of <span class="math inline">\(u_{i}\)</span> for this firm. Examining the values of the regressor <span class="math inline">\(Q\)</span> for the two firms we can see that firm #209 has very large values (in all time periods) for <span class="math inline">\(Q\)</span>. (The average value <span class="math inline">\(\bar{Q}_{i}\)</span> for the two firms appears in the seventh column.) Thus it appears (though we are only looking at two observations) that <span class="math inline">\(u_{i}\)</span> and <span class="math inline">\(Q_{i t}\)</span> are correlated. It is not reasonable to infer too much from these limited observations, but the relevance is that such correlation violates strict mean independence.</p>
<p>In the econometrics literature if the stochastic structure of <span class="math inline">\(u_{i}\)</span> is treated as unknown and possibly correlated with <span class="math inline">\(X_{i t}\)</span> then <span class="math inline">\(u_{i}\)</span> is called a fixed effect.</p>
<p>Correlation between <span class="math inline">\(u_{i}\)</span> and <span class="math inline">\(X_{i t}\)</span> will cause both pooled and random effect estimators to be biased. This is due to the classic problems of omitted variables bias and endogeneity. To see this in a generated example view Figure 17.1. This shows a scatter plot of three observations <span class="math inline">\(\left(Y_{i t}, X_{i t}\right)\)</span> from three firms. The true model is <span class="math inline">\(Y_{i t}=9-X_{i t}+u_{i}\)</span>. (The true slope coefficient is <span class="math inline">\(-1\)</span>.) The variables <span class="math inline">\(u_{i}\)</span> and <span class="math inline">\(X_{i t}\)</span> are highly correlated so the fitted pooled regression line through the nine observations has a slope close to +1. (The random effects estimator is identical.) The apparent positive relationship between <span class="math inline">\(Y\)</span> and <span class="math inline">\(X\)</span> is driven entirely by the positive correlation between <span class="math inline">\(X\)</span> and <span class="math inline">\(u\)</span>. Conditional on <span class="math inline">\(u\)</span>, however, the slope is <span class="math inline">\(-1\)</span>. Thus regression techniques which do not control for <span class="math inline">\(u_{i}\)</span> will produce biased and inconsistent estimators.</p>
<p><img src="images//2022_10_23_acbfcce1ea7ce1901e2dg-08.jpg" class="img-fluid"></p>
<p>Figure 17.1: Scatter Plot and Pooled Regression Line</p>
<p>The presence of the unstructured individual effect <span class="math inline">\(u_{i}\)</span> means that it is not possible to identify <span class="math inline">\(\beta\)</span> under a simple projection assumption such as <span class="math inline">\(\mathbb{E}\left[X_{i t} \varepsilon_{t}\right]=0\)</span>. It turns out that a sufficient condition for identification is the following. Definition 17.1 The regressor <span class="math inline">\(X_{i t}\)</span> is strictly exogenous for the error <span class="math inline">\(\varepsilon_{i t}\)</span> if</p>
<p><span class="math display">\[
\mathbb{E}\left[X_{i s} \varepsilon_{i t}\right]=0
\]</span></p>
<p>for all <span class="math inline">\(s=1, \ldots, T\)</span>.</p>
<p>Strict exogeneity is a strong projection condition, meaning that if <span class="math inline">\(X_{i s}\)</span> for any <span class="math inline">\(s \neq t\)</span> is added to (17.15) it will have a zero coefficient. Strict exogeneity is a projection analog of strict mean independence</p>
<p><span class="math display">\[
\mathbb{E}\left[\varepsilon_{i t} \mid \boldsymbol{X}_{i}\right]=0 .
\]</span></p>
<p>(17.18) implies (17.17) but not conversely. While (17.17) is sufficient for identification and asymptotic theory we will also use the stronger condition (17.18) for finite sample analysis.</p>
<p>While (17.17) and (17.18) are strong assumptions they are much weaker than (17.2) or Assumption 17.1, which require that the individual effect <span class="math inline">\(u_{i}\)</span> is also strictly mean independent. In contrast, (17.17) and (17.18) make no assumptions about <span class="math inline">\(u_{i}\)</span>.</p>
<p>Strict exogeneity (17.17) is typically inappropriate in dynamic models. In Section <span class="math inline">\(17.41\)</span> we discuss estimation under the weaker assumption of predetermined regressors.</p>
</section>
<section id="within-transformation" class="level2" data-number="16.8">
<h2 data-number="16.8" class="anchored" data-anchor-id="within-transformation"><span class="header-section-number">16.8</span> Within Transformation</h2>
<p>In the previous section we showed that if <span class="math inline">\(u_{i}\)</span> and <span class="math inline">\(X_{i t}\)</span> are correlated then pooled and random-effects estimators will be biased and inconsistent. If we leave the relationship between <span class="math inline">\(u_{i}\)</span> and <span class="math inline">\(X_{i t}\)</span> fully unstructured then the only way to consistently estimate the coefficient <span class="math inline">\(\beta\)</span> is by an estimator which is invariant to <span class="math inline">\(u_{i}\)</span>. This can be achieved by transformations which eliminate <span class="math inline">\(u_{i}\)</span>.</p>
<p>One such transformation is the within transformation. In this section we describe this transformation in detail.</p>
<p>Define the mean of a variable for a given individual as</p>
<p><span class="math display">\[
\bar{Y}_{i}=\frac{1}{T_{i}} \sum_{t \in S_{i}} Y_{i t} .
\]</span></p>
<p>We call this the individual-specific mean since it is the mean of a given individual. Contrarywise, some authors call this the time-average or time-mean since it is the average over the time periods.</p>
<p>Subtracting the individual-specific mean from the variable we obtain the deviations</p>
<p><span class="math display">\[
\dot{Y}_{i t}=Y_{i t}-\bar{Y}_{i} .
\]</span></p>
<p>This is known as the within transformation. We also refer to <span class="math inline">\(\dot{Y}_{i t}\)</span> as the demeaned values or deviations from individual means. Some authors refer to <span class="math inline">\(\dot{Y}_{i t}\)</span> as deviations from time means. What is important is that the demeaning has occured at the individual level.</p>
<p>Some algebra may also be useful. We can write the individual-specific mean as <span class="math inline">\(\bar{Y}_{i}=\left(\mathbf{1}_{i}^{\prime} \mathbf{1}_{i}\right)^{-1} \mathbf{1}_{i}^{\prime} \boldsymbol{Y}_{i}\)</span>. Stacking the observations for individual <span class="math inline">\(i\)</span> we can write the within transformation using the notation</p>
<p><span class="math display">\[
\begin{aligned}
\dot{\boldsymbol{Y}}_{i} &amp;=\boldsymbol{Y}_{i}-\mathbf{1}_{i} \bar{Y}_{i} \\
&amp;=\boldsymbol{Y}_{i}-\mathbf{1}_{i}\left(\mathbf{1}_{i}^{\prime} \mathbf{1}_{i}\right)^{-1} \mathbf{1}_{i}^{\prime} \boldsymbol{Y}_{i} \\
&amp;=\boldsymbol{M}_{i} \boldsymbol{Y}_{i}
\end{aligned}
\]</span></p>
<p>where <span class="math inline">\(\boldsymbol{M}_{i}=\boldsymbol{I}_{i}-\mathbf{1}_{i}\left(\mathbf{1}_{i}^{\prime} \mathbf{1}_{i}\right)^{-1} \mathbf{1}_{i}^{\prime}\)</span> is the individual-specific demeaning operator. Notice that <span class="math inline">\(\boldsymbol{M}_{i}\)</span> is an idempotent matrix.</p>
<p>Similarly for the regressors we define the individual-specific means and demeaned values:</p>
<p><span class="math display">\[
\begin{aligned}
\bar{X}_{i} &amp;=\frac{1}{T_{i}} \sum_{t \in S_{i}} X_{i t} \\
\dot{X}_{i t} &amp;=X_{i t}-\bar{X}_{i} \\
\dot{\boldsymbol{X}}_{i} &amp;=\boldsymbol{M}_{i} \boldsymbol{X}_{i} .
\end{aligned}
\]</span></p>
<p>We illustrate demeaning in Table 17.1. In the fourth and seventh columns we display the firm-specific means <span class="math inline">\(\bar{I}_{i}\)</span> and <span class="math inline">\(\bar{Q}_{i}\)</span> and in the fifth and eighth columns the demeaned values <span class="math inline">\(\dot{I}_{i t}\)</span> and <span class="math inline">\(\dot{Q}_{i t}\)</span>.</p>
<p>We can also define the full-sample within operator. Define <span class="math inline">\(\boldsymbol{D}=\operatorname{diag}\left\{\mathbf{1}_{T_{1}}, \ldots, \mathbf{1}_{T_{N}}\right\}\)</span> and <span class="math inline">\(\boldsymbol{M}_{\boldsymbol{D}}=\boldsymbol{I}_{n}-\)</span> <span class="math inline">\(\boldsymbol{D}\left(\boldsymbol{D}^{\prime} \boldsymbol{D}\right)^{-1} \boldsymbol{D}^{\prime}\)</span>. Note that <span class="math inline">\(\boldsymbol{M}_{\boldsymbol{D}}=\operatorname{diag}\left\{\boldsymbol{M}_{1}, \ldots, \boldsymbol{M}_{N}\right\}\)</span>. Thus</p>
<p><span class="math display">\[
\boldsymbol{M}_{\boldsymbol{D}} \boldsymbol{Y}=\dot{\boldsymbol{Y}}=\left(\begin{array}{c}
\dot{\boldsymbol{Y}}_{1} \\
\vdots \\
\dot{\boldsymbol{Y}}_{N}
\end{array}\right), \quad \boldsymbol{M}_{\boldsymbol{D}} \boldsymbol{X}=\dot{\boldsymbol{X}}=\left(\begin{array}{c}
\dot{\boldsymbol{X}}_{1} \\
\vdots \\
\dot{\boldsymbol{X}}_{N}
\end{array}\right)
\]</span></p>
<p>Now apply these operations to equation (17.15). Taking individual-specific averages we obtain</p>
<p><span class="math display">\[
\bar{Y}_{i}=\bar{X}_{i}^{\prime} \beta+u_{i}+\bar{\varepsilon}_{i}
\]</span></p>
<p>where <span class="math inline">\(\bar{\varepsilon}_{i}=\frac{1}{T_{i}} \sum_{t \in S_{i}} \varepsilon_{i t}\)</span>. Subtracting from (17.15) we obtain</p>
<p><span class="math display">\[
\dot{Y}_{i t}=\dot{X}_{i t}^{\prime} \beta+\dot{\varepsilon}_{i t}
\]</span></p>
<p>where <span class="math inline">\(\dot{\varepsilon}_{i t}=\varepsilon_{i t}-\bar{\varepsilon}_{i t}\)</span>. The individual effect <span class="math inline">\(u_{i}\)</span> has been eliminated! obtain</p>
<p>We can alternatively write this in vector notation. Applying the demeaning operator <span class="math inline">\(\boldsymbol{M}_{i}\)</span> to (17.16) we</p>
<p><span class="math display">\[
\dot{\boldsymbol{Y}}_{i}=\dot{\boldsymbol{X}}_{i} \beta+\dot{\boldsymbol{\varepsilon}}_{i} .
\]</span></p>
<p>The individual-effect <span class="math inline">\(u_{i}\)</span> is eliminated because <span class="math inline">\(\boldsymbol{M}_{i} \mathbf{1}_{i}=0\)</span>. Equation (17.22) is a vector version of (17.21).</p>
<p>The equation (17.21) is a linear equation in the transformed (demeaned) variables. As desired the individual effect <span class="math inline">\(u_{i}\)</span> has been eliminated. Consequently estimators constructed from (17.21) (or equivalently (17.22)) will be invariant to the values of <span class="math inline">\(u_{i}\)</span>. This means that the the endogeneity bias described in the previous section will be eliminated.</p>
<p>Another consequence, however, is that all time-invariant regressors are also eliminated. That is, if the original model (17.15) had included any regressors <span class="math inline">\(X_{i t}=X_{i}\)</span> which are constant over time for each individual then for these regressors the demeaned values are identically 0 . What this means is that if equation (17.21) is used to estimate <span class="math inline">\(\beta\)</span> it will be impossible to estimate (or identify) a coefficient on any regressor which is time invariant. This is not a consequence of the estimation method but rather a consequence of the model assumptions. In other words, if the individual effect <span class="math inline">\(u_{i}\)</span> has no known structure then it is impossible to disentangle the effect of any time-invariant regressor <span class="math inline">\(X_{i}\)</span>. The two have observationally equivalent effects and cannot be separately identified.</p>
<p>The within transformation can greatly reduce the variance of the regressors. This can be seen in Table 17.1 where you can see that the variation between the elements of the transformed variables <span class="math inline">\(\dot{I}_{i t}\)</span> and <span class="math inline">\(\dot{Q}_{i t}\)</span> is less than that of the untransformed variables, as much of the variation is captured by the firm-specific means.</p>
<p>It is not typically needed to directly program the within transformation, but if it is desired the following Stata commands easily do so.</p>
<table class="table">
<colgroup>
<col style="width: 100%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Stata Commands for Within Transformation</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><span class="math inline">\(* \quad \quad \mathrm{x}\)</span> is the original variable</td>
</tr>
<tr class="even">
<td style="text-align: left;"><span class="math inline">\(* \quad\)</span> id is the group identifier</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><span class="math inline">\(* \quad\)</span> xdot is the within-transformed variable</td>
</tr>
<tr class="even">
<td style="text-align: left;">egen xmean <span class="math inline">\(=\)</span> mean <span class="math inline">\((\mathrm{x})\)</span>, by(id) gen xdot <span class="math inline">\(=\mathrm{x}-\mathrm{xmean}\)</span></td>
</tr>
</tbody>
</table>
</section>
<section id="fixed-effects-estimator" class="level2" data-number="16.9">
<h2 data-number="16.9" class="anchored" data-anchor-id="fixed-effects-estimator"><span class="header-section-number">16.9</span> Fixed Effects Estimator</h2>
<p>Consider least squares applied to the demeaned equation (17.21) or equivalently (17.22). This is</p>
<p><span class="math display">\[
\begin{aligned}
\widehat{\beta}_{\mathrm{fe}} &amp;=\left(\sum_{i=1}^{N} \sum_{t \in S_{i}} \dot{X}_{i t} \dot{X}_{i t}^{\prime}\right)^{-1}\left(\sum_{i=1}^{N} \sum_{t \in S_{i}} \dot{X}_{i t} \dot{Y}_{i t}\right) \\
&amp;=\left(\sum_{i=1}^{N} \dot{\boldsymbol{X}}_{i}^{\prime} \dot{\boldsymbol{X}}_{i}\right)^{-1}\left(\sum_{i=1}^{N} \dot{\boldsymbol{X}}_{i}^{\prime} \dot{\boldsymbol{Y}}_{i}\right) \\
&amp;=\left(\sum_{i=1}^{N} \boldsymbol{X}_{i}^{\prime} \boldsymbol{M}_{i} \boldsymbol{X}_{i}\right)^{-1}\left(\sum_{i=1}^{N} \boldsymbol{X}_{i}^{\prime} \boldsymbol{M}_{i} \boldsymbol{Y}_{i}\right)
\end{aligned}
\]</span></p>
<p>This is known as the fixed-effects or within estimator of <span class="math inline">\(\beta\)</span>. It is called the fixed-effects estimator because it is appropriate for the fixed effects model (17.15). It is called the within estimator because it is based on the variation of the data within each individual.</p>
<p>The above definition implicitly assumes that the matrix <span class="math inline">\(\sum_{i=1}^{N} \dot{\boldsymbol{X}}_{i}^{\prime} \dot{\boldsymbol{X}}_{i}\)</span> is full rank. This requires that all components of <span class="math inline">\(X_{i t}\)</span> have time variation for at least some individuals in the sample.</p>
<p>The fixed effects residuals are</p>
<p><span class="math display">\[
\begin{aligned}
\widehat{\varepsilon}_{i t} &amp;=\dot{Y}_{i t}-\dot{X}_{i t}^{\prime} \widehat{\beta}_{\mathrm{fe}} \\
\widehat{\boldsymbol{\varepsilon}}_{i} &amp;=\dot{\boldsymbol{Y}}_{i}-\dot{\boldsymbol{X}}_{i} \widehat{\beta}_{\mathrm{fe}}
\end{aligned}
\]</span></p>
<p>Let us describe some of the statistical properties of the estimator under strict mean independence (17.18). By linearity and the fact <span class="math inline">\(\boldsymbol{M}_{i} \mathbf{1}_{i}=0\)</span>, we can write</p>
<p><span class="math display">\[
\widehat{\beta}_{\mathrm{fe}}-\beta=\left(\sum_{i=1}^{N} \boldsymbol{X}_{i}^{\prime} \boldsymbol{M}_{i} \boldsymbol{X}_{i}\right)^{-1}\left(\sum_{i=1}^{N} \boldsymbol{X}_{i}^{\prime} \boldsymbol{M}_{i} \boldsymbol{\varepsilon}_{i}\right)
\]</span></p>
<p>Then (17.18) implies</p>
<p><span class="math display">\[
\mathbb{E}\left[\widehat{\beta}_{\mathrm{fe}}-\beta \mid \boldsymbol{X}\right]=\left(\sum_{i=1}^{N} \boldsymbol{X}_{i}^{\prime} \boldsymbol{M}_{i} \boldsymbol{X}_{i}\right)^{-1}\left(\sum_{i=1}^{N} \boldsymbol{X}_{i}^{\prime} \boldsymbol{M}_{i} \mathbb{E}\left[\boldsymbol{\varepsilon}_{i} \mid \boldsymbol{X}_{i}\right]\right)=0
\]</span></p>
<p>Thus <span class="math inline">\(\widehat{\beta}_{\mathrm{fe}}\)</span> is unbiased for <span class="math inline">\(\beta\)</span> under (17.18).</p>
<p>Let <span class="math inline">\(\Sigma_{i}=\mathbb{E}\left[\boldsymbol{\varepsilon}_{i} \boldsymbol{\varepsilon}_{i}^{\prime} \mid \boldsymbol{X}_{i}\right]\)</span> denote the <span class="math inline">\(T_{i} \times T_{i}\)</span> conditional covariance matrix of the idiosyncratic errors. The variance of <span class="math inline">\(\widehat{\beta}_{\mathrm{fe}}\)</span> is</p>
<p><span class="math display">\[
\boldsymbol{V}_{\mathrm{fe}}=\operatorname{var}\left[\widehat{\beta}_{\mathrm{fe}} \mid \boldsymbol{X}\right]=\left(\sum_{i=1}^{N} \dot{\boldsymbol{X}}_{i}^{\prime} \dot{\boldsymbol{X}}_{i}\right)^{-1}\left(\sum_{i=1}^{N} \dot{\boldsymbol{X}}_{i}^{\prime} \Sigma_{i} \dot{\boldsymbol{X}}_{i}\right)\left(\sum_{i=1}^{N} \dot{\boldsymbol{X}}_{i}^{\prime} \dot{\boldsymbol{X}}_{i}\right)^{-1}
\]</span></p>
<p>This expression simplifies when the idiosyncratic errors are homoskedastic and serially uncorrelated:</p>
<p><span class="math display">\[
\begin{aligned}
\mathbb{E}\left[\varepsilon_{i t}^{2} \mid \boldsymbol{X}_{i}\right] &amp;=\sigma_{\varepsilon}^{2} \\
\mathbb{E}\left[\varepsilon_{i j} \varepsilon_{i t} \mid \boldsymbol{X}_{i}\right] &amp;=0
\end{aligned}
\]</span></p>
<p>for all <span class="math inline">\(j \neq t\)</span>. In this case, <span class="math inline">\(\Sigma_{i}=\boldsymbol{I}_{i} \sigma_{\varepsilon}^{2}\)</span> and (17.24) simplifies to</p>
<p><span class="math display">\[
\boldsymbol{V}_{\mathrm{fe}}^{0}=\sigma_{\varepsilon}^{2}\left(\sum_{i=1}^{N} \dot{\boldsymbol{X}}_{i}^{\prime} \dot{\boldsymbol{X}}_{i}\right)^{-1} .
\]</span></p>
<p>It is instructive to compare the variances of the fixed-effects and pooled estimators under (17.25)(17.26) and the assumption that there is no individual-specific effect <span class="math inline">\(u_{i}=0\)</span>. In this case we see that</p>
<p><span class="math display">\[
\boldsymbol{V}_{\mathrm{fe}}^{0}=\sigma_{\varepsilon}^{2}\left(\sum_{i=1}^{N} \dot{\boldsymbol{X}}_{i}^{\prime} \dot{\boldsymbol{X}}_{i}\right)^{-1} \geq \sigma_{\varepsilon}^{2}\left(\sum_{i=1}^{N} \boldsymbol{X}_{i}^{\prime} \boldsymbol{X}_{i}\right)^{-1}=\boldsymbol{V}_{\text {pool }}
\]</span></p>
<p>The inequality holds since the demeaned variables <span class="math inline">\(\dot{\boldsymbol{X}}_{i}\)</span> have reduced variation relative to the original observations <span class="math inline">\(\boldsymbol{X}_{i}\)</span>. (See Exercise 17.28.) This shows the cost of using fixed effects relative to pooled estimation. The estimation variance increases due to reduced variation in the regressors. This reduction in efficiency is a necessary by-product of the robustness of the estimator to the individual effects <span class="math inline">\(u_{i}\)</span>.</p>
</section>
<section id="differenced-estimator" class="level2" data-number="16.10">
<h2 data-number="16.10" class="anchored" data-anchor-id="differenced-estimator"><span class="header-section-number">16.10</span> Differenced Estimator</h2>
<p>The within transformation is not the only transformation which eliminates the individual-specific effect. Another important transformation which does the same is first-differencing.</p>
<p>The first-differencing transformation is <span class="math inline">\(\Delta Y_{i t}=Y_{i t}-Y_{i t-1}\)</span>. This can be applied to all but the first observation (which is essentially lost). At the level of the individual this can be written as <span class="math inline">\(\Delta \boldsymbol{Y}_{i}=\boldsymbol{D}_{i} \boldsymbol{Y}_{i}\)</span> where <span class="math inline">\(\boldsymbol{D}_{i}\)</span> is the <span class="math inline">\(\left(T_{i}-1\right) \times T_{i}\)</span> matrix differencing operator</p>
<p><span class="math display">\[
\boldsymbol{D}_{i}=\left[\begin{array}{cccccc}
-1 &amp; 1 &amp; 0 &amp; \cdots &amp; 0 &amp; 0 \\
0 &amp; -1 &amp; 1 &amp; &amp; 0 &amp; 0 \\
\vdots &amp; &amp; &amp; \ddots &amp; &amp; \vdots \\
0 &amp; 0 &amp; 0 &amp; \cdots &amp; -1 &amp; 1
\end{array}\right] .
\]</span></p>
<p>Applying the transformation <span class="math inline">\(\Delta\)</span> to (17.15) or (17.16) we obtain <span class="math inline">\(\Delta Y_{i t}=\Delta X_{i t}^{\prime} \beta+\Delta \varepsilon_{i t}\)</span> or</p>
<p><span class="math display">\[
\Delta \boldsymbol{Y}_{i}=\Delta \boldsymbol{X}_{i} \beta+\Delta \boldsymbol{\varepsilon}_{i} .
\]</span></p>
<p>We can see that the individual effect <span class="math inline">\(u_{i}\)</span> has been eliminated.</p>
<p>Least squares applied to the differenced equation (17.29) is</p>
<p><span class="math display">\[
\begin{aligned}
\widehat{\beta}_{\Delta} &amp;=\left(\sum_{i=1}^{N} \sum_{t \geq 2} \Delta X_{i t} \Delta X_{i t}^{\prime}\right)^{-1}\left(\sum_{i=1}^{N} \sum_{t \geq 2} \Delta X_{i t} \Delta Y_{i t}\right) \\
&amp;=\left(\sum_{i=1}^{N} \Delta \boldsymbol{X}_{i}^{\prime} \Delta \boldsymbol{X}_{i}\right)^{-1}\left(\sum_{i=1}^{N} \Delta \boldsymbol{X}_{i}^{\prime} \Delta \boldsymbol{Y}_{i}\right) \\
&amp;=\left(\sum_{i=1}^{N} \boldsymbol{X}_{i}^{\prime} \boldsymbol{D}_{i}^{\prime} \boldsymbol{D}_{i} \boldsymbol{X}_{i}\right)^{-1}\left(\sum_{i=1}^{N} \boldsymbol{X}_{i}^{\prime} \boldsymbol{D}_{i}^{\prime} \boldsymbol{D}_{i} \boldsymbol{Y}_{i}\right)
\end{aligned}
\]</span></p>
<p>(17.30) is called the differenced estimator. For <span class="math inline">\(T=2, \widehat{\beta}_{\Delta}=\widehat{\beta}_{\mathrm{fe}}\)</span> equals the fixed effects estimator. See Exercise 17.6. They differ, however, for <span class="math inline">\(T&gt;2\)</span>.</p>
<p>When the errors <span class="math inline">\(\varepsilon_{i t}\)</span> are serially uncorrelated and homoskedastic then the error <span class="math inline">\(\Delta \boldsymbol{\varepsilon}_{i}=\boldsymbol{D}_{i} \boldsymbol{\varepsilon}_{i}\)</span> in (17.29) has covariance matrix <span class="math inline">\(\boldsymbol{H} \sigma_{\varepsilon}^{2}\)</span> where</p>
<p><span class="math display">\[
\boldsymbol{H}=\boldsymbol{D}_{i} \boldsymbol{D}_{i}^{\prime}=\left(\begin{array}{cccc}
2 &amp; -1 &amp; 0 &amp; 0 \\
-1 &amp; 2 &amp; \ddots &amp; 0 \\
0 &amp; \ddots &amp; \ddots &amp; -1 \\
0 &amp; 0 &amp; -1 &amp; 2
\end{array}\right) .
\]</span></p>
<p>We can reduce estimation variance by using GLS. When the errors <span class="math inline">\(\varepsilon_{i t}\)</span> are i.i.d. (serially uncorrelated and homoskedastic), this is</p>
<p><span class="math display">\[
\begin{aligned}
\widetilde{\beta}_{\Delta} &amp;=\left(\sum_{i=1}^{N} \Delta \boldsymbol{X}_{i}^{\prime} \boldsymbol{H}^{-1} \Delta \boldsymbol{X}_{i}\right)^{-1}\left(\sum_{i=1}^{N} \Delta \boldsymbol{X}_{i}^{\prime} \boldsymbol{H}^{-1} \Delta \boldsymbol{Y}_{i}\right) \\
&amp;=\left(\sum_{i=1}^{N} \boldsymbol{X}_{i}^{\prime} \boldsymbol{D}_{i}^{\prime}\left(\boldsymbol{D}_{i} \boldsymbol{D}_{i}^{\prime}\right)^{-1} \boldsymbol{D}_{i} \boldsymbol{X}_{i}\right)^{-1}\left(\sum_{i=1}^{N} \boldsymbol{X}_{i}^{\prime} \boldsymbol{D}_{i}^{\prime}\left(\boldsymbol{D}_{i} \boldsymbol{D}_{i}^{\prime}\right)^{-1} \boldsymbol{D}_{i} \boldsymbol{Y}_{i}\right) \\
&amp;=\left(\sum_{i=1}^{N} \boldsymbol{X}_{i}^{\prime} \boldsymbol{M}_{i} \boldsymbol{X}_{i}\right)^{-1}\left(\sum_{i=1}^{N} \boldsymbol{X}_{i}^{\prime} \boldsymbol{M}_{i} \boldsymbol{Y}_{i}\right)
\end{aligned}
\]</span></p>
<p>where <span class="math inline">\(\boldsymbol{M}_{i}=\boldsymbol{D}_{i}^{\prime}\left(\boldsymbol{D}_{i} \boldsymbol{D}_{i}^{\prime}\right)^{-1} \boldsymbol{D}_{i}\)</span>. Recall, the matrix <span class="math inline">\(\boldsymbol{D}_{i}\)</span> is <span class="math inline">\(\left(T_{i}-1\right) \times T_{i}\)</span> with rank <span class="math inline">\(T_{i}-1\)</span> and is orthogonal to the vector of ones <span class="math inline">\(\mathbf{1}_{i}\)</span>. This means <span class="math inline">\(\boldsymbol{M}_{i}\)</span> projects orthogonally to <span class="math inline">\(\mathbf{1}_{i}\)</span> and thus equals the within transformation matrix. Hence <span class="math inline">\(\widetilde{\beta}_{\Delta}=\widehat{\beta}_{\mathrm{fe}}\)</span>, the fixed effects estimator!</p>
<p>What we have shown is that under i.i.d. errors, GLS applied to the first-differenced equation precisely equals the fixed effects estimator. Since the Gauss-Markov theorem shows that GLS has lower variance than least squares, this means that the fixed effects estimator is more efficient than first differencing under the assumption that <span class="math inline">\(\varepsilon_{i t}\)</span> is i.i.d.</p>
<p>This argument extends to any other transformation which eliminates the fixed effect. GLS applied after such a transformation is equal to the fixed effects estimator and is more efficient than least squares applied after the same transformation under i.i.d. errors. This shows that the fixed effects estimator is Gauss-Markov efficient in the class of estimators which eliminate the fixed effect, under these assumptions.</p>
</section>
<section id="dummy-variables-regression" class="level2" data-number="16.11">
<h2 data-number="16.11" class="anchored" data-anchor-id="dummy-variables-regression"><span class="header-section-number">16.11</span> Dummy Variables Regression</h2>
<p>An alternative way to estimate the fixed effects model is by least squares of <span class="math inline">\(Y_{i t}\)</span> on <span class="math inline">\(X_{i t}\)</span> and a full set of dummy variables, one for each individual in the sample. It turns out that this is algebraically equivalent to the within estimator.</p>
<p>To see this start with the error-component model without a regressor:</p>
<p><span class="math display">\[
Y_{i t}=u_{i}+\varepsilon_{i t} .
\]</span></p>
<p>Consider least squares estimation of the vector of fixed effects <span class="math inline">\(u=\left(u_{1}, \ldots, u_{N}\right)^{\prime}\)</span>. Since each fixed effect <span class="math inline">\(u_{i}\)</span> is an individual-specific mean and the least squares estimate of the intercept is the sample mean it follows that the least squares estimate of <span class="math inline">\(u_{i}\)</span> is <span class="math inline">\(\widehat{u}_{i}=\bar{Y}_{i}\)</span>. The least squares residual is then <span class="math inline">\(\widehat{\varepsilon}_{i t}=Y_{i t}-\bar{Y}_{i}=\)</span> <span class="math inline">\(\dot{Y}_{i t}\)</span>, the within transformation. If you would prefer an algebraic argument, let <span class="math inline">\(d_{i}\)</span> be a vector of <span class="math inline">\(N\)</span> dummy variables where the <span class="math inline">\(i^{t h}\)</span> element indicates the <span class="math inline">\(i^{t h}\)</span> individual. Thus the <span class="math inline">\(i^{t h}\)</span> element of <span class="math inline">\(d_{i}\)</span> is 1 and the remaining elements are zero. Notice that <span class="math inline">\(u_{i}=d_{i}^{\prime} u\)</span> and (17.32) equals <span class="math inline">\(Y_{i t}=d_{i}^{\prime} u+\varepsilon_{i t}\)</span>. This is a regression with the regressors <span class="math inline">\(d_{i}\)</span> and coefficients <span class="math inline">\(u\)</span>. We can also write this in vector notation at the level of the individual as <span class="math inline">\(\boldsymbol{Y}_{i}=\mathbf{1}_{i} d_{i}^{\prime} u+\varepsilon_{i}\)</span> or using full matrix notation as <span class="math inline">\(\boldsymbol{Y}=\boldsymbol{D} u+\boldsymbol{\varepsilon}\)</span> where <span class="math inline">\(\boldsymbol{D}=\operatorname{diag}\left\{\mathbf{1}_{T_{1}}, \ldots, \mathbf{1}_{T_{N}}\right\}\)</span>.</p>
<p>The least squares estimate of <span class="math inline">\(u\)</span> is</p>
<p><span class="math display">\[
\begin{aligned}
\widehat{\boldsymbol{u}} &amp;=\left(\boldsymbol{D}^{\prime} \boldsymbol{D}\right)^{-1}\left(\boldsymbol{D}^{\prime} \boldsymbol{Y}\right) \\
&amp;=\operatorname{diag}\left(\mathbf{1}_{i}^{\prime} \mathbf{1}_{i}\right)^{-1}\left\{\mathbf{1}_{i}^{\prime} \boldsymbol{Y}_{i}\right\}_{i=1, \ldots, n} \\
&amp;=\left\{\left(\mathbf{1}_{i}^{\prime} \mathbf{1}_{i}\right)^{-1} \mathbf{1}_{i}^{\prime} \boldsymbol{Y}_{i}\right\}_{i=1, \ldots, n} \\
&amp;=\left\{\bar{Y}_{i}\right\}_{i=1, \ldots, n} .
\end{aligned}
\]</span></p>
<p>The least squares residuals are</p>
<p><span class="math display">\[
\widehat{\boldsymbol{\varepsilon}}=\left(\boldsymbol{I}_{n}-\boldsymbol{D}\left(\boldsymbol{D}^{\prime} \boldsymbol{D}\right)^{-1} \boldsymbol{D}^{\prime}\right) \boldsymbol{Y}=\dot{\boldsymbol{Y}}
\]</span></p>
<p>as shown in (17.19). Thus the least squares residuals from the simple error-component model are the within transformed variables.</p>
<p>Now consider the error-component model with regressors, which can be written as</p>
<p><span class="math display">\[
Y_{i t}=X_{i t}^{\prime} \beta+d_{i}^{\prime} u+\varepsilon_{i t}
\]</span></p>
<p>since <span class="math inline">\(u_{i}=d_{i}^{\prime} u\)</span> as discussed above. In matrix notation</p>
<p><span class="math display">\[
\boldsymbol{Y}=\boldsymbol{X} \beta+\boldsymbol{D} u+\boldsymbol{\varepsilon} .
\]</span></p>
<p>We consider estimation of <span class="math inline">\((\beta, u)\)</span> by least squares and write the estimates as <span class="math inline">\(\boldsymbol{Y}=\boldsymbol{X} \widehat{\beta}+\boldsymbol{D} \widehat{u}+\widehat{\boldsymbol{\varepsilon}}\)</span>. We call this the dummy variable estimator of the fixed effects model.</p>
<p>By the Frisch-Waugh-Lovell Theorem (Theorem 3.5) the dummy variable estimator <span class="math inline">\(\widehat{\beta}\)</span> and residuals <span class="math inline">\(\widehat{\boldsymbol{\varepsilon}}\)</span> may be obtained by the least squares regression of the residuals from the regression of <span class="math inline">\(\boldsymbol{Y}\)</span> on <span class="math inline">\(\boldsymbol{D}\)</span> on the residuals from the regression of <span class="math inline">\(\boldsymbol{X}\)</span> on <span class="math inline">\(\boldsymbol{D}\)</span>. We learned above that the residuals from the regression on <span class="math inline">\(\boldsymbol{D}\)</span> are the within transformations. Thus the dummy variable estimator <span class="math inline">\(\widehat{\beta}\)</span> and residuals <span class="math inline">\(\widehat{\boldsymbol{\varepsilon}}\)</span> may be obtained from least squares regression of the within transformed <span class="math inline">\(\dot{Y}\)</span> on the within transformed <span class="math inline">\(\dot{X}\)</span>. This is exactly the fixed effects estimator <span class="math inline">\(\widehat{\beta}_{\mathrm{fe}}\)</span>. Thus the dummy variable and fixed effects estimators of <span class="math inline">\(\beta\)</span> are identical.</p>
<p>This is sufficiently important that we state this result as a theorem.</p>
<p>Theorem 17.1 The fixed effects estimator of <span class="math inline">\(\beta\)</span> algebraically equals the dummy variable estimator of <span class="math inline">\(\beta\)</span>. The two estimators have the same residuals.</p>
<p>This may be the most important practical application of the Frisch-Waugh-Lovell Theorem. It shows that we can estimate the coefficients either by applying the within transformation or by inclusion of dummy variables (one for each individual in the sample). This is important because in some cases one approach is more convenient than the other and it is important to know that the two methods are algebraically equivalent.</p>
<p>When <span class="math inline">\(N\)</span> is large it is advisable to use the within transformation rather than the dummy variable approach. This is because the latter requires considerably more computer memory. To see this consider the matrix <span class="math inline">\(\boldsymbol{D}\)</span> in (17.34) in the balanced case. It has <span class="math inline">\(T N^{2}\)</span> elements which must be created and stored in memory. When <span class="math inline">\(N\)</span> is large this can be excessive. For example, if <span class="math inline">\(T=10\)</span> and <span class="math inline">\(N=10,000\)</span>, the matrix <span class="math inline">\(\boldsymbol{D}\)</span> has one billion elements! Whether or not a package can technically handle a matrix of this dimension depends on several particulars (system RAM, operating system, package version), but even if it can execute the calculation the computation time is slow. Hence for fixed effects estimation with large <span class="math inline">\(N\)</span> it is recommended to use the within transformation rather than dummy variable regression.</p>
<p>The dummy variable formulation may add insight about how the fixed effects estimator achieves invariance to the fixed effects. Given the regression equation (17.34) we can write the least squares estimator of <span class="math inline">\(\beta\)</span> using the residual regression formula:</p>
<p><span class="math display">\[
\begin{aligned}
\widehat{\beta}_{\mathrm{fe}} &amp;=\left(\boldsymbol{X}^{\prime} \boldsymbol{M}_{\boldsymbol{D}} \boldsymbol{X}\right)^{-1}\left(\boldsymbol{X}^{\prime} \boldsymbol{M}_{\boldsymbol{D}} \boldsymbol{Y}\right) \\
&amp;=\left(\boldsymbol{X}^{\prime} \boldsymbol{M}_{\boldsymbol{D}} \boldsymbol{X}\right)^{-1}\left(\boldsymbol{X}^{\prime} \boldsymbol{M}_{\boldsymbol{D}}(\boldsymbol{X} \beta+\boldsymbol{D} u+\boldsymbol{\varepsilon})\right) \\
&amp;=\beta+\left(\boldsymbol{X}^{\prime} \boldsymbol{M}_{\boldsymbol{D}} \boldsymbol{X}\right)^{-1}\left(\boldsymbol{X}^{\prime} \boldsymbol{M}_{\boldsymbol{D}} \boldsymbol{\varepsilon}\right)
\end{aligned}
\]</span></p>
<p>since <span class="math inline">\(\boldsymbol{M}_{\boldsymbol{D}} \boldsymbol{D}=0\)</span>. The expression (17.35) is free of the vector <span class="math inline">\(u\)</span> and thus <span class="math inline">\(\widehat{\beta}_{\mathrm{fe}}\)</span> is invariant to <span class="math inline">\(u\)</span>. This is another demonstration that the fixed effects estimator is invariant to the actual values of the fixed effects, and thus its statistical properties do not rely on assumptions about <span class="math inline">\(u_{i}\)</span>.</p>
</section>
<section id="fixed-effects-covariance-matrix-estimation" class="level2" data-number="16.12">
<h2 data-number="16.12" class="anchored" data-anchor-id="fixed-effects-covariance-matrix-estimation"><span class="header-section-number">16.12</span> Fixed Effects Covariance Matrix Estimation</h2>
<p>First consider estimation of the classical covariance matrix <span class="math inline">\(\boldsymbol{V}_{\mathrm{fe}}^{0}\)</span> as defined in (17.27). This is</p>
<p><span class="math display">\[
\widehat{\boldsymbol{V}}_{\mathrm{fe}}^{0}=\widehat{\sigma}_{\varepsilon}^{2}\left(\dot{\boldsymbol{X}}^{\prime} \dot{\boldsymbol{X}}\right)^{-1}
\]</span></p>
<p>with</p>
<p><span class="math display">\[
\widehat{\sigma}_{\varepsilon}^{2}=\frac{1}{n-N-k} \sum_{i=1}^{n} \sum_{t \in S_{i}} \widehat{\varepsilon}_{i t}^{2}=\frac{1}{n-N-k} \sum_{i=1}^{n} \widehat{\boldsymbol{\varepsilon}}_{i} \widehat{\boldsymbol{\varepsilon}}_{i} .
\]</span></p>
<p>The <span class="math inline">\(N+k\)</span> degree of freedom adjustment is motivated by the dummy variable representation. You can verify that <span class="math inline">\(\widehat{\sigma}_{\varepsilon}^{2}\)</span> is unbiased for <span class="math inline">\(\sigma_{\varepsilon}^{2}\)</span> under assumptions (17.18), (17.25) and (17.26). See Exercise 17.8.</p>
<p>Notice that the assumptions (17.18), (17.25), and (17.26) are identical to (17.5)-(17.7) of Assumption 17.1. The assumptions (17.8)-(17.10) are not needed. Thus the fixed effect model weakens the random effects model by eliminating the assumptions on <span class="math inline">\(u_{i}\)</span> but retaining those on <span class="math inline">\(\varepsilon_{i t}\)</span>.</p>
<p>The classical covariance matrix estimator (17.36) for the fixed effects estimator is valid when the errors <span class="math inline">\(\varepsilon_{i t}\)</span> are homoskedastic and serially uncorrelated but is invalid otherwise. A covariance matrix estimator which allows <span class="math inline">\(\varepsilon_{i t}\)</span> to be heteroskedastic and serially correlated across <span class="math inline">\(t\)</span> is the cluster-robust covariance matrix estimator, clustered by individual</p>
<p><span class="math display">\[
\widehat{\boldsymbol{V}}_{\mathrm{fe}}^{\text {cluster }}=\left(\dot{\boldsymbol{X}}^{\prime} \dot{\boldsymbol{X}}\right)^{-1}\left(\sum_{i=1}^{N} \dot{\boldsymbol{X}}_{i}^{\prime} \widehat{\boldsymbol{\varepsilon}}_{i} \widehat{\boldsymbol{\varepsilon}}_{i}^{\prime} \dot{\boldsymbol{X}}_{i}\right)\left(\dot{\boldsymbol{X}}^{\prime} \dot{\boldsymbol{X}}\right)^{-1}
\]</span></p>
<p>where <span class="math inline">\(\widehat{\boldsymbol{\varepsilon}}_{i}\)</span> as the fixed effects residuals as defined in (17.23). (17.38) was first proposed by Arellano (1987). As in (4.55) <span class="math inline">\(\widehat{V}_{\text {fe }}^{\text {cluster }}\)</span> can be multiplied by a degree-of-freedom adjustment. The adjustment recommended by the theory of C. Hansen (2007) is</p>
<p><span class="math display">\[
\widehat{\boldsymbol{V}}_{\mathrm{fe}}^{\text {cluster }}=\left(\frac{N}{N-1}\right)\left(\dot{\boldsymbol{X}}^{\prime} \dot{\boldsymbol{X}}\right)^{-1}\left(\sum_{i=1}^{N} \dot{\boldsymbol{X}}_{i}^{\prime} \widehat{\boldsymbol{\varepsilon}}_{i} \widehat{\boldsymbol{\varepsilon}}_{i}^{\prime} \dot{\boldsymbol{X}}_{i}\right)\left(\dot{\boldsymbol{X}}^{\prime} \dot{\boldsymbol{X}}\right)^{-1}
\]</span></p>
<p>and that corresponding to <span class="math inline">\((4.55)\)</span> is</p>
<p><span class="math display">\[
\widehat{\boldsymbol{V}}_{\mathrm{fe}}^{\text {cluster }}=\left(\frac{n-1}{n-N-k}\right)\left(\frac{N}{N-1}\right)\left(\dot{\boldsymbol{X}}^{\prime} \dot{\boldsymbol{X}}\right)^{-1}\left(\sum_{i=1}^{N} \dot{\boldsymbol{X}}_{i}^{\prime} \widehat{\boldsymbol{\varepsilon}}_{i} \widehat{\boldsymbol{\varepsilon}}_{i}^{\prime} \dot{\boldsymbol{X}}_{i}\right)\left(\dot{\boldsymbol{X}}^{\prime} \dot{\boldsymbol{X}}\right)^{-1} \text {. }
\]</span></p>
<p>These estimators are convenient because they are simple to apply and allow for unbalanced panels.</p>
<p>In typical micropanel applications <span class="math inline">\(N\)</span> is very large and <span class="math inline">\(k\)</span> is modest. Thus the adjustment in (17.39) is minor while that in (17.40) is approximately <span class="math inline">\(\bar{T} /(\bar{T}-1)\)</span> where <span class="math inline">\(\bar{T}=n / N\)</span> is the average number of time periods per individual. When <span class="math inline">\(\bar{T}\)</span> is small this can be a very large adjustment. Hence the choice between (17.38), (17.39), and (17.40) can be substantial.</p>
<p>To understand if the degree of freedom adjustment in (17.40) is appropriate, consider the simplified setting where the residuals are constructed with the true <span class="math inline">\(\beta\)</span> but estimated fixed effects <span class="math inline">\(u_{i}\)</span>. This is a useful approximation since the number of estimated slope coefficients <span class="math inline">\(\beta\)</span> is small relative to the sample size <span class="math inline">\(n\)</span>. Then <span class="math inline">\(\widehat{\boldsymbol{\varepsilon}}_{i}=\dot{\boldsymbol{\varepsilon}}_{i}=\boldsymbol{M}_{i} \boldsymbol{\varepsilon}_{i}\)</span> so <span class="math inline">\(\dot{\boldsymbol{X}}_{i}^{\prime} \widehat{\boldsymbol{\varepsilon}}_{i}=\dot{\boldsymbol{X}}_{i}^{\prime} \boldsymbol{\varepsilon}_{i}\)</span> and (17.38) equals</p>
<p><span class="math display">\[
\widehat{\boldsymbol{V}}_{\mathrm{fe}}^{\text {cluster }}=\left(\dot{\boldsymbol{X}}^{\prime} \dot{\boldsymbol{X}}\right)^{-1}\left(\sum_{i=1}^{N} \dot{\boldsymbol{X}}_{i}^{\prime} \varepsilon_{i} \varepsilon_{i}^{\prime} \dot{\boldsymbol{X}}_{i}\right)\left(\dot{\boldsymbol{X}}^{\prime} \dot{\boldsymbol{X}}\right)^{-1}
\]</span></p>
<p>which is the idealized estimator with the true errors rather than the residuals. Since <span class="math inline">\(\mathbb{E}\left[\varepsilon_{i} \varepsilon_{i}^{\prime} \mid \boldsymbol{X}_{i}\right]=\Sigma_{i}\)</span> it follows that <span class="math inline">\(\mathbb{E}\left[\widehat{\boldsymbol{V}}_{\mathrm{fe}}^{\text {cluster }} \mid \boldsymbol{X}\right]=\boldsymbol{V}_{\mathrm{fe}}\)</span> and <span class="math inline">\(\widehat{\boldsymbol{V}}_{\mathrm{fe}}^{\text {cluster }}\)</span> is unbiased for <span class="math inline">\(\boldsymbol{V}_{\mathrm{fe}}\)</span> ! Thus no degree of freedom adjustment is required. This is despite the fact that <span class="math inline">\(N\)</span> fixed effects have been estimated. While this analysis concerns the idealized case where the residuals have been constructed with the true coefficients <span class="math inline">\(\beta\)</span> so does not translate into a direct recommendation for the feasible estimator, it still suggests that the strong ad hoc adjustment in (17.40) is unwarranted.</p>
<p>This (crude) analysis suggests that for the cluster robust covariance estimator for fixed effects regression the adjustment recommended by C. Hansen (17.39) is the most appropriate. It is typically well approximated by the unadjusted estimator (17.38). Based on current theory there is no justification for the ad hoc adjustment (17.40). The main argument for the latter is that it produces the largest standard errors and is thus the most conservative choice.</p>
<p>In current practice the estimators (17.38) and (17.40) are the most commonly used covariance matrix estimators for fixed effects estimation.</p>
<p>In Sections <span class="math inline">\(17.22\)</span> and <span class="math inline">\(17.23\)</span> we discuss covariance matrix estimation under heteroskedasticity but no serial correlation.</p>
<p>To illustrate, in Table <span class="math inline">\(17.2\)</span> we present the fixed effect regression estimates of the investment model (17.3) in the third column with cluster-robust standard errors. The trading indicator <span class="math inline">\(T_{i}\)</span> and the industry dummies cannot be included as they are time-invariant. The point estimates are similar to the random effects estimates, though the coefficients on debt and cash flow increase in magnitude.</p>
</section>
<section id="fixed-effects-estimation-in-stata" class="level2" data-number="16.13">
<h2 data-number="16.13" class="anchored" data-anchor-id="fixed-effects-estimation-in-stata"><span class="header-section-number">16.13</span> Fixed Effects Estimation in Stata</h2>
<p>There are several methods to obtain the fixed effects estimator <span class="math inline">\(\widehat{\beta}_{\mathrm{fe}}\)</span> in Stata.</p>
<p>The first method is dummy variable regression. This can be obtained by the Stata regress command, for example reg y <span class="math inline">\(\mathrm{x}\)</span> , cluster(id) where id is the group (individual) identifier. In most cases, as discussed in Section 17.11, this is not recommended due to the excessive computer memory requirements and slow computation. If this command is done it may be useful to suppress display of the full list of coefficient estimates. To do so, type quietly reg y <span class="math inline">\(x\)</span> , cluster(id) followed by estimates table, keep( <span class="math inline">\(x_{-}\)</span>cons) be se. The second command will report the coefficient(s) on <span class="math inline">\(x\)</span> only, not those on the index variable id. (Other statistics can be reported as well.) The second method is to manually create the within transformed variables as described in Section 17.8, and then use regress.</p>
<p>The third method is <span class="math inline">\(x t r e g ~ f e\)</span> which is specifically written for panel data. This estimates the slope coefficients using the partialling-out approach. The default covariance matrix estimator is classical as defined in (17.36). The cluster-robust covariance matrix (17.38) can be obtained using the options vce(robust) or <span class="math inline">\(r\)</span>.</p>
<p>The fourth method is areg absorb (id). This command is an alternative implementation of partiallingout regression. The default covariance matrix estimator is the classical (17.36). The cluster-robust covariance matrix estimator (17.40) can be obtained using the cluster(id) option. The heteroskedasticityrobust covariance matrix is obtained when <span class="math inline">\(\mathrm{r}\)</span> or <span class="math inline">\(\mathrm{v} c e\)</span> (robust) is specified but this is not recommended unless <span class="math inline">\(T_{i}\)</span> is large as will be discussed in Section <span class="math inline">\(17.22\)</span>.</p>
<p>An important difference between the Stata xtreg and areg commands is that they implement different cluster-robust covariance matrix estimators: (17.38) in the case of xtreg and (17.40) in the case of areg. As discussed in the previous section the adjustment used by areg is ad hoc and not well-justified but produces the largest and hence most conservative standard errors.</p>
<p>Another difference between the commands is how they report the equation <span class="math inline">\(R^{2}\)</span>. This difference can be huge and stems from the fact that they are estimating distinct population counter-parts. Full dummy variable regression and the areg command calculate <span class="math inline">\(R^{2}\)</span> the same way: the squared correlation between <span class="math inline">\(Y_{i t}\)</span> and the fitted regression with all predictors including the individual dummy variables. The <span class="math inline">\(x t r e g ~ f e\)</span> command reports three values for <span class="math inline">\(R^{2}\)</span> : within, between, and overall. The “within” <span class="math inline">\(R^{2}\)</span> is identical to what is obtained from a second stage regression using the within transformed variables. (The second method described above.) The “overall” <span class="math inline">\(R^{2}\)</span> is the squared correlation between <span class="math inline">\(Y_{i t}\)</span> and the fitted regression excluding the individual effects.</p>
<p>Which <span class="math inline">\(R^{2}\)</span> should be reported? The answer depends on the baseline model before regressors are added. If we view the baseline as an individual-specific mean, then the within calculation is appropriate. If the baseline is a single mean for all observations then the full regression (areg) calculation is appropriate. The latter (areg) calculation is typically much higher than the within calculation, as the fixed effects typically “explain” a large portion of the variance. In any event as there is not a single definition of <span class="math inline">\(R^{2}\)</span> it is important to be explicit about the method if it is reported.</p>
<p>In current econometric practice both xtreg and areg are used, though areg appears to be the more popular choice. Since the latter typically produces a much higher value of <span class="math inline">\(R^{2}\)</span>, reported <span class="math inline">\(R^{2}\)</span> values should be viewed skeptically unless their calculation method is documented by the author.</p>
</section>
<section id="between-estimator" class="level2" data-number="16.14">
<h2 data-number="16.14" class="anchored" data-anchor-id="between-estimator"><span class="header-section-number">16.14</span> Between Estimator</h2>
<p>The between estimator is calculated from the individual-mean equation (17.20)</p>
<p><span class="math display">\[
\bar{Y}_{i}=\bar{X}_{i}^{\prime} \beta+u_{i}+\bar{\varepsilon}_{i} .
\]</span></p>
<p>Estimation can be done at the level of individuals or at the level of observations. Least squares applied to (17.41) at the level of the <span class="math inline">\(N\)</span> individuals is</p>
<p><span class="math display">\[
\widehat{\beta}_{\mathrm{be}}=\left(\sum_{i=1}^{N} \bar{X}_{i} \bar{X}_{i}^{\prime}\right)^{-1}\left(\sum_{i=1}^{N} \bar{X}_{i} \bar{Y}_{i}\right) .
\]</span></p>
<p>Least squares applied to (17.41) at the level of observations is</p>
<p><span class="math display">\[
\widetilde{\beta}_{\mathrm{be}}=\left(\sum_{i=1}^{N} \sum_{t \in S_{i}} \bar{X}_{i} \bar{X}_{i}^{\prime}\right)^{-1}\left(\sum_{i=1}^{N} \sum_{t \in S_{i}} \bar{X}_{i} \bar{Y}_{i}\right)=\left(\sum_{i=1}^{N} T_{i} \bar{X}_{i} \bar{X}_{i}^{\prime}\right)^{-1}\left(\sum_{i=1}^{N} T_{i} \bar{X}_{i} \bar{Y}_{i}\right) .
\]</span></p>
<p>In balanced panels <span class="math inline">\(\widetilde{\beta}_{\mathrm{be}}=\widehat{\beta}_{\text {be }}\)</span> but they differ on unbalanced panels. <span class="math inline">\(\widetilde{\beta}_{\mathrm{be}}\)</span> equals weighted least squares applied at the level of individuals with weight <span class="math inline">\(T_{i}\)</span>.</p>
<p>Under the random effects assumptions (Assumption 17.1) <span class="math inline">\(\widehat{\beta}_{\text {be }}\)</span> is unbiased for <span class="math inline">\(\beta\)</span> and has variance</p>
<p><span class="math display">\[
\boldsymbol{V}_{\mathrm{be}}=\operatorname{var}\left[\widehat{\beta}_{\mathrm{be}} \mid \boldsymbol{X}\right]=\left(\sum_{i=1}^{N} \bar{X}_{i} \bar{X}_{i}^{\prime}\right)^{-1}\left(\sum_{i=1}^{N} \bar{X}_{i} \bar{X}_{i}^{\prime} \sigma_{i}^{2}\right)\left(\sum_{i=1}^{N} \bar{X}_{i} \bar{X}_{i}^{\prime}\right)^{-1}
\]</span></p>
<p>where</p>
<p><span class="math display">\[
\sigma_{i}^{2}=\operatorname{var}\left[u_{i}+\bar{\varepsilon}_{i}\right]=\sigma_{u}^{2}+\frac{\sigma_{\varepsilon}^{2}}{T_{i}}
\]</span></p>
<p>is the variance of the error in (17.41). When the panel is balanced the variance formula simplifies to</p>
<p><span class="math display">\[
\boldsymbol{V}_{\mathrm{be}}=\operatorname{var}\left[\widehat{\beta}_{\mathrm{be}} \mid \boldsymbol{X}\right]=\left(\sum_{i=1}^{N} \bar{X}_{i} \bar{X}_{i}^{\prime}\right)^{-1}\left(\sigma_{u}^{2}+\frac{\sigma_{\varepsilon}^{2}}{T}\right) .
\]</span></p>
<p>Under the random effects assumption the between estimator <span class="math inline">\(\widehat{\beta}_{\text {be }}\)</span> is unbiased for <span class="math inline">\(\beta\)</span> but is less efficient than the random effects estimator <span class="math inline">\(\widehat{\beta}_{\text {gls }}\)</span>. Consequently there seems little direct use for the between estimator in linear panel data applications.</p>
<p>Instead, its primary application is to construct an estimate of <span class="math inline">\(\sigma_{u}^{2}\)</span>. First, consider estimation of</p>
<p><span class="math display">\[
\sigma_{b}^{2}=\frac{1}{N} \sum_{i=1}^{N} \sigma_{i}^{2}=\sigma_{u}^{2}+\frac{1}{N} \sum_{i=1}^{N} \frac{\sigma_{\varepsilon}^{2}}{T_{i}}=\sigma_{u}^{2}+\frac{\sigma_{\varepsilon}^{2}}{\bar{T}}
\]</span></p>
<p>where <span class="math inline">\(\bar{T}=N / \sum_{i=1}^{N} T_{i}^{-1}\)</span> is the harmonic mean of <span class="math inline">\(T_{i}\)</span>. (In the case of a balanced panel <span class="math inline">\(\bar{T}=T\)</span>.) A natural estimator of <span class="math inline">\(\sigma_{b}^{2}\)</span> is</p>
<p><span class="math display">\[
\widehat{\sigma}_{b}^{2}=\frac{1}{N-k} \sum_{i=1}^{N} \widehat{e}_{b i}^{2} .
\]</span></p>
<p>where <span class="math inline">\(\widehat{e}_{b i}=\bar{Y}_{i}-\bar{X}_{i}^{\prime} \widehat{\beta}_{\text {be }}\)</span> are the between residuals. (Either <span class="math inline">\(\widehat{\beta}_{\text {be }}\)</span> or <span class="math inline">\(\widetilde{\beta}_{\text {be }}\)</span> can be used.)</p>
<p>From the relation <span class="math inline">\(\sigma_{b}^{2}=\sigma_{u}^{2}+\sigma_{\varepsilon}^{2} / \bar{T}\)</span> and (17.42) we can deduce an estimator for <span class="math inline">\(\sigma_{u}^{2}\)</span>. We have already described an estimator <span class="math inline">\(\widehat{\sigma}_{\varepsilon}^{2}\)</span> for <span class="math inline">\(\sigma_{\varepsilon}^{2}\)</span> in (17.37) for the fixed effects model. Since the fixed effects model holds under weaker conditions than the random effects model, <span class="math inline">\(\widehat{\sigma}_{\varepsilon}^{2}\)</span> is valid for the latter as well. This suggests the following estimator for <span class="math inline">\(\sigma_{u}^{2}\)</span></p>
<p><span class="math display">\[
\widehat{\sigma}_{u}^{2}=\widehat{\sigma}_{b}^{2}-\frac{\widehat{\sigma}_{\varepsilon}^{2}}{\bar{T}} .
\]</span></p>
<p>To summarize, the fixed effect estimator is used for <span class="math inline">\(\widehat{\sigma}_{\varepsilon}^{2}\)</span>, the between estimator for <span class="math inline">\(\widehat{\sigma}_{b}^{2}\)</span>, and <span class="math inline">\(\widehat{\sigma}_{u}^{2}\)</span> is constructed from the two.</p>
<p>It is possible for (17.43) to be negative. It is typical to use the constrained estimator</p>
<p><span class="math display">\[
\widehat{\sigma}_{u}^{2}=\max \left[0, \widehat{\sigma}_{b}^{2}-\frac{\widehat{\sigma}_{\varepsilon}^{2}}{\bar{T}}\right] .
\]</span></p>
<p>(17.44) is the most common estimator for <span class="math inline">\(\sigma_{u}^{2}\)</span> in the random effects model.</p>
<p>The between estimator <span class="math inline">\(\widehat{\beta}_{\text {be }}\)</span> can be obtained using the Stata command xtreg be. The estimator <span class="math inline">\(\widetilde{\beta}_{\text {be }}\)</span> can be obtained by xtreg be wls.</p>
</section>
<section id="feasible-gls" class="level2" data-number="16.15">
<h2 data-number="16.15" class="anchored" data-anchor-id="feasible-gls"><span class="header-section-number">16.15</span> Feasible GLS</h2>
<p>The random effects estimator can be written as</p>
<p><span class="math display">\[
\widehat{\beta}_{\mathrm{re}}=\left(\sum_{i=1}^{N} \boldsymbol{X}_{i}^{\prime} \Omega_{i}^{-1} \boldsymbol{X}_{i}\right)^{-1}\left(\sum_{i=1}^{N} \boldsymbol{X}_{i}^{\prime} \Omega_{i}^{-1} \boldsymbol{Y}_{i}\right)=\left(\sum_{i=1}^{N} \widetilde{\boldsymbol{X}}_{i}^{\prime} \widetilde{\boldsymbol{X}}_{i}\right)^{-1}\left(\sum_{i=1}^{N} \widetilde{\boldsymbol{X}}_{i}^{\prime} \widetilde{\boldsymbol{Y}}_{i}\right)
\]</span></p>
<p>where <span class="math inline">\(\widetilde{\boldsymbol{X}}_{i}=\Omega_{i}^{-1 / 2} \boldsymbol{X}_{i}\)</span> and <span class="math inline">\(\widetilde{\boldsymbol{Y}}_{i}=\Omega_{i}^{-1 / 2} \boldsymbol{Y}_{i}\)</span>. It is instructive to study these transformations.</p>
<p>Define <span class="math inline">\(\boldsymbol{P}_{i}=\mathbf{1}_{i}\left(\mathbf{1}_{i}^{\prime} \mathbf{1}_{i}\right)^{-1} \mathbf{1}_{i}^{\prime}\)</span> so that <span class="math inline">\(\boldsymbol{M}_{i}=\boldsymbol{I}_{i}-\boldsymbol{P}_{i}\)</span>. Thus while <span class="math inline">\(\boldsymbol{M}_{i}\)</span> is the within operator, <span class="math inline">\(\boldsymbol{P}_{i}\)</span> can be called the individual-mean operator since <span class="math inline">\(\boldsymbol{P}_{i} \boldsymbol{Y}_{i}=\mathbf{1}_{i} \bar{Y}_{i}\)</span>. We can write</p>
<p><span class="math display">\[
\Omega_{i}=\boldsymbol{I}_{i}+\mathbf{1}_{i} \mathbf{1}_{i}^{\prime} \sigma_{u}^{2} / \sigma_{\varepsilon}^{2}=\boldsymbol{I}_{i}+\frac{T_{i} \sigma_{u}^{2}}{\sigma_{\varepsilon}^{2}} \boldsymbol{P}_{i}=\boldsymbol{M}_{i}+\rho_{i}^{-2} \boldsymbol{P}_{i}
\]</span></p>
<p>where</p>
<p><span class="math display">\[
\rho_{i}=\frac{\sigma_{\varepsilon}}{\sqrt{\sigma_{\varepsilon}^{2}+T_{i} \sigma_{u}^{2}}} .
\]</span></p>
<p>Since the matrices <span class="math inline">\(\boldsymbol{M}_{i}\)</span> and <span class="math inline">\(\boldsymbol{P}_{i}\)</span> are idempotent and orthogonal we find that <span class="math inline">\(\Omega_{i}^{-1}=\boldsymbol{M}_{i}+\rho_{i}^{2} \boldsymbol{P}_{i}\)</span> and</p>
<p><span class="math display">\[
\Omega_{i}^{-1 / 2}=\boldsymbol{M}_{i}+\rho_{i} \boldsymbol{P}_{i}=\boldsymbol{I}_{i}-\left(1-\rho_{i}\right) \boldsymbol{P}_{i} .
\]</span></p>
<p>Therefore the transformation used by the GLS estimator is</p>
<p><span class="math display">\[
\tilde{\boldsymbol{Y}}_{i}=\left(\boldsymbol{I}_{i}-\left(1-\rho_{i}\right) \boldsymbol{P}_{i}\right) \boldsymbol{Y}_{i}=\boldsymbol{Y}_{i}-\left(1-\rho_{i}\right) \mathbf{1}_{i} \bar{Y}_{i}
\]</span></p>
<p>which is a partial within transformation.</p>
<p>The transformation as written depends on <span class="math inline">\(\rho_{i}\)</span> which is unknown. It can be replaced by the estimator</p>
<p><span class="math display">\[
\widehat{\rho}_{i}=\frac{\widehat{\sigma}_{\varepsilon}}{\sqrt{\widehat{\sigma}_{\varepsilon}^{2}+T_{i} \widehat{\sigma}_{u}^{2}}}
\]</span></p>
<p>where the estimators <span class="math inline">\(\widehat{\sigma}_{\varepsilon}^{2}\)</span> and <span class="math inline">\(\widehat{\sigma}_{u}^{2}\)</span> are given in (17.37) and (17.44). We obtain the feasible transformations</p>
<p><span class="math display">\[
\widetilde{\boldsymbol{Y}}_{i}=\boldsymbol{Y}_{i}-\left(1-\widehat{\rho}_{i}\right) \mathbf{1}_{i} \bar{Y}_{i}
\]</span></p>
<p>and</p>
<p><span class="math display">\[
\widetilde{\boldsymbol{X}}_{i}=\boldsymbol{X}_{i}-\left(1-\widehat{\rho}_{i}\right) \mathbf{1}_{i} \bar{X}_{i}^{\prime} .
\]</span></p>
<p>The feasible random effects estimator is (17.45) using (17.49) and (17.50).</p>
<p>In the previous section we noted that it is possible for <span class="math inline">\(\widehat{\sigma}_{u}^{2}=0\)</span>. In this case <span class="math inline">\(\widehat{\rho}_{i}=1\)</span> and <span class="math inline">\(\widehat{\beta}_{\text {re }}=\widehat{\beta}_{\text {pool }}\)</span>.</p>
<p>What this shows is the following. The random effects estimator (17.45) is least squares applied to the transformed variables <span class="math inline">\(\widetilde{\boldsymbol{X}}_{i}\)</span> and <span class="math inline">\(\widetilde{\boldsymbol{Y}}_{i}\)</span> defined in (17.50) and (17.49). When <span class="math inline">\(\widehat{\rho}_{i}=0\)</span> these are the within transformations, so <span class="math inline">\(\widetilde{\boldsymbol{X}}_{i}=\dot{\boldsymbol{X}}_{i}, \widetilde{\boldsymbol{Y}}_{i}=\dot{\boldsymbol{Y}}_{i}\)</span>, and <span class="math inline">\(\widehat{\beta}_{\mathrm{re}}=\widehat{\beta}_{\mathrm{fe}}\)</span> is the fixed effects estimator. When <span class="math inline">\(\widehat{\rho}_{i}=1\)</span> the data are untransformed <span class="math inline">\(\widetilde{\boldsymbol{X}}_{i}=\boldsymbol{X}_{i}, \widetilde{\boldsymbol{Y}}_{i}=\boldsymbol{Y}_{i}\)</span>, and <span class="math inline">\(\widehat{\beta}_{\mathrm{re}}=\widehat{\beta}_{\text {pool }}\)</span> is the pooled estimator. In general, <span class="math inline">\(\widetilde{\boldsymbol{X}}_{i}\)</span> and <span class="math inline">\(\widetilde{\boldsymbol{Y}}_{i}\)</span> can be viewed as partial within transformations.</p>
<p>Recalling the definition <span class="math inline">\(\widehat{\rho}_{i}=\widehat{\sigma}_{\varepsilon} / \sqrt{\widehat{\sigma}_{\varepsilon}^{2}+T_{i} \widehat{\sigma}_{u}^{2}}\)</span> we see that when the idiosyncratic error variance <span class="math inline">\(\widehat{\sigma}_{\varepsilon}^{2}\)</span> is large relative to <span class="math inline">\(T_{i} \widehat{\sigma}_{u}^{2}\)</span> then <span class="math inline">\(\widehat{\rho}_{i} \approx 1\)</span> and <span class="math inline">\(\widehat{\beta}_{\text {re }} \approx \widehat{\beta}_{\text {pool. }}\)</span>. Thus when the variance estimates suggest that the individual effect is relatively small the random effect estimator simplifies to the pooled estimator. On the other hand when the individual effect error variance <span class="math inline">\(\widehat{\sigma}_{u}^{2}\)</span> is large relative to <span class="math inline">\(\widehat{\sigma}_{\varepsilon}^{2}\)</span> then <span class="math inline">\(\widehat{\rho}_{i} \approx 0\)</span> and <span class="math inline">\(\widehat{\beta}_{\mathrm{re}} \approx \widehat{\beta}_{\mathrm{fe}}\)</span>. Thus when the variance estimates suggest that the individual effect is relatively large the random effect estimator is close to the fixed effects estimator.</p>
</section>
<section id="intercept-in-fixed-effects-regression" class="level2" data-number="16.16">
<h2 data-number="16.16" class="anchored" data-anchor-id="intercept-in-fixed-effects-regression"><span class="header-section-number">16.16</span> Intercept in Fixed Effects Regression</h2>
<p>The fixed effect estimator does not apply to any regressor which is time-invariant for all individuals. This includes an intercept. Yet some authors and packages (e.g.&nbsp;Amemiya (1971) and xtreg in Stata) report an intercept. To see how to construct an estimator of an intercept take the components regression equation adding an explicit intercept</p>
<p><span class="math display">\[
Y_{i t}=\alpha+X_{i t}^{\prime} \beta+u_{i}+\varepsilon_{i t} .
\]</span></p>
<p>We have already discussed estimation of <span class="math inline">\(\beta\)</span> by <span class="math inline">\(\widehat{\beta}_{\mathrm{fe}}\)</span>. Replacing <span class="math inline">\(\beta\)</span> in this equation with <span class="math inline">\(\widehat{\beta}_{\mathrm{fe}}\)</span> and then estimating <span class="math inline">\(\alpha\)</span> by least squares, we obtain</p>
<p><span class="math display">\[
\widehat{\alpha}_{\mathrm{fe}}=\bar{Y}-\bar{X}^{\prime} \widehat{\beta}_{\mathrm{fe}}
\]</span></p>
<p>where <span class="math inline">\(\bar{Y}\)</span> and <span class="math inline">\(\bar{X}\)</span> are averages from the full sample. This is the estimator reported by xtreg.</p>
</section>
<section id="estimation-of-fixed-effects" class="level2" data-number="16.17">
<h2 data-number="16.17" class="anchored" data-anchor-id="estimation-of-fixed-effects"><span class="header-section-number">16.17</span> Estimation of Fixed Effects</h2>
<p>For most applications researchers are interested in the coefficients <span class="math inline">\(\beta\)</span> not the fixed effects <span class="math inline">\(u_{i}\)</span>. But in some cases the fixed effects themselves are interesting. This arises when we want to measure the distribution of <span class="math inline">\(u_{i}\)</span> to understand its heterogeneity. It also arises in the context of prediction. As discussed in Section <span class="math inline">\(17.11\)</span> the fixed effects estimate <span class="math inline">\(\widehat{u}\)</span> is obtained by least squares applied to the regression (17.33). To find their solution, replace <span class="math inline">\(\beta\)</span> in (17.33) with the least squares minimizer <span class="math inline">\(\widehat{\beta}_{\mathrm{fe}}\)</span> and apply least squares. Since this is the individual-specific intercept the solution is</p>
<p><span class="math display">\[
\widehat{u}_{i}=\frac{1}{T_{i}} \sum_{t \in S_{i}}\left(Y_{i t}-X_{i t}^{\prime} \widehat{\beta}_{\mathrm{fe}}\right)=\bar{Y}_{i}-\bar{X}_{i}^{\prime} \widehat{\beta}_{\mathrm{fe}} .
\]</span></p>
<p>Alternatively, using (17.34) this is</p>
<p><span class="math display">\[
\begin{aligned}
\widehat{u} &amp;=\left(\boldsymbol{D}^{\prime} \boldsymbol{D}\right)^{-1} \boldsymbol{D}^{\prime}\left(\boldsymbol{Y}-\boldsymbol{X} \widehat{\beta}_{\mathrm{fe}}\right) \\
&amp;=\operatorname{diag}\left\{T_{i}^{-1}\right\} \sum_{i=1}^{N} d_{i} \mathbf{1}_{i}^{\prime}\left(\boldsymbol{Y}_{i}-\boldsymbol{X}_{i} \widehat{\beta}_{\mathrm{fe}}\right) \\
&amp;=\sum_{i=1}^{N} d_{i}\left(\bar{Y}_{i}-\bar{X}_{i}^{\prime} \widehat{\beta}_{\mathrm{fe}}\right) \\
&amp;=\left(\widehat{u}_{1}, \ldots, \widehat{u}_{N}\right)^{\prime}
\end{aligned}
\]</span></p>
<p>Thus the least squares estimates of the fixed effects can be obtained from the individual-specific means and does not require a regression with <span class="math inline">\(N+k\)</span> regressors.</p>
<p>If an intercept has been estimated (as discussed in the previous section) it should be subtracted from (17.51). In this case the estimated fixed effects are</p>
<p><span class="math display">\[
\widehat{u}_{i}=\bar{Y}_{i}-\bar{X}_{i}^{\prime} \widehat{\beta}_{\mathrm{fe}}-\widehat{\alpha}_{\mathrm{fe}}
\]</span></p>
<p>With either estimator when the number of time series observations <span class="math inline">\(T_{i}\)</span> is small <span class="math inline">\(\widehat{u}_{i}\)</span> will be an imprecise estimator of <span class="math inline">\(u_{i}\)</span>. Thus calculations based on <span class="math inline">\(\widehat{u}_{i}\)</span> should be interpreted cautiously.</p>
<p>The fixed effects (17.52) may be obtained in Stata after ivreg, fe using the predict u command or after areg using the predict d command.</p>
</section>
<section id="gmm-interpretation-of-fixed-effects" class="level2" data-number="16.18">
<h2 data-number="16.18" class="anchored" data-anchor-id="gmm-interpretation-of-fixed-effects"><span class="header-section-number">16.18</span> GMM Interpretation of Fixed Effects</h2>
<p>We can also interpret the fixed effects estimator through the generalized method of moments.</p>
<p>Take the fixed effects model after applying the within transformation (17.21). We can view this as a system of <span class="math inline">\(T\)</span> equations, one for each time period <span class="math inline">\(t\)</span>. This is a multivariate regression model. Using the notation of Chapter 11 define the <span class="math inline">\(T \times k T\)</span> regressor matrix</p>
<p><span class="math display">\[
\overline{\boldsymbol{X}}_{i}=\left(\begin{array}{cccc}
\dot{X}_{i 1}^{\prime} &amp; 0 &amp; \cdots &amp; 0 \\
\vdots &amp; \dot{X}_{i 2}^{\prime} &amp; &amp; \vdots \\
0 &amp; 0 &amp; \cdots &amp; \dot{X}_{i T}^{\prime}
\end{array}\right) .
\]</span></p>
<p>If we treat each time period as a separate equation we have the <span class="math inline">\(k T\)</span> moment conditions</p>
<p><span class="math display">\[
\mathbb{E}\left[\overline{\boldsymbol{X}}_{i}^{\prime}\left(\dot{\boldsymbol{Y}}_{i}-\dot{\boldsymbol{X}}_{i} \beta\right)\right]=0 .
\]</span></p>
<p>This is an overidentified system of equations when <span class="math inline">\(T \geq 3\)</span> as there are <span class="math inline">\(k\)</span> coefficients and <span class="math inline">\(k T\)</span> moments. (However, the moments are collinear due to the within transformation. There are <span class="math inline">\(k(T-1)\)</span> effective moments.) Interpreting this model in the context of multivariate regression, overidentification is achieved by the restriction that the coefficient vector <span class="math inline">\(\beta\)</span> is constant across time periods.</p>
<p>This model can be interpreted as a regression of <span class="math inline">\(\dot{\boldsymbol{Y}}_{i}\)</span> on <span class="math inline">\(\dot{\boldsymbol{X}}_{i}\)</span> using the instruments <span class="math inline">\(\overline{\boldsymbol{X}}_{i}\)</span>. The 2SLS estimator using matrix notation is</p>
<p><span class="math display">\[
\widehat{\beta}=\left(\left(\dot{\boldsymbol{X}}^{\prime} \overline{\boldsymbol{X}}\right)\left(\overline{\boldsymbol{X}}^{\prime} \overline{\boldsymbol{X}}\right)^{-1}\left(\overline{\boldsymbol{X}}^{\prime} \dot{\boldsymbol{X}}\right)\right)^{-1}\left(\left(\dot{\boldsymbol{X}}^{\prime} \overline{\boldsymbol{X}}\right)\left(\overline{\boldsymbol{X}}^{\prime} \overline{\boldsymbol{X}}\right)^{-1}\left(\overline{\boldsymbol{X}}^{\prime} \dot{\boldsymbol{Y}}\right)\right)
\]</span></p>
<p>Notice that</p>
<p><span class="math display">\[
\begin{aligned}
&amp; \overline{\boldsymbol{X}}^{\prime} \overline{\boldsymbol{X}}=\sum_{i=1}^{n}\left(\begin{array}{cccc}\dot{X}_{i 1} &amp; 0 &amp; \cdots &amp; 0 \\\vdots &amp; \dot{X}_{i 2} &amp; &amp; \vdots \\0 &amp; 0 &amp; \cdots &amp; \dot{X}_{i T}\end{array}\right)\left(\begin{array}{cccc}\dot{X}_{i 1}^{\prime} &amp; 0 &amp; \cdots &amp; 0 \\\vdots &amp; \dot{X}_{i 2}^{\prime} &amp; &amp; \vdots \\0 &amp; 0 &amp; \cdots &amp; \dot{X}_{i T}^{\prime}\end{array}\right) \\
&amp; =\left(\begin{array}{cccc}\sum_{i=1}^{n} \dot{X}_{i 1} \dot{X}_{i 1}^{\prime} &amp; 0 &amp; \cdots &amp; 0 \\\vdots &amp; \sum_{i=1}^{n} \dot{X}_{i 2} \dot{X}_{i 2}^{\prime} &amp; &amp; \vdots \\0 &amp; 0 &amp; \cdots &amp; \sum_{i=1}^{n} \dot{X}_{i T} \dot{X}_{i T}^{\prime}\end{array}\right) \text {, } \\
&amp; \overline{\boldsymbol{X}}^{\prime} \dot{\boldsymbol{X}}=\left(\begin{array}{c}\sum_{i=1}^{n} \dot{X}_{i 1} \dot{X}_{i 1}^{\prime} \\\vdots \\\sum_{i=1}^{n} \dot{X}_{i T} \dot{X}_{i T}^{\prime}\end{array}\right) \text {, }
\end{aligned}
\]</span></p>
<p>and</p>
<p><span class="math display">\[
\overline{\boldsymbol{X}}^{\prime} \dot{\boldsymbol{Y}}=\left(\begin{array}{c}
\sum_{i=1}^{n} \dot{X}_{i 1} \dot{Y}_{i 1} \\
\vdots \\
\sum_{i=1}^{n} \dot{X}_{i T} \dot{Y}_{i T}
\end{array}\right) \text {. }
\]</span></p>
<p>Thus the 2SLS estimator simplifies to</p>
<p><span class="math display">\[
\begin{aligned}
\widehat{\beta}_{2 \mathrm{sls}} &amp;=\left(\sum_{t=1}^{T}\left(\sum_{i=1}^{n} \dot{X}_{i t} \dot{X}_{i t}^{\prime}\right)\left(\sum_{i=1}^{n} \dot{X}_{i t} \dot{X}_{i t}^{\prime}\right)^{-1}\left(\sum_{i=1}^{n} \dot{X}_{i t} \dot{X}_{i t}^{\prime}\right)\right)^{-1} \\
&amp; \times\left(\sum_{t=1}^{T}\left(\sum_{i=1}^{n} \dot{X}_{i t} \dot{X}_{i t}^{\prime}\right)\left(\sum_{i=1}^{n} \dot{X}_{i t} \dot{X}_{i t}^{\prime}\right)^{-1}\left(\sum_{i=1}^{n} \dot{X}_{i t} \dot{Y}_{i t}\right)\right) \\
&amp;=\left(\sum_{t=1}^{T} \sum_{i=1}^{n} \dot{X}_{i t} \dot{X}_{i t}^{\prime}\right)^{-1}\left(\sum_{t=1}^{T} \sum_{i=1}^{n} \dot{X}_{i t} \dot{Y}_{i t}\right) \\
&amp;=\widehat{\beta}_{\mathrm{fe}}
\end{aligned}
\]</span></p>
<p>the fixed effects estimator!</p>
<p>This shows that if we treat each time period as a separate equation with its separate moment equation so that the system is over-identified, and then estimate by GMM using the 2SLS weight matrix, the resulting GMM estimator equals the simple fixed effects estimator. There is no change by adding the additional moment conditions.</p>
<p>The 2SLS estimator is the appropriate GMM estimator when the equation error is serially uncorrelated and homoskedastic. If we use a two-step efficient weight matrix which allows for heteroskedasticity and serial correlation the GMM estimator is</p>
<p><span class="math display">\[
\begin{aligned}
\widehat{\beta}_{\mathrm{gmm}} &amp;=\left(\sum_{t=1}^{T}\left(\sum_{i=1}^{n} \dot{X}_{i t} \dot{X}_{i t}^{\prime}\right)\left(\sum_{i=1}^{n} \dot{X}_{i t} \dot{X}_{i t}^{\prime} \widehat{e}_{i t}^{2}\right)^{-1}\left(\sum_{i=1}^{n} \dot{X}_{i t} \dot{X}_{i t}^{\prime}\right)\right)^{-1} \\
&amp; \times\left(\sum_{t=1}^{T}\left(\sum_{i=1}^{n} \dot{X}_{i t} \dot{X}_{i t}^{\prime}\right)\left(\sum_{i=1}^{n} \dot{X}_{i t} \dot{X}_{i t}^{\prime} \widehat{e}_{i t}^{2}\right)^{-1}\left(\sum_{i=1}^{n} \dot{X}_{i t} \dot{Y}_{i t}\right)\right)
\end{aligned}
\]</span></p>
<p>where <span class="math inline">\(\widehat{e}_{i t}\)</span> are the fixed effects residuals.</p>
<p>Notationally, this GMM estimator has been written for a balanced panel. For an unbalanced panel the sums over <span class="math inline">\(i\)</span> need to be replaced by sums over individuals observed during time period <span class="math inline">\(t\)</span>. Otherwise no changes need to be made.</p>
</section>
<section id="identification-in-the-fixed-effects-model" class="level2" data-number="16.19">
<h2 data-number="16.19" class="anchored" data-anchor-id="identification-in-the-fixed-effects-model"><span class="header-section-number">16.19</span> Identification in the Fixed Effects Model</h2>
<p>The identification of the slope coefficient <span class="math inline">\(\beta\)</span> in fixed effects regression is similar to that in conventional regression but somewhat more nuanced.</p>
<p>It is most useful to consider the within-transformed equation, which can be written as <span class="math inline">\(\dot{Y}_{i t}=\dot{X}_{i t}^{\prime} \beta+\dot{\varepsilon}_{i t}\)</span> or <span class="math inline">\(\dot{\boldsymbol{Y}}_{i}=\dot{\boldsymbol{X}}_{i} \beta+\dot{\boldsymbol{\varepsilon}}_{i}\)</span></p>
<p>From regression theory we know that the coefficient <span class="math inline">\(\beta\)</span> is the linear effect of <span class="math inline">\(\dot{X}_{i t}\)</span> on <span class="math inline">\(\dot{Y}_{i t}\)</span>. The variable <span class="math inline">\(\dot{X}_{i t}\)</span> is the deviation of the regressor from its individual-specific mean and similarly for <span class="math inline">\(\dot{Y}_{i t}\)</span>. Thus the fixed effects model does not identify the effect of the average level of <span class="math inline">\(X_{i t}\)</span> on the average level of <span class="math inline">\(Y_{i t}\)</span>, but rather the effect of the deviations in <span class="math inline">\(X_{i t}\)</span> on <span class="math inline">\(Y_{i t}\)</span>.</p>
<p>In any given sample the fixed effects estimator is only defined if <span class="math inline">\(\sum_{i=1}^{N} \dot{\boldsymbol{X}}_{i}^{\prime} \dot{\boldsymbol{X}}_{i}\)</span> is full rank. The population analog (when individuals are i.i.d.) is</p>
<p><span class="math display">\[
\mathbb{E}\left[\dot{\boldsymbol{X}}_{i}^{\prime} \dot{\boldsymbol{X}}_{i}\right]&gt;0 .
\]</span></p>
<p>Equation (17.54) is the identification condition for the fixed effects estimator. It requires that the regressor matrix is full-rank in expectation after application of the within transformation. The regressors cannot contain any variable which does not have time-variation at the individual level nor a set of regressors whose time-variation at the individual level is collinear.</p>
</section>
<section id="asymptotic-distribution-of-fixed-effects-estimator" class="level2" data-number="16.20">
<h2 data-number="16.20" class="anchored" data-anchor-id="asymptotic-distribution-of-fixed-effects-estimator"><span class="header-section-number">16.20</span> Asymptotic Distribution of Fixed Effects Estimator</h2>
<p>In this section we present an asymptotic distribution theory for the fixed effects estimator in balanced panels. Unbalanced panels are considered in the following section.</p>
<p>We use the following assumptions.</p>
<p>Assumption $17.2</p>
<ol type="1">
<li><p><span class="math inline">\(Y_{i t}=X_{i t}^{\prime} \beta+u_{i}+\varepsilon_{i t}\)</span> for <span class="math inline">\(i=1, \ldots, N\)</span> and <span class="math inline">\(t=1, \ldots, T\)</span> with <span class="math inline">\(T \geq 2\)</span>.</p></li>
<li><p>The variables <span class="math inline">\(\left(\boldsymbol{\varepsilon}_{i}, \boldsymbol{X}_{i}\right), i=1, \ldots, N\)</span>, are independent and identically distributed.</p></li>
<li><p><span class="math inline">\(\mathbb{E}\left[X_{i s} \varepsilon_{i t}\right]=0\)</span> for all <span class="math inline">\(s=1, \ldots, T\)</span>.</p></li>
<li><p><span class="math inline">\(\boldsymbol{Q}_{T}=\mathbb{E}\left[\dot{\boldsymbol{X}}_{i}^{\prime} \dot{\boldsymbol{X}}_{i}\right]&gt;0\)</span>.</p></li>
<li><p><span class="math inline">\(\mathbb{E}\left[\varepsilon_{i t}^{4}\right]&lt;\infty\)</span>.</p></li>
<li><p><span class="math inline">\(\mathbb{E}\left\|X_{i t}\right\|^{4}&lt;\infty\)</span></p></li>
</ol>
<p>Given Assumption <span class="math inline">\(17.2\)</span> we can establish asymptotic normality for <span class="math inline">\(\widehat{\beta}_{\mathrm{fe}}\)</span>.</p>
<p>Theorem 17.2 Under Assumption 17.2, as <span class="math inline">\(N \rightarrow \infty, \sqrt{N}\left(\widehat{\beta}_{\mathrm{fe}}-\beta\right) \underset{d}{\longrightarrow} \mathrm{N}\left(0, \boldsymbol{V}_{\boldsymbol{\beta}}\right)\)</span> where <span class="math inline">\(\boldsymbol{V}_{\beta}=\boldsymbol{Q}_{T}^{-1} \Omega_{T} \boldsymbol{Q}_{T}^{-1}\)</span> and <span class="math inline">\(\Omega_{T}=\mathbb{E}\left[\dot{\boldsymbol{X}}_{i}^{\prime} \boldsymbol{\varepsilon}_{i} \boldsymbol{\varepsilon}_{i}^{\prime} \dot{\boldsymbol{X}}_{i}\right]\)</span>.</p>
<p>This asymptotic distribution is derived as the number of individuals <span class="math inline">\(N\)</span> diverges to infinity while the time number of time periods <span class="math inline">\(T\)</span> is held fixed. Therefore the normalization is <span class="math inline">\(\sqrt{N}\)</span> rather than <span class="math inline">\(\sqrt{n}\)</span> (though either could be used since <span class="math inline">\(T\)</span> is fixed). This approximation is appropriate for the context of a large number of individuals. We could alternatively derive an approximation for the case where both <span class="math inline">\(N\)</span> and <span class="math inline">\(T\)</span> diverge to infinity but this would not be a stronger result. One way of thinking about this is that Theorem <span class="math inline">\(17.2\)</span> does not require <span class="math inline">\(T\)</span> to be large.</p>
<p>Theorem <span class="math inline">\(17.2\)</span> may appear standard given our arsenal of asymptotic theory but in a fundamental sense it is quite different from any other result we have introduced. Fixed effects regression is effectively estimating <span class="math inline">\(N+k\)</span> coefficients - the <span class="math inline">\(k\)</span> slope coefficients <span class="math inline">\(\beta\)</span> plus the <span class="math inline">\(N\)</span> fixed effects <span class="math inline">\(u\)</span> - and the theory specifies that <span class="math inline">\(N \rightarrow \infty\)</span>. Thus the number of estimated parameters is diverging to infinity at the same rate as sample size yet the the estimator obtains a conventional mean-zero sandwich-form asymptotic distribution. In this sense Theorem <span class="math inline">\(17.2\)</span> is new and special.</p>
<p>We now discuss the assumptions.</p>
<p>Assumption 17.2.2 states that the observations are independent across individuals <span class="math inline">\(i\)</span>. This is commonly used for panel data asymptotic theory. An important implied restriction is that it means that we exclude from the regressors any serially correlated aggregate time series variation. Assumption 17.2.3 imposes that <span class="math inline">\(X_{i t}\)</span> is strictly exogeneous for <span class="math inline">\(\varepsilon_{i t}\)</span>. This is stronger than simple projection but is weaker than strict mean independence (17.18). It does not impose any condition on the individual-specific effects <span class="math inline">\(u_{i}\)</span>.</p>
<p>Assumption 17.2.4 is the identification condition discussed in the previous section.</p>
<p>Assumptions 17.2.5 and 17.2.6 are needed for the central limit theorem.</p>
<p>We now prove Theorem 17.2. The assumptions imply that the variables <span class="math inline">\(\left(\dot{\boldsymbol{X}}_{i}, \boldsymbol{\varepsilon}_{i}\right)\)</span> are i.i.d. across <span class="math inline">\(i\)</span> and have finite fourth moments. Thus by the WLLN</p>
<p><span class="math display">\[
\frac{1}{N} \sum_{i=1}^{N} \dot{\boldsymbol{X}}_{i}^{\prime} \dot{\boldsymbol{X}}_{i} \underset{p}{\longrightarrow} \mathbb{E}\left[\dot{\boldsymbol{X}}_{i}^{\prime} \dot{\boldsymbol{X}}_{i}\right]=\boldsymbol{Q}_{T} .
\]</span></p>
<p>Assumption 17.2.3 implies</p>
<p><span class="math display">\[
\mathbb{E}\left[\dot{\boldsymbol{X}}_{i}^{\prime} \boldsymbol{\varepsilon}_{i}\right]=\sum_{t=1}^{T} \mathbb{E}\left[\dot{X}_{i t} \varepsilon_{i t}\right]=\sum_{t=1}^{T} \mathbb{E}\left[X_{i t} \varepsilon_{i t}\right]-\sum_{t=1}^{T} \sum_{j=1}^{T} \mathbb{E}\left[X_{i j} \varepsilon_{i t}\right]=0
\]</span></p>
<p>so they are mean zero. Assumptions 17.2.5 and 17.2.6 imply that <span class="math inline">\(\dot{\boldsymbol{X}}_{i}^{\prime} \boldsymbol{\varepsilon}_{i}\)</span> has a finite covariance matrix <span class="math inline">\(\Omega_{T}\)</span>. The assumptions for the CLT (Theorem 6.3) hold, thus</p>
<p><span class="math display">\[
\frac{1}{\sqrt{N}} \sum_{i=1}^{N} \dot{\boldsymbol{X}}_{i}^{\prime} \varepsilon_{i} \underset{d}{\longrightarrow} \mathrm{N}\left(0, \Omega_{T}\right)
\]</span></p>
<p>Together we find</p>
<p><span class="math display">\[
\sqrt{N}\left(\widehat{\beta}_{\mathrm{fe}}-\beta\right)=\left(\frac{1}{N} \sum_{i=1}^{N} \dot{\boldsymbol{X}}_{i}^{\prime} \dot{\boldsymbol{X}}_{i}\right)^{-1}\left(\frac{1}{\sqrt{N}} \sum_{i=1}^{N} \dot{\boldsymbol{X}}_{i}^{\prime} \boldsymbol{\varepsilon}_{i}\right) \underset{d}{\longrightarrow} \boldsymbol{Q}_{T}^{-1} \mathrm{~N}\left(0, \Omega_{T}\right)=\mathrm{N}\left(0, \boldsymbol{V}_{\beta}\right)
\]</span></p>
<p>as stated.</p>
</section>
<section id="asymptotic-distribution-for-unbalanced-panels" class="level2" data-number="16.21">
<h2 data-number="16.21" class="anchored" data-anchor-id="asymptotic-distribution-for-unbalanced-panels"><span class="header-section-number">16.21</span> Asymptotic Distribution for Unbalanced Panels</h2>
<p>In this section we extend the theory of the previous section to cover unbalanced panels under random selection. Our presentation is built on Section <span class="math inline">\(17.1\)</span> of Wooldridge (2010).</p>
<p>Think of an unbalanced panel as a shortened version of an idealized balanced panel where the shortening is due to “missing” observations due to random selection. Thus suppose that the underlying (potentially latent) variables are <span class="math inline">\(\boldsymbol{Y}_{i}=\left(Y_{i 1}, \ldots, Y_{i T}\right)^{\prime}\)</span> and <span class="math inline">\(\boldsymbol{X}_{i}=\left(X_{i 1}, \ldots, X_{i T}\right)^{\prime}\)</span>. Let <span class="math inline">\(\boldsymbol{s}_{i}=\left(s_{i 1}, \ldots, s_{i T}\right)^{\prime}\)</span> be a vector of selection indicators, meaning that <span class="math inline">\(s_{i t}=1\)</span> if the time period <span class="math inline">\(t\)</span> is observed for individual <span class="math inline">\(i\)</span> and <span class="math inline">\(s_{i t}=0\)</span> otherwise. Then we can describe the estimators algebraically as follows.</p>
<p>Let <span class="math inline">\(\boldsymbol{S}_{i}=\operatorname{diag}\left(\boldsymbol{s}_{i}\right)\)</span> and <span class="math inline">\(\boldsymbol{M}_{i}=\boldsymbol{S}_{i}-\boldsymbol{s}_{i}\left(\boldsymbol{s}_{i}^{\prime} \boldsymbol{s}_{i}\right)^{-1} \boldsymbol{s}_{i}^{\prime}\)</span>, which is idempotent. The within transformations can be written as <span class="math inline">\(\dot{\boldsymbol{Y}}_{i}=\boldsymbol{M}_{i} \boldsymbol{Y}_{i}\)</span> and <span class="math inline">\(\dot{\boldsymbol{X}}_{i}=\boldsymbol{M}_{i} \boldsymbol{X}_{i}\)</span>. They have the property that if <span class="math inline">\(s_{i t}=0\)</span> (so that time period <span class="math inline">\(t\)</span> is missing) then the <span class="math inline">\(t^{t h}\)</span> element of <span class="math inline">\(\dot{\boldsymbol{Y}}_{i}\)</span> and the <span class="math inline">\(t^{t h}\)</span> row of <span class="math inline">\(\dot{\boldsymbol{X}}_{i}\)</span> are all zeros. The missing observations have been replaced by zeros. Consequently, they do not appear in matrix products and sums.</p>
<p>The fixed effects estimator of <span class="math inline">\(\beta\)</span> based on the observed sample is</p>
<p><span class="math display">\[
\widehat{\beta}_{\mathrm{fe}}=\left(\sum_{i=1}^{N} \dot{\boldsymbol{X}}_{i}^{\prime} \dot{\boldsymbol{X}}_{i}\right)^{-1}\left(\sum_{i=1}^{N} \dot{\boldsymbol{X}}_{i}^{\prime} \dot{\boldsymbol{Y}}_{i}\right) .
\]</span></p>
<p>Centered and normalized,</p>
<p><span class="math display">\[
\sqrt{N}\left(\widehat{\beta}_{\mathrm{fe}}-\beta\right)=\left(\frac{1}{N} \sum_{i=1}^{N} \dot{\boldsymbol{X}}_{i}^{\prime} \dot{\boldsymbol{X}}_{i}\right)^{-1}\left(\frac{1}{\sqrt{N}} \sum_{i=1}^{N} \dot{\boldsymbol{X}}_{i}^{\prime} \varepsilon_{i}\right)
\]</span></p>
<p>Notationally this appears to be identical to the case of a balanced panel but the difference is that the within operator <span class="math inline">\(\boldsymbol{M}_{i}\)</span> incorporates the sample selection induced by the unbalanced panel structure.</p>
<p>To derive a distribution theory for <span class="math inline">\(\widehat{\beta}_{\text {fe we }}\)</span> need to be explicit about the stochastic nature of <span class="math inline">\(\boldsymbol{s}_{i}\)</span>. That is, why are some time periods observed and some not? We could take several approaches:</p>
<ol type="1">
<li><p>We could treat <span class="math inline">\(s_{i}\)</span> as fixed (non-random). This is the easiest approach but the most unsatisfactory.</p></li>
<li><p>We could treat <span class="math inline">\(s_{i}\)</span> as random but independent of <span class="math inline">\(\left(\boldsymbol{Y}_{i}, \boldsymbol{X}_{i}\right)\)</span>. This is known as “missing at random” and is a common assumption used to justify methods with missing observations. It is justified when the reason why observations are not observed is independent of the observations. This is appropriate, for example, in panel data sets where individuals enter and exit in “waves”. The statistical treatment is not substantially different from the case of fixed <span class="math inline">\(s_{i}\)</span>.</p></li>
<li><p>We could treat <span class="math inline">\(\left(\boldsymbol{Y}_{i}, \boldsymbol{X}_{i}, \boldsymbol{s}_{i}\right)\)</span> as jointly random but impose a condition sufficient for consistent estimation of <span class="math inline">\(\beta\)</span>. This is the approach we take below. The condition turns out to be a form of mean independence. The advantage of this approach is that it is less restrictive than full independence. The disadvantage is that we must use a conditional mean restriction rather than uncorrelatedness to identify the coefficients.</p></li>
</ol>
<p>The specific assumptions we impose are as follows.</p>
<p>Assumption 17.3</p>
<ol type="1">
<li><p><span class="math inline">\(Y_{i t}=X_{i t}^{\prime} \beta+u_{i}+\varepsilon_{i t}\)</span> for <span class="math inline">\(i=1, \ldots, N\)</span> with <span class="math inline">\(T_{i} \geq 2\)</span>.</p></li>
<li><p>The variables <span class="math inline">\(\left(\boldsymbol{\varepsilon}_{i}, \boldsymbol{X}_{i}, \boldsymbol{s}_{i}\right), i=1, \ldots, N\)</span>, are independent and identically distributed.</p></li>
<li><p><span class="math inline">\(\mathbb{E}\left[\varepsilon_{i t} \mid \boldsymbol{X}_{i}, s_{i}\right]=0\)</span>.</p></li>
<li><p><span class="math inline">\(\boldsymbol{Q}_{T}=\mathbb{E}\left[\dot{\boldsymbol{X}}_{i}^{\prime} \dot{\boldsymbol{X}}_{i}\right]&gt;0\)</span>.</p></li>
<li><p><span class="math inline">\(\mathbb{E}\left[\varepsilon_{i t}^{4}\right]&lt;\infty\)</span>.</p></li>
<li><p><span class="math inline">\(\mathbb{E}\left\|X_{i t}\right\|^{4}&lt;\infty\)</span>.</p></li>
</ol>
<p>The primary difference with Assumption <span class="math inline">\(17.2\)</span> is that we have strengthened strict exogeneity to strict mean independence. This imposes that the regression model is properly specified and that selection does not affect the mean of <span class="math inline">\(\varepsilon_{i t}\)</span>. It is less restrictive than full independence since <span class="math inline">\(\boldsymbol{s}_{i}\)</span> can affect other moments of <span class="math inline">\(\varepsilon_{i t}\)</span> and more importantly does not restrict the joint dependence between <span class="math inline">\(\boldsymbol{s}_{i}\)</span> and <span class="math inline">\(\boldsymbol{X}_{i}\)</span>.</p>
<p>Given the above development it is straightforward to establish asymptotic normality.</p>
<p>Theorem 17.3 Under Assumption 17.3, as <span class="math inline">\(N \rightarrow \infty, \sqrt{N}\left(\widehat{\beta}_{\mathrm{fe}}-\beta\right) \underset{d}{\longrightarrow} \mathrm{N}\left(0, \boldsymbol{V}_{\beta}\right)\)</span> where <span class="math inline">\(\boldsymbol{V}_{\beta}=\boldsymbol{Q}_{T}^{-1} \Omega_{T} \boldsymbol{Q}_{T}^{-1}\)</span> and <span class="math inline">\(\Omega_{T}=\mathbb{E}\left[\dot{\boldsymbol{X}}_{i}^{\prime} \boldsymbol{\varepsilon}_{i} \boldsymbol{\varepsilon}_{i}^{\prime} \dot{\boldsymbol{X}}_{i}\right]\)</span>. We now prove Theorem 17.3. The assumptions imply that the variables <span class="math inline">\(\left(\dot{\boldsymbol{X}}_{i}, \boldsymbol{\varepsilon}_{i}\right)\)</span> are i.i.d. across <span class="math inline">\(i\)</span> and have finite fourth moments. By the WLLN</p>
<p><span class="math display">\[
\frac{1}{N} \sum_{i=1}^{N} \dot{\boldsymbol{X}}_{i}^{\prime} \dot{\boldsymbol{X}}_{i} \underset{p}{\longrightarrow} \mathbb{E}\left[\dot{\boldsymbol{X}}_{i}^{\prime} \dot{\boldsymbol{X}}_{i}\right]=\boldsymbol{Q}_{T} .
\]</span></p>
<p>The random vectors <span class="math inline">\(\dot{\boldsymbol{X}}_{i}^{\prime} \boldsymbol{\varepsilon}_{i}\)</span> are i.i.d. The matrix <span class="math inline">\(\dot{\boldsymbol{X}}_{i}\)</span> is a function of <span class="math inline">\(\left(\boldsymbol{X}_{i}, \boldsymbol{s}_{i}\right)\)</span> only. Assumption 17.3.3 and the law of iterated expectations implies</p>
<p><span class="math display">\[
\mathbb{E}\left[\dot{\boldsymbol{X}}_{i}^{\prime} \boldsymbol{\varepsilon}_{i}\right]=\mathbb{E}\left[\dot{\boldsymbol{X}}_{i}^{\prime} \mathbb{E}\left[\boldsymbol{\varepsilon}_{i} \mid \boldsymbol{X}_{i}, \boldsymbol{s}_{i}\right]\right]=0 .
\]</span></p>
<p>so that <span class="math inline">\(\dot{\boldsymbol{X}}_{i}^{\prime} \varepsilon_{i}\)</span> is mean zero. Assumptions 17.3.5 and 17.3.6 and the fact that <span class="math inline">\(\boldsymbol{s}_{i}\)</span> is bounded implies that <span class="math inline">\(\dot{\boldsymbol{X}}_{i}^{\prime} \varepsilon_{i}\)</span> has a finite covariance matrix, which is <span class="math inline">\(\Omega_{T}\)</span>. The assumptions for the CLT hold, thus</p>
<p><span class="math display">\[
\frac{1}{\sqrt{N}} \sum_{i=1}^{N} \dot{\boldsymbol{X}}_{i}^{\prime} \varepsilon_{i} \underset{d}{\longrightarrow} \mathrm{N}\left(0, \Omega_{T}\right)
\]</span></p>
<p>Together we obtain the stated result.</p>
</section>
<section id="heteroskedasticity-robust-covariance-matrix-estimation" class="level2" data-number="16.22">
<h2 data-number="16.22" class="anchored" data-anchor-id="heteroskedasticity-robust-covariance-matrix-estimation"><span class="header-section-number">16.22</span> Heteroskedasticity-Robust Covariance Matrix Estimation</h2>
<p>We have introduced two covariance matrix estimators for the fixed effects estimator. The classical estimator (17.36) is appropriate for the case where the idiosyncratic errors <span class="math inline">\(\varepsilon_{i t}\)</span> are homoskedastic and serially uncorrelated. The cluster-robust estimator (17.38) allows for heteroskedasticity and arbitrary serial correlation. In this and the following section we consider the intermediate case where <span class="math inline">\(\varepsilon_{i t}\)</span> is heteroskedastic but serially uncorrelated.</p>
<p>Assume that (17.18) and (17.26) hold but not necessarily (17.25). Define the conditional variances</p>
<p><span class="math display">\[
\mathbb{E}\left[\varepsilon_{i t}^{2} \mid \boldsymbol{X}_{i}\right]=\sigma_{i t}^{2} .
\]</span></p>
<p>Then <span class="math inline">\(\Sigma_{i}=\mathbb{E}\left[\boldsymbol{\varepsilon}_{i} \boldsymbol{\varepsilon}_{i}^{\prime} \mid \boldsymbol{X}_{i}\right]=\operatorname{diag}\left(\sigma_{i t}^{2}\right)\)</span>. The covariance matrix (17.24) can be written as</p>
<p><span class="math display">\[
\boldsymbol{V}_{\mathrm{fe}}=\left(\dot{\boldsymbol{X}}^{\prime} \dot{\boldsymbol{X}}\right)^{-1}\left(\sum_{i=1}^{N} \sum_{t \in S_{i}} \dot{X}_{i t} \dot{X}_{i t}^{\prime} \sigma_{i t}^{2}\right)\left(\dot{\boldsymbol{X}}^{\prime} \dot{\boldsymbol{X}}\right)^{-1}
\]</span></p>
<p>A natural estimator of <span class="math inline">\(\sigma_{i t}^{2}\)</span> is <span class="math inline">\(\widehat{\varepsilon}_{i t}^{2}\)</span>. Replacing <span class="math inline">\(\sigma_{i t}^{2}\)</span> with <span class="math inline">\(\widehat{\varepsilon}_{i t}^{2}\)</span> in (17.56) and making a degree-of-freedom adjustment we obtain a White-type covariance matrix estimator</p>
<p><span class="math display">\[
\widehat{\boldsymbol{V}}_{\mathrm{fe}}=\frac{n}{n-N-k}\left(\dot{\boldsymbol{X}}^{\prime} \dot{\boldsymbol{X}}\right)^{-1}\left(\sum_{i=1}^{N} \sum_{t \in S_{i}} \dot{X}_{i t} \dot{X}_{i t}^{\prime} \widehat{\varepsilon}_{i t}^{2}\right)\left(\dot{\boldsymbol{X}}^{\prime} \dot{\boldsymbol{X}}\right)^{-1} .
\]</span></p>
<p>Following the insight of White (1980) it may seem appropriate to expect <span class="math inline">\(\widehat{\boldsymbol{V}}_{\text {fe }}\)</span> to be a reasonable estimator of <span class="math inline">\(\boldsymbol{V}_{\text {fe. }}\)</span>. Unfortunately this is not the case as discovered by Stock and Watson (2008). The problem is that <span class="math inline">\(\widehat{\boldsymbol{V}}_{\mathrm{fe}}\)</span> is a function of the individual-specific means <span class="math inline">\(\bar{\varepsilon}_{i}\)</span> which are negligible only if the number of time series observations <span class="math inline">\(T_{i}\)</span> are large.</p>
<p>We can see this by a simple bias calculation. Assume that the sample is balanced and that the residuals are constructed with the true <span class="math inline">\(\beta\)</span>. Then</p>
<p><span class="math display">\[
\widehat{\varepsilon}_{i t}=\dot{\varepsilon}_{i t}=\varepsilon_{i t}-\frac{1}{T} \sum_{t=1}^{T} \varepsilon_{i j} .
\]</span></p>
<p>Using (17.26) and (17.55)</p>
<p><span class="math display">\[
\mathbb{E}\left[\widehat{\varepsilon}_{i t}^{2} \mid \boldsymbol{X}_{i}\right]=\left(\frac{T-2}{T}\right) \sigma_{i t}^{2}+\frac{\bar{\sigma}_{i}^{2}}{T}
\]</span></p>
<p>where <span class="math inline">\(\bar{\sigma}_{i}^{2}=T^{-1} \sum_{t=1}^{T} \sigma_{i t}^{2}\)</span>. (See Exercise 17.10.) Using (17.57) and setting <span class="math inline">\(k=0\)</span> we obtain</p>
<p><span class="math display">\[
\begin{aligned}
\mathbb{E}\left[\widehat{\boldsymbol{V}}_{\mathrm{fe}} \mid \boldsymbol{X}\right] &amp;=\frac{T}{T-1}\left(\dot{\boldsymbol{X}}^{\prime} \dot{\boldsymbol{X}}\right)^{-1}\left(\sum_{i=1}^{N} \sum_{t \in S_{i}} \dot{X}_{i t} \dot{X}_{i t}^{\prime} \mathbb{E}\left[\widehat{\varepsilon}_{i t}^{2} \mid \boldsymbol{X}_{i}\right]\right)\left(\dot{\boldsymbol{X}}^{\prime} \dot{\boldsymbol{X}}\right)^{-1} \\
&amp;=\left(\frac{T-2}{T-1}\right) \boldsymbol{V}_{\mathrm{fe}}+\frac{1}{T-1}\left(\dot{\boldsymbol{X}}^{\prime} \dot{\boldsymbol{X}}\right)^{-1}\left(\sum_{i=1}^{N} \dot{\boldsymbol{X}}_{i}^{\prime} \dot{\boldsymbol{X}}_{i} \bar{\sigma}_{i}^{2}\right)\left(\dot{\boldsymbol{X}}^{\prime} \dot{\boldsymbol{X}}\right)^{-1} .
\end{aligned}
\]</span></p>
<p>Thus <span class="math inline">\(\widehat{\boldsymbol{V}}_{\mathrm{fe}}\)</span> is biased of order <span class="math inline">\(O\left(T^{-1}\right)\)</span>. Unless <span class="math inline">\(T \rightarrow \infty\)</span> this bias will persist as <span class="math inline">\(N \rightarrow \infty . \widehat{\boldsymbol{V}}_{\mathrm{fe}}\)</span> is unbiased in two contexts. The first is when the errors <span class="math inline">\(\varepsilon_{i t}\)</span> are homoskedastic. The second is when <span class="math inline">\(T=2\)</span>. (To show the latter requires some algebra so is omitted.)</p>
<p>To correct the bias for the case <span class="math inline">\(T&gt;2\)</span>, Stock and Watson (2008) proposed the estimator</p>
<p><span class="math display">\[
\begin{aligned}
\widetilde{\boldsymbol{V}}_{\mathrm{fe}} &amp;=\left(\frac{T-1}{T-2}\right) \widehat{\boldsymbol{V}}_{\mathrm{fe}}-\frac{1}{T-1} \widehat{\boldsymbol{B}}_{\mathrm{fe}} \\
\widehat{\boldsymbol{B}}_{\mathrm{fe}} &amp;=\left(\dot{\boldsymbol{X}}^{\prime} \dot{\boldsymbol{X}}\right)^{-1}\left(\sum_{i=1}^{N} \dot{\boldsymbol{X}}_{i}^{\prime} \dot{\boldsymbol{X}}_{i} \widehat{\sigma}_{i}^{2}\right)\left(\dot{\boldsymbol{X}}^{\prime} \dot{\boldsymbol{X}}\right)^{-1} \\
\widehat{\sigma}_{i}^{2} &amp;=\frac{1}{T-1} \sum_{t=1}^{T} \widehat{\varepsilon}_{i t}^{2} .
\end{aligned}
\]</span></p>
<p>You can check that <span class="math inline">\(\mathbb{E}\left[\widehat{\sigma}_{i}^{2} \mid \boldsymbol{X}_{i}\right]=\bar{\sigma}_{i}^{2}\)</span> and <span class="math inline">\(\mathbb{E}\left[\widetilde{\boldsymbol{V}}_{\text {fe }} \mid \boldsymbol{X}_{i}\right]=\boldsymbol{V}_{\text {fe }}\)</span> so <span class="math inline">\(\widetilde{\boldsymbol{V}}_{\text {fe }}\)</span> is unbiased for <span class="math inline">\(\boldsymbol{V}_{\text {fe }}\)</span>. (See Exercise 17.11.)</p>
<p>Stock and Watson (2008) show that <span class="math inline">\(\widetilde{\boldsymbol{V}}_{\text {fe }}\)</span> is consistent with <span class="math inline">\(T\)</span> fixed and <span class="math inline">\(N \rightarrow \infty\)</span>. In simulations they show that <span class="math inline">\(\widetilde{\boldsymbol{V}}_{\mathrm{fe}}\)</span> has excellent performance.</p>
<p>Because of the Stock-Watson analysis Stata no longer calculates the heteroskedasticity-robust covariance matrix estimator <span class="math inline">\(\widehat{\boldsymbol{V}}_{\mathrm{fe}}\)</span> when the fixed effects estimator is calculated using the xtreg command. Instead, the cluster-robust estimator <span class="math inline">\(\widehat{\boldsymbol{V}}_{\mathrm{fe}}^{\text {cluster }}\)</span> is reported when robust standard errors are requested. However, fixed effects is often implemented using the areg command which reports the biased estimator <span class="math inline">\(\widehat{\boldsymbol{V}}_{\mathrm{fe}}\)</span> if robust standard errors are requested. These leads to the practical recommendation that areg should be used with the cluster(id) option.</p>
<p>At present the corrected estimator (17.58) has not been programmed as a Stata option.</p>
</section>
<section id="heteroskedasticity-robust-estimation---unbalanced-case" class="level2" data-number="16.23">
<h2 data-number="16.23" class="anchored" data-anchor-id="heteroskedasticity-robust-estimation---unbalanced-case"><span class="header-section-number">16.23</span> Heteroskedasticity-Robust Estimation - Unbalanced Case</h2>
<p>A limitation with the bias-corrected robust covariance matrix estimator of Stock and Watson (2008) is that it was only derived for balanced panels. In this section we generalize their estimator to cover unbalanced panels.</p>
<p>The estimator is</p>
<p><span class="math display">\[
\begin{aligned}
&amp;\widetilde{\boldsymbol{V}}_{\mathrm{fe}}=\left(\dot{\boldsymbol{X}}^{\prime} \dot{\boldsymbol{X}}\right)^{-1} \widetilde{\Omega}_{\mathrm{fe}}\left(\dot{\boldsymbol{X}}^{\prime} \dot{\boldsymbol{X}}\right)^{-1} \\
&amp;\widetilde{\Omega}_{\mathrm{fe}}=\sum_{i=1}^{N} \sum_{t \in S_{i}} \dot{X}_{i t} \dot{X}_{i t}^{\prime}\left[\left(\frac{T_{i} \widehat{\varepsilon}_{i t}^{2}-\widehat{\sigma}_{i}^{2}}{T_{i}-2}\right) \mathbb{1}\left\{T_{i}&gt;2\right\}+\left(\frac{T_{i} \widehat{\varepsilon}_{i t}^{2}}{T_{i}-1}\right) \mathbb{1}\left\{T_{i}=2\right\}\right]
\end{aligned}
\]</span></p>
<p>where</p>
<p><span class="math display">\[
\widehat{\sigma}_{i}^{2}=\frac{1}{T_{i}-1} \sum_{t \in S_{i}} \widehat{\varepsilon}_{i t}^{2} .
\]</span></p>
<p>To justify this estimator, as in the previous section make the simplifying assumption that the residuals are constructed with the true <span class="math inline">\(\beta\)</span>. We calculate that</p>
<p><span class="math display">\[
\begin{aligned}
&amp;\mathbb{E}\left[\widehat{\varepsilon}_{i t}^{2} \mid \boldsymbol{X}_{i}\right]=\left(\frac{T_{i}-2}{T_{i}}\right) \sigma_{i t}^{2}+\frac{\bar{\sigma}_{i}^{2}}{T_{i}} \\
&amp;\mathbb{E}\left[\widehat{\sigma}_{i}^{2} \mid \boldsymbol{X}_{i}\right]=\bar{\sigma}_{i}^{2} .
\end{aligned}
\]</span></p>
<p>You can show that under these assumptions, <span class="math inline">\(\mathbb{E}\left[\widetilde{\boldsymbol{V}}_{\mathrm{fe}} \mid \boldsymbol{X}\right]=\boldsymbol{V}_{\mathrm{fe}}\)</span> and thus <span class="math inline">\(\widetilde{\boldsymbol{V}}_{\mathrm{fe}}\)</span> is unbiased for <span class="math inline">\(\boldsymbol{V}_{\mathrm{fe}}\)</span>. (See Exercise 17.12.)</p>
<p>In balanced panels the estimator <span class="math inline">\(\widetilde{\boldsymbol{V}}_{\mathrm{fe}}\)</span> simplifies to the Stock-Watson estimator (with <span class="math inline">\(k=0\)</span> ).</p>
</section>
<section id="hausman-test-for-random-vs-fixed-effects" class="level2" data-number="16.24">
<h2 data-number="16.24" class="anchored" data-anchor-id="hausman-test-for-random-vs-fixed-effects"><span class="header-section-number">16.24</span> Hausman Test for Random vs Fixed Effects</h2>
<p>The random effects model is a special case of the fixed effects model. Thus we can test the null hypothesis of random effects against the alternative of fixed effects. The Hausman test is typically used for this purpose. The statistic is a quadratic in the difference between the fixed effects and random effects estimators. The statistic is</p>
<p><span class="math display">\[
\begin{aligned}
H &amp;=\left(\widehat{\beta}_{\mathrm{fe}}-\widehat{\beta}_{\mathrm{re}}\right)^{\prime} \widehat{\operatorname{var}}\left[\widehat{\beta}_{\mathrm{fe}}-\widehat{\beta}_{\mathrm{re}}\right]^{-1}\left(\widehat{\beta}_{\mathrm{fe}}-\widehat{\beta}_{\mathrm{re}}\right) \\
&amp;=\left(\widehat{\beta}_{\mathrm{fe}}-\widehat{\beta}_{\mathrm{re}}\right)^{\prime}\left(\widehat{\boldsymbol{V}}_{\mathrm{fe}}-\widehat{\boldsymbol{V}}_{\mathrm{re}}\right)^{-1}\left(\widehat{\beta}_{\mathrm{fe}}-\widehat{\beta}_{\mathrm{re}}\right)
\end{aligned}
\]</span></p>
<p>where both <span class="math inline">\(\widehat{\boldsymbol{V}}_{\mathrm{fe}}\)</span> and <span class="math inline">\(\widehat{\boldsymbol{V}}_{\mathrm{re}}\)</span> take the classical (non-robust) form.</p>
<p>The test can be implemented on a subset of the coefficients <span class="math inline">\(\beta\)</span>. In particular this needs to be done if the regressors <span class="math inline">\(X_{i t}\)</span> contain time-invariant elements so that the random effects estimator contains more coefficients than the fixed effects estimator. In this case the test should be implemented only on the coefficients on the time-varying regressors.</p>
<p>An asymptotic <span class="math inline">\(100 \alpha %\)</span> test rejects if <span class="math inline">\(H\)</span> exceeds the <span class="math inline">\(1-\alpha^{t h}\)</span> quantile of the <span class="math inline">\(\chi_{k}^{2}\)</span> distribution where <span class="math inline">\(k=\)</span> <span class="math inline">\(\operatorname{dim}(\beta)\)</span>. If the test rejects this is evidence that the individual effect <span class="math inline">\(u_{i}\)</span> is correlated with the regressors so the random effects model is not appropriate. On the other hand if the test fails to reject this evidence says that the random effects hypothesis cannot be rejected.</p>
<p>It is tempting to use the Hausman test to select whether to use the fixed effects or random effects estimator. One could imagine using the random effects estimator if the Hausman test fails to reject the random effects hypothesis and using the fixed effects estimator otherwise. This is not, however, a wise approach. This procedure - selecting an estimator based on a test - is known as a pretest estimator and is biased. The bias arises because the result of the test is random and correlated with the estimators.</p>
<p>Instead, the Hausman test can be used as a specification test. If you are planning to use the random effects estimator (and believe that the random effects assumptions are appropriate in your context) the Hausman test can be used to check this assumption and provide evidence to support your approach.</p>
</section>
<section id="random-effects-or-fixed-effects" class="level2" data-number="16.25">
<h2 data-number="16.25" class="anchored" data-anchor-id="random-effects-or-fixed-effects"><span class="header-section-number">16.25</span> Random Effects or Fixed Effects?</h2>
<p>We have presented the random effects and fixed effects estimators of the regression coefficients. Which should be used in practice? How should we view the difference?</p>
<p>The basic distinction is that the random effects estimator requires the individual error <span class="math inline">\(u_{i}\)</span> to satisfy the conditional mean assumption (17.8). The fixed effects estimator does not require (17.8) and is robust to its violation. In particular, the individual effect <span class="math inline">\(u_{i}\)</span> can be arbitrarily correlated with the regressors. On the other hand the random effects estimator is efficient under random effects (Assumption 17.1). Current econometric practice is to prefer robustness over efficiency. Consequently, current practice is (nearly uniformly) to use the fixed effects estimator for linear panel data models. Random effects estimators are only used in contexts where fixed effects estimation is unknown or challenging (which occurs in many nonlinear models).</p>
<p>The labels “random effects” and “fixed effects” are misleading. These are labels which arose in the early literature and we are stuck with these labels today. In a previous era regressors were viewed as “fixed”. Viewing the individual effect as an unobserved regressor leads to the label of the individual effect as “fixed”. Today, we rarely refer to regressors as “fixed” when dealing with observational data. We view all variables as random. Consequently describing <span class="math inline">\(u_{i}\)</span> as “fixed” does not make much sense and it is hardly a contrast with the “random effect” label since under either assumption <span class="math inline">\(u_{i}\)</span> is treated as random. Once again, the labels are unfortunate but the key difference is whether <span class="math inline">\(u_{i}\)</span> is correlated with the regressors.</p>
</section>
<section id="time-trends" class="level2" data-number="16.26">
<h2 data-number="16.26" class="anchored" data-anchor-id="time-trends"><span class="header-section-number">16.26</span> Time Trends</h2>
<p>In general we expect that economic agents will experience common shocks during the same time period. For example, business cycle fluctations, inflation, and interest rates affect all agents in the economy. Therefore it is often desirable to include time effects in a panel regression model.</p>
<p>The simplest specification is a linear time trend</p>
<p><span class="math display">\[
Y_{i t}=X_{i t}^{\prime} \beta+\gamma t+u_{i}+\varepsilon_{i t} .
\]</span></p>
<p>For a introduction to time trends see Section 14.42. More flexible specifications (such as a quadratic) can also be used. For estimation it is appropriate to include the time trend <span class="math inline">\(t\)</span> as an element of the regressor vector <span class="math inline">\(X_{i t}\)</span> and then apply fixed effects.</p>
<p>In some cases the time trends may be individual-specific. Series may be growing or declining at different rates. A linear time trend specification only extracts a common time trend. To allow for individualspecific time trends we need to include an interaction effect. This can be written as</p>
<p><span class="math display">\[
Y_{i t}=X_{i t}^{\prime} \beta+\gamma_{i} t+u_{i}+\varepsilon_{i t} .
\]</span></p>
<p>In a fixed effects specification the coefficients <span class="math inline">\(\left(\gamma_{i}, u_{i}\right)\)</span> are treated as possibly correlated with the regressors. To eliminate them from the model we treat them as unknown parameters and estimate all by least squares. By the FWL theorem the estimator for <span class="math inline">\(\beta\)</span> equals least squares of <span class="math inline">\(\dot{\boldsymbol{Y}}\)</span> on <span class="math inline">\(\dot{\boldsymbol{X}}\)</span> where their elements are the residuals from the least squares regressions on a linear time trend fit separately for each individual and variable.</p>
</section>
<section id="two-way-error-components" class="level2" data-number="16.27">
<h2 data-number="16.27" class="anchored" data-anchor-id="two-way-error-components"><span class="header-section-number">16.27</span> Two-Way Error Components</h2>
<p>In the previous section we discussed inclusion of time trends and individual-specific time trends. The functional forms imposed by linear time trends are restrictive. There is no economic reason to expect the “trend” of a series to be linear. Business cycle “trends” are cyclic. This suggests that it is desirable to be more flexible than a linear (or polynomial) specifications. In this section we consider the most flexible specification where the trend is allowed to take any arbitrary shape but will require that it is common rather than individual-specific.</p>
<p>The model we consider is the two-way error component model</p>
<p><span class="math display">\[
Y_{i t}=X_{i t}^{\prime} \beta+v_{t}+u_{i}+\varepsilon_{i t} .
\]</span></p>
<p>In this model <span class="math inline">\(u_{i}\)</span> is an unobserved individual-specific effect, <span class="math inline">\(v_{t}\)</span> is an unobserved time-specific effect, and <span class="math inline">\(\varepsilon_{i t}\)</span> is an idiosyncratic error.</p>
<p>The two-way model (17.63) can be handled either using random effects or fixed effects. In a random effects framework the errors <span class="math inline">\(v_{t}\)</span> and <span class="math inline">\(u_{i}\)</span> are modeled as in Assumption 17.1. When the panel is balanced the covariance matrix of the error vector <span class="math inline">\(\boldsymbol{e}=v \otimes \mathbf{1}_{N}+\mathbf{1}_{T} \otimes u+\boldsymbol{\varepsilon}\)</span> is</p>
<p><span class="math display">\[
\operatorname{var}[\boldsymbol{e}]=\Omega=\left(\boldsymbol{I}_{T} \otimes \mathbf{1}_{N} \mathbf{1}_{N}^{\prime}\right) \sigma_{v}^{2}+\left(\mathbf{1}_{T} \mathbf{1}_{T}^{\prime} \otimes \boldsymbol{I}_{N}\right) \sigma_{u}^{2}+\boldsymbol{I}_{n} \sigma_{\varepsilon}^{2} .
\]</span></p>
<p>When the panel is unbalanced a similar but cumbersome expression for (17.64) can be derived. This variance (17.64) can be used for GLS estimation of <span class="math inline">\(\beta\)</span>.</p>
<p>More typically (17.63) is handled using fixed effects. The two-way within transformation subtracts both individual-specific means and time-specific means to eliminate both <span class="math inline">\(v_{t}\)</span> and <span class="math inline">\(u_{i}\)</span> from the two-way model (17.63). For a variable <span class="math inline">\(Y_{i t}\)</span> we define the time-specific mean as follows. Let <span class="math inline">\(S_{t}\)</span> be the set of individuals <span class="math inline">\(i\)</span> for which the observation <span class="math inline">\(t\)</span> is included in the sample and let <span class="math inline">\(N_{t}\)</span> be the number of these individuals. Then the time-specific mean at time <span class="math inline">\(t\)</span> is</p>
<p><span class="math display">\[
\widetilde{Y}_{t}=\frac{1}{N_{t}} \sum_{i \in S_{t}} Y_{i t} .
\]</span></p>
<p>This is the average across all values of <span class="math inline">\(Y_{i t}\)</span> observed at time <span class="math inline">\(t\)</span>.</p>
<p>For the case of balanced panels the two-way within transformation is</p>
<p><span class="math display">\[
\ddot{Y}_{i t}=Y_{i t}-\bar{Y}_{i}-\widetilde{Y}_{t}+\bar{Y}
\]</span></p>
<p>where <span class="math inline">\(\bar{Y}=n^{-1} \sum_{i=1}^{N} \sum_{t=1}^{T} Y_{i t}\)</span> is the full-sample mean. If <span class="math inline">\(Y_{i t}\)</span> satisfies the two-way component model</p>
<p><span class="math display">\[
Y_{i t}=v_{t}+u_{i}+\varepsilon_{i t}
\]</span></p>
<p>then <span class="math inline">\(\bar{Y}_{i}=\bar{v}+u_{i}+\bar{\varepsilon}_{i}, \widetilde{Y}_{t}=v_{t}+\bar{u}+\widetilde{\varepsilon}_{t}\)</span> and <span class="math inline">\(\bar{Y}=\bar{v}+\bar{u}+\bar{\varepsilon}\)</span>. Hence</p>
<p><span class="math display">\[
\begin{aligned}
\ddot{Y}_{i t} &amp;=v_{t}+u_{i}+\varepsilon_{i t}-\left(\bar{v}+u_{i}+\bar{\varepsilon}_{i}\right)-\left(v_{t}+\bar{u}+\widetilde{\varepsilon}_{t}\right)+\bar{v}+\bar{u}+\bar{\varepsilon} \\
&amp;=\varepsilon_{i t}-\bar{\varepsilon}_{i}-\widetilde{\varepsilon}_{t}+\bar{\varepsilon}=\ddot{\varepsilon}_{i t}
\end{aligned}
\]</span></p>
<p>so the individual and time effects are eliminated.</p>
<p>The two-way within transformation applied to (17.63) yields</p>
<p><span class="math display">\[
\ddot{Y}_{i t}=\ddot{X}_{i t}^{\prime} \beta+\ddot{\varepsilon}_{i t}
\]</span></p>
<p>which is invariant to both <span class="math inline">\(v_{t}\)</span> and <span class="math inline">\(u_{i}\)</span>. The two-way within estimator is least squares applied to (17.66).</p>
<p>For the unbalanced case there are two computational approaches to implement the estimator. Both are based on the realization that the estimator is equivalent to including dummy variables for all time periods. Let <span class="math inline">\(\tau_{t}\)</span> be a set of <span class="math inline">\(T\)</span> dummy variables where the <span class="math inline">\(t^{t h}\)</span> indicates the <span class="math inline">\(t^{t h}\)</span> time period. Thus the <span class="math inline">\(t^{t h}\)</span> element of <span class="math inline">\(\tau_{t}\)</span> is 1 and the remaining elements are zero. Set <span class="math inline">\(v=\left(\nu_{1}, \ldots, \nu_{T}\right)^{\prime}\)</span> as the vector of time fixed effects. Notice that <span class="math inline">\(v_{t}=\tau_{t}^{\prime} \nu\)</span>. We can write the two-way model as</p>
<p><span class="math display">\[
Y_{i t}=X_{i t}^{\prime} \beta+\tau_{t}^{\prime} \nu+u_{i}+\varepsilon_{i t} .
\]</span></p>
<p>This is the dummy variable representation of the two-way error components model.</p>
<p>Model (17.67) can be estimated by one-way fixed effects with regressors <span class="math inline">\(X_{i t}\)</span> and <span class="math inline">\(\tau_{t}\)</span> and coefficient vectors <span class="math inline">\(\beta\)</span> and <span class="math inline">\(\nu\)</span>. This can be implemented by standard one-way fixed effects methods including xtreg or areg in Stata. This produces estimates of the slopes <span class="math inline">\(\beta\)</span> as well as the time effects <span class="math inline">\(\nu\)</span>. To achieve identification one time dummy variable is omitted from <span class="math inline">\(\tau_{t}\)</span> so the estimated time effects are all relative to this baseline time period. This is the most common method in practice to estimate a two-way fixed effects model. As the number of time periods is typically modest this is a computationally attractive approach.</p>
<p>The second computational approach is to eliminate the time effects by residual regression. This is done by the following steps. First, subtract individual-specific means for (17.67). This yields</p>
<p><span class="math display">\[
\dot{Y}_{i t}=\dot{X}_{i t}^{\prime} \beta+\dot{\tau}_{t}^{\prime} v+\dot{\varepsilon}_{i t} .
\]</span></p>
<p>Second, regress <span class="math inline">\(\dot{Y}_{i t}\)</span> on <span class="math inline">\(\dot{\tau}_{t}\)</span> to obtain a residual <span class="math inline">\(\ddot{Y}_{i t}\)</span> and regress each element of <span class="math inline">\(\dot{X}_{i t}\)</span> on <span class="math inline">\(\dot{\tau}_{t}\)</span> to obtain a residual <span class="math inline">\(\ddot{X}_{i t}\)</span>. Third, regress <span class="math inline">\(\ddot{Y}_{i t}\)</span> on <span class="math inline">\(\ddot{X}_{i t}\)</span> to obtain the within estimator of <span class="math inline">\(\beta\)</span>. These steps eliminate the fixed effects <span class="math inline">\(v_{t}\)</span> so the estimator is invariant to their value. What is important about this two-step procedure is that the second step is not a within transformation across the time index but rather standard regression.</p>
<p>If the two-way within estimator is used then the regressors <span class="math inline">\(X_{i t}\)</span> cannot include any time-invariant variables <span class="math inline">\(X_{i}\)</span> or common time series variables <span class="math inline">\(X_{t}\)</span>. Both are eliminated by the two-way within transformation. Coefficients are only identified for regressors which have variation both across individuals and across time.</p>
<p>If desired, the relevance of the time effects can be tested by an exclusion test on the coefficients <span class="math inline">\(\nu\)</span>. If the test rejects the hypothesis of zero coefficients then this indicates that the time effects are relevant in the regression model.</p>
<p>The fixed effects estimator of (17.63) is invariant to the values of <span class="math inline">\(v_{t}\)</span> and <span class="math inline">\(u_{i}\)</span>, thus no assumptions need to be made concerning their stochastic properties.</p>
<p>To illustrate, the fourth column of Table <span class="math inline">\(17.2\)</span> presents fixed effects estimates of the investment equation, augmented to included year dummy indicators, and is thus a two-way fixed effects model. In this example the coefficient estimates and standard errors are not greatly affected by the inclusion of the year dummy variables.</p>
</section>
<section id="instrumental-variables" class="level2" data-number="16.28">
<h2 data-number="16.28" class="anchored" data-anchor-id="instrumental-variables"><span class="header-section-number">16.28</span> Instrumental Variables</h2>
<p>Take the fixed effects model</p>
<p><span class="math display">\[
Y_{i t}=X_{i t}^{\prime} \beta+u_{i}+\varepsilon_{i t} .
\]</span></p>
<p>We say <span class="math inline">\(X_{i t}\)</span> is exogenous for <span class="math inline">\(\varepsilon_{i t}\)</span> if <span class="math inline">\(\mathbb{E}\left[X_{i t} \varepsilon_{i t}\right]=0\)</span>, and we say <span class="math inline">\(X_{i t}\)</span> is endogenous for <span class="math inline">\(\varepsilon_{i t}\)</span> if <span class="math inline">\(\mathbb{E}\left[X_{i t} \varepsilon_{i t}\right] \neq 0\)</span>. In Chapter 12 we discussed several economic examples of endogeneity and the same issues apply in the panel data context. The primary difference is that in the fixed effects model we only need to be concerned if the regressors are correlated with the idiosyncratic error <span class="math inline">\(\varepsilon_{i t}\)</span>, as correlation between <span class="math inline">\(X_{i t}\)</span> and <span class="math inline">\(u_{i}\)</span> is allowed.</p>
<p>As in Chapter 12 if the regressors are endogenous the fixed effects estimator will be biased and inconsistent for the structural coefficient <span class="math inline">\(\beta\)</span>. The standard approach to handling endogeneity is to specify instrumental variables <span class="math inline">\(Z_{i t}\)</span> which are both relevant (correlated with <span class="math inline">\(X_{i t}\)</span> ) yet exogenous (uncorrelated with <span class="math inline">\(\varepsilon_{i t}\)</span> ).</p>
<p>Let <span class="math inline">\(Z_{i t}\)</span> be an <span class="math inline">\(\ell \times 1\)</span> instrumental variable where <span class="math inline">\(\ell \geq k\)</span>. As in the cross-section case, <span class="math inline">\(Z_{i t}\)</span> may contain both included exogenous variables (variables in <span class="math inline">\(X_{i t}\)</span> that are exogenous) and excluded exogenous variables (variables not in <span class="math inline">\(X_{i t}\)</span> ). Let <span class="math inline">\(\boldsymbol{Z}_{i}\)</span> be the stacked instruments by individual and <span class="math inline">\(\boldsymbol{Z}\)</span> be the stacked instruments for the full sample.</p>
<p>The dummy variable formulation of the fixed effects model is <span class="math inline">\(Y_{i t}=X_{i t}^{\prime} \beta+d_{i}^{\prime} u+\varepsilon_{i t}\)</span> where <span class="math inline">\(d_{i}\)</span> is an <span class="math inline">\(N \times 1\)</span> vector of dummy variables, one for each individual in the sample. The model in matrix notation for the full sample is</p>
<p><span class="math display">\[
\boldsymbol{Y}=\boldsymbol{X} \beta+\boldsymbol{D} u+\boldsymbol{\varepsilon} .
\]</span></p>
<p>Theorem <span class="math inline">\(17.1\)</span> shows that the fixed effects estimator for <span class="math inline">\(\beta\)</span> can be calculated by least squares estimation of (17.69). Thus the dummies <span class="math inline">\(\boldsymbol{D}\)</span> should be viewed as included exogenous variables. Consider 2SLS estimation of <span class="math inline">\(\beta\)</span> using the instruments <span class="math inline">\(\boldsymbol{Z}\)</span> for <span class="math inline">\(\boldsymbol{X}\)</span>. Since <span class="math inline">\(\boldsymbol{D}\)</span> is an included exogenous variable it should also be used as an instrument. Thus 2SLS estimation of the fixed effects model (17.68) is algebraically 2SLS of the regression (17.69) of <span class="math inline">\(\boldsymbol{Y}\)</span> on <span class="math inline">\((\boldsymbol{X}, \boldsymbol{D})\)</span> using the pair <span class="math inline">\((\boldsymbol{Z}, \boldsymbol{D})\)</span> as instruments.</p>
<p>Since the dimension of <span class="math inline">\(\boldsymbol{D}\)</span> can be excessively large, as discussed in Section 17.11, it is advisable to use residual regression to compute the 2SLS estimator as we now describe.</p>
<p>In Section 12.12, we described several alternative representations for the 2SLS estimator. The fifth (equation (12.32)) shows that the 2SLS estimator for <span class="math inline">\(\beta\)</span> equals</p>
<p><span class="math display">\[
\widehat{\beta}_{2 \text { sls }}=\left(\boldsymbol{X}^{\prime} \boldsymbol{M}_{\boldsymbol{D}} \boldsymbol{Z}\left(\boldsymbol{Z}^{\prime} \boldsymbol{M}_{\boldsymbol{D}} \boldsymbol{Z}\right)^{-1} \boldsymbol{Z}^{\prime} \boldsymbol{M}_{\boldsymbol{D}} \boldsymbol{X}\right)^{-1}\left(\boldsymbol{X}^{\prime} \boldsymbol{M}_{\boldsymbol{D}} \boldsymbol{Z}\left(\boldsymbol{Z}^{\prime} \boldsymbol{M}_{\boldsymbol{D}} \boldsymbol{Z}\right)^{-1} \boldsymbol{Z}^{\prime} \boldsymbol{M}_{\boldsymbol{D}} \boldsymbol{Y}\right)
\]</span></p>
<p>where <span class="math inline">\(\boldsymbol{M}_{\boldsymbol{D}}=\boldsymbol{I}_{n}-\boldsymbol{D}\left(\boldsymbol{D}^{\prime} \boldsymbol{D}\right)^{-1} \boldsymbol{D}^{\prime}\)</span>. The latter is the matrix within operator, thus <span class="math inline">\(\boldsymbol{M}_{\boldsymbol{D}} \boldsymbol{Y}=\dot{\boldsymbol{Y}}, \boldsymbol{M}_{\boldsymbol{D}} \boldsymbol{X}=\dot{\boldsymbol{X}}\)</span>, and <span class="math inline">\(\boldsymbol{M}_{\boldsymbol{D}} \boldsymbol{Z}=\dot{Z}\)</span>. It follows that the 2SLS estimator is</p>
<p><span class="math display">\[
\widehat{\beta}_{2 \text { sls }}=\left(\dot{\boldsymbol{X}}^{\prime} \dot{Z}\left(\dot{Z}^{\prime} \dot{Z}\right)^{-1} \dot{Z}^{\prime} \dot{\boldsymbol{X}}\right)^{-1}\left(\dot{\boldsymbol{X}}^{\prime} \dot{Z}\left(\dot{\boldsymbol{Z}}^{\prime} \dot{Z}\right)^{-1} \dot{Z}^{\prime} \dot{\boldsymbol{Y}}\right) .
\]</span></p>
<p>This is convenient. It shows that the 2SLS estimator for the fixed effects model can be calculated by applying 2SLS to the within-transformed <span class="math inline">\(Y_{i t}, X_{i t}\)</span>, and <span class="math inline">\(Z_{i t}\)</span>. The 2SLS residuals are <span class="math inline">\(\widehat{\boldsymbol{e}}=\dot{\boldsymbol{Y}}-\dot{\boldsymbol{X}} \widehat{\beta}_{2 s l s}\)</span>.</p>
<p>This estimator can be obtained using the Stata command xtivreg fe. It can also be obtained using the Stata command ivregress after making the within transformations.</p>
<p>The presentation above focused for clarity on the one-way fixed effects model. There is no substantial change in the two-way fixed effects model</p>
<p><span class="math display">\[
Y_{i t}=X_{i t}^{\prime} \beta+u_{i}+v_{t}+\varepsilon_{i t} .
\]</span></p>
<p>The easiest way to estimate the two-way model is to add <span class="math inline">\(T-1\)</span> time-period dummies to the regression model and include these dummy variables as both regressors and instruments.</p>
</section>
<section id="identification-with-instrumental-variables" class="level2" data-number="16.29">
<h2 data-number="16.29" class="anchored" data-anchor-id="identification-with-instrumental-variables"><span class="header-section-number">16.29</span> Identification with Instrumental Variables</h2>
<p>To understand the identification of the structural slope coefficient <span class="math inline">\(\beta\)</span> in the fixed effects model it is necessary to examine the reduced form equation for the endogenous regressors <span class="math inline">\(X_{i t}\)</span>. This is</p>
<p><span class="math display">\[
X_{i t}=\Gamma Z_{i t}+W_{i}+\zeta_{i t}
\]</span></p>
<p>where <span class="math inline">\(W_{i}\)</span> is a <span class="math inline">\(k \times 1\)</span> vector of fixed effects for the <span class="math inline">\(k\)</span> regressors and <span class="math inline">\(\zeta_{i t}\)</span> is an idiosyncratic error.</p>
<p>The coefficient matrix <span class="math inline">\(\Gamma\)</span> is the linear effect of <span class="math inline">\(Z_{i t}\)</span> on <span class="math inline">\(X_{i t}\)</span> holding the fixed effects <span class="math inline">\(W_{i}\)</span> constant. Thus <span class="math inline">\(\Gamma\)</span> has a similar interpretation as the coefficient <span class="math inline">\(\beta\)</span> in the fixed effects regression model. It is the effect of the variation in <span class="math inline">\(Z_{i t}\)</span> about its individual-specific mean on <span class="math inline">\(X_{i t}\)</span>.</p>
<p>The 2SLS estimator is a function of the within transformed variables. Applying the within transformation to the reduced form we find <span class="math inline">\(\dot{X}_{i t}=\Gamma \dot{Z}_{i t}+\dot{\zeta}_{i t}\)</span>. This shows that <span class="math inline">\(\Gamma\)</span> is the effect of the within-transformed instruments on the regressors. If there is no time-variation in the within-transformed instruments or there is no correlation between the instruments and the regressors after removing the individual-specific means then the coefficient <span class="math inline">\(\Gamma\)</span> will be either not identified or singular. In either case the coefficient <span class="math inline">\(\beta\)</span> will not be identified.</p>
<p>Thus for identification of the fixed effects instrumental variables model we need</p>
<p><span class="math display">\[
\mathbb{E}\left[\dot{Z}_{i}^{\prime} \dot{Z}_{i}\right]&gt;0
\]</span></p>
<p>and</p>
<p><span class="math display">\[
\operatorname{rank}\left(\mathbb{E}\left[\dot{\boldsymbol{Z}}_{i}^{\prime} \dot{\boldsymbol{X}}_{i}\right]\right)=k .
\]</span></p>
<p>Condition (17.70) is the same as the condition for identification in fixed effects regression - the instruments must have full variation after the within transformation. Condition (17.71) is analogous to the relevance condition for identification of instrumental variable regression in the cross-section context but applies to the within-transformed instruments and regressors.</p>
<p>Condition (17.71) shows that to examine instrument validity in the context of fixed effects 2SLS it is important to estimate the reduced form equation using fixed effects (within) regression. Standard tests for instrument validity ( <span class="math inline">\(F\)</span> tests on the excluded instruments) can be applied. However, since the correlation structure of the reduced form equation is in general unknown it is appropriate to use a cluster-robust covariance matrix, clustered at the level of the individual.</p>
</section>
<section id="asymptotic-distribution-of-fixed-effects-2sls-estimator" class="level2" data-number="16.30">
<h2 data-number="16.30" class="anchored" data-anchor-id="asymptotic-distribution-of-fixed-effects-2sls-estimator"><span class="header-section-number">16.30</span> Asymptotic Distribution of Fixed Effects 2SLS Estimator</h2>
<p>In this section we present an asymptotic distribution theory for the fixed effects estimator. We provide a formal theory for the case of balanced panels and discuss an extension to the unbalanced case.</p>
<p>We use the following assumptions for balanced panels.</p>
<p>Assumption $17.4</p>
<ol type="1">
<li><p><span class="math inline">\(Y_{i t}=X_{i t}^{\prime} \beta+u_{i}+\varepsilon_{i t}\)</span> for <span class="math inline">\(i=1, \ldots, N\)</span> and <span class="math inline">\(t=1, \ldots, T\)</span> with <span class="math inline">\(T \geq 2\)</span>.</p></li>
<li><p>The variables <span class="math inline">\(\left(\boldsymbol{\varepsilon}_{i}, \boldsymbol{X}_{i}, \boldsymbol{Z}_{i}\right), i=1, \ldots, N\)</span>, are independent and identically distributed.</p></li>
<li><p><span class="math inline">\(\mathbb{E}\left[Z_{i s} \varepsilon_{i t}\right]=0\)</span> for all <span class="math inline">\(s=1, \ldots, T\)</span>.</p></li>
<li><p><span class="math inline">\(\boldsymbol{Q}_{Z Z}=\mathbb{E}\left[\dot{Z}_{i}^{\prime} \dot{Z}_{i}\right]&gt;0\)</span>.</p></li>
<li><p><span class="math inline">\(\operatorname{rank}\left(\boldsymbol{Q}_{Z X}\right)=k\)</span> where <span class="math inline">\(\boldsymbol{Q}_{Z X}=\mathbb{E}\left[\dot{\boldsymbol{Z}}_{i}^{\prime} \dot{\boldsymbol{X}}_{i}\right]\)</span>.</p></li>
<li><p><span class="math inline">\(\mathbb{E}\left[\varepsilon_{i t}^{4}\right]&lt;\infty\)</span>.</p></li>
<li><p><span class="math inline">\(\mathbb{E}\left\|X_{i t}\right\|^{2}&lt;\infty\)</span>.</p></li>
<li><p><span class="math inline">\(\mathbb{E}\left\|Z_{i t}\right\|^{4}&lt;\infty\)</span>.</p></li>
</ol>
<p>Given Assumption <span class="math inline">\(17.4\)</span> we can establish asymptotic normality for <span class="math inline">\(\widehat{\beta}_{2 s l s}\)</span>.</p>
<p>Theorem 17.4 Under Assumption 17.4, as <span class="math inline">\(N \rightarrow \infty, \sqrt{N}\left(\widehat{\beta}_{2 s l s}-\beta\right) \underset{d}{\longrightarrow} \mathrm{N}\left(0, \boldsymbol{V}_{\beta}\right)\)</span> where</p>
<p><span class="math display">\[
\begin{aligned}
\boldsymbol{V}_{\beta} &amp;=\left(\boldsymbol{Q}_{Z X}^{\prime} \Omega_{Z Z}^{-1} \boldsymbol{Q}_{Z X}\right)^{-1}\left(\boldsymbol{Q}_{Z X}^{\prime} \Omega_{Z Z}^{-1} \Omega_{Z \varepsilon} \Omega_{Z Z}^{-1} \boldsymbol{Q}_{Z X}\right)\left(\boldsymbol{Q}_{Z X}^{\prime} \Omega_{Z Z}^{-1} \boldsymbol{Q}_{Z X}\right)^{-1} \\
\Omega_{Z \varepsilon} &amp;=\mathbb{E}\left[\dot{\boldsymbol{Z}}_{i}^{\prime} \boldsymbol{\varepsilon}_{i} \boldsymbol{\varepsilon}_{i}^{\prime} \dot{\boldsymbol{Z}}_{i}\right] .
\end{aligned}
\]</span></p>
<p>The proof of the result is similar to Theorem <span class="math inline">\(17.2\)</span> so is omitted. The key condition is Assumption 17.4.3, which states that the instruments are strictly exogenous for the idiosyncratic errors. The identification conditions are Assumptions 17.4.4 and 17.4.5, which were discussed in the previous section.</p>
<p>The theorem is stated for balanced panels. For unbalanced panels we can modify the theorem as in Theorem <span class="math inline">\(17.3\)</span> by adding the selection indicators <span class="math inline">\(\boldsymbol{s}_{i}\)</span> and replacing Assumption <span class="math inline">\(17.4 .3\)</span> with <span class="math inline">\(\mathbb{E}\left[\varepsilon_{i t} \mid \boldsymbol{Z}_{i}, \boldsymbol{s}_{i}\right]=\)</span> 0 , which states that the idiosyncratic errors are mean independent of the instruments and selection.</p>
<p>If the idiosyncratic errors <span class="math inline">\(\varepsilon_{i t}\)</span> are homoskedastic and serially uncorrelated then the covariance matrix simplifies to</p>
<p><span class="math display">\[
\boldsymbol{V}_{\beta}=\left(\boldsymbol{Q}_{Z X}^{\prime} \Omega_{Z Z}^{-1} \boldsymbol{Q}_{Z X}\right)^{-1} \sigma_{\varepsilon}^{2} .
\]</span></p>
<p>In this case a classical homoskedastic covariance matrix estimator can be used. Otherwise a clusterrobust covariance matrix estimator can be used, and takes the form</p>
<p><span class="math display">\[
\begin{aligned}
\widehat{\boldsymbol{V}}_{\widehat{\beta}} &amp;=\left(\dot{\boldsymbol{X}}^{\prime} \dot{\boldsymbol{Z}}\left(\dot{\boldsymbol{Z}}^{\prime} \dot{\boldsymbol{Z}}\right)^{-1} \dot{\boldsymbol{Z}}^{\prime} \dot{\boldsymbol{X}}\right)^{-1}\left(\dot{\boldsymbol{X}}^{\prime} \dot{\boldsymbol{Z}}\right)\left(\dot{\boldsymbol{Z}}^{\prime} \dot{\boldsymbol{Z}}\right)^{-1}\left(\sum_{i=1}^{N} \dot{\boldsymbol{Z}}_{i}^{\prime} \widehat{\boldsymbol{\varepsilon}}_{i} \widehat{\boldsymbol{\varepsilon}}_{i}^{\prime} \dot{\boldsymbol{Z}}_{i}\right) \\
&amp; \times\left(\dot{\boldsymbol{Z}}^{\prime} \dot{\boldsymbol{Z}}\right)^{-1}\left(\dot{\boldsymbol{Z}}^{\prime} \dot{\boldsymbol{X}}\right)\left(\dot{\boldsymbol{X}}^{\prime} \dot{\boldsymbol{Z}}\left(\dot{\boldsymbol{Z}}^{\prime} \dot{\boldsymbol{Z}}\right)^{-1} \dot{\boldsymbol{Z}}^{\prime} \dot{\boldsymbol{X}}\right)^{-1}
\end{aligned}
\]</span></p>
<p>As for the case of fixed effects regression, the heteroskedasticity-robust covariance matrix estimator is not recommended due to bias when <span class="math inline">\(T\)</span> is small, and a bias-corrected version has not been developed.</p>
<p>The Stata command xtivreg, fe by default reports the classical homoskedastic covariance matrix estimator. To obtain a cluster-robust covariance matrix use option vce (robust) orvce (cluster id).</p>
</section>
<section id="linear-gmm" class="level2" data-number="16.31">
<h2 data-number="16.31" class="anchored" data-anchor-id="linear-gmm"><span class="header-section-number">16.31</span> Linear GMM</h2>
<p>Consider the just-identified 2SLS estimator. It solves the equation <span class="math inline">\(\dot{\boldsymbol{Z}}^{\prime}(\dot{\boldsymbol{Y}}-\dot{\boldsymbol{X}} \beta)=0\)</span>. These are sample analogs of the population moment condition <span class="math inline">\(\mathbb{E}\left[\dot{\boldsymbol{Z}}_{i}^{\prime}\left(\dot{\boldsymbol{Y}}_{i}-\dot{\boldsymbol{X}}_{i} \beta\right)\right]=0\)</span>. These population conditions hold at the true <span class="math inline">\(\beta\)</span> because <span class="math inline">\(\dot{\boldsymbol{Z}}^{\prime} u=\boldsymbol{Z}^{\prime} \boldsymbol{M D} u=0\)</span> as <span class="math inline">\(u\)</span> lies in the null space of <span class="math inline">\(\boldsymbol{D}\)</span>, and <span class="math inline">\(\mathbb{E}\left[\dot{\boldsymbol{Z}}_{i}^{\prime} \boldsymbol{\varepsilon}\right]=0\)</span> is implied by Assumption 17.4.3.</p>
<p>The population orthogonality conditions hold in the overidentified case as well. In this case an alternative to 2SLS is GMM. Let <span class="math inline">\(\widehat{\boldsymbol{W}}\)</span> be an estimator of <span class="math inline">\(\boldsymbol{W}=\mathbb{E}\left[\dot{\boldsymbol{Z}}_{i}^{\prime} \varepsilon_{i} \varepsilon_{i}^{\prime} \dot{Z}_{i}\right]\)</span>, for example</p>
<p><span class="math display">\[
\widehat{\boldsymbol{W}}=\frac{1}{N} \sum_{i=1}^{N} \dot{\boldsymbol{Z}}_{i}^{\prime} \widehat{\boldsymbol{\varepsilon}}_{i} \widehat{\boldsymbol{\varepsilon}}_{i}^{\prime} \dot{\boldsymbol{Z}}_{i}
\]</span></p>
<p>where <span class="math inline">\(\widehat{\boldsymbol{\varepsilon}}_{i}\)</span> are the 2SLS fixed effects residuals. The GMM fixed effects estimator is</p>
<p><span class="math display">\[
\widehat{\beta}_{\mathrm{gmm}}=\left(\dot{\boldsymbol{X}}^{\prime} \dot{\boldsymbol{Z}} \widehat{\boldsymbol{W}}^{-1} \dot{\boldsymbol{Z}}^{\prime} \dot{\boldsymbol{X}}\right)^{-1}\left(\dot{\boldsymbol{X}}^{\prime} \dot{\boldsymbol{Z}} \widehat{\boldsymbol{W}}^{-1} \dot{\boldsymbol{Z}}^{\prime} \dot{\boldsymbol{Y}}\right) .
\]</span></p>
<p>The estimator (17.73)-(17.72) does not have a Stata command but can be obtained by generating the within transformed variables <span class="math inline">\(\dot{\boldsymbol{X}}, \dot{Z}\)</span> and <span class="math inline">\(\dot{\boldsymbol{Y}}\)</span>, and then estimating by GMM a regression of <span class="math inline">\(\dot{\boldsymbol{Y}}\)</span> on <span class="math inline">\(\dot{\boldsymbol{X}}\)</span> using <span class="math inline">\(\dot{Z}\)</span> as instruments using a weight matrix clustered by individual.</p>
</section>
<section id="estimation-with-time-invariant-regressors" class="level2" data-number="16.32">
<h2 data-number="16.32" class="anchored" data-anchor-id="estimation-with-time-invariant-regressors"><span class="header-section-number">16.32</span> Estimation with Time-Invariant Regressors</h2>
<p>One of the disappointments with the fixed effects estimator is that it cannot estimate the effect of regressors which are time-invariant. They are not identified separately from the fixed effect and are eliminated by the within transformation. In contrast, the random effects estimator allows for time-invariant regressors but does so only by assuming strict exogeneity which is stronger than typically desired in economic applications.</p>
<p>It turns out that we can consider an intermediate case which maintains the fixed effects assumptions for the time-varying regressors but uses stronger assumptions on the time-invariant regressors. For our exposition we will denote the time-varying regressors by the <span class="math inline">\(k \times 1\)</span> vector <span class="math inline">\(X_{i t}\)</span> and the time-invariant regressors by the <span class="math inline">\(\ell \times 1\)</span> vector <span class="math inline">\(Z_{i}\)</span>.</p>
<p>Consider the linear regression model</p>
<p><span class="math display">\[
Y_{i t}=X_{i t}^{\prime} \beta+Z_{i}^{\prime} \gamma+u_{i}+\varepsilon_{i t} .
\]</span></p>
<p>At the level of the individual this can be written as</p>
<p><span class="math display">\[
\boldsymbol{Y}_{i}=\boldsymbol{X}_{i} \beta+\boldsymbol{Z}_{i} \gamma+\boldsymbol{\imath}_{i} u_{i}+\boldsymbol{\varepsilon}_{i}
\]</span></p>
<p>where <span class="math inline">\(Z_{i}=\boldsymbol{\imath}_{i} Z_{i}^{\prime}\)</span>. For the full sample in matrix notation we can write this as</p>
<p><span class="math display">\[
\boldsymbol{Y}=\boldsymbol{X} \beta+\boldsymbol{Z} \gamma+\boldsymbol{u}+\boldsymbol{\varepsilon} .
\]</span></p>
<p>We maintain the assumption that the idiosyncratic errors <span class="math inline">\(\varepsilon_{i t}\)</span> are uncorrelated with both <span class="math inline">\(X_{i t}\)</span> and <span class="math inline">\(Z_{i}\)</span> at all time horizons:</p>
<p><span class="math display">\[
\begin{aligned}
\mathbb{E}\left[X_{i s} \varepsilon_{i t}\right] &amp;=0 \\
\mathbb{E}\left[Z_{i} \varepsilon_{i t}\right] &amp;=0 .
\end{aligned}
\]</span></p>
<p>In this section we consider the case where <span class="math inline">\(Z_{i}\)</span> is uncorrelated with the individual-level error <span class="math inline">\(u_{i}\)</span>, thus</p>
<p><span class="math display">\[
\mathbb{E}\left[Z_{i} u_{i}\right]=0,
\]</span></p>
<p>but the correlation of <span class="math inline">\(X_{i t}\)</span> and <span class="math inline">\(u_{i}\)</span> is left unrestricted. In this context we say that <span class="math inline">\(Z_{i}\)</span> is exogenous with respect to the fixed effect <span class="math inline">\(u_{i}\)</span> while <span class="math inline">\(X_{i t}\)</span> is endogenous with respect to <span class="math inline">\(u_{i}\)</span>. Note that this is a different type of endogeneity than considered in the sections on instrumental variables: there endogeneity meant correlation with the idiosyncratic error <span class="math inline">\(\varepsilon_{i t}\)</span>. Here endogeneity means correlation with the fixed effect <span class="math inline">\(u_{i}\)</span>.</p>
<p>We consider estimation of (17.74) by instrumental variables and thus need instruments which are uncorrelated with the error <span class="math inline">\(u_{i}+\varepsilon_{i t}\)</span>. The time-invariant regressors <span class="math inline">\(Z_{i}\)</span> satisfy this condition due to (17.76) and (17.77), thus</p>
<p><span class="math display">\[
\mathbb{E}\left[\boldsymbol{Z}_{i}^{\prime}\left(\boldsymbol{Y}_{i}-\boldsymbol{X}_{i} \beta-\boldsymbol{Z}_{i} \gamma\right)\right]=0 .
\]</span></p>
<p>While the time-varying regressors <span class="math inline">\(X_{i t}\)</span> are correlated with <span class="math inline">\(u_{i}\)</span> the within transformed variables <span class="math inline">\(\dot{X}_{i t}\)</span> are uncorrelated with <span class="math inline">\(u_{i}+\varepsilon_{i t}\)</span> under (17.75), thus</p>
<p><span class="math display">\[
\mathbb{E}\left[\dot{\boldsymbol{X}}_{i}^{\prime}\left(\boldsymbol{Y}_{i}-\boldsymbol{X}_{i} \beta-\boldsymbol{Z}_{i} \gamma\right)\right]=0 .
\]</span></p>
<p>Therefore we can estimate <span class="math inline">\((\beta, \gamma)\)</span> by instrumental variable regression using the instrument set <span class="math inline">\((\dot{\boldsymbol{X}}, \boldsymbol{Z})\)</span>. Specifically, regression of <span class="math inline">\(\boldsymbol{Y}\)</span> on <span class="math inline">\(\boldsymbol{X}\)</span> and <span class="math inline">\(\boldsymbol{Z}\)</span> treating <span class="math inline">\(\boldsymbol{X}\)</span> as endogenous, <span class="math inline">\(\boldsymbol{Z}\)</span> as exogenous, and using the instrument <span class="math inline">\(\dot{\boldsymbol{X}}\)</span>. Write this estimator as <span class="math inline">\((\widehat{\beta}, \widehat{\gamma})\)</span>. This can be implemented using the Stata ivregress command after constructing the within transformed <span class="math inline">\(\dot{\boldsymbol{X}}\)</span>.</p>
<p>This instrumental variables estimator is algebraically equal to a simple two-step estimator. The first step <span class="math inline">\(\widehat{\beta}=\widehat{\beta}_{\text {fe }}\)</span> is the fixed effects estimator. The second step sets <span class="math inline">\(\widehat{\gamma}=\left(\boldsymbol{Z}^{\prime} \boldsymbol{Z}\right)^{-1}\left(\boldsymbol{Z}^{\prime} \widehat{\boldsymbol{u}}\right)\)</span>, the least squares coefficient from the regression of the estimated fixed effect <span class="math inline">\(\widehat{u}_{i}\)</span> on <span class="math inline">\(Z_{i}\)</span>. To see this equivalence observe that the instrumental variables estimator estimator solves the sample moment equations</p>
<p><span class="math display">\[
\begin{aligned}
&amp;\dot{\boldsymbol{X}}^{\prime}(\boldsymbol{Y}-\boldsymbol{X} \beta-\boldsymbol{Z} \gamma)=0 \\
&amp;\boldsymbol{Z}^{\prime}(\boldsymbol{Y}-\boldsymbol{X} \beta-\boldsymbol{Z} \gamma)=0 .
\end{aligned}
\]</span></p>
<p>Notice that <span class="math inline">\(\dot{\boldsymbol{X}}_{i}^{\prime} \boldsymbol{Z}_{i}=\dot{\boldsymbol{X}}_{i}^{\prime} \boldsymbol{l}_{i} Z_{i}^{\prime}=0\)</span> so <span class="math inline">\(\dot{\boldsymbol{X}}^{\prime} \boldsymbol{Z}=0\)</span>. Thus (17.78) is the same as <span class="math inline">\(\dot{\boldsymbol{X}}^{\prime}(\boldsymbol{Y}-\boldsymbol{X} \beta)=0\)</span> whose solution is <span class="math inline">\(\widehat{\beta}_{\mathrm{fe}}\)</span>. Plugging this into the left-side of (17.79) we obtain</p>
<p><span class="math display">\[
\boldsymbol{Z}^{\prime}\left(\boldsymbol{Y}-\boldsymbol{X} \widehat{\beta}_{\mathrm{fe}}-\boldsymbol{Z} \gamma\right)=\boldsymbol{Z}^{\prime}\left(\overline{\boldsymbol{Y}}-\overline{\boldsymbol{X}} \widehat{\beta}_{\mathrm{fe}}-\boldsymbol{Z} \gamma\right)=\boldsymbol{Z}^{\prime}(\widehat{\boldsymbol{u}}-\boldsymbol{Z} \gamma)
\]</span></p>
<p>where <span class="math inline">\(\overline{\boldsymbol{Y}}\)</span> and <span class="math inline">\(\overline{\boldsymbol{X}}\)</span> are the stacked individual means <span class="math inline">\(\boldsymbol{\imath}_{i} \bar{Y}_{i}\)</span> and <span class="math inline">\(\boldsymbol{\imath}_{i} \bar{X}_{i}^{\prime}\)</span>. Set equal to 0 and solving we obtain the least squares estimator <span class="math inline">\(\widehat{\gamma}=\left(\boldsymbol{Z}^{\prime} \boldsymbol{Z}\right)^{-1}\left(\boldsymbol{Z}^{\prime} \widehat{\boldsymbol{u}}\right)\)</span> as claimed. This equivalence was first observed by Hausman and Taylor (1981).</p>
<p>For standard error calculation it is recommended to estimate <span class="math inline">\((\beta, \gamma)\)</span> jointly by instrumental variable regression and use a cluster-robust covariance matrix clustered at the individual level. Classical and heteroskedasticity-robust estimators are misspecified due to the individual-specific effect <span class="math inline">\(u_{i}\)</span>.</p>
<p>The estimator <span class="math inline">\((\widehat{\beta}, \widehat{\gamma})\)</span> is a special case of the Hausman-Taylor estimator described in the next section. (For an unknown reason the above estimator cannot be estimated using Stata’s xthtaylor command.)</p>
</section>
<section id="hausman-taylor-model" class="level2" data-number="16.33">
<h2 data-number="16.33" class="anchored" data-anchor-id="hausman-taylor-model"><span class="header-section-number">16.33</span> Hausman-Taylor Model</h2>
<p>Hausman and Taylor (1981) consider a generalization of the previous model. Their model is</p>
<p><span class="math display">\[
Y_{i t}=X_{1 i t}^{\prime} \beta_{1}+X_{2 i t}^{\prime} \beta_{2}+Z_{1 i}^{\prime} \gamma_{1}+Z_{2 i}^{\prime} \gamma_{2}+u_{i}+\varepsilon_{i t}
\]</span></p>
<p>where <span class="math inline">\(X_{1 i t}\)</span> and <span class="math inline">\(X_{2 i t}\)</span> are time-varying and <span class="math inline">\(Z_{1 i}\)</span> and <span class="math inline">\(Z_{2 i}\)</span> are time-invariant. Let the dimensions of <span class="math inline">\(X_{1 i t}\)</span>, <span class="math inline">\(X_{2 i t}, Z_{1 i}\)</span>, and <span class="math inline">\(Z_{2 i}\)</span> be <span class="math inline">\(k_{1}, k_{2}, \ell_{1}\)</span>, and <span class="math inline">\(\ell_{2}\)</span>, respectively.</p>
<p>Write the model in matrix notation as</p>
<p><span class="math display">\[
\boldsymbol{Y}=\boldsymbol{X}_{1} \beta_{1}+\boldsymbol{X}_{2} \beta_{2}+\boldsymbol{Z}_{1} \gamma_{1}+\boldsymbol{Z}_{2} \gamma_{2}+\boldsymbol{u}+\boldsymbol{\varepsilon} .
\]</span></p>
<p>Let <span class="math inline">\(\overline{\boldsymbol{X}}_{1}\)</span> and <span class="math inline">\(\overline{\boldsymbol{X}}_{2}\)</span> denote conformable matrices of individual-specific means and let <span class="math inline">\(\dot{\boldsymbol{X}}_{1}=\boldsymbol{X}_{1}-\overline{\boldsymbol{X}}_{1}\)</span> and <span class="math inline">\(\dot{\boldsymbol{X}}_{2}=\boldsymbol{X}_{2}-\overline{\boldsymbol{X}}_{2}\)</span> denote the within-transformed variables.</p>
<p>The Hausman-Taylor model assumes that all regressors are uncorrelated with the idiosyncratic error <span class="math inline">\(\varepsilon_{i t}\)</span> at all time horizons and that <span class="math inline">\(X_{1 i t}\)</span> and <span class="math inline">\(Z_{1 i}\)</span> are exogenous with respect to the fixed effect <span class="math inline">\(u_{i}\)</span> so that</p>
<p><span class="math display">\[
\begin{aligned}
\mathbb{E}\left[X_{1 i t} u_{i}\right] &amp;=0 \\
\mathbb{E}\left[Z_{1 i} u_{i}\right] &amp;=0 .
\end{aligned}
\]</span></p>
<p>The regressors <span class="math inline">\(X_{2 i t}\)</span> and <span class="math inline">\(Z_{2 i}\)</span>, however, are allowed to be correlated with <span class="math inline">\(u_{i}\)</span>.</p>
<p>Set <span class="math inline">\(\boldsymbol{X}=\left(\boldsymbol{X}_{1}, \boldsymbol{X}_{2}, \boldsymbol{Z}_{1}, \boldsymbol{Z}_{2}\right)\)</span> and <span class="math inline">\(\beta=\left(\beta_{1}, \beta_{2}, \gamma_{1}, \gamma_{2}\right)\)</span>. The assumptions imply the following population moment conditions</p>
<p><span class="math display">\[
\begin{aligned}
&amp;\mathbb{E}\left[\dot{\boldsymbol{X}}_{1}^{\prime}(\boldsymbol{Y}-\boldsymbol{X} \beta)\right]=0 \\
&amp;\mathbb{E}\left[\dot{\boldsymbol{X}}_{2}^{\prime}(\boldsymbol{Y}-\boldsymbol{X} \beta)\right]=0 \\
&amp;\mathbb{E}\left[\overline{\boldsymbol{X}}_{1}^{\prime}(\boldsymbol{Y}-\boldsymbol{X} \beta)\right]=0 \\
&amp;\mathbb{E}\left[\boldsymbol{Z}_{1}^{\prime}(\boldsymbol{Y}-\boldsymbol{X} \beta)\right]=0 .
\end{aligned}
\]</span></p>
<p>There are <span class="math inline">\(2 k_{1}+k_{2}+\ell_{1}\)</span> moment conditions and <span class="math inline">\(k_{1}+k_{2}+\ell_{1}+\ell_{2}\)</span> coefficients. Identification requires <span class="math inline">\(k_{1} \geq \ell_{2}\)</span> : that there are at least as many exogenous time-varying regressors as endogenous time-invariant regressors. (This includes the model of the previous section where <span class="math inline">\(k_{1}=\ell_{2}=0\)</span>.) Given the moment conditions the coefficients <span class="math inline">\(\beta=\left(\beta_{1}, \beta_{2}, \gamma_{1}, \gamma_{2}\right)\)</span> can be estimated by 2SLS regression of (17.80) using the instruments <span class="math inline">\(\boldsymbol{Z}=\left(\dot{\boldsymbol{X}}_{1}, \dot{\boldsymbol{X}}_{2}, \overline{\boldsymbol{X}}_{1}, \boldsymbol{Z}_{1}\right)\)</span> or equivalently <span class="math inline">\(\boldsymbol{Z}=\left(\boldsymbol{X}_{1}, \dot{\boldsymbol{X}}_{2}, \overline{\boldsymbol{X}}_{1}, \boldsymbol{Z}_{1}\right)\)</span>. This is 2SLS regression treating <span class="math inline">\(\boldsymbol{X}_{1}\)</span> and <span class="math inline">\(Z_{1}\)</span> as exogenous and <span class="math inline">\(\boldsymbol{X}_{2}\)</span> and <span class="math inline">\(\boldsymbol{Z}_{2}\)</span> as endogenous using the excluded instruments <span class="math inline">\(\dot{\boldsymbol{X}}_{2}\)</span> and <span class="math inline">\(\overline{\boldsymbol{X}}_{1}\)</span></p>
<p>It is recommended to use cluster-robust covariance matrix estimation clustered at the individual level. Neither conventional nor heteroskedasticity-robust covariance matrix estimators should be used as they are misspecified due to the individual-specific effect <span class="math inline">\(u_{i}\)</span>.</p>
<p>When the model is just-identified the estimators simplify as follows. <span class="math inline">\(\widehat{\beta}_{1}\)</span> and <span class="math inline">\(\widehat{\beta}_{2}\)</span> are the fixed effects estimator. <span class="math inline">\(\widehat{\gamma}_{1}\)</span> and <span class="math inline">\(\widehat{\gamma}_{2}\)</span> equal the 2 SLS estimator from a regression of <span class="math inline">\(\widehat{u}_{i}\)</span> on <span class="math inline">\(Z_{1 i}\)</span> and <span class="math inline">\(Z_{2 i}\)</span> using <span class="math inline">\(\bar{X}_{1 i}\)</span> as an instrument for <span class="math inline">\(Z_{2 i}\)</span>. (See Exercise 17.14.)</p>
<p>When the model is over-identified the equation can also be estimated by GMM with a cluster-robust weight matrix using the same equations and instruments.</p>
<p>This estimator with cluster-robust standard errors can be calculated using the Stata ivregress cluster(id) command after constructing the transformed variables <span class="math inline">\(\dot{\boldsymbol{X}}_{2}\)</span> and <span class="math inline">\(\overline{\boldsymbol{X}}_{1}\)</span>.</p>
<p>The 2SLS estimator described above corresponds with the Hausman and Taylor (1981) estimator in the just-identified case with a balanced panel.</p>
<p>Hausman and Taylor derived their estimator under the stronger assumption that the errors <span class="math inline">\(\varepsilon_{i t}\)</span> and <span class="math inline">\(u_{i}\)</span> are strictly mean independent and homoskedastic and consequently proposed a GLS-type estimator which is more efficient when these assumptions are correct. Define <span class="math inline">\(\Omega=\operatorname{diag}\left(\Omega_{i}\right)\)</span> where <span class="math inline">\(\Omega_{i}=\boldsymbol{I}_{i}+\)</span> <span class="math inline">\(\mathbf{1}_{i} \mathbf{1}_{i}^{\prime} \sigma_{u}^{2} / \sigma_{\varepsilon}^{2}\)</span> and <span class="math inline">\(\sigma_{\varepsilon}^{2}\)</span> and <span class="math inline">\(\sigma_{u}^{2}\)</span> are the variances of the error components <span class="math inline">\(\varepsilon_{i t}\)</span> and <span class="math inline">\(u_{i}\)</span>. Define as well the transformed variables <span class="math inline">\(\widetilde{\boldsymbol{Y}}=\Omega^{-1 / 2} \boldsymbol{Y}, \widetilde{\boldsymbol{X}}=\Omega^{-1 / 2} \boldsymbol{X}\)</span> and <span class="math inline">\(\widetilde{\boldsymbol{Z}}=\Omega^{-1 / 2} \boldsymbol{Z}\)</span>. The Hausman-Taylor estimator is</p>
<p><span class="math display">\[
\begin{aligned}
\widehat{\beta}_{\mathrm{ht}} &amp;=\left(\boldsymbol{X}^{\prime} \Omega^{-1} \boldsymbol{Z}\left(\boldsymbol{Z}^{\prime} \Omega^{-1} \boldsymbol{Z}\right)^{-1} \boldsymbol{Z}^{\prime} \Omega^{-1} \boldsymbol{X}\right)^{-1}\left(\boldsymbol{X}^{\prime} \Omega^{-1} \boldsymbol{Z}\left(\boldsymbol{Z}^{\prime} \Omega^{-1} \boldsymbol{Z}\right)^{-1} \boldsymbol{Z}^{\prime} \Omega^{-1} \boldsymbol{Y}\right) \\
&amp;=\left(\widetilde{\boldsymbol{X}}^{\prime} \widetilde{\boldsymbol{Z}}\left(\widetilde{\boldsymbol{Z}}^{\prime} \widetilde{\boldsymbol{Z}}\right)^{-1} \widetilde{\boldsymbol{Z}}^{\prime} \widetilde{\boldsymbol{X}}\right)^{-1}\left(\widetilde{\boldsymbol{X}}^{\prime} \widetilde{\boldsymbol{Z}}\left(\widetilde{\boldsymbol{Z}}^{\prime} \widetilde{\boldsymbol{Z}}\right)^{-1} \widetilde{\boldsymbol{Z}}^{\prime} \widetilde{\boldsymbol{Y}}\right) .
\end{aligned}
\]</span></p>
<p>Recall from (17.47) that <span class="math inline">\(\Omega_{i}^{-1 / 2}=\boldsymbol{M}_{i}+\rho_{i} \boldsymbol{P}_{i}\)</span> where <span class="math inline">\(\rho_{i}\)</span> is defined in (17.46). Thus</p>
<p><span class="math display">\[
\begin{aligned}
\widetilde{Y}_{i} &amp;=Y_{i}-\left(1-\rho_{i}\right) \bar{Y}_{i} \\
\widetilde{X}_{1 i} &amp;=X_{1 i}-\left(1-\rho_{i}\right) \bar{X}_{1 i} \\
\widetilde{X}_{2 i} &amp;=X_{2 i}-\left(1-\rho_{i}\right) \bar{X}_{2 i} \\
\widetilde{Z}_{1 i} &amp;=\rho_{i} Z_{1 i} \\
\widetilde{Z}_{2 i} &amp;=\rho_{i} Z_{2 i} \\
\widetilde{\dot{X}}_{1 i} &amp;=\dot{X}_{1 i} \\
\widetilde{\dot{X}}_{2 i} &amp;=\dot{X}_{2 i} .
\end{aligned}
\]</span></p>
<p>It follows that the Hausman-Taylor estimator can be calculated by 2SLS regression of <span class="math inline">\(\widetilde{\boldsymbol{Y}}_{i}\)</span> on <span class="math inline">\(\left(\widetilde{\boldsymbol{X}}_{1 i}, \widetilde{\boldsymbol{X}}_{2 i}, \rho_{i} \boldsymbol{Z}_{1 i}, \rho_{i} \boldsymbol{Z}_{2 i}\right)\)</span> using the instruments <span class="math inline">\(\left(\dot{\boldsymbol{X}}_{1 i}, \dot{\boldsymbol{X}}_{2 i}, \rho_{i} \overline{\boldsymbol{X}}_{1 i}, \rho_{i} \boldsymbol{Z}_{2 i}\right)\)</span></p>
<p>When the panel is balanced the coefficients <span class="math inline">\(\rho_{i}\)</span> all equal and scale out from the instruments. Thus the estimator can be calculated by 2SLS regression of <span class="math inline">\(\widetilde{\boldsymbol{Y}}_{i}\)</span> on <span class="math inline">\(\left(\widetilde{\boldsymbol{X}}_{1 i}, \widetilde{\boldsymbol{X}}_{2 i}, Z_{1 i}, \boldsymbol{Z}_{2 i}\right)\)</span> using the instruments <span class="math inline">\(\left(\dot{\boldsymbol{X}}_{1 i}, \dot{\boldsymbol{X}}_{2 i}, \overline{\boldsymbol{X}}_{1 i}, \boldsymbol{Z}_{2 i}\right)\)</span></p>
<p>In practice <span class="math inline">\(\rho_{i}\)</span> is unknown. It can be estimated as in (17.48) with the modification that the error variance is estimated from the untransformed 2SLS regression. Under the homoskedasticity assumptions used by Hausman and Taylor the estimator <span class="math inline">\(\widehat{\beta}_{\text {ht }}\)</span> has a classical asymptotic covariance matrix. When these assumptions are relaxed the covariance matrix can be estimated using cluster-robust methods. The Hausman-Taylor estimator with cluster-robust standard errors can be implemented in Stata by the command xthtaylor vce(robust). This Stata command, for an unknown reason, requires that there is at least one exogenous time-invariant variable <span class="math inline">\(\left(\ell_{1} \geq 1\right)\)</span> and at least one exogenous time-varying variable <span class="math inline">\(\left(k_{1} \geq 1\right)\)</span>, even when the model is identified. Otherwise, the estimator can be implemented using the instrumental variable method described above.</p>
<p>The Hausman-Taylor estimator was refined by Amemiya and MaCurdy (1986) and Breusch, Mizon and Schmidt (1989) who proposed more efficient versions using additional instruments which are valid under stronger orthogonality conditions. The observation that in the unbalanced case the instruments should be weighted by <span class="math inline">\(\rho_{i}\)</span> was made by Gardner (1998).</p>
<p>In the over-identified case it is unclear if it is preferred to use the simpler 2SLS estimator <span class="math inline">\(\widehat{\beta}_{2 s l s}\)</span> or the GLS-type Hausman-Taylor estimator <span class="math inline">\(\widehat{\beta}_{\mathrm{ht}}\)</span>. The advantages of <span class="math inline">\(\widehat{\beta}_{\mathrm{ht}}\)</span> are that it is asymptotically efficient under their stated homoskedasticity and serial correlation conditions and that there is an available program in Stata. The advantage of <span class="math inline">\(\widehat{\beta}_{2 \text { sls }}\)</span> is that it is much simpler to program (if doing so yourself), may have better finite sample properties (because it avoids variance-component estimation), and is the natural estimator from the the modern GMM viewpoint.</p>
<p>To illustrate, the final column of Table <span class="math inline">\(17.2\)</span> contains Hausman-Taylor estimates of the investment model treating <span class="math inline">\(Q_{i t-1}, D_{i t-1}\)</span>, and <span class="math inline">\(T_{i}\)</span> as endogenous for <span class="math inline">\(u_{i}\)</span> and <span class="math inline">\(C F_{i t-1}\)</span> and the industry dummies as exogenous. Relative to the fixed effects models this allows estimation of the coefficients on the trading indicator <span class="math inline">\(T_{i}\)</span>. The most interesting change relative to the previous estimates is that the coefficient on the trading indicator <span class="math inline">\(T_{i}\)</span> doubles in magnitude relative to the random effects estimate. This is consistent with the hypothesis that <span class="math inline">\(T_{i}\)</span> is correlated with the fixed effect and hence the random effects estimate is biased.</p>
</section>
<section id="jackknife-covariance-matrix-estimation" class="level2" data-number="16.34">
<h2 data-number="16.34" class="anchored" data-anchor-id="jackknife-covariance-matrix-estimation"><span class="header-section-number">16.34</span> Jackknife Covariance Matrix Estimation</h2>
<p>As an alternative to asymptotic inference the delete-cluster jackknife can be used for covariance matrix calculation. In the context of fixed effects estimation the delete-cluster estimators take the form</p>
<p><span class="math display">\[
\widehat{\beta}_{(-i)}=\left(\sum_{j \neq i} \dot{\boldsymbol{X}}_{j}^{\prime} \dot{\boldsymbol{X}}_{j}\right)^{-1}\left(\sum_{j \neq i} \dot{\boldsymbol{X}}_{j}^{\prime} \dot{\boldsymbol{Y}}_{j}\right)=\widehat{\beta}_{\mathrm{fe}}-\left(\sum_{i=1}^{N} \dot{\boldsymbol{X}}_{i}^{\prime} \dot{\boldsymbol{X}}_{i}\right)^{-1} \dot{\boldsymbol{X}}_{i}^{\prime} \widetilde{\boldsymbol{e}}_{i}
\]</span></p>
<p>where</p>
<p><span class="math display">\[
\begin{aligned}
&amp;\widetilde{\boldsymbol{e}}_{i}=\left(\boldsymbol{I}_{i}-\dot{\boldsymbol{X}}_{i}\left(\dot{\boldsymbol{X}}_{i}^{\prime} \dot{\boldsymbol{X}}_{i}\right)^{-1} \dot{\boldsymbol{X}}_{i}^{\prime}\right)^{-1} \widehat{\boldsymbol{e}}_{i} \\
&amp;\widehat{\boldsymbol{e}}_{i}=\dot{\boldsymbol{Y}}_{i}-\dot{\boldsymbol{X}}_{i} \widehat{\beta}_{\mathrm{fe}}
\end{aligned}
\]</span></p>
<p>The delete-cluster jackknife estimator of the variance of <span class="math inline">\(\widehat{\beta}_{\mathrm{fe}}\)</span> is</p>
<p><span class="math display">\[
\begin{aligned}
\widehat{\boldsymbol{V}}_{\widehat{\beta}}^{\text {jack }} &amp;=\frac{N-1}{N} \sum_{i=1}^{N}\left(\widehat{\beta}_{(-i)}-\bar{\beta}\right)\left(\widehat{\beta}_{(-i)}-\bar{\beta}\right)^{\prime} \\
\bar{\beta} &amp;=\frac{1}{N} \sum_{i=1}^{N} \widehat{\beta}_{(-i)} .
\end{aligned}
\]</span></p>
<p>The delete-cluster jackknife estimator <span class="math inline">\(\widehat{\boldsymbol{V}}_{\widehat{\beta}}^{\text {jack }}\)</span> is similar to the cluster-robust covariance matrix estimator.</p>
<p>For parameters which are functions <span class="math inline">\(\widehat{\theta}_{\mathrm{fe}}=r\left(\widehat{\beta}_{\mathrm{fe}}\right)\)</span> of the fixed effects estimator the delete-cluster jack- knife estimator of the variance of <span class="math inline">\(\widehat{\theta}_{\mathrm{fe}}\)</span> is</p>
<p><span class="math display">\[
\begin{aligned}
\widehat{\boldsymbol{V}}_{\widehat{\theta}}^{\mathrm{jack}} &amp;=\frac{N-1}{N} \sum_{i=1}^{N}\left(\widehat{\theta}_{(-i)}-\bar{\theta}\right)\left(\widehat{\theta}_{(-i)}-\bar{\theta}\right)^{\prime} \\
\widehat{\theta}_{(-i)} &amp;=r\left(\widehat{\beta}_{(-i)}\right) \\
\bar{\theta} &amp;=\frac{1}{N} \sum_{i=1}^{N} \widehat{\theta}_{(-i)} .
\end{aligned}
\]</span></p>
<p>The estimator <span class="math inline">\(\widehat{\boldsymbol{V}}_{\widehat{\theta}}^{\text {jack }}\)</span> is similar to the delta-method cluster-robust covariance matrix estimator for <span class="math inline">\(\widehat{\theta}\)</span>.</p>
<p>As in the context of i.i.d. samples one advantage of the jackknife covariance matrix estimator is that it does not require the user to make a technical calculation of the asymptotic distribution. A downside is an increase in computation cost as <span class="math inline">\(N\)</span> separate regressions are effectively estimated. This can be particularly costly in micro panels which have a large number <span class="math inline">\(N\)</span> of individuals.</p>
<p>In Stata jackknife standard errors for fixed effects estimators are obtained by using either <span class="math inline">\(x\)</span> treg <span class="math inline">\(f e\)</span> vce(jackknife) or areg absorb(id) cluster(id) vce(jackknife) where id is the cluster variable. For the fixed effects 2SLS estimator usextivreg fe vce(jackknife).</p>
</section>
<section id="panel-bootstrap" class="level2" data-number="16.35">
<h2 data-number="16.35" class="anchored" data-anchor-id="panel-bootstrap"><span class="header-section-number">16.35</span> Panel Bootstrap</h2>
<p>Bootstrap methods can also be applied to panel data by a straightforward application of the pairs cluster bootstrap which samples entire individuals rather than single observations. In the context of panel data we call this the panel nonparametric bootstrap.</p>
<p>The panel nonparametric bootstrap samples <span class="math inline">\(N\)</span> individual histories <span class="math inline">\(\left(\boldsymbol{Y}_{i}, \boldsymbol{X}_{i}\right)\)</span> to create the bootstrap sample. Fixed effects (or any other estimation method) is applied to the bootstrap sample to obtain the coefficient estimates. By repeating <span class="math inline">\(B\)</span> times, bootstrap standard errors for coefficients estimates, or functions of the coefficient estimates, can be calculated. Percentile-type and percentile-t confidence intervals can be calculated. The <span class="math inline">\(\mathrm{BC}_{a}\)</span> interval requires an estimator of the acceleration coefficient <span class="math inline">\(a\)</span> which is a scaled jackknife estimate of the third moment of the estimator. In panel data the delete-cluster jackknife should be used for estimation of <span class="math inline">\(a\)</span>.</p>
<p>In Stata, to obtain bootstrap standard errors and confidence intervals use either xtreg, vce(bootstrap, reps (#)) or areg, absorb(id) cluster(id) vce(bootstrap, reps(#)) where id is the cluster variable and # is the number of bootstrap replications. For the fixed effects 2SLS estimator use xtivreg, fe vce(bootstrap, reps(#)).</p>
</section>
<section id="dynamic-panel-models" class="level2" data-number="16.36">
<h2 data-number="16.36" class="anchored" data-anchor-id="dynamic-panel-models"><span class="header-section-number">16.36</span> Dynamic Panel Models</h2>
<p>The models so far considered in this chapter have been static with no dynamic relationships. In many economic contexts it is natural to expect that behavior and decisions are dynamic, explicitly depending on past behavior. In our investment equation, for example, economic models predict that a firm’s investment in any given year will depend on investment decisions from previous years. These considerations lead us to consider explicitly dynamic models.</p>
<p>The workhorse dynamic model in a panel framework is the <span class="math inline">\(p^{t h}\)</span>-order autoregression with regressors and a one-way error component structure. This is</p>
<p><span class="math display">\[
Y_{i t}=\alpha_{1} Y_{i, t-1}+\cdots+\alpha_{p} Y_{i, t-p}+X_{i t}^{\prime} \beta+u_{i}+\varepsilon_{i t} .
\]</span></p>
<p>where <span class="math inline">\(\alpha_{j}\)</span> are the autoregressive coefficients, <span class="math inline">\(X_{i t}\)</span> is a <span class="math inline">\(k\)</span> vector of regressors, <span class="math inline">\(u_{i}\)</span> is an individual effect, and <span class="math inline">\(\varepsilon_{i t}\)</span> is an idiosyncratic error. It is conventional to assume that the errors <span class="math inline">\(u_{i}\)</span> and <span class="math inline">\(\varepsilon_{i t}\)</span> are mutually independent and the <span class="math inline">\(\varepsilon_{i t}\)</span> are serially uncorrelated and mean zero. For the present we will assume that the regressors <span class="math inline">\(X_{i t}\)</span> are strictly exogenous (17.17). In Section <span class="math inline">\(17.41\)</span> we discuss predetermined regressors.</p>
<p>For many illustrations we will focus on the AR(1) model</p>
<p><span class="math display">\[
Y_{i t}=\alpha Y_{i, t-1}+u_{i}+\varepsilon_{i t}
\]</span></p>
<p>The dynamics should be interpreted individual-by-individual. The coefficient <span class="math inline">\(\alpha\)</span> in (17.82) equals the first-order autocorrelation. When <span class="math inline">\(\alpha=0\)</span> the series is serially uncorrelated (conditional on <span class="math inline">\(u_{i}\)</span> ). <span class="math inline">\(\alpha&gt;0\)</span> means <span class="math inline">\(Y_{i t}\)</span> is positively serially correlated. <span class="math inline">\(\alpha&lt;0\)</span> means <span class="math inline">\(Y_{i t}\)</span> is negatively serially correlated. An autoregressive unit root holds when <span class="math inline">\(\alpha=1\)</span>, which means that <span class="math inline">\(Y_{i t}\)</span> follows a random walk with possible drift. Since <span class="math inline">\(u_{i}\)</span> is constant for a given individual it should be treated as an individual-specific intercept. The idiosyncratic error <span class="math inline">\(\varepsilon_{i t}\)</span> plays the role of the error in a standard time series autoregression.</p>
<p>If <span class="math inline">\(|\alpha|&lt;1\)</span> the model (17.82) is stationary. By standard autoregressive backwards recursion we calculate that</p>
<p><span class="math display">\[
Y_{i t}=\sum_{j=0}^{\infty} \alpha^{j}\left(u_{i}+\varepsilon_{i t}\right)=(1-\alpha)^{-1} u_{i}+\sum_{j=0}^{\infty} \alpha^{j} \varepsilon_{i, t-j}
\]</span></p>
<p>Thus conditional on <span class="math inline">\(u_{i}\)</span> the mean and variance of <span class="math inline">\(Y_{i t}\)</span> are <span class="math inline">\((1-\alpha)^{-1} u_{i}\)</span> and <span class="math inline">\(\left(1-\alpha^{2}\right)^{-1} \sigma_{\varepsilon}^{2}\)</span>, respectively. The <span class="math inline">\(k^{t h}\)</span> autocorrelation (conditional on <span class="math inline">\(u_{i}\)</span> ) is <span class="math inline">\(\alpha^{k}\)</span>. Notice that the effect of cross-section variation in <span class="math inline">\(u_{i}\)</span> is to shift the mean but not the variance or serial correlation. This implies that if we view time series plots of <span class="math inline">\(Y_{i t}\)</span> against time for a set of individuals <span class="math inline">\(i\)</span>, the series <span class="math inline">\(Y_{i t}\)</span> will appear to have different means but have similar variances and serial correlation.</p>
<p>As with the case with time series data, serial correlation (large <span class="math inline">\(\alpha\)</span> ) can proxy for other factors such as time trends. Thus in applications it will often be useful to include time effects to eliminate spurious serial correlation.</p>
</section>
<section id="the-bias-of-fixed-effects-estimation" class="level2" data-number="16.37">
<h2 data-number="16.37" class="anchored" data-anchor-id="the-bias-of-fixed-effects-estimation"><span class="header-section-number">16.37</span> The Bias of Fixed Effects Estimation</h2>
<p>To estimate the panel autoregression (17.81) it may appear natural to use the fixed effects (within) estimator. Indeed, the within transformation eliminates the individual effect <span class="math inline">\(u_{i}\)</span>. The trouble is that the within operator induces correlation between the AR(1) lag and the error. The result is that the within estimator is inconsistent for the coefficients when <span class="math inline">\(T\)</span> is fixed. A thorough explanation appears in Nickell (1981). We describe the basic problem in this section focusing on the AR(1) model (17.82).</p>
<p>Applying the within operator to (17.82) we obtain</p>
<p><span class="math display">\[
\dot{Y}_{i t}=\alpha \dot{Y}_{i t-1}+\dot{\varepsilon}_{i t}
\]</span></p>
<p>for <span class="math inline">\(t \geq 2\)</span>. As expected the individual effect is eliminated. The difficulty is that <span class="math inline">\(\mathbb{E}\left[\dot{Y}_{i t-1} \dot{\varepsilon}_{i t}\right] \neq 0\)</span> because both <span class="math inline">\(\dot{Y}_{i t-1}\)</span> and <span class="math inline">\(\dot{\varepsilon}_{i t}\)</span> are functions of the entire time series.</p>
<p>To see this clearly in a simple example, suppose we have a balanced panel with <span class="math inline">\(T=3\)</span>. There are two observed pairs <span class="math inline">\(\left(Y_{i t}, Y_{i t-1}\right)\)</span> per individual so the within estimator equals the differenced estimator. Applying the differencing operator to (17.82) for <span class="math inline">\(t=3\)</span> we find</p>
<p><span class="math display">\[
\Delta Y_{i 3}=\alpha \Delta Y_{i 2}+\Delta \varepsilon_{i 3} .
\]</span></p>
<p>Because of the lagged dependent variable and differencing there is effectively one observation per individual. Notice that the individual effect has been eliminated.</p>
<p>The fixed effects estimator of <span class="math inline">\(\alpha\)</span> is equal to the least squares estimator applied to (17.84), which is</p>
<p><span class="math display">\[
\widehat{\alpha}_{\mathrm{fe}}=\left(\sum_{i=1}^{N} \Delta Y_{i 2}^{2}\right)^{-1}\left(\sum_{i=1}^{N} \Delta Y_{i 2} \Delta Y_{i 3}\right)=\alpha+\left(\sum_{i=1}^{N} \Delta Y_{i 2}^{2}\right)^{-1}\left(\sum_{i=1}^{N} \Delta Y_{i 2} \Delta \varepsilon_{i 3}\right) .
\]</span></p>
<p>The differenced regressor and error are negatively correlated. Indeed</p>
<p><span class="math display">\[
\begin{aligned}
\mathbb{E}\left[\Delta Y_{i 2} \Delta \varepsilon_{i 3}\right] &amp;=\mathbb{E}\left[\left(Y_{i 2}-Y_{i 1}\right)\left(\varepsilon_{i 3}-\varepsilon_{i 2}\right)\right] \\
&amp;=\mathbb{E}\left[Y_{i 2} \varepsilon_{i 3}\right]-\mathbb{E}\left[Y_{i 1} \varepsilon_{i 3}\right]-\mathbb{E}\left[Y_{i 2} \varepsilon_{i 2}\right]+\mathbb{E}\left[Y_{i 1} \varepsilon_{i 2}\right] \\
&amp;=0-0-\sigma_{\varepsilon}^{2}+0 \\
&amp;=-\sigma_{\varepsilon}^{2}
\end{aligned}
\]</span></p>
<p>Using the variance formula for <span class="math inline">\(\operatorname{AR}(1)\)</span> models (assuming <span class="math inline">\(|\alpha|&lt;1)\)</span> we calculate that <span class="math inline">\(\mathbb{E}\left[\left(\Delta Y_{i 2}\right)^{2}\right]=2 \sigma_{\varepsilon}^{2} /(1+\)</span> <span class="math inline">\(\alpha\)</span> ). It follows that the probability limit of the fixed effects estimator <span class="math inline">\(\widehat{\alpha}_{\mathrm{fe}}\)</span> of <span class="math inline">\(\alpha\)</span> in (17.84) is</p>
<p><span class="math display">\[
\operatorname{plim}_{N \rightarrow \infty}\left(\widehat{\alpha}_{\mathrm{fe}}-\alpha\right)=\frac{\mathbb{E}\left[\Delta Y_{i 2} \Delta \varepsilon_{i 3}\right]}{\mathbb{E}\left[\left(\Delta Y_{i 2}\right)^{2}\right]}=-\frac{1+\alpha}{2} .
\]</span></p>
<p>It is typical to call (17.85) the “bias” of <span class="math inline">\(\widehat{\alpha}_{\text {fe }}\)</span>, though it is technically a probability limit.</p>
<p>The bias found in (17.85) is large. For <span class="math inline">\(\alpha=0\)</span> the bias is <span class="math inline">\(-1 / 2\)</span> and increases towards 1 as <span class="math inline">\(\alpha \rightarrow 1\)</span>. Thus for any <span class="math inline">\(\alpha&lt;1\)</span> the probability limit of <span class="math inline">\(\widehat{\alpha}_{\mathrm{fe}}\)</span> is negative! This is extreme bias.</p>
<p>Now take the case <span class="math inline">\(T&gt;3\)</span>. From Nickell’s (1981) expressions and some algebra, we can calculate that the probability limit of the fixed effects estimator for <span class="math inline">\(|\alpha|&lt;1\)</span> is</p>
<p><span class="math display">\[
\operatorname{plim}_{N \rightarrow \infty}\left(\widehat{\alpha}_{\mathrm{fe}}-\alpha\right)=\frac{1+\alpha}{\frac{2 \alpha}{1-\alpha}-\frac{T-1}{1-\alpha^{T-1}}} .
\]</span></p>
<p>It follows that the bias is of order <span class="math inline">\(O(1 / T)\)</span>.</p>
<p>It is often asserted that it is okay to use fixed effects if <span class="math inline">\(T\)</span> is sufficiently large, e.g.&nbsp;<span class="math inline">\(T \geq 30\)</span>. However, from (17.86) we can calculate that for <span class="math inline">\(T=30\)</span> the bias of the fixed effects estimator is <span class="math inline">\(-0.056\)</span> when <span class="math inline">\(\alpha=0.5\)</span> and the bias is <span class="math inline">\(-0.15\)</span> when <span class="math inline">\(\alpha=0.9\)</span>. For <span class="math inline">\(T=60\)</span> and <span class="math inline">\(\alpha=0.9\)</span> the bias is <span class="math inline">\(-0.05\)</span>. These magnitudes are unacceptably large. This includes the longer time series encountered in macro panels. Thus the Nickell bias problem applies to both micro and macro panel applications.</p>
<p>The conclusion from this analysis is that the fixed effects estimator should not be used for models with lagged dependent variables even if the time series dimension <span class="math inline">\(T\)</span> is large.</p>
</section>
<section id="anderson-hsiao-estimator" class="level2" data-number="16.38">
<h2 data-number="16.38" class="anchored" data-anchor-id="anderson-hsiao-estimator"><span class="header-section-number">16.38</span> Anderson-Hsiao Estimator</h2>
<p>Anderson and Hsiao (1982) made an important breakthrough by showing that a simple instrumental variables estimator is consistent for the parameters of (17.81).</p>
<p>The method first eliminates the individual effect <span class="math inline">\(u_{i}\)</span> by first-differencing (17.81) for <span class="math inline">\(t \geq p+1\)</span></p>
<p><span class="math display">\[
\Delta Y_{i t}=\alpha_{1} \Delta Y_{i, t-1}+\alpha_{2} \Delta Y_{i, t-2}+\cdots+\alpha_{p} \Delta Y_{i, t-p}+\Delta X_{i t}^{\prime} \beta+\Delta \varepsilon_{i t} .
\]</span></p>
<p>This eliminates the individual effect <span class="math inline">\(u_{i}\)</span>. The challenge is that first-differencing induces correlation between <span class="math inline">\(\Delta Y_{i t-1}\)</span> and <span class="math inline">\(\Delta \varepsilon_{i t}\)</span> :</p>
<p><span class="math display">\[
\mathbb{E}\left[\Delta Y_{i, t-1} \Delta \varepsilon_{i t}\right]=\mathbb{E}\left[\left(Y_{i, t-1}-Y_{i, t-2}\right)\left(\varepsilon_{i t}-\varepsilon_{i t-1}\right)\right]=-\sigma_{\varepsilon}^{2} .
\]</span></p>
<p>The other regressors are not correlated with <span class="math inline">\(\Delta \varepsilon_{i t}\)</span>. For <span class="math inline">\(s&gt;1, \mathbb{E}\left[\Delta Y_{i t-s} \Delta \varepsilon_{i t}\right]=0\)</span>, and when <span class="math inline">\(X_{i t}\)</span> is strictly exogenous <span class="math inline">\(\mathbb{E}\left[\Delta X_{i t} \Delta \varepsilon_{i t}\right]=0\)</span>.</p>
<p>The correlation between <span class="math inline">\(\Delta Y_{i t-1}\)</span> and <span class="math inline">\(\Delta \varepsilon_{i t}\)</span> is endogeneity. One solution to endogeneity is to use an instrument. Anderson-Hsiao pointed out that <span class="math inline">\(Y_{i t-2}\)</span> is a valid instrument because it is correlated with <span class="math inline">\(\Delta Y_{i, t-1}\)</span> yet uncorrelated with <span class="math inline">\(\Delta \varepsilon_{i t}\)</span>.</p>
<p><span class="math display">\[
\mathbb{E}\left[Y_{i, t-2} \Delta \varepsilon_{i t}\right]=\mathbb{E}\left[Y_{i, t-2} \varepsilon_{i t}\right]-\mathbb{E}\left[Y_{i, t-2} \varepsilon_{i t-1}\right]=0 .
\]</span></p>
<p>The Anderson-Hsiao estimator is IV using <span class="math inline">\(Y_{i, t-2}\)</span> as an instrument for <span class="math inline">\(\Delta Y_{i, t-1}\)</span>. Equivalently, this is IV using the instruments <span class="math inline">\(\left(Y_{i, t-2}, \ldots, Y_{i, t-p-1}\right)\)</span> for <span class="math inline">\(\left(\Delta Y_{i, t-1}, \ldots, \Delta Y_{i, t-p}\right)\)</span>. The estimator requires <span class="math inline">\(T \geq p+2\)</span>.</p>
<p>To show that this estimator is consistent, for simplicity assume we have a balanced panel with <span class="math inline">\(T=3\)</span>, <span class="math inline">\(p=1\)</span>, and no regressors. In this case the Anderson-Hsiao IV estimator is</p>
<p><span class="math display">\[
\widehat{\alpha}_{\mathrm{iv}}=\left(\sum_{i=1}^{N} Y_{i 1} \Delta Y_{i 2}\right)^{-1}\left(\sum_{i=1}^{N} Y_{i 1} \Delta Y_{i 3}\right)=\alpha+\left(\sum_{i=1}^{N} Y_{i 1} \Delta Y_{i 2}\right)^{-1}\left(\sum_{i=1}^{N} Y_{i 1} \Delta \varepsilon_{i 3}\right) .
\]</span></p>
<p>Under the assumption that <span class="math inline">\(\varepsilon_{i t}\)</span> is serially uncorrelated, (17.88) shows that <span class="math inline">\(\mathbb{E}\left[Y_{i 1} \Delta \varepsilon_{i 3}\right]=0\)</span>. In general, <span class="math inline">\(\mathbb{E}\left[Y_{i 1} \Delta Y_{i 2}\right] \neq 0\)</span>. As <span class="math inline">\(N \rightarrow \infty\)</span></p>
<p><span class="math display">\[
\widehat{\alpha}_{\mathrm{iv}} \underset{p}{\longrightarrow} \alpha-\frac{\mathbb{E}\left[Y_{i 1} \Delta \varepsilon_{i 3}\right]}{\mathbb{E}\left[Y_{i 1} \Delta Y_{i 2}\right]}=\alpha .
\]</span></p>
<p>Thus the IV estimator is consistent for <span class="math inline">\(\alpha\)</span>.</p>
<p>The Anderson-Hsiao IV estimator relies on two critical assumptions. First, the validity of the instrument (uncorrelatedness with the equation error) relies on the assumption that the dynamics are correctly specified so that <span class="math inline">\(\varepsilon_{i t}\)</span> is serially uncorrelated. For example, many applications use an AR(1). If instead the true model is an <span class="math inline">\(\operatorname{AR}(2)\)</span> then <span class="math inline">\(Y_{i t-2}\)</span> is not a valid instrument and the IV estimates will be biased. Second, the relevance of the instrument (correlatedness with the endogenous regressor) requires <span class="math inline">\(\mathbb{E}\left[Y_{i 1} \Delta Y_{i 2}\right] \neq 0\)</span>. This turns out to be problematic and is explored further in Section 17.40. These considerations suggest that the validity and accuracy of the estimator are likely to be sensitive to these unknown features.</p>
</section>
<section id="arellano-bond-estimator" class="level2" data-number="16.39">
<h2 data-number="16.39" class="anchored" data-anchor-id="arellano-bond-estimator"><span class="header-section-number">16.39</span> Arellano-Bond Estimator</h2>
<p>The orthogonality condition (17.88) is one of many implied by the dynamic panel model. Indeed, all lags <span class="math inline">\(Y_{i t-2}, Y_{i t-3}, \ldots\)</span> are valid instruments. If <span class="math inline">\(T&gt;p+2\)</span> these can be used to potentially improve estimation efficiency. This was first pointed out by Holtz-Eakin, Newey, and Rosen (1988) and further developed by Arellano and Bond (1991).</p>
<p>Using these extra instruments has a complication that there are a different number of instruments for each time period. The solution is to view the model as a system of <span class="math inline">\(T\)</span> equations as in Section 17.18.</p>
<p>It will be useful to first write the model in vector notation. Stack the differenced regressors <span class="math inline">\(\left(\Delta Y_{i, t-1}, \ldots\right.\)</span>, <span class="math inline">\(\Delta Y_{i, t-p}, \Delta X_{i t}^{\prime}\)</span> ) into a matrix <span class="math inline">\(\Delta \boldsymbol{X}_{i}\)</span> and the coefficients into a vector <span class="math inline">\(\theta\)</span>. We can write (17.87) as <span class="math inline">\(\Delta \boldsymbol{Y}_{i}=\)</span> <span class="math inline">\(\Delta \boldsymbol{X}_{i} \theta+\Delta \boldsymbol{\varepsilon}_{i}\)</span>. Stacking all <span class="math inline">\(N\)</span> individuals this can be written as <span class="math inline">\(\Delta \boldsymbol{Y}=\Delta \boldsymbol{X} \theta+\Delta \boldsymbol{\varepsilon}\)</span>.</p>
<p>For period <span class="math inline">\(t=p+2\)</span> we have <span class="math inline">\(p+k\)</span> valid instruments <span class="math inline">\(\left[Y_{i 1} \ldots, Y_{i p}, \Delta X_{i, p+2}\right]\)</span>. For period <span class="math inline">\(t=p+3\)</span> there are <span class="math inline">\(p+1+k\)</span> valid instruments <span class="math inline">\(\left[Y_{i 1} \ldots, Y_{i p+1}, \Delta X_{i, p+3}\right]\)</span>. For period <span class="math inline">\(t=p+4\)</span> there are <span class="math inline">\(p+2+k\)</span> instruments. In general, for any <span class="math inline">\(t \geq p+2\)</span> there are <span class="math inline">\(t-2\)</span> instruments <span class="math inline">\(\left[Y_{i 1}, \ldots, Y_{i, t-2}, \Delta X_{i t}\right]\)</span>. Similarly to (17.53) we can define the instrument matrix for individual <span class="math inline">\(i\)</span> as</p>
<p><span class="math display">\[
\boldsymbol{Z}_{i}=\left[\begin{array}{ccc}
{\left[Y_{i 1}, \ldots, Y_{i p}, \Delta X_{i, p+2}^{\prime}\right]} &amp; 0 &amp; 0 \\
0 &amp; {\left[Y_{i 1}, \ldots, Y_{i, p+1}, \Delta X_{i, p+3}^{\prime}\right]} &amp; \\
0 &amp; \ddots &amp; 0 \\
&amp; 0 &amp; {\left[Y_{i 1}, Y_{i 2}, \ldots, Y_{i, T-2}, \Delta X_{i, T}^{\prime}\right]}
\end{array}\right] .
\]</span></p>
<p>This is <span class="math inline">\((T-p-1) \times \ell\)</span> where <span class="math inline">\(\ell=k(T-p-1)+((T-2)(T-1)-(p-2)(p-1)) / 2\)</span>. This instrument matrix consists of all lagged values <span class="math inline">\(Y_{i, t-2}, Y_{i, t-3}, \ldots\)</span> which are available in the data set plus the differenced strictly exogenous regressors.</p>
<p>The <span class="math inline">\(\ell\)</span> moment conditions are</p>
<p><span class="math display">\[
\mathbb{E}\left[\boldsymbol{Z}_{i}^{\prime}\left(\Delta \boldsymbol{Y}_{i}-\Delta \boldsymbol{X}_{i} \alpha\right)\right]=0
\]</span></p>
<p>If <span class="math inline">\(T&gt;p+2\)</span> then <span class="math inline">\(\ell&gt;p\)</span> and the model is overidentified. Define the <span class="math inline">\(\ell \times \ell\)</span> covariance matrix for the moment conditions</p>
<p><span class="math display">\[
\Omega=\mathbb{E}\left[\boldsymbol{Z}_{i}^{\prime} \Delta \boldsymbol{\varepsilon}_{i} \Delta \boldsymbol{\varepsilon}_{i}^{\prime} \boldsymbol{Z}_{i}\right] .
\]</span></p>
<p>Let <span class="math inline">\(\boldsymbol{Z}\)</span> denote <span class="math inline">\(\boldsymbol{Z}_{i}\)</span> stacked into a <span class="math inline">\((T-p-1) N \times \ell\)</span> matrix. The efficient GMM estimator of <span class="math inline">\(\alpha\)</span> is</p>
<p><span class="math display">\[
\widehat{\alpha}_{\mathrm{gmm}}=\left(\Delta \boldsymbol{X}^{\prime} \boldsymbol{Z} \Omega^{-1} \boldsymbol{Z}^{\prime} \Delta \boldsymbol{X}\right)^{-1}\left(\Delta \boldsymbol{X}^{\prime} \boldsymbol{Z} \Omega^{-1} \boldsymbol{Z}^{\prime} \Delta \boldsymbol{Y}\right) .
\]</span></p>
<p>If the errors <span class="math inline">\(\varepsilon_{i t}\)</span> are conditionally homoskedastic then</p>
<p><span class="math display">\[
\Omega=\mathbb{E}\left[\boldsymbol{Z}_{i}^{\prime} \boldsymbol{H} \boldsymbol{Z}_{i}\right] \sigma_{\varepsilon}^{2}
\]</span></p>
<p>where <span class="math inline">\(\boldsymbol{H}\)</span> is given in (17.31). In this case set</p>
<p><span class="math display">\[
\widehat{\Omega}_{1}=\sum_{i=1}^{N} \boldsymbol{Z}_{i}^{\prime} \boldsymbol{H} \boldsymbol{Z}_{i}
\]</span></p>
<p>as a (scaled) estimate of <span class="math inline">\(\Omega\)</span>. Under these assumptions an asymptotically efficient GMM estimator is</p>
<p><span class="math display">\[
\widehat{\alpha}_{1}=\left(\Delta \boldsymbol{X}^{\prime} \boldsymbol{Z} \widehat{\Omega}_{1}^{-1} \boldsymbol{Z}^{\prime} \Delta \boldsymbol{X}\right)^{-1}\left(\Delta \boldsymbol{X}^{\prime} \boldsymbol{Z} \widehat{\Omega}_{1}^{-1} \boldsymbol{Z}^{\prime} \Delta \boldsymbol{Y}\right) .
\]</span></p>
<p>Estimator (17.91) is known as the one-step Arellano-Bond GMM estimator.</p>
<p>Under the assumption that the error <span class="math inline">\(\varepsilon_{i t}\)</span> is homoskedastic and serially uncorrelated, a classical covariance matrix estimator for <span class="math inline">\(\widehat{\alpha}_{1}\)</span> is</p>
<p><span class="math display">\[
\widehat{\boldsymbol{V}}_{1}^{0}=\left(\Delta \boldsymbol{X}^{\prime} \boldsymbol{Z} \widehat{\Omega}_{1}^{-1} \boldsymbol{Z}^{\prime} \Delta \boldsymbol{X}\right)^{-1} \widehat{\sigma}_{\varepsilon}^{2}
\]</span></p>
<p>where <span class="math inline">\(\widehat{\sigma}_{\varepsilon}^{2}\)</span> is the sample variance of the one-step residuals <span class="math inline">\(\widehat{\boldsymbol{\varepsilon}}_{i}=\Delta \boldsymbol{Y}_{i}-\Delta \boldsymbol{X}_{i} \widehat{\alpha}\)</span>. A covariance matrix estimator which is robust to violation of these assumptions is</p>
<p><span class="math display">\[
\widehat{\boldsymbol{V}}_{1}=\left(\Delta \boldsymbol{X}^{\prime} \boldsymbol{Z} \widehat{\Omega}_{1}^{-1} \boldsymbol{Z}^{\prime} \Delta \boldsymbol{X}\right)^{-1}\left(\Delta \boldsymbol{X}^{\prime} \boldsymbol{Z} \widehat{\Omega}_{1}^{-1} \boldsymbol{Z}^{\prime} \widehat{\Omega}_{2} \boldsymbol{Z} \widehat{\Omega}_{1}^{-1} \boldsymbol{Z}^{\prime} \Delta \boldsymbol{X}\right)\left(\Delta \boldsymbol{X}^{\prime} \boldsymbol{Z} \widehat{\Omega}_{1}^{-1} \boldsymbol{Z}^{\prime} \Delta \boldsymbol{X}\right)^{-1}
\]</span></p>
<p>where</p>
<p><span class="math display">\[
\widehat{\Omega}_{2}=\sum_{i=1}^{N} \boldsymbol{Z}_{i}^{\prime} \widehat{\varepsilon}_{i} \widehat{\boldsymbol{\varepsilon}}_{i}^{\prime} \boldsymbol{Z}_{i}
\]</span></p>
<p>is a (scaled) cluster-robust estimator of <span class="math inline">\(\Omega\)</span> using the one-step residuals.</p>
<p>An asymptotically efficient two-step GMM estimator which allows heterskedasticity is</p>
<p><span class="math display">\[
\widehat{\alpha}_{2}=\left(\Delta \boldsymbol{X}^{\prime} \boldsymbol{Z} \widehat{\Omega}_{2}^{-1} \boldsymbol{Z}^{\prime} \Delta \boldsymbol{X}\right)^{-1}\left(\Delta \boldsymbol{X}^{\prime} \boldsymbol{Z} \widehat{\Omega}_{2}^{-1} \boldsymbol{Z}^{\prime} \Delta \boldsymbol{Y}\right) .
\]</span></p>
<p>Estimator (17.94) is known as the two-step Arellano-Bond GMM estimator. An appropriate robust covariance matrix estimator for <span class="math inline">\(\widehat{\alpha}_{2}\)</span> is</p>
<p><span class="math display">\[
\widehat{\boldsymbol{V}}_{2}=\left(\Delta \boldsymbol{X}^{\prime} \boldsymbol{Z} \widehat{\Omega}_{2}^{-1} \boldsymbol{Z}^{\prime} \Delta \boldsymbol{X}\right)^{-1}\left(\Delta \boldsymbol{X}^{\prime} \boldsymbol{Z} \widehat{\Omega}_{2}^{-1} \boldsymbol{Z}^{\prime} \widehat{\Omega}_{3} \boldsymbol{Z} \widehat{\Omega}_{2}^{-1} \boldsymbol{Z}^{\prime} \Delta \boldsymbol{X}\right)\left(\Delta \boldsymbol{X}^{\prime} \boldsymbol{Z} \widehat{\Omega}_{2}^{-1} \boldsymbol{Z}^{\prime} \Delta \boldsymbol{X}\right)^{-1}
\]</span></p>
<p>where</p>
<p><span class="math display">\[
\widehat{\Omega}_{3}=\sum_{i=1}^{N} \boldsymbol{Z}_{i}^{\prime} \widehat{\varepsilon}_{i} \widehat{\boldsymbol{\varepsilon}}_{i}^{\prime} \boldsymbol{Z}_{i}
\]</span></p>
<p>is a (scaled) cluster-robust estimator of <span class="math inline">\(\Omega\)</span> using the two-step residuals <span class="math inline">\(\widehat{\boldsymbol{\varepsilon}}_{i}=\Delta \boldsymbol{Y}_{i}-\Delta \boldsymbol{X}_{i} \widehat{\alpha}_{2}\)</span>. Asymptotically, <span class="math inline">\(\widehat{\boldsymbol{V}}_{2}\)</span> is equivalent to</p>
<p><span class="math display">\[
\widetilde{\boldsymbol{V}}_{2}=\left(\Delta \boldsymbol{X}^{\prime} \boldsymbol{Z} \widehat{\Omega}_{2}^{-1} \boldsymbol{Z}^{\prime} \Delta \boldsymbol{X}\right)^{-1} .
\]</span></p>
<p>The GMM estimator can be iterated until convergence to produce an iterated GMM estimator.</p>
<p>The advantage of the Arellano-Bond estimator over the Anderson-Hsiao estimator is that when <span class="math inline">\(T&gt;\)</span> <span class="math inline">\(p+2\)</span> the additional (overidentified) moment conditions reduce the asymptotic variance of the estimator and stabilize its performance. The disadvantage is that when <span class="math inline">\(T\)</span> is large using the full set of lags as instruments may cause a “many weak instruments” problem. The advised compromise is to limit the number of lags used as instruments.</p>
<p>The advantage of the one-step Arellano-Bond estimator is that the weight matrix <span class="math inline">\(\widehat{\Omega}_{1}\)</span> does not depend on residuals and is therefore less random than the two-step weight matrix <span class="math inline">\(\widehat{\Omega}_{2}\)</span>. This can result in better performance by the one-step estimator in small to moderate samples especially when the errors are approximately homoskedastic. The advantage of the two-step estimator is that it achieves asymptotic efficiency allowing for heteroskedasticity and is thus expected to perform better in large samples with non-homoskedastic errors.</p>
<p>To summarize, the Arellano-Bond estimator applies GMM to the first-differenced equation (17.87) using a set of available lags <span class="math inline">\(Y_{i, t-2}, Y_{i, t-3}, \ldots\)</span> as instruments for <span class="math inline">\(\Delta Y_{i, t-1}, \ldots, \Delta Y_{i, t-p}\)</span>.</p>
<p>The Arellano-Bond estimator may be obtained in Stata using either the xtabond or <span class="math inline">\(x t d p d\)</span> command. The default setting is the one-step estimator (17.91) and non-robust standard errors (17.92). For the twostep estimator and robust standard errors use the twostep vce (robust) options. Reported standard errors in Stata are based on Windmeijer’s (2005) finite-sample correction to the asymptotic estimator (17.96). The robust covariance matrix (17.95) nor the iterated GMM estimator are implemented.</p>
</section>
<section id="weak-instruments" class="level2" data-number="16.40">
<h2 data-number="16.40" class="anchored" data-anchor-id="weak-instruments"><span class="header-section-number">16.40</span> Weak Instruments</h2>
<p>Blundell and Bond (1998) pointed out that the Anderson-Hsiao and Arellano-Bond estimators suffer from weak instruments. This can be seen easiest in the AR(1) model with the Anderson-Hsiao estimator which uses <span class="math inline">\(Y_{i, t-2}\)</span> as an instrument for <span class="math inline">\(\Delta Y_{i, t-1}\)</span>. The reduced form equation for <span class="math inline">\(\Delta Y_{i t-1}\)</span> is</p>
<p><span class="math display">\[
\Delta Y_{i, t-1}=Y_{i, t-2} \gamma+v_{i t} .
\]</span></p>
<p>The reduced form coefficient <span class="math inline">\(\gamma\)</span> is defined by projection. Using <span class="math inline">\(\Delta Y_{i, t-1}=(\alpha-1) Y_{i, t-2}+u_{i}+\varepsilon_{i, t-1}\)</span> and <span class="math inline">\(\mathbb{E}\left[Y_{i t-2} \varepsilon_{i, t-1}\right]=0\)</span> we calculate that</p>
<p><span class="math display">\[
\gamma=\frac{\mathbb{E}\left[Y_{i, t-2} \Delta Y_{i, t-1}\right]}{\mathbb{E}\left[Y_{i t-2}^{2}\right]}=(\alpha-1)+\frac{\mathbb{E}\left[Y_{i, t-2} u_{i}\right]}{\mathbb{E}\left[Y_{i, t-2}^{2}\right]} .
\]</span></p>
<p>Assuming stationarity so that (17.83) holds,</p>
<p><span class="math display">\[
\mathbb{E}\left[Y_{i, t-2} u_{i}\right]=\mathbb{E}\left[\left(\frac{u_{i}}{1-\alpha}+\sum_{j=0}^{\infty} \alpha^{j} \varepsilon_{i, t-2-j}\right) u_{i}\right]=\frac{\sigma_{u}^{2}}{1-\alpha}
\]</span></p>
<p>and</p>
<p><span class="math display">\[
\mathbb{E}\left[Y_{i, t-2}^{2}\right]=\mathbb{E}\left[\left(\frac{u_{i}}{1-\alpha}+\sum_{j=0}^{\infty} \alpha^{j} \varepsilon_{i t-2-j}\right)^{2}\right]=\frac{\sigma_{u}^{2}}{(1-\alpha)^{2}}+\frac{\sigma_{\varepsilon}^{2}}{\left(1-\alpha^{2}\right)}
\]</span></p>
<p>where <span class="math inline">\(\sigma_{u}^{2}=\mathbb{E}\left[u_{i}^{2}\right]\)</span> and <span class="math inline">\(\sigma_{\varepsilon}^{2}=\mathbb{E}\left[\varepsilon_{i t}^{2}\right]\)</span>. Using these expressions and a fair amount of algebra, Blundell and Bond (1998) found that the reduced form coefficient equals</p>
<p><span class="math display">\[
\gamma=(\alpha-1)\left(\frac{k}{k+\sigma_{u}^{2} / \sigma_{\varepsilon}^{2}}\right)
\]</span></p>
<p>where <span class="math inline">\(k=(1-\alpha) /(1+\alpha)\)</span>. The Anderson-Hsiao instrument <span class="math inline">\(Y_{i, t-2}\)</span> is weak if <span class="math inline">\(\gamma\)</span> is close to zero. From (17.97) we see that <span class="math inline">\(\gamma=0\)</span> when either <span class="math inline">\(\alpha=1\)</span> (a unit root) or <span class="math inline">\(\sigma_{u}^{2} / \sigma_{\varepsilon}^{2}=\infty\)</span> (the idiosyncratic effect is small relative to the individualspecific effect). In either case the coefficient <span class="math inline">\(\alpha\)</span> is not identified. We know from our earlier study of the weak instruments problem (Section 12.36) that when <span class="math inline">\(\gamma\)</span> is close to zero then <span class="math inline">\(\alpha\)</span> is weakly identified and the estimators will perform poorly. This means that when the autoregressive coefficient <span class="math inline">\(\alpha\)</span> is large or the individual-specific effect dominates the idiosyncratic effect these estimators will be weakly identified, have poor performance, and conventional inference methods will be misleading. Since the value of <span class="math inline">\(\alpha\)</span> and the relative variances are unknown a priori this means that we should generically treat this class of estimators as weakly identified.</p>
<p>An alternative estimator which has improved performance is discussed in Section 17.42.</p>
</section>
<section id="dynamic-panels-with-predetermined-regressors" class="level2" data-number="16.41">
<h2 data-number="16.41" class="anchored" data-anchor-id="dynamic-panels-with-predetermined-regressors"><span class="header-section-number">16.41</span> Dynamic Panels with Predetermined Regressors</h2>
<p>The assumption that regressors are strictly exogenous is restrictive. A less restrictive assumption is that the regressors are predetermined. Dynamic panel methods can be modified to handle predetermined regressors by using their lags as instruments.</p>
<p>Definition 17.2 The regressor <span class="math inline">\(X_{i t}\)</span> is predetermined for the error <span class="math inline">\(\varepsilon_{i t}\)</span> if</p>
<p><span class="math display">\[
\mathbb{E}\left[X_{i, t-s} \varepsilon_{i t}\right]=0
\]</span></p>
<p>for all <span class="math inline">\(s \geq 0\)</span>.</p>
<p>The difference between strictly exogenous and predetermined regressors is that for the former (17.98) holds for all <span class="math inline">\(s\)</span> not just <span class="math inline">\(s \geq 0\)</span>. One way of interpreting a regression model with predetermined regressors is that the model is a projection on the complete past history of the regressors.</p>
<p>Under (17.98), leads of <span class="math inline">\(X_{i t}\)</span> can be correlated with <span class="math inline">\(\varepsilon_{i t}\)</span>, that is <span class="math inline">\(\mathbb{E}\left[X_{i t+s} \varepsilon_{i t}\right] \neq 0\)</span> for <span class="math inline">\(s \geq 1\)</span>, or equivalently <span class="math inline">\(X_{i t}\)</span> can be correlated with lags of <span class="math inline">\(\varepsilon_{i j}\)</span>, that is <span class="math inline">\(\mathbb{E}\left[X_{i t} \varepsilon_{i t-s}\right] \neq 0\)</span> for <span class="math inline">\(s \geq 1\)</span>. This means that <span class="math inline">\(X_{i t}\)</span> can respond dynamically to past values of <span class="math inline">\(Y_{i t}\)</span>, as in, for example, an unrestricted vector autoregression.</p>
<p>Consider the differenced equation (17.87)</p>
<p><span class="math display">\[
\Delta Y_{i t}=\alpha_{1} \Delta Y_{i, t-1}+\alpha_{2} \Delta Y_{i, t-2}+\cdots+\alpha_{p} \Delta Y_{i, t-p}+\Delta X_{i t}^{\prime} \beta+\Delta \varepsilon_{i t} .
\]</span></p>
<p>When the regressors are predetermined but not strictly exogenous, <span class="math inline">\(X_{i t}\)</span> and <span class="math inline">\(\varepsilon_{i t}\)</span> are uncorrelated but <span class="math inline">\(\Delta X_{i t}\)</span> and <span class="math inline">\(\Delta \varepsilon_{i t}\)</span> are correlated. To see this,</p>
<p><span class="math display">\[
\begin{aligned}
\mathbb{E}\left[\Delta X_{i t} \Delta \varepsilon_{i t}\right] &amp;=\mathbb{E}\left[X_{i t} \varepsilon_{i t}\right]-\mathbb{E}\left[X_{i, t-1} \varepsilon_{i t}\right]-\mathbb{E}\left[X_{i t} \varepsilon_{i, t-1}\right]+\mathbb{E}\left[X_{i, t-1} \varepsilon_{i, t-1}\right] \\
&amp;=-\mathbb{E}\left[X_{i t} \varepsilon_{i, t-1}\right] \neq 0 .
\end{aligned}
\]</span></p>
<p>This means that if we treat <span class="math inline">\(\Delta X_{i t}\)</span> as exogenous the coefficient estimates will be biased.</p>
<p>To solve the correlation problem we can use instruments for <span class="math inline">\(\Delta X_{i t}\)</span>. A valid instrument is <span class="math inline">\(X_{i, t-1}\)</span> because it is generally correlated with <span class="math inline">\(\Delta X_{i t}\)</span> yet uncorrelated with <span class="math inline">\(\Delta \varepsilon_{i t}\)</span>. Indeed, for any <span class="math inline">\(s \geq 1\)</span></p>
<p><span class="math display">\[
\mathbb{E}\left[X_{i, t-s} \Delta \varepsilon_{i t}\right]=\mathbb{E}\left[X_{i, t-s} \varepsilon_{i t}\right]-\mathbb{E}\left[X_{i, t-s} \varepsilon_{i, t-1}\right]=0 .
\]</span></p>
<p>Consequently, Arellano and Bond (1991) recommend the instrument set <span class="math inline">\(\left(X_{i 1}, X_{i 2}, \ldots, X_{i t-1}\right)\)</span>. When the number of time periods is large it is advised to limit the number of instrument lags to avoid the many weak instruments problem. Algebraically, GMM estimation is the same as the estimators described in Section 17.39, except that the instrument matrix (17.89) is modified to</p>
<p><img src="images//2022_10_23_acbfcce1ea7ce1901e2dg-46.jpg" class="img-fluid"></p>
<p>To understand how the model is identified we examine the reduced form equation for the regressor. For <span class="math inline">\(t=p+2\)</span> and using the first lag as an instrument the reduced form is</p>
<p><span class="math display">\[
\Delta X_{i t}=\gamma_{1} Y_{i, t-2}+\Gamma_{2} X_{i, t-1}+\zeta_{i t} .
\]</span></p>
<p>The model is identified if <span class="math inline">\(\Gamma_{2}\)</span> is full rank. This is valid (in general) when <span class="math inline">\(X_{i t}\)</span> is stationary. Identification fails, however, when <span class="math inline">\(X_{i t}\)</span> has a unit root. This indicates that the model will be weakly identified when the predetermined regressors are highly persistent.</p>
<p>The method generalizes to handle multiple lags of the predetermined regressors. To see this, write the model explicitly as</p>
<p><span class="math display">\[
Y_{i t}=\alpha_{1} Y_{i, t-1}+\cdots+\alpha_{p} Y_{i, t-p}+X_{i t}^{\prime} \beta_{1}+\cdots+X_{i, t-q}^{\prime} \beta_{q}+u_{i}+\varepsilon_{i t} .
\]</span></p>
<p>In first differences the model is</p>
<p><span class="math display">\[
\Delta Y_{i t}=\alpha_{1} \Delta Y_{i, t-1}+\cdots+\alpha_{p} \Delta Y_{i, t-p}+\Delta X_{i t}^{\prime} \beta_{1}+\cdots+\Delta X_{i, t-q}^{\prime} \beta_{q}+\Delta \varepsilon_{i t} .
\]</span></p>
<p>A sufficient set of instruments for the regressors are <span class="math inline">\(\left(X_{i t-1}, \Delta X_{i, t-1}, \ldots, \Delta X_{i, t-q}\right)\)</span> or equivalently <span class="math inline">\(\left(X_{i, t-1}, X_{i, t-2}, \ldots, X_{i, t-q-1}\right)\)</span>.</p>
<p>In many cases it is more reasonable to assume that <span class="math inline">\(X_{i t-1}\)</span> is predetermined but not <span class="math inline">\(X_{i t}\)</span>, because <span class="math inline">\(X_{i t}\)</span> and <span class="math inline">\(\varepsilon_{i t}\)</span> may be endogenous. This, for example, is the standard assumption in vector autoregressions. In this case the estimation method is modified to use the instruments <span class="math inline">\(\left(X_{i, t-2}, X_{i, t-3}, \ldots, X_{i, t-q-1}\right)\)</span>. While this weakens the exogeneity assumption it also weakens the instrument set as now the reduced form uses the second lag <span class="math inline">\(X_{i, t-2}\)</span> to predict <span class="math inline">\(\Delta X_{i t}\)</span>.</p>
<p>The advantage obtained by treating a regressor as predetermined (rather than strictly exogenous) is that it is a substantial relaxation of the dynamic assumptions. Otherwise the parameter estimates will be inconsistent due to endogeneity.</p>
<p>The major disadvantage of treating a regressor as predetermined is that it substantially reduces the strength of identification especially when the predetermined regressors are highly persistent.</p>
<p>In Stata the xtabond command by default treats independent regressors as strictly exogenous. To treat the regressors as predetermined use the option pre. By default all regressor lags are used as instruments, but the number can be limited if specified.</p>
</section>
<section id="blundell-bond-estimator" class="level2" data-number="16.42">
<h2 data-number="16.42" class="anchored" data-anchor-id="blundell-bond-estimator"><span class="header-section-number">16.42</span> Blundell-Bond Estimator</h2>
<p>Arellano and Bover (1995) and Blundell and Bond (1998) introduced a set of orthogonality conditions which reduce the weak instrument problem discussed in the Section <span class="math inline">\(17.40\)</span> and improve performance in finite samples.</p>
<p>Consider the levels AR(1) model with no regressors (17.82). Recall, least squares (pooled) regression is inconsistent because the regressor <span class="math inline">\(Y_{i, t-1}\)</span> is correlated with the error <span class="math inline">\(u_{i}\)</span>. This raises the question: Is there an instrument <span class="math inline">\(Z_{i t}\)</span> which solves this problem in the sense that <span class="math inline">\(Z_{i t}\)</span> is correlated with <span class="math inline">\(Y_{i, t-1}\)</span> yet uncorrelated with <span class="math inline">\(u_{i t}+\varepsilon_{i t}\)</span> ? Blundell-Bond propose the instrument <span class="math inline">\(\Delta Y_{i, t-1}\)</span>. Clearly, <span class="math inline">\(\Delta Y_{i, t-1}\)</span> and <span class="math inline">\(Y_{i, t-1}\)</span> are correlated so <span class="math inline">\(\Delta Y_{i, t-1}\)</span> satisfies the relevance condition. Also, <span class="math inline">\(\Delta Y_{i, t-1}\)</span> is uncorrelated with the idiosyncratic error <span class="math inline">\(\varepsilon_{i t}\)</span> when the latter is serially uncorrelated. Thus the key to the Blundell-Bond instrument is whether or not</p>
<p><span class="math display">\[
\mathbb{E}\left[\Delta Y_{i t-1} u_{i}\right]=0 .
\]</span></p>
<p>Blundell and Bond (1998) show that a sufficient condition for (17.100) is</p>
<p><span class="math display">\[
\mathbb{E}\left[\left(Y_{i 1}-\frac{u_{i}}{1-\alpha}\right) u_{i}\right]=0 .
\]</span></p>
<p>Recall that <span class="math inline">\(u_{i} /(1-\alpha)\)</span> is the conditional mean of <span class="math inline">\(Y_{i t}\)</span> under stationarity. Condition (17.101) states that the deviation of the initial condition <span class="math inline">\(Y_{i 1}\)</span> from this conditional mean is uncorrelated with the individual effect <span class="math inline">\(u_{i}\)</span>. Condition (17.101) is implied by stationarity but is somewhat weaker.</p>
<p>To see that (17.101) implies (17.100), by applying recursion to (17.87) we find that</p>
<p><span class="math display">\[
\Delta Y_{i, t-1}=\alpha^{t-3} \Delta Y_{i 2}+\sum_{j=0}^{t-3} \alpha^{j} \Delta \varepsilon_{i, t-1-j} .
\]</span></p>
<p>Also,</p>
<p><span class="math display">\[
\Delta Y_{i 2}=(\alpha-1) Y_{i 1}+u_{i}+\varepsilon_{i 2}=(\alpha-1)\left(Y_{i 1}-\frac{u_{i}}{1-\alpha}\right)+\varepsilon_{i 2} .
\]</span></p>
<p>Hence</p>
<p><span class="math display">\[
\begin{aligned}
\mathbb{E}\left[\Delta Y_{i, t-1} u_{i}\right] &amp;=\mathbb{E}\left[\left(\alpha^{t-3}(\alpha-1)\left(Y_{i 1}-\frac{u_{i}}{1-\alpha}\right)+\alpha^{t-3} \varepsilon_{i 2}+\sum_{j=0}^{t-3} \alpha^{j} \Delta \varepsilon_{i, t-1-j}\right) u_{i}\right] \\
&amp;=\alpha^{t-3}(\alpha-1) \mathbb{E}\left[\left(Y_{i 1}-\frac{u_{i}}{1-\alpha}\right) u_{i}\right] \\
&amp;=0
\end{aligned}
\]</span></p>
<p>under (17.101), as claimed.</p>
<p>Now consider the full model (17.81) with predetermined regressors. Consider the assumption that the regressors have constant correlation with the individual effect</p>
<p><span class="math display">\[
\mathbb{E}\left[X_{i t} u_{i}\right]=\mathbb{E}\left[X_{i s} u_{i}\right]
\]</span></p>
<p>for all <span class="math inline">\(s\)</span>. This implies</p>
<p><span class="math display">\[
\mathbb{E}\left[\Delta X_{i t} u_{i}\right]=0
\]</span></p>
<p>which means that the differenced predetermined regressors <span class="math inline">\(\Delta X_{i t}\)</span> can also be used as instruments for the level equation.</p>
<p>Using (17.100) and (17.102) Blundell and Bond propose the following moment conditions for GMM estimation</p>
<p><span class="math display">\[
\begin{gathered}
\mathbb{E}\left[\Delta Y_{i, t-1}\left(Y_{i t}-\alpha_{1} Y_{i, t-1}-\cdots-\alpha_{p} Y_{i, t-p}-X_{i t}^{\prime} \beta\right)\right]=0 \\
\mathbb{E}\left[\Delta X_{i, t}\left(Y_{i t}-\alpha_{1} Y_{i, t-1}-\cdots-\alpha_{p} Y_{i, t-p}-X_{i t}^{\prime} \beta\right)\right]=0
\end{gathered}
\]</span></p>
<p>for <span class="math inline">\(t=p+2, \ldots, T\)</span>. Notice that these are for the levels (undifferenced) equation while the Arellano-Bond (17.90) moments are for the differenced equation (17.87). We can write (17.103)-(17.104) in vector notation if we set <span class="math inline">\(\boldsymbol{Z}_{2 i}=\operatorname{diag}\left(\Delta Y_{i 2}, \ldots, \Delta Y_{i T-1}, \Delta X_{i 3}, \ldots, \Delta X_{i T}\right)\)</span>. Then (17.103)-(17.104) equals</p>
<p><span class="math display">\[
\mathbb{E}\left[\boldsymbol{Z}_{2 i}\left(\boldsymbol{Y}_{i}-\boldsymbol{X}_{i} \theta\right)\right]=0 .
\]</span></p>
<p>Blundell and Bond proposed combining the <span class="math inline">\(\ell\)</span> Arellano-Bond moments with the levels moments. This can be done by stacking the moment conditions (17.90) and (17.105). Recall from Section <span class="math inline">\(17.39\)</span> the variables <span class="math inline">\(\Delta \boldsymbol{Y}_{i}, \Delta \boldsymbol{X}_{i}\)</span>, and <span class="math inline">\(\boldsymbol{Z}_{i}\)</span>. Define the stacked variables <span class="math inline">\(\overline{\boldsymbol{Y}}_{i}=\left(\Delta \boldsymbol{Y}_{i}^{\prime}, \boldsymbol{Y}_{i}^{\prime}\right)^{\prime}, \overline{\boldsymbol{X}}_{i}=\left(\Delta \boldsymbol{X}_{i}^{\prime}, \boldsymbol{X}_{i}^{\prime}\right)^{\prime}\)</span> and <span class="math inline">\(\overline{\boldsymbol{Z}}_{i}=\)</span> <span class="math inline">\(\operatorname{diag}\left(\boldsymbol{Z}_{i}, \boldsymbol{Z}_{2 i}\right)\)</span>. The stacked moment conditions are</p>
<p><span class="math display">\[
\mathbb{E}\left[\overline{\boldsymbol{Z}}_{i}\left(\overline{\boldsymbol{Y}}_{i}-\overline{\boldsymbol{X}}_{i} \theta\right)\right]=0 .
\]</span></p>
<p>The Blundell-Bond estimator is found by applying GMM to this equation. They call this a systems GMM estimator. Let <span class="math inline">\(\overline{\boldsymbol{Y}}, \overline{\boldsymbol{X}}\)</span>, and <span class="math inline">\(\overline{\boldsymbol{Z}}\)</span> denote <span class="math inline">\(\overline{\boldsymbol{Y}}_{i}, \overline{\boldsymbol{X}}_{i}\)</span>, and <span class="math inline">\(\overline{\boldsymbol{Z}}_{i}\)</span> stacked into matrices. Define <span class="math inline">\(\overline{\boldsymbol{H}}=\operatorname{diag}\left(\boldsymbol{H}, \boldsymbol{I}_{T-2}\right)\)</span> where <span class="math inline">\(\boldsymbol{H}\)</span> is from (17.31) and set</p>
<p><span class="math display">\[
\widehat{\Omega}_{1}=\sum_{i=1}^{N} \overline{\boldsymbol{Z}}_{i}^{\prime} \overline{\boldsymbol{Z Z}}_{i} .
\]</span></p>
<p>The Blundell-Bond one-step GMM estimator is</p>
<p><span class="math display">\[
\widehat{\theta}_{1}=\left(\overline{\boldsymbol{X}}^{\prime} \overline{\boldsymbol{Z}} \widehat{\Omega}_{1}^{-1} \overline{\boldsymbol{Z}}^{\prime} \overline{\boldsymbol{X}}\right)^{-1}\left(\overline{\boldsymbol{X}}^{\prime} \overline{\boldsymbol{Z}} \widehat{\Omega}_{1}^{-1} \overline{\boldsymbol{Z}}^{\prime} \overline{\boldsymbol{Y}}\right) .
\]</span></p>
<p>The systems residuals are <span class="math inline">\(\widehat{\boldsymbol{\varepsilon}}_{i}=\overline{\boldsymbol{Y}}_{i}-\overline{\boldsymbol{X}}_{i} \widehat{\theta}_{1}\)</span>. A robust covariance matrix estimator is</p>
<p><span class="math display">\[
\widehat{\boldsymbol{V}}_{1}=\left(\overline{\boldsymbol{X}}^{\prime} \overline{\boldsymbol{Z}} \widehat{\Omega}_{1}^{-1} \overline{\boldsymbol{Z}}^{\prime} \overline{\boldsymbol{X}}\right)^{-1}\left(\overline{\boldsymbol{X}}^{\prime} \overline{\boldsymbol{Z}} \widehat{\Omega}_{1}^{-1} \overline{\boldsymbol{Z}}^{\prime} \widehat{\Omega}_{2} \overline{\boldsymbol{Z}} \widehat{\Omega}_{1}^{-1} \overline{\boldsymbol{Z}}^{\prime} \overline{\boldsymbol{X}}\right)\left(\overline{\boldsymbol{X}}^{\prime} \overline{\boldsymbol{Z}} \widehat{\Omega}_{1}^{-1} \overline{\boldsymbol{Z}}^{\prime} \overline{\boldsymbol{X}}\right)^{-1}
\]</span></p>
<p>where</p>
<p><span class="math display">\[
\widehat{\Omega}_{2}=\sum_{i=1}^{N} \overline{\boldsymbol{Z}}_{i}^{\prime} \widehat{\boldsymbol{\varepsilon}}_{i} \widehat{\boldsymbol{\varepsilon}}_{i}^{\prime} \overline{\boldsymbol{Z}}_{i} .
\]</span></p>
<p>The Blundell-Bond two-step GMM estimator is</p>
<p><span class="math display">\[
\widehat{\theta}_{2}=\left(\overline{\boldsymbol{X}}^{\prime} \overline{\boldsymbol{Z}} \widehat{\Omega}_{2}^{-1} \overline{\boldsymbol{Z}}^{\prime} \overline{\boldsymbol{X}}\right)^{-1}\left(\overline{\boldsymbol{X}}^{\prime} \overline{\boldsymbol{Z}}^{-1} \overline{\boldsymbol{Z}}^{\prime} \overline{\boldsymbol{Y}}\right) .
\]</span></p>
<p>The two-step systems residuals are <span class="math inline">\(\widehat{\boldsymbol{\varepsilon}}_{i}=\overline{\boldsymbol{Y}}_{i}-\overline{\boldsymbol{X}}_{i} \widehat{\theta}_{2}\)</span>. A robust covariance matrix estimator is</p>
<p><span class="math display">\[
\widehat{\boldsymbol{V}}_{2}=\left(\overline{\boldsymbol{X}}^{\prime} \overline{\boldsymbol{Z}} \widehat{\Omega}_{2}^{-1} \overline{\boldsymbol{Z}}^{\prime} \overline{\boldsymbol{X}}\right)^{-1}\left(\overline{\boldsymbol{X}}^{\prime} \overline{\boldsymbol{Z}} \widehat{\Omega}_{2}^{-1} \overline{\boldsymbol{Z}}^{\prime} \widehat{\Omega}_{3} \overline{\boldsymbol{Z}} \widehat{\Omega}_{2}^{-1} \overline{\boldsymbol{Z}}^{\prime} \overline{\boldsymbol{X}}\right)\left(\overline{\boldsymbol{X}}^{\prime} \overline{\boldsymbol{Z}}_{2}^{-1} \overline{\boldsymbol{Z}}^{\prime} \overline{\boldsymbol{X}}\right)^{-1}
\]</span></p>
<p>where</p>
<p><span class="math display">\[
\widehat{\Omega}_{3}=\sum_{i=1}^{N} \overline{\boldsymbol{Z}}_{i}^{\prime} \widehat{\boldsymbol{\varepsilon}}_{i} \widehat{\boldsymbol{\varepsilon}}_{i}^{\prime} \overline{\boldsymbol{Z}}_{i} .
\]</span></p>
<p>Asymptotically, <span class="math inline">\(\widehat{\boldsymbol{V}}_{2}\)</span> is equivalent to</p>
<p><span class="math display">\[
\widetilde{\boldsymbol{V}}_{2}=\left(\overline{\boldsymbol{X}}^{\prime} \overline{\boldsymbol{Z}} \widehat{\Omega}_{2}^{-1} \overline{\boldsymbol{Z}}^{\prime} \overline{\boldsymbol{X}}\right)^{-1} .
\]</span></p>
<p>The GMM estimator can be iterated until convergence to produce an iterated GMM estimator.</p>
<p>Simulation experiments reported in Blundell and Bond (1998) indicate that their systems GMM estimator performs substantially better than the Arellano-Bond estimator, especially when <span class="math inline">\(\alpha\)</span> is close to one or the variance ratio <span class="math inline">\(\sigma_{u}^{2} / \sigma_{\varepsilon}^{2}\)</span> is large. The explanation is that the orthogonality condition (17.103) does not suffer the weak instrument problem in these cases.</p>
<p>The advantage of the Blundell-Bond estimator is that the added orthogonality condition (17.103) greatly improves performance relative to the Arellano-Bond estimator when the latter is weakly identified. A disadvantage of the Blundell-Bond estimator is that their orthogonality condition is justified by a stationarity condition (17.101) and violation of the latter may induce estimation bias.</p>
<p>The advantages and disadvantages of the one-step versus two-step Blundell-Bond estimators are the same as described for the Arellano-Bond estimator as described in Section 17.39. Also as described there when <span class="math inline">\(T\)</span> is large it may be desired to limit the number of lags to use as instruments in order to avoid the many weak instruments problem.</p>
<p>The Blundell-Bond estimator may be obtained in Stata using either the xtdpdsys or xtdpd command. The default setting is the one-step estimator (17.106) and non-robust standard errors. For the two-step estimator and robust standard errors use the twostep vce (robust) options. Stata standard errors are Windmeijer’s (2005) finite-sample correction to the asymptotic estimate (17.110). The robust covariance matrix estimator (17.109) nor the iterated GMM estimator are implemented.</p>
</section>
<section id="forward-orthogonal-transformation" class="level2" data-number="16.43">
<h2 data-number="16.43" class="anchored" data-anchor-id="forward-orthogonal-transformation"><span class="header-section-number">16.43</span> Forward Orthogonal Transformation</h2>
<p>Arellano and Bover (1995) proposed an alternative transformation which eliminates the individualspecific effect and may have advantages in dynamic panel models. The forward orthogonal transformation is</p>
<p><span class="math display">\[
Y_{i t}^{*}=c_{i t}\left(Y_{i t}-\frac{1}{T_{i}-t}\left(Y_{i, t+1}+\cdots+Y_{i T_{i}}\right)\right)
\]</span></p>
<p>where <span class="math inline">\(c_{i t}^{2}=\left(T_{i}-t\right) /\left(T_{i}-t+1\right)\)</span>. This can be applied to all but the final observation (which is lost). Essentially, <span class="math inline">\(Y_{i t}^{*}\)</span> subtracts from <span class="math inline">\(Y_{i t}\)</span> the average of the remaining values and then rescales so that the variance is constant under the assumption of homoskedastic errors. The transformation (17.111) was originally proposed for time-series observations by Hayashi and Sims (1983).</p>
<p>At the level of the individual this can be written as <span class="math inline">\(\boldsymbol{Y}_{i}^{*}=\boldsymbol{A}_{i} \boldsymbol{Y}_{i}\)</span> where <span class="math inline">\(\boldsymbol{A}_{i}\)</span> is the <span class="math inline">\(\left(T_{i}-1\right) \times T_{i}\)</span> orthogonal deviation operator</p>
<p><span class="math display">\[
\boldsymbol{A}_{i}=\operatorname{diag}\left(\sqrt{\frac{T_{i}-1}{T_{i}}}, \ldots, \sqrt{\frac{1}{2}}\right)\left[\begin{array}{ccccccc}
1 &amp; -\frac{1}{T_{i}-1} &amp; -\frac{1}{T_{i}-1} &amp; \cdots &amp; -\frac{1}{T_{i}-1} &amp; -\frac{1}{T_{i}-1} &amp; -\frac{1}{T_{i}-1} \\
0 &amp; 1 &amp; -\frac{1}{T_{i}-2} &amp; \cdots &amp; -\frac{1}{T_{i}-2} &amp; -\frac{1}{T_{i}-2} &amp; -\frac{1}{T_{i}-2} \\
\vdots &amp; \vdots &amp; \vdots &amp; &amp; \vdots &amp; \vdots &amp; \vdots \\
0 &amp; 0 &amp; 0 &amp; \cdots &amp; 1 &amp; -\frac{1}{2} &amp; -\frac{1}{2} \\
0 &amp; 0 &amp; 0 &amp; \cdots &amp; 0 &amp; -1 &amp; 1
\end{array}\right] .
\]</span></p>
<p>Important properties of the matrix <span class="math inline">\(\boldsymbol{A}_{i}\)</span> are that <span class="math inline">\(\boldsymbol{A}_{i} \mathbf{1}_{i}=0\)</span> (so it eliminates individual effects), <span class="math inline">\(\boldsymbol{A}_{i}^{\prime} \boldsymbol{A}_{i}=\boldsymbol{M}_{i}\)</span>, and <span class="math inline">\(\boldsymbol{A}_{i} \boldsymbol{A}_{i}^{\prime}=\boldsymbol{I}_{T_{i}-1}\)</span>. These can be verified by direct multiplication.</p>
<p>Applying the transformation <span class="math inline">\(\boldsymbol{A}_{i}\)</span> to (17.81) we obtain</p>
<p><span class="math display">\[
Y_{i t}^{*}=\alpha_{1} Y_{i, t-1}^{*}+\cdots+\alpha_{p} Y_{i, t-p}^{*}+X_{i t}^{* \prime} \beta+\varepsilon_{i t}^{*} .
\]</span></p>
<p>for <span class="math inline">\(t=p+1, \ldots, T-1\)</span>. This is equivalent to first differencing (17.87) when <span class="math inline">\(T=3\)</span> but differs for <span class="math inline">\(T&gt;3\)</span>.</p>
<p>What is special about the transformed equation (17.112) is that under the assumption that <span class="math inline">\(\varepsilon_{i t}\)</span> are serially uncorrelated and homoskedastic the error vector <span class="math inline">\(\boldsymbol{\varepsilon}_{i}^{*}\)</span> has variance <span class="math inline">\(\sigma_{\varepsilon}^{2} \boldsymbol{A}_{i} \boldsymbol{A}_{i}^{\prime}=\sigma_{\varepsilon}^{2} \boldsymbol{I}_{T_{i}-1}\)</span>. This means that <span class="math inline">\(\varepsilon_{i}^{*}\)</span> has the same covariance structure as <span class="math inline">\(\varepsilon_{i}\)</span>. Thus the orthogonal transformation operator eliminates the fixed effect while preserving the covariance structure. This is in contrast to (17.87) which has serially correlated errors <span class="math inline">\(\Delta \varepsilon_{i t}\)</span>.</p>
<p>The transformed error <span class="math inline">\(\varepsilon_{i t}^{*}\)</span> is a function of <span class="math inline">\(\varepsilon_{i t}, \varepsilon_{i t+1}, \ldots, \varepsilon_{i T}\)</span>. Thus valid instruments are <span class="math inline">\(Y_{i t-1}, Y_{i t-2}, \ldots\)</span>. Using the instrument matrix <span class="math inline">\(Z_{i}\)</span> from (17.89) in the case of strictly exogenous regressors or (17.99) with predetermined regressors the <span class="math inline">\(\ell\)</span> moment conditions can be written using matrix notation as</p>
<p><span class="math display">\[
\mathbb{E}\left[\boldsymbol{Z}_{i}^{\prime}\left(\boldsymbol{Y}_{i}^{*}-\boldsymbol{X}_{i}^{*} \theta\right)\right]=0 .
\]</span></p>
<p>Define the <span class="math inline">\(\ell \times \ell\)</span> covariance matrix</p>
<p><span class="math display">\[
\Omega=\mathbb{E}\left[Z_{i}^{\prime} \varepsilon_{i}^{*} \varepsilon_{i}^{* \prime} Z_{i}\right]
\]</span></p>
<p>If the errors <span class="math inline">\(\varepsilon_{i t}\)</span> are conditionally homoskedastic then <span class="math inline">\(\Omega=\mathbb{E}\left[\boldsymbol{Z}_{i}^{\prime} \boldsymbol{Z}_{i}\right] \sigma_{\varepsilon}^{2}\)</span>. Thus an asymptotically efficient GMM estimator is 2SLS applied to the orthogonalized equation using <span class="math inline">\(Z_{i}\)</span> as an instrument. In matrix notation,</p>
<p><span class="math display">\[
\widehat{\theta}_{1}=\left(\boldsymbol{X}^{* \prime} \boldsymbol{Z}\left(\boldsymbol{Z}^{\prime} \boldsymbol{Z}\right)^{-1} \boldsymbol{Z}^{\prime} \boldsymbol{X}^{*}\right)^{-1} \boldsymbol{Y}^{*} \text {. }
\]</span></p>
<p>This is the one-step GMM estimator.</p>
<p>Given the residuals <span class="math inline">\(\widehat{\boldsymbol{\varepsilon}}_{i}=\boldsymbol{Y}_{i}^{*}-\boldsymbol{X}_{i}^{*} \widehat{\theta}_{1}\)</span> the two-step GMM estimator which is robust to heteroskedasticity and arbitrary serial correlation is</p>
<p><span class="math display">\[
\widehat{\theta}_{2}=\left(\boldsymbol{X}^{* \prime} \boldsymbol{Z} \widehat{\Omega}_{2}^{-1} \boldsymbol{Z}^{\prime} \boldsymbol{X}^{*}\right)^{-1}\left(\boldsymbol{X}^{* \prime} \boldsymbol{Z} \widehat{\Omega}_{2}^{-1} \boldsymbol{Z}^{\prime} \boldsymbol{Y}^{*}\right)
\]</span></p>
<p>where</p>
<p><span class="math display">\[
\widehat{\Omega}_{2}=\sum_{i=1}^{N} \boldsymbol{Z}_{i}^{\prime} \widehat{\boldsymbol{\varepsilon}}_{i} \widehat{\boldsymbol{\varepsilon}}_{i}^{\prime} \boldsymbol{Z}_{i} .
\]</span></p>
<p>Standard errors for <span class="math inline">\(\widehat{\theta}_{1}\)</span> and <span class="math inline">\(\widehat{\theta}_{2}\)</span> can be obtained using cluster-robust methods.</p>
<p>Forward orthogonalization may have advantages over first differencing. First, the equation errors in (17.112) have a scalar covariance structure under i.i.d. idiosyncratic errors which is expected to improve estimation precision. It also implies that the one-step estimator is 2SLS rather than GMM. Second, while there has not been a formal analysis of the weak instrument properties of the estimators after forward orthogonalization it appears that if <span class="math inline">\(T&gt;p+2\)</span> the method is less affected by weak instruments than first differencing. The disadvantages of forward orthogonalization are that it treats early observations asymmetrically from late observations, it is less thoroughly studied than first differencing, and is not available with several popular estimation methods.</p>
<p>The Stata command xtdpd includes forward orthogonalization as an option but not when levels (Blundell-Bond) instruments are included or if there are gaps in the data. An alternative is the downloadable Stata package <span class="math inline">\(x\)</span> tabond2.</p>
</section>
<section id="empirical-illustration" class="level2" data-number="16.44">
<h2 data-number="16.44" class="anchored" data-anchor-id="empirical-illustration"><span class="header-section-number">16.44</span> Empirical Illustration</h2>
<p>We illustrate the dynamic panel methods with the investment model (17.3). Estimates from two models are presented in Table 17.3. Both are estimated by Blundell-Bond two-step GMM with lags 2 through 6 as instruments, a cluster-robust weight matrix, and clustered standard errors.</p>
<p>The first column presents estimates of an AR(2) model. The estimates show that the series has a moderate amount of positive serial correlation but appears to be well modeled as an AR(1) as the AR(2) coefficient is close to zero. This pattern of serial correlation is consistent with the presence of investment projects which span two years.</p>
<p>The second column presents estimates of the dynamic version of the investment regression (17.3) excluding the trading indicator. Two lags are included of the dependent variable and each regressor. The regressors are treated as predetermined in contrast to the fixed effects regressions which treated the regressors as strictly exogenous. The regressors are not contemporaneous with the dependent variable but lagged one and two periods. This is done so that they are valid predetermined variables. Contemporaneous variables are likely endogenous so should not be treated as predetermined.</p>
<p>The estimates in the second column of Table <span class="math inline">\(17.3\)</span> complement the earlier results. The evidence shows that investment has a moderate degree of serial dependence, is positively related to the first lag of Q, and is negatively related to lagged debt. Investment appears to be positively related to change in cash flow, rather than the level. Thus an increase in cash flow in year <span class="math inline">\(t-1\)</span> leads to investment in year <span class="math inline">\(t\)</span>. Table 17.3: Estimates of Dynamic Investment Equation</p>
<table class="table">
<thead>
<tr class="header">
<th></th>
<th>AR(2)</th>
<th>AR(2) with Regressors</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(I_{i t-1}\)</span></td>
<td><span class="math inline">\(0.3191\)</span></td>
<td><span class="math inline">\(0.2519\)</span></td>
</tr>
<tr class="even">
<td></td>
<td><span class="math inline">\((0.0172)\)</span></td>
<td><span class="math inline">\((0.0220)\)</span></td>
</tr>
<tr class="odd">
<td><span class="math inline">\(I_{i t-2}\)</span></td>
<td><span class="math inline">\(0.0309\)</span></td>
<td><span class="math inline">\(0.0137\)</span></td>
</tr>
<tr class="even">
<td></td>
<td><span class="math inline">\((0.0112)\)</span></td>
<td><span class="math inline">\((0.0125)\)</span></td>
</tr>
<tr class="odd">
<td><span class="math inline">\(Q_{i t-1}\)</span></td>
<td></td>
<td><span class="math inline">\(0.0018\)</span></td>
</tr>
<tr class="even">
<td></td>
<td></td>
<td><span class="math inline">\((0.0007)\)</span></td>
</tr>
<tr class="odd">
<td><span class="math inline">\(Q_{i t-2}\)</span></td>
<td></td>
<td><span class="math inline">\(-0.0000\)</span></td>
</tr>
<tr class="even">
<td></td>
<td></td>
<td><span class="math inline">\((0.0003)\)</span></td>
</tr>
<tr class="odd">
<td><span class="math inline">\(D_{i t-1}\)</span></td>
<td></td>
<td><span class="math inline">\(-0.0154\)</span></td>
</tr>
<tr class="even">
<td></td>
<td></td>
<td><span class="math inline">\((0.0058)\)</span></td>
</tr>
<tr class="odd">
<td><span class="math inline">\(D_{i t-2}\)</span></td>
<td></td>
<td><span class="math inline">\(-0.0043\)</span></td>
</tr>
<tr class="even">
<td></td>
<td></td>
<td><span class="math inline">\((0.0054)\)</span></td>
</tr>
<tr class="odd">
<td><span class="math inline">\(C F_{i t-1}\)</span></td>
<td></td>
<td><span class="math inline">\(0.0400\)</span></td>
</tr>
<tr class="even">
<td></td>
<td></td>
<td><span class="math inline">\((0.0091)\)</span></td>
</tr>
<tr class="odd">
<td><span class="math inline">\(C F_{i t-2}\)</span></td>
<td></td>
<td><span class="math inline">\(-0.0290\)</span></td>
</tr>
<tr class="even">
<td></td>
<td></td>
<td><span class="math inline">\((0.0051)\)</span></td>
</tr>
</tbody>
</table>
<p>Two-step GMM estimates. Cluster-robust standard errors in parenthesis.</p>
<p>All regressions include time effects. GMM instruments include lags 2 through 6.</p>
</section>
<section id="exercises" class="level2" data-number="16.45">
<h2 data-number="16.45" class="anchored" data-anchor-id="exercises"><span class="header-section-number">16.45</span> Exercises</h2>
</section>
<section id="exercise-17.1" class="level2" data-number="16.46">
<h2 data-number="16.46" class="anchored" data-anchor-id="exercise-17.1"><span class="header-section-number">16.46</span> Exercise 17.1</h2>
<ol type="a">
<li><p>Show (17.11) and (17.12).</p></li>
<li><p>Show (17.13).</p></li>
</ol>
<p>Exercise 17.2 Is <span class="math inline">\(\mathbb{E}\left[\varepsilon_{i t} \mid X_{i t}\right]=0\)</span> sufficient for <span class="math inline">\(\widehat{\beta}_{\mathrm{fe}}\)</span> to be unbiased for <span class="math inline">\(\beta\)</span> ? Explain why or why not.</p>
<p>Exercise 17.3 Show that <span class="math inline">\(\operatorname{var}\left[\dot{X}_{i t}\right] \leq \operatorname{var}\left[X_{i t}\right]\)</span></p>
<p>Exercise 17.4 Show (17.24).</p>
<p>Exercise 17.5 Show (17.28).</p>
<p>Exercise 17.6 Show that when <span class="math inline">\(T=2\)</span> the differenced estimator equals the fixed effects estimator.</p>
<p>Exercise 17.7 In Section <span class="math inline">\(17.14\)</span> it is described how to estimate the individual-effect variance <span class="math inline">\(\sigma_{u}^{2}\)</span> using the between residuals. Develop an alternative estimator of <span class="math inline">\(\sigma_{u}^{2}\)</span> only using the fixed effects error variance <span class="math inline">\(\widehat{\sigma}_{\varepsilon}^{2}\)</span> and the levels error variance <span class="math inline">\(\widehat{\sigma}_{e}^{2}=n^{-1} \sum_{i=1}^{N} \sum_{t \in S_{i}} \widehat{e}_{i t}^{2}\)</span> where <span class="math inline">\(\widehat{e}_{i t}=Y_{i t}-X_{i t}^{\prime} \widehat{\beta}_{\text {fe }}\)</span> are computed from the levels variables.</p>
<p>Exercise 17.8 Verify that <span class="math inline">\(\widehat{\sigma}_{\varepsilon}^{2}\)</span> defined in (17.37) is unbiased for <span class="math inline">\(\sigma_{\varepsilon}^{2}\)</span> under (17.18), (17.25) and (17.26). Exercise 17.9 Develop a version of Theorem <span class="math inline">\(17.2\)</span> for the differenced estimator <span class="math inline">\(\widehat{\beta}_{\Delta}\)</span>. Can you weaken Assumption 17.2.3? State an appropriate version which is sufficient for asymptotic normality.</p>
<p>Exercise 17.10 Show (17.57).</p>
</section>
<section id="exercise-17.11" class="level2" data-number="16.47">
<h2 data-number="16.47" class="anchored" data-anchor-id="exercise-17.11"><span class="header-section-number">16.47</span> Exercise <span class="math inline">\(17.11\)</span></h2>
<ol type="a">
<li><p>For <span class="math inline">\(\widehat{\sigma}_{i}^{2}\)</span> defined in (17.59) show <span class="math inline">\(\mathbb{E}\left[\widehat{\sigma}_{i}^{2} \mid \boldsymbol{X}_{i}\right]=\bar{\sigma}_{i}^{2}\)</span></p></li>
<li><p>For <span class="math inline">\(\widetilde{\boldsymbol{V}}_{\mathrm{fe}}\)</span> defined in (17.58) show <span class="math inline">\(\mathbb{E}\left[\widetilde{\boldsymbol{V}}_{\mathrm{fe}} \mid \boldsymbol{X}\right]=\boldsymbol{V}_{\mathrm{fe}}\)</span>.</p></li>
</ol>
</section>
<section id="exercise-17.12" class="level2" data-number="16.48">
<h2 data-number="16.48" class="anchored" data-anchor-id="exercise-17.12"><span class="header-section-number">16.48</span> Exercise <span class="math inline">\(17.12\)</span></h2>
<ol type="a">
<li>Show (17.61).\</li>
<li>Show (17.62).\</li>
<li>For <span class="math inline">\(\widetilde{\boldsymbol{V}}_{\mathrm{fe}}\)</span> defined in (17.60) show <span class="math inline">\(\mathbb{E}\left[\widetilde{\boldsymbol{V}}_{\mathrm{fe}} \mid \boldsymbol{X}\right]=\boldsymbol{V}_{\mathrm{fe}}\)</span>.</li>
</ol>
<p>Exercise 17.13 Take the fixed effects model <span class="math inline">\(Y_{i t}=X_{i t} \beta_{1}+X_{i t}^{2} \beta_{2}+u_{i}+\varepsilon_{i t}\)</span>. A researcher estimates the model by first obtaining the within transformed <span class="math inline">\(\dot{Y}_{i t}\)</span> and <span class="math inline">\(\dot{X}_{i t}\)</span> and then regressing <span class="math inline">\(\dot{Y}_{i t}\)</span> on <span class="math inline">\(\dot{X}_{i t}\)</span> and <span class="math inline">\(\dot{X}_{i t}^{2}\)</span>. Is the correct estimation method? If not, describe the correct fixed effects estimator.</p>
<p>Exercise 17.14 In Section <span class="math inline">\(17.33\)</span> verify that in the just-identified case the 2SLS estimator <span class="math inline">\(\widehat{\beta}_{2 \text { sls }}\)</span> simplifies as claimed: <span class="math inline">\(\widehat{\beta}_{1}\)</span> and <span class="math inline">\(\widehat{\beta}_{2}\)</span> are the fixed effects estimator. <span class="math inline">\(\widehat{\gamma}_{1}\)</span> and <span class="math inline">\(\widehat{\gamma}_{2}\)</span> equal the 2SLS estimator from a regression of <span class="math inline">\(\widehat{\boldsymbol{u}}\)</span> on <span class="math inline">\(Z_{1}\)</span> and <span class="math inline">\(Z_{2}\)</span> using <span class="math inline">\(\bar{X}_{1}\)</span> as an instrument for <span class="math inline">\(Z_{2}\)</span>.</p>
<p>Exercise 17.15 In this exercise you will replicate and extend the empirical work reported in Arellano and Bond (1991) and Blundell and Bond (1998). Arellano-Bond gathered a dataset of 1031 observations from an unbalanced panel of 140 U.K. companies for 1976-1984 and is in the datafile AB1991 on the textbook webpage. The variables we will be using are log employment <span class="math inline">\((N)\)</span>, log real wages <span class="math inline">\((W)\)</span>, and log capital <span class="math inline">\((K)\)</span>. See the description file for definitions.</p>
<ol type="a">
<li><p>Estimate the panel AR(1) <span class="math inline">\(K_{i t}=\alpha K_{i t-1}+u_{i}+v_{t}+\varepsilon_{i t}\)</span> using Arellano-Bond one-step GMM with clustered standard errors. Note that the model includes year fixed effects.</p></li>
<li><p>Re-estimate using Blundell-Bond one-step GMM with clustered standard errors.</p></li>
<li><p>Explain the difference in the estimates.</p></li>
</ol>
<p>Exercise 17.16 This exercise uses the same dataset as the previous question. Blundell and Bond (1998) estimated a dynamic panel regression of log employment <span class="math inline">\(N\)</span> on log real wages <span class="math inline">\(W\)</span> and log capital <span class="math inline">\(K\)</span>. The following specification <span class="math inline">\({ }^{1}\)</span> used the Arellano-Bond one-step estimator, treating <span class="math inline">\(W_{i, t-1}\)</span> and <span class="math inline">\(K_{i, t-1}\)</span> as predetermined.</p>
<p><img src="images//2022_10_23_acbfcce1ea7ce1901e2dg-52.jpg" class="img-fluid"></p>
<p>This equation also included year dummies and the standard errors are clustered.</p>
<p><span class="math inline">\({ }^{1}\)</span> Blundell and Bond (1998), Table 4, column 3. (a) Estimate (17.114) using the Arellano-Bond one-step estimator treating <span class="math inline">\(W_{i t}\)</span> and <span class="math inline">\(K_{i t}\)</span> as strictly exogenous.</p>
<ol start="2" type="a">
<li><p>Estimate (17.114) treating <span class="math inline">\(W_{i, t-1}\)</span> and <span class="math inline">\(K_{i, t-1}\)</span> as predetermind to verify the results in (17.114). What is the difference between the estimates treating the regressors as strictly exogenous versus predetermined?</p></li>
<li><p>Estimate the equation using the Blundell-Bond one-step systems GMM estimator.</p></li>
<li><p>Interpret the coefficient estimates viewing (17.114) as a firm-level labor demand equation.</p></li>
<li><p>Describe the impact on the standard errors of the Blundell-Bond estimates in part (c) if you forget to use clustering. (You do not have to list all the standard errors, but describe the magnitude of the impact.)</p></li>
</ol>
<p>Exercise 17.17 Use the datafile Invest 1993 on the textbook webpage. You will be estimating the panel AR(1) <span class="math inline">\(D_{i t}=\alpha D_{i, t-1}+u_{i}+\varepsilon_{i t}\)</span> for <span class="math inline">\(D=\)</span> debt/ assets (this is debta in the datafile). See the description file for definitions.</p>
<ol type="a">
<li><p>Estimate the model using Arellano-Bond twostep GMM with clustered standard errors.</p></li>
<li><p>Re-estimate using Blundell-Bond twostep GMM.</p></li>
<li><p>Experiment with your results, trying twostep versus onestep, AR(1) versus AR(2), number of lags used as instruments, and classical versus robust standard errors. What makes the most difference for the coefficient estimates? For the standard errors?</p></li>
</ol>
<p>Exercise 17.18 Use the datafile Invest1993 on the textbook webpage. You will be estimating the model</p>
<p><span class="math display">\[
D_{i t}=\alpha D_{i, t-1}+\beta_{1} I_{i, t-1}+\beta_{2} Q_{i, t-1}+\beta_{3} C F_{i, t-1}+u_{i}+\varepsilon_{i t} .
\]</span></p>
<p>The variables are debta, inva, vala, and <span class="math inline">\(c f a\)</span> in the datafile. See the description file for definitions.</p>
<ol type="a">
<li><p>Estimate the above regression using Arellano-Bond two-step GMM with clustered standard errors treating all regressors as predetermined.</p></li>
<li><p>Re-estimate using Blundell-Bond two-step GMM treating all regressors as predetermined.</p></li>
<li><p>Experiment with your results, trying two-step versus one-step, number of lags used as instruments, and classical versus robust standard errors. What makes the most difference for the coefficient estimates? For the standard errors?</p></li>
</ol>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./chpt15-multiple-time-series.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Multivariate Time Series</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./chpt18-did.html" class="pagination-link">
        <span class="nav-page-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Difference in Differences</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->



</body></html>
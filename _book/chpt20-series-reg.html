<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.253">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Hansen中高级计量体系 - 19&nbsp; Series Regression</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./chpt21-rdd.html" rel="next">
<link href="./chpt19-nonparameter.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
    <div class="container-fluid d-flex justify-content-between">
      <h1 class="quarto-secondary-nav-title"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">Series Regression</span></h1>
      <button type="button" class="quarto-btn-toggle btn" aria-label="Show secondary navigation">
        <i class="bi bi-chevron-right"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Hansen中高级计量体系</a> 
        <div class="sidebar-tools-main">
    <a href="https://github.com/huhuaping/hansenEM/" title="Source Code" class="sidebar-tool px-1"><i class="bi bi-github"></i></a>
</div>
    </div>
      </div>
      <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
      </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">前言</a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./participate.html" class="sidebar-item-text sidebar-link">如何参与？</a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chpt01-intro.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chpt01-intro-chn.html" class="sidebar-item-text sidebar-link">介绍</a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./part01-reg.html" class="sidebar-item-text sidebar-link">回归</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chpt02-ce.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Conditional Expectation and Projection</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chpt02-ce-chn.html" class="sidebar-item-text sidebar-link">条件预期和预测</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chpt03-algebra.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">The Algebra of Least Squares</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chpt03-algebra-chn.html" class="sidebar-item-text sidebar-link">最小二乘代数</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chpt04-lsr.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Least Squares Regression</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chpt05-normal-reg.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Normal Regression</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./part02-LSM.html" class="sidebar-item-text sidebar-link">大样本方法</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chpt06-review.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">A Review of Large Sample Asymptotics</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chpt07-asymptotic-ls.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Asymptotic Theory for Least Squares</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chpt08-restricted-est.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Restricted Estimation</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chpt09-hypothesit-test.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Hypothesis Testing</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chpt10-resample-method.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Resampling Methods</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./part03-MEQ.html" class="sidebar-item-text sidebar-link">多方程模型</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chpt11-multi-reg.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Multivariate Regression</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chpt12-iv.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Instrumental Variables</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chpt13-gmm.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Generalized Method of Moments</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./part04-pannel.html" class="sidebar-item-text sidebar-link">面板数据</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chpt14-time-series.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Time Series</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chpt15-multiple-time-series.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Multivariate Time Series</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chpt17-panel-data.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Panel Data</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chpt18-did.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Difference in Differences</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./part05-nonpar.html" class="sidebar-item-text sidebar-link">非参方法</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chpt19-nonparameter.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Nonparametric Regression</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chpt20-series-reg.html" class="sidebar-item-text sidebar-link active"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">Series Regression</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chpt21-rdd.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">20</span>&nbsp; <span class="chapter-title">Regression Discontinuity</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chpt22-m-est.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">21</span>&nbsp; <span class="chapter-title">M-Estimators</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./part06-nonlinear.html" class="sidebar-item-text sidebar-link">非线性方法</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-6" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chpt23-nonliear-ls.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">22</span>&nbsp; <span class="chapter-title">Nonlinear Least Squares</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chpt24-quantile-reg.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">23</span>&nbsp; <span class="chapter-title">Quantile Regression</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chpt26-multiple-choice.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">24</span>&nbsp; <span class="chapter-title">Multiple Choice</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chpt27-censor-selection.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">25</span>&nbsp; <span class="chapter-title">Censoring and Selection</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chpt28-model-selection.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">26</span>&nbsp; <span class="chapter-title">Model Selection, Stein Shrinkage, and Model Averaging</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chpt29-ML.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">27</span>&nbsp; <span class="chapter-title">Machine Learning</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./summary.html" class="sidebar-item-text sidebar-link">Summary</a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">References</a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" aria-expanded="true">Appendices</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-7" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./a1-notation.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">A</span>&nbsp; <span class="chapter-title">附录a1</span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
    <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">目录</h2>
   
  <ul>
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction"><span class="toc-section-number">19.1</span>  Introduction</a></li>
  <li><a href="#polynomial-regression" id="toc-polynomial-regression" class="nav-link" data-scroll-target="#polynomial-regression"><span class="toc-section-number">19.2</span>  Polynomial Regression</a></li>
  <li><a href="#illustrating-polynomial-regression" id="toc-illustrating-polynomial-regression" class="nav-link" data-scroll-target="#illustrating-polynomial-regression"><span class="toc-section-number">19.3</span>  Illustrating Polynomial Regression</a></li>
  <li><a href="#orthogonal-polynomials" id="toc-orthogonal-polynomials" class="nav-link" data-scroll-target="#orthogonal-polynomials"><span class="toc-section-number">19.4</span>  Orthogonal Polynomials</a></li>
  <li><a href="#splines" id="toc-splines" class="nav-link" data-scroll-target="#splines"><span class="toc-section-number">19.5</span>  Splines</a></li>
  <li><a href="#illustrating-spline-regression" id="toc-illustrating-spline-regression" class="nav-link" data-scroll-target="#illustrating-spline-regression"><span class="toc-section-number">19.6</span>  Illustrating Spline Regression</a></li>
  <li><a href="#the-globallocal-nature-of-series-regression" id="toc-the-globallocal-nature-of-series-regression" class="nav-link" data-scroll-target="#the-globallocal-nature-of-series-regression"><span class="toc-section-number">19.7</span>  The Global/Local Nature of Series Regression</a></li>
  <li><a href="#stone-weierstrass-and-jackson-approximation-theory" id="toc-stone-weierstrass-and-jackson-approximation-theory" class="nav-link" data-scroll-target="#stone-weierstrass-and-jackson-approximation-theory"><span class="toc-section-number">19.8</span>  Stone-Weierstrass and Jackson Approximation Theory</a></li>
  <li><a href="#regressor-bounds" id="toc-regressor-bounds" class="nav-link" data-scroll-target="#regressor-bounds"><span class="toc-section-number">19.9</span>  Regressor Bounds</a></li>
  <li><a href="#matrix-convergence" id="toc-matrix-convergence" class="nav-link" data-scroll-target="#matrix-convergence"><span class="toc-section-number">19.10</span>  Matrix Convergence</a></li>
  <li><a href="#consistent-estimation" id="toc-consistent-estimation" class="nav-link" data-scroll-target="#consistent-estimation"><span class="toc-section-number">19.11</span>  Consistent Estimation</a></li>
  <li><a href="#convergence-rate" id="toc-convergence-rate" class="nav-link" data-scroll-target="#convergence-rate"><span class="toc-section-number">19.12</span>  Convergence Rate</a></li>
  <li><a href="#asymptotic-normality" id="toc-asymptotic-normality" class="nav-link" data-scroll-target="#asymptotic-normality"><span class="toc-section-number">19.13</span>  Asymptotic Normality</a></li>
  <li><a href="#regression-estimation" id="toc-regression-estimation" class="nav-link" data-scroll-target="#regression-estimation"><span class="toc-section-number">19.14</span>  Regression Estimation</a></li>
  <li><a href="#undersmoothing" id="toc-undersmoothing" class="nav-link" data-scroll-target="#undersmoothing"><span class="toc-section-number">19.15</span>  Undersmoothing</a></li>
  <li><a href="#residuals-and-regression-fit" id="toc-residuals-and-regression-fit" class="nav-link" data-scroll-target="#residuals-and-regression-fit"><span class="toc-section-number">19.16</span>  Residuals and Regression Fit</a></li>
  <li><a href="#cross-validation-model-selection" id="toc-cross-validation-model-selection" class="nav-link" data-scroll-target="#cross-validation-model-selection"><span class="toc-section-number">19.17</span>  Cross-Validation Model Selection</a></li>
  <li><a href="#variance-and-standard-error-estimation" id="toc-variance-and-standard-error-estimation" class="nav-link" data-scroll-target="#variance-and-standard-error-estimation"><span class="toc-section-number">19.18</span>  Variance and Standard Error Estimation</a></li>
  <li><a href="#clustered-observations" id="toc-clustered-observations" class="nav-link" data-scroll-target="#clustered-observations"><span class="toc-section-number">19.19</span>  Clustered Observations</a></li>
  <li><a href="#confidence-bands" id="toc-confidence-bands" class="nav-link" data-scroll-target="#confidence-bands"><span class="toc-section-number">19.20</span>  Confidence Bands</a></li>
  <li><a href="#uniform-approximations" id="toc-uniform-approximations" class="nav-link" data-scroll-target="#uniform-approximations"><span class="toc-section-number">19.21</span>  Uniform Approximations</a></li>
  <li><a href="#partially-linear-model" id="toc-partially-linear-model" class="nav-link" data-scroll-target="#partially-linear-model"><span class="toc-section-number">19.22</span>  Partially Linear Model</a></li>
  <li><a href="#panel-fixed-effects" id="toc-panel-fixed-effects" class="nav-link" data-scroll-target="#panel-fixed-effects"><span class="toc-section-number">19.23</span>  Panel Fixed Effects</a></li>
  <li><a href="#multiple-regressors" id="toc-multiple-regressors" class="nav-link" data-scroll-target="#multiple-regressors"><span class="toc-section-number">19.24</span>  Multiple Regressors</a></li>
  <li><a href="#additively-separable-models" id="toc-additively-separable-models" class="nav-link" data-scroll-target="#additively-separable-models"><span class="toc-section-number">19.25</span>  Additively Separable Models</a></li>
  <li><a href="#nonparametric-instrumental-variables-regression" id="toc-nonparametric-instrumental-variables-regression" class="nav-link" data-scroll-target="#nonparametric-instrumental-variables-regression"><span class="toc-section-number">19.26</span>  Nonparametric Instrumental Variables Regression</a></li>
  <li><a href="#npiv-identification" id="toc-npiv-identification" class="nav-link" data-scroll-target="#npiv-identification"><span class="toc-section-number">19.27</span>  NPIV Identification</a></li>
  <li><a href="#npiv-convergence-rate" id="toc-npiv-convergence-rate" class="nav-link" data-scroll-target="#npiv-convergence-rate"><span class="toc-section-number">19.28</span>  NPIV Convergence Rate</a></li>
  <li><a href="#nonparametric-vs-parametric-identification" id="toc-nonparametric-vs-parametric-identification" class="nav-link" data-scroll-target="#nonparametric-vs-parametric-identification"><span class="toc-section-number">19.29</span>  Nonparametric vs Parametric Identification</a></li>
  <li><a href="#example-angrist-and-lavy-1999" id="toc-example-angrist-and-lavy-1999" class="nav-link" data-scroll-target="#example-angrist-and-lavy-1999"><span class="toc-section-number">19.30</span>  Example: Angrist and Lavy (1999)</a></li>
  <li><a href="#technical-proofs" id="toc-technical-proofs" class="nav-link" data-scroll-target="#technical-proofs"><span class="toc-section-number">19.31</span>  Technical Proofs*</a></li>
  <li><a href="#exercises" id="toc-exercises" class="nav-link" data-scroll-target="#exercises"><span class="toc-section-number">19.32</span>  Exercises</a></li>
  </ul>
<div class="toc-actions"><div><i class="bi bi-github"></i></div><div class="action-links"><p><a href="https://github.com/huhuaping/hansenEM/edit/master/chpt20-series-reg.qmd" class="toc-action">Edit this page</a></p></div></div></nav>
</nav>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title d-none d-lg-block"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">Series Regression</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<section id="introduction" class="level2" data-number="19.1">
<h2 data-number="19.1" class="anchored" data-anchor-id="introduction"><span class="header-section-number">19.1</span> Introduction</h2>
<p>Chapter 19 studied nonparametric regression by kernel smoothing methods. In this chapter we study an alternative class of nonparametric methods known as series regression.</p>
<p>The basic model is identical to that examined in Chapter 19. We assume that there are random variables <span class="math inline">\((Y, X)\)</span> such that <span class="math inline">\(\mathbb{E}\left[Y^{2}\right]&lt;\infty\)</span> and satisfy the regression model</p>
<p><span class="math display">\[
\begin{aligned}
Y &amp;=m(X)+e \\
\mathbb{E}[e \mid X] &amp;=0 \\
\mathbb{E}\left[e^{2} \mid X\right] &amp;=\sigma^{2}(X) .
\end{aligned}
\]</span></p>
<p>The goal is to estimate the CEF <span class="math inline">\(m(x)\)</span>. We start with the simple setting where <span class="math inline">\(X\)</span> is scalar and consider more general cases later.</p>
<p>A series regression model is a sequence <span class="math inline">\(K=1,2, \ldots\)</span>, of approximating models <span class="math inline">\(m_{K}(x)\)</span> with <span class="math inline">\(K\)</span> parameters. In this chapter we exclusively focus on linear series models, and in particular polynomials and splines. This is because these are simple, convenient, and cover most applications of series methods in applied economics. Other series models include trigonometric polynomials, wavelets, orthogonal wavelets, B-splines, and neural networks. For a detailed review see Chen (2007).</p>
<p>Linear series regression models take the form</p>
<p><span class="math display">\[
Y=X_{K}^{\prime} \beta_{K}+e_{K}
\]</span></p>
<p>where <span class="math inline">\(X_{K}=X_{K}(X)\)</span> is a vector of regressors obtained by making transformations of <span class="math inline">\(X\)</span> and <span class="math inline">\(\beta_{K}\)</span> is a coefficient vector. There are multiple possible definitions of the coefficient <span class="math inline">\(\beta_{K}\)</span>. We define <span class="math inline">\({ }^{1}\)</span> it by projection</p>
<p><span class="math display">\[
\beta_{K}=\mathbb{E}\left[X_{K} X_{K}^{\prime}\right]^{-1} \mathbb{E}\left[X_{K} Y\right]=\mathbb{E}\left[X_{K} X_{K}^{\prime}\right]^{-1} \mathbb{E}\left[X_{K} m(X)\right] .
\]</span></p>
<p>The series regression error <span class="math inline">\(e_{K}\)</span> is defined by (20.2) and (20.3), is distinct from the regression error <span class="math inline">\(e\)</span> in (20.1), and is indexed by <span class="math inline">\(K\)</span> because it depends on the regressors <span class="math inline">\(X_{K}\)</span>. The series approximation to <span class="math inline">\(m(x)\)</span> is</p>
<p><span class="math display">\[
m_{K}(x)=X_{K}(x)^{\prime} \beta_{K} .
\]</span></p>
<p><span class="math inline">\({ }^{1}\)</span> An alternative is to define <span class="math inline">\(\beta_{K}\)</span> as the best uniform approximation as in (20.8). It is not critical so long as we are careful to be consistent with our notation. The coefficient is typically <span class="math inline">\({ }^{2}\)</span> estimated by least squares</p>
<p><span class="math display">\[
\widehat{\beta}_{K}=\left(\sum_{i=1}^{n} X_{K i} X_{K i}^{\prime}\right)^{-1}\left(\sum_{i=1}^{n} X_{K i} Y_{i}\right)=\left(\boldsymbol{X}_{K}^{\prime} \boldsymbol{X}_{K}\right)^{-1}\left(\boldsymbol{X}_{K}^{\prime} \boldsymbol{Y}\right)
\]</span></p>
<p>The estimator for <span class="math inline">\(m(x)\)</span> is</p>
<p><span class="math display">\[
\widehat{m}_{K}(x)=X_{K}(x)^{\prime} \widehat{\beta}_{K} .
\]</span></p>
<p>The difference between specific models arises due to the different choices of transformations <span class="math inline">\(X_{K}(x)\)</span>.</p>
<p>The theoretical issues we will explore in this chapter are: (1) Approximation properties of polynomials and splines; (2) Consistent estimation of <span class="math inline">\(m(x)\)</span>; (3) Asymptotic normal approximations; (4) Selection of <span class="math inline">\(K\)</span>; (5) Extensions.</p>
<p>For a textbook treatment of series regression see Li and Racine (2007). For an advanced treatment see Chen (2007). Two seminal contributions are Andrews (1991a) and Newey (1997). Two recent important papers are Belloni, Chernozhukov, Chetverikov, and Kato (2015) and Chen and Christensen (2015).</p>
</section>
<section id="polynomial-regression" class="level2" data-number="19.2">
<h2 data-number="19.2" class="anchored" data-anchor-id="polynomial-regression"><span class="header-section-number">19.2</span> Polynomial Regression</h2>
<p>The prototypical series regression model for <span class="math inline">\(m(x)\)</span> is a <span class="math inline">\(p^{t h}\)</span> order polynomial</p>
<p><span class="math display">\[
m_{K}(x)=\beta_{0}+\beta_{1} x+\beta_{2} x^{2}+\cdots+\beta_{p} x^{p} .
\]</span></p>
<p>We can write it in vector notation as (20.4) where</p>
<p><span class="math display">\[
X_{K}(x)=\left(\begin{array}{c}
1 \\
x \\
\vdots \\
x^{p}
\end{array}\right) .
\]</span></p>
<p>The number of parameters is <span class="math inline">\(K=p+1\)</span>. Notice that we index <span class="math inline">\(X_{K}(x)\)</span> and <span class="math inline">\(\beta_{K}\)</span> by <span class="math inline">\(K\)</span> as their dimensions and values vary with <span class="math inline">\(K\)</span>.</p>
<p>The implied polynomial regression model for the random pair <span class="math inline">\((Y, X)\)</span> is <span class="math inline">\((20.2)\)</span> with</p>
<p><span class="math display">\[
X_{K}=X_{K}(X)=\left(\begin{array}{c}
1 \\
X \\
\vdots \\
X^{p}
\end{array}\right)
\]</span></p>
<p>The degree of flexibility of a polynomial regression is controlled by the polynomial order <span class="math inline">\(p\)</span>. A larger <span class="math inline">\(p\)</span> yields a more flexible model while a smaller <span class="math inline">\(p\)</span> typically results in a estimator with a smaller variance.</p>
<p>In general, a linear series regression model takes the form</p>
<p><span class="math display">\[
m_{K}(x)=\beta_{1} \tau_{1}(x)+\beta_{2} \tau_{2}(x)+\cdots+\beta_{K} \tau_{K}(x)
\]</span></p>
<p>where the functions <span class="math inline">\(\tau_{j}(x)\)</span> are called the basis transformations. The polynomial regression model uses the power basis <span class="math inline">\(\tau_{j}(x)=x^{j-1}\)</span>. The model <span class="math inline">\(m_{K}(x)\)</span> is called a series regression because it is obtained by sequentially adding the series of variables <span class="math inline">\(\tau_{j}(x)\)</span>.</p>
<p><span class="math inline">\({ }^{2}\)</span> Penalized estimators have also been recommended. We do not review these methods here.</p>
</section>
<section id="illustrating-polynomial-regression" class="level2" data-number="19.3">
<h2 data-number="19.3" class="anchored" data-anchor-id="illustrating-polynomial-regression"><span class="header-section-number">19.3</span> Illustrating Polynomial Regression</h2>
<p>Consider the cps09mar dataset and a regression of <span class="math inline">\(\log (\)</span> wage <span class="math inline">\()\)</span> on experience for women with a college education (education=16), separately for white women and Black women. The classical Mincer model uses a quadratic in experience. Given the large sample sizes (4682 for white women and 517 for Black women) we can consider higher order polynomials. In Figure <span class="math inline">\(20.1\)</span> we plot least squares estimates of the CEFs using polynomials of order 2, 4, 8, and 12.</p>
<p>Examine panel (a) which shows the estimates for the sub-sample of white women. The quadratic specification appears mis-specified with a shape noticably different from the other estimates. The difference between the polynomials of order 4,8 , and 12 is relatively minor, especially for experience levels below 20.</p>
<p>Now examine panel (b) which shows the estimates for the sub-sample of Black women. This panel is quite different from panel (a). The estimates are erratic and increasingly so as the polynomial order increases. Assuming we are expecting a concave (or nearly concave) experience profile the only estimate which satisfies this is the quadratic.</p>
<p>Why the difference between panels (a) and (b)? The most likely explanation is the different sample sizes. The sub-sample of Black women has much fewer observations so the CEF is much less precisely estimated, giving rise to the erratic plots. This suggests (informally) that it may be preferred to use a smaller polynomial order <span class="math inline">\(p\)</span> in the second sub-sample, or equivalently to use a larger <span class="math inline">\(p\)</span> when the sample size <span class="math inline">\(n\)</span> is larger. The idea that model complexity - the number of coefficients <span class="math inline">\(K\)</span> - should vary with sample size <span class="math inline">\(n\)</span> is an important feature of series regression.</p>
<p>The erratic nature of the estimated polynomial regressions in Figure 20.1(b) is a common feature of higher-order estimated polynomial regressions. Better results can sometimes be obtained by a spline regression which is described in Section 20.5.</p>
<p><img src="images//2022_10_23_2b38d6d54e7725c196e7g-03.jpg" class="img-fluid"></p>
<ol type="a">
<li>White Women</li>
</ol>
<p><img src="images//2022_10_23_2b38d6d54e7725c196e7g-03(1).jpg" class="img-fluid"></p>
<ol start="2" type="a">
<li>Black Women</li>
</ol>
<p>Figure 20.1: Polynomial Estimates of Experience Profile</p>
</section>
<section id="orthogonal-polynomials" class="level2" data-number="19.4">
<h2 data-number="19.4" class="anchored" data-anchor-id="orthogonal-polynomials"><span class="header-section-number">19.4</span> Orthogonal Polynomials</h2>
<p>Standard implementation of the least squares estimator (20.5) of a polynomial regression may return a computational error message when <span class="math inline">\(p\)</span> is large. (See Section 3.24.) This is because the moments of <span class="math inline">\(X^{j}\)</span> can be highly heterogeneous across <span class="math inline">\(j\)</span> and because the variables <span class="math inline">\(X^{j}\)</span> can be highly correlated. These two factors imply in practice that the matrix <span class="math inline">\(\boldsymbol{X}_{K}^{\prime} \boldsymbol{X}_{K}\)</span> can be ill-conditioned (the ratio of the largest to smallest eigenvalue can be quite large) and some packages will return error messages rather than compute <span class="math inline">\(\widehat{\beta}_{K}\)</span>.</p>
<p>In most cases the condition of <span class="math inline">\(\boldsymbol{X}_{K}^{\prime} \boldsymbol{X}_{K}\)</span> can be dramatically improved by rescaling the observations. As discussed in Section <span class="math inline">\(3.24\)</span> a simple method for non-negative regressors is to rescale each by its sample mean, e.g.&nbsp;replace <span class="math inline">\(X_{i}^{j}\)</span> with <span class="math inline">\(X_{i}^{j} /\left(n^{-1} \sum_{i=1}^{n} X_{i}^{j}\right)\)</span>. Even better conditioning can often be obtained by rescaling <span class="math inline">\(X_{i}\)</span> to lie in <span class="math inline">\([-1,1]\)</span> before applying powers. In most applications one of these methods will be sufficient for a well-conditioned regression.</p>
<p>A computationally more robust implementation can be obtained by using orthogonal polynomials. These are linear combinations of the polynomial basis functions and produce identical regression estimators (20.6). The goal of orthogonal polynomials is to produce regressors which are either orthogonal or close to orthogonal and have similar variances so that <span class="math inline">\(\boldsymbol{X}_{K}^{\prime} \boldsymbol{X}_{K}\)</span> is close to diagonal with similar diagonal elements. These orthogonalized regressors <span class="math inline">\(X_{K}^{*}=\boldsymbol{A}_{K} X_{K}\)</span> can be written as linear combinations of the original variables <span class="math inline">\(X_{K}\)</span>. If the regressors are orthogonalized then the regression estimator (20.6) is modified by replacing <span class="math inline">\(X_{K}(x)\)</span> with <span class="math inline">\(X_{K}^{*}(x)=\boldsymbol{A}_{K} X_{K}(x)\)</span>.</p>
<p>One approach is to use sample orthogonalization. This is done by a sequence of regressions of <span class="math inline">\(X_{i}^{j}\)</span> on the previously orthogonalized variables and then rescaling. This will result in perfectly orthogonalized variables. This is what is implemented in many statistical packages under the label “orthogonal polynomials”, for example, the function poly in R. If this is done then the least squares coefficients have no meaning outside this specific sample and it is not convenient for calculation of <span class="math inline">\(\widehat{m}_{K}(x)\)</span> for values of <span class="math inline">\(x\)</span> other than sample values. This is the approach used for the examples presented in the previous section.</p>
<p>Another approach is to use an algebraic orthogonal polynomial. This is a polynomial which is orthogonal with respect to a known weight function <span class="math inline">\(w(x)\)</span>. Specifically, it is a sequence <span class="math inline">\(p_{j}(x), j=0,1,2, \ldots\)</span>, with the property that <span class="math inline">\(\int p_{j}(x) p_{\ell}(x) w(x) d x=0\)</span> for <span class="math inline">\(j \neq \ell\)</span>. This means that if <span class="math inline">\(w(x)=f(x)\)</span>, the marginal density of <span class="math inline">\(X\)</span>, then the basis transformations <span class="math inline">\(p_{j}(X)\)</span> will be mutually orthogonal (in expectation). Since we do now know the density of <span class="math inline">\(X\)</span> this is not feasible in practice, but if <span class="math inline">\(w(x)\)</span> is close to the density of <span class="math inline">\(X\)</span> then we can expect that the basis transformations will be close to mutually orthogonal. To implement an algebraic orthogonal polynomial you first should rescale your <span class="math inline">\(X\)</span> variable so that it satisfies the support for the weight function <span class="math inline">\(w(x)\)</span>.</p>
<p>The following three choices are most relevant for economic applications.</p>
<p>Legendre Polynomial. These are orthogonal with respect to the uniform density on <span class="math inline">\([-1,1]\)</span>. (So should be applied to regressors scaled to have support in <span class="math inline">\([-1,1]\)</span>.)</p>
<p><span class="math display">\[
p_{j}(x)=\frac{1}{2^{j}} \sum_{\ell=0}^{j}\left(\begin{array}{l}
j \\
\ell
\end{array}\right)^{2}(x-1)^{j-\ell}(x+1)^{\ell} .
\]</span></p>
<p>For example, the first four are <span class="math inline">\(p_{0}(x)=1, p_{1}(x)=x, p_{2}(x)=\left(3 x^{2}-1\right) / 2\)</span>, and <span class="math inline">\(p_{3}(x)=\left(5 x^{3}-3 x\right) / 2\)</span>. The best computational method is the recurrence relationship</p>
<p><span class="math display">\[
p_{j+1}(x)=\frac{(2 j+1) x p_{j}(x)-j p_{j-1}(x)}{j+1} .
\]</span></p>
<p>Laguerre Polynomial. These are orthogonal with respect to the exponential density <span class="math inline">\(e^{-x}\)</span> on <span class="math inline">\([0, \infty)\)</span>. (So should be applied to non-negative regressors scaled if possible to have approximately unit mean and/or variance.)</p>
<p><span class="math display">\[
p_{j}(x)=\sum_{\ell=0}^{j}\left(\begin{array}{l}
j \\
\ell
\end{array}\right) \frac{(-x)^{\ell}}{\ell !} .
\]</span></p>
<p>For example, the first four are <span class="math inline">\(p_{0}(x)=1, p_{1}(x)=1-x, p_{2}(x)=\left(x^{2}-4 x+2\right) / 2\)</span>, and <span class="math inline">\(p_{3}(x)=\left(-x^{3}+9 x^{2}-18 x+6\right) / 6\)</span>. The best computational method is the recurrence relationship</p>
<p><span class="math display">\[
p_{j+1}(x)=\frac{(2 j+1-x) p_{j}(x)-j p_{j-1}(x)}{j+1} .
\]</span></p>
<p>Hermite Polynomial. These are orthogonal with respect to the standard normal density on <span class="math inline">\((-\infty, \infty)\)</span>. (So should be applied to regressors scaled to have mean zero and variance one.)</p>
<p><span class="math display">\[
p_{j}(x)=j ! \sum_{\ell=0}^{\lfloor j / 2\rfloor} \frac{(-1 / 2)^{\ell} x^{\ell-2 j}}{\ell !(j-2 \ell !)} .
\]</span></p>
<p>For example, the first four are <span class="math inline">\(p_{0}(x)=1, p_{1}(x)=x, p_{2}(x)=x^{2}-1\)</span>, and <span class="math inline">\(p_{3}(x)=x^{3}-3 x\)</span>. The best computational method is the recurrence relationship</p>
<p><span class="math display">\[
p_{j+1}(x)=x p_{j}(x)-j p_{j-1}(x) .
\]</span></p>
<p>The R package orthopolynom provides a convenient set of commands to compute many orthogonal polynomials including the above.</p>
</section>
<section id="splines" class="level2" data-number="19.5">
<h2 data-number="19.5" class="anchored" data-anchor-id="splines"><span class="header-section-number">19.5</span> Splines</h2>
<p>A spline is a piecewise polynomial. Typically the order of the polynomial is pre-selected to be linear, quadratic, or cubic. The flexibility of the model is determined by the number of polynomial segments. The join points between the segments are called knots.</p>
<p>To impose smoothness and parsimony it is common to constrain the spline function to have continuous derivatives up to the order of the spline. Thus a linear spline is constrained to be continuous, a quadratic spline is constrained to have a continuous first derivative, and a cubic spline is constrained to have continuous first and second derivatives.</p>
<p>A simple way to construct a regression spline is as follows. A linear spline with one knot <span class="math inline">\(\tau\)</span> is</p>
<p><span class="math display">\[
m_{K}(x)=\beta_{0}+\beta_{1} x+\beta_{2}(x-\tau) \mathbb{1}\{x \geq \tau\} .
\]</span></p>
<p>To see that this is a linear spline, observe that for <span class="math inline">\(x \leq \tau\)</span> the function <span class="math inline">\(m_{K}(x)=\beta_{0}+\beta_{1} x\)</span> is linear with slope <span class="math inline">\(\beta_{1}\)</span>; for <span class="math inline">\(x \geq \tau\)</span> the function <span class="math inline">\(m_{K}(x)\)</span> is linear with slope <span class="math inline">\(\beta_{1}+\beta_{2}\)</span>; and the function is continuous at <span class="math inline">\(x=\tau\)</span>. Note that <span class="math inline">\(\beta_{2}\)</span> is the change in the slope at <span class="math inline">\(\tau\)</span>. A linear spline with two knots <span class="math inline">\(\tau_{1}&lt;\tau_{2}\)</span> is</p>
<p><span class="math display">\[
m_{K}(x)=\beta_{0}+\beta_{1} x+\beta_{2}\left(x-\tau_{1}\right) \mathbb{1}\left\{x \geq \tau_{2}\right\}+\beta_{3}\left(x-\tau_{2}\right) \mathbb{1}\left\{x \geq \tau_{2}\right\} .
\]</span></p>
<p>A quadratic spline with one knot is</p>
<p><span class="math display">\[
m_{K}(x)=\beta_{0}+\beta_{1} x+\beta_{2} x^{2}+\beta_{3}(x-\tau)^{2} \mathbb{1}\{x \geq \tau\} .
\]</span></p>
<p>To see that this is a quadratic spline, observe that for <span class="math inline">\(x \leq \tau\)</span> the function is the quadratic <span class="math inline">\(\beta_{0}+\beta_{1} x+\beta_{2} x^{2}\)</span> with second derivative <span class="math inline">\(m_{K}^{\prime \prime}(\tau)=2 \beta_{2}\)</span>; for <span class="math inline">\(x \geq \tau\)</span> the second derivative is <span class="math inline">\(m_{K}^{\prime \prime}(\tau)=2\left(\beta_{2}+\beta_{3}\right)\)</span>; so <span class="math inline">\(2 \beta_{3}\)</span> is the change in the second derivative at <span class="math inline">\(\tau\)</span>. The first derivative at <span class="math inline">\(x=\tau\)</span> is the continuous function <span class="math inline">\(m_{K}^{\prime}(\tau)=\)</span> <span class="math inline">\(\beta_{1}+2 \beta_{2} \tau\)</span>.</p>
<p>In general, a <span class="math inline">\(p^{t h}\)</span>-order spline with <span class="math inline">\(N\)</span> knots <span class="math inline">\(\tau_{1}&lt;\tau_{2}&lt;\cdots&lt;\tau_{N}\)</span> is</p>
<p><span class="math display">\[
m_{K}(x)=\sum_{j=0}^{p} \beta_{j} x^{j}+\sum_{k=1}^{N} \beta_{p+k}\left(x-\tau_{k}\right)^{p} \mathbb{1}\left\{x \geq \tau_{k}\right\}
\]</span></p>
<p>which has <span class="math inline">\(K=N+p+1\)</span> coefficients.</p>
<p>The implied spline regression model for the random pair <span class="math inline">\((Y, X)\)</span> is <span class="math inline">\((20.2)\)</span> where</p>
<p><span class="math display">\[
X_{K}=X_{K}(X)=\left(\begin{array}{c}
1 \\
X \\
\vdots \\
X^{p} \\
\left(X-\tau_{1}\right)^{p} \mathbb{1}\left\{X \geq \tau_{1}\right\} \\
\vdots \\
\left(X-\tau_{N}\right)^{p} \mathbb{1}\left\{X \geq \tau_{N}\right\}
\end{array}\right) .
\]</span></p>
<p>In practice a spline will depend critically on the choice of the knots <span class="math inline">\(\tau_{k}\)</span>. When <span class="math inline">\(X\)</span> is bounded with an approximately uniform distribution it is common to space the knots evenly so all segments have the same length. When the distribution of <span class="math inline">\(X\)</span> is not uniform an alternative is to set the knots at the quantiles <span class="math inline">\(j /(N+1)\)</span> so that the probability mass is equalized across segments. A third alternative is to set the knots at the points where <span class="math inline">\(m(x)\)</span> has the greatest change in curvature (see Schumaker (2007), Chapter 7). In all cases the set of knots <span class="math inline">\(\tau_{j}\)</span> can change with <span class="math inline">\(K\)</span>. Therefore a spline is a special case of an approximation of the form</p>
<p><span class="math display">\[
m_{K}(x)=\beta_{1} \tau_{1 K}(x)+\beta_{2} \tau_{2 K}(x)+\cdots+\beta_{K} \tau_{K K}(x)
\]</span></p>
<p>where the basis transformations <span class="math inline">\(\tau_{j K}(x)\)</span> depend on both <span class="math inline">\(j\)</span> and <span class="math inline">\(K\)</span>. Many authors call such approximations a sieve rather than a series because the basis transformations change with <span class="math inline">\(K\)</span>. This distinction is not critical to our treatment so for simplicity we refer to splines as series regression models.</p>
</section>
<section id="illustrating-spline-regression" class="level2" data-number="19.6">
<h2 data-number="19.6" class="anchored" data-anchor-id="illustrating-spline-regression"><span class="header-section-number">19.6</span> Illustrating Spline Regression</h2>
<p>In Section <span class="math inline">\(20.3\)</span> we illustrated regressions of log(wage) on experience for white and Black women with a college education. Now we consider a similar regression for Black men with a college education, a sub-sample with 394 observations.</p>
<p>We use a quadratic spline with four knots at experience levels of <span class="math inline">\(10,20,30\)</span>, and 40 . This is a regression model with seven coefficients. The estimated regression function is displayed in Figure <span class="math inline">\(20.2(\mathrm{a})\)</span>. An estimated <span class="math inline">\(6^{\text {th }}\)</span> order polynomial regression is also displayed for comparison (a <span class="math inline">\(6^{\text {th }}\)</span> order polynomial is an appropriate comparison because it also has seven coefficients).</p>
<p>While the spline is a quadratic over each segment, what you can see is that the first two segments (experience levels between 0-10 and 10-20 years) are essentially linear. Most of the curvature occurs in the third and fourth segments (20-30 and 30-40 years) where the estimated regression function peaks and twists into a negative slope. The estimated regression function is smooth.</p>
<p>A quadratic or cubic spline is useful when it is desired to impose smoothness as in Figure 20.2(a). In contrast, a linear spline is useful when it is desired to allow for sharp changes in slope.</p>
<p>To illustrate we consider the data set CHJ2004 which is a sample of 8684 urban Phillipino households from Cox, B. E. Hansen, and Jimenez (2004). This paper studied the crowding-out impact of a</p>
<p><img src="images//2022_10_23_2b38d6d54e7725c196e7g-07.jpg" class="img-fluid"></p>
<ol type="a">
<li>Experience Profile</li>
</ol>
<p><img src="images//2022_10_23_2b38d6d54e7725c196e7g-07(1).jpg" class="img-fluid"></p>
<ol start="2" type="a">
<li>Effect of Income on Transfers</li>
</ol>
<p>Figure 20.2: Spline Regression Estimates</p>
<p>family’s income on non-governmental (e.g., extended family) income transfers <span class="math inline">\({ }^{3}\)</span>. A model of altruistic transfers predicts that extended families will make gifts (transfers) when the recipient family’s income is sufficiently low, but will not make transfers if the recipient family’s income exceeds a threshold. A pure altruistic model predicts that the regression of transfers received on family income should have a slope of <span class="math inline">\(-1\)</span> up to this threshold and be flat above this threshold. We estimated this regression (including the same controls as the authors <span class="math inline">\({ }^{4}\)</span> ) using a linear spline with knots at 10000, 20000, 50000, 100000, and 150000 pesos. These knots were selected to give flexibility for low income levels where there are more observations. This model has a total of 22 coefficients.</p>
<p>The estimated regression function (as a function of household income) is displayed in Figure <span class="math inline">\(20.2\)</span> (b). For the first two segments (incomes levels below 20000 pesos) the regression function is negatively sloped as predicted with a slope about <span class="math inline">\(-0.7\)</span> from 0 to 10000 pesos, and <span class="math inline">\(-0.3\)</span> from 10000 to 20000 pesos. The estimated regression function is effectively flat for income levels above 20000 pesos. This shape is consistent with the pure altruism model. A linear spline model is particularly well suited for this application as it allows for discontinuous changes in slope.</p>
<p>Linear spline models with a single knot have been recently popularized by Card, Lee, Pei, and Weber (2015) with the label regression kink design.</p>
</section>
<section id="the-globallocal-nature-of-series-regression" class="level2" data-number="19.7">
<h2 data-number="19.7" class="anchored" data-anchor-id="the-globallocal-nature-of-series-regression"><span class="header-section-number">19.7</span> The Global/Local Nature of Series Regression</h2>
<p>Recall from Section <span class="math inline">\(19.18\)</span> that we described kernel regression as inherently local in nature. The Nadaraya-Watson, Local Linear, and Local Polynomial estimators of the CEF <span class="math inline">\(m(x)\)</span> are weighted averages of <span class="math inline">\(Y_{i}\)</span> for observations for which <span class="math inline">\(X_{i}\)</span> is close to <span class="math inline">\(x\)</span>.</p>
<p><span class="math inline">\({ }^{3}\)</span> Defined as the sum of transfers received domestically, from abroad, and in-kind, less gifts.</p>
<p><span class="math inline">\({ }^{4}\)</span> The controls are: age of household head, education (5 dummy categories), married, female, married female, number of children (3 dummies), size of household, employment status (2 dummies). In contrast, series regression is typically described as global in nature. The estimator <span class="math inline">\(\widehat{m}_{K}(x)=X_{K}(x)^{\prime} \widehat{\beta}_{K}\)</span> is a function of the entire sample. The coefficients of a fitted polynomial (or spline) are affected by the global shape of the function <span class="math inline">\(m(x)\)</span> and thus affect the estimator <span class="math inline">\(\widehat{m}_{K}(x)\)</span> at any point <span class="math inline">\(x\)</span>.</p>
<p>While this description has some merit it is not a complete description. As we now show, series regression estimators share the local smoothing property of kernel regression. As the number of series terms <span class="math inline">\(K\)</span> increase a series estimator <span class="math inline">\(\widehat{m}_{K}(x)=X_{K}(x)^{\prime} \widehat{\beta}_{K}\)</span> also becomes a local weighted average estimator.</p>
<p>To see this, observe that we can write the estimator as</p>
<p><span class="math display">\[
\begin{aligned}
\widehat{m}_{K}(x) &amp;=X_{K}(x)^{\prime}\left(\boldsymbol{X}_{K}^{\prime} \boldsymbol{X}_{K}\right)^{-1}\left(\boldsymbol{X}_{K}^{\prime} \boldsymbol{Y}\right) \\
&amp;=\frac{1}{n} \sum_{i=1}^{n} X_{K}(x)^{\prime} \widehat{\boldsymbol{Q}}_{K}^{-1} X_{K}\left(X_{i}\right) Y_{i} \\
&amp;=\frac{1}{n} \sum_{i=1}^{n} \widehat{w}_{K}\left(x, X_{i}\right) Y_{i}
\end{aligned}
\]</span></p>
<p>where <span class="math inline">\(\widehat{\boldsymbol{Q}}_{K}=n^{-1} \boldsymbol{X}_{K}^{\prime} \boldsymbol{X}_{K}\)</span> and <span class="math inline">\(\widehat{w}_{K}(x, u)=x_{K}(x)^{\prime} \widehat{\boldsymbol{Q}}_{K}^{-1} x_{K}(u)\)</span>. Thus <span class="math inline">\(\widehat{m}_{K}(x)\)</span> is a weighted average of <span class="math inline">\(Y_{i}\)</span> using the weights <span class="math inline">\(\widehat{w}_{K}\left(x, X_{i}\right)\)</span>. The weight function <span class="math inline">\(\widehat{w}_{K}\left(x, X_{i}\right)\)</span> appears to be maximized at <span class="math inline">\(X_{i}=x\)</span>, so <span class="math inline">\(\widehat{m}(x)\)</span> puts more weight on observations for which <span class="math inline">\(X_{i}\)</span> is close to <span class="math inline">\(x\)</span>, similarly to kernel regression.</p>
<p><img src="images//2022_10_23_2b38d6d54e7725c196e7g-08.jpg" class="img-fluid"></p>
<ol type="a">
<li><span class="math inline">\(x=0.5\)</span></li>
</ol>
<p><img src="images//2022_10_23_2b38d6d54e7725c196e7g-08(1).jpg" class="img-fluid"></p>
<ol start="2" type="a">
<li><span class="math inline">\(x=0.25\)</span></li>
</ol>
<p>Figure 20.3: Kernel Representation of Polynomial Weight Function</p>
<p>To see this more precisely, observe that because <span class="math inline">\(\widehat{\boldsymbol{Q}}_{K}\)</span> will be close in large samples to <span class="math inline">\(\boldsymbol{Q}_{K}=\mathbb{E}\left[X_{K} X_{K}^{\prime}\right]\)</span>, <span class="math inline">\(\widehat{w}_{K}(x, u)\)</span> will be close to the deterministic weight function</p>
<p><span class="math display">\[
w_{K}(x, u)=X_{K}(x)^{\prime} \boldsymbol{Q}_{K}^{-1} X_{K}(u) .
\]</span></p>
<p>Take the case <span class="math inline">\(X \sim U[0,1]\)</span>. In Figure <span class="math inline">\(20.3\)</span> we plot the weight function <span class="math inline">\(w_{K}(x, u)\)</span> as a funtion of <span class="math inline">\(u\)</span> for <span class="math inline">\(x=\)</span> <span class="math inline">\(0.5\)</span> (panel (a)) and <span class="math inline">\(x=0.25\)</span> (panel (b)) for <span class="math inline">\(p=4,8,12\)</span> in panel (a) and <span class="math inline">\(p=4\)</span>, 12 in panel (b). First, examine panel (a). Here you can see that the weight function <span class="math inline">\(w(x, u)\)</span> is symmetric in <span class="math inline">\(u\)</span> about <span class="math inline">\(x\)</span>. For <span class="math inline">\(p=4\)</span> the weight function appears similar to a quadratic in <span class="math inline">\(u\)</span>, and as <span class="math inline">\(p\)</span> increases the weight function concentrates its main weight around <span class="math inline">\(x\)</span>. However, the weight function is not non-negative. It is quite similar in shape to what are known as higher-order (or bias-reducing) kernels, which were not reviewed in the previous chapter but are part of the kernel estimation toolkit. Second, examine panel (b). Again the weight function is maximized at <span class="math inline">\(x\)</span>, but now it is asymmetric in <span class="math inline">\(u\)</span> about the point <span class="math inline">\(x\)</span>. Still, the general features from panel (a) carry over to panel (b). Namely, as <span class="math inline">\(p\)</span> increases the polynomial estimator puts most weight on observations for which <span class="math inline">\(X\)</span> is close to <span class="math inline">\(x\)</span> (just as for kernel regression), but is different from conventional kernel regression in that the weight function is not non-negative. Qualitatively similar plots are obtained for spline regression.</p>
<p>There is little formal theory (of which I am aware) which makes a formal link between series regression and kernel regression so the comments presented here are illustrative <span class="math inline">\({ }^{5}\)</span>. However, the point is that statements of the form “Series regession is a global method; Kernel regression is a local method” may not be complete. Both are global in nature when <span class="math inline">\(h\)</span> is large (kernels) or <span class="math inline">\(K\)</span> is small (series), and are local in nature when <span class="math inline">\(h\)</span> is small (kernels) or <span class="math inline">\(K\)</span> is large (series).</p>
</section>
<section id="stone-weierstrass-and-jackson-approximation-theory" class="level2" data-number="19.8">
<h2 data-number="19.8" class="anchored" data-anchor-id="stone-weierstrass-and-jackson-approximation-theory"><span class="header-section-number">19.8</span> Stone-Weierstrass and Jackson Approximation Theory</h2>
<p>A good series approximation <span class="math inline">\(m_{K}(x)\)</span> has the property that it gets close to the true CEF <span class="math inline">\(m(x)\)</span> as the complexity <span class="math inline">\(K\)</span> increases. Formal statements can be derived from the mathematical theory of the approximation of functions.</p>
<p>An elegant and famous theorem is the Stone-Weierstrass Theorem (Weierstrass, 1885, Stone, 1948) which states that any continuous function can be uniformly well approximated by a polynomial of sufficiently high order. Specifically, the theorem states that if <span class="math inline">\(m(x)\)</span> is continuous on a compact set <span class="math inline">\(S\)</span> then for any <span class="math inline">\(\epsilon&gt;0\)</span> there is some <span class="math inline">\(K\)</span> sufficiently large such that</p>
<p><span class="math display">\[
\inf _{\beta} \sup _{x \in S}\left|m(x)-X_{K}(x)^{\prime} \beta\right| \leq \epsilon .
\]</span></p>
<p>Thus the true unknown <span class="math inline">\(m(x)\)</span> can be arbitrarily well approximated by selecting a suitable polynomial.</p>
<p>Jackson (1912) strengthened this result to give convergence rates which depend on the smoothness of <span class="math inline">\(m(x)\)</span>. The basic result has been extended to spline functions. The following notation will be useful. Define the <span class="math inline">\(\beta\)</span> which minimizes the left-side of (20.7) as</p>
<p><span class="math display">\[
\beta_{K}^{*}=\underset{\beta}{\operatorname{argmin}} \sup _{x \in S}\left|m(x)-X_{K}(x)^{\prime} \beta\right|,
\]</span></p>
<p>define the approximation error</p>
<p><span class="math display">\[
r_{K}^{*}(x)=m(x)-X_{K}(x)^{\prime} \beta_{K}^{*},
\]</span></p>
<p>and define the minimized value of (20.7)</p>
<p><span class="math display">\[
\delta_{K}^{*} \stackrel{\text { def }}{=} \inf _{\beta} \sup _{x \in S}\left|m(x)-X_{K}(x)^{\prime} \beta\right|=\sup _{x \in S}\left|m(x)-X_{K}(x)^{\prime} \beta_{K}^{*}\right|=\sup _{x \in S}\left|r_{K}^{*}(x)\right| .
\]</span></p>
<p><span class="math inline">\({ }^{5}\)</span> Similar connections are made in the appendix of Chen, Liao, and Sun (2012). Theorem 20.1 If for some <span class="math inline">\(\alpha \geq 0, m^{(\alpha)}(x)\)</span> is uniformly continuous on a compact set <span class="math inline">\(S\)</span> and <span class="math inline">\(X_{K}(x)\)</span> is either a polynomial basis or a spline basis (with uniform knot spacing) of order <span class="math inline">\(s \geq \alpha\)</span>, then as <span class="math inline">\(K \rightarrow \infty\)</span></p>
<p><span class="math display">\[
\delta_{K}^{*} \leq o\left(K^{-\alpha}\right) .
\]</span></p>
<p>Furthermore, if <span class="math inline">\(m^{(2)}(x)\)</span> is uniformly continuous on <span class="math inline">\(S\)</span> and <span class="math inline">\(X_{K}(x)\)</span> is a linear spline basis, then <span class="math inline">\(\delta_{K}^{*} \leq O\left(K^{-2}\right)\)</span>.</p>
<p>For a proof for the polynomial case see Theorem <span class="math inline">\(4.3\)</span> of Lorentz (1986) or Theorem <span class="math inline">\(3.12\)</span> of Schumaker (2007) plus his equations (2.119) and (2.121). For the spline case see Theorem <span class="math inline">\(6.27\)</span> of Schumaker (2007) plus his equations (2.119) and (2.121). For the linear spline case see Theorem <span class="math inline">\(6.15\)</span> of Schumaker, equation (6.28).</p>
<p>Theorem <span class="math inline">\(20.1\)</span> is more useful than the classic Stone-Weierstrass Theorem as it gives an approximation rate which depends on the smoothness order <span class="math inline">\(\alpha\)</span>. The rate <span class="math inline">\(o\left(K^{-\alpha}\right)\)</span> in (20.11) means that the approximation error (20.10) decreases as <span class="math inline">\(K\)</span> increases and decreases at a faster rate when <span class="math inline">\(\alpha\)</span> is large. The standard interpretation is that when <span class="math inline">\(m(x)\)</span> is smoother it is possible to approximate it with fewer terms.</p>
<p>It will turn out that for our distribution theory it is sufficient to consider the case that <span class="math inline">\(m^{(2)}(x)\)</span> is uniformly continuous. For this case Theorem <span class="math inline">\(20.1\)</span> shows that polynomials and quadratic/cubic splines achieve the rate <span class="math inline">\(o\left(K^{-2}\right)\)</span> and linear splines achieve the rate <span class="math inline">\(O\left(K^{-2}\right)\)</span>. For most of of our results the latter bound will be sufficient.</p>
<p>More generally, Theorem <span class="math inline">\(20.1\)</span> makes a distinction between polynomials and splines as polynomials achieve the rate <span class="math inline">\(o\left(K^{-\alpha}\right)\)</span> adaptively (without input from the user) while splines achieve the rate <span class="math inline">\(o\left(K^{-\alpha}\right)\)</span> only if the spline order <span class="math inline">\(s\)</span> is appropriately chosen. This is an advantage for polynomials. However, as emphasized by Schumaker (2007), splines simultaneously approximate the derivatives <span class="math inline">\(m^{(q)}(x)\)</span> for <span class="math inline">\(q&lt;\)</span> <span class="math inline">\(\alpha\)</span>. Thus, for example, a quadratic spline simultaneously approximates the function <span class="math inline">\(m(x)\)</span> and its first derivative <span class="math inline">\(m^{\prime}(x)\)</span>. There is no comparable result for polynomials. This is an advantage for quadratic and cubic splines. Since economists are often more interested in marginal effects (derivatives) than in levels this may be a good reason to prefer splines over polynomials.</p>
<p>Theorem <span class="math inline">\(20.1\)</span> is a bound on the best uniform approximation error. The coefficient <span class="math inline">\(\beta_{K}^{*}\)</span> which minimizes (20.11) is not, however, the projection coefficient <span class="math inline">\(\beta_{K}\)</span> as defined in (20.3). Thus Theorem <span class="math inline">\(20.1\)</span> does not directly inform us concerning the approximation error obtained by series regression. It turns out, however, that the projection error can be easily deduced from (20.11).</p>
<p>Definition 20.1 The projection approximation error is</p>
<p><span class="math display">\[
r_{K}(x)=m(x)-X_{K}(x)^{\prime} \beta_{K}
\]</span></p>
<p>where the coefficient <span class="math inline">\(\beta_{K}\)</span> is the projection coefficient (20.3). The realized projection approximation error is <span class="math inline">\(r_{K}=r_{K}(X)\)</span>. The expected squared projection error is</p>
<p><span class="math display">\[
\delta_{K}^{2}=\mathbb{E}\left[r_{K}^{2}\right] .
\]</span></p>
<p>The projection approximation error is similar to (20.9) but evaluated using the projection coefficient rather than the minimizing coefficient <span class="math inline">\(\beta_{K}^{*}\)</span> (20.8). Assuming that <span class="math inline">\(X\)</span> has compact support <span class="math inline">\(S\)</span> the expected squared projection error satisfies</p>
<p><span class="math display">\[
\begin{aligned}
\delta_{K} &amp;=\left(\int_{S}\left(m(x)-X_{K}(x)^{\prime} \beta_{K}\right)^{2} d F(x)\right)^{1 / 2} \\
&amp; \leq\left(\int_{S}\left(m(x)-X_{K}(x)^{\prime} \beta_{K}^{*}\right)^{2} d F(x)\right)^{1 / 2} \\
&amp; \leq\left(\int_{S} \delta_{K}^{* 2} d F(x)\right)^{1 / 2} \\
&amp;=\delta_{K}^{*} .
\end{aligned}
\]</span></p>
<p>The first inequality holds because the projection coefficient <span class="math inline">\(\beta_{K}\)</span> minimizes the expected squared projection error (see Section 2.25). The second inequality is the definition of <span class="math inline">\(\delta_{K}^{*}\)</span>. Combined with Theorem <span class="math inline">\(20.1\)</span> we have established the following result.</p>
<p>Theorem <span class="math inline">\(20.2\)</span> If <span class="math inline">\(X\)</span> has compact support <span class="math inline">\(S\)</span>, for some <span class="math inline">\(\alpha \geq 0 m^{(\alpha)}(x)\)</span> is uniformly continuous on <span class="math inline">\(S\)</span>, and <span class="math inline">\(X_{K}(x)\)</span> is either a polynomial basis or a spline basis of order <span class="math inline">\(s \geq \alpha\)</span>, then as <span class="math inline">\(K \rightarrow \infty\)</span></p>
<p><span class="math display">\[
\delta_{K} \leq \delta_{K}^{*} \leq o\left(K^{-\alpha}\right) .
\]</span></p>
<p>Furthermore, if <span class="math inline">\(m^{(2)}(x)\)</span> is uniformly continuous on <span class="math inline">\(S\)</span> and <span class="math inline">\(X_{K}(x)\)</span> is a linear spline basis, then <span class="math inline">\(\delta_{K} \leq O\left(K^{-2}\right)\)</span>.</p>
<p>The available theory of the approximation of functions goes beyond the results described here. For example, there is a theory of weighted polynomial approximation (Mhaskar, 1996) which provides an analog of Theorem <span class="math inline">\(20.2\)</span> for the unbounded real line when <span class="math inline">\(X\)</span> has a density with exponential tails.</p>
</section>
<section id="regressor-bounds" class="level2" data-number="19.9">
<h2 data-number="19.9" class="anchored" data-anchor-id="regressor-bounds"><span class="header-section-number">19.9</span> Regressor Bounds</h2>
<p>The approximation result in Theorem <span class="math inline">\(20.2\)</span> assumes that the regressors <span class="math inline">\(X\)</span> have bounded support <span class="math inline">\(S\)</span>. This is conventional in series regression theory as it greatly simplifies the analysis. Bounded support implies that the regressor function <span class="math inline">\(X_{K}(x)\)</span> is bounded. Define</p>
<p><span class="math display">\[
\begin{gathered}
\zeta_{K}(x)=\left(X_{K}(x)^{\prime} \boldsymbol{Q}_{K}^{-1} X_{K}(x)\right)^{1 / 2} \\
\zeta_{K}=\sup _{x} \zeta_{K}(x)
\end{gathered}
\]</span></p>
<p>where <span class="math inline">\(\boldsymbol{Q}_{K}=\mathbb{E}\left[X_{K} X_{K}^{\prime}\right]\)</span> is the population design matrix given the regressors <span class="math inline">\(X_{K}\)</span>. This implies that for all realizations of <span class="math inline">\(X_{K}\)</span></p>
<p><span class="math display">\[
\left(X_{K}^{\prime} \boldsymbol{Q}_{K}^{-1} X_{K}\right)^{1 / 2} \leq \zeta_{K} .
\]</span></p>
<p>The constant <span class="math inline">\(\zeta_{K}(x)\)</span> is the normalized length of the regressor vector <span class="math inline">\(X_{K}(x)\)</span>. The constant <span class="math inline">\(\zeta_{K}\)</span> is the maximum normalized length. Their values are determined by the basis function transformations and the distribution of <span class="math inline">\(X\)</span>. They are invariant to rescaling <span class="math inline">\(X_{K}\)</span> or linear rotations.</p>
<p>For polynomials and splines we have explicit expressions for the rate at which <span class="math inline">\(\zeta_{K}\)</span> grows with <span class="math inline">\(K\)</span>. Theorem 20.3 If <span class="math inline">\(X\)</span> has compact support <span class="math inline">\(S\)</span> with a strictly positive density <span class="math inline">\(f(x)\)</span> on <span class="math inline">\(S\)</span> then</p>
<ol type="1">
<li><p><span class="math inline">\(\zeta_{K} \leq O(K)\)</span> for polynomials</p></li>
<li><p><span class="math inline">\(\zeta_{K} \leq O\left(K^{1 / 2}\right)\)</span> for splines.</p></li>
</ol>
<p>For a proof of Theorem <span class="math inline">\(20.3\)</span> see Newey (1997, Theorem 4).</p>
<p>Furthermore, when <span class="math inline">\(X\)</span> is uniformly distributed then we can explicitly calculate for polynomials that <span class="math inline">\(\zeta_{K}=K\)</span>, so the polynomial bound <span class="math inline">\(\zeta_{K} \leq O(K)\)</span> cannot be improved.</p>
<p>To illustrate, we plot in Figure <span class="math inline">\(20.4\)</span> the values <span class="math inline">\(\zeta_{K}(x)\)</span> for the case <span class="math inline">\(X \sim U[0,1]\)</span>. We plot <span class="math inline">\(\zeta_{K}(x)\)</span> for a polynomial of degree <span class="math inline">\(p=9\)</span> and a quadratic spline with <span class="math inline">\(N=7\)</span> knots (both satisfy <span class="math inline">\(K=10\)</span> ). You can see that the values of <span class="math inline">\(\zeta_{K}(x)\)</span> are close to 3 for both basis transformations and most values of <span class="math inline">\(x\)</span>, but <span class="math inline">\(\zeta_{K}(x)\)</span> increases sharply for <span class="math inline">\(x\)</span> near the boundary. The maximum values are <span class="math inline">\(\zeta_{K}=10\)</span> for the polynomial and <span class="math inline">\(\zeta_{K}=7.4\)</span> for the quadratic spline. While Theorem <span class="math inline">\(20.3\)</span> shows the two have different rates for large <span class="math inline">\(K\)</span>, we see for moderate <span class="math inline">\(K\)</span> that the differences are relatively minor.</p>
<p><img src="images//2022_10_23_2b38d6d54e7725c196e7g-12.jpg" class="img-fluid"></p>
<p>Figure 20.4: Normalized Regressor Length</p>
</section>
<section id="matrix-convergence" class="level2" data-number="19.10">
<h2 data-number="19.10" class="anchored" data-anchor-id="matrix-convergence"><span class="header-section-number">19.10</span> Matrix Convergence</h2>
<p>One of the challenges which arise when developing a theory for the least squares estimator is how to describe the large-sample behavior of the sample design matrix</p>
<p><span class="math display">\[
\widehat{\boldsymbol{Q}}_{K}=\frac{1}{n} \sum_{i=1}^{n} X_{K i} X_{K i}^{\prime}
\]</span></p>
<p>as <span class="math inline">\(K \rightarrow \infty\)</span>. The difficulty is that its dimension changes with <span class="math inline">\(K\)</span> so we cannot apply a standard WLLN.</p>
<p>It turns out to be convenient if we first rotate the regressor vector so that the elements are orthogonal in expectation. Thus we define the standardized regressors and design matrix as</p>
<p><span class="math display">\[
\begin{aligned}
\widetilde{X}_{K i} &amp;=\boldsymbol{Q}_{K}^{-1 / 2} X_{K i} \\
\widetilde{\boldsymbol{Q}}_{K} &amp;=\frac{1}{n} \sum_{i=1}^{n} \widetilde{X}_{K i} \widetilde{X}_{K i}^{\prime} .
\end{aligned}
\]</span></p>
<p>Note that <span class="math inline">\(\mathbb{E}\left[\widetilde{X}_{K} \widetilde{X}_{K}^{\prime}\right]=\boldsymbol{I}_{K}\)</span>. The standardized regressors are not used in practice; they are introduced only to simplify the theoretical derivations.</p>
<p>Our convergence theory will require the following fundamental rate bound on the number of coefficients <span class="math inline">\(K\)</span>.</p>
<p>Assumption $20.1</p>
<ol type="1">
<li><p><span class="math inline">\(\lambda_{\min }\left(\boldsymbol{Q}_{K}\right) \geq \underline{\lambda}&gt;0\)</span></p></li>
<li><p><span class="math inline">\(\zeta_{K}^{2} \log (K) / n \rightarrow 0\)</span> as <span class="math inline">\(n, K \rightarrow \infty\)</span></p></li>
</ol>
<p>Assumption 20.1.1 ensures that the transformation (20.18) is well defined <span class="math inline">\({ }^{6}\)</span>. Assumption 20.1.2 states that the squared maximum regressor length <span class="math inline">\(\zeta_{K}^{2}\)</span> grows slower than <span class="math inline">\(n\)</span>. Since <span class="math inline">\(\zeta_{K}\)</span> increases with <span class="math inline">\(K\)</span> this is a bound on the rate at which <span class="math inline">\(K\)</span> can increase with <span class="math inline">\(n\)</span>. By Theorem <span class="math inline">\(20.2\)</span> the rate in Assumption <span class="math inline">\(20.1 .2\)</span> holds for polynomials if <span class="math inline">\(K^{2} \log (K) / n \rightarrow 0\)</span> and for splines if <span class="math inline">\(K \log (K) / n \rightarrow 0\)</span>. In either case, this means that the number of coefficients <span class="math inline">\(K\)</span> is growing at a rate slower than <span class="math inline">\(n\)</span>.</p>
<p>We are now in a position to describe a convergence result for the standardized design matrix. The following is Lemma <span class="math inline">\(6.2\)</span> of Belloni, Chernozhukov, Chetverikov, and Kato (2015).</p>
<p>Theorem <span class="math inline">\(20.4\)</span> If Assumption <span class="math inline">\(20.1\)</span> holds then</p>
<p><span class="math display">\[
\left\|\widetilde{\boldsymbol{Q}}_{K}-\boldsymbol{I}_{K}\right\| \stackrel{p}{\longrightarrow} 0 .
\]</span></p>
<p>A proof of Theorem <span class="math inline">\(20.4\)</span> using a stronger condition than Assumption <span class="math inline">\(20.1\)</span> can be found in Section 20.31. The norm in (20.19) is the spectral norm</p>
<p><span class="math display">\[
\|\boldsymbol{A}\|=\left(\lambda_{\max }\left(\boldsymbol{A}^{\prime} \boldsymbol{A}\right)\right)^{1 / 2}
\]</span></p>
<p><span class="math inline">\({ }^{6}\)</span> Technically, what is required is that <span class="math inline">\(\lambda_{\min }\left(\boldsymbol{B}_{K} \boldsymbol{Q}_{K} \boldsymbol{B}_{K}^{\prime}\right) \geq \underline{\lambda}&gt;0\)</span> for some <span class="math inline">\(K \times K\)</span> sequence of matrices <span class="math inline">\(\boldsymbol{B}_{K}\)</span>, or equivalently that Assumption 20.1.1 holds after replacing <span class="math inline">\(X_{K}\)</span> with <span class="math inline">\(\boldsymbol{B}_{K} X_{K}\)</span>. where <span class="math inline">\(\lambda_{\max }(\boldsymbol{B})\)</span> denotes the largest eigenvalue of the matrix <span class="math inline">\(\boldsymbol{B}\)</span>. For a full description see Section A.23.</p>
<p>For the least squares estimator what is particularly important is the inverse of the sample design matrix. Fortunately we can easily deduce consistency of its inverse from (20.19) when the regressors have been orthogonalized as described.</p>
<p>Theorem 20.5 If Assumption <span class="math inline">\(20.1\)</span> holds then</p>
<p><span class="math display">\[
\left\|\widetilde{\boldsymbol{Q}}_{K}^{-1}-\boldsymbol{I}_{K}\right\| \stackrel{p}{\longrightarrow} 0
\]</span></p>
<p>and</p>
<p><span class="math display">\[
\lambda_{\max }\left(\widetilde{\boldsymbol{Q}}_{K}^{-1}\right)=1 / \lambda_{\min }\left(\widetilde{\boldsymbol{Q}}_{K}\right) \stackrel{p}{\longrightarrow} 1 .
\]</span></p>
<p>The proof of Theorem <span class="math inline">\(20.5\)</span> can be found in Section <span class="math inline">\(20.31\)</span>.</p>
</section>
<section id="consistent-estimation" class="level2" data-number="19.11">
<h2 data-number="19.11" class="anchored" data-anchor-id="consistent-estimation"><span class="header-section-number">19.11</span> Consistent Estimation</h2>
<p>In this section we give conditions for consistent estimation of <span class="math inline">\(m(x)\)</span> by the series estimator <span class="math inline">\(\widehat{m}_{K}(x)=\)</span> <span class="math inline">\(X_{K}(x)^{\prime} \widehat{\beta}_{K}\)</span>.</p>
<p>We know from standard regression theory that for any fixed <span class="math inline">\(K, \widehat{\beta}_{K} \stackrel{p}{\rightarrow} \beta_{K}\)</span> and thus <span class="math inline">\(\widehat{m}_{K}(x)=X_{K}(x)^{\prime} \widehat{\beta}_{K} \stackrel{p}{\rightarrow}\)</span> <span class="math inline">\(X_{K}(x)^{\prime} \beta_{K}\)</span> as <span class="math inline">\(n \rightarrow \infty\)</span>. Furthermore, from the Stone-Weierstrass Theorem we know that <span class="math inline">\(X_{K}(x)^{\prime} \beta_{K} \rightarrow m(x)\)</span> as <span class="math inline">\(K \rightarrow \infty\)</span>. It therefore seems reasonable to expect that <span class="math inline">\(\hat{m}_{K}(x) \stackrel{p}{\longrightarrow} m(x)\)</span> as both <span class="math inline">\(n \rightarrow \infty\)</span> and <span class="math inline">\(K \rightarrow \infty\)</span> together. Making this argument rigorous, however, is technically challenging, in part because the dimensions of <span class="math inline">\(\widehat{\beta}_{K}\)</span> and its components are changing with <span class="math inline">\(K\)</span>.</p>
<p>Since <span class="math inline">\(\widehat{m}_{K}(x)\)</span> and <span class="math inline">\(m(x)\)</span> are functions, convergence should be defined with respect to an appropriate metric. For kernel regression we focused on pointwise convergence (for each value of <span class="math inline">\(x\)</span> separately) as that is the simplest to analyze. For series regression it turns out to be simplest to describe convergence with respect to integrated squared error (ISE). We define the latter as</p>
<p><span class="math display">\[
\operatorname{ISE}(K)=\int\left(\widehat{m}_{K}(x)-m(x)\right)^{2} d F(x)
\]</span></p>
<p>where <span class="math inline">\(F\)</span> is the marginal distribution of <span class="math inline">\(X\)</span>. ISE <span class="math inline">\((K)\)</span> is the average squared distance between <span class="math inline">\(\widehat{m}_{K}(x)\)</span> and <span class="math inline">\(m(x)\)</span>, weighted by the marginal distribution of <span class="math inline">\(X\)</span>. The ISE is random, depends on both sample size <span class="math inline">\(n\)</span> and model complexity <span class="math inline">\(K\)</span>, and its distribution is determined by the joint distribution of <span class="math inline">\((Y, X)\)</span>. We can establish the following.</p>
<p>Theorem 20.6 Under Assumption <span class="math inline">\(20.1\)</span> and <span class="math inline">\(\delta_{K}=o(1)\)</span>, then as <span class="math inline">\(n, K \rightarrow \infty\)</span>,</p>
<p><span class="math display">\[
\operatorname{ISE}(K)=o_{p}(1) .
\]</span></p>
<p>The proof of Theorem <span class="math inline">\(20.6\)</span> can be found in Section <span class="math inline">\(20.31\)</span>.</p>
<p>Theorem <span class="math inline">\(20.6\)</span> shows that the series estimator <span class="math inline">\(\hat{m}_{K}(x)\)</span> is consistent in the ISE norm under mild conditions. The assumption <span class="math inline">\(\delta_{K}=o(1)\)</span> holds for polynomials and splines if <span class="math inline">\(K \rightarrow \infty\)</span> and <span class="math inline">\(m(x)\)</span> is uniformly continuous. This result is analogous to Theorem <span class="math inline">\(19.8\)</span> which showed that kernel regression estimator is consistent if <span class="math inline">\(m(x)\)</span> is continuous.</p>
</section>
<section id="convergence-rate" class="level2" data-number="19.12">
<h2 data-number="19.12" class="anchored" data-anchor-id="convergence-rate"><span class="header-section-number">19.12</span> Convergence Rate</h2>
<p>We now give a rate of convergence.</p>
<p>Theorem 20.7 Under Assumption <span class="math inline">\(20.1\)</span> and <span class="math inline">\(\sigma^{2}(x) \leq \bar{\sigma}^{2}&lt;\infty\)</span>, then as <span class="math inline">\(n, K \rightarrow \infty\)</span>,</p>
<p><span class="math display">\[
\operatorname{ISE}(K) \leq O_{p}\left(\delta_{K}^{2}+\frac{K}{n}\right)
\]</span></p>
<p>where <span class="math inline">\(\delta_{K}^{2}\)</span> is the expected squared prediction error (20.13). Furthermore, if <span class="math inline">\(m^{\prime \prime}(x)\)</span> is uniformly continuous then for polynomial or spline basis functions</p>
<p><span class="math display">\[
\operatorname{ISE}(K) \leq O_{p}\left(K^{-4}+\frac{K}{n}\right) .
\]</span></p>
<p>The proof of Theorem <span class="math inline">\(20.7\)</span> can be found in Section 20.31. It is based on Newey (1997).</p>
<p>The bound (20.25) is particularly useful as it gives an explicit rate in terms of <span class="math inline">\(K\)</span> and <span class="math inline">\(n\)</span>. The result shows that the integrated squared error is bounded in probability by two terms. The first <span class="math inline">\(K^{-4}\)</span> is the squared bias. The second <span class="math inline">\(K / n\)</span> is the estimation variance. This is analogous to the AIMSE for kernel regression (19.5). We can see that increasing the number of series terms <span class="math inline">\(K\)</span> affects the integrated squared error by decreasing the bias but increasing the variance. The fact that the estimation variance is of order <span class="math inline">\(K / n\)</span> can be intuitively explained by the fact that the regression model is estimating <span class="math inline">\(K\)</span> coefficients.</p>
<p>For polynomials and quadratic splines the bound (20.25) can be written as <span class="math inline">\(o_{p}\left(K^{-4}\right)+O_{p}(K / n)\)</span>.</p>
<p>We are interested in the sequence <span class="math inline">\(K\)</span> which minimizes the trade-off in (20.25). By examining the firstorder condition we find that the sequence which minimizes this bound is <span class="math inline">\(K \sim n^{1 / 5}\)</span>. With this choice we obtain the optimal integrated squared error <span class="math inline">\(\operatorname{ISE}(K) \leq O_{p}\left(n^{-4 / 5}\right)\)</span>. This is the same convergence rate as obtained by kernel regression under similar assumptions.</p>
<p>It is interesting to contrast the optimal rate <span class="math inline">\(K \sim n^{1 / 5}\)</span> for series regression with <span class="math inline">\(h \sim n^{-1 / 5}\)</span> for kernel regression. Essentially, one can view <span class="math inline">\(K^{-1}\)</span> in series regression as a “bandwidth” similar to kernel regression, or one can view <span class="math inline">\(1 / h\)</span> in kernel regression as the effective number of coefficients.</p>
<p>The rate <span class="math inline">\(K \sim n^{1 / 5}\)</span> means that the optimal <span class="math inline">\(K\)</span> increases very slowly with the sample size. For example, doubling your sample size implies a <span class="math inline">\(15 %\)</span> increase in the optimal number of coefficients <span class="math inline">\(K\)</span>. To obtain a doubling in the optimal number of coefficients you need to multiply the sample size by 32.</p>
<p>To illustrate, Figure <span class="math inline">\(20.5\)</span> displays the ISE rate bounds <span class="math inline">\(K^{-4}+K / n\)</span> as a function of <span class="math inline">\(K\)</span> for <span class="math inline">\(n=10,30,150\)</span>. The filled circles mark the ISE-minimizing <span class="math inline">\(K\)</span>, which are <span class="math inline">\(K=2\)</span>, 3, and 4 for the three functions. Notice that the ISE functions are steeply downward sloping for small <span class="math inline">\(K\)</span> and nearly flat for large <span class="math inline">\(K\)</span> (when <span class="math inline">\(n\)</span> is large). This is because the bias term <span class="math inline">\(K^{-4}\)</span> dominates for small values of <span class="math inline">\(K\)</span> while the variance term <span class="math inline">\(K / n\)</span> dominates for large values of <span class="math inline">\(K\)</span> and the latter flattens as <span class="math inline">\(n\)</span> increases.</p>
</section>
<section id="asymptotic-normality" class="level2" data-number="19.13">
<h2 data-number="19.13" class="anchored" data-anchor-id="asymptotic-normality"><span class="header-section-number">19.13</span> Asymptotic Normality</h2>
<p>Take a parameter <span class="math inline">\(\theta=a(m)\)</span> which is a real-valued linear function of the regression function. This includes the regression function <span class="math inline">\(m(x)\)</span> at a given point <span class="math inline">\(x\)</span>, derivatives of <span class="math inline">\(m(x)\)</span>, and integrals over <span class="math inline">\(m(x)\)</span>. Given <span class="math inline">\(\widehat{m}_{K}(x)=X_{K}(x)^{\prime} \widehat{\beta}_{K}\)</span> as an estimator for <span class="math inline">\(m(x)\)</span>, the estimator for <span class="math inline">\(\theta\)</span> is <span class="math inline">\(\widehat{\theta}_{K}=a\left(\widehat{m}_{K}\right)=a_{K}^{\prime} \widehat{\beta}_{K}\)</span> for some <span class="math inline">\(K \times 1\)</span> vector of constants <span class="math inline">\(a_{K} \neq 0\)</span>. (The relationship <span class="math inline">\(a\left(\widehat{m}_{K}\right)=a_{K}^{\prime} \widehat{\beta}_{K}\)</span> follows because <span class="math inline">\(a\)</span> is linear in <span class="math inline">\(m\)</span> and <span class="math inline">\(\widehat{m}_{K}\)</span> is linear in <span class="math inline">\(\widehat{\beta}_{K}\)</span>.)</p>
<p><img src="images//2022_10_23_2b38d6d54e7725c196e7g-16.jpg" class="img-fluid"></p>
<p>Figure 20.5: Integrated Squared Error</p>
<p>If <span class="math inline">\(K\)</span> were fixed as <span class="math inline">\(n \rightarrow \infty\)</span> then by standard asymptotic theory we would expect <span class="math inline">\(\widehat{\theta}_{K}\)</span> to be asymptotically normal with variance <span class="math inline">\(V_{K}=a_{K}^{\prime} \boldsymbol{Q}_{K}^{-1} \Omega_{K} \boldsymbol{Q}_{K}^{-1} a_{K}\)</span> where <span class="math inline">\(\Omega_{K}=\mathbb{E}\left[X_{K} X_{K}^{\prime} e^{2}\right]\)</span>. The standard justification, however, is not valid in the nonparametric case. This is in part because <span class="math inline">\(V_{K}\)</span> may diverge as <span class="math inline">\(K \rightarrow \infty\)</span>, and in part due to the finite sample bias due to the approximation error. Therefore a new theory is required. Interestingly, it turns out that in the nonparametric case <span class="math inline">\(\widehat{\theta}_{K}\)</span> is still asymptotically normal and <span class="math inline">\(V_{K}\)</span> is still the appropriate variance for <span class="math inline">\(\widehat{\theta}_{K}\)</span>. The proof is different than the parametric case as the dimensions of the matrices are increasing with <span class="math inline">\(K\)</span> and we need to be attentive to the estimator’s bias due to the series approximation.</p>
<p><img src="images//2022_10_23_2b38d6d54e7725c196e7g-16(1).jpg" class="img-fluid"></p>
<p>Assumption 20.2.1 is conditional square integrability. It implies that the conditional variance <span class="math inline">\(\mathbb{E}\left[e^{2} \mid X\right]\)</span> is bounded. It is used to verify the Lindeberg condition for the CLT. Assumption 20.2.2 states that the conditional variance is nowhere degenerate. Thus there is no <span class="math inline">\(X\)</span> for which <span class="math inline">\(Y\)</span> is perfectly predictable. This is a technical condition used to bound <span class="math inline">\(V_{K}\)</span> from below.</p>
<p>Assumption 20.2.3 states that approximation error <span class="math inline">\(\delta_{K}\)</span> declines faster than the maximal regressor length <span class="math inline">\(\zeta_{K}\)</span>. For polynomials a sufficient condition for this assumption is that <span class="math inline">\(m^{\prime \prime}(x)\)</span> is uniformly continuous. For splines a sufficient condition is that <span class="math inline">\(m^{\prime}(x)\)</span> is uniformly continuous.</p>
<p>Theorem 20.8 Under Assumption 20.2, as <span class="math inline">\(n \rightarrow \infty\)</span>,</p>
<p><span class="math display">\[
\frac{\sqrt{n}\left(\widehat{\theta}_{K}-\theta+a\left(r_{K}\right)\right)}{V_{K}^{1 / 2}} \underset{d}{\longrightarrow} \mathrm{N}(0,1) .
\]</span></p>
<p>The proof of Theorem <span class="math inline">\(20.8\)</span> can be found in Section <span class="math inline">\(20.31\)</span>.</p>
<p>Theorem <span class="math inline">\(20.8\)</span> shows that the estimator <span class="math inline">\(\widehat{\theta}_{K}\)</span> is approximately normal with bias <span class="math inline">\(-a\left(r_{K}\right)\)</span> and variance <span class="math inline">\(V_{K} / n\)</span>. The variance is the same as in the parametric case. The asymptotic bias is similar to that found in kernel regression.</p>
<p>One useful message from Theorem <span class="math inline">\(20.8\)</span> is that the classical variance formula <span class="math inline">\(V_{K}\)</span> for <span class="math inline">\(\widehat{\theta}_{K}\)</span> applies to series regression. This justifies conventional estimators for <span class="math inline">\(V_{K}\)</span> as will be discussed in Section <span class="math inline">\(20.18\)</span>.</p>
<p>Theorem <span class="math inline">\(20.8\)</span> shows that the estimator <span class="math inline">\(\widehat{\theta}_{K}\)</span> has a bias <span class="math inline">\(a\left(r_{K}\right)\)</span>. What is this? It is the same transformation of the function <span class="math inline">\(r_{K}(x)\)</span> as <span class="math inline">\(\theta=a(m)\)</span> is of the regression function <span class="math inline">\(m(x)\)</span>. For example, if <span class="math inline">\(\theta=m(x)\)</span> is the regression at a fixed point <span class="math inline">\(x\)</span> then <span class="math inline">\(a\left(r_{K}\right)=r_{K}(x)\)</span>, the approximation error at the same point. If <span class="math inline">\(\theta=m^{\prime}(x)\)</span> is the regression derivative then <span class="math inline">\(a\left(r_{K}\right)=r_{K}^{\prime}(x)\)</span> is the derivative of the approximation error.</p>
<p>This means that the bias in the estimator <span class="math inline">\(\widehat{\theta}_{K}\)</span> for <span class="math inline">\(\theta\)</span> shown in Theorem <span class="math inline">\(20.8\)</span> is simply the approximation error transformed by the functional of interest. If we are estimating the regression function then the bias is the error in approximating the regression function; if we are estimating the regression derivative then the bias is the error in the derivative in the approximation error for the regression function.</p>
</section>
<section id="regression-estimation" class="level2" data-number="19.14">
<h2 data-number="19.14" class="anchored" data-anchor-id="regression-estimation"><span class="header-section-number">19.14</span> Regression Estimation</h2>
<p>A special yet important example of a linear estimator is the regression function at a fixed point <span class="math inline">\(x\)</span>. In the notation of the previous section, <span class="math inline">\(a(m)=m(x)\)</span> and <span class="math inline">\(a_{K}=X_{K}(x)\)</span>. The series estimator of <span class="math inline">\(m(x)\)</span> is <span class="math inline">\(\widehat{\theta}_{K}=\widehat{m}_{K}(x)=X_{K}(x)^{\prime} \widehat{\beta}_{K}\)</span>. As this is a key problem of interest we restate the asymptotic result of Theorem <span class="math inline">\(20.8\)</span> for this estimator.</p>
<p>Theorem 20.9 Under Assumption 20.2, as <span class="math inline">\(n \rightarrow \infty\)</span>,</p>
<p><span class="math display">\[
\frac{\sqrt{n}\left(\hat{m}_{K}(x)-m(x)+r_{K}(x)\right)}{V_{K}^{1 / 2}(x)} \underset{d}{\longrightarrow} \mathrm{N}(0,1)
\]</span></p>
<p>where <span class="math inline">\(V_{K}(x)=X_{K}(x)^{\prime} \boldsymbol{Q}_{K}^{-1} \Omega_{K} \boldsymbol{Q}_{K}^{-1} X_{K}(x)\)</span>.</p>
<p>There are several important features about the asymptotic distribution (20.27).</p>
<p>First, as mentioned in the previous section it shows that the classical variance formula <span class="math inline">\(V_{K}(x)\)</span> applies for the series estimator <span class="math inline">\(\widehat{m}_{K}(x)\)</span>. Second, (20.27) shows that the estimator has the asymptotic bias <span class="math inline">\(r_{K}(x)\)</span>. This is due to the fact that the finite order series is an approximation to the unknown regression function <span class="math inline">\(m(x)\)</span> and this results in finite sample bias.</p>
<p>There is another fascinating connection between the asymptotic variance of Theorem <span class="math inline">\(20.9\)</span> and the regression lengths <span class="math inline">\(\zeta_{K}(x)\)</span> of (20.15). Under conditional homoskedasticity we have the simplification <span class="math inline">\(V_{K}(x)=\sigma^{2} \zeta_{K}(x)^{2}\)</span>. Thus the asymptotic variance of the regression estimator is proportional to the squared regression lengths. From Figure <span class="math inline">\(20.4\)</span> we learned that the regression length <span class="math inline">\(\zeta_{K}(x)\)</span> is much higher at the edge of the support of the regressors, especially for polynomials. This means that the precision of the series regression estimator is considerably degraded at the edge of the support.</p>
</section>
<section id="undersmoothing" class="level2" data-number="19.15">
<h2 data-number="19.15" class="anchored" data-anchor-id="undersmoothing"><span class="header-section-number">19.15</span> Undersmoothing</h2>
<p>An unpleasant aspect about Theorem <span class="math inline">\(20.9\)</span> is the bias term. An interesting trick is that this bias term can be made asymptotically negligible if we assume that <span class="math inline">\(K\)</span> increases with <span class="math inline">\(n\)</span> at a sufficiently fast rate.</p>
<p>Theorem 20.10 Under Assumption 20.2, if in addition <span class="math inline">\(n \delta_{K}^{* 2} \rightarrow 0\)</span> then</p>
<p><span class="math display">\[
\frac{\sqrt{n}\left(\widehat{m}_{K}(x)-m(x)\right)}{V_{K}^{1 / 2}(x)} \underset{d}{\longrightarrow} \mathrm{N}(0,1) \text {. }
\]</span></p>
<p>The condition <span class="math inline">\(n \delta_{K}^{* 2} \rightarrow 0\)</span> implies that the squared bias converges faster than the estimation variance so the former is asymptotically negligible. If <span class="math inline">\(m^{\prime \prime}(x)\)</span> is uniformly continuous then a sufficient condition for polynomials and quadratic splines is <span class="math inline">\(K \sim n^{1 / 4}\)</span>. For linear splines a sufficient condition is for <span class="math inline">\(K\)</span> to diverge faster than <span class="math inline">\(K^{1 / 4}\)</span>. The rate <span class="math inline">\(K \sim n^{1 / 4}\)</span> is somewhat faster than the ISE-optimal rate <span class="math inline">\(K \sim n^{1 / 5}\)</span>.</p>
<p>The assumption <span class="math inline">\(n \delta_{K}^{* 2} \rightarrow 0\)</span> is often stated by authors as an innocuous technical condition. This is misleading as it is a technical trick and should be discussed explicitly. The reason why the assumption eliminates the bias from (20.28) is that the assumption forces the estimation variance to dominate the squared bias so that the latter can be ignored. This means that the estimator itself is inefficient.</p>
<p>Because <span class="math inline">\(n \delta_{K}^{* 2} \rightarrow 0\)</span> means that <span class="math inline">\(K\)</span> is larger than optimal we say that <span class="math inline">\(\widehat{m}_{K}(x)\)</span> is undersmoothed relative to the optimal series estimator.</p>
<p>Many authors like to focus their asymptotic theory on the assumptions in Theorem <span class="math inline">\(20.10\)</span> as the distribution (20.28) appears cleaner. However, it is a poor use of asymptotic theory. There are three problems with the assumption <span class="math inline">\(n \delta_{K}^{* 2} \rightarrow 0\)</span> and the approximation (20.28). First, the estimator <span class="math inline">\(\widehat{m}_{K}(x)\)</span> is inefficient. Second, while the assumption <span class="math inline">\(n \delta_{K}^{* 2} \rightarrow 0\)</span> makes the bias of lower order than the variance it only makes the bias of slightly lower order, meaning that the accuracy of the asymptotic approximation is poor. Effectively, the estimator is still biased in finite samples. Third, <span class="math inline">\(n \delta_{K}^{* 2} \rightarrow 0\)</span> is an assumption not a rule for empirical practice. It is unclear what the statement “Assume <span class="math inline">\(n \delta_{K}^{* 2} \rightarrow 0\)</span>” means in a practical application. From this viewpoint the difference between (20.26) and (20.28) is in the assumptions not in the actual reality nor in the actual empirical practice. Eliminating a nuisance (the asymptotic bias) through an assumption is a trick not a substantive use of theory. My strong view is that the result (20.26) is more informative than (20.28). It shows that the asymptotic distribution is normal but has a non-trivial finite sample bias.</p>
</section>
<section id="residuals-and-regression-fit" class="level2" data-number="19.16">
<h2 data-number="19.16" class="anchored" data-anchor-id="residuals-and-regression-fit"><span class="header-section-number">19.16</span> Residuals and Regression Fit</h2>
<p>The fitted regression at <span class="math inline">\(x=X_{i}\)</span> is <span class="math inline">\(\widehat{m}_{K}\left(X_{i}\right)=X_{K i}^{\prime} \widehat{\beta}_{K}\)</span> and the fitted residual is <span class="math inline">\(\widehat{e}_{K i}=Y_{i}-\widehat{m}_{K}\left(X_{i}\right)\)</span>. The leave-one-out prediction errors are</p>
<p><span class="math display">\[
\widetilde{e}_{K i}=Y_{i}-\widehat{m}_{K,-i}\left(X_{i}\right)=Y_{i}-X_{K i}^{\prime} \widehat{\beta}_{K,-i}
\]</span></p>
<p>where <span class="math inline">\(\widehat{\beta}_{K,-i}\)</span> is the least squares coefficient with the <span class="math inline">\(i^{t h}\)</span> observation omitted. Using (3.44) we have the simple computational formula</p>
<p><span class="math display">\[
\widetilde{e}_{K i}=\widehat{e}_{K i}\left(1-X_{K i}^{\prime}\left(\boldsymbol{X}_{K}^{\prime} \boldsymbol{X}_{K}\right)^{-1} X_{K i}\right)^{-1} .
\]</span></p>
<p>As for kernel regression the prediction errors <span class="math inline">\(\widetilde{e}_{K i}\)</span> are better estimators of the errors than the fitted residuals <span class="math inline">\(\widehat{e}_{K i}\)</span> as the former do not have the tendency to over-fit when the number of series terms is large.</p>
</section>
<section id="cross-validation-model-selection" class="level2" data-number="19.17">
<h2 data-number="19.17" class="anchored" data-anchor-id="cross-validation-model-selection"><span class="header-section-number">19.17</span> Cross-Validation Model Selection</h2>
<p>A common method for selection of the number of series terms <span class="math inline">\(K\)</span> is cross-validation. The crossvalidation criterion is the <span class="math inline">\(\operatorname{sum}^{7}\)</span> of squared prediction errors</p>
<p><span class="math display">\[
\operatorname{CV}(K)=\sum_{i=1}^{n} \widetilde{e}_{K i}^{2}=\sum_{i=1}^{n} \widehat{e}_{K i}^{2}\left(1-X_{K i}^{\prime}\left(\boldsymbol{X}_{K}^{\prime} \boldsymbol{X}_{K}\right)^{-1} X_{K i}\right)^{-2} .
\]</span></p>
<p>The CV-selected value of <span class="math inline">\(K\)</span> is the integer which minimizes <span class="math inline">\(\mathrm{CV}(K)\)</span>.</p>
<p>As shown in Theorem <span class="math inline">\(19.7 \mathrm{CV}(K)\)</span> is an approximately unbiased estimator of the integrated meansquared error (IMSE), which is the expected integrated squared error (ISE). The proof of the result is the same for all nonparametric estimators (series as well as kernels) so does not need to be repeated here. Therefore, finding the <span class="math inline">\(K\)</span> which produces the smallest value of <span class="math inline">\(\mathrm{CV}(K)\)</span> is a good indicator that the estimator <span class="math inline">\(\widehat{m}_{K}(x)\)</span> has small IMSE.</p>
<p>For practical implementation we first designate a set of models (sets of basis transformations and number of variables <span class="math inline">\(K\)</span> ) over which to search. (For example, polynomials of order 1 through <span class="math inline">\(K_{\max }\)</span> for some pre-selected <span class="math inline">\(K_{\max }\)</span>.) For each, there is a set of regressors <span class="math inline">\(X_{K}\)</span> which are obtained by transformations of the original variables <span class="math inline">\(X\)</span>. For each set we estimate the regression by least squares, calculate the leaveone-out prediction errors, and the CV criterion. Since the errors are a linear operation this is a simple calculation. The CV-selected <span class="math inline">\(K\)</span> is the integer which produces the smallest value of <span class="math inline">\(\mathrm{CV}(K)\)</span>. <span class="math inline">\(\operatorname{Plots}\)</span> of <span class="math inline">\(\mathrm{CV}(K)\)</span> against <span class="math inline">\(K\)</span> can aid assessment and interpretation. Since the model order <span class="math inline">\(K\)</span> is an integer the CV criterion for series regression is a discrete function, unlike the case of kernel regression.</p>
<p>If it is desired to produce an estimator <span class="math inline">\(\widehat{m}_{K}(x)\)</span> with reduced bias it may be preferred to select a value of <span class="math inline">\(K\)</span> slightly higher than that selected by CV alone.</p>
<p>To illustrate, in Figure <span class="math inline">\(20.6\)</span> we plot the cross-validation functions for the polynomial regression estimates from Figure 20.1. The lowest point marks the polynomial order which minimizes the crossvalidation function. In panel (a) we plot the CV function for the sub-sample of white women. Here we see that the CV-selected order is <span class="math inline">\(p=3\)</span>, a cubic polynomial. In panel (b) we plot the CV function for the sub-sample of Black women, and find that the CV-selected order is <span class="math inline">\(p=2\)</span>, a quadratic. As expected from visual examination of Figure 20.1, the selected model is more parsimonious for panel (b), most likely because it has a substantially smaller sample size. What may be surprising is that even for panel (a), which has a large sample and smooth estimates, the CV-selected model is still relatively parsimonious.</p>
<p>A user who desires a reduced bias estimator might increase the polynomial orders to <span class="math inline">\(p=4\)</span> or even <span class="math inline">\(p=5\)</span> for the subsample of white women and to <span class="math inline">\(p=3\)</span> or <span class="math inline">\(p=4\)</span> for the subsample of Black women. Both CV functions are relatively similar across these values.</p>
<p><span class="math inline">\({ }^{7}\)</span> Some authors define <span class="math inline">\(\mathrm{CV}(K)\)</span> as the average rather than the sum.</p>
<p><img src="images//2022_10_23_2b38d6d54e7725c196e7g-20.jpg" class="img-fluid"></p>
<ol type="a">
<li>White Women</li>
</ol>
<p><img src="images//2022_10_23_2b38d6d54e7725c196e7g-20(1).jpg" class="img-fluid"></p>
<ol start="2" type="a">
<li>Black Women</li>
</ol>
<p>Figure 20.6: Cross-Validation Functions for Polynomial Estimates of Experience Profile</p>
</section>
<section id="variance-and-standard-error-estimation" class="level2" data-number="19.18">
<h2 data-number="19.18" class="anchored" data-anchor-id="variance-and-standard-error-estimation"><span class="header-section-number">19.18</span> Variance and Standard Error Estimation</h2>
<p>The exact conditional variance of the least squares estimator <span class="math inline">\(\widehat{\beta}_{K}\)</span> under independent sampling is</p>
<p><span class="math display">\[
\boldsymbol{V}_{\widehat{\beta}}=\left(\boldsymbol{X}_{K}^{\prime} \boldsymbol{X}_{K}\right)^{-1}\left(\sum_{i=1}^{n} X_{K i} X_{K i}^{\prime} \sigma^{2}\left(X_{i}\right)\right)\left(\boldsymbol{X}_{K}^{\prime} \boldsymbol{X}_{K}\right)^{-1} .
\]</span></p>
<p>The exact conditional variance for the conditional mean estimator <span class="math inline">\(\widehat{m}_{K}(x)=X_{K}(x)^{\prime} \widehat{\beta}_{K}\)</span> is</p>
<p><span class="math display">\[
V_{K}(x)=X_{K}(x)^{\prime}\left(\boldsymbol{X}_{K}^{\prime} \boldsymbol{X}_{K}\right)^{-1}\left(\sum_{i=1}^{n} X_{K i} X_{K i}^{\prime} \sigma^{2}\left(X_{i}\right)\right)\left(\boldsymbol{X}_{K}^{\prime} \boldsymbol{X}_{K}\right)^{-1} X_{K}(x) .
\]</span></p>
<p>Using the notation of Section <span class="math inline">\(20.7\)</span> this equals</p>
<p><span class="math display">\[
\frac{1}{n^{2}} \sum_{i=1}^{n} \widehat{w}_{K}\left(x, X_{i}\right)^{2} \sigma^{2}\left(X_{i}\right) .
\]</span></p>
<p>In the case of conditional homoskedasticity the latter simplifies to</p>
<p><span class="math display">\[
\frac{1}{n} \widehat{w}_{K}(x, x) \sigma^{2} \simeq \frac{1}{n} \zeta_{K}(x)^{2} \sigma^{2} .
\]</span></p>
<p>where <span class="math inline">\(\zeta_{K}(x)\)</span> is the normalized regressor length defined in (20.15). Under conditional heteroskedasticty, large samples, and <span class="math inline">\(K\)</span> large (so that <span class="math inline">\(\widehat{w}_{K}\left(x, X_{i}\right)\)</span> is a local kernel) it approximately equals</p>
<p><span class="math display">\[
\frac{1}{n} w_{K}(x, x) \sigma^{2}(x)=\frac{1}{n} \zeta_{K}(x)^{2} \sigma^{2}(x) .
\]</span></p>
<p>In either case we find that the variance is approximately</p>
<p><span class="math display">\[
V_{K}(x) \simeq \frac{1}{n} \zeta_{K}(x)^{2} \sigma^{2}(x) .
\]</span></p>
<p>This shows that the variance of the series regression estimator is a scale of <span class="math inline">\(\zeta_{K}(x)^{2}\)</span> and the conditional variance. From the plot of <span class="math inline">\(\zeta_{K}(x)\)</span> shown in Figure <span class="math inline">\(20.4\)</span> we can deduce that the series regression estimator will be relatively imprecise at the boundary of the support of <span class="math inline">\(X\)</span>.</p>
<p>The estimator of (20.31) recommended by Andrews (1991a) is the HC3 estimator</p>
<p><span class="math display">\[
\widehat{\boldsymbol{V}}_{\widehat{\beta}}=\left(\boldsymbol{X}_{K}^{\prime} \boldsymbol{X}_{K}\right)^{-1}\left(\sum_{i=1}^{n} X_{K i} X_{K i}^{\prime} \widetilde{e}_{K i}^{2}\right)\left(\boldsymbol{X}_{K}^{\prime} \boldsymbol{X}_{K}\right)^{-1}
\]</span></p>
<p>where <span class="math inline">\(\widetilde{e}_{K i}\)</span> is the leave-one-out prediction error (20.29). Alternatives include the HC1 or HC2 estimators.</p>
<p>Given (20.32) a variance estimator for <span class="math inline">\(\widehat{m}_{K}(x)=X_{K}(x)^{\prime} \widehat{\beta}_{K}\)</span> is</p>
<p><span class="math display">\[
\widehat{V}_{K}(x)=X_{K}(x)^{\prime}\left(\boldsymbol{X}_{K}^{\prime} \boldsymbol{X}_{K}\right)^{-1}\left(\sum_{i=1}^{n} X_{K i} X_{K i}^{\prime} \widetilde{e}_{K i}^{2}\right)\left(\boldsymbol{X}_{K}^{\prime} \boldsymbol{X}_{K}\right)^{-1} X_{K}(x) .
\]</span></p>
<p>A standard error for <span class="math inline">\(\widehat{m}(x)\)</span> is the square root of <span class="math inline">\(\widehat{V}_{K}(x)\)</span>.</p>
</section>
<section id="clustered-observations" class="level2" data-number="19.19">
<h2 data-number="19.19" class="anchored" data-anchor-id="clustered-observations"><span class="header-section-number">19.19</span> Clustered Observations</h2>
<p>Clustered observations are <span class="math inline">\(\left(Y_{i g}, X_{i g}\right)\)</span> for individuals <span class="math inline">\(i=1, \ldots, n_{g}\)</span> in cluster <span class="math inline">\(g=1, \ldots, G\)</span>. The model is</p>
<p><span class="math display">\[
\begin{aligned}
Y_{i g} &amp;=m\left(X_{i g}\right)+e_{i g} \\
\mathbb{E}\left[e_{i g} \mid \boldsymbol{X}_{g}\right] &amp;=0
\end{aligned}
\]</span></p>
<p>where <span class="math inline">\(\boldsymbol{X}_{g}\)</span> is the stacked <span class="math inline">\(X_{i g}\)</span>. Stack <span class="math inline">\(Y_{i g}\)</span> and <span class="math inline">\(e_{i g}\)</span> into cluster-level variables <span class="math inline">\(\boldsymbol{Y}_{g}\)</span> and <span class="math inline">\(\boldsymbol{e}_{g}\)</span>.</p>
<p>The series regression model using cluster-level notation is <span class="math inline">\(\boldsymbol{Y}_{g}=\boldsymbol{X}_{g} \beta_{K}+\boldsymbol{e}_{K g}\)</span>. We can write the series estimator as</p>
<p><span class="math display">\[
\widehat{\beta}_{K}=\left(\sum_{g=1}^{G} \boldsymbol{X}_{g}^{\prime} \boldsymbol{X}_{g}\right)^{-1}\left(\sum_{g=1}^{G} \boldsymbol{X}_{g}^{\prime} \boldsymbol{Y}_{g}\right) .
\]</span></p>
<p>The cluster-level residual vector is <span class="math inline">\(\widehat{\boldsymbol{e}}_{g}=\boldsymbol{Y}_{g}-\boldsymbol{X}_{g} \widehat{\beta}_{K}\)</span>.</p>
<p>As for parametric regression with clustered observations the standard assumption is that the clusters are mutually independent but dependence within each cluster is unstructured. We therefore use the same variance formulae as used for parametric regression. The standard estimator is</p>
<p><span class="math display">\[
\widehat{\boldsymbol{V}}_{\widehat{\beta}}^{\mathrm{CR1}}=\left(\frac{G}{G-1}\right)\left(\boldsymbol{X}_{K}^{\prime} \boldsymbol{X}_{K}\right)^{-1}\left(\sum_{g=1}^{G} \boldsymbol{X}_{g}^{\prime} \widehat{\boldsymbol{e}}_{g} \widehat{\boldsymbol{e}}_{g}^{\prime} \boldsymbol{X}_{g}\right)\left(\boldsymbol{X}_{K}^{\prime} \boldsymbol{X}_{K}\right)^{-1} .
\]</span></p>
<p>An alternative is to use the delete-cluster prediction error <span class="math inline">\(\widetilde{\boldsymbol{e}}_{g}=\boldsymbol{Y}_{g}-\boldsymbol{X}_{g} \widetilde{\beta}_{K,-g}\)</span> where</p>
<p><span class="math display">\[
\widetilde{\beta}_{K,-g}=\left(\sum_{j \neq g} \boldsymbol{X}_{j}^{\prime} \boldsymbol{X}_{j}\right)^{-1}\left(\sum_{j \neq g} \boldsymbol{X}_{j}^{\prime} \boldsymbol{Y}_{j}\right)
\]</span></p>
<p>leading to the estimator</p>
<p><span class="math display">\[
\widehat{\boldsymbol{V}}_{\widehat{\beta}}^{\mathrm{CR} 3}=\left(\boldsymbol{X}_{K}^{\prime} \boldsymbol{X}_{K}\right)^{-1}\left(\sum_{g=1}^{G} \boldsymbol{X}_{g}^{\prime} \widetilde{\boldsymbol{e}}_{g} \widetilde{\boldsymbol{e}}_{g}^{\prime} \boldsymbol{X}_{g}\right)\left(\boldsymbol{X}_{K}^{\prime} \boldsymbol{X}_{K}\right)^{-1} .
\]</span></p>
<p>There is no current theory on how to select the number of series terms <span class="math inline">\(K\)</span> for clustered observations. A reasonable choice is to minimize the delete-cluster cross-validation criterion <span class="math inline">\(\mathrm{CV}(K)=\sum_{g=1}^{G} \widetilde{\boldsymbol{e}}_{g}^{\prime} \widetilde{\boldsymbol{e}}_{g}\)</span>.</p>
</section>
<section id="confidence-bands" class="level2" data-number="19.20">
<h2 data-number="19.20" class="anchored" data-anchor-id="confidence-bands"><span class="header-section-number">19.20</span> Confidence Bands</h2>
<p>When displaying nonparametric estimators such as <span class="math inline">\(\widehat{m}_{K}(x)\)</span> it is customary to display confidence intervals. An asymptotic pointwise <span class="math inline">\(95 %\)</span> confidence interval for <span class="math inline">\(m(x)\)</span> is <span class="math inline">\(\widehat{m}_{K}(x) \pm 1.96 \widehat{V}_{K}^{1 / 2}(x)\)</span>. These confidence intervals can be plotted along with <span class="math inline">\(\widehat{m}_{K}(x)\)</span>.</p>
<p>To illustrate, Figure <span class="math inline">\(20.7\)</span> plots polynomial estimates of the regression of <span class="math inline">\(\log (\)</span> wage <span class="math inline">\()\)</span> on experience using the selected estimates from Figure 20.1, plus 95% confidence bands. Panel (a) plots the estimate for the subsample of white women using <span class="math inline">\(p=5\)</span>. Panel (b) plots the estimate for the subsample of Black women using <span class="math inline">\(p=3\)</span>. The standard errors are calculated using the formula (20.33). You can see that the confidence bands widen at the boundaries. The confidence bands are tight for the larger subsample of white women, and significantly wider for the smaller subsample of Black women. Regardless, both plots indicate that the average wage rises for experience levels up to about 20 years and then flattens for experience levels above 20 years.</p>
<p><img src="images//2022_10_23_2b38d6d54e7725c196e7g-22.jpg" class="img-fluid"></p>
<ol type="a">
<li>White Women</li>
</ol>
<p><img src="images//2022_10_23_2b38d6d54e7725c196e7g-22(1).jpg" class="img-fluid"></p>
<ol start="2" type="a">
<li>Black Women</li>
</ol>
<p>Figure 20.7: Polynomial Estimates with 95% Confidence Bands</p>
<p>There are two deficiencies with these confidence bands. First, they do not take into account the bias <span class="math inline">\(r_{K}(x)\)</span> of the series estimator. Consequently, we should interpret the confidence bounds as valid for the pseudo-true regression (the best finite <span class="math inline">\(K\)</span> approximation) rather than the true regression function <span class="math inline">\(m(x)\)</span>. Second, the above confidence intervals are based on a pointwise (in <span class="math inline">\(x\)</span> ) asymptotic distribution theory. Consequently we should interpret their coverage as having pointwise validity and be cautious about interpreting global shapes from the confidence bands.</p>
</section>
<section id="uniform-approximations" class="level2" data-number="19.21">
<h2 data-number="19.21" class="anchored" data-anchor-id="uniform-approximations"><span class="header-section-number">19.21</span> Uniform Approximations</h2>
<p>Since <span class="math inline">\(\widehat{m}_{K}(x)\)</span> is a function it is desirable to have a distribution theory which applies to the entire function, not just the estimator at a point. This can be used, for example, to construct confidence bands with uniform (in <span class="math inline">\(x\)</span> ) coverage properties. For those familiar with empirical process theory, it might be hoped that the stochastic process</p>
<p><span class="math display">\[
\eta_{K}(x)=\frac{\sqrt{n}\left(\widehat{m}_{K}(x)-m(x)\right)}{V_{K}^{1 / 2}(x)}
\]</span></p>
<p>might converge to a stochastic (Gaussian) process, but this is not the case. Effectively, the process <span class="math inline">\(\eta_{K}(x)\)</span> is not stochastically equicontinuous so conventional empirical process theory does not apply.</p>
<p>To develop a uniform theory, Belloni, Chernozhukov, Chetverikov, and Kato (2015) have introduced what are known as strong approximations. Their method shows that <span class="math inline">\(\eta_{K}(x)\)</span> is equal in distribution to a sequence of Gaussian processes plus a negligible error. Their theory (Theorem 4.4) takes the following form. Under stronger conditions than Assumption <span class="math inline">\(20.2\)</span></p>
<p><span class="math display">\[
\eta_{K}(x)={ }_{d} \frac{X_{K}(x)^{\prime}\left(\boldsymbol{Q}_{K}^{-1} \Omega_{K} \boldsymbol{Q}_{K}^{-1}\right)^{1 / 2}}{V_{K}^{1 / 2}(x)} G_{K}+o_{p}(1)
\]</span></p>
<p>uniformly in <span class="math inline">\(x\)</span>, where ” <span class="math inline">\(=d\)</span> ” means “equality in distribution” and <span class="math inline">\(G_{K} \sim \mathrm{N}\left(0, \boldsymbol{I}_{K}\right)\)</span>.</p>
<p>This shows the distributional result in Theorem <span class="math inline">\(20.10\)</span> can be interpreted as holding uniformly in <span class="math inline">\(x\)</span>. It can also be used to develop confidence bands (different from those from the previous section) with asymptotic uniform coverage.</p>
</section>
<section id="partially-linear-model" class="level2" data-number="19.22">
<h2 data-number="19.22" class="anchored" data-anchor-id="partially-linear-model"><span class="header-section-number">19.22</span> Partially Linear Model</h2>
<p>A common use of a series regression is to allow <span class="math inline">\(m(x)\)</span> to be nonparametric with respect to one variable yet linear in the other variables. This allows flexibility in a particular variable of interest. A partially linear model with vector-valued regressor <span class="math inline">\(X_{1}\)</span> and real-valued continuous <span class="math inline">\(X_{2}\)</span> takes the form</p>
<p><span class="math display">\[
m\left(x_{1}, x_{2}\right)=x_{1}^{\prime} \beta_{1}+m_{2}\left(x_{2}\right) .
\]</span></p>
<p>This model is common when <span class="math inline">\(X_{1}\)</span> are discrete (e.g.&nbsp;binary) and <span class="math inline">\(X_{2}\)</span> is continuously distributed.</p>
<p>Series methods are convenient for partially linear models as we can replace the unknown function <span class="math inline">\(m_{2}\left(x_{2}\right)\)</span> with a series expansion to obtain</p>
<p><span class="math display">\[
m(X) \simeq m_{K}(X)=X_{1}^{\prime} \beta_{1}+X_{2 K}\left(X_{2}\right)^{\prime} \beta_{2 K}=X_{K}^{\prime} \beta_{K}
\]</span></p>
<p>where <span class="math inline">\(X_{2 K}=X_{2 K}\left(x_{2}\right)\)</span> are basis transformations of <span class="math inline">\(x_{2}\)</span> (typically polynomials or splines). After transformation the regressors are <span class="math inline">\(X_{K}=\left(X_{1}^{\prime}, X_{2 K}^{\prime}\right)\)</span> with coefficients <span class="math inline">\(\beta_{K}=\left(\beta_{1}^{\prime}, \beta_{2 K}^{\prime}\right)^{\prime}\)</span>.</p>
</section>
<section id="panel-fixed-effects" class="level2" data-number="19.23">
<h2 data-number="19.23" class="anchored" data-anchor-id="panel-fixed-effects"><span class="header-section-number">19.23</span> Panel Fixed Effects</h2>
<p>The one-way error components nonparametric regression model is</p>
<p><span class="math display">\[
Y_{i t}=m\left(X_{i t}\right)+u_{i}+\varepsilon_{i t}
\]</span></p>
<p>for <span class="math inline">\(i=1, \ldots, N\)</span> and <span class="math inline">\(t=1, \ldots, T\)</span>. It is standard to treat the individual effect <span class="math inline">\(u_{i}\)</span> as a fixed effect. This model can be interpreted as a special case of the partially linear model from the previous section though the dimension of <span class="math inline">\(u_{i}\)</span> is increasing with <span class="math inline">\(N\)</span>.</p>
<p>A series estimator approximates the function <span class="math inline">\(m(x)\)</span> with <span class="math inline">\(m_{K}(x)=X_{K}(x)^{\prime} \beta_{K}\)</span> as in (20.4). This leads to the series regression model <span class="math inline">\(Y_{i t}=X_{K i t}^{\prime} \beta_{K}+u_{i}+\varepsilon_{K i t}\)</span> where <span class="math inline">\(X_{K i t}=X_{K}\left(X_{i t}\right)\)</span>.</p>
<p>The fixed effects estimator is the same as in linear panel data regression. First, the within transformation is applied to <span class="math inline">\(Y_{i t}\)</span> and to the elements of the basis transformations <span class="math inline">\(X_{K i t}\)</span>. These are <span class="math inline">\(\dot{Y}_{i t}=Y_{i t}-\bar{Y}_{i}\)</span> and <span class="math inline">\(\dot{X}_{K i t}=X_{K i t}-\bar{X}_{K i t}\)</span>. The transformed regression equation is <span class="math inline">\(\dot{Y}_{i t}=\dot{X}_{K i t}^{\prime} \beta_{K}+\dot{\varepsilon}_{K i t}\)</span>. What is important about the within transformation for the regressors is that it is applied to the transformed variables <span class="math inline">\(\dot{X}_{K i t}\)</span> not the original regressor <span class="math inline">\(X_{i t}\)</span>. For example, in a polynomial regression the within transformation is applied to the powers <span class="math inline">\(X_{i t}^{j}\)</span>. It is inappropriate to apply the within transformation to <span class="math inline">\(X_{i t}\)</span> and then construct the basis transformations.</p>
<p>The coefficient is estimated by least squares on the within transformed variables</p>
<p><span class="math display">\[
\widehat{\beta}_{K}=\left(\sum_{i=1}^{n} \sum_{t=1}^{T} \dot{X}_{K i t} \dot{X}_{K i t}^{\prime}\right)^{-1}\left(\sum_{i=1}^{n} \sum_{t=1}^{T} \dot{X}_{K i t} \dot{Y}_{i t}\right) .
\]</span></p>
<p>Variance estimators should be calculated using the clustered variance formulas, clustered at the level of the individual <span class="math inline">\(i\)</span>, as described in Section 20.19.</p>
<p>For selection of the number of series terms <span class="math inline">\(K\)</span> there is no current theory. A reasonable method is to use delete-cluster cross-validation as described in Section <span class="math inline">\(20.19\)</span>.</p>
</section>
<section id="multiple-regressors" class="level2" data-number="19.24">
<h2 data-number="19.24" class="anchored" data-anchor-id="multiple-regressors"><span class="header-section-number">19.24</span> Multiple Regressors</h2>
<p>Suppose <span class="math inline">\(X \in \mathbb{R}^{d}\)</span> is vector-valued and continuously distributed. A multivariate series approximation can be obtained as follows. Construct a set of basis transformations for each variable separately. Take their tensor cross-products. Use these as regressors. For example, a <span class="math inline">\(p^{t h}\)</span>-order polynomial is</p>
<p><span class="math display">\[
m_{K}(x)=\beta_{0}+\sum_{j_{1}=1}^{p} \cdots \sum_{j_{d}=1}^{p} x_{1}^{j_{1}} \cdots x_{d}^{j_{d}} \beta_{j_{1}, \ldots, j_{d} K}
\]</span></p>
<p>This includes all powers and cross-products. The coefficient vector has dimension <span class="math inline">\(K=1+p^{d}\)</span>.</p>
<p>The inclusion of cross-products greatly increases the number of coefficients relative to the univariate case. Consequently series applications with multiple regressors typically require large sample sizes.</p>
</section>
<section id="additively-separable-models" class="level2" data-number="19.25">
<h2 data-number="19.25" class="anchored" data-anchor-id="additively-separable-models"><span class="header-section-number">19.25</span> Additively Separable Models</h2>
<p>As discussed in the previous section, when <span class="math inline">\(X \in \mathbb{R}^{d}\)</span> a full series expansion requires a large number of coefficients, which means that estimation precision will be low unless the sample size is quite large. A common simplification is to treat the regression function <span class="math inline">\(m(x)\)</span> as additively separable in the individual regressors. This means that</p>
<p><span class="math display">\[
m(x)=m_{1}\left(x_{1}\right)+m_{2}\left(x_{2}\right)+\cdots+m_{d}\left(x_{d}\right) .
\]</span></p>
<p>We then apply series expansions (polynomials or splines) separately for each component <span class="math inline">\(m_{j}\left(x_{j}\right)\)</span>. Essentially, this is the same as the expansions discussed in the previous section but omitting the interaction terms.</p>
<p>The advantage of additive separability is the reduction in dimensionality. While an unconstrained <span class="math inline">\(p^{t h}\)</span> order polynomial has <span class="math inline">\(1+p^{d}\)</span> coefficients, an additively separable polynomial model has only <span class="math inline">\(1+d p\)</span> coefficients. This is a major reduction.</p>
<p>The disadvantage of additive separability is that the interaction effects have been eliminated. This is a substantive restriction on <span class="math inline">\(m(x)\)</span>.</p>
<p>The decision to impose additive separability can be based on an economic model which suggests the absence of interaction effects, or can be a model selection decision similar to the selection of the number of series terms.</p>
</section>
<section id="nonparametric-instrumental-variables-regression" class="level2" data-number="19.26">
<h2 data-number="19.26" class="anchored" data-anchor-id="nonparametric-instrumental-variables-regression"><span class="header-section-number">19.26</span> Nonparametric Instrumental Variables Regression</h2>
<p>The basic nonparametric instrumental variables (NPIV) model takes the form</p>
<p><span class="math display">\[
\begin{aligned}
Y &amp;=m(X)+e \\
\mathbb{E}[e \mid Z] &amp;=0
\end{aligned}
\]</span></p>
<p>where <span class="math inline">\(Y, X\)</span>, and <span class="math inline">\(Z\)</span> are real valued. Here, <span class="math inline">\(Z\)</span> is an instrumental variable and <span class="math inline">\(X\)</span> an endogenous regressor.</p>
<p>In recent years there have been many papers in the econometrics literature examining the NPIV model, exploring identification, estimation, and inference. Many of these papers are mathematically advanced. Two important and accessible contributions are Newey and Powell (2003) and Horowitz (2011). Here we describe some of the primary results.</p>
<p>A series estimator approximates the function <span class="math inline">\(m(x)\)</span> with <span class="math inline">\(m_{K}(x)=X_{K}(x)^{\prime} \beta_{K}\)</span> as in (20.4). This leads to the series structural equation</p>
<p><span class="math display">\[
Y=X_{K}^{\prime} \beta_{K}+e_{K}
\]</span></p>
<p>where <span class="math inline">\(X_{K}=X_{K}(X)\)</span>. For example, if a polynomial basis is used then <span class="math inline">\(X_{K}=\left(1, X, \ldots, X^{K-1}\right)\)</span>.</p>
<p>Since <span class="math inline">\(X\)</span> is endogenous so is the entire vector <span class="math inline">\(X_{K}\)</span>. Thus we need at least <span class="math inline">\(K\)</span> instrumental varibles. It is useful to consider the reduced form equation for <span class="math inline">\(X\)</span>. A nonparametric specification is</p>
<p><span class="math display">\[
\begin{aligned}
X &amp;=g(Z)+u \\
\mathbb{E}[u \mid Z] &amp;=0 .
\end{aligned}
\]</span></p>
<p>We can appropriate <span class="math inline">\(g(z)\)</span> by the series expansion</p>
<p><span class="math display">\[
g(z) \simeq g_{L}(z)=Z_{L}(z)^{\prime} \gamma_{L}
\]</span></p>
<p>where <span class="math inline">\(Z_{L}(z)\)</span> is an <span class="math inline">\(L \times 1\)</span> vector of basis transformations and <span class="math inline">\(\gamma_{L}\)</span> is an <span class="math inline">\(L \times 1\)</span> coefficient vector. For example, if a polynomial basis is used then <span class="math inline">\(Z_{L}(z)=\left(1, z, \ldots, z^{L-1}\right)\)</span>. Most of the literature for simplicity focuses on the case <span class="math inline">\(L=K\)</span>, but this is not essential to the method.</p>
<p>If <span class="math inline">\(L \geq K\)</span> we can then use <span class="math inline">\(Z_{L}=Z_{L}(Z)\)</span> as instruments for <span class="math inline">\(X_{K}\)</span>. The 2 SLS estimator <span class="math inline">\(\widehat{\beta}_{K, L}\)</span> of <span class="math inline">\(\beta_{K}\)</span> is</p>
<p><span class="math display">\[
\widehat{\beta}_{K, L}=\left(\boldsymbol{X}_{K}^{\prime} \boldsymbol{Z}_{L}\left(\boldsymbol{Z}_{L}^{\prime} \boldsymbol{Z}_{L}\right)^{-1} \boldsymbol{Z}_{L}^{\prime} \boldsymbol{X}_{K}\right)^{-1}\left(\boldsymbol{X}_{K}^{\prime} \boldsymbol{Z}_{L}\left(\boldsymbol{Z}_{L}^{\prime} \boldsymbol{Z}_{L}\right)^{-1} \boldsymbol{Z}_{L}^{\prime} \boldsymbol{Y}\right) .
\]</span></p>
<p>The estimator of <span class="math inline">\(m(x)\)</span> is <span class="math inline">\(\hat{m}_{K}(x)=X_{K}(x)^{\prime} \widehat{\beta}_{K, L}\)</span>. If <span class="math inline">\(L&gt;K\)</span> the linear GMM estimator can be similarly defined.</p>
<p>One way to think about the choice of instruments is to realize that we are actually estimating reduced form equations for each element of <span class="math inline">\(X_{K}\)</span>. The reduced form system is</p>
<p><span class="math display">\[
\begin{aligned}
X_{K} &amp;=\Gamma_{K}^{\prime} Z_{L}+u_{K} \\
\Gamma_{K} &amp;=\mathbb{E}\left[Z_{L} Z_{L}^{\prime}\right]^{-1} \mathbb{E}\left[Z_{L} X_{K}^{\prime}\right] .
\end{aligned}
\]</span></p>
<p>For example, suppose we use a polynomial basis with <span class="math inline">\(K=L=3\)</span>. Then the reduced form system (ignoring intercepts) is</p>
<p><span class="math display">\[
\left[\begin{array}{c}
X \\
X^{2} \\
X^{3}
\end{array}\right]=\left[\begin{array}{lll}
\Gamma_{11} &amp; \Gamma_{21} &amp; \Gamma_{31} \\
\Gamma_{12} &amp; \Gamma_{22} &amp; \Gamma_{32} \\
\Gamma_{13} &amp; \Gamma_{13} &amp; \Gamma_{23}
\end{array}\right]\left[\begin{array}{c}
Z \\
Z^{2} \\
Z^{3}
\end{array}\right]+\left[\begin{array}{l}
u_{1} \\
u_{2} \\
u_{3}
\end{array}\right] .
\]</span></p>
<p>This is modeling the conditional mean of <span class="math inline">\(X, X^{2}\)</span>, and <span class="math inline">\(X^{3}\)</span> as linear functions of <span class="math inline">\(Z, Z^{2}\)</span>, and <span class="math inline">\(Z^{3}\)</span>.</p>
<p>To understand if the coefficient <span class="math inline">\(\beta_{K}\)</span> is identified it is useful to consider the simple reduced form equation <span class="math inline">\(X=\gamma_{0}+\gamma_{1} Z+u\)</span>. Assume that <span class="math inline">\(\gamma_{1} \neq 0\)</span> so that the equation is strongly identified and assume for simplicity that <span class="math inline">\(u\)</span> is independent of <span class="math inline">\(Z\)</span> with mean zero and variance <span class="math inline">\(\sigma_{u}^{2}\)</span>. The identification properties of the reduced form are invariant to rescaling and recentering <span class="math inline">\(X\)</span> and <span class="math inline">\(Z\)</span> so without loss of generality we can set <span class="math inline">\(\gamma_{0}=0\)</span> and <span class="math inline">\(\gamma_{1}=1\)</span>. Then we can calculate that the coefficient matrix in (20.36) is</p>
<p><span class="math display">\[
\left[\begin{array}{lll}
\Gamma_{11} &amp; \Gamma_{21} &amp; \Gamma_{31} \\
\Gamma_{12} &amp; \Gamma_{22} &amp; \Gamma_{32} \\
\Gamma_{13} &amp; \Gamma_{13} &amp; \Gamma_{23}
\end{array}\right]=\left[\begin{array}{ccc}
1 &amp; 0 &amp; 0 \\
0 &amp; 1 &amp; 0 \\
3 \sigma_{u}^{2} &amp; 0 &amp; 1
\end{array}\right] .
\]</span></p>
<p>Notice that this is lower triangular and full rank. It turns out that this property holds for any values of <span class="math inline">\(K=L\)</span> so the coefficient matrix in (20.36) is full rank for any choice of <span class="math inline">\(K=L\)</span>. This means that identification of the coefficient <span class="math inline">\(\beta_{K}\)</span> is strong if the reduced form equation for <span class="math inline">\(X\)</span> is strong. Thus to check the identification condition for <span class="math inline">\(\beta_{K}\)</span> it is sufficient to check the reduced form equation for <span class="math inline">\(X\)</span>. A critically important caveat, however, as discussed in the following section, is that identification of <span class="math inline">\(\beta_{K}\)</span> does not mean that the structural function <span class="math inline">\(m(x)\)</span> is identified.</p>
<p>A simple method for pointwise inference is to use conventional methods to estimate <span class="math inline">\(V_{K, L}=\operatorname{var}\left[\widehat{\beta}_{K, L}\right]\)</span> and then estimate <span class="math inline">\(\operatorname{var}\left[\hat{m}_{K}(x)\right]\)</span> by <span class="math inline">\(X_{K}(x)^{\prime} \widehat{V}_{K, L} X_{K}(x)\)</span> as in series regression. Bootstrap methods are typically advocated to achieve better coverage. See Horowitz (2011) for details. For state-of-the-art inference methods see Chen and Pouzo (2015) and Chen and Christensen (2018).</p>
</section>
<section id="npiv-identification" class="level2" data-number="19.27">
<h2 data-number="19.27" class="anchored" data-anchor-id="npiv-identification"><span class="header-section-number">19.27</span> NPIV Identification</h2>
<p>In the previous section we discussed identication of the pseudo-true coefficient <span class="math inline">\(\beta_{K}\)</span>. In this section we discuss identification of the structural function <span class="math inline">\(m(x)\)</span>. This is considerably more challenging.</p>
<p>To understand how the function <span class="math inline">\(m(x)\)</span> is determined, apply the expectation operator <span class="math inline">\(\mathbb{E}[\cdot \mid Z=z]\)</span> to (20.34). We find</p>
<p><span class="math display">\[
\mathbb{E}[Y \mid Z=z]=\mathbb{E}[m(X) \mid Z=z]
\]</span></p>
<p>with the remainder equal to zero because <span class="math inline">\(\mathbb{E}[e \mid Z]=0\)</span>. We can write this equation as</p>
<p><span class="math display">\[
\mu(z)=\int m(x) f(x \mid z) d x
\]</span></p>
<p>where <span class="math inline">\(\mu(z)=\mathbb{E}[Y \mid Z=z]\)</span> is the CEF of <span class="math inline">\(Y\)</span> given <span class="math inline">\(Z=z\)</span> and <span class="math inline">\(f(x \mid z)\)</span> is the conditional density of <span class="math inline">\(X\)</span> given <span class="math inline">\(Z\)</span>. These two functions are identified <span class="math inline">\({ }^{8}\)</span> from the joint distribution of <span class="math inline">\((Y, X, Z)\)</span>. This means that the unknown function <span class="math inline">\(m(x)\)</span> is the solution to the integral equation (20.37). Conceptually, you can imagine estimating <span class="math inline">\(\mu(z)\)</span> and <span class="math inline">\(f(x \mid z)\)</span> using standard techniques and then finding the solution <span class="math inline">\(m(x)\)</span>. In essence, this is how <span class="math inline">\(m(x)\)</span> is defined and is the nonparametric analog of the classical relationship between the structural and reduced forms.</p>
<p>Unfortunately the solution <span class="math inline">\(m(x)\)</span> may not be unique even in situations where a linear IV model is strongly identified. It is related to what is known as the ill-posed inverse problem. The latter means that the solution <span class="math inline">\(m(x)\)</span> is not necessarily a continuous function of <span class="math inline">\(\mu(z)\)</span>. Identification requires restricting the class of allowable functions <span class="math inline">\(f(x \mid z)\)</span>. This is analogous to the linear IV model where identification requires restrictions on the reduced form equations. Specifying and understanding the needed restrictions is more subtle than in the linear case.</p>
<p>The function <span class="math inline">\(m(x)\)</span> is identified if it is the unique solution to (20.37). Equivalently, <span class="math inline">\(m(x)\)</span> is not identified if we can replace <span class="math inline">\(m(x)\)</span> in (20.37) with <span class="math inline">\(m(x)+\delta(x)\)</span> for some non-trivial function <span class="math inline">\(\delta(x)\)</span> yet the solution does not change. The latter occurs when</p>
<p><span class="math display">\[
\int \delta(x) f(x \mid z) d x=0
\]</span></p>
<p><span class="math inline">\({ }^{8}\)</span> Technically, if <span class="math inline">\(\mathbb{E}|Y|&lt;\infty\)</span>, the joint density of <span class="math inline">\((Z, X)\)</span> exists, and the marginal density of <span class="math inline">\(Z\)</span> is positive. for all <span class="math inline">\(z\)</span>. Equivalently, <span class="math inline">\(m(x)\)</span> is identified if (and only if) (20.38) holds only for the trivial function <span class="math inline">\(\delta(x)=0\)</span>.</p>
<p>Newey and Powell (2003) defined this fundamental condition as completeness.</p>
<p>Proposition 20.1 Completeness. <span class="math inline">\(m(x)\)</span> is identified if (and only if) the completeness condition holds: (20.38) for all <span class="math inline">\(z\)</span> implies <span class="math inline">\(\delta(x)=0\)</span>.</p>
<p>Completeness is a property of the reduced form conditional density <span class="math inline">\(f(x \mid z)\)</span>. It is unaffected by the structural equation <span class="math inline">\(m(x)\)</span>. This is analogous to the linear IV model where identification is a property of the reduced form equations, not a property of the structural equation.</p>
<p>As we stated above, completeness may not be satisfied even if the reduced form relationship is strong. This may be easiest to see by a constructed example <span class="math inline">\({ }^{9}\)</span>. Suppose that the reduced form is <span class="math inline">\(X=Z+u\)</span>, <span class="math inline">\(\operatorname{var}[Z]=1, u\)</span> is independent of <span class="math inline">\(Z\)</span>, and <span class="math inline">\(u\)</span> is distributed <span class="math inline">\(U[-1,1]\)</span>. This reduced form equation has <span class="math inline">\(R^{2}=\)</span> <span class="math inline">\(0.75\)</span> so is strong. The reduced form conditional density is <span class="math inline">\(f(x \mid z)=1 / 2\)</span> on <span class="math inline">\([-1+z, 1+z]\)</span>. Consider <span class="math inline">\(\delta(x)=\sin (x / \pi)\)</span>. We calculate that</p>
<p><span class="math display">\[
\int \delta(x) f(x \mid z) d x=\int_{-1+z}^{1+z} \sin (x / \pi) d x=0
\]</span></p>
<p>for every <span class="math inline">\(z\)</span>, because <span class="math inline">\(\sin (x / \pi)\)</span> is periodic on intervals of length 2 and integrates to zero over <span class="math inline">\([-1,1]\)</span>. This means that equation (20.37) holds <span class="math inline">\({ }^{10}\)</span> for <span class="math inline">\(m(x)+\sin (x / \pi)\)</span>. Thus <span class="math inline">\(m(x)\)</span> is not identified. This is despite the fact that the reduced form equation is strong.</p>
<p>While identification fails for some conditional distributions, it does not fail for all. Andrews (2017) provides classes of distributions which satisfy the completeness condition and shows that these distribution classes are quite general.</p>
<p>What does this mean in practice? If completeness fails then the structural equation is not identified and cannot be consistently estimated. Furthermore, by analogy with the weak instruments literature, we expect that if the conditional distribution is close to incomplete then the structural equation will be poorly identified and our estimators will be imprecise. Since whether or not the conditional distribution is complete is unknown (and more difficult to assess than in the linear model) this is troubling for empirical research. Effectively, in any given application we do not know whether or not the structural function <span class="math inline">\(m(x)\)</span> is identified.</p>
<p>A partial answer is provided by Freyberger (2017). He shows that the joint hypothesis of incompleteness and small asymptotic bias can be tested. By applying the test proposed in Freyberger (2017) a user can obtain evidence that their NPIV estimator is well-behaved in the sense of having low bias. Unlike Stock and Yogo (2005), however, Freyberger’s result does not address inference.</p>
</section>
<section id="npiv-convergence-rate" class="level2" data-number="19.28">
<h2 data-number="19.28" class="anchored" data-anchor-id="npiv-convergence-rate"><span class="header-section-number">19.28</span> NPIV Convergence Rate</h2>
<p>As described in Horowitz (2011), the convergence rate of <span class="math inline">\(\widehat{m}_{K}(x)\)</span> for <span class="math inline">\(m(x)\)</span> is</p>
<p><span class="math display">\[
\left|\widehat{m}_{K}(x)-m(x)\right|=O_{p}\left(K^{-s}+K^{r}\left(\frac{K}{n}\right)^{1 / 2}\right)
\]</span></p>
<p><span class="math inline">\({ }^{9}\)</span> This example was suggested by Joachim Freyberger.</p>
<p><span class="math inline">\({ }^{10}\)</span> In fact, (20.38) holds for <span class="math inline">\(m(x)+\delta(x)\)</span> for any function <span class="math inline">\(\delta(x)\)</span> which is periodic on intervals of length 2 and integrates to zero on <span class="math inline">\([-1,1]\)</span> where <span class="math inline">\(s\)</span> is the smoothness <span class="math inline">\({ }^{11}\)</span> of <span class="math inline">\(m(x)\)</span> and <span class="math inline">\(r\)</span> is the smoothness of the joint density <span class="math inline">\(f_{X Z}(x, z)\)</span> of <span class="math inline">\((X, Z)\)</span>. The first term <span class="math inline">\(K^{-s}\)</span> is the bias due to the approximation of <span class="math inline">\(m(x)\)</span> by <span class="math inline">\(m_{K}(x)\)</span> and takes the same form as for series regression. The second term <span class="math inline">\(K^{r}(K / n)^{1 / 2}\)</span> is the standard deviation of <span class="math inline">\(\widehat{m}_{K}(x)\)</span>. The component <span class="math inline">\((K / n)^{1 / 2}\)</span> is the same as for series regression. The extra component <span class="math inline">\(K^{r}\)</span> is due to the ill-posed inverse problem (see the previous section).</p>
<p>From the rate (20.39) we can calculate that the optimal number of series terms is <span class="math inline">\(K \sim n^{1 /(2 r+2 s+1)}\)</span>. Given this rate the best possible convergence rate in (20.39) is <span class="math inline">\(O_{p}\left(n^{-s /(2 r+2 s+1)}\right)\)</span>. For <span class="math inline">\(r&gt;0\)</span> these rates are slower than for series regression. If we consider the case <span class="math inline">\(s=2\)</span> these rates are <span class="math inline">\(K \sim n^{1 /(2 r+5)}\)</span> and <span class="math inline">\(O_{p}\left(n^{-2 /(2 r+5)}\right)\)</span>, which are slower than the <span class="math inline">\(K \sim n^{1 / 5}\)</span> and <span class="math inline">\(O_{p}\left(n^{-2 / 5}\right)\)</span> rates obtained by series regression.</p>
<p>A very unusual aspect of the rate (20.39) is that smoothness of <span class="math inline">\(f_{X Z}(x, z)\)</span> adversely affects the convergence rate. Larger <span class="math inline">\(r\)</span> means a slower rate of convergence. The limiting case as <span class="math inline">\(r \rightarrow \infty\)</span> (for example, joint normality of <span class="math inline">\(X\)</span> and <span class="math inline">\(Z\)</span> ) results in a logarithmic convergence rate. This seems very strange. The reason is that when the density <span class="math inline">\(f_{X Z}(x, z)\)</span> is very smooth the data contain little information about the function <span class="math inline">\(m(x)\)</span>. This is not intuitive and requires a deeper mathematical treatment.</p>
<p>A practical implication of the convergence rate (20.39) is that the number of series terms <span class="math inline">\(K\)</span> should be much smaller than for regression estimation. Estimation variance increases quickly as <span class="math inline">\(K\)</span> increases. Therefore <span class="math inline">\(K\)</span> should not be taken to be too large. In practice, however, it is unclear how to select the series order <span class="math inline">\(K\)</span> as standard cross-validation methods do not apply.</p>
</section>
<section id="nonparametric-vs-parametric-identification" class="level2" data-number="19.29">
<h2 data-number="19.29" class="anchored" data-anchor-id="nonparametric-vs-parametric-identification"><span class="header-section-number">19.29</span> Nonparametric vs Parametric Identification</h2>
<p>One of the insights from the nonparametric identification literature is that it is important to understand which features of a model are nonparametrically identified, meaning which are identified without functional form assumptions, and which are only identified based on functional form assumptions. Since functional form assumptions are dubious in most economic applications the strong implication is that researchers should strive to work only with models which are nonparametrically identified.</p>
<p>Even if a model is determined to be nonparametrically identified a researcher may estimate a linear (or another simple parametric) model. This is valid because it can be viewed as an approximation to the nonparametric structure. If, however, the model is identified only under a parametric assumption, then it cannot be viewed as an approximation and it is unclear how to interpret the model more broadly.</p>
<p>For example, in the regression model <span class="math inline">\(Y=m(X)+e\)</span> with <span class="math inline">\(\mathbb{E}[e \mid X]=0\)</span> the CEF is nonparametrically identified by Theorem 2.14. This means that researchers who estimate linear regressions (or other lowdimensional regressions) can interpret their estimated model as an approximation to the underlying CEF.</p>
<p>As another example, in the NPIV model where <span class="math inline">\(\mathbb{E}[e \mid Z]=0\)</span> the structural function <span class="math inline">\(m(x)\)</span> is identified under the completeness condition. This means that researchers who estimate linear 2SLS regressions can interpret their estimated model as an approximation to <span class="math inline">\(m(x)\)</span> (subject to the caveat that it is difficult to know if completeness holds).</p>
<p>But the analysis can also point out simple yet subtle mistakes. Take the simple IV model with one exogenous regressor <span class="math inline">\(X_{1}\)</span> and one endogenous regressor <span class="math inline">\(X_{2}\)</span></p>
<p><span class="math display">\[
\begin{aligned}
Y &amp;=\beta_{0}+\beta_{1} X_{1}+\beta_{2} X_{2}+e \\
\mathbb{E}\left[e \mid X_{1}\right] &amp;=0
\end{aligned}
\]</span></p>
<p>with no additional instruments. Suppose that an enterprising researcher suggests using the instrument <span class="math inline">\(X_{1}^{2}\)</span> for <span class="math inline">\(X_{2}\)</span>, using the reasoning that the assumptions imply that <span class="math inline">\(\mathbb{E}\left[X_{1}^{2} e\right]=0\)</span> so <span class="math inline">\(X_{1}^{2}\)</span> is a valid instrument.</p>
<p><span class="math inline">\({ }^{11}\)</span> The number of bounded derivatives. The trouble is that the basic model is not nonparametrically identified. If we write (20.40) as a partially linear nonparametric IV problem</p>
<p><span class="math display">\[
\begin{aligned}
Y &amp;=m\left(X_{1}\right)+\beta_{2} X_{2}+e \\
\mathbb{E}\left[e \mid X_{1}\right] &amp;=0
\end{aligned}
\]</span></p>
<p>then we can see that this model is not identified. We need a valid excluded instrument <span class="math inline">\(Z\)</span>. Since (20.41) is not identified, then (20.40) cannot be viewed as a valid approximation. The apparent identification of (20.40) critically rests on the unknown truth of the linearity in (20.40).</p>
<p>The point of this example is that (20.40) should never be estimated by 2 SLS using the instrument <span class="math inline">\(X_{1}^{2}\)</span> for <span class="math inline">\(X_{2}\)</span>, fundamentally because the nonparametric model (20.41) is not identified.</p>
<p>Another way to describe the mistake is to observe that <span class="math inline">\(X_{1}^{2}\)</span> is a valid instrument in (20.40) only if it is a valid exclusion restriction from the structural equation (20.40). Viewed in the context of (20.41) we can see that this is a functional form restriction. As stated above, identification based on functional form restrictions alone is highly undesirable because functional form assumptions are dubious.</p>
</section>
<section id="example-angrist-and-lavy-1999" class="level2" data-number="19.30">
<h2 data-number="19.30" class="anchored" data-anchor-id="example-angrist-and-lavy-1999"><span class="header-section-number">19.30</span> Example: Angrist and Lavy (1999)</h2>
<p>To illustrate nonparametric instrumental variables in practice we follow Horowitz (2011) by extending the empirical work reported in Angrist and Lavy (1999). Their paper is concerned with measuring the causal effect of the number of students in an elementary school classroom on academic achievement. They address this using a sample of 4067 Israeli <span class="math inline">\(4^{t h}\)</span> and <span class="math inline">\(5^{t h}\)</span> grade classrooms. The dependent variable is the classroom average score on an achievement test. Here we consider the reading score avgverb. The explanatory variables are the number of students in the classroom (classize), the number of students in the grade at the school (enrollment), and a school-level index of students’ socioeconomic status that the authors call percent disadvantaged. The variables enrollment and disadvantaged are treated as exogenous but classize is treated as endogenous because wealthier schools may be able to offer smaller class sizes.</p>
<p>The authors suggest the following instrumental variable for classsize. Israeli regulations specify that class sizes must be capped at 40. This means that classize should be perfectly predictable from enrollment. If the regulation is followed a school with up to 40 students will have one classroom in the grade and schools with 41-80 students will have two classrooms. The precise prediction is that classsize equals</p>
<p><span class="math display">\[
p=\frac{\text { enrollment }}{1+\lfloor 1-\text { enrollment } / 40\rfloor}
\]</span></p>
<p>where <span class="math inline">\(\lfloor a\rfloor\)</span> is the integer part of <span class="math inline">\(a\)</span>. Angrist and Lavy use <span class="math inline">\(p\)</span> as an instrumental variable for classize.</p>
<p>They estimate several specifications. We focus on equation (6) from their Table VII which specifies avgverb as a linear function of classize, disadvantaged, enrollment, grade4, and the interaction of classize and disadvantaged, where grade4 is a dummy indicator for <span class="math inline">\(4^{t h}\)</span> grade classrooms. The equation is estimated by instrumental variables, using <span class="math inline">\(p\)</span> and <span class="math inline">\(p \times\)</span> disadvantaged as instruments. The observations are treated as clustered at the level of the school. Their estimates show a negative and statistically significant impact of classsize on reading test scores.</p>
<p>We are interested in a nonparametric version of their equation. To keep the specification reasonably parsimonious yet flexible we use the following equation.</p>
<p><span class="math display">\[
\begin{aligned}
\text { avgverb } &amp;=\beta_{1}\left(\frac{\text { classize }}{40}\right)+\beta_{2}\left(\frac{\text { classize }}{40}\right)^{2}+\beta_{3}\left(\frac{\text { classize }}{40}\right)^{3} \\
&amp;+\beta_{4}\left(\frac{\text { disadvantaged }}{14}\right)+\beta_{5}\left(\frac{\text { disadvantaged }}{14}\right)^{2}+\beta_{6}\left(\frac{\text { disadvantaged }}{14}\right)^{3} \\
&amp;+\beta_{7}\left(\frac{\text { classize }}{40}\right)\left(\frac{\text { disadvantaged }}{14}\right)+\beta_{8} \text { enrollment }+\beta_{9} \text { grade } 4+\beta_{10}+e .
\end{aligned}
\]</span></p>
<p>This is a cubic equation in classize and disadvantaged, with a single interaction term, and linear in enrollment and grade4. The cubic in disadvantaged was selected by a delete-cluster cross-validation regression without classize. The cubic in classize was selected to allow for a minimal degree of nonparametric flexibility without overparameterization. The variables classize and disadvantaged were scaled by 40 and 14 , respectively, so that the regression is well conditioned. The scaling for classize was selected so that the variable essentially falls in <span class="math inline">\([0,1]\)</span> and the scaling for disadvantaged was selected so that its mean is 1.</p>
<p>Table 20.1: Nonparametric Instrumental Variable Regression for Reading Test Score</p>
<table class="table">
<thead>
<tr class="header">
<th style="text-align: left;">classize/40</th>
<th><span class="math inline">\(34.2\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"></td>
<td><span class="math inline">\((33.4)\)</span></td>
</tr>
<tr class="even">
<td style="text-align: left;">classize/40) <span class="math inline">\(^{2}\)</span></td>
<td><span class="math inline">\(-61.2\)</span></td>
</tr>
<tr class="odd">
<td style="text-align: left;"></td>
<td><span class="math inline">\((53.0)\)</span></td>
</tr>
<tr class="even">
<td style="text-align: left;">(classize/40) <span class="math inline">\(^{3}\)</span></td>
<td><span class="math inline">\(29.0\)</span></td>
</tr>
<tr class="odd">
<td style="text-align: left;"></td>
<td><span class="math inline">\((26.8)\)</span></td>
</tr>
<tr class="even">
<td style="text-align: left;">disadvantaged/14</td>
<td><span class="math inline">\(-12.4\)</span></td>
</tr>
<tr class="odd">
<td style="text-align: left;"></td>
<td><span class="math inline">\((1.7)\)</span></td>
</tr>
<tr class="even">
<td style="text-align: left;">(disadvantaged/14) <span class="math inline">\(^{2}\)</span></td>
<td><span class="math inline">\(3.33\)</span></td>
</tr>
<tr class="odd">
<td style="text-align: left;"></td>
<td><span class="math inline">\((0.54)\)</span></td>
</tr>
<tr class="even">
<td style="text-align: left;">(disadvantaged/14) <span class="math inline">\(^{3}\)</span></td>
<td><span class="math inline">\(-0.377\)</span></td>
</tr>
<tr class="odd">
<td style="text-align: left;"></td>
<td><span class="math inline">\((0.078)\)</span></td>
</tr>
<tr class="even">
<td style="text-align: left;">(classize/40)(disadvantaged/14)</td>
<td><span class="math inline">\(0.81\)</span></td>
</tr>
<tr class="odd">
<td style="text-align: left;"></td>
<td><span class="math inline">\((1.77)\)</span></td>
</tr>
<tr class="even">
<td style="text-align: left;">enrollment</td>
<td><span class="math inline">\(0.015\)</span></td>
</tr>
<tr class="odd">
<td style="text-align: left;"></td>
<td><span class="math inline">\((0.007)\)</span></td>
</tr>
<tr class="even">
<td style="text-align: left;">grade 4</td>
<td><span class="math inline">\(-1.96\)</span></td>
</tr>
<tr class="odd">
<td style="text-align: left;"></td>
<td><span class="math inline">\((0.16)\)</span></td>
</tr>
<tr class="even">
<td style="text-align: left;">Intercept</td>
<td><span class="math inline">\(77.0\)</span></td>
</tr>
<tr class="odd">
<td style="text-align: left;"></td>
<td><span class="math inline">\((6.9)\)</span></td>
</tr>
</tbody>
</table>
<p>The equation is estimated by 2 SLS using <span class="math inline">\((p / 40),(p / 40)^{2},(p / 40)^{3}\)</span> and <span class="math inline">\((p / 40) \times(\)</span> disadvantaged/14) as instruments for the four variables involving classize. The parameter estimates are reported in Table 20.1. The standard errors are clustered at the level of the school. Most of the individual coefficients do not have interpretable meaning, except the positive coefficient on enrollment shows that larger schools achieve slightly higher testscores, and the negative coefficient on grade4 shows that <span class="math inline">\(4^{\text {th }}\)</span> grade students have somewhat lower testscores than <span class="math inline">\(5^{\text {th }}\)</span> grade students.</p>
<p>To obtain a better interpretation of the results we display the estimated regression functions in Figure 20.8. Panel (a) displays the estimated effect of classize on reading test scores. Panel (b) displays the estimated effect of disadvantaged. In both figures the other variables are set at their sample means <span class="math inline">\({ }^{12}\)</span>.</p>
<p><span class="math inline">\({ }^{12}\)</span> If they are set at other values it does not change the qualitative nature of the plots.</p>
<p><img src="images//2022_10_23_2b38d6d54e7725c196e7g-31.jpg" class="img-fluid"></p>
<ol type="a">
<li>Effect of Classize</li>
</ol>
<p><img src="images//2022_10_23_2b38d6d54e7725c196e7g-31(1).jpg" class="img-fluid"></p>
<ol start="2" type="a">
<li>Effect of Percent Disadvantaged</li>
</ol>
<p>Figure 20.8: Nonparametric Instrumental Variables Estimates of the Effect of Classize and Disadvantaged on Reading Test Scores</p>
<p>In panel (a) we can see that increasing class size decreases the average test score. This is consistent with the results from the linear model estimated by Angrist and Lavy (1999). The estimated effect is remarkably close to linear.</p>
<p>In panel (b) we can see that increasing the percentage of disadvantaged students greatly decreases the average test score. This effect is substantially greater in magnitude than the effect of classsize. The effect also appears to be nonlinear. The effect is precisely estimated with tight pointwise confidence bands.</p>
<p>We can also use the estimated model for hypothesis testing. The question addressed by Angrist and Lavy was whether or not classsize has an effect on test scores. Within the nonparametric model estimated here this hypothesis holds under the linear restriction <span class="math inline">\(\mathbb{H}_{0}: \beta_{1}=\beta_{2}=\beta_{3}=\beta_{7}=0\)</span>. Examining the individual coefficient estimates and standard errors it is unclear if this is a significant effect as none of these four coefficient estimates is statistically different from zero. This hypothesis is better tested by a Wald test (using cluster-robust variance estimates). This statistic is <span class="math inline">\(12.7\)</span> which has an asymptotic <span class="math inline">\(\mathrm{p}\)</span>-value of <span class="math inline">\(0.013\)</span>. This suppports the hypothesis that class size has a negative effect on student performance.</p>
<p>We can also use the model to quantify the impact of class size on test scores. Consider the impact of increasing a class from 20 to 40 students. In the above model the predicted impact on test scores is</p>
<p><span class="math display">\[
\theta=\frac{1}{2} \beta_{1}+\frac{3}{4} \beta_{2}+\frac{7}{8} \beta_{3}+\frac{1}{2} \beta_{4} .
\]</span></p>
<p>This is a linear function of the coefficients. The point estimate is <span class="math inline">\(\widehat{\theta}=-2.96\)</span> with a standard error of <span class="math inline">\(1.21\)</span>. (The point estimate is identical to the difference between the endpoints of the estimated function shown in panel (a).) This is a small but substantive impact.</p>
</section>
<section id="technical-proofs" class="level2" data-number="19.31">
<h2 data-number="19.31" class="anchored" data-anchor-id="technical-proofs"><span class="header-section-number">19.31</span> Technical Proofs*</h2>
<p>Proof of Theorem 20.4. We provide a proof under the stronger assumption <span class="math inline">\(\zeta_{K}^{2} K / n \rightarrow 0\)</span>. (The proof presented by Belloni, Chernozhukov, Chetverikov, and Kato (2015) requires a more advanced treatment.) Let <span class="math inline">\(\|\boldsymbol{A}\|_{F}\)</span> denote the Frobenius norm (see Section A.23), and write the <span class="math inline">\(j^{t h}\)</span> element of <span class="math inline">\(\widetilde{X}_{K i}\)</span> as <span class="math inline">\(\widetilde{X}_{j K i}\)</span>. Using (A.18),</p>
<p><span class="math display">\[
\left\|\widetilde{\boldsymbol{Q}}_{K}-\boldsymbol{I}_{K}\right\|^{2} \leq\left\|\widetilde{\boldsymbol{Q}}_{K}-\boldsymbol{I}_{K}\right\|_{F}^{2}=\sum_{j=1}^{K} \sum_{\ell=1}^{K}\left(\frac{1}{n} \sum_{i=1}^{n}\left(\widetilde{X}_{j K i} \widetilde{X}_{\ell K i}-\mathbb{E}\left[\widetilde{X}_{j K i} \widetilde{X}_{\ell K i}\right]\right)\right)^{2} .
\]</span></p>
<p>Then</p>
<p><span class="math display">\[
\begin{aligned}
\mathbb{E}\left[\left\|\widetilde{\boldsymbol{Q}}_{K}-\boldsymbol{I}_{K}\right\|^{2}\right] &amp; \leq \sum_{j=1}^{K} \sum_{\ell=1}^{K} \operatorname{var}\left[\frac{1}{n} \sum_{i=1}^{n} \widetilde{X}_{j K i} \widetilde{X}_{\ell K i}\right] \\
&amp;=\frac{1}{n} \sum_{j=1}^{K} \sum_{\ell=1}^{K} \operatorname{var}\left[\widetilde{X}_{j K i} \widetilde{X}_{\ell K i}\right] \\
&amp; \leq \frac{1}{n} \mathbb{E}\left[\sum_{j=1}^{K} \widetilde{X}_{j K i}^{2} \sum_{\ell=1}^{K} \widetilde{X}_{\ell K i}^{2}\right] \\
&amp;=\frac{1}{n} \mathbb{E}\left[\left(\widetilde{X}_{K i}^{\prime} \widetilde{X}_{K i}\right)^{2}\right] \\
&amp; \leq \frac{\zeta_{K}^{2}}{n} \mathbb{E}\left[\widetilde{X}_{K i}^{\prime} \widetilde{X}_{K i}\right]=\frac{\zeta_{K}^{2} K}{n} \rightarrow 0
\end{aligned}
\]</span></p>
<p>where final lines use (20.17), <span class="math inline">\(\mathbb{E}\left[\widetilde{X}_{K i}^{\prime} \widetilde{X}_{K i}\right]=K\)</span>, and <span class="math inline">\(\zeta_{K}^{2} K / n \rightarrow 0\)</span>. Markov’s inequality implies (20.19).</p>
<p>Proof of Theorem 20.5. By the spectral decomposition we can write <span class="math inline">\(\widetilde{\boldsymbol{Q}}_{K}=\boldsymbol{H}^{\prime} \Lambda \boldsymbol{H}\)</span> where <span class="math inline">\(\boldsymbol{H}^{\prime} \boldsymbol{H}=\boldsymbol{I}_{K}\)</span> and <span class="math inline">\(\Lambda=\operatorname{diag}\left(\lambda_{1}, \ldots, \lambda_{K}\right)\)</span> are the eigenvalues. Then</p>
<p><span class="math display">\[
\left\|\widetilde{\boldsymbol{Q}}_{K}-\boldsymbol{I}_{K}\right\|=\left\|\boldsymbol{H}^{\prime}\left(\Lambda-\boldsymbol{I}_{K}\right) \boldsymbol{H}\right\|=\left\|\Lambda-\boldsymbol{I}_{K}\right\|=\max _{j \leq K}\left|\lambda_{j}-1\right| \underset{p}{\longrightarrow} 0
\]</span></p>
<p>by Theorem 20.4. This implies <span class="math inline">\(\min _{j \leq K}\left|\lambda_{j}\right| \underset{p}{\longrightarrow} 1\)</span> which is (20.21). Similarly</p>
<p><span class="math display">\[
\begin{aligned}
\left\|\widetilde{\boldsymbol{Q}}_{K}^{-1}-\boldsymbol{I}_{K}\right\| &amp;=\left\|\boldsymbol{H}^{\prime}\left(\Lambda^{-1}-\boldsymbol{I}_{K}\right) \boldsymbol{H}\right\| \\
&amp;=\left\|\Lambda^{-1}-\boldsymbol{I}_{K}\right\| \\
&amp;=\max _{j \leq K}\left|\lambda_{j}^{-1}-1\right| \\
&amp; \leq \frac{\max _{j \leq K}\left|1-\lambda_{j}\right|}{\min _{j \leq K}\left|\lambda_{j}\right|} \underset{p}{\longrightarrow} 0 .
\end{aligned}
\]</span></p>
<p>Proof of Theorem 20.6. Using (20.12) we can write</p>
<p><span class="math display">\[
\widehat{m}_{K}(x)-m(x)=X_{K}(x)^{\prime}\left(\widehat{\beta}_{K}-\beta_{K}\right)-r_{K}(x) .
\]</span></p>
<p>Since <span class="math inline">\(e_{K}=r_{K}+e\)</span> is a projection error it satisfies <span class="math inline">\(\mathbb{E}\left[X_{K} e_{K}\right]=0\)</span>. Since <span class="math inline">\(e\)</span> is a regression error it satisfies <span class="math inline">\(\mathbb{E}\left[X_{K} e\right]=0\)</span>. We deduce <span class="math inline">\(\mathbb{E}\left[X_{K} r_{K}\right]=0\)</span>. Hence <span class="math inline">\(\int X_{K}(x) r_{K}(x) f(x) d x=\mathbb{E}\left[X_{K} r_{K}\right]=0\)</span>. Also observe that <span class="math inline">\(\int X_{K}(x) X_{K}(x)^{\prime} d F(x)=\boldsymbol{Q}_{K}\)</span> and <span class="math inline">\(\int r_{K}(x)^{2} d F(x)=\mathbb{E}\left[r_{K}^{2}\right]=\delta_{K}^{2}\)</span>. Then</p>
<p><span class="math display">\[
\begin{aligned}
\operatorname{ISE}(K) &amp;=\int\left(X_{K}(x)^{\prime}\left(\widehat{\beta}_{K}-\beta_{K}\right)-r_{K}(x)\right)^{2} d F(x) \\
&amp;=\left(\widehat{\beta}_{K}-\beta_{K}\right)^{\prime}\left(\int X_{K}(x) X_{K}(x)^{\prime} d F(x)\right)\left(\widehat{\beta}_{K}-\beta_{K}\right) \\
&amp;-2\left(\widehat{\beta}_{K}-\beta_{K}\right)^{\prime}\left(\int X_{K}(x) r_{K}(x) d F(x)\right)+\int r_{K}(x)^{2} d F(x) \\
&amp;=\left(\widehat{\beta}_{K}-\beta_{K}\right)^{\prime} \boldsymbol{Q}_{K}\left(\widehat{\beta}_{K}-\beta_{K}\right)+\delta_{K}^{2}
\end{aligned}
\]</span></p>
<p>We calculate that</p>
<p><span class="math display">\[
\begin{aligned}
\left(\widehat{\beta}_{K}-\beta_{K}\right)^{\prime} \boldsymbol{Q}_{K}\left(\widehat{\beta}_{K}-\beta_{K}\right) &amp;=\left(\boldsymbol{e}_{K}^{\prime} \boldsymbol{X}_{K}\right)\left(\boldsymbol{X}_{K}^{\prime} \boldsymbol{X}_{K}\right)^{-1} \boldsymbol{Q}_{K}\left(\boldsymbol{X}_{K}^{\prime} \boldsymbol{X}_{K}\right)^{-1}\left(\boldsymbol{X}_{K}^{\prime} \boldsymbol{e}_{K}\right) \\
&amp;=\left(\boldsymbol{e}_{K}^{\prime} \widetilde{\boldsymbol{X}}_{K}\right)\left(\widetilde{\boldsymbol{X}}_{K}^{\prime} \widetilde{\boldsymbol{X}}_{K}\right)^{-1}\left(\widetilde{\boldsymbol{X}}_{K}^{\prime} \widetilde{\boldsymbol{X}}_{K}\right)^{-1}\left(\widetilde{\boldsymbol{X}}_{K}^{\prime} \boldsymbol{e}_{K}\right) \\
&amp;=n^{-2}\left(\boldsymbol{e}_{K}^{\prime} \widetilde{\boldsymbol{X}}_{K}\right) \widetilde{\boldsymbol{Q}}_{K}^{-1} \widetilde{\boldsymbol{Q}}_{K}^{-1}\left(\widetilde{\boldsymbol{X}}_{K}^{\prime} \boldsymbol{e}_{K}\right) \\
&amp; \leq\left(\lambda_{\max }\left(\widetilde{\boldsymbol{Q}}_{K}^{-1}\right)\right)^{2}\left(n^{-2} \boldsymbol{e}_{K}^{\prime} \widetilde{\boldsymbol{X}}_{K} \widetilde{\boldsymbol{X}}_{K}^{\prime} \boldsymbol{e}_{K}\right) \\
&amp; \leq O_{p}(1)\left(n^{-2} \boldsymbol{e}_{K}^{\prime} \boldsymbol{X}_{K} \boldsymbol{Q}_{K}^{-1} \boldsymbol{X}_{K}^{\prime} \boldsymbol{e}_{K}\right)
\end{aligned}
\]</span></p>
<p>where <span class="math inline">\(\widetilde{\boldsymbol{X}}_{K}\)</span> and <span class="math inline">\(\widetilde{\boldsymbol{Q}}_{K}\)</span> are the orthogonalized regressors as defined in (20.18). The first inequality is the Quadratic Inequality (B.18), the second is (20.21).</p>
<p>Using the fact that <span class="math inline">\(X_{K} e_{K}\)</span> are mean zero and uncorrelated, (20.17), <span class="math inline">\(\mathbb{E}\left[e_{K}^{2}\right] \leq \mathbb{E}\left[Y^{2}\right]&lt;\infty\)</span>, and Assumption 20.1.2,</p>
<p><span class="math display">\[
\begin{aligned}
\mathbb{E}\left[n^{-2} \boldsymbol{e}_{K}^{\prime} \boldsymbol{X}_{K} \boldsymbol{Q}_{K}^{-1} \boldsymbol{X}_{K}^{\prime} \boldsymbol{e}_{K}\right] &amp;=n^{-1} \mathbb{E}\left[X_{K}^{\prime} \boldsymbol{Q}_{K}^{-1} X_{K} e_{K}^{2}\right] \\
&amp; \leq \frac{\zeta_{K}^{2}}{n} \mathbb{E}\left[e_{K}^{2}\right] \leq o(1) .
\end{aligned}
\]</span></p>
<p>This shows that (20.45) is <span class="math inline">\(o_{p}\)</span> (1). Combined with (20.44) we find <span class="math inline">\(\operatorname{ISE}(K)=o_{p}(1)\)</span> as claimed.</p>
<p>Proof of Theorem 20.7. The assumption <span class="math inline">\(\sigma^{2}(x) \leq \bar{\sigma}^{2}\)</span> implies that</p>
<p><span class="math display">\[
\mathbb{E}\left[e_{K}^{2} \mid X\right]=\mathbb{E}\left[\left(r_{K}+e\right)^{2} \mid X\right]=r_{K}^{2}+\sigma^{2}(X) \leq r_{K}^{2}+\bar{\sigma}^{2} .
\]</span></p>
<p>Thus <span class="math inline">\((20.46)\)</span> is bounded by</p>
<p><span class="math display">\[
\begin{aligned}
n^{-1} \mathbb{E}\left[X_{K}^{\prime} \boldsymbol{Q}_{K}^{-1} X_{K} r_{K}^{2}\right]+n^{-1} \mathbb{E}\left[X_{K}^{\prime} \boldsymbol{Q}_{K}^{-1} X_{K}\right] \bar{\sigma}^{2} &amp; \leq \frac{\zeta_{K}^{2}}{n} \mathbb{E}\left[r_{K}^{2}\right]+n^{-1} \mathbb{E}\left[\operatorname{tr}\left(\boldsymbol{Q}_{K}^{-1} X_{K} X_{K}^{\prime}\right)\right] \bar{\sigma}^{2} \\
&amp;=\frac{\zeta_{K}^{2}}{n} \delta_{K}^{2}+n^{-1} \operatorname{tr}\left(\boldsymbol{I}_{K}\right) \bar{\sigma}^{2} \\
&amp; \leq o\left(\delta_{K}^{2}\right)+\frac{K}{n} \bar{\sigma}^{2}
\end{aligned}
\]</span></p>
<p>where the inequality is Assumption 20.1.2. This implies (20.45) is <span class="math inline">\(o_{p}\left(\delta_{K}^{2}\right)+O_{p}(K / n)\)</span>. Combined with (20.44) we find <span class="math inline">\(\operatorname{ISE}(K)=O_{p}\left(\delta_{K}^{2}+K / n\right)\)</span> as claimed.</p>
<p>Proof of Theorem 20.8. Using (20.12) and linearity</p>
<p><span class="math display">\[
\theta=a(m)=a\left(Z_{K}(x)^{\prime} \beta_{K}\right)+a\left(r_{K}\right)=a_{K}^{\prime} \beta_{K}+a\left(r_{K}\right) .
\]</span></p>
<p>Thus</p>
<p><span class="math display">\[
\begin{aligned}
\sqrt{\frac{n}{V_{K}}}\left(\widehat{\theta}_{K}-\theta+a\left(r_{K}\right)\right) &amp;=\sqrt{\frac{n}{V_{K}}} a_{K}^{\prime}\left(\widehat{\beta}_{K}-\beta_{K}\right) \\
&amp;=\sqrt{\frac{1}{n V_{K}}} a_{K}^{\prime} \widehat{\boldsymbol{Q}}_{K}^{-1} \boldsymbol{X}_{K}^{\prime} \boldsymbol{e}_{K} \\
&amp;=\frac{1}{\sqrt{n V_{K}}} a_{K}^{\prime} \boldsymbol{Q}_{K}^{-1} \boldsymbol{X}_{K}^{\prime} \boldsymbol{e} \\
&amp;+\frac{1}{\sqrt{n V_{K}}} a_{K}^{\prime}\left(\widehat{\boldsymbol{Q}}_{K}^{-1}-\boldsymbol{Q}_{K}^{-1}\right) \boldsymbol{X}_{K}^{\prime} \boldsymbol{e} \\
&amp;+\frac{1}{\sqrt{n V_{K}}} a_{K}^{\prime} \widehat{\boldsymbol{Q}}_{K}^{-1} \boldsymbol{X}_{K}^{\prime} \boldsymbol{r}_{K}
\end{aligned}
\]</span></p>
<p>where we have used <span class="math inline">\(\boldsymbol{e}_{K}=\boldsymbol{e}+\boldsymbol{r}_{K}\)</span>. We take the terms in (20.47)-(20.49) separately. We show that (20.47) is asymptotically normal and (20.48)-(20.49) are asymptotically negligible.</p>
<p>First, take (20.47). We can write</p>
<p><span class="math display">\[
\frac{1}{\sqrt{n V_{K}}} a_{K}^{\prime} \boldsymbol{Q}_{K}^{-1} \boldsymbol{X}_{K}^{\prime} \boldsymbol{e}=\frac{1}{\sqrt{n}} \sum_{i=1}^{n} \frac{1}{\sqrt{V_{K}}} a_{K}^{\prime} \boldsymbol{Q}_{K}^{-1} X_{K i} e_{i} .
\]</span></p>
<p>Observe that <span class="math inline">\(a_{K}^{\prime} \boldsymbol{Q}_{K}^{-1} X_{K i} e_{i} / \sqrt{V_{K}}\)</span> are independent across <span class="math inline">\(i\)</span>, mean zero, and have variance 1 . We will apply Theorem 6.4, for which it is sufficient to verify Lindeberg’s condition: For all <span class="math inline">\(\epsilon&gt;0\)</span></p>
<p><span class="math display">\[
\mathbb{E}\left[\frac{\left(a_{K}^{\prime} \boldsymbol{Q}_{K}^{-1} X_{K} e\right)^{2}}{V_{K}} \mathbb{1}\left\{\frac{\left(a_{K}^{\prime} \boldsymbol{Q}_{K}^{-1} X_{K} e\right)^{2}}{V_{K}} \geq n \epsilon\right\}\right] \rightarrow 0 .
\]</span></p>
<p>Pick <span class="math inline">\(\eta&gt;0\)</span>. Set <span class="math inline">\(B\)</span> sufficiently large so that <span class="math inline">\(\mathbb{E}\left[e^{2} \mathbb{1}\left\{e^{2}&gt;B\right\} \mid X\right] \leq \underline{\sigma}^{2} \eta\)</span> which is feasible by Assumption 20.2.1. Pick <span class="math inline">\(n\)</span> sufficiently large so that <span class="math inline">\(\zeta_{K}^{2} / n \leq \epsilon \underline{\sigma}^{2} / B\)</span>, which is feasible under Assumption 20.1.2.</p>
<p>By Assumption 20.2.2</p>
<p><span class="math display">\[
\begin{aligned}
V_{K} &amp;=\mathbb{E}\left[\left(a_{K}^{\prime} \boldsymbol{Q}_{K}^{-1} X_{K}\right)^{2} e^{2}\right] \\
&amp;=\mathbb{E}\left[\left(a_{K}^{\prime} \boldsymbol{Q}_{K}^{-1} X_{K}\right)^{2} \sigma\left(X^{2}\right)\right] \\
&amp; \geq \mathbb{E}\left[\left(a_{K}^{\prime} \boldsymbol{Q}_{K}^{-1} X_{K}\right)^{2} \underline{\sigma}^{2}\right] \\
&amp;=a_{K}^{\prime} \boldsymbol{Q}_{K}^{-1} \mathbb{E}\left[X_{K} X_{K}^{\prime}\right] \boldsymbol{Q}_{K}^{-1} a_{K} \underline{\sigma}^{2} \\
&amp;=a_{K}^{\prime} \boldsymbol{Q}_{K}^{-1} a_{K} \underline{\sigma}^{2} .
\end{aligned}
\]</span></p>
<p>Then by the Schwarz Inequality, (20.17), (20.52), and <span class="math inline">\(\zeta_{K}^{2} / n \leq \epsilon \underline{\sigma}^{2} / B\)</span></p>
<p><span class="math display">\[
\frac{\left(a_{K}^{\prime} \boldsymbol{Q}_{K}^{-1} X_{K}\right)^{2}}{V_{K}} \leq \frac{\left(a_{K}^{\prime} \boldsymbol{Q}_{K}^{-1} a_{K}\right)\left(X_{K}^{\prime} \boldsymbol{Q}_{K}^{-1} X_{K}\right)}{V_{K}} \leq \frac{\zeta_{K}^{2}}{\underline{\sigma}^{2}} \leq \frac{\epsilon}{B} n .
\]</span></p>
<p>Then the left-side of (20.51) is smaller than</p>
<p><span class="math display">\[
\begin{aligned}
\mathbb{E}\left[\frac{\left(a_{K}^{\prime} \boldsymbol{Q}_{K}^{-1} X_{K}\right)^{2}}{V_{K}} e^{2} \mathbb{1}\left\{e^{2} \geq B\right\}\right] &amp;=\mathbb{E}\left[\frac{\left(a_{K}^{\prime} \boldsymbol{Q}_{K}^{-1} X_{K}\right)^{2}}{V_{K}} \mathbb{E}\left[e^{2} \mathbb{1}\left\{e^{2} \geq B\right\} \mid X\right]\right] \\
&amp; \leq \mathbb{E}\left[\frac{\left(a_{K}^{\prime} \boldsymbol{Q}_{K}^{-1} X_{K}\right)^{2}}{V_{K}}\right] \underline{\sigma}^{2} \eta \\
&amp; \leq \frac{a_{K}^{\prime} \boldsymbol{Q}_{K}^{-1} a_{K}}{V_{K}} \underline{\sigma}^{2} \eta \leq \eta
\end{aligned}
\]</span></p>
<p>the final inequality by (20.52). Since <span class="math inline">\(\eta\)</span> is arbitrary this verifies (20.51) and we conclude</p>
<p><span class="math display">\[
\frac{1}{\sqrt{n V_{K}}} a_{K}^{\prime} \boldsymbol{Q}_{K}^{-1} \boldsymbol{X}_{K}^{\prime} \boldsymbol{e} \underset{d}{\longrightarrow} \mathrm{N}(0,1) \text {. }
\]</span></p>
<p>Second, take (20.48). Assumption <span class="math inline">\(20.2\)</span> implies <span class="math inline">\(\mathbb{E}\left[e^{2} \mid X\right] \leq \bar{\sigma}^{2}&lt;\infty\)</span>. Since <span class="math inline">\(\mathbb{E}[\boldsymbol{e} \mid \boldsymbol{X}]=0\)</span>, applying <span class="math inline">\(\mathbb{E}\left[e^{2} \mid X\right] \leq \bar{\sigma}^{2}\)</span>, the Schwarz and Norm Inequalities, (20.52), and Theorems <span class="math inline">\(20.4\)</span> and <span class="math inline">\(20.5\)</span>,</p>
<p><span class="math display">\[
\begin{aligned}
&amp;\mathbb{E}\left[\left(\frac{1}{\sqrt{n V_{K}}} a_{K}^{\prime}\left(\widehat{\boldsymbol{Q}}_{K}^{-1}-\boldsymbol{Q}_{K}^{-1}\right) \boldsymbol{X}_{K}^{\prime} \boldsymbol{e}\right)^{2} \mid \boldsymbol{X}\right] \\
&amp;=\frac{1}{n V_{K}} a_{K}^{\prime}\left(\widehat{\boldsymbol{Q}}_{K}^{-1}-\boldsymbol{Q}_{K}^{-1}\right) \boldsymbol{X}_{K}^{\prime} \mathbb{E}\left[\boldsymbol{e} \boldsymbol{e}^{\prime} \mid \boldsymbol{X}\right] \boldsymbol{X}_{K}\left(\widehat{\boldsymbol{Q}}_{K}^{-1}-\boldsymbol{Q}_{K}^{-1}\right) a_{K} \\
&amp;\leq \frac{\bar{\sigma}^{2}}{V_{K}} a_{K}^{\prime}\left(\widehat{\boldsymbol{Q}}_{K}^{-1}-\boldsymbol{Q}_{K}^{-1}\right) \widehat{\boldsymbol{Q}}_{K}\left(\widehat{\boldsymbol{Q}}_{K}^{-1}-\boldsymbol{Q}_{K}^{-1}\right) a_{K} \\
&amp;\leq \frac{\bar{\sigma}^{2} a_{K}^{\prime} \boldsymbol{Q}_{K}^{-1} a_{K}}{V_{K}}\left\|\left(\widehat{\boldsymbol{Q}}_{K}^{-1}-\boldsymbol{Q}_{K}^{-1}\right) \widehat{\boldsymbol{Q}}_{K}\left(\widehat{\boldsymbol{Q}}_{K}^{-1}-\boldsymbol{Q}_{K}^{-1}\right)\right\| \\
&amp;=\frac{\bar{\sigma}^{2} a_{K}^{\prime} \boldsymbol{Q}_{K}^{-1} a_{K}}{V_{K}}\left\|\left(\boldsymbol{I}_{K}-\widetilde{\boldsymbol{Q}}_{K}\right)\left(\widetilde{\boldsymbol{Q}}_{K}^{-1}-\boldsymbol{I}_{K}\right)\right\| \\
&amp;\leq \frac{\bar{\sigma}^{2}}{\underline{\sigma}^{2}}\left\|\boldsymbol{I}_{K}-\widetilde{\boldsymbol{Q}}_{K}\right\|\left\|\widetilde{\boldsymbol{Q}}_{K}^{-1}-\boldsymbol{I}_{K}\right\| \\
&amp;\leq \frac{\bar{\sigma}^{2}}{\underline{\sigma}^{2}} o_{p}(1) .
\end{aligned}
\]</span></p>
<p>This establishes that (20.48) is <span class="math inline">\(o_{p}(1)\)</span>.</p>
<p>Third, take (20.49). By the Cauchy-Schwarz inequality, the Quadratic Inequality, (20.52), and (20.21),</p>
<p><span class="math display">\[
\begin{aligned}
\left(\frac{1}{\sqrt{n v_{K}}} a_{K}^{\prime} \widehat{\boldsymbol{Q}}_{K}^{-1} \boldsymbol{X}_{K}^{\prime} \boldsymbol{r}_{K}\right)^{2} &amp; \leq \frac{a_{K}^{\prime} \boldsymbol{Q}_{K}^{-1} a_{K}}{n v_{K}} \boldsymbol{r}_{K}^{\prime} \boldsymbol{X}_{K} \widehat{\boldsymbol{Q}}_{K}^{-1} \boldsymbol{Q}_{K} \widehat{\boldsymbol{Q}}_{K}^{-1} \boldsymbol{X}_{K}^{\prime} \boldsymbol{r}_{K} \\
&amp; \leq \frac{1}{\underline{\sigma}^{2}}\left(\lambda_{\max } \widetilde{\boldsymbol{Q}}_{K}^{-1}\right)^{2} \frac{1}{n} \boldsymbol{r}_{K}^{\prime} \boldsymbol{X}_{K} \boldsymbol{Q}_{K}^{-1} \boldsymbol{X}_{K}^{\prime} \boldsymbol{r}_{K} \\
&amp; \leq O_{p}(1) \frac{1}{n} \boldsymbol{r}_{K}^{\prime} \boldsymbol{X}_{K} \boldsymbol{Q}_{K}^{-1} \boldsymbol{X}_{K}^{\prime} \boldsymbol{r}_{K} .
\end{aligned}
\]</span></p>
<p>Observe that because the observations are independent, <span class="math inline">\(\mathbb{E}\left[X_{K} r_{K}\right]=0, X_{K i}^{\prime} \boldsymbol{Q}_{K}^{-1} X_{K i} \leq \zeta_{K}^{2}\)</span>, and <span class="math inline">\(\mathbb{E}\left[r_{K}^{2}\right]=\delta_{K}^{2}\)</span>,</p>
<p><span class="math display">\[
\begin{aligned}
\mathbb{E}\left[\frac{1}{n} \boldsymbol{r}_{K}^{\prime} \boldsymbol{X}_{K} \boldsymbol{Q}_{K}^{-1} \boldsymbol{X}_{K}^{\prime} \boldsymbol{r}_{K}\right] &amp;=\mathbb{E}\left[\frac{1}{n} \sum_{i=1}^{n} r_{K i} X_{K i}^{\prime} \boldsymbol{Q}_{K}^{-1} \sum_{i j=1}^{n} X_{K j} r_{K j}\right] \\
&amp;=\mathbb{E}\left[X_{K}^{\prime} \boldsymbol{Q}_{K}^{-1} X_{K} r_{K}^{2}\right] \\
&amp; \leq \zeta_{K}^{2} \mathbb{E}\left[r_{K}^{2}\right]=\zeta_{K}^{2} \delta_{K}^{2}=o(1)
\end{aligned}
\]</span></p>
<p>under Assumption 20.2.3. Thus <span class="math inline">\(\frac{1}{n} \boldsymbol{r}_{K}^{\prime} \boldsymbol{X}_{K} \boldsymbol{Q}_{K}^{-1} \boldsymbol{X}_{K}^{\prime} \boldsymbol{r}_{K}=o_{p}(1)\)</span>, (20.54) is <span class="math inline">\(o_{p}(1)\)</span>, and (20.49) is <span class="math inline">\(o_{p}(1)\)</span>.</p>
<p>Together, we have shown that</p>
<p><span class="math display">\[
\sqrt{\frac{n}{V_{K}}}\left(\widehat{\theta}_{K}-\theta_{K}+a\left(r_{K}\right)\right) \underset{d}{\longrightarrow} \mathrm{N}(0,1)
\]</span></p>
<p>as claimed. Proof of Theorem 20.10. It is sufficient to show that</p>
<p><span class="math display">\[
\frac{\sqrt{n}}{V_{K}^{1 / 2}(x)} r_{K}(x)=o(1) \text {. }
\]</span></p>
<p>Notice that by Assumption <span class="math inline">\(20.2 .2\)</span></p>
<p><span class="math display">\[
\begin{aligned}
V_{K}(x) &amp;=X_{K}(x)^{\prime} \boldsymbol{Q}_{K}^{-1} \boldsymbol{\Omega}_{K} \boldsymbol{Q}_{K}^{-1} X_{K}(x) \\
&amp;=\mathbb{E}\left[\left(X_{K}(x)^{\prime} \boldsymbol{Q}_{K}^{-1} X_{K}\right)^{2} e^{2}\right] \\
&amp;=\mathbb{E}\left[\left(X_{K}(x)^{\prime} \boldsymbol{Q}_{K}^{-1} X_{K}\right)^{2} \sigma^{2}(X)\right] \\
&amp; \geq \mathbb{E}\left[\left(X_{K}(x)^{\prime} \boldsymbol{Q}_{K}^{-1} X_{K}\right)^{2}\right] \underline{\sigma}^{2} \\
&amp;=X_{K}(x)^{\prime} \boldsymbol{Q}_{K}^{-1} \mathbb{E}\left[X_{K} X_{K}^{\prime}\right] \boldsymbol{Q}_{K}^{-1} X_{K}(x) \underline{\sigma}^{2} \\
&amp;=X_{K}(x)^{\prime} \boldsymbol{Q}_{K}^{-1} X_{K}(x) \underline{\sigma}^{2} \\
&amp;=\zeta_{K}(x)^{2} \underline{\sigma}^{2} .
\end{aligned}
\]</span></p>
<p>Using the definitions for <span class="math inline">\(\beta_{K}^{*}, r_{K}^{*}(x)\)</span>, and <span class="math inline">\(\delta_{K}^{*}\)</span> from Section 20.8, note that</p>
<p><span class="math display">\[
r_{K}(x)=m(x)-X_{K}^{\prime}(x) \beta_{K}=r_{K}^{*}(x)+X_{K}^{\prime}(x)\left(\beta_{K}^{*}-\beta_{K}\right) .
\]</span></p>
<p>By the Triangle Inequality, the definition (20.10), the Schwarz Inequality, and definition (20.15)</p>
<p><span class="math display">\[
\begin{aligned}
\left|r_{K}(x)\right| &amp; \leq\left|r_{K}^{*}(x)\right|+\left|X_{K}^{\prime}(x)\left(\beta_{K}^{*}-\beta_{K}\right)\right| \\
&amp; \leq \delta_{K}^{*}+\left|X_{K}^{\prime}(x) \boldsymbol{Q}_{K}^{-1} X_{K}^{\prime}(x)\right|^{1 / 2}\left|\left(\beta_{K}^{*}-\beta_{K}\right)^{\prime} \boldsymbol{Q}_{K}\left(\beta_{K}^{*}-\beta_{K}\right)\right|^{1 / 2} \\
&amp;=\delta_{K}^{*}+\zeta_{K}(x)\left|\left(\beta_{K}^{*}-\beta_{K}\right)^{\prime} \boldsymbol{Q}_{K}\left(\beta_{K}^{*}-\beta_{K}\right)\right|^{1 / 2} .
\end{aligned}
\]</span></p>
<p>The coefficients satisfy the relationship</p>
<p><span class="math display">\[
\beta_{K}=\mathbb{E}\left[X_{K} X_{K}^{\prime}\right]^{-1} \mathbb{E}\left[X_{K} m(X)\right]=\beta_{K}^{*}+\mathbb{E}\left[X_{K} X_{K}^{\prime}\right]^{-1} \mathbb{E}\left[X_{K} r_{K}^{*}\right] .
\]</span></p>
<p>Thus</p>
<p><span class="math display">\[
\left(\beta_{K}^{*}-\beta_{K}\right)^{\prime} \boldsymbol{Q}_{K}\left(\beta_{K}^{*}-\beta_{K}\right)=\mathbb{E}\left[r_{K}^{*} X_{K}^{\prime}\right] \mathbb{E}\left[X_{K} X_{K}^{\prime}\right]^{-1} \mathbb{E}\left[X_{K} r_{K}^{*}\right] \leq \mathbb{E}\left[r_{K}^{2 *}\right] \leq \delta_{K}^{* 2} .
\]</span></p>
<p>The first inequality is because <span class="math inline">\(\mathbb{E}\left[r_{K}^{*} X_{K}^{\prime}\right] \mathbb{E}\left[X_{K} X_{K}^{\prime}\right]^{-1} \mathbb{E}\left[X_{K} r_{K}^{*}\right]\)</span> is a projection. The second inequality follows from the definition (20.10). We deduce that</p>
<p><span class="math display">\[
\left|r_{K}(x)\right| \leq\left(1+\zeta_{K}(x)\right) \delta_{K}^{*} \leq 2 \zeta_{K}(x) \delta_{K}^{*} .
\]</span></p>
<p>Equations (20.56), (20.57), and <span class="math inline">\(n \delta_{K}^{* 2}=o(1)\)</span> together imply that</p>
<p><span class="math display">\[
\frac{n}{V_{K}(x)} r_{K}^{2}(x) \leq \frac{4}{\underline{\sigma}^{2}} n \delta_{K}^{* 2}=o(1)
\]</span></p>
<p>which is (20.55), as required.</p>
</section>
<section id="exercises" class="level2" data-number="19.32">
<h2 data-number="19.32" class="anchored" data-anchor-id="exercises"><span class="header-section-number">19.32</span> Exercises</h2>
<p>Exercise 20.1 Take the estimated model</p>
<p><span class="math display">\[
Y=-1+2 X+5(X-1) \mathbb{1}\{X \geq 1\}-3(X-2) \mathbb{1}\{X \geq 2\}+e .
\]</span></p>
<p>What is the estimated marginal effect of <span class="math inline">\(X\)</span> on <span class="math inline">\(Y\)</span> for <span class="math inline">\(X=3\)</span> ?</p>
<p>Exercise 20.2 Take the linear spline with three knots</p>
<p><span class="math display">\[
m_{K}(x)=\beta_{0}+\beta_{1} x+\beta_{2}\left(x-\tau_{1}\right) \mathbb{1}\left\{x \geq \tau_{1}\right\}+\beta_{3}\left(x-\tau_{2}\right) \mathbb{1}\left\{x \geq \tau_{2}\right\}+\beta_{4}\left(x-\tau_{3}\right) \mathbb{1}\left\{x \geq \tau_{3}\right\} .
\]</span></p>
<p>Find the inequality restrictions on the coefficients <span class="math inline">\(\beta_{j}\)</span> so that <span class="math inline">\(m_{K}(x)\)</span> is non-decreasing.</p>
<p>Exercise 20.3 Take the linear spline from the previous question. Find the inequality restrictions on the coefficients <span class="math inline">\(\beta_{j}\)</span> so that <span class="math inline">\(m_{K}(x)\)</span> is concave.</p>
<p>Exercise 20.4 Take the quadratic spline with three knots</p>
<p><span class="math display">\[
m_{K}(x)=\beta_{0}+\beta_{1} x+\beta_{2} x^{3}+\beta_{3}\left(x-\tau_{1}\right)^{2} \mathbb{1}\left\{x \geq \tau_{1}\right\}+\beta_{4}\left(x-\tau_{2}\right)^{2} \mathbb{1}\left\{x \geq \tau_{2}\right\}+\beta_{5}\left(x-\tau_{3}\right)^{2} \mathbb{1}\left\{x \geq \tau_{3}\right\} .
\]</span></p>
<p>Find the inequality restrictions on the coefficients <span class="math inline">\(\beta_{j}\)</span> so that <span class="math inline">\(m_{K}(x)\)</span> is concave.</p>
<p>Exercise 20.5 Consider spline estimation with one knot <span class="math inline">\(\tau\)</span>. Explain why the knot <span class="math inline">\(\tau\)</span> must be within the sample support of <span class="math inline">\(X\)</span>. [Explain what happens if you estimate the regression with the knot placed outside the support of <span class="math inline">\(X]\)</span></p>
<p>Exercise 20.6 You estimate the polynomial regression model:</p>
<p><span class="math display">\[
\widehat{m}_{K}(x)=\widehat{\beta}_{0}+\widehat{\beta}_{1} x+\widehat{\beta}_{2} x^{2}+\cdots+\widehat{\beta}_{p} x^{p} .
\]</span></p>
<p>You are interested in the regression derivative <span class="math inline">\(m^{\prime}(x)\)</span> at <span class="math inline">\(x\)</span>.</p>
<ol type="a">
<li><p>Write out the estimator <span class="math inline">\(\widehat{m}_{K}^{\prime}(x)\)</span> of <span class="math inline">\(m^{\prime}(x)\)</span>.</p></li>
<li><p>Is <span class="math inline">\(\widehat{m}_{K}^{\prime}(x)\)</span> is a linear function of the coefficient estimates?</p></li>
<li><p>Use Theorem <span class="math inline">\(20.8\)</span> to obtain the asymptotic distribution of <span class="math inline">\(\widehat{m}_{K}^{\prime}(x)\)</span>.</p></li>
<li><p>Show how to construct standard errors and confidence intervals for <span class="math inline">\(\widehat{m}_{K}^{\prime}(x)\)</span>.</p></li>
</ol>
<p>Exercise 20.7 Does rescaling <span class="math inline">\(Y\)</span> or <span class="math inline">\(X\)</span> (multiplying by a constant) affect the <span class="math inline">\(\mathrm{CV}(K)\)</span> function? The <span class="math inline">\(K\)</span> which minimizes it?</p>
<p>Exercise 20.8 Take the NPIV approximating equation (20.35) and error <span class="math inline">\(e_{K}\)</span>.</p>
<ol type="a">
<li><p>Does it satisfy <span class="math inline">\(\mathbb{E}\left[e_{K} \mid Z\right]=0\)</span> ?</p></li>
<li><p>If <span class="math inline">\(L=K\)</span> can you define <span class="math inline">\(\beta_{K}\)</span> so that <span class="math inline">\(\mathbb{E}\left[Z_{K} e_{K}\right]=0\)</span> ?</p></li>
<li><p>If <span class="math inline">\(L&gt;K\)</span> does <span class="math inline">\(\mathbb{E}\left[Z_{K} e_{K}\right]=0\)</span> ?</p></li>
</ol>
<p>Exercise 20.9 Take the cps09mar dataset (full sample). (a) Estimate a <span class="math inline">\(6^{\text {th }}\)</span> order polynomial regression of <span class="math inline">\(\log (\)</span> wage <span class="math inline">\()\)</span> on experience. To reduce the ill-conditioned problem first rescale experience to lie in the interval <span class="math inline">\([0,1]\)</span> before estimating the regression.</p>
<ol start="2" type="a">
<li><p>Plot the estimated regression function along with 95% pointwise confidence intervals.</p></li>
<li><p>Interpret the findings. How do you interpret the estimated function for experience levels above 65 ?</p></li>
</ol>
<p>Exercise 20.10 Continuing the previous exercise, compute the cross-validation function (or alternatively the AIC) for polynomial orders 1 through 8.</p>
<ol type="a">
<li><p>Which order minimizes the function?</p></li>
<li><p>Plot the estimated regression function along with <span class="math inline">\(95 %\)</span> pointwise confidence intervals.</p></li>
</ol>
<p>Exercise 20.11 Take the cps09mar dataset (full sample).</p>
<ol type="a">
<li><p>Estimate a <span class="math inline">\(6^{\text {th }}\)</span> order polynomial regression of <span class="math inline">\(\log (\)</span> wage <span class="math inline">\()\)</span> on education. To reduce the ill-conditioned problem first rescale education to lie in the interval <span class="math inline">\([0,1]\)</span>.</p></li>
<li><p>Plot the estimated regression function along with <span class="math inline">\(95 %\)</span> pointwise confidence intervals.</p></li>
</ol>
<p>Exercise 20.12 Continuing the previous exercise, compute the cross-validation function (or alternatively the AIC) for polynomial orders 1 through 8.</p>
<ol type="a">
<li><p>Which order minimizes the function?</p></li>
<li><p>Plot the estimated regression function along with <span class="math inline">\(95 %\)</span> pointwise confidence intervals.</p></li>
</ol>
<p>Exercise 20.13 Take the <span class="math inline">\(\mathrm{cps} 09 \mathrm{mar}\)</span> dataset (full sample).</p>
<ol type="a">
<li><p>Estimate quadratic spline regressions of <span class="math inline">\(\log (\)</span> wage <span class="math inline">\()\)</span> on experience. Estimate four models: (1) no knots (a quadratic); (2) one knot at 20 years; (3) two knots at 20 and 40; (4) four knots at 10, 20, 30, <span class="math inline">\(\&amp; 40\)</span>. Plot the four estimates. Intrepret your findings.</p></li>
<li><p>Compare the four splines models using either cross-validation or AIC. Which is the preferred specification?</p></li>
<li><p>For your selected specification plot the estimated regression function along with 95% pointwise confidence intervals. Intrepret your findings.</p></li>
<li><p>If you also estimated a polynomial specification do you prefer the polynomial or the quadratic spline estimates?</p></li>
</ol>
<p>Exercise 20.14 Take the cps09mar dataset (full sample).</p>
<ol type="a">
<li><p>Estimate quadratic spline regressions of <span class="math inline">\(\log (\)</span> wage <span class="math inline">\()\)</span> on education. Estimate four models: (1) no knots (a quadratic); (2) one knot at 10 years; (3) three knots at 5,10 , and 15 ; (4) four knots at 4,8 , 12, &amp; 16. Plot the four estimates. Intrepret your findings.</p></li>
<li><p>Compare the four splines models using either cross-validation or AIC. Which is the preferred specification?</p></li>
<li><p>For your selected specification plot the estimated regression function along with 95% pointwise confidence intervals. Intrepret your findings. (d) If you also estimated a polynomial specification do you prefer the polynomial or the quadratic spline estimates?</p></li>
</ol>
<p>Exercise 20.15 The RR2010 dataset is from Reinhart and Rogoff (2010). It contains observations on annual U.S. GDP growth rates, inflation rates, and the debt/gdp ratio for the long time span 1791-2009. The paper made the strong claim that GDP growth slows as debt/gdp increases, and in particular that this relationship is nonlinear with debt negatively affecting growth for debt ratios exceeding <span class="math inline">\(90 %\)</span>. Their full dataset includes 44 countries, our extract only includes the United States. Let <span class="math inline">\(Y_{t}\)</span> denote GDP growth and let <span class="math inline">\(D_{t}\)</span> denote debt/gdp. We will estimate the partially linear specification</p>
<p><span class="math display">\[
Y_{t}=\alpha Y_{t-1}+m\left(D_{t-1}\right)+e_{t}
\]</span></p>
<p>using a linear spline for <span class="math inline">\(m(D)\)</span>.</p>
<ol type="a">
<li><p>Estimate (1) linear model; (2) linear spline with one knot at <span class="math inline">\(D_{t-1}=60\)</span>; (3) linear spline with two knots at 40 and 80 . Plot the three estimates.</p></li>
<li><p>For the model with one knot plot with <span class="math inline">\(95 %\)</span> confidence intervals.</p></li>
<li><p>Compare the three splines models using either cross-validation or AIC. Which is the preferred specification?</p></li>
<li><p>Interpret the findings.</p></li>
</ol>
<p>Exercise 20.16 Take the DDK2011 dataset (full sample). Use a quadratic spline to estimate the regression of testscore on percentile.</p>
<ol type="a">
<li><p>Estimate five models: (1) no knots (a quadratic); (2) one knot at 50; (3) two knots at 33 and 66; (4) three knots at 25,50 &amp; 75 ; (5) knots at 20, 40, 60, &amp; 80. Plot the five estimates. Intrepret your findings.</p></li>
<li><p>Select a model. Consider using leave-cluster-one CV.</p></li>
<li><p>For your selected specification plot the estimated regression function along with 95% pointwise confidence intervals. [Use cluster-robust standard errors.] Intrepret your findings.</p></li>
</ol>
<p>Exercise 20.17 The CH J2004 dataset is from Cox, Hansen and Jimenez (2004). As described in Section <span class="math inline">\(20.6\)</span> it contains a sample of 8684 urban Phillipino households. This paper studied the crowding-out impact of a family’s income on non-governmental transfers. Estimate an analog of Figure 20.2(b) using polynomial regression. Regress transfers on a high-order polynomial in income, and possibly a set of regression controls. Ideally, select the polynomial order by cross-validation. You will need to rescale the variable income before taking polynomial powers. Plot the estimated function along with <span class="math inline">\(95 %\)</span> pointwise confidence intervals. Comment on the similarities and differences with Figure 20.2(b). For the regression controls consider the following options: (a) Include no additional controls; (b) Follow the original paper and Figure 20.2(b) by including the variables 12-26 listed in the data description file; (c) Make a different selection, possibly based on cross-validation.</p>
<p>Exercise 20.18 The AL1999 dataset is from Angrist and Lavy (1999). It contains 4067 observations on classroom test scores and explanatory variables including those described in Section 20.30. In Section <span class="math inline">\(20.30\)</span> we report a nonparametric instrumental variables regression of reading test scores (avgverb) on classize, disadvantaged, enrollment, and a dummy for grade=4, using the Angrist-Levy variable (20.42) as an instrument. Repeat the analysis but instead of reading test scores use math test scores (avgmath) as the dependent variable. Comment on the similarities and differences with the results for reading test scores.</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./chpt19-nonparameter.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Nonparametric Regression</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./chpt21-rdd.html" class="pagination-link">
        <span class="nav-page-text"><span class="chapter-number">20</span>&nbsp; <span class="chapter-title">Regression Discontinuity</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->



</body></html>
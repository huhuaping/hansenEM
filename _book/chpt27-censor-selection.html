<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.253">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Hansen中高级计量体系 - 25&nbsp; Censoring and Selection</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./chpt28-model-selection.html" rel="next">
<link href="./chpt26-multiple-choice.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
    <div class="container-fluid d-flex justify-content-between">
      <h1 class="quarto-secondary-nav-title"><span class="chapter-number">25</span>&nbsp; <span class="chapter-title">Censoring and Selection</span></h1>
      <button type="button" class="quarto-btn-toggle btn" aria-label="Show secondary navigation">
        <i class="bi bi-chevron-right"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Hansen中高级计量体系</a> 
        <div class="sidebar-tools-main">
    <a href="https://github.com/huhuaping/hansenEM/" title="Source Code" class="sidebar-tool px-1"><i class="bi bi-github"></i></a>
</div>
    </div>
      </div>
      <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
      </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">前言</a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./participate.html" class="sidebar-item-text sidebar-link">如何参与？</a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chpt01-intro.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chpt01-intro-chn.html" class="sidebar-item-text sidebar-link">介绍</a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./part01-reg.html" class="sidebar-item-text sidebar-link">回归</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chpt02-ce.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Conditional Expectation and Projection</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chpt02-ce-chn.html" class="sidebar-item-text sidebar-link">条件预期和预测</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chpt03-algebra.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">The Algebra of Least Squares</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chpt03-algebra-chn.html" class="sidebar-item-text sidebar-link">最小二乘代数</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chpt04-lsr.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Least Squares Regression</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chpt05-normal-reg.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Normal Regression</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./part02-LSM.html" class="sidebar-item-text sidebar-link">大样本方法</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chpt06-review.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">A Review of Large Sample Asymptotics</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chpt07-asymptotic-ls.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Asymptotic Theory for Least Squares</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chpt08-restricted-est.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Restricted Estimation</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chpt09-hypothesit-test.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Hypothesis Testing</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chpt10-resample-method.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Resampling Methods</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./part03-MEQ.html" class="sidebar-item-text sidebar-link">多方程模型</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chpt11-multi-reg.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Multivariate Regression</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chpt12-iv.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Instrumental Variables</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chpt13-gmm.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Generalized Method of Moments</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./part04-pannel.html" class="sidebar-item-text sidebar-link">面板数据</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chpt14-time-series.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Time Series</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chpt15-multiple-time-series.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Multivariate Time Series</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chpt17-panel-data.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Panel Data</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chpt18-did.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Difference in Differences</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./part05-nonpar.html" class="sidebar-item-text sidebar-link">非参方法</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chpt19-nonparameter.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Nonparametric Regression</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chpt20-series-reg.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">Series Regression</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chpt21-rdd.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">20</span>&nbsp; <span class="chapter-title">Regression Discontinuity</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chpt22-m-est.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">21</span>&nbsp; <span class="chapter-title">M-Estimators</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./part06-nonlinear.html" class="sidebar-item-text sidebar-link">非线性方法</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-6" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chpt23-nonliear-ls.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">22</span>&nbsp; <span class="chapter-title">Nonlinear Least Squares</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chpt24-quantile-reg.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">23</span>&nbsp; <span class="chapter-title">Quantile Regression</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chpt26-multiple-choice.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">24</span>&nbsp; <span class="chapter-title">Multiple Choice</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chpt27-censor-selection.html" class="sidebar-item-text sidebar-link active"><span class="chapter-number">25</span>&nbsp; <span class="chapter-title">Censoring and Selection</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chpt28-model-selection.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">26</span>&nbsp; <span class="chapter-title">Model Selection, Stein Shrinkage, and Model Averaging</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chpt29-ML.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">27</span>&nbsp; <span class="chapter-title">Machine Learning</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./summary.html" class="sidebar-item-text sidebar-link">Summary</a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">References</a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" aria-expanded="true">Appendices</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-7" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./a1-notation.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">A</span>&nbsp; <span class="chapter-title">附录a1</span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
    <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">目录</h2>
   
  <ul>
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction"><span class="toc-section-number">25.1</span>  Introduction</a></li>
  <li><a href="#censoring" id="toc-censoring" class="nav-link" data-scroll-target="#censoring"><span class="toc-section-number">25.2</span>  Censoring</a></li>
  <li><a href="#censored-regression-functions" id="toc-censored-regression-functions" class="nav-link" data-scroll-target="#censored-regression-functions"><span class="toc-section-number">25.3</span>  Censored Regression Functions</a></li>
  <li><a href="#the-bias-of-least-squares-estimation" id="toc-the-bias-of-least-squares-estimation" class="nav-link" data-scroll-target="#the-bias-of-least-squares-estimation"><span class="toc-section-number">25.4</span>  The Bias of Least Squares Estimation</a></li>
  <li><a href="#tobit-estimator" id="toc-tobit-estimator" class="nav-link" data-scroll-target="#tobit-estimator"><span class="toc-section-number">25.5</span>  Tobit Estimator</a></li>
  <li><a href="#identification-in-tobit-regression" id="toc-identification-in-tobit-regression" class="nav-link" data-scroll-target="#identification-in-tobit-regression"><span class="toc-section-number">25.6</span>  Identification in Tobit Regression</a></li>
  <li><a href="#clad-and-cqr-estimators" id="toc-clad-and-cqr-estimators" class="nav-link" data-scroll-target="#clad-and-cqr-estimators"><span class="toc-section-number">25.7</span>  CLAD and CQR Estimators</a></li>
  <li><a href="#illustrating-censored-regression" id="toc-illustrating-censored-regression" class="nav-link" data-scroll-target="#illustrating-censored-regression"><span class="toc-section-number">25.8</span>  Illustrating Censored Regression</a></li>
  <li><a href="#sample-selection-bias" id="toc-sample-selection-bias" class="nav-link" data-scroll-target="#sample-selection-bias"><span class="toc-section-number">25.9</span>  Sample Selection Bias</a></li>
  <li><a href="#heckmans-model" id="toc-heckmans-model" class="nav-link" data-scroll-target="#heckmans-model"><span class="toc-section-number">25.10</span>  Heckman’s Model</a></li>
  <li><a href="#nonparametric-selection" id="toc-nonparametric-selection" class="nav-link" data-scroll-target="#nonparametric-selection"><span class="toc-section-number">25.11</span>  Nonparametric Selection</a></li>
  <li><a href="#panel-data" id="toc-panel-data" class="nav-link" data-scroll-target="#panel-data"><span class="toc-section-number">25.12</span>  Panel Data</a></li>
  <li><a href="#exercises" id="toc-exercises" class="nav-link" data-scroll-target="#exercises"><span class="toc-section-number">25.13</span>  Exercises</a></li>
  </ul>
<div class="toc-actions"><div><i class="bi bi-github"></i></div><div class="action-links"><p><a href="https://github.com/huhuaping/hansenEM/edit/master/chpt27-censor-selection.qmd" class="toc-action">Edit this page</a></p></div></div></nav>
</nav>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title d-none d-lg-block"><span class="chapter-number">25</span>&nbsp; <span class="chapter-title">Censoring and Selection</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<section id="introduction" class="level2" data-number="25.1">
<h2 data-number="25.1" class="anchored" data-anchor-id="introduction"><span class="header-section-number">25.1</span> Introduction</h2>
<p>Censored regression occurs when the dependent variable is constrained, resulting in a pile-up of observations on a boundary. Selection occurs when sampling is endogenous. Under either censoring or selection, conventional (e.g.&nbsp;least squares) estimators are biased for the population parameters of the uncensored/unselected distributions. Methods have been developed to circumvent this bias, including the Tobit, CLAD, and sample selection estimators.</p>
<p>For more detail see Maddala (1983), Amemiya (1985), Gourieroux (2000), Cameron and Trivedi (2005), and Wooldridge (2010).</p>
</section>
<section id="censoring" class="level2" data-number="25.2">
<h2 data-number="25.2" class="anchored" data-anchor-id="censoring"><span class="header-section-number">25.2</span> Censoring</h2>
<p>It is common in economic applications for a dependent variable to have a mixed discrete/continuous distribution, where the discrete component is on the boundary of support. Most commonly this boundary occurs at 0. For example, Figure 27.1(a) displays the density of tabroad (transfers from abroad) from the data file <span class="math inline">\(\mathrm{CH} \mathrm{J} 2004\)</span>. This variable is the amount <span class="math inline">\({ }^{1}\)</span> of remittances received by a Philippino household from a foreign source. For <span class="math inline">\(80 %\)</span> of households this variable equals 0 . The associated mass point is displayed by the bar at zero. For <span class="math inline">\(20 %\)</span> of households tabroad is positive and continuously distributed with a thick right tail. The associated density is displayed by the line graph.</p>
<p>Given such observations it is unclear how to proceed with a regression analysis. Should we use the full sample including the 0’s? Should we use only the sub-sample excluding the 0’s? Or should we do something else?</p>
<p>To answer these questions it is useful to have a statistical model. A classical framework is censored regression, which posits that the observed variable is a censored version of a latent continuouslydistributed variable. Without loss of generality we focus on the case of censoring from below at zero.</p>
<p>The censored regression model was proposed by Tobin (1958) to explain household consumption of durable goods. Tobin observed that in survey data, durable good consumption is zero for a positive fraction of households. He proposed treating the observations as censored realizations from a continuous</p>
<p><span class="math inline">\({ }^{1}\)</span> In thousands of Philippino pesos.</p>
<p><img src="images//2022_10_23_36a5c875f975c7792a09g-02.jpg" class="img-fluid"></p>
<ol type="a">
<li>Transfers from Abroad</li>
</ol>
<p><img src="images//2022_10_23_36a5c875f975c7792a09g-02(1).jpg" class="img-fluid"></p>
<ol start="2" type="a">
<li>Censoring Process</li>
</ol>
<p>Figure 27.1: Censored Densities</p>
<p>distribution. His model is</p>
<p><span class="math display">\[
\begin{aligned}
&amp;Y^{*}=X^{\prime} \beta+e \\
&amp;e \mid X \sim \mathrm{N}\left(0, \sigma^{2}\right) \\
&amp;Y=\max \left(Y^{*}, 0\right) .
\end{aligned}
\]</span></p>
<p>This model is known as Tobit regression or censored regression. It is also known as the Type 1 Tobit model. The variable <span class="math inline">\(Y^{*}\)</span> is latent (unobserved). The observed variable <span class="math inline">\(Y\)</span> is censored from below at zero. This means that positive values are uncensored and negative values are transformed to 0 . This censoring model replicates the observed phenomenon of a pile-up of observations at 0 .</p>
<p>The Tobit model can be justified by a latent choice framework where an individual’s optimal (unconstrained) continuously distributed choice is <span class="math inline">\(Y^{*}\)</span>. Feasible choices, however, are constrained to satisfy <span class="math inline">\(Y \geq 0\)</span>. (For example, negative purchases are not allowed.) Consequently the realized value <span class="math inline">\(Y\)</span> is a censored version of <span class="math inline">\(Y^{*}\)</span>. To justify this interpretation of the model we need to envisage a context where desired choices include negative values. This may be a strained interpretation for consumption purchases, but may be reasonable when negative values make economic sense.</p>
<p>The censoring process is depicted in Figure 27.1(b). The latent variable <span class="math inline">\(Y^{*}\)</span> has a normal density centered at <span class="math inline">\(X^{\prime} \beta\)</span>. The portion for <span class="math inline">\(Y^{*}&gt;0\)</span> is maintained while the portion for <span class="math inline">\(Y^{*}&lt;0\)</span> is transformed to a point mass at zero. The location of the density and the degree of censoring are controlled by the conditional mean <span class="math inline">\(X^{\prime} \beta\)</span>. As <span class="math inline">\(X^{\prime} \beta\)</span> moves to the right the amount of censoring is decreased. As <span class="math inline">\(X^{\prime} \beta\)</span> moves to the left the amount of censoring is increased.</p>
<p>A common “remedy” to the censoring problem is deletion of the censored observations. This creates a truncated distribution which is defined by the following transformation</p>
<p><span class="math display">\[
Y^{\#}=\left\{\begin{array}{cc}
Y &amp; \text { if } Y&gt;0 \\
\text { missing } &amp; \text { if } Y=0 .
\end{array}\right.
\]</span></p>
<p>In Figure 27.1(a) and Figure 27.1(b) the truncated distribution is the continuous portion above 0 with the mass point at 0 omitted.</p>
<p>The censoring and truncation processes are depicted in Figure 27.2(a) which plots 100 random <span class="math inline">\(^{2}\)</span> draws <span class="math inline">\(\left(Y^{*}, X\right)\)</span>. The uncensored variables are marked by the open circles and squares. The open squares are the realizations for which <span class="math inline">\(Y^{*}&gt;0\)</span> and the open circles are the realizations for which <span class="math inline">\(Y^{*}&lt;0\)</span>. The censored distribution replaces the negative values of <span class="math inline">\(Y^{*}\)</span> with 0 , and thus replaces the open with the filled circles. The censored distribution thus consists of the open squares and filled circles. The truncated distribution is obtained by deleting the censored observations so consists of just the open squares.</p>
<p><img src="images//2022_10_23_36a5c875f975c7792a09g-03.jpg" class="img-fluid"></p>
<ol type="a">
<li>Censored Regression Functions</li>
</ol>
<p><img src="images//2022_10_23_36a5c875f975c7792a09g-03(1).jpg" class="img-fluid"></p>
<ol start="2" type="a">
<li>Censoring Probability</li>
</ol>
<p>Figure 27.2: Properties of Censored Distributions</p>
<p>To summarize: we distinguish between three distributions and variables: uncensored <span class="math inline">\(\left(Y^{*}\right)\)</span>, censored <span class="math inline">\((Y)\)</span>, and truncated <span class="math inline">\(\left(Y^{\#}\right)\)</span>.</p>
<p>The censored regression model (27.1) makes several strong assumptions: (1) linearity of the conditional mean; (2) independence of the error; (3) normal distribution. The linearity assumption is not critical as we can interpret <span class="math inline">\(X^{\prime} \beta\)</span> as a series expansion or similar flexible approximation. The independence assumption, however, is quite important as its violation (e.g.&nbsp;heteroskedasticity) changes the properties of the censoring process. The normality assumption is also quite important, yet difficult to justify from first principles.</p>
</section>
<section id="censored-regression-functions" class="level2" data-number="25.3">
<h2 data-number="25.3" class="anchored" data-anchor-id="censored-regression-functions"><span class="header-section-number">25.3</span> Censored Regression Functions</h2>
<p>We can calculate some properties of the conditional distribution of the censored random variable. The conditional probability of censoring is</p>
<p><span class="math display">\[
\mathbb{P}\left[Y^{*}&lt;0 \mid X\right]=\mathbb{P}\left[e&lt;-X^{\prime} \beta \mid X\right]=\Phi\left(-\frac{X^{\prime} \beta}{\sigma}\right) .
\]</span></p>
<p><span class="math inline">\({ }^{2} X \sim U[-3,3]\)</span> and <span class="math inline">\(Y^{*} \mid X \sim \mathrm{N}(1+X, 1)\)</span> We illustrate in Figure 27.2(b). This plots the censoring probability as a function of <span class="math inline">\(X\)</span> for the example from Figure <span class="math inline">\(27.2(\mathrm{a})\)</span>. The censoring probability is <span class="math inline">\(98 %\)</span> for <span class="math inline">\(X=-3,50 %\)</span> for <span class="math inline">\(X=-1\)</span> and <span class="math inline">\(2 %\)</span> for <span class="math inline">\(X=1\)</span>.</p>
<p>The conditional mean of the uncensored, censored, and truncated distributions are</p>
<p><span class="math display">\[
\begin{aligned}
m^{*}(X) &amp;=\mathbb{E}\left[Y^{*} \mid X\right]=X^{\prime} \beta, \\
m(X) &amp;=\mathbb{E}[Y \mid X]=X^{\prime} \beta \Phi\left(\frac{X^{\prime} \beta}{\sigma}\right)+\sigma \phi\left(\frac{X^{\prime} \beta}{\sigma}\right) \\
m^{\#}(X) &amp;=\mathbb{E}\left[Y^{\#} \mid X\right]=X^{\prime} \beta+\sigma \lambda\left(\frac{X^{\prime} \beta}{\sigma}\right) .
\end{aligned}
\]</span></p>
<p>The function <span class="math inline">\(\lambda(x)=\phi(x) / \Phi(x)\)</span> in (27.3) is called the inverse Mills ratio. To obtain (27.2) and (27.3) see Theorems 5.8.4 and 5.8.6 of Probability and Statistics for Economists and Exercise 27.1.</p>
<p>Since <span class="math inline">\(Y^{*} \leq Y \leq Y^{\#}\)</span> it follows that</p>
<p><span class="math display">\[
m^{*}(x) \leq m(x) \leq m^{\#}(x)
\]</span></p>
<p>with strict inequality if the censoring probability is positive. This shows that the conditional means of the truncated and censored distributions are biased for the uncensored conditional mean.</p>
<p>We illustrate in Figure 27.2(a). The uncensored mean <span class="math inline">\(m^{*}(x)\)</span> is marked by the straight line, the censored mean <span class="math inline">\(m(x)\)</span> is marked with the dashed line, and the truncated mean <span class="math inline">\(m^{\#}(x)\)</span> is marked with the long dashes. The functions are strictly ranked with the truncated mean exhibiting the highest bias.</p>
</section>
<section id="the-bias-of-least-squares-estimation" class="level2" data-number="25.4">
<h2 data-number="25.4" class="anchored" data-anchor-id="the-bias-of-least-squares-estimation"><span class="header-section-number">25.4</span> The Bias of Least Squares Estimation</h2>
<p>If the observations <span class="math inline">\((Y, X)\)</span> are generated by the censored model (27.1) then least squares estimation using either the full sample including the censored observations or the truncated sample excluding the censored observations will be biased. Indeed, an estimator which is consistent for the CEF (such as a series estimator) will estimate the censored mean <span class="math inline">\(m(x)\)</span> or truncated mean <span class="math inline">\(m^{\#}(x)\)</span> in the censored and truncated samples, respectively, not the latent CEF <span class="math inline">\(m^{*}(x)\)</span>.</p>
<p>It is also interesting to consider the properties of the best linear predictor of <span class="math inline">\(Y\)</span> on <span class="math inline">\(X\)</span>, which is the estimand of the least squares estimator. In general, this depends on the marginal distribution of the regressors. However, when the regressors are normally distributed it takes a simple form as discovered by Greene (1981). Write the model with an explicit intercept as <span class="math inline">\(Y^{*}=\alpha+X^{\prime} \beta+e\)</span> and assume <span class="math inline">\(X \sim \mathrm{N}(0, \Sigma)\)</span>. Greene showed that the best linear predictor slope coefficient is</p>
<p><span class="math display">\[
\beta_{\mathrm{BLP}}=\beta(1-\pi)
\]</span></p>
<p>where <span class="math inline">\(\pi=\mathbb{P}[Y=0]\)</span> is the censoring probability. We derive (27.4) at the end of this section.</p>
<p>Greene’s formula (27.4) shows that the least squares slope coefficients are shrunk towards zero proportionately with the censoring percentage. While Greene’s formula is special to normal regressors it gives a baseline estimate of the bias due to censoring. The censoring proportion <span class="math inline">\(\pi\)</span> is easily estimated from the sample (e.g.&nbsp;<span class="math inline">\(\pi=0.80\)</span> in our transfers example) allowing a quick calculation of the expected bias due to censoring. This can be used as a rule of thumb. If the expected bias is sufficiently small (e.g.&nbsp;less than 5%) the resulting expected estimation bias (e.g.&nbsp;5%) may be acceptable, leading to conventional least squares estimation using the full sample without an explicit treatment of censoring. However, if the censoring proportion <span class="math inline">\(\pi\)</span> is sufficiently high (e.g.&nbsp;10%) then estimation methods which correct for censoring bias may be desired.</p>
<p>We close this section by deriving (27.4). The calculation is simplified by a trick suggested by Goldberger (1981). Notice that <span class="math inline">\(Y^{*} \sim \mathrm{N}\left(\alpha, \sigma_{Y}^{2}\right)\)</span> with <span class="math inline">\(\sigma_{Y}^{2}=\sigma^{2}+\beta^{\prime} \Sigma \beta\)</span>. Using the moments of the truncated normal distribution (Probability and Statistics for Economists, Theorems 5.7.6 and 5.7.8) and setting <span class="math inline">\(\lambda=\lambda\left(\alpha / \sigma_{Y}\right)\)</span> we can calculate that</p>
<p><span class="math display">\[
\begin{aligned}
\mathbb{E}\left[\left(Y^{*}-\alpha\right) Y^{*} \mid Y^{*}&gt;0\right] &amp;=\operatorname{var}\left[Y^{*} \mid Y^{*}&gt;0\right]+\left(\mathbb{E}\left[Y^{*} \mid Y^{*}&gt;0\right]-\alpha\right) \mathbb{E}\left[Y^{*} \mid Y^{*}&gt;0\right] \\
&amp;=\sigma_{Y}^{2}\left(1-\frac{\alpha}{\sigma_{Y}} \lambda-\lambda^{2}\right)+\sigma_{Y} \lambda\left(\alpha+\sigma_{Y} \lambda\right)=\sigma_{Y}^{2} .
\end{aligned}
\]</span></p>
<p>The projection of <span class="math inline">\(X\)</span> on <span class="math inline">\(Y^{*}\)</span> is <span class="math inline">\(X=\mathbb{E}\left[X Y^{*}\right] \sigma_{Y}^{-2}\left(Y^{*}-\alpha\right)+u\)</span> where <span class="math inline">\(u\)</span> is independent of <span class="math inline">\(Y^{*}\)</span>. This implies</p>
<p><span class="math display">\[
\begin{aligned}
\mathbb{E}\left[X Y^{*} \mid Y^{*}&gt;0\right] &amp;=\mathbb{E}\left[\left(\mathbb{E}\left[X Y^{*}\right] \sigma_{Y}^{-2}\left(Y^{*}-\alpha\right)+u\right) Y^{*} \mid Y^{*}&gt;0\right] \\
&amp;=\mathbb{E}\left[X Y^{*}\right] \sigma_{Y}^{-2} \mathbb{E}\left[\left(Y^{*}-\alpha\right) Y^{*} \mid Y^{*}&gt;0\right] \\
&amp;=\mathbb{E}\left[X Y^{*}\right]
\end{aligned}
\]</span></p>
<p>Hence</p>
<p><span class="math display">\[
\begin{aligned}
\beta_{\mathrm{BLP}} &amp;=\mathbb{E}\left[X X^{\prime}\right]^{-1} \mathbb{E}[X Y] \\
&amp;=\mathbb{E}\left[X X^{\prime}\right]^{-1} \mathbb{E}\left[X Y^{*} \mid Y^{*}&gt;0\right](1-\pi) \\
&amp;=\mathbb{E}\left[X X^{\prime}\right]^{-1} \mathbb{E}\left[X Y^{*}\right](1-\pi) \\
&amp;=\beta(1-\pi)
\end{aligned}
\]</span></p>
<p>which is (27.4) as claimed.</p>
</section>
<section id="tobit-estimator" class="level2" data-number="25.5">
<h2 data-number="25.5" class="anchored" data-anchor-id="tobit-estimator"><span class="header-section-number">25.5</span> Tobit Estimator</h2>
<p>Tobin (1958) proposed estimation of the censored regression model (27.1) by maximum likelihood.</p>
<p>The censored variable <span class="math inline">\(Y\)</span> has a conditional distribution function which is a mixture of continuous and discrete components:</p>
<p><span class="math display">\[
F(y \mid x)=\left\{\begin{array}{cc}
0, &amp; y&lt;0 \\
\Phi\left(\frac{y-x^{\prime} \beta}{\sigma}\right), &amp; y \geq 0
\end{array}\right.
\]</span></p>
<p>The associated density <span class="math inline">\({ }^{3}\)</span> function is</p>
<p><span class="math display">\[
f(y \mid x)=\Phi\left(-\frac{x^{\prime} \beta}{\sigma}\right)^{\mathbb{1}\{y=0\}}\left[\sigma^{-1} \phi\left(\frac{y-x^{\prime} \beta}{\sigma}\right)\right]^{\mathbb{1}\{y&gt;0\}} .
\]</span></p>
<p>The first component is the probability of censoring and the second component is the normal regression density.</p>
<p>The log-likelihood is the sum of the log density functions evaluated at the observations:</p>
<p><span class="math display">\[
\begin{aligned}
\ell_{n}\left(\beta, \sigma^{2}\right) &amp;=\sum_{i=1}^{n} \log f\left(Y_{i} \mid X_{i}\right) \\
&amp;=\sum_{i=1}^{n}\left(\mathbb{1}\left\{Y_{i}=0\right\} \log f\left(Y_{i} \mid X_{i}\right)+\mathbb{1}\left\{Y_{i}&gt;0\right\} \log \left[\sigma^{-1} \phi\left(\frac{Y_{i}-X_{i}^{\prime} \beta}{\sigma}\right)\right]\right) \\
&amp;=\sum_{Y_{i}=0} \log \Phi\left(-\frac{X_{i}^{\prime} \beta}{\sigma}\right)-\frac{1}{2} \sum_{Y_{i}&gt;0}\left(\log \left(2 \pi \sigma^{2}\right)+\frac{1}{\sigma^{2}}\left(Y_{i}-X_{i}^{\prime} \beta\right)^{2}\right)
\end{aligned}
\]</span></p>
<p><span class="math inline">\({ }^{3}\)</span> Since the distribution function is discontinuous at <span class="math inline">\(y=0\)</span> the density is technically the derivative with respect to a mixed continuous/discrete measure. The first component is the same as in a probit model, and the second component is the same as for the normal regression model.</p>
<p>The <span class="math inline">\(\operatorname{MLE}\left(\widehat{\beta}, \widehat{\sigma}^{2}\right)\)</span> are the values which maximize the log-likelihood <span class="math inline">\(\ell_{n}\left(\beta, \sigma^{2}\right)\)</span>. This estimator was nicknamed “Tobit” by Goldberger because of its connection with the probit estimator. Amemiya (1973) established its asymptotic normality.</p>
<p>Computation is improved, as shown by Olsen (1978), if we transform the parameters to <span class="math inline">\(\gamma=\beta / \sigma\)</span> and <span class="math inline">\(\nu=1 / \sigma\)</span>. Then the reparameterized log-likelihood equals</p>
<p><span class="math display">\[
\ell_{n}(\gamma, v)=\sum_{Y_{i}=0} \log \Phi\left(-X_{i}^{\prime} \gamma\right)+\sum_{Y_{i}&gt;0} \log (v / \sqrt{2 \pi})+\left(-\frac{1}{2}\right) \sum_{Y_{i}&gt;0}\left(Y_{i} v-X_{i}^{\prime} \gamma\right)^{2} .
\]</span></p>
<p>This is the sum of three terms, each of which is globally concave in <span class="math inline">\((\gamma, v)\)</span> (as we now discuss), so <span class="math inline">\(\ell_{n}(\gamma, v)\)</span> is globally concave in <span class="math inline">\((\gamma, v)\)</span> ensuring global convergence of Newton-based optimizers. Indeed, the third term in (27.5) is the negative of a quadratic in <span class="math inline">\((\gamma, \nu)\)</span>, so is concave. The second term in (27.5) is logarithmic in <span class="math inline">\(v\)</span>, which is concave. The first term in (27.5) is a function only of <span class="math inline">\(\gamma\)</span> and has second derivative</p>
<p><span class="math display">\[
\frac{\partial^{2}}{\partial \gamma \partial \gamma^{\prime}} \sum_{Y_{i}=0} \log \Phi\left(-X_{i}^{\prime} \gamma\right)=\sum_{Y_{i}=0} X_{i} X_{i}^{\prime} \lambda^{\prime}\left(-X_{i}^{\prime} \gamma\right)
\]</span></p>
<p>which is negative definite because the Mills ratio satisfies <span class="math inline">\(\lambda^{\prime}(u)&lt;0\)</span> (see Theorem 5.7.7 in Probability and Statistics for Economists). Hence the first term in (27.5) is concave.</p>
<p>In Stata, Tobit regression can be estimated with the tobit command. In R there are several options including the tobit command in the AER package.</p>
<p>James Tobin\ James Tobin (1918-2002) of the United States was one of the leading macroe-\ conomists of the mid-twentieth century and winner of the 1981 Nobel Memorial\ Prize in Economic Sciences. His 1958 paper introduced censored regression and\ its MLE, typically called the Tobit estimator. As a fascinating coincidence, the\ name “Tobit” also arises in the 1951 novel The Caine Mutiny, set on a U.S. Navy\ destroyer during World War II. At one point in the novel the author describes\ a crew member named “Tobit” who had “a mind like a sponge” because of his\ strong intellect. It turns out the author (Herman Wouk) and James Tobin served\ on the same Navy destroyer during WWII. Go figure!</p>
</section>
<section id="identification-in-tobit-regression" class="level2" data-number="25.6">
<h2 data-number="25.6" class="anchored" data-anchor-id="identification-in-tobit-regression"><span class="header-section-number">25.6</span> Identification in Tobit Regression</h2>
<p>The Tobit model (27.1) makes several strong assumptions. Which are critical? To investigate this question consider the nonparametric censored regression framework</p>
<p><span class="math display">\[
\begin{aligned}
Y^{*} &amp;=m(X)+e \\
\mathbb{E}[e] &amp;=0 \\
Y &amp;=\max \left(Y^{*}, 0\right)
\end{aligned}
\]</span></p>
<p>where <span class="math inline">\(e \sim F\)</span> independent of <span class="math inline">\(X\)</span>, and the regression function <span class="math inline">\(m(x)\)</span> and distribution function <span class="math inline">\(F(e)\)</span> are unknown. What is identified?</p>
<p>Suppose that the random variable <span class="math inline">\(m(X)\)</span> has unbounded support on the real line (as occurs when <span class="math inline">\(m(X)=X^{\prime} \beta\)</span> and <span class="math inline">\(X\)</span> has an unbounded distribution such as the normal). Then we can find a set <span class="math inline">\(\mathscr{X} \subset \mathbb{R}^{k}\)</span> such that for <span class="math inline">\(x \in \mathscr{X}, \mathbb{P}[Y=0 \mid X=x]=F(-m(x)) \simeq 0\)</span>. We can then imagine taking the subsample of observations for which <span class="math inline">\(X \in \mathscr{X}\)</span>. The function <span class="math inline">\(m(x)\)</span> is identified for <span class="math inline">\(x \in \mathscr{X}\)</span>, permitting the identification of the distribution <span class="math inline">\(F(e)\)</span>. As the censoring probability <span class="math inline">\(\mathbb{P}[Y=0 \mid X=x]=F(-m(x))\)</span> is globally identified the function <span class="math inline">\(m(x)\)</span> is globally identified as well. This discussion shows that so long as we maintain the assumption that <span class="math inline">\(X\)</span> and <span class="math inline">\(e\)</span> are independent, the regression function <span class="math inline">\(m(x)\)</span> and distribution function <span class="math inline">\(F(e)\)</span> are nonparametrically identified when the CEF <span class="math inline">\(m(X)\)</span> has full support. These two assumptions, however, are essential as we now discuss.</p>
<p>Suppose the full support condition fails in the sense that the regression function is bounded <span class="math inline">\(m(X) \leq\)</span> <span class="math inline">\(\bar{m}\)</span> at a value such that <span class="math inline">\(\mathbb{P}[Y=0 \mid X=x]=F(-\bar{m})&gt;0\)</span>. In this case the error distribution <span class="math inline">\(F(e)\)</span> is not identified for <span class="math inline">\(e \leq-\bar{m}\)</span>. This means that the distribution function can take any shape for <span class="math inline">\(e \leq-\bar{m}\)</span> so long as it is weakly increasing. This implies that the expectation <span class="math inline">\(\mathbb{E}[e]\)</span> is not identified so the location of <span class="math inline">\(m(x)\)</span> (the intercept of the regression) is not identified.</p>
<p>The second important assumption is that <span class="math inline">\(e\)</span> is independent of <span class="math inline">\(X\)</span>. This assumption has been relaxed by Powell <span class="math inline">\((1984,1986)\)</span> in the conditional quantile framework. The model is</p>
<p><span class="math display">\[
\begin{aligned}
Y^{*} &amp;=q_{\tau}(X)+e_{\tau} \\
\mathbb{Q}_{\tau}\left[e_{\tau} \mid X\right] &amp;=0 \\
Y &amp;=\max \left(Y^{*}, 0\right)
\end{aligned}
\]</span></p>
<p>for some <span class="math inline">\(\tau \in(0,1)\)</span>. This model defines <span class="math inline">\(q_{\tau}(x)\)</span> as the <span class="math inline">\(\tau^{t h}\)</span> conditional quantile function. Since quantiles are equivariant to monotone transformations we have the relationship</p>
<p><span class="math display">\[
\mathbb{Q}_{\tau}[Y \mid X=x]=\max \left(q_{\tau}(x), 0\right) .
\]</span></p>
<p>Thus the conditional quantile function of <span class="math inline">\(Y\)</span> is the censored quantile function of <span class="math inline">\(Y^{*}\)</span>. The function <span class="math inline">\(\mathbb{Q}_{\tau}[Y \mid X=x]\)</span> is identified from the joint distribution of <span class="math inline">\((Y, X)\)</span>. Consequently the function <span class="math inline">\(q_{\tau}(x)\)</span> is identified for any <span class="math inline">\(x\)</span> such that <span class="math inline">\(q_{\tau}(x)&gt;0\)</span>. This is an important conceptual breakthrough. Powell’s result shows that identification of <span class="math inline">\(q_{\tau}(x)\)</span> does not require the error to be independent of <span class="math inline">\(X\)</span> nor have a known distribution. The key insight is that quantiles, not means, are nonparametrically identified from a censored distribution.</p>
<p>A limitation with Powell’s result is that the function <span class="math inline">\(q_{\tau}(x)\)</span> is only identifed on sub-populations for which censoring does not exceed <span class="math inline">\(\tau %\)</span>.</p>
<p>To illustrate, Figure 27.3(a) displays the conditional quantile functions <span class="math inline">\(q_{\tau}(x)\)</span> for <span class="math inline">\(\tau=0.3,0.5,0.7\)</span>, and <span class="math inline">\(0.9\)</span> for the conditional distribution <span class="math inline">\(Y^{*} \mid X \sim N\left(\sqrt{x}-\frac{3}{2}, 2+x\right)\)</span>. The portions above zero (which are identified from the censored distribution) are plotted with solid lines. The portions below zero (which are not identified from the censored distribution) are plotted with dashed lines. We can see that in this example the quantile function <span class="math inline">\(q_{.9}(x)\)</span> is identified for all values of <span class="math inline">\(x\)</span>, the quantile function <span class="math inline">\(q_{.3}(x)\)</span> is not identified for any values of <span class="math inline">\(x\)</span>, and the quantile functions <span class="math inline">\(q_{.7}(x)\)</span> and <span class="math inline">\(q_{.5}(x)\)</span> are identified for a subset of values of <span class="math inline">\(x\)</span>. The explanation is that for any fixed value of <span class="math inline">\(X=x\)</span> we only observe the censored distribution <span class="math inline">\(Y\)</span> and so only observe the quantiles above the censoring point. There is no nonparametric information about the distribution of <span class="math inline">\(Y^{*}\)</span> below the censoring point.</p>
<p><img src="images//2022_10_23_36a5c875f975c7792a09g-08.jpg" class="img-fluid"></p>
<ol type="a">
<li><span class="math inline">\(Y^{*} \mid X \sim \mathrm{N}\left(\sqrt{X}-\frac{3}{2}, 2+X\right)\)</span></li>
</ol>
<p><img src="images//2022_10_23_36a5c875f975c7792a09g-08(1).jpg" class="img-fluid"></p>
<ol start="2" type="a">
<li>Effect of Income on Transfers</li>
</ol>
<p>Figure 27.3: Censored Regression Quantiles</p>
</section>
<section id="clad-and-cqr-estimators" class="level2" data-number="25.7">
<h2 data-number="25.7" class="anchored" data-anchor-id="clad-and-cqr-estimators"><span class="header-section-number">25.7</span> CLAD and CQR Estimators</h2>
<p>Powell <span class="math inline">\((1984,1986)\)</span> applied the quantile identification strategy described in the previous section to develop straightforward censored regression estimators.</p>
<p>The model in Powell (1984) is censored median regression:</p>
<p><span class="math display">\[
\begin{aligned}
Y^{*} &amp;=X^{\prime} \beta+e \\
\operatorname{med}[e \mid X] &amp;=0 \\
Y &amp;=\max \left(Y^{*}, 0\right) .
\end{aligned}
\]</span></p>
<p>In this model <span class="math inline">\(Y^{*}\)</span> is latent with <span class="math inline">\(\operatorname{med}\left[Y^{*} \mid X\right]=X^{\prime} \beta\)</span> and <span class="math inline">\(Y\)</span> is censored at zero. As described in the previous section the equivariance property of the median implies that the conditional median of <span class="math inline">\(Y\)</span> equals</p>
<p><span class="math display">\[
\operatorname{med}[Y \mid X]=\max \left(X^{\prime} \beta, 0\right) .
\]</span></p>
<p>This is a parametric but nonlinear median regression model for <span class="math inline">\(Y\)</span>.</p>
<p>The appropriate estimator for median regression is least absolute deviations (LAD). The censored least absolute deviations (CLAD) criterion is</p>
<p><span class="math display">\[
M_{n}(\beta)=\frac{1}{n} \sum_{i=1}^{n}\left|Y_{i}-\max \left(X_{i}^{\prime} \beta, 0\right)\right| .
\]</span></p>
<p>The CLAD estimator minimizes <span class="math inline">\(M_{n}(\beta)\)</span></p>
<p><span class="math display">\[
\widehat{\beta}_{\mathrm{CLAD}}=\underset{\beta}{\operatorname{argmin}} M_{n}(\beta) .
\]</span></p>
<p>The CLAD criterion <span class="math inline">\(M_{n}(\beta)\)</span> has similar properties as LAD criterion, namely that it is continuous, faceted, and has discontinuous first derivatives. An important difference, however, is that <span class="math inline">\(M_{n}(\beta)\)</span> is not globally convex, so minimization algorithms may converge to a local rather than a global minimum. Powell (1986) extended CLAD to censored quantile regression (CQR). The model is</p>
<p><span class="math display">\[
\begin{aligned}
Y^{*} &amp;=X^{\prime} \beta+e \\
\mathbb{Q}_{\tau}[e \mid X] &amp;=0 \\
Y &amp;=\max \left(Y^{*}, 0\right)
\end{aligned}
\]</span></p>
<p>for <span class="math inline">\(\tau \in(0,1)\)</span>. The equivariance property implies that the conditional quantile function for <span class="math inline">\(Y\)</span> is</p>
<p><span class="math display">\[
\mathbb{Q}_{\tau}[Y \mid X]=\max \left(X^{\prime} \beta, 0\right) .
\]</span></p>
<p>The CQR criterion is</p>
<p><span class="math display">\[
M_{n}(\beta ; \tau)=\frac{1}{n} \sum_{i=1}^{n} \rho_{\tau}\left(Y_{i}-\max \left(X_{i}^{\prime} \beta, 0\right)\right)
\]</span></p>
<p>where <span class="math inline">\(\rho_{\tau}(u)\)</span> is the check function (24.10). The CQR estimator minimizes this criterion</p>
<p><span class="math display">\[
\widehat{\beta}_{\mathrm{CQR}}(\tau)=\underset{\beta}{\operatorname{argmin}} M_{n}(\beta ; \tau) .
\]</span></p>
<p>As for CLAD, the criterion is not globally concave so numerical minimization is not guarenteed to converge to the global minimum.</p>
<p>Powell <span class="math inline">\((1984,1986)\)</span> shows that the CLAD and CQR estimators are asymptotically normal by similar arguments as for quantile regression. An important technical difference with quantile regression is that the CLAD and CQR estimators require stronger conditions for identification. As we discussed in the previous section the quantile function <span class="math inline">\(X^{\prime} \beta\)</span> is only identified for regions where it is positive. This means that we require a positive fraction of the population to satisfy <span class="math inline">\(X^{\prime} \beta&gt;0\)</span>. Furthermore, the relevant design matrix (24.18) is defined on this sub-population, and must be full rank for conventional inference. Essentially, there must be sufficient variation in the regressors over the region of the sample space where there is no censoring.</p>
<p>CLAD can be estimated in Stata with the add-on package clad. In R, CLAD and CQR can be estimated with the crq command in the package quantreg.</p>
</section>
<section id="illustrating-censored-regression" class="level2" data-number="25.8">
<h2 data-number="25.8" class="anchored" data-anchor-id="illustrating-censored-regression"><span class="header-section-number">25.8</span> Illustrating Censored Regression</h2>
<p>To illustrate the methods we revisit of the applications reported in Section 20.6, where we used a linear spline to estimate the impact of income on non-governmental transfers for a sample of 8684 Phillipino households. The least squares estimates indicated a sharp discontinuity in the conditional mean around 20,000 pesos. The dependent variable is the sum of transfers received domestically, from abroad, and in-kind, less gifts. Each of these four sub-variables is non-negative. If we apply the model to any of these sub-variables there is substantial censoring. To illustrate, we set the dependent variable to equal the sum of transfers received domestically, from abroad, and in-kind, for which the censoring proportion is <span class="math inline">\(18 %\)</span>. This proportion is sufficiently high that we should expect significant censoring bias if censoring is ignored.</p>
<p>We estimate the same model as reported in Section <span class="math inline">\(20.6\)</span> and displayed in Figure 20.2(b), which is a linear spline in income with 5 knots and 15 additional control regressors. We estimated the equation using four methods: (a) least squares; (b) Tobit regression; (c) LAD; (d) CLAD. We display the estimated regression as a function of income (with remaining regressors set at sample means) in Figure 27.3(b).</p>
<p>The basic insight - that the regression has a slope close to <span class="math inline">\(-1\)</span> for low income levels and is flat for high income levels with a sharp discontinuity at an income level of 20,000 pesos - is remarkably robust across the four estimates. What is noticably different, however, is the level of the regression function. The least squares estimate is several thousand pesos above the others. The fact that the LAD and CLAD estimates have a meaningfully different level should not be surprising. The dependent variable is highly skewed, so the mean and median are quite different (the unconditional mean and median are 7700 and 1200 , respectively). This implies a level shift of the regression function. This does not explain, however, why the Tobit estimate also is substantially shifted down. Instead, this can be explained by censoring bias. Since the regression function is negatively sloped the censoring probability is increasing in income, so the bias of the least squares estimator is positive and increasing in the income level. The LAD and CLAD estimates are quite similar even though the LAD estimates do not account for censoring. Overall, the CLAD estimates are the preferred choice because they are robust to both censoring and non-normality.</p>
</section>
<section id="sample-selection-bias" class="level2" data-number="25.9">
<h2 data-number="25.9" class="anchored" data-anchor-id="sample-selection-bias"><span class="header-section-number">25.9</span> Sample Selection Bias</h2>
<p>While econometric models typically assume random sampling, actual observations are typically gathered non-randomly. This can induce estimation bias if selection (presence in the sample) is endogeneous. The following are examples of potential sample selection.</p>
<ol type="1">
<li><p>Wage regression. Wages are only observed for individuals who have wage income, which means that the individual is a member of the labor force and has a wage-paying job. The decision to work may be endogenously related to the person’s observed and unobserved characteristics.</p></li>
<li><p>Program evaluation. The goal is to measure the impact of a program such as workforce training through a pilot program. Endogenous selection arises when individuals volunteer to participate (rather than being randomly assigned). Individuals who volunteer for a training program may have abilities which are correlated with outcomes.</p></li>
<li><p>Surveys. While a survey may be randomly distributed the act of completing the survey is nonrandom. Most surveys have low response rates. Endogenous selection arises when the decision to complete and return the survey is correlated with the survey responses.</p></li>
<li><p>Ratings. We are routinely asked to rate products, services, and experiences. Most people do not respond to the request. Endogenous selection arises when the decision to rate the product is correlated with the response.</p></li>
</ol>
<p>To understand the effect of sample selection it is useful to view sampling as a two-stage process. In the first stage the random variables <span class="math inline">\((Y, X)\)</span> are drawn. In the second stage the pair is either selected into the sample <span class="math inline">\((S=1)\)</span> or unobserved <span class="math inline">\((S=0)\)</span>. The sample then consists of the pairs <span class="math inline">\((Y, X)\)</span> for which <span class="math inline">\(S=1\)</span>. Suppose that the variables satisfy the latent regression model <span class="math inline">\(Y=X^{\prime} \beta+e\)</span> with <span class="math inline">\(\mathbb{E}[e \mid X]=0\)</span>. Then the CEF in the observed (selected) sample is</p>
<p><span class="math display">\[
\mathbb{E}[Y \mid X, S=1]=X^{\prime} \beta+\mathbb{E}[e \mid X, S=1] .
\]</span></p>
<p>Selection bias occurs when the second term is non-zero. To understand this further suppose that selection can be modelled as <span class="math inline">\(S=\mathbb{1}\left\{X^{\prime} \gamma+u&gt;0\right\}\)</span> for some error <span class="math inline">\(u\)</span>. This is consistent with a latent utility framework where <span class="math inline">\(X^{\prime} \gamma+u\)</span> is the latent utility of participation. Given this framework we can write the CEF of <span class="math inline">\(Y\)</span> in the selected sample as</p>
<p><span class="math display">\[
\mathbb{E}[Y \mid X, S=1]=X^{\prime} \beta+\mathbb{E}\left[e \mid u&gt;-X^{\prime} \gamma\right] .
\]</span></p>
<p>Let <span class="math inline">\(e=\rho u+\varepsilon\)</span> be the projection of <span class="math inline">\(e\)</span> on <span class="math inline">\(u\)</span>. Suppose that the errors are independent of <span class="math inline">\(X\)</span>, and <span class="math inline">\(u\)</span> and <span class="math inline">\(\varepsilon\)</span> are mutually independent. Then the above expression equals</p>
<p><span class="math display">\[
\mathbb{E}[Y \mid X, S=1]=X^{\prime} \beta+\rho \mathbb{E}\left[u \mid u&gt;-X^{\prime} \gamma\right]=X^{\prime} \beta+\rho g\left(X^{\prime} \gamma\right)
\]</span></p>
<p>for some function <span class="math inline">\(g(u)\)</span>. When <span class="math inline">\(u \sim \mathrm{N}(0,1), g(u)=\phi(u) / \Phi(u)=\lambda(u)\)</span> (see Exercise 27.7) so the expression equals</p>
<p><span class="math display">\[
\mathbb{E}[Y \mid X, S=1]=X^{\prime} \beta+\rho \lambda\left(X^{\prime} \gamma\right) .
\]</span></p>
<p>This is the same as (27.3) in the special case <span class="math inline">\(\rho=\sigma\)</span> and <span class="math inline">\(\gamma=\beta / \sigma\)</span>. This, as shown in Figure 27.2(a), deviates from the latent <span class="math inline">\(\mathrm{CEF} X^{\prime} \beta\)</span></p>
<p>One way to interpret this effect is that the regression function (27.6) contains two components: <span class="math inline">\(X^{\prime} \beta\)</span> and <span class="math inline">\(\rho \lambda\left(X^{\prime} \gamma\right)\)</span>. A linear regression on <span class="math inline">\(X\)</span> omits the second term and thus inherits omitted variables bias as <span class="math inline">\(X\)</span> and <span class="math inline">\(\lambda\left(X^{\prime} \gamma\right)\)</span> are correlated. The extent of omitted variables bias depends on the magnitude of <span class="math inline">\(\rho\)</span> which is the coefficient from the projection of <span class="math inline">\(e\)</span> on <span class="math inline">\(u\)</span>. When the errors <span class="math inline">\(e\)</span> and <span class="math inline">\(u\)</span> are independent (when selection is exogenous) then <span class="math inline">\(\rho=0\)</span> and (27.6) simplifies to <span class="math inline">\(X^{\prime} \beta\)</span> and there is no omitted term. Thus sample selection bias arises if (and only if) selection is correlated with the equation error.</p>
<p>Furthermore, the omitted selection term <span class="math inline">\(\lambda\left(X^{\prime} \gamma\right)\)</span> only impacts estimated marginal effects if the slope coefficients <span class="math inline">\(\gamma\)</span> are non-zero. In contrast suppose that <span class="math inline">\(X^{\prime} \gamma=\gamma_{0}\)</span>, a constant. Then (27.6) equals <span class="math inline">\(\mathbb{E}[Y \mid X, S=1]=\)</span> <span class="math inline">\(X^{\prime} \beta+\rho \lambda\left(\gamma_{0}\right)\)</span> so the impact of selection is an intercept shift. If our focus is on marginal effects sample selection bias only arises when the selection equation has non-trivial dependence on the regressors <span class="math inline">\(X\)</span>.</p>
<p>In Figure 27.2(a) we saw that censoring attenuates (flattens) the regression function. While the selection CEF (27.6) takes a similar form it is broader and can have a different impact. In contrast to the censoring case, selection can both steepen as well as flatten the regression function. In general it is difficult to predict the effect of selection on regression functions.</p>
<p>As we have shown, endogenous selection changes the conditional expectation. If samples are generated by endogenous selection then estimation will be biased for the parameters of interest. Without information on the selection process there is little that can be done to “correct” the bias other than to be aware of its presence. In the next section we discuss one approach which corrects for sample selection bias when we have information on the selection process.</p>
</section>
<section id="heckmans-model" class="level2" data-number="25.10">
<h2 data-number="25.10" class="anchored" data-anchor-id="heckmans-model"><span class="header-section-number">25.10</span> Heckman’s Model</h2>
<p>Heckman (1979) showed that sample selection bias can be corrected if we have a sample which includes the non-selected observations. Suppose that the observations <span class="math inline">\(\left\{Y_{i}, X_{i}, Z_{i}\right\}\)</span> are a random sample where <span class="math inline">\(Y\)</span> is a selected variable (such as wage, which is only observed if a person has wage income). Heckman’s approach is to build a joint model of the full sample (not just the selected sample) and use this to estimate the model parameters.</p>
<p>Heckman’s model is</p>
<p><span class="math display">\[
\begin{aligned}
Y^{*} &amp;=X^{\prime} \beta+e \\
S^{*} &amp;=Z^{\prime} \gamma+u \\
S &amp;=\mathbb{1}\left\{S^{*}&gt;0\right\} \\
Y &amp;=\left\{\begin{array}{cc}
Y^{*} &amp; \text { if } S=1 \\
\text { missing } &amp; \text { if } S=0
\end{array}\right.
\end{aligned}
\]</span></p>
<p>with</p>
<p><span class="math display">\[
\left(\begin{array}{c}
e \\
u
\end{array}\right) \sim \mathrm{N}\left(0,\left(\begin{array}{cc}
\sigma^{2} &amp; \sigma_{21} \\
\sigma_{21} &amp; 1
\end{array}\right)\right)
\]</span></p>
<p>The model specifies that the latent variables <span class="math inline">\(Y^{*}\)</span> and <span class="math inline">\(S^{*}\)</span> are linear in regressors <span class="math inline">\(X\)</span> and <span class="math inline">\(Z\)</span> with structural errors <span class="math inline">\(e\)</span> and <span class="math inline">\(u\)</span>. The variable <span class="math inline">\(S\)</span> indicates selection and follows a probit equation. The variable <span class="math inline">\(Y\)</span> equals the latent variable <span class="math inline">\(Y^{*}\)</span> if selected <span class="math inline">\((S=1)\)</span> and otherwise is missing. The model specifies that the errors are jointly normal with covariance <span class="math inline">\(\sigma_{21}\)</span>. The variance of <span class="math inline">\(u\)</span> is not identified so is normalized to equal 1 .</p>
<p>In Heckman’s classic example, <span class="math inline">\(Y^{*}\)</span> is the wage (or log(wage)) an individual would receive if they were employed, <span class="math inline">\(S\)</span> is employment status, and <span class="math inline">\(Y\)</span> is observed wage. The coefficients <span class="math inline">\(\beta\)</span> are those of the wage regression; the coefficients <span class="math inline">\(\gamma\)</span> are those which determine employment status. The error <span class="math inline">\(e\)</span> is unobserved ability and other unobserved factors which determine an individual’s wages; the error <span class="math inline">\(u\)</span> is the unobserved factors which determine employment status; and the two are likely to be correlated.</p>
<p>Based on the same calculations as discussed in the previous section, the CEF of <span class="math inline">\(Y\)</span> in the selected sample is</p>
<p><span class="math display">\[
\mathbb{E}[Y \mid X, Z, S=1]=X^{\prime} \beta+\sigma_{21} \lambda\left(Z^{\prime} \gamma\right)
\]</span></p>
<p>where <span class="math inline">\(\lambda(x)\)</span> is the inverse Mills ratio.</p>
<p>Heckman proposed a two-step estimator of the coefficients. The insight is that the coefficient <span class="math inline">\(\gamma\)</span> is identified by the probit regression of <span class="math inline">\(S\)</span> on <span class="math inline">\(Z\)</span>. Given <span class="math inline">\(\gamma\)</span> the coefficients <span class="math inline">\(\beta\)</span> and <span class="math inline">\(\sigma_{21}\)</span> are identified by least squares regression of <span class="math inline">\(Y\)</span> on <span class="math inline">\(\left(X, \lambda\left(Z^{\prime} \gamma\right)\right)\)</span> using the selected sample. The steps are as follows.</p>
<ol type="1">
<li><p>Construct (if necessary) the binary variable <span class="math inline">\(S\)</span> from the observed series <span class="math inline">\(Y\)</span>.</p></li>
<li><p>Estimate the coefficient <span class="math inline">\(\widehat{\gamma}\)</span> by probit regression of <span class="math inline">\(S\)</span> on <span class="math inline">\(Z\)</span>.</p></li>
<li><p>Construct the variables <span class="math inline">\(\widehat{\lambda}_{i}=\lambda\left(Z_{i}^{\prime} \widehat{\gamma}\right)\)</span>.</p></li>
<li><p>Estimate the coefficients <span class="math inline">\(\left(\widehat{\beta}, \widehat{\sigma}_{21}\right)\)</span> by least-squares regression of <span class="math inline">\(Y_{i}\)</span> on <span class="math inline">\(\left(X_{i}, \widehat{\lambda}_{i}\right)\)</span> using the sub-sample with <span class="math inline">\(S_{i}=1\)</span>.</p></li>
</ol>
<p>Heckman showed that the estimator <span class="math inline">\(\widehat{\beta}\)</span> is consistent and asymptotically normal. The variable <span class="math inline">\(\widehat{\lambda}_{i}\)</span> is a generated regressor (see Section 12.26) which affects covariance matrix estimation. The method is sometimes called “Heckit” as it is an analog of probit, logit, and Tobit regression.</p>
<p>As a by-product we obtain an estimator of the covariance <span class="math inline">\(\sigma_{21}\)</span>. This parameter indicates the magnitude of sample selection endogeneity. If selection is exogenous then <span class="math inline">\(\sigma_{21}=0\)</span>. The null hypothesis of exogenous selection can be tested by examining the t-statistic for <span class="math inline">\(\widehat{\sigma}_{21}\)</span>.</p>
<p>An alternative to two-step estimation is joint maximum likelihood. The joint density of <span class="math inline">\(S\)</span> and <span class="math inline">\(Y\)</span> is</p>
<p><span class="math display">\[
f(s, y \mid x, z)=\mathbb{P}[S=0 \mid x, z]^{1-s} f(y, S=1 \mid x, z)^{s} \text {. }
\]</span></p>
<p>The selection probability is <span class="math inline">\(\mathbb{P}[S=0 \mid x, z]=1-\Phi\left(z^{\prime} \gamma\right)\)</span>. The conditional density component is</p>
<p><span class="math display">\[
\begin{aligned}
f(y, S=1, \mid x, z) &amp;=\int_{0}^{\infty} f\left(y, s^{*} \mid x, z\right) d s^{*} \\
&amp;=\int_{0}^{\infty} f\left(s^{*} \mid y, x, z\right) f(y \mid x, z) d s^{*} \\
&amp;=\left(1-F\left(s^{*} \mid y, x, z\right)\right) f(y \mid x, z) .
\end{aligned}
\]</span></p>
<p>The first equality holds because <span class="math inline">\(S=1\)</span> is the same as <span class="math inline">\(S^{*}&gt;0\)</span>. The second factors the joint density into the product of the conditional of <span class="math inline">\(S^{*}\)</span> given <span class="math inline">\(Y\)</span> and the marginal of <span class="math inline">\(Y\)</span>. The marginal density of <span class="math inline">\(Y\)</span> is <span class="math inline">\(\sigma^{-1} \phi\left(\left(y-x^{\prime} \beta\right) / \sigma\right)\)</span>. The conditional distribution of <span class="math inline">\(S^{*}\)</span> given <span class="math inline">\(Y\)</span> is <span class="math inline">\(\mathrm{N}\left(Z^{\prime} \gamma+\frac{\sigma_{21}}{\sigma^{2}}\left(Y-X^{\prime} \beta\right), 1-\frac{\sigma_{21}}{\sigma^{2}}\right)\)</span>. Making these substitutions we obtain the joint mixed density</p>
<p><span class="math display">\[
f(s, y \mid x, z)=\left(1-\Phi\left(z^{\prime} \gamma\right)\right)^{1-s}\left[\Phi\left(\frac{z^{\prime} \gamma+\frac{\sigma_{21}}{\sigma^{2}}\left(y-x^{\prime} \beta\right)}{\sqrt{1-\frac{\sigma_{21}}{\sigma^{2}}}}\right) \frac{1}{\sigma} \phi\left(\frac{y-x^{\prime} \beta}{\sigma}\right)\right]^{s} .
\]</span></p>
<p>Evaluated at the observations we obtain the log-likelihood function</p>
<p><span class="math display">\[
\ell_{n}\left(\beta, \gamma, \sigma^{2}, \sigma_{21}\right)=\sum_{S_{i}=0} \log \left(1-\Phi\left(Z_{i}^{\prime} \gamma\right)\right)+\sum_{S_{i}=1}\left[\log \Phi\left(\frac{Z_{i}^{\prime} \gamma+\frac{\sigma_{21}}{\sigma^{2}}\left(Y_{i}-X_{i}^{\prime} \beta\right)}{\sqrt{1-\frac{\sigma_{21}}{\sigma^{2}}}}\right)-\frac{1}{2} \log \left(2 \pi \sigma^{2}\right)-\frac{1}{2 \sigma^{2}}\left(Y_{i}-X_{i}^{\prime} \beta\right)^{2}\right]
\]</span></p>
<p>The maximum likelihood estimator <span class="math inline">\(\left(\widehat{\beta}, \widehat{\gamma}, \widehat{\sigma}^{2}, \widehat{\sigma}_{21}\right)\)</span> maximizes the log-likelihood.</p>
<p>The MLE is the preferred estimation method for final reporting. It can be computationally demanding in some applications, however, so the two-step estimator can be useful for preliminary analysis.</p>
<p>In Stata the two-step estimator and joint MLE can be obtained with the heckman command.</p>
</section>
<section id="nonparametric-selection" class="level2" data-number="25.11">
<h2 data-number="25.11" class="anchored" data-anchor-id="nonparametric-selection"><span class="header-section-number">25.11</span> Nonparametric Selection</h2>
<p>A nonparametric selection model is</p>
<p><span class="math display">\[
\begin{aligned}
Y^{*} &amp;=m(X)+e \\
S^{*} &amp;=g(Z)+u \\
S &amp;=\mathbb{1}\left\{S^{*}&gt;0\right\} \\
Y &amp;=\left\{\begin{array}{cc}
Y^{*} &amp; \text { if } S=1 \\
\text { missing } &amp; \text { if } S=0
\end{array}\right.
\end{aligned}
\]</span></p>
<p>where the distribution of <span class="math inline">\((e, u)\)</span> is unknown. For simplicity we assume that <span class="math inline">\((e, u)\)</span> are independent of <span class="math inline">\((X, Z)\)</span>.</p>
<p>Selection occurs if <span class="math inline">\(u&gt;-g(Z)\)</span>. This is unaffected by monotonically increasing transformations. Therefore the distribution of <span class="math inline">\(u\)</span> is not separately identified from the function <span class="math inline">\(g(Z)\)</span>. Consequently we can normalize the distribution of <span class="math inline">\(u\)</span> to a convenient form. Here we use the normal distribution: <span class="math inline">\(u \sim \Phi(x)\)</span>.</p>
<p>Since the functions <span class="math inline">\(m(X)\)</span> and <span class="math inline">\(g(Z)\)</span> are nonparametric we can use series methods to approximate them by linear models of the form <span class="math inline">\(m(X)=X^{\prime} \beta\)</span> and <span class="math inline">\(g(Z)=Z^{\prime} \gamma\)</span> after suitable variable transformation. We will use this latter notation to link the models to estimation methods.</p>
<p>The conditional probability of selection is</p>
<p><span class="math display">\[
p(Z)=\mathbb{P}[S=1 \mid Z]=\mathbb{P}\left[u&gt;-Z^{\prime} \gamma \mid Z\right]=\Phi\left(Z^{\prime} \gamma\right) .
\]</span></p>
<p>The probability <span class="math inline">\(p(Z)\)</span> is known as the propensity score; it is nonparametrically identified from the joint distribution of <span class="math inline">\((S, Z)\)</span>, so the function <span class="math inline">\(g(Z)=Z^{\prime} \gamma\)</span> is identified. The coefficient <span class="math inline">\(\gamma\)</span> and propensity score can be estimated by binary choice methods, for example by a series probit regression.</p>
<p>The CEF of <span class="math inline">\(Y\)</span> given selection is</p>
<p><span class="math display">\[
\mathbb{E}[Y \mid X, Z, S=1]=X^{\prime} \beta+h_{1}\left(Z^{\prime} \gamma\right)
\]</span></p>
<p>where <span class="math inline">\(h_{1}(x)=\mathbb{E}[e \mid u&gt;-x]\)</span>. In general <span class="math inline">\(h_{1}(x)\)</span> can take a range of possible shapes. When <span class="math inline">\((e, u)\)</span> are jointly normal with covariance <span class="math inline">\(\sigma_{21}\)</span> then <span class="math inline">\(h_{1}(x)=\sigma_{21} \lambda(x)\)</span> where <span class="math inline">\(\lambda(x)=\phi(x) / \Phi(x)\)</span> is the inverse Mills ratio. There are two alternative representations of the CEF which are potentially useful. Since <span class="math inline">\(g(Z)=\Phi^{-1}(p(Z))\)</span> we have the representation</p>
<p><span class="math display">\[
\left.\mathbb{E}[Y \mid X, Z, S=1]=X^{\prime} \beta+h_{2}(p(Z))\right)
\]</span></p>
<p>where <span class="math inline">\(h_{2}(x)=h_{1}\left(\Phi^{-1}(x)\right)\)</span>. Also, because <span class="math inline">\(\lambda(x)\)</span> is invertible we have the representation</p>
<p><span class="math display">\[
\mathbb{E}[Y \mid X, Z, S=1]=X^{\prime} \beta+h_{3}\left(\lambda\left(Z^{\prime} \gamma\right)\right)
\]</span></p>
<p>where <span class="math inline">\(h_{3}(x)=h_{1}\left(\lambda^{-1}(x)\right)\)</span>.</p>
<p>The three equations (27.8)-(27.10) suggest three two-step approaches to nonparametric estimation which we now describe. Each is based on a first-step binary choice estimator <span class="math inline">\(\widehat{\gamma}\)</span> of <span class="math inline">\(\gamma\)</span>.</p>
<p>Equation (27.8) suggests a regression of <span class="math inline">\(Y\)</span> on <span class="math inline">\(X\)</span> and a series expansion in <span class="math inline">\(Z^{\prime} \widehat{\gamma}\)</span>, for example a loworder polynomial in <span class="math inline">\(Z^{\prime} \widehat{\gamma}\)</span></p>
<p>Equation (27.9) suggests a regression of <span class="math inline">\(Y\)</span> on <span class="math inline">\(X\)</span> and a series expansion in the propensity score <span class="math inline">\(\widehat{p}=\)</span> <span class="math inline">\(\Phi\left(Z^{\prime} \widehat{\gamma}\right)\)</span>, for example a low-order polynomial in <span class="math inline">\(\hat{p}\)</span>.</p>
<p>Equation (27.10) suggests a regression of <span class="math inline">\(Y\)</span> on <span class="math inline">\(X\)</span> and a series expansion in <span class="math inline">\(\hat{\lambda}=\lambda\left(Z^{\prime} \hat{\gamma}\right)\)</span>, for example a low-order polynomial in <span class="math inline">\(\hat{\lambda}\)</span>.</p>
<p>The advantage of expansions based on (27.10) is that it will be first-order accurate in the leading case of the normal distribution. This means that for distributions close to the normal, series expansions will be accurate even with a small number of terms. The advantage of expansions based on (27.9) is interpretability: The regression is expressed as a function of the propensity score.</p>
<p>Das, Newey, and Vella (2003) provide a detailed asymptotic theory for this class of estimators focusing on those based on (27.9). They provide conditions under which the models are identified, the estimators consistent, and asymptotically normally distributed.</p>
<p>These nonparametric selection estimators are two-step estimators with generated regressors (see Section 12.26). Therefore conventional covariance matrix estimators and standard errors are inconsistent. Asymptotically valid covariance matrix estimators can be constructed using GMM. An alternative is to use bootstrap methods. The latter should be implemented as an explicit two-step estimator so that the first-step estimation is treated by the bootstrap distribution.</p>
<p>A standard recommendation is that the regressors <span class="math inline">\(Z\)</span> in the selection equation should include at least one relevant variable which is a valid exclusion from the regressors <span class="math inline">\(X\)</span> in the main equation. The reason is that otherwise the series expansions for <span class="math inline">\(m(x)\)</span> and <span class="math inline">\(h\left(Z^{\prime} \gamma\right)\)</span> can be highly collinear and not separately identified. This insight applies to the parametric case as well. One difficulty is that in applications it may be challenging to identify variables which affect selection <span class="math inline">\(S^{*}\)</span> but not the outcome <span class="math inline">\(Y^{*}\)</span>.</p>
</section>
<section id="panel-data" class="level2" data-number="25.12">
<h2 data-number="25.12" class="anchored" data-anchor-id="panel-data"><span class="header-section-number">25.12</span> Panel Data</h2>
<p>A panel censored regression (panel Tobit) equation is</p>
<p><span class="math display">\[
\begin{aligned}
&amp;Y_{i t}^{*}=X_{i t}^{\prime} \beta+u_{i}+e_{i t} \\
&amp;Y_{i t}=\max \left(Y_{i t}^{*}, 0\right) .
\end{aligned}
\]</span></p>
<p>The individual effect <span class="math inline">\(u_{i}\)</span> can be treated as a random effect (uncorrelated with the errors) or a fixed effect (unstructured correlation).</p>
<p>A random effects estimator can be derived under the assumption of joint normality of the errors. This is implemented in the Stata command xttobit. The advantage is that the procedure is simple to implement. The disadvantages are those typically associated with random effects estimators.</p>
<p>A fixed effects estimator was developed by Honoré (1992). His key insight is the following, which we illustrate assuming <span class="math inline">\(T=2\)</span>. If the errors <span class="math inline">\(\left(e_{i 1}, e_{i 2}\right)\)</span> are independent of <span class="math inline">\(\left(X_{i 1}, X_{i 2}, u_{i}\right)\)</span> then the distribution of <span class="math inline">\(\left(Y_{i 1}^{*}, Y_{i 2}^{*}\right)\)</span> conditional on <span class="math inline">\(\left(X_{i 1}, X_{i 2}\right)\)</span> is symmetric about the 45 degree line through the point <span class="math inline">\(\left(\Delta X^{\prime} \beta, 0\right)\)</span> in <span class="math inline">\(\left(Y_{1}, Y_{2}\right)\)</span> space. This distribution does not depend on the fixed effect <span class="math inline">\(u_{i}\)</span>. From this symmetry and the censoring rules Honoré derived moment conditions which identify the coefficients <span class="math inline">\(\beta\)</span> and allow estimation by GMM. Honoré (1992) provides a complete asymptotic distribution theory. Honoré has provided a Stata command Pantob which implements his estimator and is available on his website. honore/stata/. A panel sample selection model is</p>
<p><span class="math display">\[
\begin{aligned}
Y_{i t}^{*} &amp;=X_{i t}^{\prime} \beta+u_{i}+e_{i t} \\
S_{i t}^{*} &amp;=Z_{i t}^{\prime} \gamma+\eta_{i}+v_{i t} \\
S_{i t} &amp;=\mathbb{1}\left\{S_{i t}^{*}&gt;0\right\} \\
Y_{i t} &amp;=\left\{\begin{array}{cc}
Y_{i t}^{*} &amp; \text { if } S_{i t}=1 \\
\text { missing } &amp; \text { if } S_{i t}=0
\end{array}\right.
\end{aligned}
\]</span></p>
<p>A method to estimated this model is presented in Kyriazidou (1997). Again for exposition we focus on the <span class="math inline">\(T=2\)</span> case. Her estimator is motivated by the observation that <span class="math inline">\(\beta\)</span> could be consistently estimated by least squares applied to the sub-sample where <span class="math inline">\(S_{i 1}=S_{i 2}=1\)</span> (both observations are selected) and <span class="math inline">\(Z_{i 1}^{\prime} \gamma=Z_{i 2}^{\prime} \gamma\)</span> (both observations have same probability of selection). The parameter <span class="math inline">\(\gamma\)</span> is identified up to scale by the selection equation so can be estimated as <span class="math inline">\(\widehat{\gamma}\)</span> by the methods described in Section <span class="math inline">\(25.13\)</span> (e.g.&nbsp;Chamberlain (1980, 1984)). Given <span class="math inline">\(\hat{\gamma}\)</span> we estimate <span class="math inline">\(\beta\)</span> by kernel-weighted least squares on the sub-sample with <span class="math inline">\(S_{i 1}=S_{i 2}=1\)</span>, with kernel weights depending on <span class="math inline">\(\left(Z_{i 1}-Z_{i 2}\right)^{\prime} \widehat{\gamma}\)</span>. Kyriazidou (1997) provides a complete distribution theory.</p>
</section>
<section id="exercises" class="level2" data-number="25.13">
<h2 data-number="25.13" class="anchored" data-anchor-id="exercises"><span class="header-section-number">25.13</span> Exercises</h2>
<p>Exercise 27.1 Derive (27.2) and (27.3). Hint: Use Theorems <span class="math inline">\(5.7\)</span> and <span class="math inline">\(5.8\)</span> of Probability and Statistics for Economists.</p>
<p>Exercise 27.2 Take the model</p>
<p><span class="math display">\[
\begin{aligned}
Y^{*} &amp;=X^{\prime} \beta+e \\
e &amp; \sim \mathrm{N}\left(0, \sigma^{2}\right) \\
Y &amp;=\left\{\begin{array}{cc}
Y^{*} &amp; \text { if } Y^{*} \leq \tau \\
\text { missing } &amp; \text { if } Y^{*}&gt;\tau
\end{array} .\right.
\end{aligned}
\]</span></p>
<p>In this model, we say that <span class="math inline">\(Y\)</span> is capped from above. Suppose you regress <span class="math inline">\(Y\)</span> on <span class="math inline">\(X\)</span>. Is OLS consistent for <span class="math inline">\(\beta\)</span> ? Describe the nature of the effect of the mis-measured observation on the OLS estimator.</p>
<p>Exercise 27.3 Take the model</p>
<p><span class="math display">\[
\begin{aligned}
Y &amp;=X^{\prime} \beta+e \\
e &amp; \sim \mathrm{N}\left(0, \sigma^{2}\right) .
\end{aligned}
\]</span></p>
<p>Let <span class="math inline">\(\widehat{\beta}\)</span> denote the OLS estimator for <span class="math inline">\(\beta\)</span> based on an available sample.</p>
<ol type="a">
<li><p>Suppose that an observation is in the sample only if <span class="math inline">\(X_{1}&gt;0\)</span> where <span class="math inline">\(X_{1}\)</span> is an element of <span class="math inline">\(X\)</span>. Is <span class="math inline">\(\widehat{\beta}\)</span> consistent for <span class="math inline">\(\beta\)</span> ? Obtain an expression for its probability limit.</p></li>
<li><p>Suppose that an observation is in the sample only if <span class="math inline">\(Y&gt;0\)</span>. Is <span class="math inline">\(\widehat{\beta}\)</span> consistent for <span class="math inline">\(\widehat{\beta}\)</span> ? Obtain an expression for its probability limit.</p></li>
</ol>
<p>Exercise 27.4 For the censored conditional mean (27.2) propose a NLLS estimator of <span class="math inline">\((\beta, \sigma)\)</span>.</p>
<p>Exercise 27.5 For the truncated conditional mean (27.3) propose a NLLS estimator of <span class="math inline">\((\beta, \sigma)\)</span>. Exercise 27.6 A latent variable <span class="math inline">\(Y^{*}\)</span> is generated by</p>
<p><span class="math display">\[
\begin{gathered}
Y^{*}=\beta_{0}+X \beta_{1}+e \\
e \mid X \sim \mathrm{N}\left(0, \sigma^{2}(X)\right) \\
\sigma^{2}(X)=\gamma_{0}+X^{2} \gamma_{1} \\
Y=\max \left(Y^{*}, 0\right) .
\end{gathered}
\]</span></p>
<p>where <span class="math inline">\(X\)</span> is scalar. Assume <span class="math inline">\(\gamma_{0}&gt;0\)</span> and <span class="math inline">\(\gamma_{1}&gt;0\)</span>. The parameters are <span class="math inline">\(\beta, \gamma_{0}, \gamma_{1}\)</span>. Find the log-likelihood function for the conditional distribution of <span class="math inline">\(Y\)</span> given <span class="math inline">\(X\)</span>.</p>
<p>Exercise 27.7 Take the model</p>
<p><span class="math display">\[
\begin{aligned}
S &amp;=\mathbb{1}\left\{X^{\prime} \gamma+u&gt;0\right\} \\
Y &amp;=\left\{\begin{array}{cc}
X^{\prime} \beta+e &amp; \text { if } S=1 \\
\text { missing } &amp; \text { if } S=0
\end{array}\right.\\
\left(\begin{array}{c}
e \\
u
\end{array}\right) &amp; \sim \mathrm{N}\left(0,\left(\begin{array}{cc}
\sigma^{2} &amp; \sigma_{21} \\
\sigma_{21} &amp; 1
\end{array}\right)\right)
\end{aligned}
\]</span></p>
<p>Show <span class="math inline">\(\mathbb{E}[Y \mid X, S=1]=X^{\prime} \beta+\sigma_{21} \lambda\left(X^{\prime} \gamma\right)\)</span>.</p>
<p>Exercise 27.8 Show (27.7).</p>
<p>Exercise 27.9 Take the <span class="math inline">\(\mathrm{CH} \mathrm{J} 2004\)</span> dataset. The variables tinkind and income are household transfers received in-kind and household income, respectively. Divide both variables by 1000 to standardize. Create the regressor Dincome <span class="math inline">\(=(\)</span> income <span class="math inline">\(-1) \times \mathbb{1}\{\)</span> income <span class="math inline">\(&gt;1\}\)</span>.</p>
<ol type="a">
<li><p>Estimate a linear regression of tinkind on income and Dincome. Interpret the results.</p></li>
<li><p>Calculate the percentage of censored observations (the percentage for which tinkind= 0 . Do you expect censoring bias to be a problem in this example?</p></li>
<li><p>Suppose you try and fix the problem by omitting the censored observations. Estimate the regression on the subsample of observations for which tinkind <span class="math inline">\(&gt;0\)</span>.</p></li>
<li><p>Estimate a Tobit regression of of tinkind on income and Dincome.</p></li>
<li><p>Estimate the same regression using CLAD.</p></li>
<li><p>Interpret and explain the differences between your results in (a)-(e).</p></li>
</ol>
<p>Exercise 27.10 Take the cps09mar dataset and the subsample of individuals with at least 12 years of education. Create wage <span class="math inline">\(=\)</span> earnings/(hours <span class="math inline">\(\times\)</span> weeks <span class="math inline">\()\)</span> and lwage <span class="math inline">\(=\log (\)</span> wage <span class="math inline">\()\)</span>.</p>
<ol type="a">
<li><p>Estimate a linear regression of lwage on education and education <span class="math inline">\({ }^{\wedge}\)</span>. Interpret the results.</p></li>
<li><p>Suppose the wage data had been capped about <span class="math inline">\(\$ 30\)</span> /hour. Create a variable cwage which is lwage capped at 3.4. Estimate a linear regression of cwage on education and education <span class="math inline">\(\wedge\)</span> 2. How would you interpret these results if you were unaware that the dependent variable was capped?</p></li>
<li><p>Suppose you try and fix the problem by omitting the capped observations. Estimate the regression on the subsample of observations for which cwage is less than 3.4. (d) Estimate a Tobit regression of cwage on education and education^{}2 with upper censoring at <span class="math inline">\(3.4\)</span>.</p></li>
<li><p>Estimate the same regression using CLAD. You may need to impose an upper censoring of 3.3.</p></li>
<li><p>Interpret and explain the differences between your results in (a)-(e).</p></li>
</ol>
<p>Exercise 27.11 Take the DDK2011 dataset. Create a variable testscore which is totalscore standardized to have mean zero and variance one. The variable tracking is a dummy indicating that the students were tracked (separated by initial test score). The varible percentile is the student’s percentile in the initial distribution. For the following regressions cluster by school.</p>
<ol type="a">
<li><p>Estimate a linear regression of testscore on tracking, percentile, and percentile^{}2. Interpret the results.</p></li>
<li><p>Suppose the scores were censored from below. Create a variable ctest which is testscore censored at 0 . Estimate a linear regression of ctest on tracking, percentile, and percentile <span class="math inline">\({ }^{\wedge}\)</span>. How would you interpret these results if you were unaware that the dependent variable was censored?</p></li>
<li><p>Suppose you try and fix the problem by omitting the censored observations. Estimate the regression on the subsample of observations for which ctest is positive.</p></li>
<li><p>Interpret and explain the differences between your results in (a), (b), and (c).</p></li>
</ol>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./chpt26-multiple-choice.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">24</span>&nbsp; <span class="chapter-title">Multiple Choice</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./chpt28-model-selection.html" class="pagination-link">
        <span class="nav-page-text"><span class="chapter-number">26</span>&nbsp; <span class="chapter-title">Model Selection, Stein Shrinkage, and Model Averaging</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->



</body></html>
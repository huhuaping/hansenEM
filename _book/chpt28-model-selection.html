<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.253">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Hansen中高级计量体系 - 26&nbsp; Model Selection, Stein Shrinkage, and Model Averaging</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./chpt29-ML.html" rel="next">
<link href="./chpt27-censor-selection.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
    <div class="container-fluid d-flex justify-content-between">
      <h1 class="quarto-secondary-nav-title"><span class="chapter-number">26</span>&nbsp; <span class="chapter-title">Model Selection, Stein Shrinkage, and Model Averaging</span></h1>
      <button type="button" class="quarto-btn-toggle btn" aria-label="Show secondary navigation">
        <i class="bi bi-chevron-right"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Hansen中高级计量体系</a> 
        <div class="sidebar-tools-main">
    <a href="https://github.com/huhuaping/hansenEM/" title="Source Code" class="sidebar-tool px-1"><i class="bi bi-github"></i></a>
</div>
    </div>
      </div>
      <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
      </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">前言</a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./participate.html" class="sidebar-item-text sidebar-link">如何参与？</a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chpt01-intro.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chpt01-intro-chn.html" class="sidebar-item-text sidebar-link">介绍</a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./part01-reg.html" class="sidebar-item-text sidebar-link">回归</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chpt02-ce.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Conditional Expectation and Projection</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chpt02-ce-chn.html" class="sidebar-item-text sidebar-link">条件预期和预测</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chpt03-algebra.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">The Algebra of Least Squares</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chpt03-algebra-chn.html" class="sidebar-item-text sidebar-link">最小二乘代数</a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chpt04-lsr.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Least Squares Regression</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chpt05-normal-reg.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Normal Regression</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./part02-LSM.html" class="sidebar-item-text sidebar-link">大样本方法</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chpt06-review.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">A Review of Large Sample Asymptotics</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chpt07-asymptotic-ls.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Asymptotic Theory for Least Squares</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chpt08-restricted-est.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Restricted Estimation</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chpt09-hypothesit-test.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Hypothesis Testing</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chpt10-resample-method.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Resampling Methods</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./part03-MEQ.html" class="sidebar-item-text sidebar-link">多方程模型</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chpt11-multi-reg.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Multivariate Regression</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chpt12-iv.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Instrumental Variables</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chpt13-gmm.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Generalized Method of Moments</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./part04-pannel.html" class="sidebar-item-text sidebar-link">面板数据</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chpt14-time-series.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Time Series</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chpt15-multiple-time-series.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Multivariate Time Series</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chpt17-panel-data.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Panel Data</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chpt18-did.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Difference in Differences</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./part05-nonpar.html" class="sidebar-item-text sidebar-link">非参方法</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chpt19-nonparameter.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Nonparametric Regression</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chpt20-series-reg.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">Series Regression</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chpt21-rdd.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">20</span>&nbsp; <span class="chapter-title">Regression Discontinuity</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chpt22-m-est.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">21</span>&nbsp; <span class="chapter-title">M-Estimators</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./part06-nonlinear.html" class="sidebar-item-text sidebar-link">非线性方法</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-6" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chpt23-nonliear-ls.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">22</span>&nbsp; <span class="chapter-title">Nonlinear Least Squares</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chpt24-quantile-reg.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">23</span>&nbsp; <span class="chapter-title">Quantile Regression</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chpt26-multiple-choice.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">24</span>&nbsp; <span class="chapter-title">Multiple Choice</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chpt27-censor-selection.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">25</span>&nbsp; <span class="chapter-title">Censoring and Selection</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chpt28-model-selection.html" class="sidebar-item-text sidebar-link active"><span class="chapter-number">26</span>&nbsp; <span class="chapter-title">Model Selection, Stein Shrinkage, and Model Averaging</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./chpt29-ML.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">27</span>&nbsp; <span class="chapter-title">Machine Learning</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./summary.html" class="sidebar-item-text sidebar-link">Summary</a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">References</a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" aria-expanded="true">Appendices</a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" aria-expanded="true">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-7" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./a1-notation.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">A</span>&nbsp; <span class="chapter-title">附录a1</span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
    <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">目录</h2>
   
  <ul>
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction"><span class="toc-section-number">26.1</span>  Introduction</a></li>
  <li><a href="#model-selection" id="toc-model-selection" class="nav-link" data-scroll-target="#model-selection"><span class="toc-section-number">26.2</span>  Model Selection</a></li>
  <li><a href="#bayesian-information-criterion" id="toc-bayesian-information-criterion" class="nav-link" data-scroll-target="#bayesian-information-criterion"><span class="toc-section-number">26.3</span>  Bayesian Information Criterion</a></li>
  <li><a href="#akaike-information-criterion" id="toc-akaike-information-criterion" class="nav-link" data-scroll-target="#akaike-information-criterion"><span class="toc-section-number">26.4</span>  Akaike Information Criterion</a></li>
  <li><a href="#cross-validation" id="toc-cross-validation" class="nav-link" data-scroll-target="#cross-validation"><span class="toc-section-number">26.5</span>  Cross-Validation</a></li>
  <li><a href="#bayesian-information-criterion-1" id="toc-bayesian-information-criterion-1" class="nav-link" data-scroll-target="#bayesian-information-criterion-1"><span class="toc-section-number">26.6</span>  Bayesian Information Criterion</a></li>
  <li><a href="#akaike-information-criterion-1" id="toc-akaike-information-criterion-1" class="nav-link" data-scroll-target="#akaike-information-criterion-1"><span class="toc-section-number">26.7</span>  Akaike Information Criterion</a></li>
  <li><a href="#bayesian-information-criterion-2" id="toc-bayesian-information-criterion-2" class="nav-link" data-scroll-target="#bayesian-information-criterion-2"><span class="toc-section-number">26.8</span>  Bayesian Information Criterion</a></li>
  <li><a href="#akaike-information-criterion-for-regression" id="toc-akaike-information-criterion-for-regression" class="nav-link" data-scroll-target="#akaike-information-criterion-for-regression"><span class="toc-section-number">26.9</span>  Akaike Information Criterion for Regression</a></li>
  <li><a href="#akaike-information-criterion-for-likelihood" id="toc-akaike-information-criterion-for-likelihood" class="nav-link" data-scroll-target="#akaike-information-criterion-for-likelihood"><span class="toc-section-number">26.10</span>  Akaike Information Criterion for Likelihood</a></li>
  <li><a href="#mallows-criterion" id="toc-mallows-criterion" class="nav-link" data-scroll-target="#mallows-criterion"><span class="toc-section-number">26.11</span>  Mallows Criterion</a></li>
  <li><a href="#hold-out-criterion" id="toc-hold-out-criterion" class="nav-link" data-scroll-target="#hold-out-criterion"><span class="toc-section-number">26.12</span>  Hold-Out Criterion</a></li>
  <li><a href="#cross-validation-criterion" id="toc-cross-validation-criterion" class="nav-link" data-scroll-target="#cross-validation-criterion"><span class="toc-section-number">26.13</span>  Cross-Validation Criterion</a></li>
  <li><a href="#k-fold-cross-validation" id="toc-k-fold-cross-validation" class="nav-link" data-scroll-target="#k-fold-cross-validation"><span class="toc-section-number">26.14</span>  K-Fold Cross-Validation</a></li>
  <li><a href="#many-selection-criteria-are-similar" id="toc-many-selection-criteria-are-similar" class="nav-link" data-scroll-target="#many-selection-criteria-are-similar"><span class="toc-section-number">26.15</span>  Many Selection Criteria are Similar</a></li>
  <li><a href="#relation-with-likelihood-ratio-testing" id="toc-relation-with-likelihood-ratio-testing" class="nav-link" data-scroll-target="#relation-with-likelihood-ratio-testing"><span class="toc-section-number">26.16</span>  Relation with Likelihood Ratio Testing</a></li>
  <li><a href="#consistent-selection" id="toc-consistent-selection" class="nav-link" data-scroll-target="#consistent-selection"><span class="toc-section-number">26.17</span>  Consistent Selection</a></li>
  <li><a href="#asymptotic-selection-optimality" id="toc-asymptotic-selection-optimality" class="nav-link" data-scroll-target="#asymptotic-selection-optimality"><span class="toc-section-number">26.18</span>  Asymptotic Selection Optimality</a></li>
  <li><a href="#focused-information-criterion" id="toc-focused-information-criterion" class="nav-link" data-scroll-target="#focused-information-criterion"><span class="toc-section-number">26.19</span>  Focused Information Criterion</a></li>
  <li><a href="#best-subset-and-stepwise-regression" id="toc-best-subset-and-stepwise-regression" class="nav-link" data-scroll-target="#best-subset-and-stepwise-regression"><span class="toc-section-number">26.20</span>  Best Subset and Stepwise Regression</a></li>
  <li><a href="#backward-stepwise-regression" id="toc-backward-stepwise-regression" class="nav-link" data-scroll-target="#backward-stepwise-regression"><span class="toc-section-number">26.21</span>  Backward Stepwise Regression</a></li>
  <li><a href="#forward-stepwise-regression" id="toc-forward-stepwise-regression" class="nav-link" data-scroll-target="#forward-stepwise-regression"><span class="toc-section-number">26.22</span>  Forward Stepwise Regression</a></li>
  <li><a href="#the-mse-of-model-selection-estimators" id="toc-the-mse-of-model-selection-estimators" class="nav-link" data-scroll-target="#the-mse-of-model-selection-estimators"><span class="toc-section-number">26.23</span>  The MSE of Model Selection Estimators</a></li>
  <li><a href="#inference-after-model-selection" id="toc-inference-after-model-selection" class="nav-link" data-scroll-target="#inference-after-model-selection"><span class="toc-section-number">26.24</span>  Inference After Model Selection</a></li>
  <li><a href="#empirical-illustration" id="toc-empirical-illustration" class="nav-link" data-scroll-target="#empirical-illustration"><span class="toc-section-number">26.25</span>  Empirical Illustration</a></li>
  <li><a href="#shrinkage-methods" id="toc-shrinkage-methods" class="nav-link" data-scroll-target="#shrinkage-methods"><span class="toc-section-number">26.26</span>  Shrinkage Methods</a></li>
  <li><a href="#james-stein-shrinkage-estimator" id="toc-james-stein-shrinkage-estimator" class="nav-link" data-scroll-target="#james-stein-shrinkage-estimator"><span class="toc-section-number">26.27</span>  James-Stein Shrinkage Estimator</a></li>
  <li><a href="#interpretation-of-the-stein-effect" id="toc-interpretation-of-the-stein-effect" class="nav-link" data-scroll-target="#interpretation-of-the-stein-effect"><span class="toc-section-number">26.28</span>  Interpretation of the Stein Effect</a></li>
  <li><a href="#positive-part-estimator" id="toc-positive-part-estimator" class="nav-link" data-scroll-target="#positive-part-estimator"><span class="toc-section-number">26.29</span>  Positive Part Estimator</a></li>
  <li><a href="#shrinkage-towards-restrictions" id="toc-shrinkage-towards-restrictions" class="nav-link" data-scroll-target="#shrinkage-towards-restrictions"><span class="toc-section-number">26.30</span>  Shrinkage Towards Restrictions</a></li>
  <li><a href="#group-james-stein" id="toc-group-james-stein" class="nav-link" data-scroll-target="#group-james-stein"><span class="toc-section-number">26.31</span>  Group James-Stein</a></li>
  <li><a href="#empirical-illustrations" id="toc-empirical-illustrations" class="nav-link" data-scroll-target="#empirical-illustrations"><span class="toc-section-number">26.32</span>  Empirical Illustrations</a></li>
  <li><a href="#model-averaging" id="toc-model-averaging" class="nav-link" data-scroll-target="#model-averaging"><span class="toc-section-number">26.33</span>  Model Averaging</a></li>
  <li><a href="#smoothed-bic-and-aic" id="toc-smoothed-bic-and-aic" class="nav-link" data-scroll-target="#smoothed-bic-and-aic"><span class="toc-section-number">26.34</span>  Smoothed BIC and AIC</a></li>
  <li><a href="#mallows-model-averaging" id="toc-mallows-model-averaging" class="nav-link" data-scroll-target="#mallows-model-averaging"><span class="toc-section-number">26.35</span>  Mallows Model Averaging</a></li>
  <li><a href="#jackknife-cv-model-averaging" id="toc-jackknife-cv-model-averaging" class="nav-link" data-scroll-target="#jackknife-cv-model-averaging"><span class="toc-section-number">26.36</span>  Jackknife (CV) Model Averaging</a></li>
  <li><a href="#granger-ramanathan-averaging" id="toc-granger-ramanathan-averaging" class="nav-link" data-scroll-target="#granger-ramanathan-averaging"><span class="toc-section-number">26.37</span>  Granger-Ramanathan Averaging</a></li>
  <li><a href="#empirical-illustration-1" id="toc-empirical-illustration-1" class="nav-link" data-scroll-target="#empirical-illustration-1"><span class="toc-section-number">26.38</span>  Empirical Illustration</a></li>
  <li><a href="#technical-proofs" id="toc-technical-proofs" class="nav-link" data-scroll-target="#technical-proofs"><span class="toc-section-number">26.39</span>  Technical Proofs*</a></li>
  <li><a href="#exercises" id="toc-exercises" class="nav-link" data-scroll-target="#exercises"><span class="toc-section-number">26.40</span>  Exercises</a></li>
  </ul>
<div class="toc-actions"><div><i class="bi bi-github"></i></div><div class="action-links"><p><a href="https://github.com/huhuaping/hansenEM/edit/master/chpt28-model-selection.qmd" class="toc-action">Edit this page</a></p></div></div></nav>
</nav>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title d-none d-lg-block"><span class="chapter-number">26</span>&nbsp; <span class="chapter-title">Model Selection, Stein Shrinkage, and Model Averaging</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<section id="introduction" class="level2" data-number="26.1">
<h2 data-number="26.1" class="anchored" data-anchor-id="introduction"><span class="header-section-number">26.1</span> Introduction</h2>
<p>The chapter reviews model selection, James-Stein shrinkage, and model averaging.</p>
<p>Model selection is a tool for selecting one model (or estimator) out of a set of models. Different model selection methods are distinguished by the criteria used to rank and compare models.</p>
<p>Model averaging is a generalization of model selection. Models and estimators are averaged using data-dependent weights.</p>
<p>James-Stein shrinkage modifies classical estimators by shrinking towards a reasonable target. Shrinking reduces mean squared error.</p>
<p>Two excellent monographs on model selection and averaging are Burnham and Anderson (1998) and Claeskens and Hjort (2008). James-Stein shrinkage theory is thoroughly covered in Lehmann and Casella (1998). See also Wasserman (2006) and Efron (2010).</p>
</section>
<section id="model-selection" class="level2" data-number="26.2">
<h2 data-number="26.2" class="anchored" data-anchor-id="model-selection"><span class="header-section-number">26.2</span> Model Selection</h2>
<p>In the course of an applied project an economist will routinely estimate multiple models. Indeed, most applied papers include tables displaying the results from different specifications. The question arises: Which model is best? Which should be used in practice? How can we select the best choice? This is the question of model selection.</p>
<p>Take, for example, a wage regression. Suppose we want a model which includes education, experience, region, and marital status. How should we proceed? Should we estimate a simple linear model plus a quadratic in experience? Should education enter linearly, a simple spline as in Figure 2.6(a), or with separate dummies for each education level? Should marital status enter as a simple dummy (married or not) or allowing for all recorded categories? Should interactions be included? Which? How many? Taken together we need to select the specific regressors to include in the regression model.</p>
<p>Model “selection” may be mis-named. It would be more appropriate to call the issue “estimator selection”. When we examine a table containing the results from multiple regressions we are comparing multiple estimates of the same regression. One estimator may include fewer variables than another; that is a restricted estimator. One may be estimated by least squares and another by 2SLS. Another could be nonparametric. The underlying model is the same; the difference is the estimator. Regardless, the literature has adopted the term “model selection” and we will adhere to this convention. To gain some basic understanding it may be helpful to start with a stylzed example. Suppose that we have a <span class="math inline">\(K \times 1\)</span> estimator <span class="math inline">\(\widehat{\theta}\)</span> which has expectation <span class="math inline">\(\theta\)</span> and known covariance matrix <span class="math inline">\(\boldsymbol{V}\)</span>. An alternative feasible estimator is <span class="math inline">\(\widetilde{\theta}=0\)</span>. The latter may seem like a silly estimator but it captures the feature that model selection typically concerns exclusion restrictions. In this context we can compare the accuracy of the two estimators by their weighted mean-squared error (WMSE). For a given weight matrix <span class="math inline">\(\boldsymbol{W}\)</span> define</p>
<p><span class="math display">\[
\text { wmse }[\widehat{\theta}]=\operatorname{tr}\left(\mathbb{E}\left[(\widehat{\theta}-\theta)(\widehat{\theta}-\theta)^{\prime}\right] \boldsymbol{W}\right)=\mathbb{E}\left[(\widehat{\theta}-\theta)^{\prime} \boldsymbol{W}(\widehat{\theta}-\theta)\right] \text {. }
\]</span></p>
<p>The calculations simplify by setting <span class="math inline">\(\boldsymbol{W}=\boldsymbol{V}^{-1}\)</span> which we do for our remaining calculations.</p>
<p>For our two estimators we calculate that</p>
<p><span class="math display">\[
\begin{aligned}
\text { wmse }[\hat{\theta}] &amp;=K \\
\text { wmse }[\widetilde{\theta}] &amp;=\theta^{\prime} \boldsymbol{V}^{-1} \theta \stackrel{\text { def }}{=} \lambda .
\end{aligned}
\]</span></p>
<p>(See Exercise 28.1) The WMSE of <span class="math inline">\(\widehat{\theta}\)</span> is smaller if <span class="math inline">\(K&lt;\lambda\)</span> and the WMSE of <span class="math inline">\(\widetilde{\theta}\)</span> is smaller if <span class="math inline">\(K&gt;\lambda\)</span>. One insight from this simple analysis is that we should prefer smaller (simpler) models when potentially omitted variables have small coefficients relative to estimation variance, and should prefer larger (more complicated) models when these variables have large coefficients relative to estimation variance. Another insight is that this choice is infeasible because <span class="math inline">\(\lambda\)</span> is unknown.</p>
<p>The comparison between (28.1) and (28.2) is a basic bias-variance trade-off. The estimator <span class="math inline">\(\widehat{\theta}\)</span> is unbiased but has a variance contribution of <span class="math inline">\(K\)</span>. The estimator <span class="math inline">\(\widetilde{\theta}\)</span> has zero variance but has a squared bias contribution <span class="math inline">\(\lambda\)</span>. The WMSE combines these two components.</p>
<p>Selection based on WMSE suggests that we should ideally select the estimator <span class="math inline">\(\widehat{\theta}\)</span> if <span class="math inline">\(K&lt;\lambda\)</span> and select <span class="math inline">\(\tilde{\theta}\)</span> if <span class="math inline">\(K&gt;\lambda\)</span>. A feasible implementation replaces <span class="math inline">\(\lambda\)</span> with an estimator. A plug-in estimator is <span class="math inline">\(\hat{\lambda}=\widehat{\theta}^{\prime} \boldsymbol{V}^{-1} \widehat{\theta}=W\)</span>, the Wald statistic for the test of <span class="math inline">\(\theta=0\)</span>. However, the estimator <span class="math inline">\(\widehat{\lambda}\)</span> has expectation</p>
<p><span class="math display">\[
\mathbb{E}[\widehat{\lambda}]=\mathbb{E}\left[\widehat{\theta}^{\prime} \boldsymbol{V}^{-1} \hat{\theta}\right]=\theta^{\prime} \boldsymbol{V}^{-1 \prime} \theta+\mathbb{E}\left[(\widehat{\theta}-\theta)^{\prime} \boldsymbol{V}^{-1}(\widehat{\theta}-\theta)\right]=\lambda+K
\]</span></p>
<p>so is biased. An unbiased estimator is <span class="math inline">\(\tilde{\lambda}=\widehat{\lambda}-K\)</span>. Notice that <span class="math inline">\(\tilde{\lambda}&gt;K\)</span> is the same as <span class="math inline">\(W&gt;2 K\)</span>. This leads to the model-selection rule: Use <span class="math inline">\(\widehat{\theta}\)</span> if <span class="math inline">\(W&gt;2 K\)</span> and use <span class="math inline">\(\widetilde{\theta}\)</span> otherwise.</p>
<p>This is an overly-simplistic setting but highlights the fundamental ingredients of criterion-based model selection. Comparing the MSE of different estimators typically involves a trade-off between the bias and variance with more complicated models exhibiting less bias but increased estimation variance. The actual trade-off is unknown because the bias depends on the unknown true parameters. The bias, however, can be estimated, giving rise to empirical estimates of the MSE and empirical model selection rules.</p>
<p>A large number of model selection criteria have been proposed. We list here those most frequently used in applied econometrics.</p>
<p>We first list selection criteria for the linear regression model <span class="math inline">\(Y=X^{\prime} \beta+e\)</span> with <span class="math inline">\(\sigma^{2}=\mathbb{E}\left[e^{2}\right]\)</span> and a <span class="math inline">\(k \times 1\)</span> coefficient vector <span class="math inline">\(\beta\)</span>. Let <span class="math inline">\(\widehat{\beta}\)</span> be the least squares estimator, <span class="math inline">\(\widehat{e}_{i}\)</span> the least squares residual, and <span class="math inline">\(\widehat{\sigma}^{2}=n^{-1} \sum_{i=1}^{n} \widehat{e}_{i}^{2}\)</span> the variance estimator. The number of estimated parameters <span class="math inline">\(\left(\beta\right.\)</span> and <span class="math inline">\(\left.\sigma^{2}\right)\)</span> is <span class="math inline">\(K=k+1\)</span>.</p>
</section>
<section id="bayesian-information-criterion" class="level2" data-number="26.3">
<h2 data-number="26.3" class="anchored" data-anchor-id="bayesian-information-criterion"><span class="header-section-number">26.3</span> Bayesian Information Criterion</h2>
<p><span class="math display">\[
\mathrm{BIC}=n+n \log \left(2 \pi \widehat{\sigma}^{2}\right)+K \log (n) .
\]</span></p>
</section>
<section id="akaike-information-criterion" class="level2" data-number="26.4">
<h2 data-number="26.4" class="anchored" data-anchor-id="akaike-information-criterion"><span class="header-section-number">26.4</span> Akaike Information Criterion</h2>
<p><span class="math display">\[
\mathrm{AIC}=n+n \log \left(2 \pi \widehat{\sigma}^{2}\right)+2 K .
\]</span></p>
</section>
<section id="cross-validation" class="level2" data-number="26.5">
<h2 data-number="26.5" class="anchored" data-anchor-id="cross-validation"><span class="header-section-number">26.5</span> Cross-Validation</h2>
<p><span class="math display">\[
\mathrm{CV}=\sum_{i=1}^{n} \widetilde{e}_{i}^{2}
\]</span></p>
<p>where <span class="math inline">\(\widetilde{e}_{i}\)</span> are the least squares leave-one-out prediction errors.</p>
<p>We next list two commonly-used selection criteria for likelihood-based estimation. Let <span class="math inline">\(f(y, \theta)\)</span> be a parametric density with a <span class="math inline">\(K \times 1\)</span> parameter <span class="math inline">\(\theta\)</span>. The likelihood <span class="math inline">\(L_{n}(\theta)=\prod_{i=1}^{n} f\left(Y_{i}, \theta\right)\)</span> is the density evaluated at the observations. The maximum likelihood estimator <span class="math inline">\(\widehat{\theta} \operatorname{maximizes} \ell_{n}(\theta)=\log L_{n}(\theta)\)</span>.</p>
</section>
<section id="bayesian-information-criterion-1" class="level2" data-number="26.6">
<h2 data-number="26.6" class="anchored" data-anchor-id="bayesian-information-criterion-1"><span class="header-section-number">26.6</span> Bayesian Information Criterion</h2>
<p><span class="math display">\[
\mathrm{BIC}=-2 \ell_{n}(\widehat{\theta})+K \log (n) .
\]</span></p>
</section>
<section id="akaike-information-criterion-1" class="level2" data-number="26.7">
<h2 data-number="26.7" class="anchored" data-anchor-id="akaike-information-criterion-1"><span class="header-section-number">26.7</span> Akaike Information Criterion</h2>
<p><span class="math display">\[
\mathrm{AIC}=-2 \ell_{n}(\widehat{\theta})+2 K .
\]</span></p>
<p>In the following sections we derive and discuss these and other model selection criteria.</p>
</section>
<section id="bayesian-information-criterion-2" class="level2" data-number="26.8">
<h2 data-number="26.8" class="anchored" data-anchor-id="bayesian-information-criterion-2"><span class="header-section-number">26.8</span> Bayesian Information Criterion</h2>
<p>The Bayesian Information Criterion (BIC), also known as the Schwarz Criterion, was introduced by Schwarz (1978). It is appropriate for parametric models estimated by maximum likelihood and is used to select the model with the highest approximate probability of being the true model.</p>
<p>Let <span class="math inline">\(\pi(\theta)\)</span> be the prior density for <span class="math inline">\(\theta\)</span>. The joint density of <span class="math inline">\(Y\)</span> and <span class="math inline">\(\theta\)</span> is <span class="math inline">\(f(y, \theta) \pi(\theta)\)</span>. The marginal density of <span class="math inline">\(Y\)</span> is</p>
<p><span class="math display">\[
p(y)=\int f(y, \theta) \pi(\theta) d \theta
\]</span></p>
<p>The marginal density <span class="math inline">\(p(Y)\)</span> evaluated at the observations is known as the marginal likelihood.</p>
<p>Schwarz (1978) established the following approximation.</p>
<p>Theorem 28.1 Schwarz. If the model <span class="math inline">\(f(y, \theta)\)</span> satisfies standard regularity conditions and the prior <span class="math inline">\(\pi(\theta)\)</span> is diffuse then</p>
<p><span class="math display">\[
-2 \log p(Y)=-2 \ell_{n}(\widehat{\theta})+K \log (n)+O(1)
\]</span></p>
<p>where the <span class="math inline">\(O(1)\)</span> term is bounded as <span class="math inline">\(n \rightarrow \infty\)</span>.</p>
<p>A heuristic proof for normal linear regression is given in Section 28.32. A “diffuse” prior is one which distributes weight uniformly over the parameter space.</p>
<p>Schwarz’s theorem shows that the marginal likelihood approximately equals the maximized likelihood multiplied by an adjustment depending on the number of estimated parameters and the sample size. The approximation (28.6) is commonly called the Bayesian Information Criterion or BIC. The BIC is a penalized <span class="math inline">\(\log\)</span> likelihood. The term <span class="math inline">\(K \log (n)\)</span> can be interpreted as an over-parameterization penalty. The multiplication of the log likelihood by <span class="math inline">\(-2\)</span> is traditional as it puts the criterion into the same units as a log-likelihood statistic. In the context of normal linear regression we have calculated in (5.6) that</p>
<p><span class="math display">\[
\ell_{n}(\widehat{\theta})=-\frac{n}{2}(\log (2 \pi)+1)-\frac{n}{2} \log \left(\widehat{\sigma}^{2}\right)
\]</span></p>
<p>where <span class="math inline">\(\widehat{\sigma}^{2}\)</span> is the residual variance estimate. Hence BIC equals (28.3) with <span class="math inline">\(K=k+1\)</span>.</p>
<p>Since <span class="math inline">\(n \log (2 \pi)+n\)</span> does not vary across models this term is often omitted. It is better, however, to define the BIC as described above so that different parametric families are comparable. It is also useful to know that some authors define the BIC by dividing the above expression by <span class="math inline">\(n\)</span> (e.g.&nbsp;<span class="math inline">\(\mathrm{BIC}=\log \left(2 \pi \widehat{\sigma}^{2}\right)+\)</span> <span class="math inline">\(K \log (n) / n)\)</span> which does not change the rankings between models. However, this is an unwise choice because it alters the scaling, making it difficult to compare the degree of difference between models.</p>
<p>Now suppose that we have two models <span class="math inline">\(\mathscr{M}_{1}\)</span> and <span class="math inline">\(\mathscr{M}_{2}\)</span> which have marginal likelihoods <span class="math inline">\(p_{1}(Y)\)</span> and <span class="math inline">\(p_{2}(Y)\)</span>. Assume that both models have equal prior probability. Bayes Theorem states that the probability that a model is true given the data is proportional to its marginal likelihood. Specifically</p>
<p><span class="math display">\[
\begin{aligned}
&amp;\mathbb{P}\left[\mathscr{M}_{1} \mid Y\right]=\frac{p_{1}(Y)}{p_{1}(Y)+p_{2}(Y)} \\
&amp;\mathbb{P}\left[\mathscr{M}_{2} \mid Y\right]=\frac{p_{2}(Y)}{p_{1}(Y)+p_{2}(Y)} .
\end{aligned}
\]</span></p>
<p>Bayes selection picks the model with highest probability. Thus if <span class="math inline">\(p_{1}(Y)&gt;p_{2}(Y)\)</span> we select <span class="math inline">\(\mathscr{M}_{1}\)</span>. If <span class="math inline">\(p_{1}(Y)&lt;p_{2}(Y)\)</span> we select <span class="math inline">\(\mathscr{M}_{2}\)</span>.</p>
<p>Finding the model with highest marginal likelihood is the same as finding the model with lowest value of <span class="math inline">\(-2 \log p(Y)\)</span>. Theorem <span class="math inline">\(28.1\)</span> shows that the latter approximately equals the BIC. BIC selection picks the model with the lowest <span class="math inline">\({ }^{1}\)</span> value of BIC. Thus BIC selection is approximate Bayes selection.</p>
<p>The above discussion concerned two models but applies to any number of models. BIC selection picks the model with the smallest BIC. For implementation you simply estimate each model, calculate its BIC, and compare. model.</p>
<p>The BIC may be obtained in Stata by using the command estimates stats after an estimated</p>
</section>
<section id="akaike-information-criterion-for-regression" class="level2" data-number="26.9">
<h2 data-number="26.9" class="anchored" data-anchor-id="akaike-information-criterion-for-regression"><span class="header-section-number">26.9</span> Akaike Information Criterion for Regression</h2>
<p>The Akaike Information Criterion (AIC) was introduced by Akaike (1973). It is used to select the model whose estimated density is closest to the true density. It is designed for parametric models estimated by maximum likelihood.</p>
<p>Let <span class="math inline">\(\widehat{f}(y)\)</span> be an estimator of the unknown true density <span class="math inline">\(g(y)\)</span> of the observation vector <span class="math inline">\(Y=\left(Y_{1}, \ldots, Y_{n}\right)\)</span>. For example, the normal linear regression estimate of <span class="math inline">\(g(y)\)</span> is <span class="math inline">\(\widehat{f}(y)=\prod_{i=1}^{n} \phi_{\widehat{\sigma}}\left(Y_{i}-X_{i}^{\prime} \widehat{\beta}\right)\)</span>.</p>
<p>To measure the distance between the two densities <span class="math inline">\(g\)</span> and <span class="math inline">\(\widehat{f}\)</span> Akaike used the Kullback-Leibler information criterion (KLIC)</p>
<p><span class="math display">\[
\operatorname{KLIC}(g, f)=\int g(y) \log \left(\frac{g(y)}{f(y)}\right) d y .
\]</span></p>
<p>Notice that <span class="math inline">\(\operatorname{KLIC}(g, f)=0\)</span> when <span class="math inline">\(f(y)=g(y)\)</span>. By Jensen’s inequality,</p>
<p><span class="math display">\[
\operatorname{KLIC}(g, f)=-\int g(y) \log \left(\frac{f(y)}{g(y)}\right) d y \geq-\log \int f(y) d y=0 .
\]</span></p>
<p>Thus <span class="math inline">\(\operatorname{KLIC}(g, f)\)</span> is a non-negative measure of the deviation of <span class="math inline">\(f\)</span> from <span class="math inline">\(g\)</span>, with small values indicating a smaller deviation.</p>
<p><span class="math inline">\({ }^{1}\)</span> When the BIC is negative this means taking the most negative value. The KLIC distance between the true and estimated densities is</p>
<p><span class="math display">\[
\begin{aligned}
\operatorname{KLIC}(g, \widehat{f}) &amp;=\int g(y) \log \left(\frac{g(y)}{\widehat{f}(y)}\right) d y \\
&amp;=\int g(y) \log (g(y)) d y-\int g(y) \log (\widehat{f}(y)) d y .
\end{aligned}
\]</span></p>
<p>This is random as it depends on the estimator <span class="math inline">\(\widehat{f}\)</span>. Akaike proposed the expected KLIC distance</p>
<p><span class="math display">\[
\mathbb{E}[\operatorname{KLIC}(g, \widehat{f})]=\int g(y) \log (g(y)) d y-\mathbb{E}\left[\int g(y) \log (\widehat{f}(y)) d y\right] .
\]</span></p>
<p>The first term in (28.8) does not depend on the model. So minimization of expected KLIC distance is minimization of the second term. Multiplied by 2 (similarly to the BIC) this is</p>
<p><span class="math display">\[
T=-2 \mathbb{E}\left[\int g(y) \log (\widehat{f}(y)) d y\right] .
\]</span></p>
<p>The expectation is over the random estimator <span class="math inline">\(\widehat{f}\)</span>.</p>
<p>An alternative interpretation is to notice that the integral in (28.9) is an expectation over <span class="math inline">\(Y\)</span> with respect to the true data density <span class="math inline">\(g(y)\)</span>. Thus we can write (28.9) as</p>
<p><span class="math display">\[
T=-2 \mathbb{E}[\log (\widehat{f}(\widetilde{Y}))]
\]</span></p>
<p>where <span class="math inline">\(\tilde{Y}\)</span> is an independent copy of <span class="math inline">\(Y\)</span>. The key to understand this expression is that both the estimator <span class="math inline">\(\widehat{f}\)</span> and the evaluation points <span class="math inline">\(\widetilde{Y}\)</span> are random and independent. <span class="math inline">\(T\)</span> is the expected log-likelihood fit using the estimated model <span class="math inline">\(\widehat{f}\)</span> of an out-of-sample realization <span class="math inline">\(\widetilde{Y}\)</span>. Thus <span class="math inline">\(T\)</span> can be interpreted as an expected predictive log likelihood. Models with low values of <span class="math inline">\(T\)</span> have good fit based on the out-of-sample loglikelihood.</p>
<p>To gain further understanding we consider the simple case of the normal linear regression model with <span class="math inline">\(K\)</span> regressors. The log density of the model for the observations is</p>
<p><span class="math display">\[
\log f(Y, \boldsymbol{X}, \theta)=-\frac{n}{2} \log \left(2 \pi \sigma^{2}\right)-\frac{1}{2 \sigma^{2}} \sum_{i=1}^{n}\left(Y_{i}-X_{i}^{\prime} \beta\right)^{2} .
\]</span></p>
<p>The expected value at the true parameter values is <span class="math inline">\(-\frac{n}{2} \log \left(2 \pi \sigma^{2}\right)-\frac{n}{2}\)</span>. This means that the idealized value of <span class="math inline">\(T\)</span> is <span class="math inline">\(T_{0}=n \log \left(2 \pi \sigma^{2}\right)+n\)</span>. This would be the value obtained if there were no estimation error.</p>
<p>To simplify the calculations, we add the assumption that the variance <span class="math inline">\(\sigma^{2}\)</span> is known.</p>
<p>Theorem 28.2 Suppose <span class="math inline">\(\widehat{f}(y)\)</span> is an estimated normal linear regression model with <span class="math inline">\(K\)</span> regressors and a known variance <span class="math inline">\(\sigma^{2}\)</span>. Suppose that the true density <span class="math inline">\(g(y)\)</span> is a conditionally homoskedastic regression with variance <span class="math inline">\(\sigma^{2}\)</span>. Then</p>
<p><span class="math display">\[
\begin{aligned}
T &amp;=n \log \left(2 \pi \sigma^{2}\right)+n+K=T_{0}+K \\
\mathbb{E}\left[-2 \ell_{n}(\widehat{\theta})\right] &amp;=n \log \left(2 \pi \sigma^{2}\right)+n-K=T_{0}-K .
\end{aligned}
\]</span></p>
<p>The proof is given in Section <span class="math inline">\(28.32\)</span>. These expressions are interesting. Expression (28.12) shows that the expected KLIC distance <span class="math inline">\(T\)</span> equals the idealized value <span class="math inline">\(T_{0}\)</span> plus <span class="math inline">\(K\)</span>. The latter is the cost of parameter estimation, measured in terms of expected KLIC distance. By estimating parameters (rather than using the true values) the expected KLIC distance increases by <span class="math inline">\(K\)</span>.</p>
<p>Expression (28.13) shows the converse story. It shows that the sample log-likelihood function is smaller than the idealized value <span class="math inline">\(T_{0}\)</span> by <span class="math inline">\(K\)</span>. This is the cost of in-sample over-fitting. The sample loglikelihood is an in-sample measure of fit and therefore understates the population log-likelihood. The two expressions together show that the expected sample log-likelihood is smaller than the target value <span class="math inline">\(T\)</span> by <span class="math inline">\(2 K\)</span>. This is the combined cost of over-fitting and parameter estimation.</p>
<p>Combining these expressions we can suggest an unbiased estimator for <span class="math inline">\(T\)</span>. In the normal regression model we use (28.4). Since <span class="math inline">\(n \log (2 \pi)+n\)</span> does not vary across models it are often omitted. Thus for linear regression it is common to use the definition <span class="math inline">\(\mathrm{AIC}=n \log \left(\widehat{\sigma}^{2}\right)+2 K\)</span>.</p>
<p>Interestingly the AIC takes a similar form to the BIC. Both the AIC and BIC are penalized log likelihoods, and both penalties are proportional to the number of estimated parameters <span class="math inline">\(K\)</span>. The difference is that the AIC penalty is <span class="math inline">\(2 K\)</span> while the BIC penalty is <span class="math inline">\(K \log (n)\)</span>. Since <span class="math inline">\(2&lt;\log (n)\)</span> if <span class="math inline">\(n \geq 8\)</span> the BIC uses a stronger parameterization penalty.</p>
<p>Selecting a model by the AIC is equivalent to calculating the AIC for each model and selecting the model with the lowest <span class="math inline">\({ }^{2}\)</span> value.</p>
<p>Theorem 28.3 Under the assumptions of Theorem 28.2, <span class="math inline">\(\mathbb{E}[\mathrm{AIC}]=T\)</span>. AIC is thus an unbiased estimator of <span class="math inline">\(T\)</span>.</p>
<p>One of the interesting features of these results are that they are exact - there is no approximation and they do not require that the true error is normally distributed. The critical assumption is conditional homoskedasticity. If homoskedasticity fails then the AIC loses its validity.</p>
<p>The AIC may be obtained in Stata by using the command estimates stats after an estimated model.</p>
</section>
<section id="akaike-information-criterion-for-likelihood" class="level2" data-number="26.10">
<h2 data-number="26.10" class="anchored" data-anchor-id="akaike-information-criterion-for-likelihood"><span class="header-section-number">26.10</span> Akaike Information Criterion for Likelihood</h2>
<p>For the general likelihood context Akaike proposed the criterion (28.7). Here, <span class="math inline">\(\widehat{\theta}\)</span> is the maximum likelihood estimator, <span class="math inline">\(\ell_{n}(\widehat{\theta})\)</span> is the maximized log-likelihood function, and <span class="math inline">\(K\)</span> is the number of estimated parameters. This specializes to (28.4) for the case of a normal linear regression model.</p>
<p>As for regression, AIC selection is performed by estimating a set of models, calculating AIC for each, and selecting the model with the smallest AIC.</p>
<p>The advantages of the AIC are that it is simple to calculate, easy to implement, and straightforward to interpret. It is intuitive as it is a simple penalized likelihood.</p>
<p>The disadvantage is that its simplicity may be deceptive. The proof shows that the criterion is based on a quadratic approximation to the log likelihood and an asymptotic chi-square approximation to the classical Wald statistic. When these conditions fail then the AIC may not be accurate. For example, if the model is an approximate (quasi) likelihood rather than a true likelihood then the failure of the information matrix equality implies that the classical Wald statistic is not asymptotically chi-square. In this case the accuracy of AIC fails. Another problem is that many nonlinear models have parameter regions where parametric identification fails. In these models the quadratic approximation to the log</p>
<p><span class="math inline">\({ }^{2}\)</span> When the AIC is negative this means taking the most negative value. likelihood function fails to hold uniformly in the parameter space so the accuracy of the AIC fails. These qualifications point to challenges in interpretation of the AIC in nonlinear models.</p>
<p>The following is an analog of Theorem 28.3.</p>
<p>Theorem 28.4 Under standard regularity conditions for maximum likelihood estimation, plus the assumption that certain statistics (identified in the proof) are uniformly integrable, <span class="math inline">\(\mathbb{E}[\mathrm{AIC}]=T+O\left(n^{1 / 2}\right)\)</span>. AIC is thus an approximately unbiased estimator of <span class="math inline">\(T\)</span></p>
<p>A sketch of the proof is given in Section <span class="math inline">\(28.32\)</span>.</p>
<p>This result shows that the AIC is, in general, a reasonable estimator of the KLIC fit of an estimated parametric model. The theorem holds broadly for maximum likelihood estimation and thus the AIC can be used in a wide variety of contexts.</p>
</section>
<section id="mallows-criterion" class="level2" data-number="26.11">
<h2 data-number="26.11" class="anchored" data-anchor-id="mallows-criterion"><span class="header-section-number">26.11</span> Mallows Criterion</h2>
<p>The Mallows Criterion was proposed by Mallows (1973) and is often called the <span class="math inline">\(C_{p}\)</span> criterion. It is appropriate for linear estimators of homoskedastic regression models.</p>
<p>Take the homoskedastic regression framework</p>
<p><span class="math display">\[
\begin{aligned}
Y &amp;=m+e \\
m &amp;=m(X) \\
\mathbb{E}[e \mid X] &amp;=0 \\
\mathbb{E}\left[e^{2} \mid X\right] &amp;=\sigma^{2} .
\end{aligned}
\]</span></p>
<p>Write the first equation in vector notation for the <span class="math inline">\(n\)</span> observations as <span class="math inline">\(\boldsymbol{Y}=\boldsymbol{m}+\boldsymbol{e}\)</span>. Let <span class="math inline">\(\widehat{\boldsymbol{m}}=\boldsymbol{A} \boldsymbol{Y}\)</span> be a linear estimator of <span class="math inline">\(\boldsymbol{m}\)</span>, meaning that <span class="math inline">\(\boldsymbol{A}\)</span> is some <span class="math inline">\(n \times n\)</span> function of the regressor matrix <span class="math inline">\(\boldsymbol{X}\)</span> only. The residuals are <span class="math inline">\(\widehat{\boldsymbol{e}}=\boldsymbol{Y}-\widehat{\boldsymbol{m}}\)</span>. The class of linear estimators includes least squares, weighted least squares, kernel regression, local linear regression, and series regression. For example, the least squares estimator using a regressor matrix <span class="math inline">\(\boldsymbol{Z}\)</span> is the case <span class="math inline">\(\boldsymbol{A}=\boldsymbol{Z}\left(\boldsymbol{Z}^{\prime} \boldsymbol{Z}\right)^{-1} \boldsymbol{Z}^{\prime}\)</span>.</p>
<p>Mallows (1973) proposed the criterion</p>
<p><span class="math display">\[
C_{p}=\widehat{\boldsymbol{e}}^{\prime} \widehat{\boldsymbol{e}}+2 \widetilde{\sigma}^{2} \operatorname{tr}(\boldsymbol{A})
\]</span></p>
<p>where <span class="math inline">\(\widetilde{\sigma}^{2}\)</span> is a preliminary estimator of <span class="math inline">\(\sigma^{2}\)</span> (typically based on fitting a large model). In the case of least squares regression with <span class="math inline">\(K\)</span> coefficients this simplifies to</p>
<p><span class="math display">\[
C_{p}=n \widehat{\sigma}^{2}+2 K \widetilde{\sigma}^{2} .
\]</span></p>
<p>The Mallows crierion can be used similarly to the AIC. A set of regression models are estimated and the criterion <span class="math inline">\(C_{p}\)</span> calculated for each. The model with the smallest value of <span class="math inline">\(C_{p}\)</span> is the Mallows-selected model.</p>
<p>Mallows designed the criterion <span class="math inline">\(C_{p}\)</span> as an unbiased estimator of the following measure of fit</p>
<p><span class="math display">\[
R=\mathbb{E}\left[\sum_{i=1}^{n}\left(\widehat{m}_{i}-m_{i}\right)^{2}\right] .
\]</span></p>
<p>This is the expected squared difference between the estimated and true regressions evaluated at the observations.</p>
<p>An alternative motivation for <span class="math inline">\(R\)</span> is in terms of prediction accuracy. Consider an independent set of observations <span class="math inline">\(\widetilde{Y}_{i}, i=1, \ldots, n\)</span>, which have the same regressors <span class="math inline">\(X_{i}\)</span> as those in sample. Consider prediction of <span class="math inline">\(\widetilde{Y}_{i}\)</span> given <span class="math inline">\(X_{i}\)</span> and the fitted regression. The least squares predictor is <span class="math inline">\(\widehat{m}_{i}\)</span>. The sum of expected squared prediction errors is</p>
<p><span class="math display">\[
\text { MSFE }=\sum_{i=1}^{n} \mathbb{E}\left[\left(\widetilde{Y}_{i}-\widehat{m}_{i}\right)^{2}\right] .
\]</span></p>
<p>The best possible (infeasible) value of this quantity is</p>
<p><span class="math display">\[
\operatorname{MSFE}_{0}=\sum_{i=1}^{n} \mathbb{E}\left[\left(\widetilde{Y}_{i}-m_{i}\right)^{2}\right] .
\]</span></p>
<p>The difference is the prediction accuracy of the estimator:</p>
<p><span class="math display">\[
\begin{aligned}
\operatorname{MSFE}^{-\operatorname{MSFE}_{0}} &amp;=\sum_{i=1}^{n} \mathbb{E}\left[\left(\widetilde{Y}_{i}-\widehat{m}_{i}\right)^{2}\right]-\sum_{i=1}^{n} \mathbb{E}\left[\left(\widetilde{Y}_{i}-m_{i}\right)^{2}\right] \\
&amp;=\mathbb{E}\left[\sum_{i=1}^{n}\left(\widehat{m}_{i}-m_{i}\right)^{2}\right] \\
&amp;=R
\end{aligned}
\]</span></p>
<p>which equals Mallows’ measure of fit. Thus <span class="math inline">\(R\)</span> is a measure of prediction accuracy.</p>
<p>We stated that the Mallows criterion is an unbiased estimator of <span class="math inline">\(R\)</span>. More accurately, the adjusted criterion <span class="math inline">\(C_{p}^{*}=C_{p}-\boldsymbol{e}^{\prime} \boldsymbol{e}\)</span> is unbiased for <span class="math inline">\(R\)</span>. When comparing models <span class="math inline">\(C_{p}\)</span> and <span class="math inline">\(C_{p}^{*}\)</span> are equivalent so this substitution has no consequence for model selection.</p>
<p>Theorem 28.5 If <span class="math inline">\(\widehat{\boldsymbol{m}}=\boldsymbol{A} \boldsymbol{Y}\)</span> is a linear estimator, the regression error is conditionally mean zero and homoskedastic, and <span class="math inline">\(\widetilde{\sigma}^{2}\)</span> is unbiased for <span class="math inline">\(\sigma^{2}\)</span>, then</p>
<p><span class="math display">\[
\mathbb{E}\left[C_{p}^{*}\right]=R
\]</span></p>
<p>so the adjusted Mallows criterion <span class="math inline">\(C_{p}^{*}\)</span> is an unbiased estimator of <span class="math inline">\(R\)</span>.</p>
<p>The proof is given in Section 28.32.</p>
</section>
<section id="hold-out-criterion" class="level2" data-number="26.12">
<h2 data-number="26.12" class="anchored" data-anchor-id="hold-out-criterion"><span class="header-section-number">26.12</span> Hold-Out Criterion</h2>
<p>Dividing the sample into two parts, one for estimation and the second for evaluation, creates a simple device for model evaluation and selection. This procedure is often labelled hold-out evaluation. In the recent machine learning literature the data division is typically described as a training sample and a test sample.</p>
<p>The sample is typically divided randomly so that the estimation (training) sample has <span class="math inline">\(N\)</span> observations and the evaluation (test) sample has <span class="math inline">\(P\)</span> observations, where <span class="math inline">\(N+P=n\)</span>. There is no universal rule for the choice of <span class="math inline">\(N \&amp; P\)</span>, but <span class="math inline">\(N=P=n / 2\)</span> is a standard choice. For more complicated procedures, such as the evaluation of model selection methods, it is desirable to make a tripartite division of the sample into (1) training, (2) model selection, and (3) final estimation and assessment. This can be particularly useful when it is desired to obtain a parameter estimator whose distribution is not distorted by the model selection process. Such divisions are most suited for a context of an extremely large sample.</p>
<p>Take the standard case of a bipartite division where <span class="math inline">\(1 \leq i \leq N\)</span> is the estimation sample and <span class="math inline">\(N+1 \leq\)</span> <span class="math inline">\(i \leq N+P\)</span> is the evaluation sample. On the estimation sample we construct the parameter estimates, for example the least squares coefficients</p>
<p><span class="math display">\[
\widetilde{\beta}_{N}=\left(\sum_{i=1}^{N} X_{i} X_{i}^{\prime}\right)^{-1}\left(\sum_{i=1}^{N} X_{i} Y_{i}\right)
\]</span></p>
<p>Combining this coefficient with the evaluation sample we calculate the prediction errors <span class="math inline">\(\widetilde{e}_{N, i}=Y_{i}-X_{i}^{\prime} \widetilde{\beta}_{N}\)</span> for <span class="math inline">\(i \geq N+1\)</span>.</p>
<p>In Section <span class="math inline">\(4.12\)</span> we defined the mean squared forecast error (MSFE) based on a estimation sample of size <span class="math inline">\(N\)</span> as the expectation of the squared out-of-sample prediction error <span class="math inline">\(\operatorname{MSFE}_{N}=\mathbb{E}\left[\widetilde{e}_{N, i}^{2}\right]\)</span>. The hold-out estimator of the MSFE is the average of the squared prediction errors</p>
<p><span class="math display">\[
\widetilde{\sigma}_{N, P}^{2}=\frac{1}{P} \sum_{i=N+1}^{N+P} \widetilde{e}_{N, i}^{2} .
\]</span></p>
<p>We can see that <span class="math inline">\(\widetilde{\sigma}_{N, P}^{2}\)</span> is unbiased for <span class="math inline">\(\mathrm{MSFE}_{N}\)</span>.</p>
<p>When <span class="math inline">\(N=P\)</span> we can improve estimation of the MSFE by flipping the procedure. Exchanging the roles of estimation and evaluation samples we obtain a second MSFE estimator, say <span class="math inline">\(\widetilde{\omega}_{N, P}^{2}\)</span>. The global estimator is their average <span class="math inline">\(\widetilde{\sigma}_{N, P}^{* 2}=\left(\widetilde{\sigma}_{N, P}^{2}+\widetilde{\omega}_{N, P}^{2}\right) / 2\)</span>. This estimator also has expectation MSFE <span class="math inline">\({ }_{N}\)</span> but has reduced variance.</p>
<p>The estimated MSFE <span class="math inline">\(\widetilde{\sigma}_{N, P}^{* 2}\)</span> can be used for model selection. The quantity <span class="math inline">\(\widetilde{\sigma}_{N, P}^{* 2}\)</span> is calculated for a set of proposed models. The selected model is the one with the smallest value of <span class="math inline">\(\widetilde{\sigma}_{N, P}^{* 2}\)</span>. The method is intuitive, general, and flexible, and does not rely on technical assumptions.</p>
<p>The hold-out method has two disadvantages. First, if our goal is estimation using the full sample, our desired estimate is <span class="math inline">\(\mathrm{MSFE}_{n}\)</span>, not <span class="math inline">\(\operatorname{MSFE}_{N}\)</span>. Hold-out estimation provides an estimator of the MSFE based on estimation using a substantially reduced sample size, and is thus biased for the MSFE based on estimation using the full sample. Second, the estimator <span class="math inline">\(\widetilde{\sigma}_{N, P}^{* 2}\)</span> is sensitive to the random sorting of the observations into the estimation and evaluation samples. This affects model selection. Results can depend on the initial sample sorting and are therefore partially arbitrary.</p>
</section>
<section id="cross-validation-criterion" class="level2" data-number="26.13">
<h2 data-number="26.13" class="anchored" data-anchor-id="cross-validation-criterion"><span class="header-section-number">26.13</span> Cross-Validation Criterion</h2>
<p>In applied statistics and machine learning the default method for model selection and tuning parameter selection is cross-validation. We have introduced some of the concepts throughout the textbook, and review and unify the concepts at this point. Cross-validation is closely related to the hold-out criterion introduced in the previous section.</p>
<p>In Section <span class="math inline">\(3.20\)</span> we defined the leave-one-out estimator as that obtained by applying an estimation formula to the sample omitting the <span class="math inline">\(i^{t h}\)</span> observation. This is identical to the hold-out problem as described previously, where the estimation sample is <span class="math inline">\(N=n-1\)</span> and the evaluation sample is <span class="math inline">\(P=1\)</span>. The estimator obtained omitting observation <span class="math inline">\(i\)</span> is written as <span class="math inline">\(\widehat{\beta}_{(-i)}\)</span>. The prediction error is <span class="math inline">\(\widetilde{e}_{i}=Y_{i}-X_{i}^{\prime} \widehat{\beta}_{(-i)}\)</span>. The out-of-sample mean squared error “estimate” is <span class="math inline">\(\widetilde{e}_{i}^{2}\)</span>. This is repeated <span class="math inline">\(n\)</span> times, once for each observation <span class="math inline">\(i\)</span>, and the MSFE estimate is the average of the <span class="math inline">\(n\)</span> squared prediction errors</p>
<p><span class="math display">\[
\mathrm{CV}=\frac{1}{n} \sum_{i=1}^{n} \widetilde{e}_{i}^{2} .
\]</span></p>
<p>The estimator CV is called the cross-validation (CV) criterion. It is a natural generalization of the hold-out criterion and eliminates the two disadvantages described in the previous section. First, the CV criterion is an unbiased estimator of MSFE <span class="math inline">\({ }_{n-1}\)</span>, which is essentially the same as MSFE . Thus CV <span class="math inline">\(_{n}\)</span>. is essentially unbiased for model selection. Second, the CV criterion does not depend on a random sorting of the observations. As there is no random component the criterion takes the same value in any implementation.</p>
<p>In least squares estimation the CV criterion has a simple computational implementation. Theorem 3.7 shows that the leave-one-out least squares estimator (3.42) equals</p>
<p><span class="math display">\[
\widehat{\beta}_{(-i)}=\widehat{\beta}-\frac{1}{\left(1-h_{i i}\right)}\left(\boldsymbol{X}^{\prime} \boldsymbol{X}\right)^{-1} X_{i} \widehat{e}_{i}
\]</span></p>
<p>where <span class="math inline">\(\widehat{e}_{i}\)</span> are the least squares residuals and <span class="math inline">\(h_{i i}\)</span> are the leverage values. The prediction error thus equals</p>
<p><span class="math display">\[
\widetilde{e}_{i}=Y_{i}-X_{i}^{\prime} \widehat{\beta}_{(-i)}=\left(1-h_{i i}\right)^{-1} \widehat{e}_{i}
\]</span></p>
<p>where the second equality is from Theorem 3.7. Consequently the CV criterion is</p>
<p><span class="math display">\[
\mathrm{CV}=\frac{1}{n} \sum_{i=1}^{n} \widetilde{e}_{i}^{2}=\frac{1}{n} \sum_{i=1}^{n}\left(1-h_{i i}\right)^{-2} \widehat{e}_{i}^{2} .
\]</span></p>
<p>Recall as well that in our study of nonparametric regression (Section 19.12) we defined the crossvalidation criterion for kernel regression as the weighted average of the squared prediction errors</p>
<p><span class="math display">\[
\mathrm{CV}=\frac{1}{n} \sum_{i=1}^{n} \tilde{e}_{i}^{2} w\left(X_{i}\right) .
\]</span></p>
<p>Theorem <span class="math inline">\(19.7\)</span> showed that <span class="math inline">\(\mathrm{CV}\)</span> is approximately unbiased for the integrated mean squared error (IMSE), which is a standard measure of accuracy for nonparametric regression. These results show that CV is an unbiased estimator for both the MSFE and IMSE, showing a close connection between these measures of accuracy.</p>
<p>In Section <span class="math inline">\(20.17\)</span> and equation (20.30) we defined the CV criterion for series regression as in (28.5). Selecting variables for series regression is identical to model selection. The results as described above show that the CV criterion is an estimator for the MSFE and IMSE of the regression model and is therefore a good candidate for assessing model accuracy. The validity of the CV criterion is much broader than the AIC as the theorems for CV do not require conditional homoskedasticity. This is not an artifact of the proof method; cross-validation is inherently more robust than AIC or BIC.</p>
<p>Implementation of CV model selection is the same as for the other criteria. A set of regression models are estimated. For each the CV criterion is calculated. The model with the smallest value of CV is the CVselected model.</p>
<p>The CV method is also much broader in concept and potential application. It applies to any estimation method so long as a “leave one out” error can be calculated. It can also be applied to other loss functions beyond squared error loss. For example, a cross-validation estimate of absolute loss is</p>
<p><span class="math display">\[
\mathrm{CV}=\frac{1}{n} \sum_{i=1}^{n}\left|\widetilde{e}_{i}\right| .
\]</span></p>
<p>Computationally and conceptually it is straightforward to select models by minimizing such criterion. However, the properties of applying CV to general criterion is not known.</p>
<p>Stata does not have a standard command to calculate the CV criterion for regression models.</p>
</section>
<section id="k-fold-cross-validation" class="level2" data-number="26.14">
<h2 data-number="26.14" class="anchored" data-anchor-id="k-fold-cross-validation"><span class="header-section-number">26.14</span> K-Fold Cross-Validation</h2>
<p>There are two deficiencies with the CV criterion which can be alleviated by the closely related K-fold cross-validation criterion. The first deficiency is that CV calculation can be computationally costly when sample sizes are very large or the estimation method is other than least squares. For estimators other than least squares it may be necessary to calculate <span class="math inline">\(n\)</span> separate estimations. This can be computationally prohibitive in some contexts. A second deficiency is that the CV criterion, viewed as an estimator of <span class="math inline">\(\operatorname{MSFE}_{n}\)</span>, has a high variance. The source is that the leave-one-out estimators <span class="math inline">\(\widehat{\beta}_{(-i)}\)</span> have minimal variation across <span class="math inline">\(i\)</span> and are therefore highly correlated.</p>
<p>An alternative is is to split the sample into <span class="math inline">\(K\)</span> groups (or “folds”) and treat each group as a hold-out sample. This effectively reduces the number of estimations from <span class="math inline">\(n\)</span> to <span class="math inline">\(K\)</span>. (This <span class="math inline">\(K\)</span> is not the number of estimated coefficients. I apologize for the possible confusion in notation but this is the standard label.) A common choice is <span class="math inline">\(K=10\)</span>, leading to what is known as <span class="math inline">\(\mathbf{1 0}\)</span>-fold cross-validation.</p>
<p>The method works by the following steps. This description is for estimation of a regression model <span class="math inline">\(Y=g(X, \theta)+e\)</span> with estimator <span class="math inline">\(\widehat{\theta}\)</span></p>
<ol type="1">
<li><p>Randomly sort the observations.</p></li>
<li><p>Split the observations into folds <span class="math inline">\(k=1, \ldots, K\)</span> of (roughly) equal size <span class="math inline">\(n_{k} \simeq n / K\)</span>. Let <span class="math inline">\(I_{k}\)</span> denote the observations in fold <span class="math inline">\(k\)</span></p></li>
<li><p>For <span class="math inline">\(k=1, . ., K\)</span></p></li>
</ol>
<ol type="a">
<li><p>Exclude fold <span class="math inline">\(I_{k}\)</span> from the dataset. This produces a sample with <span class="math inline">\(n-n_{k}\)</span> observations.</p></li>
<li><p>Calculate the estimator <span class="math inline">\(\widehat{\theta}_{(-k)}\)</span> on this sample.</p></li>
<li><p>Calculate the prediction errors <span class="math inline">\(\widetilde{e}_{i}=Y_{i}-g\left(X_{i}, \widehat{\theta}_{(-k)}\right)\)</span> for <span class="math inline">\(i \in I_{k}\)</span>.</p></li>
<li><p>Calculate <span class="math inline">\(\mathrm{CV}_{k}=n_{k}^{-1} \sum_{i \in I_{k}} \widetilde{e}_{i}^{2}\)</span></p></li>
</ol>
<p> 1. Calculate <span class="math inline">\(\mathrm{CV}=K^{-1} \sum_{k=1}^{K} \mathrm{CV}_{k}\)</span>.</p>
<p>If <span class="math inline">\(K=n\)</span> the method is identical to leave-one-out cross validation.</p>
<p>A useful feature of <span class="math inline">\(K\)</span>-fold CV is that we can calculate an approximate standard error. It is based on the approximation <span class="math inline">\(\operatorname{var}[\mathrm{CV}] \simeq K^{-1} \operatorname{var}\left[\mathrm{CV}_{k}\right]\)</span> which is based on the idea that <span class="math inline">\(\mathrm{CV}_{k}\)</span> are approximately uncorrelated acros folds. This leads to the standard error</p>
<p><span class="math display">\[
s(\mathrm{CV})=\sqrt{\frac{1}{K(K-1)} \sum_{k=1}^{K}\left(\mathrm{CV}_{k}-\mathrm{CV}\right)^{2}} .
\]</span></p>
<p>This is similar to a clustered variance formula, where the folds are treated as clusters. The standard error <span class="math inline">\(s\)</span> (CV) can be reported to assess the precision of CV as an estimate of the MSFE.</p>
<p>One disadvantage of K-fold cross-validation is that CV can be sensitive to the initial random sorting of the observations, leading to partially arbitrary results. This problem can be reduced by a technique called repeated CV, which repeats the K-fold CV algorithm <span class="math inline">\(M\)</span> times (each time with a different random sorting), leading to <span class="math inline">\(M\)</span> values of <span class="math inline">\(C\)</span>. These are averaged to produce the repeated CV value. As <span class="math inline">\(M\)</span> increases, the randomness due to sorting is eliminated. An associated standard error can be obtained by taking the square root of the average squared standard errors.</p>
<p>CV model selection is typically implemented by selecting the model with the smallest value of CV. An alternative implementation is known as the one standard error (1se) rule and selects the most parsimonious model whose value of CV is within one standard error of the minimum CV. The (informal) idea is that models whose value of <span class="math inline">\(\mathrm{CV}\)</span> is within one standard error of one another are not statistically distinguishable, and all else held equal we should lean towards parsimony. The 1se rule is the default in the popular cv.glmnet R function. The lse rule is an oversmoothing choice, meaning that it leans towards higher bias and reduced variance. In contrast, for inference many econometricians recommend undersmoothing bandwidths, which means selecting a less parsimonious model than the CV minimizing choice.</p>
</section>
<section id="many-selection-criteria-are-similar" class="level2" data-number="26.15">
<h2 data-number="26.15" class="anchored" data-anchor-id="many-selection-criteria-are-similar"><span class="header-section-number">26.15</span> Many Selection Criteria are Similar</h2>
<p>For the linear regression model many selection criteria have been introduced. However, many of these alternative criteria are quite similar to one another. In this section we review some of these connections. The following discussion is for the standard regression model <span class="math inline">\(Y=X^{\prime} \beta+e\)</span> with <span class="math inline">\(n\)</span> observations, <span class="math inline">\(K\)</span> estimated coefficients, and least squares variance estimator <span class="math inline">\(\widehat{\sigma}_{K}^{2}\)</span>.</p>
<p>Shibata (1980) proposed the criteria</p>
<p><span class="math display">\[
\text { Shibata }=\widehat{\sigma}_{K}^{2}\left(1+\frac{2 K}{n}\right)
\]</span></p>
<p>as an estimator of the MSFE. Recalling the Mallows criterion for regression (28.15) we see that Shibata = <span class="math inline">\(C_{p} / n\)</span> if we replace the preliminary estimator <span class="math inline">\(\widetilde{\sigma}^{2}\)</span> with <span class="math inline">\(\widehat{\sigma}_{K}^{2}\)</span>. Thus the two are quite similar in practice.</p>
<p>Taking logarithms and using the approximation <span class="math inline">\(\log (1+x) \simeq x\)</span> for small <span class="math inline">\(x\)</span></p>
<p><span class="math display">\[
n \log (\text { Shibata })=n \log \left(\widehat{\sigma}_{K}^{2}\right)+n \log \left(1+\frac{2 K}{n}\right) \simeq n \log \left(\widehat{\sigma}_{K}^{2}\right)+2 K=\text { AIC. }
\]</span></p>
<p>Thus minimization of Shibata’s criterion and AIC are similar.</p>
<p>Akaike (1969) proposed the Final Prediction Error Criteria</p>
<p><span class="math display">\[
\mathrm{FPE}=\widehat{\sigma}_{K}^{2}\left(\frac{1+K / n}{1-K / n}\right) .
\]</span></p>
<p>Using the expansions <span class="math inline">\((1-x)^{-1} \simeq 1+x\)</span> and <span class="math inline">\((1+x)^{2} \simeq 1+2 x\)</span> we see that <span class="math inline">\(\mathrm{FPE} \simeq\)</span> Shibata.</p>
<p>Craven and Wahba (1979) proposed Generalized Cross Validation</p>
<p><span class="math display">\[
\mathrm{GCV}=\frac{n \widehat{\sigma}_{K}^{2}}{(n-K)^{2}} .
\]</span></p>
<p>By the expansion <span class="math inline">\((1-x)^{-2} \simeq 1+2 x\)</span> we find that</p>
<p><span class="math display">\[
n \mathrm{GCV}=\frac{\widehat{\sigma}_{K}^{2}}{(1-K / n)^{2}} \simeq \widehat{\sigma}_{K}^{2}\left(1+\frac{2 K}{n}\right)=\text { Shibata. }
\]</span></p>
<p>The above calculations show that the WMSE, AIC, Shibata, FPE, GCV, and Mallows criterion are all close approximations to one another when <span class="math inline">\(K / n\)</span> is small. Differences arise in finite samples for large <span class="math inline">\(K\)</span>. However, the above analysis shows that there is no fundamental difference between these criteria. They are all estimating the same target. This is in contrast to BIC which uses a different parameterization penalty and is asymptotically distinct. Interestingly there also is a connection between <span class="math inline">\(\mathrm{CV}\)</span> and the above criteria. Again using the expansion <span class="math inline">\((1-x)^{-2} \simeq 1+2 x\)</span> we find that</p>
<p><span class="math display">\[
\begin{aligned}
\mathrm{CV} &amp;=\sum_{i=1}^{n}\left(1-h_{i i}\right)^{-2} \widehat{e}_{i}^{2} \\
&amp; \simeq \sum_{i=1}^{n} \widehat{e}_{i}^{2}+\sum_{i=1}^{n} 2 h_{i i} \widehat{e}_{i}^{2} \\
&amp;=n \widehat{\sigma}_{K}^{2}+2 \sum_{i=1}^{n} X_{i}^{\prime}\left(\boldsymbol{X}^{\prime} \boldsymbol{X}\right)^{-1} X_{i} \widehat{e}_{i}^{2} \\
&amp;=n \widehat{\sigma}_{K}^{2}+2 \operatorname{tr}\left(\left(\boldsymbol{X}^{\prime} \boldsymbol{X}\right)^{-1}\left(\sum_{i=1}^{n} X_{i} X_{i}^{\prime} \widehat{e}_{i}^{2}\right)\right) \\
&amp; \simeq n \widehat{\sigma}_{K}^{2}+2 \operatorname{tr}\left(\left(\mathbb{E}\left[X X^{\prime}\right]\right)^{-1}\left(\mathbb{E}\left[X X^{\prime} e^{2}\right]\right)\right) \\
&amp;=n \widehat{\sigma}_{K}^{2}+2 K \sigma^{2} \\
&amp; \simeq S h i b a t a .
\end{aligned}
\]</span></p>
<p>The third-to-last line holds asymptotically by the WLLN. The following equality holds under conditional homoskedasiticity. The final approximation replaces <span class="math inline">\(\sigma^{2}\)</span> by the estimator <span class="math inline">\(\widehat{\sigma}_{K}^{2}\)</span>. This calculation shows that under the assumption of conditional homoskedasticity the CV criterion is similar to the other criteria. It differs under heteroskedasticity, however, which is one of its primary advantages.</p>
</section>
<section id="relation-with-likelihood-ratio-testing" class="level2" data-number="26.16">
<h2 data-number="26.16" class="anchored" data-anchor-id="relation-with-likelihood-ratio-testing"><span class="header-section-number">26.16</span> Relation with Likelihood Ratio Testing</h2>
<p>Since the AIC and BIC are penalized log-likelihoods, AIC and BIC selection are related to likelihood ratio testing. Suppose we have two nested models <span class="math inline">\(\mathscr{M}_{1}\)</span> and <span class="math inline">\(\mathscr{M}_{2}\)</span> with log-likelihoods <span class="math inline">\(\ell_{1 n}\left(\widehat{\theta}_{1}\right)\)</span> and <span class="math inline">\(\ell_{2 n}\left(\widehat{\theta}_{2}\right)\)</span> and <span class="math inline">\(K_{1}&lt;K_{2}\)</span> estimated parameters. AIC selects <span class="math inline">\(\mathscr{M}_{1}\)</span> if <span class="math inline">\(\operatorname{AIC}\left(K_{1}\right)&lt;\operatorname{AIC}\left(K_{2}\right)\)</span> which occurs when</p>
<p><span class="math display">\[
\left.-2 \ell_{1 n}\left(\widehat{\theta}_{1}\right)+2 K_{1}&lt;-2 \ell_{2 n}\left(\widehat{\theta}_{2}\right)\right)+2 K_{2}
\]</span></p>
<p>or</p>
<p><span class="math display">\[
\mathrm{LR}=2\left(\ell_{2 n}\left(\widehat{\theta}_{2}\right)-\ell_{1 n}\left(\widehat{\theta}_{1}\right)\right)&lt;2 r
\]</span></p>
<p>where <span class="math inline">\(r=K_{2}-K_{1}\)</span>. Thus AIC selection is similar to selection by likelihood ratio testing with a different critical value. Rather than using a critical value from the chi-square distribution the “critical value” is <span class="math inline">\(2 r\)</span>. This is not to say that AIC selection is testing (it is not). But rather that there is a similar structure in the decision.</p>
<p>There are two useful practical implications. One is that when test statistics are reported in their <span class="math inline">\(F\)</span> form (which divide by the difference in coefficients <span class="math inline">\(r\)</span> ) then the AIC “critical value” is 2 . The AIC selects the restricted (smaller) model if <span class="math inline">\(F&lt;2\)</span>. It selects the unrestricted (larger) model if <span class="math inline">\(F&gt;2\)</span>.</p>
<p>Another useful implication is in the case of considering a single coefficient (when <span class="math inline">\(r=1\)</span> ). AIC selects the coefficient (the larger model) if <span class="math inline">\(\mathrm{LR}&gt;2\)</span>. In contrast a <span class="math inline">\(5 %\)</span> significance test “selects” the larger model (rejects the smaller) if LR <span class="math inline">\(&gt;3.84\)</span>. Thus AIC is more generous in terms of selecting larger models. An equivalent way of seeing this is that AIC selects the coefficient if the t-ratio exceeds <span class="math inline">\(1.41\)</span> while the <span class="math inline">\(5 %\)</span> significance test selects if the t-ratio exceeds <span class="math inline">\(1.96\)</span>.</p>
<p>Similar comments apply to BIC selection though the effective critical values are different. For comparing models with coefficients <span class="math inline">\(K_{1}&lt;K_{2}\)</span> the BIC selects <span class="math inline">\(\mathscr{M}_{1}\)</span> if <span class="math inline">\(\mathrm{LR}&lt;\log (n) r\)</span>. The “critical value” for an <span class="math inline">\(F\)</span> statistic is <span class="math inline">\(\log (n)\)</span>. Hence BIC selection becomes stricter as sample sizes increase.</p>
</section>
<section id="consistent-selection" class="level2" data-number="26.17">
<h2 data-number="26.17" class="anchored" data-anchor-id="consistent-selection"><span class="header-section-number">26.17</span> Consistent Selection</h2>
<p>An important property of a model selection procedure is whether it selects a true model in large samples. We call such a procedure consistent.</p>
<p>To discuss this further we need to thoughtfully define what is a “true” model. The answer depends on the type of model.</p>
<p>When a model is a parametric density or distribution <span class="math inline">\(f(y, \theta)\)</span> with <span class="math inline">\(\theta \in \Theta\)</span> (as in likelihood estimation) then the model is true if there is some <span class="math inline">\(\theta_{0} \in \Theta\)</span> such that <span class="math inline">\(f\left(y, \theta_{0}\right)\)</span> equals the true density or distribution. Notice that it is important in this context both that the function class <span class="math inline">\(f(y, \theta)\)</span> and parameter space <span class="math inline">\(\Theta\)</span> are appropriately defined.</p>
<p>In a semiparametric conditional moment condition model which states <span class="math inline">\(\mathbb{E}[g(Y, X, \theta) \mid X]=0\)</span> with <span class="math inline">\(\theta \in \Theta\)</span> then the model is true if there is some <span class="math inline">\(\theta_{0} \in \Theta\)</span> such that <span class="math inline">\(\mathbb{E}\left[g\left(Y, X, \theta_{0}\right) \mid X\right]=0\)</span>. This includes the regression model <span class="math inline">\(Y=m(X, \theta)+e\)</span> with <span class="math inline">\(\mathbb{E}[e \mid X]=0\)</span> where the model is true if there is some <span class="math inline">\(\theta_{0} \in \Theta\)</span> such that <span class="math inline">\(m\left(X, \theta_{0}\right)=\mathbb{E}[Y \mid X]\)</span>. It also includes the homoskedastic regression model which adds the requirement that <span class="math inline">\(\mathbb{E}\left[e^{2} \mid X\right]=\sigma^{2}\)</span> is a constant.</p>
<p>In a semiparametric unconditional moment condition model <span class="math inline">\(\mathbb{E}[g(Y, X, \theta)]=0\)</span> then the model is true if there is some <span class="math inline">\(\theta_{0} \in \Theta\)</span> such that <span class="math inline">\(\mathbb{E}\left[g\left(Y, X, \theta_{0}\right)\right]=0\)</span>. A subtle issue here is that when the model is just identified and <span class="math inline">\(\Theta\)</span> is unrestricted then this condition typically holds and so the model is typically true. This includes least squares regression interpreted as a projection and just-identified instrumental variables regression.</p>
<p>In a nonparametric model such as <span class="math inline">\(Y \sim f \in \mathscr{F}\)</span> where <span class="math inline">\(\mathscr{F}\)</span> is some function class (such as second-order differentiable densities) then the model is true if the true density is a member of the function class <span class="math inline">\(\mathscr{F}\)</span>.</p>
<p>A complication arises that there may be multiple true models. This cannot occur when models are strictly non-nested (meaning that there is no common element in both model classes) but strictly nonnested models are rare. Most models have non-trivial intersections. For example, the linear regression models <span class="math inline">\(Y=\alpha+X_{1}^{\prime} \beta_{1}+e\)</span> and <span class="math inline">\(Y=\alpha+X_{2}^{\prime} \beta_{2}+e\)</span> with <span class="math inline">\(X_{1}\)</span> and <span class="math inline">\(X_{2}\)</span> containing no common elements may appear non-nested but they intersect when <span class="math inline">\(\beta_{1}=0\)</span> and <span class="math inline">\(\beta_{2}=0\)</span>. As another example consider the linear model <span class="math inline">\(Y=\alpha+X^{\prime} \beta+e\)</span> and <span class="math inline">\(\log\)</span>-linear model <span class="math inline">\(\log (Y)=\alpha+X^{\prime} \beta+e\)</span>. If we add the assumption that <span class="math inline">\(e \sim \mathrm{N}\left(0, \sigma^{2}\right)\)</span> then the models are non-intersecting. But if we relax normality and instead use the conditional mean assumption <span class="math inline">\(\mathbb{E}[e \mid X]=0\)</span> then the models are intersecting when <span class="math inline">\(\beta_{1}=0\)</span> and <span class="math inline">\(\beta_{2}=0\)</span>.</p>
<p>The most common type of intersecting models are nested. In regression this occurs when the two models are <span class="math inline">\(Y=X_{1}^{\prime} \beta_{1}+e\)</span> and <span class="math inline">\(Y=X_{1}^{\prime} \beta_{1}+X_{2}^{\prime} \beta_{2}+e\)</span>. If <span class="math inline">\(\beta_{2} \neq 0\)</span> then only the second model is true. But if <span class="math inline">\(\beta_{2}=0\)</span> then both are true models.</p>
<p>In general, given a set of models <span class="math inline">\(\overline{\mathscr{M}}=\left\{\mathscr{M}_{1}, \ldots, \mathscr{M}_{M}\right\}\)</span> a subset <span class="math inline">\(\overline{\mathscr{M}}^{*}\)</span> are true models (as described above) while the remainder are not true models.</p>
<p>A model selection rule <span class="math inline">\(\widehat{M}\)</span> selects one model from the set <span class="math inline">\(\bar{M}\)</span>. We say a method is consistent if it asymptotically selects a true model.</p>
<p>Definition 28.1 A model selection rule is model selection consistent if <span class="math inline">\(\mathbb{P}\left[\widehat{M} \in \bar{M}^{*}\right] \rightarrow 1\)</span> as <span class="math inline">\(n \rightarrow \infty\)</span></p>
<p>This states that the model selection rule selects a true model with probability tending to 1 as the sample size diverges.</p>
<p>A broad class of model selection methods satisfy this definition of consistency. To see this consider the class of information criteria</p>
<p><span class="math display">\[
\mathrm{IC}=-2 \ell_{n}(\widehat{\theta})+c(n, K) .
\]</span></p>
<p>This includes AIC <span class="math inline">\((c=2 K), \mathrm{BIC}(c=K \log (n))\)</span>, and testing-based selection ( <span class="math inline">\(c\)</span> equals a fixed quantile of the <span class="math inline">\(\chi_{K}^{2}\)</span> distribution).</p>
<p>Theorem 28.6 Under standard regularity conditions for maximum likelihood estimation, selection based on IC is model selection consistent if <span class="math inline">\(c(n, K)=o(n)\)</span> as <span class="math inline">\(n \rightarrow \infty\)</span>.</p>
<p>The proof is given in Section <span class="math inline">\(28.32\)</span>.</p>
<p>This result covers AIC, BIC and testing-based selection. Thus all are model selection consistent.</p>
<p>A major limitation with this result is that the definition of model selection consistency is weak. A model may be true but over-parameterized. To understand the distinction consider the models <span class="math inline">\(Y=\)</span> <span class="math inline">\(X_{1}^{\prime} \beta_{1}+e\)</span> and <span class="math inline">\(Y=X_{1}^{\prime} \beta_{1}+X_{2}^{\prime} \beta_{2}+e\)</span>. If <span class="math inline">\(\beta_{2}=0\)</span> then both <span class="math inline">\(\mathscr{M}_{1}\)</span> and <span class="math inline">\(\mathscr{M}_{2}\)</span> are true, but <span class="math inline">\(\mathscr{M}_{1}\)</span> is the preferred model as it is more parsimonious. When two nested models are both true models it is conventional to think of the more parsimonious model as the correct model. In this context we do not describe the larger model as an incorrect model but rather as over-parameterized. If a selection rule asymptotically selects an over-parameterized model we say that it “over-selects”.</p>
<p>Definition 28.2 A model selection rule asymptotically over-selects if there are models <span class="math inline">\(\mathscr{M}_{1} \subset \mathscr{M}_{2}\)</span> such that <span class="math inline">\(\liminf _{n \rightarrow \infty} \mathbb{P}\left[\widehat{\mathscr{M}}=\mathscr{M}_{2} \mid \mathscr{M}_{1}\right]&gt;0\)</span>.</p>
<p>The definition states that over-selection occurs when two models are nested and the smaller model is true (so both models are true models but the smaller model is more parsimonious) if the larger model is asymptotically selected with positive probability.</p>
<p>Theorem 28.7 Under standard regularity conditions for maximum likelihood estimation, selection based on IC asymptotically over-selects if <span class="math inline">\(c(n, K)=O(1)\)</span> as <span class="math inline">\(n \rightarrow \infty\)</span>.</p>
<p>The proof is given in Section <span class="math inline">\(28.32\)</span>.</p>
<p>This result includes both AIC and testing-based selection. Thus these procedures over-select. For example, if the models are <span class="math inline">\(Y=X_{1}^{\prime} \beta_{1}+e\)</span> and <span class="math inline">\(Y=X_{1}^{\prime} \beta_{1}+X_{2}^{\prime} \beta_{2}+e\)</span> and <span class="math inline">\(\beta_{2}=0\)</span> holds, then these procedures select the over-parameterized regression with positive probability.</p>
<p>Following this line of reasoning, it is useful to draw a distinction between true and parsimonious models. We define the set of parsimonious models <span class="math inline">\(\bar{M}^{0} \subset \overline{\mathscr{M}}^{*}\)</span> as the set of true models with the fewest number of parameters. When the models in <span class="math inline">\(\bar{M}^{*}\)</span> are nested then <span class="math inline">\(\overline{\mathscr{M}}^{0}\)</span> will be a singleton. In the regression example with <span class="math inline">\(\beta_{2}=0\)</span> then <span class="math inline">\(\mathscr{M}_{1}\)</span> is the unique parsimonious model among <span class="math inline">\(\left\{\mathscr{M}_{1}, \mathscr{M}_{2}\right\}\)</span>. We introduce a stronger consistency definition for procedures which asymptotically select parsimonious models. Definition 28.3 A model selection rule is consistent for parsimonious models if <span class="math inline">\(\mathbb{P}\left[\widehat{\mathscr{M}} \in \overline{\mathscr{M}}^{0}\right] \rightarrow 1\)</span> as <span class="math inline">\(n \rightarrow \infty\)</span></p>
<p>Of the methods we have reviewed, only BIC selection is consistent for parsimonious models, as we now show.</p>
<p>Theorem 28.8 Under standard regularity conditions for maximum likelihood estimation, selection based on IC is consistent for parsimonious models if for all <span class="math inline">\(K_{2}&gt;K_{1}\)</span></p>
<p><span class="math display">\[
c\left(n, K_{2}\right)-c\left(n, K_{1}\right) \rightarrow \infty
\]</span></p>
<p>as <span class="math inline">\(n \rightarrow \infty\)</span>, yet <span class="math inline">\(c(n, K)=o(n)\)</span> as <span class="math inline">\(n \rightarrow \infty\)</span>.</p>
<p>The proof is given in Section 28.32.</p>
<p>The condition includes BIC because <span class="math inline">\(c\left(n, K_{2}\right)-c\left(n, K_{1}\right)=\left(K_{2}-K_{1}\right) \log (n) \rightarrow \infty\)</span> if <span class="math inline">\(K_{2}&gt;K_{1}\)</span>.</p>
<p>Some economists have interpreted Theorem <span class="math inline">\(28.8\)</span> as indicating that BIC selection is preferred over the other methods. This is an incorrect deduction. In the next section we show that the other selection procedures are asymptotically optimal in terms of model fit and in terms of out-of-sample forecasting. Thus consistent model selection is only one of several desirable statistical properties.</p>
</section>
<section id="asymptotic-selection-optimality" class="level2" data-number="26.18">
<h2 data-number="26.18" class="anchored" data-anchor-id="asymptotic-selection-optimality"><span class="header-section-number">26.18</span> Asymptotic Selection Optimality</h2>
<p>Regressor selection by the AIC/Shibata/Mallows/CV class turns out to be asymptotically optimal with respect to out-of-sample prediction under quite broad conditions. This may appear to conflict with the results of the previous section but it does not as there is a critical difference between the goals of consistent model selection and accurate prediction.</p>
<p>Our analysis will be in the homoskedastic regression model conditioning on the regressor matrix <span class="math inline">\(\boldsymbol{X}\)</span>. We write the regression model as</p>
<p><span class="math display">\[
\begin{aligned}
Y &amp;=m+e \\
m &amp;=\sum_{j=1}^{\infty} X_{j} \beta_{j} \\
\mathbb{E}[e \mid X] &amp;=0 \\
\mathbb{E}\left[e^{2} \mid X\right] &amp;=\sigma^{2}
\end{aligned}
\]</span></p>
<p>where <span class="math inline">\(X=\left(X_{1}, X_{2}, \ldots\right)\)</span>. We can also write the regression equation in matrix notation as <span class="math inline">\(\boldsymbol{Y}=\boldsymbol{m}+\boldsymbol{e}\)</span>.</p>
<p>The <span class="math inline">\(K^{t h}\)</span> regression model uses the first <span class="math inline">\(K\)</span> regressors <span class="math inline">\(X_{K}=\left(X_{1}, X_{2}, \ldots, X_{K}\right)\)</span>. The least squares estimates in matrix notation are</p>
<p><span class="math display">\[
\boldsymbol{Y}=\boldsymbol{X}_{K} \widehat{\beta}_{K}+\widehat{\boldsymbol{e}}_{K} .
\]</span></p>
<p>As in Section <span class="math inline">\(28.6\)</span> define the fitted values <span class="math inline">\(\widehat{\boldsymbol{m}}=\boldsymbol{X}_{K} \widehat{\beta}_{K}\)</span> and regression fit (sum of expected squared prediction errors) as</p>
<p><span class="math display">\[
R_{n}(K)=\mathbb{E}\left[(\widehat{\boldsymbol{m}}-\boldsymbol{m})^{\prime}(\widehat{\boldsymbol{m}}-\boldsymbol{m}) \mid \boldsymbol{X}\right]
\]</span></p>
<p>though now we index <span class="math inline">\(R\)</span> by sample size <span class="math inline">\(n\)</span> and model <span class="math inline">\(K\)</span>.</p>
<p>In any sample there is an optimal model <span class="math inline">\(K\)</span> which minimizes <span class="math inline">\(R_{n}(K)\)</span> :</p>
<p><span class="math display">\[
K_{n}^{\mathrm{opt}}=\underset{K}{\operatorname{argmin}} R_{n}(K) .
\]</span></p>
<p>Model <span class="math inline">\(K_{n}^{\text {opt }}\)</span> obtains the minimized value of <span class="math inline">\(R_{n}(K)\)</span></p>
<p><span class="math display">\[
R_{n}^{\mathrm{opt}}=R_{n}\left(K_{n}^{\mathrm{opt}}\right)=\min _{K} R_{n}(K) .
\]</span></p>
<p>Now consider model selection using the Mallow’s criterion for regression models</p>
<p><span class="math display">\[
C_{p}(K)=\widehat{\boldsymbol{e}}_{K}^{\prime} \widehat{\boldsymbol{e}}_{K}+2 \sigma^{2} K
\]</span></p>
<p>where we explicitly index by <span class="math inline">\(K\)</span>, and for simplicity we assume the error variance <span class="math inline">\(\sigma^{2}\)</span> is known. (The results are unchanged if it is replaced by a consistent estimator.) Let the selected model be</p>
<p><span class="math display">\[
\widehat{K}_{n}=\underset{K}{\operatorname{argmin}} C_{p}(K) .
\]</span></p>
<p>Prediction accuracy using the Mallows-selected model is <span class="math inline">\(R_{n}\left(\widehat{K}_{n}\right)\)</span>. We say that a selection procedure is asymptotically optimal if the prediction accuracy is asymptotically equivalent with the infeasible optimum. This can be written as</p>
<p><span class="math display">\[
\frac{R_{n}\left(\widehat{K}_{n}\right)}{R_{n}^{\mathrm{opt}}} \underset{p}{\longrightarrow} 1 .
\]</span></p>
<p>We consider convergence in (28.18) in terms of the risk ratio because <span class="math inline">\(R_{n}^{\text {opt }}\)</span> diverges as the sample size increases.</p>
<p>Li (1987) established the asymptotic optimality (28.18). His result depends on the following conditions.</p>
<p><img src="images//2022_10_23_101d59f261a704807a3bg-17.jpg" class="img-fluid"></p>
<ol type="1">
<li><p>The observations <span class="math inline">\(\left(Y_{i}, X_{\boldsymbol{i}}\right), i=1, \ldots, n\)</span>, are independent and identically distributed.</p></li>
<li><p><span class="math inline">\(\mathbb{E}[e \mid X]=0\)</span>.</p></li>
<li><p><span class="math inline">\(\mathbb{E}\left[e^{2} \mid X\right]=\sigma^{2}\)</span>.</p></li>
<li><p><span class="math inline">\(\mathbb{E}\left[|e|^{4 r} \mid X\right] \leq B&lt;\infty\)</span> for some <span class="math inline">\(r&gt;1\)</span>.</p></li>
<li><p><span class="math inline">\(R_{n}^{\mathrm{opt}} \rightarrow \infty\)</span> as <span class="math inline">\(n \rightarrow \infty\)</span></p></li>
<li><p>The estimated models are nested.</p></li>
</ol>
<p>Assumptions 28.1.2 and 28.1.3 state that the true model is a conditionally homoskedastic regression. Assumption 28.1.4 is a technical condition, that a conditional moment of the error is uniformly bounded. Assumption 28.1.5 is subtle. It effectively states that there is no correctly specified finite-dimensional model. To see this, suppose that there is a <span class="math inline">\(K_{0}\)</span> such that the model is correctly specified, meaning that <span class="math inline">\(m_{i}=\sum_{j=1}^{K_{0}} X_{j i} \beta_{j}\)</span>. In this case we can show that for <span class="math inline">\(K \geq K_{0}, R_{n}(K)=R_{n}\left(K_{0}\right)\)</span> does not change with <span class="math inline">\(n\)</span>, violating Assumption 28.1.5. Assumption 28.1.6 is a technical condition that restricts the number of estimated models. This assumption can be generalized to allow non-nested models, but in this case an alternative restriction on the number of estimated models is needed.</p>
<p>Theorem 28.9 Assumption <span class="math inline">\(28.1\)</span> implies (28.18). Thus Mallows selection is asymptotically equivalent to using the infeasible optimal model.</p>
<p>The proof is given in Section 28.32.</p>
<p>Theorem <span class="math inline">\(28.9\)</span> states that Mallows selection in a conditional homoskedastic regression is asymptotically optimal. The key assumptions are homoskedasticity and that all finite-dimensional models are misspecified (incomplete), meaning that there are always omitted variables. The latter means that regardless of the sample size there is always a trade-off between omitted variables bias and estimation variance. The theorem as stated is specific for Mallows selection but extends to AIC, Shibata, GCV, FPE, and CV with some additional technical considerations. The primary message is that the selection methods discussed in the previous section asymptotically select a sequence of models which are best-fitting in the sense of minimizing the prediction error.</p>
<p>Using a similar argument, Andrews (1991c) showed that selection by cross-validation satisfies the same asymptotic optimality condition without requiring conditional homoskedasticity. The treatment is a bit more technical so we do not review it here. This indicates an important advantage for crossvalidation selection over the other methods.</p>
</section>
<section id="focused-information-criterion" class="level2" data-number="26.19">
<h2 data-number="26.19" class="anchored" data-anchor-id="focused-information-criterion"><span class="header-section-number">26.19</span> Focused Information Criterion</h2>
<p>Claeskens and Hjort (2003) introduced the Focused Information Criterion (FIC) as an estimator of the MSE of a scalar parameter. The criterion is appropriate in correctly-specified likelihood models when one of the estimated models nests the other models. Let <span class="math inline">\(f(y, \theta)\)</span> be a parametric model density with a <span class="math inline">\(K \times 1\)</span> parameter <span class="math inline">\(\theta\)</span>.</p>
<p>The class of models (sub-models) allowed are those defined by a set of differentiable restrictions <span class="math inline">\(r(\theta)=0\)</span>. Let <span class="math inline">\(\widetilde{\theta}\)</span> be the restricted MLE which maximizes the likelihood subject to <span class="math inline">\(r(\theta)=0\)</span>.</p>
<p>A key feature of the FIC is that it focuses on a real-valued parameter <span class="math inline">\(\mu=g(\theta)\)</span> where <span class="math inline">\(g\)</span> is some differentiable function. Claeskens and Hjort call <span class="math inline">\(\mu\)</span> the target parameter. The choice of <span class="math inline">\(\mu\)</span> is made by the researcher and is a critical choice. In most applications <span class="math inline">\(\mu\)</span> is the key coefficient in the application (for example, the returns to schooling in a wage regression). The unrestricted MLE for <span class="math inline">\(\mu\)</span> is <span class="math inline">\(\widehat{\mu}=g(\widehat{\theta})\)</span>, the restricted MLE is <span class="math inline">\(\widetilde{\mu}=g(\widetilde{\theta})\)</span>.</p>
<p>Estimation accuracy is measured by the MSE of the estimator of the target parameter, which is the squared bias plus the variance:</p>
<p><span class="math display">\[
\operatorname{mse}[\widetilde{\mu}]=\mathbb{E}\left[(\widetilde{\mu}-\mu)^{2}\right]=(\mathbb{E}[\widetilde{\mu}]-\mu)^{2}+\operatorname{var}[\widetilde{\mu}] .
\]</span></p>
<p>It turns out to be convenient to normalize the MSE by that of the unrestricted estimator. We define this as the Focus</p>
<p><span class="math display">\[
\mathrm{F}=\operatorname{mse}[\widetilde{\mu}]-\operatorname{mse}[\widehat{\mu}] .
\]</span></p>
<p>The Claeskens-Hjort FIC is an estimator of F. Specifically,</p>
<p><span class="math display">\[
\mathrm{FIC}=(\widetilde{\mu}-\widehat{\mu})^{2}-2 \widehat{\boldsymbol{G}}^{\prime} \widehat{\boldsymbol{V}}_{\widehat{\theta}} \widehat{\boldsymbol{R}}\left(\widehat{\boldsymbol{R}}^{\prime} \widehat{\boldsymbol{V}}_{\widehat{\theta}} \widehat{\boldsymbol{R}}^{-1} \widehat{\boldsymbol{R}}^{\prime} \widehat{\boldsymbol{V}}_{\widehat{\theta}} \widehat{\boldsymbol{G}}\right.
\]</span></p>
<p>where <span class="math inline">\(\widehat{\boldsymbol{V}}_{\widehat{\theta}}, \widehat{\boldsymbol{G}}\)</span> and <span class="math inline">\(\widehat{\boldsymbol{R}}\)</span> are estimators of <span class="math inline">\(\operatorname{var}[\widehat{\theta}], \boldsymbol{G}=\frac{\partial}{\partial \theta^{\prime}} g(\theta)\)</span> and <span class="math inline">\(\boldsymbol{R}=\frac{\partial}{\partial \theta^{\prime}} r(\theta)\)</span>.</p>
<p>In a least squares regression <span class="math inline">\(\boldsymbol{Y}=\boldsymbol{X} \beta+\boldsymbol{e}\)</span> with a linear restriction <span class="math inline">\(\boldsymbol{R}^{\prime} \beta=0\)</span> and linear parameter of interest <span class="math inline">\(\mu=\boldsymbol{G}^{\prime} \beta\)</span> the FIC equals</p>
<p><span class="math display">\[
\begin{aligned}
\mathrm{FIC} &amp;=\left(\boldsymbol{G}^{\prime} \boldsymbol{R}\left(\boldsymbol{R}^{\prime}\left(\boldsymbol{X}^{\prime} \boldsymbol{X}\right)^{-1} \boldsymbol{R}\right)^{-1} \boldsymbol{R}^{\prime}\left(\boldsymbol{X}^{\prime} \boldsymbol{X}\right)^{-1} \widehat{\beta}\right)^{2} \\
&amp;-2 \widehat{\sigma}^{2} \boldsymbol{G}^{\prime}\left(\boldsymbol{X}^{\prime} \boldsymbol{X}\right)^{-1} \boldsymbol{R}\left(\boldsymbol{R}^{\prime}\left(\boldsymbol{X}^{\prime} \boldsymbol{X}\right)^{-1} \boldsymbol{R}\right)^{-1} \boldsymbol{R}^{\prime}\left(\boldsymbol{X}^{\prime} \boldsymbol{X}\right)^{-1} \boldsymbol{G} .
\end{aligned}
\]</span></p>
<p>The FIC is used similarly to AIC. The FIC is calculated for each sub-model of interest and the model with the lowest value of FIC is selected.</p>
<p>The advantage of the FIC is that it is specifically targeted to minimize the MSE of the target parameter. The FIC is therefore appropriate when the goal is to estimate a specific target parameter. A disadvantage is that it does not necessarily produce a model with good estimates of the other parameters. For example, in a linear regression <span class="math inline">\(Y=X_{1} \beta_{1}+X_{2} \beta_{2}+e\)</span>, if <span class="math inline">\(X_{1}\)</span> and <span class="math inline">\(X_{2}\)</span> are uncorrelated and the focus parameter is <span class="math inline">\(\beta_{1}\)</span> then the FIC will tend to select the sub-model without <span class="math inline">\(X_{2}\)</span>, and thus the selected model will produce a highly biased estimate of <span class="math inline">\(\beta_{2}\)</span>. Consequently when using the FIC it is dubious if attention should be paid to estimates other than those of <span class="math inline">\(\mu\)</span>.</p>
<p>Computationally it may be convenient to implement the FIC using an alternative formulation. Define the adjusted focus</p>
<p><span class="math display">\[
\mathrm{F}^{*}=n(\mathrm{~F}+2 \operatorname{mse}[\widehat{\mu}])=n(\operatorname{mse}[\widetilde{\mu}]+\operatorname{mse}[\widehat{\mu}]) .
\]</span></p>
<p>This adds the same quantity to all models and therefore does not alter the minimizing model. Multiplication by <span class="math inline">\(n\)</span> puts the FIC in units which are easier for reporting. The estimate of the adjusted focus is an adjusted FIC and can be written as</p>
<p><span class="math display">\[
\begin{aligned}
\text { FIC }^{*} &amp;=n(\widetilde{\mu}-\widehat{\mu})^{2}+2 n \widehat{\boldsymbol{V}}_{\widetilde{\mu}} \\
&amp;=n(\widetilde{\mu}-\widehat{\mu})^{2}+2 n s(\widetilde{\mu})^{2}
\end{aligned}
\]</span></p>
<p>where</p>
<p><span class="math display">\[
\widehat{\boldsymbol{V}}_{\widetilde{\mu}}=\widehat{\boldsymbol{G}}^{\prime}\left(\boldsymbol{I}_{k}-\widehat{\boldsymbol{V}}_{\widehat{\theta}} \widehat{\boldsymbol{R}}\left(\widehat{\boldsymbol{R}}^{\prime} \widehat{\boldsymbol{V}}_{\widehat{\theta}} \widehat{\boldsymbol{R}}\right)^{-1} \widehat{\boldsymbol{R}}^{\prime} \widehat{\boldsymbol{V}}_{\widehat{\theta}}\right) \widehat{\boldsymbol{G}}
\]</span></p>
<p>is an estimator of <span class="math inline">\(\operatorname{var}[\widetilde{\mu}]\)</span> and <span class="math inline">\(s(\widetilde{\mu})=\widehat{V}_{\widetilde{\mu}}^{1 / 2}\)</span> is a standard error for <span class="math inline">\(\widetilde{\mu}\)</span>.</p>
<p>This means that <span class="math inline">\(\mathrm{FIC}^{*}\)</span> can be easily calculated using conventional software without additional programming. The estimator <span class="math inline">\(\widehat{\mu}\)</span> can be calculated from the full model (the long regression) and the estimator <span class="math inline">\(\widetilde{\mu}\)</span> and its standard error <span class="math inline">\(s(\widetilde{\mu})\)</span> from the restricted model (the short regression). The formula (28.20) can then be applied to obtain FIC <span class="math inline">\(^{*}\)</span>.</p>
<p>The formula (28.19) also provides an intuitive understanding of the FIC. When we minimize FIC* we are minimizing the variance of the estimator of the target parameter <span class="math inline">\(\left(\widehat{\boldsymbol{V}}_{\widetilde{\mu}}\right)\)</span> while not altering the estimate <span class="math inline">\(\widetilde{\mu}\)</span> too much from the unrestricted estimate <span class="math inline">\(\widehat{\mu}\)</span>.</p>
<p>When selecting from amongst just two models, the FIC selects the restricted model if <span class="math inline">\((\widetilde{\mu}-\widehat{\mu})^{2}+2 \widehat{\boldsymbol{V}}_{\widetilde{\mu}}&lt;\)</span> 0 which is the same as <span class="math inline">\((\widetilde{\mu}-\widehat{\mu})^{2} / \widehat{V}_{\widetilde{\mu}}&lt;2\)</span>. The statistic to the left of the inequality is the squared t-statistic in the restricted model for testing the hypothesis that <span class="math inline">\(\mu\)</span> equals the unrestricted estimator <span class="math inline">\(\widehat{\mu}\)</span> but ignoring the estimation error in the latter. Thus a simple implementation (when just comparing two models) is to estimate the long and short regressions, take the difference in the two estimates of the coefficient of interest, and compute a t-ratio using the standard error from the short (restricted) regression. If this t-ratio exceeds <span class="math inline">\(1.4\)</span> the FIC selects the long regression estimate. If the t-ratio is smaller than <span class="math inline">\(1.4\)</span> the FIC selects the short regression estimate. Claeskens and Hjort motivate the FIC using a local misspecification asymptotic framework. We use a simpler heuristic motivation. First take the unrestricted MLE. Under standard conditions <span class="math inline">\(\widehat{\mu}\)</span> has asymptotic variance <span class="math inline">\(\boldsymbol{G}^{\prime} \boldsymbol{V}_{\theta} \boldsymbol{G}\)</span> where <span class="math inline">\(\boldsymbol{V}_{\theta}=\mathscr{I}^{-1}\)</span>. As the estimator is asymptotically unbiased it follows that</p>
<p><span class="math display">\[
\operatorname{mse}[\widehat{\mu}] \simeq \operatorname{var}[\widehat{\mu}] \simeq n^{-1} \boldsymbol{G}^{\prime} \boldsymbol{V}_{\theta} \boldsymbol{G} .
\]</span></p>
<p>Second take the restricted MLE. Under standard conditions <span class="math inline">\(\widetilde{\mu}\)</span> has asymptotic variance</p>
<p><span class="math display">\[
\boldsymbol{G}^{\prime}\left(\boldsymbol{V}_{\theta}-\boldsymbol{V}_{\theta} \boldsymbol{R}\left(\boldsymbol{R}^{\prime} \boldsymbol{V}_{\theta} \boldsymbol{R}\right)^{-1} \boldsymbol{R} \boldsymbol{V}_{\theta}\right) \boldsymbol{G} .
\]</span></p>
<p><span class="math inline">\(\widetilde{\mu}\)</span> also has a probability limit, say <span class="math inline">\(\mu_{R}\)</span>, which (generally) differs from <span class="math inline">\(\mu\)</span>. Together we find that</p>
<p><span class="math display">\[
\operatorname{mse}[\widetilde{\mu}] \simeq B+n^{-1} \boldsymbol{G}^{\prime}\left(\boldsymbol{V}_{\theta}-\boldsymbol{V}_{\theta} \boldsymbol{R}\left(\boldsymbol{R}^{\prime} \boldsymbol{V}_{\theta} \boldsymbol{R}\right)^{-1} \boldsymbol{R} \boldsymbol{V}_{\theta}\right) \boldsymbol{G}
\]</span></p>
<p>where <span class="math inline">\(B=\left(\mu-\mu_{R}\right)^{2}\)</span>. Subtracting, we find that the Focus is</p>
<p><span class="math display">\[
\mathrm{F} \simeq B-n^{-1} \boldsymbol{G}^{\prime} \boldsymbol{V}_{\theta} \boldsymbol{R}\left(\boldsymbol{R}^{\prime} \boldsymbol{V}_{\theta} \boldsymbol{R}\right)^{-1} \boldsymbol{R} \boldsymbol{V}_{\theta} \boldsymbol{G} .
\]</span></p>
<p>The plug-in estimator <span class="math inline">\(\widehat{B}=(\widehat{\mu}-\widetilde{\mu})^{2}\)</span> of <span class="math inline">\(B\)</span> is biased because</p>
<p><span class="math display">\[
\begin{aligned}
\mathbb{E}[\widehat{B}] &amp;=(\mathbb{E}[\widehat{\mu}-\widetilde{\mu}])^{2}+\operatorname{var}[\widehat{\mu}-\widetilde{\mu}] \\
&amp; \simeq B+\operatorname{var}[\widehat{\mu}]-\operatorname{var}[\widetilde{\mu}] \\
&amp; \simeq B+n^{-1} \boldsymbol{G}^{\prime} \boldsymbol{V}_{\theta} \boldsymbol{R}\left(\boldsymbol{R}^{\prime} \boldsymbol{V}_{\theta} \boldsymbol{R}\right)^{-1} \boldsymbol{R}_{\theta} \boldsymbol{G} .
\end{aligned}
\]</span></p>
<p>It follows that an approximately unbiased estimator for <span class="math inline">\(F\)</span> is</p>
<p><span class="math display">\[
\widehat{B}-2 n^{-1} \boldsymbol{G}^{\prime} \boldsymbol{V}_{\theta} \boldsymbol{R}\left(\boldsymbol{R}^{\prime} \boldsymbol{V}_{\theta} \boldsymbol{R}\right)^{-1} \boldsymbol{R} \boldsymbol{V}_{\theta} \boldsymbol{G} .
\]</span></p>
<p>The FIC is obtained by replacing the unknown <span class="math inline">\(\boldsymbol{G}, \boldsymbol{R}\)</span>, and <span class="math inline">\(n^{-1} \boldsymbol{V}_{\theta}\)</span> by estimates.</p>
</section>
<section id="best-subset-and-stepwise-regression" class="level2" data-number="26.20">
<h2 data-number="26.20" class="anchored" data-anchor-id="best-subset-and-stepwise-regression"><span class="header-section-number">26.20</span> Best Subset and Stepwise Regression</h2>
<p>Suppose that we have a set of potential regressors <span class="math inline">\(\left\{X_{1}, \ldots, X_{K}\right\}\)</span> and we want to select a subset of the regressors to use in a regression. Let <span class="math inline">\(S_{m}\)</span> denote a subset of the regressors, and let <span class="math inline">\(m=1, \ldots, M\)</span> denote the set of potential subsets. Given a model selection criterion (e.g.&nbsp;AIC, Mallows, or CV) the best subset model is the one which minimizes the criterion across the <span class="math inline">\(M\)</span> models. This is implemented by estimating the <span class="math inline">\(M\)</span> models and comparing the model selection criteria.</p>
<p>If <span class="math inline">\(K\)</span> is small it is computationally feasible to compare all subset models. However, when <span class="math inline">\(K\)</span> is large this may not be feasible. This is because the number of potential subsets is <span class="math inline">\(M=2^{K}\)</span> which increases quickly with <span class="math inline">\(K\)</span>. For example, <span class="math inline">\(K=10\)</span> implies <span class="math inline">\(M=1024, K=20\)</span> implies <span class="math inline">\(M \geq 1,000,000\)</span>, and <span class="math inline">\(K=40\)</span> implies <span class="math inline">\(M\)</span> exceeds one trillion. It simply does not make sense to estimate all subset regressions in such cases.</p>
<p>If the goal is to find the set of regressors which produces the smallest selection criterion it seems likely that we should be able to find an approximating set of regressors at much reduced computation cost. Some specific algorithms to implement this goal are as called stepwise, stagewise, and least angle regression. None of these procedures actually achieve the goal of minimizing any specific selection criterion; rather they are viewed as useful computational approximations. There is also some potential confusion as different authors seem to use the same terms for somewhat different implementations. We use the terms here as described in Hastie, Tibshirani, and Friedman (2008).</p>
<p>In the following descriptions we use <span class="math inline">\(\operatorname{SSE}(m)\)</span> to refer to the sum of squared residuals from a fitted model and <span class="math inline">\(C(m)\)</span> to refer to the selection criterion used for model comparison (AIC is most typically used).</p>
</section>
<section id="backward-stepwise-regression" class="level2" data-number="26.21">
<h2 data-number="26.21" class="anchored" data-anchor-id="backward-stepwise-regression"><span class="header-section-number">26.21</span> Backward Stepwise Regression</h2>
<ol type="1">
<li><p>Start with all regressors <span class="math inline">\(\left\{X_{1}, \ldots, X_{K}\right\}\)</span> included in the “active set”.</p></li>
<li><p>For <span class="math inline">\(m=0, \ldots, K-1\)</span></p></li>
</ol>
<ol type="a">
<li><p>Estimate the regression of <span class="math inline">\(Y\)</span> on the active set.</p></li>
<li><p>Identify the regressor whose omission will have the smallest impact on <span class="math inline">\(C(m)\)</span>.</p></li>
<li><p>Put this regressor in slot <span class="math inline">\(K-m\)</span> and delete from the active set.</p></li>
<li><p>Calculate <span class="math inline">\(C(m)\)</span> and store in slot <span class="math inline">\(K-m\)</span>.</p></li>
</ol>
<p> 1. The model with the smallest value of <span class="math inline">\(C(m)\)</span> is the selected model.</p>
<p>Backward stepwise regression requires <span class="math inline">\(K&lt;n\)</span> so that regression with all variables is feasible. It produces an ordering of the regressors from “most relevant” to “least relevant”. A simplified version is to exit the loop when <span class="math inline">\(C(m)\)</span> increases. (This may not yield the same result as completing the loop.) For the case of AIC selection, step (b) can be implemented by calculating the classical (homoskedastic) t-ratio for each active regressor and find the regressor with the smallest absolute t-ratio. (See Exercise 28.3.)</p>
</section>
<section id="forward-stepwise-regression" class="level2" data-number="26.22">
<h2 data-number="26.22" class="anchored" data-anchor-id="forward-stepwise-regression"><span class="header-section-number">26.22</span> Forward Stepwise Regression</h2>
<ol type="1">
<li><p>Start with the null set <span class="math inline">\(\{\varnothing\}\)</span> as the “active set” and all regressors <span class="math inline">\(\left\{X_{1}, \ldots, X_{K}\right\}\)</span> as the “inactive set”.</p></li>
<li><p>For <span class="math inline">\(m=1, \ldots, \min (n-1, K)\)</span></p></li>
</ol>
<ol type="a">
<li><p>Estimate the regression of <span class="math inline">\(Y\)</span> on the active set.</p></li>
<li><p>Identify the regressor in the inactive set whose inclusion will have the largest impact on <span class="math inline">\(C(m)\)</span>.</p></li>
<li><p>Put this regressor in slot <span class="math inline">\(m\)</span> and move it from the inactive to the active set.</p></li>
<li><p>Calculate <span class="math inline">\(C(m)\)</span> and store in slot <span class="math inline">\(m\)</span>.</p></li>
</ol>
<p> 1. The model with the smallest value of <span class="math inline">\(C(m)\)</span> is the selected model.</p>
<p>A simplified version is to exit the loop when <span class="math inline">\(C(m)\)</span> increases. (This may not yield the same answer as completing the loop.) For the case of AIC selection step (b) can be implemented by finding the regressor in the inactive set with the largest absolute correlation with the residual from step (a). (See Exercise 28.4.)</p>
<p>There are combined algorithms which check both forward and backward movements at each step. The algorithms can also be implemented with the regressors organized in groups (so that all elements are either included or excluded at each step). There are also old-fashioned versions which use significance testing rather than selection criterion (this is generally not advised).</p>
<p>Stepwise regression based on old-fashioned significance testing can be implemented in Stata using the stepwise command. If attention is confined to models which include regressors one-at-a-time, AIC selection can be implemented by setting the significance level equal to <span class="math inline">\(p=0.32\)</span>. Thus the command stepwise, <span class="math inline">\(\operatorname{pr}\)</span> (.32) implements backward stepwise regression with the AIC criterion, and stepwise, pe (.32) implements forward stepwise regression with the AIC criterion.</p>
<p>Stepwise regression can be implemented in R using the lars command.</p>
</section>
<section id="the-mse-of-model-selection-estimators" class="level2" data-number="26.23">
<h2 data-number="26.23" class="anchored" data-anchor-id="the-mse-of-model-selection-estimators"><span class="header-section-number">26.23</span> The MSE of Model Selection Estimators</h2>
<p>Model selection can lead to estimators with poor sampling performance. In this section we show that the mean squared error of estimation is not necessarily improved, and can be considerably worsened, by model selection.</p>
<p>To keep things simple consider an estimator with an exact normal distribution and known covariance matrix. Normalizing the latter to the identity we consider the setting</p>
<p><span class="math display">\[
\widehat{\theta} \sim \mathrm{N}\left(\theta, I_{K}\right)
\]</span></p>
<p>and the class of model selection estimators</p>
<p><span class="math display">\[
\widehat{\theta}_{\mathrm{pms}}=\left\{\begin{array}{lll}
\widehat{\theta} &amp; \text { if } &amp; \widehat{\theta}^{\prime} \widehat{\theta}&gt;c \\
0 &amp; \text { if } &amp; \widehat{\theta}^{\prime} \hat{\theta} \leq c
\end{array}\right.
\]</span></p>
<p>for some <span class="math inline">\(c\)</span>. AIC sets <span class="math inline">\(c=2 K\)</span>, BIC sets <span class="math inline">\(c=K \log (n)\)</span>, and <span class="math inline">\(5 %\)</span> significance testing sets <span class="math inline">\(c\)</span> to equal the <span class="math inline">\(95 %\)</span> quantile of the <span class="math inline">\(\chi_{K}^{2}\)</span> distribution. It is common to call <span class="math inline">\(\widehat{\theta}_{\mathrm{pms}}\)</span> a post-model-selection (PMS) estimator</p>
<p>We can explicitly calculate the MSE of <span class="math inline">\(\widehat{\theta}_{\mathrm{pms}}\)</span>.</p>
<p>Theorem 28.10 If <span class="math inline">\(\widehat{\theta} \sim \mathrm{N}\left(\theta, \boldsymbol{I}_{K}\right)\)</span> then</p>
<p><span class="math display">\[
\operatorname{mse}\left[\widehat{\theta}_{\mathrm{pms}}\right]=K+(2 \lambda-K) F_{K+2}(c, \lambda)-\lambda F_{K+4}(c, \lambda)
\]</span></p>
<p>where <span class="math inline">\(F_{r}(x, \lambda)\)</span> is the non-central chi-square distribution function with <span class="math inline">\(r\)</span> degrees of freedom and non-centrality parameter <span class="math inline">\(\lambda=\theta^{\prime} \theta\)</span>.</p>
<p>The proof is given in Section <span class="math inline">\(28.32\)</span>.</p>
<p>The MSE is determined only by <span class="math inline">\(K, \lambda\)</span>, and <span class="math inline">\(c . \lambda=\theta^{\prime} \theta\)</span> turns out to be an important parameter for the MSE. As the squared Euclidean length, it indexes the magnitude of the coefficient <span class="math inline">\(\theta\)</span>.</p>
<p>We can see the following limiting cases. If <span class="math inline">\(\lambda=0\)</span> then mse <span class="math inline">\(\left[\widehat{\theta}_{\mathrm{pms}}\right]=K\left(1-F_{K+2}(c, 0)\right)\)</span>. As <span class="math inline">\(\lambda \rightarrow \infty\)</span> then mse <span class="math inline">\(\left[\widehat{\theta}_{\mathrm{pms}}\right] \rightarrow K\)</span>. The unrestricted estimator obtains if <span class="math inline">\(c=0\)</span>, in which case mse <span class="math inline">\(\left[\widehat{\theta}_{\mathrm{pms}}\right]=K\)</span>. As <span class="math inline">\(c \rightarrow \infty\)</span>, mse <span class="math inline">\(\left[\widehat{\theta}_{\mathrm{pms}}\right] \rightarrow \lambda\)</span>. The latter fact implies that the PMS estimator based on the BIC has MSE <span class="math inline">\(\rightarrow \infty\)</span> as <span class="math inline">\(n \rightarrow \infty\)</span>.</p>
<p>Using Theorem <span class="math inline">\(28.10\)</span> we can numerically calculate the MSE. In Figure 28.1(a) and (b) we plot the MSE of a set of estimators for a range of values of <span class="math inline">\(\sqrt{\lambda}\)</span>. Panel (a) is for <span class="math inline">\(K=1\)</span>, panel (b) is for <span class="math inline">\(K=5\)</span>. Note that the MSE of the unselected estimator <span class="math inline">\(\widehat{\theta}\)</span> is invariant to <span class="math inline">\(\lambda\)</span>, so its MSE plot is a flat line at <span class="math inline">\(K\)</span>. The other estimators plotted are AIC selection ( <span class="math inline">\(c=2 K\)</span> ), 5% significance testing selection (chi-square critical value), and BIC selection <span class="math inline">\((c=K \log (n))\)</span> for <span class="math inline">\(n=200\)</span> and <span class="math inline">\(n=1000\)</span>.</p>
<p>In the plots you can see that the PMS estimators have lower MSE than the unselected estimator roughly for <span class="math inline">\(\lambda&lt;K\)</span> but higher MSE for <span class="math inline">\(\lambda&gt;K\)</span>. The AIC estimator has MSE which is least distorted from the unselected estimator, reaching a peak of about <span class="math inline">\(1.5\)</span> for <span class="math inline">\(K=1\)</span>. The BIC estimators, however, have very large MSE for larger values of <span class="math inline">\(\lambda\)</span>, and the distortion is growing as <span class="math inline">\(n\)</span> increases. The MSE of the selection estimators increases with <span class="math inline">\(\lambda\)</span> until it reaches a peak and then slowly decreases and asymptotes back to <span class="math inline">\(K\)</span>. Furthermore, the MSE of BIC is unbounded as <span class="math inline">\(n\)</span> diverges. Thus for very large sample sizes the MSE of a BIC-selected estimator can be a very large multiple of the MSE of the unselected estimator. The plots show that if <span class="math inline">\(\lambda\)</span> is small there are advantages to model selection as MSE can be greatly reduced. However if <span class="math inline">\(\lambda\)</span> is large then MSE can be greatly increased if BIC is used, and moderately increased if AIC is used. A sensible reading of the plots leads to the practical recommendation to not use the BIC for model selection, and use the AIC with care.</p>
<p><img src="images//2022_10_23_101d59f261a704807a3bg-23.jpg" class="img-fluid"></p>
<ol type="a">
<li>MSE, <span class="math inline">\(K=1\)</span></li>
</ol>
<p><img src="images//2022_10_23_101d59f261a704807a3bg-23(1).jpg" class="img-fluid"></p>
<ol start="2" type="a">
<li>MSE, <span class="math inline">\(K=5\)</span></li>
</ol>
<p>Figure 28.1: MSE of Post-Model-Selection Estimators</p>
<p>The numerical calculations show that MSE is reduced by selection when <span class="math inline">\(\lambda\)</span> is small but increased when <span class="math inline">\(\lambda\)</span> is moderately large. What does this mean in practice? <span class="math inline">\(\lambda\)</span> is small when <span class="math inline">\(\theta\)</span> is small which means the compared models are similar in terms of estimation accuracy. In these contexts model selection can be valuable as it helps select smaller models to improve precision. However when <span class="math inline">\(\lambda\)</span> is moderately large (which means that <span class="math inline">\(\theta\)</span> is moderately large) the smaller model has meaningful omitted variable bias, yet the selection criteria have difficulty detecting which model to use. The conservative BIC selection procedure tends to select the smaller model and thus incurs greater bias resulting in high MSE. These considerations suggest that it is better to use the AIC when selecting among models with similar estimation precision. Unfortunately it is impossible to known a priori the appropriate models.</p>
<p>The results of this section may appear to contradict Theorem <span class="math inline">\(28.8\)</span> which showed that the BIC is consistent for parsimonious models as for all <span class="math inline">\(\lambda&gt;0\)</span> in the plots the correct parsimonious model is the larger model. Yet BIC is not selecting this model with sufficient frequency to produce a low MSE. There is no contradiction. The consistency of the BIC appears in the lower portion of the plots where the MSE of the BIC estimator is very small, and approaching zero as <span class="math inline">\(\lambda \rightarrow 0\)</span>. The fact that the MSE of the AIC estimator somewhat exceeds that of the BIC in this region is due to the over-selection property of the AIC.</p>
</section>
<section id="inference-after-model-selection" class="level2" data-number="26.24">
<h2 data-number="26.24" class="anchored" data-anchor-id="inference-after-model-selection"><span class="header-section-number">26.24</span> Inference After Model Selection</h2>
<p>Economists are typically interested in inferential questions such as hypothesis tests and confidence intervals. If an econometric model has been selected by a procedure such as AIC or CV what are the properties of statistical tests applied to the selected model?</p>
<p>To be concrete, consider the regression model <span class="math inline">\(Y=X_{1} \beta_{1}+X_{2} \beta_{2}+e\)</span> and selection of the variable <span class="math inline">\(X_{2}\)</span>. That is, we compare <span class="math inline">\(Y=X_{1} \beta_{1}+e\)</span> with <span class="math inline">\(Y=X_{1} \beta_{1}+X_{2} \beta_{2}+e\)</span>. It is not too deep a realization that in this context it is inappropriate to conduct conventional inference for <span class="math inline">\(\beta_{2}\)</span> in the selected model. If we select the smaller model there is no estimate of <span class="math inline">\(\beta_{2}\)</span>. If we select the larger it is because the <span class="math inline">\(\mathrm{t}\)</span>-ratio for <span class="math inline">\(\beta_{2}\)</span> exceeds the critical value. The distribution of the t-ratio, conditional on exceeding a critical value, is not conventionally distributed and there seems little point to push this issue further.</p>
<p>The more interesting and subtle question is the impact on inference concerning <span class="math inline">\(\beta_{1}\)</span>. This indeed is a context of typical interest. An economist is interested in the impact of <span class="math inline">\(X_{1}\)</span> on <span class="math inline">\(Y\)</span> given a set of controls <span class="math inline">\(X_{2}\)</span>. It is common to select across these controls to find a suitable empirical model. Once this has been obtained we want to make inferential statements about <span class="math inline">\(\beta_{1}\)</span>. Has selection over the controls impacted inference?</p>
<p>We illustrate the issue numerically. Suppose that <span class="math inline">\(\left(X_{1}, X_{2}\right)\)</span> are jointly normal with unit variances and correlation <span class="math inline">\(\rho, e\)</span> is independent and standard normal, and <span class="math inline">\(n=30\)</span>. We estimate the long regression of <span class="math inline">\(Y\)</span> on <span class="math inline">\(\left(X_{1}, X_{2}\right)\)</span> and the short regression of <span class="math inline">\(Y\)</span> on <span class="math inline">\(X_{1}\)</span> alone. We construct the t-statistic <span class="math inline">\({ }^{3}\)</span> for <span class="math inline">\(\beta_{2}=0\)</span> in the long regression and select the long regression if the t-statistic is significant at the <span class="math inline">\(5 %\)</span> level and select the short regression if the <span class="math inline">\(\mathrm{t}\)</span>-statistic is not significant. We construct the standard <span class="math inline">\(95 %\)</span> confidence interval <span class="math inline">\({ }^{4}\)</span> for <span class="math inline">\(\beta_{1}\)</span> in the selected regression. These confidence intervals will have exact <span class="math inline">\(95 %\)</span> coverage when there is no selection and the estimated model is correct, so deviations from <span class="math inline">\(95 %\)</span> are due to model selection and misspecification. We calculate the actual coverage probability by simulation using one million replications, varying <span class="math inline">\({ }^{5} \beta_{2}\)</span> and <span class="math inline">\(\rho\)</span>.</p>
<p><img src="images//2022_10_23_101d59f261a704807a3bg-24.jpg" class="img-fluid"></p>
<p>Figure 28.2: Coverage Probability of Post-Model Selection</p>
<p>We display in Figure <span class="math inline">\(28.2\)</span> the coverage probabilities as a function of <span class="math inline">\(\beta_{2}\)</span> for several values of <span class="math inline">\(\rho\)</span>. If the</p>
<p><span class="math inline">\({ }^{3}\)</span> Using the homoskedastic variance formula and assuming the error variance is known. This is done to focus on the selection issue rather than covariance matrix estimation.</p>
<p><span class="math inline">\({ }^{4}\)</span> Using the homoskedastic variance formula and assuming the correct error variance is known.</p>
<p><span class="math inline">\({ }^{5}\)</span> The coverage probability is invariant to <span class="math inline">\(\beta_{1}\)</span>. regressors are uncorrelated <span class="math inline">\((\rho=0)\)</span> then the actual coverage probability equals the nominal level of <span class="math inline">\(0.95\)</span>. This is because the t-statistic for <span class="math inline">\(\beta_{2}\)</span> is independent of those for <span class="math inline">\(\beta_{1}\)</span> in this normal regression model and the coefficients on <span class="math inline">\(X_{1}\)</span> in the short and long regression are identical.</p>
<p>This invariance breaks down for <span class="math inline">\(\rho \neq 0\)</span>. As <span class="math inline">\(\rho\)</span> increases the coverage probability of the confidence intervals fall below the nominal level. The distortion is strongly affected by the value of <span class="math inline">\(\beta_{2}\)</span>. For <span class="math inline">\(\beta_{2}=0\)</span> the distortion is mild. The reason is that when <span class="math inline">\(\beta_{2}=0\)</span> the selection t-statistic selects the short regression with high probability (95%) which leads to approximately valid inference. Also, as <span class="math inline">\(\beta_{2} \rightarrow \infty\)</span> the coverage probability converges to the nominal level. The reason is that for large <span class="math inline">\(\beta_{2}\)</span> the selection t-statistic selects the long regression with high probability, again leading to approximately valid inference. The distortion is large, however, for intermediate values of <span class="math inline">\(\beta_{2}\)</span>. For <span class="math inline">\(\rho=0.5\)</span> the coverage probability falls to <span class="math inline">\(88 %\)</span>, and for <span class="math inline">\(\rho=0.8\)</span> the probability is low as <span class="math inline">\(62 %\)</span>. The reason is that for intermediate values of <span class="math inline">\(\beta_{2}\)</span> the selection <span class="math inline">\(\mathrm{t}\)</span>-statistic selects both models with meaningful probability, and this selection decision is correlated with the t-statistics for <span class="math inline">\(\beta_{1}\)</span>. The degree of under-coverage is enormous and greatly troubling.</p>
<p>The message from this display is that inference after model selection is problematic. Conventional inference procedures do not have conventional distributions and the distortions are potentially unbounded.</p>
</section>
<section id="empirical-illustration" class="level2" data-number="26.25">
<h2 data-number="26.25" class="anchored" data-anchor-id="empirical-illustration"><span class="header-section-number">26.25</span> Empirical Illustration</h2>
<p>We illustrate the model selection methods with an application. Take the CPS dataset and the subsample of Asian women which has <span class="math inline">\(n=1149\)</span> observations. Consider a log wage regression with primary interest on the return to experience measured as the percentage difference between expected wages between 0 and 30 years of experience. We consider and compare nine least squares regressions. All include an indicator for married and three indicators for the region. The estimated models range in complexity concerning the impact of education and experience.</p>
<p>Table 28.1: Estimates of Return to Experience among Asian Women</p>
<table class="table">
<colgroup>
<col style="width: 9%">
<col style="width: 6%">
<col style="width: 16%">
<col style="width: 6%">
<col style="width: 6%">
<col style="width: 14%">
<col style="width: 16%">
<col style="width: 6%">
<col style="width: 6%">
<col style="width: 6%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;"></th>
<th>Model 1</th>
<th>Model 2</th>
<th>Model 3</th>
<th>Model 4</th>
<th>Model 5</th>
<th>Model 6</th>
<th>Model 7</th>
<th>Model 8</th>
<th>Model 9</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Return</td>
<td><span class="math inline">\(13 %\)</span></td>
<td><span class="math inline">\(22 %\)</span></td>
<td><span class="math inline">\(20 %\)</span></td>
<td><span class="math inline">\(29 %\)</span></td>
<td><span class="math inline">\(40 %\)</span></td>
<td><span class="math inline">\(37 %\)</span></td>
<td><span class="math inline">\(33 %\)</span></td>
<td><span class="math inline">\(47 %\)</span></td>
<td><span class="math inline">\(45 %\)</span></td>
</tr>
<tr class="even">
<td style="text-align: left;">s.e.</td>
<td>7</td>
<td>8</td>
<td>7</td>
<td>11</td>
<td>11</td>
<td>11</td>
<td>17</td>
<td>18</td>
<td>17</td>
</tr>
<tr class="odd">
<td style="text-align: left;">BIC</td>
<td>956</td>
<td><span class="math inline">\(\mathbf{9 0 7}\)</span></td>
<td>924</td>
<td>964</td>
<td>913</td>
<td>931</td>
<td>977</td>
<td>925</td>
<td>943</td>
</tr>
<tr class="even">
<td style="text-align: left;">AIC</td>
<td>915</td>
<td>861</td>
<td>858</td>
<td>914</td>
<td>858</td>
<td><span class="math inline">\(\mathbf{8 5 5}\)</span></td>
<td>916</td>
<td>860</td>
<td>857</td>
</tr>
<tr class="odd">
<td style="text-align: left;">CV</td>
<td>405</td>
<td>387</td>
<td>386</td>
<td>405</td>
<td>385</td>
<td><span class="math inline">\(\mathbf{3 8 5}\)</span></td>
<td>406</td>
<td>387</td>
<td>386</td>
</tr>
<tr class="even">
<td style="text-align: left;">FIC</td>
<td>86</td>
<td>48</td>
<td>53</td>
<td>58</td>
<td><span class="math inline">\(\mathbf{3 2}\)</span></td>
<td>34</td>
<td>86</td>
<td>71</td>
<td>68</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Education</td>
<td>College</td>
<td>Spline</td>
<td>Dummy</td>
<td>College</td>
<td>Spline</td>
<td>Dummy</td>
<td>College</td>
<td>Spline</td>
<td>Dummy</td>
</tr>
<tr class="even">
<td style="text-align: left;">Experience</td>
<td>2</td>
<td>2</td>
<td>2</td>
<td>4</td>
<td>4</td>
<td>4</td>
<td>6</td>
<td>6</td>
<td>6</td>
</tr>
</tbody>
</table>
<p>Terms for experience:</p>
<ul>
<li><p>Models 1-3 include include experience and its square.</p></li>
<li><p>Models 4-6 include powers of experience up to power 4 .</p></li>
<li><p>Models 7-9 include powers of experience up to power 6 .</p></li>
</ul>
<p>Terms for education: - Models 1, 4, and 7 include a single dummy variable college indicating that years of education is 16 or higher.</p>
<ul>
<li><p>Models 2, 5, and 8 include a linear spline in education with a single knot at education=9.</p></li>
<li><p>Models 3, 6, and 9 include six dummy variables, for education equalling 12, 13, 14, 16, 18, and 20.</p></li>
</ul>
<p>Table <span class="math inline">\(28.1\)</span> reports key estimates from the nine models. Reported are the estimate of the return to experience as a percentage wage difference, its standard error (HC1), the BIC, AIC, CV, and FIC*, the latter treating the return to experience as the focus. What we can see is that the estimates vary meaningfully, ranging from <span class="math inline">\(13 %\)</span> to <span class="math inline">\(47 %\)</span>. Some of the estimates also have moderately large standard errors. (In most models the return to experience is “statistically significant”, but by large standard errors we mean that it is difficult to pin down the precise value of the return to experience.) We can also see that the most important factors impacting the magnitude of the point estimate is going beyond the quadratic specification for experience, and going beyond the simplest specification for education. Another item to notice is that the standard errors are most affected by the number of experience terms.</p>
<p>The BIC picks a parsimonious model with the linear spline in education and a quadratic in experience. The AIC and CV select a less parsimonious model with the full dummy specification for education and a <span class="math inline">\(4^{\text {th }}\)</span> order polynomial in experience. The FIC selects an intermediate model, with a linear spline in education and a <span class="math inline">\(4^{t h}\)</span> order polynomial in experience.</p>
<p>When selecting a model using information criteria it is useful to examine several criteria. In applications, decisions should be made by a combination of judgment as well as the formal criteria. In this case the cross-validation criterion selects model 6 which has the estimate <span class="math inline">\(37 %\)</span>, but near-similar values of the CV criterion are obtained by models 3 and 9 which have the estimates <span class="math inline">\(20 %\)</span> and <span class="math inline">\(45 %\)</span>. The FIC, which focuses on this specific coefficient, selects model 5 which has the point estimate <span class="math inline">\(40 %\)</span> which is similar to the CV-selected model. Overall based on this evidence the CV-selected model and its point estimate of <span class="math inline">\(37 %\)</span> seems an appropriate choice. However, the uncertainty reflected by the flatness of the CV criterion suggests that uncertainty remains in the choice of specification.</p>
</section>
<section id="shrinkage-methods" class="level2" data-number="26.26">
<h2 data-number="26.26" class="anchored" data-anchor-id="shrinkage-methods"><span class="header-section-number">26.26</span> Shrinkage Methods</h2>
<p>Shrinkage methods are a broad class of estimators which reduce variance by moving an estimator <span class="math inline">\(\hat{\theta}\)</span> towards a pre-selected point such as the zero vector. In high dimensions the reduction in variance more than compensates for the increase in bias resulting in improved efficiency when measured by mean squared error. This and the next few sections review material presented in Chapter 15 of Probability and Statistics for Economists.</p>
<p>The simplest shrinkage estimator takes the form <span class="math inline">\(\widetilde{\theta}=(1-w) \widehat{\theta}\)</span> for some shrinkage weight <span class="math inline">\(w \in[0,1]\)</span>. Setting <span class="math inline">\(w=0\)</span> we obtain <span class="math inline">\(\widetilde{\theta}=\widehat{\theta}\)</span> (no shrinkage) and setting <span class="math inline">\(w=1\)</span> we obtain <span class="math inline">\(\widetilde{\theta}=0\)</span> (full shrinkage). It is straightforward to calculate the MSE of this estimator. Assume <span class="math inline">\(\widehat{\theta} \sim(\theta, V)\)</span>. Then <span class="math inline">\(\widetilde{\theta}\)</span> has bias</p>
<p><span class="math display">\[
\operatorname{bias}[\widetilde{\theta}]=\mathbb{E}[\widetilde{\theta}]-\theta=-w \theta,
\]</span></p>
<p>variance</p>
<p><span class="math display">\[
\operatorname{var}[\widetilde{\theta}]=(1-w)^{2} \boldsymbol{V},
\]</span></p>
<p>and weighted mean squared error (using the weight matrix <span class="math inline">\(\boldsymbol{W}=\boldsymbol{V}^{-1}\)</span> )</p>
<p><span class="math display">\[
\text { wmse }[\widetilde{\theta}]=K(1-w)^{2}+w^{2} \lambda
\]</span></p>
<p>where <span class="math inline">\(\lambda=\theta^{\prime} \boldsymbol{V}^{-1} \theta\)</span>. Theorem 28.11 If <span class="math inline">\(\widehat{\theta} \sim(\theta, V)\)</span> and <span class="math inline">\(\widetilde{\theta}=(1-w) \widehat{\theta}\)</span> then</p>
<ol type="1">
<li><p>wmse <span class="math inline">\([\widetilde{\theta}]&lt;\)</span> wmse <span class="math inline">\([\hat{\theta}]\)</span> if <span class="math inline">\(0&lt;w&lt;2 K /(K+\lambda)\)</span>.</p></li>
<li><p>wmse <span class="math inline">\([\widetilde{\theta}]\)</span> is minimized by the shrinkage weight <span class="math inline">\(w_{0}=K /(K+\lambda)\)</span>.</p></li>
<li><p>The minimized WMSE is wmse <span class="math inline">\([\widetilde{\theta}]=K \lambda /(K+\lambda)\)</span>.</p></li>
</ol>
<p>For the proof see Exercise <span class="math inline">\(28.6\)</span>.</p>
<p>Part 1 of the theorem shows that the shrinkage estimator has reduced WMSE for a range of values of the shrinkage weight <span class="math inline">\(w\)</span>. Part 2 of the theorem shows that the WMSE-minimizing shrinkage weight is a simple function of <span class="math inline">\(K\)</span> and <span class="math inline">\(\lambda\)</span>. The latter is a measure of the magnitude of <span class="math inline">\(\theta\)</span> relative to the estimation variance. When <span class="math inline">\(\lambda\)</span> is large (the coefficients are large) then the optimal shrinkage weight <span class="math inline">\(w_{0}\)</span> is small; when <span class="math inline">\(\lambda\)</span> is small (the coefficients are small) then the optimal shrinkage weight <span class="math inline">\(w_{0}\)</span> is large. Part 3 calculates the associated optimal WMSE. This can be substantially less than the WMSE of the original estimator <span class="math inline">\(\widehat{\theta}\)</span>. For example, if <span class="math inline">\(\lambda=K\)</span> then wmse <span class="math inline">\([\widetilde{\theta}]=K / 2\)</span>, one-half the WMSE of the original estimator.</p>
<p>To construct the optimal shrinkage weight we need the unknown <span class="math inline">\(\lambda\)</span>. An unbiased estimator is <span class="math inline">\(\hat{\lambda}=\)</span> <span class="math inline">\(\widehat{\theta}^{\prime} \boldsymbol{V}^{-1} \widehat{\theta}-K\)</span> (see Exercise 28.7) implying the shrinkage weight</p>
<p><span class="math display">\[
\widehat{w}=\frac{K}{\widehat{\theta}^{\prime} \boldsymbol{V}^{-1} \widehat{\theta}} .
\]</span></p>
<p>Replacing <span class="math inline">\(K\)</span> with a free parameter <span class="math inline">\(c\)</span> (which we call the shrinkage coefficient) we obtain</p>
<p><span class="math display">\[
\widetilde{\theta}=\left(1-\frac{c}{\widehat{\theta}^{\prime} \boldsymbol{V}^{-1} \widehat{\theta}}\right) \widehat{\theta} .
\]</span></p>
<p>This is often called a Stein-Rule estimator.</p>
<p>This estimator has many appealing properties. It can be viewed as a smoothed selection estimator. The quantity <span class="math inline">\(\widehat{\theta}^{\prime} \boldsymbol{V}^{-1} \widehat{\theta}\)</span> is a Wald statistic for the hypothesis <span class="math inline">\(\mathbb{H}_{0}: \theta=0\)</span>. Thus when this Wald statistic is large (when the evidence suggests the hypothesis of a zero coefficient is false) the shrinkage estimator is close to the original estimator <span class="math inline">\(\widehat{\theta}\)</span>. However when this Wald statistic is small (when the evidence is consistent with the hypothesis of a zero coefficient) then the shrinkage estimator moves the original estimator towards zero.</p>
</section>
<section id="james-stein-shrinkage-estimator" class="level2" data-number="26.27">
<h2 data-number="26.27" class="anchored" data-anchor-id="james-stein-shrinkage-estimator"><span class="header-section-number">26.27</span> James-Stein Shrinkage Estimator</h2>
<p>James and Stein (1961) made the following discovery.</p>
<p>Theorem 28.12 Assume that <span class="math inline">\(\widehat{\theta} \sim \mathrm{N}(\theta, V), \widetilde{\theta}\)</span> is defined in (28.25), and <span class="math inline">\(K&gt;2\)</span>.</p>
<ol type="1">
<li><p>If <span class="math inline">\(0&lt;c&lt;2(K-2)\)</span> then wmse <span class="math inline">\([\widetilde{\theta}]&lt;\)</span> wmse <span class="math inline">\([\widehat{\theta}]\)</span>.</p></li>
<li><p>The WMSE is minimized by setting <span class="math inline">\(c=K-2\)</span> and equals</p></li>
</ol>
<p><span class="math display">\[
\text { wmse }[\widetilde{\theta}]=K-(K-2)^{2} \mathbb{E}\left[Q_{K}^{-1}\right]
\]</span></p>
<p>where <span class="math inline">\(Q_{K} \sim \chi_{K}^{2}(\lambda)\)</span>. See Theorem <span class="math inline">\(15.3\)</span> of Probability and Statistics for Economists.</p>
<p>This result stunned the world of statistics. Part 1 shows that the shrinkage estimator has strictly smaller WMSE for all values of the parameters and thus dominates the original estimator. The latter is the MLE so this result shows that the MLE is dominated and thus inadmissible. This is a stunning result because it had previously been assumed that it would be impossible to find an estimator which dominates the MLE.</p>
<p>Theorem <span class="math inline">\(28.12\)</span> critically depends on the condition <span class="math inline">\(K&gt;2\)</span>. This means that shrinkage achieves uniform improvements only in dimensions three or larger.</p>
<p>The minimizing choice for the shrinkage coefficient <span class="math inline">\(c=K-2\)</span> leads to what is commonly known as the James-Stein estimator</p>
<p><span class="math display">\[
\widetilde{\theta}=\left(1-\frac{K-2}{\widehat{\theta}^{\prime} \boldsymbol{V}^{-1} \widehat{\theta}}\right) \widehat{\theta} .
\]</span></p>
<p>In practice <span class="math inline">\(\boldsymbol{V}\)</span> is unknown so we substitute an estimator <span class="math inline">\(\widehat{\boldsymbol{V}}\)</span>. This leads to</p>
<p><span class="math display">\[
\widetilde{\theta}_{\mathrm{JS}}=\left(1-\frac{K-2}{\widehat{\theta}^{\prime} \widehat{\boldsymbol{V}}^{-1} \widehat{\theta}}\right) \widehat{\theta}
\]</span></p>
<p>which is fully feasible as it does not depend on unknowns or tuning parameters. The substitution of <span class="math inline">\(\widehat{\boldsymbol{V}}\)</span> for <span class="math inline">\(\boldsymbol{V}\)</span> can be justified by finite sample or asymptotic arguments.</p>
</section>
<section id="interpretation-of-the-stein-effect" class="level2" data-number="26.28">
<h2 data-number="26.28" class="anchored" data-anchor-id="interpretation-of-the-stein-effect"><span class="header-section-number">26.28</span> Interpretation of the Stein Effect</h2>
<p>The James-Stein Theorem appears to conflict with classical statistical theory. The original estimator <span class="math inline">\(\widehat{\theta}\)</span> is the maximum likelihood estimator. It is unbiased. It is minimum variance unbiased. It is CramerRao efficient. How can it be that the James-Stein shrinkage estimator achieves uniformly smaller mean squared error?</p>
<p>Part of the answer is that classical theory has caveats. The Cramer-Rao Theorem, for example, restricts attention to unbiased estimators and thus precludes consideration of shrinkage estimators. The James-Stein estimator has reduced MSE, but is not Cramer-Rao efficient because it is biased. Therefore the James-Stein Theorem does not conflict with the Cramer-Rao Theorem. Rather, they are complementary results. On the one hand, the Cramer-Rao Theorem describes the best possible variance when unbiasedness is an important property for estimation. On the other hand, the James-Stein Theorem shows that if unbiasedness is not a critical property but instead MSE is important, then there are better estimators than the MLE.</p>
<p>The James-Stein Theorem may also appear to conflict with our results from Section <span class="math inline">\(28.16\)</span> which showed that selection estimators do not achieve uniform MSE improvements over the MLE. This may appear to be a conflict because the James-Stein estimator has a similar form to a selection estimator. The difference is that selection estimators are hard threshold rules - they are discontinuous functions of the data - while the James-Stein estimator is a soft threshold rule - it is a continuous function of the data. Hard thresholding tends to result in high variance; soft thresholding tends to result in low variance. The James-Stein estimator is able to achieve reduced variance because it is a soft threshold function.</p>
<p>The MSE improvements achieved by the James-Stein estimator are greatest when <span class="math inline">\(\lambda\)</span> is small. This occurs when the parameters <span class="math inline">\(\theta\)</span> are small in magnitude relative to the estimation variance <span class="math inline">\(\boldsymbol{V}\)</span>. This means that the user needs to choose the centering point wisely.</p>
</section>
<section id="positive-part-estimator" class="level2" data-number="26.29">
<h2 data-number="26.29" class="anchored" data-anchor-id="positive-part-estimator"><span class="header-section-number">26.29</span> Positive Part Estimator</h2>
<p>The simple James-Stein estimator has the odd property that it can “over-shrink”. When <span class="math inline">\(\widehat{\theta}^{\prime} \boldsymbol{V}^{-1} \widehat{\theta}&lt;K-2\)</span> then <span class="math inline">\(\widetilde{\theta}\)</span> has the opposite sign from <span class="math inline">\(\widehat{\theta}\)</span>. This does not make sense and suggests that further improvements can be made. The standard solution is to use “positive-part” trimming by bounding the shrinkage weight (28.24) between zero and one. This estimator can be written as</p>
<p><span class="math display">\[
\begin{aligned}
\widetilde{\theta}^{+} &amp;=\left\{\begin{array}{cc}
\widetilde{\theta}, &amp; \widehat{\theta}^{\prime} \boldsymbol{V}^{-1} \widehat{\theta} \geq K-2 \\
0, &amp; \widehat{\theta}^{\prime} \boldsymbol{V}^{-1} \widehat{\theta}&lt;K-2
\end{array}\right.\\
&amp;=\left(1-\frac{K-2}{\hat{\theta}^{\prime} \boldsymbol{V}^{-1} \widehat{\theta}}\right)_{+} \widehat{\theta}
\end{aligned}
\]</span></p>
<p>where <span class="math inline">\((a)_{+}=\max [a, 0]\)</span> is the “positive-part” function. Alternatively, it can be written as</p>
<p><span class="math display">\[
\widetilde{\theta}^{+}=\widehat{\theta}-\left(\frac{K-2}{\widehat{\theta}^{\prime} \boldsymbol{V}^{-1} \widehat{\theta}}\right)_{1} \widehat{\theta}
\]</span></p>
<p>where <span class="math inline">\((a)_{1}=\min [a, 1]\)</span></p>
<p>The positive part estimator simultaneously performs “selection” as well as “shrinkage”. If <span class="math inline">\(\widehat{\theta}^{\prime} \boldsymbol{V}^{-1} \widehat{\theta}\)</span> is sufficiently small, <span class="math inline">\(\widetilde{\theta}^{+}\)</span>“selects” 0 . When <span class="math inline">\(\widehat{\theta}^{\prime} \boldsymbol{V}^{-1} \widehat{\theta}\)</span> is of moderate size, <span class="math inline">\(\widetilde{\theta}^{+}\)</span>shrinks <span class="math inline">\(\widehat{\theta}\)</span> towards zero. When <span class="math inline">\(\widehat{\theta}^{\prime} \boldsymbol{V}^{-1} \widehat{\theta}\)</span> is very large, <span class="math inline">\(\widetilde{\theta}^{+}\)</span>is close to the original estimator <span class="math inline">\(\widehat{\theta}\)</span>.</p>
<p>Consistent with our intuition the positive part estimator has uniformly lower WMSE than the unadjusted James-Stein estimator.</p>
<p>Theorem 28.13 Under the assumptions of Theorem <span class="math inline">\(28.12\)</span></p>
<p><span class="math display">\[
\operatorname{wmse}\left[\widetilde{\theta}^{+}\right]&lt;\operatorname{wmse}[\tilde{\theta}] .
\]</span></p>
<p>For a proof see Theorem <span class="math inline">\(15.6\)</span> of Probability and Statistics for Economists. Theorem <span class="math inline">\(15.7\)</span> of Probability and Statistics for Economists provides an explicit numerical evaluation of the MSE for the positive-part estimator.</p>
<p>In Figure <span class="math inline">\(28.3\)</span> we plot wmse <span class="math inline">\(\left[\widetilde{\theta}^{+}\right] / K\)</span> as a function of <span class="math inline">\(\lambda / K\)</span> for <span class="math inline">\(K=4,6,12\)</span>, and 48 . The plots are uniformly below 1 (the normalized WMSE of the MLE) and substantially so for small and moderate values of <span class="math inline">\(\lambda\)</span>. The WMSE functions fall as <span class="math inline">\(K\)</span> increases, demonstrating that the MSE reductions are more substantial when <span class="math inline">\(K\)</span> is large.</p>
<p>In summary, the positive-part transformation is an important improvement over the unadjusted James-Stein estimator. It is more reasonable and reduces the mean squared error. The broader message is that imposing boundary conditions on shrinkage weights can improve estimation efficiency.</p>
</section>
<section id="shrinkage-towards-restrictions" class="level2" data-number="26.30">
<h2 data-number="26.30" class="anchored" data-anchor-id="shrinkage-towards-restrictions"><span class="header-section-number">26.30</span> Shrinkage Towards Restrictions</h2>
<p>The classical James-Stein estimator does not have direct use in applications because it is rare that we wish to shrink an entire parameter vector towards a specific point. Rather, it is more common to shrink a parameter vector towards a set of restrictions. Here are a few examples:</p>
<ol type="1">
<li>Shrink a long regression towards a short regression.</li>
</ol>
<p><img src="images//2022_10_23_101d59f261a704807a3bg-30.jpg" class="img-fluid"></p>
<p>Figure 28.3: WMSE of James-Stein Estimator</p>
<p> 1. Shrink a regression towards an intercept-only model.</p>
<ol start="2" type="1">
<li><p>Shrink the regression coefficients towards a set of restrictions.</p></li>
<li><p>Shrink a set of estimates (or coefficients) towards their common mean.</p></li>
<li><p>Shrink a set of estimates (or coefficients) towards a parametric model.</p></li>
<li><p>Shrink a nonparametric series model towards a parametric model.</p></li>
</ol>
<p>The way to think generally about these applications it that the researcher wants to allow for generality with the large model but believes that the smaller model may be a useful approximation. A shrinkage estimator allows the data to smoothly select between these two options depending on the strength of information for the two specifications.</p>
<p>Let <span class="math inline">\(\widehat{\theta} \sim \mathrm{N}(\theta, \boldsymbol{V})\)</span> be the original estimator, for example a set of regression coefficient estimates. The normality assumption is used for the exact theory but can be justified based on an asymtotic approximation as well. The researcher considers a set of <span class="math inline">\(q&gt;2\)</span> linear restrictions which can be written as <span class="math inline">\(\boldsymbol{R}^{\prime} \theta=\boldsymbol{r}\)</span> where <span class="math inline">\(\boldsymbol{R}\)</span> is <span class="math inline">\(K \times q\)</span> and <span class="math inline">\(\boldsymbol{r}\)</span> is <span class="math inline">\(q \times 1\)</span>. A minimum distance estimator for <span class="math inline">\(\theta\)</span> is</p>
<p><span class="math display">\[
\widehat{\theta}_{\boldsymbol{R}}=\widehat{\theta}-\boldsymbol{V} \boldsymbol{R}\left(\boldsymbol{R}^{\prime} \boldsymbol{V} \boldsymbol{R}\right)^{-1}\left(\boldsymbol{R}^{\prime} \widehat{\theta}-\boldsymbol{r}\right) .
\]</span></p>
<p>The James-Stein estimator with positive-part trimming is</p>
<p><span class="math display">\[
\widetilde{\theta}^{+}=\widehat{\theta}-\left(\frac{q-2}{\left(\widehat{\theta}-\widehat{\theta}_{\boldsymbol{R}}\right)^{\prime} \boldsymbol{V}^{-1}\left(\widehat{\theta}-\widehat{\theta}_{\boldsymbol{R}}\right)}\right)_{1}\left(\widehat{\theta}-\widehat{\theta}_{\boldsymbol{R}}\right) .
\]</span></p>
<p>The function <span class="math inline">\((a)_{1}=\min [a, 1]\)</span> bounds the shrinkage weight below one.</p>
<p>Theorem 28.14 Under the assumptions of Theorem 28.12, if <span class="math inline">\(q&gt;2\)</span> then</p>
<p><span class="math display">\[
\operatorname{wmse}\left[\widetilde{\theta}^{+}\right]&lt;\operatorname{wmse}[\widetilde{\theta}] .
\]</span></p>
<p>The shrinkage estimator achieves uniformly smaller MSE if the number of restrictions is three or greater. The number of restrictions <span class="math inline">\(q\)</span> plays the same role as the number of parameters <span class="math inline">\(K\)</span> in the classical James-Stein estimator. Shrinkage achieves greater gains when there are more restrictions <span class="math inline">\(q\)</span>, and achieves greater gains when the restrictions are close to being satisfied in the population. If the imposed restrictions are far from satisfied then the shrinkage estimator will have similar performance as the original estimator. It is therefore important to select the restrictions carefully.</p>
<p>In practice the covariance matrix <span class="math inline">\(\boldsymbol{V}\)</span> is unknown so it is replaced by an estimator <span class="math inline">\(\widehat{\boldsymbol{V}}\)</span>. Thus the feasible version of the estimators equal</p>
<p><span class="math display">\[
\widehat{\theta}_{\boldsymbol{R}}=\widehat{\theta}-\widehat{\boldsymbol{V}} \boldsymbol{R}\left(\boldsymbol{R}^{\prime} \widehat{\boldsymbol{V}} \boldsymbol{R}\right)^{-1}\left(\boldsymbol{R}^{\prime} \widehat{\theta}-\boldsymbol{r}\right)
\]</span></p>
<p>and</p>
<p><span class="math display">\[
\widetilde{\theta}^{+}=\widehat{\theta}-\left(\frac{q-2}{J}\right)_{1}\left(\widehat{\theta}-\widehat{\theta}_{\boldsymbol{R}}\right)
\]</span></p>
<p>where</p>
<p><span class="math display">\[
J=\left(\widehat{\theta}-\widehat{\theta}_{\boldsymbol{R}}\right)^{\prime} \widehat{\boldsymbol{V}}^{-1}\left(\widehat{\theta}-\widehat{\theta}_{\boldsymbol{R}}\right) .
\]</span></p>
<p>It is insightful to notice that <span class="math inline">\(J\)</span> is the minimum distance statistic for the test of the hypothesis <span class="math inline">\(\mathbb{H}_{0}\)</span> : <span class="math inline">\(\boldsymbol{R}^{\prime} \theta=\boldsymbol{r}\)</span> against <span class="math inline">\(\mathbb{H}_{1}: \boldsymbol{R}^{\prime} \theta \neq \boldsymbol{r}\)</span>. Thus the degree of shrinkage is a smoothed version of the standard test of the restrictions. When <span class="math inline">\(J\)</span> is large (so the evidence indicates that the restrictions are false) the shrinkage estimator is close to the unrestricted estimator <span class="math inline">\(\widehat{\theta}\)</span>. When <span class="math inline">\(J\)</span> is small (so the evidence indicates that the restrictions could be correct) the shrinkage estimator equals the restricted estimator <span class="math inline">\(\widehat{\theta}_{\boldsymbol{R}}\)</span>. For intermediate values of <span class="math inline">\(J\)</span> the shrinkage estimator shrinks <span class="math inline">\(\widehat{\theta}\)</span> towards <span class="math inline">\(\widehat{\theta}_{\boldsymbol{R}}\)</span>.</p>
<p>We can substitute for <span class="math inline">\(J\)</span> any similar asymptotically chi-square statistic, including the Wald, Likelihood Ratio, and Score statistics. We can also use the F statistic (which is commonly produced by statistical software) if we multiply by <span class="math inline">\(q\)</span>. These substitutions do not produce the same exact finite sample distribution but are asymptotically equivalent.</p>
<p>In linear regression we have some very convenient simplifications available. In general, <span class="math inline">\(\widehat{\boldsymbol{V}}\)</span> can be a heteroskedastic-robust or cluster-robust covariance matrix estimator. However, if the dimension <span class="math inline">\(K\)</span> of the unrestricted estimator is quite large or has sparse dummy variables then these covariance matrix estimators are ill-behaved and it may be better to use a classical covariance matrix estimator to perform the shrinkage. If this is done then <span class="math inline">\(\widehat{\boldsymbol{V}}=\left(\boldsymbol{X}^{\prime} \boldsymbol{X}\right)^{-1} s^{2}, \widehat{\theta}_{\boldsymbol{R}}\)</span> is the constrained least squares estimator (in most applications the least squares estimator of the short regression) and <span class="math inline">\(J\)</span> is a conventional (homoskedastic) Wald statistic for a test of the restrictions. We can write the latter in F statistic form</p>
<p><span class="math display">\[
J=\frac{n\left(\widehat{\sigma}_{R}^{2}-\widehat{\sigma}^{2}\right)}{s^{2}}
\]</span></p>
<p>where <span class="math inline">\(\widehat{\sigma}_{R}^{2}\)</span> and <span class="math inline">\(\widehat{\sigma}^{2}\)</span> are the least squares error variance estimators from the restricted and unrestricted models. The shrinkage weight <span class="math inline">\(\left((q-2) / J_{1}\right.\)</span> can be easily calculated from standard regression output.</p>
</section>
<section id="group-james-stein" class="level2" data-number="26.31">
<h2 data-number="26.31" class="anchored" data-anchor-id="group-james-stein"><span class="header-section-number">26.31</span> Group James-Stein</h2>
<p>The James-Stein estimator can be applied to groups (blocks) of parameters. Suppose we have the parameter vector <span class="math inline">\(\theta=\left(\theta_{1}, \theta_{2}, \ldots, \theta_{G}\right)\)</span> partitioned into <span class="math inline">\(G\)</span> groups each of dimension <span class="math inline">\(K_{g} \geq 3\)</span>. We have a standard estimator <span class="math inline">\(\widehat{\theta}=\left(\widehat{\theta}_{1}, \widehat{\theta}_{2}, \ldots, \widehat{\theta}_{G}\right)\)</span> (for example, least squares regression or MLE) with covariance matrix <span class="math inline">\(\boldsymbol{V}\)</span>. The group James-Stein estimator is</p>
<p><span class="math display">\[
\begin{aligned}
\widetilde{\theta} &amp;=\left(\widetilde{\theta}_{1}, \widetilde{\theta}_{2}, \ldots, \widetilde{\theta}_{G}\right) \\
\widetilde{\theta}_{g} &amp;=\widehat{\theta}_{g}\left(1-\frac{K_{g}-2}{\hat{\theta}_{g}^{\prime} \boldsymbol{V}_{g}^{-1} \widehat{\theta}_{g}}\right)_{+}
\end{aligned}
\]</span></p>
<p>where <span class="math inline">\(\boldsymbol{V}_{g}\)</span> is the <span class="math inline">\(g^{t h}\)</span> diagonal block of <span class="math inline">\(\boldsymbol{V}\)</span>. A feasible version of the estimator replaces <span class="math inline">\(\boldsymbol{V}\)</span> with <span class="math inline">\(\widehat{\boldsymbol{V}}\)</span> and <span class="math inline">\(\boldsymbol{V}_{g}\)</span> with <span class="math inline">\(\widehat{V}_{g}\)</span>.</p>
<p>The group James-Stein estimator separately shrinks each block of coefficients. The advantage relative to the classical James-Stein estimator is that this allows the shrinkage weight to vary across blocks. Some parameter blocks can use a large amount of shrinkage while others a minimal amount. Since the positive-part trimming is used the estimator simultaneously performs shrinkage and selection. Blocks with small effects will be shrunk to zero and eliminated. The disadvantage of the estimator is that the benefits of shrinkage may be reduced because the shrinkage dimension is reduced. The trade-off between these factors will depend on how heterogeneous the optimal shrinkage weight varies across the parameters.</p>
<p>The groups should be selected based on two criteria. First, they should be selected so that the groups separate variables by expected amount of shrinkage. Thus coefficients which are expected to be “large” relative to their estimation variance should be grouped together and coefficients which are expected to be “small” should be grouped together. This will allow the estimated shrinkage weights to vary according to the group. For example, a researcher may expect high-order coefficients in a polynomial regression to be small relative to their estimation variance. Hence it is appropriate to group the polynomial variables into “low order” and “high order”. Second, the groups should be selected so that the researcher’s loss (utility) is separable across groups of coefficients. This is because the optimality theory (given below) relies on the assumption that the loss is separable. To understand the implications of these recommendations consider a wage regression. Our interpretation of the education and experience coefficients are separable if we use them for separate purposes, such as for estimation of the return to education and the return to experience. In this case it is appropriate to separate the education and experience coefficients into different groups.</p>
<p>For an optimality theory we define weighted MSE with respect to the block-diagonal weight matrix <span class="math inline">\(\boldsymbol{W}=\operatorname{diag}\left(\boldsymbol{V}_{1}^{-1}, \ldots, \boldsymbol{V}_{G}^{-1}\right)\)</span></p>
<p>Theorem 28.15 Under the assumptions of Theorem 28.12, if WMSE is defined with respect to <span class="math inline">\(\boldsymbol{W}=\operatorname{diag}\left(\boldsymbol{V}_{1}^{-1}, \ldots, \boldsymbol{V}_{G}^{-1}\right)\)</span> and <span class="math inline">\(K_{g}&gt;2\)</span> for all <span class="math inline">\(g=1, \ldots, G\)</span> then</p>
<p><span class="math display">\[
\operatorname{wmse}[\widetilde{\theta}]&lt;\operatorname{wmse}[\widehat{\theta}] \text {. }
\]</span></p>
<p>The proof is a simple extension of the classical James-Stein theory. The block diagonal structure of <span class="math inline">\(W\)</span> means that the WMSE is the sum of the WMSE of each group. The classical James-Stein theory can be applied to each group finding that the WMSE is reduced by shrinkage group-by-group. Thus the total WMSE is reduced by shrinkage.</p>
</section>
<section id="empirical-illustrations" class="level2" data-number="26.32">
<h2 data-number="26.32" class="anchored" data-anchor-id="empirical-illustrations"><span class="header-section-number">26.32</span> Empirical Illustrations</h2>
<p>We illustrate James-Stein shrinkage with three empirical applications.</p>
<p>The first application is to the sample used in Section 28.18, the CPS dataset with the subsample of Asian women ( <span class="math inline">\(n=1149\)</span> ) focusing on the return to experience profile. We consider shrinkage of Model 9 ( <span class="math inline">\(6^{t h}\)</span> order polynomial in experience) towards Model 3 ( <span class="math inline">\(2^{\text {nd }}\)</span> order polynomial in experience). The difference in the number of estimated coefficients is 4 . We set <span class="math inline">\(\widehat{\boldsymbol{V}}\)</span> to equal the <span class="math inline">\(\mathrm{HCl}\)</span> covariance matrix estimator. The empirically-determined shrinkage weight is <span class="math inline">\(0.46\)</span>, meaning that the Stein Rule estimator is approximately an equal weighted average of the estimates from the two models. The estimated experience profiles are displayed in Figure 28.4(a).</p>
<p><img src="images//2022_10_23_101d59f261a704807a3bg-33.jpg" class="img-fluid"></p>
<ol type="a">
<li>Experience Profile</li>
</ol>
<p><img src="images//2022_10_23_101d59f261a704807a3bg-33(1).jpg" class="img-fluid"></p>
<ol start="2" type="a">
<li>Firm Effects</li>
</ol>
<p>Figure 28.4: Shrinkage Illustrations</p>
<p>The two least squares estimates are visually distinct. The <span class="math inline">\(6^{t h}\)</span> order polynomial (Model 9) shows a steep return to experience for the first 10 years, then a wobbly experience profile up to 40 years, and declining above that. It also shows a dip around 25 years. The quadratic specification misses some of these features. The James-Stein estimator is essentially an average of the two profiles. It retains most features of the quartic specification, except that it smooths out the unappealing 25-year dip.</p>
<p>The second application is to the Invest1993 data set used in Chapter 17. This is a panel data set of annual observations on investment decisions by corporations. We focus on the firm-specific effects. These are of interest when studying firm heterogeneity and are important for firm-specific forecasting. Accurate estimation of firm effects is challenging when the number of time series observations per firm is small.</p>
<p>To keep the analysis focused we restrict attention to firms which are traded on either the NYSE or AMEX and to the last ten years of the sample (1982-1991). Since the regressors are lagged this means that there are at most nine time-series observations per firm. The sample has a total of <span class="math inline">\(N=786\)</span> firms and <span class="math inline">\(n=5692\)</span> observations for estimation. Our baseline model is the two-way fixed effects linear regression as reported in the fourth column of Table 17.2. Our restricted model replaces the firm fixed effects with 19 industry-specific dummy variables. This is similar to the first column of Table <span class="math inline">\(17.2\)</span> except that the trading dummy is omitted and time dummies are added. The Stein Rule estimator thus shrinks the fixed effects model towards the industry effects model. The latter will do well if most of the fixed effects are explained by industry rather than firm-specific variation.</p>
<p>Due to the large number of estimated coefficients in the unrestricted model we use the homoskedastic weight matrix as a simplification. This allows the calculation of the shrinkage weight using the simple formula (28.28) for the statistic <span class="math inline">\(J\)</span>. The heteroskedastic covariance matrix is not appropriate and the cluster-robust covariance matrix will not be reliable due to the sparse dummy specification.</p>
<p>The empirically-determined shrinkage weight is <span class="math inline">\(0.35\)</span> which means that the Stein Rule estimator puts about <span class="math inline">\(1 / 3\)</span> weight on the industry-effect specification and <span class="math inline">\(2 / 3\)</span> weight on the firm-specific specification.</p>
<p>To report our results we focus on the distribution of the firm-specific effects. For the fixed effects model these are the estimated fixed effects. For the industry-effect model these are the estimated industry dummy coefficients (for each firm). For the Stein Rule estimates they are a weighted average of the two. We estimate <span class="math inline">\({ }^{6}\)</span> the densities of the estimated firm-specific effects from the fixed-effects and Stein Rule estimators, and plot them in Figure 28.4(b).</p>
<p>You can see that the fixed-effects estimate of the firm-specific density is more dispersed while the Stein estimator is sharper and more peaked indicating that the fixed effects estimator attributes more variation in firm-specific factors than the Stein estimator. The Stein estimator pulls the fixed effects towards their common mean, adjusting for the randomness due to their estimation. Our expectation is that the Stein estimates, if used for an application such as firm-specific forecasting, will be more accurate because they will have reduced variance relative to the fixed effects estimates.</p>
<p>The third application uses the CPS dataset with the subsample of Black men ( <span class="math inline">\(n=2413)\)</span> focusing on the return to education across U.S. regions (Northeast, Midwest, South, West). Suppose you are asked to flexibly estimate the return to education for Black men allowing for the return to education to vary across the regions. Given the model selection information from Section <span class="math inline">\(28.18\)</span> a natural baseline is model 6 augmented to allow for greater variation across regions. A flexible specification interacts the six education dummy variables with the four regional dummies (omitting the intercept), which adds 18 coefficients and allows the return to education to vary without restriction in each region.</p>
<p>The least squares estimate of the return to education by region is displayed in Figure 28.5(a). For simplicity we combine the omitted education group (less than 12 years education) as “11 years”. The estimates appear noisy due to the small samples. One feature which we can see is that the four lines track one another for years of education between 12 and 18. That is, they are roughly linear in years of education with the same slope but different intercepts.</p>
<p>To improve the precision of the estimates we shrink the four profiles towards Model 6 . This means that we are shrinking the profiles not towards each other but towards the model with the same effect of education but regional-specific intercepts. Again we use the HCl covariance matrix estimate. The number of restrictions is 18 . The empirically-determined shrinkage weight is <span class="math inline">\(0.49\)</span> which means that the Stein Rule estimator puts equal weight on the two models.</p>
<p>The Stein Rule estimates are displayed in Figure 28.5(b). The estimates are less noisy than panel (a) and it is easier to see the patterns. The four lines track each other and are approximately linear over 1218. For 20 years of education the four lines disperse which seems likely due to small samples. In panel (b) it is easier to see the patterns across regions. It appears that the northeast region has the highest wages (conditional on education) while the west region has the lowest wages. This ranking is constant for nearly all levels of education.</p>
<p>While the Stein Rule estimates shrink the nonparametric estimates towards the common-educationfactor specification it does not impose the latter specification. The Stein Rule estimator has the ability to</p>
<p><span class="math inline">\({ }^{6}\)</span> The two densities are estimated with a common bandwidth to aid comparison. The bandwidth was selected to compromise between those selected for the two samples.</p>
<p><img src="images//2022_10_23_101d59f261a704807a3bg-35.jpg" class="img-fluid"></p>
<ol type="a">
<li>Least Squares Estimates</li>
</ol>
<p><img src="images//2022_10_23_101d59f261a704807a3bg-35(1).jpg" class="img-fluid"></p>
<ol start="2" type="a">
<li>Stein Rule Estimates</li>
</ol>
<p>Figure 28.5: Stein Rule Estimation of Education Profiles Across Regions</p>
<p>put near zero weight on the common-factor model. The fact that the estimates put <span class="math inline">\(1 / 2\)</span> weight on both models is the choice selected by the Stein Rule and is data-driven.</p>
<p>The message from these three applications is that the James-Stein shrinkage approach can be constructively used to reduce estimation variance in economic applications. These applications illustrate common forms of potential applications: Shrinkage of a flexible specification towards a simpler specification; Shrinkage of heterogeneous estimates towards homogeneous estimates; Shrinkage of fixed effects towards group dummy estimates. These three applications also employed moderately large sample sizes ( <span class="math inline">\(n=1149,2413\)</span>, and 5692 ) yet found shrinkage weights near <span class="math inline">\(50 %\)</span>. This shows that the benefits of Stein shrinkage are not confined to “small” samples but rather can be constructive used in moderately large samples with complicated structures.</p>
</section>
<section id="model-averaging" class="level2" data-number="26.33">
<h2 data-number="26.33" class="anchored" data-anchor-id="model-averaging"><span class="header-section-number">26.33</span> Model Averaging</h2>
<p>Recall that the problem of model selection is how to select a single model from a general set of models. The James-Stein shrinkage estimator smooths between two nested models by taking a weighted average of two estimators. More generally we can take an average of an arbitrary number of estimators. These estimators are known as model averaging estimators. The key issue for estimation is how to select the averaging weights.</p>
<p>Suppose we have a set of <span class="math inline">\(M\)</span> models <span class="math inline">\(\overline{\mathscr{M}}=\left\{\mathcal{M}_{1}, \ldots, \mathcal{M}_{M}\right\}\)</span>. For each model there is an estimator <span class="math inline">\(\widehat{\theta}_{m}\)</span> of the parameter <span class="math inline">\(\theta\)</span>. The natural way to think about multiple models, parameters, and estimators is the same as for model selection. All models are subsets of a general superset (overlapping) model which contains all submodels as special cases.</p>
<p>Corresponding to the set of models we introduce a set of weights <span class="math inline">\(w=\left\{w_{1}, \ldots, w_{M}\right\}\)</span>. It is common to restrict the weights to be non-negative and sum to one. The set of such weights is called the <span class="math inline">\(\mathbb{R}^{M}\)</span> probability simplex. Definition 28.4 Probability Simplex. The set <span class="math inline">\(\mathscr{S} \subset \mathbb{R}^{M}\)</span> of vectors such that <span class="math inline">\(\sum_{m=1}^{M} w_{m}=1\)</span> and <span class="math inline">\(w_{i} \geq 1\)</span> for <span class="math inline">\(i=1, \ldots, M\)</span>.</p>
<p>The probability simplex in <span class="math inline">\(\mathbb{R}^{2}\)</span> and <span class="math inline">\(\mathbb{R}^{3}\)</span> is shown in the two panels of Figure <span class="math inline">\(28.6\)</span>. The simplex in <span class="math inline">\(\mathbb{R}^{2}\)</span> (panel (a)) is the line between the vertices <span class="math inline">\((1,0)\)</span> and <span class="math inline">\((0,1)\)</span>. An example element is the point <span class="math inline">\((.7, .3)\)</span> indicated by the point <span class="math inline">\(w\)</span>. This is the weight vector which puts weight <span class="math inline">\(0.7\)</span> on model 1 and weight <span class="math inline">\(0.3\)</span> on model 2 . The vertice <span class="math inline">\((1,0)\)</span> is the weight vector which puts all weight on model 1, corresponding to model selection, and similarly the vertice <span class="math inline">\((0,1)\)</span> is the weight vector which puts all weight on model 2 .</p>
<p>The simplex in <span class="math inline">\(\mathbb{R}^{3}\)</span> (panel (b)) is the equilateral triangle formed between <span class="math inline">\((1,0,0),(0,1,0)\)</span>, and <span class="math inline">\((0,0,1)\)</span>. An example element is the point <span class="math inline">\((.1, .5, .4)\)</span> indicated by the pont <span class="math inline">\(w\)</span>. The edges are weight vectors which are averages between two of the three models. For example the bottom edge are weight vectors which divide the weight between models 1 and 2, placing no weight on model 3. The vertices are weight vectors which put all weight on one of the three models and correspond to model selection.\</p>
<p><img src="images//2022_10_23_101d59f261a704807a3bg-36.jpg" class="img-fluid"></p>
<p>Figure 28.6: Probability Simplex in <span class="math inline">\(\mathbb{R}^{2}\)</span> and <span class="math inline">\(\mathbb{R}^{3}\)</span></p>
<p>Since the weights on the probability simplex sum to one, an alternative representation is to eliminate one weight by substitution. Thus we can set <span class="math inline">\(w_{M}=1-\sum_{m=1}^{M-1} w_{m}\)</span> and define the set of vectors <span class="math inline">\(w=\left\{w_{1}, \ldots, w_{M-1}\right\}\)</span> which lie in the <span class="math inline">\(\mathbb{R}^{M-1}\)</span> unit simplex, which is the region bracketed by the probability simplex and the origin.</p>
<p>Given a weight vector we define the averaging estimator</p>
<p><span class="math display">\[
\widehat{\theta}(w)=\sum_{m=1}^{M} w_{m} \widehat{\theta}_{m} .
\]</span></p>
<p>Selection estimators emerge as the special case where the weight vector <span class="math inline">\(w\)</span> is a unit vector, e.g.&nbsp;the vertices in Figure 28.6.</p>
<p>It is not absolutely necessary to restrict the weight vector of an averaging estimator to lie in the probability simplex <span class="math inline">\(\mathscr{S}\)</span>, but in most cases it is a sensible restriction which improves performance. The unadjusted James-Stein estimator, for example, is an averaging estimator which does not enforce nonnegativity of the weights. The positive-part version, however, imposes non-negativity and achieves reduced MSE as a result.</p>
<p>In Section <span class="math inline">\(28.19\)</span> and Theorem <span class="math inline">\(28.11\)</span> we explored the MSE of a simple shrinkage estimator which shrinks an unrestricted estimator towards the zero vector. This is the same as a model averaging estimator where one of the two estimators is the zero vector. In Theorem <span class="math inline">\(28.11\)</span> we showed that the MSE of the optimal shrinkage (model averaging) estimator is less than the unrestricted estimator. This result extends to the case of averaging between an arbitrary number of estimators. The MSE of the optimal averaging estimator is less than the MSE of the estimator of the full model in any given sample.</p>
<p>The optimal averaging weights, however, are unknown. A number of methods have been proposed for selection of the averaging weights.</p>
<p>One simple method is equal weighting. This is achieved by setting <span class="math inline">\(w_{m}=1 / M\)</span> and results in the estimator</p>
<p><span class="math display">\[
\widehat{\theta}^{*}=\frac{1}{M} \sum_{m=1}^{M} \widehat{\theta}_{m} .
\]</span></p>
<p>The advantages of equal weighting are that it is simple, easy to motivate, and no randomness is introduced by estimation of the weights. The variance of the equal weighting estimator can be calculated because the weights are fixed. Another important advantage is that the estimator can be constructed in contexts where it is unknown how to construct empirical-based weights, for example when averaging models from completely different probability families. The disadvantages of equal weighting are that the method can be sensitive to the set of models considered, there is no guarantee that the estimator will perform better than the unrestricted estimator, and sample information is inefficiently used. In practice, equal weighting is best used in contexts where the set of models have been pre-screened so that all are considered “reasonable” models. From the standpoint of econometric methodology equal weighting is not a proper statistical method as it is an incomplete methodology.</p>
<p>Despite these concerns equal weighting can be constructively employed when summarizing information for a non-technical audience. The relevant context is when you have a small number of reasonable but distinct estimates typically made using different assumptions. The distinct estimates are presented to illustrate the range of possible results and the average taken to represent the “consensus” or “recommended” estimate.</p>
<p>As mentioned above, a number of methods have been proposed for selection of the averaging weights. In the following sections we outline four popular methods: Smoothed BIC, Smoothed AIC, Mallows averaging, and Jackknife averaging.</p>
</section>
<section id="smoothed-bic-and-aic" class="level2" data-number="26.34">
<h2 data-number="26.34" class="anchored" data-anchor-id="smoothed-bic-and-aic"><span class="header-section-number">26.34</span> Smoothed BIC and AIC</h2>
<p>Recall that Schwarz’s Theorem <span class="math inline">\(28.1\)</span> states that for a probability model <span class="math inline">\(f(y, \theta)\)</span> and a diffuse prior the marginal likelihood <span class="math inline">\(p(Y)\)</span> satisfies</p>
<p><span class="math display">\[
-2 \log p(Y) \simeq-2 \ell_{n}(\widehat{\theta})+K \log (n)=\mathrm{BIC} .
\]</span></p>
<p>This has been been interpreted to mean that the model with the highest value of the right-hand-side approximately has the highest marginal likelihood and is thus the model with the highest probability of being the true model.</p>
<p>There is another interpretation of Schwarz’s result. The marginal likelihood is approximately proportional to the probability that the model is true, conditional on the data. Schwarz’s Theorem implies that this is approximately</p>
<p><span class="math display">\[
p(Y) \simeq \exp (-\mathrm{BIC} / 2)
\]</span></p>
<p>which is a simple exponential transformation of the BIC. Weighting by posterior probability can be achieved by setting model weights proportional to this transformation. These are known as BIC weights and produce the smoothed BIC estimator.</p>
<p>To describe the method completely, we have a set of models <span class="math inline">\(\overline{\mathscr{M}}=\left\{\mathscr{M}_{1}, \ldots, \mathscr{M}_{M}\right\}\)</span>. Each model <span class="math inline">\(f_{m}\left(y, \theta_{m}\right)\)</span> depends on a <span class="math inline">\(K_{m} \times 1\)</span> parameter vector <span class="math inline">\(\theta_{m}\)</span> which is estimated by the maximum likelihood. The maximized likelihood is <span class="math inline">\(L_{m}\left(\widehat{\theta}_{m}\right)=f_{m}\left(Y, \widehat{\theta}_{m}\right)\)</span>. The BIC for model <span class="math inline">\(m\)</span> is <span class="math inline">\(\operatorname{BIC}_{m}=-2 \log L_{m}\left(\widehat{\theta}_{m}\right)+K_{m} \log (n)\)</span>.</p>
<p>The <span class="math inline">\(\mathrm{BIC}\)</span> weights are</p>
<p><span class="math display">\[
w_{m}=\frac{\exp \left(-\mathrm{BIC}_{m} / 2\right)}{\sum_{j=1}^{M} \exp \left(-\mathrm{BIC}_{j} / 2\right)} .
\]</span></p>
<p>Some properties of the BIC weights are as follows. They are non-negative so all models receive positive weight. Some models can receive weight arbitrarily close to zero and in practice many estimated models may receive BIC weight that is essentially zero. The model which is selected by BIC receives the greatest weight and models which have BIC values close to the minimum receive weights closest to the largest weight. Models whose <span class="math inline">\(\mathrm{BIC}\)</span> is not close to the minimum receive weight near zero.</p>
<p>The Smoothed BIC (SBIC) estimator is</p>
<p><span class="math display">\[
\widehat{\theta}_{\text {sbic }}=\sum_{m=1}^{M} w_{m} \widehat{\theta}_{m} .
\]</span></p>
<p>The SBIC estimator is a smoother function of the data than BIC selection as there are no discontinuous jumps across models.</p>
<p>An advantage of the smoothed BIC weights and estimator is that it can be used to combine models from different probability families. As for the BIC it is important that all models are estimated on the same sample. It is also important that the full formula is used for the BIC (no omission of constants) when combining models from different probability families.</p>
<p>Computationally it is better to implement smoothed BIC with what are called “BIC differences” rather than the actual values of the BIC, as the formula as written can produce numerical overflow problems. The difficulty is due to the exponentiation in the formula. This problem can be eliminated as follows. Let</p>
<p><span class="math display">\[
\mathrm{BIC}^{*}=\min _{1 \leq m \leq M} \mathrm{BIC}_{m}
\]</span></p>
<p>denote the lowest BIC among the models and define the BIC differences</p>
<p><span class="math display">\[
\Delta \mathrm{BIC}_{m}=\mathrm{BIC}_{m}-\mathrm{BIC}^{*} .
\]</span></p>
<p>Then</p>
<p><span class="math display">\[
\begin{aligned}
w_{m} &amp;=\frac{\exp \left(-\mathrm{BIC}_{m} / 2\right)}{\sum_{j=1}^{M} \exp \left(-\mathrm{BIC}_{j} / 2\right)} \\
&amp;=\frac{\exp \left(-\mathrm{BIC}_{m} / 2\right) \exp \left(\mathrm{BIC}^{*} / 2\right)}{\sum_{j=1}^{M} \exp \left(-\mathrm{BIC}_{j} / 2\right) \exp \left(\mathrm{BIC}^{*} / 2\right)} \\
&amp;=\frac{\exp \left(-\Delta \mathrm{BIC}_{m} / 2\right)}{\sum_{j=1}^{M} \exp \left(-\Delta \mathrm{BIC}_{j} / 2\right)} .
\end{aligned}
\]</span></p>
<p>Thus the weights are algebraically identically whether computed on <span class="math inline">\(\mathrm{BIC}_{m}\)</span> or <span class="math inline">\(\Delta \mathrm{BIC}_{m}\)</span>. Since <span class="math inline">\(\Delta \mathrm{BIC}_{m}\)</span> are of smaller magnitude than <span class="math inline">\(\mathrm{BIC}_{m}\)</span> overflow problems are less likely to occur.</p>
<p>Because of the properties of the exponential, if <span class="math inline">\(\Delta \mathrm{BIC}_{m} \geq 10\)</span> then <span class="math inline">\(w_{m} \leq 0.01\)</span>. Thus smoothed BIC typically concentrates weight on models whose BIC values are close to the minimum. This means that in practice smoothed BIC puts effective non-zero weight on a small number of models. Burnham and Anderson (1998) follow a suggestion they credit to Akaike that if we make the same transformation to the AIC as to the BIC to obtain the smoothed BIC weights we obtain frequentist approximate probabilities for the models. Specifically they propose the AIC weights</p>
<p><span class="math display">\[
w_{m}=\frac{\exp \left(-\mathrm{AIC}_{m} / 2\right)}{\sum_{j=1}^{M} \exp \left(-\mathrm{AIC}_{j} / 2\right)} .
\]</span></p>
<p>They do not provide a strong theoretical justification for this specific choice of transformation but it seems natural given the smoothed BIC formula and works well in simulations.</p>
<p>The algebraic properties of the AIC weights are similar to those of the BIC weights. All models receive positive weight though some receive weight which is arbitrarily close to zero. The model with the smallest AIC receives the greatest AIC weight, and models with similar AIC values receive similar AIC weights.</p>
<p>Computationally the AIC weights should be computed using AIC differences. Define</p>
<p><span class="math display">\[
\begin{aligned}
\mathrm{AIC}^{*} &amp;=\min _{1 \leq m \leq M} \mathrm{AIC}_{m} \\
\Delta \mathrm{AIC}_{m} &amp;=\mathrm{AIC}_{m}-\mathrm{AIC}^{*} .
\end{aligned}
\]</span></p>
<p>The AIC weights algebraically equal</p>
<p><span class="math display">\[
w_{m}=\frac{\exp \left(-\Delta \mathrm{AIC}_{m} \mathrm{AIC}_{m} / 2\right)}{\sum_{j=1}^{M} \exp \left(-\Delta \mathrm{AIC}_{j} / 2\right)} .
\]</span></p>
<p>As for the BIC weights <span class="math inline">\(w_{m} \leq 0.01\)</span> if <span class="math inline">\(\Delta \mathrm{AIC}_{m} \geq 10\)</span> so the AIC weights will concentrated on models whose AIC values are close to the minimum. However, in practice it is common that the AIC criterion is less concentrated than the BIC criterion as the AIC puts a smaller penalty on large penalizations. The AIC weights tend to be more spread out across models than the corresponding BIC weights.</p>
<p>The Smoothed AIC (SAIC) estimator is</p>
<p><span class="math display">\[
\widehat{\theta}_{\text {saic }}=\sum_{m=1}^{M} w_{m} \widehat{\theta}_{m} .
\]</span></p>
<p>The SAIC estimator is a smoother function of the data than AIC selection.</p>
<p>Recall that both AIC selection and BIC selection are model selection consistent in the sense that as the sample size gets large the probability that the selected model is a true model is arbtrarily close to one. Furthermore, BIC is consistent for parsimonious models and AIC asymptotically over-selects.</p>
<p>These properties extend to SBIC and SAIC. In large samples SAIC and SBIC weights will concentrate exclusively on true models; the weight on incorrect models will asymptotically approach zero. However, SAIC will asymptotically spread weight across both parsimonious true models and overparameterized true models, while SBIC asymptotically concentrates weight only on parsimonious true models.</p>
<p>An interesting property of the smoothed estimators is the possibility of asymptotically spreading weight across equal-fitting parsimonious models. Suppose we have two non-nested models with the same number of parameters and the same KLIC value so they are equal approximations. In large samples both SBIC and SAIC will be weighted averages of the two estimators rather than simply selecting one of the two.</p>
</section>
<section id="mallows-model-averaging" class="level2" data-number="26.35">
<h2 data-number="26.35" class="anchored" data-anchor-id="mallows-model-averaging"><span class="header-section-number">26.35</span> Mallows Model Averaging</h2>
<p>In linear regression the Mallows criterion (28.14) applies directly to the model averaging estimator (28.29). The homoskedastic regression model is</p>
<p><span class="math display">\[
\begin{aligned}
Y &amp;=m+e \\
m &amp;=m(X) \\
\mathbb{E}[e \mid X] &amp;=0 \\
\mathbb{E}\left[e^{2} \mid X\right] &amp;=\sigma^{2} .
\end{aligned}
\]</span></p>
<p>Suppose that there are <span class="math inline">\(M\)</span> models for <span class="math inline">\(m(X)\)</span>, each which takes the form <span class="math inline">\(\beta_{m}^{\prime} X_{m}\)</span> for some <span class="math inline">\(K_{m} \times 1\)</span> regression vector <span class="math inline">\(X_{m}\)</span>. The <span class="math inline">\(m^{t h}\)</span> model estimator of the coefficient is <span class="math inline">\(\widehat{\beta}_{m}=\left(\boldsymbol{X}_{m}^{\prime} \boldsymbol{X}_{m}\right)^{-1} \boldsymbol{X}_{m}^{\prime} \boldsymbol{Y}\)</span>, and the estimator of the vector <span class="math inline">\(\boldsymbol{m}\)</span> is <span class="math inline">\(\widehat{\boldsymbol{m}}_{m}=\boldsymbol{P}_{m} \boldsymbol{Y}\)</span> where <span class="math inline">\(\boldsymbol{P}_{m}=\boldsymbol{X}_{m}\left(\boldsymbol{X}_{m}^{\prime} \boldsymbol{X}_{m}\right)^{-1} \boldsymbol{X}_{m}^{\prime}\)</span>. The corresponding residual vector is <span class="math inline">\(\widehat{\boldsymbol{e}}_{m}=\)</span> <span class="math inline">\(\left(\boldsymbol{I}_{n}-\boldsymbol{P}_{m}\right) \boldsymbol{Y}\)</span>.</p>
<p>The model averaging estimator for fixed weights is</p>
<p><span class="math display">\[
\widehat{\boldsymbol{m}}_{m}(w)=\sum_{m=1}^{M} w_{m} \boldsymbol{P}_{m} \boldsymbol{Y}=\boldsymbol{P}(w) \boldsymbol{Y}
\]</span></p>
<p>where</p>
<p><span class="math display">\[
\boldsymbol{P}(w)=\sum_{m=1}^{M} w_{m} \boldsymbol{P}_{m} .
\]</span></p>
<p>The model averaging residual is</p>
<p><span class="math display">\[
\widehat{\boldsymbol{e}}(w)=\left(\boldsymbol{I}_{n}-\boldsymbol{P}(w)\right) \boldsymbol{Y}=\sum_{m=1}^{M} w_{m}\left(\boldsymbol{I}_{n}-\boldsymbol{P}_{m}\right) \boldsymbol{Y} .
\]</span></p>
<p>The estimator <span class="math inline">\(\widehat{\boldsymbol{m}}_{m}(w)\)</span> is linear in <span class="math inline">\(\boldsymbol{Y}\)</span> so the Mallows criterion can be applied. It equals</p>
<p><span class="math display">\[
\begin{aligned}
C(w) &amp;=\widehat{\boldsymbol{e}}(w)^{\prime} \widehat{\boldsymbol{e}}(w)+2 \widetilde{\sigma}^{2} \operatorname{tr}(\boldsymbol{P}(w)) \\
&amp;=\widehat{\boldsymbol{e}}(w)^{\prime} \widehat{\boldsymbol{e}}(w)+2 \widetilde{\sigma}^{2} \sum_{m=1}^{M} w_{m} K_{m}
\end{aligned}
\]</span></p>
<p>where <span class="math inline">\(\widetilde{\sigma}^{2}\)</span> is a preliminary <span class="math inline">\({ }^{7}\)</span> estimator of <span class="math inline">\(\sigma^{2}\)</span>.</p>
<p>In the case of model selection the Mallows penalty is proportional to the number of estimated coefficients. In the model averaging case the Mallows penalty is the average number of estimated coefficients.</p>
<p>The Mallows-selected weight vector is that which minimizes the Mallows criterion. It equals</p>
<p><span class="math display">\[
\widehat{w}_{\mathrm{mma}}=\underset{w \in \mathscr{S}}{\operatorname{argmin}} C(w) .
\]</span></p>
<p>Computationally it is useful to observe that <span class="math inline">\(C(w)\)</span> is a quadratric function in <span class="math inline">\(w\)</span>. Indeed, by defining the <span class="math inline">\(n \times M\)</span> matrix <span class="math inline">\(\widehat{\boldsymbol{E}}=\left[\widehat{\boldsymbol{e}}_{1}, \ldots, \widehat{\boldsymbol{e}}_{M}\right]\)</span> of residual vectors and the <span class="math inline">\(M \times 1\)</span> vector <span class="math inline">\(\boldsymbol{K}=\left[K_{1}, \ldots, K_{M}\right]\)</span> the criterion is</p>
<p><span class="math display">\[
C(w)=w^{\prime} \widehat{\boldsymbol{E}}^{\prime} \widehat{\boldsymbol{E}} w+2 \widetilde{\sigma}^{2} \boldsymbol{K}^{\prime} w .
\]</span></p>
<p>The probability simplex <span class="math inline">\(\mathscr{S}\)</span> is defined by one equality and <span class="math inline">\(2 M\)</span> inequality constraints. The minimization problem (28.30) falls in the category of quadratic programming which means optimization of a</p>
<p><span class="math inline">\({ }^{7}\)</span> It is typical to use the bias-corrected least squares variance estimator from the largest model. quadratic subject to linear equality and inequality constraints. This is a well-studied area of numerical optimization and numerical solutions are widely available. In R use the command solve.QP in the package quadprog. In MATLAB use the command quadprog.</p>
<p>Figure <span class="math inline">\(28.7\)</span> illustrates the Mallows weight computation problem. Displayed is the probability simplex <span class="math inline">\(\mathscr{S}\)</span> in <span class="math inline">\(\mathbb{R}^{3}\)</span>. The axes are the weight vectors. The ellipses are the contours of the unconstrained sum of squared errors as a function of the weight vectors projected onto the constrained set <span class="math inline">\(\sum_{m=1}^{M} w_{m}=1\)</span>. This is the extension of the probability simplex as a two-dimensional plane in <span class="math inline">\(\mathbb{R}^{3}\)</span>. The midpoint of the contours is the minimizing weight vector allowing for weights outside <span class="math inline">\([0,1]\)</span>. The point where the lowest contour ellipse hits the probability simplex is the solution (28.30), the Mallows selected weight vector. In the left panel is displayed an example where the solution is the vertex <span class="math inline">\((0,1,0)\)</span> so the selected weight vector puts all weight on model 2. In the right panel is displayed an example where the solution lies on the edge between <span class="math inline">\((1,0,0)\)</span> and <span class="math inline">\((0,0,1)\)</span>, meaning that the selected weight vector averages models 1 and 3 but puts no weight on model 2. Since the contour sets are ellipses and the constraint set is a simplex, solution points tend to be on edges and vertices meaning that some models receive zero weight. In fact, where there are a large number of models a generic feature of the solution is that most models receive zero weight; the selected weight vector puts positive weight on a small subset of the eligible models.\</p>
<p><img src="images//2022_10_23_101d59f261a704807a3bg-41.jpg" class="img-fluid"></p>
<p>Figure 28.7: Mallows Weight Selection</p>
<p>Once the weights <span class="math inline">\(\widehat{w}\)</span> are obtained the model averaging estimator of the coefficients are found by averaging the model estimates <span class="math inline">\(\widehat{\beta}_{m}\)</span> using the weights.</p>
<p>In the special case of two nested models the Mallows criterion can be written as</p>
<p><span class="math display">\[
\begin{aligned}
C(w) &amp;=(w, 1-w)\left(\begin{array}{cc}
\widehat{\boldsymbol{e}}_{1}^{\prime} \widehat{\boldsymbol{e}}_{1} &amp; \widehat{\boldsymbol{e}}_{1}^{\prime} \widehat{\boldsymbol{e}}_{2} \\
\widehat{\boldsymbol{e}}_{2}^{\prime} \widehat{\boldsymbol{e}}_{1} &amp; \widehat{\boldsymbol{e}}_{2}^{\prime} \widehat{\boldsymbol{e}}_{2}
\end{array}\right)\left(\begin{array}{c}
w \\
1-w
\end{array}\right)+2 \widetilde{\sigma}^{2}\left(w K_{1}+(1-w) K_{2}\right) \\
&amp;=(w, 1-w)\left(\begin{array}{ll}
\widehat{\boldsymbol{e}}_{1}^{\prime} \widehat{\boldsymbol{e}}_{1} &amp; \widehat{\boldsymbol{e}}_{2}^{\prime} \widehat{\boldsymbol{e}}_{2} \\
\widehat{\boldsymbol{e}}_{2}^{\prime} \widehat{\boldsymbol{e}}_{2} &amp; \widehat{\boldsymbol{e}}_{2}^{\prime} \widehat{\boldsymbol{e}}_{2}
\end{array}\right)\left(\begin{array}{c}
1-w \\
w
\end{array}\right)+2 \widetilde{\sigma}^{2}\left(w K_{1}+(1-w) K_{2}\right) \\
&amp;=w^{2}\left(\widehat{\boldsymbol{e}}_{1}^{\prime} \widehat{\boldsymbol{e}}_{1}-\widehat{\boldsymbol{e}}_{2}^{\prime} \widehat{\boldsymbol{e}}_{2}\right)+\widehat{\boldsymbol{e}}_{2}^{\prime} \widehat{\boldsymbol{e}}_{2}-2 \widetilde{\sigma}^{2}\left(K_{2}-K_{1}\right) w+2 \widetilde{\sigma}^{2}
\end{aligned}
\]</span></p>
<p>where we assume <span class="math inline">\(K_{1}&lt;K_{2}\)</span> so that <span class="math inline">\(\widehat{\boldsymbol{e}}_{1}^{\prime} \widehat{\boldsymbol{e}}_{2}=\boldsymbol{Y}^{\prime}\left(\boldsymbol{I}_{n}-\boldsymbol{P}_{1}\right)\left(\boldsymbol{I}_{n}-\boldsymbol{P}_{2}\right) \boldsymbol{Y}=\boldsymbol{Y}^{\prime}\left(\boldsymbol{I}_{n}-\boldsymbol{P}_{2}\right) \boldsymbol{Y}=\widehat{\boldsymbol{e}}_{2}^{\prime} \widehat{\boldsymbol{e}}_{2}\)</span>. The minimizer of this criterion is</p>
<p><span class="math display">\[
\widehat{w}=\left(\frac{\widetilde{\sigma}^{2}\left(K_{2}-K_{1}\right)}{\widehat{\boldsymbol{e}}_{1}^{\prime} \widehat{\boldsymbol{e}}_{1}-\widehat{\boldsymbol{e}}_{2}^{\prime} \widehat{\boldsymbol{e}}_{2}}\right)_{1} .
\]</span></p>
<p>This is the same as the Stein Rule weight (28.27) with a slightly different shrinkage constant. Thus the Mallows averaging estimator for <span class="math inline">\(M=2\)</span> is a Stein Rule estimator. Hence for <span class="math inline">\(M&gt;2\)</span> the Mallows averaging estimator is a generalization of the James-Stein estimator to multiple models.</p>
<p>Based on the latter observation, B. E. Hansen (2014) shows that the MMA estimator has lower WMSE than the unrestricted least squares estimator when the models are nested linear regressions, the errors are homoskedastic, and the models are separated by 4 coefficients or greater. The latter condition is analogous to the conditions for improvements in the Stein Rule theory.</p>
<p>B. E. Hansen (2007) showed that the MMA estimator asymptotically achieves the same MSE as the infeasible optimal best weighted average using the theory of Li (1987) under similar conditions. This shows that using model selection tools to select the averaging weights is asymptotically optimal for regression fitting and point forecasting.</p>
</section>
<section id="jackknife-cv-model-averaging" class="level2" data-number="26.36">
<h2 data-number="26.36" class="anchored" data-anchor-id="jackknife-cv-model-averaging"><span class="header-section-number">26.36</span> Jackknife (CV) Model Averaging</h2>
<p>A disadvantage of Mallows selection is that the criterion is valid only when the errors are conditionally homoskedastic. In constrast, selection by cross-validation does not require homoskedasticity. Therefore it seems sensible to use cross-validation rather than Mallows to select the weight vectors. It turns out that this is a simple extension with excellent finite sample performance. In the Machine Learning literature this method is called stacking.</p>
<p>A fitted averaging regression (with fixed weights) can be written as</p>
<p><span class="math display">\[
Y_{i}=\sum_{m=1}^{M} w_{m} X_{m i}^{\prime} \widehat{\beta}_{m}+\widehat{e}_{i}(w)
\]</span></p>
<p>where <span class="math inline">\(\widehat{\beta}_{m}\)</span> are the least squares coefficient estimates from Model <span class="math inline">\(m\)</span>. The corresponding leave-one-out equation is</p>
<p><span class="math display">\[
Y_{i}=\sum_{m=1}^{M} w_{m} X_{m i}^{\prime} \widehat{\beta}_{m,(-i)}+\widetilde{e}_{i}(w)
\]</span></p>
<p>where <span class="math inline">\(\widehat{\beta}_{m,(-i)}\)</span> are the least squares coefficient estimates from Model <span class="math inline">\(m\)</span> when observation <span class="math inline">\(i\)</span> is deleted. The leave-one-out prediction errors satisfy the simple relationship</p>
<p><span class="math display">\[
\widetilde{e}_{i}(w)=\sum_{m=1}^{M} w_{m} \widetilde{e}_{m i}
\]</span></p>
<p>where <span class="math inline">\(\widetilde{e}_{m i}\)</span> are the leave-one-out prediction errors for model <span class="math inline">\(m\)</span>. In matrix notation <span class="math inline">\(\widetilde{\boldsymbol{e}}(w)=\widetilde{\boldsymbol{E}} w\)</span> where <span class="math inline">\(\widetilde{\boldsymbol{E}}\)</span> is the <span class="math inline">\(n \times M\)</span> matrix of leave-one-out prediction errors.</p>
<p>This means that the jackknife estimate of variance (or equivalently the cross-validation criterion) equals</p>
<p><span class="math display">\[
\mathrm{CV}(w)=w^{\prime} \widetilde{\boldsymbol{E}}^{\prime} \widetilde{\boldsymbol{E}} w
\]</span></p>
<p>which is a quadratic function of the weight vector. The cross-validation choice for weight vector is the minimizer</p>
<p><span class="math display">\[
\widehat{w}_{\mathrm{jma}}=\underset{w \in \mathscr{S}}{\operatorname{argmin}} \mathrm{CV}(w) .
\]</span></p>
<p>Given the weights the coefficient estimates (and any other parameter of interest) are found by taking weighted averages of the model estimates using the weight vector <span class="math inline">\(\widehat{w}_{\text {jma. }}\)</span>. B. E. Hansen and Racine (2012) call this the Jackknife Model Averaging (JMA) estimator.</p>
<p>The algebraic properties of the solution are similar to Mallows. Since (28.31) minimizes a quadratic function subject to a simplex constraint, solutions tend to be on edges and vertices which means that many (or most) models receive zero weight. Hence JMA weight selection simultaneously performs selection and shrinkage. The solution is found numerically by quadratic programming which is computationally simple and fast even when the number of models <span class="math inline">\(M\)</span> is large.</p>
<p>B. E. Hansen and Racine (2012) showed that the JMA estimator is asymptotically equivalent to the infeasible optimal weighted average across least squares estimates based on a regression fit criteria. Their results hold under quite mild conditions including conditional heteroskedasticity. This result is similar to Andrews (1991c) generalization of Li (1987)’s result for model selection.</p>
<p>The implication of this theory is that JMA weight selection is computationally simple and has excellent sampling performance.</p>
</section>
<section id="granger-ramanathan-averaging" class="level2" data-number="26.37">
<h2 data-number="26.37" class="anchored" data-anchor-id="granger-ramanathan-averaging"><span class="header-section-number">26.37</span> Granger-Ramanathan Averaging</h2>
<p>A method similar to JMA based on hold-out samples was proposed for forecast combination by Granger and Ramanathan (1984), and has emerged as a popular method in the modern machine learning literature.</p>
<p>Randomly split the sample into two parts: an estimation an an evaluation sample. Using the estimation sample, estimate the <span class="math inline">\(M\)</span> regression models, obtaining the coefficients <span class="math inline">\(\widehat{\beta}_{m}\)</span>. Using these coefficients and the evaluation sample construct the fitted values <span class="math inline">\(\widetilde{Y}_{m i}=X_{m i}^{\prime} \widehat{\beta}_{m}\)</span> for the <span class="math inline">\(M\)</span> models. Then estimate the model weights by a least squares regression of <span class="math inline">\(Y_{i}\)</span> on <span class="math inline">\(\widetilde{Y}_{m i}\)</span> and no intercept using the evaluation sample. This regression is</p>
<p><span class="math display">\[
Y_{i}=\sum_{m=1}^{M} \widehat{w}_{m} \widetilde{Y}_{m i}+\widehat{e}_{i} .
\]</span></p>
<p>The least squares coefficients <span class="math inline">\(\widehat{w}_{m}\)</span> are the Granger-Ramanathan weights.</p>
<p>Based on an informal argument Granger and Ramanathan (1984) recommended an unconstrained least squares regression to obtain the weights but this is not advised as this produces extremely erratic empirical weights, especially when <span class="math inline">\(M\)</span> is large. Instead, it is recommended to use constrained regression, imposing the constraints <span class="math inline">\(\widehat{w}_{m} \geq 0\)</span> and <span class="math inline">\(\sum_{m=1}^{M} \widehat{w}_{m}=1\)</span>. To impose the non-negativity constraints it is best to use quadratic programming.</p>
<p>This Granger-Ramanathan approach is best suited for applications with a very large sample size where the efficiency loss from the hold-out sample split is not a concern.</p>
</section>
<section id="empirical-illustration-1" class="level2" data-number="26.38">
<h2 data-number="26.38" class="anchored" data-anchor-id="empirical-illustration-1"><span class="header-section-number">26.38</span> Empirical Illustration</h2>
<p>We illustrate the model averaging methods with the empirical application from Section 28.18, which reported wage regression estimates for the CPS sub-sample of Asian women focusing on the return to experience between 0 and 30 years.</p>
<p>Table <span class="math inline">\(28.2\)</span> reports the model averaging weights obtained using the methods of SBIC, SAIC, Mallows model averaging (MMA), and jackknife model averaging (JMA). Also reported in the final column is the weighted average estimate of the return to experience as a percentage.</p>
<p>The results show that the methods put weight on somewhat different models. The SBIC puts nearly all weight on model 2 . The SAIC puts nearly <span class="math inline">\(1 / 2\)</span> of the weight on model 6 with most of the remainder split between models 5 and 9. MMA puts nearly <span class="math inline">\(1 / 2\)</span> of the weight on model <span class="math inline">\(9,30 %\)</span> on 5 , and <span class="math inline">\(9 %\)</span> on model 1. JMA is similar to MMA but more emphasis on parsimony, with <span class="math inline">\(1 / 2\)</span> of the weight on model 5 , <span class="math inline">\(17 %\)</span> on model <span class="math inline">\(9,17 %\)</span> on model 1 , and <span class="math inline">\(8 %\)</span> on model 3 . One of the interesting things about the MMA/JMA methods is that they can split weight between quite different models, e.g.&nbsp;models 1 and 9.</p>
<p>The averaging estimators from the non-BIC methods are similar to one another but SBIC produces a much smaller estimate than the other methods.</p>
<p>Table 28.2: Model Averaging Weights and Estimates of Return to Experience among Asian Women</p>
<table class="table">
<colgroup>
<col style="width: 5%">
<col style="width: 9%">
<col style="width: 9%">
<col style="width: 9%">
<col style="width: 9%">
<col style="width: 9%">
<col style="width: 9%">
<col style="width: 9%">
<col style="width: 9%">
<col style="width: 9%">
<col style="width: 8%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;"></th>
<th>Model 1</th>
<th>Model 2</th>
<th>Model 3</th>
<th>Model 4</th>
<th>Model 5</th>
<th>Model 6</th>
<th>Model 7</th>
<th>Model 8</th>
<th>Model 9</th>
<th>Return</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">SBIC</td>
<td><span class="math inline">\(.02\)</span></td>
<td><span class="math inline">\(.96\)</span></td>
<td><span class="math inline">\(.00\)</span></td>
<td><span class="math inline">\(.00\)</span></td>
<td><span class="math inline">\(.04\)</span></td>
<td><span class="math inline">\(.00\)</span></td>
<td><span class="math inline">\(.00\)</span></td>
<td><span class="math inline">\(.00\)</span></td>
<td><span class="math inline">\(.00\)</span></td>
<td><span class="math inline">\(22 %\)</span></td>
</tr>
<tr class="even">
<td style="text-align: left;">SAIC</td>
<td><span class="math inline">\(.00\)</span></td>
<td><span class="math inline">\(.02\)</span></td>
<td><span class="math inline">\(.10\)</span></td>
<td><span class="math inline">\(.00\)</span></td>
<td><span class="math inline">\(.15\)</span></td>
<td><span class="math inline">\(.44\)</span></td>
<td><span class="math inline">\(.00\)</span></td>
<td><span class="math inline">\(.06\)</span></td>
<td><span class="math inline">\(.22\)</span></td>
<td><span class="math inline">\(38 %\)</span></td>
</tr>
<tr class="odd">
<td style="text-align: left;">MMA</td>
<td><span class="math inline">\(.09\)</span></td>
<td><span class="math inline">\(.02\)</span></td>
<td><span class="math inline">\(.02\)</span></td>
<td><span class="math inline">\(.00\)</span></td>
<td><span class="math inline">\(.30\)</span></td>
<td><span class="math inline">\(.00\)</span></td>
<td><span class="math inline">\(.00\)</span></td>
<td><span class="math inline">\(.00\)</span></td>
<td><span class="math inline">\(.57\)</span></td>
<td><span class="math inline">\(39 %\)</span></td>
</tr>
<tr class="even">
<td style="text-align: left;">JMA</td>
<td><span class="math inline">\(.17\)</span></td>
<td><span class="math inline">\(.00\)</span></td>
<td><span class="math inline">\(.08\)</span></td>
<td><span class="math inline">\(.00\)</span></td>
<td><span class="math inline">\(.57\)</span></td>
<td><span class="math inline">\(.01\)</span></td>
<td><span class="math inline">\(.00\)</span></td>
<td><span class="math inline">\(.00\)</span></td>
<td><span class="math inline">\(.17\)</span></td>
<td><span class="math inline">\(34 %\)</span></td>
</tr>
</tbody>
</table>
</section>
<section id="technical-proofs" class="level2" data-number="26.39">
<h2 data-number="26.39" class="anchored" data-anchor-id="technical-proofs"><span class="header-section-number">26.39</span> Technical Proofs*</h2>
<p>Proof of Theorem 28.1 We establish the theorem under the simplifying assumptions of the normal linear regression model with a <span class="math inline">\(K \times 1\)</span> coefficient vector <span class="math inline">\(\beta\)</span> and known variance <span class="math inline">\(\sigma^{2}\)</span>. The likelihood function is</p>
<p><span class="math display">\[
L_{n}(\beta)=\left(2 \pi \sigma^{2}\right)^{-n / 2} \exp \left(-\frac{1}{2 \sigma^{2}} \sum_{i=1}^{n}\left(Y_{i}-X_{i}^{\prime} \beta\right)^{2}\right) .
\]</span></p>
<p>Evaluated at the MLE <span class="math inline">\(\widehat{\beta}\)</span> this equals</p>
<p><span class="math display">\[
L_{n}(\widehat{\beta})=\left(2 \pi \sigma^{2}\right)^{-n / 2} \exp \left(-\frac{\sum_{i=1}^{n} \widehat{e}_{i}^{2}}{2 \sigma^{2}}\right) .
\]</span></p>
<p>Using (8.21) we can write</p>
<p><span class="math display">\[
\begin{aligned}
L_{n}(\beta) &amp;=\left(2 \pi \sigma^{2}\right)^{-n / 2} \exp \left(-\frac{1}{2 \sigma^{2}}\left(\sum_{i=1}^{n} \widehat{e}_{i}^{2}+(\widehat{\beta}-\beta)^{\prime} \boldsymbol{X}^{\prime} \boldsymbol{X}(\widehat{\beta}-\beta)\right)\right) \\
&amp;=L_{n}(\widehat{\beta}) \exp \left(-\frac{1}{2 \sigma^{2}}(\widehat{\beta}-\beta)^{\prime} \boldsymbol{X}^{\prime} \boldsymbol{X}(\widehat{\beta}-\beta)\right) .
\end{aligned}
\]</span></p>
<p>For a diffuse prior <span class="math inline">\(\pi(\beta)=C\)</span> the marginal likelihood is</p>
<p><span class="math display">\[
\begin{aligned}
p(Y) &amp;=L_{n}(\widehat{\beta}) \int \exp \left(-\frac{1}{2 \sigma^{2}}(\widehat{\beta}-\beta)^{\prime} \boldsymbol{X}^{\prime} \boldsymbol{X}(\widehat{\beta}-\beta)\right) C d \beta \\
&amp;=L_{n}(\widehat{\beta}) n^{-K / 2}\left(2 \pi \sigma^{2}\right)^{K / 2} \operatorname{det}\left(\frac{1}{n} \boldsymbol{X}^{\prime} \boldsymbol{X}\right)^{-1 / 2} C
\end{aligned}
\]</span></p>
<p>where the final equality is the multivariate normal integral. Rewriting and taking logs</p>
<p><span class="math display">\[
\begin{aligned}
-2 \log p(Y) &amp;=-2 \log L_{n}(\widehat{\beta})+K \log n-K \log \left(2 \pi \sigma^{2}\right)+\log \operatorname{det}\left(\frac{1}{n} \boldsymbol{X}^{\prime} \boldsymbol{X}\right)+\log C \\
&amp;=-2 \ell_{n}(\widehat{\beta})+K \log n+O(1) .
\end{aligned}
\]</span></p>
<p>This is the theorem.</p>
<p>Proof of Theorem 28.2 From (28.11)</p>
<p><span class="math display">\[
\begin{aligned}
\int g(y) \log f(y, \widehat{\theta}) d y &amp;=-\frac{n}{2} \log \left(2 \pi \sigma^{2}\right)-\frac{1}{2 \sigma^{2}} \sum_{i=1}^{n} \int\left(y-X_{i}^{\prime} \widehat{\beta}\right)^{2} g\left(y \mid X_{i}\right) d y \\
&amp;=-\frac{n}{2} \log \left(2 \pi \sigma^{2}\right)-\frac{1}{2 \sigma^{2}} \sum_{i=1}^{n}\left(\sigma^{2}+(\widehat{\beta}-\beta)^{\prime} X_{i} X_{i}^{\prime}(\widehat{\beta}-\beta)\right) \\
&amp;=-\frac{n}{2} \log \left(2 \pi \sigma^{2}\right)-\frac{n}{2}-\frac{1}{2 \sigma^{2}} \boldsymbol{e}^{\prime} \boldsymbol{P} \boldsymbol{e} .
\end{aligned}
\]</span></p>
<p>Thus</p>
<p><span class="math display">\[
T=-2 \mathbb{E}\left[\int g(y) \log \widehat{f}(y) d y\right]=n \log \left(2 \pi \sigma^{2}\right)+n+\frac{1}{\sigma^{2}} \mathbb{E}[\boldsymbol{e} \boldsymbol{P} \boldsymbol{e}]=n \log \left(2 \pi \sigma^{2}\right)+n+K .
\]</span></p>
<p>This is (28.12). The final equality holds under the assumption of conditional homoskedasticity.</p>
<p>Evaluating (28.11) at <span class="math inline">\(\widehat{\beta}\)</span> we obtain the log likelihood</p>
<p><span class="math display">\[
-2 \ell_{n}(\widehat{\beta})=n \log \left(2 \pi \sigma^{2}\right)+\frac{1}{\sigma^{2}} \sum_{i=1}^{n} \widehat{e}_{i}^{2}=n \log \left(2 \pi \sigma^{2}\right)+\frac{1}{\sigma^{2}} \boldsymbol{e}^{\prime} \boldsymbol{M} \boldsymbol{e} .
\]</span></p>
<p>This has expectation</p>
<p><span class="math display">\[
-\mathbb{E}\left[2 \ell_{n}(\widehat{\beta})\right]=n \log \left(2 \pi \sigma^{2}\right)+\frac{1}{\sigma^{2}} \mathbb{E}\left[\boldsymbol{e}^{\prime} \boldsymbol{M e}\right]=n \log \left(2 \pi \sigma^{2}\right)+n-K .
\]</span></p>
<p>This is (28.13). The final equality holds under conditional homoskedasticity.</p>
<p>Proof of Theorem 28.4 The proof uses Taylor expansions similar to those used for the asymptotic distribution theory of the MLE in nonlinear models. We avoid technical details so this is not a full proof.</p>
<p>Write the model density as <span class="math inline">\(f(y, \theta)\)</span> and the estimated model as <span class="math inline">\(\widehat{f}(y)=f(y, \widehat{\theta})\)</span>. Recall from (28.10) that we can write the target <span class="math inline">\(T\)</span> as</p>
<p><span class="math display">\[
T=-2 \mathbb{E}[\log f(\widetilde{Y}, \widehat{\theta})]
\]</span></p>
<p>where <span class="math inline">\(\widetilde{Y}\)</span> is an independent copy of <span class="math inline">\(Y\)</span>. Let <span class="math inline">\(\widetilde{\theta}\)</span> be the MLE calculated on the sample <span class="math inline">\(\widetilde{Y} . \widetilde{\theta}\)</span> is an independent copy of <span class="math inline">\(\widehat{\theta}\)</span>. By symmetry we can write <span class="math inline">\(T\)</span> as</p>
<p><span class="math display">\[
T=-2 \mathbb{E}[\log f(Y, \widetilde{\theta})] .
\]</span></p>
<p>Define the Hessian <span class="math inline">\(H=-\frac{\partial}{\partial \theta \partial \theta^{\prime}} \mathbb{E}[\log f(Y, \theta)]&gt;0\)</span>. Now take a second-order Taylor series expansion of the <span class="math inline">\(\log\)</span> likelihood <span class="math inline">\(\log f(Y, \widetilde{\theta})\)</span> about <span class="math inline">\(\widehat{\theta}\)</span>. This is</p>
<p><span class="math display">\[
\begin{aligned}
\log f(Y, \widetilde{\theta}) &amp;=\log f(Y, \widehat{\theta})+\frac{\partial}{\partial \theta^{\prime}} \log f(Y, \widehat{\theta})(\widetilde{\theta}-\widehat{\theta})-\frac{1}{2}(\widetilde{\theta}-\widehat{\theta})^{\prime} H(\widetilde{\theta}-\widehat{\theta})+O_{p}\left(n^{-1 / 2}\right) \\
&amp;=\log f(Y, \widehat{\theta})-\frac{n}{2}(\widetilde{\theta}-\widehat{\theta})^{\prime} H(\widetilde{\theta}-\widehat{\theta})+O_{p}\left(n^{-1 / 2}\right) .
\end{aligned}
\]</span></p>
<p>The second equality holds because of the first-order condition for the MLE <span class="math inline">\(\widehat{\theta}\)</span>. If the <span class="math inline">\(O_{p}\left(n^{-1 / 2}\right)\)</span> term in (28.34) is uniformly integrable (28.33) and (28.34) imply that</p>
<p><span class="math display">\[
\begin{aligned}
T &amp;=-\mathbb{E}[2 \log f(Y, \widehat{\theta})]+\mathbb{E}\left[n(\widetilde{\theta}-\widehat{\theta})^{\prime} H(\widetilde{\theta}-\widehat{\theta})\right]+O\left(n^{-1 / 2}\right) \\
&amp;=-\mathbb{E}[2 \log L(\widehat{\theta})]+\mathbb{E}\left[n(\widetilde{\theta}-\theta)^{\prime} H(\widetilde{\theta}-\theta)\right]+\mathbb{E}\left[n(\widehat{\theta}-\theta)^{\prime} H(\widehat{\theta}-\theta)\right] \\
&amp;+2 \mathbb{E}\left[n(\widetilde{\theta}-\theta)^{\prime} H(\widehat{\theta}-\theta)\right]+O\left(n^{-1 / 2}\right) \\
&amp;=-\mathbb{E}\left[2 \ell_{n}(\widehat{\theta})\right]+\mathbb{E}\left[\chi_{K}^{2}\right]+\mathbb{E}\left[\widetilde{\chi}_{K}^{2}\right]+O\left(n^{-1 / 2}\right) \\
&amp;=-\mathbb{E}\left[2 \ell_{n}(\widehat{\theta})\right]+2 K+O\left(n^{-1 / 2}\right)
\end{aligned}
\]</span></p>
<p>where <span class="math inline">\(\chi_{K}^{2}\)</span> and <span class="math inline">\(\widetilde{\chi}_{K}^{2}\)</span> are chi-square random variables with <span class="math inline">\(K\)</span> degrees of freedom. The second-to-last equality holds if</p>
<p><span class="math display">\[
n(\widehat{\theta}-\theta)^{\prime} H(\widehat{\theta}-\theta) \underset{d}{\longrightarrow} \chi_{K}^{2}
\]</span></p>
<p>and the Wald statistic on the left-side of (28.35) is uniformly integrable. The asymptotic convergence (28.35) holds for the MLE under standard regularity conditions (including correct specification).</p>
<p>Proof of Theorem 28.5 Using matrix notation we can write <span class="math inline">\(\widehat{\boldsymbol{m}}-\boldsymbol{m}=-\left(\boldsymbol{I}_{n}-\boldsymbol{A}\right) \boldsymbol{m}+\boldsymbol{A} \boldsymbol{e}\)</span>. We can then write the fit as</p>
<p><span class="math display">\[
\begin{aligned}
R &amp;=\mathbb{E}\left[(\widehat{\boldsymbol{m}}-\boldsymbol{m})^{\prime}(\widehat{\boldsymbol{m}}-\boldsymbol{m}) \mid \boldsymbol{X}\right] \\
&amp;=\mathbb{E}\left[\boldsymbol{m}^{\prime}\left(\boldsymbol{I}_{n}-\boldsymbol{A}^{\prime}\right)\left(\boldsymbol{I}_{n}-\boldsymbol{A}\right) \boldsymbol{m}-2 \boldsymbol{m}^{\prime}\left(\boldsymbol{I}_{n}-\boldsymbol{A}^{\prime}\right) \boldsymbol{A} \boldsymbol{e}+\boldsymbol{e}^{\prime} \boldsymbol{A}^{\prime} \boldsymbol{A} \boldsymbol{e} \mid \boldsymbol{X}\right] \\
&amp;=\boldsymbol{m}^{\prime}\left(\boldsymbol{I}_{n}-\boldsymbol{A}^{\prime}\right)\left(\boldsymbol{I}_{n}-\boldsymbol{A}\right) \boldsymbol{m}+\sigma^{2} \operatorname{tr}\left(\boldsymbol{A}^{\prime} \boldsymbol{A}\right) .
\end{aligned}
\]</span></p>
<p>Notice that this calculation relies on the assumption of conditional homoskedasticity.</p>
<p>Now consider the Mallows criterion. We find that</p>
<p><span class="math display">\[
\begin{aligned}
C_{p}^{*} &amp;=\widehat{\boldsymbol{e}}^{\prime} \widehat{\boldsymbol{e}}+2 \widetilde{\sigma}^{2} \operatorname{tr}(\boldsymbol{A})-\boldsymbol{e}^{\prime} \boldsymbol{e} \\
&amp;=(\boldsymbol{m}+\boldsymbol{e})^{\prime}\left(\boldsymbol{I}_{n}-\boldsymbol{A}^{\prime}\right)\left(\boldsymbol{I}_{n}-\boldsymbol{A}\right)(\boldsymbol{m}+\boldsymbol{e})+2 \widetilde{\sigma}^{2} \operatorname{tr}(\boldsymbol{A})-\boldsymbol{e}^{\prime} \boldsymbol{e} \\
&amp;=\boldsymbol{m}^{\prime}\left(\boldsymbol{I}_{n}-\boldsymbol{A}^{\prime}\right)\left(\boldsymbol{I}_{n}-\boldsymbol{A}\right) \boldsymbol{m}+2 \boldsymbol{m}^{\prime}\left(\boldsymbol{I}_{n}-\boldsymbol{A}^{\prime}\right)\left(\boldsymbol{I}_{n}-\boldsymbol{A}\right) \boldsymbol{e}+\boldsymbol{e}^{\prime} \boldsymbol{A}^{\prime} \boldsymbol{A} \boldsymbol{e}-2 \boldsymbol{e}^{\prime} \boldsymbol{A} \boldsymbol{e}+2 \widetilde{\sigma}^{2} \operatorname{tr}(\boldsymbol{A}) .
\end{aligned}
\]</span></p>
<p>Taking expectations and using the assumptions of conditional homoskedasticity and <span class="math inline">\(\mathbb{E}\left[\widetilde{\sigma}^{2} \mid \boldsymbol{X}\right]=\sigma^{2}\)</span></p>
<p><span class="math display">\[
\mathbb{E}\left[C_{p}^{*} \mid \boldsymbol{X}\right]=\boldsymbol{m}^{\prime}\left(\boldsymbol{I}_{n}-\boldsymbol{A}^{\prime}\right)\left(\boldsymbol{I}_{n}-\boldsymbol{A}\right) \boldsymbol{m}+\sigma^{2} \operatorname{tr}\left(\boldsymbol{A}^{\prime} \boldsymbol{A}\right)=R
\]</span></p>
<p>This is the result as stated.</p>
<p>Proof of Theorem 28.6 Take any two models <span class="math inline">\(\mathscr{M}_{1}\)</span> and <span class="math inline">\(\mathscr{M}_{2}\)</span> where <span class="math inline">\(\mathcal{M}_{1} \notin \overline{\mathscr{M}}^{*}\)</span> and <span class="math inline">\(\mathscr{M}_{2} \in \overline{\mathscr{M}}^{*}\)</span>. Let their information criteria be written as</p>
<p><span class="math display">\[
\begin{aligned}
&amp;\mathrm{IC}_{1}=-2 \ell_{1}\left(\widehat{\theta}_{1}\right)+c\left(n, K_{1}\right) \\
&amp;\mathrm{IC}_{2}=-2 \ell_{2}\left(\widehat{\theta}_{2}\right)+c\left(n, K_{2}\right) .
\end{aligned}
\]</span></p>
<p>Model <span class="math inline">\(\mathscr{M}_{1}\)</span> is selected over <span class="math inline">\(\mathscr{M}_{2}\)</span> if</p>
<p><span class="math display">\[
\mathrm{LR}&lt;c\left(n, K_{2}\right)-c\left(n, K_{1}\right)
\]</span></p>
<p>where <span class="math inline">\(\mathrm{LR}=2\left(\ell_{2}\left(\widehat{\theta}_{2}\right)-\ell\left(\widehat{\theta}_{1}\right)\right)\)</span> is the likelihood ratio statistic for testing <span class="math inline">\(\mathcal{M}_{1}\)</span> against <span class="math inline">\(\mathcal{M}_{2}\)</span>. Since we have assumed that <span class="math inline">\(\mathscr{M}_{1}\)</span> is not a true model while <span class="math inline">\(\mathscr{M}_{2}\)</span> is true, then LR diverges to <span class="math inline">\(+\infty\)</span> at rate <span class="math inline">\(n\)</span>. This means that for any <span class="math inline">\(\alpha&gt;0, n^{-1+\alpha} \mathrm{LR} \underset{p}{\rightarrow}+\infty\)</span>. Furthermore, the assumptions imply <span class="math inline">\(n^{-1+\alpha}\left(c\left(n, K_{1}\right)-c\left(n, K_{2}\right)\right) \longrightarrow 0\)</span>. Fix <span class="math inline">\(\epsilon&gt;0\)</span>. There is an <span class="math inline">\(n\)</span> sufficiently large such that <span class="math inline">\(n^{-1+\alpha}\left(c\left(n, K_{1}\right)-c\left(n, K_{2}\right)\right)&lt;\epsilon\)</span>. Thus</p>
<p><span class="math display">\[
\begin{aligned}
\mathbb{P}\left[\widehat{M}=\mathscr{M}_{1}\right] &amp; \leq \mathbb{P}\left[n^{-1+\alpha} \operatorname{LR}&lt;n^{-1+\alpha}\left(c\left(n, K_{2}\right)-c\left(n, K_{1}\right)\right)\right] \\
&amp; \leq \mathbb{P}[\mathrm{LR}&lt;\epsilon] \rightarrow 0 .
\end{aligned}
\]</span></p>
<p>Since this holds for any <span class="math inline">\(\mathscr{M}_{1} \notin \overline{\mathscr{M}}^{*}\)</span> we deduce that the selected model is in <span class="math inline">\(\overline{\mathscr{M}}^{*}\)</span> with probability approaching one. This means that the selection criterion is model selection consistent as claimed.</p>
<p>Proof of Theorem 28.7 Take the setting as described in the proof of Theorem <span class="math inline">\(28.6\)</span> but now assume <span class="math inline">\(\mathscr{M}_{1} \subset\)</span> <span class="math inline">\(\mathcal{M}_{2}\)</span> and <span class="math inline">\(\mathscr{M}_{1}, \mathscr{M}_{2} \in \overline{\mathcal{M}}^{*}\)</span>. The likelihood ratio statistic satisfies LR <span class="math inline">\(\underset{d}{\longrightarrow} \chi_{r}^{2}\)</span> where <span class="math inline">\(r=K_{2}-K_{1}\)</span>. Let</p>
<p><span class="math display">\[
B=\limsup _{n \rightarrow \infty}\left(c\left(n, K_{1}\right)-c\left(n, K_{2}\right)\right)&lt;\infty .
\]</span></p>
<p>Letting <span class="math inline">\(F_{r}(u)\)</span> denote the <span class="math inline">\(\chi_{r}^{2}\)</span> distribution function</p>
<p><span class="math display">\[
\begin{aligned}
\mathbb{P}\left[\widehat{\mathscr{M}}=\mathscr{M}_{2}\right] &amp;=\mathbb{P}\left[\operatorname{LR}&gt;\left(c\left(n, K_{2}\right)-c\left(n, K_{1}\right)\right)\right] \\
&amp; \geq \mathbb{P}[\operatorname{LR}&gt;B] \\
&amp; \rightarrow \mathbb{P}\left[\chi_{r}^{2}&gt;B\right]=1-F_{r}(B)&gt;0
\end{aligned}
\]</span></p>
<p>because <span class="math inline">\(\chi_{r}^{2}\)</span> has support over the positive real line and <span class="math inline">\(B&lt;\infty\)</span>. This shows that the selection criterion asymptotically over-selects with positive probability.</p>
<p>Proof of Theorem <span class="math inline">\(28.8\)</span> Since <span class="math inline">\(c(n, K)=o(n)\)</span> the procedure is model selection consistent. Take two models <span class="math inline">\(\mathscr{M}_{1}, \mathscr{M}_{2} \in \overline{\mathscr{M}}^{*}\)</span> with <span class="math inline">\(K_{1}&lt;K_{2}\)</span>. Since both models are true then LR <span class="math inline">\(=O_{p}(1)\)</span>. Fix <span class="math inline">\(\epsilon&gt;0\)</span>. There is a <span class="math inline">\(B&lt;\infty\)</span> such that <span class="math inline">\(\mathrm{LR} \leq B\)</span> with probability exceeding <span class="math inline">\(1-\epsilon\)</span>. By (28.16) there is an <span class="math inline">\(n\)</span> sufficiently large such that <span class="math inline">\(c\left(n, K_{2}\right)-c\left(n, K_{1}\right)&gt;B\)</span>. Thus</p>
<p><span class="math display">\[
\mathbb{P}\left[\widehat{\mathscr{M}}=\mathscr{M}_{2}\right] \leq \mathbb{P}\left[\mathrm{LR}&gt;\left(c\left(n, K_{2}\right)-c\left(n, K_{1}\right)\right)\right] \leq \mathbb{P}[\mathrm{LR}&gt;B] \leq \epsilon .
\]</span></p>
<p>Since <span class="math inline">\(\epsilon\)</span> is arbitrary <span class="math inline">\(\mathbb{P}\left[\widehat{\mathscr{M}}=\mathscr{M}_{2}\right] \longrightarrow 0\)</span> as claimed.</p>
<p>Proof of Theorem 28.9 First, we examine <span class="math inline">\(R_{n}(K)\)</span>. Write the predicted values in matrix notation as <span class="math inline">\(\widehat{\boldsymbol{m}}_{K}=\)</span> <span class="math inline">\(\boldsymbol{X}_{K} \widehat{\beta}_{K}=\boldsymbol{P}_{K} \boldsymbol{Y}\)</span> where <span class="math inline">\(\boldsymbol{P}_{K}=\boldsymbol{X}_{K}\left(\boldsymbol{X}_{K}^{\prime} \boldsymbol{X}_{K}\right)^{-1} \boldsymbol{X}_{K}^{\prime}\)</span>. It is useful to observe that <span class="math inline">\(\boldsymbol{m}-\widehat{\boldsymbol{m}}_{K}=\boldsymbol{M}_{K} \boldsymbol{m}-\boldsymbol{P}_{K} \boldsymbol{e}\)</span> where <span class="math inline">\(\boldsymbol{M}_{K}=\boldsymbol{I}_{K}-\boldsymbol{P}_{K}\)</span>. We find that the prediction risk equals</p>
<p><span class="math display">\[
\begin{aligned}
R_{n}(K) &amp;=\mathbb{E}\left[\left(\boldsymbol{m}-\widehat{\boldsymbol{m}}_{K}\right)^{\prime}\left(\boldsymbol{m}-\widehat{\boldsymbol{m}}_{K}\right) \mid \boldsymbol{X}\right] \\
&amp;=\mathbb{E}\left[\left(\boldsymbol{M}_{K} \boldsymbol{m}-\boldsymbol{P}_{K} \boldsymbol{e}\right)^{\prime}\left(\boldsymbol{M}_{K} \boldsymbol{m}-\boldsymbol{P}_{K} \boldsymbol{e}\right) \mid \boldsymbol{X}\right] \\
&amp;=\boldsymbol{m}^{\prime} \boldsymbol{M}_{K} \boldsymbol{m}+\mathbb{E}\left[\boldsymbol{e}^{\prime} \boldsymbol{P}_{K} \boldsymbol{e} \mid \boldsymbol{X}\right] \\
&amp;=\boldsymbol{m}^{\prime} \boldsymbol{M}_{K} \boldsymbol{m}+\sigma^{2} K .
\end{aligned}
\]</span></p>
<p>The choice of regressors affects <span class="math inline">\(R_{n}(K)\)</span> through the two terms in the final line. The first term <span class="math inline">\(\boldsymbol{m}^{\prime} \boldsymbol{M}_{K} \boldsymbol{m}\)</span> is the squared bias due to omitted variables. As <span class="math inline">\(K\)</span> increases this term decreases reflecting reduced omitted variables bias. The second term <span class="math inline">\(\sigma^{2} K\)</span> is estimation variance. It is increasing in the number of regressors. Increasing the number of regressors affects the quality of out-of-sample prediction by reducing the bias but increasing the variance. We next examine the adjusted Mallows criterion. We find that</p>
<p><span class="math display">\[
\begin{aligned}
C_{n}^{*}(K) &amp;=\widehat{\boldsymbol{e}}_{K}^{\prime} \widehat{\boldsymbol{e}}_{K}+2 \sigma^{2} K-\boldsymbol{e}^{\prime} \boldsymbol{e} \\
&amp;=(\boldsymbol{m}+\boldsymbol{e})^{\prime} \boldsymbol{M}_{K}(\boldsymbol{m}+\boldsymbol{e})+2 \sigma^{2} K-\boldsymbol{e}^{\prime} \boldsymbol{e} \\
&amp;=\boldsymbol{m}^{\prime} \boldsymbol{M}_{K} \boldsymbol{m}+2 \boldsymbol{m}^{\prime} \boldsymbol{M}_{K} \boldsymbol{e}-\boldsymbol{e}^{\prime} \boldsymbol{P}_{K} \boldsymbol{e}+2 \sigma^{2} K .
\end{aligned}
\]</span></p>
<p>The next step is to show that</p>
<p><span class="math display">\[
\sup _{K}\left|\frac{C_{n}^{*}(K)-R_{n}(K)}{R_{n}(K)}\right| \underset{p}{\longrightarrow} 0
\]</span></p>
<p>as <span class="math inline">\(n \rightarrow \infty\)</span>. To establish (28.36), observe that</p>
<p><span class="math display">\[
C_{n}^{*}(K)-R_{n}(K)=2 \boldsymbol{m}^{\prime} \boldsymbol{M}_{K} \boldsymbol{e}-\boldsymbol{e}^{\prime} \boldsymbol{P}_{K} \boldsymbol{e}+\sigma^{2} K .
\]</span></p>
<p>Pick <span class="math inline">\(\epsilon&gt;0\)</span> and some sequence <span class="math inline">\(B_{n} \rightarrow \infty\)</span> such that <span class="math inline">\(B_{n} /\left(R_{n}^{\text {opt }}\right)^{r} \rightarrow 0\)</span>. (This is feasible by Assumption 28.1.5.) By Boole’s inequality (B.24), Whittle’s inequality (B.48), the facts that <span class="math inline">\(\boldsymbol{m}^{\prime} \boldsymbol{M}_{K} \boldsymbol{m} \leq R_{n}(K)\)</span> and <span class="math inline">\(R_{n}(K) \geq \sigma^{2} K\)</span>, <span class="math inline">\(B_{n} /\left(R_{n}^{\text {opt }}\right)^{r} \rightarrow 0\)</span>, and <span class="math inline">\(\sum_{K=1}^{\infty} K^{-r}&lt;\infty\)</span></p>
<p><span class="math display">\[
\begin{aligned}
&amp; \mathbb{P}\left[\sup _{K}\left|\frac{\boldsymbol{m}^{\prime} \boldsymbol{M}_{K} \boldsymbol{e}}{R_{n}(K)}\right|&gt;\epsilon \mid \boldsymbol{X}\right] \leq \sum_{K=1}^{\infty} \mathbb{P}\left[\left|\frac{\boldsymbol{m}^{\prime} \boldsymbol{M}_{K} \boldsymbol{e}}{R_{n}(K)}\right|&gt;\epsilon \mid \boldsymbol{X}\right] \\
&amp; \leq \frac{C_{1} r}{\epsilon^{2 r}} \sum_{K=1}^{\infty} \frac{\left|\boldsymbol{m}^{\prime} \boldsymbol{M}_{K} \boldsymbol{m}\right|^{r}}{R_{n}(K)^{2 r}} \\
&amp; \leq \frac{C_{1 r}}{\epsilon^{2 r}} \sum_{K=1}^{\infty} \frac{1}{R_{n}(K)^{r}} \\
&amp; =\frac{C_{1} r}{\epsilon^{2 r}} \sum_{K=1}^{B_{n}} \frac{1}{R_{n}(K)^{r}}+\frac{C_{1 r}}{\epsilon^{2 r}} \sum_{K=B_{n}+1}^{\infty} \frac{1}{R_{n}(K)^{r}} \\
&amp; \leq \frac{C_{1 r}}{\epsilon^{2 r}} \frac{B_{n}}{\left(R_{n}^{\mathrm{opt}}\right)^{r}}+\frac{C_{1 r}}{\epsilon^{2 r} \sigma^{2 r}} \sum_{K=B_{n}+1}^{\infty} \frac{1}{K^{r}} \\
&amp; \rightarrow 0 \text {. }
\end{aligned}
\]</span></p>
<p>By a similar argument but using Whittle’s inequality (B.49), <span class="math inline">\(\operatorname{tr}\left(\boldsymbol{P}_{K} \boldsymbol{P}_{K}\right)=\operatorname{tr}\left(\boldsymbol{P}_{K}\right)=K\)</span>, and <span class="math inline">\(K \leq \sigma^{-2} R_{n}(K)\)</span></p>
<p><span class="math display">\[
\begin{aligned}
\mathbb{P}\left[\sup _{K}\left|\frac{\boldsymbol{e}^{\prime} \boldsymbol{P}_{K} \boldsymbol{e}-\sigma^{2} K}{R_{n}(K)}\right|&gt;\epsilon \mid \boldsymbol{X}\right] &amp; \leq \sum_{K=1}^{\infty} \mathbb{P}\left[\left|\frac{\boldsymbol{e}^{\prime} \boldsymbol{P}_{K} \boldsymbol{e}-\mathbb{E}\left(\boldsymbol{e}^{\prime} \boldsymbol{P}_{K} \boldsymbol{e}\right)}{R_{n}(K)}\right|&gt;\epsilon \mid \boldsymbol{X}\right] \\
&amp; \leq \frac{C_{2 r}}{\epsilon^{2 r}} \sum_{K=1}^{\infty} \frac{\operatorname{tr}\left(\boldsymbol{P}_{K} \boldsymbol{P}_{K}\right)^{r}}{R_{n}(K)^{2 r}} \\
&amp;=\frac{C_{2 r}}{\epsilon^{2 r}} \sum_{K=1}^{\infty} \frac{K^{r}}{R_{n}(K)^{2 r}} \\
&amp; \leq \frac{C_{1 r}}{\epsilon^{2 r} \sigma^{2 r}} \sum_{K=1}^{\infty} \frac{1}{R_{n}(K)^{r}} \\
&amp; \rightarrow 0 .
\end{aligned}
\]</span></p>
<p>Together these imply (28.36).</p>
<p>Finally we show that (28.36) implies (28.18). The argument is similar to the standard consistency proof for nonlinear estimators. (28.36) states that <span class="math inline">\(C_{n}^{*}(K)\)</span> converges uniformly in probability to <span class="math inline">\(R_{n}(K)\)</span>. This implies that the minimizer of <span class="math inline">\(C_{n}^{*}(K)\)</span> converges in probability to that of <span class="math inline">\(R_{n}(K)\)</span>. Formally, because <span class="math inline">\(K_{n}^{\mathrm{opt}}\)</span> minimizes <span class="math inline">\(R_{n}(K)\)</span></p>
<p><span class="math display">\[
\begin{aligned}
0 &amp; \leq \frac{R_{n}\left(\widehat{K}_{n}\right)-R_{n}\left(K_{n}^{\mathrm{opt}}\right)}{R_{n}\left(\widehat{K}_{n}\right)} \\
&amp;=\frac{C_{n}^{*}\left(\widehat{K}_{n}\right)-R_{n}\left(K_{n}^{\mathrm{opt}}\right)}{R_{n}\left(\widehat{K}_{n}\right)}-\frac{C_{n}^{*}\left(\widehat{K}_{n}\right)-R_{n}\left(\widehat{K}_{n}\right)}{-R_{n}\left(\widehat{K}_{n}\right)} \\
&amp; \leq \frac{C_{n}^{*}\left(\widehat{K}_{n}\right)-R_{n}\left(K_{n}^{\mathrm{opt}}\right)}{R_{n}\left(\widehat{K}_{n}\right)}+o_{p}(1) \\
&amp; \leq \frac{C_{n}^{*}\left(K_{n}^{\mathrm{opt}}\right)-R_{n}\left(K_{n}^{\mathrm{opt}}\right)}{R_{n}\left(K_{n}^{\mathrm{opt}}\right)}+o_{p}(1) \\
&amp; \leq o_{p}(1) .
\end{aligned}
\]</span></p>
<p>The second inequality is (28.36). The following uses the facts that <span class="math inline">\(\widehat{K}_{n}\)</span> minimizes <span class="math inline">\(C_{n}^{*}(K)\)</span> and <span class="math inline">\(K_{n}^{\text {opt }}\)</span> minimizes <span class="math inline">\(R_{n}(K)\)</span>. The final is (28.36). This is (28.18).</p>
<p>Before providing the proof of Theorem <span class="math inline">\(28.10\)</span> we present two technical results related to the noncentral chi-square density function with degree of freedom <span class="math inline">\(K\)</span> and non-centrality parameter <span class="math inline">\(\lambda\)</span> which equals</p>
<p><span class="math display">\[
f_{K}(x, \lambda)=\sum_{i=0}^{\infty} \frac{e^{-\lambda / 2}}{i !}\left(\frac{\lambda}{2}\right)^{i} f_{K+2 i}(x)
\]</span></p>
<p>where <span class="math inline">\(f_{r}(x)=\frac{x^{r / 2-1} e^{-x / 2}}{\left.2^{r / 2} \Gamma(r / 2)\right)}\)</span> is the <span class="math inline">\(\chi_{K}^{2}\)</span> density function.</p>
<p>Theorem 28.16 The non-central chi-square density function (28.37) obeys the recursive relationship <span class="math inline">\(f_{K}(x, \lambda)=\frac{K}{x} f_{K+2}(x, \lambda)+\frac{\lambda}{x} f_{K+4}(x, \lambda)\)</span>.</p>
<p>The proof of Theorem <span class="math inline">\(28.16\)</span> is a straightforward manipulation of the non-central chi-square density function (28.37).</p>
<p>The second technical result is from Bock (1975, Theorems A&amp;B).</p>
<p>Theorem 28.17 If <span class="math inline">\(X \sim \mathrm{N}\left(\theta, \boldsymbol{I}_{K}\right)\)</span> then for any function <span class="math inline">\(h(u)\)</span></p>
<p><span class="math display">\[
\begin{aligned}
\mathbb{E}\left[X h\left(X^{\prime} X\right)\right] &amp;=\theta \mathbb{E}\left[h\left(Q_{K+2}\right)\right] \\
\mathbb{E}\left[X^{\prime} X h\left(X^{\prime} X\right)\right] &amp;=K \mathbb{E}\left[h\left(Q_{K+2}\right)\right]+\lambda \mathbb{E}\left[h\left(Q_{K+4}\right)\right]
\end{aligned}
\]</span></p>
<p>where <span class="math inline">\(\lambda=\theta^{\prime} \theta\)</span> and <span class="math inline">\(Q_{r} \sim \chi_{r}^{2}(\lambda)\)</span>, a non-central chi-square random variable with <span class="math inline">\(r\)</span> degrees of freedom and non-centrality parameter <span class="math inline">\(\lambda\)</span>.</p>
<p>Proof of Theorem 28.17 To show (28.38) we first show that for <span class="math inline">\(Z \sim \mathrm{N}(\mu, 1)\)</span> then for any function <span class="math inline">\(g(u)\)</span></p>
<p><span class="math display">\[
\mathbb{E}\left[Z g\left(Z^{2}\right)\right]=\mu \mathbb{E}\left[g\left(Q_{3}\right)\right] .
\]</span></p>
<p>Assume <span class="math inline">\(\mu&gt;0\)</span>. Using the change-of-variables <span class="math inline">\(y=x^{2}\)</span></p>
<p><span class="math display">\[
\begin{aligned}
\mathbb{E}\left[Z g\left(Z^{2}\right)\right] &amp;=\int_{-\infty}^{\infty} \frac{x}{\sqrt{2 \pi}} g\left(x^{2}\right) \exp \left(-\frac{1}{2}(x-\mu)^{2}\right) d x \\
&amp;=\int_{0}^{\infty} \frac{y}{2 \sqrt{2 \pi}} e^{-\left(y+\mu^{2}\right) / 2}\left(e^{\sqrt{y} \mu}-e^{-\sqrt{y} \mu}\right) g(y) d y .
\end{aligned}
\]</span></p>
<p>By expansion and Legendre’s duplication formula</p>
<p><span class="math display">\[
e^{x}-e^{-x}=2 \sum_{i=0}^{\infty} \frac{x^{1+2 i}}{(1+2 i) !}=\sqrt{\pi} x \sum_{i=0}^{\infty} \frac{\left(x^{2} / 2\right)^{i}}{2^{i} i ! \Gamma(i+3 / 2)} .
\]</span></p>
<p>Then (28.41) equals</p>
<p><span class="math display">\[
\mu \int_{0}^{\infty} y e^{-\left(y+\mu^{2}\right) / 2} \sum_{i=0}^{\infty} \frac{\left(\mu^{2} / 2\right)^{i} y^{i+1 / 2}}{2^{3 / 2+i} i ! \Gamma(i+3 / 2)} g(y) d y=\mu \int_{0}^{\infty} y f_{3}\left(y, \mu^{2}\right) g(y) d y=\mu \mathbb{E}\left[g\left(Q_{3}\right)\right]
\]</span></p>
<p>where <span class="math inline">\(f_{3}(y, \lambda)\)</span> is the non-central chi-square density (28.37) with 3 degrees of freedom. This is (28.40).</p>
<p>Take the <span class="math inline">\(j^{t h}\)</span> row of (28.38). Write <span class="math inline">\(X^{\prime} X=X_{j}^{2}+J\)</span>, where <span class="math inline">\(X_{j} \sim \mathrm{N}\left(\theta_{j}, 1\right)\)</span> and <span class="math inline">\(J \sim \chi_{K-1}^{2}\left(\lambda-\theta_{j}^{2}\right)\)</span> are independent. Setting <span class="math inline">\(g(u)=h(u+J)\)</span> and using (28.41)</p>
<p><span class="math display">\[
\begin{aligned}
\mathbb{E}\left[X_{j} h\left(X^{\prime} X\right)\right] &amp;=\mathbb{E}\left[X_{j} h\left(X_{j}^{2}+J\right)\right] \\
&amp;=\mathbb{E}\left[\mathbb{E}\left[X_{j} g\left(X_{j}^{2}\right) \mid J\right]\right] \\
&amp;=\mathbb{E}\left[\theta_{j} \mathbb{E}\left[g\left(Q_{3}\right) \mid J\right]\right] \\
&amp;=\theta_{j} \mathbb{E}\left[h\left(Q_{3}+J\right)\right] \\
&amp;=\theta_{j} \mathbb{E}\left[h\left(Q_{K+2}\right)\right]
\end{aligned}
\]</span></p>
<p>which is (28.38). The final equality uses the fact that <span class="math inline">\(Q_{3}+J \sim Q_{K+2}\)</span>.</p>
<p>Observe that <span class="math inline">\(X^{\prime} X\)</span> has density <span class="math inline">\(f_{K}(x, \lambda)\)</span>. Using Theorem <span class="math inline">\(28.16\)</span></p>
<p><span class="math display">\[
\begin{aligned}
\mathbb{E}\left[X^{\prime} X\left(X^{\prime} X\right)\right] &amp;=\int_{0}^{\infty} x h(x) f_{K}(x, \lambda) d x \\
&amp;=K \int_{0}^{\infty} h(x) f_{K+2}(x, \lambda) d x+\lambda \int_{0}^{\infty} h(x) f_{K+4}(x, \lambda) d x \\
&amp;=K \mathbb{E}\left[h\left(Q_{K+2}\right)\right]+\lambda \mathbb{E}\left[h\left(Q_{K+4}\right)\right]
\end{aligned}
\]</span></p>
<p>which is (28.39).</p>
<p>Proof of Theorem 28.10 By the quadratic structure we can calculate that</p>
<p><span class="math display">\[
\begin{aligned}
\operatorname{MSE}\left[\widehat{\theta}^{*}\right] &amp;=\mathbb{E}\left[\left(\widehat{\theta}-\theta-\widehat{\theta} \mathbb{1}\left\{\widehat{\theta}^{\prime} \hat{\theta} \leq c\right\}\right)^{\prime}\left(\widehat{\theta}-\theta-\widehat{\theta} \mathbb{1}\left\{\widehat{\theta}^{\prime} \widehat{\theta} \leq c\right\}\right)\right] \\
&amp;=\mathbb{E}\left[(\widehat{\theta}-\theta)^{\prime}(\widehat{\theta}-\theta)\right]-\mathbb{E}\left[\widehat{\theta}^{\prime} \widehat{\theta} \mathbb{1}\left\{\widehat{\theta}^{\prime} \widehat{\theta} \leq c\right\}\right]+2 \mathbb{E}\left[\theta^{\prime} \widehat{\theta} \mathbb{1}\left\{\widehat{\theta}^{\prime} \widehat{\theta} \leq c\right\}\right] \\
&amp;=K-K \mathbb{E}\left[\mathbb{1}\left\{Q_{K+2} \leq c\right\}\right]-\lambda \mathbb{E}\left[\mathbb{1}\left\{Q_{K+4} \leq c\right\}\right]+2 \lambda \mathbb{E}\left[\mathbb{1}\left\{Q_{K+2} \leq c\right\}\right] \\
&amp;=K+(2 \lambda-K) F_{K+2}(c, \lambda)-\lambda F_{K+4}(c, \lambda) .
\end{aligned}
\]</span></p>
<p>The third equality uses the two results from Theorem 28.17, setting <span class="math inline">\(h(u)=\mathbb{1}\{u \leq c\}\)</span>.</p>
</section>
<section id="exercises" class="level2" data-number="26.40">
<h2 data-number="26.40" class="anchored" data-anchor-id="exercises"><span class="header-section-number">26.40</span> Exercises</h2>
<p>Exercise 28.1 Verify equations (28.1)-(28.2).</p>
<p>Exercise 28.2 Find the Mallows criterion for the weighted least squares estimator of a linear regression <span class="math inline">\(Y_{i}=X_{i}^{\prime} \beta+e_{i}\)</span> with weights <span class="math inline">\(\omega_{i}\)</span> (assume conditional homoskedasticity).</p>
<p>Exercise 28.3 Backward Stepwise Regression. Verify the claim that for the case of AIC selection, step (b) of the algorithm can be implemented by calculating the classical (homoskedastic) t-ratio for each active regressor and find the regressor with the smallest absolute t-ratio.</p>
<p>Hint: Use the relationship between likelihood ratio and F statistics and the equality between <span class="math inline">\(\mathrm{F}\)</span> and Wald statistics to show that for tests on one coefficient the smallest change in the AIC is identical to identifying the smallest squared t statistic.</p>
<p>Exercise 28.4 Forward Stepwise Regression. Verify the claim that for the case of AIC selection, step (b) of the algorithm can be implemented by identifying the regressor in the inactive set with the greatest absolute correlation with the residual from step (a).</p>
<p>Hint: This is challenging. First show that the goal is to find the regressor which will most decrease SSE <span class="math inline">\(=\widehat{\boldsymbol{e}}^{\prime} \widehat{\boldsymbol{e}}=\|\widehat{\boldsymbol{e}}\|^{2}\)</span>. Use a geometric argument to show that the regressor most parallel to <span class="math inline">\(\widehat{\boldsymbol{e}}\)</span> will most decreases <span class="math inline">\(\|\widehat{\boldsymbol{e}}\|\)</span>. Show that this regressor has the greatest absolute correlation with <span class="math inline">\(\widehat{\boldsymbol{e}}\)</span>.</p>
<p>Exercise 28.5 An economist estimates several models and reports a single selected specification, stating that “the other specifications had insignificant coefficients”. How should we interpret the reported parameter estimates and t-ratios?</p>
<p>Exercise 28.6 Verify Theorem 28.11, including (28.21), (28.22), and (28.23).</p>
<p>Exercise 28.7 Under the assumptions of Theorem 28.11, show that <span class="math inline">\(\hat{\lambda}=\widehat{\theta}^{\prime} \boldsymbol{V}^{-1} \widehat{\theta}-K\)</span> is an unbiased estimator of <span class="math inline">\(\lambda=\theta^{\prime} \boldsymbol{V}^{-1} \theta\)</span>.</p>
<p>Exercise 28.8 Prove Theorem <span class="math inline">\(28.14\)</span> for the simpler case of the unadjusted (not positive part) Stein estimator <span class="math inline">\(\widetilde{\theta}, \boldsymbol{V}=\boldsymbol{I}_{K}\)</span> and <span class="math inline">\(r=0\)</span>.</p>
<p>Extra challenge: Show under these assumptions that</p>
<p><span class="math display">\[
\begin{aligned}
\operatorname{wmse}[\tilde{\theta}] &amp;=K-(q-2)^{2} J_{q}\left(\lambda_{R}\right) \\
\lambda_{\boldsymbol{R}} &amp;=\theta^{\prime} \boldsymbol{R}\left(\boldsymbol{R}^{\prime} \boldsymbol{R}\right)^{-1} \boldsymbol{R}^{\prime} \theta .
\end{aligned}
\]</span></p>
<p>Exercise 28.9 Suppose you have two unbiased estimators <span class="math inline">\(\widehat{\theta}_{1}\)</span> and <span class="math inline">\(\widehat{\theta}_{2}\)</span> of a parameter vector <span class="math inline">\(\widehat{\theta}\)</span> with covariance matrices <span class="math inline">\(\boldsymbol{V}_{1}\)</span> and <span class="math inline">\(\boldsymbol{V}_{2}\)</span>. Take the goal of minimizing the unweighted mean squared error, e.g.&nbsp;<span class="math inline">\(\operatorname{tr} \boldsymbol{V}_{1}\)</span> for <span class="math inline">\(\widehat{\theta}_{1}\)</span>. Assume that <span class="math inline">\(\widehat{\theta}_{1}\)</span> and <span class="math inline">\(\widehat{\theta}_{2}\)</span> are uncorrelated.</p>
<ol type="a">
<li>Show that the optimal weighted average estimator equals</li>
</ol>
<p><span class="math display">\[
\frac{\frac{1}{\operatorname{tr} \boldsymbol{V}_{1}} \widehat{\theta}_{1}+\frac{1}{\operatorname{tr} \boldsymbol{V}_{2}} \widehat{\theta}_{2}}{\frac{1}{\operatorname{tr} \boldsymbol{V}_{1}}+\frac{1}{\operatorname{tr} \boldsymbol{V}_{2}}} .
\]</span></p>
<ol start="2" type="a">
<li><p>Generalize to the case of <span class="math inline">\(M\)</span> unbiased uncorrelated estimators.</p></li>
<li><p>Interpret the formulae. Exercise 28.10 You estimate <span class="math inline">\(M\)</span> linear regressions <span class="math inline">\(Y=X_{m}^{\prime} \beta_{m}+e_{m}\)</span> by least squares. Let <span class="math inline">\(\widehat{Y}_{m i}=X_{m i}^{\prime} \widehat{\beta}_{m}\)</span> be the fitted values.</p></li>
<li><p>Show that the Mallows averaging criterion is the same as</p></li>
</ol>
<p><span class="math display">\[
\sum_{i=1}^{n}\left(Y_{i}-w_{1} \widehat{Y}_{1 i}-w_{2} \widehat{Y}_{2 i}-\cdots-w_{M} \widehat{Y}_{M i}\right)^{2}+2 \sigma^{2} \sum_{m=1}^{M} w_{m} k_{m}
\]</span></p>
<ol start="2" type="a">
<li>Assume the models are nested with <span class="math inline">\(M\)</span> the largest model. If the previous criterion were minimized over <span class="math inline">\(w\)</span> in the probability simplex but the penalty was omitted, what would be the solution? (What would be the minimizing weight vector?)</li>
</ol>
<p>Exercise 28.11 You estimate <span class="math inline">\(M\)</span> linear regressions <span class="math inline">\(Y=X_{m}^{\prime} \beta_{m}+e_{m}\)</span> by least squares. Let <span class="math inline">\(\widetilde{Y}_{m i}=X_{m i}^{\prime} \widehat{\beta}_{m(-i)}\)</span> be the predicted values from the leave-one-out regressions. Show that the JMA criterion equals</p>
<p><span class="math display">\[
\sum_{i=1}^{n}\left(Y_{i}-w_{1} \widetilde{Y}_{1 i}-w_{2} \widetilde{Y}_{2 i}-\cdots-w_{M} \widetilde{Y}_{M i}\right)^{2}
\]</span></p>
<p>Exercise 28.12 Using the cps09mar dataset perform an analysis similar to that presented in Section <span class="math inline">\(28.18\)</span> but instead use the sub-sample of Hispanic women. This sample has 3003 observations. Which models are selected by BIC, AIC, CV and FIC? (The precise information criteria you examine may be limited depending on your software.) How do you interpret the results? Which model/estimate would you select as your preferred choice?</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./chpt27-censor-selection.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">25</span>&nbsp; <span class="chapter-title">Censoring and Selection</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./chpt29-ML.html" class="pagination-link">
        <span class="nav-page-text"><span class="chapter-number">27</span>&nbsp; <span class="chapter-title">Machine Learning</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->



</body></html>
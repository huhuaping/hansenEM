[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Hansen中高级计量体系",
    "section": "",
    "text": "手稿未出版，盛名已远扬。计量泰斗Hansen Bruce个人官网的传奇大作《Econometrics》，2022年终于横空出世，正式出版了。\n\nHansen B. Econometrics[M]. Princeton: Princeton University Press, 2022.\n\n作为硕士和博士的中高级计量讲义，如果能将之引入中国，传播给国内学子，绝对是大功一件。\n本项目志在如此，希望能与同道之士携手努力，大家共献绵薄，各展所长，日进寸功，添砖加瓦，展示其中的精奥！\n\n本项目的在线网址 https://hansenem.netlify.app.\n本项目完全开源，代码公开于两个托管平台（完全同步）：gihub仓库https://github.com/huhuaping/hansenEM，以及gitee仓库https://gitee.com/kevinhhp/hansenEM\n本项目将基于R编程语言生态，并使用全新的出版工具Quarto进行书稿展示。\n\n如果有兴趣参与本项目，请联系huhuaping01[at]qq.com"
  },
  {
    "objectID": "participate.html",
    "href": "participate.html",
    "title": "如何参与？",
    "section": "",
    "text": "建议使用Window操作系统"
  },
  {
    "objectID": "participate.html#sec-software",
    "href": "participate.html#sec-software",
    "title": "如何参与？",
    "section": "准备软件环境",
    "text": "准备软件环境\n\n必要软件\n以下为必备安装软件：\n\n安装R语言底层环境：R V4.2版本及以上（免费下载地址https://cran.r-project.org/）\n安装R语言编译环境（IDE）：Rstudio V2022.07版本及以上（免费下载地址https://www.rstudio.com/products/rstudio/download/）\n\n\n\n推荐软件\n以下为建议安装软件：\n\n使用版本控制工具：安装gitbash（Windows操作系统用户，免费下载地址https://gitforwindows.org/）。主要用于项目（project）的内容维护和成员工作协同。\n\n\n\n互联网服务\n为了更好地团队协作和在线办公，建议使用如下相关互联网产品/服务：\n\n建议注册代码托管平台的账号服务：一是国际码农最大社区github（https://github.com/，国内访问速度慢，偶尔会抽风）；二是国内代码托管平台gitee（https://gitee.com/）。提示：注册账号时最好使用高校邮箱（形如xxx\\@xxx.edu.cn），可以享受更多教育服务和优惠。本项目的所有代码和资料同步托管于上述两个平台，大家根据自身情况任意选用。"
  },
  {
    "objectID": "participate.html#主要参与方式",
    "href": "participate.html#主要参与方式",
    "title": "如何参与？",
    "section": "主要参与方式",
    "text": "主要参与方式\n首先需要声明的是：所有关注本项目的任何贡献和努力都是受到欢迎和支持的。\n当然，根据参与人个人知识和技能的差异，我们支持如下的松散参与和紧密参与两种方式，简单地：\n\n松散参与：参与者仅需对项目工作感兴趣（中高级计量分析），并不要求具备其他太多的相关知识和技能（这里先略过不提）。只需一台电脑，自行独立工作，通过传统互动（面谈/Email/QQ/微信等）提交自己的任何工作贡献。\n紧密参与：参与者对项目工作感兴趣（中高级计量分析），且具备其他初步相关知识和技能（一种或多种），例如：统计编程语言（R/python/stata等）、现代标记语言（markdown/Rmarkdown）、版本控制语言（git）等。需要在自己电脑上开展工作，与项目其他成员通过代码托管平台（见前面 Section 2.3）在线互动（github或gitee平台等），提交自己的任何工作贡献。\n\n\n松散参与：本地贡献\n主要过程如下：\n\n电脑安装基本的必要软件R和Rstudio（见前面 Section 2.1 ）。\n从托管平台（见前面 Section 2.3 ）拷贝项目到自己的本地电脑，然后解压缩项目文件。本项目完全开源，代码公开于两个托管平台（完全同步）：\n\ngihub仓库https://github.com/huhuaping/hansenEM\ngitee仓库https://gitee.com/kevinhhp/hansenEM\n\n使用编译工具Rstudio软件，打开项目文件hansenEM.Rproj\n在编译工具Rstudio软件下开展相关工作（后面会详谈如何开展自己的工作）。\n自行独立工作，通过传统互动（面谈/Email/QQ/微信等）提交自己的任何工作贡献。\n\n\n\n紧密参与：在线协作\n\n电脑安装基本的必要软件R和Rstudio（见前面 Section 2.1）以及版本控制软件gitbash（见前面 Section 2.2），同时注册代码托管平台账号（github或gitee平台，见前面 Section 2.3）。\n从托管平台（见前面 Section 2.3 ）拷贝项目到自己的本地电脑，然后解压缩项目文件。本项目完全开源，代码公开于两个托管平台（完全同步）：\n\ngihub仓库https://github.com/huhuaping/hansenEM\ngitee仓库https://gitee.com/kevinhhp/hansenEM\n\n使用编译工具Rstudio软件，打开项目文件hansenEM.Rproj\n在编译工具Rstudio软件下开展相关工作（后面会详谈如何开展自己的工作）。\n在自己电脑上开展工作，并与项目其他成员通过代码托管平台（见前面 Section 2.3）在线互动（github或gitee平台），在线提交自己的任何工作贡献。"
  },
  {
    "objectID": "chpt01-intro.html",
    "href": "chpt01-intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "The term “econometrics” is believed to have been crafted by Ragnar Frisch (1895-1973) of Norway, one of the three principal founders of the Econometric Society, first editor of the journal Econometrica, and co-winner of the first Nobel Memorial Prize in Economic Sciences in 1969. It is therefore fitting that we turn to Frisch’s own words in the introduction to the first issue of Econometrica to describe the discipline.\nA word of explanation regarding the term econometrics may be in order. Its definition is implied in the statement of the scope of the [Econometric] Society, in Section I of the Constitution, which reads: “The Econometric Society is an international society for the advancement of economic theory in its relation to statistics and mathematics…. Its main object shall be to promote studies that aim at a unification of the theoretical-quantitative and the empirical-quantitative approach to economic problems….”\nBut there are several aspects of the quantitative approach to economics, and no single one of these aspects, taken by itself, should be confounded with econometrics. Thus, econometrics is by no means the same as economic statistics. Nor is it identical with what we call general economic theory, although a considerable portion of this theory has a defininitely quantitative character. Nor should econometrics be taken as synonomous with the application of mathematics to economics. Experience has shown that each of these three viewpoints, that of statistics, economic theory, and mathematics, is a necessary, but not by itself a sufficient, condition for a real understanding of the quantitative relations in modern economic life. It is the unification of all three that is powerful. And it is this unification that constitutes econometrics.\nRagnar Frisch, Econometrica, (1933), 1, pp. 1-2.\nThis definition remains valid today, although some terms have evolved somewhat in their usage. Today, we would say that econometrics is the unified study of economic models, mathematical statistics, and economic data.\nWithin the field of econometrics there are sub-divisions and specializations. Econometric theory concerns the development of tools and methods, and the study of the properties of econometric methods. Applied econometrics is a term describing the development of quantitative economic models and the application of econometric methods to these models using economic data."
  },
  {
    "objectID": "chpt01-intro.html#the-probability-approach-to-econometrics",
    "href": "chpt01-intro.html#the-probability-approach-to-econometrics",
    "title": "1  Introduction",
    "section": "1.2 The Probability Approach to Econometrics",
    "text": "1.2 The Probability Approach to Econometrics\nThe unifying methodology of modern econometrics was articulated by Trygve Haavelmo (1911-1999) of Norway, winner of the 1989 Nobel Memorial Prize in Economic Sciences, in his seminal paper “The probability approach in econometrics” (1944). Haavelmo argued that quantitative economic models must necessarily be probability models (by which today we would mean stochastic). Deterministic models are blatently inconsistent with observed economic quantities, and it is incoherent to apply deterministic models to non-deterministic data. Economic models should be explicitly designed to incorporate randomness; stochastic errors should not be simply added to deterministic models to make them random. Once we acknowledge that an economic model is a probability model, it follows naturally that an appropriate tool way to quantify, estimate, and conduct inferences about the economy is through the powerful theory of mathematical statistics. The appropriate method for a quantitative economic analysis follows from the probabilistic construction of the economic model.\nHaavelmo’s probability approach was quickly embraced by the economics profession. Today no quantitative work in economics shuns its fundamental vision.\nWhile all economists embrace the probability approach, there has been some evolution in its implementation.\nThe structural approach is the closest to Haavelmo’s original idea. A probabilistic economic model is specified, and the quantitative analysis performed under the assumption that the economic model is correctly specified. Researchers often describe this as “taking their model seriously”. The structural approach typically leads to likelihood-based analysis, including maximum likelihood and Bayesian estimation.\nA criticism of the structural approach is that it is misleading to treat an economic model as correctly specified. Rather, it is more accurate to view a model as a useful abstraction or approximation. In this case, how should we interpret structural econometric analysis? The quasi-structural approach to inference views a structural economic model as an approximation rather than the truth. This theory has led to the concepts of the pseudo-true value (the parameter value defined by the estimation problem), the quasi-likelihood function, quasi-MLE, and quasi-likelihood inference.\nClosely related is the semiparametric approach. A probabilistic economic model is partially specified but some features are left unspecified. This approach typically leads to estimation methods such as least squares and the generalized method of moments. The semiparametric approach dominates contemporary econometrics, and is the main focus of this textbook.\nAnother branch of quantitative structural economics is the calibration approach. Similar to the quasi-structural approach, the calibration approach interprets structural models as approximations and hence inherently false. The difference is that the calibrationist literature rejects mathematical statistics (deeming classical theory as inappropriate for approximate models) and instead selects parameters by matching model and data moments using non-statistical \\(a d h o c^{1}\\) methods."
  },
  {
    "objectID": "chpt01-intro.html#trygve-haavelmo",
    "href": "chpt01-intro.html#trygve-haavelmo",
    "title": "1  Introduction",
    "section": "1.3 Trygve Haavelmo",
    "text": "1.3 Trygve Haavelmo\nThe founding ideas of the field of econometrics are largely due to the Norweigen econometrician Trygve Haavelmo (1911-1999). His advocacy of probability models revolutionized the field, and his use of formal mathematical reasoning laid the foundation for subsequent generations. He was awarded the Nobel Memorial Prize in Economic Sciences in \\(1989 .\\)\n\\({ }^{1}\\) Ad hoc means “for this purpose” - a method designed for a specific problem - and not based on a generalizable principle."
  },
  {
    "objectID": "chpt01-intro.html#econometric-terms",
    "href": "chpt01-intro.html#econometric-terms",
    "title": "1  Introduction",
    "section": "1.4 Econometric Terms",
    "text": "1.4 Econometric Terms\nIn a typical application, an econometrician has a set of repeated measurements on a set of variables. For example, in a labor application the variables could include weekly earnings, educational attainment, age, and other descriptive characteristics. We call this information the data, dataset, or sample.\nWe use the term observations to refer to distinct repeated measurements on the variables. An individual observation often corresponds to a specific economic unit, such as a person, household, corporation, firm, organization, country, state, city or other geographical region. An individual observation could also be a measurement at a point in time, such as quarterly GDP or a daily interest rate.\nEconomists typically denote variables by the italicized roman characters \\(Y, X\\), and/or \\(Z\\). The convention in econometrics is to use the character \\(Y\\) to denote the variable to be explained, while the characters \\(X\\) and \\(Z\\) are used to denote the conditioning (explaining) variables. Following mathematical practice, random variables and vectors are denoted by upper case roman characters such as \\(Y\\) and \\(X\\). We make an exception for equation errors which we typically denote by the lower case letters \\(e, u\\), or \\(v\\).\nReal numbers (elements of the real line \\(\\mathbb{R}\\), also called scalars) are written using lower case italics such as \\(x\\). Vectors (elements of \\(\\mathbb{R}^{k}\\) ) are typically also written using lower case italics such as \\(x\\), or using lower case bold italics such as \\(\\boldsymbol{x}\\). We use bold in matrix algebraic expressions for compatibility with matrix notation.\nMatrices are written using upper case bold italics such as \\(\\boldsymbol{X}\\). Our notation will not make a distinction between random and non-random matrices. Typically we use \\(\\boldsymbol{U}, \\boldsymbol{V}, \\boldsymbol{X}, \\boldsymbol{Y}, \\boldsymbol{Z}\\) to denote random matrices and use \\(\\boldsymbol{A}, \\boldsymbol{B}, \\boldsymbol{C}, \\boldsymbol{W}\\) to denote non-random matrices.\nWe denote the number of observations by the natural number \\(n\\), and subscript the variables by the index \\(i\\) to denote the individual observation, e.g. \\(Y_{i}\\). In some contexts we use indices other than \\(i\\), such as in time series applications where the index \\(t\\) is common. In panel studies we typically use the double index \\(i t\\) to refer to individual \\(i\\) at a time period \\(t\\).\nWe typically use Greek letters such as \\(\\beta, \\theta\\), and \\(\\sigma^{2}\\) to denote unknown parameters (scalar or vectors). Parameter matrices are written using upper case Latin boldface, e.g. \\(\\boldsymbol{A}\\). Estimators are typically denoted by putting a hat ” \\(\\wedge\\) “, tilde” \\(\"\\), or bar “-” over the corresponding letter, e.g. \\(\\widehat{\\beta}\\) and \\(\\widetilde{\\beta}\\) are estimators of \\(\\beta\\), and \\(\\widehat{A}\\) is an estimator of \\(\\boldsymbol{A}\\).\nThe covariance matrix of an econometric estimator will typically be written using the upper case boldface \\(\\boldsymbol{V}\\), often with a subscript to denote the estimator, e.g. \\(\\boldsymbol{V}_{\\widehat{\\beta}}=\\operatorname{var}[\\widehat{\\beta}]\\) as the covariance matrix for \\(\\widehat{\\beta}\\). Hopefully without causing confusion, we will use the notation \\(\\boldsymbol{V}_{\\beta}=\\) avar \\([\\widehat{\\beta}]\\) to denote the asymptotic covariance matrix of \\(\\sqrt{n}(\\widehat{\\beta}-\\beta)\\) (the variance of the asymptotic distribution). Covariance matrix estimators will be denoted by appending hats or tildes, e.g. \\(\\widehat{V}_{\\beta}\\) is an estimator of \\(\\boldsymbol{V}_{\\beta}\\)."
  },
  {
    "objectID": "chpt01-intro.html#observational-data",
    "href": "chpt01-intro.html#observational-data",
    "title": "1  Introduction",
    "section": "1.5 Observational Data",
    "text": "1.5 Observational Data\nA common econometric question is to quantify the causal impact of one set of variables on another variable. For example, a concern in labor economics is the returns to schooling - the change in earnings induced by increasing a worker’s education, holding other variables constant. Another issue of interest is the earnings gap between men and women.\nIdeally, we would use experimental data to answer these questions. To measure the returns to schooling, an experiment might randomly divide children into groups, mandate different levels of education to the different groups, and then follow the children’s wage path after they mature and enter the labor force. The differences between the groups would be direct measurements of the effects of different levels of education. However, experiments such as this would be widely condemned as immoral! Consequently, in economics experimental data sets are typically narrow in scope. Instead, most economic data is observational. To continue the above example, through data collection we can record the level of a person’s education and their wage. With such data we can measure the joint distribution of these variables and assess their joint dependence. But from observational data it is difficult to infer causality as we are not able to manipulate one variable to see the direct effect on the other. For example, a person’s level of education is (at least partially) determined by that person’s choices. These factors are likely to be affected by their personal abilities and attitudes towards work. The fact that a person is highly educated suggests a high level of ability, which suggests a high relative wage. This is an alternative explanation for an observed positive correlation between educational levels and wages. High ability individuals do better in school, and therefore choose to attain higher levels of education, and their high ability is the fundamental reason for their high wages. The point is that multiple explanations are consistent with a positive correlation between schooling levels and education. Knowledge of the joint distribution alone may not be able to distinguish between these explanations.\nMost economic data sets are observational, not experimental. This means that all variables must be treated as random and possibly jointly determined.\nThis discussion means that it is difficult to infer causality from observational data alone. Causal inference requires identification, and this is based on strong assumptions. We will discuss these issues on occasion throughout the text."
  },
  {
    "objectID": "chpt01-intro.html#standard-data-structures",
    "href": "chpt01-intro.html#standard-data-structures",
    "title": "1  Introduction",
    "section": "1.6 Standard Data Structures",
    "text": "1.6 Standard Data Structures\nThere are five major types of economic data sets: cross-sectional, time series, panel, clustered, and spatial. They are distinguished by the dependence structure across observations.\nCross-sectional data sets have one observation per individual. Surveys and administrative records are a typical source for cross-sectional data. In typical applications, the individuals surveyed are persons, households, firms, or other economic agents. In many contemporary econometric cross-section studies the sample size \\(n\\) is quite large. It is conventional to assume that cross-sectional observations are mutually independent. Most of this text is devoted to the study of cross-section data.\nTime series data are indexed by time. Typical examples include macroeconomic aggregates, prices, and interest rates. This type of data is characterized by serial dependence. Most aggregate economic data is only available at a low frequency (annual, quarterly, or monthly) so the sample size is typically much smaller than in cross-section studies. An exception is financial data where data are available at a high frequency (daily, hourly, or by transaction) so sample sizes can be quite large.\nPanel data combines elements of cross-section and time series. These data sets consist of a set of individuals (typically persons, households, or corporations) measured repeatedly over time. The common modeling assumption is that the individuals are mutually independent of one another, but a given individual’s observations are mutually dependent. In some panel data contexts the number of time series observations \\(T\\) per individual is small while the number of individuals \\(n\\) is large. In other panel data contexts (for example when countries or states are taken as the unit of measurement) the number of individuals \\(n\\) can be small while the number of time series observations \\(T\\) can be moderately large. An important issue in econometric panel data is the treatment of error components.\nClustered samples are increasing popular in applied economics and are related to panel data. In clustered sampling the observations are grouped into “clusters” which are treated as mutually independent yet allowed to be dependent within the cluster. The major difference with panel data is that clustered sampling typically does not explicitly model error component structures, nor the dependence within clusters, but rather is concerned with inference which is robust to arbitrary forms of within-cluster correlation.\nSpatial dependence is another model of interdependence. The observations are treated as mutually dependent according to a spatial measure (for example, geographic proximity). Unlike clustering, spatial models allow all observations to be mutually dependent, and typically rely on explicit modeling of the dependence relationships. Spatial dependence can also be viewed as a generalization of time series dependence."
  },
  {
    "objectID": "chpt01-intro.html#data-structures",
    "href": "chpt01-intro.html#data-structures",
    "title": "1  Introduction",
    "section": "1.7 Data Structures",
    "text": "1.7 Data Structures\n\nCross-section\nTime-series\nPanel\nClustered\nSpatial\n\nAs we mentioned above, most of this text will be devoted to cross-sectional data under the assumption of mutually independent observations. By mutual independence we mean that the \\(i^{t h}\\) observation \\(\\left(Y_{i}, X_{i}\\right)\\) is independent of the \\(j^{t h}\\) observation \\(\\left(Y_{j}, X_{j}\\right)\\) for \\(i \\neq j\\). In this case we say that the data are independently distributed. (Sometimes the label “independent” is misconstrued. It is a statement about the relationship between observations \\(i\\) and \\(j\\), not a statement about the relationship between \\(Y_{i}\\) and \\(X_{i}\\).)\nFurthermore, if the data is randomly gathered, it is reasonable to model each observation as a draw from the same probability distribution. In this case we say that the data are identically distributed. If the observations are mutually independent and identically distributed, we say that the observations are independent and identically distributed, i.i.d., or a random sample. For most of this text we will assume that our observations come from a random sample.\nDefinition 1.1 The variables \\(\\left(Y_{i}, X_{i}\\right)\\) are a sample from the distribution \\(F\\) if they are identically distributed with distribution \\(F\\).\nDefinition 1.2 The variables \\(\\left(Y_{i}, X_{i}\\right)\\) are a random sample if they are mutually independent and identically distributed (i.i.d.) across \\(i=1, \\ldots, n\\).\nIn the random sampling framework, we think of an individual observation \\(\\left(Y_{i}, X_{i}\\right)\\) as a realization from a joint probability distribution \\(F(y, x)\\) which we call the population. This “population” is infinitely large. This abstraction can be a source of confusion as it does not correspond to a physical population in the real world. It is an abstraction because the distribution \\(F\\) is unknown, and the goal of statistical inference is to learn about features of \\(F\\) from the sample. The assumption of random sampling provides the mathematical foundation for treating economic statistics with the tools of mathematical statistics.\nThe random sampling framework was a major intellectual breakthrough of the late 19th century, allowing the application of mathematical statistics to the social sciences. Before this conceptual development, methods from mathematical statistics had not been applied to economic data as the latter was viewed as non-random. The random sampling framework enabled economic samples to be treated as random, a necessary precondition for the application of statistical methods."
  },
  {
    "objectID": "chpt01-intro.html#econometric-software",
    "href": "chpt01-intro.html#econometric-software",
    "title": "1  Introduction",
    "section": "1.8 Econometric Software",
    "text": "1.8 Econometric Software\nEconomists use a variety of econometric, statistical, and programming software.\nStata is a powerful statistical program with a broad set of pre-programmed econometric and statistical tools. It is quite popular among economists, and is continuously being updated with new methods. It is an excellent package for most econometric analysis, but is limited when you want to use new or lesscommon econometric methods which have not yet been programed. At many points in this textbook specific Stata estimation methods and commands are described. These commands are valid for Stata version \\(16 .\\)\nMATLAB, GAUSS, and OxMetrics are high-level matrix programming languages with a wide variety of built-in statistical functions. Many econometric methods have been programed in these languages and are available on the web. The advantage of these packages is that you are in complete control of your analysis, and it is easier to program new methods than in Stata. Some disadvantages are that you have to do much of the programming yourself, programming complicated procedures takes significant time, and programming errors are hard to prevent and difficult to detect and eliminate. Of these languages, GAUSS used to be quite popular among econometricians, but currently MATLAB is more popular.\nAn intermediate choice is R. R has the capabilities of the above high-level matrix programming languages, but also has many built-in statistical environments which can replicate much of the functionality of Stata. R is the dominant programming language in the statistics field, so methods developed in that arena are most commonly available in R. Uniquely, R is open-source, user-contributed, and best of all, completely free! A growing group of econometricians are enthusiastic fans of \\(R\\).\nFor highly-intensive computational tasks, some economists write their programs in a standard programming language such as Fortran or C. This can lead to major gains in computational speed, at the cost of increased time in programming and debugging.\nThere are many other packages which are used by econometricians, include Eviews, Gretl, PcGive, Python, Julia, RATS, and SAS.\nAs the packages described above have distinct advantages many empirical economists use multiple packages. As a student of econometrics you will learn at least one of these packages and probably more than one. My advice is that all students of econometrics should develop a basic level of familiarity with Stata, MATLAB, and R."
  },
  {
    "objectID": "chpt01-intro.html#replication",
    "href": "chpt01-intro.html#replication",
    "title": "1  Introduction",
    "section": "1.9 Replication",
    "text": "1.9 Replication\nScientific research needs to be documented and replicable. For social science research using observational data this requires careful documentation and archiving of the research methods, data manipulations, and coding. The best practice is as follows. Accompanying each published paper an author should create a complete replication package (set of data files, documentation, and program code files). This package should contain the source (raw) data used for analysis, and code which executes the empirical analysis and other numerical work reported in the paper. In most cases this is a set of programs which may need to be executed sequentially. (For example, there may be an initial program which “cleans” and manipulates the data, and then a second set of programs which estimate the reported models.) The ideal is full documentation and clarity. This package should be posted on the author(s) website, and posted at the journal website when that is an option.\nA complicating factor is that many current economic data sets have restricted access and cannot be shared without permission. In these cases the data cannot be posted nor shared. The computed code, however, can and should be posted.\nMost journals in economics require authors of published papers to make their datasets generally available. For example:"
  },
  {
    "objectID": "chpt01-intro.html#econometrica-states",
    "href": "chpt01-intro.html#econometrica-states",
    "title": "1  Introduction",
    "section": "1.10 Econometrica states:",
    "text": "1.10 Econometrica states:\nEconometrica has the policy that all empirical, experimental and simulation results must be replicable. Therefore, authors of accepted papers must submit data sets, programs, and information on empirical analysis, experiments and simulations that are needed for replication and some limited sensitivity analysis.\nThe American Economic Review states:\nIt is the policy of the American Economic Association to publish papers only if the data and code used in the analysis are clearly and precisely documented and access to the data and code is non-exclusive to the authors. Authors of accepted papers that contain empirical work, simulations, or experimental work must provide, prior to acceptance, information about the data, programs, and other details of the computations sufficient to permit replication, as well as information about access to data and programs.\nThe Journal of Political Economy states:\nIt is the policy of the Journal of Political Economy to publish papers only if the data used in the analysis are clearly and precisely documented and are readily available to any researcher for purposes of replication.\nIf you are interested in using the data from a published paper, first check the journal’s website, as many journals archive data and replication programs online. Second, check the website(s) of the paper’s author(s). Most academic economists maintain webpages, and some make available replication files complete with data and programs. If these investigations fail, email the author(s), politely requesting the data. You may need to be persistent.\nAs a matter of professional etiquette, all authors absolutely have the obligation to make their data and programs available. Unfortunately, many fail to do so, and typically for poor reasons. The irony of the situation is that it is typically in the best interests of a scholar to make as much of their work (including all data and programs) freely available, as this only increases the likelihood of their work being cited and having an impact.\nKeep this in mind as you start your own empirical project. Remember that as part of your end product, you will need (and want) to provide all data and programs to the community of scholars. The greatest form of flattery is to learn that another scholar has read your paper, wants to extend your work, or wants to use your empirical methods. In addition, public openness provides a healthy incentive for transparency and integrity in empirical analysis."
  },
  {
    "objectID": "chpt01-intro.html#data-files-for-textbook",
    "href": "chpt01-intro.html#data-files-for-textbook",
    "title": "1  Introduction",
    "section": "1.11 Data Files for Textbook",
    "text": "1.11 Data Files for Textbook\nOn the textbook webpage bhansen/econometrics/ there are posted a number of files containing data sets which are used in this textbook both for illustration and for end-ofchapter empirical exercises. For most of the data sets there are four files: (1) Description (pdf format); (2) Excel data file; (3) Text data file; (4) Stata data file. The three data files are identical in content: the observations and variables are listed in the same order in each, and all have variable labels.\nFor example, the text makes frequent reference to a wage data set extracted from the Current Population Survey. This data set is named cps09mar, and is represented by the files cps09mar_description.pdf, cps09mar.xlsx, cps09mar .txt, and cps09mar. dta.\nThe data sets currently included are\n\nAB1991\nData file from Arellano and Bond (1991)\nAJR2001\nData file from Acemoglu, Johnson, and Robinson (2001)\nAK1991\nData file from Angrist and Krueger (1991)\nAL1999\nData file from Angrist and Lavy (1999)\nBMN2016\nData file from Bernheim, Meer and Novarro (2016)\ncps09mar\nhousehold survey data extracted from the March 2009 Current Population Survey\nCard1995\nData file from Card (1995)\nCHJ2004\nData file from Cox, B. E. Hansen and Jimenez (2004)\nCK1994\nData file from Card and Krueger (1994)\nCMR2008\nDate file from Card, Mas, and Rothstein (2008) - DDK2011\nData file from Duflo, Dupas, and Kremer (2011)\nDS2004\nData file from DiTella and Schargrodsky (2004)\nFRED-MD and FRED-QD\nU.S. monthly and quarterly macroeconomic databases from McCracken and Ng (2015)\nInvest1993\nData file from Hall and Hall (1993)\nLM2007\nData file from Ludwig and Miller (2007) and Cattaneo, Titiunik, and Vazquez-Bare (2017)\nKilian2009\nData file from Kilian (2009)\nKoppelman\nData file from Forinash and Koppelman (1993), Koppelman and Wen (2000) and Wen and Koppelman (2001)\nMRW1992\nData file from Mankiw, Romer, and Weil (1992)\nNerlove1963\nData file from Nerlov (1963)\nPSS2017\nData file from Papageorgiou, Saam, and Schulte (2017)\nRR2010\nData file from Reinhard and Rogoff (2010)"
  },
  {
    "objectID": "chpt01-intro.html#reading-the-manuscript",
    "href": "chpt01-intro.html#reading-the-manuscript",
    "title": "1  Introduction",
    "section": "1.12 Reading the Manuscript",
    "text": "1.12 Reading the Manuscript\nI have endeavored to use a unified notation and nomenclature. The development of the material is cumulative, with later chapters building on the earlier ones. Nevertheless, every attempt has been made to make each chapter self-contained so readers can pick and choose topics according to their interests.\nTo fully understand econometric methods it is necessary to have a mathematical understanding of its mechanics, and this includes the mathematical proofs of the main results. Consequently, this text is selfcontained with nearly all results proved with full mathematical rigor. The mathematical development and proofs aim at brevity and conciseness (sometimes described as mathematical elegance), but also at pedagogy. To understand a mathematical proof it is not sufficient to simply read the proof, you need to follow it and re-create it for yourself.\nNevertheless, many readers will not be interested in each mathematical detail, explanation, or proof. This is okay. To use a method it may not be necessary to understand the mathematical details. Accordingly I have placed the more technical mathematical proofs and details in chapter appendices. These appendices and other technical sections are marked with an asterisk \\(\\left(^{*}\\right)\\). These sections can be skipped without any loss in exposition.\nKey concepts in matrix algebra and a set of useful inequalities are reviewed in Appendices A & B. It may be useful to read or review Appendix A.1-A.11 before starting Chapter 3, and review Appendix B before Chapter 6 . It is not necessary to understand all the material in the appendices. They are intended to be reference material and some of the results are not used in this textbook."
  },
  {
    "objectID": "chpt02-ce.html",
    "href": "chpt02-ce.html",
    "title": "2  Conditional Expectation and Projection",
    "section": "",
    "text": "The most commonly applied econometric tool is least squares estimation, also known as regression. Least squares is a tool to estimate the conditional mean of one variable (the dependent variable) given another set of variables (the regressors, conditioning variables, or covariates).\nIn this chapter we abstract from estimation and focus on the probabilistic foundation of the conditional expectation model and its projection approximation. This includes a review of probability theory. For a background in intermediate probability theory see Chapters 1-5 of Probability and Statistics for Economists."
  },
  {
    "objectID": "chpt02-ce.html#the-distribution-of-wages",
    "href": "chpt02-ce.html#the-distribution-of-wages",
    "title": "2  Conditional Expectation and Projection",
    "section": "2.2 The Distribution of Wages",
    "text": "2.2 The Distribution of Wages\nSuppose that we are interested in wage rates in the United States. Since wage rates vary across workers we cannot describe wage rates by a single number. Instead, we can describe wages using a probability distribution. Formally, we view the wage of an individual worker as a random variable wage with the probability distribution\n\\[\nF(y)=\\mathbb{P}[\\text { wage } \\leq y] .\n\\]\nWhen we say that a person’s wage is random we mean that we do not know their wage before it is measured, and we treat observed wage rates as realizations from the distribution \\(F\\). Treating unobserved wages as random variables and observed wages as realizations is a powerful mathematical abstraction which allows us to use the tools of mathematical probability.\nA useful thought experiment is to imagine dialing a telephone number selected at random, and then asking the person who responds to tell us their wage rate. (Assume for simplicity that all workers have equal access to telephones and that the person who answers your call will answer honestly.) In this thought experiment, the wage of the person you have called is a single draw from the distribution \\(F\\) of wages in the population. By making many such phone calls we can learn the full distribution.\nWhen a distribution function \\(F\\) is differentiable we define the probability density function\n\\[\nf(y)=\\frac{d}{d y} F(y) .\n\\]\nThe density contains the same information as the distribution function, but the density is typically easier to visually interpret.\n\n\nWage Density\n\n\n\nLog Wage Density\n\nFigure 2.1: Density of Wages and Log Wages\nIn Figure 2.1(a) we display an estimate \\({ }^{1}\\) of the probability density function of U.S. wage rates in \\(2009 .\\) We see that the density is peaked around \\(\\$ 15\\), and most of the probability mass appears to lie between \\(\\$ 10\\) and \\(\\$ 40\\). These are ranges for typical wage rates in the U.S. population.\nImportant measures of central tendency are the median and the mean. The median \\(m\\) of a continuous distribution \\(F\\) is the unique solution to\n\\[\nF(m)=\\frac{1}{2} .\n\\]\nThe median U.S. wage is \\(\\$ 19.23\\). The median is a robust \\({ }^{2}\\) measure of central tendency, but it is tricky to use for many calculations as it is not a linear operator.\nThe mean or expectation of a random variable \\(Y\\) with discrete support is\n\\[\n\\mu=\\mathbb{E}[Y]=\\sum_{j=1}^{\\infty} \\tau_{j} \\mathbb{P}\\left[Y=\\tau_{j}\\right] .\n\\]\nFor a continuous random variable with density \\(f(y)\\) the expectation is\n\\[\n\\mu=\\mathbb{E}[Y]=\\int_{-\\infty}^{\\infty} y f(y) d y .\n\\]\nHere we have used the common and convenient convention of using the single character \\(Y\\) to denote a random variable, rather than the more cumbersome label wage. An alternative notation which includes both discrete and continuous random variables as special cases is to write the integral as \\(\\int_{-\\infty}^{\\infty} y d F(y)\\).\nThe expectation is a convenient measure of central tendency because it is a linear operator and arises naturally in many economic models. A disadvantage of the expectation is that it is not robust \\({ }^{3}\\) especially\n\\({ }^{1}\\) The distribution and density are estimated nonparametrically from the sample of 50,742 full-time non-military wageearners reported in the March 2009 Current Population Survey. The wage rate is constructed as annual individual wage and salary earnings divided by hours worked.\n\\({ }^{2}\\) The median is not sensitive to pertubations in the tails of the distribution.\n\\({ }^{3}\\) The expectation is sensitive to pertubations in the tails of the distribution. in the presence of substantial skewness or thick tails, both which are features of the wage distribution as can be seen in Figure 2.1(a). Another way of viewing this is that \\(64 %\\) of workers earn less than the mean wage of \\(\\$ 23.90\\), suggesting that it is incorrect to describe the mean \\(\\$ 23.90\\) as a “typical” wage rate.\nIn this context it is useful to transform the data by taking the natural logarithm” \\({ }^{4}\\). Figure \\(2.1\\) (b) shows the density of \\(\\log\\) hourly wages \\(\\log (\\) wage \\()\\) for the same population. The density of log wages is less skewed and fat-tailed than the density of the level of wages, so its mean\n\\[\n\\mathbb{E}[\\log (\\text { wage })]=2.95\n\\]\nis a better (more robust) measure \\({ }^{5}\\) of central tendency of the distribution. For this reason, wage regressions typically use log wages as a dependent variable rather than the level of wages.\nAnother useful way to summarize the probability distribution \\(F(y)\\) is in terms of its quantiles. For any \\(\\alpha \\in(0,1)\\), the \\(\\alpha^{t h}\\) quantile of the continuous \\({ }^{6}\\) distribution \\(F\\) is the real number \\(q_{\\alpha}\\) which satisfies \\(F\\left(q_{\\alpha}\\right)=\\alpha\\). The quantile function \\(q_{\\alpha}\\), viewed as a function of \\(\\alpha\\), is the inverse of the distribution function \\(F\\). The most commonly used quantile is the median, that is, \\(q_{0.5}=m\\). We sometimes refer to quantiles by the percentile representation of \\(\\alpha\\) and in this case they are called percentiles. E.g. the median is the \\(50^{t h}\\) percentile."
  },
  {
    "objectID": "chpt02-ce.html#conditional-expectation",
    "href": "chpt02-ce.html#conditional-expectation",
    "title": "2  Conditional Expectation and Projection",
    "section": "2.3 Conditional Expectation",
    "text": "2.3 Conditional Expectation\nWe saw in Figure 2.1(b) the density of log wages. Is this distribution the same for all workers, or does the wage distribution vary across subpopulations? To answer this question, we can compare wage distributions for different groups - for example, men and women. To investigate, we plot in Figure \\(2.2\\) (a) the densities of log wages for U.S. men and women. We can see that the two wage densities take similar shapes but the density for men is somewhat shifted to the right.\nThe values \\(3.05\\) and \\(2.81\\) are the mean log wages in the subpopulations of men and women workers. They are called the conditional expectation (or conditional mean) of log wages given gender. We can write their specific values as\n\\[\n\\begin{gathered}\n\\mathbb{E}[\\log (\\text { wage }) \\mid \\text { gender }=\\text { man }]=3.05 \\\\\n\\mathbb{E}[\\log (\\text { wage }) \\mid \\text { gender }=\\text { woman }]=2.81 .\n\\end{gathered}\n\\]\nWe call these expectations “conditional” as they are conditioning on a fixed value of the variable gender. While you might not think of a person’s gender as a random variable, it is random from the viewpoint of econometric analysis. If you randomly select an individual, the gender of the individual is unknown and thus random. (In the population of U.S. workers, the probability that a worker is a woman happens to be \\(43 %\\).) In observational data, it is most appropriate to view all measurements as random variables, and the means of subpopulations are then conditional means.\nIt is important to mention at this point that we in no way attribute causality or interpretation to the difference in the conditional expectation of log wages between men and women. There are multiple potential explanations.\nAs the two densities in Figure 2.2(a) appear similar, a hasty inference might be that there is not a meaningful difference between the wage distributions of men and women. Before jumping to this conclusion let us examine the differences in the distributions more carefully. As we mentioned above, the\n\\({ }^{4}\\) Throughout the text, we will use \\(\\log (y)\\) or \\(\\log y\\) to denote the natural logarithm of \\(y\\).\n\\({ }^{5}\\) More precisely, the geometric mean \\(\\exp (\\mathbb{E}[\\log W])=\\$ 19.11\\) is a robust measure of central tendency.\n\\({ }^{6}\\) If \\(F\\) is not continuous the definition is \\(q_{\\alpha}=\\inf \\{y: F(y) \\geq \\alpha\\}\\)\n\n\nWomen and Men\n\n\n\nBy Gender and Race\n\nFigure 2.2: Log Wage Density by Gender and Race\nprimary difference between the two densities appears to be their means. This difference equals\n\\[\n\\begin{aligned}\n\\mathbb{E}[\\log (\\text { wage }) \\mid \\text { gender }=\\text { man }]-\\mathbb{E}[\\log (\\text { wage }) \\mid \\text { gender }=\\text { woman }] &=3.05-2.81 \\\\\n&=0.24 .\n\\end{aligned}\n\\]\nA difference in expected log wages of \\(0.24\\) is often interpreted as an average \\(24 %\\) difference between the wages of men and women, which is quite substantial. (For a more complete explanation see Section 2.4.)\nConsider further splitting the male and female subpopulations by race, dividing the population into whites, Blacks, and other races. We display the log wage density functions of four of these groups in Figure \\(2.2\\) (b). Again we see that the primary difference between the four density functions is their central tendency.\nFocusing on the means of these distributions, Table \\(2.1\\) reports the mean log wage for each of the six sub-populations.\nTable 2.1: Mean Log Wages by Gender and Race\n\n\n\n\nmen\nwomen\n\n\n\n\nwhite\n\\(3.07\\)\n\\(2.82\\)\n\n\nBlack\n\\(2.86\\)\n\\(2.73\\)\n\n\nother\n\\(3.03\\)\n\\(2.86\\)\n\n\n\nOnce again we stress that we in no way attribute causality or interpretation to the differences across the entries of the table. The reason why we use these particular sub-populations to illustrate conditional expectation is because differences in economic outcomes between gender and racial groups in the United States (and elsewhere) are widely discussed; part of the role of social science is to carefully document such patterns, and part of its role is to craft models and explanations. Conditional expectations (by themselves) can help in the documentation and description; conditional expectations by themselves are neither a model nor an explanation.\nThe entries in Table \\(2.1\\) are the conditional means of \\(\\log (\\) wage \\()\\) given gender and race. For example\n\\[\n\\mathbb{E}[\\log (\\text { wage }) \\mid \\text { gender }=\\text { man, race }=\\text { white }]=3.07\n\\]\nand\n\\[\n\\mathbb{E}[\\log (\\text { wage }) \\mid \\text { gender }=\\text { woman, race }=\\text { Black }]=2.73 \\text {. }\n\\]\nOne benefit of focusing on conditional means is that they reduce complicated distributions to a single summary measure, and thereby facilitate comparisons across groups. Because of this simplifying property, conditional means are the primary interest of regression analysis and are a major focus in econometrics.\nTable \\(2.1\\) allows us to easily calculate average wage differences between groups. For example, we can see that the wage gap between men and women continues after disaggregation by race, as the average gap between white men and white women is \\(25 %\\), and that between Black men and Black women is \\(13 %\\). We also can see that there is a race gap, as the average wages of Blacks are substantially less than the other race categories. In particular, the average wage gap between white men and Black men is \\(21 %\\), and that between white women and Black women is \\(9 %\\)."
  },
  {
    "objectID": "chpt02-ce.html#logs-and-percentages",
    "href": "chpt02-ce.html#logs-and-percentages",
    "title": "2  Conditional Expectation and Projection",
    "section": "2.4 Logs and Percentages",
    "text": "2.4 Logs and Percentages\nIn this section we want to motivate and clarify the use of the logarithm in regression analysis by making two observations. First, when applied to numbers the difference of logarithms approximately equals the percentage difference. Second, when applied to averages the difference in logarithms approximately equals the percentage difference in the geometric mean. We now explain these ideas and the nature of the approximations involved.\nTake two positive numbers \\(a\\) and \\(b\\). The percentage difference between \\(a\\) and \\(b\\) is\n\\[\np=100\\left(\\frac{a-b}{b}\\right) .\n\\]\nRewriting,\n\\[\n\\frac{a}{b}=1+\\frac{p}{100}\n\\]\nTaking natural logarithms,\n\\[\n\\log a-\\log b=\\log \\left(1+\\frac{p}{100}\\right) .\n\\]\nA useful approximation for small \\(x\\) is\n\\[\n\\log (1+x) \\simeq x .\n\\]\nThis can be derived from the infinite series expansion of \\(\\log (1+x)\\) :\n\\[\n\\log (1+x)=x-\\frac{x^{2}}{2}+\\frac{x^{3}}{3}-\\frac{x^{4}}{4}+\\cdots=x+O\\left(x^{2}\\right) .\n\\]\nThe symbol \\(O\\left(x^{2}\\right.\\) ) means that the remainder is bounded by \\(A x^{2}\\) as \\(x \\rightarrow 0\\) for some \\(A<\\infty\\). Numerically, the approximation \\(\\log (1+x) \\simeq x\\) is within \\(0.001\\) for \\(|x| \\leq 0.1\\), and the approximation error increases with \\(|x|\\)\nApplying (2.3) to (2.2) and multiplying by 100 we find\n\\[\np \\simeq 100(\\log a)-\\log b) .\n\\]\nThis shows that 100 multiplied by the difference in logarithms is approximately the percentage difference. Numerically, the approximation error is less than \\(0.1\\) percentage points for \\(|p| \\leq 10\\).\nNow consider the difference in the expectation of log transformed random variables. Take two random variables \\(X_{1}, X_{2}>0\\). Define their geometric means \\(\\theta_{1}=\\exp \\left(\\mathbb{E}\\left[\\log X_{1}\\right]\\right)\\) and \\(\\theta_{2}=\\exp \\left(\\mathbb{E}\\left[\\log X_{2}\\right]\\right)\\) and their percentage difference\n\\[\np=100\\left(\\frac{\\theta_{2}-\\theta_{1}}{\\theta_{1}}\\right) .\n\\]\nThe difference in the expectation of the log transforms (multiplied by 100) is\n\\[\n100\\left(\\mathbb{E}\\left[\\log X_{2}\\right]-\\mathbb{E}\\left[\\log X_{1}\\right]\\right)=100\\left(\\log \\theta_{2}-\\log \\theta_{1}\\right) \\simeq p\n\\]\nthe percentage difference between \\(\\theta_{2}\\) and \\(\\theta_{1}\\). In words, the difference between the average of the log transformed variables is (approximately) the percentage difference in the geometric means.\nThe reason why this latter observation is important is because many econometric equations take the semi-log form\n\\[\n\\begin{aligned}\n&\\mathbb{E}[\\log Y \\mid \\operatorname{group}=1]=\\mu_{1} \\\\\n&\\mathbb{E}[\\log Y \\mid \\operatorname{group}=2]=\\mu_{2}\n\\end{aligned}\n\\]\nand considerable attention is given to the difference \\(\\mu_{1}-\\mu_{2}\\). For example, in the previous section we compared the average log wages for men and women and found that the difference is \\(0.24\\). In that section we stated that this difference is often interpreted as the average percentage difference. This is not quite right, but is not quite wrong either. What the above calculation shows is that this difference is approximately the percentage difference in the geometric mean. So \\(\\mu_{1}-\\mu_{2}\\) is an average percentage difference, where “average” refers to geometric rather than arithmetic mean.\nTo compare different measures of percentage difference see Table 2.2. In the first two columns we report average wages for men and women in the CPS population using four “averages”: arithmetic mean, median, geometric mean, and mean log. For both groups the arithmetic mean is higher than the median and geometric mean, and the latter two are similar to one another. This is a common feature of skewed distributions such as the wage distribution. The third column reports the percentage difference between the first two columns (using men’s wages as the base). For example, the first entry of \\(34 %\\) states that the mean wage for men is \\(34 %\\) higher than the mean wage for women. The next entries show that the median and geometric mean for men is \\(26 %\\) higher than those for women. The final entry in this column is 100 times the simple difference between the mean log wage, which is \\(24 %\\). As shown above, the difference in the mean of the log transformation is approximately the percentage difference in the geometric mean, and this approximation is excellent for differences under \\(10 %\\).\nLet’s summarize this analysis. It is common to take logarithms of variables and make comparisons between conditional means. We have shown that these differences are measures of the percentage difference in the geometric mean. Thus the common description that the difference between expected log transforms (such as the \\(0.24\\) difference between those for men and women’s wages) is an approximate percentage difference (e.g. a 24% difference in men’s wages relative to women’s) is correct, so long as we realize that we are implicitly comparing geometric means."
  },
  {
    "objectID": "chpt02-ce.html#conditional-expectation-function",
    "href": "chpt02-ce.html#conditional-expectation-function",
    "title": "2  Conditional Expectation and Projection",
    "section": "2.5 Conditional Expectation Function",
    "text": "2.5 Conditional Expectation Function\nAn important determinant of wages is education. In many empirical studies economists measure educational attainment by the number of years \\({ }^{7}\\) of schooling. We will write this variable as education.\n\\({ }^{7}\\) Here, education is defined as years of schooling beyond kindergarten. A high school graduate has education=12, a college graduate has education=16, a Master’s degree has education=18, and a professional degree (medical, law or PhD) has educa- Table 2.2: Average Wages and Percentage Differences\n\n\n\n\nmen\nwomen\n% Difference\n\n\n\n\nArithmetic Mean\n\\(\\$ 26.80\\)\n\\(\\$ 20.00\\)\n\\(34 %\\)\n\n\nMedian\n\\(\\$ 21.14\\)\n\\(\\$ 16.83\\)\n\\(26 %\\)\n\n\nGeometric Mean\n\\(\\$ 21.03\\)\n\\(\\$ 16.64\\)\n\\(26 %\\)\n\n\nMean log Wage\n\\(3.05\\)\n\\(2.81\\)\n\\(24 %\\)\n\n\n\nThe conditional expectation of \\(\\log (\\) wage \\()\\) given gender, race, and education is a single number for each category. For example\n\\[\n\\mathbb{E}[\\log (\\text { wage }) \\mid \\text { gender }=\\text { man, race }=\\text { white, education }=12]=2.84 .\n\\]\nWe display in Figure \\(2.3\\) the conditional expectation of \\(\\log\\) (wage) as a function of education, separately for (white) men and women. The plot is quite revealing. We see that the conditional expectation is increasing in years of education, but at a different rate for schooling levels above and below nine years. Another striking feature of Figure \\(2.3\\) is that the gap between men and women is roughly constant for all education levels. As the variables are measured in logs this implies a constant average percentage gap between men and women regardless of educational attainment.\n\nFigure 2.3: Expected Log Wage as a Function of Education tion=20. In many cases it is convenient to simplify the notation by writing variables using single characters, typically \\(Y, X\\), and/or \\(Z\\). It is conventional in econometrics to denote the dependent variable (e.g. \\(\\log (\\) wage \\()\\) ) by the letter \\(Y\\), a conditioning variable (such as gender) by the letter \\(X\\), and multiple conditioning variables (such as race, education and gender) by the subscripted letters \\(X_{1}, X_{2}, \\ldots, X_{k}\\).\nConditional expectations can be written with the generic notation\n\\[\n\\mathbb{E}\\left[Y \\mid X_{1}=x_{1}, X_{2}=x_{2}, \\ldots, X_{k}=x_{k}\\right]=m\\left(x_{1}, x_{2}, \\ldots, x_{k}\\right) \\text {. }\n\\]\nWe call this the conditional expectation function (CEF). The CEF is a function of \\(\\left(x_{1}, x_{2}, \\ldots, x_{k}\\right)\\) as it varies with the variables. For example, the conditional expectation of \\(Y=\\log (\\) wage \\()\\) given \\(\\left(X_{1}, X_{2}\\right)=(g e n d e r\\), race) is given by the six entries of Table \\(2.1 .\\)\nFor greater compactness we typically write the conditioning variables as a vector in \\(\\mathbb{R}^{k}\\) :\n\\[\nX=\\left(\\begin{array}{c}\nX_{1} \\\\\nX_{2} \\\\\n\\vdots \\\\\nX_{k}\n\\end{array}\\right)\n\\]\nGiven this notation, the CEF can be compactly written as\n\\[\n\\mathbb{E}[Y \\mid X=x]=m(x) .\n\\]\nThe CEF \\(m(x)=\\mathbb{E}[Y \\mid X=x]\\) is a function of \\(x \\in \\mathbb{R}^{k}\\). It says: “When \\(X\\) takes the value \\(x\\) then the average value of \\(Y\\) is \\(m(x)\\).” Sometimes it is useful to view the CEF as a function of the random variable \\(X\\). In this case we evaluate the function \\(m(x)\\) at \\(X\\), and write \\(m(X)\\) or \\(\\mathbb{E}[Y \\mid X]\\). This is random as it is a function of the random variable \\(X\\)."
  },
  {
    "objectID": "chpt02-ce.html#continuous-variables",
    "href": "chpt02-ce.html#continuous-variables",
    "title": "2  Conditional Expectation and Projection",
    "section": "2.6 Continuous Variables",
    "text": "2.6 Continuous Variables\nIn the previous sections we implicitly assumed that the conditioning variables are discrete. However, many conditioning variables are continuous. In this section, we take up this case and assume that the variables \\((Y, X)\\) are continuously distributed with a joint density function \\(f(y, x)\\).\nAs an example, take \\(Y=\\log (\\) wage \\()\\) and \\(X=\\) experience, the latter the number of years of potential labor market experience \\({ }^{8}\\). The contours of their joint density are plotted in Figure \\(2.4\\) (a) for the population of white men with 12 years of education.\nGiven the joint density \\(f(y, x)\\) the variable \\(x\\) has the marginal density\n\\[\nf_{X}(x)=\\int_{-\\infty}^{\\infty} f(y, x) d y .\n\\]\nFor any \\(x\\) such that \\(f_{X}(x)>0\\) the conditional density of \\(Y\\) given \\(X\\) is defined as\n\\[\nf_{Y \\mid X}(y \\mid x)=\\frac{f(y, x)}{f_{X}(x)} .\n\\]\nThe conditional density is a renormalized slice of the joint density \\(f(y, x)\\) holding \\(x\\) fixed. The slice is renormalized (divided by \\(f_{X}(x)\\) so that it integrates to one) and is thus a density. We can visualize this by slicing the joint density function at a specific value of \\(x\\) parallel with the \\(y\\)-axis. For example, take the density contours in Figure 2.4(a) and slice through the contour plot at a specific value of experience, and\n\\({ }^{8}\\) As there is no direct measure for experience, we instead define experience as age-education-6\n\n\nJoint Density of Log Wage and Experience\n\n\n\nConditional Density of Log Wage given Experience\n\nFigure 2.4: Log Wage and Experience\nthen renormalize the slice so that it is a proper density. This gives us the conditional density of log(wage) for white men with 12 years of education and this level of experience. We do this for three levels of experience \\((5,10\\), and 25 years), and plot these densities in Figure \\(2.4\\) (b). We can see that the distribution of wages shifts to the right and becomes more diffuse as experience increases.\nThe CEF of \\(Y\\) given \\(X=x\\) is the expectation of the conditional density (2.5)\n\\[\nm(x)=\\mathbb{E}[Y \\mid X=x]=\\int_{-\\infty}^{\\infty} y f_{Y \\mid X}(y \\mid x) d y .\n\\]\nIntuitively, \\(m(x)\\) is the expectation of \\(Y\\) for the idealized subpopulation where the conditioning variables are fixed at \\(x\\). When \\(X\\) is continuously distributed this subpopulation is infinitely small.\nThis definition (2.6) is appropriate when the conditional density (2.5) is well defined. However, Theorem \\(2.13\\) in Section \\(2.31\\) will show that \\(m(x)\\) can be defined for any random variables \\((Y, X)\\) so long as \\(\\mathbb{E}|Y|<\\infty\\)\nIn Figure 2.4(a) the CEF of \\(\\log\\) (wage) given experience is plotted as the solid line. We can see that the CEF is a smooth but nonlinear function. The CEF is initially increasing in experience, flattens out around experience \\(=30\\), and then decreases for high levels of experience."
  },
  {
    "objectID": "chpt02-ce.html#law-of-iterated-expectations",
    "href": "chpt02-ce.html#law-of-iterated-expectations",
    "title": "2  Conditional Expectation and Projection",
    "section": "2.7 Law of Iterated Expectations",
    "text": "2.7 Law of Iterated Expectations\nAn extremely useful tool from probability theory is the law of iterated expectations. An important special case is known as the Simple Law. Theorem 2.1 Simple Law of Iterated Expectations\nIf \\(\\mathbb{E}|Y|<\\infty\\) then for any random vector \\(X\\),\n\\[\n\\mathbb{E}[\\mathbb{E}[Y \\mid X]]=\\mathbb{E}[Y] .\n\\]\nThis states that the expectation of the conditional expectation is the unconditional expectation. In other words the average of the conditional averages is the unconditional average. For discrete \\(X\\)\n\\[\n\\mathbb{E}[\\mathbb{E}[Y \\mid X]]=\\sum_{j=1}^{\\infty} \\mathbb{E}\\left[Y \\mid X=x_{j}\\right] \\mathbb{P}\\left[X=x_{j}\\right] .\n\\]\nFor continuous \\(X\\)\n\\[\n\\mathbb{E}[\\mathbb{E}[Y \\mid X]]=\\int_{\\mathbb{R}^{k}} \\mathbb{E}[Y \\mid X=x] f_{X}(x) d x .\n\\]\nGoing back to our investigation of average log wages for men and women, the simple law states that\n\\[\n\\begin{aligned}\n&\\mathbb{E}[\\log (\\text { wage }) \\mid \\text { gender }=\\text { man }] \\mathbb{P}[\\text { gender }=\\text { man }] \\\\\n&+\\mathbb{E}[\\log (\\text { wage }) \\mid \\text { gender }=\\text { woman }] \\mathbb{P}[\\text { gender }=\\text { woman }] \\\\\n&=\\mathbb{E}[\\log (\\text { wage })]\n\\end{aligned}\n\\]\nOr numerically,\n\\[\n3.05 \\times 0.57+2.81 \\times 0.43=2.95 \\text {. }\n\\]\nThe general law of iterated expectations allows two sets of conditioning variables.\nTheorem 2.2 Law of Iterated Expectations If \\(\\mathbb{E}|Y|<\\infty\\) then for any random vectors \\(X_{1}\\) and \\(X_{2}\\),\n\\[\n\\mathbb{E}\\left[\\mathbb{E}\\left[Y \\mid X_{1}, X_{2}\\right] \\mid X_{1}\\right]=\\mathbb{E}\\left[Y \\mid X_{1}\\right] .\n\\]\nNotice the way the law is applied. The inner expectation conditions on \\(X_{1}\\) and \\(X_{2}\\), while the outer expectation conditions only on \\(X_{1}\\). The iterated expectation yields the simple answer \\(\\mathbb{E}\\left[Y \\mid X_{1}\\right]\\), the expectation conditional on \\(X_{1}\\) alone. Sometimes we phrase this as: “The smaller information set wins.”\nAs an example\n\\[\n\\begin{aligned}\n&\\mathbb{E}[\\log (\\text { wage }) \\mid \\text { gender }=\\text { man, race }=\\text { white }] \\mathbb{P}[\\text { race }=\\text { white } \\mid \\text { gender }=\\text { man }] \\\\\n&+\\mathbb{E}[\\log (\\text { wage }) \\mid \\text { gender }=\\text { man, race }=\\text { Black }] \\mathbb{P}[\\text { race }=\\text { Black } \\mid \\text { gender }=\\text { man }] \\\\\n&+\\mathbb{E}[\\log (\\text { wage }) \\mid \\text { gender }=\\text { man, race }=\\text { other }] \\mathbb{P}[\\text { race }=\\text { other } \\mid \\text { gender }=\\text { man }] \\\\\n&=\\mathbb{E}[\\log (\\text { wage }) \\mid \\text { gender }=\\text { man }]\n\\end{aligned}\n\\]\nor numerically\n\\[\n3.07 \\times 0.84+2.86 \\times 0.08+3.03 \\times 0.08=3.05 \\text {. }\n\\]\nA property of conditional expectations is that when you condition on a random vector \\(X\\) you can effectively treat it as if it is constant. For example, \\(\\mathbb{E}[X \\mid X]=X\\) and \\(\\mathbb{E}[g(X) \\mid X]=g(X)\\) for any function \\(g(\\cdot)\\). The general property is known as the Conditioning Theorem.\nTheorem 2.3 Conditioning Theorem If \\(\\mathbb{E}|Y|<\\infty\\) then\n\\[\n\\mathbb{E}[g(X) Y \\mid X]=g(X) \\mathbb{E}[Y \\mid X] .\n\\]\nIf in addition \\(\\mathbb{E}|g(X)|<\\infty\\) then\n\\[\n\\mathbb{E}[g(X) Y]=\\mathbb{E}[g(X) \\mathbb{E}[Y \\mid X]] .\n\\]\nThe proofs of Theorems 2.1, \\(2.2\\) and \\(2.3\\) are given in Section \\(2.33 .\\)"
  },
  {
    "objectID": "chpt02-ce.html#cef-error",
    "href": "chpt02-ce.html#cef-error",
    "title": "2  Conditional Expectation and Projection",
    "section": "2.8 CEF Error",
    "text": "2.8 CEF Error\nThe CEF error \\(e\\) is defined as the difference between \\(Y\\) and the CEF evaluated at \\(X\\) :\n\\[\ne=Y-m(X) .\n\\]\nBy construction, this yields the formula\n\\[\nY=m(X)+e .\n\\]\nIn (2.9) it is useful to understand that the error \\(e\\) is derived from the joint distribution of \\((Y, X)\\), and so its properties are derived from this construction.\nMany authors in econometrics denote the CEF error using the Greek letter \\(\\varepsilon\\). I do not follow this convention because the error \\(e\\) is a random variable similar to \\(Y\\) and \\(X\\), and it is typical to use Latin characters for random variables.\nA key property of the CEF error is that it has a conditional expectation of zero. To see this, by the linearity of expectations, the definition \\(m(X)=\\mathbb{E}[Y \\mid X]\\), and the Conditioning Theorem\n\\[\n\\begin{aligned}\n\\mathbb{E}[e \\mid X] &=\\mathbb{E}[(Y-m(X)) \\mid X] \\\\\n&=\\mathbb{E}[Y \\mid X]-\\mathbb{E}[m(X) \\mid X] \\\\\n&=m(X)-m(X)=0 .\n\\end{aligned}\n\\]\nThis fact can be combined with the law of iterated expectations to show that the unconditional expectation is also zero.\n\\[\n\\mathbb{E}[e]=\\mathbb{E}[\\mathbb{E}[e \\mid X]]=\\mathbb{E}[0]=0 .\n\\]\nWe state this and some other results formally.\nTheorem 2.4 Properties of the CEF error\nIf \\(\\mathbb{E}|Y|<\\infty\\) then\n\n\\(\\mathbb{E}[e \\mid X]=0\\).\n\\(\\mathbb{E}[e]=0\\).\nIf \\(\\mathbb{E}|Y|^{r}<\\infty\\) for \\(r \\geq 1\\) then \\(\\mathbb{E}|e|^{r}<\\infty\\).\nFor any function \\(h(x)\\) such that \\(\\mathbb{E}|h(X) e|<\\infty\\) then \\(\\mathbb{E}[h(X) e]=0\\). The proof of the third result is deferred to Section 2.33. The fourth result, whose proof is left to Exercise 2.3, implies that \\(e\\) is uncorrelated with any function of the regressors.\n\nThe equations\n\\[\n\\begin{aligned}\nY &=m(X)+e \\\\\n\\mathbb{E}[e \\mid X] &=0\n\\end{aligned}\n\\]\ntogether imply that \\(m(X)\\) is the CEF of \\(Y\\) given \\(X\\). It is important to understand that this is not a restriction. These equations hold true by definition.\nThe condition \\(\\mathbb{E}[e \\mid X]=0\\) is implied by the definition of \\(e\\) as the difference between \\(Y\\) and the CEF \\(m(X)\\). The equation \\(\\mathbb{E}[e \\mid X]=0\\) is sometimes called a conditional mean restriction, because the conditional mean of the error \\(e\\) is restricted to equal zero. The property is also sometimes called mean independence, for the conditional mean of \\(e\\) is 0 and thus independent of \\(X\\). However, it does not imply that the distribution of \\(e\\) is independent of \\(X\\). Sometimes the assumption ” \\(e\\) is independent of \\(X\\) ” is added as a convenient simplification, but it is not generic feature of the conditional mean. Typically and generally, \\(e\\) and \\(X\\) are jointly dependent even though the conditional mean of \\(e\\) is zero.\nAs an example, the contours of the joint density of the regression error \\(e\\) and experience are plotted in Figure \\(2.5\\) for the same population as Figure 2.4. Notice that the shape of the conditional distribution varies with the level of experience.\n\nLabor Market Experience (Years)\nFigure 2.5: Joint Density of Regression Error and Experience\nAs a simple example of a case where \\(X\\) and \\(e\\) are mean independent yet dependent let \\(e=X u\\) where \\(X\\) and \\(u\\) are independent \\(\\mathrm{N}(0,1)\\). Then conditional on \\(X\\) the error \\(e\\) has the distribution \\(\\mathrm{N}\\left(0, X^{2}\\right)\\). Thus \\(\\mathbb{E}[e \\mid X]=0\\) and \\(e\\) is mean independent of \\(X\\), yet \\(e\\) is not fully independent of \\(X\\). Mean independence does not imply full independence."
  },
  {
    "objectID": "chpt02-ce.html#intercept-only-model",
    "href": "chpt02-ce.html#intercept-only-model",
    "title": "2  Conditional Expectation and Projection",
    "section": "2.9 Intercept-Only Model",
    "text": "2.9 Intercept-Only Model\nA special case of the regression model is when there are no regressors \\(X\\). In this case \\(m(X)=\\mathbb{E}[Y]=\\mu\\), the unconditional expectation of \\(Y\\). We can still write an equation for \\(Y\\) in the regression format:\n\\[\n\\begin{aligned}\nY &=\\mu+e \\\\\n\\mathbb{E}[e] &=0 .\n\\end{aligned}\n\\]\nThis is useful for it unifies the notation."
  },
  {
    "objectID": "chpt02-ce.html#regression-variance",
    "href": "chpt02-ce.html#regression-variance",
    "title": "2  Conditional Expectation and Projection",
    "section": "2.10 Regression Variance",
    "text": "2.10 Regression Variance\nAn important measure of the dispersion about the CEF function is the unconditional variance of the CEF error \\(e\\). We write this as\n\\[\n\\sigma^{2}=\\operatorname{var}[e]=\\mathbb{E}\\left[(e-\\mathbb{E}[e])^{2}\\right]=\\mathbb{E}\\left[e^{2}\\right] .\n\\]\nTheorem 2.4.3 implies the following simple but useful result.\nTheorem 2.5 If \\(\\mathbb{E}\\left[Y^{2}\\right]<\\infty\\) then \\(\\sigma^{2}<\\infty\\).\nWe can call \\(\\sigma^{2}\\) the regression variance or the variance of the regression error. The magnitude of \\(\\sigma^{2}\\) measures the amount of variation in \\(Y\\) which is not “explained” or accounted for in the conditional expectation \\(\\mathbb{E}[Y \\mid X]\\).\nThe regression variance depends on the regressors \\(X\\). Consider two regressions\n\\[\n\\begin{aligned}\n&Y=\\mathbb{E}\\left[Y \\mid X_{1}\\right]+e_{1} \\\\\n&Y=\\mathbb{E}\\left[Y \\mid X_{1}, X_{2}\\right]+e_{2} .\n\\end{aligned}\n\\]\nWe write the two errors distinctly as \\(e_{1}\\) and \\(e_{2}\\) as they are different - changing the conditioning information changes the conditional expectation and therefore the regression error as well.\nIn our discussion of iterated expectations we have seen that by increasing the conditioning set the conditional expectation reveals greater detail about the distribution of \\(Y\\). What is the implication for the regression error?\nIt turns out that there is a simple relationship. We can think of the conditional expectation \\(\\mathbb{E}[Y \\mid X]\\) as the “explained portion” of \\(Y\\). The remainder \\(e=Y-\\mathbb{E}[Y \\mid X]\\) is the “unexplained portion”. The simple relationship we now derive shows that the variance of this unexplained portion decreases when we condition on more variables. This relationship is monotonic in the sense that increasing the amount of information always decreases the variance of the unexplained portion.\nTheorem 2.6 If \\(\\mathbb{E}\\left[Y^{2}\\right]<\\infty\\) then\n\\[\n\\operatorname{var}[Y] \\geq \\operatorname{var}\\left[Y-\\mathbb{E}\\left[Y \\mid X_{1}\\right]\\right] \\geq \\operatorname{var}\\left[Y-\\mathbb{E}\\left[Y \\mid X_{1}, X_{2}\\right]\\right] .\n\\]\nTheorem \\(2.6\\) says that the variance of the difference between \\(Y\\) and its conditional expectation (weakly) decreases whenever an additional variable is added to the conditioning information.\nThe proof of Theorem \\(2.6\\) is given in Section 2.33."
  },
  {
    "objectID": "chpt02-ce.html#best-predictor",
    "href": "chpt02-ce.html#best-predictor",
    "title": "2  Conditional Expectation and Projection",
    "section": "2.11 Best Predictor",
    "text": "2.11 Best Predictor\nSuppose that given a random vector \\(X\\) we want to predict or forecast \\(Y\\). We can write any predictor as a function \\(g(X)\\) of \\(X\\). The (ex-post) prediction error is the realized difference \\(Y-g(X)\\). A non-stochastic measure of the magnitude of the prediction error is the expectation of its square\n\\[\n\\mathbb{E}\\left[(Y-g(X))^{2}\\right] .\n\\]\nWe can define the best predictor as the function \\(g(X)\\) which minimizes (2.10). What function is the best predictor? It turns out that the answer is the CEF \\(m(X)\\). This holds regardless of the joint distribution of \\((Y, X)\\).\nTo see this, note that the mean squared error of a predictor \\(g(X)\\) is\n\\[\n\\begin{aligned}\n\\mathbb{E}\\left[(Y-g(X))^{2}\\right] &=\\mathbb{E}\\left[(e+m(X)-g(X))^{2}\\right] \\\\\n&=\\mathbb{E}\\left[e^{2}\\right]+2 \\mathbb{E}[e(m(X)-g(X))]+\\mathbb{E}\\left[(m(X)-g(X))^{2}\\right] \\\\\n&=\\mathbb{E}\\left[e^{2}\\right]+\\mathbb{E}\\left[(m(X)-g(X))^{2}\\right] \\\\\n& \\geq \\mathbb{E}\\left[e^{2}\\right] \\\\\n&=\\mathbb{E}\\left[(Y-m(X))^{2}\\right] .\n\\end{aligned}\n\\]\nThe first equality makes the substitution \\(Y=m(X)+e\\) and the third equality uses Theorem 2.4.4. The right-hand-side after the third equality is minimized by setting \\(g(X)=m(X)\\), yielding the inequality in the fourth line. The minimum is finite under the assumption \\(\\mathbb{E}\\left[Y^{2}\\right]<\\infty\\) as shown by Theorem \\(2.5\\).\nWe state this formally in the following result.\nTheorem 2.7 Conditional Expectation as Best Predictor If \\(\\mathbb{E}\\left[Y^{2}\\right]<\\infty\\), then for any predictor \\(g(X)\\),\n\\[\n\\mathbb{E}\\left[(Y-g(X))^{2}\\right] \\geq \\mathbb{E}\\left[(Y-m(X))^{2}\\right]\n\\]\nwhere \\(m(X)=\\mathbb{E}[Y \\mid X]\\)\nIt may be helpful to consider this result in the context of the intercept-only model\n\\[\n\\begin{aligned}\nY &=\\mu+e \\\\\n\\mathbb{E}[e] &=0 .\n\\end{aligned}\n\\]\nTheorem \\(2.7\\) shows that the best predictor for \\(Y\\) (in the class of constants) is the unconditional mean \\(\\mu=\\mathbb{E}[Y]\\) in the sense that the mean minimizes the mean squared prediction error."
  },
  {
    "objectID": "chpt02-ce.html#conditional-variance",
    "href": "chpt02-ce.html#conditional-variance",
    "title": "2  Conditional Expectation and Projection",
    "section": "2.12 Conditional Variance",
    "text": "2.12 Conditional Variance\nWhile the conditional mean is a good measure of the location of a conditional distribution it does not provide information about the spread of the distribution. A common measure of the dispersion is the conditional variance. We first give the general definition of the conditional variance of a random variable \\(Y\\).\nDefinition 2.1 If \\(\\mathbb{E}\\left[Y^{2}\\right]<\\infty\\), the conditional variance of \\(Y\\) given \\(X=x\\) is\n\\[\n\\sigma^{2}(x)=\\operatorname{var}[Y \\mid X=x]=\\mathbb{E}\\left[(Y-\\mathbb{E}[Y \\mid X=x])^{2} \\mid X=x\\right] .\n\\]\nThe conditional variance treated as a random variable is \\(\\operatorname{var}[Y \\mid X]=\\sigma^{2}(X)\\).\nThe conditional variance is distinct from the unconditional variance var \\([Y]\\). The difference is that the conditional variance is a function of the conditioning variables. Notice that the conditional variance is the conditional second moment, centered around the conditional first moment.\nGiven this definition we define the conditional variance of the regression error.\nDefinition 2.2 If \\(\\mathbb{E}\\left[e^{2}\\right]<\\infty\\), the conditional variance of the regression error \\(e\\) given \\(X=x\\) is\n\\[\n\\sigma^{2}(x)=\\operatorname{var}[e \\mid X=x]=\\mathbb{E}\\left[e^{2} \\mid X=x\\right] .\n\\]\nThe conditional variance of \\(e\\) treated as a random variable is \\(\\operatorname{var}[e \\mid X]=\\sigma^{2}(X)\\).\nAgain, the conditional variance \\(\\sigma^{2}(x)\\) is distinct from the unconditional variance \\(\\sigma^{2}\\). The conditional variance is a function of the regressors, the unconditional variance is not. Generally, \\(\\sigma^{2}(x)\\) is a non-trivial function of \\(x\\) and can take any form subject to the restriction that it is non-negative. One way to think about \\(\\sigma^{2}(x)\\) is that it is the conditional mean of \\(e^{2}\\) given \\(X\\). Notice as well that \\(\\sigma^{2}(x)=\\operatorname{var}[Y \\mid X=x]\\) so it is equivalently the conditional variance of the dependent variable.\nThe variance of \\(Y\\) is in a different unit of measurement than \\(Y\\). To convert the variance to the same unit of measure we define the conditional standard deviation as its square root \\(\\sigma(x)=\\sqrt{\\sigma^{2}(x)}\\).\nAs an example of how the conditional variance depends on observables, compare the conditional log wage densities for men and women displayed in Figure 2.2. The difference between the densities is not purely a location shift but is also a difference in spread. Specifically, we can see that the density for men’s log wages is somewhat more spread out than that for women, while the density for women’s wages is somewhat more peaked. Indeed, the conditional standard deviation for men’s wages is \\(3.05\\) and that for women is \\(2.81\\). So while men have higher average wages they are also somewhat more dispersed.\nThe unconditional variance is related to the conditional variance by the following identity.\nTheorem 2.8 If \\(\\mathbb{E}\\left[Y^{2}\\right]<\\infty\\) then\n\\[\n\\operatorname{var}[Y]=\\mathbb{E}[\\operatorname{var}[Y \\mid X]]+\\operatorname{var}[\\mathbb{E}[Y \\mid X]] .\n\\]\nSee Theorem \\(4.14\\) of Probability and Statistics for Economists. Theorem \\(2.8\\) decomposes the unconditional variance into what are sometimes called the “within group variance” and the “across group variance”. For example, if \\(X\\) is education level, then the first term is the expected variance of the conditional expectation by education level. The second term is the variance after controlling for education.\nThe regression error has a conditional mean of zero, so its unconditional error variance equals the expected conditional variance, or equivalently can be found by the law of iterated expectations.\n\\[\n\\sigma^{2}=\\mathbb{E}\\left[e^{2}\\right]=\\mathbb{E}\\left[\\mathbb{E}\\left[e^{2} \\mid X\\right]\\right]=\\mathbb{E}\\left[\\sigma^{2}(X)\\right] .\n\\]\nThat is, the unconditional error variance is the average conditional variance.\nGiven the conditional variance we can define a rescaled error\n\\[\nu=\\frac{e}{\\sigma(X)} \\text {. }\n\\]\nWe calculate that since \\(\\sigma(X)\\) is a function of \\(X\\)\n\\[\n\\mathbb{E}[u \\mid X]=\\mathbb{E}\\left[\\frac{e}{\\sigma(X)} \\mid X\\right]=\\frac{1}{\\sigma(X)} \\mathbb{E}[e \\mid X]=0\n\\]\nand\n\\[\n\\operatorname{var}[u \\mid X]=\\mathbb{E}\\left[u^{2} \\mid X\\right]=\\mathbb{E}\\left[\\frac{e^{2}}{\\sigma^{2}(X)} \\mid X\\right]=\\frac{1}{\\sigma^{2}(X)} \\mathbb{E}\\left[e^{2} \\mid X\\right]=\\frac{\\sigma^{2}(X)}{\\sigma^{2}(X)}=1 .\n\\]\nThus \\(u\\) has a conditional expectation of zero and a conditional variance of 1 .\nNotice that (2.11) can be rewritten as\n\\[\ne=\\sigma(X) u .\n\\]\nand substituting this for \\(e\\) in the CEF equation (2.9), we find that\n\\[\nY=m(X)+\\sigma(X) u .\n\\]\nThis is an alternative (mean-variance) representation of the CEF equation.\nMany econometric studies focus on the conditional expectation \\(m(x)\\) and either ignore the conditional variance \\(\\sigma^{2}(x)\\), treat it as a constant \\(\\sigma^{2}(x)=\\sigma^{2}\\), or treat it as a nuisance parameter (a parameter not of primary interest). This is appropriate when the primary variation in the conditional distribution is in the mean but can be short-sighted in other cases. Dispersion is relevant to many economic topics, including income and wealth distribution, economic inequality, and price dispersion. Conditional dispersion (variance) can be a fruitful subject for investigation.\nThe perverse consequences of a narrow-minded focus on the mean is parodied in a classic joke:\nAn economist was standing with one foot in a bucket of boiling water and the other foot in a bucket of ice. When asked how he felt, he replied, “On average I feel just fine.”\nClearly, the economist in question ignored variance!"
  },
  {
    "objectID": "chpt02-ce.html#homoskedasticity-and-heteroskedasticity",
    "href": "chpt02-ce.html#homoskedasticity-and-heteroskedasticity",
    "title": "2  Conditional Expectation and Projection",
    "section": "2.13 Homoskedasticity and Heteroskedasticity",
    "text": "2.13 Homoskedasticity and Heteroskedasticity\nAn important special case obtains when the conditional variance \\(\\sigma^{2}(x)\\) is a constant and independent of \\(x\\). This is called homoskedasticity.\nDefinition 2.3 The error is homoskedastic if \\(\\sigma^{2}(x)=\\sigma^{2}\\) does not depend on \\(x\\).\nIn the general case where \\(\\sigma^{2}(x)\\) depends on \\(x\\) we say that the error \\(e\\) is heteroskedastic.\nDefinition 2.4 The error is heteroskedastic if \\(\\sigma^{2}(x)\\) depends on \\(x\\).\nIt is helpful to understand that the concepts homoskedasticity and heteroskedasticity concern the conditional variance, not the unconditional variance. By definition, the unconditional variance \\(\\sigma^{2}\\) is a constant and independent of the regressors \\(X\\). So when we talk about the variance as a function of the regressors we are talking about the conditional variance \\(\\sigma^{2}(x)\\).\nSome older or introductory textbooks describe heteroskedasticity as the case where “the variance of \\(e\\) varies across observations”. This is a poor and confusing definition. It is more constructive to understand that heteroskedasticity means that the conditional variance \\(\\sigma^{2}(x)\\) depends on observables.\nOlder textbooks also tend to describe homoskedasticity as a component of a correct regression specification and describe heteroskedasticity as an exception or deviance. This description has influenced many generations of economists but it is unfortunately backwards. The correct view is that heteroskedasticity is generic and “standard”, while homoskedasticity is unusual and exceptional. The default in empirical work should be to assume that the errors are heteroskedastic, not the converse.\nIn apparent contradiction to the above statement we will still frequently impose the homoskedasticity assumption when making theoretical investigations into the properties of estimation and inference methods. The reason is that in many cases homoskedasticity greatly simplifies the theoretical calculations and it is therefore quite advantageous for teaching and learning. It should always be remembered, however, that homoskedasticity is never imposed because it is believed to be a correct feature of an empirical model but rather because of its simplicity."
  },
  {
    "objectID": "chpt02-ce.html#heteroskedastic-or-heteroscedastic",
    "href": "chpt02-ce.html#heteroskedastic-or-heteroscedastic",
    "title": "2  Conditional Expectation and Projection",
    "section": "2.14 Heteroskedastic or Heteroscedastic?",
    "text": "2.14 Heteroskedastic or Heteroscedastic?\nThe spelling of the words homoskedastic and heteroskedastic have been somewhat controversial. Early econometrics textbooks were split, with some using a “c” as in heteroscedastic and some ” \\(\\mathrm{k}\\) ” as in heteroskedastic. McCulloch (1985) pointed out that the word is derived from Greek roots.\n\\ means “to scatter”. Since the proper transliteration of the Greek letter \\(\\kappa\\) in \\(\\sigma \\kappa \\varepsilon \\delta \\alpha v v v \\mu \\iota\\) is ” \\(\\mathrm{k}\\) “, this implies that the correct English spelling of the two words is with a” \\(\\mathrm{k}\\) ” as in homoskedastic and heteroskedastic."
  },
  {
    "objectID": "chpt02-ce.html#regression-derivative",
    "href": "chpt02-ce.html#regression-derivative",
    "title": "2  Conditional Expectation and Projection",
    "section": "2.15 Regression Derivative",
    "text": "2.15 Regression Derivative\nOne way to interpret the CEF \\(m(x)=\\mathbb{E}[Y \\mid X=x]\\) is in terms of how marginal changes in the regressors \\(X\\) imply changes in the conditional expectation of the response variable \\(Y\\). It is typical to consider marginal changes in a single regressor, say \\(X_{1}\\), holding the remainder fixed. When a regressor \\(X_{1}\\) is continuously distributed, we define the marginal effect of a change in \\(X_{1}\\), holding the variables \\(X_{2}, \\ldots, X_{k}\\) fixed, as the partial derivative of the CEF\n\\[\n\\frac{\\partial}{\\partial x_{1}} m\\left(x_{1}, \\ldots, x_{k}\\right)\n\\]\nWhen \\(X_{1}\\) is discrete we define the marginal effect as a discrete difference. For example, if \\(X_{1}\\) is binary, then the marginal effect of \\(X_{1}\\) on the CEF is\n\\[\nm\\left(1, x_{2}, \\ldots, x_{k}\\right)-m\\left(0, x_{2}, \\ldots, x_{k}\\right)\n\\]\nWe can unify the continuous and discrete cases with the notation\n\\[\n\\nabla_{1} m(x)=\\left\\{\\begin{array}{cc}\n\\frac{\\partial}{\\partial x_{1}} m\\left(x_{1}, \\ldots, x_{k}\\right), & \\text { if } X_{1} \\text { is continuous } \\\\\nm\\left(1, x_{2}, \\ldots, x_{k}\\right)-m\\left(0, x_{2}, \\ldots, x_{k}\\right), & \\text { if } X_{1} \\text { is binary. }\n\\end{array}\\right.\n\\]\nCollecting the \\(k\\) effects into one \\(k \\times 1\\) vector, we define the regression derivative with respect to \\(X\\) :\n\\[\n\\nabla m(x)=\\left[\\begin{array}{c}\n\\nabla_{1} m(x) \\\\\n\\nabla_{2} m(x) \\\\\n\\vdots \\\\\n\\nabla_{k} m(x)\n\\end{array}\\right]\n\\]\nWhen all elements of \\(X\\) are continuous, then we have the simplification \\(\\nabla m(x)=\\frac{\\partial}{\\partial x} m(x)\\), the vector of partial derivatives.\nThere are two important points to remember concerning our definition of the regression derivative. First, the effect of each variable is calculated holding the other variables constant. This is the ceteris paribus concept commonly used in economics. But in the case of a regression derivative, the conditional expectation does not literally hold all else constant. It only holds constant the variables included in the conditional expectation. This means that the regression derivative depends on which regressors are included. For example, in a regression of wages on education, experience, race and gender, the regression derivative with respect to education shows the marginal effect of education on expected wages, holding constant experience, race, and gender. But it does not hold constant an individual’s unobservable characteristics (such as ability), nor variables not included in the regression (such as the quality of education).\nSecond, the regression derivative is the change in the conditional expectation of \\(Y\\), not the change in the actual value of \\(Y\\) for an individual. It is tempting to think of the regression derivative as the change in the actual value of \\(Y\\), but this is not a correct interpretation. The regression derivative \\(\\nabla m(x)\\) is the change in the actual value of \\(Y\\) only if the error \\(e\\) is unaffected by the change in the regressor \\(X\\). We return to a discussion of causal effects in Section 2.30."
  },
  {
    "objectID": "chpt02-ce.html#linear-cef",
    "href": "chpt02-ce.html#linear-cef",
    "title": "2  Conditional Expectation and Projection",
    "section": "2.16 Linear CEF",
    "text": "2.16 Linear CEF\nAn important special case is when the CEF \\(m(x)=\\mathbb{E}[Y \\mid X=x]\\) is linear in \\(x\\). In this case we can write the mean equation as\n\\[\nm(x)=x_{1} \\beta_{1}+x_{2} \\beta_{2}+\\cdots+x_{k} \\beta_{k}+\\beta_{k+1} .\n\\]\nNotationally it is convenient to write this as a simple function of the vector \\(x\\). An easy way to do so is to augment the regressor vector \\(X\\) by listing the number ” 1 ” as an element. We call this the “constant” and the corresponding coefficient is called the “intercept”. Equivalently, specify that the final element \\({ }^{9}\\) of the vector \\(x\\) is \\(x_{k}=1\\). Thus (2.4) has been redefined as the \\(k \\times 1\\) vector\n\\[\nX=\\left(\\begin{array}{c}\nX_{1} \\\\\nX_{2} \\\\\n\\vdots \\\\\nX_{k-1} \\\\\n1\n\\end{array}\\right)\n\\]\nWith this redefinition, the CEF is\n\\[\nm(x)=x_{1} \\beta_{1}+x_{2} \\beta_{2}+\\cdots+\\beta_{k}=x^{\\prime} \\beta\n\\]\nwhere\n\\[\n\\beta=\\left(\\begin{array}{c}\n\\beta_{1} \\\\\n\\vdots \\\\\n\\beta_{k}\n\\end{array}\\right)\n\\]\nis a \\(k \\times 1\\) coefficient vector. This is the linear CEF model. It is also often called the linear regression model, or the regression of \\(Y\\) on \\(X\\).\nIn the linear CEF model the regression derivative is simply the coefficient vector. That is \\(\\nabla m(x)=\\beta\\). This is one of the appealing features of the linear CEF model. The coefficients have simple and natural interpretations as the marginal effects of changing one variable, holding the others constant.\n\\[\n\\begin{aligned}\n&\\text { Linear CEF Model } \\\\\n&\\begin{aligned}\nY &=X^{\\prime} \\beta+e \\\\\n\\mathbb{E}[e \\mid X] &=0\n\\end{aligned}\n\\end{aligned}\n\\]\nIf in addition the error is homoskedastic we call this the homoskedastic linear CEF model."
  },
  {
    "objectID": "chpt02-ce.html#homoskedastic-linear-cef-model",
    "href": "chpt02-ce.html#homoskedastic-linear-cef-model",
    "title": "2  Conditional Expectation and Projection",
    "section": "2.17 Homoskedastic Linear CEF Model",
    "text": "2.17 Homoskedastic Linear CEF Model\n\\[\n\\begin{aligned}\nY &=X^{\\prime} \\beta+e \\\\\n\\mathbb{E}[e \\mid X] &=0 \\\\\n\\mathbb{E}\\left[e^{2} \\mid X\\right] &=\\sigma^{2}\n\\end{aligned}\n\\]\n\\({ }^{9}\\) The order doesn’t matter. It could be any element."
  },
  {
    "objectID": "chpt02-ce.html#linear-cef-with-nonlinear-effects",
    "href": "chpt02-ce.html#linear-cef-with-nonlinear-effects",
    "title": "2  Conditional Expectation and Projection",
    "section": "2.18 Linear CEF with Nonlinear Effects",
    "text": "2.18 Linear CEF with Nonlinear Effects\nThe linear CEF model of the previous section is less restrictive than it might appear, as we can include as regressors nonlinear transformations of the original variables. In this sense, the linear CEF framework is flexible and can capture many nonlinear effects.\nFor example, suppose we have two scalar variables \\(X_{1}\\) and \\(X_{2}\\). The CEF could take the quadratic form\n\\[\nm\\left(x_{1}, x_{2}\\right)=x_{1} \\beta_{1}+x_{2} \\beta_{2}+x_{1}^{2} \\beta_{3}+x_{2}^{2} \\beta_{4}+x_{1} x_{2} \\beta_{5}+\\beta_{6} .\n\\]\nThis equation is quadratic in the regressors \\(\\left(x_{1}, x_{2}\\right)\\) yet linear in the coefficients \\(\\beta=\\left(\\beta_{1}, \\ldots, \\beta_{6}\\right)^{\\prime}\\). We still call (2.14) a linear CEF because it is a linear function of the coefficients. At the same time, it has nonlinear effects because it is nonlinear in the underlying variables \\(x_{1}\\) and \\(x_{2}\\). The key is to understand that (2.14) is quadratic in the variables \\(\\left(x_{1}, x_{2}\\right)\\) yet linear in the coefficients \\(\\beta\\).\nTo simplify the expression we define the transformations \\(x_{3}=x_{1}^{2}, x_{4}=x_{2}^{2}, x_{5}=x_{1} x_{2}\\), and \\(x_{6}=1\\), and redefine the regressor vector as \\(x=\\left(x_{1}, \\ldots, x_{6}\\right)^{\\prime}\\). With this redefinition, \\(m\\left(x_{1}, x_{2}\\right)=x^{\\prime} \\beta\\) which is linear in \\(\\beta\\). For most econometric purposes (estimation and inference on \\(\\beta\\) ) the linearity in \\(\\beta\\) is all that is important.\nAn exception is in the analysis of regression derivatives. In nonlinear equations such as (2.14) the regression derivative should be defined with respect to the original variables not with respect to the transformed variables. Thus\n\\[\n\\begin{aligned}\n&\\frac{\\partial}{\\partial x_{1}} m\\left(x_{1}, x_{2}\\right)=\\beta_{1}+2 x_{1} \\beta_{3}+x_{2} \\beta_{5} \\\\\n&\\frac{\\partial}{\\partial x_{2}} m\\left(x_{1}, x_{2}\\right)=\\beta_{2}+2 x_{2} \\beta_{4}+x_{1} \\beta_{5} .\n\\end{aligned}\n\\]\nWe see that in the model (2.14), the regression derivatives are not a simple coefficient, but are functions of several coefficients plus the levels of \\(\\left(x_{1}, x_{2}\\right)\\). Consequently it is difficult to interpret the coefficients individually. It is more useful to interpret them as a group.\nWe typically call \\(\\beta_{5}\\) the interaction effect. Notice that it appears in both regression derivative equations and has a symmetric interpretation in each. If \\(\\beta_{5}>0\\) then the regression derivative with respect to \\(x_{1}\\) is increasing in the level of \\(x_{2}\\) (and the regression derivative with respect to \\(x_{2}\\) is increasing in the level of \\(x_{1}\\) ), while if \\(\\beta_{5}<0\\) the reverse is true."
  },
  {
    "objectID": "chpt02-ce.html#linear-cef-with-dummy-variables",
    "href": "chpt02-ce.html#linear-cef-with-dummy-variables",
    "title": "2  Conditional Expectation and Projection",
    "section": "2.19 Linear CEF with Dummy Variables",
    "text": "2.19 Linear CEF with Dummy Variables\nWhen all regressors take a finite set of values it turns out the CEF can be written as a linear function of regressors.\nThis simplest example is a binary variable which takes only two distinct values. For example, in traditional data sets the variable gender takes only the values man and woman (or male and female). Binary variables are extremely common in econometric applications and are alternatively called dummy variables or indicator variables.\nConsider the simple case of a single binary regressor. In this case the conditional expectation can only take two distinct values. For example,\n\\[\n\\mathbb{E}[Y \\mid \\text { gender }]=\\left\\{\\begin{array}{llc}\n\\mu_{0} & \\text { if } \\quad \\text { gender }=\\text { man } \\\\\n\\mu_{1} & \\text { if gender }=\\text { woman. }\n\\end{array}\\right.\n\\]\nTo facilitate a mathematical treatment we record dummy variables with the values \\(\\{0,1\\}\\). For example\n\\[\nX_{1}=\\left\\{\\begin{array}{llc}\n0 & \\text { if } & \\text { gender }=\\text { man } \\\\\n1 & \\text { if } & \\text { gender }=\\text { woman } .\n\\end{array}\\right.\n\\]\nGiven this notation we write the conditional expectation as a linear function of the dummy variable \\(X_{1}\\). Thus \\(\\mathbb{E}\\left[Y \\mid X_{1}\\right]=\\beta_{1} X_{1}+\\beta_{2}\\) where \\(\\beta_{1}=\\mu_{1}-\\mu_{0}\\) and \\(\\beta_{2}=\\mu_{0}\\). In this simple regression equation the intercept \\(\\beta_{2}\\) is equal to the conditional expectation of \\(Y\\) for the \\(X_{1}=0\\) subpopulation (men) and the slope \\(\\beta_{1}\\) is equal to the difference in the conditional expectations between the two subpopulations.\nAlternatively, we could have defined \\(X_{1}\\) as\n\\[\nX_{1}= \\begin{cases}1 & \\text { if } \\quad \\text { gender }=\\text { man } \\\\ 0 & \\text { if } \\quad \\text { gender }=\\text { woman } .\\end{cases}\n\\]\nIn this case, the regression intercept is the expectation for women (rather than for men) and the regression slope has switched signs. The two regressions are equivalent but the interpretation of the coefficients has changed. Therefore it is always important to understand the precise definitions of the variables, and illuminating labels are helpful. For example, labelling \\(X_{1}\\) as “gender” does not help distinguish between definitions (2.15) and (2.16). Instead, it is better to label \\(X_{1}\\) as “women” or “female” if definition (2.15) is used, or as “men” or “male” if (2.16) is used.\nNow suppose we have two dummy variables \\(X_{1}\\) and \\(X_{2}\\). For example, \\(X_{2}=1\\) if the person is married, else \\(X_{2}=0\\). The conditional expectation given \\(X_{1}\\) and \\(X_{2}\\) takes at most four possible values:\n\nIn this case we can write the conditional mean as a linear function of \\(X, X_{2}\\) and their product \\(X_{1} X_{2}\\) :\n\\[\n\\mathbb{E}\\left[Y \\mid X_{1}, X_{2}\\right]=\\beta_{1} X_{1}+\\beta_{2} X_{2}+\\beta_{3} X_{1} X_{2}+\\beta_{4}\n\\]\nwhere \\(\\beta_{1}=\\mu_{10}-\\mu_{00}, \\beta_{2}=\\mu_{01}-\\mu_{00}, \\beta_{3}=\\mu_{11}-\\mu_{10}-\\mu_{01}+\\mu_{00}\\), and \\(\\beta_{4}=\\mu_{00}\\).\nWe can view the coefficient \\(\\beta_{1}\\) as the effect of gender on expected log wages for unmarried wage earners, the coefficient \\(\\beta_{2}\\) as the effect of marriage on expected log wages for men wage earners, and the coefficient \\(\\beta_{3}\\) as the difference between the effects of marriage on expected log wages among women and among men. Alternatively, it can also be interpreted as the difference between the effects of gender on expected log wages among married and non-married wage earners. Both interpretations are equally valid. We often describe \\(\\beta_{3}\\) as measuring the interaction between the two dummy variables, or the interaction effect, and describe \\(\\beta_{3}=0\\) as the case when the interaction effect is zero.\nIn this setting we can see that the CEF is linear in the three variables \\(\\left(X_{1}, X_{2}, X_{1} X_{2}\\right)\\). To put the model in the framework of Section \\(2.15\\) we define the regressor \\(X_{3}=X_{1} X_{2}\\) and the regressor vector as\n\\[\nX=\\left(\\begin{array}{c}\nX_{1} \\\\\nX_{2} \\\\\nX_{3} \\\\\n1\n\\end{array}\\right) .\n\\]\nSo while we started with two dummy variables, the number of regressors (including the intercept) is four.\nIf there are three dummy variables \\(X_{1}, X_{2}, X_{3}\\), then \\(\\mathbb{E}\\left[Y \\mid X_{1}, X_{2}, X_{3}\\right]\\) takes at most \\(2^{3}=8\\) distinct values and can be written as the linear function\n\\[\n\\mathbb{E}\\left[Y \\mid X_{1}, X_{2}, X_{3}\\right]=\\beta_{1} X_{1}+\\beta_{2} X_{2}+\\beta_{3} X_{3}+\\beta_{4} X_{1} X_{2}+\\beta_{5} X_{1} X_{3}+\\beta_{6} X_{2} X_{3}+\\beta_{7 X 1} X_{2} X_{3}+\\beta_{8}\n\\]\nwhich has eight regressors including the intercept. In general, if there are \\(p\\) dummy variables \\(X_{1}, \\ldots, X_{p}\\) then the CEF \\(\\mathbb{E}\\left[Y \\mid X_{1}, X_{2}, \\ldots, X_{p}\\right]\\) takes at most \\(2^{p}\\) distinct values and can be written as a linear function of the \\(2^{p}\\) regressors including \\(X_{1}, X_{2}, \\ldots, X_{p}\\) and all cross-products. A linear regression model which includes all \\(2^{p}\\) binary interactions is called a saturated dummy variable regression model. It is a complete model of the conditional expectation. In contrast, a model with no interactions equals\n\\[\n\\mathbb{E}\\left[Y \\mid X_{1}, X_{2}, \\ldots, X_{p}\\right]=\\beta_{1} X_{1}+\\beta_{2} X_{2}+\\cdots+\\beta_{p} X_{p}+\\beta_{p} .\n\\]\nThis has \\(p+1\\) coefficients instead of \\(2^{p}\\).\nWe started this section by saying that the conditional expectation is linear whenever all regressors take only a finite number of possible values. How can we see this? Take a categorical variable, such as race. For example, we earlier divided race into three categories. We can record categorical variables using numbers to indicate each category, for example\n\\[\nX_{3}=\\left\\{\\begin{array}{lll}\n1 & \\text { if } & \\text { white } \\\\\n2 & \\text { if } & \\text { Black } \\\\\n3 & \\text { if } & \\text { other. }\n\\end{array}\\right.\n\\]\nWhen doing so, the values of \\(X_{3}\\) have no meaning in terms of magnitude, they simply indicate the relevant category.\nWhen the regressor is categorical the conditional expectation of \\(Y\\) given \\(X_{3}\\) takes a distinct value for each possibility:\n\\[\n\\mathbb{E}\\left[Y \\mid X_{3}\\right]=\\left\\{\\begin{array}{lll}\n\\mu_{1} & \\text { if } & X_{3}=1 \\\\\n\\mu_{2} & \\text { if } & X_{3}=2 \\\\\n\\mu_{3} & \\text { if } & X_{3}=3 .\n\\end{array}\\right.\n\\]\nThis is not a linear function of \\(X_{3}\\) itself, but it can be made a linear function by constructing dummy variables for two of the three categories. For example\n\\[\n\\begin{aligned}\n&X_{4}=\\left\\{\\begin{array}{llc}\n1 & \\text { if } & \\text { Black } \\\\\n0 & \\text { if } & \\text { not Black }\n\\end{array}\\right. \\\\\n&X_{5}=\\left\\{\\begin{array}{lll}\n1 & \\text { if } & \\text { other } \\\\\n0 & \\text { if } & \\text { not other. }\n\\end{array}\\right.\n\\end{aligned}\n\\]\nIn this case, the categorical variable \\(X_{3}\\) is equivalent to the pair of dummy variables \\(\\left(X_{4}, X_{5}\\right)\\). The explicit relationship is\n\\[\nX_{3}=\\left\\{\\begin{array}{lll}\n1 & \\text { if } & X_{4}=0 \\text { and } X_{5}=0 \\\\\n2 & \\text { if } & X_{4}=1 \\text { and } X_{5}=0 \\\\\n3 & \\text { if } & X_{4}=0 \\text { and } X_{5}=1\n\\end{array}\\right.\n\\]\nGiven these transformations, we can write the conditional expectation of \\(Y\\) as a linear function of \\(X_{4}\\) and \\(X_{5}\\)\n\\[\n\\mathbb{E}\\left[Y \\mid X_{3}\\right]=\\mathbb{E}\\left[Y \\mid X_{4}, X_{5}\\right]=\\beta_{1} X_{4}+\\beta_{2} X_{5}+\\beta_{3} .\n\\]\nWe can write the CEF as either \\(\\mathbb{E}\\left[Y \\mid X_{3}\\right]\\) or \\(\\mathbb{E}\\left[Y \\mid X_{4}, X_{5}\\right]\\) (they are equivalent), but it is only linear as a function of \\(X_{4}\\) and \\(X_{5}\\).\nThis setting is similar to the case of two dummy variables, with the difference that we have not included the interaction term \\(X_{4} X_{5}\\). This is because the event \\(\\left\\{X_{4}=1\\right.\\) and \\(\\left.X_{5}=1\\right\\}\\) is empty by construction, so \\(X_{4} X_{5}=0\\) by definition."
  },
  {
    "objectID": "chpt02-ce.html#best-linear-predictor",
    "href": "chpt02-ce.html#best-linear-predictor",
    "title": "2  Conditional Expectation and Projection",
    "section": "2.20 Best Linear Predictor",
    "text": "2.20 Best Linear Predictor\nWhile the conditional expectation \\(m(X)=\\mathbb{E}[Y \\mid X]\\) is the best predictor of \\(Y\\) among all functions of \\(X\\), its functional form is typically unknown. In particular, the linear CEF model is empirically unlikely to be accurate unless \\(X\\) is discrete and low-dimensional so all interactions are included. Consequently, in most cases it is more realistic to view the linear specification (2.13) as an approximation. In this section we derive a specific approximation with a simple interpretation.\nTheorem \\(2.7\\) showed that the conditional expectation \\(m(X)\\) is the best predictor in the sense that it has the lowest mean squared error among all predictors. By extension, we can define an approximation to the CEF by the linear function with the lowest mean squared error among all linear predictors.\nFor this derivation we require the following regularity condition.\nAssumption \\(2.1\\)\n\n\\(\\mathbb{E}\\left[Y^{2}\\right]<\\infty\\)\n\\(\\mathbb{E}\\|X\\|^{2}<\\infty\\)\n\\(\\boldsymbol{Q}_{X X}=\\mathbb{E}\\left[X X^{\\prime}\\right]\\) is positive definite.\n\nIn Assumption 2.1.2 we use \\(\\|x\\|=\\left(x^{\\prime} x\\right)^{1 / 2}\\) to denote the Euclidean length of the vector \\(x\\).\nThe first two parts of Assumption \\(2.1\\) imply that the variables \\(Y\\) and \\(X\\) have finite means, variances, and covariances. The third part of the assumption is more technical, and its role will become apparent shortly. It is equivalent to imposing that the columns of the matrix \\(\\boldsymbol{Q}_{X X}=\\mathbb{E}\\left[X X^{\\prime}\\right]\\) are linearly independent and that the matrix is invertible.\nA linear predictor for \\(Y\\) is a function \\(X^{\\prime} \\beta\\) for some \\(\\beta \\in \\mathbb{R}^{k}\\). The mean squared prediction error is\n\\[\nS(\\beta)=\\mathbb{E}\\left[\\left(Y-X^{\\prime} \\beta\\right)^{2}\\right] .\n\\]\nThe best linear predictor of \\(Y\\) given \\(X\\), written \\(\\mathscr{P}[Y \\mid X]\\), is found by selecting the \\(\\beta\\) which minimizes \\(S(\\beta)\\).\nDefinition 2.5 The Best Linear Predictor of \\(Y\\) given \\(X\\) is\n\\[\n\\mathscr{P}[Y \\mid X]=X^{\\prime} \\beta\n\\]\nwhere \\(\\beta\\) minimizes the mean squared prediction error\n\\[\nS(\\beta)=\\mathbb{E}\\left[\\left(Y-X^{\\prime} \\beta\\right)^{2}\\right] .\n\\]\nThe minimizer\n\\[\n\\beta=\\underset{b \\in \\mathbb{R}^{k}}{\\operatorname{argmin}} S(b)\n\\]\nis called the Linear Projection Coefficient. We now calculate an explicit expression for its value. The mean squared prediction error (2.17) can be written out as a quadratic function of \\(\\beta\\) :\n\\[\nS(\\beta)=\\mathbb{E}\\left[Y^{2}\\right]-2 \\beta^{\\prime} \\mathbb{E}[X Y]+\\beta^{\\prime} \\mathbb{E}\\left[X X^{\\prime}\\right] \\beta .\n\\]\nThe quadratic structure of \\(S(\\beta)\\) means that we can solve explicitly for the minimizer. The first-order condition for minimization (from Appendix A.20) is\n\\[\n0=\\frac{\\partial}{\\partial \\beta} S(\\beta)=-2 \\mathbb{E}[X Y]+2 \\mathbb{E}\\left[X X^{\\prime}\\right] \\beta .\n\\]\nRewriting \\((2.20)\\) as\n\\[\n2 \\mathbb{E}[X Y]=2 \\mathbb{E}\\left[X X^{\\prime}\\right] \\beta\n\\]\nand dividing by 2 , this equation takes the form\n\\[\n\\boldsymbol{Q}_{X Y}=\\boldsymbol{Q}_{X X} \\beta\n\\]\nwhere \\(\\boldsymbol{Q}_{X Y}=\\mathbb{E}[X Y]\\) is \\(k \\times 1\\) and \\(\\boldsymbol{Q}_{X X}=\\mathbb{E}\\left[X X^{\\prime}\\right]\\) is \\(k \\times k\\). The solution is found by inverting the matrix \\(\\boldsymbol{Q}_{X X}\\), and is written\n\\[\n\\beta=\\boldsymbol{Q}_{X X}^{-1} \\boldsymbol{Q}_{X Y}\n\\]\nor\n\\[\n\\beta=\\left(\\mathbb{E}\\left[X X^{\\prime}\\right]\\right)^{-1} \\mathbb{E}[X Y] .\n\\]\nIt is worth taking the time to understand the notation involved in the expression (2.22). \\(\\boldsymbol{Q}_{X X}\\) is a \\(k \\times k\\) matrix and \\(\\boldsymbol{Q}_{X Y}\\) is a \\(k \\times 1\\) column vector. Therefore, alternative expressions such as \\(\\frac{\\mathbb{E}[X Y]}{\\mathbb{E}\\left[X X^{\\prime}\\right]}\\) or \\(\\mathbb{E}[X Y]\\left(\\mathbb{E}\\left[X X^{\\prime}\\right]\\right)^{-1}\\) are incoherent and incorrect. We also can now see the role of Assumption 2.1.3. It is equivalent to assuming that \\(\\boldsymbol{Q}_{X X}\\) has an inverse \\(\\boldsymbol{Q}_{X X}^{-1}\\) which is necessary for the solution to the normal equations (2.21) to be unique, and equivalently for \\((2.22)\\) to be uniquely defined. In the absence of Assumption \\(2.1 .3\\) there could be multiple solutions to the equation (2.21).\nWe now have an explicit expression for the best linear predictor:\n\\[\n\\mathscr{P}[Y \\mid X]=X^{\\prime}\\left(\\mathbb{E}\\left[X X^{\\prime}\\right]\\right)^{-1} \\mathbb{E}[X Y] .\n\\]\nThis expression is also referred to as the linear projection of \\(Y\\) on \\(X\\).\nThe projection error is\n\\[\ne=Y-X^{\\prime} \\beta .\n\\]\nThis equals the error (2.9) from the regression equation when (and only when) the conditional expectation is linear in \\(X\\), otherwise they are distinct.\nRewriting, we obtain a decomposition of \\(Y\\) into linear predictor and error\n\\[\nY=X^{\\prime} \\beta+e .\n\\]\nIn general, we call equation (2.24) or \\(X^{\\prime} \\beta\\) the best linear predictor of \\(Y\\) given \\(X\\), or the linear projection of \\(Y\\) on \\(X\\). Equation (2.24) is also often called the regression of \\(Y\\) on \\(X\\) but this can sometimes be confusing as economists use the term “regression” in many contexts. (Recall that we said in Section \\(2.15\\) that the linear CEF model is also called the linear regression model.)\nAn important property of the projection error \\(e\\) is\n\\[\n\\mathbb{E}[X e]=0 .\n\\]\nTo see this, using the definitions (2.23) and (2.22) and the matrix properties \\(\\boldsymbol{A} \\boldsymbol{A}^{-1}=\\boldsymbol{I}\\) and \\(\\boldsymbol{I} \\boldsymbol{a}=\\boldsymbol{a}\\),\n\\[\n\\begin{aligned}\n\\mathbb{E}[X e] &=\\mathbb{E}\\left[X\\left(Y-X^{\\prime} \\beta\\right)\\right] \\\\\n&=\\mathbb{E}[X Y]-\\mathbb{E}\\left[X X^{\\prime}\\right]\\left(\\mathbb{E}\\left[X X^{\\prime}\\right]\\right)^{-1} \\mathbb{E}[X Y] \\\\\n&=0\n\\end{aligned}\n\\]\nas claimed.\nEquation (2.25) is a set of \\(k\\) equations, one for each regressor. In other words, (2.25) is equivalent to\n\\[\n\\mathbb{E}\\left[X_{j} e\\right]=0\n\\]\nfor \\(j=1, \\ldots, k\\). As in (2.12), the regressor vector \\(X\\) typically contains a constant, e.g. \\(X_{k}=1\\). In this case (2.27) for \\(j=k\\) is the same as\n\\[\n\\mathbb{E}[e]=0 .\n\\]\nThus the projection error has a mean of zero when the regressor vector contains a constant. (When \\(X\\) does not have a constant (2.28) is not guaranteed. As it is desirable for \\(e\\) to have a zero mean this is a good reason to always include a constant in any regression model.)\nIt is also useful to observe that because \\(\\operatorname{cov}\\left(X_{j}, e\\right)=\\mathbb{E}\\left[X_{j} e\\right]-\\mathbb{E}\\left[X_{j}\\right] \\mathbb{E}[e]\\), then (2.27)-(2.28) together imply that the variables \\(X_{j}\\) and \\(e\\) are uncorrelated.\nThis completes the derivation of the model. We summarize some of the most important properties.\nTheorem 2.9 Properties of Linear Projection Model Under Assumption 2.1,\n\nThe moments \\(\\mathbb{E}\\left[X X^{\\prime}\\right]\\) and \\(\\mathbb{E}[X Y]\\) exist with finite elements.\nThe linear projection coefficient (2.18) exists, is unique, and equals\n\n\\[\n\\beta=\\left(\\mathbb{E}\\left[X X^{\\prime}\\right]\\right)^{-1} \\mathbb{E}[X Y] .\n\\]\n 1. The best linear predictor of \\(Y\\) given \\(X\\) is\n\\[\n\\mathscr{P}(Y \\mid X)=X^{\\prime}\\left(\\mathbb{E}\\left[X X^{\\prime}\\right]\\right)^{-1} \\mathbb{E}[X Y] .\n\\]\n 1. The projection error \\(e=Y-X^{\\prime} \\beta\\) exists. It satisfies \\(\\mathbb{E}\\left[e^{2}\\right]<\\infty\\) and \\(\\mathbb{E}[X e]=0\\).\n\nIf \\(X\\) contains an constant, then \\(\\mathbb{E}[e]=0\\).\nIf \\(\\mathbb{E}|Y|^{r}<\\infty\\) and \\(\\mathbb{E}\\|X\\|^{r}<\\infty\\) for \\(r \\geq 2\\) then \\(\\mathbb{E}|e|^{r}<\\infty\\).\n\nA complete proof of Theorem \\(2.9\\) is given in Section 2.33.\nIt is useful to reflect on the generality of Theorem 2.9. The only restriction is Assumption 2.1. Thus for any random variables \\((Y, X)\\) with finite variances we can define a linear equation (2.24) with the properties listed in Theorem 2.9. Stronger assumptions (such as the linear CEF model) are not necessary. In this sense the linear model (2.24) exists quite generally. However, it is important not to misinterpret the generality of this statement. The linear equation (2.24) is defined as the best linear predictor. It is not necessarily a conditional mean, nor a parameter of a structural or causal economic model. Linear Projection Model\n\\[\n\\begin{aligned}\nY &=X^{\\prime} \\beta+e \\\\\n\\mathbb{E}[X e] &=0 \\\\\n\\beta &=\\left(\\mathbb{E}\\left[X X^{\\prime}\\right]\\right)^{-1} \\mathbb{E}[X Y]\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "chpt02-ce.html#invertibility-and-identification",
    "href": "chpt02-ce.html#invertibility-and-identification",
    "title": "2  Conditional Expectation and Projection",
    "section": "2.21 Invertibility and Identification",
    "text": "2.21 Invertibility and Identification\nThe linear projection coefficient \\(\\beta=\\left(\\mathbb{E}\\left[X X^{\\prime}\\right]\\right)^{-1} \\mathbb{E}[X Y]\\) exists and is unique as long as the \\(k \\times k\\) matrix \\(\\boldsymbol{Q}_{X X}=\\mathbb{E}\\left[X X^{\\prime}\\right]\\) is invertible. The matrix \\(\\boldsymbol{Q}_{X X}\\) is often called the design matrix as in experimental settings the researcher is able to control \\(\\boldsymbol{Q}_{X X}\\) by manipulating the distribution of the regressors \\(X\\).\nObserve that for any non-zero \\(\\alpha \\in \\mathbb{R}^{k}\\),\n\\[\n\\alpha^{\\prime} \\boldsymbol{Q}_{X X} \\alpha=\\mathbb{E}\\left[\\alpha^{\\prime} X X^{\\prime} \\alpha\\right]=\\mathbb{E}\\left[\\left(\\alpha^{\\prime} X\\right)^{2}\\right] \\geq 0\n\\]\nso \\(\\boldsymbol{Q}_{X X}\\) by construction is positive semi-definite, conventionally written as \\(\\boldsymbol{Q}_{X X} \\geq 0\\). The assumption that it is positive definite means that this is a strict inequality, \\(\\mathbb{E}\\left[\\left(\\alpha^{\\prime} X\\right)^{2}\\right]>0\\). This is conventionally written as \\(\\boldsymbol{Q}_{X X}>0\\). This condition means that there is no non-zero vector \\(\\alpha\\) such that \\(\\alpha^{\\prime} X=0\\) identically. Positive definite matrices are invertible. Thus when \\(\\boldsymbol{Q}_{X X}>0\\) then \\(\\beta=\\left(\\mathbb{E}\\left[X X^{\\prime}\\right]\\right)^{-1} \\mathbb{E}[X Y]\\) exists and is uniquely defined. In other words, if we can exclude the possibility that a linear function of \\(X\\) is degenerate, then \\(\\beta\\) is uniquely defined.\nTheorem \\(2.5\\) shows that the linear projection coefficient \\(\\beta\\) is identified (uniquely determined) under Assumption 2.1. The key is invertibility of \\(\\boldsymbol{Q}_{X X}\\). Otherwise, there is no unique solution to the equation\n\\[\n\\boldsymbol{Q}_{X X} \\beta=\\boldsymbol{Q}_{X Y} .\n\\]\nWhen \\(\\boldsymbol{Q}_{X X}\\) is not invertible there are multiple solutions to (2.29). In this case the coefficient \\(\\beta\\) is not identified as it does not have a unique value."
  },
  {
    "objectID": "chpt02-ce.html#minimization",
    "href": "chpt02-ce.html#minimization",
    "title": "2  Conditional Expectation and Projection",
    "section": "2.22 Minimization",
    "text": "2.22 Minimization\nThe mean squared prediction error (2.19) is a function with vector argument of the form\n\\[\nf(x)=a-2 b^{\\prime} x+x^{\\prime} \\boldsymbol{C} x\n\\]\nwhere \\(\\boldsymbol{C}>0\\). For any function of this form, the unique minimizer is\n\\[\nx=\\boldsymbol{C}^{-1} b .\n\\]\nTo see that this is the unique minimizer we present two proofs. The first uses matrix calculus. From Appendix A.20\n\\[\n\\begin{gathered}\n\\frac{\\partial}{\\partial x}\\left(b^{\\prime} x\\right)=b \\\\\n\\frac{\\partial}{\\partial x}\\left(x^{\\prime} \\boldsymbol{C} x\\right)=2 \\boldsymbol{C} x \\\\\n\\frac{\\partial^{2}}{\\partial x \\partial x^{\\prime}}\\left(x^{\\prime} \\boldsymbol{C} x\\right)=2 \\boldsymbol{C} .\n\\end{gathered}\n\\]\nUsing (2.31) and (2.32), we find\n\\[\n\\frac{\\partial}{\\partial x} f(x)=-2 b+2 \\boldsymbol{C} x .\n\\]\nThe first-order condition for minimization sets this derivative equal to zero. Thus the solution satisfies \\(-2 b+2 \\boldsymbol{C} x=0\\). Solving for \\(x\\) we find (2.30). Using (2.33) we also find\n\\[\n\\frac{\\partial^{2}}{\\partial x \\partial x^{\\prime}} f(x)=2 \\boldsymbol{C}>0\n\\]\nwhich is the second-order condition for minimization. This shows that (2.30) is the unique minimizer of \\(f(x)\\).\nOur second proof is algebraic. Re-write \\(f(x)\\) as\n\\[\nf(x)=\\left(a-b^{\\prime} \\boldsymbol{C}^{-1} b\\right)+\\left(x-\\boldsymbol{C}^{-1} b\\right)^{\\prime} \\boldsymbol{C}\\left(x-\\boldsymbol{C}^{-1} b\\right) .\n\\]\nThe first term does not depend on \\(x\\) so does not affect the minimizer. The second term is a quadratic form in a positive definite matrix. This means that for any non-zero \\(\\alpha, \\alpha^{\\prime} \\boldsymbol{C} \\alpha>0\\). Thus for \\(x \\neq C^{-1} b\\), the second-term is strictly positive, yet for \\(x=C^{-1} b\\) this term equals zero. It is therefore minimized at \\(x=C^{-1} b\\) as claimed."
  },
  {
    "objectID": "chpt02-ce.html#illustrations-of-best-linear-predictor",
    "href": "chpt02-ce.html#illustrations-of-best-linear-predictor",
    "title": "2  Conditional Expectation and Projection",
    "section": "2.23 Illustrations of Best Linear Predictor",
    "text": "2.23 Illustrations of Best Linear Predictor\nWe illustrate the best linear predictor (projection) using three log wage equations introduced in earlier sections.\nFor our first example, we consider a model with the two dummy variables for gender and race similar to Table 2.1. As we learned in Section 2.17, the entries in this table can be equivalently expressed by a linear CEF. For simplicity, let’s consider the CEF of \\(\\log (\\) wage \\()\\) as a function of Black and female.\n\\[\n\\mathbb{E}[\\log (\\text { wage }) \\mid \\text { Black, female }]=-0.20 \\text { Black }-0.24 \\text { female }+0.10 \\text { Black } \\times \\text { female }+3.06 \\text {. }\n\\]\nThis is a CEF as the variables are binary and all interactions are included.\nNow consider a simpler model omitting the interaction effect. This is the linear projection on the variables Black and female\n\\[\n\\mathscr{P}[\\log (\\text { wage }) \\mid \\text { Black, female }]=-0.15 \\text { Black }-0.23 \\text { female }+3.06 .\n\\]\nWhat is the difference? The full CEF (2.34) shows that the race gap is differentiated by gender: it is \\(20 %\\) for Black men (relative to non-Black men) and \\(10 %\\) for Black women (relative to non-Black women). The projection model (2.35) simplifies this analysis, calculating an average \\(15 %\\) wage gap for Black wage earners, ignoring the role of gender. Notice that this is despite the fact that gender is included in (2.35).\n\n\nProjections onto Education\n\n\n\nProjections onto Experience\n\nFigure 2.6: Projections of Log Wage onto Education and Experience\nFor our second example we consider the CEF of log wages as a function of years of education for white men which was illustrated in Figure \\(2.3\\) and is repeated in Figure 2.6(a). Superimposed on the figure are two projections. The first (given by the dashed line) is the linear projection of log wages on years of education\n\\[\n\\mathscr{P}[\\log (\\text { wage }) \\mid \\text { education }]=0.11 \\text { education }+1.5 \\text {. }\n\\]\nThis simple equation indicates an average \\(11 %\\) increase in wages for every year of education. An inspection of the Figure shows that this approximation works well for education \\(\\geq 9\\), but under-predicts for individuals with lower levels of education. To correct this imbalance we use a linear spline equation which allows different rates of return above and below 9 years of education:\n\\[\n\\begin{aligned}\n&\\mathscr{P}[\\log (\\text { wage }) \\mid \\text { education, }(\\text { education }-9) \\times \\mathbb{1} \\text { education }>9\\}] \\\\\n&=0.02 \\text { education }+0.10 \\times(\\text { education }-9) \\times \\mathbb{1} \\text { education }>9\\}+2.3 .\n\\end{aligned}\n\\]\nThis equation is displayed in Figure 2.6(a) using the solid line, and appears to fit much better. It indicates a \\(2 %\\) increase in mean wages for every year of education below 9 , and a \\(12 %\\) increase in mean wages for every year of education above 9 . It is still an approximation to the conditional mean but it appears to be fairly reasonable.\nFor our third example we take the CEF of log wages as a function of years of experience for white men with 12 years of education, which was illustrated in Figure \\(2.4\\) and is repeated as the solid line in Figure 2.6(b). Superimposed on the figure are two projections. The first (given by the dot-dashed line) is the linear projection on experience\n\\[\n\\mathscr{P}[\\log (\\text { wage }) \\mid \\text { experience }]=0.011 \\text { experience }+2.5\n\\]\nand the second (given by the dashed line) is the linear projection on experience and its square\n\\[\n\\mathscr{P}[\\log (\\text { wage }) \\mid \\text { experience }]=0.046 \\text { experience }-0.0007 \\text { experience }^{2}+2.3 \\text {. }\n\\]\nIt is fairly clear from an examination of Figure \\(2.6(\\mathrm{~b})\\) that the first linear projection is a poor approximation. It over-predicts wages for young and old workers, under-predicts for the rest, and misses the strong downturn in expected wages for older wage-earners. The second projection fits much better. We can call this equation a quadratic projection because the function is quadratic in experience."
  },
  {
    "objectID": "chpt02-ce.html#linear-predictor-error-variance",
    "href": "chpt02-ce.html#linear-predictor-error-variance",
    "title": "2  Conditional Expectation and Projection",
    "section": "2.24 Linear Predictor Error Variance",
    "text": "2.24 Linear Predictor Error Variance\nAs in the CEF model, we define the error variance as \\(\\sigma^{2}=\\mathbb{E}\\left[e^{2}\\right]\\). Setting \\(Q_{Y Y}=\\mathbb{E}\\left[Y^{2}\\right]\\) and \\(\\boldsymbol{Q}_{Y X}=\\) \\(\\mathbb{E}\\left[Y X^{\\prime}\\right]\\) we can write \\(\\sigma^{2}\\) as\n\\[\n\\begin{aligned}\n\\sigma^{2} &=\\mathbb{E}\\left[\\left(Y-X^{\\prime} \\beta\\right)^{2}\\right] \\\\\n&=\\mathbb{E}\\left[Y^{2}\\right]-2 \\mathbb{E}\\left[Y X^{\\prime}\\right] \\beta+\\beta^{\\prime} \\mathbb{E}\\left[X X^{\\prime}\\right] \\beta \\\\\n&=Q_{Y Y}-2 \\boldsymbol{Q}_{Y X} \\boldsymbol{Q}_{X X}^{-1} \\boldsymbol{Q}_{X Y}+\\boldsymbol{Q}_{Y X} \\boldsymbol{Q}_{X X}^{-1} \\boldsymbol{Q}_{X X} \\boldsymbol{Q}_{X X}^{-1} \\boldsymbol{Q}_{X Y} \\\\\n&=Q_{Y Y}-\\boldsymbol{Q}_{Y X} \\boldsymbol{Q}_{X X}^{-1} \\boldsymbol{Q}_{X Y} \\\\\n& \\stackrel{\\text { def }}{=} Q_{Y Y \\cdot X} .\n\\end{aligned}\n\\]\nOne useful feature of this formula is that it shows that \\(Q_{Y Y \\cdot X}=Q_{Y Y}-\\boldsymbol{Q}_{Y X} \\boldsymbol{Q}_{X X}^{-1} \\boldsymbol{Q}_{X Y}\\) equals the variance of the error from the linear projection of \\(Y\\) on \\(X\\)."
  },
  {
    "objectID": "chpt02-ce.html#regression-coefficients",
    "href": "chpt02-ce.html#regression-coefficients",
    "title": "2  Conditional Expectation and Projection",
    "section": "2.25 Regression Coefficients",
    "text": "2.25 Regression Coefficients\nSometimes it is useful to separate the constant from the other regressors and write the linear projection equation in the format\n\\[\nY=X^{\\prime} \\beta+\\alpha+e\n\\]\nwhere \\(\\alpha\\) is the intercept and \\(X\\) does not contain a constant.\nTaking expectations of this equation, we find\n\\[\n\\mathbb{E}[Y]=\\mathbb{E}\\left[X^{\\prime} \\beta\\right]+\\mathbb{E}[\\alpha]+\\mathbb{E}[e]\n\\]\nor \\(\\mu_{Y}=\\mu_{X}^{\\prime} \\beta+\\alpha\\) where \\(\\mu_{Y}=\\mathbb{E}[Y]\\) and \\(\\mu_{X}=\\mathbb{E}[X]\\), since \\(\\mathbb{E}[e]=0\\) from (2.28). (While \\(X\\) does not contain a constant, the equation does so (2.28) still applies.) Rearranging, we find \\(\\alpha=\\mu_{Y}-\\mu_{X}^{\\prime} \\beta\\). Subtracting this equation from (2.37) we find\n\\[\nY-\\mu_{Y}=\\left(X-\\mu_{X}\\right)^{\\prime} \\beta+e,\n\\]\na linear equation between the centered variables \\(Y-\\mu_{Y}\\) and \\(X-\\mu_{X}\\). (They are centered at their means so are mean-zero random variables.) Because \\(X-\\mu_{X}\\) is uncorrelated with \\(e\\), (2.38) is also a linear projection. Thus by the formula for the linear projection model,\n\\[\n\\begin{aligned}\n\\beta &=\\left(\\mathbb{E}\\left[\\left(X-\\mu_{X}\\right)\\left(X-\\mu_{X}\\right)^{\\prime}\\right]\\right)^{-1} \\mathbb{E}\\left[\\left(X-\\mu_{X}\\right)\\left(Y-\\mu_{Y}\\right)\\right] \\\\\n&=\\operatorname{var}[X]^{-1} \\operatorname{cov}(X, Y)\n\\end{aligned}\n\\]\na function only of the covariances \\({ }^{10}\\) of \\(X\\) and \\(Y\\).\nTheorem 2.10 In the linear projection model \\(Y=X^{\\prime} \\beta+\\alpha+e\\),\n\\[\n\\alpha=\\mu_{Y}-\\mu_{X}^{\\prime} \\beta\n\\]\nand\n\\[\n\\beta=\\operatorname{var}[X]^{-1} \\operatorname{cov}(X, Y) .\n\\]"
  },
  {
    "objectID": "chpt02-ce.html#regression-sub-vectors",
    "href": "chpt02-ce.html#regression-sub-vectors",
    "title": "2  Conditional Expectation and Projection",
    "section": "2.26 Regression Sub-Vectors",
    "text": "2.26 Regression Sub-Vectors\nLet the regressors be partitioned as\n\\[\nX=\\left(\\begin{array}{l}\nX_{1} \\\\\nX_{2}\n\\end{array}\\right)\n\\]\nWe can write the projection of \\(Y\\) on \\(X\\) as\n\\[\n\\begin{aligned}\nY &=X^{\\prime} \\beta+e \\\\\n&=X_{1}^{\\prime} \\beta_{1}+X_{2}^{\\prime} \\beta_{2}+e \\\\\n\\mathbb{E}[X e] &=0 .\n\\end{aligned}\n\\]\nIn this section we derive formulae for the sub-vectors \\(\\beta_{1}\\) and \\(\\beta_{2}\\).\nPartition \\(\\boldsymbol{Q}_{X X}\\) conformably with \\(X\\)\n\\[\n\\boldsymbol{Q}_{X X}=\\left[\\begin{array}{ll}\n\\boldsymbol{Q}_{11} & \\boldsymbol{Q}_{12} \\\\\n\\boldsymbol{Q}_{21} & \\boldsymbol{Q}_{22}\n\\end{array}\\right]=\\left[\\begin{array}{ll}\n\\mathbb{E}\\left[X_{1} X_{1}^{\\prime}\\right] & \\mathbb{E}\\left[X_{1} X_{2}^{\\prime}\\right] \\\\\n\\mathbb{E}\\left[X_{2} X_{1}^{\\prime}\\right] & \\mathbb{E}\\left[X_{2} X_{2}^{\\prime}\\right]\n\\end{array}\\right]\n\\]\nand similarly\n\\[\n\\boldsymbol{Q}_{X Y}=\\left[\\begin{array}{l}\n\\boldsymbol{Q}_{1 Y} \\\\\n\\boldsymbol{Q}_{2 Y}\n\\end{array}\\right]=\\left[\\begin{array}{c}\n\\mathbb{E}\\left[X_{1} Y\\right] \\\\\n\\mathbb{E}\\left[X_{2} Y\\right]\n\\end{array}\\right] .\n\\]\nBy the partitioned matrix inversion formula (A.3)\n\\[\n\\boldsymbol{Q}_{X X}^{-1}=\\left[\\begin{array}{ll}\n\\boldsymbol{Q}_{11} & \\boldsymbol{Q}_{12} \\\\\n\\boldsymbol{Q}_{21} & \\boldsymbol{Q}_{22}\n\\end{array}\\right]^{-1} \\stackrel{\\operatorname{def}}{=}\\left[\\begin{array}{ll}\n\\boldsymbol{Q}^{11} & \\boldsymbol{Q}^{12} \\\\\n\\boldsymbol{Q}^{21} & \\boldsymbol{Q}^{22}\n\\end{array}\\right]=\\left[\\begin{array}{cc}\n\\boldsymbol{Q}_{11 \\cdot 2}^{-1} & -\\boldsymbol{Q}_{11 \\cdot 2}^{-1} \\boldsymbol{Q}_{12} \\boldsymbol{Q}_{22}^{-1} \\\\\n-\\boldsymbol{Q}_{22 \\cdot 1}^{-1} \\boldsymbol{Q}_{21} \\boldsymbol{Q}_{11}^{-1} & \\boldsymbol{Q}_{22 \\cdot 1}^{-1}\n\\end{array}\\right]\n\\]\n\\({ }^{10}\\) The covariance matrix between vectors \\(X\\) and \\(Z\\) is \\(\\operatorname{cov}(X, Z)=\\mathbb{E}\\left[(X-\\mathbb{E}[X])(Z-\\mathbb{E}[Z])^{\\prime}\\right]\\). The covariance matrix of the \\(\\operatorname{vector} X\\) is \\(\\operatorname{var}[X]=\\operatorname{cov}(X, X)=\\mathbb{E}\\left[(X-\\mathbb{E}[X])(X-\\mathbb{E}[X])^{\\prime}\\right]\\). where \\(\\boldsymbol{Q}_{11 \\cdot 2} \\stackrel{\\text { def }}{=} \\boldsymbol{Q}_{11}-\\boldsymbol{Q}_{12} \\boldsymbol{Q}_{22}^{-1} \\boldsymbol{Q}_{21}\\) and \\(\\boldsymbol{Q}_{22 \\cdot 1} \\stackrel{\\text { def }}{=} \\boldsymbol{Q}_{22}-\\boldsymbol{Q}_{21} \\boldsymbol{Q}_{11}^{-1} \\boldsymbol{Q}_{12}\\). Thus\n\\[\n\\begin{aligned}\n\\beta &=\\left(\\begin{array}{l}\n\\beta_{1} \\\\\n\\beta_{2}\n\\end{array}\\right) \\\\\n&=\\left[\\begin{array}{cc}\n\\boldsymbol{Q}_{11 \\cdot 2}^{-1} & -\\boldsymbol{Q}_{11 \\cdot 2}^{-1} \\boldsymbol{Q}_{12} \\boldsymbol{Q}_{22}^{-1} \\\\\n-\\boldsymbol{Q}_{22 \\cdot 1}^{-1} \\boldsymbol{Q}_{21} \\boldsymbol{Q}_{11}^{-1} & \\boldsymbol{Q}_{22 \\cdot 1}^{-1}\n\\end{array}\\right]\\left[\\begin{array}{l}\n\\boldsymbol{Q}_{1 Y} \\\\\n\\boldsymbol{Q}_{2 Y}\n\\end{array}\\right] \\\\\n&=\\left(\\begin{array}{c}\n\\boldsymbol{Q}_{11 \\cdot 2}^{-1}\\left(\\boldsymbol{Q}_{1 y}-\\boldsymbol{Q}_{12} \\boldsymbol{Q}_{22}^{-1} \\boldsymbol{Q}_{2 Y}\\right) \\\\\n\\boldsymbol{Q}_{22 \\cdot 1}^{-1}\\left(\\boldsymbol{Q}_{2 y}-\\boldsymbol{Q}_{21} \\boldsymbol{Q}_{11}^{-1} \\boldsymbol{Q}_{1 Y}\\right)\n\\end{array}\\right) \\\\\n&=\\left(\\begin{array}{c}\n\\boldsymbol{Q}_{11 \\cdot 2}^{-1} \\boldsymbol{Q}_{1 Y \\cdot 2} \\\\\n\\boldsymbol{Q}_{22 \\cdot 1}^{-1} \\boldsymbol{Q}_{2 Y \\cdot 1}\n\\end{array}\\right)\n\\end{aligned}\n\\]\nWe have shown that \\(\\beta_{1}=\\boldsymbol{Q}_{11 \\cdot 2}^{-1} \\boldsymbol{Q}_{1 Y \\cdot 2}\\) and \\(\\beta_{2}=\\boldsymbol{Q}_{22 \\cdot 1}^{-1} \\boldsymbol{Q}_{2 Y \\cdot 1}\\)."
  },
  {
    "objectID": "chpt02-ce.html#coefficient-decomposition",
    "href": "chpt02-ce.html#coefficient-decomposition",
    "title": "2  Conditional Expectation and Projection",
    "section": "2.27 Coefficient Decomposition",
    "text": "2.27 Coefficient Decomposition\nIn the previous section we derived formulae for the coefficient sub-vectors \\(\\beta_{1}\\) and \\(\\beta_{2}\\). We now use these formulae to give a useful interpretation of the coefficients in terms of an iterated projection.\nTake equation (2.42) for the case \\(\\operatorname{dim}\\left(X_{1}\\right)=1\\) so that \\(\\beta_{1} \\in \\mathbb{R}\\).\n\\[\nY=X_{1} \\beta_{1}+X_{2}^{\\prime} \\beta_{2}+e .\n\\]\nNow consider the projection of \\(X_{1}\\) on \\(X_{2}\\) :\n\\[\n\\begin{aligned}\nX_{1} &=X_{2}^{\\prime} \\gamma_{2}+u_{1} \\\\\n\\mathbb{E}\\left[X_{2} u_{1}\\right] &=0 .\n\\end{aligned}\n\\]\nFrom (2.22) and (2.36), \\(\\gamma_{2}=\\boldsymbol{Q}_{22}^{-1} \\boldsymbol{Q}_{21}\\) and \\(\\mathbb{E}\\left[u_{1}^{2}\\right]=\\boldsymbol{Q}_{11 \\cdot 2}=\\boldsymbol{Q}_{11}-\\boldsymbol{Q}_{12} \\boldsymbol{Q}_{22}^{-1} \\boldsymbol{Q}_{21}\\). We can also calculate that\n\\[\n\\mathbb{E}\\left[u_{1} Y\\right]=\\mathbb{E}\\left[\\left(X_{1}-\\gamma_{2}^{\\prime} X_{2}\\right) Y\\right]=\\mathbb{E}\\left[X_{1} Y\\right]-\\gamma_{2}^{\\prime} \\mathbb{E}\\left[X_{2} Y\\right]=\\boldsymbol{Q}_{1 Y}-\\boldsymbol{Q}_{12} \\boldsymbol{Q}_{22}^{-1} \\boldsymbol{Q}_{2 Y}=\\boldsymbol{Q}_{1 Y \\cdot 2} .\n\\]\nWe have found that\n\\[\n\\beta_{1}=\\boldsymbol{Q}_{11 \\cdot 2}^{-1} \\boldsymbol{Q}_{1 Y \\cdot 2}=\\frac{\\mathbb{E}\\left[u_{1} Y\\right]}{\\mathbb{E}\\left[u_{1}^{2}\\right]}\n\\]\nthe coefficient from the simple regression of \\(Y\\) on \\(u_{1}\\).\nWhat this means is that in the multivariate projection equation (2.44), the coefficient \\(\\beta_{1}\\) equals the projection coefficient from a regression of \\(Y\\) on \\(u_{1}\\), the error from a projection of \\(X_{1}\\) on the other regressors \\(X_{2}\\). The error \\(u_{1}\\) can be thought of as the component of \\(X_{1}\\) which is not linearly explained by the other regressors. Thus the coefficient \\(\\beta_{1}\\) equals the linear effect of \\(X_{1}\\) on \\(Y\\) after stripping out the effects of the other variables.\nThere was nothing special in the choice of the variable \\(X_{1}\\). This derivation applies symmetrically to all coefficients in a linear projection. Each coefficient equals the simple regression of \\(Y\\) on the error from a projection of that regressor on all the other regressors. Each coefficient equals the linear effect of that variable on \\(Y\\) after linearly controlling for all the other regressors."
  },
  {
    "objectID": "chpt02-ce.html#omitted-variable-bias",
    "href": "chpt02-ce.html#omitted-variable-bias",
    "title": "2  Conditional Expectation and Projection",
    "section": "2.28 Omitted Variable Bias",
    "text": "2.28 Omitted Variable Bias\nAgain, let the regressors be partitioned as in (2.41). Consider the projection of \\(Y\\) on \\(X_{1}\\) only. Perhaps this is done because the variables \\(X_{2}\\) are not observed. This is the equation\n\\[\n\\begin{aligned}\nY &=X_{1}^{\\prime} \\gamma_{1}+u \\\\\n\\mathbb{E}\\left[X_{1} u\\right] &=0 .\n\\end{aligned}\n\\]\nNotice that we have written the coefficient as \\(\\gamma_{1}\\) rather than \\(\\beta_{1}\\) and the error as \\(u\\) rather than \\(e\\). This is because (2.45) is different than (2.42). Goldberger (1991) introduced the catchy labels long regression for (2.42) and short regression for (2.45) to emphasize the distinction.\nTypically, \\(\\beta_{1} \\neq \\gamma_{1}\\), except in special cases. To see this, we calculate\n\\[\n\\begin{aligned}\n\\gamma_{1} &=\\left(\\mathbb{E}\\left[X_{1} X_{1}^{\\prime}\\right]\\right)^{-1} \\mathbb{E}\\left[X_{1} Y\\right] \\\\\n&=\\left(\\mathbb{E}\\left[X_{1} X_{1}^{\\prime}\\right]\\right)^{-1} \\mathbb{E}\\left[X_{1}\\left(X_{1}^{\\prime} \\beta_{1}+X_{2}^{\\prime} \\beta_{2}+e\\right)\\right] \\\\\n&=\\beta_{1}+\\left(\\mathbb{E}\\left[X_{1} X_{1}^{\\prime}\\right]\\right)^{-1} \\mathbb{E}\\left[X_{1} X_{2}^{\\prime}\\right] \\beta_{2} \\\\\n&=\\beta_{1}+\\Gamma_{12} \\beta_{2}\n\\end{aligned}\n\\]\nwhere \\(\\Gamma_{12}=\\boldsymbol{Q}_{11}^{-1} \\boldsymbol{Q}_{12}\\) is the coefficient matrix from a projection of \\(X_{2}\\) on \\(X_{1}\\) where we use the notation from Section \\(2.22\\).\nObserve that \\(\\gamma_{1}=\\beta_{1}+\\Gamma_{12} \\beta_{2} \\neq \\beta_{1}\\) unless \\(\\Gamma_{12}=0\\) or \\(\\beta_{2}=0\\). Thus the short and long regressions have different coefficients. They are the same only under one of two conditions. First, if the projection of \\(X_{2}\\) on \\(X_{1}\\) yields a set of zero coefficients (they are uncorrelated), or second, if the coefficient on \\(X_{2}\\) in (2.42) is zero. The difference \\(\\Gamma_{12} \\beta_{2}\\) between \\(\\gamma_{1}\\) and \\(\\beta_{1}\\) is known as omitted variable bias. It is the consequence of omission of a relevant correlated variable.\nTo avoid omitted variables bias the standard advice is to include all potentially relevant variables in estimated models. By construction, the general model will be free of such bias. Unfortunately in many cases it is not feasible to completely follow this advice as many desired variables are not observed. In this case, the possibility of omitted variables bias should be acknowledged and discussed in the course of an empirical investigation.\nFor example, suppose \\(Y\\) is log wages, \\(X_{1}\\) is education, and \\(X_{2}\\) is intellectual ability. It seems reasonable to suppose that education and intellectual ability are positively correlated (highly able individuals attain higher levels of education) which means \\(\\Gamma_{12}>0\\). It also seems reasonable to suppose that conditional on education, individuals with higher intelligence will earn higher wages on average, so that \\(\\beta_{2}>0\\). This implies that \\(\\Gamma_{12} \\beta_{2}>0\\) and \\(\\gamma_{1}=\\beta_{1}+\\Gamma_{12} \\beta_{2}>\\beta_{1}\\). Therefore, it seems reasonable to expect that in a regression of wages on education with intelligence omitted (as the latter is not measured), the coefficient on education is higher than in a regression where intelligence is included. In other words, in this context the omitted variable biases the regression coefficient upwards. It is possible, for example, that \\(\\beta_{1}=0\\) so that education has no direct effect on wages yet \\(\\gamma_{1}=\\Gamma_{12} \\beta_{2}>0\\) meaning that the regression coefficient on education alone is positive, but is a consequence of the unmodeled correlation between education and intellectual ability.\nUnfortunately, the above simple characterization of omitted variable bias does not immediately carry over to more complicated settings, as discovered by Luca, Magnus, and Peracchi (2018). For example, suppose we compare three nested projections\n\\[\n\\begin{aligned}\n&Y=X_{1}^{\\prime} \\gamma_{1}+u_{1} \\\\\n&Y=X_{1}^{\\prime} \\delta_{1}+X_{2}^{\\prime} \\delta_{2}+u_{2} \\\\\n&Y=X_{1}^{\\prime} \\beta_{1}+X_{2}^{\\prime} \\beta_{2}+X_{3}^{\\prime} \\beta_{3}+e .\n\\end{aligned}\n\\]\nWe can call them short, medium, and long regressions. Suppose that the parameter of interest is \\(\\beta_{1}\\) in the long regression. We are interested in the consequences of omitting \\(X_{3}\\) when estimating the medium regression, and of omitting both \\(X_{2}\\) and \\(X_{3}\\) when estimating the short regression. In particular we are interested in the question: Is it better to estimate the short or medium regression, given that both omit \\(X_{3}\\) ? Intuition suggests that the medium regression should be “less biased” but it is worth investigating in greater detail. By similar calculations to those above, we find that\n\\[\n\\begin{aligned}\n&\\gamma_{1}=\\beta_{1}+\\Gamma_{12} \\beta_{2}+\\Gamma_{13} \\beta_{3} \\\\\n&\\delta_{1}=\\beta_{1}+\\Gamma_{13 \\cdot 2} \\beta_{3}\n\\end{aligned}\n\\]\nwhere \\(\\Gamma_{13 \\cdot 2}=\\boldsymbol{Q}_{11 \\cdot 2}^{-1} \\boldsymbol{Q}_{13 \\cdot 2}\\) using the notation from Section \\(2.22\\).\nWe see that the bias in the short regression coefficient is \\(\\Gamma_{12} \\beta_{2}+\\Gamma_{13} \\beta_{3}\\) which depends on both \\(\\beta_{2}\\) and \\(\\beta_{3}\\), while that for the medium regression coefficient is \\(\\Gamma_{13 \\cdot 2} \\beta_{3}\\) which only depends on \\(\\beta_{3}\\). So the bias for the medium regression is less complicated and intuitively seems more likely to be smaller than that of the short regression. However it is impossible to strictly rank the two. It is quite possible that \\(\\gamma_{1}\\) is less biased than \\(\\delta_{1}\\). Thus as a general rule it is unknown if estimation of the medium regression will be less biased than estimation of the short regression."
  },
  {
    "objectID": "chpt02-ce.html#best-linear-approximation",
    "href": "chpt02-ce.html#best-linear-approximation",
    "title": "2  Conditional Expectation and Projection",
    "section": "2.29 Best Linear Approximation",
    "text": "2.29 Best Linear Approximation\nThere are alternative ways we could construct a linear approximation \\(X^{\\prime} \\beta\\) to the conditional expectation \\(m(X)\\). In this section we show that one alternative approach turns out to yield the same answer as the best linear predictor.\nWe start by defining the mean-square approximation error of \\(X^{\\prime} \\beta\\) to \\(m(X)\\) as the expected squared difference between \\(X^{\\prime} \\beta\\) and the conditional expectation \\(m(X)\\)\n\\[\nd(\\beta)=\\mathbb{E}\\left[\\left(m(X)-X^{\\prime} \\beta\\right)^{2}\\right] .\n\\]\nThe function \\(d(\\beta)\\) is a measure of the deviation of \\(X^{\\prime} \\beta\\) from \\(m(X)\\). If the two functions are identical then \\(d(\\beta)=0\\), otherwise \\(d(\\beta)>0\\). We can also view the mean-square difference \\(d(\\beta)\\) as a density-weighted average of the function \\(\\left(m(X)-X^{\\prime} \\beta\\right)^{2}\\) since\n\\[\nd(\\beta)=\\int_{\\mathbb{R}^{k}}\\left(m(x)-x^{\\prime} \\beta\\right)^{2} f_{X}(x) d x\n\\]\nwhere \\(f_{X}(x)\\) is the marginal density of \\(X\\).\nWe can then define the best linear approximation to the conditional \\(m(X)\\) as the function \\(X^{\\prime} \\beta\\) obtained by selecting \\(\\beta\\) to minimize \\(d(\\beta)\\) :\n\\[\n\\beta=\\underset{b \\in \\mathbb{R}^{k}}{\\operatorname{argmin}} d(b) .\n\\]\nSimilar to the best linear predictor we are measuring accuracy by expected squared error. The difference is that the best linear predictor (2.18) selects \\(\\beta\\) to minimize the expected squared prediction error, while the best linear approximation (2.46) selects \\(\\beta\\) to minimize the expected squared approximation error.\nDespite the different definitions, it turns out that the best linear predictor and the best linear approximation are identical. By the same steps as in (2.18) plus an application of conditional expectations we can find that\n\\[\n\\begin{aligned}\n\\beta &=\\left(\\mathbb{E}\\left[X X^{\\prime}\\right]\\right)^{-1} \\mathbb{E}[X m(X)] \\\\\n&=\\left(\\mathbb{E}\\left[X X^{\\prime}\\right]\\right)^{-1} \\mathbb{E}[X Y]\n\\end{aligned}\n\\]\n(see Exercise 2.19). Thus (2.46) equals (2.18). We conclude that the definition (2.46) can be viewed as an alternative motivation for the linear projection coefficient."
  },
  {
    "objectID": "chpt02-ce.html#regression-to-the-mean",
    "href": "chpt02-ce.html#regression-to-the-mean",
    "title": "2  Conditional Expectation and Projection",
    "section": "2.30 Regression to the Mean",
    "text": "2.30 Regression to the Mean\nThe term regression originated in an influential paper by Francis Galton (1886) where he examined the joint distribution of the stature (height) of parents and children. Effectively, he was estimating the conditional expectation of children’s height given their parent’s height. Galton discovered that this conditional expectation was approximately linear with a slope of \\(2 / 3\\). This implies that on average a child’s height is more mediocre (average) than his or her parent’s height. Galton called this phenomenon regression to the mean, and the label regression has stuck to this day to describe most conditional relationships.\nOne of Galton’s fundamental insights was to recognize that if the marginal distributions of \\(Y\\) and \\(X\\) are the same (e.g. the heights of children and parents in a stable environment) then the regression slope in a linear projection is always less than one.\nTo be more precise, take the simple linear projection\n\\[\nY=X \\beta+\\alpha+e\n\\]\nwhere \\(Y\\) equals the height of the child and \\(X\\) equals the height of the parent. Assume that \\(Y\\) and \\(X\\) have the same expectation so that \\(\\mu_{Y}=\\mu_{X}=\\mu\\). Then from (2.39) \\(\\alpha=(1-\\beta) \\mu\\) so we can write the linear projection (2.49) as\n\\[\n\\mathscr{P}(Y \\mid X)=(1-\\beta) \\mu+X \\beta .\n\\]\nThis shows that the projected height of the child is a weighted average of the population expectation \\(\\mu\\) and the parent’s height \\(X\\) with weights \\(\\beta\\) and \\(1-\\beta\\). When the height distribution is stable across generations so that \\(\\operatorname{var}[Y]=\\operatorname{var}[X]\\), then this slope is the simple correlation of \\(Y\\) and \\(X\\). Using (2.40)\n\\[\n\\beta=\\frac{\\operatorname{cov}(X, Y)}{\\operatorname{var}[X]}=\\operatorname{corr}(X, Y) .\n\\]\nBy the Cauchy-Schwarz inequality (B.32), \\(-1 \\leq \\operatorname{corr}(X, Y) \\leq 1\\), with \\(\\operatorname{corr}(X, Y)=1\\) only in the degenerate case \\(Y=X\\). Thus if we exclude degeneracy, \\(\\beta\\) is strictly less than 1 .\nThis means that on average, a child’s height is more mediocre (closer to the population average) than the parent’s.\nA common error - known as the regression fallacy - is to infer from \\(\\beta<1\\) that the population is converging, meaning that its variance is declining towards zero. This is a fallacy because we derived the implication \\(\\beta<1\\) under the assumption of constant means and variances. So certainly \\(\\beta<1\\) does not imply that the variance \\(Y\\) is less than than the variance of \\(X\\).\nAnother way of seeing this is to examine the conditions for convergence in the context of equation (2.49). Since \\(X\\) and \\(e\\) are uncorrelated, it follows that\n\\[\n\\operatorname{var}[Y]=\\beta^{2} \\operatorname{var}[X]+\\operatorname{var}[e] .\n\\]\nThen \\(\\operatorname{var}[Y]<\\operatorname{var}[X]\\) if and only if\n\\[\n\\beta^{2}<1-\\frac{\\operatorname{var}[e]}{\\operatorname{var}[X]}\n\\]\nwhich is not implied by the simple condition \\(|\\beta|<1\\).\nThe regression fallacy arises in related empirical situations. Suppose you sort families into groups by the heights of the parents, and then plot the average heights of each subsequent generation over time. If the population is stable, the regression property implies that the plots lines will converge-children’s height will be more average than their parents. The regression fallacy is to incorrectly conclude that the population is converging. A message to be learned from this example is that such plots are misleading for inferences about convergence. The regression fallacy is subtle. It is easy for intelligent economists to succumb to its temptation. A famous example is The Triumph of Mediocrity in Business by Horace Secrist published in 1933. In this book, Secrist carefully and with great detail documented that in a sample of department stores over 19201930, when he divided the stores into groups based on 1920-1921 profits, and plotted the average profits of these groups for the subsequent 10 years, he found clear and persuasive evidence for convergence “toward mediocrity”. Of course, there was no discovery - regression to the mean is a necessary feature of stable distributions."
  },
  {
    "objectID": "chpt02-ce.html#reverse-regression",
    "href": "chpt02-ce.html#reverse-regression",
    "title": "2  Conditional Expectation and Projection",
    "section": "2.31 Reverse Regression",
    "text": "2.31 Reverse Regression\nGalton noticed another interesting feature of the bivariate distribution. There is nothing special about a regression of \\(Y\\) on \\(X\\). We can also regress \\(X\\) on \\(Y\\). (In his heredity example this is the best linear predictor of the height of parents given the height of their children.) This regression takes the form\n\\[\nX=Y \\beta^{*}+\\alpha^{*}+e^{*} .\n\\]\nThis is sometimes called the reverse regression. In this equation, the coefficients \\(\\alpha^{*}, \\beta^{*}\\) and error \\(e^{*}\\) are defined by linear projection. In a stable population we find that\n\\[\n\\begin{gathered}\n\\beta^{*}=\\operatorname{corr}(X, Y)=\\beta \\\\\n\\alpha^{*}=(1-\\beta) \\mu=\\alpha\n\\end{gathered}\n\\]\nwhich are exactly the same as in the projection of \\(Y\\) on \\(X\\) ! The intercept and slope have exactly the same values in the forward and reverse projections! [This equality is not particularly imporant; it is an artifact of the assumption that \\(X\\) and \\(Y\\) have the same variances.]\nWhile this algebraic discovery is quite simple, it is counter-intuitive. Instead, a common yet mistaken guess for the form of the reverse regression is to take the equation (2.49), divide through by \\(\\beta\\) and rewrite to find the equation\n\\[\nX=Y \\frac{1}{\\beta}-\\frac{\\alpha}{\\beta}-\\frac{1}{\\beta} e\n\\]\nsuggesting that the projection of \\(X\\) on \\(Y\\) should have a slope coefficient of \\(1 / \\beta\\) instead of \\(\\beta\\), and intercept of \\(-\\alpha / \\beta\\) rather than \\(\\alpha\\). What went wrong? Equation (2.51) is perfectly valid because it is a simple manipulation of the valid equation (2.49). The trouble is that (2.51) is neither a CEF nor a linear projection. Inverting a projection (or CEF) does not yield a projection (or CEF). Instead, (2.50) is a valid projection, not (2.51).\nIn any event, Galton’s finding was that when the variables are standardized, the slope in both projections ( \\(Y\\) on \\(X\\), and \\(X\\) on \\(Y\\) ) equals the correlation and both equations exhibit regression to the mean. It is not a causal relation, but a natural feature of joint distributions."
  },
  {
    "objectID": "chpt02-ce.html#limitations-of-the-best-linear-projection",
    "href": "chpt02-ce.html#limitations-of-the-best-linear-projection",
    "title": "2  Conditional Expectation and Projection",
    "section": "2.32 Limitations of the Best Linear Projection",
    "text": "2.32 Limitations of the Best Linear Projection\nLet’s compare the linear projection and linear CEF models.\nFrom Theorem 2.4.4 we know that the CEF error has the property \\(\\mathbb{E}[X e]=0\\). Thus a linear CEF is the best linear projection. However, the converse is not true as the projection error does not necessarily satisfy \\(\\mathbb{E}[e \\mid X]=0\\). Furthermore, the linear projection may be a poor approximation to the CEF.\nTo see these points in a simple example, suppose that the true process is \\(Y=X+X^{2}\\) with \\(X \\sim \\mathrm{N}(0,1)\\). In this case the true CEF is \\(m(x)=x+x^{2}\\) and there is no error. Now consider the linear projection of \\(Y\\) on \\(X\\) and a constant, namely the model \\(Y=\\beta X+\\alpha+e\\). Since \\(X \\sim \\mathrm{N}(0,1)\\) then \\(X\\) and \\(X^{2}\\) are uncorrelated and the linear projection takes the form \\(\\mathscr{P}[Y \\mid X]=X+1\\). This is quite different from the true CEF \\(m(X)=\\) \\(X+X^{2}\\). The projection error equals \\(e=X^{2}-1\\) which is a deterministic function of \\(X\\) yet is uncorrelated with \\(X\\). We see in this example that a projection error need not be a CEF error and a linear projection can be a poor approximation to the CEF.\n\nFigure 2.7: Conditional Expectation and Two Linear Projections\nAnother defect of linear projection is that it is sensitive to the marginal distribution of the regressors when the conditional mean is nonlinear. We illustrate the issue in Figure \\(2.7\\) for a constructed \\({ }^{11}\\) joint distribution of \\(Y\\) and \\(X\\). The thick line is the nonlinear CEF of \\(Y\\) given \\(X\\). The data are divided in two groups - Group 1 and Group 2 - which have different marginal distributions for the regressor \\(X\\), and Group 1 has a lower mean value of \\(X\\) than Group 2. The separate linear projections of \\(Y\\) on \\(X\\) for these two groups are displayed in the figure by the thin lines. These two projections are distinct approximations to the CEF. A defect with linear projection is that it leads to the incorrect conclusion that the effect of \\(X\\) on \\(Y\\) is different for individuals in the two groups. This conclusion is incorrect because in fact there is no difference in the conditional expectation function. The apparent difference is a by-product of linear approximations to a nonlinear expectation combined with different marginal distributions for the conditioning variables.\n\\({ }^{11}\\) The \\(X\\) in Group 1 are \\(\\mathrm{N}(2,1)\\), those in Group 2 are \\(\\mathrm{N}(4,1)\\), and the conditional distribution of \\(Y\\) given \\(X\\) is \\(\\mathrm{N}(m(X), 1)\\) where \\(m(x)=2 x-x^{2} / 6\\). The functions are plotted over \\(0 \\leq x \\leq 6\\)."
  },
  {
    "objectID": "chpt02-ce.html#random-coefficient-model",
    "href": "chpt02-ce.html#random-coefficient-model",
    "title": "2  Conditional Expectation and Projection",
    "section": "2.33 Random Coefficient Model",
    "text": "2.33 Random Coefficient Model\nA model which is notationally similar to but conceptually distinct from the linear CEF model is the linear random coefficient model. It takes the form \\(Y=X^{\\prime} \\eta\\) where the individual-specific coefficient \\(\\eta\\) is random and independent of \\(X\\). For example, if \\(X\\) is years of schooling and \\(Y\\) is log wages, then \\(\\eta\\) is the individual-specific returns to schooling. If a person obtains an extra year of schooling, \\(\\eta\\) is the actual change in their wage. The random coefficient model allows the returns to schooling to vary in the population. Some individuals might have a high return to education (a high \\(\\eta\\) ) and others a low return, possibly 0 , or even negative.\nIn the linear CEF model the regressor coefficient equals the regression derivative - the change in the conditional expectation due to a change in the regressors, \\(\\beta=\\nabla m(X)\\). This is not the effect on a given individual, it is the effect on the population average. In contrast, in the random coefficient model the random vector \\(\\eta=\\nabla\\left(X^{\\prime} \\eta\\right)\\) is the true causal effect - the change in the response variable \\(Y\\) itself due to a change in the regressors.\nIt is interesting, however, to discover that the linear random coefficient model implies a linear CEF. To see this, let \\(\\beta=\\mathbb{E}[\\eta]\\) and \\(\\Sigma=\\operatorname{var}[\\eta]\\) denote the mean and covariance matrix of \\(\\eta\\) and then decompose the random coefficient as \\(\\eta=\\beta+u\\) where \\(u\\) is distributed independently of \\(X\\) with mean zero and covariance matrix \\(\\Sigma\\). Then we can write\n\\[\n\\mathbb{E}[Y \\mid X]=X^{\\prime} \\mathbb{E}[\\eta \\mid X]=X^{\\prime} \\mathbb{E}[\\eta]=X^{\\prime} \\beta\n\\]\nso the CEF is linear in \\(X\\), and the coefficient \\(\\beta\\) equals the expectation of the random coefficient \\(\\eta\\).\nWe can thus write the equation as a linear CEF \\(Y=X^{\\prime} \\beta+e\\) where \\(e=X^{\\prime} u\\) and \\(u=\\eta-\\beta\\). The error is conditionally mean zero: \\(\\mathbb{E}[e \\mid X]=0\\). Furthermore\n\\[\n\\operatorname{var}[e \\mid X]=X^{\\prime} \\operatorname{var}[\\eta] X=X^{\\prime} \\Sigma X\n\\]\nso the error is conditionally heteroskedastic with its variance a quadratic function of \\(X\\).\nTheorem 2.11 In the linear random coefficient model \\(Y=X^{\\prime} \\eta\\) with \\(\\eta\\) independent of \\(X, \\mathbb{E}\\|X\\|^{2}<\\infty\\), and \\(\\mathbb{E}\\|\\eta\\|^{2}<\\infty\\), then\n\\[\n\\begin{aligned}\n\\mathbb{E}[Y \\mid X] &=X^{\\prime} \\beta \\\\\n\\operatorname{var}[Y \\mid X] &=X^{\\prime} \\Sigma X\n\\end{aligned}\n\\]\nwhere \\(\\beta=\\mathbb{E}[\\eta]\\) and \\(\\Sigma=\\operatorname{var}[\\eta]\\)"
  },
  {
    "objectID": "chpt02-ce.html#causal-effects",
    "href": "chpt02-ce.html#causal-effects",
    "title": "2  Conditional Expectation and Projection",
    "section": "2.34 Causal Effects",
    "text": "2.34 Causal Effects\nSo far we have avoided the concept of causality, yet often the underlying goal of an econometric analysis is to measure a causal relationship between variables. It is often of great interest to understand the causes and effects of decisions, actions, and policies. For example, we may be interested in the effect of class sizes on test scores, police expenditures on crime rates, climate change on economic activity, years of schooling on wages, institutional structure on growth, the effectiveness of rewards on behavior, the consequences of medical procedures for health outcomes, or any variety of possible causal relationships. In each case the goal is to understand what is the actual effect on the outcome due to a change in an input. We are not just interested in the conditional expectation or linear projection, we would like to know the actual change.\nTwo inherent barriers are: (1) the causal effect is typically specific to an individual; and (2) the causal effect is typically unobserved.\nConsider the effect of schooling on wages. The causal effect is the actual difference a person would receive in wages if we could change their level of education holding all else constant. This is specific to each individual as their employment outcomes in these two distinct situations are individual. The causal effect is unobserved because the most we can observe is their actual level of education and their actual wage, but not the counterfactual wage if their education had been different.\nTo be concrete suppose that there are two individuals, Jennifer and George, and both have the possibility of being high-school graduates or college graduates, and both would have received different wages given their choices. For example, suppose that Jennifer would have earned \\(\\$ 10\\) an hour as a high-school graduate and \\(\\$ 20\\) an hour as a college graduate while George would have earned \\(\\$ 8\\) as a high-school graduate and \\(\\$ 12\\) as a college graduate. In this example the causal effect of schooling is \\(\\$ 10\\) a hour for Jennifer and \\(\\$ 4\\) an hour for George. The causal effects are specific to the individual and neither causal effect is observed.\nRubin (1974) developed the potential outcomes framework (also known as the Rubin causal model) to clarify the issues. Let \\(Y\\) be a scalar outcome (for example, wages) and \\(D\\) be a binary treatment (for example, college attendence). The specification of treatment as binary is not essential but simplifies the notation. A flexible model describing the impact of the treatment on the outcome is\n\\[\nY=h(D, U)\n\\]\nwhere \\(U\\) is an \\(\\ell \\times 1\\) unobserved random factor and \\(h\\) is a functional relationship. It is also common to use the simplified notation \\(Y(0)=h(0, U)\\) and \\(Y(1)=h(1, U)\\) for the potential outcomes associated with non-treatment and treatment, respectively. The notation implicitly holds \\(U\\) fixed. The potential outcomes are specific to each individual as they depend on \\(U\\). For example, if \\(Y\\) is an individual’s wage, the unobservables \\(U\\) could include characteristics such as the individual’s abilities, skills, work ethic, interpersonal connections, and preferences, all of which potentially influence their wage. In our example these factors are summarized by the labels “Jennifer” and “George”.\nRubin described the effect as causal when we vary \\(D\\) while holding \\(U\\) constant. In our example this means changing an individual’s education while holding constant their other attributes.\nDefinition 2.6 In the model (2.52) the causal effect of \\(D\\) on \\(Y\\) is\n\\[\nC(U)=Y(1)-Y(0)=h(1, U)-h(0, U),\n\\]\nthe change in \\(Y\\) due to treatment while holding \\(U\\) constant.\nIt may be helpful to understand that (2.53) is a definition and does not necessarily describe causality in a fundamental or experimental sense. Perhaps it would be more appropriate to label (2.53) as a structural effect (the effect within the structural model).\nThe causal effect of treatment \\(C(U)\\) defined in (2.53) is heterogeneous and random as the potential outcomes \\(Y(0)\\) and \\(Y(1)\\) vary across individuals. Also, we do not observe both \\(Y(0)\\) and \\(Y(1)\\) for a given individual, but rather only the realized value\n\\[\nY=\\left\\{\\begin{array}{lll}\nY(0) & \\text { if } & D=0 \\\\\nY(1) & \\text { if } & D=1 .\n\\end{array}\\right.\n\\]\nTable 2.3: Example Distribution\n|College Graduate|0|0|6|10|\\(\\$ 17.00\\)| |:—————|:|:|:|-:|———:| |Difference | | | | | \\(\\$ 8.25\\)|\nConsequently the causal effect \\(C(U)\\) is unobserved.\nRubin’s goal was to learn features of the distribution of \\(C(U)\\) including its expected value which he called the average causal effect. He defined it as follows.\nDefinition 2.7 In the model (2.52) the average causal effect of \\(D\\) on \\(Y\\) is\n\\[\n\\mathrm{ACE}=\\mathbb{E}[C(U)]=\\int_{\\mathbb{R}^{\\ell}} C(u) f(u) d u\n\\]\nwhere \\(f(u)\\) is the density of \\(U\\).\nThe ACE is the population average of the causal effect. Extending our Jennifer & George example, suppose that half of the population are like Jennifer and the other half are like George. Then the average causal effect of college on wages is \\((10+4) / 2=\\$ 7\\) an hour.\nTo estimate the ACE a reasonable starting place is to compare average \\(Y\\) for treated and untreated individuals. In our example this is the difference between the average wage among college graduates and high school graduates. This is the same as the coefficient in a regression of the outcome \\(Y\\) on the treatment \\(D\\). Does this equal the ACE?\nThe answer depends on the relationship between treatment \\(D\\) and the unobserved component \\(U\\). If \\(D\\) is randomly assigned as in an experiment then \\(D\\) and \\(U\\) are independent and the regression coefficient equals the ACE. However, if \\(D\\) and \\(U\\) are dependent then the regression coefficient and ACE are different. To see this, observe that the difference between the average outcomes of the treated and untreated populations are\n\\[\n\\mathbb{E}[Y \\mid D=1]-\\mathbb{E}[Y \\mid D=0]=\\int_{\\mathbb{R}^{\\ell}} h(1, u) f(u \\mid D=1) d u-\\int_{\\mathbb{R}^{\\ell}} h(1, u) f(u \\mid D=0) d u\n\\]\nwhere \\(f(u \\mid D)\\) is the conditional density of \\(U\\) given \\(D\\). If \\(U\\) is independent of \\(D\\) then \\(f(u \\mid D)=f(u)\\) and the above expression equals \\(\\int_{\\mathbb{R}^{\\ell}}(h(1, u)-h(0, u)) f(u) d u=\\) ACE. However, if \\(U\\) and \\(D\\) are dependent this equality fails.\nTo illustrate, let’s return to our example of Jennifer and George. Suppose that all high school students take an aptitude test. If a student gets a high \\((\\mathrm{H})\\) score they go to college with probability \\(3 / 4\\), and if a student gets a low (L) score they go to college with probability \\(1 / 4\\). Suppose further that Jennifer gets an aptitude score of \\(\\mathrm{H}\\) with probability 3/4, while George gets a score of \\(\\mathrm{H}\\) with probability \\(1 / 4\\). Given this situation, \\(62.5 %\\) of Jennifer’s will go to college \\({ }^{12}\\) while \\(37.5 %\\) of George’s will go to college \\({ }^{13}\\).\nAn econometrician who randomly samples 32 individuals and collects data on educational attainment and wages will find the wage distribution displayed in Table 2.3.\n\\(12 \\mathbb{P}[\\) college \\(\\mid\\) Jennifer \\(]=\\mathbb{P}[\\) college \\(\\mid H] \\mathbb{P}[H \\mid\\) Jennifer \\(]+\\mathbb{P}[\\) college \\(\\mid L] \\mathbb{P}[L \\mid\\) Jennifer \\(]=(3 / 4)^{2}+(1 / 4)^{2} .\\)\n\\(13 \\mathbb{P}[\\) college \\(\\mid\\) George \\(]=\\mathbb{P}[\\) college \\(\\mid H] \\mathbb{P}[H \\mid\\) George \\(]+\\mathbb{P}[\\) college \\(\\mid L] \\mathbb{P}[L \\mid\\) George \\(]=(3 / 4)(1 / 4)+(1 / 4)(3 / 4)\\). Our econometrician finds that the average wage among high school graduates is \\(\\$ 8.75\\) while the average wage among college graduates is \\(\\$ 17.00\\). The difference of \\(\\$ 8.25\\) is the econometrician’s regression coefficient for the effect of college on wages. But \\(\\$ 8.25\\) overstates the true ACE of \\(\\$ 7\\). The reason is that college attendence is determined by an aptitude test which is correlated with an individual’s causal effect. Jennifer has both a high causal effect and is more likely to attend college, so the observed difference in wages overstates the causal effect of college.\nTo visualize Table \\(2.3\\) examine Figure 2.8. The four points are the four education/wage pairs from the table, with the size of the points calibrated to the wage distribution. The two lines are the econometrician’s regression line and the average causal effect. The Jennifer’s in the population correspond to the points above the two lines, the George’s in the population correspond to the points below the two lines. Because most Jennifer’s go to College, and most George’s do not, the regression line is tilted away from the average causal effect towards the two large points.\n\nFigure 2.8: Average Causal Effect vs Regression\nOur first lesson from this analysis is that we need to be cautious about interpreting regression coefficients as causal effects. Unless the regressors (e.g. education attainment) can be interpreted as randomly assigned it is inappropriate to interpret the regression coefficients causally.\nOur second lesson will be that a causal interpretation can be obtained if we condition on a sufficiently rich set of covariates. We now explore this issue.\nSuppose that the observables include a set of covariates \\(X\\) in addition to the outcome \\(Y\\) and treatment \\(D\\). We extend the potential outcomes model (2.52) to include \\(X\\) :\n\\[\nY=h(D, X, U) .\n\\]\nWe also extend the definition of a causal effect to allow conditioning on \\(X\\).\nDefinition \\(2.8\\) In the model (2.54) the causal effect of \\(D\\) on \\(Y\\) is\n\\[\nC(X, U)=h(1, X, U)-h(0, X, U),\n\\]\nthe change in \\(Y\\) due to treatment holding \\(X\\) and \\(U\\) constant.\nThe conditional average causal effect of \\(D\\) on \\(Y\\) conditional on \\(X=x\\) is\n\\[\n\\operatorname{ACE}(x)=\\mathbb{E}[C(X, U) \\mid X=x]=\\int_{\\mathbb{R}^{\\ell}} C(x, u) f(u \\mid x) d u\n\\]\nwhere \\(f(u \\mid x)\\) is the conditional density of \\(U\\) given \\(X\\).\nThe unconditional average causal effect of \\(D\\) on \\(Y\\) is\n\\[\n\\mathrm{ACE}=\\mathbb{E}[C(X, U)]=\\int \\operatorname{ACE}(x) f(x) d x\n\\]\nwhere \\(f(x)\\) is the density of \\(X\\).\nThe conditional average causal effect \\(\\operatorname{ACE}(x)\\) is the ACE for the sub-population with characteristics \\(X=x\\). Given observations on \\((Y, D, X)\\) we want to measure the causal effect of \\(D\\) on \\(Y\\), and are interested if this can be obtained by a regression of \\(Y\\) on \\((D, X)\\). We would like to interpret the coefficient on \\(D\\) as a causal effect. Is this appropriate?\nOur previous analysis showed that a causal interpretation obtains when \\(U\\) is independent of the regressors. While this is sufficient it is stronger than necessary. Instead, the following is sufficient.\nDefinition 2.9 Conditional Independence Assumption (CIA). Conditional on \\(X\\), the random variables \\(D\\) and \\(U\\) are statistically independent.\nThe CIA implies that the conditional density of \\(U\\) given \\((D, X)\\) only depends on \\(X\\), thus \\(f(u \\mid D, X)=\\) \\(f(u \\mid X)\\). This implies that the regression of \\(Y\\) on \\((D, X)\\) equals\n\\[\n\\begin{aligned}\nm(d, x) &=\\mathbb{E}[Y \\mid D=d, X=x] \\\\\n&=\\mathbb{E}[h(d, x, U) \\mid D=d, X=x] \\\\\n&=\\int h(d, x, u) f(u \\mid x) d u .\n\\end{aligned}\n\\]\nUnder the CIA the treatment effect measured by the regression is\n\\[\n\\begin{aligned}\n\\nabla m(d, x) &=m(1, x)-m(0, x) \\\\\n&=\\int h(1, x, u) f(u \\mid x) d u-\\int h(0, x, u) f(u \\mid x) d u \\\\\n&=\\int C(x, u) f(u \\mid x) d u \\\\\n&=\\operatorname{ACE}(x) .\n\\end{aligned}\n\\]\nThis is the conditional ACE. Thus under the CIA the regression coefficient equals the ACE.\nWe deduce that the regression of \\(Y\\) on \\((D, X)\\) reveals the causal impact of treatment when the CIA holds. This means that regression analysis can be interpreted causally when we can make the case that the regressors \\(X\\) are sufficient to control for factors which are correlated with treatment.\nTheorem 2.12 In the structural model (2.54), the Conditional Independence Assumption implies \\(\\nabla m(d, x)=\\operatorname{ACE}(x)\\), that the regression derivative with respect to treatment equals the conditional ACE.\nThis is a fascinating result. It shows that whenever the unobservable is independent of the treatment variable after conditioning on appropriate regressors, the regression derivative equals the conditional causal effect. This means the CEF has causal economic meaning, giving strong justification to estimation of the CEF.\nIt is important to understand the critical role of the CIA. If CIA fails then the equality (2.55) of the regression derivative and the ACE fails. The CIA states that conditional on \\(X\\) the variables \\(U\\) and \\(D\\) are independent. This means that treatment \\(D\\) is not affected by the unobserved individual factors \\(U\\) and is effectively random. It is a strong assumption. In the wage/education example it means that education is not selected by individuals based on their unobserved characteristics.\nHowever, it is also helpful to understand that the CIA is weaker than full independence of \\(U\\) from the regressors \\((D, X)\\). What is required is only that \\(U\\) and \\(D\\) are independent after conditioning on \\(X\\). If \\(X\\) is sufficiently rich this may not be restrictive.\nReturning to our example, we require a variable \\(X\\) which breaks the dependence between \\(D\\) and \\(U\\). In our example, this variable is the aptitude test score, because the decision to attend college was based on the test score. It follows that educational attainment and type are independent once we condition on the test score.\nTo see this, observe that if a student’s test score is \\(\\mathrm{H}\\) the probability they go to college \\((D=1)\\) is \\(3 / 4\\) for both Jennifers and Georges. Similarly, if their test score is \\(\\mathrm{L}\\) the probability they go to college is \\(1 / 4\\) for both types. This means that college attendence is independent of type, conditional on the aptitude test score.\nThe conditional ACE depends on the test score. Among students who receive a high test score, \\(3 / 4\\) are Jennifers and \\(1 / 4\\) are Georges. Thus the conditional ACE for students with a score of \\(\\mathrm{H}\\) is \\((3 / 4) \\times 10+\\) \\((1 / 4) \\times 4=\\$ 8.50\\). Among students who receive a low test score, \\(1 / 4\\) are Jennifers and \\(3 / 4\\) are Georges. Thus the ACE for students with a score of \\(\\mathrm{L}\\) is \\((1 / 4) \\times 10+(3 / 4) \\times 4=\\$ 5.50\\). The unconditional ACE is the average, \\(\\mathrm{ACE}=(8.50+5.50) / 2=\\$ 7\\), because \\(50 %\\) of students each receive scores of \\(\\mathrm{H}\\) and \\(\\mathrm{L}\\).\nTheorem \\(2.12\\) shows that the conditional ACE is revealed by a regression which includes test scores. To see this in the wage distribution, suppose that the econometrician collects data on the aptitude test score as well as education and wages. Given a random sample of 32 individuals we would expect to find the wage distribution in Table \\(2.4\\).\nDefine a dummy highscore to indicate students who received a high test score. The regression of wages on college attendance and test scores with their interaction is\n\\[\n\\mathbb{E}[\\text { wage } \\mid \\text { college, highscore }]=1.00 \\text { highscore }+5.50 \\text { college }+3.00 \\text { highscore } \\times \\text { college }+8.50 \\text {. }\n\\]\nThe coefficient on college, \\(\\$ 5.50\\), is the regression derivative of college attendance for those with low test scores, and the sum of this coefficient with the interaction coefficient \\(\\$ 3.00\\) equals \\(\\$ 8.50\\) which is the Table 2.4: Example Distribution 2\n\n\n\n\n\n\n\n\n\n\n\n\n\\(\\$ 8\\)\n\\(\\$ 10\\)\n\\(\\$ 12\\)\n\\(\\$ 20\\)\nMean\n\n\n\n\nHigh-School Graduate + High Test Score\n1\n3\n0\n0\n\\(\\$ 9.50\\)\n\n\nCollege Graduate + High Test Score\n0\n0\n3\n9\n\\(\\$ 18.00\\)\n\n\nHigh-School Graduate + Low Test Score\n9\n3\n0\n0\n\\(\\$ 8.50\\)\n\n\nCollege Graduate + Low Test Score\n0\n0\n3\n1\n\\(\\$ 14.00\\)\n\n\n\nregression derivative for college attendance for those with high test scores. \\(\\$ 5.50\\) and \\(\\$ 8.50\\) equal the conditional causal effects as calculated above.\nThis shows that from the regression (2.56) an econometrician will find that the effect of college on wages is \\(\\$ 8.50\\) for those with high test scores and \\(\\$ 5.50\\) for those with low test scores with an average effect of \\(\\$ 7\\) (because \\(50 %\\) of students receive high and low test scores). This is the true average causal effect of college on wages. Thus the regression coefficient on college in (2.56) can be interpreted causally, while a regression omitting the aptitude test score does not reveal the causal effect of education.\nTo summarize our findings, we have shown how it is possible that a simple regression will give a false measurement of a causal effect, but a more careful regression can reveal the true causal effect. The key is to condition on a suitably rich set of covariates such that the remaining unobserved factors affecting the outcome are independent of the treatment variable."
  },
  {
    "objectID": "chpt02-ce.html#existence-and-uniqueness-of-the-conditional-expectation",
    "href": "chpt02-ce.html#existence-and-uniqueness-of-the-conditional-expectation",
    "title": "2  Conditional Expectation and Projection",
    "section": "2.35 Existence and Uniqueness of the Conditional Expectation*",
    "text": "2.35 Existence and Uniqueness of the Conditional Expectation*\nIn Sections \\(2.3\\) and \\(2.6\\) we defined the conditional expectation when the conditioning variables \\(X\\) are discrete and when the variables \\((Y, X)\\) have a joint density. We have explored these cases because these are the situations where the conditional mean is easiest to describe and understand. However, the conditional mean exists quite generally without appealing to the properties of either discrete or continuous random variables.\nTo justify this claim we now present a deep result from probability theory. What it says is that the conditional mean exists for all joint distributions \\((Y, X)\\) for which \\(Y\\) has a finite mean.\nTheorem 2.13 Existence of the Conditional Expectation If \\(\\mathbb{E}|Y|<\\infty\\) then there exists a function \\(m(x)\\) such that for all sets \\(\\mathscr{X}\\) for which \\(\\mathbb{P}[X \\in \\mathscr{X}]\\) is defined,\n\\[\n\\mathbb{E}[\\mathbb{1}\\{X \\in \\mathscr{X}\\} Y]=\\mathbb{E}[\\mathbb{1}\\{X \\in \\mathscr{X}\\} m(X)]\n\\]\nThe function \\(m(X)\\) is almost everywhere unique, in the sense that if \\(h(x)\\) satisfies (2.57), then there is a set \\(S\\) such that \\(\\mathbb{P}[S]=1\\) and \\(m(x)=h(x)\\) for \\(x \\in S\\). The function \\(m(x)\\) is called the conditional expectation and is written \\(m(x)=\\mathbb{E}[Y \\mid X=x]\\)\nSee, for example, Ash (1972), Theorem 6.3.3.\nThe conditional expectation \\(m(x)\\) defined by (2.57) specializes to (2.6) when \\((Y, X)\\) have a joint density. The usefulness of definition (2.57) is that Theorem \\(2.13\\) shows that the conditional expectation \\(m(X)\\) exists for all finite-mean distributions. This definition allows \\(Y\\) to be discrete or continuous, for \\(X\\) to be scalar or vector-valued, and for the components of \\(X\\) to be discrete or continuously distributed.\nYou may have noticed that Theorem \\(2.13\\) applies only to sets \\(\\mathscr{X}\\) for which \\(\\mathbb{P}[X \\in \\mathscr{X}]\\) is defined. This is a technical issue - measurability - which we largely side-step in this textbook. Formal probability theory only applies to sets which are measurable - for which probabilities are defined - as it turns out that not all sets satisfy measurability. This is not a practical concern for applications, so we defer such distinctions for formal theoretical treatments."
  },
  {
    "objectID": "chpt02-ce.html#identification",
    "href": "chpt02-ce.html#identification",
    "title": "2  Conditional Expectation and Projection",
    "section": "2.36 Identification*",
    "text": "2.36 Identification*\nA critical and important issue in structural econometric modeling is identification, meaning that a parameter is uniquely determined by the distribution of the observed variables. It is relatively straightforward in the context of the unconditional and conditional expectation, but it is worthwhile to introduce and explore the concept at this point for clarity.\nLet \\(F\\) denote the distribution of the observed data, for example the distribution of the pair \\((Y, X)\\). Let \\(\\mathscr{F}\\) be a collection of distributions \\(F\\). Let \\(\\theta\\) be a parameter of interest (for example, the expectation \\(\\mathbb{E}[Y])\\).\nDefinition 2.10 A parameter \\(\\theta \\in \\mathbb{R}\\) is identified on \\(\\mathscr{F}\\) if for all \\(F \\in \\mathscr{F}\\), there is a uniquely determined value of \\(\\theta\\).\nEquivalently, \\(\\theta\\) is identified if we can write it as a mapping \\(\\theta=g(F)\\) on the set \\(\\mathscr{F}\\). The restriction to the set \\(\\mathscr{F}\\) is important. Most parameters are identified only on a strict subset of the space of all distributions.\nTake, for example, the expectation \\(\\mu=\\mathbb{E}[Y]\\). It is uniquely determined if \\(\\mathbb{E}|Y|<\\infty\\), so \\(\\mu\\) is identified for the set \\(\\mathscr{F}=\\{F: \\mathbb{E}|Y|<\\infty\\}\\).\nNext, consider the conditional expectation. Theorem \\(2.13\\) demonstrates that \\(\\mathbb{E}|Y|<\\infty\\) is a sufficient condition for identification.\nTheorem 2.14 Identification of the Conditional Expectation If \\(\\mathbb{E}|Y|<\\infty\\), the conditional expectation \\(m(x)=\\mathbb{E}[Y \\mid X=x]\\) is identified almost everywhere.\nIt might seem as if identification is a general property for parameters so long as we exclude degenerate cases. This is true for moments of observed data, but not necessarily for more complicated models. As a case in point, consider the context of censoring. Let \\(Y\\) be a random variable with distribution \\(F\\). Instead of observing \\(Y\\), we observe \\(Y^{*}\\) defined by the censoring rule\n\\[\nY^{*}=\\left\\{\\begin{array}{cc}\nY & \\text { if } Y \\leq \\tau \\\\\n\\tau & \\text { if } Y>\\tau\n\\end{array}\\right.\n\\]\nThat is, \\(Y^{*}\\) is capped at the value \\(\\tau\\). A common example is income surveys, where income responses are “top-coded” meaning that incomes above the top code \\(\\tau\\) are recorded as the top code. The observed variable \\(Y^{*}\\) has distribution\n\\[\nF^{*}(u)=\\left\\{\\begin{array}{cc}\nF(u) & \\text { for } u \\leq \\tau \\\\\n1 & \\text { for } u \\geq \\tau .\n\\end{array}\\right.\n\\]\nWe are interested in features of the distribution \\(F\\) not the censored distribution \\(F^{*}\\). For example, we are interested in the expected wage \\(\\mu=\\mathbb{E}[Y]\\). The difficulty is that we cannot calculate \\(\\mu\\) from \\(F^{*}\\) except in the trivial case where there is no censoring \\(\\mathbb{P}[Y \\geq \\tau]=0\\). Thus the expectation \\(\\mu\\) is not generically identified from the censored distribution.\nA typical solution to the identification problem is to assume a parametric distribution. For example, let \\(\\mathscr{F}\\) be the set of normal distributions \\(Y \\sim \\mathrm{N}\\left(\\mu, \\sigma^{2}\\right)\\). It is possible to show that the parameters \\(\\left(\\mu, \\sigma^{2}\\right)\\) are identified for all \\(F \\in \\mathscr{F}\\). That is, if we know that the uncensored distribution is normal we can uniquely determine the parameters from the censored distribution. This is often called parametric identification as identification is restricted to a parametric class of distributions. In modern econometrics this is generally viewed as a second-best solution as identification has been achieved only through the use of an arbitrary and unverifiable parametric assumption.\nA pessimistic conclusion might be that it is impossible to identify parameters of interest from censored data without parametric assumptions. Interestingly, this pessimism is unwarranted. It turns out that we can identify the quantiles \\(q_{\\alpha}\\) of \\(F\\) for \\(\\alpha \\leq \\mathbb{P}[Y \\leq \\tau]\\). For example, if \\(20 %\\) of the distribution is censored we can identify all quantiles for \\(\\alpha \\in(0,0.8)\\). This is often called nonparametric identification as the parameters are identified without restriction to a parametric class.\nWhat we have learned from this little exercise is that in the context of censored data moments can only be parametrically identified while non-censored quantiles are nonparametrically identified. Part of the message is that a study of identification can help focus attention on what can be learned from the data distributions available."
  },
  {
    "objectID": "chpt02-ce.html#technical-proofs",
    "href": "chpt02-ce.html#technical-proofs",
    "title": "2  Conditional Expectation and Projection",
    "section": "2.37 Technical Proofs*",
    "text": "2.37 Technical Proofs*\nProof of Theorem 2.1 For convenience, assume that the variables have a joint density \\(f(y, x)\\). Since \\(\\mathbb{E}[Y \\mid X]\\) is a function of the random vector \\(X\\) only, to calculate its expectation we integrate with respect to the density \\(f_{X}(x)\\) of \\(X\\), that is\n\\[\n\\mathbb{E}[\\mathbb{E}[Y \\mid X]]=\\int_{\\mathbb{R}^{k}} \\mathbb{E}[Y \\mid X] f_{X}(x) d x .\n\\]\nSubstituting in (2.6) and noting that \\(f_{Y \\mid X}(y \\mid x) f_{X}(x)=f(y, x)\\), we find that the above expression equals\n\\[\n\\int_{\\mathbb{R}^{k}}\\left(\\int_{\\mathbb{R}} y f_{Y \\mid X}(y \\mid x) d y\\right) f_{X}(x) d x=\\int_{\\mathbb{R}^{k}} \\int_{\\mathbb{R}} y f(y, x) d y d x=\\mathbb{E}[Y]\n\\]\nthe unconditional expectation of \\(Y\\).\nProof of Theorem 2.2 Again assume that the variables have a joint density. It is useful to observe that\n\\[\nf\\left(y \\mid x_{1}, x_{2}\\right) f\\left(x_{2} \\mid x_{1}\\right)=\\frac{f\\left(y, x_{1}, x_{2}\\right)}{f\\left(x_{1}, x_{2}\\right)} \\frac{f\\left(x_{1}, x_{2}\\right)}{f\\left(x_{1}\\right)}=f\\left(y, x_{2} \\mid x_{1}\\right)\n\\]\nthe density of \\(\\left(Y, X_{2}\\right)\\) given \\(X_{1}\\). Here, we have abused notation and used a single symbol \\(f\\) to denote the various unconditional and conditional densities to reduce notational clutter.\nNote that\n\\[\n\\mathbb{E}\\left[Y \\mid X_{1}=x_{1}, X_{2}=x_{2}\\right]=\\int_{\\mathbb{R}} y f\\left(y \\mid x_{1}, x_{2}\\right) d y .\n\\]\nIntegrating (2.59) with respect to the conditional density of \\(X_{2}\\) given \\(X_{1}\\), and applying (2.58) we find that\n\\[\n\\begin{aligned}\n\\mathbb{E}\\left[\\mathbb{E}\\left[Y \\mid X_{1}, X_{2}\\right] \\mid X_{1}=x_{1}\\right] &=\\int_{\\mathbb{R}^{k_{2}}} \\mathbb{E}\\left[Y \\mid X_{1}=x_{1}, X_{2}=x_{2}\\right] f\\left(x_{2} \\mid x_{1}\\right) d x_{2} \\\\\n&=\\int_{\\mathbb{R}^{k_{2}}}\\left(\\int_{\\mathbb{R}} y f\\left(y \\mid x_{1}, x_{2}\\right) d y\\right) f\\left(x_{2} \\mid x_{1}\\right) d x_{2} \\\\\n&=\\int_{\\mathbb{R}^{k_{2}}} \\int_{\\mathbb{R}} y f\\left(y \\mid x_{1}, x_{2}\\right) f\\left(x_{2} \\mid x_{1}\\right) d y d x_{2} \\\\\n&=\\int_{\\mathbb{R}^{k_{2}}} \\int_{\\mathbb{R}} y f\\left(y, x_{2} \\mid x_{1}\\right) d y d x_{2} \\\\\n&=\\mathbb{E}\\left[Y \\mid X_{1}=x_{1}\\right] .\n\\end{aligned}\n\\]\nThis implies \\(\\mathbb{E}\\left[\\mathbb{E}\\left[Y \\mid X_{1}, X_{2}\\right] \\mid X_{1}\\right]=\\mathbb{E}\\left[Y \\mid X_{1}\\right]\\) as stated.\nProof of Theorem 2.3\n\\[\n\\mathbb{E}[g(X) Y \\mid X=x]=\\int_{\\mathbb{R}} g(x) y f_{Y \\mid X}(y \\mid x) d y=g(x) \\int_{\\mathbb{R}} y f_{Y \\mid X}(y \\mid x) d y=g(x) \\mathbb{E}[Y \\mid X=x]\n\\]\nThis implies \\(\\mathbb{E}[g(X) Y \\mid X]=g(X) \\mathbb{E}[Y \\mid X]\\) which is (2.7). Equation (2.8) follows by applying the simple law of iterated expectations (Theorem 2.1) to (2.7).\nProof of Theorem 2.4 Applying Minkowski’s inequality (B.34) to \\(e=Y-m(X)\\),\n\\[\n\\left(\\mathbb{E}|e|^{r}\\right)^{1 / r}=\\left(\\mathbb{E}|Y-m(X)|^{r}\\right)^{1 / r} \\leq\\left(\\mathbb{E}|Y|^{r}\\right)^{1 / r}+\\left(\\mathbb{E}|m(X)|^{r}\\right)^{1 / r}<\\infty,\n\\]\nwhere the two parts on the right-hand-side are finite because \\(\\mathbb{E}|Y|^{r}<\\infty\\) by assumption and \\(\\mathbb{E}|m(X)|^{r}<\\) \\(\\infty\\) by the conditional expectation inequality (B.29). The fact that \\(\\left(\\mathbb{E}|e|^{r}\\right)^{1 / r}<\\infty\\) implies \\(\\mathbb{E}|e|^{r}<\\infty\\).\nProof of Theorem 2.6 The assumption that \\(\\mathbb{E}\\left[Y^{2}\\right]<\\infty\\) implies that all the conditional expectations below exist.\nUsing the law of iterated expectations (Theorem 2.2) \\(\\mathbb{E}\\left[Y \\mid X_{1}\\right]=\\mathbb{E}\\left(\\mathbb{E}\\left[Y \\mid X_{1}, X_{2}\\right] \\mid X_{1}\\right)\\) and the conditional Jensen’s inequality (B.28),\n\\[\n\\left(\\mathbb{E}\\left[Y \\mid X_{1}\\right]\\right)^{2}=\\left(\\mathbb{E}\\left(\\mathbb{E}\\left[Y \\mid X_{1}, X_{2}\\right] \\mid X_{1}\\right)\\right)^{2} \\leq \\mathbb{E}\\left[\\left(\\mathbb{E}\\left[Y \\mid X_{1}, X_{2}\\right]\\right)^{2} \\mid X_{1}\\right] .\n\\]\nTaking unconditional expectations, this implies\n\\[\n\\mathbb{E}\\left[\\left(\\mathbb{E}\\left[Y \\mid X_{1}\\right]\\right)^{2}\\right] \\leq \\mathbb{E}\\left[\\left(\\mathbb{E}\\left[Y \\mid X_{1}, X_{2}\\right]\\right)^{2}\\right] .\n\\]\nSimilarly,\n\\[\n(\\mathbb{E}[Y])^{2} \\leq \\mathbb{E}\\left[\\left(\\mathbb{E}\\left[Y \\mid X_{1}\\right]\\right)^{2}\\right] \\leq \\mathbb{E}\\left[\\left(\\mathbb{E}\\left[Y \\mid X_{1}, X_{2}\\right]\\right)^{2}\\right] .\n\\]\nThe variables \\(Y, \\mathbb{E}\\left[Y \\mid X_{1}\\right]\\), and \\(\\mathbb{E}\\left[Y \\mid X_{1}, X_{2}\\right]\\) all have the same expectation \\(\\mathbb{E}[Y]\\), so the inequality (2.60) implies that the variances are ranked monotonically:\n\\[\n0 \\leq \\operatorname{var}\\left(\\mathbb{E}\\left[Y \\mid X_{1}\\right]\\right) \\leq \\operatorname{var}\\left(\\mathbb{E}\\left[Y \\mid X_{1}, X_{2}\\right]\\right) .\n\\]\nDefine \\(e=Y-\\mathbb{E}[Y \\mid X]\\) and \\(u=\\mathbb{E}[Y \\mid X]-\\mu\\) so that we have the decomposition \\(Y-\\mu=e+u\\). Notice \\(\\mathbb{E}[e \\mid X]=0\\) and \\(u\\) is a function of \\(X\\). Thus by the conditioning theorem (Theorem 2.3), \\(\\mathbb{E}[e u]=0\\) so \\(e\\) and \\(u\\) are uncorrelated. It follows that\n\\[\n\\operatorname{var}[Y]=\\operatorname{var}[e]+\\operatorname{var}[u]=\\operatorname{var}[Y-\\mathbb{E}[Y \\mid X]]+\\operatorname{var}[\\mathbb{E}[Y \\mid X]]\n\\]\nThe monotonicity of the variances of the conditional expectation (2.61) applied to the variance decomposition (2.62) implies the reverse monotonicity of the variances of the differences, completing the proof.\nProof of Theorem 2.9 For part 1, by the expectation inequality (B.30), (A.17) and Assumption 2.1,\n\\[\n\\left\\|\\mathbb{E}\\left[X X^{\\prime}\\right]\\right\\| \\leq \\mathbb{E}\\left\\|X X^{\\prime}\\right\\|=\\mathbb{E}\\|X\\|^{2}<\\infty .\n\\]\nSimilarly, using the expectation inequality (B.30), the Cauchy-Schwarz inequality (B.32), and Assumption 2.1,\n\\[\n\\|\\mathbb{E}[X Y]\\| \\leq \\mathbb{E}\\|X Y\\| \\leq\\left(\\mathbb{E}\\|X\\|^{2}\\right)^{1 / 2}\\left(\\mathbb{E}\\left[Y^{2}\\right]\\right)^{1 / 2}<\\infty .\n\\]\nThus the moments \\(\\mathbb{E}[X Y]\\) and \\(\\mathbb{E}\\left[X X^{\\prime}\\right]\\) are finite and well defined.\nFor part 2, the coefficient \\(\\beta=\\left(\\mathbb{E}\\left[X X^{\\prime}\\right]\\right)^{-1} \\mathbb{E}[X Y]\\) is well defined because \\(\\left(\\mathbb{E}\\left[X X^{\\prime}\\right]\\right)^{-1}\\) exists under Assumption 2.1.\nPart 3 follows from Definition \\(2.5\\) and part \\(2 .\\)\nFor part 4, first note that\n\\[\n\\begin{aligned}\n\\mathbb{E}\\left[e^{2}\\right] &=\\mathbb{E}\\left[\\left(Y-X^{\\prime} \\beta\\right)^{2}\\right] \\\\\n&=\\mathbb{E}\\left[Y^{2}\\right]-2 \\mathbb{E}\\left[Y X^{\\prime}\\right] \\beta+\\beta^{\\prime} \\mathbb{E}\\left[X X^{\\prime}\\right] \\beta \\\\\n&=\\mathbb{E}\\left[Y^{2}\\right]-\\mathbb{E}\\left[Y X^{\\prime}\\right]\\left(\\mathbb{E}\\left[X X^{\\prime}\\right]\\right)^{-1} \\mathbb{E}[X Y] \\\\\n& \\leq \\mathbb{E}\\left[Y^{2}\\right]<\\infty .\n\\end{aligned}\n\\]\nThe first inequality holds because \\(\\mathbb{E}\\left[Y X^{\\prime}\\right]\\left(\\mathbb{E}\\left[X X^{\\prime}\\right]\\right)^{-1} \\mathbb{E}[X Y]\\) is a quadratic form and therefore necessarily non-negative. Second, by the expectation inequality (B.30), the Cauchy-Schwarz inequality (B.32), and Assumption 2.1,\n\\[\n\\|\\mathbb{E}[X e]\\| \\leq \\mathbb{E}\\|X e\\|=\\left(\\mathbb{E}\\|X\\|^{2}\\right)^{1 / 2}\\left(\\mathbb{E}\\left[e^{2}\\right]\\right)^{1 / 2}<\\infty .\n\\]\nIt follows that the expectation \\(\\mathbb{E}[X e]\\) is finite, and is zero by the calculation (2.26).\nFor part 6, applying Minkowski’s inequality (B.34) to \\(e=Y-X^{\\prime} \\beta\\),\n\\[\n\\begin{aligned}\n\\left(\\mathbb{E}|e|^{r}\\right)^{1 / r} &=\\left(\\mathbb{E}\\left|Y-X^{\\prime} \\beta\\right|^{r}\\right)^{1 / r} \\\\\n& \\leq\\left(\\mathbb{E}|Y|^{r}\\right)^{1 / r}+\\left(\\mathbb{E}\\left|X^{\\prime} \\beta\\right|^{r}\\right)^{1 / r} \\\\\n& \\leq\\left(\\mathbb{E}|Y|^{r}\\right)^{1 / r}+\\left(\\mathbb{E}\\|X\\|^{r}\\right)^{1 / r}\\|\\beta\\|<\\infty,\n\\end{aligned}\n\\]\nthe final inequality by assumption."
  },
  {
    "objectID": "chpt02-ce.html#exercises",
    "href": "chpt02-ce.html#exercises",
    "title": "2  Conditional Expectation and Projection",
    "section": "2.38 Exercises",
    "text": "2.38 Exercises\nExercise 2.1 Find \\(\\mathbb{E}\\left[\\mathbb{E}\\left[\\mathbb{E}\\left[Y \\mid X_{1}, X_{2}, X_{3}\\right] \\mid X_{1}, X_{2}\\right] \\mid X_{1}\\right]\\)\nExercise 2.2 If \\(\\mathbb{E}[Y \\mid X]=a+b X\\), find \\(\\mathbb{E}[Y X]\\) as a function of moments of \\(X\\).\nExercise 2.3 Prove Theorem 2.4.4 using the law of iterated expectations. Exercise 2.4 Suppose that the random variables \\(Y\\) and \\(X\\) only take the values 0 and 1 , and have the following joint probability distribution\n\n\n\n\n\\(X=0\\)\n\\(X=1\\)\n\n\n\n\n\\(Y=0\\)\n\\(.1\\)\n\\(.2\\)\n\n\n\\(Y=1\\)\n\\(.4\\)\n\\(.3\\)\n\n\n\nFind \\(\\mathbb{E}[Y \\mid X], \\mathbb{E}\\left[Y^{2} \\mid X\\right]\\), and \\(\\operatorname{var}[Y \\mid X]\\) for \\(X=0\\) and \\(X=1\\)\nExercise 2.5 Show that \\(\\sigma^{2}(X)\\) is the best predictor of \\(e^{2}\\) given \\(X\\) :\n\nWrite down the mean-squared error of a predictor \\(h(X)\\) for \\(e^{2}\\).\nWhat does it mean to be predicting \\(e^{2}\\) ?\nShow that \\(\\sigma^{2}(X)\\) minimizes the mean-squared error and is thus the best predictor.\n\nExercise 2.6 Use \\(Y=m(X)+e\\) to show that \\(\\operatorname{var}[Y]=\\operatorname{var}[m(X)]+\\sigma^{2}\\)\nExercise 2.7 Show that the conditional variance can be written as \\(\\sigma^{2}(X)=\\mathbb{E}\\left[Y^{2} \\mid X\\right]-(\\mathbb{E}[Y \\mid X])^{2}\\).\nExercise 2.8 Suppose that \\(Y\\) is discrete-valued, taking values only on the non-negative integers, and the conditional distribution of \\(Y\\) given \\(X=x\\) is Poisson:\n\\[\n\\mathbb{P}[Y=j \\mid X=x]=\\frac{\\exp \\left(-x^{\\prime} \\beta\\right)\\left(x^{\\prime} \\beta\\right)^{j}}{j !}, \\quad j=0,1,2, \\ldots\n\\]\nCompute \\(\\mathbb{E}[Y \\mid X]\\) and \\(\\operatorname{var}[Y \\mid X]\\). Does this justify a linear regression model of the form \\(Y=X^{\\prime} \\beta+e\\) ?\n\\[\n\\text { Hint: If } \\mathbb{P}[Y=j]=\\frac{\\exp (-\\lambda) \\lambda^{j}}{j !} \\text { then } \\mathbb{E}[Y]=\\lambda \\text { and } \\operatorname{var}[Y]=\\lambda \\text {. }\n\\]\nExercise 2.9 Suppose you have two regressors: \\(X_{1}\\) is binary (takes values 0 and 1) and \\(X_{2}\\) is categorical with 3 categories \\((A, B, C)\\). Write \\(\\mathbb{E}\\left[Y \\mid X_{1}, X_{2}\\right]\\) as a linear regression.\nExercise 2.10 True or False. If \\(Y=X \\beta+e, X \\in \\mathbb{R}\\), and \\(\\mathbb{E}[e \\mid X]=0\\), then \\(\\mathbb{E}\\left[X^{2} e\\right]=0\\).\nExercise 2.11 True or False. If \\(Y=X \\beta+e, X \\in \\mathbb{R}\\), and \\(\\mathbb{E}[X e]=0\\), then \\(\\mathbb{E}\\left[X^{2} e\\right]=0\\).\nExercise 2.12 True or False. If \\(Y=X^{\\prime} \\beta+e\\) and \\(\\mathbb{E}[e \\mid X]=0\\), then \\(e\\) is independent of \\(X\\).\nExercise 2.13 True or False. If \\(Y=X^{\\prime} \\beta+e\\) and \\(\\mathbb{E}[X e]=0\\), then \\(\\mathbb{E}[e \\mid X]=0\\).\nExercise 2.14 True or False. If \\(Y=X^{\\prime} \\beta+e, \\mathbb{E}[e \\mid X]=0\\), and \\(\\mathbb{E}\\left[e^{2} \\mid X\\right]=\\sigma^{2}\\), then \\(e\\) is independent of \\(X\\).\nExercise 2.15 Consider the intercept-only model \\(Y=\\alpha+e\\) with \\(\\alpha\\) the best linear predictor. Show that \\(\\alpha=\\mathbb{E}[Y]\\)\nExercise 2.16 Let \\(X\\) and \\(Y\\) have the joint density \\(f(x, y)=\\frac{3}{2}\\left(x^{2}+y^{2}\\right)\\) on \\(0 \\leq x \\leq 1,0 \\leq y \\leq 1\\). Compute the coefficients of the best linear predictor \\(Y=\\alpha+\\beta X+e\\). Compute the conditional expectation \\(m(x)=\\) \\(\\mathbb{E}[Y \\mid X=x]\\). Are the best linear predictor and conditional expectation different? Exercise 2.17 Let \\(X\\) be a random variable with \\(\\mu=\\mathbb{E}[X]\\) and \\(\\sigma^{2}=\\operatorname{var}[X]\\). Define\n\\[\ng\\left(x, \\mu, \\sigma^{2}\\right)=\\left(\\begin{array}{c}\nx-\\mu \\\\\n(x-\\mu)^{2}-\\sigma^{2}\n\\end{array}\\right) .\n\\]\nShow that \\(\\mathbb{E}[g(X, m, s)]=0\\) if and only if \\(m=\\mu\\) and \\(s=\\sigma^{2}\\).\nExercise 2.18 Suppose that \\(X=\\left(1, X_{2}, X_{3}\\right)\\) where \\(X_{3}=\\alpha_{1}+\\alpha_{2} X_{2}\\) is a linear function of \\(X_{2}\\).\n\nShow that \\(\\boldsymbol{Q}_{X X}=\\mathbb{E}\\left[X X^{\\prime}\\right]\\) is not invertible.\nUse a linear transformation of \\(X\\) to find an expression for the best linear predictor of \\(Y\\) given \\(X\\). (Be explicit, do not just use the generalized inverse formula.)\n\nExercise 2.19 Show (2.47)-(2.48), namely that for\n\\[\nd(\\beta)=\\mathbb{E}\\left[\\left(m(X)-X^{\\prime} \\beta\\right)^{2}\\right]\n\\]\nthen\n\\[\n\\beta=\\underset{b \\in \\mathbb{R}^{k}}{\\operatorname{argmin}} d(b)=\\left(\\mathbb{E}\\left[X X^{\\prime}\\right]\\right)^{-1} \\mathbb{E}[X m(X)]=\\left(\\mathbb{E}\\left[X X^{\\prime}\\right]\\right)^{-1} \\mathbb{E}[X Y] .\n\\]\nHint: To show \\(\\mathbb{E}[X m(X)]=\\mathbb{E}[X Y]\\) use the law of iterated expectations.\nExercise 2.20 Verify that (2.57) holds with \\(m(X)\\) defined in (2.6) when \\((Y, X)\\) have a joint density \\(f(y, x)\\).\nExercise 2.21 Consider the short and long projections\n\\[\n\\begin{gathered}\nY=X \\gamma_{1}+e \\\\\nY=X \\beta_{1}+X^{2} \\beta_{2}+u\n\\end{gathered}\n\\]\n\nUnder what condition does \\(\\gamma_{1}=\\beta_{1}\\) ?\nTake the long projection is \\(Y=X \\theta_{1}+X^{3} \\theta_{2}+v\\). Is there a condition under which \\(\\gamma_{1}=\\theta_{1}\\) ?\n\nExercise 2.22 Take the homoskedastic model\n\\[\n\\begin{aligned}\nY &=X_{1}^{\\prime} \\beta_{1}+X_{2}^{\\prime} \\beta_{2}+e \\\\\n\\mathbb{E}\\left[e \\mid X_{1}, X_{2}\\right] &=0 \\\\\n\\mathbb{E}\\left[e^{2} \\mid X_{1}, X_{2}\\right] &=\\sigma^{2} \\\\\n\\mathbb{E}\\left[X_{2} \\mid X_{1}\\right] &=\\Gamma X_{1} .\n\\end{aligned}\n\\]\nAssume \\(\\Gamma \\neq 0\\). Suppose the parameter \\(\\beta_{1}\\) is of interest. We know that the exclusion of \\(X_{2}\\) creates omited variable bias in the projection coefficient on \\(X_{2}\\). It also changes the equation error. Our question is: what is the effect on the homoskedasticity property of the induced equation error? Does the exclusion of \\(X_{2}\\) induce heteroskedasticity or not? Be specific."
  },
  {
    "objectID": "chpt03-algebra.html",
    "href": "chpt03-algebra.html",
    "title": "3  The Algebra of Least Squares",
    "section": "",
    "text": "In this chapter we introduce the popular least squares estimator. Most of the discussion will be algebraic, with questions of distribution and inference deferred to later chapters."
  },
  {
    "objectID": "chpt03-algebra.html#samples",
    "href": "chpt03-algebra.html#samples",
    "title": "3  The Algebra of Least Squares",
    "section": "3.2 Samples",
    "text": "3.2 Samples\nIn Section \\(2.18\\) we derived and discussed the best linear predictor of \\(Y\\) given \\(X\\) for a pair of random variables \\((Y, X) \\in \\mathbb{R} \\times \\mathbb{R}^{k}\\) and called this the linear projection model. We are now interested in estimating the parameters of this model, in particular the projection coefficient\n\\[\n\\beta=\\left(\\mathbb{E}\\left[X X^{\\prime}\\right]\\right)^{-1} \\mathbb{E}[X Y] .\n\\]\nWe can estimate \\(\\beta\\) from samples which include joint measurements of \\((Y, X)\\). For example, supposing we are interested in estimating a wage equation, we would use a dataset with observations on wages (or weekly earnings), education, experience (or age), and demographic characteristics (gender, race, location). One possible dataset is the Current Population Survey (CPS), a survey of U.S. households which includes questions on employment, income, education, and demographic characteristics.\nNotationally we wish to distinguish observations (realizations) from the underlying random variables. The random variables are \\((Y, X)\\). The observations are \\(\\left(Y_{i}, X_{i}\\right)\\). From the vantage of the researcher the latter are numbers. From the vantage of statistical theory we view them as realizations of random variables. For individual observations we append a subscript \\(i\\) which runs from 1 to \\(n\\), thus the \\(i^{t h}\\) observation is \\(\\left(Y_{i}, X_{i}\\right)\\). The number \\(n\\) is the sample size. The dataset or sample is \\(\\left\\{\\left(Y_{i}, X_{i}\\right): i=1, \\ldots, n\\right\\}\\).\nFrom the viewpoint of empirical analysis a dataset is an array of numbers. It is typically organized as a table where each column is a variable and each row is an observation. For empirical analysis the dataset is fixed in the sense that they are numbers presented to the researcher. For statistical analysis we view the dataset as random, or more precisely as a realization of a random process.\nThe individual observations could be draws from a common (homogeneous) distribution or could be draws from heterogeneous distributions. The simplest approach is to assume homogeneity-that the observations are realizations from an identical underlying population \\(F\\).\nAssumption 3.1 The variables \\(\\left\\{\\left(Y_{1}, X_{1}\\right), \\ldots,\\left(Y_{i}, X_{i}\\right), \\ldots,\\left(Y_{n}, X_{n}\\right)\\right\\}\\) are identically distributed; they are draws from a common distribution \\(F\\). This assumption does not need to be viewed as literally true. Rather it is a useful modeling device so that parameters such as \\(\\beta\\) are well defined. This assumption should be interpreted as how we view an observation a priori, before we actually observe it. If I tell you that we have a sample with \\(n=59\\) observations set in no particular order, then it makes sense to view two observations, say 17 and 58 , as draws from the same distribution. We have no reason to expect anything special about either observation.\nIn econometric theory we refer to the underlying common distribution \\(F\\) as the population. Some authors prefer the label data-generating-process (DGP). You can think of it as a theoretical concept or an infinitely-large potential population. In contrast, we refer to the observations available to us \\(\\left\\{\\left(Y_{i}, X_{i}\\right)\\right.\\) : \\(i=1, \\ldots, n\\}\\) as the sample or dataset. In some contexts the dataset consists of all potential observations, for example administrative tax records may contain every single taxpayer in a political unit. Even in this case we can view the observations as if they are random draws from an underlying infinitely-large population as this will allow us to apply the tools of statistical theory.\nThe linear projection model applies to the random variables \\((Y, X)\\). This is the probability model described in Section 2.18. The model is\n\\[\nY=X^{\\prime} \\beta+e\n\\]\nwhere the linear projection coefficient \\(\\beta\\) is defined as\n\\[\n\\beta=\\underset{b \\in \\mathbb{R}^{k}}{\\operatorname{argmin}} S(b),\n\\]\nthe minimizer of the expected squared error\n\\[\nS(\\beta)=\\mathbb{E}\\left[\\left(Y-X^{\\prime} \\beta\\right)^{2}\\right] .\n\\]\nThe coefficient has the explicit solution (3.1)."
  },
  {
    "objectID": "chpt03-algebra.html#moment-estimators",
    "href": "chpt03-algebra.html#moment-estimators",
    "title": "3  The Algebra of Least Squares",
    "section": "3.3 Moment Estimators",
    "text": "3.3 Moment Estimators\nWe want to estimate the coefficient \\(\\beta\\) defined in (3.1) from the sample of observations. Notice that \\(\\beta\\) is written as a function of certain population expectations. In this context an appropriate estimator is the same function of the sample moments. Let’s explain this in detail.\nTo start, suppose that we are interested in the population mean \\(\\mu\\) of a random variable \\(Y\\) with distribution function \\(F\\)\n\\[\n\\mu=\\mathbb{E}[Y]=\\int_{-\\infty}^{\\infty} y d F(y) .\n\\]\nThe expectation \\(\\mu\\) is a function of the distribution \\(F\\). To estimate \\(\\mu\\) given \\(n\\) random variables \\(Y_{i}\\) from \\(F\\) a natural estimator is the sample mean\n\\[\n\\widehat{\\mu}=\\bar{Y}=\\frac{1}{n} \\sum_{i=1}^{n} Y_{i} .\n\\]\nNotice that we have written this using two pieces of notation. The notation \\(\\bar{Y}\\) with the bar on top is conventional for a sample mean. The notation \\(\\widehat{\\mu}\\) with the hat ” \\(\\wedge\\) ” is conventional in econometrics to denote an estimator of the parameter \\(\\mu\\). In this case \\(\\bar{Y}\\) is the estimator of \\(\\mu\\), so \\(\\widehat{\\mu}\\) and \\(\\bar{Y}\\) are the same. The sample mean \\(\\bar{Y}\\) can be viewed as the natural analog of the population mean (3.5) because \\(\\bar{Y}\\) equals the expectation (3.5) with respect to the empirical distribution - the discrete distribution which puts weight \\(1 / n\\) on each observation \\(Y_{i}\\). There are other justifications for \\(\\bar{Y}\\) as an estimator for \\(\\mu\\). We will defer these discussions for now. Suffice it to say that it is the conventional estimator. Now suppose that we are interested in a set of population expectations of possibly nonlinear functions of a random vector \\(Y\\), say \\(\\mu=\\mathbb{E}[h(Y)]\\). For example, we may be interested in the first two moments of \\(Y, \\mathbb{E}[Y]\\) and \\(\\mathbb{E}\\left[Y^{2}\\right]\\). In this case the natural estimator is the vector of sample means,\n\\[\n\\widehat{\\mu}=\\frac{1}{n} \\sum_{i=1}^{n} h\\left(Y_{i}\\right) .\n\\]\nWe call \\(\\widehat{\\mu}\\) the moment estimator for \\(\\mu\\). For example, if \\(h(y)=\\left(y, y^{2}\\right)^{\\prime}\\) then \\(\\widehat{\\mu}_{1}=n^{-1} \\sum_{i=1}^{n} Y_{i}\\) and \\(\\widehat{\\mu}_{2}=\\) \\(n^{-1} \\sum_{i=1}^{n} Y_{i}^{2}\\)\nNow suppose that we are interested in a nonlinear function of a set of moments. For example, consider the variance of \\(Y\\)\n\\[\n\\sigma^{2}=\\operatorname{var}[Y]=\\mathbb{E}\\left[Y^{2}\\right]-(\\mathbb{E}[Y])^{2} .\n\\]\nIn general, many parameters of interest can be written as a function of moments of \\(Y\\). Notationally, \\(\\beta=g(\\mu)\\) and \\(\\mu=\\mathbb{E}[h(Y)]\\). Here, \\(Y\\) are the random variables, \\(h(Y)\\) are functions (transformations) of the random variables, and \\(\\mu\\) is the expectation of these functions. \\(\\beta\\) is the parameter of interest, and is the (nonlinear) function \\(g(\\cdot)\\) of these expectations.\nIn this context a natural estimator of \\(\\beta\\) is obtained by replacing \\(\\mu\\) with \\(\\widehat{\\mu}\\). Thus \\(\\widehat{\\beta}=g(\\widehat{\\mu})\\). The estimator \\(\\widehat{\\beta}\\) is often called a plug-in estimator. We also call \\(\\widehat{\\beta}\\) a moment, or moment-based, estimator of \\(\\beta\\) since it is a natural extension of the moment estimator \\(\\widehat{\\mu}\\).\nTake the example of the variance \\(\\sigma^{2}=\\operatorname{var}[Y]\\). Its moment estimator is\n\\[\n\\widehat{\\sigma}^{2}=\\widehat{\\mu}_{2}-\\widehat{\\mu}_{1}^{2}=\\frac{1}{n} \\sum_{i=1}^{n} Y_{i}^{2}-\\left(\\frac{1}{n} \\sum_{i=1}^{n} Y_{i}\\right)^{2}\n\\]\nThis is not the only possible estimator for \\(\\sigma^{2}\\) (there is also the well-known bias-corrected estimator) but \\(\\widehat{\\sigma}^{2}\\) is a straightforward and simple choice."
  },
  {
    "objectID": "chpt03-algebra.html#least-squares-estimator",
    "href": "chpt03-algebra.html#least-squares-estimator",
    "title": "3  The Algebra of Least Squares",
    "section": "3.4 Least Squares Estimator",
    "text": "3.4 Least Squares Estimator\nThe linear projection coefficient \\(\\beta\\) is defined in (3.3) as the minimizer of the expected squared error \\(S(\\beta)\\) defined in (3.4). For given \\(\\beta\\), the expected squared error is the expectation of the squared error \\(\\left(Y-X^{\\prime} \\beta\\right)^{2}\\). The moment estimator of \\(S(\\beta)\\) is the sample average:\n\\[\n\\widehat{S}(\\beta)=\\frac{1}{n} \\sum_{i=1}^{n}\\left(Y_{i}-X_{i}^{\\prime} \\beta\\right)^{2}=\\frac{1}{n} \\operatorname{SSE}(\\beta)\n\\]\nwhere\n\\[\n\\operatorname{SSE}(\\beta)=\\sum_{i=1}^{n}\\left(Y_{i}-X_{i}^{\\prime} \\beta\\right)^{2}\n\\]\nis called the sum of squared errors function.\nSince \\(\\widehat{S}(\\beta)\\) is a sample average we can interpret it as an estimator of the expected squared error \\(S(\\beta)\\). Examining \\(\\widehat{S}(\\beta)\\) as a function of \\(\\beta\\) is informative about how \\(S(\\beta)\\) varies with \\(\\beta\\). Since the projection coefficient minimizes \\(S(\\beta)\\) an analog estimator minimizes (3.6).\nWe define the estimator \\(\\widehat{\\beta}\\) as the minimizer of \\(\\widehat{S}(\\beta)\\).\nDefinition \\(3.1\\) The least squares estimator is \\(\\widehat{\\beta}=\\underset{\\beta \\in \\mathbb{R}^{k}}{\\operatorname{argmin}} \\widehat{S}(\\beta)\\)\\ where \\(\\widehat{S}(\\beta)=\\frac{1}{n} \\sum_{i=1}^{n}\\left(Y_{i}-X_{i}^{\\prime} \\beta\\right)^{2}\\)\nAs \\(\\widehat{S}(\\beta)\\) is a scale multiple of \\(\\operatorname{SSE}(\\beta)\\) we may equivalently define \\(\\widehat{\\beta}\\) as the minimizer of \\(\\operatorname{SSE}(\\beta)\\). Hence \\(\\widehat{\\beta}\\) is commonly called the least squares (LS) estimator of \\(\\beta\\). The estimator is also commonly refered to as the ordinary least squares (OLS) estimator. For the origin of this label see the historical discussion on Adrien-Marie Legendre below. Here, as is common in econometrics, we put a hat ” \\(\\wedge\\) ” over the parameter \\(\\beta\\) to indicate that \\(\\widehat{\\beta}\\) is a sample estimator of \\(\\beta\\). This is a helpful convention. Just by seeing the symbol \\(\\widehat{\\beta}\\) we can immediately interpret it as an estimator (because of the hat) of the parameter \\(\\beta\\). Sometimes when we want to be explicit about the estimation method, we will write \\(\\widehat{\\beta}_{\\text {ols }}\\) to signify that it is the OLS estimator. It is also common to see the notation \\(\\widehat{\\beta}_{n}\\), where the subscript ” \\(n\\) ” indicates that the estimator depends on the sample size \\(n\\).\nIt is important to understand the distinction between population parameters such as \\(\\beta\\) and sample estimators such as \\(\\widehat{\\beta}\\). The population parameter \\(\\beta\\) is a non-random feature of the population while the sample estimator \\(\\widehat{\\beta}\\) is a random feature of a random sample. \\(\\beta\\) is fixed, while \\(\\widehat{\\beta}\\) varies across samples."
  },
  {
    "objectID": "chpt03-algebra.html#solving-for-least-squares-with-one-regressor",
    "href": "chpt03-algebra.html#solving-for-least-squares-with-one-regressor",
    "title": "3  The Algebra of Least Squares",
    "section": "3.5 Solving for Least Squares with One Regressor",
    "text": "3.5 Solving for Least Squares with One Regressor\nFor simplicity, we start by considering the case \\(k=1\\) so that there is a scalar regressor \\(X\\) and a scalar coefficient \\(\\beta\\). To illustrate, Figure 3.1(a) displays a scatter \\(\\operatorname{plot}^{1}\\) of 20 pairs \\(\\left(Y_{i}, X_{i}\\right)\\).\nThe sum of squared errors \\(\\operatorname{SSE}(\\beta)\\) is a function of \\(\\beta\\). Given \\(\\beta\\) we calculate the “error” \\(Y_{i}-X_{i} \\beta\\) by taking the vertical distance between \\(Y_{i}\\) and \\(X_{i} \\beta\\). This can be seen in Figure 3.1(a) by the vertical lines which connect the observations to the straight line. These vertical lines are the errors \\(Y_{i}-X_{i} \\beta\\). The sum of squared errors is the sum of the 20 squared lengths.\nThe sum of squared errors is the function\n\\[\n\\operatorname{SSE}(\\beta)=\\sum_{i=1}^{n}\\left(Y_{i}-X_{i} \\beta\\right)^{2}=\\left(\\sum_{i=1}^{n} Y_{i}^{2}\\right)-2 \\beta\\left(\\sum_{i=1}^{n} X_{i} Y_{i}\\right)+\\beta^{2}\\left(\\sum_{i=1}^{n} X_{i}^{2}\\right) .\n\\]\nThis is a quadratic function of \\(\\beta\\). The sum of squared error function is displayed in Figure \\(3.1\\) (b) over the range \\([2,4]\\). The coefficient \\(\\beta\\) ranges along the \\(x\\)-axis. The sum of squared errors \\(\\operatorname{SSE}(\\beta)\\) as a function of \\(\\beta\\) is displayed on the \\(y\\)-axis.\nThe OLS estimator \\(\\widehat{\\beta}\\) minimizes this function. From elementary algebra we know that the minimizer of the quadratic function \\(a-2 b x+c x^{2}\\) is \\(x=b / c\\). Thus the minimizer of \\(\\operatorname{SSE}(\\beta)\\) is\n\\[\n\\widehat{\\beta}=\\frac{\\sum_{i=1}^{n} X_{i} Y_{i}}{\\sum_{i=1}^{n} X_{i}^{2}}\n\\]\nFor example, the minimizer of the sum of squared error function displayed in Figure 3.1(b) is \\(\\widehat{\\beta}=3.07\\), and is marked on the \\(\\mathrm{x}\\)-axis.\nThe intercept-only model is the special case \\(X_{i}=1\\). In this case we find\n\\[\n\\widehat{\\beta}=\\frac{\\sum_{i=1}^{n} 1 Y_{i}}{\\sum_{i=1}^{n} 1^{2}}=\\frac{1}{n} \\sum_{i=1}^{n} Y_{i}=\\bar{Y},\n\\]\n\\({ }^{1}\\) The observations were generated by simulation as \\(X \\sim U[0,1]\\) and \\(Y \\sim \\mathrm{N}[3 X, 1]\\).\n\n\nDeviation from Fitted Line\n\n\n\nSum of Squared Error Function\n\nFigure 3.1: Regression With One Regressor\nthe sample mean of \\(Y_{i}\\). Here, as is common, we put a bar “-” over \\(Y\\) to indicate that the quantity is a sample mean. This shows that the OLS estimator in the intercept-only model is the sample mean.\nTechnically, the estimator \\(\\widehat{\\beta}\\) in (3.7) only exists if the denominator is non-zero. Since it is a sum of squares it is necessarily non-negative. Thus \\(\\widehat{\\beta}\\) exists if \\(\\sum_{i=1}^{n} X_{i}^{2}>0\\)."
  },
  {
    "objectID": "chpt03-algebra.html#solving-for-least-squares-with-multiple-regressors",
    "href": "chpt03-algebra.html#solving-for-least-squares-with-multiple-regressors",
    "title": "3  The Algebra of Least Squares",
    "section": "3.6 Solving for Least Squares with Multiple Regressors",
    "text": "3.6 Solving for Least Squares with Multiple Regressors\nWe now consider the case with \\(k>1\\) so that the coefficient \\(\\beta \\in \\mathbb{R}^{k}\\) is a vector.\nTo illustrate, Figure \\(3.2\\) displays a scatter plot of 100 triples \\(\\left(Y_{i}, X_{1 i}, X_{2 i}\\right)\\). The regression function \\(x^{\\prime} \\beta=x_{1} \\beta_{1}+x_{2} \\beta_{2}\\) is a 2-dimensional surface and is shown as the plane in Figure 3.2.\nThe sum of squared errors \\(\\operatorname{SSE}(\\beta)\\) is a function of the vector \\(\\beta\\). For any \\(\\beta\\) the error \\(Y_{i}-X_{i}^{\\prime} \\beta\\) is the vertical distance between \\(Y_{i}\\) and \\(X_{i}^{\\prime} \\beta\\). This can be seen in Figure \\(3.2\\) by the vertical lines which connect the observations to the plane. As in the single regressor case these vertical lines are the errors \\(e_{i}=Y_{i}-\\) \\(X_{i}^{\\prime} \\beta\\). The sum of squared errors is the sum of the 100 squared lengths.\nThe sum of squared errors can be written as\n\\[\n\\operatorname{SSE}(\\beta)=\\sum_{i=1}^{n} Y_{i}^{2}-2 \\beta^{\\prime} \\sum_{i=1}^{n} X_{i} Y_{i}+\\beta^{\\prime} \\sum_{i=1}^{n} X_{i} X_{i}^{\\prime} \\beta .\n\\]\nAs in the single regressor case this is a quadratic function in \\(\\beta\\). The difference is that in the multiple regressor case this is a vector-valued quadratic function. To visualize the sum of squared errors function Figure 3.3(a) displays \\(\\operatorname{SSE}(\\beta)\\). Another way to visualize a 3-dimensional surface is by a contour plot. A contour plot of the same \\(\\operatorname{SSE}(\\beta)\\) function is shown in Figure 3.3(b). The contour lines are points in the \\(\\left(\\beta_{1}, \\beta_{2}\\right)\\) space where \\(\\operatorname{SSE}(\\beta)\\) takes the same value. The contour lines are elliptical because \\(\\operatorname{SSE}(\\beta)\\) is quadratic.\n\nFigure 3.2: Regression with Two Variables\nThe least squares estimator \\(\\widehat{\\beta}\\) minimizes \\(\\operatorname{SSE}(\\beta)\\). A simple way to find the minimum is by solving the first-order conditions. The latter are\n\\[\n0=\\frac{\\partial}{\\partial \\beta} \\operatorname{SSE}(\\widehat{\\beta})=-2 \\sum_{i=1}^{n} X_{i} Y_{i}+2 \\sum_{i=1}^{n} X_{i} X_{i}^{\\prime} \\widehat{\\beta}\n\\]\nWe have written this using a single expression, but it is actually a system of \\(k\\) equations with \\(k\\) unknowns (the elements of \\(\\widehat{\\beta}\\) ).\nThe solution for \\(\\widehat{\\beta}\\) may be found by solving the system of \\(k\\) equations in (3.9). We can write this solution compactly using matrix algebra. Dividing (3.9) by 2 we obtain\n\\[\n\\sum_{i=1}^{n} X_{i} X_{i}^{\\prime} \\widehat{\\beta}=\\sum_{i=1}^{n} X_{i} Y_{i} .\n\\]\nThis is a system of equations of the form \\(\\boldsymbol{A} \\boldsymbol{b}=\\boldsymbol{c}\\) where \\(\\boldsymbol{A}\\) is \\(k \\times k\\) and \\(\\boldsymbol{b}\\) and \\(\\boldsymbol{c}\\) are \\(k \\times 1\\). The solution is \\(\\boldsymbol{b}=\\boldsymbol{A}^{-1} \\boldsymbol{c}\\), and can be obtained by pre-multiplying \\(\\boldsymbol{A} \\boldsymbol{b}=\\boldsymbol{c}\\) by \\(\\boldsymbol{A}^{-1}\\) and using the matrix inverse property \\(\\boldsymbol{A}^{-1} \\boldsymbol{A}=\\boldsymbol{I}_{k}\\). Applied to (3.10) we find an explicit formula for the least squares estimator\n\\[\n\\widehat{\\beta}=\\left(\\sum_{i=1}^{n} X_{i} X_{i}^{\\prime}\\right)^{-1}\\left(\\sum_{i=1}^{n} X_{i} Y_{i}\\right) .\n\\]\nThis is the natural estimator of the best linear projection coefficient \\(\\beta\\) defined in (3.3), and could also be called the linear projection estimator.\n\n\nSum of Squared Error Function\n\n\n\nSSE Contour\n\nFigure 3.3: SSE with Two Regressors\nRecall that we claimed that \\(\\widehat{\\beta}\\) in (3.11) is the minimizer of \\(\\operatorname{SSE}(\\beta)\\), and found it by solving the firstorder conditions. To be complete we should verify the second-order conditions. We calculate that\n\\[\n\\frac{\\partial^{2}}{\\partial \\beta \\partial \\beta^{\\prime}} \\operatorname{SSE}(\\beta)=2 \\sum_{i=1}^{n} X_{i} X_{i}^{\\prime}\n\\]\nwhich is a positive semi-definite matrix. If actually positive definite, then the second-order condition for minimization is satisfied, in which case \\(\\widehat{\\beta}\\) is the unique minimizer of \\(\\operatorname{SSE}(\\beta)\\).\nReturning to the example sum of squared errors function \\(\\operatorname{SSE}(\\beta)\\) displayed in Figure \\(3.3\\), the least squares estimator \\(\\widehat{\\beta}\\) is the the pair \\(\\left(\\widehat{\\beta}_{1}, \\widehat{\\beta}_{2}\\right)\\) which minimize this function; visually it is the low spot in the 3-dimensional graph, and is marked in Figure 3.3(b) as the center point of the contour plots.\nTake equation (3.11) and suppose that \\(k=1\\). In this case \\(X_{i}\\) is scalar so \\(X_{i} X_{i}^{\\prime}=X_{i}^{2}\\). Then (3.11) simplifies to the expression (3.7) previously derived. The expression (3.11) is a notationally simple generalization but requires a careful attention to vector and matrix manipulations.\nAlternatively, equation (3.1) writes the projection coefficient \\(\\beta\\) as an explicit function of the population moments \\(\\boldsymbol{Q}_{X Y}\\) and \\(\\boldsymbol{Q}_{X X}\\). Their moment estimators are the sample moments\n\\[\n\\begin{aligned}\n\\widehat{\\boldsymbol{Q}}_{X Y} &=\\frac{1}{n} \\sum_{i=1}^{n} X_{i} Y_{i} \\\\\n\\widehat{\\boldsymbol{Q}}_{X X} &=\\frac{1}{n} \\sum_{i=1}^{n} X_{i} X_{i}^{\\prime}\n\\end{aligned}\n\\]\nThe moment estimator of \\(\\beta\\) replaces the population moments in (3.1) with the sample moments:\n\\[\n\\begin{aligned}\n\\widehat{\\beta} &=\\widehat{\\boldsymbol{Q}}_{X X}^{-1} \\widehat{\\boldsymbol{Q}}_{X Y} \\\\\n&=\\left(\\frac{1}{n} \\sum_{i=1}^{n} X_{i} X_{i}^{\\prime}\\right)^{-1}\\left(\\frac{1}{n} \\sum_{i=1}^{n} X_{i} Y_{i}\\right) \\\\\n&=\\left(\\sum_{i=1}^{n} X_{i} X_{i}^{\\prime}\\right)^{-1}\\left(\\sum_{i=1}^{n} X_{i} Y_{i}\\right)\n\\end{aligned}\n\\]\nwhich is identical with (3.11).\nTechnically, the estimator \\(\\widehat{\\beta}\\) is unique and equals (3.11) only if the inverted matrix is actually invertible, which holds if (and only if) this matrix is positive definite. This excludes the case that \\(X_{i}\\) contains redundant regressors. This will be discussed further in Section 3.24.\nTheorem 3.1 If \\(\\sum_{i=1}^{n} X_{i} X_{i}^{\\prime}>0\\), the least squares estimator is unique and equals\n\\[\n\\widehat{\\beta}=\\left(\\sum_{i=1}^{n} X_{i} X_{i}^{\\prime}\\right)^{-1}\\left(\\sum_{i=1}^{n} X_{i} Y_{i}\\right) .\n\\]"
  },
  {
    "objectID": "chpt03-algebra.html#adrien-marie-legendre",
    "href": "chpt03-algebra.html#adrien-marie-legendre",
    "title": "3  The Algebra of Least Squares",
    "section": "3.7 Adrien-Marie Legendre",
    "text": "3.7 Adrien-Marie Legendre\nThe method of least squares was published in 1805 by the French mathematician Adrien-Marie Legendre (1752-1833). Legendre proposed least squares as a solution to the algebraic problem of solving a system of equations when the number of equations exceeded the number of unknowns. This was a vexing and common problem in astronomical measurement. As viewed by Legendre, (3.2) is a set of \\(n\\) equations with \\(k\\) unknowns. As the equations cannot be solved exactly, Legendre’s goal was to select \\(\\beta\\) to make the set of errors as small as possible. He proposed the sum of squared error criterion and derived the algebraic solution presented above. As he noted, the first-order conditions (3.9) is a system of \\(k\\) equations with \\(k\\) unknowns which can be solved by “ordinary” methods. Hence the method became known as Ordinary Least Squares and to this day we still use the abbreviation OLS to refer to Legendre’s estimation method."
  },
  {
    "objectID": "chpt03-algebra.html#illustration",
    "href": "chpt03-algebra.html#illustration",
    "title": "3  The Algebra of Least Squares",
    "section": "3.8 Illustration",
    "text": "3.8 Illustration\nWe illustrate the least squares estimator in practice with the data set used to calculate the estimates reported in Chapter 2. This is the March 2009 Current Population Survey, which has extensive information on the U.S. population. This data set is described in more detail in Section 3.22. For this illustration we use the sub-sample of married (spouse present) Black female wage earners with 12 years potential work experience. This sub-sample has 20 observations \\({ }^{2}\\).\nIn Table \\(3.1\\) we display the observations for reference. Each row is an individual observation which are the data for an individual person. The columns correspond to the variables (measurements) for the individuals. The second column is the reported wage (total annual earnings divided by hours worked). The third column is the natural logarithm of the wage. The fourth column is years of education. The fifth and six columns are further transformations, specifically the square of education and the product of education and \\(\\log\\) (wage). The bottom row are the sums of the elements in that column.\nTable 3.1: Observations From CPS Data Set\n\n\n\n\n\n\n\n\n\n\n\nObservation\nwage\n\\(\\log (\\) wage)\neducation\neducation \\(^{2}\\)\neducation \\(\\times \\log (\\) wage \\()\\)\n\n\n\n\n1\n\\(37.93\\)\n\\(3.64\\)\n18\n324\n\\(65.44\\)\n\n\n2\n\\(40.87\\)\n\\(3.71\\)\n18\n324\n\\(66.79\\)\n\n\n3\n\\(14.18\\)\n\\(2.65\\)\n13\n169\n\\(34.48\\)\n\n\n4\n\\(16.83\\)\n\\(2.82\\)\n16\n256\n\\(45.17\\)\n\n\n5\n\\(33.17\\)\n\\(3.50\\)\n16\n256\n\\(56.03\\)\n\n\n6\n\\(29.81\\)\n\\(3.39\\)\n18\n324\n\\(61.11\\)\n\n\n7\n\\(54.62\\)\n\\(4.00\\)\n16\n256\n\\(64.00\\)\n\n\n8\n\\(43.08\\)\n\\(3.76\\)\n18\n324\n\\(67.73\\)\n\n\n9\n\\(14.42\\)\n\\(2.67\\)\n12\n144\n\\(32.03\\)\n\n\n10\n\\(14.90\\)\n\\(2.70\\)\n16\n256\n\\(43.23\\)\n\n\n11\n\\(21.63\\)\n\\(3.07\\)\n18\n324\n\\(55.44\\)\n\n\n12\n\\(11.09\\)\n\\(2.41\\)\n16\n256\n\\(38.50\\)\n\n\n13\n\\(10.00\\)\n\\(2.30\\)\n13\n169\n\\(29.93\\)\n\n\n14\n\\(31.73\\)\n\\(3.46\\)\n14\n196\n\\(48.40\\)\n\n\n15\n\\(11.06\\)\n\\(2.40\\)\n12\n144\n\\(28.84\\)\n\n\n16\n\\(18.75\\)\n\\(2.93\\)\n16\n256\n\\(46.90\\)\n\n\n17\n\\(27.35\\)\n\\(3.31\\)\n14\n196\n\\(46.32\\)\n\n\n18\n\\(24.04\\)\n\\(3.18\\)\n16\n256\n\\(50.76\\)\n\n\n19\n\\(36.06\\)\n\\(3.59\\)\n18\n324\n\\(64.53\\)\n\n\n20\n\\(23.08\\)\n\\(3.14\\)\n16\n256\n\\(50.22\\)\n\n\nSum\n515\n\\(62.64\\)\n314\n5010\n\\(995.86\\)\n\n\n\nPutting the variables into the standard regression notation, let \\(Y_{i}\\) be \\(\\log (w a g e)\\) and \\(X_{i}\\) be years of education and an intercept. Then from the column sums in Table \\(3.1\\) we have\n\\[\n\\sum_{i=1}^{n} X_{i} Y_{i}=\\left(\\begin{array}{c}\n995.86 \\\\\n62.64\n\\end{array}\\right)\n\\]\nand\n\\[\n\\sum_{i=1}^{n} X_{i} X_{i}^{\\prime}=\\left(\\begin{array}{cc}\n5010 & 314 \\\\\n314 & 20\n\\end{array}\\right)\n\\]\nTaking the inverse we obtain\n\\[\n\\left(\\sum_{i=1}^{n} X_{i} X_{i}^{\\prime}\\right)^{-1}=\\left(\\begin{array}{cc}\n0.0125 & -0.196 \\\\\n-0.196 & 3.124\n\\end{array}\\right) .\n\\]\n\\({ }^{2}\\) This sample was selected specifically so that it has a small number of observations, facilitating exposition. Thus by matrix multiplication\n\\[\n\\widehat{\\beta}=\\left(\\begin{array}{cc}\n0.0125 & -0.196 \\\\\n-0.196 & 3.124\n\\end{array}\\right)\\left(\\begin{array}{c}\n995.86 \\\\\n62.64\n\\end{array}\\right)=\\left(\\begin{array}{c}\n0.155 \\\\\n0.698\n\\end{array}\\right) .\n\\]\nIn practice the regression estimates \\(\\widehat{\\beta}\\) are computed by computer software without the user taking the explicit steps listed above. However, it is useful to understand that the least squares estimator can be calculated by simple algebraic operations. If your data is in a spreadsheet similar to Table 3.1, then the listed transformations (logarithm, squares, cross-products, column sums) can be computed by spreadsheet operations. \\(\\widehat{\\beta}\\) could then be calculated by matrix inversion and multiplication. Once again, this is rarely done by applied economists because computer software is available to ease the process.\nWe often write the estimated equation using the format\n\\[\n\\widehat{\\log (\\text { wage })}=0.155 \\text { education }+0.698 \\text {. }\n\\]\nAn interpretation of the estimated equation is that each year of education is associated with a \\(16 %\\) increase in mean wages.\nAnother use of the estimated equation (3.12) is for prediction. Suppose one individual has 12 years of education and a second has 16. Using (3.12) we find that the first’s expected log wage is\n\\[\n\\widehat{\\log (\\text { wag } e)}=0.155 \\times 12+0.698=2.56\n\\]\nand for the second\n\\[\n\\widehat{\\log (\\text { wage })}=0.155 \\times 16+0.698=3.18 .\n\\]\nEquation (3.12) is called a bivariate regression as there are two variables. It is also called a simple regression as there is a single regressor. A multiple regression has two or more regressors and allows a more detailed investigation. Let’s take an example similar to (3.12) but include all levels of experience. This time we use the sub-sample of single (never married) Asian men which has 268 observations. Including as regressors years of potential work experience (experience) and its square (experience \\({ }^{2} / 100\\) ) (we divide by 100 to simplify reporting) we obtain the estimates\n\\[\n\\widehat{\\log (\\text { wage })}=0.143 \\text { education }+0.036 \\text { experience }-0.071 \\text { experience }^{2} / 100+0.575 \\text {. }\n\\]\nThese estimates suggest a \\(14 %\\) increase in mean wages per year of education holding experience constant."
  },
  {
    "objectID": "chpt03-algebra.html#least-squares-residuals",
    "href": "chpt03-algebra.html#least-squares-residuals",
    "title": "3  The Algebra of Least Squares",
    "section": "3.9 Least Squares Residuals",
    "text": "3.9 Least Squares Residuals\nAs a by-product of estimation we define the fitted value \\(\\widehat{Y}_{i}=X_{i}^{\\prime} \\widehat{\\beta}\\) and the residual\n\\[\n\\widehat{e}_{i}=Y_{i}-\\widehat{Y}_{i}=Y_{i}-X_{i}^{\\prime} \\widehat{\\beta}\n\\]\nSometimes \\(\\widehat{Y}_{i}\\) is called the predicted value but this is a misleading label. The fitted value \\(\\widehat{Y}_{i}\\) is a function of the entire sample including \\(Y_{i}\\), and thus cannot be interpreted as a valid prediction of \\(Y_{i}\\). It is thus more accurate to describe \\(\\widehat{Y}_{i}\\) as a fitted rather than a predicted value.\nNote that \\(Y_{i}=\\widehat{Y}_{i}+\\widehat{e}_{i}\\) and\n\\[\nY_{i}=X_{i}^{\\prime} \\widehat{\\beta}+\\widehat{e}_{i} .\n\\]\nWe make a distinction between the error \\(e_{i}\\) and the residual \\(\\widehat{e}_{i}\\). The error \\(e_{i}\\) is unobservable while the residual \\(\\widehat{e}_{i}\\) is an estimator. These two variables are frequently mislabeled which can cause confusion. Equation (3.9) implies that\n\\[\n\\sum_{i=1}^{n} X_{i} \\widehat{e}_{i}=0 .\n\\]\nTo see this by a direct calculation, using (3.14) and (3.11),\n\\[\n\\begin{aligned}\n\\sum_{i=1}^{n} X_{i} \\widehat{e}_{i} &=\\sum_{i=1}^{n} X_{i}\\left(Y_{i}-X_{i}^{\\prime} \\widehat{\\beta}\\right) \\\\\n&=\\sum_{i=1}^{n} X_{i} Y_{i}-\\sum_{i=1}^{n} X_{i} X_{i}^{\\prime} \\widehat{\\beta} \\\\\n&=\\sum_{i=1}^{n} X_{i} Y_{i}-\\sum_{i=1}^{n} X_{i} X_{i}^{\\prime}\\left(\\sum_{i=1}^{n} X_{i} X_{i}^{\\prime}\\right)^{-1}\\left(\\sum_{i=1}^{n} X_{i} Y_{i}\\right) \\\\\n&=\\sum_{i=1}^{n} X_{i} Y_{i}-\\sum_{i=1}^{n} X_{i} Y_{i}=0 .\n\\end{aligned}\n\\]\nWhen \\(X_{i}\\) contains a constant an implication of (3.16) is\n\\[\n\\frac{1}{n} \\sum_{i=1}^{n} \\widehat{e}_{i}=0 .\n\\]\nThus the residuals have a sample mean of zero and the sample correlation between the regressors and the residual is zero. These are algebraic results and hold true for all linear regression estimates."
  },
  {
    "objectID": "chpt03-algebra.html#demeaned-regressors",
    "href": "chpt03-algebra.html#demeaned-regressors",
    "title": "3  The Algebra of Least Squares",
    "section": "3.10 Demeaned Regressors",
    "text": "3.10 Demeaned Regressors\nSometimes it is useful to separate the constant from the other regressors and write the linear projection equation in the format\n\\[\nY_{i}=X_{i}^{\\prime} \\beta+\\alpha+e_{i}\n\\]\nwhere \\(\\alpha\\) is the intercept and \\(X_{i}\\) does not contain a constant. The least squares estimates and residuals can be written as \\(Y_{i}=X_{i}^{\\prime} \\widehat{\\beta}+\\widehat{\\alpha}+\\widehat{e}_{i}\\).\nIn this case (3.16) can be written as the equation system\n\\[\n\\begin{array}{r}\n\\sum_{i=1}^{n}\\left(Y_{i}-X_{i}^{\\prime} \\widehat{\\beta}-\\widehat{\\alpha}\\right)=0 \\\\\n\\sum_{i=1}^{n} X_{i}\\left(Y_{i}-X_{i}^{\\prime} \\widehat{\\beta}-\\widehat{\\alpha}\\right)=0 .\n\\end{array}\n\\]\nThe first equation implies\n\\[\n\\widehat{\\alpha}=\\bar{Y}-\\bar{X}^{\\prime} \\widehat{\\beta} .\n\\]\nSubtracting from the second we obtain\n\\[\n\\sum_{i=1}^{n} X_{i}\\left(\\left(Y_{i}-\\bar{Y}\\right)-\\left(X_{i}-\\bar{X}\\right)^{\\prime} \\widehat{\\beta}\\right)=0 .\n\\]\nSolving for \\(\\widehat{\\beta}\\) we find\n\\[\n\\begin{aligned}\n\\widehat{\\beta} &=\\left(\\sum_{i=1}^{n} X_{i}\\left(X_{i}-\\bar{X}\\right)^{\\prime}\\right)^{-1}\\left(\\sum_{i=1}^{n} X_{i}\\left(Y_{i}-\\bar{Y}\\right)\\right) \\\\\n&=\\left(\\sum_{i=1}^{n}\\left(X_{i}-\\bar{X}\\right)\\left(X_{i}-\\bar{X}\\right)^{\\prime}\\right)^{-1}\\left(\\sum_{i=1}^{n}\\left(X_{i}-\\bar{X}\\right)\\left(Y_{i}-\\bar{Y}\\right)\\right) .\n\\end{aligned}\n\\]\nThus the OLS estimator for the slope coefficients is OLS with demeaned data and no intercept.\nThe representation (3.18) is known as the demeaned formula for the least squares estimator."
  },
  {
    "objectID": "chpt03-algebra.html#model-in-matrix-notation",
    "href": "chpt03-algebra.html#model-in-matrix-notation",
    "title": "3  The Algebra of Least Squares",
    "section": "3.11 Model in Matrix Notation",
    "text": "3.11 Model in Matrix Notation\nFor many purposes, including computation, it is convenient to write the model and statistics in matrix notation. The \\(n\\) linear equations \\(Y_{i}=X_{i}^{\\prime} \\beta+e_{i}\\) make a system of \\(n\\) equations. We can stack these \\(n\\) equations together as\n\\[\n\\begin{aligned}\n&Y_{1}=X_{1}^{\\prime} \\beta+e_{1} \\\\\n&Y_{2}=X_{2}^{\\prime} \\beta+e_{2} \\\\\n&\\vdots \\\\\n&Y_{n}=X_{n}^{\\prime} \\beta+e_{n} .\n\\end{aligned}\n\\]\nDefine\n\\[\n\\boldsymbol{Y}=\\left(\\begin{array}{c}\nY_{1} \\\\\nY_{2} \\\\\n\\vdots \\\\\nY_{n}\n\\end{array}\\right), \\quad \\boldsymbol{X}=\\left(\\begin{array}{c}\nX_{1}^{\\prime} \\\\\nX_{2}^{\\prime} \\\\\n\\vdots \\\\\nX_{n}^{\\prime}\n\\end{array}\\right), \\quad \\boldsymbol{e}=\\left(\\begin{array}{c}\ne_{1} \\\\\ne_{2} \\\\\n\\vdots \\\\\ne_{n}\n\\end{array}\\right)\n\\]\nObserve that \\(\\boldsymbol{Y}\\) and \\(\\boldsymbol{e}\\) are \\(n \\times 1\\) vectors and \\(\\boldsymbol{X}\\) is an \\(n \\times k\\) matrix. The system of \\(n\\) equations can be compactly written in the single equation\n\\[\n\\boldsymbol{Y}=\\boldsymbol{X} \\beta+\\boldsymbol{e} .\n\\]\nSample sums can be written in matrix notation. For example\n\\[\n\\begin{aligned}\n&\\sum_{i=1}^{n} X_{i} X_{i}^{\\prime}=\\boldsymbol{X}^{\\prime} \\boldsymbol{X} \\\\\n&\\sum_{i=1}^{n} X_{i} Y_{i}=\\boldsymbol{X}^{\\prime} \\boldsymbol{Y} .\n\\end{aligned}\n\\]\nTherefore the least squares estimator can be written as\n\\[\n\\widehat{\\beta}=\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{Y}\\right) .\n\\]\nThe matrix version of (3.15) and estimated version of (3.19) is\n\\[\n\\boldsymbol{Y}=\\boldsymbol{X} \\widehat{\\beta}+\\widehat{\\boldsymbol{e}} .\n\\]\nEquivalently the residual vector is\n\\[\n\\widehat{\\boldsymbol{e}}=\\boldsymbol{Y}-\\boldsymbol{X} \\widehat{\\beta}\n\\]\nUsing the residual vector we can write (3.16) as\n\\[\n\\boldsymbol{X}^{\\prime} \\widehat{\\boldsymbol{e}}=0\n\\]\nIt can also be useful to write the sum of squared error criterion as\n\\[\n\\operatorname{SSE}(\\beta)=(\\boldsymbol{Y}-\\boldsymbol{X} \\beta)^{\\prime}(\\boldsymbol{Y}-\\boldsymbol{X} \\beta) .\n\\]\nUsing matrix notation we have simple expressions for most estimators. This is particularly convenient for computer programming as most languages allow matrix notation and manipulation. Theorem 3.2 Important Matrix Expressions\n\\[\n\\begin{aligned}\n\\widehat{\\beta} &=\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{Y}\\right) \\\\\n\\widehat{\\boldsymbol{e}} &=\\boldsymbol{Y}-\\boldsymbol{X} \\widehat{\\beta} \\\\\n\\boldsymbol{X}^{\\prime} \\widehat{\\boldsymbol{e}} &=0 .\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "chpt03-algebra.html#early-use-of-matrices",
    "href": "chpt03-algebra.html#early-use-of-matrices",
    "title": "3  The Algebra of Least Squares",
    "section": "3.12 Early Use of Matrices",
    "text": "3.12 Early Use of Matrices\nThe earliest known treatment of the use of matrix methods to solve simultaneous systems is found in Chapter 8 of the Chinese text The Nine Chapters on the Mathematical Art, written by several generations of scholars from the \\(10^{\\text {th }}\\) to \\(2^{\\text {nd }}\\) century BCE."
  },
  {
    "objectID": "chpt03-algebra.html#projection-matrix",
    "href": "chpt03-algebra.html#projection-matrix",
    "title": "3  The Algebra of Least Squares",
    "section": "3.13 Projection Matrix",
    "text": "3.13 Projection Matrix\nDefine the matrix\n\\[\n\\boldsymbol{P}=\\boldsymbol{X}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\boldsymbol{X}^{\\prime}\n\\]\nObserve that\n\\[\n\\boldsymbol{P} \\boldsymbol{X}=\\boldsymbol{X}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\boldsymbol{X}^{\\prime} \\boldsymbol{X}=\\boldsymbol{X} .\n\\]\nThis is a property of a projection matrix. More generally, for any matrix \\(\\boldsymbol{Z}\\) which can be written as \\(\\boldsymbol{Z}=\\boldsymbol{X} \\boldsymbol{\\Gamma}\\) for some matrix \\(\\Gamma\\) (we say that \\(\\boldsymbol{Z}\\) lies in the range space of \\(\\boldsymbol{X}\\) ), then\n\\[\n\\boldsymbol{P Z}=\\boldsymbol{P} \\boldsymbol{X} \\boldsymbol{\\Gamma}=\\boldsymbol{X}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\boldsymbol{X}^{\\prime} \\boldsymbol{X} \\boldsymbol{\\Gamma}=\\boldsymbol{X} \\boldsymbol{\\Gamma}=\\boldsymbol{Z} .\n\\]\nAs an important example, if we partition the matrix \\(\\boldsymbol{X}\\) into two matrices \\(\\boldsymbol{X}_{1}\\) and \\(\\boldsymbol{X}_{2}\\) so that \\(\\boldsymbol{X}=\\) \\(\\left[\\begin{array}{ll}\\boldsymbol{X}_{1} & \\boldsymbol{X}_{2}\\end{array}\\right]\\) then \\(\\boldsymbol{P} \\boldsymbol{X}_{1}=\\boldsymbol{X}_{1}\\). (See Exercise 3.7.)\nThe projection matrix \\(\\boldsymbol{P}\\) has the algebraic property that it is idempotent: \\(\\boldsymbol{P} \\boldsymbol{P}=\\boldsymbol{P}\\). See Theorem 3.3.2 below. For the general properties of projection matrices see Section A.11.\nThe matrix \\(\\boldsymbol{P}\\) creates the fitted values in a least squares regression:\n\\[\n\\boldsymbol{P} \\boldsymbol{Y}=\\boldsymbol{X}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\boldsymbol{X}^{\\prime} \\boldsymbol{Y}=\\boldsymbol{X} \\widehat{\\boldsymbol{\\beta}}=\\widehat{\\boldsymbol{Y}} \\text {. }\n\\]\nBecause of this property \\(\\boldsymbol{P}\\) is also known as the hat matrix.\nA special example of a projection matrix occurs when \\(X=\\mathbf{1}_{n}\\) is an \\(n\\)-vector of ones. Then\n\\[\n\\boldsymbol{P}=\\mathbf{1}_{n}\\left(\\mathbf{1}_{n}^{\\prime} \\mathbf{1}_{n}\\right)^{-1} \\mathbf{1}_{n}^{\\prime}=\\frac{1}{n} \\mathbf{1}_{n} \\mathbf{1}_{n}^{\\prime} .\n\\]\nNote that in this case\n\\[\n\\boldsymbol{P} \\boldsymbol{Y}=\\mathbf{1}_{n}\\left(\\mathbf{1}_{n}^{\\prime} \\mathbf{1}_{n}\\right)^{-1} \\mathbf{1}_{n}^{\\prime} \\boldsymbol{Y}=\\mathbf{1}_{n} \\bar{Y}\n\\]\ncreates an \\(n\\)-vector whose elements are the sample mean \\(\\bar{Y}\\).\nThe projection matrix \\(\\boldsymbol{P}\\) appears frequently in algebraic manipulations in least squares regression. The matrix has the following important properties. Theorem 3.3 The projection matrix \\(\\boldsymbol{P}=\\boldsymbol{X}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\boldsymbol{X}^{\\prime}\\) for any \\(n \\times k \\boldsymbol{X}\\) with \\(n \\geq\\) \\(k\\) has the following algebraic properties.\n\n\\(\\boldsymbol{P}\\) is symmetric \\(\\left(\\boldsymbol{P}^{\\prime}=\\boldsymbol{P}\\right)\\).\n\\(\\boldsymbol{P}\\) is idempotent \\((\\boldsymbol{P P}=\\boldsymbol{P})\\).\n\\(\\operatorname{tr} \\boldsymbol{P}=k\\).\nThe eigenvalues of \\(\\boldsymbol{P}\\) are 1 and 0 .\n\\(\\boldsymbol{P}\\) has \\(k\\) eigenvalues equalling 1 and \\(n-k\\) equalling 0 .\n\\(\\operatorname{rank}(\\boldsymbol{P})=k\\).\n\nWe close this section by proving the claims in Theorem 3.3. Part 1 holds because\n\\[\n\\begin{aligned}\n\\boldsymbol{P}^{\\prime} &=\\left(\\boldsymbol{X}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\boldsymbol{X}^{\\prime}\\right)^{\\prime} \\\\\n&=\\left(\\boldsymbol{X}^{\\prime}\\right)^{\\prime}\\left(\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1}\\right)^{\\prime}(\\boldsymbol{X})^{\\prime} \\\\\n&=\\boldsymbol{X}\\left(\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{\\prime}\\right)^{-1} \\boldsymbol{X}^{\\prime} \\\\\n&=\\boldsymbol{X}\\left((\\boldsymbol{X})^{\\prime}\\left(\\boldsymbol{X}^{\\prime}\\right)^{\\prime}\\right)^{-1} \\boldsymbol{X}^{\\prime}=\\boldsymbol{P} .\n\\end{aligned}\n\\]\nTo establish part 2, the fact that \\(\\boldsymbol{P X}=\\boldsymbol{X}\\) implies that\n\\[\n\\boldsymbol{P} \\boldsymbol{P}=\\boldsymbol{P} \\boldsymbol{X}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\boldsymbol{X}^{\\prime}=\\boldsymbol{X}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\boldsymbol{X}^{\\prime}=\\boldsymbol{P}\n\\]\nas claimed. For part 3 ,\n\\[\n\\operatorname{tr} \\boldsymbol{P}=\\operatorname{tr}\\left(\\boldsymbol{X}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\boldsymbol{X}^{\\prime}\\right)=\\operatorname{tr}\\left(\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)=\\operatorname{tr}\\left(\\boldsymbol{I}_{k}\\right)=k .\n\\]\nSee Appendix A.5 for definition and properties of the trace operator.\nAppendix A.11 shows that part 4 holds for any idempotent matrix. For part 5, since \\(\\operatorname{tr} \\boldsymbol{P}\\) equals the sum of the \\(n\\) eigenvalues and \\(\\operatorname{tr} \\boldsymbol{P}=k\\) by part 3, it follows that there are \\(k\\) eigenvalues equalling 1 and the remainder \\(n-k\\) equalling 0 .\nFor part 6, observe that \\(\\boldsymbol{P}\\) is positive semi-definite because its eigenvalues are all non-negative. By Theorem A.4.5 its rank equals the number of positive eigenvalues, which is \\(k\\) as claimed."
  },
  {
    "objectID": "chpt03-algebra.html#annihilator-matrix",
    "href": "chpt03-algebra.html#annihilator-matrix",
    "title": "3  The Algebra of Least Squares",
    "section": "3.14 Annihilator Matrix",
    "text": "3.14 Annihilator Matrix\nDefine\n\\[\n\\boldsymbol{M}=\\boldsymbol{I}_{n}-\\boldsymbol{P}=\\boldsymbol{I}_{n}-\\boldsymbol{X}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\boldsymbol{X}^{\\prime}\n\\]\nwhere \\(\\boldsymbol{I}_{n}\\) is the \\(n \\times n\\) identity matrix. Note that\n\\[\n\\boldsymbol{M} \\boldsymbol{X}=\\left(\\boldsymbol{I}_{n}-\\boldsymbol{P}\\right) \\boldsymbol{X}=\\boldsymbol{X}-\\boldsymbol{P} \\boldsymbol{X}=\\boldsymbol{X}-\\boldsymbol{X}=0 .\n\\]\nThus \\(\\boldsymbol{M}\\) and \\(\\boldsymbol{X}\\) are orthogonal. We call \\(\\boldsymbol{M}\\) the annihilator matrix due to the property that for any matrix \\(\\boldsymbol{Z}\\) in the range space of \\(\\boldsymbol{X}\\) then\n\\[\nM Z=Z-P Z=0 .\n\\]\nFor example, \\(\\boldsymbol{M} \\boldsymbol{X}_{1}=0\\) for any subcomponent \\(\\boldsymbol{X}_{1}\\) of \\(\\boldsymbol{X}\\), and \\(\\boldsymbol{M P}=0\\) (see Exercise 3.7).\nThe annihilator matrix \\(\\boldsymbol{M}\\) has similar properties with \\(\\boldsymbol{P}\\), including that \\(\\boldsymbol{M}\\) is symmetric \\(\\left(\\boldsymbol{M}^{\\prime}=\\boldsymbol{M}\\right)\\) and idempotent \\((\\boldsymbol{M} M=\\boldsymbol{M})\\). It is thus a projection matrix. Similarly to Theorem 3.3.3 we can calculate\n\\[\n\\operatorname{tr} M=n-k .\n\\]\n(See Exercise 3.9.) One implication is that the rank of \\(\\boldsymbol{M}\\) is \\(n-k\\).\nWhile \\(\\boldsymbol{P}\\) creates fitted values, \\(\\boldsymbol{M}\\) creates least squares residuals:\n\\[\nM Y=Y-P Y=Y-X \\widehat{\\beta}=\\widehat{\\boldsymbol{e}} .\n\\]\nAs discussed in the previous section, a special example of a projection matrix occurs when \\(\\boldsymbol{X}=\\mathbf{1}_{n}\\) is an \\(n\\)-vector of ones, so that \\(\\boldsymbol{P}=\\mathbf{1}_{n}\\left(\\mathbf{1}_{n}^{\\prime} \\mathbf{1}_{n}\\right)^{-1} \\mathbf{1}_{n}^{\\prime}\\). The associated annihilator matrix is\n\\[\n\\boldsymbol{M}=\\boldsymbol{I}_{n}-\\boldsymbol{P}=\\boldsymbol{I}_{n}-\\mathbf{1}_{n}\\left(\\mathbf{1}_{n}^{\\prime} \\mathbf{1}_{n}\\right)^{-1} \\mathbf{1}_{n}^{\\prime} .\n\\]\nWhile \\(\\boldsymbol{P}\\) creates a vector of sample means, \\(\\boldsymbol{M}\\) creates demeaned values:\n\\[\n\\boldsymbol{M Y}=\\boldsymbol{Y}-\\mathbf{1}_{n} \\bar{Y} .\n\\]\nFor simplicity we will often write the right-hand-side as \\(Y-\\bar{Y}\\). The \\(i^{t h}\\) element is \\(Y_{i}-\\bar{Y}\\), the demeaned value of \\(Y_{i}\\)\nWe can also use (3.23) to write an alternative expression for the residual vector. Substituting \\(\\boldsymbol{Y}=\\) \\(\\boldsymbol{X} \\beta+\\boldsymbol{e}\\) into \\(\\widehat{\\boldsymbol{e}}=\\boldsymbol{M} \\boldsymbol{Y}\\) and using \\(\\boldsymbol{M} \\boldsymbol{X}=\\mathbf{0}\\) we find\n\\[\n\\widehat{\\boldsymbol{e}}=\\boldsymbol{M} \\boldsymbol{Y}=\\boldsymbol{M}(\\boldsymbol{X} \\beta+\\boldsymbol{e})=\\boldsymbol{M} \\boldsymbol{e}\n\\]\nwhich is free of dependence on the regression coefficient \\(\\beta\\)."
  },
  {
    "objectID": "chpt03-algebra.html#estimation-of-error-variance",
    "href": "chpt03-algebra.html#estimation-of-error-variance",
    "title": "3  The Algebra of Least Squares",
    "section": "3.15 Estimation of Error Variance",
    "text": "3.15 Estimation of Error Variance\nThe error variance \\(\\sigma^{2}=\\mathbb{E}\\left[e^{2}\\right]\\) is a moment, so a natural estimator is a moment estimator. If \\(e_{i}\\) were observed we would estimate \\(\\sigma^{2}\\) by\n\\[\n\\widetilde{\\sigma}^{2}=\\frac{1}{n} \\sum_{i=1}^{n} e_{i}^{2} .\n\\]\nHowever, this is infeasible as \\(e_{i}\\) is not observed. In this case it is common to take a two-step approach to estimation. The residuals \\(\\widehat{e}_{i}\\) are calculated in the first step, and then we substitute \\(\\widehat{e}_{i}\\) for \\(e_{i}\\) in expression (3.25) to obtain the feasible estimator\n\\[\n\\widehat{\\sigma}^{2}=\\frac{1}{n} \\sum_{i=1}^{n} \\widehat{e}_{i}^{2} .\n\\]\nIn matrix notation, we can write (3.25) and (3.26) as \\(\\widetilde{\\sigma}^{2}=n^{-1} \\boldsymbol{e}^{\\prime} \\boldsymbol{e}\\) and\n\\[\n\\widehat{\\sigma}^{2}=n^{-1} \\widehat{\\boldsymbol{e}}^{\\prime} \\widehat{\\boldsymbol{e}} .\n\\]\nRecall the expressions \\(\\widehat{\\boldsymbol{e}}=\\boldsymbol{M} \\boldsymbol{Y}=\\boldsymbol{M} \\boldsymbol{e}\\) from (3.23) and (3.24). Applied to (3.27) we find\n\\[\n\\widehat{\\sigma}^{2}=n^{-1} \\widehat{\\boldsymbol{e}}^{\\prime} \\widehat{\\boldsymbol{e}}=n^{-1} \\boldsymbol{e}^{\\prime} \\boldsymbol{M M} \\boldsymbol{M}=n^{-1} \\boldsymbol{e}^{\\prime} \\boldsymbol{M} \\boldsymbol{e}\n\\]\nthe third equality because \\(M M=M\\).\nAn interesting implication is that\n\\[\n\\widetilde{\\sigma}^{2}-\\widehat{\\sigma}^{2}=n^{-1} \\boldsymbol{e}^{\\prime} \\boldsymbol{e}-n^{-1} \\boldsymbol{e}^{\\prime} \\boldsymbol{M} \\boldsymbol{e}=n^{-1} \\boldsymbol{e}^{\\prime} \\boldsymbol{P} \\boldsymbol{e} \\geq 0 .\n\\]\nThe final inequality holds because \\(\\boldsymbol{P}\\) is positive semi-definite and \\(\\boldsymbol{e}^{\\prime} \\boldsymbol{P} \\boldsymbol{e}\\) is a quadratic form. This shows that the feasible estimator \\(\\widehat{\\sigma}^{2}\\) is numerically smaller than the idealized estimator (3.25)."
  },
  {
    "objectID": "chpt03-algebra.html#analysis-of-variance",
    "href": "chpt03-algebra.html#analysis-of-variance",
    "title": "3  The Algebra of Least Squares",
    "section": "3.16 Analysis of Variance",
    "text": "3.16 Analysis of Variance\nAnother way of writing (3.23) is\n\\[\n\\boldsymbol{Y}=\\boldsymbol{P} \\boldsymbol{Y}+\\boldsymbol{M} \\boldsymbol{Y}=\\widehat{\\boldsymbol{Y}}+\\widehat{\\boldsymbol{e}} .\n\\]\nThis decomposition is orthogonal, that is\n\\[\n\\widehat{\\boldsymbol{Y}}^{\\prime} \\widehat{\\boldsymbol{e}}=(\\boldsymbol{P} \\boldsymbol{Y})^{\\prime}(\\boldsymbol{M} \\boldsymbol{Y})=\\boldsymbol{Y}^{\\prime} \\boldsymbol{P} \\boldsymbol{M} \\boldsymbol{Y}=0 .\n\\]\nIt follows that\n\\[\n\\boldsymbol{Y}^{\\prime} \\boldsymbol{Y}=\\widehat{\\boldsymbol{Y}}^{\\prime} \\widehat{\\boldsymbol{Y}}+2 \\widehat{\\boldsymbol{Y}}^{\\prime} \\widehat{\\boldsymbol{e}}+\\widehat{\\boldsymbol{e}}^{\\prime} \\widehat{\\boldsymbol{e}}=\\widehat{\\boldsymbol{Y}}^{\\prime} \\widehat{\\boldsymbol{Y}}+\\widehat{\\boldsymbol{e}}^{\\prime} \\widehat{\\boldsymbol{e}}\n\\]\nor\n\\[\n\\sum_{i=1}^{n} Y_{i}^{2}=\\sum_{i=1}^{n} \\widehat{Y}_{i}^{2}+\\sum_{i=1}^{n} \\widehat{e}_{i}^{2}\n\\]\nSubtracting \\(\\bar{Y}\\) from both sides of (3.29) we obtain\n\\[\n\\boldsymbol{Y}-\\mathbf{1}_{n} \\bar{Y}=\\widehat{\\boldsymbol{Y}}-\\mathbf{1}_{n} \\bar{Y}+\\widehat{\\boldsymbol{e}}\n\\]\nThis decomposition is also orthogonal when \\(X\\) contains a constant, as\n\\[\n\\left(\\widehat{\\boldsymbol{Y}}-\\mathbf{1}_{n} \\bar{Y}\\right)^{\\prime} \\widehat{\\boldsymbol{e}}=\\widehat{\\boldsymbol{Y}}^{\\prime} \\widehat{\\boldsymbol{e}}-\\bar{Y} \\mathbf{1}_{n}^{\\prime} \\widehat{\\boldsymbol{e}}=0\n\\]\nunder (3.17). It follows that\n\\[\n\\left(\\boldsymbol{Y}-\\mathbf{1}_{n} \\bar{Y}\\right)^{\\prime}\\left(\\boldsymbol{Y}-\\mathbf{1}_{n} \\bar{Y}\\right)=\\left(\\widehat{\\boldsymbol{Y}}-\\mathbf{1}_{n} \\bar{Y}\\right)^{\\prime}\\left(\\widehat{\\boldsymbol{Y}}-\\mathbf{1}_{n} \\bar{Y}\\right)+\\widehat{\\boldsymbol{e}}^{\\prime} \\widehat{\\boldsymbol{e}}\n\\]\nor\n\\[\n\\sum_{i=1}^{n}\\left(Y_{i}-\\bar{Y}\\right)^{2}=\\sum_{i=1}^{n}\\left(\\widehat{Y}_{i}-\\bar{Y}\\right)^{2}+\\sum_{i=1}^{n} \\widehat{e}_{i}^{2} .\n\\]\nThis is commonly called the analysis-of-variance formula for least squares regression.\nA commonly reported statistic is the coefficient of determination or R-squared:\n\\[\nR^{2}=\\frac{\\sum_{i=1}^{n}\\left(\\widehat{Y}_{i}-\\bar{Y}\\right)^{2}}{\\sum_{i=1}^{n}\\left(Y_{i}-\\bar{Y}\\right)^{2}}=1-\\frac{\\sum_{i=1}^{n} \\widehat{e}_{i}^{2}}{\\sum_{i=1}^{n}\\left(Y_{i}-\\bar{Y}\\right)^{2}} .\n\\]\nIt is often described as “the fraction of the sample variance of \\(Y\\) which is explained by the least squares fit”. \\(R^{2}\\) is a crude measure of regression fit. We have better measures of fit, but these require a statistical (not just algebraic) analysis and we will return to these issues later. One deficiency with \\(R^{2}\\) is that it increases when regressors are added to a regression (see Exercise 3.16) so the “fit” can be always increased by increasing the number of regressors.\nThe coefficient of determination was introduced by Wright (1921)."
  },
  {
    "objectID": "chpt03-algebra.html#projections",
    "href": "chpt03-algebra.html#projections",
    "title": "3  The Algebra of Least Squares",
    "section": "3.17 Projections",
    "text": "3.17 Projections\nOne way to visualize least squares fitting is as a projection operation.\nWrite the regressor matrix as \\(\\boldsymbol{X}=\\left[\\boldsymbol{X}_{1} \\boldsymbol{X}_{2} \\ldots \\boldsymbol{X}_{k}\\right]\\) where \\(\\boldsymbol{X}_{j}\\) is the \\(j^{t h}\\) column of \\(\\boldsymbol{X}\\). The range space \\(\\mathscr{R}(\\boldsymbol{X})\\) of \\(\\boldsymbol{X}\\) is the space consisting of all linear combinations of the columns \\(\\boldsymbol{X}_{1}, \\boldsymbol{X}_{2}, \\ldots, \\boldsymbol{X}_{k} . \\mathscr{R}(\\boldsymbol{X})\\) is a \\(k\\) dimensional surface contained in \\(\\mathbb{R}^{n}\\). If \\(k=2\\) then \\(\\mathscr{R}(\\boldsymbol{X})\\) is a plane. The operator \\(\\boldsymbol{P}=\\boldsymbol{X}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\boldsymbol{X}^{\\prime}\\) projects vectors onto \\(\\mathscr{R}(\\boldsymbol{X})\\). The fitted values \\(\\widehat{\\boldsymbol{Y}}=\\boldsymbol{P} \\boldsymbol{Y}\\) are the projection of \\(\\boldsymbol{Y}\\) onto \\(\\mathscr{R}(\\boldsymbol{X})\\).\nTo visualize examine Figure 3.4. This displays the case \\(n=3\\) and \\(k=2\\). Displayed are three vectors \\(\\boldsymbol{Y}, \\boldsymbol{X}_{1}\\), and \\(\\boldsymbol{X}_{2}\\), which are each elements of \\(\\mathbb{R}^{3}\\). The plane created by \\(\\boldsymbol{X}_{1}\\) and \\(\\boldsymbol{X}_{2}\\) is the range space \\(\\mathscr{R}(\\boldsymbol{X})\\). Regression fitted values are linear combinations of \\(\\boldsymbol{X}_{1}\\) and \\(\\boldsymbol{X}_{2}\\) and so lie on this plane. The fitted value \\(\\widehat{\\boldsymbol{Y}}\\) is the vector on this plane closest to \\(\\boldsymbol{Y}\\). The residual \\(\\widehat{\\boldsymbol{e}}=\\boldsymbol{Y}-\\widehat{\\boldsymbol{Y}}\\) is the difference between the two. The angle between the vectors \\(\\widehat{\\boldsymbol{Y}}\\) and \\(\\widehat{\\boldsymbol{e}}\\) is \\(90^{\\circ}\\), and therefore they are orthogonal as shown.\n\nFigure 3.4: Projection of \\(\\boldsymbol{Y}\\) onto \\(\\boldsymbol{X}_{1}\\) and \\(\\boldsymbol{X}_{2}\\)"
  },
  {
    "objectID": "chpt03-algebra.html#regression-components",
    "href": "chpt03-algebra.html#regression-components",
    "title": "3  The Algebra of Least Squares",
    "section": "3.18 Regression Components",
    "text": "3.18 Regression Components\nPartition \\(\\boldsymbol{X}=\\left[\\begin{array}{ll}\\boldsymbol{X}_{1} & \\boldsymbol{X}_{2}\\end{array}\\right]\\) and \\(\\beta=\\left(\\beta_{1}, \\beta_{2}\\right)\\). The regression model can be written as\n\\[\n\\boldsymbol{Y}=\\boldsymbol{X}_{1} \\beta_{1}+\\boldsymbol{X}_{2} \\beta_{2}+\\boldsymbol{e} .\n\\]\nThe OLS estimator of \\(\\beta=\\left(\\beta_{1}^{\\prime}, \\beta_{2}^{\\prime}\\right)^{\\prime}\\) is obtained by regression of \\(\\boldsymbol{Y}\\) on \\(\\boldsymbol{X}=\\left[\\boldsymbol{X}_{1} \\boldsymbol{X}_{2}\\right]\\) and can be written as\n\\[\n\\boldsymbol{Y}=\\boldsymbol{X} \\widehat{\\beta}+\\widehat{\\boldsymbol{e}}=\\boldsymbol{X}_{1} \\widehat{\\boldsymbol{\\beta}}_{1}+\\boldsymbol{X}_{2} \\widehat{\\boldsymbol{\\beta}}_{2}+\\widehat{\\boldsymbol{e}} .\n\\]\nWe are interested in algebraic expressions for \\(\\widehat{\\beta}_{1}\\) and \\(\\widehat{\\beta}_{2}\\).\nLet’s first focus on \\(\\widehat{\\beta}_{1}\\). The least squares estimator by definition is found by the joint minimization\n\\[\n\\left(\\widehat{\\beta}_{1}, \\widehat{\\beta}_{2}\\right)=\\underset{\\beta_{1}, \\beta_{2}}{\\operatorname{argmin}} \\operatorname{SSE}\\left(\\beta_{1}, \\beta_{2}\\right)\n\\]\nwhere\n\\[\n\\operatorname{SSE}\\left(\\beta_{1}, \\beta_{2}\\right)=\\left(\\boldsymbol{Y}-\\boldsymbol{X}_{1} \\beta_{1}-\\boldsymbol{X}_{2} \\beta_{2}\\right)^{\\prime}\\left(\\boldsymbol{Y}-\\boldsymbol{X}_{1} \\beta_{1}-\\boldsymbol{X}_{2} \\beta_{2}\\right) .\n\\]\nAn equivalent expression for \\(\\widehat{\\beta}_{1}\\) can be obtained by concentration (nested minimization). The solution (3.33) can be written as\n\\[\n\\widehat{\\beta}_{1}=\\underset{\\beta_{1}}{\\operatorname{argmin}}\\left(\\min _{\\beta_{2}} \\operatorname{SSE}\\left(\\beta_{1}, \\beta_{2}\\right)\\right) .\n\\]\nThe inner expression \\(\\min _{\\beta_{2}} \\operatorname{SSE}\\left(\\beta_{1}, \\beta_{2}\\right)\\) minimizes over \\(\\beta_{2}\\) while holding \\(\\beta_{1}\\) fixed. It is the lowest possible sum of squared errors given \\(\\beta_{1}\\). The outer minimization \\(\\operatorname{argmin}_{\\beta_{1}}\\) finds the coefficient \\(\\beta_{1}\\) which minimizes the “lowest possible sum of squared errors given \\(\\beta_{1}\\)”. This means that \\(\\widehat{\\beta}_{1}\\) as defined in (3.33) and (3.34) are algebraically identical.\nExamine the inner minimization problem in (3.34). This is simply the least squares regression of \\(\\boldsymbol{Y}-\\boldsymbol{X}_{1} \\beta_{1}\\) on \\(\\boldsymbol{X}_{2}\\). This has solution\n\\[\n\\underset{\\beta_{2}}{\\operatorname{argmin}} \\operatorname{SSE}\\left(\\beta_{1}, \\beta_{2}\\right)=\\left(\\boldsymbol{X}_{2}^{\\prime} \\boldsymbol{X}_{2}\\right)^{-1}\\left(\\boldsymbol{X}_{2}^{\\prime}\\left(\\boldsymbol{Y}-\\boldsymbol{X}_{1} \\beta_{1}\\right)\\right)\n\\]\nwith residuals\n\\[\n\\begin{aligned}\n\\boldsymbol{Y}-\\boldsymbol{X}_{1} \\beta_{1}-\\boldsymbol{X}_{2}\\left(\\boldsymbol{X}_{2}^{\\prime} \\boldsymbol{X}_{2}\\right)^{-1}\\left(\\boldsymbol{X}_{2}^{\\prime}\\left(\\boldsymbol{Y}-\\boldsymbol{X}_{1} \\beta_{1}\\right)\\right) &=\\left(\\boldsymbol{M}_{2} \\boldsymbol{Y}-\\boldsymbol{M}_{2} \\boldsymbol{X}_{1} \\beta_{1}\\right) \\\\\n&=\\boldsymbol{M}_{2}\\left(\\boldsymbol{Y}-\\boldsymbol{X}_{1} \\beta_{1}\\right)\n\\end{aligned}\n\\]\nwhere\n\\[\n\\boldsymbol{M}_{2}=\\boldsymbol{I}_{n}-\\boldsymbol{X}_{2}\\left(\\boldsymbol{X}_{2}^{\\prime} \\boldsymbol{X}_{2}\\right)^{-1} \\boldsymbol{X}_{2}^{\\prime}\n\\]\nis the annihilator matrix for \\(\\boldsymbol{X}_{2}\\). This means that the inner minimization problem (3.34) has minimized value\n\\[\n\\begin{aligned}\n\\min _{\\beta_{2}} \\operatorname{SSE}\\left(\\beta_{1}, \\beta_{2}\\right) &=\\left(\\boldsymbol{Y}-\\boldsymbol{X}_{1} \\beta_{1}\\right)^{\\prime} \\boldsymbol{M}_{2} \\boldsymbol{M}_{2}\\left(\\boldsymbol{Y}-\\boldsymbol{X}_{1} \\beta_{1}\\right) \\\\\n&=\\left(\\boldsymbol{Y}-\\boldsymbol{X}_{1} \\beta_{1}\\right)^{\\prime} \\boldsymbol{M}_{2}\\left(\\boldsymbol{Y}-\\boldsymbol{X}_{1} \\beta_{1}\\right)\n\\end{aligned}\n\\]\nwhere the second equality holds because \\(\\boldsymbol{M}_{2}\\) is idempotent. Substituting this into (3.34) we find\n\\[\n\\begin{aligned}\n\\widehat{\\beta}_{1} &=\\underset{\\beta_{1}}{\\operatorname{argmin}}\\left(\\boldsymbol{Y}-\\boldsymbol{X}_{1} \\beta_{1}\\right)^{\\prime} \\boldsymbol{M}_{2}\\left(\\boldsymbol{Y}-\\boldsymbol{X}_{1} \\beta_{1}\\right) \\\\\n&=\\left(\\boldsymbol{X}_{1}^{\\prime} \\boldsymbol{M}_{2} \\boldsymbol{X}_{1}\\right)^{-1}\\left(\\boldsymbol{X}_{1}^{\\prime} \\boldsymbol{M}_{2} \\boldsymbol{Y}\\right) .\n\\end{aligned}\n\\]\nBy a similar argument we find\n\\[\n\\widehat{\\beta}_{2}=\\left(\\boldsymbol{X}_{2}^{\\prime} \\boldsymbol{M}_{1} \\boldsymbol{X}_{2}\\right)^{-1}\\left(\\boldsymbol{X}_{2}^{\\prime} \\boldsymbol{M}_{1} \\boldsymbol{Y}\\right)\n\\]\nwhere\n\\[\n\\boldsymbol{M}_{1}=\\boldsymbol{I}_{n}-\\boldsymbol{X}_{1}\\left(\\boldsymbol{X}_{1}^{\\prime} \\boldsymbol{X}_{1}\\right)^{-1} \\boldsymbol{X}_{1}^{\\prime}\n\\]\nis the annihilator matrix for \\(\\boldsymbol{X}_{1}\\). Theorem 3.4 The least squares estimator \\(\\left(\\widehat{\\beta}_{1}, \\widehat{\\beta}_{2}\\right)\\) for (3.32) has the algebraic solution\n\\[\n\\begin{aligned}\n&\\widehat{\\beta}_{1}=\\left(\\boldsymbol{X}_{1}^{\\prime} \\boldsymbol{M}_{2} \\boldsymbol{X}_{1}\\right)^{-1}\\left(\\boldsymbol{X}_{1}^{\\prime} \\boldsymbol{M}_{2} \\boldsymbol{Y}\\right) \\\\\n&\\widehat{\\beta}_{2}=\\left(\\boldsymbol{X}_{2}^{\\prime} \\boldsymbol{M}_{1} \\boldsymbol{X}_{2}\\right)^{-1}\\left(\\boldsymbol{X}_{2}^{\\prime} \\boldsymbol{M}_{1} \\boldsymbol{Y}\\right)\n\\end{aligned}\n\\]\nwhere \\(\\boldsymbol{M}_{1}\\) and \\(\\boldsymbol{M}_{2}\\) are defined in (3.36) and (3.35), respectively."
  },
  {
    "objectID": "chpt03-algebra.html#regression-components-alternative-derivation",
    "href": "chpt03-algebra.html#regression-components-alternative-derivation",
    "title": "3  The Algebra of Least Squares",
    "section": "3.19 Regression Components (Alternative Derivation)*",
    "text": "3.19 Regression Components (Alternative Derivation)*\nAn alternative proof of Theorem \\(3.4\\) uses an algebraic argument based on the population calculations from Section 2.22. Since this is a classic derivation we present it here for completeness.\nPartition \\(\\widehat{\\boldsymbol{Q}}_{X X}\\) as\n\\[\n\\widehat{\\boldsymbol{Q}}_{X X}=\\left[\\begin{array}{ll}\n\\widehat{\\boldsymbol{Q}}_{11} & \\widehat{\\boldsymbol{Q}}_{12} \\\\\n\\widehat{\\boldsymbol{Q}}_{21} & \\widehat{\\boldsymbol{Q}}_{22}\n\\end{array}\\right]=\\left[\\begin{array}{ll}\n\\frac{1}{n} \\boldsymbol{X}_{1}^{\\prime} \\boldsymbol{X}_{1} & \\frac{1}{n} \\boldsymbol{X}_{1}^{\\prime} \\boldsymbol{X}_{2} \\\\\n\\frac{1}{n} \\boldsymbol{X}_{2}^{\\prime} \\boldsymbol{X}_{1} & \\frac{1}{n} \\boldsymbol{X}_{2}^{\\prime} \\boldsymbol{X}_{2}\n\\end{array}\\right]\n\\]\nand similarly \\(\\widehat{\\boldsymbol{Q}}_{X Y}\\) as\n\\[\n\\widehat{\\boldsymbol{Q}}_{X Y}=\\left[\\begin{array}{l}\n\\widehat{\\boldsymbol{Q}}_{1 Y} \\\\\n\\widehat{\\boldsymbol{Q}}_{2 Y}\n\\end{array}\\right]=\\left[\\begin{array}{c}\n\\frac{1}{n} \\boldsymbol{X}_{1}^{\\prime} \\boldsymbol{Y} \\\\\n\\frac{1}{n} \\boldsymbol{X}_{2}^{\\prime} \\boldsymbol{Y}\n\\end{array}\\right]\n\\]\nBy the partitioned matrix inversion formula (A.3)\n\\[\n\\widehat{\\boldsymbol{Q}}_{X X}^{-1}=\\left[\\begin{array}{ll}\n\\widehat{\\boldsymbol{Q}}_{11} & \\widehat{\\boldsymbol{Q}}_{12} \\\\\n\\widehat{\\boldsymbol{Q}}_{21} & \\widehat{\\boldsymbol{Q}}_{22}\n\\end{array}\\right]^{-1} \\stackrel{\\operatorname{def}}{=}\\left[\\begin{array}{cc}\n\\widehat{\\boldsymbol{Q}}^{11} & \\widehat{\\boldsymbol{Q}}^{12} \\\\\n\\widehat{\\boldsymbol{Q}}^{21} & \\widehat{\\boldsymbol{Q}}^{22}\n\\end{array}\\right]=\\left[\\begin{array}{cc}\n\\widehat{\\boldsymbol{Q}}_{11 \\cdot 2}^{-1} & -\\widehat{\\boldsymbol{Q}}_{11 \\cdot 2}^{-1} \\widehat{\\boldsymbol{Q}}_{12} \\widehat{\\boldsymbol{Q}}_{22}^{-1} \\\\\n-\\widehat{\\boldsymbol{Q}}_{22 \\cdot 1}^{-1} \\widehat{\\boldsymbol{Q}}_{21} \\widehat{\\boldsymbol{Q}}_{11}^{-1} & \\widehat{\\boldsymbol{Q}}_{22 \\cdot 1}^{-1}\n\\end{array}\\right]\n\\]\nwhere \\(\\widehat{\\boldsymbol{Q}}_{11 \\cdot 2}=\\widehat{\\boldsymbol{Q}}_{11}-\\widehat{\\boldsymbol{Q}}_{12} \\widehat{\\boldsymbol{Q}}_{22}^{-1} \\widehat{\\boldsymbol{Q}}_{21}\\) and \\(\\widehat{\\boldsymbol{Q}}_{22 \\cdot 1}=\\widehat{\\boldsymbol{Q}}_{22}-\\widehat{\\boldsymbol{Q}}_{21} \\widehat{\\boldsymbol{Q}}_{11}^{-1} \\widehat{\\boldsymbol{Q}}_{12}\\). Thus\n\\[\n\\begin{aligned}\n\\widehat{\\beta} &=\\left(\\begin{array}{c}\n\\widehat{\\beta}_{1} \\\\\n\\widehat{\\beta}_{2}\n\\end{array}\\right) \\\\\n&=\\left[\\begin{array}{cc}\n\\widehat{\\boldsymbol{Q}}_{11 \\cdot 2}^{-1} & -\\widehat{\\boldsymbol{Q}}_{11 \\cdot 2}^{-1} \\widehat{\\boldsymbol{Q}}_{12} \\widehat{\\boldsymbol{Q}}_{22}^{-1} \\\\\n-\\widehat{\\boldsymbol{Q}}_{22 \\cdot 1}^{-1} \\widehat{\\boldsymbol{Q}}_{21} \\widehat{\\boldsymbol{Q}}_{11}^{-1} & \\widehat{\\boldsymbol{Q}}_{22 \\cdot 1}^{-1}\n\\end{array}\\right]\\left[\\begin{array}{c}\n\\widehat{\\boldsymbol{Q}}_{1 Y} \\\\\n\\widehat{\\boldsymbol{Q}}_{2 Y}\n\\end{array}\\right] \\\\\n&=\\left(\\begin{array}{c}\n\\widehat{\\mathbf{Q}}_{11 \\cdot 2}^{-1} \\widehat{\\boldsymbol{Q}}_{1 Y \\cdot 2} \\\\\n\\widehat{\\mathbf{Q}}_{22 \\cdot 1}^{-1} \\widehat{\\mathbf{Q}}_{2 Y \\cdot 1}\n\\end{array}\\right)\n\\end{aligned}\n\\]\nNow\n\\[\n\\begin{aligned}\n\\widehat{\\boldsymbol{Q}}_{11 \\cdot 2} &=\\widehat{\\boldsymbol{Q}}_{11}-\\widehat{\\boldsymbol{Q}}_{12} \\widehat{\\boldsymbol{Q}}_{22}^{-1} \\widehat{\\boldsymbol{Q}}_{21} \\\\\n&=\\frac{1}{n} \\boldsymbol{X}_{1}^{\\prime} \\boldsymbol{X}_{1}-\\frac{1}{n} \\boldsymbol{X}_{1}^{\\prime} \\boldsymbol{X}_{2}\\left(\\frac{1}{n} \\boldsymbol{X}_{2}^{\\prime} \\boldsymbol{X}_{2}\\right)^{-1} \\frac{1}{n} \\boldsymbol{X}_{2}^{\\prime} \\boldsymbol{X}_{1} \\\\\n&=\\frac{1}{n} \\boldsymbol{X}_{1}^{\\prime} \\boldsymbol{M}_{2} \\boldsymbol{X}_{1}\n\\end{aligned}\n\\]\nand\n\\[\n\\begin{aligned}\n\\widehat{\\boldsymbol{Q}}_{1 y \\cdot 2} &=\\widehat{\\boldsymbol{Q}}_{1 Y}-\\widehat{\\boldsymbol{Q}}_{12} \\widehat{\\boldsymbol{Q}}_{22}^{-1} \\widehat{\\boldsymbol{Q}}_{2 Y} \\\\\n&=\\frac{1}{n} \\boldsymbol{X}_{1}^{\\prime} \\boldsymbol{Y}-\\frac{1}{n} \\boldsymbol{X}_{1}^{\\prime} \\boldsymbol{X}_{2}\\left(\\frac{1}{n} \\boldsymbol{X}_{2}^{\\prime} \\boldsymbol{X}_{2}\\right)^{-1} \\frac{1}{n} \\boldsymbol{X}_{2}^{\\prime} \\boldsymbol{Y} \\\\\n&=\\frac{1}{n} \\boldsymbol{X}_{1}^{\\prime} \\boldsymbol{M}_{2} \\boldsymbol{Y} .\n\\end{aligned}\n\\]\nEquation (3.38) follows.\nSimilarly to the calculation for \\(\\widehat{\\boldsymbol{Q}}_{11 \\cdot 2}\\) and \\(\\widehat{\\boldsymbol{Q}}_{1 Y \\cdot 2}\\) you can show that \\(\\widehat{\\boldsymbol{Q}}_{2 Y \\cdot 1}=\\frac{1}{n} \\boldsymbol{X}_{2}^{\\prime} \\boldsymbol{M}_{1} \\boldsymbol{Y}\\) and \\(\\widehat{\\boldsymbol{Q}}_{22 \\cdot 1}=\\) \\(\\frac{1}{n} \\boldsymbol{X}_{2}^{\\prime} \\boldsymbol{M}_{1} \\boldsymbol{X}_{2}\\). This establishes (3.37). Together, this is Theorem 3.4."
  },
  {
    "objectID": "chpt03-algebra.html#residual-regression",
    "href": "chpt03-algebra.html#residual-regression",
    "title": "3  The Algebra of Least Squares",
    "section": "3.20 Residual Regression",
    "text": "3.20 Residual Regression\nAs first recognized by Frisch and Waugh (1933) and extended by Lovell (1963), expressions (3.37) and (3.38) can be used to show that the least squares estimators \\(\\widehat{\\beta}_{1}\\) and \\(\\widehat{\\beta}_{2}\\) can be found by a two-step regression procedure.\nTake (3.38). Since \\(\\boldsymbol{M}_{1}\\) is idempotent, \\(\\boldsymbol{M}_{1}=\\boldsymbol{M}_{1} \\boldsymbol{M}_{1}\\) and thus\n\\[\n\\begin{aligned}\n\\widehat{\\beta}_{2} &=\\left(\\boldsymbol{X}_{2}^{\\prime} \\boldsymbol{M}_{1} \\boldsymbol{X}_{2}\\right)^{-1}\\left(\\boldsymbol{X}_{2}^{\\prime} \\boldsymbol{M}_{1} \\boldsymbol{Y}\\right) \\\\\n&=\\left(\\boldsymbol{X}_{2}^{\\prime} \\boldsymbol{M}_{1} \\boldsymbol{M}_{1} \\boldsymbol{X}_{2}\\right)^{-1}\\left(\\boldsymbol{X}_{2}^{\\prime} \\boldsymbol{M}_{1} \\boldsymbol{M}_{1} \\boldsymbol{Y}\\right) \\\\\n&=\\left(\\widetilde{\\boldsymbol{X}}_{2}^{\\prime} \\widetilde{\\boldsymbol{X}}_{2}\\right)^{-1}\\left(\\widetilde{\\boldsymbol{X}}_{2}^{\\prime} \\widetilde{\\boldsymbol{e}}_{1}\\right)\n\\end{aligned}\n\\]\nwhere \\(\\widetilde{\\boldsymbol{X}}_{2}=\\boldsymbol{M}_{1} \\boldsymbol{X}_{2}\\) and \\(\\widetilde{\\boldsymbol{e}}_{1}=\\boldsymbol{M}_{1} \\boldsymbol{Y}\\).\nThus the coefficient estimator \\(\\widehat{\\beta}_{2}\\) is algebraically equal to the least squares regression of \\(\\widetilde{\\boldsymbol{e}}_{1}\\) on \\(\\widetilde{\\boldsymbol{X}}_{2}\\). Notice that these two are \\(\\boldsymbol{Y}\\) and \\(\\boldsymbol{X}_{2}\\), respectively, premultiplied by \\(\\boldsymbol{M}_{1}\\). But we know that pre-multiplication by \\(\\boldsymbol{M}_{1}\\) creates least squares residuals. Therefore \\(\\widetilde{\\boldsymbol{e}}_{1}\\) is simply the least squares residual from a regression of \\(\\boldsymbol{Y}\\) on \\(\\boldsymbol{X}_{1}\\), and the columns of \\(\\widetilde{\\boldsymbol{X}}_{2}\\) are the least squares residuals from the regressions of the columns of \\(\\boldsymbol{X}_{2}\\) on \\(\\boldsymbol{X}_{1}\\).\nWe have proven the following theorem.\nTheorem 3.5 Frisch-Waugh-Lovell (FWL)\nIn the model (3.31), the OLS estimator of \\(\\beta_{2}\\) and the OLS residuals \\(\\widehat{\\boldsymbol{e}}\\) may be computed by either the OLS regression (3.32) or via the following algorithm:\n\nRegress \\(\\boldsymbol{Y}\\) on \\(\\boldsymbol{X}_{1}\\), obtain residuals \\(\\widetilde{\\boldsymbol{e}}_{1}\\);\nRegress \\(\\boldsymbol{X}_{2}\\) on \\(\\boldsymbol{X}_{1}\\), obtain residuals \\(\\widetilde{\\boldsymbol{X}}_{2}\\);\nRegress \\(\\widetilde{\\boldsymbol{e}}_{1}\\) on \\(\\widetilde{\\boldsymbol{X}}_{2}\\), obtain OLS estimates \\(\\widehat{\\beta}_{2}\\) and residuals \\(\\widehat{\\boldsymbol{e}}\\).\n\nIn some contexts (such as panel data models, to be introduced in Chapter 17), the FWL theorem can be used to greatly speed computation.\nThe FWL theorem is a direct analog of the coefficient representation obtained in Section 2.23. The result obtained in that section concerned the population projection coefficients; the result obtained here concern the least squares estimators. The key message is the same. In the least squares regression (3.32) the estimated coefficient \\(\\widehat{\\beta}_{2}\\) algebraically equals the regression of \\(\\boldsymbol{Y}\\) on the regressors \\(\\boldsymbol{X}_{2}\\) after the regressors \\(X_{1}\\) have been linearly projected out. Similarly, the coefficient estimate \\(\\widehat{\\beta}_{1}\\) algebraically equals the regression of \\(\\boldsymbol{Y}\\) on the regressors \\(\\boldsymbol{X}_{1}\\) after the regressors \\(\\boldsymbol{X}_{2}\\) have been linearly projected out. This result can be insightful when interpreting regression coefficients.\nA common application of the FWL theorem is the demeaning formula for regression obtained in (3.18). Partition \\(\\boldsymbol{X}=\\left[\\boldsymbol{X}_{1} \\boldsymbol{X}_{2}\\right]\\) where \\(\\boldsymbol{X}_{1}=\\mathbf{1}_{n}\\) is a vector of ones and \\(\\boldsymbol{X}_{2}\\) is a matrix of observed regressors. In this case \\(\\boldsymbol{M}_{1}=\\boldsymbol{I}_{n}-\\mathbf{1}_{n}\\left(\\mathbf{1}_{n}^{\\prime} \\mathbf{1}_{n}\\right)^{-1} \\mathbf{1}_{n}^{\\prime}\\). Observe that \\(\\widetilde{\\boldsymbol{X}}_{2}=\\boldsymbol{M}_{1} \\boldsymbol{X}_{2}=\\boldsymbol{X}_{2}-\\overline{\\boldsymbol{X}}_{2}\\) and \\(\\boldsymbol{M}_{1} \\boldsymbol{Y}=\\boldsymbol{Y}-\\overline{\\boldsymbol{Y}}\\) are the “demeaned” variables. The FWL theorem says that \\(\\widehat{\\beta}_{2}\\) is the OLS estimate from a regression of \\(Y_{i}-\\bar{Y}\\) on \\(X_{2 i}-\\bar{X}_{2}\\) :\n\\[\n\\widehat{\\beta}_{2}=\\left(\\sum_{i=1}^{n}\\left(X_{2 i}-\\bar{X}_{2}\\right)\\left(X_{2 i}-\\bar{X}_{2}\\right)^{\\prime}\\right)^{-1}\\left(\\sum_{i=1}^{n}\\left(X_{2 i}-\\bar{X}_{2}\\right)\\left(Y_{i}-\\bar{Y}\\right)\\right)\n\\]\nThis is (3.18).\nRagnar Frisch\\ Ragnar Frisch (1895-1973) was co-winner with Jan Tinbergen of the first No-\\ bel Memorial Prize in Economic Sciences in 1969 for their work in developing\\ and applying dynamic models for the analysis of economic problems. Frisch\\ made a number of foundational contributions to modern economics beyond the\\ Frisch-Waugh-Lovell Theorem, including formalizing consumer theory, produc-\\ tion theory, and business cycle theory."
  },
  {
    "objectID": "chpt03-algebra.html#leverage-values",
    "href": "chpt03-algebra.html#leverage-values",
    "title": "3  The Algebra of Least Squares",
    "section": "3.21 Leverage Values",
    "text": "3.21 Leverage Values\nThe leverage values for the regressor matrix \\(\\boldsymbol{X}\\) are the diagonal elements of the projection matrix \\(\\boldsymbol{P}=\\boldsymbol{X}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\boldsymbol{X}^{\\prime}\\). There are \\(n\\) leverage values, and are typically written as \\(h_{i i}\\) for \\(i=1, \\ldots, n\\). Since\n\\[\n\\boldsymbol{P}=\\left(\\begin{array}{c}\nX_{1}^{\\prime} \\\\\nX_{2}^{\\prime} \\\\\n\\vdots \\\\\nX_{n}^{\\prime}\n\\end{array}\\right)\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1}\\left(\\begin{array}{llll}\nX_{1} & X_{2} & \\cdots & X_{n}\n\\end{array}\\right)\n\\]\nthey are\n\\[\nh_{i i}=X_{i}^{\\prime}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} X_{i} .\n\\]\nThe leverage value \\(h_{i i}\\) is a normalized length of the observed regressor vector \\(X_{i}\\). They appear frequently in the algebraic and statistical analysis of least squares regression, including leave-one-out regression, influential observations, robust covariance matrix estimation, and cross-validation.\nA few properties of the leverage values are now listed.\nTheorem 3.6\n\n\\(0 \\leq h_{i i} \\leq 1\\).\n\\(h_{i i} \\geq 1 / n\\) if \\(X\\) includes an intercept.\n\\(\\sum_{i=1}^{n} h_{i i}=k\\).\n\nWe prove Theorem \\(3.6\\) below.\nThe leverage value \\(h_{i i}\\) measures how unusual the \\(i^{t h}\\) observation \\(X_{i}\\) is relative to the other observations in the sample. A large \\(h_{i i}\\) occurs when \\(X_{i}\\) is quite different from the other sample values. A measure of overall unusualness is the maximum leverage value\n\\[\n\\bar{h}=\\max _{1 \\leq i \\leq n} h_{i i} .\n\\]\nIt is common to say that a regression design is balanced when the leverage values are all roughly equal to one another. From Theorem 3.6.3 we deduce that complete balance occurs when \\(h_{i i}=\\bar{h}=k / n\\). An example of complete balance is when the regressors are all orthogonal dummy variables, each of which have equal occurrance of 0’s and 1’s.\nA regression design is unbalanced if some leverage values are highly unequal from the others. The most extreme case is \\(\\bar{h}=1\\). An example where this occurs is when there is a dummy regressor which takes the value 1 for only one observation in the sample.\nThe maximal leverage value (3.41) will change depending on the choice of regressors. For example, consider equation (3.13), the wage regression for single Asian men which has \\(n=268\\) observations. This regression has \\(\\bar{h}=0.33\\). If the squared experience regressor is omitted the leverage drops to \\(\\bar{h}=0.10\\). If a cubic in experience is added it increases to \\(\\bar{h}=0.76\\). And if a fourth and fifth power are added it increases to \\(\\bar{h}=0.99\\).\nSome inference procedures (such as robust covariance matrix estimation and cross-validation) are sensitive to high leverage values. We will return to these issues later.\nWe now prove Theorem 3.6. For part 1 let \\(s_{i}\\) be an \\(n \\times 1\\) unit vector with a 1 in the \\(i^{t h}\\) place and zeros elsewhere so that \\(h_{i i}=s_{i}^{\\prime} \\boldsymbol{P} s_{i}\\). Then applying the Quadratic Inequality (B.18) and Theorem 3.3.4,\n\\[\nh_{i i}=s_{i}^{\\prime} \\boldsymbol{P} s_{i} \\leq s_{i}^{\\prime} s_{i} \\lambda_{\\max }(\\boldsymbol{P})=1\n\\]\nas claimed.\nFor part 2 partition \\(X_{i}=\\left(1, Z_{i}^{\\prime}\\right)^{\\prime}\\). Without loss of generality we can replace \\(Z_{i}\\) with the demeaned values \\(Z_{i}^{*}=Z_{i}-\\bar{Z}\\). Then since \\(Z_{i}^{*}\\) and the intercept are orthgonal\n\\[\n\\begin{aligned}\nh_{i i} &=\\left(1, Z_{i}^{* \\prime}\\right)\\left[\\begin{array}{cc}\nn & 0 \\\\\n0 & Z^{* \\prime} Z^{*}\n\\end{array}\\right]^{-1}\\left(\\begin{array}{c}\n1 \\\\\nZ_{i}^{*}\n\\end{array}\\right) \\\\\n&=\\frac{1}{n}+Z_{i}^{* \\prime}\\left(Z^{* \\prime} Z^{*}\\right)^{-1} Z_{i}^{*} \\geq \\frac{1}{n} .\n\\end{aligned}\n\\]\nFor part 3, \\(\\sum_{i=1}^{n} h_{i i}=\\operatorname{tr} \\boldsymbol{P}=k\\) where the second equality is Theorem 3.3.3."
  },
  {
    "objectID": "chpt03-algebra.html#leave-one-out-regression",
    "href": "chpt03-algebra.html#leave-one-out-regression",
    "title": "3  The Algebra of Least Squares",
    "section": "3.22 Leave-One-Out Regression",
    "text": "3.22 Leave-One-Out Regression\nThere are a number of statistical procedures - residual analysis, jackknife variance estimation, crossvalidation, two-step estimation, hold-out sample evaluation - which make use of estimators constructed on sub-samples. Of particular importance is the case where we exclude a single observation and then repeat this for all observations. This is called leave-one-out (LOO) regression.\nSpecifically, the leave-one-out estimator of the regression coefficient \\(\\beta\\) is the least squares estimator constructed using the full sample excluding a single observation \\(i\\). This can be written as\n\\[\n\\begin{aligned}\n\\widehat{\\beta}_{(-i)} &=\\left(\\sum_{j \\neq i} X_{j} X_{j}^{\\prime}\\right)^{-1}\\left(\\sum_{j \\neq i} X_{j} Y_{j}\\right) \\\\\n&=\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}-X_{i} X_{i}^{\\prime}\\right)^{-1}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{Y}-X_{i} Y_{i}\\right) \\\\\n&=\\left(\\boldsymbol{X}_{(-i)}^{\\prime} \\boldsymbol{X}_{(-i)}\\right)^{-1} \\boldsymbol{X}_{(-i)}^{\\prime} \\boldsymbol{Y}_{(-i)} .\n\\end{aligned}\n\\]\nHere, \\(\\boldsymbol{X}_{(-i)}\\) and \\(\\boldsymbol{Y}_{(-i)}\\) are the data matrices omitting the \\(i^{t h}\\) row. The notation \\(\\widehat{\\beta}_{(-i)}\\) or \\(\\widehat{\\beta}_{-i}\\) is commonly used to denote an estimator with the \\(i^{t h}\\) observation omitted. There is a leave-one-out estimator for each observation, \\(i=1, \\ldots, n\\), so we have \\(n\\) such estimators.\nThe leave-one-out predicted value for \\(Y_{i}\\) is \\(\\widetilde{Y}_{i}=X_{i}^{\\prime} \\widehat{\\beta}_{(-i)}\\). This is the predicted value obtained by estimating \\(\\beta\\) on the sample without observation \\(i\\) and then using the covariate vector \\(X_{i}\\) to predict \\(Y_{i}\\). Notice that \\(\\widetilde{Y}_{i}\\) is an authentic prediction as \\(Y_{i}\\) is not used to construct \\(\\widetilde{Y}_{i}\\). This is in contrast to the fitted values \\(\\widehat{Y}_{i}\\) which are functions of \\(Y_{i}\\).\nThe leave-one-out residual, prediction error, or prediction residual is \\(\\widetilde{e}_{i}=Y_{i}-\\widetilde{Y}_{i}\\). The prediction errors may be used as estimators of the errors instead of the residuals. The prediction errors are better estimators than the residuals because the former are based on authentic predictions.\nThe leave-one-out formula (3.42) gives the unfortunate impression that the leave-one-out coefficients and errors are computationally cumbersome, requiring \\(n\\) separate regressions. In the context of linear regression this is fortunately not the case. There are simple linear expressions for \\(\\widehat{\\beta}_{(-i)}\\) and \\(\\widetilde{e}_{i}\\).\nTheorem 3.7 The leave-one-out estimator and prediction error equal\n\\[\n\\widehat{\\beta}_{(-i)}=\\widehat{\\beta}-\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} X_{i} \\widetilde{e}_{i}\n\\]\nand\n\\[\n\\widetilde{e}_{i}=\\left(1-h_{i i}\\right)^{-1} \\widehat{e}_{i}\n\\]\nwhere \\(h_{i i}\\) are the leverage values as defined in (3.40).\nWe prove Theorem \\(3.7\\) at the end of the section.\nEquation (3.43) shows that the leave-one-out coefficients can be calculated by a simple linear operation and do not need to be calculated using \\(n\\) separate regressions. Another interesting feature of equation (3.44) is that the prediction errors \\(\\widetilde{e}_{i}\\) are a simple scaling of the least squares residuals \\(\\widehat{e}_{i}\\) with the scaling dependent on the leverage values \\(h_{i i}\\). If \\(h_{i i}\\) is small then \\(\\widetilde{e}_{i} \\simeq \\widehat{e}_{i}\\). However if \\(h_{i i}\\) is large then \\(\\widetilde{e}_{i}\\) can be quite different from \\(\\widehat{e}_{i}\\). Thus the difference between the residuals and predicted values depends on the leverage values, that is, how unusual is \\(X_{i}\\). To write (3.44) in vector notation, define\n\\[\n\\begin{aligned}\n\\boldsymbol{M}^{*} &=\\left(\\boldsymbol{I}_{n}-\\operatorname{diag}\\left\\{h_{11}, . ., h_{n n}\\right\\}\\right)^{-1} \\\\\n&=\\operatorname{diag}\\left\\{\\left(1-h_{11}\\right)^{-1}, \\ldots,\\left(1-h_{n n}\\right)^{-1}\\right\\}\n\\end{aligned}\n\\]\nThen (3.44) is equivalent to\n\\[\n\\widetilde{\\boldsymbol{e}}=\\boldsymbol{M}^{*} \\widehat{\\boldsymbol{e}} .\n\\]\nOne use of the prediction errors is to estimate the out-of-sample mean squared error:\n\\[\n\\widetilde{\\sigma}^{2}=\\frac{1}{n} \\sum_{i=1}^{n} \\widetilde{e}_{i}^{2}=\\frac{1}{n} \\sum_{i=1}^{n}\\left(1-h_{i i}\\right)^{-2} \\widehat{e}_{i}^{2} .\n\\]\nThis is known as the sample mean squared prediction error. Its square root \\(\\widetilde{\\sigma}=\\sqrt{\\widetilde{\\sigma}^{2}}\\) is the prediction standard error.\nWe complete the section with a proof of Theorem 3.7. The leave-one-out estimator (3.42) can be written as\n\\[\n\\widehat{\\beta}_{(-i)}=\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}-X_{i} X_{i}^{\\prime}\\right)^{-1}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{Y}-X_{i} Y_{i}\\right) .\n\\]\nMultiply (3.47) by \\(\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}-X_{i} X_{i}^{\\prime}\\right)\\). We obtain\n\\[\n\\widehat{\\beta}_{(-i)}-\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} X_{i} X_{i}^{\\prime} \\widehat{\\beta}_{(-i)}=\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{Y}-X_{i} Y_{i}\\right)=\\widehat{\\beta}-\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} X_{i} Y_{i} .\n\\]\nRewriting\n\\[\n\\widehat{\\beta}_{(-i)}=\\widehat{\\beta}-\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} X_{i}\\left(Y_{i}-X_{i}^{\\prime} \\widehat{\\beta}_{(-i)}\\right)=\\widehat{\\beta}-\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} X_{i} \\widetilde{e}_{i}\n\\]\nwhich is (3.43). Premultiplying this expression by \\(X_{i}^{\\prime}\\) and using definition (3.40) we obtain\n\\[\nX_{i}^{\\prime} \\widehat{\\beta}_{(-i)}=X_{i}^{\\prime} \\widehat{\\beta}-X_{i}^{\\prime}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} X_{i} \\widetilde{e}_{i}=X_{i}^{\\prime} \\widehat{\\beta}-h_{i i} \\widetilde{e}_{i} .\n\\]\nUsing the definitions for \\(\\widehat{e}_{i}\\) and \\(\\widetilde{e}_{i}\\) we obtain \\(\\widetilde{e}_{i}=\\widehat{e}_{i}+h_{i i} \\widetilde{e}_{i}\\). Re-writing we obtain (3.44)."
  },
  {
    "objectID": "chpt03-algebra.html#influential-observations",
    "href": "chpt03-algebra.html#influential-observations",
    "title": "3  The Algebra of Least Squares",
    "section": "3.23 Influential Observations",
    "text": "3.23 Influential Observations\nAnother use of the leave-one-out estimator is to investigate the impact of influential observations, sometimes called outliers. We say that observation \\(i\\) is influential if its omission from the sample induces a substantial change in a parameter estimate of interest.\nFor illustration consider Figure \\(3.5\\) which shows a scatter plot of realizations \\(\\left(Y_{i}, X_{i}\\right)\\). The 25 observations shown with the open circles are generated by \\(X_{i} \\sim U[1,10]\\) and \\(Y_{i} \\sim \\mathrm{N}\\left(X_{i}, 4\\right)\\). The \\(26^{\\text {th }}\\) observation shown with the filled circle is \\(X_{26}=9, Y_{26}=0\\). (Imagine that \\(Y_{26}=0\\) was incorrectly recorded due to a mistaken key entry.) The figure shows both the least squares fitted line from the full sample and that obtained after deletion of the \\(26^{\\text {th }}\\) observation from the sample. In this example we can see how the \\(26^{\\text {th }}\\) observation (the “outlier”) greatly tilts the least squares fitted line towards the \\(26^{\\text {th }}\\) observation. In fact, the slope coefficient decreases from \\(0.97\\) (which is close to the true value of \\(1.00\\) ) to \\(0.56\\), which is substantially reduced. Neither \\(Y_{26}\\) nor \\(X_{26}\\) are unusual values relative to their marginal distributions so this outlier would not have been detected from examination of the marginal distributions of the data. The change in the slope coefficient of \\(-0.41\\) is meaningful and should raise concern to an applied economist.\nFrom (3.43) we know that\n\\[\n\\widehat{\\beta}-\\widehat{\\beta}_{(-i)}=\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} X_{i} \\widetilde{e}_{i}\n\\]\n\nFigure 3.5: Impact of an Influential Observation on the Least-Squares Estimator\nBy direct calculation of this quantity for each observation \\(i\\), we can directly discover if a specific observation \\(i\\) is influential for a coefficient estimate of interest.\nFor a general assessment, we can focus on the predicted values. The difference between the fullsample and leave-one-out predicted values is\n\\[\n\\widehat{Y}_{i}-\\widetilde{Y}_{i}=X_{i}^{\\prime} \\widehat{\\beta}-X_{i}^{\\prime} \\widehat{\\beta}_{(-i)}=X_{i}^{\\prime}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} X_{i} \\widetilde{e}_{i}=h_{i i} \\widetilde{e}_{i}\n\\]\nwhich is a simple function of the leverage values \\(h_{i i}\\) and prediction errors \\(\\widetilde{e}_{i}\\). Observation \\(i\\) is influential for the predicted value if \\(\\left|h_{i i} \\widetilde{e}_{i}\\right|\\) is large, which requires that both \\(h_{i i}\\) and \\(\\left|\\widetilde{e}_{i}\\right|\\) are large.\nOne way to think about this is that a large leverage value \\(h_{i i}\\) gives the potential for observation \\(i\\) to be influential. A large \\(h_{i i}\\) means that observation \\(i\\) is unusual in the sense that the regressor \\(X_{i}\\) is far from its sample mean. We call an observation with large \\(h_{i i}\\) a leverage point. A leverage point is not necessarily influential as the latter also requires that the prediction error \\(\\widetilde{e}_{i}\\) is large.\nTo determine if any individual observations are influential in this sense several diagnostics have been proposed (some names include DFITS, Cook’s Distance, and Welsch Distance). Unfortunately, from a statistical perspective it is difficult to recommend these diagnostics for applications as they are not based on statistical theory. Probably the most relevant measure is the change in the coefficient estimates given in (3.48). The ratio of these changes to the coefficient’s standard error is called its DFBETA, and is a postestimation diagnostic available in Stata. While there is no magic threshold, the concern is whether or not an individual observation meaningfully changes an estimated coefficient of interest. A simple diagnostic for influential observations is to calculate\n\\[\n\\text { Influence }=\\max _{1 \\leq i \\leq n}\\left|\\widehat{Y}_{i}-\\widetilde{Y}_{i}\\right|=\\max _{1 \\leq i \\leq n}\\left|h_{i i} \\widetilde{e}_{i}\\right| .\n\\]\nThis is the largest (absolute) change in the predicted value due to a single observation. If this diagnostic is large relative to the distribution of \\(Y\\) it may indicate that that observation is influential.\nIf an observation is determined to be influential what should be done? As a common cause of influential observations is data error, the influential observations should be examined for evidence that the observation was mis-recorded. Perhaps the observation falls outside of permitted ranges, or some observables are inconsistent (for example, a person is listed as having a job but receives earnings of \\(\\$ 0\\) ). If it is determined that an observation is incorrectly recorded, then the observation is typically deleted from the sample. This process is often called “cleaning the data”. The decisions made in this process involve a fair amount of individual judgment. [When this is done the proper practice is to retain the source data in its original form and create a program file which executes all cleaning operations (for example deletion of individual observations). The cleaned data file can be saved at this point, and then used for the subsequent statistical analysis. The point of retaining the source data and a specific program file which cleans the data is twofold: so that all decisions are documented, and so that modifications can be made in revisions and future research.] It is also possible that an observation is correctly measured, but unusual and influential. In this case it is unclear how to proceed. Some researchers will try to alter the specification to properly model the influential observation. Other researchers will delete the observation from the sample. The motivation for this choice is to prevent the results from being skewed or determined by individual observations. This latter practice is viewed skeptically by many researchers who believe it reduces the integrity of reported empirical results.\nFor an empirical illustration consider the log wage regression (3.13) for single Asian men. This regression, which has 268 observations, has Influence \\(=0.29\\). This means that the most influential observation, when deleted, changes the predicted (fitted) value of the dependent variable \\(\\log (\\) wage) by \\(0.29\\), or equivalently the average wage by \\(29 %\\). This is a meaningful change and suggests further investigation. We examine the influential observation, and find that its leverage \\(h_{i i}\\) is \\(0.33\\). It is a moderately large leverage value, meaning that the regressor \\(X_{i}\\) is somewhat unusual. Examining further, we find that this individual is 65 years old with 8 years education, so that his potential work experience is 51 years. This is the highest experience in the subsample - the next highest is 41 years. The large leverage is due to his unusual characteristics (very low education and very high experience) within this sample. Essentially, regression (3.13) is attempting to estimate the conditional mean at experience \\(=51\\) with only one observation. It is not surprising that this observation determines the fit and is thus influential. A reasonable conclusion is the regression function can only be estimated over a smaller range of experience. We restrict the sample to individuals with less than 45 years experience, re-estimate, and obtain the following estimates.\n\\[\n\\widehat{\\log (\\text { wage })}=0.144 \\text { education }+0.043 \\text { experience }-0.095 \\text { experience }^{2} / 100+0.531 \\text {. }\n\\]\nFor this regression, we calculate that Influence \\(=0.11\\), which is greatly reduced relative to the regression (3.13). Comparing (3.49) with (3.13), the slope coefficient for education is essentially unchanged, but the coefficients on experience and its square have slightly increased.\nBy eliminating the influential observation equation (3.49) can be viewed as a more robust estimate of the conditional mean for most levels of experience. Whether to report (3.13) or (3.49) in an application is largely a matter of judgment."
  },
  {
    "objectID": "chpt03-algebra.html#cps-data-set",
    "href": "chpt03-algebra.html#cps-data-set",
    "title": "3  The Algebra of Least Squares",
    "section": "3.24 CPS Data Set",
    "text": "3.24 CPS Data Set\nIn this section we describe the data set used in the empirical illustrations.\nThe Current Population Survey (CPS) is a monthly survey of about 57,000 U.S. households conducted by the Bureau of the Census of the Bureau of Labor Statistics. The CPS is the primary source of information on the labor force characteristics of the U.S. population. The survey covers employment, earnings, educational attainment, income, poverty, health insurance coverage, job experience, voting and registration, computer usage, veteran status, and other variables. Details can be found at . html.\nFrom the March 2009 survey we extracted the individuals with non-allocated variables who were fulltime employed (defined as those who had worked at least 36 hours per week for at least 48 weeks the past year), and excluded those in the military. This sample has 50,742 individuals. We extracted 14 variables from the CPS on these individuals and created the data set cps09mar. This data set, and all others used in this textbook, are available at http: . wisc . edu/ bhansen/econometrics/."
  },
  {
    "objectID": "chpt03-algebra.html#numerical-computation",
    "href": "chpt03-algebra.html#numerical-computation",
    "title": "3  The Algebra of Least Squares",
    "section": "3.25 Numerical Computation",
    "text": "3.25 Numerical Computation\nModern econometric estimation involves large samples and many covariates. Consequently, calculation of even simple statistics such as the least squares estimator requires a large number (millions) of arithmetic operations. In practice most economists don’t need to think much about this as it is done swiftly and effortlessly on personal computers. Nevertheless it is useful to understand the underlying calculation methods as choices can occasionally make substantive differences.\nWhile today nearly all statistical computations are made using statistical software running on electronic computers, this was not always the case. In the nineteenth and early twentieth centures “computer” was a job label for workers who made computations by hand. Computers were employed by astronomers and statistical laboratories. This fascinating job (and the fact that most computers employed in laboratories were women) has entered popular culture. For example the lives of several computers who worked for the early U.S. space program is described in the book and popular movie Hidden Figures, a fictional computer/astronaut is the protagonist of the novel The Calculating Stars, and the life of computer/astronomer Henrietta Swan Leavitt is dramatized in the play Silent Sky.\nUntil programmable electronic computers became available in the 1960s economics graduate students were routinely employed as computers. Sample sizes were considerably smaller than those seen today, but still the effort required to calculate by hand a regression with even \\(n=100\\) observations and \\(k=5\\) variables is considerable! If you are a current graduate student you should feel fortunate that the profession has moved on from the era of human computers! (Now research assistants do more elevated tasks such as writing Stata, R, and MATLAB code.)\nTo obtain the least squares estimate \\(\\widehat{\\beta}=\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{Y}\\right)\\) we need to either invert \\(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\) or solve a system of equations. To be specific, let \\(\\boldsymbol{A}=\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\) and \\(\\boldsymbol{c}=\\boldsymbol{X}^{\\prime} \\boldsymbol{Y}\\) so that the least squares estimate can be written as either the solution to\n\\[\n\\boldsymbol{A} \\widehat{\\beta}=\\boldsymbol{c}\n\\]\nor as\n\\[\n\\widehat{\\beta}=A^{-1} \\boldsymbol{c} .\n\\]\nThe equations (3.50) and (3.51) are algebraically identical but they suggest two distinct numerical approaches to obtain \\(\\widehat{\\beta}\\). (3.50) suggests solving a system of \\(k\\) equations. (3.51) suggests finding \\(A^{-1}\\) and then multiplying by \\(\\boldsymbol{c}\\). While the two expressions are algebraically identical the implied numerical approaches are different. In a nutshell, solving the system of equations (3.50) is numerically preferred to the matrix inversion problem (3.51). Directly solving (3.50) is faster and produces a solution with a higher degree of numerical accuracy. Thus (3.50) is generally recommended over (3.51). However, in most practical applications the choice will not make any practical difference. Contexts where the choice may make a difference is when the matrix \\(\\boldsymbol{A}\\) is ill-conditioned (to be discussed in Section 3.24) or of extremely high dimension.\nNumerical methods to solve the system of equations (3.50) and calculate \\(\\boldsymbol{A}^{-1}\\) are discussed in Sections A.18 and A.19, respectively.\nStatistical packages use a variety of matrix methods to solve (3.50). Stata uses the sweep algorithm which is a variant of the Gauss-Jordan algorithm discussed in Section A.18. (For the sweep algorithm see Goodnight (1979).) In R, solve (A, b) uses the QR decomposition. In MATLAB, A b uses the Cholesky decomposition when \\(A\\) is positive definite and the QR decomposition otherwise."
  },
  {
    "objectID": "chpt03-algebra.html#collinearity-errors",
    "href": "chpt03-algebra.html#collinearity-errors",
    "title": "3  The Algebra of Least Squares",
    "section": "3.26 Collinearity Errors",
    "text": "3.26 Collinearity Errors\nFor the least squares estimator to be uniquely defined the regressors cannot be linearly dependent. However, it is quite easy to attempt to calculate a regression with linearly dependent regressors. This can occur for many reasons, including the following.\n\nIncluding the same regressor twice.\nIncluding regressors which are a linear combination of one another, such as education, experience and age in the CPS data set example (recall, experience is defined as age-education-6).\nIncluding a dummy variable and its square.\nEstimating a regression on a sub-sample for which a dummy variable is either all zeros or all ones.\nIncluding a dummy variable interaction which yields all zeros.\nIncluding more regressors than observations.\n\nIn any of the above cases the regressors are linearly dependent so \\(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\) is singular and the least squares estimator is not unique. If you attempt to estimate the regression, you are likely to encounter an error message. (A possible exception is MATLAB using “A \\(\\backslash \\mathrm{b}\\)”, as discussed below.) The message may be that “system is exactly singular”, “system is computationally singular”, a variable is “omitted because of collinearity”, or a coefficient is listed as “NA”. In some cases (such as estimation in R using explicit matrix computation or MATLAB using the regress command) the program will stop execution. In other cases the program will continue to run. In Stata (and in the Im package in R), a regression will be reported but one or more variables will be omitted.\nIf any of these warnings or error messages appear, the correct response is to stop and examine the regression coding and data. Did you make an unintended mistake? Have you included a linearly dependent regressor? Are you estimating on a subsample for which the variables (in particular dummy variables) have no variation? If you can determine that one of these scenarios caused the error, the solution is immediately apparent. You need to respecify your model (either sample or regressors) so that the redundancy is eliminated. All empirical researchers encounter this error in the course of empirical work. You should not, however, simply accept output if the package has selected variables for omission. It is the researcher’s job to understand the underlying cause and enact a suitable remedy.\nThere is also a possibility that the statistical package will not detect and report the matrix singularity. If you compute in MATLAB using explicit matrix operations and use the recommended A \\(\\backslash \\mathrm{b}\\) command to compute the least squares estimator MATLAB may return a numerical solution without an error message even when the regressors are algebraically dependent. It is therefore recommended that you perform a numerical check for matrix singularity when using explicit matrix operations in MATLAB.\nHow can we numerically check if a matrix \\(\\boldsymbol{A}\\) is singular? A standard diagnostic is the reciprocal condition number\n\\[\nC=\\frac{\\lambda_{\\min }(\\boldsymbol{A})}{\\lambda_{\\max }(\\boldsymbol{A})} .\n\\]\nIf \\(C=0\\) then \\(\\boldsymbol{A}\\) is singular. If \\(C=1\\) then \\(\\boldsymbol{A}\\) is perfectly balanced. If \\(C\\) is extremely small we say that \\(\\boldsymbol{A}\\) is ill-conditioned. The reciprocal condition number can be calculated in MATLAB or R by the rcond command. Unfortunately, there is no accepted tolerance for how small \\(C\\) should be before regarding \\(\\boldsymbol{A}\\) as numerically singular, in part since rcond (A) can return a positive (but small) result even if \\(\\boldsymbol{A}\\) is algebraically singular. However, in double precision (which is typically used for computation) numerical accuracy is bounded by \\(2^{-52} \\simeq 2 \\mathrm{e}-16\\), suggesting the minimum bound \\(C \\geq 2 \\mathrm{e}-16\\).\nChecking for numerical singularity is complicated by the fact that low values of \\(C\\) can also be caused by unbalanced or highly correlated regressors.\nTo illustrate, consider a wage regression using the sample from (3.13) on powers of experience \\(X\\) from 1 through \\(k\\) (e.g. \\(X, X^{2}, X^{3}, \\ldots, X^{k}\\) ). We calculated the reciprocal condition number \\(C\\) for each \\(k\\), and found that \\(C\\) is decreasing as \\(k\\) increases, indicating increasing ill-conditioning. Indeed, for \\(k=\\) 5, we find \\(C=6 \\mathrm{e}-17\\), which is lower than double precision accuracy. This means that a regression on \\(\\left(X, X^{2}, X^{3}, X^{4}, X^{5}\\right)\\) is ill-conditioned. The regressor matrix, however, is not singular. The low value of \\(C\\) is not due to algebraic singularity but rather is due to a lack of balance and high collinearity.\nIll-conditioned regressors have the potential problem that the numerical results (the reported coefficient estimates) will be inaccurate. It may not be a concern in most applications as this only occurs in extreme cases. Nevertheless, we should try and avoid ill-conditioned regressions whenever possible.\nThere are strategies which can reduce or even eliminate ill-conditioning. Often it is sufficient to rescale the regressors. A simple rescaling which often works for non-negative regressors is to divide each by its sample mean, thus replace \\(X_{j i}\\) with \\(X_{j i} / \\bar{X}_{j}\\). In the above example with the powers of experience, this means replacing \\(X_{i}^{2}\\) with \\(X_{i}^{2} /\\left(n^{-1} \\sum_{i=1}^{n} X_{i}^{2}\\right)\\), etc. Doing so dramatically reduces the ill-conditioning. With this scaling, regressions for \\(k \\leq 11\\) satisfy \\(C \\geq 1 \\mathrm{e}-15\\). Another rescaling specific to a regression with powers is to first rescale the regressor to lie in \\([-1,1]\\) before taking powers. With this scaling, regressions for \\(k \\leq 16\\) satisfy \\(C \\geq 1 \\mathrm{e}-15\\). A simpler scaling option is to rescale the regressor to lie in \\([0,1]\\) before taking powers. With this scaling, regressions for \\(k \\leq 9\\) satisfy \\(C \\geq 1 \\mathrm{e}-15\\). This is often sufficient for applications.\nIll-conditioning can often be completely eliminated by orthogonalization of the regressors. This is achieved by sequentially regressing each variable (each column in \\(\\boldsymbol{X}\\) ) on the preceeding variables (each preceeding column), taking the residual, and then rescaling to have a unit variance. This will produce regressors which algebraically satisfy \\(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}=n \\boldsymbol{I}_{n}\\) and have a condition number of \\(C=1\\). If we apply this method to the above example, we obtain a condition number close to 1 for \\(k \\leq 20\\).\nWhat this shows is that when a regression has a small condition number it is important to examine the specification carefully. It is possible that the regressors are linearly dependent in which case one or more regressors will need to be omitted. It is also possible that the regressors are badly scaled in which case it may be useful to rescale some of the regressors. It is also possible that the variables are highly collinear in which case a possible solution is orthogonalization. These choices should be made by the researcher not by an automated software program."
  },
  {
    "objectID": "chpt03-algebra.html#programming",
    "href": "chpt03-algebra.html#programming",
    "title": "3  The Algebra of Least Squares",
    "section": "3.27 Programming",
    "text": "3.27 Programming\nMost packages allow both interactive programming (where you enter commands one-by-one) and batch programming (where you run a pre-written sequence of commands from a file). Interactive programming can be useful for exploratory analysis but eventually all work should be executed in batch mode. This is the best way to control and document your work.\nBatch programs are text files where each line executes a single command. For Stata, this file needs to have the filename extension “.do”, and for MATLAB “.m”. For R there is no specific naming requirements, though it is typical to use the extension “.r”. When writing batch files it is useful to include comments for documentation and readability. To execute a program file you type a command within the program.\nStata: do chapter3 executes the file chapter3. do.\nMATLAB: run chapter3 executes the file chapter3.m.\nR: source (‘chapter3.r’) executes the file chapter3.r.\nThere are similarities and differences between the commands used in these packages. For example:\n\nDifferent symbols are used to create comments. \\(*\\) in Stata, # in \\(\\mathrm{R}\\), and \\(%\\) in MATLAB.\nMATLAB uses the symbol ; to separate lines. Stata and R use a hard return.\nStata uses \\(\\ln ()\\) to compute natural logarithms. R and MATLAB use \\(\\log ()\\).\nThe symbol \\(=\\) is used to define a variable. \\(\\mathrm{R}\\) prefers \\(<-\\). Double equality \\(==\\) is used to test equality.\n\nWe now illustrate programming files for Stata, R, and MATLAB, which execute a portion of the empirical illustrations from Sections \\(3.7\\) and 3.21. For the R and MATLAB code we illustrate using explicit matrix operations. Alternatively, R and MATLAB have built-in functions which implement least squares regression without the need for explicit matrix operations. In \\(\\mathrm{R}\\) the standard function is \\(1 \\mathrm{~m}\\). In MATLAB the standard function is regress. The advantage of using explicit matrix operations as shown below is that you know exactly what computations are done and it is easier to go “out of the box” to execute new procedures. The advantage of using built-in functions is that coding is simplified and you are much less likely to make a coding error."
  },
  {
    "objectID": "chpt03-algebra.html#exercises",
    "href": "chpt03-algebra.html#exercises",
    "title": "3  The Algebra of Least Squares",
    "section": "3.28 Exercises",
    "text": "3.28 Exercises\nExercise 3.1 Let \\(Y\\) be a random variable with \\(\\mu=\\mathbb{E}[Y]\\) and \\(\\sigma^{2}=\\operatorname{var}[Y]\\). Define\n\\[\ng\\left(y, \\mu, \\sigma^{2}\\right)=\\left(\\begin{array}{c}\ny-\\mu \\\\\n(y-\\mu)^{2}-\\sigma^{2}\n\\end{array}\\right) .\n\\]\nLet \\(\\left(\\widehat{\\mu}, \\widehat{\\sigma}^{2}\\right)\\) be the values such that \\(\\bar{g}_{n}\\left(\\widehat{\\mu}, \\widehat{\\sigma}^{2}\\right)=0\\) where \\(\\bar{g}_{n}(m, s)=n^{-1} \\sum_{i=1}^{n} g\\left(y_{i}, m, s\\right)\\). Show that \\(\\widehat{\\mu}\\) and \\(\\widehat{\\sigma}^{2}\\) are the sample mean and variance.\nExercise 3.2 Consider the OLS regression of the \\(n \\times 1\\) vector \\(\\boldsymbol{Y}\\) on the \\(n \\times k\\) matrix \\(\\boldsymbol{X}\\). Consider an alternative set of regressors \\(\\boldsymbol{Z}=\\boldsymbol{X} \\boldsymbol{C}\\), where \\(\\boldsymbol{C}\\) is a \\(k \\times k\\) non-singular matrix. Thus, each column of \\(\\boldsymbol{Z}\\) is a mixture of some of the columns of \\(\\boldsymbol{X}\\). Compare the OLS estimates and residuals from the regression of \\(\\boldsymbol{Y}\\) on \\(\\boldsymbol{X}\\) to the OLS estimates from the regression of \\(\\boldsymbol{Y}\\) on \\(\\boldsymbol{Z}\\).\nExercise 3.3 Using matrix algebra, show \\(\\boldsymbol{X}^{\\prime} \\widehat{\\boldsymbol{e}}=0\\).\nExercise 3.4 Let \\(\\widehat{\\boldsymbol{e}}\\) be the OLS residual from a regression of \\(\\boldsymbol{Y}\\) on \\(\\boldsymbol{X}=\\left[\\boldsymbol{X}_{1} \\boldsymbol{X}_{2}\\right]\\). Find \\(\\boldsymbol{X}_{2}^{\\prime} \\widehat{\\boldsymbol{e}}\\).\nExercise 3.5 Let \\(\\widehat{\\boldsymbol{e}}\\) be the OLS residual from a regression of \\(\\boldsymbol{Y}\\) on \\(\\boldsymbol{X}\\). Find the OLS coefficient from a regression of \\(\\widehat{\\boldsymbol{e}}\\) on \\(\\boldsymbol{X}\\).\nExercise 3.6 Let \\(\\widehat{\\boldsymbol{Y}}=\\boldsymbol{X}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\boldsymbol{X}^{\\prime} \\boldsymbol{Y}\\). Find the OLS coefficient from a regression of \\(\\widehat{\\boldsymbol{Y}}\\) on \\(\\boldsymbol{X}\\).\nExercise 3.7 Show that if \\(\\boldsymbol{X}=\\left[\\boldsymbol{X}_{1} \\boldsymbol{X}_{2}\\right]\\) then \\(\\boldsymbol{P} \\boldsymbol{X}_{1}=\\boldsymbol{X}_{1}\\) and \\(\\boldsymbol{M} \\boldsymbol{X}_{1}=0 .\\)\nExercise 3.8 Show that \\(M\\) is idempotent: \\(M M=M\\).\nExercise 3.9 Show that \\(\\operatorname{tr} M=n-k\\).\nExercise 3.10 Show that if \\(\\boldsymbol{X}=\\left[\\boldsymbol{X}_{1} \\boldsymbol{X}_{2}\\right]\\) and \\(\\boldsymbol{X}_{1}^{\\prime} \\boldsymbol{X}_{2}=0\\) then \\(\\boldsymbol{P}=\\boldsymbol{P}_{1}+\\boldsymbol{P}_{2}\\).\nExercise 3.11 Show that when \\(X\\) contains a constant, \\(n^{-1} \\sum_{i=1}^{n} \\widehat{Y}_{i}=\\bar{Y}\\).\nExercise 3.12 A dummy variable takes on only the values 0 and 1 . It is used for categorical variables. Let \\(\\boldsymbol{D}_{1}\\) and \\(\\boldsymbol{D}_{2}\\) be vectors of 1’s and 0’s, with the \\(i^{\\text {th }}\\) element of \\(\\boldsymbol{D}_{1}\\) equaling 1 and that of \\(\\boldsymbol{D}_{2}\\) equaling 0 if the person is a man, and the reverse if the person is a woman. Suppose that there are \\(n_{1}\\) men and \\(n_{2}\\) women in the sample. Consider fitting the following three equations by OLS\n\\[\n\\begin{aligned}\n&\\boldsymbol{Y}=\\mu+\\boldsymbol{D}_{1} \\alpha_{1}+\\boldsymbol{D}_{2} \\alpha_{2}+\\boldsymbol{e} \\\\\n&\\boldsymbol{Y}=\\boldsymbol{D}_{1} \\alpha_{1}+\\boldsymbol{D}_{2} \\alpha_{2}+\\boldsymbol{e} \\\\\n&\\boldsymbol{Y}=\\mu+\\boldsymbol{D}_{1} \\phi+\\boldsymbol{e}\n\\end{aligned}\n\\]\nCan all three equations (3.52), (3.53), and (3.54) be estimated by OLS? Explain if not.\n\nCompare regressions (3.53) and (3.54). Is one more general than the other? Explain the relationship between the parameters in (3.53) and (3.54).\nCompute \\(\\mathbf{1}_{n}^{\\prime} \\boldsymbol{D}_{1}\\) and \\(\\mathbf{1}_{n}^{\\prime} \\boldsymbol{D}_{2}\\), where \\(\\mathbf{1}_{n}\\) is an \\(n \\times 1\\) vector of ones.\n\nExercise 3.13 Let \\(\\boldsymbol{D}_{1}\\) and \\(\\boldsymbol{D}_{2}\\) be defined as in the previous exercise.\n\nIn the OLS regression\n\n\\[\n\\boldsymbol{Y}=\\boldsymbol{D}_{1} \\widehat{\\gamma}_{1}+\\boldsymbol{D}_{2} \\widehat{\\gamma}_{2}+\\widehat{\\boldsymbol{u}}\n\\]\nshow that \\(\\widehat{\\gamma}_{1}\\) is the sample mean of the dependent variable among the men of the sample \\(\\left(\\bar{Y}_{1}\\right)\\), and that \\(\\widehat{\\gamma}_{2}\\) is the sample mean among the women \\(\\left(\\bar{Y}_{2}\\right)\\).\n\nLet \\(\\boldsymbol{X}(n \\times k)\\) be an additional matrix of regressors. Describe in words the transformations\n\n\\[\n\\begin{aligned}\n&\\boldsymbol{Y}^{*}=\\boldsymbol{Y}-\\boldsymbol{D}_{1} \\bar{Y}_{1}-\\boldsymbol{D}_{2} \\bar{Y}_{2} \\\\\n&\\boldsymbol{X}^{*}=\\boldsymbol{X}-\\boldsymbol{D}_{1} \\bar{X}_{1}^{\\prime}-\\boldsymbol{D}_{2} \\bar{X}_{2}^{\\prime}\n\\end{aligned}\n\\]\nwhere \\(\\bar{X}_{1}\\) and \\(\\bar{X}_{2}\\) are the \\(k \\times 1\\) means of the regressors for men and women, respectively. (c) Compare \\(\\widetilde{\\beta}\\) from the OLS regression\n\\[\n\\boldsymbol{Y}^{*}=\\boldsymbol{X}^{*} \\widetilde{\\boldsymbol{\\beta}}+\\widetilde{\\boldsymbol{e}}\n\\]\nwith \\(\\widehat{\\beta}\\) from the OLS regression\n\\[\n\\boldsymbol{Y}=\\boldsymbol{D}_{1} \\widehat{\\alpha}_{1}+\\boldsymbol{D}_{2} \\widehat{\\alpha}_{2}+\\boldsymbol{X} \\widehat{\\boldsymbol{\\beta}}+\\widehat{\\boldsymbol{e}} .\n\\]\nExercise 3.14 Let \\(\\widehat{\\beta}_{n}=\\left(\\boldsymbol{X}_{n}^{\\prime} \\boldsymbol{X}_{n}\\right)^{-1} \\boldsymbol{X}_{n}^{\\prime} \\boldsymbol{Y}_{n}\\) denote the OLS estimate when \\(\\boldsymbol{Y}_{n}\\) is \\(n \\times 1\\) and \\(\\boldsymbol{X}_{n}\\) is \\(n \\times k\\). A new observation \\(\\left(Y_{n+1}, X_{n+1}\\right)\\) becomes available. Prove that the OLS estimate computed using this additional observation is\n\\[\n\\widehat{\\beta}_{n+1}=\\widehat{\\beta}_{n}+\\frac{1}{1+X_{n+1}^{\\prime}\\left(\\boldsymbol{X}_{n}^{\\prime} \\boldsymbol{X}_{n}\\right)^{-1} X_{n+1}}\\left(\\boldsymbol{X}_{n}^{\\prime} \\boldsymbol{X}_{n}\\right)^{-1} X_{n+1}\\left(Y_{n+1}-X_{n+1}^{\\prime} \\widehat{\\beta}_{n}\\right)\n\\]\nExercise 3.15 Prove that \\(R^{2}\\) is the square of the sample correlation between \\(\\boldsymbol{Y}\\) and \\(\\widehat{\\boldsymbol{Y}}\\).\nExercise 3.16 Consider two least squares regressions\n\\[\n\\boldsymbol{Y}=\\boldsymbol{X}_{1} \\widetilde{\\beta}_{1}+\\widetilde{\\boldsymbol{e}}\n\\]\nand\n\\[\n\\boldsymbol{Y}=\\boldsymbol{X}_{1} \\widehat{\\beta}_{1}+\\boldsymbol{X}_{2} \\widehat{\\beta}_{2}+\\widehat{\\boldsymbol{e}} .\n\\]\nLet \\(R_{1}^{2}\\) and \\(R_{2}^{2}\\) be the \\(R\\)-squared from the two regressions. Show that \\(R_{2}^{2} \\geq R_{1}^{2}\\). Is there a case (explain) when there is equality \\(R_{2}^{2}=R_{1}^{2}\\) ?\nExercise 3.17 For \\(\\widetilde{\\sigma}^{2}\\) defined in (3.46), show that \\(\\widetilde{\\sigma}^{2} \\geq \\widehat{\\sigma}^{2}\\). Is equality possible?\nExercise 3.18 For which observations will \\(\\widehat{\\beta}_{(-i)}=\\widehat{\\beta}\\) ?\nExercise 3.19 For the intercept-only model \\(Y_{i}=\\beta+e_{i}\\), show that the leave-one-out prediction error is\n\\[\n\\widetilde{e}_{i}=\\left(\\frac{n}{n-1}\\right)\\left(Y_{i}-\\bar{Y}\\right) .\n\\]\nExercise 3.20 Define the leave-one-out estimator of \\(\\sigma^{2}\\),\n\\[\n\\widehat{\\sigma}_{(-i)}^{2}=\\frac{1}{n-1} \\sum_{j \\neq i}\\left(Y_{j}-X_{j}^{\\prime} \\widehat{\\beta}_{(-i)}\\right)^{2} .\n\\]\nThis is the estimator obtained from the sample with observation \\(i\\) omitted. Show that\n\\[\n\\widehat{\\sigma}_{(-i)}^{2}=\\frac{n}{n-1} \\widehat{\\sigma}^{2}-\\frac{\\widehat{e}_{i}^{2}}{(n-1)\\left(1-h_{i i}\\right)} .\n\\]\nExercise 3.21 Consider the least squares regression estimators\n\\[\nY_{i}=X_{1 i} \\widehat{\\beta}_{1}+X_{2 i} \\widehat{\\beta}_{2}+\\widehat{e}_{i}\n\\]\nand the “one regressor at a time” regression estimators\n\\[\nY_{i}=X_{1 i} \\widetilde{\\beta}_{1}+\\widetilde{e}_{1 i}, \\quad Y_{i}=X_{2 i} \\widetilde{\\beta}_{2}+\\widetilde{e}_{2 i}\n\\]\nUnder what condition does \\(\\widetilde{\\beta}_{1}=\\widehat{\\beta}_{1}\\) and \\(\\widetilde{\\beta}_{2}=\\widehat{\\beta}_{2}\\) ? Exercise 3.22 You estimate a least squares regression\n\\[\nY_{i}=X_{1 i}^{\\prime} \\widetilde{\\beta}_{1}+\\widetilde{u}_{i}\n\\]\nand then regress the residuals on another set of regressors\n\\[\n\\widetilde{u}_{i}=X_{2 i}^{\\prime} \\widetilde{\\beta}_{2}+\\widetilde{e}_{i}\n\\]\nDoes this second regression give you the same estimated coefficients as from estimation of a least squares regression on both set of regressors?\n\\[\nY_{i}=X_{1 i}^{\\prime} \\widehat{\\beta}_{1}+X_{2 i}^{\\prime} \\widehat{\\beta}_{2}+\\widehat{e}_{i}\n\\]\nIn other words, is it true that \\(\\widetilde{\\beta}_{2}=\\widehat{\\beta}_{2}\\) ? Explain your reasoning.\nExercise 3.23 The data matrix is \\((\\boldsymbol{Y}, \\boldsymbol{X})\\) with \\(\\boldsymbol{X}=\\left[\\boldsymbol{X}_{1}, \\boldsymbol{X}_{2}\\right]\\), and consider the transformed regressor matrix \\(\\boldsymbol{Z}=\\left[\\boldsymbol{X}_{1}, \\boldsymbol{X}_{2}-\\boldsymbol{X}_{1}\\right]\\). Suppose you do a least squares regression of \\(\\boldsymbol{Y}\\) on \\(\\boldsymbol{X}\\), and a least squares regression of \\(\\boldsymbol{Y}\\) on \\(\\boldsymbol{Z}\\). Let \\(\\widehat{\\sigma}^{2}\\) and \\(\\widetilde{\\sigma}^{2}\\) denote the residual variance estimates from the two regressions. Give a formula relating \\(\\widehat{\\sigma}^{2}\\) and \\(\\widetilde{\\sigma}^{2}\\) ? (Explain your reasoning.)\nExercise 3.24 Use the cps09mar data set described in Section \\(3.22\\) and available on the textbook website. Take the sub-sample used for equation (3.49) (see Section 3.25) for data construction)\n\nEstimate equation (3.49) and compute the equation \\(R^{2}\\) and sum of squared errors.\nRe-estimate the slope on education using the residual regression approach. Regress log(wage) on experience and its square, regress education on experience and its square, and the residuals on the residuals. Report the estimates from this final regression, along with the equation \\(R^{2}\\) and sum of squared errors. Does the slope coefficient equal the value in (3.49)? Explain.\nAre the \\(R^{2}\\) and sum-of-squared errors from parts (a) and (b) equal? Explain.\n\nExercise 3.25 Estimate equation (3.49) as in part (a) of the previous question. Let \\(\\widehat{e}_{i}\\) be the OLS residual, \\(\\widehat{Y}_{i}\\) the predicted value from the regression, \\(X_{1 i}\\) be education and \\(X_{2 i}\\) be experience. Numerically calculate the following:\\ (a) \\(\\sum_{i=1}^{n} \\widehat{e}_{i}\\)\\ (b) \\(\\sum_{i=1}^{n} X_{1 i} \\widehat{e}_{i}\\)\\ (c) \\(\\sum_{i=1}^{n} X_{2 i} \\widehat{e}_{i}\\)\\ (d) \\(\\sum_{i=1}^{n} X_{1 i}^{2} \\widehat{e}_{i}\\)\\ (e) \\(\\sum_{i=1}^{n} X_{2 i}^{2} \\widehat{e}_{i}\\)\\ (f) \\(\\sum_{i=1}^{n} \\widehat{Y}_{i} \\widehat{e}_{i}\\)\\ (g) \\(\\sum_{i=1}^{n} \\widehat{e}_{i}^{2}\\)\nAre these calculations consistent with the theoretical properties of OLS? Explain.\nExercise 3.26 Use the cps09mar data set. (a) Estimate a log wage regression for the subsample of white male Hispanics. In addition to education, experience, and its square, include a set of binary variables for regions and marital status. For regions, create dummy variables for Northeast, South, and West so that Midwest is the excluded group. For marital status, create variables for married, widowed or divorced, and separated, so that single (never married) is the excluded group.\n\nRepeat using a different econometric package. Compare your results. Do they agree?"
  },
  {
    "objectID": "chpt04-lsr.html",
    "href": "chpt04-lsr.html",
    "title": "4  Least Squares Regression",
    "section": "",
    "text": "In this chapter we investigate some finite-sample properties of the least squares estimator in the linear regression model. In particular we calculate its finite-sample expectation and covariance matrix and propose standard errors for the coefficient estimators."
  },
  {
    "objectID": "chpt04-lsr.html#random-sampling",
    "href": "chpt04-lsr.html#random-sampling",
    "title": "4  Least Squares Regression",
    "section": "4.2 Random Sampling",
    "text": "4.2 Random Sampling\nAssumption \\(3.1\\) specified that the observations have identical distributions. To derive the finitesample properties of the estimators we will need to additionally specify the dependence structure across the observations.\nThe simplest context is when the observations are mutually independent in which case we say that they are independent and identically distributed or i.i.d. It is also common to describe i.i.d. observations as a random sample. Traditionally, random sampling has been the default assumption in crosssection (e.g. survey) contexts. It is quite convenient as i.i.d. sampling leads to straightforward expressions for estimation variance. The assumption seems appropriate (meaning that it should be approximately valid) when samples are small and relatively dispersed. That is, if you randomly sample 1000 people from a large country such as the United States it seems reasonable to model their responses as mutually independent.\nAssumption 4.1 The random variables \\(\\left\\{\\left(Y_{1}, X_{1}\\right), \\ldots,\\left(Y_{i}, X_{i}\\right), \\ldots,\\left(Y_{n}, X_{n}\\right)\\right\\}\\) are independent and identically distributed.\nFor most of this chapter we will use Assumption \\(4.1\\) to derive properties of the OLS estimator.\nAssumption \\(4.1\\) means that if you take any two individuals \\(i \\neq j\\) in a sample, the values \\(\\left(Y_{i}, X_{i}\\right)\\) are independent of the values \\(\\left(Y_{j}, X_{j}\\right)\\) yet have the same distribution. Independence means that the decisions and choices of individual \\(i\\) do not affect the decisions of individual \\(j\\) and conversely.\nThis assumption may be violated if individuals in the sample are connected in some way, for example if they are neighbors, members of the same village, classmates at a school, or even firms within a specific industry. In this case it seems plausible that decisions may be inter-connected and thus mutually dependent rather than independent. Allowing for such interactions complicates inference and requires specialized treatment. A currently popular approach which allows for mutual dependence is known as clustered dependence which assumes that that observations are grouped into “clusters” (for example, schools). We will discuss clustering in more detail in Section 4.21."
  },
  {
    "objectID": "chpt04-lsr.html#sample-mean",
    "href": "chpt04-lsr.html#sample-mean",
    "title": "4  Least Squares Regression",
    "section": "4.3 Sample Mean",
    "text": "4.3 Sample Mean\nWe start with the simplest setting of the intercept-only model\n\\[\n\\begin{aligned}\nY &=\\mu+e \\\\\n\\mathbb{E}[e] &=0 .\n\\end{aligned}\n\\]\nwhich is equivalent to the regression model with \\(k=1\\) and \\(X=1\\). In the intercept model \\(\\mu=\\mathbb{E}[Y]\\) is the expectation of \\(Y\\). (See Exercise 2.15.) The least squares estimator \\(\\widehat{\\mu}=\\bar{Y}\\) equals the sample mean as shown in equation (3.8).\nWe now calculate the expectation and variance of the estimator \\(\\bar{Y}\\). Since the sample mean is a linear function of the observations its expectation is simple to calculate\n\\[\n\\mathbb{E}[\\bar{Y}]=\\mathbb{E}\\left[\\frac{1}{n} \\sum_{i=1}^{n} Y_{i}\\right]=\\frac{1}{n} \\sum_{i=1}^{n} \\mathbb{E}\\left[Y_{i}\\right]=\\mu .\n\\]\nThis shows that the expected value of the least squares estimator (the sample mean) equals the projection coefficient (the population expectation). An estimator with the property that its expectation equals the parameter it is estimating is called unbiased.\nDefinition \\(4.1\\) An estimator \\(\\widehat{\\theta}\\) for \\(\\theta\\) is unbiased if \\(\\mathbb{E}[\\widehat{\\theta}]=\\theta\\)\nWe next calculate the variance of the estimator \\(\\bar{Y}\\) under Assumption 4.1. Making the substitution \\(Y_{i}=\\mu+e_{i}\\) we find\n\\[\n\\bar{Y}-\\mu=\\frac{1}{n} \\sum_{i=1}^{n} e_{i} .\n\\]\nThen\n\\[\n\\begin{aligned}\n\\operatorname{var}[\\bar{Y}] &=\\mathbb{E}\\left[(\\bar{Y}-\\mu)^{2}\\right] \\\\\n&=\\mathbb{E}\\left[\\left(\\frac{1}{n} \\sum_{i=1}^{n} e_{i}\\right)\\left(\\frac{1}{n} \\sum_{j=1}^{n} e_{j}\\right)\\right] \\\\\n&=\\frac{1}{n^{2}} \\sum_{i=1}^{n} \\sum_{j=1}^{n} \\mathbb{E}\\left[e_{i} e_{j}\\right] \\\\\n&=\\frac{1}{n^{2}} \\sum_{i=1}^{n} \\sigma^{2} \\\\\n&=\\frac{1}{n} \\sigma^{2} .\n\\end{aligned}\n\\]\nThe second-to-last inequality is because \\(\\mathbb{E}\\left[e_{i} e_{j}\\right]=\\sigma^{2}\\) for \\(i=j\\) yet \\(\\mathbb{E}\\left[e_{i} e_{j}\\right]=0\\) for \\(i \\neq j\\) due to independence.\nWe have shown that \\(\\operatorname{var}[\\bar{Y}]=\\frac{1}{n} \\sigma^{2}\\). This is the familiar formula for the variance of the sample mean."
  },
  {
    "objectID": "chpt04-lsr.html#linear-regression-model",
    "href": "chpt04-lsr.html#linear-regression-model",
    "title": "4  Least Squares Regression",
    "section": "4.4 Linear Regression Model",
    "text": "4.4 Linear Regression Model\nWe now consider the linear regression model. Throughout this chapter we maintain the following.\nAssumption 4.2 Linear Regression Model The variables \\((Y, X)\\) satisfy the linear regression equation\n\\[\n\\begin{aligned}\nY &=X^{\\prime} \\beta+e \\\\\n\\mathbb{E}[e \\mid X] &=0 .\n\\end{aligned}\n\\]\nThe variables have finite second moments\n\\[\n\\begin{aligned}\n&\\mathbb{E}\\left[Y^{2}\\right]<\\infty \\\\\n&\\mathbb{E}\\|X\\|^{2}<\\infty\n\\end{aligned}\n\\]\nand an invertible design matrix\n\\[\n\\boldsymbol{Q}_{X X}=\\mathbb{E}\\left[X X^{\\prime}\\right]>0 .\n\\]\nWe will consider both the general case of heteroskedastic regression where the conditional variance \\(\\mathbb{E}\\left[e^{2} \\mid X\\right]=\\sigma^{2}(X)\\) is unrestricted, and the specialized case of homoskedastic regression where the conditional variance is constant. In the latter case we add the following assumption.\nAssumption 4.3 Homoskedastic Linear Regression Model In addition to Assumption \\(4.2\\)\n\\[\n\\mathbb{E}\\left[e^{2} \\mid X\\right]=\\sigma^{2}(X)=\\sigma^{2}\n\\]\nis independent of \\(X\\)."
  },
  {
    "objectID": "chpt04-lsr.html#expectation-of-least-squares-estimator",
    "href": "chpt04-lsr.html#expectation-of-least-squares-estimator",
    "title": "4  Least Squares Regression",
    "section": "4.5 Expectation of Least Squares Estimator",
    "text": "4.5 Expectation of Least Squares Estimator\nIn this section we show that the OLS estimator is unbiased in the linear regression model. This calculation can be done using either summation notation or matrix notation. We will use both.\nFirst take summation notation. Observe that under (4.1)-(4.2)\n\\[\n\\mathbb{E}\\left[Y_{i} \\mid X_{1}, \\ldots, X_{n}\\right]=\\mathbb{E}\\left[Y_{i} \\mid X_{i}\\right]=X_{i}^{\\prime} \\beta .\n\\]\nThe first equality states that the conditional expectation of \\(Y_{i}\\) given \\(\\left\\{X_{1}, \\ldots, X_{n}\\right\\}\\) only depends on \\(X_{i}\\) because the observations are independent across \\(i\\). The second equality is the assumption of a linear conditional expectation. Using definition (3.11), the conditioning theorem (Theorem 2.3), the linearity of expectations, (4.4), and properties of the matrix inverse,\n\\[\n\\begin{aligned}\n\\mathbb{E}\\left[\\widehat{\\beta} \\mid X_{1}, \\ldots, X_{n}\\right] &=\\mathbb{E}\\left[\\left(\\sum_{i=1}^{n} X_{i} X_{i}^{\\prime}\\right)^{-1}\\left(\\sum_{i=1}^{n} X_{i} Y_{i}\\right) \\mid X_{1}, \\ldots, X_{n}\\right] \\\\\n&=\\left(\\sum_{i=1}^{n} X_{i} X_{i}^{\\prime}\\right)^{-1} \\mathbb{E}\\left[\\left(\\sum_{i=1}^{n} X_{i} Y_{i}\\right) \\mid X_{1}, \\ldots, X_{n}\\right] \\\\\n&=\\left(\\sum_{i=1}^{n} X_{i} X_{i}^{\\prime}\\right)^{-1} \\sum_{i=1}^{n} \\mathbb{E}\\left[X_{i} Y_{i} \\mid X_{1}, \\ldots, X_{n}\\right] \\\\\n&=\\left(\\sum_{i=1}^{n} X_{i} X_{i}^{\\prime}\\right)^{-1} \\sum_{i=1}^{n} X_{i} \\mathbb{E}\\left[Y_{i} \\mid X_{i}\\right] \\\\\n&=\\left(\\sum_{i=1}^{n} X_{i} X_{i}^{\\prime}\\right)^{-1} \\sum_{i=1}^{n} X_{i} X_{i}^{\\prime} \\beta \\\\\n&=\\beta .\n\\end{aligned}\n\\]\nNow let’s show the same result using matrix notation. (4.4) implies\n\\[\n\\mathbb{E}[\\boldsymbol{Y} \\mid \\boldsymbol{X}]=\\left(\\begin{array}{c}\n\\vdots \\\\\n\\mathbb{E}\\left[Y_{i} \\mid \\boldsymbol{X}\\right] \\\\\n\\vdots\n\\end{array}\\right)=\\left(\\begin{array}{c}\n\\vdots \\\\\nX_{i}^{\\prime} \\beta \\\\\n\\vdots\n\\end{array}\\right)=\\boldsymbol{X} \\beta .\n\\]\nSimilarly\n\\[\n\\mathbb{E}[\\boldsymbol{e} \\mid \\boldsymbol{X}]=\\left(\\begin{array}{c}\n\\vdots \\\\\n\\mathbb{E}\\left[e_{i} \\mid \\boldsymbol{X}\\right] \\\\\n\\vdots\n\\end{array}\\right)=\\left(\\begin{array}{c}\n\\vdots \\\\\n\\mathbb{E}\\left[e_{i} \\mid X_{i}\\right] \\\\\n\\vdots\n\\end{array}\\right)=0 .\n\\]\nUsing \\(\\widehat{\\beta}=\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{Y}\\right)\\), the conditioning theorem, the linearity of expectations, (4.5), and the properties of the matrix inverse,\n\\[\n\\begin{aligned}\n\\mathbb{E}[\\widehat{\\beta} \\mid \\boldsymbol{X}] &=\\mathbb{E}\\left[\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\boldsymbol{X}^{\\prime} \\boldsymbol{Y} \\mid \\boldsymbol{X}\\right] \\\\\n&=\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\boldsymbol{X}^{\\prime} \\mathbb{E}[\\boldsymbol{Y} \\mid \\boldsymbol{X}] \\\\\n&=\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\boldsymbol{X}^{\\prime} \\boldsymbol{X} \\beta \\\\\n&=\\beta .\n\\end{aligned}\n\\]\nAt the risk of belaboring the derivation, another way to calculate the same result is as follows. Insert \\(\\boldsymbol{Y}=\\boldsymbol{X} \\beta+\\boldsymbol{e}\\) into the formula for \\(\\widehat{\\beta}\\) to obtain\n\\[\n\\begin{aligned}\n\\widehat{\\beta} &=\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1}\\left(\\boldsymbol{X}^{\\prime}(\\boldsymbol{X} \\beta+\\boldsymbol{e})\\right) \\\\\n&=\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\boldsymbol{X}^{\\prime} \\boldsymbol{X} \\beta+\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{e}\\right) \\\\\n&=\\beta+\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\boldsymbol{X}^{\\prime} \\boldsymbol{e} .\n\\end{aligned}\n\\]\nThis is a useful linear decomposition of the estimator \\(\\widehat{\\beta}\\) into the true parameter \\(\\beta\\) and the stochastic component \\(\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\boldsymbol{X}^{\\prime} \\boldsymbol{e}\\). Once again, we can calculate that\n\\[\n\\begin{aligned}\n\\mathbb{E}[\\widehat{\\beta}-\\beta \\mid \\boldsymbol{X}] &=\\mathbb{E}\\left[\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\boldsymbol{X}^{\\prime} \\boldsymbol{e} \\mid \\boldsymbol{X}\\right] \\\\\n&=\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\boldsymbol{X}^{\\prime} \\mathbb{E}[\\boldsymbol{e} \\mid \\boldsymbol{X}]=0 .\n\\end{aligned}\n\\]\nRegardless of the method we have shown that \\(\\mathbb{E}[\\widehat{\\beta} \\mid \\boldsymbol{X}]=\\beta\\). We have shown the following theorem.\nTheorem 4.1 Expectation of Least Squares Estimator In the linear regression model (Assumption 4.2) with i.i.d. sampling (Assumption 4.1)\n\\[\n\\mathbb{E}[\\widehat{\\beta} \\mid \\boldsymbol{X}]=\\beta .\n\\]\nEquation (4.7) says that the estimator \\(\\widehat{\\beta}\\) is unbiased for \\(\\beta\\), conditional on \\(\\boldsymbol{X}\\). This means that the conditional distribution of \\(\\widehat{\\beta}\\) is centered at \\(\\beta\\). By “conditional on \\(X\\)” this means that the distribution is unbiased for any realization of the regressor matrix \\(\\boldsymbol{X}\\). In conditional models we simply refer to this as saying \\(\" \\widehat{\\beta}\\) is unbiased for \\(\\beta\\) “.\nIt is worth mentioning that Theorem 4.1, and all finite sample results in this chapter, make the implicit assumption that \\(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\) is full rank with probability one."
  },
  {
    "objectID": "chpt04-lsr.html#variance-of-least-squares-estimator",
    "href": "chpt04-lsr.html#variance-of-least-squares-estimator",
    "title": "4  Least Squares Regression",
    "section": "4.6 Variance of Least Squares Estimator",
    "text": "4.6 Variance of Least Squares Estimator\nIn this section we calculate the conditional variance of the OLS estimator.\nFor any \\(r \\times 1\\) random vector \\(Z\\) define the \\(r \\times r\\) covariance matrix\n\\[\n\\operatorname{var}[Z]=\\mathbb{E}\\left[(Z-\\mathbb{E}[Z])(Z-\\mathbb{E}[Z])^{\\prime}\\right]=\\mathbb{E}\\left[Z Z^{\\prime}\\right]-(\\mathbb{E}[Z])(\\mathbb{E}[Z])^{\\prime}\n\\]\nand for any pair \\((Z, X)\\) define the conditional covariance matrix\n\\[\n\\operatorname{var}[Z \\mid X]=\\mathbb{E}\\left[(Z-\\mathbb{E}[Z \\mid X])(Z-\\mathbb{E}[Z \\mid X])^{\\prime} \\mid X\\right] .\n\\]\nWe define \\(\\boldsymbol{V}_{\\widehat{\\beta}} \\stackrel{\\text { def }}{=} \\operatorname{var}[\\widehat{\\beta} \\mid \\boldsymbol{X}]\\) as the conditional covariance matrix of the regression coefficient estimators. We now derive its form.\nThe conditional covariance matrix of the \\(n \\times 1\\) regression error \\(\\boldsymbol{e}\\) is the \\(n \\times n\\) matrix\n\\[\n\\operatorname{var}[\\boldsymbol{e} \\mid \\boldsymbol{X}]=\\mathbb{E}\\left[\\boldsymbol{e} \\boldsymbol{e}^{\\prime} \\mid \\boldsymbol{X}\\right] \\stackrel{\\text { def }}{=} \\boldsymbol{D} .\n\\]\nThe \\(i^{t h}\\) diagonal element of \\(\\boldsymbol{D}\\) is\n\\[\n\\mathbb{E}\\left[e_{i}^{2} \\mid \\boldsymbol{X}\\right]=\\mathbb{E}\\left[e_{i}^{2} \\mid X_{i}\\right]=\\sigma_{i}^{2}\n\\]\nwhile the \\(i j^{t h}\\) off-diagonal element of \\(\\boldsymbol{D}\\) is\n\\[\n\\mathbb{E}\\left[e_{i} e_{j} \\mid \\boldsymbol{X}\\right]=\\mathbb{E}\\left(e_{i} \\mid X_{i}\\right) \\mathbb{E}\\left[e_{j} \\mid X_{j}\\right]=0\n\\]\nwhere the first equality uses independence of the observations (Assumption 4.1) and the second is (4.2). Thus \\(\\boldsymbol{D}\\) is a diagonal matrix with \\(i^{t h}\\) diagonal element \\(\\sigma_{i}^{2}\\) :\n\\[\n\\boldsymbol{D}=\\operatorname{diag}\\left(\\sigma_{1}^{2}, \\ldots, \\sigma_{n}^{2}\\right)=\\left(\\begin{array}{cccc}\n\\sigma_{1}^{2} & 0 & \\cdots & 0 \\\\\n0 & \\sigma_{2}^{2} & \\cdots & 0 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & \\cdots & \\sigma_{n}^{2}\n\\end{array}\\right)\n\\]\nIn the special case of the linear homoskedastic regression model (4.3), then \\(\\mathbb{E}\\left[e_{i}^{2} \\mid X_{i}\\right]=\\sigma_{i}^{2}=\\sigma^{2}\\) and we have the simplification \\(\\boldsymbol{D}=\\boldsymbol{I}_{n} \\sigma^{2}\\). In general, however, \\(\\boldsymbol{D}\\) need not necessarily take this simplified form.\nFor any \\(n \\times r\\) matrix \\(\\boldsymbol{A}=\\boldsymbol{A}(\\boldsymbol{X})\\),\n\\[\n\\operatorname{var}\\left[\\boldsymbol{A}^{\\prime} \\boldsymbol{Y} \\mid \\boldsymbol{X}\\right]=\\operatorname{var}\\left[\\boldsymbol{A}^{\\prime} \\boldsymbol{e} \\mid \\boldsymbol{X}\\right]=\\boldsymbol{A}^{\\prime} \\boldsymbol{D} \\boldsymbol{A} .\n\\]\nIn particular, we can write \\(\\widehat{\\beta}=\\boldsymbol{A}^{\\prime} \\boldsymbol{Y}\\) where \\(\\boldsymbol{A}=\\boldsymbol{X}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1}\\) and thus\n\\[\n\\boldsymbol{V}_{\\widehat{\\beta}}=\\operatorname{var}[\\widehat{\\beta} \\mid \\boldsymbol{X}]=\\boldsymbol{A}^{\\prime} \\boldsymbol{D} \\boldsymbol{A}=\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\boldsymbol{X}^{\\prime} \\boldsymbol{D} \\boldsymbol{X}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} .\n\\]\nIt is useful to note that\n\\[\n\\boldsymbol{X}^{\\prime} \\boldsymbol{D} \\boldsymbol{X}=\\sum_{i=1}^{n} X_{i} X_{i}^{\\prime} \\sigma_{i}^{2},\n\\]\na weighted version of \\(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\).\nIn the special case of the linear homoskedastic regression model, \\(\\boldsymbol{D}=\\boldsymbol{I}_{n} \\sigma^{2}\\), so \\(\\boldsymbol{X}^{\\prime} \\boldsymbol{D} \\boldsymbol{X}=\\boldsymbol{X}^{\\prime} \\boldsymbol{X} \\sigma^{2}\\), and the covariance matrix simplifies to \\(\\boldsymbol{V}_{\\widehat{\\beta}}=\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\sigma^{2}\\).\nTheorem 4.2 Variance of Least Squares Estimator In the linear regression model (Assumption 4.2) with i.i.d. sampling (Assumption 4.1)\n\\[\n\\boldsymbol{V}_{\\widehat{\\beta}}=\\operatorname{var}[\\widehat{\\beta} \\mid \\boldsymbol{X}]=\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{D} \\boldsymbol{X}\\right)\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1}\n\\]\nwhere \\(\\boldsymbol{D}\\) is defined in (4.8). If in addition the error is homoskedastic (Assumption 4.3) then (4.10) simplifies to \\(\\boldsymbol{V}_{\\widehat{\\beta}}=\\sigma^{2}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1}\\)."
  },
  {
    "objectID": "chpt04-lsr.html#unconditional-moments",
    "href": "chpt04-lsr.html#unconditional-moments",
    "title": "4  Least Squares Regression",
    "section": "4.7 Unconditional Moments",
    "text": "4.7 Unconditional Moments\nThe previous sections derived the form of the conditional expectation and variance of the least squares estimator where we conditioned on the regressor matrix \\(\\boldsymbol{X}\\). What about the unconditional expectation and variance?\nIndeed, it is not obvious if \\(\\widehat{\\beta}\\) has a finite expectation or variance. Take the case of a single dummy variable regressor \\(D_{i}\\) with no intercept. Assume \\(\\mathbb{P}\\left[D_{i}=1\\right]=p<1\\). Then\n\\[\n\\widehat{\\beta}=\\frac{\\sum_{i=1}^{n} D_{i} Y_{i}}{\\sum_{i=1}^{n} D_{i}}\n\\]\nis well defined if \\(\\sum_{i=1}^{n} D_{i}>0\\). However, \\(\\mathbb{P}\\left[\\sum_{i=1}^{n} D_{i}=0\\right]=(1-p)^{n}>0\\). This means that with positive (but small) probability \\(\\widehat{\\beta}\\) does not exist. Consequently \\(\\widehat{\\beta}\\) has no finite moments! We ignore this complication in practice but it does pose a conundrum for theory. This existence problem arises whenever there are discrete regressors.\nThis dilemma is avoided when the regressors have continuous distributions. A clean statement was obtained by Kinal (1980) under the assumption of normal regressors and errors. Theorem 4.3 Kinal (1980)\nIn the linear regression model with i.i.d. sampling, if in addition \\((X, e)\\) have a joint normal distribution, then for any \\(r, \\mathbb{E}\\|\\widehat{\\beta}\\|^{r}<\\infty\\) if and only if \\(r<n-k+1\\).\nThis shows that when the errors and regressors are normally distributed that the least squares estimator possesses all moments up to \\(n-k\\) which includes all moments of practical interest. The normality assumption is not critical for this result. What is key is the assumption that the regressors are continuously distributed.\nThe law of iterated expectations (Theorem 2.1) combined with Theorems \\(4.1\\) and \\(4.3\\) allow us to deduce that the least squares estimator is unconditionally unbiased. Under the normality assumption Theorem \\(4.3\\) allows us to apply the law of iterated expectations, and thus using Theorems \\(4.1\\) we deduce that if \\(n>k\\)\n\\[\n\\mathbb{E}[\\widehat{\\beta}]=\\mathbb{E}[\\mathbb{E}[\\widehat{\\beta} \\mid \\boldsymbol{X}]]=\\beta .\n\\]\nHence \\(\\widehat{\\beta}\\) is unconditionally unbiased as asserted.\nFurthermore, if \\(n-k>1\\) then \\(\\mathbb{E}\\|\\widehat{\\beta}\\|^{2}<\\infty\\) and \\(\\widehat{\\beta}\\) has a finite unconditional variance. Using Theorem \\(2.8\\) we can calculate explicitly that\n\\[\n\\operatorname{var}[\\widehat{\\beta}]=\\mathbb{E}[\\operatorname{var}[\\widehat{\\beta} \\mid \\boldsymbol{X}]]+\\operatorname{var}[\\mathbb{E}[\\widehat{\\beta} \\mid \\boldsymbol{X}]]=\\mathbb{E}\\left[\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{D} \\boldsymbol{X}\\right)\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1}\\right]\n\\]\nthe second equality because \\(\\mathbb{E}[\\widehat{\\beta} \\mid \\boldsymbol{X}]=\\beta\\) has zero variance. In the homoskedastic case this simplifies to\n\\[\n\\operatorname{var}[\\widehat{\\beta}]=\\sigma^{2} \\mathbb{E}\\left[\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1}\\right] .\n\\]\nIn both cases the expectation cannot pass through the matrix inverse because this is a nonlinear function. Thus there is not a simple expression for the unconditional variance, other than stating that is it the expectation of the conditional variance."
  },
  {
    "objectID": "chpt04-lsr.html#gauss-markov-theorem",
    "href": "chpt04-lsr.html#gauss-markov-theorem",
    "title": "4  Least Squares Regression",
    "section": "4.8 Gauss-Markov Theorem",
    "text": "4.8 Gauss-Markov Theorem\nThe Gauss-Markov Theorem is one of the most celebrated results in econometric theory. It provides a classical justification for the least squares estimator, showing that it is lowest variance among unbiased estimators.\nWrite the homoskedastic linear regression model in vector format as\n\\[\n\\begin{aligned}\n\\boldsymbol{Y} &=\\boldsymbol{X} \\beta+\\boldsymbol{e} \\\\\n\\mathbb{E}[\\boldsymbol{e} \\mid \\boldsymbol{X}] &=0 \\\\\n\\operatorname{var}[\\boldsymbol{e} \\mid \\boldsymbol{X}] &=\\boldsymbol{I}_{n} \\sigma^{2} .\n\\end{aligned}\n\\]\nIn this model we know that the least squares estimator is unbiased for \\(\\beta\\) and has covariance matrix \\(\\sigma^{2}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1}\\). The question raised in this section is if there exists an alternative unbiased estimator \\(\\widetilde{\\beta}\\) which has a smaller covariance matrix.\nThe following version of the theorem is due to B. E. Hansen (2021).\nTheorem 4.4 Gauss-Markov Take the homoskedastic linear regression model (4.11)-(4.13). If \\(\\widetilde{\\beta}\\) is an unbiased estimator of \\(\\beta\\) then\n\\[\n\\operatorname{var}[\\widetilde{\\beta} \\mid \\boldsymbol{X}] \\geq \\sigma^{2}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1}\n\\]\nTheorem \\(4.4\\) provides a lower bound on the covariance matrix of unbiased estimators under the assumption of homoskedasticity. It says that no unbiased estimator can have a variance matrix smaller (in the positive definite sense) than \\(\\sigma^{2}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1}\\). Since the variance of the OLS estimator is exactly equal to this bound this means that no unbiased estimator has a lower variance than OLS. Consequently we describe OLS as efficient in the class of unbiased estimators.\nThis earliest version of Theorem \\(4.4\\) was articulated by Carl Friedrich Gauss in 1823. Andreı̆ Andreevich Markov provided a textbook treatment of the theorem in 1912, and clarified the central role of unbiasedness, which Gauss had only assumed implicitly.\nTheir versions of the Theorem restricted attention to linear estimators of \\(\\beta\\), which are estimators that can be written as \\(\\widetilde{\\beta}=\\boldsymbol{A}^{\\prime} \\boldsymbol{Y}\\), where \\(\\boldsymbol{A}=\\boldsymbol{A}(\\boldsymbol{X})\\) is an \\(m \\times n\\) function of the regressors \\(\\boldsymbol{X}\\). Linearity in this context means “linear in \\(\\boldsymbol{Y}\\)”. This restriction simplifies variance calculations, but greatly limits the class of estimators.This classical version of the Theorem gave rise to the description of OLS as the best linear unbiased estimator (BLUE). However, Theorem \\(4.4\\) as stated above shows that OLS is the best unbiased estimator (BUE).\nThe derivation of the Gauss-Markov Theorem under the restriction to linear estimators is straightforward, so we now provide this demonstration. For \\(\\widetilde{\\beta}=\\boldsymbol{A}^{\\prime} \\boldsymbol{Y}\\) we have\n\\[\n\\mathbb{E}[\\widetilde{\\beta} \\mid \\boldsymbol{X}]=\\boldsymbol{A}^{\\prime} \\mathbb{E}[\\boldsymbol{Y} \\mid \\boldsymbol{X}]=\\boldsymbol{A}^{\\prime} \\boldsymbol{X} \\beta,\n\\]\nthe second equality because \\(\\mathbb{E}[\\boldsymbol{Y} \\mid \\boldsymbol{X}]=\\boldsymbol{X} \\beta\\). Then \\(\\widetilde{\\beta}\\) is unbiased for all \\(\\beta\\) if (and only if) \\(\\boldsymbol{A}^{\\prime} \\boldsymbol{X}=\\boldsymbol{I}_{k}\\). Furthermore, we saw in (4.9) that\n\\[\n\\operatorname{var}[\\widetilde{\\beta} \\mid \\boldsymbol{X}]=\\operatorname{var}\\left[\\boldsymbol{A}^{\\prime} \\boldsymbol{Y} \\mid \\boldsymbol{X}\\right]=\\boldsymbol{A}^{\\prime} \\boldsymbol{D} \\boldsymbol{A}=\\boldsymbol{A}^{\\prime} \\boldsymbol{A} \\boldsymbol{\\sigma}^{2}\n\\]\nthe last equality using the homoskedasticity assumption (4.13). To establish the Theorem we need to show that for any such matrix \\(\\boldsymbol{A}\\),\n\\[\n\\boldsymbol{A}^{\\prime} \\boldsymbol{A} \\geq\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\text {. }\n\\]\nSet \\(\\boldsymbol{C}=\\boldsymbol{A}-\\boldsymbol{X}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1}\\). Note that \\(\\boldsymbol{X}^{\\prime} \\boldsymbol{C}=0\\). We calculate that\n\\[\n\\begin{aligned}\n\\boldsymbol{A}^{\\prime} \\boldsymbol{A}-\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} &=\\left(\\boldsymbol{C}+\\boldsymbol{X}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1}\\right)^{\\prime}\\left(\\boldsymbol{C}+\\boldsymbol{X}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1}\\right)-\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\\\\n&=\\boldsymbol{C}^{\\prime} \\boldsymbol{C}+\\boldsymbol{C}^{\\prime} \\boldsymbol{X}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1}+\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\boldsymbol{X}^{\\prime} \\boldsymbol{C} \\\\\n&+\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1}-\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\\\\n&=\\boldsymbol{C}^{\\prime} \\boldsymbol{C} \\geq 0\n\\end{aligned}\n\\]\nThe final inequality states that the matrix \\(\\boldsymbol{C}^{\\prime} \\boldsymbol{C}\\) is positive semi-definite which is a property of quadratic forms (see Appendix A.10). We have shown (4.14) as required.\nThe above derivation imposed the restriction that the estimator \\(\\widetilde{\\beta}\\) is linear in \\(\\boldsymbol{Y}\\). The proof of Theorem \\(4.4\\) in the general case is considerably more advanced. Here, we provide a simplified sketch of the argument for interested readers, with a complete proof in Section 4.24. For simplicity, treat the regressors \\(X\\) as fixed, and suppose that \\(Y\\) has a density \\(f(y)\\) with bounded support \\(\\mathscr{Y}\\). Without loss of generality assume that the true coefficient equals \\(\\beta_{0}=0\\).\nSince \\(Y\\) has bounded support \\(\\mathscr{Y}\\) there is a set \\(B \\subset \\mathbb{R}^{m}\\) such that \\(\\left|y X^{\\prime} \\beta / \\sigma^{2}\\right|<1\\) for all \\(\\beta \\in B\\) and \\(y \\in \\mathscr{Y}\\). For such values of \\(\\beta\\), define the auxiliary density function\n\\[\nf_{\\beta}(y)=f(y)\\left(1+y X^{\\prime} \\beta / \\sigma^{2}\\right) .\n\\]\nUnder the assumptions, \\(0 \\leq f_{\\beta}(y) \\leq 2 f(y), f_{\\beta}(y)\\) has support \\(\\mathscr{Y}\\), and \\(\\int_{\\mathscr{Y}} f_{\\beta}(y) d y=1\\). To see the later, observe that \\(\\int_{\\mathscr{Y}} y f(y) d y=X^{\\prime} \\beta_{0}=0\\) under the normalization \\(\\beta_{0}=0\\), and thus\n\\[\n\\int_{\\mathscr{Y}} f_{\\beta}(y) d y=\\int_{\\mathscr{Y}} f(y) d y+\\int_{\\mathscr{Y}} f(y) y d y X^{\\prime} \\beta / \\sigma^{2}=1\n\\]\nbecause \\(\\int_{\\mathscr{Y}} f(y) d y=1\\). Thus \\(f_{\\beta}\\) is a parametric family of density functions. Evaluated at \\(\\beta_{0}\\) we see that \\(f_{0}=f\\), which means that \\(f_{\\beta}\\) is a correctly-specified parametric family with true parameter value \\(\\beta_{0}=0\\).\nTo illustrate, take the case \\(X=1\\). Figure 4.1 displays an example density \\(f(y)=(3 / 4)\\left(1-y^{2}\\right)\\) on \\([-1,1]\\) with auxiliary density \\(f_{\\beta}(y)=f(y)(1+y)\\). We can see how the auxiliary density is a tilted version of the original density \\(f(y)\\).\n\nFigure 4.1: Original and Auxiliary Density\nLet \\(\\mathbb{E}_{\\beta}\\) denote expectation with respect to the auxiliary distribution. Since \\(\\int_{\\mathscr{Y}} y f(y) d y=0\\) and \\(\\int_{\\mathscr{Y}} y^{2} f(y) d y=\\) \\(\\sigma^{2}\\), we find\n\\[\n\\mathbb{E}_{\\beta}[Y]=\\int_{\\mathscr{Y}} y f_{\\beta}(y) d y=\\int_{\\mathscr{Y}} y f(y) d y+\\int_{\\mathscr{Y}} y^{2} f(y) d y X^{\\prime} \\beta / \\sigma^{2}=X^{\\prime} \\beta .\n\\]\nThis shows that \\(f_{\\beta}\\) is a regression model with regression coefficient \\(\\beta\\).\nIn Figure 4.1, the means of the two densities are indicated by the arrows to the \\(\\mathrm{x}\\)-axis. In this example we can see how the auxiliary density has a larger expected value, because the density has been tilted to the right.\nThe parametric family \\(f_{\\beta}\\) over \\(\\beta \\in B\\) has the following properties: its expectation is \\(X^{\\prime} \\beta\\), its variance is finite, the true value \\(\\beta_{0}\\) lies in the interior of \\(B\\), and the support of the distribution does not depend on \\(\\beta\\).\nThe likelihood score of the auxiliary density function for an observation, using the fact that \\(Y_{i}=e_{i}\\), is\n\\[\nS_{i}=\\left.\\frac{\\partial}{\\partial \\beta}\\left(\\log f_{\\beta}\\left(Y_{i}\\right)\\right)\\right|_{\\beta=0}=\\left.\\frac{\\partial}{\\partial \\beta}\\left(\\log f\\left(e_{i}\\right)+\\log \\left(1+e_{i} X_{i}^{\\prime} \\beta / \\sigma^{2}\\right)\\right)\\right|_{\\beta=0}=X_{i} e_{i} / \\sigma^{2} .\n\\]\nTherefore the information matrix is\n\\[\n\\mathscr{I}=\\sum_{i=1}^{n} \\mathbb{E}\\left[S_{i} S_{i}^{\\prime}\\right]=\\sum_{i=1}^{n} X_{i} X_{i}^{\\prime} \\mathbb{E}\\left[e_{i}^{2}\\right] / \\sigma^{4}=\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right) / \\sigma^{2} .\n\\]\nBy assumption, \\(\\widetilde{\\beta}\\) is unbiased. The Cramér-Rao lower bound states that\n\\[\n\\operatorname{var}[\\widetilde{\\beta}] \\geq \\mathscr{I}^{-1}=\\sigma^{2}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} .\n\\]\nThis is the variance lower bound, completing the proof of Theorem 4.4.\nThe above argument is rather tricky. At its core is the observation that the model \\(f_{\\beta}\\) is a submodel of the set of all linear regression models. The Cramér-Rao bound over any regular parametric submodel is a lower bound on the variance of any unbiased estimator. This means that the Cramér-Rao bound over \\(f_{\\beta}\\) is a lower bound for unbiased estimation of the regression coefficient. The model \\(f_{\\beta}\\) was selected judiciously so that its Cramér-Rao bound equals the variance of the least squares estimator, and this is sufficient to establish the bound."
  },
  {
    "objectID": "chpt04-lsr.html#generalized-least-squares",
    "href": "chpt04-lsr.html#generalized-least-squares",
    "title": "4  Least Squares Regression",
    "section": "4.9 Generalized Least Squares",
    "text": "4.9 Generalized Least Squares\nTake the linear regression model in matrix format\n\\[\n\\boldsymbol{Y}=\\boldsymbol{X} \\beta+\\boldsymbol{e} .\n\\]\nConsider a generalized situation where the observation errors are possibly correlated and/or heteroskedastic. Specifically, suppose that\n\\[\n\\begin{gathered}\n\\mathbb{E}[\\boldsymbol{e} \\mid \\boldsymbol{X}]=0 \\\\\n\\operatorname{var}[\\boldsymbol{e} \\mid \\boldsymbol{X}]=\\Sigma \\sigma^{2}\n\\end{gathered}\n\\]\nfor some \\(n \\times n\\) matrix \\(\\Sigma>0\\), possibly a function of \\(\\boldsymbol{X}\\), and some scalar \\(\\sigma^{2}\\). This includes the independent sampling framework where \\(\\Sigma\\) is diagonal but allows for non-diagonal covariance matrices as well. As a scaled covariance matrix, \\(\\Sigma\\) is necessarily symmetric and positive semi-definite.\nUnder these assumptions, by arguments similar to the previous sections we can calculate the expectation and variance of the OLS estimator:\n\\[\n\\begin{gathered}\n\\mathbb{E}[\\widehat{\\beta} \\mid \\boldsymbol{X}]=\\beta \\\\\n\\operatorname{var}[\\widehat{\\beta} \\mid \\boldsymbol{X}]=\\sigma^{2}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1}\\left(\\boldsymbol{X}^{\\prime} \\Sigma \\boldsymbol{X}\\right)\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1}\n\\end{gathered}\n\\]\n(see Exercise 4.5).\nAitken (1935) established a generalization of the Gauss-Markov Theorem. The following statement is due to B. E. Hansen (2021). Theorem 4.5 Take the linear regression model (4.17)-(4.19). If \\(\\widetilde{\\beta}\\) is an unbiased estimator of \\(\\beta\\) then\n\\[\n\\operatorname{var}[\\widetilde{\\beta} \\mid \\boldsymbol{X}] \\geq \\sigma^{2}\\left(\\boldsymbol{X}^{\\prime} \\Sigma^{-1} \\boldsymbol{X}\\right)^{-1}\n\\]\nWe defer the proof to Section 4.24. See also Exercise 4.6.\nTheorem \\(4.5\\) provides a lower bound on the covariance matrix of unbiased estimators. Theorem \\(4.4\\) was the special case \\(\\Sigma=\\boldsymbol{I}_{n}\\).\nWhen \\(\\Sigma\\) is known, Aitken (1935) constructed an estimator which achieves the lower bound in Theorem 4.5. Take the linear model (4.17) and pre-multiply by \\(\\Sigma^{-1 / 2}\\). This produces the equation \\(\\tilde{\\boldsymbol{Y}}=\\widetilde{\\boldsymbol{X}} \\beta+\\widetilde{\\boldsymbol{e}}\\) where \\(\\tilde{\\boldsymbol{Y}}=\\Sigma^{-1 / 2} \\boldsymbol{Y}, \\widetilde{\\boldsymbol{X}}=\\Sigma^{-1 / 2} \\boldsymbol{X}\\), and \\(\\widetilde{\\boldsymbol{e}}=\\Sigma^{-1 / 2} \\boldsymbol{e}\\). Consider OLS estimation of \\(\\beta\\) in this equation.\n\\[\n\\begin{aligned}\n\\widetilde{\\beta}_{\\text {gls }} &=\\left(\\widetilde{\\boldsymbol{X}}^{\\prime} \\widetilde{\\boldsymbol{X}}\\right)^{-1} \\widetilde{\\boldsymbol{X}}^{\\prime} \\widetilde{\\boldsymbol{Y}} \\\\\n&=\\left(\\left(\\Sigma^{-1 / 2} \\boldsymbol{X}\\right)^{\\prime}\\left(\\Sigma^{-1 / 2} \\boldsymbol{X}\\right)\\right)^{-1}\\left(\\Sigma^{-1 / 2} \\boldsymbol{X}\\right)^{\\prime}\\left(\\Sigma^{-1 / 2} \\boldsymbol{Y}\\right) \\\\\n&=\\left(\\boldsymbol{X}^{\\prime} \\Sigma^{-1} \\boldsymbol{X}\\right)^{-1} \\boldsymbol{X}^{\\prime} \\Sigma^{-1} \\boldsymbol{Y} .\n\\end{aligned}\n\\]\nThis is called the Generalized Least Squares (GLS) estimator of \\(\\beta\\).\nYou can calculate that\n\\[\n\\begin{gathered}\n\\mathbb{E}\\left[\\widetilde{\\beta}_{\\text {gls }} \\mid \\boldsymbol{X}\\right]=\\beta \\\\\n\\operatorname{var}\\left[\\widetilde{\\beta}_{\\text {gls }} \\mid \\boldsymbol{X}\\right]=\\sigma^{2}\\left(\\boldsymbol{X}^{\\prime} \\Sigma^{-1} \\boldsymbol{X}\\right)^{-1} .\n\\end{gathered}\n\\]\nThis shows that the GLS estimator is unbiased and has a covariance matrix which equals the lower bound from Theorem 4.5. This shows that the lower bound is sharp. GLS is thus efficient in the class of unbiased estimators.\nIn the linear regression model with independent observations and known conditional variances, so that \\(\\Sigma=\\boldsymbol{D}=\\operatorname{diag}\\left(\\sigma_{1}^{2}, \\ldots, \\sigma_{n}^{2}\\right)\\), the GLS estimator takes the form\n\\[\n\\begin{aligned}\n\\widetilde{\\beta}_{\\mathrm{gls}} &=\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{D}^{-1} \\boldsymbol{X}\\right)^{-1} \\boldsymbol{X}^{\\prime} \\boldsymbol{D}^{-1} \\boldsymbol{Y} \\\\\n&=\\left(\\sum_{i=1}^{n} \\sigma_{i}^{-2} X_{i} X_{i}^{\\prime}\\right)^{-1}\\left(\\sum_{i=1}^{n} \\sigma_{i}^{-2} X_{i} Y_{i}\\right) .\n\\end{aligned}\n\\]\nThe assumption \\(\\Sigma>0\\) in this case reduces to \\(\\sigma_{i}^{2}>0\\) for \\(i=1, \\ldots n\\).\nIn most settings the matrix \\(\\Sigma\\) is unknown so the GLS estimator is not feasible. However, the form of the GLS estimator motivates feasible versions, effectively by replacing \\(\\Sigma\\) with a suitable estimator."
  },
  {
    "objectID": "chpt04-lsr.html#residuals",
    "href": "chpt04-lsr.html#residuals",
    "title": "4  Least Squares Regression",
    "section": "4.10 Residuals",
    "text": "4.10 Residuals\nWhat are some properties of the residuals \\(\\widehat{e}_{i}=Y_{i}-X_{i}^{\\prime} \\widehat{\\beta}\\) and prediction errors \\(\\widetilde{e}_{i}=Y_{i}-X_{i}^{\\prime} \\widehat{\\beta}_{(-i)}\\) in the context of the linear regression model?\nRecall from (3.24) that we can write the residuals in vector notation as \\(\\widehat{\\boldsymbol{e}}=\\boldsymbol{M} \\boldsymbol{e}\\) where \\(\\boldsymbol{M}=\\boldsymbol{I}_{n}-\\) \\(\\boldsymbol{X}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\boldsymbol{X}^{\\prime}\\) is the orthogonal projection matrix. Using the properties of conditional expectation\n\\[\n\\mathbb{E}[\\widehat{\\boldsymbol{e}} \\mid \\boldsymbol{X}]=\\mathbb{E}[\\boldsymbol{M e} \\mid \\boldsymbol{X}]=\\boldsymbol{M} \\mathbb{E}[\\boldsymbol{e} \\mid \\boldsymbol{X}]=0\n\\]\nand\n\\[\n\\operatorname{var}[\\widehat{\\boldsymbol{e}} \\mid \\boldsymbol{X}]=\\operatorname{var}[\\boldsymbol{M} \\boldsymbol{e} \\mid \\boldsymbol{X}]=\\boldsymbol{M} \\operatorname{var}[\\boldsymbol{e} \\mid \\boldsymbol{X}] \\boldsymbol{M}=\\boldsymbol{M D} \\boldsymbol{M}\n\\]\nwhere \\(\\boldsymbol{D}\\) is defined in (4.8).\nWe can simplify this expression under the assumption of conditional homoskedasticity\n\\[\n\\mathbb{E}\\left[e^{2} \\mid X\\right]=\\sigma^{2} .\n\\]\nIn this case (4.25) simplifies to\n\\[\n\\operatorname{var}[\\widehat{\\boldsymbol{e}} \\mid \\boldsymbol{X}]=\\boldsymbol{M} \\sigma^{2} .\n\\]\nIn particular, for a single observation \\(i\\) we can find the variance of \\(\\widehat{e}_{i}\\) by taking the \\(i^{t h}\\) diagonal element of (4.26). Since the \\(i^{t h}\\) diagonal element of \\(M\\) is \\(1-h_{i i}\\) as defined in (3.40) we obtain\n\\[\n\\operatorname{var}\\left[\\widehat{e}_{i} \\mid \\boldsymbol{X}\\right]=\\mathbb{E}\\left[\\widehat{e}_{i}^{2} \\mid \\boldsymbol{X}\\right]=\\left(1-h_{i i}\\right) \\sigma^{2} .\n\\]\nAs this variance is a function of \\(h_{i i}\\) and hence \\(X_{i}\\) the residuals \\(\\widehat{e}_{i}\\) are heteroskedastic even if the errors \\(e_{i}\\) are homoskedastic. Notice as well that (4.27) implies \\(\\widehat{e}_{i}^{2}\\) is a biased estimator of \\(\\sigma^{2}\\).\nSimilarly, recall from (3.45) that the prediction errors \\(\\widetilde{e}_{i}=\\left(1-h_{i i}\\right)^{-1} \\widehat{e}_{i}\\) can be written in vector notation as \\(\\widetilde{\\boldsymbol{e}}=\\boldsymbol{M}^{*} \\widehat{\\boldsymbol{e}}\\) where \\(\\boldsymbol{M}^{*}\\) is a diagonal matrix with \\(i^{t h}\\) diagonal element \\(\\left(1-h_{i i}\\right)^{-1}\\). Thus \\(\\widetilde{\\boldsymbol{e}}=\\boldsymbol{M}^{*} \\boldsymbol{M} \\boldsymbol{e}\\). We can calculate that\n\\[\n\\mathbb{E}[\\tilde{\\boldsymbol{e}} \\mid \\boldsymbol{X}]=\\boldsymbol{M}^{*} \\boldsymbol{M} \\mathbb{E}[\\boldsymbol{e} \\mid \\boldsymbol{X}]=0\n\\]\nand\n\\[\n\\operatorname{var}[\\widetilde{\\boldsymbol{e}} \\mid \\boldsymbol{X}]=\\boldsymbol{M}^{*} \\boldsymbol{M} \\operatorname{var}[\\boldsymbol{e} \\mid \\boldsymbol{X}] \\boldsymbol{M} \\boldsymbol{M}^{*}=\\boldsymbol{M}^{*} \\boldsymbol{M D} \\boldsymbol{M} \\boldsymbol{M}^{*}\n\\]\nwhich simplifies under homoskedasticity to\n\\[\n\\operatorname{var}[\\widetilde{\\boldsymbol{e}} \\mid \\boldsymbol{X}]=\\boldsymbol{M}^{*} \\boldsymbol{M} \\boldsymbol{M} \\boldsymbol{M}^{*} \\sigma^{2}=\\boldsymbol{M}^{*} \\boldsymbol{M} \\boldsymbol{M}^{*} \\sigma^{2} .\n\\]\nThe variance of the \\(i^{t h}\\) prediction error is then\n\\[\n\\begin{aligned}\n\\operatorname{var}\\left[\\widetilde{e}_{i} \\mid \\boldsymbol{X}\\right] &=\\mathbb{E}\\left[\\widetilde{e}_{i}^{2} \\mid \\boldsymbol{X}\\right] \\\\\n&=\\left(1-h_{i i}\\right)^{-1}\\left(1-h_{i i}\\right)\\left(1-h_{i i}\\right)^{-1} \\sigma^{2} \\\\\n&=\\left(1-h_{i i}\\right)^{-1} \\sigma^{2} .\n\\end{aligned}\n\\]\nA residual with constant conditional variance can be obtained by rescaling. The standardized residuals are\n\\[\n\\bar{e}_{i}=\\left(1-h_{i i}\\right)^{-1 / 2} \\widehat{e}_{i},\n\\]\nand in vector notation\n\\[\n\\overline{\\boldsymbol{e}}=\\left(\\bar{e}_{1}, \\ldots, \\bar{e}_{n}\\right)^{\\prime}=\\boldsymbol{M}^{* 1 / 2} \\boldsymbol{M e} .\n\\]\nFrom the above calculations, under homoskedasticity,\n\\[\n\\operatorname{var}[\\overline{\\boldsymbol{e}} \\mid \\boldsymbol{X}]=\\boldsymbol{M}^{* 1 / 2} \\boldsymbol{M} \\boldsymbol{M}^{* 1 / 2} \\sigma^{2}\n\\]\nand\n\\[\n\\operatorname{var}\\left[\\bar{e}_{i} \\mid \\boldsymbol{X}\\right]=\\mathbb{E}\\left[\\bar{e}_{i}^{2} \\mid \\boldsymbol{X}\\right]=\\sigma^{2}\n\\]\nand thus these standardized residuals have the same bias and variance as the original errors when the latter are homoskedastic."
  },
  {
    "objectID": "chpt04-lsr.html#estimation-of-error-variance",
    "href": "chpt04-lsr.html#estimation-of-error-variance",
    "title": "4  Least Squares Regression",
    "section": "4.11 Estimation of Error Variance",
    "text": "4.11 Estimation of Error Variance\nThe error variance \\(\\sigma^{2}=\\mathbb{E}\\left[e^{2}\\right]\\) can be a parameter of interest even in a heteroskedastic regression or a projection model. \\(\\sigma^{2}\\) measures the variation in the “unexplained” part of the regression. Its method of moments estimator (MME) is the sample average of the squared residuals:\n\\[\n\\widehat{\\sigma}^{2}=\\frac{1}{n} \\sum_{i=1}^{n} \\widehat{e}_{i}^{2} .\n\\]\nIn the linear regression model we can calculate the expectation of \\(\\widehat{\\sigma}^{2}\\). From (3.28) and the properties of the trace operator observe that\n\\[\n\\widehat{\\sigma}^{2}=\\frac{1}{n} \\boldsymbol{e}^{\\prime} \\boldsymbol{M} \\boldsymbol{e}=\\frac{1}{n} \\operatorname{tr}\\left(\\boldsymbol{e}^{\\prime} \\boldsymbol{M} \\boldsymbol{e}\\right)=\\frac{1}{n} \\operatorname{tr}\\left(\\boldsymbol{M e}^{\\prime}\\right) .\n\\]\nThen\n\\[\n\\begin{aligned}\n\\mathbb{E}\\left[\\widehat{\\sigma}^{2} \\mid \\boldsymbol{X}\\right] &=\\frac{1}{n} \\operatorname{tr}\\left(\\mathbb{E}\\left[\\boldsymbol{M e e}^{\\prime} \\mid \\boldsymbol{X}\\right]\\right) \\\\\n&=\\frac{1}{n} \\operatorname{tr}\\left(\\boldsymbol{M}\\left[\\boldsymbol{e} \\boldsymbol{e}^{\\prime} \\mid \\boldsymbol{X}\\right]\\right) \\\\\n&=\\frac{1}{n} \\operatorname{tr}(\\boldsymbol{M D}) \\\\\n&=\\frac{1}{n} \\sum_{i=1}^{n}\\left(1-h_{i i}\\right) \\sigma_{i}^{2}\n\\end{aligned}\n\\]\nThe final equality holds because the trace is the sum of the diagonal elements of \\(\\boldsymbol{M D}\\), and because \\(\\boldsymbol{D}\\) is diagonal the diagonal elements of \\(M D\\) are the product of the diagonal elements of \\(M\\) and \\(\\boldsymbol{D}\\) which are \\(1-h_{i i}\\) and \\(\\sigma_{i}^{2}\\), respectively.\nAdding the assumption of conditional homoskedasticity \\(\\mathbb{E}\\left[e^{2} \\mid X\\right]=\\sigma^{2}\\) so that \\(\\boldsymbol{D}=\\boldsymbol{I}_{n} \\sigma^{2}\\), then (4.30) simplifies to\n\\[\n\\mathbb{E}\\left[\\widehat{\\sigma}^{2} \\mid \\boldsymbol{X}\\right]=\\frac{1}{n} \\operatorname{tr}\\left(\\boldsymbol{M} \\sigma^{2}\\right)=\\sigma^{2}\\left(\\frac{n-k}{n}\\right)\n\\]\nthe final equality by (3.22). This calculation shows that \\(\\widehat{\\sigma}^{2}\\) is biased towards zero. The order of the bias depends on \\(k / n\\), the ratio of the number of estimated coefficients to the sample size.\nAnother way to see this is to use (4.27). Note that\n\\[\n\\mathbb{E}\\left[\\widehat{\\sigma}^{2} \\mid \\boldsymbol{X}\\right]=\\frac{1}{n} \\sum_{i=1}^{n} \\mathbb{E}\\left[\\widehat{e}_{i}^{2} \\mid \\boldsymbol{X}\\right]=\\frac{1}{n} \\sum_{i=1}^{n}\\left(1-h_{i i}\\right) \\sigma^{2}=\\left(\\frac{n-k}{n}\\right) \\sigma^{2}\n\\]\nthe last equality using Theorem 3.6.\nSince the bias takes a scale form a classic method to obtain an unbiased estimator is by rescaling. Define\n\\[\ns^{2}=\\frac{1}{n-k} \\sum_{i=1}^{n} \\widehat{e}_{i}^{2} .\n\\]\nBy the above calculation \\(\\mathbb{E}\\left[s^{2} \\mid \\boldsymbol{X}\\right]=\\sigma^{2}\\) and \\(\\mathbb{E}\\left[s^{2}\\right]=\\sigma^{2}\\). Hence the estimator \\(s^{2}\\) is unbiased for \\(\\sigma^{2}\\). Consequently, \\(s^{2}\\) is known as the bias-corrected estimator for \\(\\sigma^{2}\\) and in empirical practice \\(s^{2}\\) is the most widely used estimator for \\(\\sigma^{2}\\). Interestingly, this is not the only method to construct an unbiased estimator for \\(\\sigma^{2}\\). An estimator constructed with the standardized residuals \\(\\bar{e}_{i}\\) from (4.28) is\n\\[\n\\bar{\\sigma}^{2}=\\frac{1}{n} \\sum_{i=1}^{n} \\bar{e}_{i}^{2}=\\frac{1}{n} \\sum_{i=1}^{n}\\left(1-h_{i i}\\right)^{-1} \\widehat{e}_{i}^{2} .\n\\]\nYou can show (see Exercise 4.9) that\n\\[\n\\mathbb{E}\\left[\\bar{\\sigma}^{2} \\mid \\boldsymbol{X}\\right]=\\sigma^{2}\n\\]\nand thus \\(\\bar{\\sigma}^{2}\\) is unbiased for \\(\\sigma^{2}\\) (in the homoskedastic linear regression model).\nWhen \\(k / n\\) is small the estimators \\(\\widehat{\\sigma}^{2}, s^{2}\\) and \\(\\bar{\\sigma}^{2}\\) are likely to be similar to one another. However, if \\(k / n\\) is large then \\(s^{2}\\) and \\(\\bar{\\sigma}^{2}\\) are generally preferred to \\(\\widehat{\\sigma}^{2}\\). Consequently it is best to use one of the biascorrected variance estimators in applications."
  },
  {
    "objectID": "chpt04-lsr.html#mean-square-forecast-error",
    "href": "chpt04-lsr.html#mean-square-forecast-error",
    "title": "4  Least Squares Regression",
    "section": "4.12 Mean-Square Forecast Error",
    "text": "4.12 Mean-Square Forecast Error\nOne use of an estimated regression is to predict out-of-sample. Consider an out-of-sample realization \\(\\left(Y_{n+1}, X_{n+1}\\right)\\) where \\(X_{n+1}\\) is observed but not \\(Y_{n+1}\\). Given the coefficient estimator \\(\\widehat{\\beta}\\) the standard point estimator of \\(\\mathbb{E}\\left[Y_{n+1} \\mid X_{n+1}\\right]=X_{n+1}^{\\prime} \\beta\\) is \\(\\widetilde{Y}_{n+1}=X_{n+1}^{\\prime} \\widehat{\\beta}\\). The forecast error is the difference between the actual value \\(Y_{n+1}\\) and the point forecast \\(\\widetilde{Y}_{n+1}\\). This is the forecast error \\(\\widetilde{e}_{n+1}=Y_{n+1}-\\widetilde{Y}_{n+1}\\). The meansquared forecast error (MSFE) is its expected squared value \\(\\operatorname{MSFE}_{n}=\\mathbb{E}\\left[\\widetilde{e}_{n+1}^{2}\\right]\\). In the linear regression model \\(\\widetilde{e}_{n+1}=e_{n+1}-X_{n+1}^{\\prime}(\\widehat{\\beta}-\\beta)\\) so\n\\[\n\\operatorname{MSFE}_{n}=\\mathbb{E}\\left[e_{n+1}^{2}\\right]-2 \\mathbb{E}\\left[e_{n+1} X_{n+1}^{\\prime}(\\widehat{\\beta}-\\beta)\\right]+\\mathbb{E}\\left[X_{n+1}^{\\prime}(\\widehat{\\beta}-\\beta)(\\widehat{\\beta}-\\beta)^{\\prime} X_{n+1}\\right] .\n\\]\nThe first term in (4.33) is \\(\\sigma^{2}\\). The second term in (4.33) is zero because \\(e_{n+1} X_{n+1}^{\\prime}\\) is independent of \\(\\widehat{\\beta}-\\beta\\) and both are mean zero. Using the properties of the trace operator the third term in (4.33) is\n\\[\n\\begin{aligned}\n&\\operatorname{tr}\\left(\\mathbb{E}\\left[X_{n+1} X_{n+1}^{\\prime}\\right] \\mathbb{E}\\left[(\\widehat{\\beta}-\\beta)(\\widehat{\\beta}-\\beta)^{\\prime}\\right]\\right) \\\\\n&=\\operatorname{tr}\\left(\\mathbb{E}\\left[X_{n+1} X_{n+1}^{\\prime}\\right] \\mathbb{E}\\left[\\mathbb{E}\\left[(\\widehat{\\beta}-\\beta)(\\widehat{\\beta}-\\beta)^{\\prime} \\mid \\boldsymbol{X}\\right]\\right]\\right) \\\\\n&=\\operatorname{tr}\\left(\\mathbb{E}\\left[X_{n+1} X_{n+1}^{\\prime}\\right] \\mathbb{E}\\left[\\boldsymbol{V}_{\\widehat{\\beta}}\\right]\\right) \\\\\n&=\\mathbb{E}\\left[\\operatorname{tr}\\left(\\left(X_{n+1} X_{n+1}^{\\prime}\\right) \\boldsymbol{V}_{\\widehat{\\beta}}\\right)\\right] \\\\\n&=\\mathbb{E}\\left[X_{n+1}^{\\prime} \\boldsymbol{V}_{\\widehat{\\beta}} X_{n+1}\\right]\n\\end{aligned}\n\\]\nwhere we use the fact that \\(X_{n+1}\\) is independent of \\(\\widehat{\\beta}\\), the definition \\(\\boldsymbol{V}_{\\widehat{\\beta}}=\\mathbb{E}\\left[(\\widehat{\\beta}-\\beta)(\\widehat{\\beta}-\\beta)^{\\prime} \\mid \\boldsymbol{X}\\right]\\), and the fact that \\(X_{n+1}\\) is independent of \\(\\boldsymbol{V}_{\\widehat{\\beta}}\\). Thus\n\\[\n\\operatorname{MSFE}_{n}=\\sigma^{2}+\\mathbb{E}\\left[X_{n+1}^{\\prime} \\boldsymbol{V}_{\\widehat{\\beta}} X_{n+1}\\right] .\n\\]\nUnder conditional homoskedasticity this simplifies to\n\\[\n\\operatorname{MSFE}_{n}=\\sigma^{2}\\left(1+\\mathbb{E}\\left[X_{n+1}^{\\prime}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} X_{n+1}\\right]\\right) .\n\\]\nA simple estimator for the MSFE is obtained by averaging the squared prediction errors (3.46)\n\\[\n\\widetilde{\\sigma}^{2}=\\frac{1}{n} \\sum_{i=1}^{n} \\widetilde{e}_{i}^{2}\n\\]\nwhere \\(\\widetilde{e}_{i}=Y_{i}-X_{i}^{\\prime} \\widehat{\\beta}_{(-i)}=\\widehat{e}_{i}\\left(1-h_{i i}\\right)^{-1}\\). Indeed, we can calculate that\n\\[\n\\begin{aligned}\n\\mathbb{E}\\left[\\widetilde{\\sigma}^{2}\\right] &=\\mathbb{E}\\left[\\widetilde{e}_{i}^{2}\\right] \\\\\n&=\\mathbb{E}\\left[\\left(e_{i}-X_{i}^{\\prime}\\left(\\widehat{\\beta}_{(-i)}-\\beta\\right)\\right)^{2}\\right] \\\\\n&=\\sigma^{2}+\\mathbb{E}\\left[X_{i}^{\\prime}\\left(\\widehat{\\beta}_{(-i)}-\\beta\\right)\\left(\\widehat{\\beta}_{(-i)}-\\beta\\right)^{\\prime} X_{i}\\right] .\n\\end{aligned}\n\\]\nBy a similar calculation as in (4.34) we find\n\\[\n\\mathbb{E}\\left[\\widetilde{\\sigma}^{2}\\right]=\\sigma^{2}+\\mathbb{E}\\left[X_{i}^{\\prime} \\boldsymbol{V}_{\\widehat{\\beta}_{(-i)}} X_{i}\\right]=\\operatorname{MSFE}_{n-1} .\n\\]\nThis is the MSFE based on a sample of size \\(n-1\\) rather than size \\(n\\). The difference arises because the in-sample prediction errors \\(\\widetilde{e}_{i}\\) for \\(i \\leq n\\) are calculated using an effective sample size of \\(n-1\\), while the out-of sample prediction error \\(\\widetilde{e}_{n+1}\\) is calculated from a sample with the full \\(n\\) observations. Unless \\(n\\) is very small we should expect \\(\\operatorname{MSFE}_{n-1}\\) (the MSFE based on \\(n-1\\) observations) to be close to \\(\\mathrm{MSFE}_{n}\\) (the MSFE based on \\(n\\) observations). Thus \\(\\widetilde{\\sigma}^{2}\\) is a reasonable estimator for MSFE \\(n\\).\nTheorem 4.6 MSFE In the linear regression model (Assumption 4.2) and i.i.d. sampling (Assumption 4.1)\n\\[\n\\operatorname{MSFE}_{n}=\\mathbb{E}\\left[\\widetilde{e}_{n+1}^{2}\\right]=\\sigma^{2}+\\mathbb{E}\\left[X_{n+1}^{\\prime} \\boldsymbol{V}_{\\widehat{\\beta}} X_{n+1}\\right]\n\\]\nwhere \\(\\boldsymbol{V}_{\\widehat{\\beta}}=\\operatorname{var}[\\widehat{\\beta} \\mid \\boldsymbol{X}]\\). Furthermore, \\(\\widetilde{\\sigma}^{2}\\) defined in (3.46) is an unbiased estimator of \\(\\operatorname{MSFE}_{n-1}\\), because \\(\\mathbb{E}\\left[\\widetilde{\\sigma}^{2}\\right]=\\operatorname{MSFE}_{n-1}\\)."
  },
  {
    "objectID": "chpt04-lsr.html#covariance-matrix-estimation-under-homoskedasticity",
    "href": "chpt04-lsr.html#covariance-matrix-estimation-under-homoskedasticity",
    "title": "4  Least Squares Regression",
    "section": "4.13 Covariance Matrix Estimation Under Homoskedasticity",
    "text": "4.13 Covariance Matrix Estimation Under Homoskedasticity\nFor inference we need an estimator of the covariance matrix \\(\\boldsymbol{V}_{\\widehat{\\beta}}\\) of the least squares estimator. In this section we consider the homoskedastic regression model (Assumption 4.3).\nUnder homoskedasticity the covariance matrix takes the simple form\n\\[\n\\boldsymbol{V}_{\\widehat{\\beta}}^{0}=\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\sigma^{2}\n\\]\nwhich is known up to the scale \\(\\sigma^{2}\\). In Section \\(4.11\\) we discussed three estimators of \\(\\sigma^{2}\\). The most commonly used choice is \\(s^{2}\\) leading to the classic covariance matrix estimator\n\\[\n\\widehat{\\boldsymbol{V}}_{\\widehat{\\beta}}^{0}=\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} s^{2} .\n\\]\nSince \\(s^{2}\\) is conditionally unbiased for \\(\\sigma^{2}\\) it is simple to calculate that \\(\\widehat{\\boldsymbol{V}}_{\\widehat{\\beta}}^{0}\\) is conditionally unbiased for \\(\\boldsymbol{V}_{\\widehat{\\beta}}\\) under the assumption of homoskedasticity:\n\\[\n\\mathbb{E}\\left[\\widehat{\\boldsymbol{V}}_{\\widehat{\\beta}}^{0} \\mid \\boldsymbol{X}\\right]=\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\mathbb{E}\\left[s^{2} \\mid \\boldsymbol{X}\\right]=\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\sigma^{2}=\\boldsymbol{V}_{\\widehat{\\beta}} .\n\\]\nThis was the dominant covariance matrix estimator in applied econometrics for many years and is still the default method in most regression packages. For example, Stata uses the covariance matrix estimator (4.35) by default in linear regression unless an alternative is specified. If the estimator (4.35) is used but the regression error is heteroskedastic it is possible for \\(\\widehat{\\boldsymbol{V}}_{\\widehat{\\beta}}^{0}\\) to be quite biased for the correct covariance matrix \\(\\boldsymbol{V}_{\\widehat{\\beta}}=\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{D} \\boldsymbol{X}\\right)\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1}\\). For example, suppose \\(k=1\\) and \\(\\sigma_{i}^{2}=X_{i}^{2}\\) with \\(\\mathbb{E}[X]=0\\). The ratio of the true variance of the least squares estimator to the expectation of the variance estimator is\n\\[\n\\frac{\\boldsymbol{V}_{\\widehat{\\beta}}}{\\mathbb{E}\\left[\\widehat{\\boldsymbol{V}}_{\\widehat{\\beta}}^{0} \\mid \\boldsymbol{X}\\right]}=\\frac{\\sum_{i=1}^{n} X_{i}^{4}}{\\sigma^{2} \\sum_{i=1}^{n} X_{i}^{2}} \\simeq \\frac{\\mathbb{E}\\left[X^{4}\\right]}{\\left(\\mathbb{E}\\left[X^{2}\\right]\\right)^{2}} \\stackrel{\\text { def }}{=} \\kappa\n\\]\n(Notice that we use the fact that \\(\\sigma_{i}^{2}=X_{i}^{2}\\) implies \\(\\sigma^{2}=\\mathbb{E}\\left[\\sigma_{i}^{2}\\right]=\\mathbb{E}\\left[X^{2}\\right]\\).) The constant \\(\\kappa\\) is the standardized fourth moment (or kurtosis) of the regressor \\(X\\) and can be any number greater than one. For example, if \\(X \\sim \\mathrm{N}\\left(0, \\sigma^{2}\\right)\\) then \\(\\kappa=3\\), so the true variance \\(\\boldsymbol{V}_{\\widehat{\\beta}}\\) is three times larger than the expected homoskedastic estimator \\(\\widehat{\\boldsymbol{V}}_{\\widehat{\\beta}}^{0}\\). But \\(\\kappa\\) can be much larger. Take, for example, the variable wage in the CPS data set. It satisfies \\(\\kappa=30\\) so that if the conditional variance equals \\(\\sigma_{i}^{2}=X_{i}^{2}\\) then the true variance \\(\\boldsymbol{V}_{\\widehat{\\beta}}\\) is 30 times larger than the expected homoskedastic estimator \\(\\widehat{\\boldsymbol{V}}_{\\widehat{\\beta}}^{0}\\). While this is an extreme case the point is that the classic covariance matrix estimator (4.35) may be quite biased when the homoskedasticity assumption fails."
  },
  {
    "objectID": "chpt04-lsr.html#covariance-matrix-estimation-under-heteroskedasticity",
    "href": "chpt04-lsr.html#covariance-matrix-estimation-under-heteroskedasticity",
    "title": "4  Least Squares Regression",
    "section": "4.14 Covariance Matrix Estimation Under Heteroskedasticity",
    "text": "4.14 Covariance Matrix Estimation Under Heteroskedasticity\nIn the previous section we showed that that the classic covariance matrix estimator can be highly biased if homoskedasticity fails. In this section we show how to construct covariance matrix estimators which do not require homoskedasticity.\nRecall that the general form for the covariance matrix is\n\\[\n\\boldsymbol{V}_{\\widehat{\\beta}}=\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{D} \\boldsymbol{X}\\right)\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} .\n\\]\nwith \\(\\boldsymbol{D}\\) defined in (4.8). This depends on the unknown matrix \\(\\boldsymbol{D}\\) which we can write as\n\\[\n\\boldsymbol{D}=\\operatorname{diag}\\left(\\sigma_{1}^{2}, \\ldots, \\sigma_{n}^{2}\\right)=\\mathbb{E}\\left[\\boldsymbol{e} \\boldsymbol{e}^{\\prime} \\mid \\boldsymbol{X}\\right]=\\mathbb{E}[\\widetilde{\\boldsymbol{D}} \\mid \\boldsymbol{X}]\n\\]\nwhere \\(\\widetilde{\\boldsymbol{D}}=\\operatorname{diag}\\left(e_{1}^{2}, \\ldots, e_{n}^{2}\\right)\\). Thus \\(\\widetilde{\\boldsymbol{D}}\\) is a conditionally unbiased estimator for \\(\\boldsymbol{D}\\). If the squared errors \\(e_{i}^{2}\\) were observable, we could construct an unbiased estimator for \\(\\boldsymbol{V}_{\\widehat{\\beta}}\\) as\n\\[\n\\begin{aligned}\n\\widehat{\\boldsymbol{V}}_{\\widehat{\\beta}}^{\\text {ideal }} &=\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1}\\left(\\boldsymbol{X}^{\\prime} \\widetilde{\\boldsymbol{D}} \\boldsymbol{X}\\right)\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\\\\n&=\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1}\\left(\\sum_{i=1}^{n} X_{i} X_{i}^{\\prime} e_{i}^{2}\\right)\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1}\n\\end{aligned}\n\\]\nIndeed,\n\\[\n\\begin{aligned}\n\\mathbb{E}\\left[\\widehat{\\boldsymbol{V}}_{\\widehat{\\beta}}^{\\text {ideal }} \\mid \\boldsymbol{X}\\right] &=\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1}\\left(\\sum_{i=1}^{n} X_{i} X_{i}^{\\prime} \\mathbb{E}\\left[e_{i}^{2} \\mid \\boldsymbol{X}\\right]\\right)\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\\\\n&=\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1}\\left(\\sum_{i=1}^{n} X_{i} X_{i}^{\\prime} \\sigma_{i}^{2}\\right)\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\\\\n&=\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{D} \\boldsymbol{X}\\right)\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1}=\\boldsymbol{V}_{\\widehat{\\beta}}\n\\end{aligned}\n\\]\nverifying that \\(\\widehat{\\boldsymbol{V}}_{\\widehat{\\beta}}^{\\text {ideal }}\\) is unbiased for \\(\\boldsymbol{V}_{\\widehat{\\beta}}\\). Since the errors \\(e_{i}^{2}\\) are unobserved \\(\\widehat{\\boldsymbol{V}}_{\\widehat{\\beta}}^{\\text {ideal }}\\) is not a feasible estimator. However, we can replace \\(e_{i}^{2}\\) with the squared residuals \\(\\widehat{e}_{i}^{2}\\). Making this substitution we obtain the estimator\n\\[\n\\widehat{\\boldsymbol{V}}_{\\widehat{\\beta}}^{\\mathrm{HC} 0}=\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1}\\left(\\sum_{i=1}^{n} X_{i} X_{i}^{\\prime} \\widehat{e}_{i}^{2}\\right)\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} .\n\\]\nThe label “HC” refers to “heteroskedasticity-consistent”. The label “HC0” refers to this being the baseline heteroskedasticity-consistent covariance matrix estimator.\nWe know, however, that \\(\\widehat{e}_{i}^{2}\\) is biased towards zero (recall equation (4.27)). To estimate the variance \\(\\sigma^{2}\\) the unbiased estimator \\(s^{2}\\) scales the moment estimator \\(\\widehat{\\sigma}^{2}\\) by \\(n /(n-k)\\). Making the same adjustment we obtain the estimator\n\\[\n\\widehat{\\boldsymbol{V}}_{\\widehat{\\beta}}^{\\mathrm{HC1}}=\\left(\\frac{n}{n-k}\\right)\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1}\\left(\\sum_{i=1}^{n} X_{i} X_{i}^{\\prime} \\widehat{e}_{i}^{2}\\right)\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} .\n\\]\nWhile the scaling by \\(n /(n-k)\\) is \\(a d h o c, \\mathrm{HCl}\\) is often recommended over the unscaled HC0 estimator.\nAlternatively, we could use the standardized residuals \\(\\bar{e}_{i}\\) or the prediction errors \\(\\widetilde{e}_{i}\\), yielding the “HC2” and “HC3” estimators\n\\[\n\\begin{aligned}\n\\widehat{\\boldsymbol{V}}_{\\widehat{\\beta}}^{\\mathrm{HC} 2} &=\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1}\\left(\\sum_{i=1}^{n} X_{i} X_{i}^{\\prime} \\bar{e}_{i}^{2}\\right)\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\\\\n&=\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1}\\left(\\sum_{i=1}^{n}\\left(1-h_{i i}\\right)^{-1} X_{i} X_{i}^{\\prime} \\widehat{e}_{i}^{2}\\right)\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1}\n\\end{aligned}\n\\]\nand\n\\[\n\\begin{aligned}\n\\widehat{\\boldsymbol{V}}_{\\widehat{\\beta}}^{\\mathrm{HC} 3} &=\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1}\\left(\\sum_{i=1}^{n} X_{i} X_{i}^{\\prime} \\tilde{e}_{i}^{2}\\right)\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\\\\n&=\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1}\\left(\\sum_{i=1}^{n}\\left(1-h_{i i}\\right)^{-2} X_{i} X_{i}^{\\prime} \\widehat{e}_{i}^{2}\\right)\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} .\n\\end{aligned}\n\\]\nThe four estimators \\(\\mathrm{HC}\\), \\(\\mathrm{HC1}\\), HC2, and HC3 are collectively called robust, heteroskedasticityconsistent, or heteroskedasticity-robust covariance matrix estimators. The HC0 estimator was first developed by Eicker (1963) and introduced to econometrics by White (1980) and is sometimes called the Eicker-White or White covariance matrix estimator. The degree-of-freedom adjustment in \\(\\mathrm{HCl}\\) was recommended by Hinkley (1977) and is the default robust covariance matrix estimator implemented in Stata. It is implement by the “, \\(r\\)” option. In current applied econometric practice this is the most popular covariance matrix estimator. The HC2 estimator was introduced by Horn, Horn and Duncan (1975) and is implemented using the vce (hc2) option in Stata. The HC3 estimator was derived by MacKinnon and White (1985) from the jackknife principle (see Section 10.3), and by Andrews (1991a) based on the principle of leave-one-out cross-validation, and is implemented using the vce(hc3) option in Stata.\nSince \\(\\left(1-h_{i i}\\right)^{-2}>\\left(1-h_{i i}\\right)^{-1}>1\\) it is straightforward to show that\n\\[\n\\widehat{\\boldsymbol{V}}_{\\widehat{\\beta}}^{\\mathrm{HC} 0}<\\widehat{\\boldsymbol{V}}_{\\widehat{\\beta}}^{\\mathrm{HC} 2}<\\widehat{\\boldsymbol{V}}_{\\widehat{\\beta}}^{\\mathrm{HC} 3} .\n\\]\n(See Exercise 4.10.) The inequality \\(\\boldsymbol{A}<\\boldsymbol{B}\\) when applied to matrices means that the matrix \\(\\boldsymbol{B}-\\boldsymbol{A}\\) is positive definite. In general, the bias of the covariance matrix estimators is complicated but simplify under the assumption of homoskedasticity (4.3). For example, using (4.27),\n\\[\n\\begin{aligned}\n\\mathbb{E}\\left[\\widehat{\\boldsymbol{V}}_{\\widehat{\\beta}}^{\\mathrm{HC} 0} \\mid \\boldsymbol{X}\\right] &=\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1}\\left(\\sum_{i=1}^{n} X_{i} X_{i}^{\\prime} \\mathbb{E}\\left[\\widehat{e}_{i}^{2} \\mid \\boldsymbol{X}\\right]\\right)\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\\\\n&=\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1}\\left(\\sum_{i=1}^{n} X_{i} X_{i}^{\\prime}\\left(1-h_{i i}\\right) \\sigma^{2}\\right)\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\\\\n&=\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\sigma^{2}-\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1}\\left(\\sum_{i=1}^{n} X_{i} X_{i}^{\\prime} h_{i i}\\right)\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\sigma^{2} \\\\\n&<\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\sigma^{2}=\\boldsymbol{V}_{\\widehat{\\beta}}\n\\end{aligned}\n\\]\nThis calculation shows that \\(\\widehat{\\boldsymbol{V}}_{\\widehat{\\beta}}^{\\mathrm{HC} 0}\\) is biased towards zero.\nBy a similar calculation (again under homoskedasticity) we can calculate that the HC2 estimator is unbiased\n\\[\n\\mathbb{E}\\left[\\widehat{\\boldsymbol{V}}_{\\widehat{\\beta}}^{\\mathrm{HC} 2} \\mid \\boldsymbol{X}\\right]=\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\sigma^{2} .\n\\]\n(See Exercise 4.11.)\nIt might seem rather odd to compare the bias of heteroskedasticity-robust estimators under the assumption of homoskedasticity but it does give us a baseline for comparison.\nAnother interesting calculation shows that in general (that is, without assuming homoskedasticity) the HC3 estimator is biased away from zero. Indeed, using the definition of the prediction errors (3.44)\n\\[\n\\widetilde{e}_{i}=Y_{i}-X_{i}^{\\prime} \\widehat{\\beta}_{(-i)}=e_{i}-X_{i}^{\\prime}\\left(\\widehat{\\beta}_{(-i)}-\\beta\\right)\n\\]\nso\n\\[\n\\widetilde{e}_{i}^{2}=e_{i}^{2}-2 X_{i}^{\\prime}\\left(\\widehat{\\beta}_{(-i)}-\\beta\\right) e_{i}+\\left(X_{i}^{\\prime}\\left(\\widehat{\\beta}_{(-i)}-\\beta\\right)\\right)^{2} .\n\\]\nNote that \\(e_{i}\\) and \\(\\widehat{\\beta}_{(-i)}\\) are functions of non-overlapping observations and are thus independent. Hence \\(\\mathbb{E}\\left[\\left(\\widehat{\\beta}_{(-i)}-\\beta\\right) e_{i} \\mid \\boldsymbol{X}\\right]=0\\) and\n\\[\n\\begin{aligned}\n\\mathbb{E}\\left[\\widetilde{e}_{i}^{2} \\mid \\boldsymbol{X}\\right] &=\\mathbb{E}\\left[e_{i}^{2} \\mid \\boldsymbol{X}\\right]-2 X_{i}^{\\prime} \\mathbb{E}\\left[\\left(\\widehat{\\beta}_{(-i)}-\\beta\\right) e_{i} \\mid \\boldsymbol{X}\\right]+\\mathbb{E}\\left[\\left(X_{i}^{\\prime}\\left(\\widehat{\\beta}_{(-i)}-\\beta\\right)\\right)^{2} \\mid \\boldsymbol{X}\\right] \\\\\n&=\\sigma_{i}^{2}+\\mathbb{E}\\left[\\left(X_{i}^{\\prime}\\left(\\widehat{\\beta}_{(-i)}-\\beta\\right)\\right)^{2} \\mid \\boldsymbol{X}\\right] \\\\\n& \\geq \\sigma_{i}^{2} .\n\\end{aligned}\n\\]\nIt follows that\n\\[\n\\begin{aligned}\n\\mathbb{E}\\left[\\widehat{\\boldsymbol{V}}_{\\widehat{\\beta}}^{\\mathrm{HC} 3} \\mid \\boldsymbol{X}\\right] &=\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1}\\left(\\sum_{i=1}^{n} X_{i} X_{i}^{\\prime} \\mathbb{E}\\left[\\tilde{e}_{i}^{2} \\mid \\boldsymbol{X}\\right]\\right)\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\\\\n& \\geq\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1}\\left(\\sum_{i=1}^{n} X_{i} X_{i}^{\\prime} \\sigma_{i}^{2}\\right)\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1}=\\boldsymbol{V}_{\\widehat{\\beta}}\n\\end{aligned}\n\\]\nThis means that the HC3 estimator is conservative in the sense that it is weakly larger (in expectation) than the correct variance for any realization of \\(\\boldsymbol{X}\\).\nWe have introduced five covariance matrix estimators, including the homoskedastic estimator \\(\\widehat{\\boldsymbol{V}}_{\\widehat{\\beta}}^{0}\\) and the four HC estimators. Which should you use? The classic estimator \\(\\widehat{\\boldsymbol{V}}_{\\widehat{\\beta}}^{0}\\) is typically a poor choice as it is only valid under the unlikely homoskedasticity restriction. For this reason it is not typically used in contemporary econometric research. Unfortunately, standard regression packages set their default choice as \\(\\widehat{\\boldsymbol{V}}_{\\widehat{\\beta}}^{0}\\) so users must intentionally select a robust covariance matrix estimator.\nOf the four robust estimators \\(\\mathrm{HCl}\\) is the most commonly used as it is the default robust covariance matrix option in Stata. However, HC2 and HC3 are preferred. HC2 is unbiased (under homoskedasticity) and HC3 is conservative for any \\(\\boldsymbol{X}\\). In most applications \\(\\mathrm{HC} 1, \\mathrm{HC} 2\\), and \\(\\mathrm{HC} 3\\) will be similar so this choice will not matter. The context where the estimators can differ substantially is when the sample has a large leverage value \\(h_{i i}\\) for at least one observation. You can see this by comparing the formulas (4.37), (4.38), and (4.39) and noting that the only difference is the scaling by the leverage values \\(h_{i i}\\). If there is an observation with \\(h_{i i}\\) close to one, then \\(\\left(1-h_{i i}\\right)^{-1}\\) and \\(\\left(1-h_{i i}\\right)^{-2}\\) will be large, giving this observation much greater weight in the covariance matrix formula."
  },
  {
    "objectID": "chpt04-lsr.html#standard-errors",
    "href": "chpt04-lsr.html#standard-errors",
    "title": "4  Least Squares Regression",
    "section": "4.15 Standard Errors",
    "text": "4.15 Standard Errors\nA variance estimator such as \\(\\widehat{\\boldsymbol{V}}_{\\widehat{\\beta}}\\) is an estimator of the variance of the distribution of \\(\\widehat{\\beta}\\). A more easily interpretable measure of spread is its square root - the standard deviation. This is so important when discussing the distribution of parameter estimators we have a special name for estimates of their standard deviation.\nDefinition \\(4.2\\) A standard error \\(s(\\widehat{\\beta})\\) for a real-valued estimator \\(\\widehat{\\beta}\\) is an estimator of the standard deviation of the distribution of \\(\\widehat{\\beta}\\).\nWhen \\(\\beta\\) is a vector with estimator \\(\\widehat{\\beta}\\) and covariance matrix estimator \\(\\widehat{\\boldsymbol{V}}_{\\widehat{\\beta}}\\), standard errors for individual elements are the square roots of the diagonal elements of \\(\\widehat{\\boldsymbol{V}}_{\\widehat{\\beta}}\\). That is,\n\\[\ns\\left(\\widehat{\\beta}_{j}\\right)=\\sqrt{\\widehat{\\boldsymbol{V}}_{\\widehat{\\beta}_{j}}}=\\sqrt{\\left[\\widehat{\\boldsymbol{V}}_{\\widehat{\\beta}}\\right]_{j j}}\n\\]\nWhen the classical covariance matrix estimator (4.35) is used the standard error takes the simple form\n\\[\ns\\left(\\widehat{\\beta}_{j}\\right)=s \\sqrt{\\left[\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1}\\right]_{j j}} .\n\\]\nAs we discussed in the previous section there are multiple possible covariance matrix estimators so standard errors are not unique. It is therefore important to understand what formula and method is used by an author when studying their work. It is also important to understand that a particular standard error may be relevant under one set of model assumptions but not under another set of assumptions.\nTo illustrate, we return to the log wage regression (3.12) of Section 3.7. We calculate that \\(s^{2}=0.160\\). Therefore the homoskedastic covariance matrix estimate is\n\\[\n\\widehat{\\boldsymbol{V}}_{\\widehat{\\beta}}^{0}=\\left(\\begin{array}{cc}\n5010 & 314 \\\\\n314 & 20\n\\end{array}\\right)^{-1} 0.160=\\left(\\begin{array}{cc}\n0.002 & -0.031 \\\\\n-0.031 & 0.499\n\\end{array}\\right)\n\\]\nWe also calculate that\n\\[\n\\sum_{i=1}^{n}\\left(1-h_{i i}\\right)^{-1} X_{i} X_{i}^{\\prime} \\widehat{e}_{i}^{2}=\\left(\\begin{array}{cc}\n763.26 & 48.513 \\\\\n48.513 & 3.1078\n\\end{array}\\right) .\n\\]\nTherefore the HC2 covariance matrix estimate is\n\\[\n\\begin{aligned}\n\\widehat{\\boldsymbol{V}}_{\\widehat{\\beta}}^{\\mathrm{HC} 2} &=\\left(\\begin{array}{cc}\n5010 & 314 \\\\\n314 & 20\n\\end{array}\\right)^{-1}\\left(\\begin{array}{cc}\n763.26 & 48.513 \\\\\n48.513 & 3.1078\n\\end{array}\\right)\\left(\\begin{array}{cc}\n5010 & 314 \\\\\n314 & 20\n\\end{array}\\right)^{-1} \\\\\n&=\\left(\\begin{array}{cc}\n0.001 & -0.015 \\\\\n-0.015 & 0.243\n\\end{array}\\right) .\n\\end{aligned}\n\\]\nThe standard errors are the square roots of the diagonal elements of these matrices. A conventional format to write the estimated equation with standard errors is\n\nAlternatively, standard errors could be calculated using the other formulae. We report the different standard errors in the following table.\nTable 4.1: Standard Errors\n\n\n\n\nEducation\nIntercept\n\n\n\n\nHomoskedastic (4.35)\n\\(0.045\\)\n\\(0.707\\)\n\n\nHC0 (4.36)\n\\(0.029\\)\n\\(0.461\\)\n\n\nHC1 \\((4.37)\\)\n\\(0.030\\)\n\\(0.486\\)\n\n\nHC2 \\((4.38)\\)\n\\(0.031\\)\n\\(0.493\\)\n\n\nHC3 \\((4.39)\\)\n\\(0.033\\)\n\\(0.527\\)\n\n\n\nThe homoskedastic standard errors are noticeably different (larger in this case) than the others. The robust standard errors are reasonably close to one another though the HC3 standard errors are larger than the others."
  },
  {
    "objectID": "chpt04-lsr.html#estimation-with-sparse-dummy-variables",
    "href": "chpt04-lsr.html#estimation-with-sparse-dummy-variables",
    "title": "4  Least Squares Regression",
    "section": "4.16 Estimation with Sparse Dummy Variables",
    "text": "4.16 Estimation with Sparse Dummy Variables\nThe heteroskedasticity-robust covariance matrix estimators can be quite imprecise in some contexts. One is in the presence of sparse dummy variables - when a dummy variable only takes the value 1 or 0 for very few observations. In these contexts one component of the covariance matrix is estimated on just those few observations and will be imprecise. This is effectively hidden from the user. To see the problem, let \\(D\\) be a dummy variable (takes on the values 1 and 0 ) and consider the dummy variable regression\n\\[\nY=\\beta_{1} D+\\beta_{2}+e .\n\\]\nThe number of observations for which \\(D_{i}=1\\) is \\(n_{1}=\\sum_{i=1}^{n} D_{i}\\). The number of observations for which \\(D_{i}=0\\) is \\(n_{2}=n-n_{1}\\). We say the design is sparse if \\(n_{1}\\) or \\(n_{2}\\) is small.\nTo simplify our analysis, we take the extreme case \\(n_{1}=1\\). The ideas extend to the case of \\(n_{1}>1\\) but small, though with less dramatic effects.\nIn the regression model (4.45) we can calculate that the true covariance matrix of the least squares estimator for the coefficients under the simplifying assumption of conditional homoskedasticity is\n\\[\n\\boldsymbol{V}_{\\widehat{\\beta}}=\\sigma^{2}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1}=\\sigma^{2}\\left(\\begin{array}{ll}\n1 & 1 \\\\\n1 & n\n\\end{array}\\right)^{-1}=\\frac{\\sigma^{2}}{n-1}\\left(\\begin{array}{cc}\nn & -1 \\\\\n-1 & 1\n\\end{array}\\right)\n\\]\nIn particular, the variance of the estimator for the coefficient on the dummy variable is\n\\[\nV_{\\widehat{\\beta}_{1}}=\\sigma^{2} \\frac{n}{n-1} .\n\\]\nEssentially, the coefficient \\(\\beta_{1}\\) is estimated from a single observation so its variance is roughly unaffected by sample size. An important message is that certain coefficient estimators in the presence of sparse dummy variables will be imprecise, regardless of the sample size. A large sample alone is not sufficient to ensure precise estimation.\nNow let’s examine the standard HC1 covariance matrix estimator (4.37). The regression has perfect fit for the observation for which \\(D_{i}=1\\) so the corresponding residual is \\(\\widehat{e}_{i}=0\\). It follows that \\(D_{i} \\widehat{e}_{i}=0\\) for all \\(i\\) (either \\(D_{i}=0\\) or \\(\\widehat{e}_{i}=0\\) ). Hence\n\\[\n\\sum_{i=1}^{n} X_{i} X_{i}^{\\prime} \\hat{e}_{i}^{2}=\\left(\\begin{array}{cc}\n0 & 0 \\\\\n0 & \\sum_{i=1}^{n} \\widehat{e}_{i}^{2}\n\\end{array}\\right)=\\left(\\begin{array}{cc}\n0 & 0 \\\\\n0 & (n-2) s^{2}\n\\end{array}\\right)\n\\]\nwhere \\(s^{2}=(n-2)^{-1} \\sum_{i=1}^{n} \\widehat{e}_{i}^{2}\\) is the bias-corrected estimator of \\(\\sigma^{2}\\). Together we find that\n\\[\n\\begin{aligned}\n\\widehat{\\boldsymbol{V}}_{\\widehat{\\beta}}^{\\mathrm{HC1}} &=\\left(\\frac{n}{n-2}\\right) \\frac{1}{(n-1)^{2}}\\left(\\begin{array}{cc}\nn & -1 \\\\\n-1 & 1\n\\end{array}\\right)\\left(\\begin{array}{cc}\n0 & 0 \\\\\n0 & (n-2) s^{2}\n\\end{array}\\right)\\left(\\begin{array}{cc}\nn & -1 \\\\\n-1 & 1\n\\end{array}\\right) \\\\\n&=s^{2} \\frac{n}{(n-1)^{2}}\\left(\\begin{array}{cc}\n1 & -1 \\\\\n-1 & 1\n\\end{array}\\right) .\n\\end{aligned}\n\\]\nIn particular, the estimator for \\(V_{\\widehat{\\beta}_{1}}\\) is\n\\[\n\\widehat{V}_{\\widehat{\\beta}_{1}}^{\\mathrm{HC} 1}=s^{2} \\frac{n}{(n-1)^{2}}\n\\]\nIt has expectation\n\\[\n\\mathbb{E}\\left[\\widehat{V}_{\\widehat{\\beta}_{1}}^{\\mathrm{HC1}}\\right]=\\sigma^{2} \\frac{n}{(n-1)^{2}}=\\frac{V_{\\widehat{\\beta}_{1}}}{n-1}<<V_{\\widehat{\\beta}_{1}} .\n\\]\nThe variance estimator \\(\\widehat{V}_{\\widehat{\\beta}_{1}}^{\\mathrm{HCl}}\\) is extremely biased for \\(V_{\\widehat{\\beta}_{1}}\\). It is too small by a multiple of \\(n\\) ! The reported variance - and standard error - is misleadingly small. The variance estimate erroneously mis-states the precision of \\(\\widehat{\\beta}_{1}\\).\nThe fact that \\(\\widehat{V}_{\\widehat{\\beta}_{1}}^{\\mathrm{HCl}}\\) is biased is unlikely to be noticed by an applied researcher. Nothing in the reported output will alert a researcher to the problem. Another way to see the issue is to consider the estimator \\(\\widehat{\\theta}=\\widehat{\\beta}_{1}+\\widehat{\\beta}_{2}\\) for the sum of the coefficients \\(\\theta=\\beta_{1}+\\beta_{2}\\). This estimator has true variance \\(\\sigma^{2}\\). The variance estimator, however is \\(\\widehat{\\boldsymbol{V}}_{\\widehat{\\theta}}^{\\mathrm{HC1}}=0\\) ! (It equals the sum of the four elements in \\(\\widehat{\\boldsymbol{V}}_{\\widehat{\\beta}}^{\\mathrm{HC1}}\\) ). Clearly, the estimator ” 0 ” is biased for the true value \\(\\sigma^{2}\\).\nAnother insight is to examine the leverage values. The (single) observation with \\(D_{i}=1\\) has\n\\[\nh_{i i}=\\frac{1}{n-1}\\left(\\begin{array}{ll}\n1 & 1\n\\end{array}\\right)\\left(\\begin{array}{cc}\nn & -1 \\\\\n-1 & 1\n\\end{array}\\right)\\left(\\begin{array}{l}\n1 \\\\\n1\n\\end{array}\\right)=1 .\n\\]\nThis is an extreme leverage value.\nA possible solution is to replace the biased covariance matrix estimator \\(\\widehat{V}_{\\widehat{\\beta}_{1}}^{\\mathrm{HC1}}\\) with the unbiased estimator \\(\\widehat{V}_{\\widehat{\\beta}_{1}}^{\\mathrm{HC} 2}\\) (unbiased under homoskedasticity) or the conservative estimator \\(\\widehat{V}_{\\widehat{\\beta}_{1}}^{\\mathrm{HC}} .\\) Neither approach can be done in the extreme sparse case \\(n_{1}=1\\) (for \\(\\widehat{V}_{\\widehat{\\beta}_{1}}^{\\mathrm{HC} 2}\\) and \\(\\widehat{V}_{\\widehat{\\beta}_{1}}^{\\mathrm{HC}}\\) cannot be calculated if \\(h_{i i}=1\\) for any observation) but applies otherwise. When \\(h_{i i}=1\\) for an observation then \\(\\widehat{V}_{\\widehat{\\beta}_{1}}^{\\mathrm{HC} 2}\\) and \\(\\widehat{V}_{\\widehat{\\beta}_{1}}^{\\mathrm{HC} 3}\\) cannot be calculated. In this case unbiased covariance matrix estimation appears to be impossible.\nIt is unclear if there is a best practice to avoid this situation. Once possibility is to calculate the maximum leverage value. If it is very large calculate the standard errors using several methods to see if variation occurs."
  },
  {
    "objectID": "chpt04-lsr.html#computation",
    "href": "chpt04-lsr.html#computation",
    "title": "4  Least Squares Regression",
    "section": "4.17 Computation",
    "text": "4.17 Computation\nWe illustrate methods to compute standard errors for equation (3.13) extending the code of Section \\(3.25 .\\)\nStata do File (continued)\n\nHomoskedastic formula (4.35):\n\nreg wage education experience exp2 if \\((\\mathrm{mnwf}==1)\\)\n\n\\(\\quad\\) HC1 formula (4.37):\n\nreg wage education experience exp2 if \\((\\operatorname{mnwf}==1), \\mathrm{r}\\)\n\n\\(\\mathrm{HC} 2\\) formula (4.38):\n\nreg wage education experience \\(\\exp 2\\) if \\((\\mathrm{mnwf}==1)\\), vce \\((\\mathrm{hc} 2)\\)\n\n\\(\\quad\\) HC3 formula (4.39):\n\nreg wage education experience exp2 if (mnwf \\(==1)\\), vce \\((\\mathrm{hc} 3)\\)"
  },
  {
    "objectID": "chpt04-lsr.html#measures-of-fit",
    "href": "chpt04-lsr.html#measures-of-fit",
    "title": "4  Least Squares Regression",
    "section": "4.18 Measures of Fit",
    "text": "4.18 Measures of Fit\nAs we described in the previous chapter a commonly reported measure of regression fit is the regression \\(R^{2}\\) defined as\n\\[\nR^{2}=1-\\frac{\\sum_{i=1}^{n} \\widehat{e}_{i}^{2}}{\\sum_{i=1}^{n}\\left(Y_{i}-\\bar{Y}\\right)^{2}}=1-\\frac{\\widehat{\\sigma}^{2}}{\\widehat{\\sigma}_{Y}^{2}} .\n\\]\nwhere \\(\\widehat{\\sigma}_{Y}^{2}=n^{-1} \\sum_{i=1}^{n}\\left(Y_{i}-\\bar{Y}\\right)^{2} \\cdot R^{2}\\) is an estimator of the population parameter\n\\[\n\\rho^{2}=\\frac{\\operatorname{var}\\left[X^{\\prime} \\beta\\right]}{\\operatorname{var}[Y]}=1-\\frac{\\sigma^{2}}{\\sigma_{Y}^{2}} .\n\\]\nHowever, \\(\\widehat{\\sigma}^{2}\\) and \\(\\widehat{\\sigma}_{Y}^{2}\\) are biased. Theil (1961) proposed replacing these by the unbiased versions \\(s^{2}\\) and \\(\\widetilde{\\sigma}_{Y}^{2}=(n-1)^{-1} \\sum_{i=1}^{n}\\left(Y_{i}-\\bar{Y}\\right)^{2}\\) yielding what is known as R-bar-squared or adjusted R-squared:\n\\[\n\\bar{R}^{2}=1-\\frac{s^{2}}{\\widetilde{\\sigma}_{Y}^{2}}=1-\\frac{(n-1)^{-1} \\sum_{i=1}^{n} \\widehat{e}_{i}^{2}}{(n-k)^{-1} \\sum_{i=1}^{n}\\left(Y_{i}-\\bar{Y}\\right)^{2}} .\n\\]\nWhile \\(\\bar{R}^{2}\\) is an improvement on \\(R^{2}\\) a much better improvement is\n\\[\n\\widetilde{R}^{2}=1-\\frac{\\sum_{i=1}^{n} \\widetilde{e}_{i}^{2}}{\\sum_{i=1}^{n}\\left(Y_{i}-\\bar{Y}\\right)^{2}}=1-\\frac{\\widetilde{\\sigma}^{2}}{\\widehat{\\sigma}_{Y}^{2}}\n\\]\nwhere \\(\\widetilde{e}_{i}\\) are the prediction errors (3.44) and \\(\\widetilde{\\sigma}^{2}\\) is the MSPE from (3.46). As described in Section (4.12) \\(\\widetilde{\\sigma}^{2}\\) is a good estimator of the out-of-sample mean-squared forecast error so \\(\\widetilde{R}^{2}\\) is a good estimator of the percentage of the forecast variance which is explained by the regression forecast. In this sense \\(\\widetilde{R}^{2}\\) is a good measure of fit.\nOne problem with \\(R^{2}\\) which is partially corrected by \\(\\bar{R}^{2}\\) and fully corrected by \\(\\widetilde{R}^{2}\\) is that \\(R^{2}\\) necessarily increases when regressors are added to a regression model. This occurs because \\(R^{2}\\) is a negative function of the sum of squared residuals which cannot increase when a regressor is added. In contrast, \\(\\bar{R}^{2}\\) and \\(\\widetilde{R}^{2}\\) are non-monotonic in the number of regressors. \\(\\widetilde{R}^{2}\\) can even be negative, which occurs when an estimated model predicts worse than a constant-only model.\nIn the statistical literature the MSPE \\(\\widetilde{\\sigma}^{2}\\) is known as the leave-one-out cross validation criterion and is popular for model comparison and selection, especially in high-dimensional and nonparametric contexts. It is equivalent to use \\(\\widetilde{R}^{2}\\) or \\(\\widetilde{\\sigma}^{2}\\) to compare and select models. Models with high \\(\\widetilde{R}^{2}\\) (or low \\(\\widetilde{\\sigma}^{2}\\) ) are better models in terms of expected out of sample squared error. In contrast, \\(R^{2}\\) cannot be used for model selection as it necessarily increases when regressors are added to a regression model. \\(\\bar{R}^{2}\\) is also an inappropriate choice for model selection (it tends to select models with too many parameters) though a justification of this assertion requires a study of the theory of model selection. Unfortunately, \\(\\bar{R}^{2}\\) is routinely used by some economists, possibly as a hold-over from previous generations.\nIn summary, it is recommended to omit \\(R^{2}\\) and \\(\\bar{R}^{2}\\). If a measure of fit is desired, report \\(\\widetilde{R}^{2}\\) or \\(\\widetilde{\\sigma}^{2}\\)."
  },
  {
    "objectID": "chpt04-lsr.html#empirical-example",
    "href": "chpt04-lsr.html#empirical-example",
    "title": "4  Least Squares Regression",
    "section": "4.19 Empirical Example",
    "text": "4.19 Empirical Example\nWe again return to our wage equation but use a much larger sample of all individuals with at least 12 years of education. For regressors we include years of education, potential work experience, experience squared, and dummy variable indicators for the following: female, female union member, male union member, married female \\({ }^{1}\\), married male, formerly married female \\({ }^{2}\\), formerly married male, Hispanic, Black, American Indian, Asian, and mixed race \\({ }^{3}\\). The available sample is 46,943 so the parameter estimates are quite precise and reported in Table 4.2. For standard errors we use the unbiased HC2 formula.\nTable \\(4.2\\) displays the parameter estimates in a standard tabular format. Parameter estimates and standard errors are reported for all coefficients. In addition to the coefficient estimates the table also reports the estimated error standard deviation and the sample size. These are useful summary measures of fit which aid readers.\nTable 4.2: OLS Estimates of Linear Equation for \\(\\log (\\) wage \\()\\)\n\n\n\n\n\\(\\widehat{\\beta}\\)\n\\(s(\\widehat{\\beta})\\)\n\n\n\n\nEducation\n\\(0.117\\)\n\\(0.001\\)\n\n\nExperience\n\\(0.033\\)\n\\(0.001\\)\n\n\nExperience \\(^{2} / 100\\)\n\\(-0.056\\)\n\\(0.002\\)\n\n\nFemale\n\\(-0.098\\)\n\\(0.011\\)\n\n\nFemale Union Member\n\\(0.023\\)\n\\(0.020\\)\n\n\nMale Union Member\n\\(0.095\\)\n\\(0.020\\)\n\n\nMarried Female\n\\(0.016\\)\n\\(0.010\\)\n\n\nMarried Male\n\\(0.211\\)\n\\(0.010\\)\n\n\nFormerly Married Female\n\\(-0.006\\)\n\\(0.012\\)\n\n\nFormerly Married Male\n\\(0.083\\)\n\\(0.015\\)\n\n\nHispanic\n\\(-0.108\\)\n\\(0.008\\)\n\n\nBlack\n\\(-0.096\\)\n\\(0.008\\)\n\n\nAmerican Indian\n\\(-0.137\\)\n\\(0.027\\)\n\n\nAsian\n\\(-0.038\\)\n\\(0.013\\)\n\n\nMixed Race\n\\(-0.041\\)\n\\(0.021\\)\n\n\nIntercept\n\\(0.909\\)\n\\(0.021\\)\n\n\n\\(\\widehat{\\sigma}\\)\n\\(0.565\\)\n\n\n\nSample Size\n46,943\n\n\n\n\nStandard errors are heteroskedasticity-consistent (Horn-Horn-Duncan formula).\nAs a general rule it is advisable to always report standard errors along with parameter estimates. This allows readers to assess the precision of the parameter estimates, and as we will discuss in later chapters, form confidence intervals and t-tests for individual coefficients if desired.\nThe results in Table \\(4.2\\) confirm our earlier findings that the return to a year of education is approximately \\(12 %\\), the return to experience is concave, single women earn approximately \\(10 %\\) less then single men, and Blacks earn about \\(10 %\\) less than whites. In addition, we see that Hispanics earn about \\(11 %\\) less than whites, American Indians \\(14 %\\) less, and Asians and Mixed races about \\(4 %\\) less. We also see there\n\\({ }^{1}\\) Defining “married” as marital code 1,2 , or \\(3 .\\)\n\\({ }^{2}\\) Defining “formerly married” as marital code 4,5 , or 6 .\n\\({ }^{3}\\) Race code 6 or higher. are wage premiums for men who are members of a labor union (about \\(10 %\\) ), married (about 21%) or formerly married (about \\(8 %\\) ), but no similar premiums are apparent for women."
  },
  {
    "objectID": "chpt04-lsr.html#multicollinearity",
    "href": "chpt04-lsr.html#multicollinearity",
    "title": "4  Least Squares Regression",
    "section": "4.20 Multicollinearity",
    "text": "4.20 Multicollinearity\nAs discussed in Section 3.24, if \\(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\) is singular then \\(\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1}\\) and \\(\\widehat{\\beta}\\) are not defined. This situation is called strict multicollinearity as the columns of \\(\\boldsymbol{X}\\) are linearly dependent, i.e., there is some \\(\\alpha \\neq 0\\) such that \\(\\boldsymbol{X} \\alpha=0\\). Most commonly this arises when sets of regressors are included which are identically related. In Section \\(3.24\\) we discussed possible causes of strict multicollinearity and discussed the related problem of ill-conditioning which can cause numerical inaccuracies in severe cases.\nA related common situation is near multicollinearity which is often called “multicollinearity” for brevity. This is the situation when the regressors are highly correlated. An implication of near multicollinearity is that individual coefficient estimates will be imprecise. This is not necessarily a problem for econometric analysis if the reported standard errors are accurate. However, robust standard errors can be sensitive to large leverage values which can occur under near multicollinearity. This leads to the undesirable situation where the coefficient estimates are imprecise yet the standard errors are misleadingly small.\nWe can see the impact of near multicollinearity on precision in a simple homoskedastic linear regression model with two regressors\n\\[\nY=X_{1} \\beta_{1}+X_{2} \\beta_{2}+e\n\\]\nand\n\\[\n\\frac{1}{n} \\boldsymbol{X}^{\\prime} \\boldsymbol{X}=\\left(\\begin{array}{ll}\n1 & \\rho \\\\\n\\rho & 1\n\\end{array}\\right) .\n\\]\nIn this case\n\\[\n\\operatorname{var}[\\widehat{\\beta} \\mid \\boldsymbol{X}]=\\frac{\\sigma^{2}}{n}\\left(\\begin{array}{ll}\n1 & \\rho \\\\\n\\rho & 1\n\\end{array}\\right)^{-1}=\\frac{\\sigma^{2}}{n\\left(1-\\rho^{2}\\right)}\\left(\\begin{array}{cc}\n1 & -\\rho \\\\\n-\\rho & 1\n\\end{array}\\right) .\n\\]\nThe correlation \\(\\rho\\) indexes collinearity since as \\(\\rho\\) approaches 1 the matrix becomes singular. We can see the effect of collinearity on precision by observing that the variance of a coefficient estimate \\(\\sigma^{2}\\left[n\\left(1-\\rho^{2}\\right)\\right]^{-1}\\) approaches infinity as \\(\\rho\\) approaches 1 . Thus the more “collinear” are the regressors the worse the precision of the individual coefficient estimates.\nWhat is happening is that when the regressors are highly dependent it is statistically difficult to disentangle the impact of \\(\\beta_{1}\\) from that of \\(\\beta_{2}\\). As a consequence the precision of individual estimates are reduced.\nMany early-generation textbooks overemphasized multicollinearity. An amusing parody of these texts is Micronumerosity, Chapter \\(23.3\\) of Goldberger’s A Course in Econometrics (1991). Among the witty remarks of his chapter are the following.\nThe extreme case, ‘exact micronumerosity’, arises when \\(n=0\\), in which case the sample estimate of \\(\\mu\\) is not unique. (Technically, there is a violation of the rank condition \\(n>0\\) : the matrix 0 is singular.)\nTests for the presence of micronumerosity require the judicious use of various fingers. Some researchers prefer a single finger, others use their toes, still others let their thumbs rule.\nA generally reliable guide may be obtained by counting the number of observations. Most of the time in econometric analysis, when \\(n\\) is close to zero, it is also far from infinity.\nArthur S. Goldberger, A Course in Econometrics (1991), pp. \\(249 .\\) To understand Goldberger’s basic point you should notice that the estimation variance \\(\\sigma^{2}\\left[n\\left(1-\\rho^{2}\\right)\\right]^{-1}\\) depends equally and symmetrically on the correlation \\(\\rho\\) and the sample size \\(n\\). He was pointing out that the only statistical implication of multicollinearity in the homoskedastic model is a lack of precision. Small sample sizes have the exact same implication.\n\n\n\n\n\n\nArthur S. Goldberger\n\n\n\n\nArt Goldberger (1930-2009) was one of the most distinguished members of the\n\n\nDepartment of Economics at the University of Wisconsin. His Ph.D. thesis devel-\n\n\noped a pioneering macroeconometric forecasting model (the Klein-Goldberger\n\n\nmodel). Most of his remaining career focused on microeconometric issues. He\n\n\nwas the leading pioneer of what has been called the Wisconsin Tradition of em-\n\n\npirical work - a combination of formal econometric theory with a careful critical\n\n\nanalysis of empirical work. Goldberger wrote a series of highly regarded and in-\n\n\nfluential graduate econometric textbooks, including Econometric Theory (1964),\n\n\nTopics in Regression Analysis (1968), and A Course in Econometrics (1991)."
  },
  {
    "objectID": "chpt04-lsr.html#clustered-sampling",
    "href": "chpt04-lsr.html#clustered-sampling",
    "title": "4  Least Squares Regression",
    "section": "4.21 Clustered Sampling",
    "text": "4.21 Clustered Sampling\nIn Section \\(4.2\\) we briefly mentioned clustered sampling as an alternative to the assumption of random sampling. We now introduce the framework in more detail and extend the primary results of this chapter to encompass clustered dependence.\nIt might be easiest to understand the idea of clusters by considering a concrete example. Duflo, Dupas, and Kremer (2011) investigate the impact of tracking (assigning students based on initial test score) on educational attainment in a randomized experiment. An extract of their data set is available on the textbook webpage in the file DDK2011.\nIn 2005, 140 primary schools in Kenya received funding to hire an extra first grade teacher to reduce class sizes. In half of the schools (selected randomly) students were assigned to classrooms based on an initial test score (“tracking”); in the remaining schools the students were randomly assigned to classrooms. For their analysis the authors restricted attention to the 121 schools which initially had a single first-grade class.\nThe key regression \\({ }^{4}\\) in the paper is\n\\[\n\\text { TestScore }_{i g}=-0.071+0.138 \\text { Tracking }_{g}+e_{i g}\n\\]\nwhere TestScore \\({ }_{i g}\\) is the standardized test score (normalized to have mean 0 and variance 1) of student \\(i\\) in school \\(g\\), and Tracking \\(g\\) is a dummy equal to 1 if school \\(g\\) was tracking. The OLS estimates indicate that schools which tracked the students had an overall increase in test scores by about \\(0.14\\) standard deviations, which is meaningful. More general versions of this regression are estimated, many of which take the form\n\\[\n\\text { TestScore }_{i g}=\\alpha+\\gamma \\text { Tracking }_{g}+X_{i g}^{\\prime} \\beta+e_{i g}\n\\]\nwhere \\(X_{i g}\\) is a set of controls specific to the student (including age, gender, and initial test score).\n\\({ }^{4}\\) Table 2, column (1). Duflo, Dupas and Kremer (2011) report a coefficient estimate of \\(0.139\\), perhaps due to a slightly different calculation to standardize the test score. A difficulty with applying the classical regression framework is that student achievement is likely correlated within a given school. Student achievement may be affected by local demographics, individual teachers, and classmates, all of which imply dependence. These concerns, however, do not suggest that achievement will be correlated across schools, so it seems reasonable to model achievement across schools as mutually independent. We call such dependence clustered.\nIn clustering contexts it is convenient to double index the observations as \\(\\left(Y_{i g}, X_{i g}\\right)\\) where \\(g=1, \\ldots, G\\) indexes the cluster and \\(i=1, \\ldots, n_{g}\\) indexes the individual within the \\(g^{t h}\\) cluster. The number of observations per cluster \\(n_{g}\\) may vary across clusters. The number of clusters is \\(G\\). The total number of observations is \\(n=\\sum_{g=1}^{G} n_{g}\\). In the Kenyan schooling example the number of clusters (schools) in the estimation sample is \\(G=121\\), the number of students per school varies from 19 to 62 , and the total number of observations is \\(n=5795\\).\nWhile it is typical to write the observations using the double index notation \\(\\left(Y_{i g}, X_{i g}\\right)\\) it is also useful to use cluster-level notation. Let \\(\\boldsymbol{Y}_{g}=\\left(Y_{1 g}, \\ldots, Y_{n_{g} g}\\right)^{\\prime}\\) and \\(\\boldsymbol{X}_{g}=\\left(X_{1 g}, \\ldots, X_{n_{g} g}\\right)^{\\prime}\\) denote the \\(n_{g} \\times 1\\) vector of dependent variables and \\(n_{g} \\times k\\) matrix of regressors for the \\(g^{t h}\\) cluster. A linear regression model can be written by individual as\n\\[\nY_{i g}=X_{i g}^{\\prime} \\beta+e_{i g}\n\\]\nand using cluster notation as\n\\[\n\\boldsymbol{Y}_{g}=\\boldsymbol{X}_{g} \\beta+\\boldsymbol{e}_{g}\n\\]\nwhere \\(\\boldsymbol{e}_{g}=\\left(e_{1 g}, \\ldots, e_{n_{g} g}\\right)^{\\prime}\\) is a \\(n_{g} \\times 1\\) error vector. We can also stack the observations into full sample matrices and write the model as\n\\[\n\\boldsymbol{Y}=\\boldsymbol{X} \\beta+\\boldsymbol{e} .\n\\]\nUsing this notation we can write the sums over the observations using the double sum \\(\\sum_{g=1}^{G} \\sum_{i=1}^{n_{g}}\\). This is the sum across clusters of the sum across observations within each cluster. The OLS estimator can be written as\n\\[\n\\begin{aligned}\n\\widehat{\\beta} &=\\left(\\sum_{g=1}^{G} \\sum_{i=1}^{n_{g}} X_{i g} X_{i g}^{\\prime}\\right)^{-1}\\left(\\sum_{g=1}^{G} \\sum_{i=1}^{n_{g}} X_{i g} Y_{i g}\\right) \\\\\n&=\\left(\\sum_{g=1}^{G} \\boldsymbol{X}_{g}^{\\prime} \\boldsymbol{X}_{g}\\right)^{-1}\\left(\\sum_{g=1}^{G} \\boldsymbol{X}_{g}^{\\prime} \\boldsymbol{Y}_{g}\\right) \\\\\n&=\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{Y}\\right)\n\\end{aligned}\n\\]\nThe residuals are \\(\\widehat{e}_{i g}=Y_{i g}-X_{i g}^{\\prime} \\widehat{\\beta}\\) in individual level notation and \\(\\widehat{\\boldsymbol{e}}_{g}=\\boldsymbol{Y}_{g}-\\boldsymbol{X}_{g} \\widehat{\\beta}\\) in cluster level notation.\nThe standard clustering assumption is that the clusters are known to the researcher and that the observations are independent across clusters.\nAssumption 4.4 The clusters \\(\\left(\\boldsymbol{Y}_{g}, \\boldsymbol{X}_{g}\\right)\\) are mutually independent across clusters \\(g\\).\nIn our example clusters are schools. In other common applications cluster dependence has been assumed within individual classrooms, families, villages, regions, and within larger units such as industries and states. This choice is up to the researcher though the justification will depend on the context, the nature of the data, and will reflect information and assumptions on the dependence structure across observations. The model is a linear regression under the assumption\n\\[\n\\mathbb{E}\\left[\\boldsymbol{e}_{g} \\mid \\boldsymbol{X}_{g}\\right]=0 .\n\\]\nThis is the same as assuming that the individual errors are conditionally mean zero\n\\[\n\\mathbb{E}\\left[e_{i g} \\mid \\boldsymbol{X}_{g}\\right]=0\n\\]\nor that the conditional expectation of \\(\\boldsymbol{Y}_{g}\\) given \\(\\boldsymbol{X}_{g}\\) is linear. As in the independent case equation (4.50) means that the linear regression model is correctly specified. In the clustered regression model this requires that all interaction effects within clusters have been accounted for in the specification of the individual regressors \\(X_{i g}\\).\nIn the regression (4.46) the conditional expectation is necessarily linear and satisfies (4.50) since the\n\\ controls, (4.50) requires that the achievement of any student is unaffected by the individual controls (e.g. age, gender, and initial test score) of other students within the same school.\nGiven (4.50) we can calculate the expectation of the OLS estimator. Substituting (4.48) into (4.49) we find\n\\[\n\\widehat{\\beta}-\\beta=\\left(\\sum_{g=1}^{G} \\boldsymbol{X}_{g}^{\\prime} \\boldsymbol{X}_{g}\\right)^{-1}\\left(\\sum_{g=1}^{G} \\boldsymbol{X}_{g}^{\\prime} \\boldsymbol{e}_{g}\\right)\n\\]\nThe mean of \\(\\widehat{\\beta}-\\beta\\) conditioning on all the regressors is\n\\[\n\\begin{aligned}\n\\mathbb{E}[\\widehat{\\beta}-\\beta \\mid \\boldsymbol{X}] &=\\left(\\sum_{g=1}^{G} \\boldsymbol{X}_{g}^{\\prime} \\boldsymbol{X}_{g}\\right)^{-1}\\left(\\sum_{g=1}^{G} \\boldsymbol{X}_{g}^{\\prime} \\mathbb{E}\\left[\\boldsymbol{e}_{g} \\mid \\boldsymbol{X}\\right]\\right) \\\\\n&=\\left(\\sum_{g=1}^{G} \\boldsymbol{X}_{g}^{\\prime} \\boldsymbol{X}_{g}\\right)^{-1}\\left(\\sum_{g=1}^{G} \\boldsymbol{X}_{g}^{\\prime} \\mathbb{E}\\left[\\boldsymbol{e}_{g} \\mid \\boldsymbol{X}_{g}\\right]\\right) \\\\\n&=0 .\n\\end{aligned}\n\\]\nThe first equality holds by linearity, the second by Assumption 4.4, and the third by (4.50).\nThis shows that OLS is unbiased under clustering if the conditional expectation is linear.\nTheorem 4.7 In the clustered linear regression model (Assumption \\(4.4\\) and (4.50)) \\(\\mathbb{E}[\\widehat{\\beta} \\mid \\boldsymbol{X}]=\\beta\\).\nNow consider the covariance matrix of \\(\\widehat{\\beta}\\). Let \\(\\Sigma_{g}=\\mathbb{E}\\left[\\boldsymbol{e}_{g} \\boldsymbol{e}_{g}^{\\prime} \\mid \\boldsymbol{X}_{g}\\right]\\) denote the \\(n_{g} \\times n_{g}\\) conditional covariance matrix of the errors within the \\(g^{t h}\\) cluster. Since the observations are independent across clusters,\n\\[\n\\begin{aligned}\n\\operatorname{var}\\left[\\left(\\sum_{g=1}^{G} \\boldsymbol{X}_{g}^{\\prime} \\boldsymbol{e}_{g}\\right) \\mid \\boldsymbol{X}\\right] &=\\sum_{g=1}^{G} \\operatorname{var}\\left[\\boldsymbol{X}_{g}^{\\prime} \\boldsymbol{e}_{g} \\mid \\boldsymbol{X}_{g}\\right] \\\\\n&=\\sum_{g=1}^{G} \\boldsymbol{X}_{g}^{\\prime} \\mathbb{E}\\left[\\boldsymbol{e}_{g} \\boldsymbol{e}_{g}^{\\prime} \\mid \\boldsymbol{X}_{g}\\right] \\boldsymbol{X}_{g} \\\\\n&=\\sum_{g=1}^{G} \\boldsymbol{X}_{g}^{\\prime} \\Sigma_{g} \\boldsymbol{X}_{g} \\\\\n& \\stackrel{\\text { def }}{=} \\Omega_{n}\n\\end{aligned}\n\\]\nIt follows that\n\\[\n\\boldsymbol{V}_{\\widehat{\\beta}}=\\operatorname{var}[\\widehat{\\beta} \\mid \\boldsymbol{X}]=\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\Omega_{n}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} .\n\\]\nThis differs from the formula in the independent case due to the correlation between observations within clusters. The magnitude of the difference depends on the degree of correlation between observations within clusters and the number of observations within clusters. To see this, suppose that all clusters have the same number of observations \\(n_{g}=N, \\mathbb{E}\\left[e_{i g}^{2} \\mid \\boldsymbol{X}_{g}\\right]=\\sigma^{2}, \\mathbb{E}\\left[e_{i g} e_{\\ell g} \\mid \\boldsymbol{X}_{g}\\right]=\\sigma^{2} \\rho\\) for \\(i \\neq \\ell\\), and the regressors \\(X_{i g}\\) do not vary within a cluster. In this case the exact variance of the OLS estimator equals \\({ }^{5}\\) (after some calculations)\n\\[\n\\boldsymbol{V}_{\\widehat{\\beta}}=\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\sigma^{2}(1+\\rho(N-1)) .\n\\]\nIf \\(\\rho>0\\) the exact variance is appropriately a multiple \\(\\rho N\\) of the conventional formula. In the Kenyan school example the average cluster size is 48 . If \\(\\rho=0.25\\) this means the exact variance exceeds the conventional formula by a factor of about twelve. In this case the correct standard errors (the square root of the variance) are a multiple of about three times the conventional formula. This is a substantial difference and should not be neglected.\nArellano (1987) proposed a cluster-robust covariance matrix estimator which is an extension of the White estimator. Recall that the insight of the White covariance estimator is that the squared error \\(e_{i}^{2}\\) is unbiased for \\(\\mathbb{E}\\left[e_{i}^{2} \\mid X_{i}\\right]=\\sigma_{i}^{2}\\). Similarly, with cluster dependence the matrix \\(\\boldsymbol{e}_{g} \\boldsymbol{e}_{g}^{\\prime}\\) is unbiased for \\(\\mathbb{E}\\left[\\boldsymbol{e}_{g} \\boldsymbol{e}_{g}^{\\prime} \\mid \\boldsymbol{X}_{g}\\right]=\\Sigma_{g}\\). This means that an unbiased estimator for (4.51) is \\(\\widetilde{\\Omega}_{n}=\\sum_{g=1}^{G} \\boldsymbol{X}_{g}^{\\prime} \\boldsymbol{e}_{g} \\boldsymbol{e}_{g}^{\\prime} \\boldsymbol{X}_{g}\\). This is not feasible, but we can replace the unknown errors by the OLS residuals to obtain Arellano’s estimator:\n\\[\n\\begin{aligned}\n\\widehat{\\Omega}_{n} &=\\sum_{g=1}^{G} \\boldsymbol{X}_{g}^{\\prime} \\widehat{\\boldsymbol{e}}_{g} \\widehat{\\boldsymbol{e}}_{g}^{\\prime} \\boldsymbol{X}_{g} \\\\\n&=\\sum_{g=1}^{G} \\sum_{i=1}^{n_{g}} \\sum_{\\ell=1}^{n_{g}} X_{i g} X_{\\ell g}^{\\prime} \\widehat{e}_{i g} \\widehat{e}_{\\ell g} \\\\\n&=\\sum_{g=1}^{G}\\left(\\sum_{i=1}^{n_{g}} X_{i g} \\widehat{e}_{i g}\\right)\\left(\\sum_{\\ell=1}^{n_{g}} X_{\\ell g} \\widehat{e}_{\\ell g}\\right)^{\\prime} .\n\\end{aligned}\n\\]\nThe three expressions in (4.54) give three equivalent formulae which could be used to calculate \\(\\widehat{\\Omega}_{n}\\). The final expression writes \\(\\widehat{\\Omega}_{n}\\) in terms of the cluster sums \\(\\sum_{\\ell=1}^{n_{g}} X_{\\ell g} \\widehat{e}_{\\ell g}\\) which is the basis for our example \\(\\mathrm{R}\\) and MATLAB codes shown below.\nGiven the expressions (4.51)-(4.52) a natural cluster covariance matrix estimator takes the form\n\\[\n\\widehat{\\boldsymbol{V}}_{\\widehat{\\beta}}=a_{n}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\widehat{\\Omega}_{n}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1}\n\\]\nwhere \\(a_{n}\\) is a possible finite-sample adjustment. The Stata cluster command uses\n\\[\na_{n}=\\left(\\frac{n-1}{n-k}\\right)\\left(\\frac{G}{G-1}\\right) .\n\\]\nThe factor \\(G /(G-1)\\) was derived by Chris Hansen (2007) in the context of equal-sized clusters to improve performance when the number of clusters \\(G\\) is small. The factor \\((n-1) /(n-k)\\) is an \\(a d\\) hoc generalization which nests the adjustment used in (4.37) since \\(G=n\\) implies the simplification \\(a_{n}=n /(n-k)\\).\nAlternative cluster-robust covariance matrix estimators can be constructed using cluster-level prediction errors such as \\(\\widetilde{\\boldsymbol{e}}_{g}=\\boldsymbol{Y}_{g}-\\boldsymbol{X}_{g} \\widehat{\\beta}_{(-g)}\\) where \\(\\widehat{\\beta}_{(-g)}\\) is the least squares estimator omitting cluster \\(g\\). As in Section 3.20, we can show that\n\\[\n\\widetilde{\\boldsymbol{e}}_{g}=\\left(\\boldsymbol{I}_{n_{g}}-\\boldsymbol{X}_{g}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\boldsymbol{X}_{g}^{\\prime}\\right)^{-1} \\widehat{\\boldsymbol{e}}_{g}\n\\]\n\\({ }^{5}\\) This formula is due to Moulton (1990). and\n\\[\n\\widehat{\\beta}_{(-g)}=\\widehat{\\beta}-\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\boldsymbol{X}_{g}^{\\prime} \\widetilde{\\boldsymbol{e}}_{g} .\n\\]\nWe then have the robust covariance matrix estimator\n\\[\n\\widehat{\\boldsymbol{V}}_{\\widehat{\\beta}}^{\\mathrm{CR} 3}=\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1}\\left(\\sum_{g=1}^{G} \\boldsymbol{X}_{g}^{\\prime} \\widetilde{\\boldsymbol{e}}_{g} \\widetilde{\\boldsymbol{e}}_{g}^{\\prime} \\boldsymbol{X}_{g}\\right)\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} .\n\\]\nThe label “CR” refers to “cluster-robust” and “CR3” refers to the analogous formula for the HC3 estimator.\nSimilarly to the heteroskedastic-robust case you can show that CR3 is a conservative estimator for \\(\\boldsymbol{V}_{\\widehat{\\beta}}\\) in the sense that the conditional expectation of \\(\\widehat{\\boldsymbol{V}}_{\\widehat{\\beta}}^{\\mathrm{CR} 3}\\) exceeds \\(\\boldsymbol{V}_{\\widehat{\\beta}}\\). This covariance matrix estimator is more cumbersome to implement, however, as the cluster-level prediction errors (4.57) cannot be calculated in a simple linear operation and requires a loop across clusters to calculate.\nTo illustrate in the context of the Kenyan schooling example we present the regression of student test scores on the school-level tracking dummy with two standard errors displayed. The first (in parenthesis) is the conventional robust standard error. The second [in square brackets] is the clustered standard error (4.55)-(4.56) where clustering is at the level of the school.\n\nWe can see that the cluster-robust standard errors are roughly three times the conventional robust standard errors. Consequently, confidence intervals for the coefficients are greatly affected by the choice.\nFor illustration, we list here the commands needed to produce the regression results with clustered standard errors in Stata, R, and MATLAB.\n\nYou can see that clustered standard errors are simple to calculate in Stata.\n\nProgramming clustered standard errors in \\(\\mathrm{R}\\) is also relatively easy due to the convenient rowsum command which sums variables within clusters.\n\nHere we see that programming clustered standard errors in MATLAB is less convenient than the other packages but still can be executed with just a few lines of code. This example uses the accumarray command which is similar to the rowsum command in \\(\\mathrm{R}\\) but only can be applied to vectors (hence the loop across the regressors) and works best if the clusterid variable are indices (which is why the original schoolid variable is transformed into indices in schoolidx. Application of these commands requires care and attention."
  },
  {
    "objectID": "chpt04-lsr.html#inference-with-clustered-samples",
    "href": "chpt04-lsr.html#inference-with-clustered-samples",
    "title": "4  Least Squares Regression",
    "section": "4.22 Inference with Clustered Samples",
    "text": "4.22 Inference with Clustered Samples\nIn this section we give some cautionary remarks and general advice about cluster-robust inference in econometric practice. There has been remarkably little theoretical research about the properties of cluster-robust methods - until quite recently - so these remarks may become dated rather quickly.\nIn many respects cluster-robust inference should be viewed similarly to heteroskedaticity-robust inference where a “cluster” in the cluster-robust case is interpreted similarly to an “observation” in the heteroskedasticity-robust case. In particular, the effective sample size should be viewed as the number of clusters, not the “sample size” \\(n\\). This is because the cluster-robust covariance matrix estimator effectively treats each cluster as a single observation and estimates the covariance matrix based on the variation across cluster means. Hence if there are only \\(G=50\\) clusters inference should be viewed as (at best) similar to heteroskedasticity-robust inference with \\(n=50\\) observations. This is a bit unsettling when the number of regressors is large (say \\(k=20\\) ) for then the covariance matrix will be estimated imprecisely.\nFurthermore, most cluster-robust theory (for example, the work of Chris Hansen (2007)) assumes that the clusters are homogeneous including the assumption that the cluster sizes are all identical. This turns out to be a very important simplication. When this is violated - when, for example, cluster sizes are highly heterogeneous - the regression should be viewed as roughly equivalent to the heteroskedastic case with an extremely high degree of heteroskedasticity. Cluster sums have variances which are proportional to the cluster sizes so if the latter is heterogeneous so will be the variances of the cluster sums. This also has a large effect on finite sample inference. When clusters are heterogeneous then cluster-robust inference is similar to heteroskedasticity-robust inference with highly heteroskedastic observations.\nPut together, if the number of clusters \\(G\\) is small and the number of observations per cluster is highly varied then we should interpret inferential statements with a great degree of caution. Unfortunately, small \\(G\\) with heterogeneous cluster sizes is commonplace. Many empirical studies on U.S. data cluster at the “state” level meaning that there are 50 or 51 clusters (the District of Columbia is typically treated as a state). The number of observations vary considerably across states since the populations are highly unequal. Thus when you read empirical papers with individual-level data but clustered at the “state” level you should be cautious and recognize that this is equivalent to inference with a small number of extremely heterogeneous observations.\nA further complication occurs when we are interested in treatment as in the tracking example given in the previous section. In many cases (including Duflo, Dupas, and Kremer (2011)) the interest is in the effect of a treatment applied at the cluster level (e.g., schools). In many cases (not, however, Duflo, Dupas, and Kremer (2011)), the number of treated clusters is small relative to the total number of clusters; in an extreme case there is just a single treated cluster. Based on the reasoning given above these applications should be interpreted as equivalent to heteroskedasticity-robust inference with a sparse dummy variable as discussed in Section 4.16. As discussed there, standard error estimates can be erroneously small. In the extreme of a single treated cluster (in the example, if only a single school was tracked) then the estimated coefficient on tracking will be very imprecisely estimated yet will have a misleadingly small cluster standard error. In general, reported standard errors will greatly understate the imprecision of parameter estimates."
  },
  {
    "objectID": "chpt04-lsr.html#at-what-level-to-cluster",
    "href": "chpt04-lsr.html#at-what-level-to-cluster",
    "title": "4  Least Squares Regression",
    "section": "4.23 At What Level to Cluster?",
    "text": "4.23 At What Level to Cluster?\nA practical question which arises in the context of cluster-robust inference is “At what level should we cluster?” In some examples you could cluster at a very fine level, such as families or classrooms, or at higher levels of aggregation, such as neighborhoods, schools, towns, counties, or states. What is the correct level at which to cluster? Rules of thumb have been advocated by practitioners but at present there is little formal analysis to provide useful guidance. What do we know?\nFirst, suppose cluster dependence is ignored or imposed at too fine a level (e.g. clustering by households instead of villages). Then variance estimators will be biased as they will omit covariance terms. As correlation is typically positive, this suggests that standard errors will be too small giving rise to spurious indications of significance and precision.\nSecond, suppose cluster dependence is imposed at too aggregate a measure (e.g. clustering by states rather than villages). This does not cause bias. But the variance estimators will contain many extra components so the precision of the covariance matrix estimator will be poor. This means that reported standard errors will be imprecise - more random - than if clustering had been less aggregate.\nThese considerations show that there is a trade-off between bias and variance in the estimation of the covariance matrix by cluster-robust methods. It is not at all clear-based on current theory - what to do. I state this emphatically. We really do not know what is the “correct” level at which to do cluster-robust inference. This is a very interesting question and should certainly be explored by econometric research. One challenge is that in empirical practice many people have observed: “Clustering is important. Standard errors change a lot whether or not we cluster. Therefore we should only report clustered standard errors.” The flaw in this reasoning is that we do not know why in a specific empirical example the standard errors change under clustering. One possibility is that clustering reduces bias and thus is more accurate. The other possibility is that clustering adds sampling noise and is thus less accurate. In reality it is likely that both factors are present.\nIn any event a researcher should be aware of the number of clusters used in the reported calculations and should treat the number of clusters as the effective sample size for assessing inference. If the number of clusters is, say, \\(G=20\\), this should be treated as a very small sample.\nTo illustrate the thought experiment consider the empirical example of Duflo, Dupas, and Kremer (2011). They reported standard errors clustered at the school level and the application uses 111 schools. Thus \\(G=111\\) is the effective sample size. The number of observations (students) ranges from 19 to 62 , which is reasonably homogeneous. This seems like a well balanced application of clustered variance estimation. However, one could imagine clustering at a different level of aggregation. We might consider clustering at a less aggregate level such as the classroom level, but this cannot be done in this particular application as there was only one classroom per school. Clustering at a more aggregate level could be done in this application at the level of the “zone”. However, there are only 9 zones. Thus if we cluster by zone, \\(G=9\\) is the effective sample size which would lead to imprecise standard errors. In this particular example clustering at the school level (as done by the authors) is indeed the prudent choice."
  },
  {
    "objectID": "chpt04-lsr.html#technical-proofs",
    "href": "chpt04-lsr.html#technical-proofs",
    "title": "4  Least Squares Regression",
    "section": "4.24 Technical Proofs*",
    "text": "4.24 Technical Proofs*\nProof of Theorems \\(\\mathbf{4 . 4}\\) and \\(\\mathbf{4 . 5}\\) Theorem \\(4.4\\) is a special case so we focus on Theorem 4.5. This argument is taken from B. E. Hansen (2021).\nOur approach is to calculate the Cramér-Rao bound for a carefully crafted parametric model. This is based on an insight of Newey (1990, Appendix B) for the simpler context of a population expectation.\nWithout loss of generality, assume that the true coefficient equals \\(\\beta_{0}=0\\) and that \\(\\sigma^{2}=1\\). These are merely normalizations which simplify the notation. Also assume that \\(\\boldsymbol{Y}\\) has a joint density \\(f(\\boldsymbol{y})\\). This assumption can be avoided through use of the Radon-Nikodym derivative.\nDefine the truncation function \\(\\mathbb{R}^{n} \\rightarrow \\mathbb{R}^{n}\\)\n\\[\n\\psi_{c}(\\boldsymbol{y})=\\boldsymbol{y} \\mathbb{1}\\left\\{\\left|\\boldsymbol{X}^{\\prime} \\Sigma^{-1} \\boldsymbol{y}\\right| \\leq c\\right\\}-\\mathbb{E}\\left[\\boldsymbol{Y} \\mathbb{1}\\left\\{\\left|\\boldsymbol{X}^{\\prime} \\Sigma^{-1} \\boldsymbol{Y}\\right| \\leq c\\right\\}\\right] .\n\\]\nNotice that it satisfies \\(\\left|\\psi_{c}(\\boldsymbol{y})\\right| \\leq 2 c, \\mathbb{E}\\left[\\psi_{c}(\\boldsymbol{Y})\\right]=0\\), and\n\\[\n\\mathbb{E}\\left[\\boldsymbol{Y} \\psi_{c}(\\boldsymbol{Y})^{\\prime}\\right]=\\mathbb{E}\\left[\\boldsymbol{Y} \\boldsymbol{Y}^{\\prime} \\mathbb{1}\\left\\{\\left|\\boldsymbol{X}^{\\prime} \\Sigma^{-1} \\boldsymbol{Y}\\right| \\leq c\\right\\}\\right] \\stackrel{\\text { def }}{=} \\Sigma_{c} .\n\\]\nAs \\(c \\rightarrow \\infty, \\Sigma_{c} \\rightarrow \\mathbb{E}\\left[\\boldsymbol{Y} \\boldsymbol{Y}^{\\prime}\\right]=\\Sigma\\). Pick \\(c\\) sufficiently large so that \\(\\Sigma_{c}>0\\), which is feasible because \\(\\Sigma>0\\).\nDefine the auxiliary joint density function\n\\[\nf_{\\beta}(\\boldsymbol{y})=f(\\boldsymbol{y})\\left(1+\\psi_{c}(\\boldsymbol{y})^{\\prime} \\Sigma_{c}^{-1} \\boldsymbol{X} \\beta\\right)\n\\]\nfor parameters \\(\\beta\\) in the set\n\\[\nB=\\left\\{\\beta \\in \\mathbb{R}^{m}:\\|\\beta\\| \\leq \\frac{1}{2 c}\\right\\} .\n\\]\nThe bounds imply that for \\(\\beta \\in B\\) and all \\(y\\)\n\\[\n\\left|\\psi_{c}(\\boldsymbol{y})^{\\prime} \\Sigma_{c}^{-1} \\boldsymbol{X} \\beta\\right|<1 .\n\\]\nThis implies that \\(f_{\\beta}\\) has the same support as \\(f\\) and satisfies the bounds\n\\[\n0<f_{\\beta}(y)<2 f(y) .\n\\]\nWe calculate that\n\\[\n\\begin{aligned}\n\\int f_{\\beta}(\\boldsymbol{y}) d \\boldsymbol{y} &=\\int f(\\boldsymbol{y}) d \\boldsymbol{y}+\\int \\psi_{c}(\\boldsymbol{y})^{\\prime} \\Sigma_{c}^{-1} \\boldsymbol{X} \\beta f_{\\beta}(\\boldsymbol{y}) d \\boldsymbol{y} \\\\\n&=1+\\mathbb{E}\\left[\\psi_{c}(\\boldsymbol{Y})\\right]^{\\prime} \\Sigma_{c}^{-1} \\boldsymbol{X} \\beta \\\\\n&=1\n\\end{aligned}\n\\]\nthe last equality because \\(\\mathbb{E}\\left[\\psi_{c}(\\boldsymbol{Y})\\right]=0\\). Together, these facts imply that \\(f_{\\beta}\\) is a valid density function, and over \\(\\beta \\in B\\) is a parametric family for \\(\\boldsymbol{Y}\\). Evaluated at \\(\\beta_{0}=0\\), which is in the interior of \\(B\\), we see \\(f_{0}=f\\). This means that \\(f_{\\beta}\\) is a correctly-specified parametric family with the true parameter value \\(\\beta_{0}\\).\nLet \\(\\mathbb{E}_{\\beta}\\) denote expectation under the density \\(f_{\\beta}\\). The expectation of \\(\\boldsymbol{Y}\\) in this model is\n\\[\n\\begin{aligned}\n\\mathbb{E}_{\\beta}[\\boldsymbol{Y}] &=\\int \\boldsymbol{y} f_{\\beta}(\\boldsymbol{y}) d \\boldsymbol{y} \\\\\n&=\\int \\boldsymbol{y} f(\\boldsymbol{y}) d \\boldsymbol{y}+\\int \\boldsymbol{y} \\psi_{c}(\\boldsymbol{y})^{\\prime} \\Sigma_{c}^{-1} \\boldsymbol{X} \\beta f_{\\beta}(\\boldsymbol{y}) d \\boldsymbol{y} \\\\\n&=\\mathbb{E}[\\boldsymbol{Y}]+\\mathbb{E}\\left[\\boldsymbol{Y} \\psi_{c}(\\boldsymbol{Y})^{\\prime}\\right] \\Sigma_{c}^{-1} \\boldsymbol{X} \\beta \\\\\n&=\\boldsymbol{X} \\beta\n\\end{aligned}\n\\]\nbecause \\(\\mathbb{E}[\\boldsymbol{Y}]=0\\) and \\(\\mathbb{E}\\left[\\boldsymbol{Y}_{c}(\\boldsymbol{Y})^{\\prime}\\right]=\\Sigma_{c}\\). Thus, the model \\(f_{\\beta}\\) is a linear regression with regression coefficient \\(\\beta\\).\nThe bound (4.61) implies\n\\[\n\\mathbb{E}_{\\beta}\\left[\\|\\boldsymbol{Y}\\|^{2}\\right]=\\int\\|\\boldsymbol{y}\\|^{2} f_{\\beta}(\\boldsymbol{y}) d \\boldsymbol{y} \\leq 2 \\int\\|\\boldsymbol{y}\\|^{2} f(\\boldsymbol{y}) d \\boldsymbol{y}=2 \\mathbb{E}\\left[\\|\\boldsymbol{Y}\\|^{2}\\right]=2 \\operatorname{tr}(\\Sigma)<\\infty .\n\\]\nThis means that \\(f_{\\beta}\\) has a finite variance for all \\(\\beta \\in B\\).\nThe likelihood score for \\(f_{\\beta}\\) is\n\\[\n\\begin{aligned}\nS &=\\left.\\frac{\\partial}{\\partial \\beta} \\log f_{\\beta}(\\boldsymbol{Y})\\right|_{\\beta=0} \\\\\n&=\\left.\\frac{\\partial}{\\partial \\beta} \\log \\left(1+\\psi_{c}(\\boldsymbol{Y})^{\\prime} \\Sigma_{c}^{-1} \\boldsymbol{X} \\beta\\right)\\right|_{\\beta=0} \\\\\n&=\\boldsymbol{X}^{\\prime} \\Sigma_{c}^{-1} \\psi_{c}(\\boldsymbol{Y}) .\n\\end{aligned}\n\\]\nThe information matrix is\n\\[\n\\begin{aligned}\n\\mathscr{I}_{c} &=\\mathbb{E}\\left[S S^{\\prime}\\right] \\\\\n&=\\boldsymbol{X}^{\\prime} \\Sigma_{c}^{-1} \\mathbb{E}\\left[\\psi_{c}(\\boldsymbol{Y}) \\psi_{c}(\\boldsymbol{Y})^{\\prime}\\right] \\Sigma_{c}^{-1} \\boldsymbol{X} \\\\\n& \\leq \\boldsymbol{X}^{\\prime} \\Sigma_{c}^{-1} \\boldsymbol{X},\n\\end{aligned}\n\\]\nwhere the inequality is\n\\[\n\\mathbb{E}\\left[\\psi_{c}(\\boldsymbol{Y}) \\psi_{c}(\\boldsymbol{Y})^{\\prime}\\right]=\\Sigma_{c}-\\mathbb{E}\\left[\\boldsymbol{Y} \\mathbb{1}\\left\\{\\left|\\boldsymbol{X}^{\\prime} \\Sigma^{-1} \\boldsymbol{Y}\\right| \\leq c\\right\\}\\right] \\mathbb{E}\\left[\\boldsymbol{Y} \\mathbb{1}\\left\\{\\left|\\boldsymbol{X}^{\\prime} \\Sigma^{-1} \\boldsymbol{Y}\\right| \\leq c\\right\\}\\right]^{\\prime} \\leq \\Sigma_{c} .\n\\]\nBy assumption, the estimator \\(\\widetilde{\\beta}\\) is unbiased for \\(\\beta\\). The model \\(f_{\\beta}\\) is regular (it is correctly specified as it contains the true density \\(f\\), the support of \\(Y\\) does not depend on \\(\\beta\\), and the true value \\(\\beta_{0}=0\\) lies in the interior of \\(B\\) ). Thus by the Cramér-Rao Theorem (Theorem \\(10.6\\) of Probability and Statistics for Economists)\n\\[\n\\operatorname{var}[\\widetilde{\\beta}] \\geq \\mathscr{I}_{c}^{-1} \\geq\\left(\\boldsymbol{X}^{\\prime} \\Sigma_{c}^{-1} \\boldsymbol{X}\\right)^{-1}\n\\]\nwhere the second inequality is (4.62). Since this holds for all \\(c\\), and \\(\\Sigma_{c} \\rightarrow \\Sigma\\) as \\(c \\rightarrow \\infty\\),\n\\[\n\\operatorname{var}[\\widetilde{\\beta}] \\geq \\limsup _{c \\rightarrow \\infty}\\left(\\boldsymbol{X}^{\\prime} \\Sigma_{c}^{-1} \\boldsymbol{X}\\right)^{-1}=\\left(\\boldsymbol{X}^{\\prime} \\Sigma^{-1} \\boldsymbol{X}\\right)^{-1} .\n\\]\nThis is the variance lower bound."
  },
  {
    "objectID": "chpt04-lsr.html#exercises",
    "href": "chpt04-lsr.html#exercises",
    "title": "4  Least Squares Regression",
    "section": "4.25 Exercises",
    "text": "4.25 Exercises\nExercise 4.1 For some integer \\(k\\), set \\(\\mu_{k}=\\mathbb{E}\\left[Y^{k}\\right]\\).\n\nConstruct an estimator \\(\\widehat{\\mu}_{k}\\) for \\(\\mu_{k}\\).\nShow that \\(\\widehat{\\mu}_{k}\\) is unbiased for \\(\\mu_{k}\\).\nCalculate the variance of \\(\\widehat{\\mu}_{k}\\), say \\(\\operatorname{var}\\left[\\widehat{\\mu}_{k}\\right]\\). What assumption is needed for \\(\\operatorname{var}\\left[\\widehat{\\mu}_{k}\\right]\\) to be finite?\nPropose an estimator of \\(\\operatorname{var}\\left[\\widehat{\\mu}_{k}\\right]\\)\n\nExercise 4.2 Calculate \\(\\mathbb{E}\\left[(\\bar{Y}-\\mu)^{3}\\right]\\), the skewness of \\(\\bar{Y}\\). Under what condition is it zero?\nExercise 4.3 Explain the difference between \\(\\bar{Y}\\) and \\(\\mu\\). Explain the difference between \\(n^{-1} \\sum_{i=1}^{n} X_{i} X_{i}^{\\prime}\\) and \\(\\mathbb{E}\\left[X_{i} X_{i}^{\\prime}\\right]\\)\nExercise 4.4 True or False. If \\(Y=X^{\\prime} \\beta+e, X \\in \\mathbb{R}, \\mathbb{E}[e \\mid X]=0\\), and \\(\\widehat{e}_{i}\\) is the OLS residual from the regression of \\(Y_{i}\\) on \\(X_{i}\\), then \\(\\sum_{i=1}^{n} X_{i}^{2} \\widehat{e}_{i}=0\\).\nExercise 4.5 Prove (4.20) and (4.21).\nExercise 4.6 Prove Theorem \\(4.5\\) under the restriction to linear estimators.\nExercise 4.7 Let \\(\\widetilde{\\beta}\\) be the GLS estimator (4.22) under the assumptions (4.18) and (4.19). Assume that \\(\\Sigma\\) is known and \\(\\sigma^{2}\\) is fdunknown. Define the residual vector \\(\\widetilde{\\boldsymbol{e}}=\\boldsymbol{Y}-\\boldsymbol{X} \\widetilde{\\beta}\\), and an estimator for \\(\\sigma^{2}\\)\n\\[\n\\widetilde{\\sigma}^{2}=\\frac{1}{n-k} \\widetilde{\\boldsymbol{e}}^{\\prime} \\Sigma^{-1} \\widetilde{\\boldsymbol{e}}\n\\]\n\nShow (4.23).\nShow (4.24).\nProve that \\(\\widetilde{\\boldsymbol{e}}=\\boldsymbol{M}_{1} \\boldsymbol{e}\\), where \\(\\boldsymbol{M}_{1}=\\boldsymbol{I}-\\boldsymbol{X}\\left(\\boldsymbol{X}^{\\prime} \\Sigma^{-1} \\boldsymbol{X}\\right)^{-1} \\boldsymbol{X}^{\\prime} \\Sigma^{-1}\\).\nProve that \\(\\boldsymbol{M}_{1}^{\\prime} \\Sigma^{-1} \\boldsymbol{M}_{1}=\\Sigma^{-1}-\\Sigma^{-1} \\boldsymbol{X}\\left(\\boldsymbol{X}^{\\prime} \\Sigma^{-1} \\boldsymbol{X}\\right)^{-1} \\boldsymbol{X}^{\\prime} \\Sigma^{-1}\\). (e) Find \\(\\mathbb{E}\\left[\\widetilde{\\sigma}^{2} \\mid \\boldsymbol{X}\\right]\\).\nIs \\(\\widetilde{\\sigma}^{2}\\) a reasonable estimator for \\(\\sigma^{2}\\) ?\n\nExercise 4.8 Let \\(\\left(Y_{i}, X_{i}\\right)\\) be a random sample with \\(\\mathbb{E}[Y \\mid X]=X^{\\prime} \\beta\\). Consider the Weighted Least Squares (WLS) estimator \\(\\widetilde{\\beta}_{\\text {wls }}=\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{W} \\boldsymbol{X}\\right)^{-1}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{W} \\boldsymbol{Y}\\right)\\) where \\(\\boldsymbol{W}=\\operatorname{diag}\\left(w_{1}, \\ldots, w_{n}\\right)\\) and \\(w_{i}=X_{j i}^{-2}\\), where \\(X_{j i}\\) is one of the \\(X_{i}\\).\n\nIn which contexts would \\(\\widetilde{\\beta}_{\\mathrm{wls}}\\) be a good estimator?\nUsing your intuition, in which situations do you expect \\(\\widetilde{\\beta}_{\\text {wls }}\\) to perform better than OLS?\n\nExercise 4.9 Show (4.32) in the homoskedastic regression model.\nExercise 4.10 Prove (4.40).\nExercise 4.11 Show (4.41) in the homoskedastic regression model.\nExercise 4.12 Let \\(\\mu=\\mathbb{E}[Y], \\sigma^{2}=\\mathbb{E}\\left[(Y-\\mu)^{2}\\right]\\) and \\(\\mu_{3}=\\mathbb{E}\\left[(Y-\\mu)^{3}\\right]\\) and consider the sample mean \\(\\bar{Y}=\\) \\(\\frac{1}{n} \\sum_{i=1}^{n} Y_{i}\\). Find \\(\\mathbb{E}\\left[(\\bar{Y}-\\mu)^{3}\\right]\\) as a function of \\(\\mu, \\sigma^{2}, \\mu_{3}\\) and \\(n\\).\nExercise 4.13 Take the simple regression model \\(Y=X \\beta+e, X \\in \\mathbb{R}, \\mathbb{E}[e \\mid X]=0\\). Define \\(\\sigma_{i}^{2}=\\mathbb{E}\\left[e_{i}^{2} \\mid X_{i}\\right]\\) and \\(\\mu_{3 i}=\\mathbb{E}\\left[e_{i}^{3} \\mid X_{i}\\right]\\) and consider the OLS coefficient \\(\\widehat{\\beta}\\). Find \\(\\mathbb{E}\\left[(\\widehat{\\beta}-\\beta)^{3} \\mid \\boldsymbol{X}\\right]\\).\nExercise 4.14 Take a regression model \\(Y=X \\beta+e\\) with \\(\\mathbb{E}[e \\mid X]=0\\) and i.i.d. observations \\(\\left(Y_{i}, X_{i}\\right)\\) and scalar \\(X\\). The parameter of interest is \\(\\theta=\\beta^{2}\\). Consider the OLS estimators \\(\\widehat{\\beta}\\) and \\(\\widehat{\\theta}=\\widehat{\\beta}^{2}\\).\n\nFind \\(\\mathbb{E}[\\widehat{\\theta} \\mid \\boldsymbol{X}]\\) using our knowledge of \\(\\mathbb{E}[\\widehat{\\beta} \\mid \\boldsymbol{X}]\\) and \\(V_{\\widehat{\\beta}}=\\operatorname{var}[\\widehat{\\beta} \\mid \\boldsymbol{X}]\\). Is \\(\\widehat{\\theta}\\) biased for \\(\\theta\\) ?\nSuggest an (approximate) biased-corrected estimator \\(\\widehat{\\theta}^{*}\\) using an estimator \\(\\widehat{V}_{\\widehat{\\beta}}\\) for \\(V_{\\widehat{\\beta}}\\).\nFor \\(\\widehat{\\theta}^{*}\\) to be potentially unbiased, which estimator of \\(V_{\\widehat{\\beta}}\\) is most appropriate?\n\nUnder which conditions is \\(\\widehat{\\theta}^{*}\\) unbiased?\nExercise 4.15 Consider an i.i.d. sample \\(\\left\\{Y_{i}, X_{i}\\right\\} i=1, \\ldots, n\\) where \\(X\\) is \\(k \\times 1\\). Assume the linear conditional expectation model \\(Y=X^{\\prime} \\beta+e\\) with \\(\\mathbb{E}[e \\mid X]=0\\). Assume that \\(n^{-1} \\boldsymbol{X}^{\\prime} \\boldsymbol{X}=\\boldsymbol{I}_{k}\\) (orthonormal regressors). Consider the OLS estimator \\(\\widehat{\\beta}\\).\n\nFind \\(\\boldsymbol{V}_{\\widehat{\\beta}}=\\operatorname{var}[\\widehat{\\beta}]\\)\nIn general, are \\(\\widehat{\\beta}_{j}\\) and \\(\\widehat{\\beta}_{\\ell}\\) for \\(j \\neq \\ell\\) correlated or uncorrelated?\nFind a sufficient condition so that \\(\\widehat{\\beta}_{j}\\) and \\(\\widehat{\\beta}_{\\ell}\\) for \\(j \\neq \\ell\\) are uncorrelated.\n\nExercise 4.16 Take the linear homoskedastic CEF\n\\[\n\\begin{aligned}\nY^{*} &=X^{\\prime} \\beta+e \\\\\n\\mathbb{E}[e \\mid X] &=0 \\\\\n\\mathbb{E}\\left[e^{2} \\mid X\\right] &=\\sigma^{2}\n\\end{aligned}\n\\]\nand suppose that \\(Y^{*}\\) is measured with error. Instead of \\(Y^{*}\\), we observe \\(Y=Y^{*}+u\\) where \\(u\\) is measurement error. Suppose that \\(e\\) and \\(u\\) are independent and\n\\[\n\\begin{aligned}\n\\mathbb{E}[u \\mid X] &=0 \\\\\n\\mathbb{E}\\left[u^{2} \\mid X\\right] &=\\sigma_{u}^{2}(X)\n\\end{aligned}\n\\]\n\nDerive an equation for \\(Y\\) as a function of \\(X\\). Be explicit to write the error term as a function of the structural errors \\(e\\) and \\(u\\). What is the effect of this measurement error on the model (4.63)?\nDescribe the effect of this measurement error on OLS estimation of \\(\\beta\\) in the feasible regression of the observed \\(Y\\) on \\(X\\).\nDescribe the effect (if any) of this measurement error on standard error calculation for \\(\\widehat{\\beta}\\).\n\nExercise 4.17 Suppose that for the random variables \\((Y, X)\\) with \\(X>0\\) an economic model implies\n\\[\n\\mathbb{E}[Y \\mid X]=(\\gamma+\\theta X)^{1 / 2} .\n\\]\nA friend suggests that you estimate \\(\\gamma\\) and \\(\\theta\\) by the linear regression of \\(Y^{2}\\) on \\(X\\), that is, to estimate the equation\n\\[\nY^{2}=\\alpha+\\beta X+e .\n\\]\n\nInvestigate your friend’s suggestion. Define \\(u=Y-(\\gamma+\\theta X)^{1 / 2}\\). Show that \\(\\mathbb{E}[u \\mid X]=0\\) is implied by (4.64).\nUse \\(Y=(\\gamma+\\theta X)^{1 / 2}+u\\) to calculate \\(\\mathbb{E}\\left[Y^{2} \\mid X\\right]\\). What does this tell you about the implied equation (4.65)?\nCan you recover either \\(\\gamma\\) and/or \\(\\theta\\) from estimation of (4.65)? Are additional assumptions required?\nIs this a reasonable suggestion?\n\nExercise 4.18 Take the model\n\\[\n\\begin{aligned}\nY &=X_{1}^{\\prime} \\beta_{1}+X_{2}^{\\prime} \\beta_{2}+e \\\\\n\\mathbb{E}[e \\mid X] &=0 \\\\\n\\mathbb{E}\\left[e^{2} \\mid X\\right] &=\\sigma^{2}\n\\end{aligned}\n\\]\nwhere \\(X=\\left(X_{1}, X_{2}\\right)\\), with \\(X_{1} k_{1} \\times 1\\) and \\(X_{2} k_{2} \\times 1\\). Consider the short regression \\(Y_{i}=X_{1 i}^{\\prime} \\widehat{\\beta}_{1}+\\widehat{e}_{i}\\) and define the error variance estimator \\(s^{2}=\\left(n-k_{1}\\right)^{-1} \\sum_{i=1}^{n} \\widehat{e}_{i}^{2}\\). Find \\(\\mathbb{E}\\left[s^{2} \\mid \\boldsymbol{X}\\right]\\).\nExercise 4.19 Let \\(\\boldsymbol{Y}\\) be \\(n \\times 1, \\boldsymbol{X}\\) be \\(n \\times k\\), and \\(\\boldsymbol{X}^{*}=\\boldsymbol{X} \\boldsymbol{C}\\) where \\(\\boldsymbol{C}\\) is \\(k \\times k\\) and full-rank. Let \\(\\widehat{\\beta}\\) be the least squares estimator from the regression of \\(Y\\) on \\(X\\), and let \\(\\widehat{V}\\) be the estimate of its asymptotic covariance matrix. Let \\(\\widehat{\\beta}^{*}\\) and \\(\\widehat{\\boldsymbol{V}}^{*}\\) be those from the regression of \\(\\boldsymbol{Y}\\) on \\(\\boldsymbol{X}^{*}\\). Derive an expression for \\(\\widehat{\\boldsymbol{V}}^{*}\\) as a function of \\(\\widehat{V}\\).\nExercise 4.20 Take the model in vector notation\n\\[\n\\begin{aligned}\n\\boldsymbol{Y} &=\\boldsymbol{X} \\beta+\\boldsymbol{e} \\\\\n\\mathbb{E}[\\boldsymbol{e} \\mid \\boldsymbol{X}] &=0 \\\\\n\\mathbb{E}\\left[\\boldsymbol{e} \\boldsymbol{e}^{\\prime} \\mid \\boldsymbol{X}\\right] &=\\Sigma .\n\\end{aligned}\n\\]\nAssume for simplicity that \\(\\Sigma\\) is known. Consider the OLS and GLS estimators \\(\\widehat{\\beta}=\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{Y}\\right)\\) and \\(\\widetilde{\\beta}=\\left(\\boldsymbol{X}^{\\prime} \\Sigma^{-1} \\boldsymbol{X}\\right)^{-1}\\left(\\boldsymbol{X}^{\\prime} \\Sigma^{-1} \\boldsymbol{Y}\\right)\\). Compute the (conditional) covariance between \\(\\widehat{\\beta}\\) and \\(\\widetilde{\\beta}\\) :\n\\[\n\\mathbb{E}\\left[(\\widehat{\\beta}-\\beta)(\\widetilde{\\beta}-\\beta)^{\\prime} \\mid \\boldsymbol{X}\\right]\n\\]\nFind the (conditional) covariance matrix for \\(\\widehat{\\beta}-\\widetilde{\\beta}\\) :\n\\[\n\\mathbb{E}\\left[(\\widehat{\\beta}-\\widetilde{\\beta})(\\widehat{\\beta}-\\beta)^{\\prime} \\mid \\boldsymbol{X}\\right] .\n\\]\nExercise 4.21 The model is\n\\[\n\\begin{aligned}\nY_{i} &=X_{i}^{\\prime} \\beta+e_{i} \\\\\n\\mathbb{E}\\left[e_{i} \\mid X_{i}\\right] &=0 \\\\\n\\mathbb{E}\\left[e_{i}^{2} \\mid X_{i}\\right] &=\\sigma_{i}^{2} \\\\\n\\Sigma &=\\operatorname{diag}\\left\\{\\sigma_{1}^{2}, \\ldots, \\sigma_{n}^{2}\\right\\} .\n\\end{aligned}\n\\]\nThe parameter \\(\\beta\\) is estimated by OLS \\(\\widehat{\\beta}=\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\boldsymbol{X}^{\\prime} \\boldsymbol{Y}\\) and GLS \\(\\widetilde{\\beta}=\\left(\\boldsymbol{X}^{\\prime} \\Sigma^{-1} \\boldsymbol{X}\\right)^{-1} \\boldsymbol{X}^{\\prime} \\Sigma^{-1} \\boldsymbol{Y}\\). Let \\(\\widehat{\\boldsymbol{e}}=\\boldsymbol{Y}-\\boldsymbol{X} \\widehat{\\beta}\\) and \\(\\widetilde{\\boldsymbol{e}}=\\boldsymbol{Y}-\\boldsymbol{X} \\widetilde{\\beta}\\) denote the residuals. Let \\(\\widehat{R}^{2}=1-\\widehat{\\boldsymbol{e}}^{\\prime} \\widehat{\\boldsymbol{e}} /\\left(\\boldsymbol{Y}^{* \\prime} \\boldsymbol{Y}^{*}\\right)\\) and \\(\\widetilde{R}^{2}=1-\\widetilde{\\boldsymbol{e}}^{\\prime} \\widetilde{\\boldsymbol{e}} /\\left(\\boldsymbol{Y}^{* \\prime} \\boldsymbol{Y}^{*}\\right)\\) denote the equation \\(R^{2}\\) where \\(\\boldsymbol{Y}^{*}=\\boldsymbol{Y}-\\bar{Y}\\). If the error \\(e_{i}\\) is truly heteroskedastic will \\(\\widehat{R}^{2}\\) or \\(\\widetilde{R}^{2}\\) be smaller?\nExercise 4.22 An economist friend tells you that the assumption that the observations \\(\\left(Y_{i}, X_{i}\\right)\\) are i.i.d. implies that the regression \\(Y=X^{\\prime} \\beta+e\\) is homoskedastic. Do you agree with your friend? How would you explain your position?\nExercise 4.23 Take the linear regression model with \\(\\mathbb{E}[\\boldsymbol{Y} \\mid \\boldsymbol{X}]=\\boldsymbol{X} \\beta\\). Define the ridge regression estimator\n\\[\n\\widehat{\\beta}=\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}+\\boldsymbol{I}_{k} \\lambda\\right)^{-1} \\boldsymbol{X}^{\\prime} \\boldsymbol{Y}\n\\]\nwhere \\(\\lambda>0\\) is a fixed constant. Find \\(E[\\widehat{\\beta} \\mid \\boldsymbol{X}]\\). Is \\(\\widehat{\\beta}\\) biased for \\(\\beta\\) ?\nExercise 4.24 Continue the empirical analysis in Exercise 3.24.\n\nCalculate standard errors using the homoskedasticity formula and using the four covariance matrices from Section \\(4.14 .\\)\nRepeat in a second programming language. Are they identical?\n\nExercise 4.25 Continue the empirical analysis in Exercise 3.26. Calculate standard errors using the HC3 method. Repeat in your second programming language. Are they identical?\nExercise 4.26 Extend the empirical analysis reported in Section \\(4.21\\) using the DDK2011 dataset on the textbook website.. Do a regression of standardized test score (totalscore normalized to have zero mean and variance 1) on tracking, age, gender, being assigned to the contract teacher, and student’s percentile in the initial distribution. (The sample size will be smaller as some observations have missing variables.) Calculate standard errors using both the conventional robust formula, and clustering based on the school.\n\nCompare the two sets of standard errors. Which standard error changes the most by clustering? Which changes the least?\nHow does the coefficient on tracking change by inclusion of the individual controls (in comparison to the results from (4.60))?"
  },
  {
    "objectID": "chpt05-normal-reg.html",
    "href": "chpt05-normal-reg.html",
    "title": "5  Normal Regression",
    "section": "",
    "text": "This chapter introduces the normal regression model, which is a special case of the linear regression model. It is important as normality allows precise distributional characterizations and sharp inferences. It also provides a baseline for comparison with alternative inference methods, such as asymptotic approximations and the bootstrap.\nThe normal regression model is a fully parametric setting where maximum likelihood estimation is appropriate. Therefore in this chapter we introduce likelihood methods. The method of maximum likelihood is a powerful statistical method for parametric models (such as the normal regression model) and is widely used in econometric practice.\nWe start the chapter with a review of the definition and properties of the normal distribution. For detail and mathematical proofs see Chapter 5 of Probability and Statistics for Economists."
  },
  {
    "objectID": "chpt05-normal-reg.html#the-normal-distribution",
    "href": "chpt05-normal-reg.html#the-normal-distribution",
    "title": "5  Normal Regression",
    "section": "5.2 The Normal Distribution",
    "text": "5.2 The Normal Distribution\nWe say that a random variable \\(Z\\) has the standard normal distribution, or Gaussian, written \\(Z \\sim\\) \\(\\mathrm{N}(0,1)\\), if it has the density\n\\[\n\\phi(x)=\\frac{1}{\\sqrt{2 \\pi}} \\exp \\left(-\\frac{x^{2}}{2}\\right), \\quad-\\infty<x<\\infty .\n\\]\nThe standard normal density is typically written with the symbol \\(\\phi(x)\\) and the corresponding distribution function by \\(\\Phi(x)\\). Plots of the standard normal density function \\(\\phi(x)\\) and distribution function \\(\\Phi(x)\\) are displayed in Figure 5.1.\n\n\nNormal Density\n\n\n\nNormal Distribution\n\nFigure 5.1: Standard Normal Density and Distribution\nTheorem 5.1 If \\(Z \\sim \\mathrm{N}(0,1)\\) then\n\nAll integer moments of \\(Z\\) are finite.\nAll odd moments of \\(Z\\) equal 0 .\nFor any positive integer \\(m\\)\n\n\\[\n\\mathbb{E}\\left[Z^{2 m}\\right]=(2 m-1) ! !=(2 m-1) \\times(2 m-3) \\times \\cdots \\times 1 .\n\\]\n 1. For any \\(r>0\\)\n\\[\n\\mathbb{E}|Z|^{r}=\\frac{2^{r / 2}}{\\sqrt{\\pi}} \\Gamma\\left(\\frac{r+1}{2}\\right)\n\\]\nwhere \\(\\Gamma(t)=\\int_{0}^{\\infty} u^{t-1} e^{-u} d u\\) is the gamma function.\nIf \\(Z \\sim \\mathrm{N}(0,1)\\) and \\(X=\\mu+\\sigma Z\\) for \\(\\mu \\in \\mathbb{R}\\) and \\(\\sigma \\geq 0\\) then \\(X\\) has the univariate normal distribution, written \\(X \\sim \\mathrm{N}\\left(\\mu, \\sigma^{2}\\right)\\). By change-of-variables \\(X\\) has the density\n\\[\nf(x)=\\frac{1}{\\sqrt{2 \\pi \\sigma^{2}}} \\exp \\left(-\\frac{(x-\\mu)^{2}}{2 \\sigma^{2}}\\right), \\quad-\\infty<x<\\infty .\n\\]\nThe expectation and variance of \\(X\\) are \\(\\mu\\) and \\(\\sigma^{2}\\), respectively.\nThe normal distribution and its relatives (the chi-square, student t, F, non-central chi-square, and F) are frequently used for inference to calculate critical values and \\(\\mathrm{p}\\)-values. This involves evaluating the normal cdf \\(\\Phi(x)\\) and its inverse. Since the cdf \\(\\Phi(x)\\) is not available in closed form, statistical textbooks have traditionally provided tables for this purpose. Such tables are not used currently as these calculations are embedded in modern statistical software. For convenience, we list the appropriate commands in MATLAB, R, and Stata to compute the cumulative distribution function of commonly used statistical distributions.\n\nHere we list the appropriate commands to compute the inverse probabilities (quantiles) of the same distributions.\n\n\n\n\n\n\n\n\n\nTo calculate \\(x\\) which solves \\(p=\\mathbb{P}[Z \\leq x]\\) for given \\(p\\)\n\n\n\n\n\n\n\n\\(\\mathrm{N}(0,1)\\)\nMATLAB\n\\(\\mathrm{R}\\)\nStata\n\n\n\n\\(\\operatorname{norminv}(\\mathrm{p})\\)\n\\(\\mathrm{qnorm}(\\mathrm{p})\\)\ninvnormal \\((\\mathrm{p})\\)\n\n\n\\(t_{r}\\)\n\\(\\operatorname{tinv}(\\mathrm{p}, \\mathrm{r})\\)\n\\(\\mathrm{qchisq}(\\mathrm{p}, \\mathrm{r})\\)\ninvchi2 \\((\\mathrm{r}, \\mathrm{p})\\)\n\n\n\\(F_{r, k}\\)\n\\(\\mathrm{finv}(\\mathrm{p}, \\mathrm{r}, \\mathrm{k})\\)\n\\(\\mathrm{qf}(\\mathrm{p}, \\mathrm{r}, \\mathrm{k})\\)\ninvttail \\((\\mathrm{r}, 1-\\mathrm{p})\\)\n\n\n\\(\\chi_{r}^{2}(d)\\)\n\\(\\mathrm{ncx2inv}(\\mathrm{p}, \\mathrm{r}, \\mathrm{d})\\)\n\\(\\mathrm{qchisq}(\\mathrm{p}, \\mathrm{r}, \\mathrm{d})\\)\ninvnchi2 \\((\\mathrm{r}, \\mathrm{d}, \\mathrm{p})\\)\n\n\n\\(F_{r, k}(d)\\)\n\\(\\mathrm{ncfinv}(\\mathrm{p}, \\mathrm{r}, \\mathrm{k}, \\mathrm{d})\\)\n\\(\\mathrm{qf}(\\mathrm{p}, \\mathrm{r}, \\mathrm{k}, \\mathrm{d})\\)\ninvnFtail \\((\\mathrm{r}, \\mathrm{k}, \\mathrm{d}, 1-\\mathrm{p})\\)"
  },
  {
    "objectID": "chpt05-normal-reg.html#multivariate-normal-distribution",
    "href": "chpt05-normal-reg.html#multivariate-normal-distribution",
    "title": "5  Normal Regression",
    "section": "5.3 Multivariate Normal Distribution",
    "text": "5.3 Multivariate Normal Distribution\nWe say that the \\(k\\)-vector \\(Z\\) has a multivariate standard normal distribution, written \\(Z \\sim \\mathrm{N}\\left(0, \\boldsymbol{I}_{k}\\right)\\), if it has the joint density\n\\[\nf(x)=\\frac{1}{(2 \\pi)^{k / 2}} \\exp \\left(-\\frac{x^{\\prime} x}{2}\\right), \\quad x \\in \\mathbb{R}^{k}\n\\]\nThe mean and covariance matrix of \\(Z\\) are 0 and \\(\\boldsymbol{I}_{k}\\), respectively. The multivariate joint density factors into the product of univariate normal densities, so the elements of \\(Z\\) are mutually independent standard normals. If \\(Z \\sim \\mathrm{N}\\left(0, \\boldsymbol{I}_{k}\\right)\\) and \\(X=\\mu+\\boldsymbol{B} Z\\) then the \\(k\\)-vector \\(X\\) has a multivariate normal distribution, written \\(X \\sim \\mathrm{N}(\\mu, \\Sigma)\\) where \\(\\Sigma=\\boldsymbol{B} \\boldsymbol{B}^{\\prime} \\geq 0\\). If \\(\\Sigma>0\\) then by change-of-variables \\(X\\) has the joint density function\n\\[\nf(x)=\\frac{1}{(2 \\pi)^{k / 2} \\operatorname{det}(\\Sigma)^{1 / 2}} \\exp \\left(-\\frac{(x-\\mu)^{\\prime} \\Sigma^{-1}(x-\\mu)}{2}\\right), \\quad x \\in \\mathbb{R}^{k} .\n\\]\nThe expectation and covariance matrix of \\(X\\) are \\(\\mu\\) and \\(\\Sigma\\), respectively. By setting \\(k=1\\) you can check that the multivariate normal simplifies to the univariate normal.\nAn important property of normal random vectors is that affine functions are multivariate normal.\nTheorem 5.2 If \\(X \\sim \\mathrm{N}(\\mu, \\Sigma)\\) and \\(Y=\\boldsymbol{a}+\\boldsymbol{B} X\\), then \\(Y \\sim \\mathrm{N}\\left(\\boldsymbol{a}+\\boldsymbol{B} \\mu, \\boldsymbol{B} \\Sigma \\boldsymbol{B}^{\\prime}\\right)\\).\nOne simple implication of Theorem \\(5.2\\) is that if \\(X\\) is multivariate normal then each component of \\(X\\) is univariate normal.\nAnother useful property of the multivariate normal distribution is that uncorrelatedness is the same as independence. That is, if a vector is multivariate normal, subsets of variables are independent if and only if they are uncorrelated.\nTheorem 5.3 Properties of the Multivariate Normal Distribution\n\nThe expectation and covariance matrix of \\(X \\sim \\mathrm{N}(\\mu, \\Sigma)\\) are \\(\\mathbb{E}[X]=\\mu\\) and \\(\\operatorname{var}[X]=\\Sigma\\).\nIf \\((X, Y)\\) are multivariate normal, \\(X\\) and \\(Y\\) are uncorrelated if and only if they are independent.\nIf \\(X \\sim \\mathrm{N}(\\mu, \\Sigma)\\) and \\(Y=\\boldsymbol{a}+\\boldsymbol{B} X\\), then \\(Y \\sim \\mathrm{N}\\left(\\boldsymbol{a}+\\boldsymbol{B} \\mu, \\boldsymbol{B} \\Sigma \\boldsymbol{B}^{\\prime}\\right)\\).\nIf \\(X \\sim \\mathrm{N}\\left(0, \\boldsymbol{I}_{k}\\right)\\) then \\(X^{\\prime} X \\sim \\chi_{k}^{2}\\), chi-square with \\(k\\) degrees of freedom.\nIf \\(X \\sim \\mathrm{N}(0, \\Sigma)\\) with \\(\\Sigma>0\\) then \\(X^{\\prime} \\Sigma^{-1} X \\sim \\chi_{k}^{2}\\) where \\(k=\\operatorname{dim}(X)\\).\nIf \\(X \\sim \\mathrm{N}(\\mu, \\Sigma)\\) with \\(\\Sigma>0, r \\times r\\), then \\(X^{\\prime} \\Sigma^{-1} X \\sim \\chi_{r}^{2}(\\lambda)\\) where \\(\\lambda=\\mu^{\\prime} \\Sigma^{-1} \\mu\\).\nIf \\(Z \\sim \\mathrm{N}(0,1)\\) and \\(Q \\sim \\chi_{k}^{2}\\) are independent then \\(Z / \\sqrt{Q / k} \\sim t_{k}\\), student t with \\(k\\) degrees of freedom.\nIf \\((Y, X)\\) are multivariate normal\n\n\\[\n\\left(\\begin{array}{l}\nY \\\\\nX\n\\end{array}\\right) \\sim \\mathrm{N}\\left(\\left(\\begin{array}{l}\n\\mu_{Y} \\\\\n\\mu_{X}\n\\end{array}\\right),\\left(\\begin{array}{cc}\n\\Sigma_{Y Y} & \\Sigma_{Y X} \\\\\n\\Sigma_{X Y} & \\Sigma_{X X}\n\\end{array}\\right)\\right)\n\\]\nwith \\(\\Sigma_{Y Y}>0\\) and \\(\\Sigma_{X X}>0\\), then the conditional distributions are\n\\[\n\\begin{aligned}\n&Y \\mid X \\sim \\mathrm{N}\\left(\\mu_{Y}+\\Sigma_{Y X} \\Sigma_{X X}^{-1}\\left(X-\\mu_{X}\\right), \\Sigma_{Y Y}-\\Sigma_{Y X} \\Sigma_{X X}^{-1} \\Sigma_{X Y}\\right) \\\\\n&X \\mid Y \\sim \\mathrm{N}\\left(\\mu_{X}+\\Sigma_{X Y} \\Sigma_{Y Y}^{-1}\\left(Y-\\mu_{Y}\\right), \\Sigma_{X X}-\\Sigma_{X Y} \\Sigma_{Y Y}^{-1} \\Sigma_{Y X}\\right) .\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "chpt05-normal-reg.html#joint-normality-and-linear-regression",
    "href": "chpt05-normal-reg.html#joint-normality-and-linear-regression",
    "title": "5  Normal Regression",
    "section": "5.4 Joint Normality and Linear Regression",
    "text": "5.4 Joint Normality and Linear Regression\ngiven \\(X\\)Suppose the variables \\((Y, X)\\) are jointly normally distributed. Consider the best linear predictor of \\(Y\\)\n\\[\nY=X^{\\prime} \\beta+\\alpha+e .\n\\]\nBy the properties of the best linear predictor, \\(\\mathbb{E}[X e]=0\\) and \\(\\mathbb{E}[e]=0\\), so \\(X\\) and \\(e\\) are uncorrelated. Since \\((e, X)\\) is an affine transformation of the normal vector \\((Y, X)\\) it follows that \\((e, X)\\) is jointly normal (Theorem 5.2). Since \\((e, X)\\) is jointly normal and uncorrelated they are independent (Theorem 5.3). Independence implies that\n\\[\n\\mathbb{E}[e \\mid X]=\\mathbb{E}[e]=0\n\\]\nand\n\\[\n\\mathbb{E}\\left[e^{2} \\mid X\\right]=\\mathbb{E}\\left[e^{2}\\right]=\\sigma^{2}\n\\]\nwhich are properties of a homoskedastic linear CEF.\nWe have shown that when \\((Y, X)\\) are jointly normally distributed they satisfy a normal linear CEF\n\\[\nY=X^{\\prime} \\beta+\\alpha+e\n\\]\nwhere\n\\[\ne \\sim \\mathrm{N}\\left(0, \\sigma^{2}\\right)\n\\]\nis independent of \\(X\\). This result can also be deduced from Theorem 5.3.7.\nThis is a classical motivation for the linear regression model."
  },
  {
    "objectID": "chpt05-normal-reg.html#normal-regression-model",
    "href": "chpt05-normal-reg.html#normal-regression-model",
    "title": "5  Normal Regression",
    "section": "5.5 Normal Regression Model",
    "text": "5.5 Normal Regression Model\nThe normal regression model is the linear regression model with an independent normal error\n\\[\n\\begin{gathered}\nY=X^{\\prime} \\beta+e \\\\\ne \\sim \\mathrm{N}\\left(0, \\sigma^{2}\\right) .\n\\end{gathered}\n\\]\nAs we learned in Section 5.4, the normal regression model holds when \\((Y, X)\\) are jointly normally distributed. Normal regression, however, does not require joint normality. All that is required is that the conditional distribution of \\(Y\\) given \\(X\\) is normal (the marginal distribution of \\(X\\) is unrestricted). In this sense the normal regression model is broader than joint normality. Notice that for notational convenience we have written (5.1) so that \\(X\\) contains the intercept.\nNormal regression is a parametric model where likelihood methods can be used for estimation, testing, and distribution theory. The likelihood is the name for the joint probability density of the data, evaluated at the observed sample, and viewed as a function of the parameters. The maximum likelihood estimator is the value which maximizes this likelihood function. Let us now derive the likelihood of the normal regression model.\nFirst, observe that model (5.1) is equivalent to the statement that the conditional density of \\(Y\\) given \\(X\\) takes the form\n\\[\nf(y \\mid x)=\\frac{1}{\\left(2 \\pi \\sigma^{2}\\right)^{1 / 2}} \\exp \\left(-\\frac{1}{2 \\sigma^{2}}\\left(y-x^{\\prime} \\beta\\right)^{2}\\right)\n\\]\nUnder the assumption that the observations are mutually independent this implies that the conditional density of \\(\\left(Y_{1}, \\ldots, Y_{n}\\right)\\) given \\(\\left(X_{1}, \\ldots, X_{n}\\right)\\) is\n\\[\n\\begin{aligned}\nf\\left(y_{1}, \\ldots, y_{n} \\mid x_{1}, \\ldots, x_{n}\\right) &=\\prod_{i=1}^{n} f\\left(y_{i} \\mid x_{i}\\right) \\\\\n&=\\prod_{i=1}^{n} \\frac{1}{\\left(2 \\pi \\sigma^{2}\\right)^{1 / 2}} \\exp \\left(-\\frac{1}{2 \\sigma^{2}}\\left(y_{i}-x_{i}^{\\prime} \\beta\\right)^{2}\\right) \\\\\n&=\\frac{1}{\\left(2 \\pi \\sigma^{2}\\right)^{n / 2}} \\exp \\left(-\\frac{1}{2 \\sigma^{2}} \\sum_{i=1}^{n}\\left(y_{i}-x_{i}^{\\prime} \\beta\\right)^{2}\\right) \\\\\n& \\stackrel{\\operatorname{def}}{=} L_{n}\\left(\\beta, \\sigma^{2}\\right) .\n\\end{aligned}\n\\]\nThis is called the likelihood function when evaluated at the sample data.\nFor convenience it is typical to work with the natural logarithm\n\\[\n\\log L_{n}\\left(\\beta, \\sigma^{2}\\right)=-\\frac{n}{2} \\log \\left(2 \\pi \\sigma^{2}\\right)-\\frac{1}{2 \\sigma^{2}} \\sum_{i=1}^{n}\\left(Y_{i}-X_{i}^{\\prime} \\beta\\right)^{2} \\stackrel{\\text { def }}{=} \\ell_{n}\\left(\\beta, \\sigma^{2}\\right)\n\\]\nwhich is called the log-likelihood function.\nThe maximum likelihood estimator (MLE) \\(\\left(\\widehat{\\beta}_{\\mathrm{mle}}, \\widehat{\\sigma}_{\\mathrm{mle}}^{2}\\right)\\) is the value which maximizes the log-likelihood. We can write the maximization problem as\n\\[\n\\left(\\widehat{\\beta}_{\\mathrm{mle}}, \\widehat{\\sigma}_{\\mathrm{mle}}^{2}\\right)=\\underset{\\beta \\in \\mathbb{R}^{k}, \\sigma^{2}>0}{\\operatorname{argmax}} \\ell_{n}\\left(\\beta, \\sigma^{2}\\right) .\n\\]\nIn most applications of maximum likelihood the MLE must be found by numerical methods. However in the case of the normal regression model we can find an explicit expression for \\(\\widehat{\\beta}_{\\text {mle }}\\) and \\(\\widehat{\\sigma}_{\\text {mle }}^{2}\\).\nThe maximizers \\(\\left(\\widehat{\\beta}_{\\text {mle }}, \\widehat{\\sigma}_{\\text {mle }}^{2}\\right)\\) of (5.3) jointly solve the first-order conditions (FOC)\n\\[\n\\begin{aligned}\n&0=\\left.\\frac{\\partial}{\\partial \\beta} \\ell_{n}\\left(\\beta, \\sigma^{2}\\right)\\right|_{\\beta=\\widehat{\\beta}_{\\mathrm{mle}}, \\sigma^{2}=\\widehat{\\sigma}_{\\mathrm{mle}}^{2}}=\\frac{1}{\\widehat{\\sigma}_{\\mathrm{mle}}^{2}} \\sum_{i=1}^{n} X_{i}\\left(Y_{i}-X_{i}^{\\prime} \\widehat{\\beta}_{\\mathrm{mle}}\\right) \\\\\n&0=\\left.\\frac{\\partial}{\\partial \\sigma^{2}} \\ell_{n}\\left(\\beta, \\sigma^{2}\\right)\\right|_{\\beta=\\widehat{\\beta}_{\\mathrm{mle}}, \\sigma^{2}=\\widehat{\\sigma}_{\\mathrm{mle}}^{2}}=-\\frac{n}{2 \\widehat{\\sigma}_{\\mathrm{mle}}^{2}}+\\frac{1}{2 \\widehat{\\sigma}_{\\mathrm{mle}}^{4}} \\sum_{i=1}^{n}\\left(Y_{i}-X_{i}^{\\prime} \\widehat{\\beta}_{\\mathrm{mle}}\\right)^{2} .\n\\end{aligned}\n\\]\nThe first FOC (5.4) is proportional to the first-order conditions for the least squares minimization problem of Section 3.6. It follows that the MLE satisfies\n\\[\n\\widehat{\\beta}_{\\mathrm{mle}}=\\left(\\sum_{i=1}^{n} X_{i} X_{i}^{\\prime}\\right)^{-1}\\left(\\sum_{i=1}^{n} X_{i} Y_{i}\\right)=\\widehat{\\beta}_{\\mathrm{ols}} .\n\\]\nThat is, the MLE for \\(\\beta\\) is algebraically identical to the OLS estimator.\nSolving the second FOC (5.5) for \\(\\widehat{\\sigma}_{\\mathrm{mle}}^{2}\\) we find\n\\[\n\\widehat{\\sigma}_{\\mathrm{mle}}^{2}=\\frac{1}{n} \\sum_{i=1}^{n}\\left(Y_{i}-X_{i}^{\\prime} \\widehat{\\beta}_{\\mathrm{mle}}\\right)^{2}=\\frac{1}{n} \\sum_{i=1}^{n}\\left(Y_{i}-X_{i}^{\\prime} \\widehat{\\beta}_{\\mathrm{ols}}\\right)^{2}=\\frac{1}{n} \\sum_{i=1}^{n} \\widehat{e}_{i}^{2}=\\widehat{\\sigma}_{\\mathrm{ols}}^{2}\n\\]\nThus the MLE for \\(\\sigma^{2}\\) is identical to the OLS/moment estimator from (3.26).\nSince the OLS estimator and MLE under normality are equivalent, \\(\\widehat{\\beta}\\) is described by some authors as the maximum likelihood estimator, and by other authors as the least squares estimator. It is important to remember, however, that \\(\\widehat{\\beta}\\) is only the MLE when the error \\(e\\) has a known normal distribution and not otherwise.\nPlugging the estimators into (5.2) we obtain the maximized log-likelihood\n\\[\n\\ell_{n}\\left(\\widehat{\\beta}_{\\text {mle }}, \\widehat{\\sigma}_{\\text {mle }}^{2}\\right)=-\\frac{n}{2} \\log \\left(2 \\pi \\widehat{\\sigma}_{\\text {mle }}^{2}\\right)-\\frac{n}{2} .\n\\]\nThe log-likelihood is typically reported as a measure of fit.\nIt may seem surprising that the MLE \\(\\widehat{\\beta}_{\\mathrm{mle}}\\) is algebraically equal to the OLS estimator despite emerging from quite different motivations. It is not completely accidental. The least squares estimator minimizes a particular sample loss function - the sum of squared error criterion - and most loss functions are equivalent to the likelihood of a specific parametric distribution, in this case the normal regression model. In this sense it is not surprising that the least squares estimator can be motivated as either the minimizer of a sample loss function or as the maximizer of a likelihood function."
  },
  {
    "objectID": "chpt05-normal-reg.html#distribution-of-ols-coefficient-vector",
    "href": "chpt05-normal-reg.html#distribution-of-ols-coefficient-vector",
    "title": "5  Normal Regression",
    "section": "5.6 Distribution of OLS Coefficient Vector",
    "text": "5.6 Distribution of OLS Coefficient Vector\nIn the normal linear regression model we can derive exact sampling distributions for the OLS/MLE estimator, residuals, and variance estimator. In this section we derive the distribution of the OLS coefficient estimator.\nThe normality assumption \\(e \\mid X \\sim \\mathrm{N}\\left(0, \\sigma^{2}\\right)\\) combined with independence of the observations has the multivariate implication\n\\[\n\\boldsymbol{e} \\mid \\boldsymbol{X} \\sim \\mathrm{N}\\left(0, \\boldsymbol{I}_{n} \\sigma^{2}\\right) .\n\\]\nThat is, the error vector \\(\\boldsymbol{e}\\) is independent of \\(\\boldsymbol{X}\\) and is normally distributed.\nRecall that the OLS estimator satisfies\n\\[\n\\widehat{\\beta}-\\beta=\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\boldsymbol{X}^{\\prime} \\boldsymbol{e}\n\\]\nwhich is a linear function of \\(\\boldsymbol{e}\\). Since linear functions of normals are also normal (Theorem 5.2) this implies that conditional on \\(\\boldsymbol{X}\\),\n\\[\n\\begin{aligned}\n\\widehat{\\beta}-\\beta \\mid \\boldsymbol{X} \\sim\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\boldsymbol{X}^{\\prime} \\mathrm{N}\\left(0, \\boldsymbol{I}_{n} \\sigma^{2}\\right) \\\\\n& \\sim \\mathrm{N}\\left(0, \\sigma^{2}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1}\\right) \\\\\n=\\mathrm{N}\\left(0, \\sigma^{2}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1}\\right) .\n\\end{aligned}\n\\]\nThis shows that under the assumption of normal errors the OLS estimator has an exact normal distribution.\nTheorem 5.4 In the normal regression model,\n\\[\n\\widehat{\\beta} \\mid \\boldsymbol{X} \\sim \\mathrm{N}\\left(\\beta, \\sigma^{2}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1}\\right) .\n\\]\nTheorems \\(5.2\\) and \\(5.4\\) imply that any affine function of the OLS estimator is also normally distributed including individual components. Letting \\(\\beta_{j}\\) and \\(\\widehat{\\beta}_{j}\\) denote the \\(j^{t h}\\) elements of \\(\\beta\\) and \\(\\widehat{\\beta}\\), we have\n\\[\n\\widehat{\\beta}_{j} \\mid \\boldsymbol{X} \\sim \\mathrm{N}\\left(\\beta_{j}, \\sigma^{2}\\left[\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1}\\right]_{j j}\\right)\n\\]\nTheorem \\(5.4\\) is a statement about the conditional distribution. What about the unconditional distribution? In Section \\(4.7\\) we presented Kinal’s theorem about the existence of moments for the joint normal regression model. We re-state the result here.\nTheorem 5.5 Kinal (1980) If \\((Y, X)\\) are jointly normal, then for any \\(r, \\mathbb{E}\\|\\widehat{\\beta}\\|^{r}<\\) \\(\\infty\\) if and only if \\(r<n-k+1\\)."
  },
  {
    "objectID": "chpt05-normal-reg.html#distribution-of-ols-residual-vector",
    "href": "chpt05-normal-reg.html#distribution-of-ols-residual-vector",
    "title": "5  Normal Regression",
    "section": "5.7 Distribution of OLS Residual Vector",
    "text": "5.7 Distribution of OLS Residual Vector\nConsider the OLS residual vector. Recall from (3.24) that \\(\\widehat{\\boldsymbol{e}}=\\boldsymbol{M} \\boldsymbol{e}\\) where \\(\\boldsymbol{M}=\\boldsymbol{I}_{n}-\\boldsymbol{X}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\boldsymbol{X}^{\\prime}\\). This shows that \\(\\widehat{\\boldsymbol{e}}\\) is linear in \\(\\boldsymbol{e}\\). So conditional on \\(\\boldsymbol{X}\\)\n\\[\n\\widehat{\\boldsymbol{e}}=\\boldsymbol{M} \\boldsymbol{e} \\mid \\boldsymbol{X} \\sim \\mathrm{N}\\left(0, \\sigma^{2} \\boldsymbol{M} \\boldsymbol{M}\\right)=\\mathrm{N}\\left(0, \\sigma^{2} \\boldsymbol{M}\\right)\n\\]\nthe final equality because \\(M\\) is idempotent (see Section 3.12). This shows that the residual vector has an exact normal distribution.\nFurthermore, it is useful to find the joint distribution of \\(\\widehat{\\beta}\\) and \\(\\widehat{\\boldsymbol{e}}\\). This is easiest done by writing the two as a stacked linear function of the error \\(\\boldsymbol{e}\\). Indeed,\n\\[\n\\left(\\begin{array}{c}\n\\widehat{\\beta}-\\beta \\\\\n\\widehat{\\boldsymbol{e}}\n\\end{array}\\right)=\\left(\\begin{array}{c}\n\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\boldsymbol{X}^{\\prime} \\boldsymbol{e} \\\\\n\\boldsymbol{M} \\boldsymbol{e}\n\\end{array}\\right)=\\left(\\begin{array}{c}\n\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\boldsymbol{X}^{\\prime} \\\\\n\\boldsymbol{M}\n\\end{array}\\right) \\boldsymbol{e}\n\\]\nwhich is a linear function of \\(\\boldsymbol{e}\\). The vector has a joint normal distribution with covariance matrix\n\\[\n\\left(\\begin{array}{cc}\n\\sigma^{2}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} & 0 \\\\\n0 & \\sigma^{2} \\boldsymbol{M}\n\\end{array}\\right)\n\\]\nThe off-diagonal block is zero because \\(\\boldsymbol{X}^{\\prime} \\boldsymbol{M}=0\\) from (3.21). Since this is zero it follows that \\(\\widehat{\\beta}\\) and \\(\\widehat{\\boldsymbol{e}}\\) are statistically independent (Theorem 5.3.2). Theorem 5.6 In the normal regression model, \\(\\widehat{\\boldsymbol{e}} \\mid \\boldsymbol{X} \\sim \\mathrm{N}\\left(0, \\sigma^{2} \\boldsymbol{M}\\right)\\) and is independent of \\(\\widehat{\\beta}\\).\nThe fact that \\(\\widehat{\\beta}\\) and \\(\\widehat{\\boldsymbol{e}}\\) are independent implies that \\(\\widehat{\\beta}\\) is independent of any function of the residual vector including individual residuals \\(\\widehat{e}_{i}\\) and the variance estimators \\(s^{2}\\) and \\(\\widehat{\\sigma}^{2}\\)."
  },
  {
    "objectID": "chpt05-normal-reg.html#distribution-of-variance-estimator",
    "href": "chpt05-normal-reg.html#distribution-of-variance-estimator",
    "title": "5  Normal Regression",
    "section": "5.8 Distribution of Variance Estimator",
    "text": "5.8 Distribution of Variance Estimator\nNext, consider the variance estimator \\(s^{2}\\) from (4.31). Using (3.28) it satisfies \\((n-k) s^{2}=\\widehat{\\boldsymbol{e}}^{\\prime} \\widehat{\\boldsymbol{e}}=\\boldsymbol{e}^{\\prime} \\boldsymbol{M} \\boldsymbol{e}\\). The spectral decomposition of \\(\\boldsymbol{M}\\) (equation (A.4)) is \\(\\boldsymbol{M}=\\boldsymbol{H} \\Lambda \\boldsymbol{H}^{\\prime}\\) where \\(\\boldsymbol{H}^{\\prime} \\boldsymbol{H}=\\boldsymbol{I}_{n}\\) and \\(\\Lambda\\) is diagonal with the eigenvalues of \\(\\boldsymbol{M}\\) on the diagonal. Since \\(\\boldsymbol{M}\\) is idempotent with rank \\(n-k\\) (see Section 3.12) it has \\(n-k\\) eigenvalues equalling 1 and \\(k\\) eigenvalues equalling 0 , so\n\\[\n\\Lambda=\\left[\\begin{array}{cc}\n\\boldsymbol{I}_{n-k} & \\mathbf{0} \\\\\n\\mathbf{0} & \\mathbf{0}_{k}\n\\end{array}\\right] .\n\\]\nLet \\(\\boldsymbol{u}=\\boldsymbol{H}^{\\prime} \\boldsymbol{e} \\sim \\mathrm{N}\\left(\\mathbf{0}, \\boldsymbol{I}_{n} \\sigma^{2}\\right)\\) (see Exercise 5.2) and partition \\(\\boldsymbol{u}=\\left(\\boldsymbol{u}_{1}^{\\prime}, \\boldsymbol{u}_{2}^{\\prime}\\right)^{\\prime}\\) where \\(\\boldsymbol{u}_{1} \\sim \\mathrm{N}\\left(0, \\boldsymbol{I}_{n-k} \\sigma^{2}\\right)\\). Then\n\\[\n\\begin{aligned}\n(n-k) s^{2} &=\\boldsymbol{e}^{\\prime} \\boldsymbol{M} \\boldsymbol{e} \\\\\n&=\\boldsymbol{e}^{\\prime} \\boldsymbol{H}\\left[\\begin{array}{cc}\n\\boldsymbol{I}_{n-k} & \\mathbf{0} \\\\\n\\mathbf{0} & \\mathbf{0}\n\\end{array}\\right] \\boldsymbol{H}^{\\prime} \\boldsymbol{e} \\\\\n&=\\boldsymbol{u}^{\\prime}\\left[\\begin{array}{cc}\n\\boldsymbol{I}_{n-k} & \\mathbf{0} \\\\\n\\mathbf{0} & \\mathbf{0}\n\\end{array}\\right] \\boldsymbol{u} \\\\\n&=\\boldsymbol{u}_{1}^{\\prime} \\boldsymbol{u}_{1} \\\\\n& \\sim \\sigma^{2} \\chi_{n-k}^{2} .\n\\end{aligned}\n\\]\nWe see that in the normal regression model the exact distribution of \\(s^{2}\\) is a scaled chi-square.\nSince \\(\\widehat{\\boldsymbol{e}}\\) is independent of \\(\\widehat{\\beta}\\) it follows that \\(s^{2}\\) is independent of \\(\\widehat{\\beta}\\) as well.\nTheorem 5.7 In the normal regression model,\n\\[\n\\frac{(n-k) s^{2}}{\\sigma^{2}} \\sim \\chi_{n-k}^{2}\n\\]\nand is independent of \\(\\widehat{\\beta}\\)."
  },
  {
    "objectID": "chpt05-normal-reg.html#t-statistic",
    "href": "chpt05-normal-reg.html#t-statistic",
    "title": "5  Normal Regression",
    "section": "5.9 t-statistic",
    "text": "5.9 t-statistic\nAn alternative way of writing (5.7) is\n\\[\n\\frac{\\widehat{\\beta}_{j}-\\beta_{j}}{\\sqrt{\\sigma^{2}\\left[\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1}\\right]_{j j}}} \\sim \\mathrm{N}(0,1) .\n\\]\nThis is sometimes called a standardized statistic as the distribution is the standard normal.\nNow take the standardized statistic and replace the unknown variance \\(\\sigma^{2}\\) with its estimator \\(s^{2}\\). We call this a t-ratio or t-statistic\n\\[\nT=\\frac{\\widehat{\\beta}_{j}-\\beta_{j}}{\\sqrt{s^{2}\\left[\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1}\\right]_{j j}}}=\\frac{\\widehat{\\beta}_{j}-\\beta_{j}}{s\\left(\\widehat{\\beta}_{j}\\right)}\n\\]\nwhere \\(s\\left(\\widehat{\\beta}_{j}\\right)\\) is the classical (homoskedastic) standard error for \\(\\widehat{\\beta}_{j}\\) from (4.42). We will sometimes write the t-statistic as \\(T\\left(\\beta_{j}\\right)\\) to explicitly indicate its dependence on the parameter value \\(\\beta_{j}\\), and sometimes will simplify notation and write the \\(\\mathrm{t}\\)-statistic as \\(T\\) when the dependence is clear from the context.\nWith algebraic re-scaling we can write the t-statistic as the ratio of the standardized statistic and the square root of the scaled variance estimator. Since the distributions of these two components are normal and chi-square, respectively, and independent, we deduce that the t-statistic has the distribution\n\\[\n\\begin{aligned}\nT &=\\frac{\\widehat{\\beta}_{j}-\\beta_{j}}{\\sqrt{\\sigma^{2}\\left[\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1}\\right]_{j j}}} / \\sqrt{\\frac{(n-k) s^{2}}{\\sigma^{2}} /(n-k)} \\\\\n& \\sim \\frac{\\mathrm{N}(0,1)}{\\sqrt{\\chi_{n-k}^{2} /(n-k)}} \\\\\n& \\sim t_{n-k}\n\\end{aligned}\n\\]\na student \\(t\\) distribution with \\(n-k\\) degrees of freedom.\nThis derivation shows that the t-ratio has a sampling distribution which depends only on the quantity \\(n-k\\). The distribution does not depend on any other features of the data. In this context, we say that the distribution of the t-ratio is pivotal, meaning that it does not depend on unknowns.\nThe trick behind this result is scaling the centered coefficient by its standard error, and recognizing that each depends on the unknown \\(\\sigma\\) only through scale. Thus the ratio of the two does not depend on \\(\\sigma\\). This trick (scaling to eliminate dependence on unknowns) is known as studentization.\nTheorem 5.8 In the normal regression model, \\(T \\sim t_{n-k}\\).\nAn important caveat about Theorem \\(5.8\\) is that it only applies to the t-statistic constructed with the homoskedastic (old-fashioned) standard error. It does not apply to a t-statistic constructed with any of the robust standard errors. In fact, the robust t-statistics can have finite sample distributions which deviate considerably from \\(t_{n-k}\\) even when the regression errors are independent \\(\\mathrm{N}\\left(0, \\sigma^{2}\\right)\\). Thus the distributional result in Theorem \\(5.8\\) and the use of the t distribution in finite samples is only exact when applied to classical t-statistics under the normality assumption."
  },
  {
    "objectID": "chpt05-normal-reg.html#confidence-intervals-for-regression-coefficients",
    "href": "chpt05-normal-reg.html#confidence-intervals-for-regression-coefficients",
    "title": "5  Normal Regression",
    "section": "5.10 Confidence Intervals for Regression Coefficients",
    "text": "5.10 Confidence Intervals for Regression Coefficients\nThe OLS estimator \\(\\widehat{\\beta}\\) is a point estimator for a coefficient \\(\\beta\\). A broader concept is a set or interval estimator which takes the form \\(\\widehat{C}=[\\widehat{L}, \\widehat{U}]\\). The goal of an interval estimator \\(\\widehat{C}\\) is to contain the true value, e.g. \\(\\beta \\in \\widehat{C}\\), with high probability.\nThe interval estimator \\(\\widehat{C}\\) is a function of the data and hence is random. An interval estimator \\(\\widehat{C}\\) is called a \\(1-\\alpha\\) confidence interval when \\(\\mathbb{P}[\\beta \\in \\widehat{C}]=1-\\alpha\\) for a selected value of \\(\\alpha\\). The value \\(1-\\alpha\\) is called the coverage probability. Typical choices for the coverage probability \\(1-\\alpha\\) are \\(0.95\\) or \\(0.90\\).\nThe probability calculation \\(\\mathbb{P}[\\beta \\in \\widehat{C}]\\) is easily mis-interpreted as treating \\(\\beta\\) as random and \\(\\widehat{C}\\) as fixed. (The probability that \\(\\beta\\) is in \\(\\widehat{C}\\).) This is not the appropriate interpretation. Instead, the correct interpretation is that the probability \\(\\mathbb{P}[\\beta \\in \\widehat{C}]\\) treats the point \\(\\beta\\) as fixed and the set \\(\\widehat{C}\\) as random. It is the probability that the random set \\(\\widehat{C}\\) covers (or contains) the fixed true coefficient \\(\\beta\\).\nThere is not a unique method to construct confidence intervals. For example, one simple (yet silly) interval is\n\\[\n\\widehat{C}=\\left\\{\\begin{array}{cc}\n\\mathbb{R} & \\text { with probability } 1-\\alpha \\\\\n\\{\\widehat{\\beta}\\} & \\text { with probability } \\alpha .\n\\end{array}\\right.\n\\]\nIf \\(\\widehat{\\beta}\\) has a continuous distribution, then by construction \\(\\mathbb{P}[\\beta \\in \\widehat{C}]=1-\\alpha\\), so this confidence interval has perfect coverage. However, \\(\\widehat{C}\\) is uninformative about \\(\\widehat{\\beta}\\) and is therefore not useful.\nInstead, a good choice for a confidence interval for the regression coefficient \\(\\beta\\) is obtained by adding and subtracting from the estimator \\(\\widehat{\\beta}\\) a fixed multiple of its standard error:\n\\[\n\\widehat{C}=[\\widehat{\\beta}-c \\times s(\\widehat{\\beta}), \\quad \\widehat{\\beta}+c \\times s(\\widehat{\\beta})]\n\\]\nwhere \\(c>0\\) is a pre-specified constant. This confidence interval is symmetric about the point estimator \\(\\widehat{\\beta}\\) and its length is proportional to the standard error \\(s(\\widehat{\\beta})\\).\nEquivalently, \\(\\widehat{C}\\) is the set of parameter values for \\(\\beta\\) such that the t-statistic \\(T(\\beta)\\) is smaller (in absolute value) than \\(c\\), that is\n\\[\n\\widehat{C}=\\{\\beta:|T(\\beta)| \\leq c\\}=\\left\\{\\beta:-c \\leq \\frac{\\widehat{\\beta}-\\beta}{s(\\widehat{\\beta})} \\leq c\\right\\} .\n\\]\nThe coverage probability of this confidence interval is\n\\[\n\\begin{aligned}\n\\mathbb{P}[\\beta \\in \\widehat{C}] &=\\mathbb{P}[|T(\\beta)| \\leq c] \\\\\n&=\\mathbb{P}[-c \\leq T(\\beta) \\leq c] .\n\\end{aligned}\n\\]\nSince the t-statistic \\(T(\\beta)\\) has the \\(t_{n-k}\\) distribution, (5.9) equals \\(F(c)-F(-c)\\), where \\(F(u)\\) is the student \\(t\\) distribution function with \\(n-k\\) degrees of freedom. Since \\(F(-c)=1-F(c)\\) (see Exercise 5.8), we can write (5.9) as\n\\[\n\\mathbb{P}[\\beta \\in \\widehat{C}]=2 F(c)-1 .\n\\]\nThis is the coverage probability of the interval \\(\\widehat{C}\\), and only depends on the constant \\(c\\).\nAs we mentioned before, a confidence interval has the coverage probability \\(1-\\alpha\\). This requires selecting the constant \\(c\\) so that \\(F(c)=1-\\alpha / 2\\). This holds if \\(c\\) equals the \\(1-\\alpha / 2\\) quantile of the \\(t_{n-k}\\) distribution. As there is no closed form expression for these quantiles we compute their values numerically. For example, by tinv (1-alpha/2,n-k) in MATLAB. With this choice the confidence interval (5.8) has exact coverage probability \\(1-\\alpha\\). By default, Stata reports \\(95 %\\) confidence intervals \\(\\widehat{C}\\) for each estimated regression coefficient using the same formula.\nTheorem 5.9 In the normal regression model, (5.8) with \\(c=F^{-1}(1-\\alpha / 2)\\) has coverage probability \\(\\mathbb{P}[\\beta \\in \\widehat{C}]=1-\\alpha\\). When the degree of freedom is large the distinction between the student \\(t\\) and the normal distribution is negligible. In particular, for \\(n-k \\geq 61\\) we have \\(c \\leq 2.00\\) for a \\(95 %\\) interval. Using this value we obtain the most commonly used confidence interval in applied econometric practice:\n\\[\n\\widehat{C}=[\\widehat{\\beta}-2 s(\\widehat{\\beta}), \\quad \\widehat{\\beta}+2 s(\\widehat{\\beta})] .\n\\]\nThis is a useful rule-of-thumb. This \\(95 %\\) confidence interval \\(\\widehat{C}\\) is simple to compute and can be easily calculated from coefficient estimates and standard errors.\nTheorem 5.10 In the normal regression model, if \\(n-k \\geq 61\\) then (5.10) has coverage probability \\(\\mathbb{P}[\\beta \\in \\widehat{C}] \\geq 0.95\\).\nConfidence intervals are a simple yet effective tool to assess estimation uncertainty. When reading a set of empirical results look at the estimated coefficient estimates and the standard errors. For a parameter of interest compute the confidence interval \\(\\widehat{C}\\) and consider the meaning of the spread of the suggested values. If the range of values in the confidence interval are too wide to learn about \\(\\beta\\) then do not jump to a conclusion about \\(\\beta\\) based on the point estimate alone."
  },
  {
    "objectID": "chpt05-normal-reg.html#confidence-intervals-for-error-variance",
    "href": "chpt05-normal-reg.html#confidence-intervals-for-error-variance",
    "title": "5  Normal Regression",
    "section": "5.11 Confidence Intervals for Error Variance",
    "text": "5.11 Confidence Intervals for Error Variance\nWe can also construct a confidence interval for the regression error variance \\(\\sigma^{2}\\) using the sampling distribution of \\(s^{2}\\) from Theorem 5.7. This states that in the normal regression model\n\\[\n\\frac{(n-k) s^{2}}{\\sigma^{2}} \\sim \\chi_{n-k}^{2} .\n\\]\nLet \\(F(u)\\) denote the \\(\\chi_{n-k}^{2}\\) distribution function and for some \\(\\alpha\\) set \\(c_{1}=F^{-1}(\\alpha / 2)\\) and \\(c_{2}=F^{-1}(1-\\alpha / 2)\\) (the \\(\\alpha / 2\\) and \\(1-\\alpha / 2\\) quantiles of the \\(\\chi_{n-k}^{2}\\) distribution). Equation (5.11) implies that\n\\[\n\\mathbb{P}\\left[c_{1} \\leq \\frac{(n-k) s^{2}}{\\sigma^{2}} \\leq c_{2}\\right]=F\\left(c_{2}\\right)-F\\left(c_{1}\\right)=1-\\alpha .\n\\]\nRewriting the inequalities we find\n\\[\n\\mathbb{P}\\left[\\frac{(n-k) s^{2}}{c_{2}} \\leq \\sigma^{2} \\leq \\frac{(n-k) s^{2}}{c_{1}}\\right]=1-\\alpha .\n\\]\nThis shows that an exact \\(1-\\alpha\\) confidence interval for \\(\\sigma^{2}\\) is\n\\[\n\\widehat{C}=\\left[\\frac{(n-k) s^{2}}{c_{2}}, \\quad \\frac{(n-k) s^{2}}{c_{1}}\\right] .\n\\]\nTheorem 5.11 In the normal regression model (5.12) has coverage probability \\(\\mathbb{P}\\left[\\sigma^{2} \\in \\widehat{C}\\right]=1-\\alpha\\).\nThe confidence interval (5.12) for \\(\\sigma^{2}\\) is asymmetric about the point estimate \\(s^{2}\\) due to the latter’s asymmetric sampling distribution."
  },
  {
    "objectID": "chpt05-normal-reg.html#t-test",
    "href": "chpt05-normal-reg.html#t-test",
    "title": "5  Normal Regression",
    "section": "5.12 t Test",
    "text": "5.12 t Test\nA typical goal in an econometric exercise is to assess whether or not a coefficient \\(\\beta\\) equals a specific value \\(\\beta_{0}\\). Often the specific value to be tested is \\(\\beta_{0}=0\\) but this is not essential. This is called hypothesis testing, a subject which will be explored in detail in Chapter 9. In this section and the following we give a short introduction specific to the normal regression model.\nFor simplicity write the coefficient to be tested as \\(\\beta\\). The null hypothesis is\n\\[\n\\mathbb{M}_{0}: \\beta=\\beta_{0} .\n\\]\nThis states that the hypothesis is that the true value of \\(\\beta\\) equals the hypothesized value \\(\\beta_{0}\\).\nThe alternative hypothesis is the complement of \\(\\mathbb{M}_{0}\\), and is written as\n\\[\n\\mathbb{H}_{1}: \\beta \\neq \\beta_{0} .\n\\]\nThis states that the true value of \\(\\beta\\) does not equal the hypothesized value.\nWe are interested in testing \\(\\mathbb{M}_{0}\\) against \\(\\mathbb{M}_{1}\\). The method is to design a statistic which is informative about \\(\\mathbb{M}_{1}\\). If the observed value of the statistic is consistent with random variation under the assumption that \\(\\mathbb{M}_{0}\\) is true, then we deduce that there is no evidence against \\(\\mathbb{H}_{0}\\) and consequently do not reject \\(\\mathbb{H}_{0}\\). However, if the statistic takes a value which is unlikely to occur under the assumption that \\(\\mathbb{M}_{0}\\) is true, then we deduce that there is evidence against \\(\\mathbb{M}_{0}\\) and consequently we reject \\(\\mathbb{M}_{0}\\) in favor of \\(\\mathbb{M}_{1}\\). The main steps are to design a test statistic and to characterize its sampling distribution.\nThe standard statistic to test \\(\\mathbb{M}_{0}\\) against \\(\\mathbb{M}_{1}\\) is the absolute value of the t-statistic\n\\[\n|T|=\\left|\\frac{\\widehat{\\beta}-\\beta_{0}}{s(\\widehat{\\beta})}\\right| .\n\\]\nIf \\(\\mathbb{M}_{0}\\) is true then we expect \\(|T|\\) to be small, but if \\(\\mathbb{M}_{1}\\) is true then we would expect \\(|T|\\) to be large. Hence the standard rule is to reject \\(\\mathbb{M}_{0}\\) in favor of \\(\\mathbb{M}_{1}\\) for large values of the t-statistic \\(|T|\\) and otherwise fail to reject \\(\\mathbb{H}_{0}\\). Thus the hypothesis test takes the form\n\\[\n\\text { Reject } \\mathbb{M}_{0} \\text { if }|T|>c \\text {. }\n\\]\nThe constant \\(c\\) which appears in the statement of the test is called the critical value. Its value is selected to control the probability of false rejections. When the null hypothesis is true \\(T\\) has an exact \\(t_{n-k}\\) distribution in the normal regression model. Thus for a given value of \\(c\\) the probability of false rejection is\n\\[\n\\begin{aligned}\n\\mathbb{P}\\left[\\text { Reject } \\mathbb{H}_{0} \\mid \\mathbb{B}_{0}\\right] &=\\mathbb{P}\\left[|T|>c \\mid \\mathbb{M}_{0}\\right] \\\\\n&=\\mathbb{P}\\left[T>c \\mid \\mathbb{H}_{0}\\right]+\\mathbb{P}\\left[T<-c \\mid \\mathbb{M}_{0}\\right] \\\\\n&=1-F(c)+F(-c) \\\\\n&=2(1-F(c))\n\\end{aligned}\n\\]\nwhere \\(F(u)\\) is the \\(t_{n-k}\\) distribution function. This is the probability of false rejection and is decreasing in the critical value \\(c\\). We select the value \\(c\\) so that this probability equals a pre-selected value called the significance level which is typically written as \\(\\alpha\\). It is conventional to set \\(\\alpha=0.05\\), though this is not a hard rule. We then select \\(c\\) so that \\(F(c)=1-\\alpha / 2\\), which means that \\(c\\) is the \\(1-\\alpha / 2\\) quantile (inverse CDF) of the \\(t_{n-k}\\) distribution, the same as used for confidence intervals. With this choice the decision rule “Reject \\(\\mathbb{M}_{0}\\) if \\(|T|>c\\)” has a significance level (false rejection probability) of \\(\\alpha\\). Theorem 5.12 In the normal regression model if the null hypothesis (5.13) is true, then for \\(|T|\\) defined in (5.14) \\(T \\sim t_{n-k}\\). If \\(c\\) is set so that \\(\\mathbb{P}\\left[\\left|t_{n-k}\\right| \\geq c\\right]=\\) \\(\\alpha\\),then the test “Reject \\(\\mathbb{M}_{0}\\) in favor of \\(\\mathbb{M}_{1}\\) if \\(|T|>c\\)” has significance level \\(\\alpha\\).\nTo report the result of a hypothesis test we need to pre-determine the significance level \\(\\alpha\\) in order to calculate the critical value \\(c\\). This can be inconvenient and arbitrary. A simplification is to report what is known as the p-value of the test. In general, when a test takes the form “Reject \\(\\mathbb{B}_{0}\\) if \\(S>c\\)” and \\(S\\) has null distribution \\(G(u)\\) then the p-value of the test is \\(p=1-G(S)\\). A test with significance level \\(\\alpha\\) can be restated as “Reject \\(\\mathbb{M}_{0}\\) if \\(p<\\alpha\\)”. It is sufficient to report the p-value \\(p\\) and we can interpret the value of \\(p\\) as indexing the test’s strength of rejection of the null hypothesis. Thus a \\(\\mathrm{p}\\)-value of \\(0.07\\) might be interpreted as “nearly significant”, \\(0.05\\) as “borderline significant”, and \\(0.001\\) as “highly significant”. In the context of the normal regression model the p-value of a t-statistic \\(|T|\\) is \\(p=2\\left(1-F_{n-k}(|T|)\\right)\\) where \\(F_{n-k}\\) is the \\(t_{n-k}\\) CDF. For example, in MATLAB the calculation is \\(2 *(1-\\mathrm{t} c d f(\\mathrm{abs}(\\mathrm{t}), \\mathrm{n}-\\mathrm{k}))\\). In Stata, the default is that for any estimated regression, t-statistics for each estimated coefficient are reported along with their p-values calculated using this same formula. These t-statistics test the hypotheses that each coefficient is zero.\nA p-value reports the strength of evidence against \\(\\mathbb{M}_{0}\\) but is not itself a probability. A common misunderstanding is that the p-value is the “probability that the null hypothesis is true”. This is an incorrect interpretation. It is a statistic, is random, and is a measure of the evidence against \\(\\mathbb{M}_{0}\\). Nothing more."
  },
  {
    "objectID": "chpt05-normal-reg.html#likelihood-ratio-test",
    "href": "chpt05-normal-reg.html#likelihood-ratio-test",
    "title": "5  Normal Regression",
    "section": "5.13 Likelihood Ratio Test",
    "text": "5.13 Likelihood Ratio Test\nIn the previous section we described the t-test as the standard method to test a hypothesis on a single coefficient in a regression. In many contexts, however, we want to simultaneously assess a set of coefficients. In the normal regression model, this can be done by an \\(F\\) test which can be derived from the likelihood ratio test.\nPartition the regressors as \\(X=\\left(X_{1}^{\\prime}, X_{2}^{\\prime}\\right)\\) and similarly partition the coefficient vector as \\(\\beta=\\left(\\beta_{1}^{\\prime}, \\beta_{2}^{\\prime}\\right)^{\\prime}\\). The regression model can be written as\n\\[\nY=X_{1}^{\\prime} \\beta_{1}+X_{2}^{\\prime} \\beta_{2}+e .\n\\]\nLet \\(k=\\operatorname{dim}(X), k_{1}=\\operatorname{dim}\\left(X_{1}\\right)\\), and \\(q=\\operatorname{dim}\\left(X_{2}\\right)\\), so that \\(k=k_{1}+q\\). Partition the variables so that the hypothesis is that the second set of coefficients are zero, or\n\\[\n\\mathbb{H}_{0}: \\beta_{2}=0 .\n\\]\nIf \\(\\mathbb{M}_{0}\\) is true then the regressors \\(X_{2}\\) can be omitted from the regression. In this case we can write (5.15) as\n\\[\nY=X_{1}^{\\prime} \\beta_{1}+e .\n\\]\nWe call (5.17) the null model. The alternative hypothesis is that at least one element of \\(\\beta_{2}\\) is non-zero and is written as \\(\\mathbb{H}_{1}: \\beta_{2} \\neq 0\\).\nWhen models are estimated by maximum likelihood a well-accepted testing procedure is to reject \\(\\mathbb{M}_{0}\\) in favor of \\(\\mathbb{M}_{1}\\) for large values of the Likelihood Ratio - the ratio of the maximized likelihood function under \\(\\mathbb{H}_{1}\\) and \\(\\mathbb{H}_{0}\\), respectively. We now construct this statistic in the normal regression model. Recall from (5.6) that the maximized log-likelihood equals\n\\[\n\\ell_{n}\\left(\\widehat{\\beta}, \\widehat{\\sigma}^{2}\\right)=-\\frac{n}{2} \\log \\left(2 \\pi \\widehat{\\sigma}^{2}\\right)-\\frac{n}{2} .\n\\]\nWe similarly calculate the maximized log-likelihood for the constrained model (5.17). By the same steps for derivation of the unconstrained MLE we find that the MLE for (5.17) is OLS of \\(Y\\) on \\(X_{1}\\). We can write this estimator as\n\\[\n\\widetilde{\\beta}_{1}=\\left(\\boldsymbol{X}_{1}^{\\prime} \\boldsymbol{X}_{1}\\right)^{-1} \\boldsymbol{X}_{1}^{\\prime} \\boldsymbol{Y}\n\\]\nwith residual \\(\\widetilde{e}_{i}=Y_{i}-X_{1 i}^{\\prime} \\widetilde{\\beta}_{1}\\) and error variance estimate \\(\\widetilde{\\sigma}^{2}=\\frac{1}{n} \\sum_{i=1}^{n} \\widetilde{e}_{i}^{2}\\). We use tildes ” \\(\\sim\\) ” rather than hats ” \\(\\wedge\\) ” above the constrained estimates to distinguish them from the unconstrained estimates. You can calculate similar to (5.6) that the maximized constrained log-likelihood is\n\\[\n\\ell_{n}\\left(\\widetilde{\\beta}_{1}, \\widetilde{\\sigma}^{2}\\right)=-\\frac{n}{2} \\log \\left(2 \\pi \\widetilde{\\sigma}^{2}\\right)-\\frac{n}{2} .\n\\]\nA classic testing procedure is to reject \\(\\mathbb{H}_{0}\\) for large values of the ratio of the maximized likelihoods. Equivalently the test rejects \\(\\mathbb{H}_{0}\\) for large values of twice the difference in the log-likelihood functions. (Multiplying the likelihood difference by two turns out to be a useful scaling.) This equals\n\\[\n\\begin{aligned}\n\\mathrm{LR} &=2\\left(\\ell_{n}\\left(\\widehat{\\beta}, \\widehat{\\sigma}^{2}\\right)-\\ell_{n}\\left(\\widetilde{\\beta}_{1}, \\widetilde{\\sigma}^{2}\\right)\\right) \\\\\n&=2\\left(\\left(-\\frac{n}{2} \\log \\left(2 \\pi \\widehat{\\sigma}^{2}\\right)-\\frac{n}{2}\\right)-\\left(-\\frac{n}{2} \\log \\left(2 \\pi \\widetilde{\\sigma}^{2}\\right)-\\frac{n}{2}\\right)\\right) \\\\\n&=n \\log \\left(\\frac{\\widetilde{\\sigma}^{2}}{\\widehat{\\sigma}^{2}}\\right) .\n\\end{aligned}\n\\]\nThe likelihood ratio test rejects \\(\\mathbb{H}_{0}\\) for large values of LR, or equivalently (see Exercise \\(5.10\\) ) for large values of\n\\[\n\\mathrm{F}=\\frac{\\left(\\widetilde{\\sigma}^{2}-\\widehat{\\sigma}^{2}\\right) / q}{\\widehat{\\sigma}^{2} /(n-k)} .\n\\]\nThis is known as the \\(F\\) statistic for the test of hypothesis \\(\\mathbb{M}_{0}\\) against \\(\\mathbb{M}_{1}\\).\nTo develop an appropriate critical value we need the null distribution of \\(F\\). Recall from (3.28) that \\(n \\widehat{\\sigma}^{2}=\\boldsymbol{e}^{\\prime} \\boldsymbol{M} \\boldsymbol{e}\\) where \\(\\boldsymbol{M}=\\boldsymbol{I}_{n}-\\boldsymbol{P}\\) with \\(\\boldsymbol{P}=\\boldsymbol{X}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\boldsymbol{X}^{\\prime}\\). Similarly, under \\(\\mathbb{H}_{0}, n \\widetilde{\\sigma}^{2}=\\boldsymbol{e}^{\\prime} \\boldsymbol{M}_{1} \\boldsymbol{e}\\) where \\(\\boldsymbol{M}=\\) \\(\\boldsymbol{I}_{n}-\\boldsymbol{P}_{1}\\) with \\(\\boldsymbol{P}_{1}=\\boldsymbol{X}_{1}\\left(\\boldsymbol{X}_{1}^{\\prime} \\boldsymbol{X}_{1}\\right)^{-1} \\boldsymbol{X}_{1}^{\\prime}\\). You can calculate that \\(\\boldsymbol{M}_{1}-\\boldsymbol{M}=\\boldsymbol{P}-\\boldsymbol{P}_{1}\\) is idempotent with rank \\(q\\). Furthermore, \\(\\left(\\boldsymbol{M}_{1}-\\boldsymbol{M}\\right) \\boldsymbol{M}=0\\). It follows that \\(\\boldsymbol{e}^{\\prime}\\left(\\boldsymbol{M}_{1}-\\boldsymbol{M}\\right) \\boldsymbol{e} \\sim \\chi_{q}^{2}\\) and is independent of \\(\\boldsymbol{e}^{\\prime} \\boldsymbol{M} \\boldsymbol{e}\\). Hence\n\\[\n\\mathrm{F}=\\frac{\\boldsymbol{e}^{\\prime}\\left(\\boldsymbol{M}_{1}-\\boldsymbol{M}\\right) \\boldsymbol{e} / q}{\\boldsymbol{e}^{\\prime} \\boldsymbol{M} \\boldsymbol{e} /(n-k)} \\sim \\frac{\\chi_{q}^{2} / q}{\\chi_{n-k}^{2} /(n-k)} \\sim F_{q, n-k}\n\\]\nan exact \\(F\\) distribution with degrees of freedom \\(q\\) and \\(n-k\\), respectively. Thus under \\(\\mathbb{H}_{0}\\), the \\(F\\) statistic has an exact \\(F\\) distribution.\nThe critical values are selected from the upper tail of the \\(F\\) distribution. For a given significance level \\(\\alpha\\) (typically \\(\\alpha=0.05\\) ) we select the critical value \\(c\\) so that \\(\\mathbb{P}\\left[F_{q, n-k} \\geq c\\right]=\\alpha\\). For example, in MATLAB the expression is \\(f \\operatorname{inv}(1-\\alpha, \\mathrm{q}, \\mathrm{n}-\\mathrm{k})\\). The test rejects \\(\\mathbb{M}_{0}\\) in favor of \\(\\mathbb{B}_{1}\\) if \\(F>c\\) and does not reject \\(\\mathbb{H}_{0}\\) otherwise. The p-value of the test is \\(p=1-G_{q, n-k}(F)\\) where \\(G_{q, n-k}(u)\\) is the \\(F_{q, n-k}\\) distribution function. In MATLAB, the p-value is computed as \\(1-\\mathrm{f} c d f(\\mathrm{f}, \\mathrm{q}, \\mathrm{n}-\\mathrm{k})\\). It is equivalent to reject \\(\\mathbb{H}_{0}\\) if \\(F>c\\) or \\(p<\\alpha\\).\nIn Stata, the command to test multiple coefficients takes the form ‘test X1 X2’ where X1 and X2 are the names of the variables whose coefficients are tested. Stata then reports the F statistic for the hypothesis that the coefficients are jointly zero along with the p-value calculated using the \\(F\\) distribution.\nTheorem 5.13 In the normal regression model, if the null hypothesis (5.16) is true, then for \\(F\\) defined in (5.19), \\(F \\sim F_{q, n-k}\\). If \\(c\\) is set so that \\(\\mathbb{P}\\left[F_{q, n-k} \\geq c\\right]=\\alpha\\) then the test “Reject \\(\\mathbb{M}_{0}\\) in favor of \\(\\mathbb{M}_{1}\\) if \\(F>c\\)” has significance level \\(\\alpha\\). Theorem \\(5.13\\) justifies the \\(F\\) test in the normal regression model with critical values from the \\(F\\) distribution."
  },
  {
    "objectID": "chpt05-normal-reg.html#information-bound-for-normal-regression",
    "href": "chpt05-normal-reg.html#information-bound-for-normal-regression",
    "title": "5  Normal Regression",
    "section": "5.14 Information Bound for Normal Regression",
    "text": "5.14 Information Bound for Normal Regression\nThis section requires a familiarity with the theory of the Cramér-Rao Lower Bound. See Chapter 10 of Probability and Statistics for Economists.\nThe likelihood scores for the normal regression model are\n\\[\n\\frac{\\partial}{\\partial \\beta} \\ell_{n}\\left(\\beta, \\sigma^{2}\\right)=\\frac{1}{\\sigma^{2}} \\sum_{i=1}^{n} X_{i}\\left(Y_{i}-X_{i}^{\\prime} \\beta\\right)=\\frac{1}{\\sigma^{2}} \\sum_{i=1}^{n} X_{i} e_{i}\n\\]\nand\n\\[\n\\frac{\\partial}{\\partial \\sigma^{2}} \\ell_{n}\\left(\\beta, \\sigma^{2}\\right)=-\\frac{n}{2 \\sigma^{2}}+\\frac{1}{2 \\sigma^{4}} \\sum_{i=1}^{n}\\left(Y_{i}-X_{i}^{\\prime} \\beta\\right)^{2}=\\frac{1}{2 \\sigma^{4}} \\sum_{i=1}^{n}\\left(e_{i}^{2}-\\sigma^{2}\\right)\n\\]\nIt follows that the information matrix is\n\\[\n\\mathscr{I}=\\operatorname{var}\\left[\\begin{array}{c}\n\\frac{\\partial}{\\partial \\beta} \\ell\\left(\\beta, \\sigma^{2}\\right) \\\\\n\\frac{\\partial}{\\partial \\sigma^{2}} \\ell\\left(\\beta, \\sigma^{2}\\right)\n\\end{array} \\mid \\boldsymbol{X}\\right]=\\left(\\begin{array}{cc}\n\\frac{1}{\\sigma^{2}} \\boldsymbol{X}^{\\prime} \\boldsymbol{X} & 0 \\\\\n0 & \\frac{2 \\sigma^{4}}{n}\n\\end{array}\\right)\n\\]\n(see Exercise 5.11). The Cramér-Rao Lower Bound is\n\\[\n\\mathscr{I}^{-1}=\\left(\\begin{array}{cc}\n\\sigma^{2}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} & 0 \\\\\n0 & \\frac{2 \\sigma^{4}}{n}\n\\end{array}\\right)\n\\]\nThis shows that the lower bound for estimation of \\(\\beta\\) is \\(\\sigma^{2}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1}\\) and the lower bound for \\(\\sigma^{2}\\) is \\(2 \\sigma^{4} / n\\).\nThe unbiased variance estimator \\(s^{2}\\) of \\(\\sigma^{2}\\) has variance \\(2 \\sigma^{4} /(n-k)\\) (see Exercise 5.12) which is larger than the Cramér-Rao lower bound \\(2 \\sigma^{4} / n\\). Thus in contrast to the coefficient estimator, the variance estimator is not Cramér-Rao efficient."
  },
  {
    "objectID": "chpt05-normal-reg.html#exercises",
    "href": "chpt05-normal-reg.html#exercises",
    "title": "5  Normal Regression",
    "section": "5.15 Exercises",
    "text": "5.15 Exercises\nExercise 5.1 Show that if \\(Q \\sim \\chi_{r}^{2}\\), then \\(\\mathbb{E}[Q]=r\\) and \\(\\operatorname{var}[Q]=2 r\\).\nHint: Use the representation \\(Q=\\sum_{i=1}^{n} Z_{i}^{2}\\) with \\(Z_{i}\\) independent \\(\\mathrm{N}(0,1)\\).\nExercise 5.2 Show that if \\(\\boldsymbol{e} \\sim \\mathrm{N}\\left(0, \\boldsymbol{I}_{n} \\sigma^{2}\\right)\\) and \\(\\boldsymbol{H}^{\\prime} \\boldsymbol{H}=\\boldsymbol{I}_{n}\\) then \\(\\boldsymbol{u}=\\boldsymbol{H}^{\\prime} \\boldsymbol{e} \\sim \\mathrm{N}\\left(0, \\boldsymbol{I}_{n} \\sigma^{2}\\right)\\).\nExercise 5.3 Show that if \\(\\boldsymbol{e} \\sim \\mathrm{N}(0, \\Sigma)\\) and \\(\\Sigma=\\boldsymbol{A} \\boldsymbol{A}^{\\prime}\\) then \\(\\boldsymbol{u}=\\boldsymbol{A}^{-1} \\boldsymbol{e} \\sim \\mathrm{N}\\left(0, \\boldsymbol{I}_{n}\\right)\\).\nExercise 5.4 Show that \\(\\operatorname{argmax}_{\\theta \\in \\Theta} \\ell_{n}(\\theta)=\\operatorname{argmax}_{\\theta \\in \\Theta} L_{n}(\\theta)\\).\nExercise 5.5 For the regression in-sample predicted values \\(\\widehat{Y}_{i}\\) show that \\(\\widehat{Y}_{i} \\mid \\boldsymbol{X} \\sim \\mathrm{N}\\left(X_{i}^{\\prime} \\beta, \\sigma^{2} h_{i i}\\right)\\) where \\(h_{i i}\\) are the leverage values (3.40).\nExercise 5.6 In the normal regression model show that the leave-one out prediction errors \\(\\widetilde{e}_{i}\\) and the standardized residuals \\(\\bar{e}_{i}\\) are independent of \\(\\widehat{\\beta}\\), conditional on \\(\\boldsymbol{X}\\).\nHint: Use (3.45) and (4.29). Exercise 5.7 In the normal regression model show that the robust covariance matrices \\(\\widehat{\\boldsymbol{V}}_{\\widehat{\\beta}}^{\\mathrm{HC} 0}, \\widehat{\\boldsymbol{V}}_{\\widehat{\\beta}}^{\\mathrm{HCl}}\\), \\(\\widehat{\\boldsymbol{V}}_{\\widehat{\\beta}}^{\\mathrm{HC} 2}\\), and \\(\\widehat{\\boldsymbol{V}}_{\\widehat{\\beta}}^{\\mathrm{HC}}\\) are independent of the OLS estimator \\(\\widehat{\\beta}\\), conditional on \\(\\boldsymbol{X}\\).\nExercise 5.8 Let \\(F(u)\\) be the distribution function of a random variable \\(X\\) whose density is symmetric about zero. (This includes the standard normal and the student \\(t\\).) Show that \\(F(-u)=1-F(u)\\).\nExercise 5.9 Let \\(\\widehat{C}_{\\beta}=[L, U]\\) be a \\(1-\\alpha\\) confidence interval for \\(\\beta\\), and consider the transformation \\(\\theta=g(\\beta)\\) where \\(g(\\cdot)\\) is monotonically increasing. Consider the confidence interval \\(\\widehat{C}_{\\theta}=[g(L), g(U)]\\) for \\(\\theta\\). Show that \\(\\mathbb{P}\\left[\\theta \\in \\widehat{C}_{\\theta}\\right]=\\mathbb{P}\\left[\\beta \\in \\widehat{C}_{\\beta}\\right]\\). Use this result to develop a confidence interval for \\(\\sigma\\).\nExercise 5.10 Show that the test “Reject \\(\\mathbb{M}_{0}\\) if \\(L R \\geq c_{1}\\)” for LR defined in (5.18), and the test “Reject \\(\\mathbb{H}_{0}\\) if \\(\\mathrm{F} \\geq c_{2}\\)” for \\(\\mathrm{F}\\) defined in (5.19), yield the same decisions if \\(c_{2}=\\left(\\exp \\left(c_{1} / n\\right)-1\\right)(n-k) / q\\). Does this mean that the two tests are equivalent?\nExercise 5.11 Show (5.20).\nExercise 5.12 In the normal regression model let \\(s^{2}\\) be the unbiased estimator of the error variance \\(\\sigma^{2}\\) from (4.31).\n\nShow that \\(\\operatorname{var}\\left[s^{2}\\right]=2 \\sigma^{4} /(n-k)\\).\nShow that \\(\\operatorname{var}\\left[s^{2}\\right]\\) is strictly larger than the Cramér-Rao Lower Bound for \\(\\sigma^{2}\\)."
  },
  {
    "objectID": "chpt06-review.html",
    "href": "chpt06-review.html",
    "title": "6  A Review of Large Sample Asymptotics",
    "section": "",
    "text": "The most widely-used tool in sampling theory is large sample asymptotics. By “asymptotics” we mean approximating a finite-sample sampling distribution by taking its limit as the sample size diverges to infinity. In this chapter we provide a brief review of the main results of large sample asymptotics. It is meant as a reference, not as a teaching guide. Asymptotic theory is covered in detail in Chapters 7-9 of Probability and Statistics for Economists. If you have not previous studied asymptotic theory in detail you should study these chapters before proceeding."
  },
  {
    "objectID": "chpt06-review.html#modes-of-convergence",
    "href": "chpt06-review.html#modes-of-convergence",
    "title": "6  A Review of Large Sample Asymptotics",
    "section": "6.2 Modes of Convergence",
    "text": "6.2 Modes of Convergence\nDefinition 6.1 A sequence of random vectors \\(Z_{n} \\in \\mathbb{R}^{k}\\) converges in probability to \\(Z\\) as \\(n \\rightarrow \\infty\\), denoted \\(Z_{n} \\underset{p}{\\rightarrow} Z\\) or alternatively \\(\\operatorname{plim}_{n \\rightarrow \\infty} Z_{n}=Z\\), if for all \\(\\delta>0\\)\n\\[\n\\lim _{n \\rightarrow \\infty} \\mathbb{P}\\left[\\left\\|Z_{n}-Z\\right\\| \\leq \\delta\\right]=1 .\n\\]\nWe call \\(Z\\) the probability limit (or plim) of \\(Z_{n}\\).\nThe above definition treats random variables and random vectors simultaneously using the vector norm. It is useful to know that for a random vector, (6.1) holds if and only if each element in the vector converges in probability to its limit.\nDefinition 6.2 Let \\(Z_{n}\\) be a sequence of random vectors with distributions \\(F_{n}(u)=\\mathbb{P}\\left[Z_{n} \\leq u\\right]\\). We say that \\(Z_{n}\\) converges in distribution to \\(Z\\) as \\(n \\rightarrow \\infty\\), denoted \\(Z_{n} \\underset{d}{\\rightarrow} Z\\), if for all \\(u\\) at which \\(F(u)=\\mathbb{P}[Z \\leq u]\\) is continuous, \\(F_{n}(u) \\rightarrow\\) \\(F(u)\\) as \\(n \\rightarrow \\infty\\). We refer to \\(Z\\) and its distribution \\(F(u)\\) as the asymptotic distribution, large sample distribution, or limit distribution of \\(Z_{n}\\)."
  },
  {
    "objectID": "chpt06-review.html#weak-law-of-large-numbers",
    "href": "chpt06-review.html#weak-law-of-large-numbers",
    "title": "6  A Review of Large Sample Asymptotics",
    "section": "6.3 Weak Law of Large Numbers",
    "text": "6.3 Weak Law of Large Numbers\nTheorem 6.1 Weak Law of Large Numbers (WLLN)\nIf \\(Y_{i} \\in \\mathbb{R}^{k}\\) are i.i.d. and \\(\\mathbb{E}\\|Y\\|<\\infty\\), then as \\(n \\rightarrow \\infty\\),\n\\[\n\\bar{Y}=\\frac{1}{n} \\sum_{i=1}^{n} Y_{i} \\underset{p}{\\longrightarrow}[Y] .\n\\]\nThe WLLN shows that the sample mean \\(\\bar{Y}\\) converges in probability to the true population expectation \\(\\mu\\). The result applies to any transformation of a random vector with a finite mean.\nTheorem 6.2 If \\(Y_{i} \\in \\mathbb{R}^{k}\\) are i.i.d., \\(h(y): \\mathbb{R}^{k} \\rightarrow \\mathbb{R}^{q}\\), and \\(\\mathbb{E}\\|h(Y)\\|<\\infty\\), then \\(\\widehat{\\mu}=\\) \\(\\frac{1}{n} \\sum_{i=1}^{n} h\\left(Y_{i}\\right) \\underset{p}{\\rightarrow} \\mu=\\mathbb{E}[h(Y)]\\) as \\(n \\rightarrow \\infty\\).\nAn estimator which converges in probability to the population value is called consistent.\nDefinition 6.3 An estimator \\(\\widehat{\\theta}\\) of \\(\\theta\\) is consistent if \\(\\widehat{\\theta} \\underset{p}{\\longrightarrow} \\theta\\) as \\(n \\rightarrow \\infty\\)."
  },
  {
    "objectID": "chpt06-review.html#central-limit-theorem",
    "href": "chpt06-review.html#central-limit-theorem",
    "title": "6  A Review of Large Sample Asymptotics",
    "section": "6.4 Central Limit Theorem",
    "text": "6.4 Central Limit Theorem\nTheorem 6.3 Multivariate Lindeberg-Lévy Central Limit Theorem (CLT). If \\(Y_{i} \\in \\mathbb{R}^{k}\\) are i.i.d. and \\(\\mathbb{E}\\|Y\\|^{2}<\\infty\\), then as \\(n \\rightarrow \\infty\\)\n\\[\n\\sqrt{n}(\\bar{Y}-\\mu) \\underset{d}{\\longrightarrow} \\mathrm{N}(0, \\boldsymbol{V})\n\\]\nwhere \\(\\mu=\\mathbb{E}[Y]\\) and \\(\\boldsymbol{V}=\\mathbb{E}\\left[(Y-\\mu)(Y-\\mu)^{\\prime}\\right]\\).\nThe central limit theorem shows that the distribution of the sample mean is approximately normal in large samples. For some applications it may be useful to notice that Theorem \\(6.3\\) does not impose any restrictions on \\(\\boldsymbol{V}\\) other than that the elements are finite. Therefore this result allows for the possibility of singular \\(V\\).\nThe following two generalizations allow for heterogeneous random variables. Theorem 6.4 Multivariate Lindeberg CLT. Suppose that for all \\(n, Y_{n i} \\in \\mathbb{R}^{k}, i=\\) \\(1, \\ldots, r_{n}\\), are independent but not necessarily identically distributed with expectations \\(\\mathbb{E}\\left[Y_{n i}\\right]=0\\) and variance matrices \\(\\boldsymbol{V}_{n i}=\\mathbb{E}\\left[Y_{n i} Y_{n i}^{\\prime}\\right]\\). Set \\(\\overline{\\boldsymbol{V}}_{n}=\\sum_{i=1}^{n} \\boldsymbol{V}_{n i}\\). Suppose \\(v_{n}^{2}=\\lambda_{\\min }\\left(\\overline{\\boldsymbol{V}}_{n}\\right)>0\\) and for all \\(\\epsilon>0\\)\n\\[\n\\lim _{n \\rightarrow \\infty} \\frac{1}{v_{n}^{2}} \\sum_{i=1}^{r_{n}} \\mathbb{E}\\left[\\left\\|Y_{n i}\\right\\|^{2} \\mathbb{1}\\left\\{\\left\\|Y_{n i}\\right\\|^{2} \\geq \\epsilon v_{n}^{2}\\right\\}\\right]=0 .\n\\]\nThen as \\(n \\rightarrow \\infty\\)\n\\[\n\\overline{\\boldsymbol{V}}_{n}^{-1 / 2} \\sum_{i=1}^{r_{n}} Y_{n i} \\underset{d}{\\longrightarrow} \\mathrm{N}\\left(0, \\boldsymbol{I}_{k}\\right) .\n\\]\nTheorem 6.5 Suppose \\(Y_{n i} \\in \\mathbb{R}^{k}\\) are independent but not necessarily identically distributed with expectations \\(\\mathbb{E}\\left[Y_{n i}\\right]=0\\) and variance matrices \\(\\boldsymbol{V}_{n i}=\\) \\(\\mathbb{E}\\left[Y_{n i} Y_{n i}^{\\prime}\\right]\\). Suppose\n\\[\n\\frac{1}{n} \\sum_{i=1}^{n} \\boldsymbol{V}_{n i} \\rightarrow \\boldsymbol{V}>0\n\\]\nand for some \\(\\delta>0\\)\n\\[\n\\sup _{n, i} \\mathbb{E}\\left\\|Y_{n i}\\right\\|^{2+\\delta}<\\infty .\n\\]\nThen as \\(n \\rightarrow \\infty\\)\n\\[\n\\sqrt{n} \\bar{Y} \\underset{d}{\\longrightarrow} \\mathrm{N}(0, \\boldsymbol{V})\n\\]"
  },
  {
    "objectID": "chpt06-review.html#continuous-mapping-theorem-and-delta-method",
    "href": "chpt06-review.html#continuous-mapping-theorem-and-delta-method",
    "title": "6  A Review of Large Sample Asymptotics",
    "section": "6.5 Continuous Mapping Theorem and Delta Method",
    "text": "6.5 Continuous Mapping Theorem and Delta Method\nContinuous functions are limit-preserving. There are two forms of the continuous mapping theorem, for convergence in probability and convergence in distribution.\nTheorem 6.6 Continuous Mapping Theorem (CMT). Let \\(Z_{n} \\in \\mathbb{R}^{k}\\) and \\(g(u):\\) \\(\\mathbb{R}^{k} \\rightarrow \\mathbb{R}^{q}\\). If \\(Z_{n} \\underset{p}{\\longrightarrow}\\) as \\(n \\rightarrow \\infty\\) and \\(g(u)\\) is continuous at \\(c\\) then \\(g\\left(Z_{n}\\right) \\underset{p}{\\longrightarrow} g(c)\\) as \\(n \\rightarrow \\infty\\)\nTheorem 6.7 Continuous Mapping Theorem. If \\(Z_{n} \\underset{d}{\\longrightarrow} Z\\) as \\(n \\rightarrow \\infty\\) and \\(g:\\) \\(\\mathbb{R}^{m} \\rightarrow \\mathbb{R}^{k}\\) has the set of discontinuity points \\(D_{g}\\) such that \\(\\mathbb{P}\\left[Z \\in D_{g}\\right]=0\\), then \\(g\\left(Z_{n}\\right) \\underset{d}{\\longrightarrow} g(Z)\\) as \\(n \\rightarrow \\infty\\) Differentiable functions of asymptotically normal random estimators are asymptotically normal.\nTheorem 6.8 Delta Method. Let \\(\\mu \\in \\mathbb{R}^{k}\\) and \\(g(u): \\mathbb{R}^{k} \\rightarrow \\mathbb{R}^{q}\\). If \\(\\sqrt{n}(\\widehat{\\mu}-\\mu) \\underset{d}{\\rightarrow}\\), where \\(g(u)\\) is continuously differentiable in a neighborhood of \\(\\mu\\), then as \\(n \\rightarrow\\) \\(\\infty\\)\n\\[\n\\sqrt{n}(g(\\widehat{\\mu})-g(\\mu)) \\underset{d}{\\longrightarrow} \\boldsymbol{G}^{\\prime} \\xi\n\\]\nwhere \\(\\boldsymbol{G}(u)=\\frac{\\partial}{\\partial u} g(u)^{\\prime}\\) and \\(\\boldsymbol{G}=\\boldsymbol{G}(\\mu)\\). In particular, if \\(\\xi \\sim \\mathrm{N}(0, \\boldsymbol{V})\\) then as \\(n \\rightarrow \\infty\\)\n\\[\n\\sqrt{n}(g(\\widehat{\\mu})-g(\\mu)) \\underset{d}{\\longrightarrow} \\mathrm{N}\\left(0, \\boldsymbol{G}^{\\prime} \\boldsymbol{V} \\boldsymbol{G}\\right) .\n\\]"
  },
  {
    "objectID": "chpt06-review.html#smooth-function-model",
    "href": "chpt06-review.html#smooth-function-model",
    "title": "6  A Review of Large Sample Asymptotics",
    "section": "6.6 Smooth Function Model",
    "text": "6.6 Smooth Function Model\nThe smooth function model is \\(\\theta=g(\\mu)\\) where \\(\\mu=\\mathbb{E}[h(Y)]\\) and \\(g(\\mu)\\) is smooth in a suitable sense.\nThe parameter \\(\\theta=g(\\mu)\\) is not a population moment so it does not have a direct moment estimator. Instead, it is common to use a plug-in estimator formed by replacing the unknown \\(\\mu\\) with its point estimator \\(\\widehat{\\mu}\\) and then “plugging” this into the expression for \\(\\theta\\). The first step is the sample mean \\(\\widehat{\\mu}=n^{-1} \\sum_{i=1}^{n} h\\left(Y_{i}\\right)\\). The second step is the transformation \\(\\widehat{\\theta}=g(\\widehat{\\mu})\\). The hat ” \\(\\wedge\\) ” indicates that \\(\\widehat{\\theta}\\) is a sample estimator of \\(\\theta\\). The smooth function model includes a broad class of estimators including sample variances and the least squares estimator.\nTheorem 6.9 If \\(Y_{i} \\in \\mathbb{R}^{m}\\) are i.i.d., \\(h(u): \\mathbb{R}^{m} \\rightarrow \\mathbb{R}^{k}, \\mathbb{E}\\|h(Y)\\|<\\infty\\), and \\(g(u):\\) \\(\\mathbb{R}^{k} \\rightarrow \\mathbb{R}^{q}\\) is continuous at \\(\\mu\\), then \\(\\widehat{\\theta} \\underset{p}{\\longrightarrow} \\theta\\) as \\(n \\rightarrow \\infty\\).\nTheorem 6.10 If \\(Y_{i} \\in \\mathbb{R}^{m}\\) are i.i.d., \\(h(u): \\mathbb{R}^{m} \\rightarrow \\mathbb{R}^{k}, \\mathbb{E}\\|h(Y)\\|^{2}<\\infty, g(u): \\mathbb{R}^{k} \\rightarrow\\) \\(\\mathbb{R}^{q}\\), and \\(\\boldsymbol{G}(u)=\\frac{\\partial}{\\partial u} g(u)^{\\prime}\\) is continuous in a neighborhood of \\(\\mu\\), then as \\(n \\rightarrow \\infty\\)\n\\[\n\\sqrt{n}(\\widehat{\\theta}-\\theta) \\underset{d}{\\longrightarrow} \\mathrm{N}\\left(0, \\boldsymbol{V}_{\\theta}\\right)\n\\]\nwhere \\(\\boldsymbol{V}_{\\theta}=\\boldsymbol{G}^{\\prime} \\boldsymbol{V} \\boldsymbol{G}, \\boldsymbol{V}=\\mathbb{E}\\left[(h(Y)-\\mu)(h(Y)-\\mu)^{\\prime}\\right]\\), and \\(\\boldsymbol{G}=\\boldsymbol{G}(\\mu)\\).\nTheorem \\(6.9\\) establishes the consistency of \\(\\widehat{\\theta}\\) for \\(\\theta\\) and Theorem \\(6.10\\) establishes its asymptotic normality. It is instructive to compare the conditions. Consistency requires that \\(h(Y)\\) has a finite expectation; asymptotic normality requires that \\(h(Y)\\) has a finite variance. Consistency requires that \\(g(u)\\) be continuous; asymptotic normality requires that \\(g(u)\\) is continuously differentiable."
  },
  {
    "objectID": "chpt06-review.html#stochastic-order-symbols",
    "href": "chpt06-review.html#stochastic-order-symbols",
    "title": "6  A Review of Large Sample Asymptotics",
    "section": "6.7 Stochastic Order Symbols",
    "text": "6.7 Stochastic Order Symbols\nIt is convenient to have simple symbols for random variables and vectors which converge in probability to zero or are stochastically bounded. In this section we introduce some of the most common notation.\nLet \\(Z_{n}\\) and \\(a_{n}, n=1,2, \\ldots\\) be sequences of random variables and constants. The notation\n\\[\nZ_{n}=o_{p}(1)\n\\]\n(“small oh-P-one”) means that \\(Z_{n} \\underset{p}{\\longrightarrow} 0\\) as \\(n \\rightarrow \\infty\\). We also write\n\\[\nZ_{n}=o_{p}\\left(a_{n}\\right)\n\\]\nif \\(a_{n}^{-1} Z_{n}=o_{p}(1)\\)\nSimilarly, the notation \\(Z_{n}=O_{p}\\) (1) (“big oh-P-one”) means that \\(Z_{n}\\) is bounded in probability. Precisely, for any \\(\\epsilon>0\\) there is a constant \\(M_{\\epsilon}<\\infty\\) such that\n\\[\n\\limsup _{n \\rightarrow \\infty} \\mathbb{P}\\left[\\left|Z_{n}\\right|>M_{\\epsilon}\\right] \\leq \\epsilon .\n\\]\nFurthermore, we write\n\\[\nZ_{n}=O_{p}\\left(a_{n}\\right)\n\\]\nif \\(a_{n}^{-1} Z_{n}=O_{p}(1)\\).\n\\(O_{p}(1)\\) is weaker than \\(o_{p}(1)\\) in the sense that \\(Z_{n}=o_{p}(1)\\) implies \\(Z_{n}=O_{p}(1)\\) but not the reverse. However, if \\(Z_{n}=O_{p}\\left(a_{n}\\right)\\) then \\(Z_{n}=o_{p}\\left(b_{n}\\right)\\) for any \\(b_{n}\\) such that \\(a_{n} / b_{n} \\rightarrow 0\\).\nA random sequence with a bounded moment is stochastically bounded.\nTheorem 6.11 If \\(Z_{n}\\) is a random vector which satisfies \\(\\mathbb{E}\\left\\|Z_{n}\\right\\|^{\\delta}=O\\left(a_{n}\\right)\\) for some sequence \\(a_{n}\\) and \\(\\delta>0\\), then \\(Z_{n}=O_{p}\\left(a_{n}^{1 / \\delta}\\right)\\). Similarly, \\(\\mathbb{E}\\left\\|Z_{n}\\right\\|^{\\delta}=o\\left(a_{n}\\right)\\) implies \\(Z_{n}=o_{p}\\left(a_{n}^{1 / \\delta}\\right)\\).\nThere are many simple rules for manipulating \\(o_{p}(1)\\) and \\(O_{p}(1)\\) sequences which can be deduced from the continuous mapping theorem. For example,\n\\[\n\\begin{aligned}\no_{p}(1)+o_{p}(1) &=o_{p}(1) \\\\\no_{p}(1)+O_{p}(1) &=O_{p}(1) \\\\\nO_{p}(1)+O_{p}(1) &=O_{p}(1) \\\\\no_{p}(1) o_{p}(1) &=o_{p}(1) \\\\\no_{p}(1) O_{p}(1) &=o_{p}(1) \\\\\nO_{p}(1) O_{p}(1) &=O_{p}(1) .\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "chpt06-review.html#convergence-of-moments",
    "href": "chpt06-review.html#convergence-of-moments",
    "title": "6  A Review of Large Sample Asymptotics",
    "section": "6.8 Convergence of Moments",
    "text": "6.8 Convergence of Moments\nWe give a sufficient condition for the existence of the mean of the asymptotic distribution, define uniform integrability, provide a primitive condition for uniform integrability, and show that uniform integrability is the key condition under which \\(\\mathbb{E}\\left[Z_{n}\\right]\\) converges to \\(\\mathbb{E}[Z]\\). Theorem 6.12 If \\(Z_{n} \\underset{d}{\\longrightarrow} Z\\) and \\(\\mathbb{E}\\left\\|Z_{n}\\right\\| \\leq C\\) then \\(\\mathbb{E}\\|Z\\| \\leq C\\).\nDefinition 6.4 The random vector \\(Z_{n}\\) is uniformly integrable as \\(n \\rightarrow \\infty\\) if\n\\[\n\\lim _{M \\rightarrow \\infty} \\limsup _{n \\rightarrow \\infty} \\mathbb{E}\\left[\\left\\|Z_{n}\\right\\| \\mathbb{1}\\left\\{\\left\\|Z_{n}\\right\\|>M\\right\\}\\right]=0\n\\]\nTheorem 6.13 If for some \\(\\delta>0\\), \\(\\mathbb{E}\\left\\|Z_{n}\\right\\|^{1+\\delta} \\leq C<\\infty\\), then \\(Z_{n}\\) is uniformly integrable.\nTheorem 6.14 If \\(Z_{n} \\underset{d}{\\longrightarrow} Z\\) and \\(Z_{n}\\) is uniformly integrable then \\(\\mathbb{E}\\left[Z_{n}\\right] \\longrightarrow \\mathbb{E}[Z]\\).\nThe following is a uniform stochastic bound.\nTheorem 6.15 If \\(\\left|Y_{i}\\right|^{r}\\) is uniformly integrable, then as \\(n \\rightarrow \\infty\\)\n\\[\nn^{-1 / r} \\max _{1 \\leq i \\leq n}\\left|Y_{i}\\right| \\underset{p}{\\longrightarrow} 0\n\\]\nEquation (6.6) implies that if \\(Y\\) has \\(r\\) finite moments then the largest observation will diverge at a rate slower than \\(n^{1 / r}\\). The higher the moments, the slower the rate of divergence."
  },
  {
    "objectID": "chpt07-asymptotic-ls.html",
    "href": "chpt07-asymptotic-ls.html",
    "title": "7  Asymptotic Theory for Least Squares",
    "section": "",
    "text": "It turns out that the asymptotic theory of least squares estimation applies equally to the projection model and the linear CEF model. Therefore the results in this chapter will be stated for the broader projection model described in Section 2.18. Recall that the model is \\(Y=X^{\\prime} \\beta+e\\) with the linear projection coefficient \\(\\beta=\\left(\\mathbb{E}\\left[X X^{\\prime}\\right]\\right)^{-1} \\mathbb{E}[X Y]\\).\nMaintained assumptions in this chapter will be random sampling (Assumption 1.2) and finite second moments (Assumption 2.1). We restate these here for clarity.\nAssumption 7.1\n\nThe variables \\(\\left(Y_{i}, X_{i}\\right), i=1, \\ldots, n\\), are i.i.d.\n\\(\\mathbb{E}\\left[Y^{2}\\right]<\\infty\\).\n\\(\\mathbb{E}\\|X\\|^{2}<\\infty\\).\n\\(\\boldsymbol{Q}_{X X}=\\mathbb{E}\\left[X X^{\\prime}\\right]\\) is positive definite.\n\nThe distributional results will require a strengthening of these assumptions to finite fourth moments. We discuss the specific conditions in Section 7.3."
  },
  {
    "objectID": "chpt07-asymptotic-ls.html#consistency-of-least-squares-estimator",
    "href": "chpt07-asymptotic-ls.html#consistency-of-least-squares-estimator",
    "title": "7  Asymptotic Theory for Least Squares",
    "section": "7.2 Consistency of Least Squares Estimator",
    "text": "7.2 Consistency of Least Squares Estimator\nIn this section we use the weak law of large numbers (WLLN, Theorem 6.1 and Theorem 6.2) and continuous mapping theorem (CMT, Theorem 6.6) to show that the least squares estimator \\(\\widehat{\\beta}\\) is consistent for the projection coefficient \\(\\beta\\).\nThis derivation is based on three key components. First, the OLS estimator can be written as a continuous function of a set of sample moments. Second, the WLLN shows that sample moments converge in probability to population moments. And third, the CMT states that continuous functions preserve convergence in probability. We now explain each step in brief and then in greater detail. First, observe that the OLS estimator\n\\[\n\\widehat{\\beta}=\\left(\\frac{1}{n} \\sum_{i=1}^{n} X_{i} X_{i}^{\\prime}\\right)^{-1}\\left(\\frac{1}{n} \\sum_{i=1}^{n} X_{i} Y_{i}\\right)=\\widehat{\\boldsymbol{Q}}_{X X}^{-1} \\widehat{\\boldsymbol{Q}}_{X Y}\n\\]\nis a function of the sample moments \\(\\widehat{\\boldsymbol{Q}}_{X X}=\\frac{1}{n} \\sum_{i=1}^{n} X_{i} X_{i}^{\\prime}\\) and \\(\\widehat{\\boldsymbol{Q}}_{X Y}=\\frac{1}{n} \\sum_{i=1}^{n} X_{i} Y_{i}\\).\nSecond, by an application of the WLLN these sample moments converge in probability to their population expectations. Specifically, the fact that \\(\\left(Y_{i}, X_{i}\\right)\\) are mutually i.i.d. implies that any function of \\(\\left(Y_{i}, X_{i}\\right)\\) is i.i.d., including \\(X_{i} X_{i}^{\\prime}\\) and \\(X_{i} Y_{i}\\). These variables also have finite expectations under Assumption 7.1. Under these conditions, the WLLN implies that as \\(n \\rightarrow \\infty\\),\n\\[\n\\widehat{\\boldsymbol{Q}}_{X X}=\\frac{1}{n} \\sum_{i=1}^{n} X_{i} X_{i}^{\\prime} \\underset{p}{\\longrightarrow}\\left[X X^{\\prime}\\right]=\\boldsymbol{Q}_{X X}\n\\]\nand\n\\[\n\\widehat{\\boldsymbol{Q}}_{X Y}=\\frac{1}{n} \\sum_{i=1}^{n} X_{i} Y_{i} \\underset{p}{\\longrightarrow}[X Y]=\\boldsymbol{Q}_{X Y}\n\\]\nThird, the CMT allows us to combine these equations to show that \\(\\widehat{\\beta}\\) converges in probability to \\(\\beta\\). Specifically, as \\(n \\rightarrow \\infty\\),\n\\[\n\\widehat{\\beta}=\\widehat{\\boldsymbol{Q}}_{X X}^{-1} \\widehat{\\boldsymbol{Q}}_{X Y} \\underset{p}{\\longrightarrow} \\boldsymbol{Q}_{X X}^{-1} \\boldsymbol{Q}_{X Y}=\\beta .\n\\]\nWe have shown that \\(\\widehat{\\beta} \\underset{p}{\\rightarrow} \\beta\\) as \\(n \\rightarrow \\infty\\). In words, the OLS estimator converges in probability to the projection coefficient vector \\(\\beta\\) as the sample size \\(n\\) gets large.\nTo fully understand the application of the CMT we walk through it in detail. We can write\n\\[\n\\widehat{\\beta}=g\\left(\\widehat{\\boldsymbol{Q}}_{X X}, \\widehat{\\boldsymbol{Q}}_{X Y}\\right)\n\\]\nwhere \\(g(\\boldsymbol{A}, \\boldsymbol{b})=\\boldsymbol{A}^{-1} \\boldsymbol{b}\\) is a function of \\(\\boldsymbol{A}\\) and \\(\\boldsymbol{b}\\). The function \\(\\boldsymbol{g}(\\boldsymbol{A}, \\boldsymbol{b})\\) is a continuous function of \\(\\boldsymbol{A}\\) and \\(\\boldsymbol{b}\\) at all values of the arguments such that \\(A^{-1}\\) exists. Assumption \\(7.1\\) specifies that \\(\\boldsymbol{Q}_{X X}\\) is positive definite, which means that \\(\\boldsymbol{Q}_{X X}^{-1}\\) exists. Thus \\(\\boldsymbol{g}(\\boldsymbol{A}, \\boldsymbol{b})\\) is continuous at \\(\\boldsymbol{A}=\\boldsymbol{Q}_{X X}\\). This justifies the application of the CMT in (7.2).\nFor a slightly different demonstration of (7.2) recall that (4.6) implies that\n\\[\n\\widehat{\\beta}-\\beta=\\widehat{\\boldsymbol{Q}}_{X X}^{-1} \\widehat{\\boldsymbol{Q}}_{X e}\n\\]\nwhere\n\\[\n\\widehat{\\boldsymbol{Q}}_{X e}=\\frac{1}{n} \\sum_{i=1}^{n} X_{i} e_{i} .\n\\]\nThe WLLN and (2.25) imply\n\\[\n\\widehat{\\boldsymbol{Q}}_{X e} \\underset{p}{\\longrightarrow} \\mathbb{E}[X e]=0 .\n\\]\nTherefore\n\\[\n\\widehat{\\beta}-\\beta=\\widehat{\\boldsymbol{Q}}_{X X}^{-1} \\widehat{\\boldsymbol{Q}}_{X e} \\underset{p}{\\longrightarrow} \\boldsymbol{Q}_{X X}^{-1} 0=0\n\\]\nwhich is the same as \\(\\widehat{\\beta} \\underset{p}{\\vec{p}}\\). Theorem 7.1 Consistency of Least Squares. Under Assumption 7.1, \\(\\widehat{\\boldsymbol{Q}}_{X X} \\vec{p}\\) \\(\\boldsymbol{Q}_{X X}, \\widehat{\\boldsymbol{Q}}_{X Y} \\underset{p}{\\boldsymbol{Q}_{X Y}}, \\widehat{\\boldsymbol{Q}}_{X X}^{-1} \\vec{p} \\boldsymbol{Q}_{X X}^{-1}, \\widehat{\\boldsymbol{Q}}_{X e} \\underset{p}{\\rightarrow} 0\\), and \\(\\widehat{\\beta} \\underset{p}{\\overrightarrow{3}} \\beta\\) as \\(n \\rightarrow \\infty\\)\nTheorem \\(7.1\\) states that the OLS estimator \\(\\widehat{\\beta}\\) converges in probability to \\(\\beta\\) as \\(n\\) increases and thus \\(\\widehat{\\beta}\\) is consistent for \\(\\beta\\). In the stochastic order notation, Theorem \\(7.1\\) can be equivalently written as\n\\[\n\\widehat{\\beta}=\\beta+o_{p}(1) .\n\\]\nTo illustrate the effect of sample size on the least squares estimator consider the least squares regression\n\\[\n\\log (\\text { wage })=\\beta_{1} \\text { education }+\\beta_{2} \\text { experience }+\\beta_{3} \\text { experience }^{2}+\\beta_{4}+e .\n\\]\nWe use the sample of 24,344 white men from the March 2009 CPS. We randomly sorted the observations and sequentially estimated the model by least squares starting with the first 5 observations and continuing until the full sample is used. The sequence of estimates are displayed in Figure 7.1. You can see how the least squares estimate changes with the sample size. As the number of observations increases it settles down to the full-sample estimate \\(\\widehat{\\beta}_{1}=0.114\\).\n\nFigure 7.1: The Least-Squares Estimator as a Function of Sample Size"
  },
  {
    "objectID": "chpt07-asymptotic-ls.html#asymptotic-normality",
    "href": "chpt07-asymptotic-ls.html#asymptotic-normality",
    "title": "7  Asymptotic Theory for Least Squares",
    "section": "7.3 Asymptotic Normality",
    "text": "7.3 Asymptotic Normality\nWe started this chapter discussing the need for an approximation to the distribution of the OLS estimator \\(\\widehat{\\beta}\\). In Section \\(7.2\\) we showed that \\(\\widehat{\\beta}\\) converges in probability to \\(\\beta\\). Consistency is a good first step, but in itself does not describe the distribution of the estimator. In this section we derive an approximation typically called the asymptotic distribution.\nThe derivation starts by writing the estimator as a function of sample moments. One of the moments must be written as a sum of zero-mean random vectors and normalized so that the central limit theorem can be applied. The steps are as follows.\nTake equation (7.3) and multiply it by \\(\\sqrt{n}\\). This yields the expression\n\\[\n\\sqrt{n}(\\widehat{\\beta}-\\beta)=\\left(\\frac{1}{n} \\sum_{i=1}^{n} X_{i} X_{i}^{\\prime}\\right)^{-1}\\left(\\frac{1}{\\sqrt{n}} \\sum_{i=1}^{n} X_{i} e_{i}\\right)\n\\]\nThis shows that the normalized and centered estimator \\(\\sqrt{n}(\\widehat{\\beta}-\\beta)\\) is a function of the sample average \\(n^{-1} \\sum_{i=1}^{n} X_{i} X_{i}^{\\prime}\\) and the normalized sample average \\(n^{-1 / 2} \\sum_{i=1}^{n} X_{i} e_{i}\\).\nThe random pairs \\(\\left(Y_{i}, X_{i}\\right)\\) are i.i.d., meaning that they are independent across \\(i\\) and identically distributed. Any function of \\(\\left(Y_{i}, X_{i}\\right)\\) is also i.i.d. This includes \\(e_{i}=Y_{i}-X_{i}^{\\prime} \\beta\\) and the product \\(X_{i} e_{i}\\). The latter is mean-zero \\((\\mathbb{E}[X e]=0)\\) and has \\(k \\times k\\) covariance matrix\n\\[\n\\Omega=\\mathbb{E}\\left[(X e)(X e)^{\\prime}\\right]=\\mathbb{E}\\left[X X^{\\prime} e^{2}\\right] .\n\\]\nWe show below that \\(\\Omega\\) has finite elements under a strengthening of Assumption 7.1. Since \\(X_{i} e_{i}\\) is i.i.d., mean zero, and finite variance, the central limit theorem (Theorem 6.3) implies\n\\[\n\\frac{1}{\\sqrt{n}} \\sum_{i=1}^{n} X_{i} e_{i} \\underset{d}{\\longrightarrow} \\mathrm{N}(0, \\Omega)\n\\]\nWe state the required conditions here.\nAssumption 7.2\n\nThe variables \\(\\left(Y_{i}, X_{\\boldsymbol{i}}\\right), i=1, \\ldots, n\\), are i.i.d..\n\\(\\mathbb{E}\\left[Y^{4}\\right]<\\infty\\).\n\\(\\mathbb{E}\\|X\\|^{4}<\\infty\\).\n\\(\\boldsymbol{Q}_{X X}=\\mathbb{E}\\left[X X^{\\prime}\\right]\\) is positive definite.\n\nAssumption \\(7.2\\) implies that \\(\\Omega<\\infty\\). To see this, take its \\(j \\ell^{t h}\\) element, \\(\\mathbb{E}\\left[X_{j} X_{\\ell} e^{2}\\right]\\). Theorem 2.9.6 shows that \\(\\mathbb{E}\\left[e^{4}\\right]<\\infty\\). By the expectation inequality (B.30), the \\(j \\ell^{t h}\\) element of \\(\\Omega\\) is bounded by\n\\[\n\\left|\\mathbb{E}\\left[X_{j} X_{\\ell} e^{2}\\right]\\right| \\leq \\mathbb{E}\\left|X_{j} X_{\\ell} e^{2}\\right|=\\mathbb{E}\\left[\\left|X_{j}\\right|\\left|X_{\\ell}\\right| e^{2}\\right] .\n\\]\nBy two applications of the Cauchy-Schwarz inequality (B.32), this is smaller than\n\\[\n\\left(\\mathbb{E}\\left[X_{j}^{2} X_{\\ell}^{2}\\right]\\right)^{1 / 2}\\left(\\mathbb{E}\\left[e^{4}\\right]\\right)^{1 / 2} \\leq\\left(\\mathbb{E}\\left[X_{j}^{4}\\right]\\right)^{1 / 4}\\left(\\mathbb{E}\\left[X_{\\ell}^{4}\\right]\\right)^{1 / 4}\\left(\\mathbb{E}\\left[e^{4}\\right]\\right)^{1 / 2}<\\infty\n\\]\nwhere the finiteness holds under Assumption 7.2.2 and 7.2.3. Thus \\(\\Omega<\\infty\\).\nAn alternative way to show that the elements of \\(\\Omega\\) are finite is by using a matrix norm \\(\\|\\cdot\\|\\) (See Appendix A.23). Then by the expectation inequality, the Cauchy-Schwarz inequality, Assumption 7.2.3, and \\(\\mathbb{E}\\left[e^{4}\\right]<\\infty\\),\n\\[\n\\|\\Omega\\| \\leq \\mathbb{E}\\left\\|X X^{\\prime} e^{2}\\right\\|=\\mathbb{E}\\left[\\|X\\|^{2} e^{2}\\right] \\leq\\left(\\mathbb{E}\\|X\\|^{4}\\right)^{1 / 2}\\left(\\mathbb{E}\\left[e^{4}\\right]\\right)^{1 / 2}<\\infty .\n\\]\nThis is a more compact argument (often described as more elegant) but such manipulations should not be done without understanding the notation and the applicability of each step of the argument.\nRegardless, the finiteness of the covariance matrix means that we can apply the multivariate CLT (Theorem 6.3).\nTheorem 7.2 Assumption \\(7.2\\) implies that\n\\[\n\\Omega<\\infty\n\\]\nand\n\\[\n\\frac{1}{\\sqrt{n}} \\sum_{i=1}^{n} X_{i} e_{i} \\underset{d}{\\longrightarrow} \\mathrm{N}(0, \\Omega)\n\\]\nas \\(n \\rightarrow \\infty\\)\nPutting together (7.1), (7.5), and (7.7),\n\\[\n\\sqrt{n}(\\widehat{\\beta}-\\beta) \\underset{d}{\\longrightarrow} \\boldsymbol{Q}_{X X}^{-1} \\mathrm{~N}(0, \\Omega)=\\mathrm{N}\\left(0, \\boldsymbol{Q}_{X X}^{-1} \\Omega \\boldsymbol{Q}_{X X}^{-1}\\right)\n\\]\nas \\(n \\rightarrow \\infty\\). The final equality follows from the property that linear combinations of normal vectors are also normal (Theorem 5.2).\nWe have derived the asymptotic normal approximation to the distribution of the least squares estimator.\nTheorem 7.3 Asymptotic Normality of Least Squares Estimator Under Assumption 7.2, as \\(n \\rightarrow \\infty\\)\n\\[\n\\sqrt{n}(\\widehat{\\beta}-\\beta) \\underset{d}{\\longrightarrow} \\mathrm{N}\\left(0, \\boldsymbol{V}_{\\beta}\\right)\n\\]\nwhere \\(\\boldsymbol{Q}_{X X}=\\mathbb{E}\\left[X X^{\\prime}\\right], \\Omega=\\mathbb{E}\\left[X X^{\\prime} e^{2}\\right]\\), and\n\\[\n\\boldsymbol{V}_{\\beta}=\\boldsymbol{Q}_{X X}^{-1} \\Omega \\boldsymbol{Q}_{X X}^{-1} .\n\\]\nIn the stochastic order notation, Theorem \\(7.3\\) implies that \\(\\widehat{\\beta}=\\beta+O_{p}\\left(n^{-1 / 2}\\right)\\) which is stronger than (7.4).\nThe matrix \\(\\boldsymbol{V}_{\\beta}=\\boldsymbol{Q}_{X X}^{-1} \\Omega \\boldsymbol{Q}_{X X}^{-1}\\) is the variance of the asymptotic distribution of \\(\\sqrt{n}(\\widehat{\\beta}-\\beta)\\). Consequently, \\(\\boldsymbol{V}_{\\beta}\\) is often referred to as the asymptotic covariance matrix of \\(\\widehat{\\beta}\\). The expression \\(\\boldsymbol{V}_{\\beta}=\\boldsymbol{Q}_{X X}^{-1} \\Omega \\boldsymbol{Q}_{X X}^{-1}\\) is called a sandwich form as the matrix \\(\\Omega\\) is sandwiched between two copies of \\(\\boldsymbol{Q}_{X X}^{-1}\\). It is useful to compare the variance of the asymptotic distribution given in (7.8) and the finite-sample conditional variance in the CEF model as given in (4.10):\n\\[\n\\boldsymbol{V}_{\\widehat{\\beta}}=\\operatorname{var}[\\widehat{\\beta} \\mid \\boldsymbol{X}]=\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{D} \\boldsymbol{X}\\right)\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} .\n\\]\nNotice that \\(\\boldsymbol{V}_{\\widehat{\\beta}}\\) is the exact conditional variance of \\(\\widehat{\\beta}\\) and \\(\\boldsymbol{V}_{\\beta}\\) is the asymptotic variance of \\(\\sqrt{n}(\\widehat{\\beta}-\\beta)\\). Thus \\(\\boldsymbol{V}_{\\beta}\\) should be (roughly) \\(n\\) times as large as \\(\\boldsymbol{V}_{\\widehat{\\beta}}\\), or \\(\\boldsymbol{V}_{\\beta} \\approx n \\boldsymbol{V}_{\\widehat{\\beta}}\\). Indeed, multiplying (7.9) by \\(n\\) and distributing we find\n\\[\nn \\boldsymbol{V}_{\\widehat{\\beta}}=\\left(\\frac{1}{n} \\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1}\\left(\\frac{1}{n} \\boldsymbol{X}^{\\prime} \\boldsymbol{D} \\boldsymbol{X}\\right)\\left(\\frac{1}{n} \\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1}\n\\]\nwhich looks like an estimator of \\(\\boldsymbol{V}_{\\beta}\\). Indeed, as \\(n \\rightarrow \\infty, n \\boldsymbol{V}_{\\widehat{\\beta}} \\underset{p}{\\rightarrow} \\boldsymbol{V}_{\\beta}\\). The expression \\(\\boldsymbol{V}_{\\widehat{\\beta}}\\) is useful for practical inference (such as computation of standard errors and tests) as it is the variance of the estimator \\(\\widehat{\\beta}\\), while \\(V_{\\beta}\\) is useful for asymptotic theory as it is well defined in the limit as \\(n\\) goes to infinity. We will make use of both symbols and it will be advisable to adhere to this convention.\nThere is a special case where \\(\\Omega\\) and \\(\\boldsymbol{V}_{\\beta}\\) simplify. Suppose that\n\\[\n\\operatorname{cov}\\left(X X^{\\prime}, e^{2}\\right)=0 .\n\\]\nCondition (7.10) holds in the homoskedastic linear regression model but is somewhat broader. Under (7.10) the asymptotic variance formulae simplify as\n\\[\n\\begin{aligned}\n\\Omega &=\\mathbb{E}\\left[X X^{\\prime}\\right] \\mathbb{E}\\left[e^{2}\\right]=\\boldsymbol{Q}_{X X} \\sigma^{2} \\\\\n\\boldsymbol{V}_{\\beta} &=\\boldsymbol{Q}_{X X}^{-1} \\Omega \\boldsymbol{Q}_{X X}^{-1}=\\boldsymbol{Q}_{X X}^{-1} \\sigma^{2} \\equiv \\boldsymbol{V}_{\\beta}^{0} .\n\\end{aligned}\n\\]\nIn (7.11) we define \\(\\boldsymbol{V}_{\\beta}^{0}=\\boldsymbol{Q}_{X X}^{-1} \\sigma^{2}\\) whether (7.10) is true or false. When (7.10) is true then \\(\\boldsymbol{V}_{\\beta}=\\boldsymbol{V}_{\\beta}^{0}\\), otherwise \\(\\boldsymbol{V}_{\\beta} \\neq \\boldsymbol{V}_{\\beta}^{0}\\). We call \\(\\boldsymbol{V}_{\\beta}^{0}\\) the homoskedastic asymptotic covariance matrix.\nTheorem \\(7.3\\) states that the sampling distribution of the least squares estimator, after rescaling, is approximately normal when the sample size \\(n\\) is sufficiently large. This holds true for all joint distributions of \\((Y, X)\\) which satisfy the conditions of Assumption 7.2. Consequently, asymptotic normality is routinely used to approximate the finite sample distribution of \\(\\sqrt{n}(\\widehat{\\beta}-\\beta)\\).\nA difficulty is that for any fixed \\(n\\) the sampling distribution of \\(\\widehat{\\beta}\\) can be arbitrarily far from the normal distribution. The normal approximation improves as \\(n\\) increases, but how large should \\(n\\) be in order for the approximation to be useful? Unfortunately, there is no simple answer to this reasonable question. The trouble is that no matter how large is the sample size, the normal approximation is arbitrarily poor for some data distribution satisfying the assumptions. We illustrate this problem using a simulation. Let \\(Y=\\beta_{1} X+\\beta_{2}+e\\) where \\(X\\) is \\(\\mathrm{N}(0,1)\\) and \\(e\\) is independent of \\(X\\) with the Double Pareto density \\(f(e)=\\frac{\\alpha}{2}|e|^{-\\alpha-1},|e| \\geq 1\\). If \\(\\alpha>2\\) the error \\(e\\) has zero mean and variance \\(\\alpha /(\\alpha-2)\\). As \\(\\alpha\\) approaches 2 , however, its variance diverges to infinity. In this context the normalized least squares slope estimator \\(\\sqrt{n \\frac{\\alpha-2}{\\alpha}}\\left(\\widehat{\\beta}_{1}-\\beta_{1}\\right)\\) has the \\(\\mathrm{N}(0,1)\\) asymptotic distribution for any \\(\\alpha>2\\). In Figure \\(7.2(\\) a) we display the finite sample densities of the normalized estimator \\(\\sqrt{n \\frac{\\alpha-2}{\\alpha}}\\left(\\widehat{\\beta}_{1}-\\beta_{1}\\right)\\), setting \\(n=100\\) and varying the parameter \\(\\alpha\\). For \\(\\alpha=3.0\\) the density is very close to the \\(\\mathrm{N}(0,1)\\) density. As \\(\\alpha\\) diminishes the density changes significantly, concentrating most of the probability mass around zero.\nAnother example is shown in Figure 7.2(b). Here the model is \\(Y=\\beta+e\\) where\n\\[\ne=\\frac{u^{r}-\\mathbb{E}\\left[u^{r}\\right]}{\\left(\\mathbb{E}\\left[u^{2 r}\\right]-\\left(\\mathbb{E}\\left[u^{r}\\right]\\right)^{2}\\right)^{1 / 2}}\n\\]\nand \\(u \\sim \\mathrm{N}(0,1)\\). We show the sampling distribution of \\(\\sqrt{n}(\\widehat{\\beta}-\\beta)\\) for \\(n=100\\), varying \\(r=1,4,6\\) and 8 . As \\(r\\) increases, the sampling distribution becomes highly skewed and non-normal. The lesson from Figure \\(7.2\\) is that the \\(\\mathrm{N}(0,1)\\) asymptotic approximation is never guaranteed to be accurate.\n\n\nDouble Pareto Error\n\n\n\nError Process (7.12)\n\nFigure 7.2: Density of Normalized OLS Estimator"
  },
  {
    "objectID": "chpt07-asymptotic-ls.html#joint-distribution",
    "href": "chpt07-asymptotic-ls.html#joint-distribution",
    "title": "7  Asymptotic Theory for Least Squares",
    "section": "7.4 Joint Distribution",
    "text": "7.4 Joint Distribution\nTheorem \\(7.3\\) gives the joint asymptotic distribution of the coefficient estimators. We can use the result to study the covariance between the coefficient estimators. For simplicity, take the case of two regressors, no intercept, and homoskedastic error. Assume the regressors are mean zero, variance one, with correlation \\(\\rho\\). Then using the formula for inversion of a \\(2 \\times 2\\) matrix,\n\\[\n\\boldsymbol{V}_{\\beta}^{0}=\\sigma^{2} \\boldsymbol{Q}_{X X}^{-1}=\\frac{\\sigma^{2}}{1-\\rho^{2}}\\left[\\begin{array}{cc}\n1 & -\\rho \\\\\n-\\rho & 1\n\\end{array}\\right] .\n\\]\nThus if \\(X_{1}\\) and \\(X_{2}\\) are positively correlated \\((\\rho>0)\\) then \\(\\widehat{\\beta}_{1}\\) and \\(\\widehat{\\beta}_{2}\\) are negatively correlated (and viceversa).\nFor illustration, Figure 7.3(a) displays the probability contours of the joint asymptotic distribution of \\(\\widehat{\\beta}_{1}-\\beta_{1}\\) and \\(\\widehat{\\beta}_{2}-\\beta_{2}\\) when \\(\\beta_{1}=\\beta_{2}=0\\) and \\(\\rho=0.5\\). The coefficient estimators are negatively correlated because the regressors are positively correlated. This means that if \\(\\widehat{\\beta}_{1}\\) is unusually negative, it is likely that \\(\\widehat{\\beta}_{2}\\) is unusually positive, or conversely. It is also unlikely that we will observe both \\(\\widehat{\\beta}_{1}\\) and \\(\\widehat{\\beta}_{2}\\) unusually large and of the same sign.\nThis finding that the correlation of the regressors is of opposite sign of the correlation of the coefficient estimates is sensitive to the assumption of homoskedasticity. If the errors are heteroskedastic then this relationship is not guaranteed.\nThis can be seen through a simple constructed example. Suppose that \\(X_{1}\\) and \\(X_{2}\\) only take the values \\(\\{-1,+1\\}\\), symmetrically, with \\(\\mathbb{P}\\left[X_{1}=X_{2}=1\\right]=\\mathbb{P}\\left[X_{1}=X_{2}=-1\\right]=3 / 8\\), and \\(\\mathbb{P}\\left[X_{1}=1, X_{2}=-1\\right]=\\) \\(\\mathbb{P}\\left[X_{1}=-1, X_{2}=1\\right]=1 / 8\\). You can check that the regressors are mean zero, unit variance and correlation \\(0.5\\), which is identical with the setting displayed in Figure 7.3(a).\nNow suppose that the error is heteroskedastic. Specifically, suppose that \\(\\mathbb{E}\\left[e^{2} \\mid X_{1}=X_{2}\\right]=5 / 4\\) and \\(\\mathbb{E}\\left[e^{2} \\mid X_{1} \\neq X_{2}\\right]=1 / 4\\). You can check that \\(\\mathbb{E}\\left[e^{2}\\right]=1\\), \\(\\mathbb{E}\\left[X_{1}^{2} e^{2}\\right]=\\mathbb{E}\\left[X_{2}^{2} e^{2}\\right]=1\\) and \\(\\mathbb{E}\\left[X_{1} X_{2} e_{i}^{2}\\right]=7 / 8\\). There-\n\n\nHomoskedastic Case\n\n\n\nHeteroskedastic Case\n\nFigure 7.3: Contours of Joint Distribution of \\(\\widehat{\\beta}_{1}\\) and \\(\\widehat{\\beta}_{2}\\)\nfore\n\\[\n\\begin{aligned}\n\\boldsymbol{V}_{\\beta} &=\\boldsymbol{Q}_{X X}^{-1} \\Omega \\boldsymbol{Q}_{X X}^{-1} \\\\\n&=\\frac{9}{16}\\left[\\begin{array}{cc}\n1 & -\\frac{1}{2} \\\\\n-\\frac{1}{2} & 1\n\\end{array}\\right]\\left[\\begin{array}{cc}\n1 & \\frac{7}{8} \\\\\n\\frac{7}{8} & 1\n\\end{array}\\right]\\left[\\begin{array}{cc}\n1 & -\\frac{1}{2} \\\\\n-\\frac{1}{2} & 1\n\\end{array}\\right] \\\\\n&=\\frac{4}{3}\\left[\\begin{array}{cc}\n1 & \\frac{1}{4} \\\\\n\\frac{1}{4} & 1\n\\end{array}\\right]\n\\end{aligned}\n\\]\nThus the coefficient estimators \\(\\widehat{\\beta}_{1}\\) and \\(\\widehat{\\beta}_{2}\\) are positively correlated (their correlation is \\(1 / 4\\).) The joint probability contours of their asymptotic distribution is displayed in Figure 7.3(b). We can see how the two estimators are positively associated.\nWhat we found through this example is that in the presence of heteroskedasticity there is no simple relationship between the correlation of the regressors and the correlation of the parameter estimators.\nWe can extend the above analysis to study the covariance between coefficient sub-vectors. For example, partitioning \\(X^{\\prime}=\\left(X_{1}^{\\prime}, X_{2}^{\\prime}\\right)\\) and \\(\\beta^{\\prime}=\\left(\\beta_{1}^{\\prime}, \\beta_{2}^{\\prime}\\right)\\), we can write the general model as\n\\[\nY=X_{1}^{\\prime} \\beta_{1}+X_{2}^{\\prime} \\beta_{2}+e\n\\]\nand the coefficient estimates as \\(\\widehat{\\beta}^{\\prime}=\\left(\\widehat{\\beta}_{1}^{\\prime}, \\widehat{\\beta}_{2}^{\\prime}\\right)\\). Make the partitions\n\\[\n\\boldsymbol{Q}_{X X}=\\left[\\begin{array}{ll}\n\\boldsymbol{Q}_{11} & \\boldsymbol{Q}_{12} \\\\\n\\boldsymbol{Q}_{21} & \\boldsymbol{Q}_{22}\n\\end{array}\\right], \\quad \\Omega=\\left[\\begin{array}{ll}\n\\Omega_{11} & \\Omega_{12} \\\\\n\\Omega_{21} & \\Omega_{22}\n\\end{array}\\right] .\n\\]\nFrom (2.43)\n\\[\n\\boldsymbol{Q}_{X X}^{-1}=\\left[\\begin{array}{cc}\n\\boldsymbol{Q}_{11 \\cdot 2}^{-1} & -\\boldsymbol{Q}_{11 \\cdot 2}^{-1} \\boldsymbol{Q}_{12} \\boldsymbol{Q}_{22}^{-1} \\\\\n-\\boldsymbol{Q}_{22 \\cdot 1}^{-1} \\boldsymbol{Q}_{21} \\boldsymbol{Q}_{11}^{-1} & \\boldsymbol{Q}_{22 \\cdot 1}^{-1}\n\\end{array}\\right]\n\\]\nwhere \\(\\boldsymbol{Q}_{11 \\cdot 2}=\\boldsymbol{Q}_{11}-\\boldsymbol{Q}_{12} \\boldsymbol{Q}_{22}^{-1} \\boldsymbol{Q}_{21}\\) and \\(\\boldsymbol{Q}_{22 \\cdot 1}=\\boldsymbol{Q}_{22}-\\boldsymbol{Q}_{21} \\boldsymbol{Q}_{11}^{-1} \\boldsymbol{Q}_{12}\\). Thus when the error is homoskedastic\n\\[\n\\operatorname{cov}\\left(\\widehat{\\beta}_{1}, \\widehat{\\beta}_{2}\\right)=-\\sigma^{2} \\boldsymbol{Q}_{11 \\cdot 2}^{-1} \\boldsymbol{Q}_{12} \\boldsymbol{Q}_{22}^{-1}\n\\]\nwhich is a matrix generalization of the two-regressor case.\nIn general you can show that (Exercise 7.5)\n\\[\n\\boldsymbol{V}_{\\boldsymbol{\\beta}}=\\left[\\begin{array}{ll}\n\\boldsymbol{V}_{11} & \\boldsymbol{V}_{12} \\\\\n\\boldsymbol{V}_{21} & \\boldsymbol{V}_{22}\n\\end{array}\\right]\n\\]\nwhere\n\\[\n\\begin{aligned}\n\\boldsymbol{V}_{11} &=\\boldsymbol{Q}_{11 \\cdot 2}^{-1}\\left(\\Omega_{11}-\\boldsymbol{Q}_{12} \\boldsymbol{Q}_{22}^{-1} \\Omega_{21}-\\Omega_{12} \\boldsymbol{Q}_{22}^{-1} \\boldsymbol{Q}_{21}+\\boldsymbol{Q}_{12} \\boldsymbol{Q}_{22}^{-1} \\Omega_{22} \\boldsymbol{Q}_{22}^{-1} \\boldsymbol{Q}_{21}\\right) \\boldsymbol{Q}_{11 \\cdot 2}^{-1} \\\\\n\\boldsymbol{V}_{21} &=\\boldsymbol{Q}_{22 \\cdot 1}^{-1}\\left(\\Omega_{21}-\\boldsymbol{Q}_{21} \\boldsymbol{Q}_{11}^{-1} \\Omega_{11}-\\Omega_{22} \\boldsymbol{Q}_{22}^{-1} \\boldsymbol{Q}_{21}+\\boldsymbol{Q}_{21} \\boldsymbol{Q}_{11}^{-1} \\Omega_{12} \\boldsymbol{Q}_{22}^{-1} \\boldsymbol{Q}_{21}\\right) \\boldsymbol{Q}_{11 \\cdot 2}^{-1} \\\\\n\\boldsymbol{V}_{22} &=\\boldsymbol{Q}_{22 \\cdot 1}^{-1}\\left(\\Omega_{22}-\\boldsymbol{Q}_{21} \\boldsymbol{Q}_{11}^{-1} \\Omega_{12}-\\Omega_{21} \\boldsymbol{Q}_{11}^{-1} \\boldsymbol{Q}_{12}+\\boldsymbol{Q}_{21} \\boldsymbol{Q}_{11}^{-1} \\Omega_{11} \\boldsymbol{Q}_{11}^{-1} \\boldsymbol{Q}_{12}\\right) \\boldsymbol{Q}_{22 \\cdot 1}^{-1}\n\\end{aligned}\n\\]\nUnfortunately, these expressions are not easily interpretable."
  },
  {
    "objectID": "chpt07-asymptotic-ls.html#consistency-of-error-variance-estimators",
    "href": "chpt07-asymptotic-ls.html#consistency-of-error-variance-estimators",
    "title": "7  Asymptotic Theory for Least Squares",
    "section": "7.5 Consistency of Error Variance Estimators",
    "text": "7.5 Consistency of Error Variance Estimators\nUsing the methods of Section \\(7.2\\) we can show that the estimators \\(\\widehat{\\sigma}^{2}=n^{-1} \\sum_{i=1}^{n} \\widehat{e}_{i}^{2}\\) and \\(s^{2}=(n-k)^{-1} \\sum_{i=1}^{n} \\widehat{e}_{i}^{2}\\) are consistent for \\(\\sigma^{2}\\).\nThe trick is to write the residual \\(\\widehat{e}_{i}\\) as equal to the error \\(e_{i}\\) plus a deviation\n\\[\n\\widehat{e}_{i}=Y_{i}-X_{i}^{\\prime} \\widehat{\\beta}=e_{i}-X_{i}^{\\prime}(\\widehat{\\beta}-\\beta) .\n\\]\nThus the squared residual equals the squared error plus a deviation\n\\[\n\\widehat{e}_{i}^{2}=e_{i}^{2}-2 e_{i} X_{i}^{\\prime}(\\widehat{\\beta}-\\beta)+(\\widehat{\\beta}-\\beta)^{\\prime} X_{i} X_{i}^{\\prime}(\\widehat{\\beta}-\\beta) .\n\\]\nSo when we take the average of the squared residuals we obtain the average of the squared errors, plus two terms which are (hopefully) asymptotically negligible. This average is:\n\\[\n\\widehat{\\sigma}^{2}=\\frac{1}{n} \\sum_{i=1}^{n} e_{i}^{2}-2\\left(\\frac{1}{n} \\sum_{i=1}^{n} e_{i} X_{i}^{\\prime}\\right)(\\widehat{\\beta}-\\beta)+(\\widehat{\\beta}-\\beta)^{\\prime}\\left(\\frac{1}{n} \\sum_{i=1}^{n} X_{i} X_{i}^{\\prime}\\right)(\\widehat{\\beta}-\\beta) .\n\\]\nThe WLLN implies that\n\\[\n\\begin{aligned}\n&\\frac{1}{n} \\sum_{i=1}^{n} e_{i}^{2} \\underset{p}{\\longrightarrow} \\sigma^{2} \\\\\n&\\frac{1}{n} \\sum_{i=1}^{n} e_{i} X_{i}^{\\prime} \\underset{p}{\\longrightarrow} \\mathbb{E}\\left[e X^{\\prime}\\right]=0 \\\\\n&\\frac{1}{n} \\sum_{i=1}^{n} X_{i} X_{i}^{\\prime} \\underset{p}{\\longrightarrow}\\left[X X^{\\prime}\\right]=\\boldsymbol{Q}_{X X}\n\\end{aligned}\n\\]\nTheorem \\(7.1\\) shows that \\(\\widehat{\\beta} \\underset{p}{\\rightarrow} \\beta\\). Hence (7.18) converges in probability to \\(\\sigma^{2}\\) as desired.\nFinally, since \\(n /(n-k) \\rightarrow 1\\) as \\(n \\rightarrow \\infty\\) it follows that \\(s^{2}=\\left(\\frac{n}{n-k}\\right) \\widehat{\\sigma}^{2} \\underset{p}{\\rightarrow} \\sigma^{2}\\). Thus both estimators are consistent. Theorem 7.4 Under Assumption 7.1, \\(\\widehat{\\sigma}^{2} \\underset{p}{\\longrightarrow} \\sigma^{2}\\) and \\(s^{2} \\underset{p}{\\rightarrow} \\sigma^{2}\\) as \\(n \\rightarrow \\infty\\)."
  },
  {
    "objectID": "chpt07-asymptotic-ls.html#homoskedastic-covariance-matrix-estimation",
    "href": "chpt07-asymptotic-ls.html#homoskedastic-covariance-matrix-estimation",
    "title": "7  Asymptotic Theory for Least Squares",
    "section": "7.6 Homoskedastic Covariance Matrix Estimation",
    "text": "7.6 Homoskedastic Covariance Matrix Estimation\nTheorem \\(7.3\\) shows that \\(\\sqrt{n}(\\widehat{\\beta}-\\beta)\\) is asymptotically normal with asymptotic covariance matrix \\(\\boldsymbol{V}_{\\beta}\\). For asymptotic inference (confidence intervals and tests) we need a consistent estimator of \\(\\boldsymbol{V}_{\\beta}\\). Under homoskedasticity \\(\\boldsymbol{V}_{\\beta}\\) simplifies to \\(\\boldsymbol{V}_{\\beta}^{0}=\\boldsymbol{Q}_{X X}^{-1} \\sigma^{2}\\) and in this section we consider the simplified problem of estimating \\(V_{\\beta}^{0}\\).\nThe standard moment estimator of \\(\\boldsymbol{Q}_{X X}\\) is \\(\\widehat{\\boldsymbol{Q}}_{X X}\\) defined in (7.1) and thus an estimator for \\(\\boldsymbol{Q}_{X X}^{-1}\\) is \\(\\widehat{\\boldsymbol{Q}}_{X X}^{-1}\\). The standard estimator of \\(\\sigma^{2}\\) is the unbiased estimator \\(s^{2}\\) defined in (4.31). Thus a natural plug-in estimator for \\(\\boldsymbol{V}_{\\beta}^{0}=\\boldsymbol{Q}_{X X}^{-1} \\sigma^{2}\\) is \\(\\widehat{\\boldsymbol{V}}_{\\beta}^{0}=\\widehat{\\boldsymbol{Q}}_{X X}^{-1} s^{2}\\).\nConsistency of \\(\\widehat{\\boldsymbol{V}}_{\\beta}^{0}\\) for \\(\\boldsymbol{V}_{\\beta}^{0}\\) follows from consistency of the moment estimators \\(\\widehat{\\boldsymbol{Q}}_{X X}\\) and \\(s^{2}\\) and an application of the continuous mapping theorem. Specifically, Theorem \\(7.1\\) established \\(\\widehat{\\boldsymbol{Q}}_{X X} \\underset{p}{\\rightarrow} \\boldsymbol{Q}_{X X}\\), and Theorem \\(7.4\\) established \\(s^{2} \\underset{p}{\\rightarrow} \\sigma^{2}\\). The function \\(\\boldsymbol{V}_{\\beta}^{0}=\\boldsymbol{Q}_{X X}^{-1} \\sigma^{2}\\) is a continuous function of \\(\\boldsymbol{Q}_{X X}\\) and \\(\\sigma^{2}\\) so long as \\(\\boldsymbol{Q}_{X X}>0\\), which holds true under Assumption 7.1.4. It follows by the CMT that\n\\[\n\\widehat{\\boldsymbol{V}}_{\\beta}^{0}=\\widehat{\\boldsymbol{Q}}_{X X}^{-1} s^{2} \\underset{p}{\\longrightarrow} \\boldsymbol{Q}_{X X}^{-1} \\sigma^{2}=\\boldsymbol{V}_{\\beta}^{0}\n\\]\nso that \\(\\widehat{\\boldsymbol{V}}_{\\beta}^{0}\\) is consistent for \\(\\boldsymbol{V}_{\\beta}^{0}\\).\nTheorem 7.5 Under Assumption 7.1, \\(\\widehat{\\boldsymbol{V}}_{\\beta}^{0} \\underset{p}{\\rightarrow} \\boldsymbol{V}_{\\beta}^{0}\\) as \\(n \\rightarrow \\infty\\)\nIt is instructive to notice that Theorem \\(7.5\\) does not require the assumption of homoskedasticity. That is, \\(\\widehat{V}_{\\beta}^{0}\\) is consistent for \\(\\boldsymbol{V}_{\\beta}^{0}\\) regardless if the regression is homoskedastic or heteroskedastic. However, \\(\\boldsymbol{V}_{\\beta}^{0}=\\boldsymbol{V}_{\\beta}=\\operatorname{avar}[\\widehat{\\beta}]\\) only under homoskedasticity. Thus, in the general case \\(\\widehat{\\boldsymbol{V}}_{\\beta}^{0}\\) is consistent for a welldefined but non-useful object."
  },
  {
    "objectID": "chpt07-asymptotic-ls.html#heteroskedastic-covariance-matrix-estimation",
    "href": "chpt07-asymptotic-ls.html#heteroskedastic-covariance-matrix-estimation",
    "title": "7  Asymptotic Theory for Least Squares",
    "section": "7.7 Heteroskedastic Covariance Matrix Estimation",
    "text": "7.7 Heteroskedastic Covariance Matrix Estimation\nTheorems \\(7.3\\) established that the asymptotic covariance matrix of \\(\\sqrt{n}(\\widehat{\\beta}-\\beta)\\) is \\(\\boldsymbol{V}_{\\beta}=\\boldsymbol{Q}_{X X}^{-1} \\Omega \\boldsymbol{Q}_{X X}^{-1}\\). We now consider estimation of this covariance matrix without imposing homoskedasticity. The standard approach is to use a plug-in estimator which replaces the unknowns with sample moments.\nAs described in the previous section a natural estimator for \\(\\boldsymbol{Q}_{X X}^{-1}\\) is \\(\\widehat{\\boldsymbol{Q}}_{X X}^{-1}\\) where \\(\\widehat{\\boldsymbol{Q}}_{X X}\\) defined in (7.1). The moment estimator for \\(\\Omega\\) is\n\\[\n\\widehat{\\Omega}=\\frac{1}{n} \\sum_{i=1}^{n} X_{i} X_{i}^{\\prime} \\widehat{e}_{i}^{2},\n\\]\nleading to the plug-in covariance matrix estimator\n\\[\n\\widehat{\\boldsymbol{V}}_{\\beta}^{\\mathrm{HC} 0}=\\widehat{\\boldsymbol{Q}}_{X X}^{-1} \\widehat{\\Omega} \\widehat{\\boldsymbol{Q}}_{X X}^{-1} .\n\\]\nYou can check that \\(\\widehat{\\boldsymbol{V}}_{\\beta}^{\\mathrm{HC} 0}=n \\widehat{\\boldsymbol{V}}_{\\widehat{\\beta}}^{\\mathrm{HC} 0}\\) where \\(\\widehat{\\boldsymbol{V}}_{\\widehat{\\beta}}^{\\mathrm{HC} 0}\\) is the HC0 covariance matrix estimator from (4.36).\nAs shown in Theorem 7.1, \\(\\widehat{\\boldsymbol{Q}}_{X X}^{-1} \\underset{p}{\\rightarrow} \\boldsymbol{Q}_{X X}^{-1}\\), so we just need to verify the consistency of \\(\\widehat{\\Omega}\\). The key is to replace the squared residual \\(\\widehat{e}_{i}^{2}\\) with the squared error \\(e_{i}^{2}\\), and then show that the difference is asymptotically negligible.\nSpecifically, observe that\n\\[\n\\begin{aligned}\n\\widehat{\\Omega} &=\\frac{1}{n} \\sum_{i=1}^{n} X_{i} X_{i}^{\\prime} \\widehat{e}_{i}^{2} \\\\\n&=\\frac{1}{n} \\sum_{i=1}^{n} X_{i} X_{i}^{\\prime} e_{i}^{2}+\\frac{1}{n} \\sum_{i=1}^{n} X_{i} X_{i}^{\\prime}\\left(\\widehat{e}_{i}^{2}-e_{i}^{2}\\right) .\n\\end{aligned}\n\\]\nThe first term is an average of the i.i.d. random variables \\(X_{i} X_{i}^{\\prime} e_{i}^{2}\\), and therefore by the WLLN converges in probability to its expectation, namely,\n\\[\n\\frac{1}{n} \\sum_{i=1}^{n} X_{i} X_{i}^{\\prime} e_{i}^{2} \\underset{p}{\\longrightarrow}\\left[X X^{\\prime} e^{2}\\right]=\\Omega .\n\\]\nTechnically, this requires that \\(\\Omega\\) has finite elements, which was shown in (7.6).\nTo establish that \\(\\widehat{\\Omega}\\) is consistent for \\(\\Omega\\) it remains to show that\n\\[\n\\frac{1}{n} \\sum_{i=1}^{n} X_{i} X_{i}^{\\prime}\\left(\\widehat{e}_{i}^{2}-e_{i}^{2}\\right) \\underset{p}{\\longrightarrow} 0\n\\]\nThere are multiple ways to do this. A reasonably straightforward yet slightly tedious derivation is to start by applying the triangle inequality (B.16) using a matrix norm:\n\\[\n\\begin{aligned}\n\\left\\|\\frac{1}{n} \\sum_{i=1}^{n} X_{i} X_{i}^{\\prime}\\left(\\widehat{e}_{i}^{2}-e_{i}^{2}\\right)\\right\\| & \\leq \\frac{1}{n} \\sum_{i=1}^{n}\\left\\|X_{i} X_{i}^{\\prime}\\left(\\widehat{e}_{i}^{2}-e_{i}^{2}\\right)\\right\\| \\\\\n&=\\frac{1}{n} \\sum_{i=1}^{n}\\left\\|X_{i}\\right\\|^{2}\\left|\\widehat{e}_{i}^{2}-e_{i}^{2}\\right| .\n\\end{aligned}\n\\]\nThen recalling the expression for the squared residual (7.17), apply the triangle inequality (B.1) and then the Schwarz inequality (B.12) twice\n\\[\n\\begin{aligned}\n\\left|\\widehat{e}_{i}^{2}-e_{i}^{2}\\right| & \\leq 2\\left|e_{i} X_{i}^{\\prime}(\\widehat{\\beta}-\\beta)\\right|+(\\widehat{\\beta}-\\beta)^{\\prime} X_{i} X_{i}^{\\prime}(\\widehat{\\beta}-\\beta) \\\\\n&=2\\left|e_{i}\\right|\\left|X_{i}^{\\prime}(\\widehat{\\beta}-\\beta)\\right|+\\left|(\\widehat{\\beta}-\\beta)^{\\prime} X_{i}\\right|^{2} \\\\\n& \\leq 2\\left|e_{i}\\right|\\left\\|X_{i}\\right\\|\\|\\widehat{\\beta}-\\beta\\|+\\left\\|X_{i}\\right\\|^{2}\\|\\widehat{\\beta}-\\beta\\|^{2}\n\\end{aligned}\n\\]\nCombining (7.21) and (7.22), we find\n\\[\n\\begin{aligned}\n\\left\\|\\frac{1}{n} \\sum_{i=1}^{n} X_{i} X_{i}^{\\prime}\\left(\\widehat{e}_{i}^{2}-e_{i}^{2}\\right)\\right\\| & \\leq 2\\left(\\frac{1}{n} \\sum_{i=1}^{n}\\left\\|X_{i}\\right\\|^{3}\\left|e_{i}\\right|\\right)\\|\\widehat{\\beta}-\\beta\\|+\\left(\\frac{1}{n} \\sum_{i=1}^{n}\\left\\|X_{i}\\right\\|^{4}\\right)\\|\\widehat{\\beta}-\\beta\\|^{2} \\\\\n&=o_{p}(1) .\n\\end{aligned}\n\\]\nThe expression is \\(o_{p}(1)\\) because \\(\\|\\widehat{\\beta}-\\beta\\| \\underset{p}{\\longrightarrow} 0\\) and both averages in parenthesis are averages of random variables with finite expectation under Assumption \\(7.2\\) (and are thus \\(O_{p}(1)\\) ). Indeed, by Hölder’s inequality (B.31)\n\\[\n\\mathbb{E}\\left[\\|X\\|^{3}|e|\\right] \\leq\\left(\\mathbb{E}\\left[\\left(\\|X\\|^{3}\\right)^{4 / 3}\\right]\\right)^{3 / 4}\\left(\\mathbb{E}\\left[e^{4}\\right]\\right)^{1 / 4}=\\left(\\mathbb{E}\\|X\\|^{4}\\right)^{3 / 4}\\left(\\mathbb{E}\\left[e^{4}\\right]\\right)^{1 / 4}<\\infty .\n\\]\nWe have established (7.20) as desired. Theorem 7.6 Under Assumption 7.2, as \\(n \\rightarrow \\infty, \\widehat{\\Omega} \\underset{p}{\\longrightarrow} \\Omega\\) and \\(\\widehat{\\boldsymbol{V}}_{\\beta}^{\\mathrm{HC} 0} \\underset{p}{\\longrightarrow} \\boldsymbol{V}_{\\beta}\\)\nFor an alternative proof of this result, see Section 7.20."
  },
  {
    "objectID": "chpt07-asymptotic-ls.html#summary-of-covariance-matrix-notation",
    "href": "chpt07-asymptotic-ls.html#summary-of-covariance-matrix-notation",
    "title": "7  Asymptotic Theory for Least Squares",
    "section": "7.8 Summary of Covariance Matrix Notation",
    "text": "7.8 Summary of Covariance Matrix Notation\nThe notation we have introduced may be somewhat confusing so it is helpful to write it down in one place.\nThe exact variance of \\(\\widehat{\\beta}\\) (under the assumptions of the linear regression model) and the asymptotic variance of \\(\\sqrt{n}(\\widehat{\\beta}-\\beta)\\) (under the more general assumptions of the linear projection model) are\n\\[\n\\begin{aligned}\n&\\boldsymbol{V}_{\\widehat{\\beta}}=\\operatorname{var}[\\widehat{\\beta} \\mid \\boldsymbol{X}]=\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{D} \\boldsymbol{X}\\right)\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\\\\n&\\boldsymbol{V}_{\\beta}=\\operatorname{avar}[\\sqrt{n}(\\widehat{\\beta}-\\beta)]=\\boldsymbol{Q}_{X X}^{-1} \\Omega \\boldsymbol{Q}_{X X}^{-1}\n\\end{aligned}\n\\]\nThe HC0 estimators of these two covariance matrices are\n\\[\n\\begin{aligned}\n\\widehat{\\boldsymbol{V}}_{\\widehat{\\beta}}^{\\mathrm{HC} 0} &=\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1}\\left(\\sum_{i=1}^{n} X_{i} X_{i}^{\\prime} \\widehat{e}_{i}^{2}\\right)\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\\\\n\\widehat{\\boldsymbol{V}}_{\\beta}^{\\mathrm{HC} 0} &=\\widehat{\\boldsymbol{Q}}_{X X}^{-1} \\widehat{\\Omega} \\widehat{\\boldsymbol{Q}}_{X X}^{-1}\n\\end{aligned}\n\\]\nand satisfy the simple relationship \\(\\widehat{\\boldsymbol{V}}_{\\beta}^{\\mathrm{HC} 0}=n \\widehat{\\boldsymbol{V}}_{\\widehat{\\beta}}^{\\mathrm{HC}}\\).\nSimilarly, under the assumption of homoskedasticity the exact and asymptotic variances simplify to\n\\[\n\\begin{aligned}\n\\boldsymbol{V}_{\\widehat{\\beta}}^{0} &=\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\sigma^{2} \\\\\n\\boldsymbol{V}_{\\beta}^{0} &=\\boldsymbol{Q}_{X X}^{-1} \\sigma^{2} .\n\\end{aligned}\n\\]\nTheir standard estimators are\n\\[\n\\begin{aligned}\n\\widehat{\\boldsymbol{V}}_{\\widehat{\\beta}}^{0} &=\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} s^{2} \\\\\n\\widehat{\\boldsymbol{V}}_{\\beta}^{0} &=\\widehat{\\boldsymbol{Q}}_{X X}^{-1} s^{2}\n\\end{aligned}\n\\]\nwhich also satisfy the relationship \\(\\widehat{\\boldsymbol{V}}_{\\beta}^{0}=n \\widehat{\\boldsymbol{V}}_{\\widehat{\\beta}}^{0}\\).\nThe exact formula and estimators are useful when constructing test statistics and standard errors. However, for theoretical purposes the asymptotic formula (variances and their estimates) are more useful as these retain non-generate limits as the sample sizes diverge. That is why both sets of notation are useful."
  },
  {
    "objectID": "chpt07-asymptotic-ls.html#alternative-covariance-matrix-estimators",
    "href": "chpt07-asymptotic-ls.html#alternative-covariance-matrix-estimators",
    "title": "7  Asymptotic Theory for Least Squares",
    "section": "7.9 Alternative Covariance Matrix Estimators*",
    "text": "7.9 Alternative Covariance Matrix Estimators*\nIn Section \\(7.7\\) we introduced \\(\\widehat{\\boldsymbol{V}}_{\\beta}^{\\mathrm{HC} 0}\\) as an estimator of \\(\\boldsymbol{V}_{\\beta} \\cdot \\widehat{\\boldsymbol{V}}_{\\beta}^{\\mathrm{HC} 0}\\) is a scaled version of \\(\\widehat{\\boldsymbol{V}}_{\\widehat{\\beta}}^{\\mathrm{HC} 0}\\) from Section 4.14, where we also introduced the alternative HC1, HC2, and HC3 heteroskedasticity-robust covariance matrix estimators. We now discuss the consistency properties of these estimators.\nTo do so we introduce their scaled versions, e.g. \\(\\widehat{\\boldsymbol{V}}_{\\beta}^{\\mathrm{HC} 1}=n \\widehat{\\boldsymbol{V}}_{\\widehat{\\beta}}^{\\mathrm{HC} 1}, \\widehat{\\boldsymbol{V}}_{\\beta}^{\\mathrm{HC} 2}=n \\widehat{\\boldsymbol{V}}_{\\widehat{\\beta}}^{\\mathrm{HC} 2}\\), and \\(\\widehat{\\boldsymbol{V}}_{\\beta}^{\\mathrm{HC} 3}=n \\widehat{\\boldsymbol{V}}_{\\widehat{\\beta}}^{\\mathrm{HC} 3}\\). These are (alternative) estimators of the asymptotic covariance matrix \\(\\boldsymbol{V}_{\\beta}\\). First, consider \\(\\widehat{\\boldsymbol{V}}_{\\beta}^{\\mathrm{HC} 1}\\). Notice that \\(\\widehat{\\boldsymbol{V}}_{\\beta}^{\\mathrm{HC} 1}=n \\widehat{\\boldsymbol{V}}_{\\widehat{\\beta}}^{\\mathrm{HC1}}=\\frac{n}{n-k} \\widehat{\\boldsymbol{V}}_{\\beta}^{\\mathrm{HC} 0}\\) where \\(\\widehat{\\boldsymbol{V}}_{\\beta}^{\\mathrm{HC}}\\) was defined in (7.19) and shown consistent for \\(\\boldsymbol{V}_{\\beta}\\) in Theorem 7.6. If \\(k\\) is fixed as \\(n \\rightarrow \\infty\\), then \\(\\frac{n}{n-k} \\rightarrow 1\\) and thus\n\\[\n\\widehat{\\boldsymbol{V}}_{\\beta}^{\\mathrm{HC} 1}=(1+o(1)) \\widehat{\\boldsymbol{V}}_{\\beta}^{\\mathrm{HC} 0} \\underset{p}{\\longrightarrow} \\boldsymbol{V}_{\\beta} .\n\\]\nThus \\(\\widehat{\\boldsymbol{V}}_{\\beta}^{\\mathrm{HC} 1}\\) is consistent for \\(\\boldsymbol{V}_{\\beta}\\).\nThe alternative estimators \\(\\widehat{\\boldsymbol{V}}_{\\beta}^{\\mathrm{HC} 2}\\) and \\(\\widehat{\\boldsymbol{V}}_{\\beta}^{\\mathrm{HC} 3}\\) take the form (7.19) but with \\(\\widehat{\\Omega}\\) replaced by\n\\[\n\\widetilde{\\Omega}=\\frac{1}{n} \\sum_{i=1}^{n}\\left(1-h_{i i}\\right)^{-2} X_{i} X_{i}^{\\prime} \\widehat{e}_{i}^{2}\n\\]\nand\n\\[\n\\bar{\\Omega}=\\frac{1}{n} \\sum_{i=1}^{n}\\left(1-h_{i i}\\right)^{-1} X_{i} X_{i}^{\\prime} \\hat{e}_{i}^{2},\n\\]\nrespectively. To show that these estimators also consistent for \\(\\boldsymbol{V}_{\\beta}\\) given \\(\\widehat{\\Omega} \\underset{p}{\\vec{a}} \\Omega\\) it is sufficient to show that the differences \\(\\widetilde{\\Omega}-\\widehat{\\Omega}\\) and \\(\\bar{\\Omega}-\\widehat{\\Omega}\\) converge in probability to zero as \\(n \\rightarrow \\infty\\).\nThe trick is the fact that the leverage values are asymptotically negligible:\n\\[\nh_{n}^{*}=\\max _{1 \\leq i \\leq n} h_{i i}=o_{p}(1) .\n\\]\n(See Theorem \\(7.17\\) in Section 7.21.) Then using the triangle inequality (B.16)\n\\[\n\\begin{aligned}\n\\|\\bar{\\Omega}-\\widehat{\\Omega}\\| & \\leq \\frac{1}{n} \\sum_{i=1}^{n}\\left\\|X_{i} X_{i}^{\\prime}\\right\\| \\widehat{e}_{i}^{2}\\left|\\left(1-h_{i i}\\right)^{-1}-1\\right| \\\\\n& \\leq\\left(\\frac{1}{n} \\sum_{i=1}^{n}\\left\\|X_{i}\\right\\|^{2} \\widehat{e}_{i}^{2}\\right)\\left|\\left(1-h_{n}^{*}\\right)^{-1}-1\\right| .\n\\end{aligned}\n\\]\nThe sum in parenthesis can be shown to be \\(O_{p}(1)\\) under Assumption \\(7.2\\) by the same argument as in in the proof of Theorem 7.6. (In fact, it can be shown to converge in probability to \\(\\mathbb{E}\\left[\\|X\\|^{2} e^{2}\\right]\\).) The term in absolute values is \\(o_{p}(1)\\) by (7.24). Thus the product is \\(o_{p}(1)\\) which means that \\(\\bar{\\Omega}=\\widehat{\\Omega}+o_{p}(1) \\underset{p}{\\longrightarrow}\\).\nSimilarly,\n\\[\n\\begin{aligned}\n\\|\\widetilde{\\Omega}-\\widehat{\\Omega}\\| & \\leq \\frac{1}{n} \\sum_{i=1}^{n}\\left\\|X_{i} X_{i}^{\\prime}\\right\\| \\widehat{e}_{i}^{2}\\left|\\left(1-h_{i i}\\right)^{-2}-1\\right| \\\\\n& \\leq\\left(\\frac{1}{n} \\sum_{i=1}^{n}\\left\\|X_{i}\\right\\|^{2} \\widehat{e}_{i}^{2}\\right)\\left|\\left(1-h_{n}^{*}\\right)^{-2}-1\\right| \\\\\n&=o_{p}(1) .\n\\end{aligned}\n\\]\nTheorem 7.7 Under Assumption 7.2, as \\(n \\rightarrow \\infty, \\widetilde{\\Omega} \\underset{p}{\\longrightarrow} \\Omega, \\bar{\\Omega} \\underset{p}{\\longrightarrow} \\Omega, \\widehat{V}_{\\beta}^{\\mathrm{HC1}} \\underset{p}{\\longrightarrow}\\) \\(\\boldsymbol{V}_{\\beta}, \\widehat{\\boldsymbol{V}}_{\\beta}^{\\mathrm{HC} 2} \\underset{p}{\\longrightarrow} \\boldsymbol{V}_{\\beta}\\), and \\(\\widehat{\\boldsymbol{V}}_{\\beta}^{\\mathrm{HC} 3} \\underset{p}{\\longrightarrow} \\boldsymbol{V}_{\\beta}\\)\nTheorem \\(7.7\\) shows that the alternative covariance matrix estimators are also consistent for the asymptotic covariance matrix.\nTo simplify notation, for the remainder of the chapter we will use the notation \\(\\widehat{\\boldsymbol{V}}_{\\beta}\\) and \\(\\widehat{\\boldsymbol{V}}_{\\widehat{\\beta}}\\) to refer to any of the heteroskedasticity-consistent covariance matrix estimators \\(\\mathrm{HC}\\), \\(\\mathrm{HC} 1\\), HC2, and \\(\\mathrm{HC3}\\), as they all have the same asymptotic limits."
  },
  {
    "objectID": "chpt07-asymptotic-ls.html#functions-of-parameters",
    "href": "chpt07-asymptotic-ls.html#functions-of-parameters",
    "title": "7  Asymptotic Theory for Least Squares",
    "section": "7.10 Functions of Parameters",
    "text": "7.10 Functions of Parameters\nIn most serious applications a researcher is actually interested in a specific transformation of the coefficient vector \\(\\beta=\\left(\\beta_{1}, \\ldots, \\beta_{k}\\right)\\). For example, the researcher may be interested in a single coefficient \\(\\beta_{j}\\) or a ratio \\(\\beta_{j} / \\beta_{l}\\). More generally, interest may focus on a quantity such as consumer surplus which could be a complicated function of the coefficients. In any of these cases we can write the parameter of interest \\(\\theta\\) as a function of the coefficients, e.g. \\(\\theta=r(\\beta)\\) for some function \\(r: \\mathbb{R}^{k} \\rightarrow \\mathbb{R}^{q}\\). The estimate of \\(\\theta\\) is\n\\[\n\\widehat{\\theta}=r(\\widehat{\\beta}) .\n\\]\nBy the continuous mapping theorem (Theorem 6.6) and the fact \\(\\widehat{\\beta} \\underset{p}{\\longrightarrow} \\beta\\) we can deduce that \\(\\widehat{\\theta}\\) is consistent for \\(\\theta\\) if the function \\(r(\\cdot)\\) is continuous.\nTheorem 7.8 Under Assumption 7.1, if \\(r(\\beta)\\) is continuous at the true value of \\(\\beta\\) then as \\(n \\rightarrow \\infty, \\widehat{\\theta} \\underset{p}{\\longrightarrow} \\theta\\)\nFurthermore, if the transformation is sufficiently smooth, by the Delta Method (Theorem 6.8) we can show that \\(\\widehat{\\theta}\\) is asymptotically normal.\nAssumption 7.3 \\(r(\\beta): \\mathbb{R}^{k} \\rightarrow \\mathbb{R}^{q}\\) is continuously differentiable at the true value of \\(\\beta\\) and \\(\\boldsymbol{R}=\\frac{\\partial}{\\partial \\beta} r(\\beta)^{\\prime}\\) has rank \\(q\\).\nTheorem 7.9 Asymptotic Distribution of Functions of Parameters Under Assumptions \\(7.2\\) and 7.3, as \\(n \\rightarrow \\infty\\),\n\\[\n\\sqrt{n}(\\widehat{\\theta}-\\theta) \\underset{d}{\\longrightarrow} \\mathrm{N}\\left(0, \\boldsymbol{V}_{\\theta}\\right)\n\\]\nwhere \\(\\boldsymbol{V}_{\\theta}=\\boldsymbol{R}^{\\prime} \\boldsymbol{V}_{\\beta} \\boldsymbol{R}\\).\nIn many cases the function \\(r(\\beta)\\) is linear:\n\\[\nr(\\beta)=\\boldsymbol{R}^{\\prime} \\beta\n\\]\nfor some \\(k \\times q\\) matrix \\(\\boldsymbol{R}\\). In particular if \\(\\boldsymbol{R}\\) is a “selector matrix”\n\\[\n\\boldsymbol{R}=\\left(\\begin{array}{l}\n\\boldsymbol{I} \\\\\n0\n\\end{array}\\right)\n\\]\nthen we can partition \\(\\beta=\\left(\\beta_{1}^{\\prime}, \\beta_{2}^{\\prime}\\right)^{\\prime}\\) so that \\(\\boldsymbol{R}^{\\prime} \\beta=\\beta_{1}\\). Then\n\\[\n\\boldsymbol{V}_{\\boldsymbol{\\theta}}=\\left(\\begin{array}{ll}\n\\boldsymbol{I} & 0\n\\end{array}\\right) \\boldsymbol{V}_{\\beta}\\left(\\begin{array}{l}\n\\boldsymbol{I} \\\\\n0\n\\end{array}\\right)=\\boldsymbol{V}_{11},\n\\]\nthe upper-left sub-matrix of \\(V_{11}\\) given in (7.14). In this case (7.25) states that\n\\[\n\\sqrt{n}\\left(\\widehat{\\beta}_{1}-\\beta_{1}\\right) \\underset{d}{\\longrightarrow} \\mathrm{N}\\left(0, V_{11}\\right) .\n\\]\nThat is, subsets of \\(\\widehat{\\beta}\\) are approximately normal with variances given by the conformable subcomponents of \\(V\\).\nTo illustrate the case of a nonlinear transformation take the example \\(\\theta=\\beta_{j} / \\beta_{l}\\) for \\(j \\neq l\\). Then\n\\[\n\\boldsymbol{R}=\\frac{\\partial}{\\partial \\beta} r(\\beta)=\\left(\\begin{array}{c}\n\\frac{\\partial}{\\partial \\beta_{1}}\\left(\\beta_{j} / \\beta_{l}\\right) \\\\\n\\vdots \\\\\n\\frac{\\partial}{\\partial \\beta_{j}}\\left(\\beta_{j} / \\beta_{l}\\right) \\\\\n\\vdots \\\\\n\\frac{\\partial}{\\partial \\beta_{\\ell}}\\left(\\beta_{j} / \\beta_{l}\\right) \\\\\n\\vdots \\\\\n\\frac{\\partial}{\\partial \\beta_{k}}\\left(\\beta_{j} / \\beta_{l}\\right)\n\\end{array}\\right)=\\left(\\begin{array}{c}\n0 \\\\\n\\vdots \\\\\n1 / \\beta_{l} \\\\\n\\vdots \\\\\n-\\beta_{j} / \\beta_{l}^{2} \\\\\n\\vdots \\\\\n0\n\\end{array}\\right)\n\\]\nso\n\\[\n\\boldsymbol{V}_{\\theta}=\\boldsymbol{V}_{j j} / \\beta_{l}^{2}+\\boldsymbol{V}_{l l} \\beta_{j}^{2} / \\beta_{l}^{4}-2 \\boldsymbol{V}_{j l} \\beta_{j} / \\beta_{l}^{3}\n\\]\nwhere \\(\\boldsymbol{V}_{a b}\\) denotes the \\(a b^{t h}\\) element of \\(\\boldsymbol{V}_{\\beta}\\).\nFor inference we need an estimator of the asymptotic covariance matrix \\(\\boldsymbol{V}_{\\theta}=\\boldsymbol{R}^{\\prime} \\boldsymbol{V}_{\\beta} \\boldsymbol{R}\\). For this it is typical to use the plug-in estimator\n\\[\n\\widehat{\\boldsymbol{R}}=\\frac{\\partial}{\\partial \\beta} r(\\widehat{\\beta})^{\\prime} .\n\\]\nThe derivative in (7.27) may be calculated analytically or numerically. By analytically, we mean working out the formula for the derivative and replacing the unknowns by point estimates. For example, if \\(\\theta=\\) \\(\\beta_{j} / \\beta_{l}\\) then \\(\\frac{\\partial}{\\partial \\beta} r(\\beta)\\) is (7.26). However in some cases the function \\(r(\\beta)\\) may be extremely complicated and a formula for the analytic derivative may not be easily available. In this case numerical differentiation may be preferable. Let \\(\\delta_{l}=(0 \\cdots 1 \\cdots 0)^{\\prime}\\) be the unit vector with the ” 1 ” in the \\(l^{\\text {th }}\\) place. The \\(j l^{t h}\\) element of a numerical derivative \\(\\widehat{\\boldsymbol{R}}\\) is\nfor some small \\(\\epsilon\\).\n\\[\n\\widehat{\\boldsymbol{R}}_{j l}=\\frac{r_{j}\\left(\\widehat{\\beta}+\\delta_{l} \\epsilon\\right)-r_{j}(\\widehat{\\beta})}{\\epsilon}\n\\]\nThe estimator of \\(\\boldsymbol{V}_{\\theta}\\) is\n\\[\n\\widehat{\\boldsymbol{V}}_{\\theta}=\\widehat{\\boldsymbol{R}}^{\\prime} \\widehat{\\boldsymbol{V}}_{\\beta} \\widehat{\\boldsymbol{R}} \\text {. }\n\\]\nAlternatively, the homoskedastic covariance matrix estimator could be used leading to a homoskedastic covariance matrix estimator for \\(\\theta\\).\n\\[\n\\widehat{\\boldsymbol{V}}_{\\theta}^{0}=\\widehat{\\boldsymbol{R}}^{\\prime} \\widehat{\\boldsymbol{V}}_{\\beta}^{0} \\widehat{\\boldsymbol{R}}=\\widehat{\\boldsymbol{R}}^{\\prime} \\widehat{\\boldsymbol{Q}}_{X X}^{-1} \\widehat{\\boldsymbol{R}} s^{2} .\n\\]\nGiven (7.27), (7.28) and (7.29) are simple to calculate using matrix operations.\nAs the primary justification for \\(\\widehat{\\boldsymbol{V}}_{\\theta}\\) is the asymptotic approximation (7.25), \\(\\widehat{\\boldsymbol{V}}_{\\theta}\\) is often called an asymptotic covariance matrix estimator.\nThe estimator \\(\\widehat{\\boldsymbol{V}}_{\\theta}\\) is consistent for \\(\\boldsymbol{V}_{\\theta}\\) under the conditions of Theorem \\(7.9\\) because \\(\\widehat{\\boldsymbol{V}}_{\\beta} \\vec{p}_{\\boldsymbol{V}}\\) by Theorem \\(7.6\\) and\n\\[\n\\widehat{\\boldsymbol{R}}=\\frac{\\partial}{\\partial \\beta} r(\\widehat{\\beta})^{\\prime} \\underset{p}{\\Rightarrow} \\frac{\\partial}{\\partial \\beta} r(\\beta)^{\\prime}=\\boldsymbol{R}\n\\]\nbecause \\(\\widehat{\\beta} \\underset{p}{\\longrightarrow} \\beta\\) and the function \\(\\frac{\\partial}{\\partial \\beta} r(\\beta)^{\\prime}\\) is continuous in \\(\\beta\\). Theorem 7.10 Under Assumptions \\(7.2\\) and 7.3, as \\(n \\rightarrow \\infty, \\widehat{\\boldsymbol{V}}_{\\theta} \\underset{p}{\\longrightarrow} \\boldsymbol{V}_{\\theta}\\)\nTheorem 7.10 shows that \\(\\widehat{\\boldsymbol{V}}_{\\theta}\\) is consistent for \\(\\boldsymbol{V}_{\\theta}\\) and thus may be used for asymptotic inference. In practice we may set\n\\[\n\\widehat{\\boldsymbol{V}}_{\\widehat{\\theta}}=\\widehat{\\boldsymbol{R}}^{\\prime} \\widehat{\\boldsymbol{V}}_{\\widehat{\\beta}} \\widehat{\\boldsymbol{R}}=n^{-1} \\widehat{\\boldsymbol{R}}^{\\prime} \\widehat{\\boldsymbol{V}}_{\\beta} \\widehat{\\boldsymbol{R}}\n\\]\nas an estimator of the variance of \\(\\widehat{\\theta}\\)."
  },
  {
    "objectID": "chpt07-asymptotic-ls.html#asymptotic-standard-errors",
    "href": "chpt07-asymptotic-ls.html#asymptotic-standard-errors",
    "title": "7  Asymptotic Theory for Least Squares",
    "section": "7.11 Asymptotic Standard Errors",
    "text": "7.11 Asymptotic Standard Errors\nAs described in Section 4.15, a standard error is an estimator of the standard deviation of the distribution of an estimator. Thus if \\(\\widehat{\\boldsymbol{V}}_{\\widehat{\\beta}}\\) is an estimator of the covariance matrix of \\(\\widehat{\\beta}\\) then standard errors are the square roots of the diagonal elements of this matrix. These take the form\n\\[\ns\\left(\\widehat{\\beta}_{j}\\right)=\\sqrt{\\widehat{\\boldsymbol{V}}_{\\widehat{\\beta}_{j}}}=\\sqrt{\\left[\\widehat{\\boldsymbol{V}}_{\\widehat{\\beta}}\\right]_{j j}} .\n\\]\nStandard errors for \\(\\hat{\\theta}\\) are constructed similarly. Supposing that \\(\\theta=h(\\beta)\\) is real-valued then the standard error for \\(\\widehat{\\theta}\\) is the square root of \\((7.30)\\)\n\\[\ns(\\widehat{\\theta})=\\sqrt{\\widehat{\\boldsymbol{R}}^{\\prime} \\widehat{\\boldsymbol{V}}_{\\widehat{\\beta}} \\widehat{\\boldsymbol{R}}}=\\sqrt{n^{-1} \\widehat{\\boldsymbol{R}}^{\\prime} \\widehat{\\boldsymbol{V}}_{\\beta} \\widehat{\\boldsymbol{R}}}\n\\]\nWhen the justification is based on asymptotic theory we call \\(s\\left(\\widehat{\\beta}_{j}\\right)\\) or \\(s(\\widehat{\\theta})\\) an asymptotic standard error for \\(\\widehat{\\beta}_{j}\\) or \\(\\widehat{\\theta}\\). When reporting your results it is good practice to report standard errors for each reported estimate and this includes functions and transformations of your parameter estimates. This helps users of the work (including yourself) assess the estimation precision.\nWe illustrate using the log wage regression\n\\[\n\\log (\\text { wage })=\\beta_{1} \\text { education }+\\beta_{2} \\text { experience }+\\beta_{3} \\text { experience }^{2} / 100+\\beta_{4}+e .\n\\]\nConsider the following three parameters of interest.\n\nPercentage return to education:\n\n\\[\n\\theta_{1}=100 \\beta_{1}\n\\]\n(100 times the partial derivative of the conditional expectation of \\(\\log (\\) wage) with respect to education.)\n 1. Percentage return to experience for individuals with 10 years of experience:\n\\[\n\\theta_{2}=100 \\beta_{2}+20 \\beta_{3}\n\\]\n(100 times the partial derivative of the conditional expectation of log wages with respect to experience, evaluated at experience \\(=10\\).) 3. Experience level which maximizes expected log wages:\n\\[\n\\theta_{3}=-50 \\beta_{2} / \\beta_{3}\n\\]\n(The level of experience at which the partial derivative of the conditional expectation of log(wage) with respect to experience equals 0 .)\nThe \\(4 \\times 1\\) vector \\(\\boldsymbol{R}\\) for these three parameters is\n\\[\n\\boldsymbol{R}=\\left(\\begin{array}{c}\n100 \\\\\n0 \\\\\n0 \\\\\n0\n\\end{array}\\right), \\quad\\left(\\begin{array}{c}\n0 \\\\\n100 \\\\\n20 \\\\\n0\n\\end{array}\\right), \\quad\\left(\\begin{array}{c}\n0 \\\\\n-50 / \\beta_{3} \\\\\n50 \\beta_{2} / \\beta_{3}^{2} \\\\\n0\n\\end{array}\\right),\n\\]\nrespectively.\nWe use the subsample of married Black women (all experience levels) which has 982 observations. The point estimates and standard errors are\n\nThe standard errors are the square roots of the HC2 covariance matrix estimate\n\\[\n\\overline{\\boldsymbol{V}}_{\\widehat{\\beta}}=\\left(\\begin{array}{cccc}\n0.632 & 0.131 & -0.143 & -11.1 \\\\\n0.131 & 0.390 & -0.731 & -6.25 \\\\\n-0.143 & -0.731 & 1.48 & 9.43 \\\\\n-11.1 & -6.25 & 9.43 & 246\n\\end{array}\\right) \\times 10^{-4} .\n\\]\nWe calculate that\n\\[\n\\begin{aligned}\n& \\widehat{\\theta}_{1}=100 \\widehat{\\beta}_{1}=100 \\times 0.118=11.8 \\\\\n& s\\left(\\widehat{\\theta}_{1}\\right)=\\sqrt{100^{2} \\times 0.632 \\times 10^{-4}}=0.8 \\\\\n& \\widehat{\\theta}_{2}=100 \\widehat{\\beta}_{2}+20 \\widehat{\\beta}_{3}=100 \\times 0.016-20 \\times 0.022=1.16 \\\\\n& s\\left(\\widehat{\\theta}_{2}\\right)=\\sqrt{\\left(\\begin{array}{ll}100 & 20\\end{array}\\right)\\left(\\begin{array}{cc}0.390 & -0.731 \\\\-0.731 & 1.48\\end{array}\\right)\\left(\\begin{array}{c}100 \\\\20\\end{array}\\right) \\times 10^{-4}}=0.55 \\\\\n& \\widehat{\\theta}_{3}=-50 \\widehat{\\beta}_{2} / \\widehat{\\beta}_{3}=50 \\times 0.016 / 0.022=35.2\n\\end{aligned}\n\\]\nThe calculations show that the estimate of the percentage return to education is \\(12 %\\) per year with a standard error of 0.8. The estimate of the percentage return to experience for those with 10 years of experience is \\(1.2 %\\) per year with a standard error of \\(0.6\\). The estimate of the experience level which maximizes expected log wages is 35 years with a standard error of 7 .\nIn Stata the nlcom command can be used after estimation to perform the same calculations. To illustrate, after estimation of (7.31) use the commands given below. In each case, Stata reports the coefficient estimate, asymptotic standard error, and \\(95 %\\) confidence interval.\n\nStata Commands\\ nlcom 100_b[education]\\ nlcom 100_b[experience]+20_b[exp2]\\ nlcom -50_b[experience \\(] / 0_{-} \\mathrm{b}[\\exp 2]\\)]. This is reasonably tight.\nPercentage return to experience (per year) for individuals with 10 years experience. A \\(90 %\\) asymptotic confidence interval is \\(1.1 \\pm 1.645 \\times 0.4=[0.5,1.8]\\). The interval is positive but broad. This indicates that the return to experience is positive, but of uncertain magnitude. Experience level which maximizes expected log wages. An \\(80 %\\) asymptotic confidence interval is \\(35 \\pm 1.28 \\times 7=[26,44]\\). This is rather imprecise, indicating that the estimates are not very informative regarding this parameter."
  },
  {
    "objectID": "chpt07-asymptotic-ls.html#regression-intervals",
    "href": "chpt07-asymptotic-ls.html#regression-intervals",
    "title": "7  Asymptotic Theory for Least Squares",
    "section": "7.12 Regression Intervals",
    "text": "7.12 Regression Intervals\nIn the linear regression model the conditional expectation of \\(Y\\) given \\(X=x\\) is\n\\[\nm(x)=\\mathbb{E}[Y \\mid X=x]=x^{\\prime} \\beta .\n\\]\nIn some cases we want to estimate \\(m(x)\\) at a particular point \\(x\\). Notice that this is a linear function of \\(\\beta\\). Letting \\(r(\\beta)=x^{\\prime} \\beta\\) and \\(\\theta=r(\\beta)\\) we see that \\(\\hat{m}(x)=\\widehat{\\theta}=x^{\\prime} \\widehat{\\beta}\\) and \\(\\boldsymbol{R}=x\\) so \\(s(\\widehat{\\theta})=\\sqrt{x^{\\prime} \\widehat{\\boldsymbol{V}}_{\\widehat{\\beta}} x}\\). Thus an asymptotic \\(95 %\\) confidence interval for \\(m(x)\\) is\n\\[\n\\left[x^{\\prime} \\widehat{\\beta} \\pm 1.96 \\sqrt{x^{\\prime} \\widehat{\\boldsymbol{V}}_{\\widehat{\\beta}} x}\\right] .\n\\]\nIt is interesting to observe that if this is viewed as a function of \\(x\\) the width of the confidence interval is dependent on \\(x\\).\nTo illustrate we return to the log wage regression (3.12) of Section 3.7. The estimated regression equation is\n\\[\n\\widehat{\\log (\\text { wage })}=x^{\\prime} \\widehat{\\beta}=0.155 x+0.698\n\\]\nwhere \\(x=e d u c a t i o n\\). The covariance matrix estimate from (4.43) is\n\\[\n\\widehat{\\boldsymbol{V}}_{\\widehat{\\beta}}=\\left(\\begin{array}{cc}\n0.001 & -0.015 \\\\\n-0.015 & 0.243\n\\end{array}\\right) .\n\\]\nThus the \\(95 %\\) confidence interval for the regression is\n\\[\n0.155 x+0.698 \\pm 1.96 \\sqrt{0.001 x^{2}-0.030 x+0.243} .\n\\]\nThe estimated regression and 95% intervals are shown in Figure 7.4(a). Notice that the confidence bands take a hyperbolic shape. This means that the regression line is less precisely estimated for large and small values of education.\nPlots of the estimated regression line and confidence intervals are especially useful when the regression includes nonlinear terms. To illustrate consider the log wage regression (7.31) which includes experience and its square and covariance matrix estimate (7.32). We are interested in plotting the regression estimate and regression intervals as a function of experience. Since the regression also includes education, to plot the estimates in a simple graph we fix education at a specific value. We select education=12. This only affects the level of the estimated regression since education enters without an interaction. Define the points of evaluation\n\\[\nz(x)=\\left(\\begin{array}{c}\n12 \\\\\nx \\\\\nx^{2} / 100 \\\\\n1\n\\end{array}\\right)\n\\]\nwhere \\(x=\\) experience.\n\n\nWage on Education\n\n\n\nWage on Experience\n\nFigure 7.4: Regression Intervals\nThe \\(95 %\\) regression interval for education \\(=12\\) as a function of \\(x=\\) experience is\n\\[\n\\begin{aligned}\n& 0.118 \\times 12+0.016 x-0.022 x^{2} / 100+0.947 \\\\\n& \\pm 1.96 \\sqrt{z(x)^{\\prime}\\left(\\begin{array}{cccc}0.632 & 0.131 & -0.143 & -11.1 \\\\0.131 & 0.390 & -0.731 & -6.25 \\\\-0.143 & -0.731 & 1.48 & 9.43 \\\\-11.1 & -6.25 & 9.43 & 246\\end{array}\\right) z(x) \\times 10^{-4}} \\\\\n& =0.016 x-.00022 x^{2}+2.36 \\\\\n& \\pm 0.0196 \\sqrt{70.608-9.356 x+0.54428 x^{2}-0.01462 x^{3}+0.000148 x^{4}} \\text {. }\n\\end{aligned}\n\\]\nThe estimated regression and 95% intervals are shown in Figure 7.4(b). The regression interval widens greatly for small and large values of experience indicating considerable uncertainty about the effect of experience on mean wages for this population. The confidence bands take a more complicated shape than in Figure 7.4(a) due to the nonlinear specification."
  },
  {
    "objectID": "chpt07-asymptotic-ls.html#forecast-intervals",
    "href": "chpt07-asymptotic-ls.html#forecast-intervals",
    "title": "7  Asymptotic Theory for Least Squares",
    "section": "7.13 Forecast Intervals",
    "text": "7.13 Forecast Intervals\nSuppose we are given a value of the regressor vector \\(X_{n+1}\\) for an individual outside the sample and we want to forecast (guess) \\(Y_{n+1}\\) for this individual. This is equivalent to forecasting \\(Y_{n+1}\\) given \\(X_{n+1}=x\\) which will generally be a function of \\(x\\). A reasonable forecasting rule is the conditional expectation \\(m(x)\\) as it is the mean-square minimizing forecast. A point forecast is the estimated conditional expectation \\(\\widehat{m}(x)=x^{\\prime} \\widehat{\\beta}\\). We would also like a measure of uncertainty for the forecast.\nThe forecast error is \\(\\widehat{e}_{n+1}=Y_{n+1}-\\widehat{m}(x)=e_{n+1}-x^{\\prime}(\\widehat{\\beta}-\\beta)\\). As the out-of-sample error \\(e_{n+1}\\) is inde- pendent of the in-sample estimator \\(\\widehat{\\beta}\\) this has conditional variance\n\\[\n\\begin{aligned}\n\\mathbb{E}\\left[\\widehat{e}_{n+1}^{2} \\mid X_{n+1}=x\\right] &=\\mathbb{E}\\left[e_{n+1}^{2}-2 x^{\\prime}(\\widehat{\\beta}-\\beta) e_{n+1}+x^{\\prime}(\\widehat{\\beta}-\\beta)(\\widehat{\\beta}-\\beta)^{\\prime} x \\mid X_{n+1}=x\\right] \\\\\n&=\\mathbb{E}\\left[e_{n+1}^{2} \\mid X_{n+1}=x\\right]+x^{\\prime} \\mathbb{E}\\left[(\\widehat{\\beta}-\\beta)(\\widehat{\\beta}-\\beta)^{\\prime}\\right] x \\\\\n&=\\sigma^{2}(x)+x^{\\prime} \\boldsymbol{V}_{\\widehat{\\beta}} x .\n\\end{aligned}\n\\]\nUnder homoskedasticity, \\(\\mathbb{E}\\left[e_{n+1}^{2} \\mid X_{n+1}\\right]=\\sigma^{2}\\). In this case a simple estimator of (7.36) is \\(\\widehat{\\sigma}^{2}+x^{\\prime} \\boldsymbol{V}_{\\widehat{\\beta}} x\\) so a standard error for the forecast is \\(\\widehat{s}(x)=\\sqrt{\\widehat{\\sigma}^{2}+x^{\\prime} \\boldsymbol{V}_{\\widehat{\\beta}} x}\\). Notice that this is different from the standard error for the conditional expectation.\nThe conventional 95% forecast interval for \\(Y_{n+1}\\) uses a normal approximation and equals \\(\\left[x^{\\prime} \\widehat{\\beta} \\pm 2 \\widehat{s}(x)\\right]\\). It is difficult, however, to fully justify this choice. It would be correct if we have a normal approximation to the ratio\n\\[\n\\frac{e_{n+1}-x^{\\prime}(\\widehat{\\beta}-\\beta)}{\\widehat{s}(x)} .\n\\]\nThe difficulty is that the equation error \\(e_{n+1}\\) is generally non-normal and asymptotic theory cannot be applied to a single observation. The only special exception is the case where \\(e_{n+1}\\) has the exact distribution \\(\\mathrm{N}\\left(0, \\sigma^{2}\\right)\\) which is generally invalid.\nAn accurate forecast interval would use the conditional distribution of \\(e_{n+1}\\) given \\(X_{n+1}=x\\), which is more challenging to estimate. Due to this difficulty many applied forecasters use the simple approximate interval \\(\\left[x^{\\prime} \\widehat{\\beta} \\pm 2 \\widehat{s}(x)\\right]\\) despite the lack of a convincing justification."
  },
  {
    "objectID": "chpt07-asymptotic-ls.html#wald-statistic",
    "href": "chpt07-asymptotic-ls.html#wald-statistic",
    "title": "7  Asymptotic Theory for Least Squares",
    "section": "7.14 Wald Statistic",
    "text": "7.14 Wald Statistic\nLet \\(\\theta=r(\\beta): \\mathbb{R}^{k} \\rightarrow \\mathbb{R}^{q}\\) be any parameter vector of interest, \\(\\widehat{\\theta}\\) its estimator, and \\(\\widehat{\\boldsymbol{V}}_{\\widehat{\\theta}}\\) its covariance matrix estimator. Consider the quadratic form\n\\[\nW(\\theta)=(\\widehat{\\theta}-\\theta)^{\\prime} \\widehat{\\mathbf{V}}_{\\widehat{\\theta}}^{-1}(\\widehat{\\theta}-\\theta)=n(\\widehat{\\theta}-\\theta)^{\\prime} \\widehat{\\boldsymbol{V}}_{\\theta}^{-1}(\\widehat{\\theta}-\\theta) .\n\\]\nwhere \\(\\widehat{\\boldsymbol{V}}_{\\theta}=n \\widehat{\\boldsymbol{V}}_{\\widehat{\\theta}}\\). When \\(q=1\\), then \\(W(\\theta)=T(\\theta)^{2}\\) is the square of the t-ratio. When \\(q>1, W(\\theta)\\) is typically called a Wald statistic as it was proposed by Wald (1943). We are interested in its sampling distribution.\nThe asymptotic distribution of \\(W(\\theta)\\) is simple to derive given Theorem \\(7.9\\) and Theorem 7.10. They show that \\(\\sqrt{n}(\\widehat{\\theta}-\\theta) \\underset{d}{\\longrightarrow} Z \\sim \\mathrm{N}\\left(0, \\boldsymbol{V}_{\\theta}\\right)\\) and \\(\\widehat{\\boldsymbol{V}}_{\\theta} \\underset{p}{\\longrightarrow} \\boldsymbol{V}_{\\theta}\\). It follows that\n\\[\nW(\\theta)=\\sqrt{n}(\\widehat{\\theta}-\\theta)^{\\prime} \\widehat{\\boldsymbol{V}}_{\\theta}^{-1} \\sqrt{n}(\\widehat{\\theta}-\\theta) \\underset{d}{\\longrightarrow} Z^{\\prime} \\boldsymbol{V}_{\\theta}^{-1} Z\n\\]\na quadratic in the normal random vector \\(Z\\). As shown in Theorem \\(5.3 .5\\) the distribution of this quadratic form is \\(\\chi_{q}^{2}\\), a chi-square random variable with \\(q\\) degrees of freedom.\nTheorem 7.13 Under Assumptions 7.2, \\(7.3\\) and 7.4, as \\(n \\rightarrow \\infty, W(\\theta) \\underset{d}{\\longrightarrow} \\chi_{q}^{2}\\).\nTheorem \\(7.13\\) is used to justify multivariate confidence regions and multivariate hypothesis tests."
  },
  {
    "objectID": "chpt07-asymptotic-ls.html#homoskedastic-wald-statistic",
    "href": "chpt07-asymptotic-ls.html#homoskedastic-wald-statistic",
    "title": "7  Asymptotic Theory for Least Squares",
    "section": "7.15 Homoskedastic Wald Statistic",
    "text": "7.15 Homoskedastic Wald Statistic\nUnder the conditional homoskedasticity assumption \\(\\mathbb{E}\\left[e^{2} \\mid X\\right]=\\sigma^{2}\\) we can construct the Wald statistic using the homoskedastic covariance matrix estimator \\(\\widehat{\\boldsymbol{V}}_{\\theta}^{0}\\) defined in (7.29). This yields a homoskedastic Wald statistic\n\\[\nW^{0}(\\theta)=(\\widehat{\\theta}-\\theta)^{\\prime}\\left(\\widehat{\\boldsymbol{V}}_{\\widehat{\\theta}}^{0}\\right)^{-1}(\\widehat{\\theta}-\\theta)=n(\\widehat{\\theta}-\\theta)^{\\prime}\\left(\\widehat{\\boldsymbol{V}}_{\\theta}^{0}\\right)^{-1}(\\widehat{\\theta}-\\theta) .\n\\]\nUnder the assumption of conditional homoskedasticity it has the same asymptotic distribution as \\(W(\\theta)\\)\nTheorem 7.14 Under Assumptions 7.2, 7.3, and \\(\\mathbb{E}\\left[e^{2} \\mid X\\right]=\\sigma^{2}>0\\), as \\(n \\rightarrow \\infty\\), \\(W^{0}(\\theta) \\underset{d}{\\longrightarrow} \\chi_{q}^{2}\\)"
  },
  {
    "objectID": "chpt07-asymptotic-ls.html#confidence-regions",
    "href": "chpt07-asymptotic-ls.html#confidence-regions",
    "title": "7  Asymptotic Theory for Least Squares",
    "section": "7.16 Confidence Regions",
    "text": "7.16 Confidence Regions\nA confidence region \\(\\widehat{C}\\) is a set estimator for \\(\\theta \\in \\mathbb{R}^{q}\\) when \\(q>1\\). A confidence region \\(\\widehat{C}\\) is a set in \\(\\mathbb{R}^{q}\\) intended to cover the true parameter value with a pre-selected probability \\(1-\\alpha\\). Thus an ideal confidence region has the coverage probability \\(\\mathbb{P}[\\theta \\in \\widehat{C}]=1-\\alpha\\). In practice it is typically not possible to construct a region with exact coverage but we can calculate its asymptotic coverage.\nWhen the parameter estimator satisfies the conditions of Theorem \\(7.13\\) a good choice for a confidence region is the ellipse\n\\[\n\\widehat{C}=\\left\\{\\theta: W(\\theta) \\leq c_{1-\\alpha}\\right\\}\n\\]\nwith \\(c_{1-\\alpha}\\) the \\(1-\\alpha\\) quantile of the \\(\\chi_{q}^{2}\\) distribution. (Thus \\(F_{q}\\left(c_{1-\\alpha}\\right)=1-\\alpha\\).) It can be computed by, for example, chi2inv \\((1-\\alpha, q)\\) in MATLAB.\nTheorem \\(7.13\\) implies\n\\[\n\\mathbb{P}[\\theta \\in \\widehat{C}] \\rightarrow \\mathbb{P}\\left[\\chi_{q}^{2} \\leq c_{1-\\alpha}\\right]=1-\\alpha\n\\]\nwhich shows that \\(\\widehat{C}\\) has asymptotic coverage \\(1-\\alpha\\).\nTo illustrate the construction of a confidence region, consider the estimated regression (7.31) of\n\nSuppose that the two parameters of interest are the percentage return to education \\(\\theta_{1}=100 \\beta_{1}\\) and the percentage return to experience for individuals with 10 years experience \\(\\theta_{2}=100 \\beta_{2}+20 \\beta_{3}\\). These two parameters are a linear transformation of the regression parameters with point estimates\n\\[\n\\widehat{\\theta}=\\left(\\begin{array}{cccc}\n100 & 0 & 0 & 0 \\\\\n0 & 100 & 20 & 0\n\\end{array}\\right) \\widehat{\\beta}=\\left(\\begin{array}{c}\n11.8 \\\\\n1.2\n\\end{array}\\right),\n\\]\nand have the covariance matrix estimate\n\\[\n\\widehat{\\boldsymbol{V}}_{\\widehat{\\theta}}=\\left(\\begin{array}{cccc}\n0 & 100 & 0 & 0 \\\\\n0 & 0 & 100 & 20\n\\end{array}\\right) \\widehat{\\boldsymbol{V}}_{\\widehat{\\beta}}\\left(\\begin{array}{cc}\n0 & 0 \\\\\n100 & 0 \\\\\n0 & 100 \\\\\n0 & 20\n\\end{array}\\right)=\\left(\\begin{array}{cc}\n0.632 & 0.103 \\\\\n0.103 & 0.157\n\\end{array}\\right)\n\\]\nwith inverse\n\\[\n\\widehat{\\boldsymbol{V}}_{\\widehat{\\theta}}^{-1}=\\left(\\begin{array}{cc}\n1.77 & -1.16 \\\\\n-1.16 & 7.13\n\\end{array}\\right) .\n\\]\nThus the Wald statistic is\n\\[\n\\begin{aligned}\nW(\\theta) &=(\\widehat{\\theta}-\\theta)^{\\prime} \\widehat{\\boldsymbol{V}}_{\\widehat{\\theta}}^{-1}(\\widehat{\\theta}-\\theta) \\\\\n&=\\left(\\begin{array}{c}\n11.8-\\theta_{1} \\\\\n1.2-\\theta_{2}\n\\end{array}\\right)^{\\prime}\\left(\\begin{array}{cc}\n1.77 & -1.16 \\\\\n-1.16 & 7.13\n\\end{array}\\right)\\left(\\begin{array}{c}\n11.8-\\theta_{1} \\\\\n1.2-\\theta_{2}\n\\end{array}\\right) \\\\\n&=1.77\\left(11.8-\\theta_{1}\\right)^{2}-2.32\\left(11.8-\\theta_{1}\\right)\\left(1.2-\\theta_{2}\\right)+7.13\\left(1.2-\\theta_{2}\\right)^{2} .\n\\end{aligned}\n\\]\nThe \\(90 %\\) quantile of the \\(\\chi_{2}^{2}\\) distribution is \\(4.605\\) (we use the \\(\\chi_{2}^{2}\\) distribution as the dimension of \\(\\theta\\) is two) so an asymptotic \\(90 %\\) confidence region for the two parameters is the interior of the ellipse \\(W(\\theta)=\\) \\(4.605\\) which is displayed in Figure 7.5. Since the estimated correlation of the two coefficient estimates is modest (about \\(0.3\\) ) the region is modestly elliptical.\n\nFigure 7.5: Confidence Region for Return to Experience and Return to Education"
  },
  {
    "objectID": "chpt07-asymptotic-ls.html#edgeworth-expansion",
    "href": "chpt07-asymptotic-ls.html#edgeworth-expansion",
    "title": "7  Asymptotic Theory for Least Squares",
    "section": "7.17 Edgeworth Expansion*",
    "text": "7.17 Edgeworth Expansion*\nTheorem \\(7.11\\) showed that the t-ratio \\(T(\\theta)\\) is asymptotically normal. In practice this means that we use the normal distribution to approximate the finite sample distribution of \\(T\\). How good is this approximation? Some insight into the accuracy of the normal approximation can be obtained by an Edgeworth expansion which is a higher-order approximation to the distribution of \\(T\\). The following result is an application of Theorem \\(9.11\\) of Probability and Statistics for Economists.\nTheorem 7.15 Under Assumptions 7.2, 7.3, \\(\\Omega>0, \\mathbb{E}\\|e\\|^{16}<\\infty, \\mathbb{E}\\|X\\|^{16}<\\) \\(\\infty, g(\\beta)\\) has five continuous derivatives in a neighborhood of \\(\\beta\\), and \\(\\mathbb{E}\\left[\\exp \\left(t\\left(\\|e\\|^{4}+\\|X\\|^{4}\\right)\\right)\\right] \\leq B<1\\), as \\(n \\rightarrow \\infty\\)\n\\[\n\\mathbb{P}[T(\\theta) \\leq x]=\\Phi(x)+n^{-1 / 2} p_{1}(x) \\phi(x)+n^{-1} p_{2}(x) \\phi(x)+o\\left(n^{-1}\\right)\n\\]\nuniformly in \\(x\\), where \\(p_{1}(x)\\) is an even polynomial of order 2 and \\(p_{2}(x)\\) is an odd polynomial of degree 5 with coefficients depending on the moments of \\(e\\) and \\(X\\) up to order \\(16 .\\)\nTheorem \\(7.15\\) shows that the finite sample distribution of the t-ratio can be approximated up to \\(o\\left(n^{-1}\\right)\\) by the sum of three terms, the first being the standard normal distribution, the second a \\(O\\left(n^{-1 / 2}\\right)\\) adjustment, and the third a \\(O\\left(n^{-1}\\right)\\) adjustment.\nConsider a one-sided confidence interval \\(\\widehat{C}=\\left[\\widehat{\\theta}-z_{1-\\alpha} s(\\widehat{\\theta}), \\infty\\right)\\) where \\(z_{1-\\alpha}\\) is the \\(1-\\alpha^{t h}\\) quantile of \\(Z \\sim \\mathrm{N}(0,1)\\), thus \\(\\Phi\\left(z_{1-\\alpha}\\right)-1-\\alpha\\). Then\n\\[\n\\begin{aligned}\n\\mathbb{P}[\\theta \\in \\widehat{C}] &=\\mathbb{P}\\left[T(\\theta) \\leq z_{1-\\alpha}\\right] \\\\\n&=\\Phi\\left(z_{1-\\alpha}\\right)+n^{-1 / 2} p_{1}\\left(z_{1-\\alpha}\\right) \\phi\\left(z_{1-\\alpha}\\right)+O\\left(n^{-1}\\right) \\\\\n&=1-\\alpha+O\\left(n^{-1 / 2}\\right) .\n\\end{aligned}\n\\]\nThis means that the actual coverage is within \\(O\\left(n^{-1 / 2}\\right)\\) of the desired \\(1-\\alpha\\) level.\nNow consider a two-sided interval \\(\\widehat{C}=\\left[\\widehat{\\theta}-z_{1-\\alpha / 2} s(\\widehat{\\theta}), \\widehat{\\theta}+z_{1-\\alpha / 2} s(\\widehat{\\theta})\\right]\\). It has coverage\n\\[\n\\begin{aligned}\n\\mathbb{P}[\\theta \\in \\widehat{C}] &=\\mathbb{P}\\left[|T(\\theta)| \\leq z_{1-\\alpha / 2}\\right] \\\\\n&=2 \\Phi\\left(z_{1-\\alpha / 2}\\right)-1+n^{-1} 2 p_{2}\\left(z_{1-\\alpha / 2}\\right) \\phi\\left(z_{1-\\alpha / 2}\\right)+o\\left(n^{-1}\\right) \\\\\n&=1-\\alpha+O\\left(n^{-1}\\right) .\n\\end{aligned}\n\\]\nThis means that the actual coverage is within \\(O\\left(n^{-1}\\right)\\) of the desired \\(1-\\alpha\\) level. The accuracy is better than the one-sided interval because the \\(O\\left(n^{-1 / 2}\\right)\\) term in the Edgeworth expansion has offsetting effects in the two tails of the distribution."
  },
  {
    "objectID": "chpt07-asymptotic-ls.html#uniformly-consistent-residuals",
    "href": "chpt07-asymptotic-ls.html#uniformly-consistent-residuals",
    "title": "7  Asymptotic Theory for Least Squares",
    "section": "7.18 Uniformly Consistent Residuals*",
    "text": "7.18 Uniformly Consistent Residuals*\nIt seems natural to view the residuals \\(\\widehat{e}_{i}\\) as estimators of the unknown errors \\(e_{i}\\). Are they consistent? In this section we develop a convergence result.\nWe can write the residual as\n\\[\n\\widehat{e}_{i}=Y_{i}-X_{i}^{\\prime} \\widehat{\\beta}=e_{i}-X_{i}^{\\prime}(\\widehat{\\beta}-\\beta) .\n\\]\nSince \\(\\widehat{\\beta}-\\beta \\underset{p}{\\longrightarrow} 0\\) it seems reasonable to guess that \\(\\widehat{e}_{i}\\) will be close to \\(e_{i}\\) if \\(n\\) is large.\nWe can bound the difference in (7.39) using the Schwarz inequality (B.12) to find\n\\[\n\\left|\\widehat{e}_{i}-e_{i}\\right|=\\left|X_{i}^{\\prime}(\\widehat{\\beta}-\\beta)\\right| \\leq\\left\\|X_{i}\\right\\|\\|\\widehat{\\beta}-\\beta\\| .\n\\]\nTo bound (7.40) we can use \\(\\|\\widehat{\\beta}-\\beta\\|=O_{p}\\left(n^{-1 / 2}\\right)\\) from Theorem 7.3. We also need to bound the random variable \\(\\left\\|X_{i}\\right\\|\\). If the regressor is bounded, that is, \\(\\left\\|X_{i}\\right\\| \\leq B<\\infty\\), then \\(\\left|\\widehat{e}_{i}-e_{i}\\right| \\leq B\\|\\widehat{\\beta}-\\beta\\|=O_{p}\\left(n^{-1 / 2}\\right)\\). However if the regressor does not have bounded support then we have to be more careful.\nThe key is Theorem \\(6.15\\) which shows that \\(\\mathbb{E}\\|X\\|^{r}<\\infty\\) implies \\(X_{i}=o_{p}\\left(n^{1 / r}\\right)\\) uniformly in \\(i\\), or\n\\[\nn^{-1 / r} \\max _{1 \\leq i \\leq n}\\left\\|X_{i}\\right\\| \\underset{p}{\\longrightarrow} 0 .\n\\]\nApplied to (7.40) we obtain\n\\[\n\\max _{1 \\leq i \\leq n}\\left|\\widehat{e}_{i}-e_{i}\\right| \\leq \\max _{1 \\leq i \\leq n}\\left\\|X_{i}\\right\\|\\|\\widehat{\\beta}-\\beta\\|=o_{p}\\left(n^{-1 / 2+1 / r}\\right) .\n\\]\nWe have shown the following.\nTheorem 7.16 Under Assumption \\(7.2\\) and \\(\\mathbb{E}\\|X\\|^{r}<\\infty\\), then\n\\[\n\\max _{1 \\leq i \\leq n}\\left|\\widehat{e}_{i}-e_{i}\\right|=o_{p}\\left(n^{-1 / 2+1 / r}\\right) .\n\\]\nThe rate of convergence in (7.41) depends on \\(r\\). Assumption \\(7.2\\) requires \\(r \\geq 4\\) so the rate of convergence is at least \\(o_{p}\\left(n^{-1 / 4}\\right)\\). As \\(r\\) increases the rate improves.\nWe mentioned in Section \\(7.7\\) that there are multiple ways to prove the consistency of the covariance matrix estimator \\(\\widehat{\\Omega}\\). We now show that Theorem \\(7.16\\) provides one simple method to establish (7.23) and thus Theorem 7.6. Let \\(q_{n}=\\max _{1 \\leq i \\leq n}\\left|\\widehat{e}_{i}-e_{i}\\right|=o_{p}\\left(n^{-1 / 4}\\right)\\). Since \\(\\widehat{e}_{i}^{2}-e_{i}^{2}=2 e_{i}\\left(\\widehat{e}_{i}-e_{i}\\right)+\\left(\\widehat{e}_{i}-e_{i}\\right)^{2}\\), then\n\\[\n\\begin{aligned}\n\\left\\|\\frac{1}{n} \\sum_{i=1}^{n} X_{i} X_{i}^{\\prime}\\left(\\widehat{e}_{i}^{2}-e_{i}^{2}\\right)\\right\\| & \\leq \\frac{1}{n} \\sum_{i=1}^{n}\\left\\|X_{i} X_{i}^{\\prime}\\right\\|\\left|\\widehat{e}_{i}^{2}-e_{i}^{2}\\right| \\\\\n& \\leq \\frac{2}{n} \\sum_{i=1}^{n}\\left\\|X_{i}\\right\\|^{2}\\left|e _ { i } \\left\\|\\widehat{e}_{i}-e_{i}\\left|+\\frac{1}{n} \\sum_{i=1}^{n}\\left\\|X_{i}\\right\\|^{2}\\right| \\widehat{e}_{i}-\\left.e_{i}\\right|^{2}\\right.\\right.\\\\\n& \\leq \\frac{2}{n} \\sum_{i=1}^{n}\\left\\|X_{i}\\right\\|^{2}\\left|e_{i}\\right| q_{n}+\\frac{1}{n} \\sum_{i=1}^{n}\\left\\|X_{i}\\right\\|^{2} q_{n}^{2} \\\\\n& \\leq o_{p}\\left(n^{-1 / 4}\\right) .\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "chpt07-asymptotic-ls.html#asymptotic-leverage",
    "href": "chpt07-asymptotic-ls.html#asymptotic-leverage",
    "title": "7  Asymptotic Theory for Least Squares",
    "section": "7.19 Asymptotic Leverage*",
    "text": "7.19 Asymptotic Leverage*\nRecall the definition of leverage from (3.40) \\(h_{i i}=X_{i}^{\\prime}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} X_{i}\\). These are the diagonal elements of the projection matrix \\(\\boldsymbol{P}\\) and appear in the formula for leave-one-out prediction errors and \\(\\mathrm{HC} 2\\) and HC3 covariance matrix estimators. We can show that under i.i.d. sampling the leverage values are uniformly asymptotically small.\nLet \\(\\lambda_{\\min }(\\boldsymbol{A})\\) and \\(\\lambda_{\\max }(\\boldsymbol{A})\\) denote the smallest and largest eigenvalues of a symmetric square matrix \\(\\boldsymbol{A}\\) and note that \\(\\lambda_{\\max }\\left(\\boldsymbol{A}^{-1}\\right)=\\left(\\lambda_{\\min }(\\boldsymbol{A})\\right)^{-1}\\). Since \\(\\frac{1}{n} \\boldsymbol{X}^{\\prime} \\boldsymbol{X} \\underset{p}{\\rightarrow} \\boldsymbol{Q}_{X X}>0\\), by the CMT \\(\\lambda_{\\min }\\left(\\frac{1}{n} \\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right) \\underset{p}{\\rightarrow} \\lambda_{\\min }\\left(\\boldsymbol{Q}_{X X}\\right)>0\\). (The latter is positive since \\(\\boldsymbol{Q}_{X X}\\) is positive definite and thus all its eigenvalues are positive.) Then by the Quadratic Inequality (B.18)\n\\[\n\\begin{aligned}\nh_{i i} &=X_{i}^{\\prime}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} X_{i} \\\\\n& \\leq \\lambda_{\\max }\\left(\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1}\\right)\\left(X_{i}^{\\prime} X_{i}\\right) \\\\\n&=\\left(\\lambda_{\\min }\\left(\\frac{1}{n} \\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)\\right)^{-1} \\frac{1}{n}\\left\\|X_{i}\\right\\|^{2} \\\\\n& \\leq\\left(\\lambda_{\\min }\\left(\\boldsymbol{Q}_{X X}\\right)+o_{p}(1)\\right)^{-1} \\frac{1}{n} \\max _{1 \\leq i \\leq n}\\left\\|X_{i}\\right\\|^{2} .\n\\end{aligned}\n\\]\nTheorem \\(6.15\\) shows that \\(\\mathbb{E}\\|X\\|^{r}<\\infty\\) implies \\(\\max _{1 \\leq i \\leq n}\\left\\|X_{i}\\right\\|^{2}=\\left(\\max _{1 \\leq i \\leq n}\\left\\|X_{i}\\right\\|\\right)^{2}=o_{p}\\left(n^{2 / r}\\right)\\) and thus (7.42) is \\(o_{p}\\left(n^{2 / r-1}\\right)\\)\nTheorem 7.17 If \\(X_{i}\\) is i.i.d., \\(\\boldsymbol{Q}_{X X}>0\\), and \\(\\mathbb{E}\\|X\\|^{r}<\\infty\\) for some \\(r \\geq 2\\), then \\(\\max _{1 \\leq i \\leq n} h_{i i}=o_{p}\\left(n^{2 / r-1}\\right)\\).\nFor any \\(r \\geq 2\\) then \\(h_{i i}=o_{p}\\) (1) (uniformly in \\(i \\leq n\\) ). Larger \\(r\\) implies a faster rate of convergence. For example \\(r=4\\) implies \\(h_{i i}=o_{p}\\left(n^{-1 / 2}\\right)\\).\nTheorem (7.17) implies that under random sampling with finite variances and large samples no individual observation should have a large leverage value. Consequently, individual observations should not be influential unless one of these conditions is violated."
  },
  {
    "objectID": "chpt07-asymptotic-ls.html#exercises",
    "href": "chpt07-asymptotic-ls.html#exercises",
    "title": "7  Asymptotic Theory for Least Squares",
    "section": "7.20 Exercises",
    "text": "7.20 Exercises\nExercise 7.1 Take the model \\(Y=X_{1}^{\\prime} \\beta_{1}+X_{2}^{\\prime} \\beta_{2}+e\\) with \\(\\mathbb{E}[X e]=0\\). Suppose that \\(\\beta_{1}\\) is estimated by regressing \\(Y\\) on \\(X_{1}\\) only. Find the probability limit of this estimator. In general, is it consistent for \\(\\beta_{1}\\) ? If not, under what conditions is this estimator consistent for \\(\\beta_{1}\\) ?\nExercise 7.2 Take the model \\(Y=X^{\\prime} \\beta+e\\) with \\(\\mathbb{E}[X e]=0\\). Define the ridge regression estimator\n\\[\n\\widehat{\\beta}=\\left(\\sum_{i=1}^{n} X_{i} X_{i}^{\\prime}+\\lambda \\boldsymbol{I}_{k}\\right)^{-1}\\left(\\sum_{i=1}^{n} X_{i} Y_{i}\\right)\n\\]\nhere \\(\\lambda>0\\) is a fixed constant. Find the probability limit of \\(\\widehat{\\beta}\\) as \\(n \\rightarrow \\infty\\). Is \\(\\widehat{\\beta}\\) consistent for \\(\\beta\\) ?\nExercise 7.3 For the ridge regression estimator (7.43), set \\(\\lambda=c n\\) where \\(c>0\\) is fixed as \\(n \\rightarrow \\infty\\). Find the probability limit of \\(\\widehat{\\beta}\\) as \\(n \\rightarrow \\infty\\). Exercise 7.4 Verify some of the calculations reported in Section 7.4. Specifically, suppose that \\(X_{1}\\) and \\(X_{2}\\) only take the values \\(\\{-1,+1\\}\\), symmetrically, with\n\\[\n\\begin{aligned}\n\\mathbb{P}\\left[X_{1}=X_{2}=1\\right] &=\\mathbb{P}\\left[X_{1}=X_{2}=-1\\right]=3 / 8 \\\\\n\\mathbb{P}\\left[X_{1}=1, X_{2}=-1\\right] &=\\mathbb{P}\\left[X_{1}=-1, X_{2 i}=1\\right]=1 / 8 \\\\\n\\mathbb{E}\\left[e_{i}^{2} \\mid X_{1}=X_{2}\\right] &=\\frac{5}{4} \\\\\n\\mathbb{E}\\left[e_{i}^{2} \\mid X_{1} \\neq X_{2}\\right] &=\\frac{1}{4} .\n\\end{aligned}\n\\]\nVerify the following:\\ (a) \\(\\mathbb{E}\\left[X_{1}\\right]=0\\)\\ (b) \\(\\mathbb{E}\\left[X_{1}^{2}\\right]=1\\)\\ (c) \\(\\mathbb{E}\\left[X_{1} X_{2}\\right]=\\frac{1}{2}\\)\\ (d) \\(\\mathbb{E}\\left[e^{2}\\right]=1\\)\\ (e) \\(\\mathbb{E}\\left[X_{1}^{2} e^{2}\\right]=1\\)\\ (f) \\(\\mathbb{E}\\left[X_{1} X_{2} e^{2}\\right]=\\frac{7}{8}\\).\nExercise 7.5 Show (7.13)-(7.16).\nExercise \\(7.6\\) The model is\n\\[\n\\begin{aligned}\nY &=X^{\\prime} \\beta+e \\\\\n\\mathbb{E}[X e] &=0 \\\\\n\\Omega &=\\mathbb{E}\\left[X X^{\\prime} e^{2}\\right] .\n\\end{aligned}\n\\]\nFind the method of moments estimators \\((\\widehat{\\beta}, \\widehat{\\Omega})\\) for \\((\\beta, \\Omega)\\).\nExercise 7.7 Of the variables \\(\\left(Y^{*}, Y, X\\right)\\) only the pair \\((Y, X)\\) are observed. In this case we say that \\(Y^{*}\\) is a latent variable. Suppose\n\\[\n\\begin{aligned}\nY^{*} &=X^{\\prime} \\beta+e \\\\\n\\mathbb{E}[X e] &=0 \\\\\nY &=Y^{*}+u\n\\end{aligned}\n\\]\nwhere \\(u\\) is a measurement error satisfying\n\\[\n\\begin{aligned}\n\\mathbb{E}[X u] &=0 \\\\\n\\mathbb{E}\\left[Y^{*} u\\right] &=0 .\n\\end{aligned}\n\\]\nLet \\(\\widehat{\\beta}\\) denote the OLS coefficient from the regression of \\(Y\\) on \\(X\\).\n\nIs \\(\\beta\\) the coefficient from the linear projection of \\(Y\\) on \\(X\\) ? (b) Is \\(\\widehat{\\beta}\\) consistent for \\(\\beta\\) as \\(n \\rightarrow \\infty\\) ?\nFind the asymptotic distribution of \\(\\sqrt{n}(\\widehat{\\beta}-\\beta)\\) as \\(n \\rightarrow \\infty\\).\n\nExercise 7.8 Find the asymptotic distribution of \\(\\sqrt{n}\\left(\\widehat{\\sigma}^{2}-\\sigma^{2}\\right)\\) as \\(n \\rightarrow \\infty\\).\nExercise 7.9 The model is \\(Y=X \\beta+e\\) with \\(\\mathbb{E}[e \\mid X]=0\\) and \\(X \\in \\mathbb{R}\\). Consider the two estimators\n\\[\n\\begin{aligned}\n&\\widehat{\\beta}=\\frac{\\sum_{i=1}^{n} X_{i} Y_{i}}{\\sum_{i=1}^{n} X_{i}^{2}} \\\\\n&\\widetilde{\\beta}=\\frac{1}{n} \\sum_{i=1}^{n} \\frac{Y_{i}}{X_{i}} .\n\\end{aligned}\n\\]\n\nUnder the stated assumptions are both estimators consistent for \\(\\beta\\) ?\nAre there conditions under which either estimator is efficient?\n\nExercise 7.10 In the homoskedastic regression model \\(Y=X^{\\prime} \\beta+e\\) with \\(\\mathbb{E}[e \\mid x]=0\\) and \\(\\mathbb{E}\\left[e^{2} \\mid X\\right]=\\sigma^{2}\\) suppose \\(\\widehat{\\beta}\\) is the OLS estimator of \\(\\beta\\) with covariance matrix estimator \\(\\widehat{\\boldsymbol{V}}_{\\widehat{\\beta}}\\) based on a sample of size \\(n\\). Let \\(\\widehat{\\sigma}^{2}\\) be the estimator of \\(\\sigma^{2}\\). You wish to forecast an out-of-sample value of \\(Y_{n+1}\\) given that \\(X_{n+1}=x\\). Thus the available information is the sample, the estimates \\(\\left(\\widehat{\\beta}, \\widehat{\\boldsymbol{V}}_{\\widehat{\\beta}}, \\widehat{\\sigma}^{2}\\right)\\), the residuals \\(\\widehat{e}_{i}\\), and the out-of-sample value of the regressors \\(X_{n+1}\\).\n\nFind a point forecast of \\(Y_{n+1}\\).\nFind an estimator of the variance of this forecast.\n\nExercise 7.11 Take a regression model with i.i.d. observations \\(\\left(Y_{i}, X_{i}\\right)\\) with \\(X \\in \\mathbb{R}\\)\n\\[\n\\begin{aligned}\nY &=X \\beta+e \\\\\n\\mathbb{E}[e \\mid X] &=0 \\\\\n\\Omega &=\\mathbb{E}\\left[X^{2} e^{2}\\right] .\n\\end{aligned}\n\\]\nLet \\(\\widehat{\\beta}\\) be the OLS estimator of \\(\\beta\\) with residuals \\(\\widehat{e}_{i}=Y_{i}-X_{i} \\widehat{\\beta}\\). Consider the estimators of \\(\\Omega\\)\n\\[\n\\begin{aligned}\n&\\widetilde{\\Omega}=\\frac{1}{n} \\sum_{i=1}^{n} X_{i}^{2} e_{i}^{2} \\\\\n&\\widehat{\\Omega}=\\frac{1}{n} \\sum_{i=1}^{n} X_{i}^{2} \\widehat{e}_{i}^{2} .\n\\end{aligned}\n\\]\n\nFind the asymptotic distribution of \\(\\sqrt{n}(\\widetilde{\\Omega}-\\Omega)\\) as \\(n \\rightarrow \\infty\\).\nFind the asymptotic distribution of \\(\\sqrt{n}(\\widehat{\\Omega}-\\Omega)\\) as \\(n \\rightarrow \\infty\\).\nHow do you use the regression assumption \\(\\mathbb{E}\\left[e_{i} \\mid X_{i}\\right]=0\\) in your answer to (b)?\n\nExercise 7.12 Consider the model\n\\[\n\\begin{aligned}\nY &=\\alpha+\\beta X+e \\\\\n\\mathbb{E}[e] &=0 \\\\\n\\mathbb{E}[X e] &=0\n\\end{aligned}\n\\]\nwith both \\(Y\\) and \\(X\\) scalar. Assuming \\(\\alpha>0\\) and \\(\\beta<0\\) suppose the parameter of interest is the area under the regression curve (e.g. consumer surplus), which is \\(A=-\\alpha^{2} / 2 \\beta\\).\nLet \\(\\widehat{\\theta}=(\\widehat{\\alpha}, \\widehat{\\beta})^{\\prime}\\) be the least squares estimators of \\(\\theta=(\\alpha, \\beta)^{\\prime}\\) so that \\(\\sqrt{n}(\\widehat{\\theta}-\\theta) \\rightarrow{ }_{d} N\\left(0, \\boldsymbol{V}_{\\theta}\\right)\\) and let \\(\\widehat{\\boldsymbol{V}}_{\\theta}\\) be a standard estimator for \\(\\boldsymbol{V}_{\\theta}\\).\n\nGiven the above, describe an estimator of \\(A\\).\nConstruct an asymptotic \\(1-\\eta\\) confidence interval for \\(A\\).\n\nExercise 7.13 Consider an i.i.d. sample \\(\\left\\{Y_{i}, X_{i}\\right\\} i=1, \\ldots, n\\) where \\(Y\\) and \\(X\\) are scalar. Consider the reverse projection model \\(X=Y \\gamma+u\\) with \\(\\mathbb{E}[Y u]=0\\) and define the parameter of interest as \\(\\theta=1 / \\gamma\\).\n\nPropose an estimator \\(\\widehat{\\gamma}\\) of \\(\\gamma\\).\nPropose an estimator \\(\\widehat{\\theta}\\) of \\(\\theta\\).\nFind the asymptotic distribution of \\(\\widehat{\\theta}\\).\nFind an asymptotic standard error for \\(\\widehat{\\theta}\\).\n\nExercise 7.14 Take the model\n\\[\n\\begin{aligned}\nY &=X_{1} \\beta_{1}+X_{2} \\beta_{2}+e \\\\\n\\mathbb{E}[X e] &=0\n\\end{aligned}\n\\]\nwith both \\(\\beta_{1} \\in \\mathbb{R}\\) and \\(\\beta_{2} \\in \\mathbb{R}\\), and define the parameter \\(\\theta=\\beta_{1} \\beta_{2}\\).\n\nWhat is the appropriate estimator \\(\\widehat{\\theta}\\) for \\(\\theta\\) ?\nFind the asymptotic distribution of \\(\\widehat{\\theta}\\) under standard regularity conditions.\nShow how to calculate an asymptotic \\(95 %\\) confidence interval for \\(\\theta\\).\n\nExercise 7.15 Take the linear model \\(Y=X \\beta+e\\) with \\(\\mathbb{E}[e \\mid X]=0\\) and \\(X \\in \\mathbb{R}\\). Consider the estimator\n\\[\n\\widehat{\\beta}=\\frac{\\sum_{i=1}^{n} X_{i}^{3} Y_{i}}{\\sum_{i=1}^{n} X_{i}^{4}}\n\\]\nFind the asymptotic distribution of \\(\\sqrt{n}(\\widehat{\\beta}-\\beta)\\) as \\(n \\rightarrow \\infty\\).\nExercise 7.16 From an i.i.d. sample \\(\\left(Y_{i}, X_{i}\\right)\\) of size \\(n\\) you randomly take half the observations. You estimate a least squares regression of \\(Y\\) on \\(X\\) using only this sub-sample. Is the estimated slope coefficient \\(\\widehat{\\beta}\\) consistent for the population projection coefficient? Explain your reasoning.\nExercise 7.17 An economist reports a set of parameter estimates, including the coefficient estimates \\(\\widehat{\\beta}_{1}=1.0, \\widehat{\\beta}_{2}=0.8\\), and standard errors \\(s\\left(\\widehat{\\beta}_{1}\\right)=0.07\\) and \\(s\\left(\\widehat{\\beta}_{2}\\right)=0.07\\). The author writes “The estimates show that \\(\\beta_{1}\\) is larger than \\(\\beta_{2} . \"\\)\n\nWrite down the formula for an asymptotic 95% confidence interval for \\(\\theta=\\beta_{1}-\\beta_{2}\\), expressed as a function of \\(\\widehat{\\beta}_{1}, \\widehat{\\beta}_{2}, s\\left(\\widehat{\\beta}_{1}\\right), s\\left(\\widehat{\\beta}_{2}\\right)\\) and \\(\\widehat{\\rho}\\), where \\(\\widehat{\\rho}\\) is the estimated correlation between \\(\\widehat{\\beta}_{1}\\) and \\(\\widehat{\\beta}_{2}\\).\nCan \\(\\widehat{\\rho}\\) be calculated from the reported information? (c) Is the author correct? Does the reported information support the author’s claim?\n\nExercise 7.18 Suppose an economic model suggests\n\\[\nm(x)=\\mathbb{E}[Y \\mid X=x]=\\beta_{0}+\\beta_{1} x+\\beta_{2} x^{2}\n\\]\nwhere \\(X \\in \\mathbb{R}\\). You have a random sample \\(\\left(Y_{i}, X_{i}\\right), i=1, \\ldots, n\\).\n\nDescribe how to estimate \\(m(x)\\) at a given value \\(x\\).\nDescribe (be specific) an appropriate confidence interval for \\(m(x)\\).\n\nExercise 7.19 Take the model \\(Y=X^{\\prime} \\beta+e\\) with \\(\\mathbb{E}[X e]=0\\) and suppose you have observations \\(i=1, \\ldots, 2 n\\). (The number of observations is \\(2 n\\).) You randomly split the sample in half, (each has \\(n\\) observations), calculate \\(\\widehat{\\beta}_{1}\\) by least squares on the first sample, and \\(\\widehat{\\beta}_{2}\\) by least squares on the second sample. What is the asymptotic distribution of \\(\\sqrt{n}\\left(\\widehat{\\beta}_{1}-\\widehat{\\beta}_{2}\\right)\\) ?\nExercise 7.20 The variables \\(\\left\\{Y_{i}, X_{i}, W_{i}\\right\\}\\) are a random sample. The parameter \\(\\beta\\) is estimated by minimizing the criterion function\n\\[\nS(\\beta)=\\sum_{i=1}^{n} W_{i}\\left(Y_{i}-X_{i}^{\\prime} \\beta\\right)^{2}\n\\]\nThat is \\(\\widehat{\\beta}=\\operatorname{argmin}_{\\beta} S(\\beta)\\).\n\nFind an explicit expression for \\(\\widehat{\\beta}\\).\nWhat population parameter \\(\\beta\\) is \\(\\widehat{\\beta}\\) estimating? Be explicit about any assumptions you need to impose. Do not make more assumptions than necessary.\nFind the probability limit for \\(\\widehat{\\beta}\\) as \\(n \\rightarrow \\infty\\).\nFind the asymptotic distribution of \\(\\sqrt{n}(\\widehat{\\beta}-\\beta)\\) as \\(n \\rightarrow \\infty\\).\n\nExercise 7.21 Take the model\n\\[\n\\begin{aligned}\nY &=X^{\\prime} \\beta+e \\\\\n\\mathbb{E}[e \\mid X] &=0 \\\\\n\\mathbb{E}\\left[e^{2} \\mid X\\right] &=Z^{\\prime} \\gamma\n\\end{aligned}\n\\]\nwhere \\(Z\\) is a (vector) function of \\(X\\). The sample is \\(i=1, \\ldots, n\\) with i.i.d. observations. Assume that \\(Z^{\\prime} \\gamma>0\\) for all \\(Z\\). Suppose you want to forecast \\(Y_{n+1}\\) given \\(X_{n+1}=x\\) and \\(Z_{n+1}=z\\) for an out-of-sample observation \\(n+1\\). Describe how you would construct a point forecast and a forecast interval for \\(Y_{n+1}\\).\nExercise 7.22 Take the model\n\\[\n\\begin{aligned}\nY &=X^{\\prime} \\beta+e \\\\\n\\mathbb{E}[e \\mid X] &=0 \\\\\nZ &=X^{\\prime} \\beta \\gamma+u \\\\\n\\mathbb{E}[u \\mid X] &=0\n\\end{aligned}\n\\]\nwhere \\(X\\) is a \\(k\\) vector and \\(Z\\) is scalar. Your goal is to estimate the scalar parameter \\(\\gamma\\). You use a two-step estimator: - Estimate \\(\\widehat{\\beta}\\) by least squares of \\(Y\\) on \\(X\\).\n\nEstimate \\(\\widehat{\\gamma}\\) by least squares of \\(Z\\) on \\(X^{\\prime} \\widehat{\\beta}\\).\n\n\nShow that \\(\\widehat{\\gamma}\\) is consistent for \\(\\gamma\\).\nFind the asymptotic distribution of \\(\\widehat{\\gamma}\\) when \\(\\gamma=0\\)\n\nExercise 7.23 The model is \\(Y=X+e\\) with \\(\\mathbb{E}[e \\mid X]=0\\) and \\(X \\in \\mathbb{R}\\). Consider the estimator\n\\[\n\\widetilde{\\beta}=\\frac{1}{n} \\sum_{i=1}^{n} \\frac{Y_{i}}{X_{i}} .\n\\]\nFind conditions under which \\(\\widetilde{\\beta}\\) is consistent for \\(\\beta\\) as \\(n \\rightarrow \\infty\\).\nExercise 7.24 The parameter \\(\\beta\\) is defined in the model \\(Y=X^{*} \\beta+e\\) where \\(e\\) is independent of \\(X^{*} \\geq 0\\), \\(\\mathbb{E}[e]=0, \\mathbb{E}\\left[e^{2}\\right]=\\sigma^{2}\\). The observables are \\((Y, X)\\) where \\(X=X^{*} v\\) and \\(v>0\\) is random scale measurement error, independent of \\(X^{*}\\) and \\(e\\). Consider the least squares estimator \\(\\widehat{\\beta}\\) for \\(\\beta\\).\n\nFind the plim of \\(\\widehat{\\beta}\\) expressed in terms of \\(\\beta\\) and moments of \\((X, v, e)\\).\nCan you find a non-trivial condition under which \\(\\widehat{\\beta}\\) is consistent for \\(\\beta\\) ? (By non-trivial we mean something other than \\(v=1\\).)\n\nExercise 7.25 Take the projection model \\(Y=X^{\\prime} \\beta+e\\) with \\(\\mathbb{E}[X e]=0\\). For a positive function \\(w(x)\\) let \\(W_{i}=w\\left(X_{i}\\right)\\). Consider the estimator\n\\[\n\\widetilde{\\beta}=\\left(\\sum_{i=1}^{n} W_{i} X_{i} X_{i}^{\\prime}\\right)^{-1}\\left(\\sum_{i=1}^{n} W_{i} X_{i} Y_{i}\\right) .\n\\]\nFind the probability limit (as \\(n \\rightarrow \\infty\\) ) of \\(\\widetilde{\\beta}\\). Do you need to add an assumption? Is \\(\\widetilde{\\beta}\\) consistent for \\(\\widetilde{\\beta}\\) ? If not, under what assumption is \\(\\widetilde{\\beta}\\) consistent for \\(\\beta\\) ?\nExercise 7.26 Take the regression model\n\\[\n\\begin{aligned}\nY &=X^{\\prime} \\beta+e \\\\\n\\mathbb{E}[e \\mid X] &=0 \\\\\n\\mathbb{E}\\left[e^{2} \\mid X=x\\right] &=\\sigma^{2}(x)\n\\end{aligned}\n\\]\nwith \\(X \\in \\mathbb{R}^{k}\\). Assume that \\(\\mathbb{P}[e=0]=0\\). Consider the infeasible estimator\n\\[\n\\widetilde{\\beta}=\\left(\\sum_{i=1}^{n} e_{i}^{-2} X_{i} X_{i}^{\\prime}\\right)^{-1}\\left(\\sum_{i=1}^{n} e_{i}^{-2} X_{i} Y_{i}\\right) .\n\\]\nThis is a WLS estimator using the weights \\(e_{i}^{-2}\\).\n\nFind the asymptotic distribution of \\(\\widetilde{\\beta}\\).\nContrast your result with the asymptotic distribution of infeasible GLS. Exercise 7.27 The model is \\(Y=X^{\\prime} \\beta+e\\) with \\(\\mathbb{E}[e \\mid X]=0\\). An econometrician is worried about the impact of some unusually large values of the regressors. The model is thus estimated on the subsample for which \\(\\left|X_{i}\\right| \\leq c\\) for some fixed \\(c\\). Let \\(\\widetilde{\\beta}\\) denote the OLS estimator on this subsample. It equals\n\n\\[\n\\widetilde{\\beta}=\\left(\\sum_{i=1}^{n} X_{i} X_{i}^{\\prime} \\mathbb{1}\\left\\{\\left|X_{i}\\right| \\leq c\\right\\}\\right)^{-1}\\left(\\sum_{i=1}^{n} X_{i} Y_{i} \\mathbb{1}\\left\\{\\left|X_{i}\\right| \\leq c\\right\\}\\right) .\n\\]\n\nShow that \\(\\widetilde{\\beta} \\underset{p}{\\longrightarrow} \\beta\\).\nFind the asymptotic distribution of \\(\\sqrt{n}(\\widetilde{\\beta}-\\beta)\\).\n\nExercise 7.28 As in Exercise 3.26, use the cps09mar dataset and the subsample of white male Hispanics. Estimate the regression\n\\[\n\\widehat{\\log (\\text { wage })}=\\beta_{1} \\text { education }+\\beta_{2} \\text { experience }+\\beta_{3} \\text { experience }^{2} / 100+\\beta_{4} .\n\\]\n\nReport the coefficient estimates and robust standard errors.\nLet \\(\\theta\\) be the ratio of the return to one year of education to the return to one year of experience for experience \\(=10\\). Write \\(\\theta\\) as a function of the regression coefficients and variables. Compute \\(\\widehat{\\theta}\\) from the estimated model.\nWrite out the formula for the asymptotic standard error for \\(\\hat{\\theta}\\) as a function of the covariance matrix for \\(\\widehat{\\beta}\\). Compute \\(s(\\widehat{\\theta})\\) from the estimated model.\nConstruct a \\(90 %\\) asymptotic confidence interval for \\(\\theta\\) from the estimated model.\nCompute the regression function at education \\(=12\\) and experience \\(=20\\). Compute a 95% confidence interval for the regression function at this point.\nConsider an out-of-sample individual with 16 years of education and 5 years experience. Construct an \\(80 %\\) forecast interval for their log wage and wage. [To obtain the forecast interval for the wage, apply the exponential function to both endpoints.]"
  },
  {
    "objectID": "chpt08-restricted-est.html",
    "href": "chpt08-restricted-est.html",
    "title": "8  Restricted Estimation",
    "section": "",
    "text": "In the linear projection model\n\\[\n\\begin{aligned}\nY &=X^{\\prime} \\beta+e \\\\\n\\mathbb{E}[X e] &=0\n\\end{aligned}\n\\]\na common task is to impose a constraint on the coefficient vector \\(\\beta\\). For example, partitioning \\(X^{\\prime}=\\) \\(\\left(X_{1}^{\\prime}, X_{2}^{\\prime}\\right)\\) and \\(\\beta^{\\prime}=\\left(\\beta_{1}^{\\prime}, \\beta_{2}^{\\prime}\\right)\\) a typical constraint is an exclusion restriction of the form \\(\\beta_{2}=0\\). In this case the constrained model is\n\\[\n\\begin{aligned}\nY &=X_{1}^{\\prime} \\beta_{1}+e \\\\\n\\mathbb{E}[X e] &=0 .\n\\end{aligned}\n\\]\nAt first glance this appears the same as the linear projection model but there is one important difference: the error \\(e\\) is uncorrelated with the entire regressor vector \\(X^{\\prime}=\\left(X_{1}^{\\prime}, X_{2}^{\\prime}\\right)\\) not just the included regressor \\(X_{1}\\).\nIn general, a set of \\(q\\) linear constraints on \\(\\beta\\) takes the form\n\\[\n\\boldsymbol{R}^{\\prime} \\beta=\\boldsymbol{c}\n\\]\nwhere \\(\\boldsymbol{R}\\) is \\(k \\times q, \\operatorname{rank}(\\boldsymbol{R})=q<k\\), and \\(\\boldsymbol{c}\\) is \\(q \\times 1\\). The assumption that \\(\\boldsymbol{R}\\) is full rank means that the constraints are linearly independent (there are no redundant or contradictory constraints). We define the restricted parameter space \\(B\\) as the set of values of \\(\\beta\\) which satisfy (8.1), that is\n\\[\nB=\\left\\{\\beta: \\boldsymbol{R}^{\\prime} \\beta=\\boldsymbol{c}\\right\\} .\n\\]\nSometimes we will call (8.1) a constraint and sometimes a restriction. They are the same thing. Similarly sometimes we will call estimators which satisfy (8.1) constrained estimators and sometimes restricted estimators. They mean the same thing.\nThe constraint \\(\\beta_{2}=0\\) discussed above is a special case of the constraint (8.1) with\n\\[\n\\boldsymbol{R}=\\left(\\begin{array}{c}\n0 \\\\\n\\boldsymbol{I}_{k_{2}}\n\\end{array}\\right)\n\\]\na selector matrix, and \\(\\boldsymbol{c}=0 .\\) Another common restriction is that a set of coefficients sum to a known constant, i.e. \\(\\beta_{1}+\\beta_{2}=1\\). For example, this constraint arises in a constant-return-to-scale production function. Other common restrictions include the equality of coefficients \\(\\beta_{1}=\\beta_{2}\\), and equal and offsetting coefficients \\(\\beta_{1}=-\\beta_{2}\\).\nA typical reason to impose a constraint is that we believe (or have information) that the constraint is true. By imposing the constraint we hope to improve estimation efficiency. The goal is to obtain consistent estimates with reduced variance relative to the unconstrained estimator.\nThe questions then arise: How should we estimate the coefficient vector \\(\\beta\\) imposing the linear restriction (8.1)? If we impose such constraints what is the sampling distribution of the resulting estimator? How should we calculate standard errors? These are the questions explored in this chapter."
  },
  {
    "objectID": "chpt08-restricted-est.html#constrained-least-squares",
    "href": "chpt08-restricted-est.html#constrained-least-squares",
    "title": "8  Restricted Estimation",
    "section": "8.2 Constrained Least Squares",
    "text": "8.2 Constrained Least Squares\nAn intuitively appealing method to estimate a constrained linear projection is to minimize the least squares criterion subject to the constraint \\(\\boldsymbol{R}^{\\prime} \\beta=\\boldsymbol{c}\\).\nThe constrained least squares estimator is\n\\[\n\\widetilde{\\beta}_{\\mathrm{cls}}=\\underset{\\boldsymbol{R}^{\\prime} \\beta=\\boldsymbol{c}}{\\operatorname{argmin}} \\operatorname{SSE}(\\beta)\n\\]\nwhere\n\\[\n\\operatorname{SSE}(\\beta)=\\sum_{i=1}^{n}\\left(Y_{i}-X_{i}^{\\prime} \\beta\\right)^{2}=\\boldsymbol{Y}^{\\prime} \\boldsymbol{Y}-2 \\boldsymbol{Y}^{\\prime} \\boldsymbol{X} \\beta+\\beta^{\\prime} \\boldsymbol{X}^{\\prime} \\boldsymbol{X} \\beta\n\\]\nThe estimator \\(\\widetilde{\\beta}_{\\text {cls }}\\) minimizes the sum of squared errors over all \\(\\beta \\in B\\), or equivalently such that the restriction (8.1) holds. We call \\(\\widetilde{\\beta}_{\\text {cls }}\\) the constrained least squares (CLS) estimator. We use the convention of using a tilde ” \\(\\sim\\) ” rather than a hat ” \\(\\wedge\\) ” to indicate that \\(\\widetilde{\\beta}_{\\text {cls }}\\) is a restricted estimator in contrast to the unrestricted least squares estimator \\(\\widehat{\\beta}\\) and write it as \\(\\widetilde{\\beta}_{\\text {cls }}\\) to be clear that the estimation method is CLS.\nOne method to find the solution to (8.3) is the technique of Lagrange multipliers. The problem (8.3) is equivalent to finding the critical points of the Lagrangian\n\\[\n\\mathscr{L}(\\beta, \\lambda)=\\frac{1}{2} \\operatorname{SSE}(\\beta)+\\lambda^{\\prime}\\left(\\boldsymbol{R}^{\\prime} \\beta-\\boldsymbol{c}\\right)\n\\]\nover \\((\\beta, \\lambda)\\) where \\(\\lambda\\) is an \\(s \\times 1\\) vector of Lagrange multipliers. The solution is a saddlepoint. The Lagrangian is minimized over \\(\\beta\\) while maximized over \\(\\lambda\\). The first-order conditions for the solution of (8.5) are\n\\[\n\\frac{\\partial}{\\partial \\beta} \\mathscr{L}\\left(\\widetilde{\\beta}_{\\mathrm{cls}}, \\widetilde{\\lambda}_{\\mathrm{cls}}\\right)=-\\boldsymbol{X}^{\\prime} \\boldsymbol{Y}+\\boldsymbol{X}^{\\prime} \\boldsymbol{X} \\widetilde{\\beta}_{\\mathrm{cls}}+\\boldsymbol{R} \\widetilde{\\lambda}_{\\mathrm{cls}}=0\n\\]\nand\n\\[\n\\frac{\\partial}{\\partial \\lambda} \\mathscr{L}\\left(\\widetilde{\\beta}_{\\mathrm{cls}}, \\widetilde{\\lambda}_{\\mathrm{cls}}\\right)=\\boldsymbol{R}^{\\prime} \\widetilde{\\beta}-\\boldsymbol{c}=0\n\\]\nPremultiplying (8.6) by \\(\\boldsymbol{R}^{\\prime}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1}\\) we obtain\n\\[\n-\\boldsymbol{R}^{\\prime} \\widehat{\\beta}+\\boldsymbol{R}^{\\prime} \\widetilde{\\beta}_{\\mathrm{cls}}+\\boldsymbol{R}^{\\prime}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\boldsymbol{R} \\tilde{\\lambda}_{\\text {cls }}=0\n\\]\nwhere \\(\\widehat{\\beta}=\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\boldsymbol{X}^{\\prime} \\boldsymbol{Y}\\) is the unrestricted least squares estimator. Imposing \\(\\boldsymbol{R}^{\\prime} \\widetilde{\\beta}_{\\text {cls }}-\\boldsymbol{c}=0\\) from (8.7) and solving for \\(\\widetilde{\\lambda}_{\\text {cls we find }}\\)\n\\[\n\\tilde{\\lambda}_{\\text {cls }}=\\left[\\boldsymbol{R}^{\\prime}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\boldsymbol{R}\\right]^{-1}\\left(\\boldsymbol{R}^{\\prime} \\widehat{\\beta}-\\boldsymbol{c}\\right) .\n\\]\nNotice that \\(\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1}>0\\) and \\(\\boldsymbol{R}\\) full rank imply that \\(\\boldsymbol{R}^{\\prime}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\boldsymbol{R}>0\\) and is hence invertible. (See Section A.10.) Substituting this expression into (8.6) and solving for \\(\\widetilde{\\beta}_{\\text {cls }}\\) we find the solution to the constrained minimization problem (8.3)\n\\[\n\\widetilde{\\beta}_{\\text {cls }}=\\widehat{\\beta}_{\\text {ols }}-\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\boldsymbol{R}\\left[\\boldsymbol{R}^{\\prime}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\boldsymbol{R}\\right]^{-1}\\left(\\boldsymbol{R}^{\\prime} \\widehat{\\beta}_{\\text {ols }}-\\boldsymbol{c}\\right) .\n\\]\n(See Exercise \\(8.5\\) to verify that (8.8) satisfies (8.1).)\nThis is a general formula for the CLS estimator. It also can be written as\n\\[\n\\widetilde{\\beta}_{\\mathrm{cls}}=\\widehat{\\beta}_{\\mathrm{ols}}-\\widehat{\\boldsymbol{Q}}_{X X}^{-1} \\boldsymbol{R}\\left(\\boldsymbol{R}^{\\prime} \\widehat{\\boldsymbol{Q}}_{X X}^{-1} \\boldsymbol{R}\\right)^{-1}\\left(\\boldsymbol{R}^{\\prime} \\widehat{\\beta}_{\\mathrm{ols}}-\\boldsymbol{c}\\right) .\n\\]\nThe CLS residuals are \\(\\widetilde{e}_{i}=Y_{i}-X_{i}^{\\prime} \\widetilde{\\beta}_{\\text {cls }}\\) and are written in vector notation as \\(\\widetilde{\\boldsymbol{e}}\\).\nTo illustrate we generated a random sample of 100 observations for the variables \\(\\left(Y, X_{1}, X_{2}\\right)\\) and calculated the sum of squared errors function for the regression of \\(Y\\) on \\(X_{1}\\) and \\(X_{2}\\). Figure \\(8.1\\) displays contour plots of the sum of squared errors function. The center of the contour plots is the least squares minimizer \\(\\widehat{\\beta}_{\\text {ols }}=(0.33,0.26)^{\\prime}\\). Suppose it is desired to estimate the coefficients subject to the constraint \\(\\beta_{1}+\\beta_{2}=1\\). This constraint is displayed in the figure by the straight line. The constrained least squares estimator is the point on this straight line which yields the smallest sum of squared errors. This is the point which intersects with the lowest contour plot. The solution is the point where a contour plot is tangent to the constraint line and is marked as \\(\\widetilde{\\beta}_{\\mathrm{cls}}=(0.52,0.48)^{\\prime}\\).\n\nFigure 8.1: Constrained Least Squares Criterion\nIn Stata constrained least squares is implemented using the cnsreg command."
  },
  {
    "objectID": "chpt08-restricted-est.html#exclusion-restriction",
    "href": "chpt08-restricted-est.html#exclusion-restriction",
    "title": "8  Restricted Estimation",
    "section": "8.3 Exclusion Restriction",
    "text": "8.3 Exclusion Restriction\nWhile (8.8) is a general formula for CLS, in most cases the estimator can be found by applying least squares to a reparameterized equation. To illustrate let us return to the first example presented at the beginning of the chapter - a simple exclusion restriction. Recall that the unconstrained model is\n\\[\nY=X_{1}^{\\prime} \\beta_{1}+X_{2}^{\\prime} \\beta_{2}+e\n\\]\nthe exclusion restriction is \\(\\beta_{2}=0\\), and the constrained equation is\n\\[\nY=X_{1}^{\\prime} \\beta_{1}+e .\n\\]\nIn this setting the CLS estimator is OLS of \\(Y\\) on \\(X_{1}\\). (See Exercise 8.1.) We can write this as\n\\[\n\\widetilde{\\beta}_{1}=\\left(\\sum_{i=1}^{n} X_{1 i} X_{1 i}^{\\prime}\\right)^{-1}\\left(\\sum_{i=1}^{n} X_{1 i} Y_{i}\\right) .\n\\]\nThe CLS estimator of the entire vector \\(\\beta^{\\prime}=\\left(\\beta_{1}^{\\prime}, \\beta_{2}^{\\prime}\\right)\\) is\n\\[\n\\widetilde{\\beta}=\\left(\\begin{array}{c}\n\\widetilde{\\beta}_{1} \\\\\n0\n\\end{array}\\right) .\n\\]\nIt is not immediately obvious but (8.8) and (8.13) are algebraically identical. To see this the first component of (8.8) with (8.2) is\n\\[\n\\widetilde{\\beta}_{1}=\\left(\\begin{array}{ll}\n\\boldsymbol{I}_{k_{2}} & 0\n\\end{array}\\right)\\left[\\widehat{\\beta}-\\widehat{\\boldsymbol{Q}}_{X X}^{-1}\\left(\\begin{array}{c}\n0 \\\\\n\\boldsymbol{I}_{k_{2}}\n\\end{array}\\right)\\left[\\left(\\begin{array}{ll}\n0 & \\boldsymbol{I}_{k_{2}}\n\\end{array}\\right) \\widehat{\\boldsymbol{Q}}_{X X}^{-1}\\left(\\begin{array}{c}\n0 \\\\\n\\boldsymbol{I}_{k_{2}}\n\\end{array}\\right)\\right]^{-1}\\left(\\begin{array}{cc}\n0 & \\boldsymbol{I}_{k_{2}}\n\\end{array}\\right) \\widehat{\\beta}\\right] .\n\\]\nUsing (3.39) this equals\n\\[\n\\begin{aligned}\n& \\widetilde{\\beta}_{1}=\\widehat{\\beta}_{1}-\\widehat{\\boldsymbol{Q}}^{12}\\left(\\widehat{\\boldsymbol{Q}}^{22}\\right)^{-1} \\widehat{\\beta}_{2} \\\\\n& =\\widehat{\\beta}_{1}+\\widehat{\\boldsymbol{Q}}_{11 \\cdot 2}^{-1} \\widehat{\\boldsymbol{Q}}_{12} \\widehat{\\boldsymbol{Q}}_{22}^{-1} \\widehat{\\boldsymbol{Q}}_{22 \\cdot 1} \\widehat{\\beta}_{2} \\\\\n& =\\widehat{\\boldsymbol{Q}}_{11 \\cdot 2}^{-1}\\left(\\widehat{\\boldsymbol{Q}}_{1 Y}-\\widehat{\\boldsymbol{Q}}_{12} \\widehat{\\boldsymbol{Q}}_{22}^{-1} \\widehat{\\boldsymbol{Q}}_{2 Y}\\right) \\\\\n& +\\widehat{\\boldsymbol{Q}}_{11 \\cdot 2}^{-1} \\widehat{\\boldsymbol{Q}}_{12} \\widehat{\\boldsymbol{Q}}_{22}^{-1} \\widehat{\\boldsymbol{Q}}_{22 \\cdot 1} \\widehat{\\boldsymbol{Q}}_{22 \\cdot 1}^{-1}\\left(\\widehat{\\boldsymbol{Q}}_{2 y}-\\widehat{\\boldsymbol{Q}}_{21} \\widehat{\\boldsymbol{Q}}_{11}^{-1} \\widehat{\\boldsymbol{Q}}_{1 Y}\\right) \\\\\n& =\\widehat{\\boldsymbol{Q}}_{11 \\cdot 2}^{-1}\\left(\\widehat{\\boldsymbol{Q}}_{1 Y}-\\widehat{\\boldsymbol{Q}}_{12} \\widehat{\\boldsymbol{Q}}_{22}^{-1} \\widehat{\\mathbf{Q}}_{21} \\widehat{\\boldsymbol{Q}}_{11}^{-1} \\widehat{\\boldsymbol{Q}}_{1 Y}\\right) \\\\\n& =\\widehat{\\boldsymbol{Q}}_{11 \\cdot 2}^{-1}\\left(\\widehat{\\boldsymbol{Q}}_{11}-\\widehat{\\boldsymbol{Q}}_{12} \\widehat{\\boldsymbol{Q}}_{22}^{-1} \\widehat{\\boldsymbol{Q}}_{21}\\right) \\widehat{\\boldsymbol{Q}}_{11}^{-1} \\widehat{\\boldsymbol{Q}}_{1 Y} \\\\\n& =\\widehat{\\boldsymbol{Q}}_{11}^{-1} \\widehat{\\boldsymbol{Q}}_{1 Y}\n\\end{aligned}\n\\]\nwhich is (8.13) as originally claimed."
  },
  {
    "objectID": "chpt08-restricted-est.html#finite-sample-properties",
    "href": "chpt08-restricted-est.html#finite-sample-properties",
    "title": "8  Restricted Estimation",
    "section": "8.4 Finite Sample Properties",
    "text": "8.4 Finite Sample Properties\nIn this section we explore some of the properties of the CLS estimator in the linear regression model\n\\[\n\\begin{aligned}\nY &=X^{\\prime} \\beta+e \\\\\n\\mathbb{E}[e \\mid X] &=0 .\n\\end{aligned}\n\\]\nFirst, it is useful to write the estimator and the residuals as linear functions of the error vector. These are algebraic relationships and do not rely on the linear regression assumptions. Theorem 8.1 The CLS estimator satisfies\n\n\\(\\boldsymbol{R}^{\\prime} \\widehat{\\beta}-\\boldsymbol{c}=\\boldsymbol{R}^{\\prime}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\boldsymbol{X}^{\\prime} \\boldsymbol{e}\\)\n\\(\\widetilde{\\beta}_{\\mathrm{cls}}-\\beta=\\left(\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\boldsymbol{X}^{\\prime}-\\boldsymbol{A} \\boldsymbol{X}^{\\prime}\\right) \\boldsymbol{e}\\)\n\\(\\widetilde{\\boldsymbol{e}}=\\left(\\boldsymbol{I}-\\boldsymbol{P}+\\boldsymbol{X} \\boldsymbol{A} \\boldsymbol{X}^{\\prime}\\right) \\boldsymbol{e}\\)\n\\(\\boldsymbol{I}_{n}-\\boldsymbol{P}+\\boldsymbol{X} \\boldsymbol{A} \\boldsymbol{X}^{\\prime}\\) is symmetric and idempotent\n\\(\\operatorname{tr}\\left(\\boldsymbol{I}_{n}-\\boldsymbol{P}+\\boldsymbol{X} \\boldsymbol{A} \\boldsymbol{X}^{\\prime}\\right)=n-k+q\\)\n\nwhere \\(\\boldsymbol{P}=\\boldsymbol{X}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\boldsymbol{X}^{\\prime}\\) and \\(\\boldsymbol{A}=\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\boldsymbol{R}\\left(\\boldsymbol{R}^{\\prime}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\boldsymbol{R}\\right)^{-1} \\boldsymbol{R}^{\\prime}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1}\\)\nFor a proof see Exercise 8.6.\nGiven the linearity of Theorem 8.1.2 it is not hard to show that the CLS estimator is unbiased for \\(\\beta\\).\nTheorem 8.2 In the linear regression model (8.14)-(8.15) under (8.1), \\(\\mathbb{E}\\left[\\widetilde{\\beta}_{\\mathrm{cls}} \\mid \\boldsymbol{X}\\right]=\\beta\\)\nFor a proof see Exercise 8.7.\nWe can also calculate the covariance matrix of \\(\\widetilde{\\beta}_{\\text {cls }}\\). First, for simplicity take the case of conditional homoskedasticity.\nTheorem 8.3 In the homoskedastic linear regression model (8.14)-(8.15) with \\(\\mathbb{E}\\left[e^{2} \\mid X\\right]=\\sigma^{2}\\), under (8.1),\n\\[\n\\begin{aligned}\n\\boldsymbol{V}_{\\widetilde{\\beta}}^{0} &=\\operatorname{var}\\left[\\widetilde{\\beta}_{\\mathrm{cls}} \\mid \\boldsymbol{X}\\right] \\\\\n&=\\left(\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1}-\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\boldsymbol{R}\\left(\\boldsymbol{R}^{\\prime}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\boldsymbol{R}\\right)^{-1} \\boldsymbol{R}^{\\prime}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1}\\right) \\sigma^{2}\n\\end{aligned}\n\\]\nFor a proof see Exercise 8.8.\nWe use the \\(\\boldsymbol{V}_{\\tilde{\\beta}}^{0}\\) notation to emphasize that this is the covariance matrix under the assumption of conditional homoskedasticity.\nFor inference we need an estimate of \\(\\boldsymbol{V}_{\\widetilde{\\beta}}^{0}\\). A natural estimator is\n\\[\n\\widehat{\\boldsymbol{V}}_{\\widetilde{\\beta}}^{0}=\\left(\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1}-\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\boldsymbol{R}\\left(\\boldsymbol{R}^{\\prime}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\boldsymbol{R}\\right)^{-1} \\boldsymbol{R}^{\\prime}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1}\\right) s_{\\mathrm{cls}}^{2}\n\\]\nwhere\n\\[\ns_{\\mathrm{cls}}^{2}=\\frac{1}{n-k+q} \\sum_{i=1}^{n} \\widetilde{e}_{i}^{2}\n\\]\nis a biased-corrected estimator of \\(\\sigma^{2}\\). Standard errors for the components of \\(\\beta\\) are then found by taking the squares roots of the diagonal elements of \\(\\widehat{\\boldsymbol{V}}_{\\widetilde{\\beta}}\\), for example\n\\[\ns\\left(\\widehat{\\beta}_{j}\\right)=\\sqrt{\\left[\\widehat{\\boldsymbol{V}}_{\\widetilde{\\beta}}^{0}\\right]_{j j}} .\n\\]\nThe estimator (8.16) has the property that it is unbiased for \\(\\sigma^{2}\\) under conditional homoskedasticity. To see this, using the properties of Theorem 8.1,\n\\[\n\\begin{aligned}\n(n-k+q) s_{\\mathrm{cls}}^{2} &=\\widetilde{\\boldsymbol{e}}^{\\prime} \\widetilde{\\boldsymbol{e}} \\\\\n&=\\boldsymbol{e}^{\\prime}\\left(\\boldsymbol{I}_{n}-\\boldsymbol{P}+\\boldsymbol{X} \\boldsymbol{A} \\boldsymbol{X}^{\\prime}\\right)\\left(\\boldsymbol{I}_{n}-\\boldsymbol{P}+\\boldsymbol{X} \\boldsymbol{A} \\boldsymbol{X}^{\\prime}\\right) \\boldsymbol{e} \\\\\n&=\\boldsymbol{e}^{\\prime}\\left(\\boldsymbol{I}_{n}-\\boldsymbol{P}+\\boldsymbol{X} \\boldsymbol{A} \\boldsymbol{X}^{\\prime}\\right) \\boldsymbol{e} .\n\\end{aligned}\n\\]\nWe defer the remainder of the proof to Exercise 8.9.\nTheorem 8.4 In the homoskedastic linear regression model (8.14)-(8.15) with \\(\\mathbb{E}\\left[e^{2} \\mid X\\right]=\\sigma^{2}\\), under (8.1), \\(\\mathbb{E}\\left[s_{\\text {cls }}^{2} \\mid \\boldsymbol{X}\\right]=\\sigma^{2}\\) and \\(\\mathbb{E}\\left[\\widehat{\\boldsymbol{V}}_{\\widetilde{\\beta}}^{0} \\mid \\boldsymbol{X}\\right]=\\boldsymbol{V}_{\\widetilde{\\beta}}^{0} .\\)\nNow consider the distributional properties in the normal regression model \\(Y=X^{\\prime} \\beta+e\\) with \\(e \\sim\\) \\(\\mathrm{N}\\left(0, \\sigma^{2}\\right)\\). By the linearity of Theorem 8.1.2, conditional on \\(\\boldsymbol{X}, \\widetilde{\\beta}_{\\text {cls }}-\\beta\\) is normal. Given Theorems \\(8.2\\) and \\(8.3\\) we deduce that \\(\\widetilde{\\beta}_{\\mathrm{cls}} \\sim \\mathrm{N}\\left(\\beta, \\boldsymbol{V}_{\\widetilde{\\beta}}^{0}\\right)\\).\nSimilarly, from Exericise \\(8.1\\) we know \\(\\widetilde{\\boldsymbol{e}}=\\left(\\boldsymbol{I}_{n}-\\boldsymbol{P}+\\boldsymbol{X} \\boldsymbol{A} \\boldsymbol{X}^{\\prime}\\right) \\boldsymbol{e}\\) is linear in \\(\\boldsymbol{e}\\) so is also conditionally normal. Furthermore, since \\(\\left(\\boldsymbol{I}_{n}-\\boldsymbol{P}+\\boldsymbol{X} \\boldsymbol{A} \\boldsymbol{X}^{\\prime}\\right)\\left(\\boldsymbol{X}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1}-\\boldsymbol{X} \\boldsymbol{A}\\right)=0, \\widetilde{\\boldsymbol{e}}\\) and \\(\\widetilde{\\beta}_{\\text {cls }}\\) are uncorrelated and thus independent. Thus \\(s_{\\text {cls }}^{2}\\) and \\(\\widetilde{\\beta}_{\\text {cls }}\\) are independent.\nFrom (8.17) and the fact that \\(\\boldsymbol{I}_{n}-\\boldsymbol{P}+\\boldsymbol{X} \\boldsymbol{A} \\boldsymbol{X}^{\\prime}\\) is idempotent with rank \\(n-k+q\\) it follows that\n\\[\ns_{\\text {cls }}^{2} \\sim \\sigma^{2} \\chi_{n-k+q}^{2} /(n-k+q) .\n\\]\nIt follows that the \\(\\mathrm{t}\\)-statistic has the exact distribution\n\\[\nT=\\frac{\\widehat{\\beta}_{j}-\\beta_{j}}{s\\left(\\widehat{\\beta}_{j}\\right)} \\sim \\frac{\\mathrm{N}(0,1)}{\\sqrt{\\chi_{n-k+q}^{2} /(n-k+q)}} \\sim t_{n-k+q}\n\\]\na student \\(t\\) distribution with \\(n-k+q\\) degrees of freedom.\nThe relevance of this calculation is that the “degrees of freedom” for CLS regression equal \\(n-k+q\\) rather than \\(n-k\\) as in OLS. Essentially the model has \\(k-q\\) free parameters instead of \\(k\\). Another way of thinking about this is that estimation of a model with \\(k\\) coefficients and \\(q\\) restrictions is equivalent to estimation with \\(k-q\\) coefficients.\nWe summarize the properties of the normal regression model. Theorem 8.5 In the normal linear regression model (8.14)-(8.15) with constraint (8.1),\n\\[\n\\begin{aligned}\n\\widetilde{\\beta}_{\\mathrm{cls}} \\sim \\mathrm{N}\\left(\\beta, \\boldsymbol{V}_{\\widetilde{\\beta}}^{0}\\right) \\\\\n\\frac{(n-k+q) s_{\\mathrm{cls}}^{2}}{\\sigma^{2}} \\sim \\chi_{n-k+q}^{2} \\\\\nT & \\sim t_{n-k+q} .\n\\end{aligned}\n\\]\nAn interesting relationship is that in the homoskedastic regression model\n\\[\n\\begin{aligned}\n\\operatorname{cov}\\left(\\widehat{\\beta}_{\\mathrm{ols}}-\\widetilde{\\beta}_{\\mathrm{cls}}, \\widetilde{\\beta}_{\\mathrm{cls}} \\mid \\boldsymbol{X}\\right) &=\\mathbb{E}\\left[\\left(\\widehat{\\beta}_{\\mathrm{ols}}-\\widetilde{\\beta}_{\\mathrm{cls}}\\right)\\left(\\widetilde{\\beta}_{\\mathrm{cls}}-\\beta\\right)^{\\prime} \\mid \\boldsymbol{X}\\right] \\\\\n&=\\mathbb{E}\\left[\\boldsymbol{A} \\boldsymbol{X}^{\\prime} \\boldsymbol{e} \\boldsymbol{e}^{\\prime}\\left(\\boldsymbol{X}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1}-\\boldsymbol{X} \\boldsymbol{A}\\right) \\mid \\boldsymbol{X}\\right] \\\\\n&=\\boldsymbol{A} \\boldsymbol{X}^{\\prime}\\left(\\boldsymbol{X}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1}-\\boldsymbol{X} \\boldsymbol{A}\\right) \\sigma^{2}=0 .\n\\end{aligned}\n\\]\nThis means that \\(\\widehat{\\beta}_{\\text {ols }}-\\widetilde{\\beta}_{\\text {cls }}\\) and \\(\\widetilde{\\beta}_{\\text {cls }}\\) are conditionally uncorrelated and hence independent. A corollary is\n\\[\n\\operatorname{cov}\\left(\\widehat{\\beta}_{\\text {ols }}, \\widetilde{\\beta}_{\\text {cls }} \\mid \\boldsymbol{X}\\right)=\\operatorname{var}\\left[\\widetilde{\\beta}_{\\text {cls }} \\mid \\boldsymbol{X}\\right] .\n\\]\nA second corollary is\n\\[\n\\begin{aligned}\n\\operatorname{var}\\left[\\widehat{\\beta}_{\\mathrm{ols}}-\\widetilde{\\beta}_{\\mathrm{cls}} \\mid \\boldsymbol{X}\\right] &=\\operatorname{var}\\left[\\widehat{\\beta}_{\\mathrm{ols}} \\mid \\boldsymbol{X}\\right]-\\operatorname{var}\\left[\\widetilde{\\beta}_{\\mathrm{cls}} \\mid \\boldsymbol{X}\\right] \\\\\n&=\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\boldsymbol{R}\\left(\\boldsymbol{R}^{\\prime}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\boldsymbol{R}\\right)^{-1} \\boldsymbol{R}^{\\prime}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\sigma^{2}\n\\end{aligned}\n\\]\nThis also shows that the difference between the CLS and OLS variances matrices equals\n\\[\n\\operatorname{var}\\left[\\widehat{\\beta}_{\\text {ols }} \\mid \\boldsymbol{X}\\right]-\\operatorname{var}\\left[\\widetilde{\\beta}_{\\mathrm{cls}} \\mid \\boldsymbol{X}\\right]=\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\boldsymbol{R}\\left(\\boldsymbol{R}^{\\prime}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\boldsymbol{R}\\right)^{-1} \\boldsymbol{R}^{\\prime}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\sigma^{2} \\geq 0\n\\]\nthe final equality meaning positive semi-definite. It follows that \\(\\operatorname{var}\\left[\\widehat{\\beta}_{\\text {ols }} \\mid \\boldsymbol{X}\\right] \\geq \\operatorname{var}\\left[\\widetilde{\\beta}_{\\text {cls }} \\mid \\boldsymbol{X}\\right]\\) in the positive definite sense, and thus CLS is more efficient than OLS. Both estimators are unbiased (in the linear regression model) and CLS has a lower covariance matrix (in the linear homoskedastic regression model).\nThe relationship (8.18) is rather interesting and will appear again. The expression says that the variance of the difference between the estimators is equal to the difference between the variances. This is rather special. It occurs generically when we are comparing an efficient and an inefficient estimator. We call (8.18) the Hausman Equality as it was first pointed out in econometrics by Hausman (1978)."
  },
  {
    "objectID": "chpt08-restricted-est.html#minimum-distance",
    "href": "chpt08-restricted-est.html#minimum-distance",
    "title": "8  Restricted Estimation",
    "section": "8.5 Minimum Distance",
    "text": "8.5 Minimum Distance\nThe previous section explored the finite sample distribution theory under the assumptions of the linear regression model, homoskedastic regression model, and normal regression model. We now return to the general projection model where we do not impose linearity, homoskedasticity, nor normality. We are interested in the question: Can we do better than CLS in this setting?\nA minimum distance estimator tries to find a parameter value satisfying the constraint which is as close as possible to the unconstrained estimator. Let \\(\\widehat{\\beta}\\) be the unconstrained least squares estimator, and for some \\(k \\times k\\) positive definite weight matrix \\(\\widehat{W}\\) define the quadratic criterion function\n\\[\nJ(\\beta)=n(\\widehat{\\beta}-\\beta)^{\\prime} \\widehat{\\boldsymbol{W}}(\\widehat{\\beta}-\\beta) .\n\\]\nThis is a (squared) weighted Euclidean distance between \\(\\widehat{\\beta}\\) and \\(\\beta . J(\\beta)\\) is small if \\(\\beta\\) is close to \\(\\widehat{\\beta}\\), and is minimized at zero only if \\(\\beta=\\widehat{\\beta}\\). A minimum distance estimator \\(\\widetilde{\\beta}_{\\mathrm{md}}\\) for \\(\\beta\\) minimizes \\(J(\\beta)\\) subject to the constraint (8.1), that is,\n\\[\n\\widetilde{\\beta}_{\\mathrm{md}}=\\underset{\\boldsymbol{R}^{\\prime} \\beta=\\boldsymbol{c}}{\\operatorname{argmin}} J(\\beta) .\n\\]\nThe CLS estimator is the special case when \\(\\widehat{\\boldsymbol{W}}=\\widehat{\\boldsymbol{Q}}_{X X}\\) and we write this criterion function as\n\\[\nJ^{0}(\\beta)=n(\\widehat{\\beta}-\\beta)^{\\prime} \\widehat{\\boldsymbol{Q}}_{X X}(\\widehat{\\beta}-\\beta) .\n\\]\nTo see the equality of CLS and minimum distance rewrite the least squares criterion as follows. Substitute the unconstrained least squares fitted equation \\(Y_{i}=X_{i}^{\\prime} \\widehat{\\beta}+\\widehat{e}_{i}\\) into \\(\\operatorname{SSE}(\\beta)\\) to obtain\n\\[\n\\begin{aligned}\n\\operatorname{SSE}(\\beta) &=\\sum_{i=1}^{n}\\left(Y_{i}-X_{i}^{\\prime} \\beta\\right)^{2} \\\\\n&=\\sum_{i=1}^{n}\\left(X_{i}^{\\prime} \\widehat{\\beta}+\\widehat{e}_{i}-X_{i}^{\\prime} \\beta\\right)^{2} \\\\\n&=\\sum_{i=1}^{n} \\widehat{e}_{i}^{2}+(\\widehat{\\beta}-\\beta)^{\\prime}\\left(\\sum_{i=1}^{n} X_{i} X_{i}^{\\prime}\\right)(\\widehat{\\beta}-\\beta) \\\\\n&=n \\widehat{\\sigma}^{2}+J^{0}(\\beta)\n\\end{aligned}\n\\]\nwhere the third equality uses the fact that \\(\\sum_{i=1}^{n} X_{i} \\widehat{e}_{i}=0\\), and the last line uses \\(\\sum_{i=1}^{n} X_{i} X_{i}^{\\prime}=n \\widehat{\\mathbf{Q}}_{X X}\\). The expression (8.21) only depends on \\(\\beta\\) through \\(J^{0}(\\beta)\\). Thus minimization of \\(\\operatorname{SSE}(\\beta)\\) and \\(J^{0}(\\beta)\\) are equivalent, and hence \\(\\widetilde{\\beta}_{\\mathrm{md}}=\\widetilde{\\widetilde{\\beta}}_{\\text {cls }}\\) when \\(\\widehat{\\boldsymbol{W}}=\\widehat{\\boldsymbol{Q}}_{X X}\\).\nWe can solve for \\(\\widetilde{\\beta}_{\\mathrm{md}}\\) explicitly by the method of Lagrange multipliers. The Lagrangian is\n\\[\n\\mathscr{L}(\\beta, \\lambda)=\\frac{1}{2} J(\\beta, \\widehat{\\boldsymbol{W}})+\\lambda^{\\prime}\\left(\\boldsymbol{R}^{\\prime} \\beta-\\boldsymbol{c}\\right) .\n\\]\nThe solution to the pair of first order conditions is\n\\[\n\\begin{aligned}\n&\\widetilde{\\lambda}_{\\mathrm{md}}=n\\left(\\boldsymbol{R}^{\\prime} \\widehat{\\boldsymbol{W}}^{-1} \\boldsymbol{R}\\right)^{-1}\\left(\\boldsymbol{R}^{\\prime} \\widehat{\\boldsymbol{\\beta}}-\\boldsymbol{c}\\right) \\\\\n&\\widetilde{\\beta}_{\\mathrm{md}}=\\widehat{\\boldsymbol{\\beta}}-\\widehat{\\boldsymbol{W}}^{-1} \\boldsymbol{R}\\left(\\boldsymbol{R}^{\\prime} \\widehat{\\boldsymbol{W}}^{-1} \\boldsymbol{R}\\right)^{-1}\\left(\\boldsymbol{R}^{\\prime} \\widehat{\\beta}-\\boldsymbol{c}\\right) .\n\\end{aligned}\n\\]\n(See Exercise 8.10.) Comparing (8.23) with (8.9) we can see that \\(\\widetilde{\\beta}_{\\mathrm{md}}\\) specializes to \\(\\widetilde{\\beta}_{\\text {cls }}\\) when we set \\(\\widehat{\\boldsymbol{W}}=\\widehat{\\boldsymbol{Q}}_{X X}\\)\nAn obvious question is which weight matrix \\(\\widehat{\\boldsymbol{W}}\\) is best. We will address this question after we derive the asymptotic distribution for a general weight matrix."
  },
  {
    "objectID": "chpt08-restricted-est.html#asymptotic-distribution",
    "href": "chpt08-restricted-est.html#asymptotic-distribution",
    "title": "8  Restricted Estimation",
    "section": "8.6 Asymptotic Distribution",
    "text": "8.6 Asymptotic Distribution\nWe first show that the class of minimum distance estimators are consistent for the population parameters when the constraints are valid.\nAssumption 8.1 \\(\\boldsymbol{R}^{\\prime} \\beta=\\boldsymbol{c}\\) where \\(\\boldsymbol{R}\\) is \\(k \\times q\\) with \\(\\operatorname{rank}(\\boldsymbol{R})=q\\). Assumption 8.2 \\(\\widehat{W} \\underset{p}{\\longrightarrow} W>0\\).\nTheorem 8.6 Consistency Under Assumptions 7.1, 8.1, and 8.2, \\(\\widetilde{\\beta}_{\\mathrm{md}} \\underset{p}{\\longrightarrow} \\beta\\) as \\(n \\rightarrow \\infty\\).\nFor a proof see Exercise 8.11.\nTheorem \\(8.6\\) shows that consistency holds for any weight matrix with a positive definite limit so includes the CLS estimator.\nSimilarly, the constrained estimators are asymptotically normally distributed.\nTheorem 8.7 Asymptotic Normality Under Assumptions 7.2, 8.1, and 8.2,\n\\[\n\\sqrt{n}\\left(\\widetilde{\\beta}_{\\mathrm{md}}-\\beta\\right) \\underset{d}{\\longrightarrow} \\mathrm{N}\\left(0, \\boldsymbol{V}_{\\beta}(\\boldsymbol{W})\\right)\n\\]\nas \\(n \\rightarrow \\infty\\), where\n\\[\n\\begin{gathered}\n\\boldsymbol{V}_{\\beta}(\\boldsymbol{W})=\\boldsymbol{V}_{\\beta}-\\boldsymbol{W}^{-1} \\boldsymbol{R}\\left(\\boldsymbol{R}^{\\prime} \\boldsymbol{W}^{-1} \\boldsymbol{R}\\right)^{-1} \\boldsymbol{R}^{\\prime} \\boldsymbol{V}_{\\beta} \\\\\n-\\boldsymbol{V}_{\\beta} \\boldsymbol{R}\\left(\\boldsymbol{R}^{\\prime} \\boldsymbol{W}^{-1} \\boldsymbol{R}\\right)^{-1} \\boldsymbol{R}^{\\prime} \\boldsymbol{W}^{-1} \\\\\n+\\boldsymbol{W}^{-1} \\boldsymbol{R}\\left(\\boldsymbol{R}^{\\prime} \\boldsymbol{W}^{-1} \\boldsymbol{R}\\right)^{-1} \\boldsymbol{R}^{\\prime} \\boldsymbol{V}_{\\beta} \\boldsymbol{R}\\left(\\boldsymbol{R}^{\\prime} \\boldsymbol{W}^{-1} \\boldsymbol{R}\\right)^{-1} \\boldsymbol{R}^{\\prime} \\boldsymbol{W}^{-1}\n\\end{gathered}\n\\]\nand \\(\\boldsymbol{V}_{\\beta}=\\boldsymbol{Q}_{X X}^{-1} \\Omega \\boldsymbol{Q}_{X X}^{-1}\\)\nFor a proof see Exercise 8.12.\nTheorem \\(8.7\\) shows that the minimum distance estimator is asymptotically normal for all positive definite weight matrices. The asymptotic variance depends on \\(\\boldsymbol{W}\\). The theorem includes the CLS estimator as a special case by setting \\(\\boldsymbol{W}=\\boldsymbol{Q}_{X X}\\).\nTheorem 8.8 Asymptotic Distribution of CLS Estimator Under Assumptions \\(7.2\\) and 8.1, as \\(n \\rightarrow \\infty\\)\n\\[\n\\sqrt{n}\\left(\\widetilde{\\beta}_{\\mathrm{cls}}-\\beta\\right) \\underset{d}{\\longrightarrow} \\mathrm{N}\\left(0, \\boldsymbol{V}_{\\mathrm{cls}}\\right)\n\\]\nwhere\n\\[\n\\begin{aligned}\n\\boldsymbol{V}_{\\mathrm{cls}} &=\\boldsymbol{V}_{\\beta}-\\boldsymbol{Q}_{X X}^{-1} \\boldsymbol{R}\\left(\\boldsymbol{R}^{\\prime} \\boldsymbol{Q}_{X X}^{-1} \\boldsymbol{R}\\right)^{-1} \\boldsymbol{R}^{\\prime} \\boldsymbol{V}_{\\beta} \\\\\n&-\\boldsymbol{V}_{\\beta} \\boldsymbol{R}\\left(\\boldsymbol{R}^{\\prime} \\boldsymbol{Q}_{X X}^{-1} \\boldsymbol{R}\\right)^{-1} \\boldsymbol{R}^{\\prime} \\boldsymbol{Q}_{X X}^{-1} \\\\\n&+\\boldsymbol{Q}_{X X}^{-1} \\boldsymbol{R}\\left(\\boldsymbol{R}^{\\prime} \\boldsymbol{Q}_{X X}^{-1} \\boldsymbol{R}\\right)^{-1} \\boldsymbol{R}^{\\prime} \\boldsymbol{V}_{\\beta} \\boldsymbol{R}\\left(\\boldsymbol{R}^{\\prime} \\boldsymbol{Q}_{X X}^{-1} \\boldsymbol{R}\\right)^{-1} \\boldsymbol{R}^{\\prime} \\boldsymbol{Q}_{X X}^{-1}\n\\end{aligned}\n\\]\nFor a proof see Exercise 8.13."
  },
  {
    "objectID": "chpt08-restricted-est.html#variance-estimation-and-standard-errors",
    "href": "chpt08-restricted-est.html#variance-estimation-and-standard-errors",
    "title": "8  Restricted Estimation",
    "section": "8.7 Variance Estimation and Standard Errors",
    "text": "8.7 Variance Estimation and Standard Errors\nEarlier we introduced the covariance matrix estimator under the assumption of conditional homoskedasticity. We now introduce an estimator which does not impose homoskedasticity.\nThe asymptotic covariance matrix \\(\\boldsymbol{V}_{\\text {cls }}\\) may be estimated by replacing \\(\\boldsymbol{V}_{\\beta}\\) with a consistent estimator such as \\(\\widehat{\\boldsymbol{V}}_{\\beta}\\). A more efficient estimator can be obtained by using the restricted coefficient estimator which we now show. Given the constrained least squares squares residuals \\(\\widetilde{e}_{i}=Y_{i}-X_{i}^{\\prime} \\widetilde{\\beta}_{\\text {cls }}\\) we can estimate the matrix \\(\\Omega=\\mathbb{E}\\left[X X^{\\prime} e^{2}\\right]\\) by\n\\[\n\\widetilde{\\Omega}=\\frac{1}{n-k+q} \\sum_{i=1}^{n} X_{i} X_{i}^{\\prime} \\widetilde{e}_{i}^{2} .\n\\]\nNotice that we have used an adjusted degrees of freedom. This is an \\(a d\\) hoc adjustment designed to mimic that used for estimation of the error variance \\(\\sigma^{2}\\). The moment estimator of \\(\\boldsymbol{V}_{\\beta}\\) is\n\\[\n\\widetilde{\\boldsymbol{V}}_{\\beta}=\\widehat{\\boldsymbol{Q}}_{X X}^{-1} \\widetilde{\\Omega} \\widehat{\\boldsymbol{Q}}_{X X}^{-1}\n\\]\nand that for \\(\\boldsymbol{V}_{\\mathrm{cls}}\\) is\n\\[\n\\begin{aligned}\n\\widetilde{\\boldsymbol{V}}_{\\mathrm{cls}} &=\\widetilde{\\boldsymbol{V}}_{\\beta}-\\widehat{\\boldsymbol{Q}}_{X X}^{-1} \\boldsymbol{R}\\left(\\boldsymbol{R}^{\\prime} \\widehat{\\boldsymbol{Q}}_{X X}^{-1} \\boldsymbol{R}\\right)^{-1} \\boldsymbol{R}^{\\prime} \\widetilde{\\boldsymbol{V}}_{\\beta} \\\\\n&-\\widetilde{\\boldsymbol{V}}_{\\beta} \\boldsymbol{R}\\left(\\boldsymbol{R}^{\\prime} \\widehat{\\boldsymbol{Q}}_{X X}^{-1} \\boldsymbol{R}\\right)^{-1} \\boldsymbol{R}^{\\prime} \\widehat{\\boldsymbol{Q}}_{\\boldsymbol{x x}}^{-1} \\\\\n&+\\widehat{\\boldsymbol{Q}}_{X X}^{-1} \\boldsymbol{R}\\left(\\boldsymbol{R}^{\\prime} \\widehat{\\boldsymbol{Q}}_{X X}^{-1} \\boldsymbol{R}\\right)^{-1} \\boldsymbol{R}^{\\prime} \\widetilde{\\boldsymbol{V}}_{\\beta} \\boldsymbol{R}\\left(\\boldsymbol{R}^{\\prime} \\widehat{\\boldsymbol{Q}}_{X X}^{-1} \\boldsymbol{R}\\right)^{-1} \\boldsymbol{R}^{\\prime} \\widehat{\\boldsymbol{Q}}_{X X}^{-1}\n\\end{aligned}\n\\]\nWe can calculate standard errors for any linear combination \\(h^{\\prime} \\widetilde{\\beta}_{\\text {cls }}\\) such that \\(h\\) does not lie in the range space of \\(\\boldsymbol{R}\\). A standard error for \\(h^{\\prime} \\widetilde{\\beta}\\) is\n\\[\ns\\left(h^{\\prime} \\widetilde{\\boldsymbol{\\beta}}_{\\mathrm{cls}}\\right)=\\left(n^{-1} h^{\\prime} \\tilde{\\boldsymbol{V}}_{\\mathrm{cls}} h\\right)^{1 / 2} .\n\\]"
  },
  {
    "objectID": "chpt08-restricted-est.html#efficient-minimum-distance-estimator",
    "href": "chpt08-restricted-est.html#efficient-minimum-distance-estimator",
    "title": "8  Restricted Estimation",
    "section": "8.8 Efficient Minimum Distance Estimator",
    "text": "8.8 Efficient Minimum Distance Estimator\nTheorem \\(8.7\\) shows that minimum distance estimators, which include CLS as a special case, are asymptotically normal with an asymptotic covariance matrix which depends on the weight matrix \\(\\boldsymbol{W}\\). The asymptotically optimal weight matrix is the one which minimizes the asymptotic variance \\(\\boldsymbol{V}_{\\beta}(\\boldsymbol{W})\\). This turns out to be \\(\\boldsymbol{W}=\\boldsymbol{V}_{\\beta}^{-1}\\) as is shown in Theorem \\(8.9\\) below. Since \\(\\boldsymbol{V}_{\\beta}^{-1}\\) is unknown this weight matrix cannot be used for a feasible estimator but we can replace \\(\\boldsymbol{V}_{\\beta}^{-1}\\) with a consistent estimator \\(\\widehat{\\boldsymbol{V}}_{\\beta}^{-1}\\) and the asymptotic distribution (and efficiency) are unchanged. We call the minimum distance estimator with \\(\\widehat{\\boldsymbol{W}}=\\widehat{\\boldsymbol{V}}_{\\beta}^{-1}\\) the efficient minimum distance estimator and takes the form\n\\[\n\\widetilde{\\beta}_{\\text {emd }}=\\widehat{\\beta}-\\widehat{\\boldsymbol{V}}_{\\beta} \\boldsymbol{R}\\left(\\boldsymbol{R}^{\\prime} \\widehat{\\boldsymbol{V}}_{\\beta} \\boldsymbol{R}\\right)^{-1}\\left(\\boldsymbol{R}^{\\prime} \\widehat{\\beta}-\\boldsymbol{c}\\right) .\n\\]\nThe asymptotic distribution of (8.25) can be deduced from Theorem 8.7. (See Exercises \\(8.14\\) and 8.15, and the proof in Section 8.16.)\nTheorem 8.9 Efficient Minimum Distance Estimator Under Assumptions \\(7.2\\) and 8.1,\n\\[\n\\sqrt{n}\\left(\\widetilde{\\beta}_{\\mathrm{emd}}-\\beta\\right) \\underset{d}{\\longrightarrow} \\mathrm{N}\\left(0, \\boldsymbol{V}_{\\beta, \\mathrm{emd}}\\right)\n\\]\nas \\(n \\rightarrow \\infty\\), where\n\\[\n\\boldsymbol{V}_{\\beta, \\mathrm{emd}}=\\boldsymbol{V}_{\\beta}-\\boldsymbol{V}_{\\beta} \\boldsymbol{R}\\left(\\boldsymbol{R}^{\\prime} \\boldsymbol{V}_{\\beta} \\boldsymbol{R}\\right)^{-1} \\boldsymbol{R}^{\\prime} \\boldsymbol{V}_{\\beta}\n\\]\nSince\n\\[\n\\boldsymbol{V}_{\\beta, \\mathrm{emd}} \\leq \\boldsymbol{V}_{\\beta}\n\\]\nthe estimator (8.25) has lower asymptotic variance than the unrestricted estimator. Furthermore, for any \\(\\boldsymbol{W}\\),\n\\[\n\\boldsymbol{V}_{\\beta, \\mathrm{emd}} \\leq \\boldsymbol{V}_{\\beta}(\\boldsymbol{W})\n\\]\nso (8.25) is asymptotically efficient in the class of minimum distance estimators.\nTheorem \\(8.9\\) shows that the minimum distance estimator with the smallest asymptotic variance is (8.25). One implication is that the constrained least squares estimator is generally inefficient. The interesting exception is the case of conditional homoskedasticity in which case the optimal weight matrix is \\(\\boldsymbol{W}=\\left(\\boldsymbol{V}_{\\beta}^{0}\\right)^{-1}\\) so in this case CLS is an efficient minimum distance estimator. Otherwise when the error is conditionally heteroskedastic there are asymptotic efficiency gains by using minimum distance rather than least squares.\nThe fact that CLS is generally inefficient is counter-intuitive and requires some reflection. Standard intuition suggests to apply the same estimation method (least squares) to the unconstrained and constrained models and this is the common empirical practice. But Theorem \\(8.9\\) shows that this is inefficient. Why? The reason is that the least squares estimator does not make use of the regressor \\(X_{2}\\). It ignores the information \\(\\mathbb{E}\\left[X_{2} e\\right]=0\\). This information is relevant when the error is heteroskedastic and the excluded regressors are correlated with the included regressors.\nInequality (8.27) shows that the efficient minimum distance estimator \\(\\widetilde{\\beta}_{\\text {emd }}\\) has a smaller asymptotic variance than the unrestricted least squares estimator \\(\\widehat{\\beta}\\). This means that efficient estimation is attained by imposing correct restrictions when we use the minimum distance method."
  },
  {
    "objectID": "chpt08-restricted-est.html#exclusion-restriction-revisited",
    "href": "chpt08-restricted-est.html#exclusion-restriction-revisited",
    "title": "8  Restricted Estimation",
    "section": "8.9 Exclusion Restriction Revisited",
    "text": "8.9 Exclusion Restriction Revisited\nWe return to the example of estimation with a simple exclusion restriction. The model is\n\\[\nY=X_{1}^{\\prime} \\beta_{1}+X_{2}^{\\prime} \\beta_{2}+e\n\\]\nwith the exclusion restriction \\(\\beta_{2}=0\\). We have introduced three estimators of \\(\\beta_{1}\\). The first is unconstrained least squares applied to (8.10) which can be written as \\(\\widehat{\\beta}_{1}=\\widehat{\\boldsymbol{Q}}_{11 \\cdot 2}^{-1} \\widehat{\\boldsymbol{Q}}_{1 Y \\cdot 2}\\). From Theorem \\(7.25\\) and equation (7.14) its asymptotic variance is\n\\[\n\\operatorname{avar}\\left[\\widehat{\\beta}_{1}\\right]=\\boldsymbol{Q}_{11 \\cdot 2}^{-1}\\left(\\Omega_{11}-\\boldsymbol{Q}_{12} \\boldsymbol{Q}_{22}^{-1} \\Omega_{21}-\\Omega_{12} \\boldsymbol{Q}_{22}^{-1} \\boldsymbol{Q}_{21}+\\boldsymbol{Q}_{12} \\boldsymbol{Q}_{22}^{-1} \\Omega_{22} \\boldsymbol{Q}_{22}^{-1} \\boldsymbol{Q}_{21}\\right) \\boldsymbol{Q}_{11 \\cdot 2}^{-1}\n\\]\nThe second estimator of \\(\\beta_{1}\\) is CLS, which can be written as \\(\\widetilde{\\beta}_{1}=\\widehat{\\boldsymbol{Q}}_{11}^{-1} \\widehat{\\boldsymbol{Q}}_{1 Y}\\). Its asymptotic variance can be deduced from Theorem 8.8, but it is simpler to apply the CLT directly to show that\n\\[\n\\operatorname{avar}\\left[\\widetilde{\\beta}_{1}\\right]=\\boldsymbol{Q}_{11}^{-1} \\Omega_{11} \\boldsymbol{Q}_{11}^{-1} .\n\\]\nThe third estimator of \\(\\beta_{1}\\) is efficient minimum distance. Applying (8.25), it equals\n\\[\n\\bar{\\beta}_{1}=\\widehat{\\beta}_{1}-\\widehat{\\boldsymbol{V}}_{12} \\widehat{\\boldsymbol{V}}_{22}^{-1} \\widehat{\\beta}_{2}\n\\]\nwhere we have partitioned\n\\[\n\\widehat{\\boldsymbol{V}}_{\\beta}=\\left[\\begin{array}{ll}\n\\widehat{\\boldsymbol{V}}_{11} & \\widehat{\\boldsymbol{V}}_{12} \\\\\n\\widehat{\\boldsymbol{V}}_{21} & \\widehat{\\boldsymbol{V}}_{22}\n\\end{array}\\right]\n\\]\nFrom Theorem \\(8.9\\) its asymptotic variance is\n\\[\n\\operatorname{avar}\\left[\\bar{\\beta}_{1}\\right]=\\boldsymbol{V}_{11}-\\boldsymbol{V}_{12} \\boldsymbol{V}_{22}^{-1} \\boldsymbol{V}_{21}\n\\]\nSee Exercise \\(8.16\\) to verify equations (8.29), (8.30), and (8.31).\nIn general the three estimators are different and they have different asymptotic variances. It is instructive to compare the variances to assess whether or not the constrained estimator is more efficient than the unconstrained estimator.\nFirst, assume conditional homoskedasticity. In this case the two covariance matrices simplify to \\(\\operatorname{avar}\\left[\\widehat{\\beta}_{1}\\right]=\\sigma^{2} \\boldsymbol{Q}_{11 \\cdot 2}^{-1}\\) and \\(\\operatorname{avar}\\left[\\widetilde{\\beta}_{1}\\right]=\\sigma^{2} \\boldsymbol{Q}_{11}^{-1}\\). If \\(\\boldsymbol{Q}_{12}=0\\) (so \\(X_{1}\\) and \\(X_{2}\\) are uncorrelated) then these two variance matrices are equal and the two estimators have equal asymptotic efficiency. Otherwise, since \\(\\boldsymbol{Q}_{12} \\boldsymbol{Q}_{22}^{-1} \\boldsymbol{Q}_{21} \\geq 0\\), then \\(\\boldsymbol{Q}_{11} \\geq \\boldsymbol{Q}_{11}-\\boldsymbol{Q}_{12} \\boldsymbol{Q}_{22}^{-1} \\boldsymbol{Q}_{21}\\) and consequently\n\\[\n\\boldsymbol{Q}_{11}^{-1} \\sigma^{2} \\leq\\left(\\boldsymbol{Q}_{11}-\\boldsymbol{Q}_{12} \\boldsymbol{Q}_{22}^{-1} \\boldsymbol{Q}_{21}\\right)^{-1} \\sigma^{2} .\n\\]\nThis means that under conditional homoskedasticity \\(\\widetilde{\\beta}_{1}\\) has a lower asymptotic covariance matrix than \\(\\widehat{\\beta}_{1}\\). Therefore in this context constrained least squares is more efficient than unconstrained least squares. This is consistent with our intuition that imposing a correct restriction (excluding an irrelevant regressor) improves estimation efficiency.\nHowever, in the general case of conditional heteroskedasticity this ranking is not guaranteed. In fact what is really amazing is that the variance ranking can be reversed. The CLS estimator can have a larger asymptotic variance than the unconstrained least squares estimator.\nTo see this let’s use the simple heteroskedastic example from Section 7.4. In that example, \\(Q_{11}=\\) \\(Q_{22}=1, Q_{12}=\\frac{1}{2}, \\Omega_{11}=\\Omega_{22}=1\\), and \\(\\Omega_{12}=\\frac{7}{8}\\). We can calculate (see Exercise 8.17) that \\(Q_{11 \\cdot 2}=\\frac{3}{4}\\) and\n\\[\n\\begin{aligned}\n\\operatorname{avar}\\left[\\widehat{\\beta}_{1}\\right] &=\\frac{2}{3} \\\\\n\\operatorname{avar}\\left[\\widetilde{\\beta}_{1}\\right] &=1 \\\\\n\\operatorname{avar}\\left[\\bar{\\beta}_{1}\\right] &=\\frac{5}{8} .\n\\end{aligned}\n\\]\nThus the CLS estimator \\(\\widetilde{\\beta}_{1}\\) has a larger variance than the unrestricted least squares estimator \\(\\widehat{\\beta}_{1}\\) ! The minimum distance estimator has the smallest variance of the three, as expected.\nWhat we have found is that when the estimation method is least squares, deleting the irrelevant variable \\(X_{2}\\) can actually increase estimation variance, or equivalently, adding an irrelevant variable can decrease the estimation variance. To repeat this unexpected finding, we have shown that it is possible for least squares applied to the short regression (8.11) to be less efficient for estimation of \\(\\beta_{1}\\) than least squares applied to the long regression (8.10) even though the constraint \\(\\beta_{2}=0\\) is valid! This result is strongly counter-intuitive. It seems to contradict our initial motivation for pursuing constrained estimation - to improve estimation efficiency.\nIt turns out that a more refined answer is appropriate. Constrained estimation is desirable but not necessarily CLS. While least squares is asymptotically efficient for estimation of the unconstrained projection model it is not an efficient estimator of the constrained projection model."
  },
  {
    "objectID": "chpt08-restricted-est.html#variance-and-standard-error-estimation",
    "href": "chpt08-restricted-est.html#variance-and-standard-error-estimation",
    "title": "8  Restricted Estimation",
    "section": "8.10 Variance and Standard Error Estimation",
    "text": "8.10 Variance and Standard Error Estimation\nWe have discussed covariance matrix estimation for CLS but not yet for the EMD estimator.\nThe asymptotic covariance matrix (8.26) may be estimated by replacing \\(\\boldsymbol{V}_{\\beta}\\) with a consistent estimator. It is best to construct the variance estimate using \\(\\widetilde{\\beta}_{\\text {emd. }}\\). The EMD residuals are \\(\\widetilde{e}_{i}=Y_{i}-X_{i}^{\\prime} \\widetilde{\\beta}_{\\text {emd }}\\). Using these we can estimate the matrix \\(\\Omega=\\mathbb{E}\\left[X X^{\\prime} e^{2}\\right]\\) by\n\\[\n\\widetilde{\\Omega}=\\frac{1}{n-k+q} \\sum_{i=1}^{n} X_{i} X_{i}^{\\prime} \\widetilde{e}_{i}^{2} \\text {. }\n\\]\nFollowing the formula for CLS we recommend an adjusted degrees of freedom. Given \\(\\widetilde{\\Omega}\\) the moment estimator of \\(\\boldsymbol{V}_{\\beta}\\) is \\(\\widetilde{\\boldsymbol{V}}_{\\beta}=\\widehat{\\boldsymbol{Q}}_{X X}^{-1} \\widetilde{\\Omega} \\widehat{\\boldsymbol{Q}}_{X X}^{-1}\\). Given this, we construct the variance estimator\n\\[\n\\widetilde{\\boldsymbol{V}}_{\\beta, \\mathrm{emd}}=\\widetilde{\\boldsymbol{V}}_{\\beta}-\\widetilde{\\boldsymbol{V}}_{\\beta} \\boldsymbol{R}\\left(\\boldsymbol{R}^{\\prime} \\widetilde{\\boldsymbol{V}}_{\\beta} \\boldsymbol{R}\\right)^{-1} \\boldsymbol{R}^{\\prime} \\widetilde{\\boldsymbol{V}}_{\\beta} .\n\\]\nA standard error for \\(h^{\\prime} \\widetilde{\\beta}\\) is then\n\\[\ns\\left(h^{\\prime} \\widetilde{\\beta}\\right)=\\left(n^{-1} h^{\\prime} \\widetilde{\\boldsymbol{V}}_{\\beta, \\text { emd }} h\\right)^{1 / 2} .\n\\]"
  },
  {
    "objectID": "chpt08-restricted-est.html#hausman-equality",
    "href": "chpt08-restricted-est.html#hausman-equality",
    "title": "8  Restricted Estimation",
    "section": "8.11 Hausman Equality",
    "text": "8.11 Hausman Equality\nForm (8.25) we have\n\\[\n\\begin{aligned}\n\\sqrt{n}\\left(\\widehat{\\beta}_{\\mathrm{ols}}-\\widetilde{\\beta}_{\\mathrm{emd}}\\right) &=\\widehat{\\boldsymbol{V}}_{\\beta} \\boldsymbol{R}\\left(\\boldsymbol{R}^{\\prime} \\widehat{\\boldsymbol{V}}_{\\beta} \\boldsymbol{R}\\right)^{-1} \\sqrt{n}\\left(\\boldsymbol{R}^{\\prime} \\widehat{\\beta}_{\\mathrm{ols}}-\\boldsymbol{c}\\right) \\\\\n& \\underset{d}{\\mathrm{~N}}\\left(0, \\boldsymbol{V}_{\\beta} \\boldsymbol{R}\\left(\\boldsymbol{R}^{\\prime} \\boldsymbol{V}_{\\beta} \\boldsymbol{R}\\right)^{-1} \\boldsymbol{R}^{\\prime} \\boldsymbol{V}_{\\beta}\\right)\n\\end{aligned}\n\\]\nIt follows that the asymptotic variances of the estimators satisfy the relationship\n\\[\n\\operatorname{avar}\\left[\\widehat{\\beta}_{\\mathrm{ols}}-\\widetilde{\\beta}_{\\mathrm{emd}}\\right]=\\operatorname{avar}\\left[\\widehat{\\beta}_{\\mathrm{ols}}\\right]-\\operatorname{avar}\\left[\\widetilde{\\beta}_{\\mathrm{emd}}\\right] .\n\\]\nWe call (8.37) the Hausman Equality: the asymptotic variance of the difference between an efficient and another estimator is the difference in the asymptotic variances."
  },
  {
    "objectID": "chpt08-restricted-est.html#example-mankiw-romer-and-weil-1992",
    "href": "chpt08-restricted-est.html#example-mankiw-romer-and-weil-1992",
    "title": "8  Restricted Estimation",
    "section": "8.12 Example: Mankiw, Romer and Weil (1992)",
    "text": "8.12 Example: Mankiw, Romer and Weil (1992)\nWe illustrate the methods by replicating some of the estimates reported in a well-known paper by Mankiw, Romer, and Weil (1992). The paper investigates the implications of the Solow growth model using cross-country regressions. A key equation in their paper regresses the change between 1960 and 1985 in \\(\\log\\) GDP per capita on (1) \\(\\log\\) GDP in 1960, (2) the log of the ratio of aggregate investment to Table 8.1: Estimates of Solow Growth Model\n\n\n\n\n\n\n\n\n\n\n\\(\\widehat{\\beta}_{\\text {ols }}\\)\n\\(\\widehat{\\beta}_{\\text {cls }}\\)\n\\(\\widehat{\\beta}_{\\mathrm{emd}}\\)\n\n\n\n\n\\(\\log G D P_{1960}\\)\n\\(-0.29\\)\n\\(-0.30\\)\n\\(-0.30\\)\n\n\n\n\\((0.05)\\)\n\\((0.05)\\)\n\\((0.05)\\)\n\n\n\\(\\log \\frac{I}{\\text { GDP }}\\)\n\\(0.52\\)\n\\(0.50\\)\n\\(0.46\\)\n\n\n\n\\((0.11)\\)\n\\((0.09)\\)\n\\((0.08)\\)\n\n\n\\(\\log (n+g+\\delta)\\)\n\\(-0.51\\)\n\\(-0.74\\)\n\\(-0.71\\)\n\n\n\n\\((0.24)\\)\n\\((0.08)\\)\n\\((0.07)\\)\n\n\n\\(\\log (\\) School \\()\\)\n\\(0.23\\)\n\\(0.24\\)\n\\(0.25\\)\n\n\n\n\\((0.07)\\)\n\\((0.07)\\)\n\\((0.06)\\)\n\n\nIntercept\n\\(3.02\\)\n\\(2.46\\)\n\\(2.48\\)\n\n\n\n\\((0.74)\\)\n\\((0.44)\\)\n\\((0.44)\\)\n\n\n\nStandard errors are heteroskedasticity-consistent\nGDP, (3) the log of the sum of the population growth rate \\(n\\), the technological growth rate \\(g\\), and the rate of depreciation \\(\\delta\\), and (4) the log of the percentage of the working-age population that is in secondary school (School), the latter a proxy for human-capital accumulation.\nThe data is available on the textbook webpage in the file MRW1992.\nThe sample is 98 non-oil-producing countries and the data was reported in the published paper. As \\(g\\) and \\(\\delta\\) were unknown the authors set \\(g+\\delta=0.05\\). We report least squares estimates in the first column of Table 8.1. The estimates are consistent with the Solow theory due to the positive coefficients on investment and human capital and negative coefficient for population growth. The estimates are also consistent with the convergence hypothesis (that income levels tend towards a common mean over time) as the coefficient on intial GDP is negative.\nThe authors show that in the Solow model the \\(2^{n d}, 3^{r d}\\) and \\(4^{t h}\\) coefficients sum to zero. They reestimated the equation imposing this constraint. We present constrained least squares estimates in the second column of Table \\(8.1\\) and efficient minimum distance estimates in the third column. Most of the coefficients and standard errors only exhibit small changes by imposing the constraint. The one exception is the coefficient on log population growth which increases in magnitude and its standard error decreases substantially. The differences between the CLS and EMD estimates are modest.\nWe now present Stata, R and MATLAB code which implements these estimates.\nYou may notice that the Stata code has a section which uses the Mata matrix programming language. This is used because Stata does not implement the efficient minimum distance estimator, so needs to be separately programmed. As illustrated here, the Mata language allows a Stata user to implement methods using commands which are quite similar to MATLAB."
  },
  {
    "objectID": "chpt08-restricted-est.html#misspecification",
    "href": "chpt08-restricted-est.html#misspecification",
    "title": "8  Restricted Estimation",
    "section": "8.13 Misspecification",
    "text": "8.13 Misspecification\nWhat are the consequences for a constrained estimator \\(\\widetilde{\\beta}\\) if the constraint (8.1) is incorrect? To be specific suppose that the truth is\n\\[\n\\boldsymbol{R}^{\\prime} \\beta=\\boldsymbol{c}^{*}\n\\]\nwhere \\(\\boldsymbol{c}^{*}\\) is not necessarily equal to \\(\\boldsymbol{c}\\).\nThis situation is a generalization of the analysis of “omitted variable bias” from Section \\(2.24\\) where we found that the short regression (e.g. (8.12)) is estimating a different projection coefficient than the long regression (e.g. (8.10)).\nOne answer is to apply formula (8.23) to find that\n\\[\n\\widetilde{\\beta}_{\\mathrm{md}} \\underset{p}{\\rightarrow} \\beta_{\\mathrm{md}}^{*}=\\beta-\\boldsymbol{W}^{-1} \\boldsymbol{R}\\left(\\boldsymbol{R}^{\\prime} \\boldsymbol{W}^{-1} \\boldsymbol{R}\\right)^{-1}\\left(\\boldsymbol{c}^{*}-\\boldsymbol{c}\\right) .\n\\]\nThe second term, \\(\\boldsymbol{W}^{-1} \\boldsymbol{R}\\left(\\boldsymbol{R}^{\\prime} \\boldsymbol{W}^{-1} \\boldsymbol{R}\\right)^{-1}\\left(\\boldsymbol{c}^{*}-\\boldsymbol{c}\\right)\\), shows that imposing an incorrect constraint leads to inconsistency - an asymptotic bias. We can call the limiting value \\(\\beta_{\\mathrm{md}}^{*}\\) the minimum-distance projection coefficient or the pseudo-true value implied by the restriction.\nHowever, we can say more.\nFor example, we can describe some characteristics of the approximating projections. The CLS estimator projection coefficient has the representation\n\\[\n\\beta_{\\mathrm{cls}}^{*}=\\underset{\\boldsymbol{R}^{\\prime} \\beta=\\boldsymbol{c}}{\\operatorname{argmin}} \\mathbb{E}\\left[\\left(Y-X^{\\prime} \\beta\\right)^{2}\\right],\n\\]\nthe best linear predictor subject to the constraint (8.1). The minimum distance estimator converges in probability to\n\\[\n\\beta_{\\mathrm{md}}^{*}=\\underset{\\boldsymbol{R}^{\\prime} \\beta=\\boldsymbol{c}}{\\operatorname{argmin}}\\left(\\beta-\\beta_{0}\\right)^{\\prime} \\boldsymbol{W}\\left(\\beta-\\beta_{0}\\right)\n\\]\nwhere \\(\\beta_{0}\\) is the true coefficient. That is, \\(\\beta_{\\mathrm{md}}^{*}\\) is the coefficient vector satisfying (8.1) closest to the true value in the weighted Euclidean norm. These calculations show that the constrained estimators are still reasonable in the sense that they produce good approximations to the true coefficient conditional on being required to satisfy the constraint.\nWe can also show that \\(\\widetilde{\\beta}_{\\mathrm{md}}\\) has an asymptotic normal distribution. The trick is to define the pseudotrue value\n\\[\n\\beta_{n}^{*}=\\beta-\\widehat{\\boldsymbol{W}}^{-1} \\boldsymbol{R}\\left(\\boldsymbol{R}^{\\prime} \\widehat{\\boldsymbol{W}}^{-1} \\boldsymbol{R}\\right)^{-1}\\left(\\boldsymbol{c}^{*}-\\boldsymbol{c}\\right) .\n\\]\n(Note that (8.38) and (8.39) are different!) Then\n\\[\n\\begin{aligned}\n\\sqrt{n}\\left(\\widetilde{\\beta}_{\\mathrm{md}}-\\beta_{n}^{*}\\right)=& \\sqrt{n}(\\widehat{\\beta}-\\beta)-\\widehat{\\boldsymbol{W}}^{-1} \\boldsymbol{R}\\left(\\boldsymbol{R}^{\\prime} \\widehat{\\boldsymbol{W}}^{-1} \\boldsymbol{R}\\right)^{-1} \\sqrt{n}\\left(\\boldsymbol{R}^{\\prime} \\widehat{\\beta}-\\boldsymbol{c}^{*}\\right) \\\\\n&=\\left(\\boldsymbol{I}_{k}-\\widehat{\\boldsymbol{W}}^{-1} \\boldsymbol{R}\\left(\\boldsymbol{R}^{\\prime} \\widehat{\\boldsymbol{W}}^{-1} \\boldsymbol{R}\\right)^{-1} \\boldsymbol{R}^{\\prime}\\right) \\sqrt{n}(\\widehat{\\beta}-\\beta) \\\\\n& \\underset{d}{\\longrightarrow}\\left(\\boldsymbol{I}_{k}-\\boldsymbol{W}^{-1} \\boldsymbol{R}\\left(\\boldsymbol{R}^{\\prime} \\boldsymbol{W}^{-1} \\boldsymbol{R}\\right)^{-1} \\boldsymbol{R}^{\\prime}\\right) \\mathrm{N}\\left(0, \\boldsymbol{V}_{\\beta}\\right) \\\\\n&=\\mathrm{N}\\left(0, \\boldsymbol{V}_{\\beta}(\\boldsymbol{W})\\right)\n\\end{aligned}\n\\]\nIn particular\n\\[\n\\sqrt{n}\\left(\\widetilde{\\beta}_{\\mathrm{emd}}-\\beta_{n}^{*}\\right) \\underset{d}{\\longrightarrow} \\mathrm{N}\\left(0, \\boldsymbol{V}_{\\beta}^{*}\\right) .\n\\]\nThis means that even when the constraint (8.1) is misspecified the conventional covariance matrix estimator (8.35) and standard errors (8.36) are appropriate measures of the sampling variance though the distributions are centered at the pseudo-true values (projections) \\(\\beta_{n}^{*}\\) rather than \\(\\beta\\). The fact that the estimators are biased is an unavoidable consequence of misspecification.\nAn alternative approach to the asymptotic distribution theory under misspecification uses the concept of local alternatives. It is a technical device which might seem a bit artificial but it is a powerful method to derive useful distributional approximations in a wide variety of contexts. The idea is to index the true coefficient \\(\\beta_{n}\\) by \\(n\\) via the relationship\n\\[\n\\boldsymbol{R}^{\\prime} \\beta_{n}=\\boldsymbol{c}+\\delta n^{-1 / 2} .\n\\]\nfor some \\(\\delta \\in \\mathbb{R}^{q}\\). Equation (8.41) specifies that \\(\\beta_{n}\\) violates (8.1) and thus the constraint is misspecified. However, the constraint is “close” to correct as the difference \\(\\boldsymbol{R}^{\\prime} \\beta_{n}-\\boldsymbol{c}=\\delta n^{-1 / 2}\\) is “small” in the sense that it decreases with the sample size \\(n\\). We call (8.41) local misspecification.\nThe asymptotic theory is derived as \\(n \\rightarrow \\infty\\) under the sequence of probability distributions with the coefficients \\(\\beta_{n}\\). The way to think about this is that the true value of the parameter is \\(\\beta_{n}\\) and it is “close” to satisfying (8.1). The reason why the deviation is proportional to \\(n^{-1 / 2}\\) is because this is the only choice under which the localizing parameter \\(\\delta\\) appears in the asymptotic distribution but does not dominate it. The best way to see this is to work through the asymptotic approximation.\nSince \\(\\beta_{n}\\) is the true coefficient value, then \\(Y=X^{\\prime} \\beta_{n}+e\\) and we have the standard representation for the unconstrained estimator, namely\n\\[\n\\sqrt{n}\\left(\\widehat{\\beta}-\\beta_{n}\\right)=\\left(\\frac{1}{n} \\sum_{i=1}^{n} X_{i} X_{i}^{\\prime}\\right)^{-1}\\left(\\frac{1}{\\sqrt{n}} \\sum_{i=1}^{n} X_{i} e_{i}\\right) \\underset{d}{\\longrightarrow} \\mathrm{N}\\left(0, \\boldsymbol{V}_{\\beta}\\right) .\n\\]\nThere is no difference under fixed (classical) or local asymptotics since the right-hand-side is independent of the coefficient \\(\\beta_{n}\\).\nA difference arises for the constrained estimator. Using (8.41), \\(\\boldsymbol{c}=\\boldsymbol{R}^{\\prime} \\beta_{n}-\\delta n^{-1 / 2}\\) so\n\\[\n\\boldsymbol{R}^{\\prime} \\widehat{\\beta}-\\boldsymbol{c}=\\boldsymbol{R}^{\\prime}\\left(\\widehat{\\beta}-\\beta_{n}\\right)+\\delta n^{-1 / 2}\n\\]\nand\n\\[\n\\begin{aligned}\n\\widetilde{\\beta}_{\\mathrm{md}} &=\\widehat{\\beta}-\\widehat{\\boldsymbol{W}}^{-1} \\boldsymbol{R}\\left(\\boldsymbol{R}^{\\prime} \\widehat{\\boldsymbol{W}}^{-1} \\boldsymbol{R}\\right)^{-1}\\left(\\boldsymbol{R}^{\\prime} \\widehat{\\beta}-\\boldsymbol{c}\\right) \\\\\n&=\\widehat{\\beta}-\\widehat{\\boldsymbol{W}}^{-1} \\boldsymbol{R}\\left(\\boldsymbol{R}^{\\prime} \\widehat{\\boldsymbol{W}}^{-1} \\boldsymbol{R}\\right)^{-1} \\boldsymbol{R}^{\\prime}\\left(\\widehat{\\beta}-\\beta_{n}\\right)+\\widehat{\\boldsymbol{W}}^{-1} \\boldsymbol{R}\\left(\\boldsymbol{R}^{\\prime} \\widehat{\\boldsymbol{W}}^{-1} \\boldsymbol{R}\\right)^{-1} \\delta n^{-1 / 2} .\n\\end{aligned}\n\\]\nIt follows that\n\\[\n\\sqrt{n}\\left(\\widetilde{\\beta}_{\\mathrm{md}}-\\beta_{n}\\right)=\\left(\\boldsymbol{I}_{k}-\\widehat{\\boldsymbol{W}}^{-1} \\boldsymbol{R}\\left(\\boldsymbol{R}^{\\prime} \\widehat{\\boldsymbol{W}}^{-1} \\boldsymbol{R}\\right)^{-1} \\boldsymbol{R}^{\\prime}\\right) \\sqrt{n}\\left(\\widehat{\\beta}-\\beta_{n}\\right)+\\widehat{\\boldsymbol{W}}^{-1} \\boldsymbol{R}\\left(\\boldsymbol{R}^{\\prime} \\widehat{\\boldsymbol{W}}^{-1} \\boldsymbol{R}\\right)^{-1} \\delta .\n\\]\nThe first term is asymptotically normal (from 8.42)). The second term converges in probability to a constant. This is because the \\(n^{-1 / 2}\\) local scaling in (8.41) is exactly balanced by the \\(\\sqrt{n}\\) scaling of the estimator. No alternative rate would have produced this result.\nConsequently we find that the asymptotic distribution equals\n\\[\n\\sqrt{n}\\left(\\widetilde{\\beta}_{\\mathrm{md}}-\\beta_{n}\\right) \\underset{d}{\\longrightarrow} \\mathrm{N}\\left(0, \\boldsymbol{V}_{\\beta}\\right)+\\boldsymbol{W}^{-1} \\boldsymbol{R}\\left(\\boldsymbol{R}^{\\prime} \\boldsymbol{W}^{-1} \\boldsymbol{R}\\right)^{-1} \\delta=\\mathrm{N}\\left(\\delta^{*}, \\boldsymbol{V}_{\\beta}(\\boldsymbol{W})\\right)\n\\]\nwhere \\(\\delta^{*}=\\boldsymbol{W}^{-1} \\boldsymbol{R}\\left(\\boldsymbol{R}^{\\prime} \\boldsymbol{W}^{-1} \\boldsymbol{R}\\right)^{-1} \\delta\\)\nThe asymptotic distribution (8.43) is an approximation of the sampling distribution of the restricted estimator under misspecification. The distribution (8.43) contains an asymptotic bias component \\(\\delta^{*}\\). The approximation is not fundamentally different from (8.40) - they both have the same asymptotic variances and both reflect the bias due to misspecification. The difference is that (8.40) puts the bias on the left-side of the convergence arrow while (8.43) has the bias on the right-side. There is no substantive difference between the two. However, (8.43) is more convenient for some purposes such as the analysis of the power of tests as we will explore in the next chapter."
  },
  {
    "objectID": "chpt08-restricted-est.html#nonlinear-constraints",
    "href": "chpt08-restricted-est.html#nonlinear-constraints",
    "title": "8  Restricted Estimation",
    "section": "8.14 Nonlinear Constraints",
    "text": "8.14 Nonlinear Constraints\nIn some cases it is desirable to impose nonlinear constraints on the parameter vector \\(\\beta\\). They can be written as\n\\[\nr(\\beta)=0\n\\]\nwhere \\(r: \\mathbb{R}^{k} \\rightarrow \\mathbb{R}^{q}\\). This includes the linear constraints (8.1) as a special case. An example of (8.44) which cannot be written as (8.1) is \\(\\beta_{1} \\beta_{2}=1\\), which is (8.44) with \\(r(\\beta)=\\beta_{1} \\beta_{2}-1\\).\nThe constrained least squares and minimum distance estimators of \\(\\beta\\) subject to (8.44) solve the minimization problems\n\\[\n\\begin{gathered}\n\\widetilde{\\beta}_{\\mathrm{cls}}=\\underset{r(\\beta)=0}{\\operatorname{argmin} \\operatorname{SSE}(\\beta)} \\\\\n\\widetilde{\\beta}_{\\mathrm{md}}=\\underset{r(\\beta)=0}{\\operatorname{argmin}} J(\\beta)\n\\end{gathered}\n\\]\nwhere \\(\\operatorname{SSE}(\\beta)\\) and \\(J(\\beta)\\) are defined in (8.4) and (8.19), respectively. The solutions solve the Lagrangians\n\\[\n\\mathscr{L}(\\beta, \\lambda)=\\frac{1}{2} \\operatorname{SSE}(\\beta)+\\lambda^{\\prime} r(\\beta)\n\\]\nor\n\\[\n\\mathscr{L}(\\beta, \\lambda)=\\frac{1}{2} J(\\beta)+\\lambda^{\\prime} r(\\beta)\n\\]\n\\(\\operatorname{over}(\\beta, \\lambda)\\)\nComputationally there is no general closed-form solution so they must be found numerically. Algorithms to numerically solve (8.45) and (8.46) are known as constrained optimization methods and are available in programming languages including MATLAB and R. See Chapter 12 of Probability and Statistics for Economists.\nAssumption 8.3\n\n\\(r(\\beta)=0\\).\n\\(r(\\beta)\\) is continuously differentiable at the true \\(\\beta\\).\n\\(\\operatorname{rank}(\\boldsymbol{R})=q\\), where \\(\\boldsymbol{R}=\\frac{\\partial}{\\partial \\beta} r(\\beta)^{\\prime}\\).\n\nThe asymptotic distribution is a simple generalization of the case of a linear constraint but the proof is more delicate. Theorem 8.10 Under Assumptions 7.2, 8.2, and 8.3, for \\(\\widetilde{\\beta}=\\widetilde{\\beta}_{\\mathrm{md}}\\) and \\(\\widetilde{\\beta}=\\widetilde{\\beta}_{\\text {cls }}\\) defined in (8.45) and (8.46),\n\\[\n\\sqrt{n}(\\widetilde{\\beta}-\\beta) \\underset{d}{\\longrightarrow} \\mathrm{N}\\left(0, \\boldsymbol{V}_{\\beta}(\\boldsymbol{W})\\right)\n\\]\nas \\(n \\rightarrow \\infty\\) where \\(\\boldsymbol{V}_{\\beta}(\\boldsymbol{W})\\) is defined in (8.24). For \\(\\widetilde{\\beta}_{\\text {cls }}, \\boldsymbol{W}=\\boldsymbol{Q}_{X X}\\) and \\(\\boldsymbol{V}_{\\beta}(\\boldsymbol{W})=\\) \\(\\boldsymbol{V}_{\\text {cls }}\\) as defined in Theorem 8.8. \\(\\boldsymbol{V}_{\\beta}(\\boldsymbol{W})\\) is minimized with \\(\\boldsymbol{W}=\\boldsymbol{V}_{\\beta}^{-1}\\) in which case the asymptotic variance is\n\\[\n\\boldsymbol{V}_{\\beta}^{*}=\\boldsymbol{V}_{\\beta}-\\boldsymbol{V}_{\\beta} \\boldsymbol{R}\\left(\\boldsymbol{R}^{\\prime} \\boldsymbol{V}_{\\beta} \\boldsymbol{R}\\right)^{-1} \\boldsymbol{R}^{\\prime} \\boldsymbol{V}_{\\beta} .\n\\]\nThe asymptotic covariance matrix for the efficient minimum distance estimator can be estimated by\n\\[\n\\widehat{\\boldsymbol{V}}_{\\beta}^{*}=\\widehat{\\boldsymbol{V}}_{\\beta}-\\widehat{\\boldsymbol{V}}_{\\beta} \\widehat{\\boldsymbol{R}}\\left(\\widehat{\\boldsymbol{R}}^{\\prime} \\widehat{\\boldsymbol{V}}_{\\beta} \\widehat{\\boldsymbol{R}}^{-1} \\widehat{\\boldsymbol{R}}^{\\prime} \\widehat{\\boldsymbol{V}}_{\\beta}\\right.\n\\]\nwhere\n\\[\n\\widehat{\\boldsymbol{R}}=\\frac{\\partial}{\\partial \\beta} r\\left(\\widetilde{\\beta}_{\\mathrm{md}}\\right)^{\\prime} .\n\\]\nStandard errors for the elements of \\(\\widetilde{\\beta}_{\\mathrm{md}}\\) are the square roots of the diagonal elements of \\(\\widehat{\\boldsymbol{V}}_{\\widetilde{\\beta}}^{*}=n^{-1} \\widehat{\\boldsymbol{V}}_{\\beta}^{*}\\)."
  },
  {
    "objectID": "chpt08-restricted-est.html#inequality-restrictions",
    "href": "chpt08-restricted-est.html#inequality-restrictions",
    "title": "8  Restricted Estimation",
    "section": "8.15 Inequality Restrictions",
    "text": "8.15 Inequality Restrictions\nInequality constraints on the parameter vector \\(\\beta\\) take the form\n\\[\nr(\\beta) \\geq 0\n\\]\nfor some function \\(r: \\mathbb{R}^{k} \\rightarrow \\mathbb{R}^{q}\\). The most common example is a non-negative constraint \\(\\beta_{1} \\geq 0\\).\nThe constrained least squares and minimum distance estimators can be written as\n\\[\n\\widetilde{\\beta}_{\\mathrm{cls}}=\\underset{r(\\beta) \\geq 0}{\\operatorname{argmin}} \\operatorname{SSE}(\\beta)\n\\]\nand\n\\[\n\\widetilde{\\beta}_{\\mathrm{md}}=\\underset{r(\\beta) \\geq 0}{\\operatorname{argmin}} J(\\beta) .\n\\]\nExcept in special cases the constrained estimators do not have simple algebraic solutions. An important exception is when there is a single non-negativity constraint, e.g. \\(\\beta_{1} \\geq 0\\) with \\(q=1\\). In this case the constrained estimator can be found by the following approach. Compute the uncontrained estimator \\(\\widehat{\\beta}\\). If \\(\\widehat{\\beta}_{1} \\geq 0\\) then \\(\\widetilde{\\beta}=\\widehat{\\beta}\\). Otherwise if \\(\\widehat{\\beta}_{1}<0\\) then impose \\(\\beta_{1}=0\\) (eliminate the regressor \\(X_{1}\\) ) and re-estimate. This method yields the constrained least squares estimator. While this method works when there is a single non-negativity constraint, it does not immediately generalize to other contexts.\nThe computation problems (8.50) and (8.51) are examples of quadratic programming. Quick computer algorithms are available in programming languages including MATLAB and R.\nInference on inequality-constrained estimators is unfortunately quite challenging. The conventional asymptotic theory gives rise to the following dichotomy. If the true parameter satisfies the strict inequality \\(r(\\beta)>0\\) then asymptotically the estimator is not subject to the constraint and the inequalityconstrained estimator has an asymptotic distribution equal to the unconstrained case. However if the true parameter is on the boundary, e.g., \\(r(\\beta)=0\\), then the estimator has a truncated structure. This is easiest to see in the one-dimensional case. If we have an estimator \\(\\widehat{\\beta}\\) which satisfies \\(\\sqrt{n}(\\widehat{\\beta}-\\beta) \\underset{d}{\\rightarrow} Z=\\) \\(\\mathrm{N}\\left(0, V_{\\beta}\\right)\\) and \\(\\beta=0\\), then the constrained estimator \\(\\widetilde{\\beta}=\\max [\\widehat{\\beta}, 0]\\) will have the asymptotic distribution \\(\\sqrt{n} \\widetilde{\\beta} \\underset{d}{\\longrightarrow} \\max [Z, 0]\\), a “half-normal” distribution."
  },
  {
    "objectID": "chpt08-restricted-est.html#technical-proofs",
    "href": "chpt08-restricted-est.html#technical-proofs",
    "title": "8  Restricted Estimation",
    "section": "8.16 Technical Proofs*",
    "text": "8.16 Technical Proofs*\nProof of Theorem 8.9, equation (8.28) Let \\(\\boldsymbol{R}_{\\perp}\\) be a full rank \\(k \\times(k-q)\\) matrix satisfying \\(\\boldsymbol{R}_{\\perp}^{\\prime} \\boldsymbol{V}_{\\beta} \\boldsymbol{R}=0\\) and then set \\(\\boldsymbol{C}=\\left[\\boldsymbol{R}, \\boldsymbol{R}_{\\perp}\\right]\\) which is full rank and invertible. Then we can calculate that\n\\[\n\\boldsymbol{C}^{\\prime} \\boldsymbol{V}_{\\beta}^{*} \\boldsymbol{C}=\\left[\\begin{array}{cc}\n\\boldsymbol{R}^{\\prime} \\boldsymbol{V}_{\\beta}^{*} \\boldsymbol{R} & \\boldsymbol{R}^{\\prime} \\boldsymbol{V}_{\\beta}^{*} \\boldsymbol{R}_{\\perp} \\\\\n\\boldsymbol{R}_{\\perp}^{\\prime} \\boldsymbol{V}_{\\beta}^{*} \\boldsymbol{R} & \\boldsymbol{R}_{\\perp}^{\\prime} \\boldsymbol{V}_{\\beta}^{*} \\boldsymbol{R}_{\\perp}\n\\end{array}\\right]=\\left[\\begin{array}{cc}\n0 & 0 \\\\\n0 & \\boldsymbol{R}_{\\perp}^{\\prime} \\boldsymbol{V}_{\\beta} \\boldsymbol{R}_{\\perp}\n\\end{array}\\right]\n\\]\nand\n\\[\n\\begin{aligned}\n&\\boldsymbol{C}^{\\prime} \\boldsymbol{V}_{\\beta}(\\boldsymbol{W}) \\boldsymbol{C} \\\\\n&=\\left[\\begin{array}{cc}\n\\boldsymbol{R}^{\\prime} \\boldsymbol{V}_{\\beta}^{*}(\\boldsymbol{W}) \\boldsymbol{R} & \\boldsymbol{R}^{\\prime} \\boldsymbol{V}_{\\beta}^{*}(\\boldsymbol{W}) \\boldsymbol{R}_{\\perp} \\\\\n\\boldsymbol{R}_{\\perp}^{\\prime} \\boldsymbol{V}_{\\beta}^{*}(\\boldsymbol{W}) \\boldsymbol{R} & \\boldsymbol{R}_{\\perp}^{\\prime} \\boldsymbol{V}_{\\beta}^{*}(\\boldsymbol{W}) \\boldsymbol{R}_{\\perp}\n\\end{array}\\right] \\\\\n&=\\left[\\begin{array}{cc}\n0 & 0 \\\\\n0 & \\boldsymbol{R}_{\\perp}^{\\prime} \\boldsymbol{V}_{\\beta} \\boldsymbol{R}_{\\perp}+\\boldsymbol{R}_{\\perp}^{\\prime} \\boldsymbol{W} \\boldsymbol{R}\\left(\\boldsymbol{R}^{\\prime} \\boldsymbol{W} \\boldsymbol{R}\\right)^{-1} \\boldsymbol{R}^{\\prime} \\boldsymbol{V}_{\\beta} \\boldsymbol{R}\\left(\\boldsymbol{R}^{\\prime} \\boldsymbol{W} \\boldsymbol{R}\\right)^{-1} \\boldsymbol{R}^{\\prime} \\boldsymbol{W} \\boldsymbol{R}_{\\perp}\n\\end{array}\\right] .\n\\end{aligned}\n\\]\nThus\n\\[\n\\begin{aligned}\n&\\boldsymbol{C}^{\\prime}\\left(\\boldsymbol{V}_{\\beta}(\\boldsymbol{W})-\\boldsymbol{V}_{\\beta}^{*}\\right) \\boldsymbol{C} \\\\\n&=\\boldsymbol{C}^{\\prime} \\boldsymbol{V}_{\\beta}(\\boldsymbol{W}) \\boldsymbol{C}-\\boldsymbol{C}^{\\prime} \\boldsymbol{V}_{\\beta}^{*} \\boldsymbol{C} \\\\\n&=\\left[\\begin{array}{cc}\n0 & 0 \\\\\n0 & \\boldsymbol{R}_{\\perp}^{\\prime} \\boldsymbol{W} \\boldsymbol{R}\\left(\\boldsymbol{R}^{\\prime} \\boldsymbol{W} \\boldsymbol{R}\\right)^{-\\mathbf{1}} \\boldsymbol{R}^{\\prime} \\boldsymbol{V}_{\\beta} \\boldsymbol{R}\\left(\\boldsymbol{R}^{\\prime} \\boldsymbol{W} \\boldsymbol{R}\\right)^{-\\mathbf{1}} \\boldsymbol{R}^{\\prime} \\boldsymbol{W} \\boldsymbol{R}_{\\perp}\n\\end{array}\\right] \\\\\n&\\geq 0\n\\end{aligned}\n\\]\nSince \\(\\boldsymbol{C}\\) is invertible it follows that \\(\\boldsymbol{V}_{\\beta}(\\boldsymbol{W})-\\boldsymbol{V}_{\\beta}^{*} \\geq 0\\) which is (8.28).\nProof of Theorem 8.10 We show the result for the minimum distance estimator \\(\\widetilde{\\beta}=\\widetilde{\\beta}_{\\mathrm{md}}\\) as the proof for the constrained least squares estimator is similar. For simplicity we assume that the constrained estimator is consistent \\(\\widetilde{\\beta} \\underset{p}{\\vec{p}} \\beta\\). This can be shown with more effort, but requires a deeper treatment than appropriate for this textbook.\nFor each element \\(r_{j}(\\beta)\\) of the \\(q\\)-vector \\(r(\\beta)\\), by the mean value theorem there exists a \\(\\beta_{j}^{*}\\) on the line segment joining \\(\\widetilde{\\beta}\\) and \\(\\beta\\) such that\n\\[\nr_{j}(\\widetilde{\\beta})=r_{j}(\\beta)+\\frac{\\partial}{\\partial \\beta} r_{j}\\left(\\beta_{j}^{*}\\right)^{\\prime}(\\widetilde{\\beta}-\\beta) .\n\\]\nLet \\(\\boldsymbol{R}_{n}^{*}\\) be the \\(k \\times q\\) matrix\n\\[\n\\boldsymbol{R}^{*}=\\left[\\begin{array}{llll}\n\\frac{\\partial}{\\partial \\beta} r_{1}\\left(\\beta_{1}^{*}\\right) & \\frac{\\partial}{\\partial \\beta} r_{2}\\left(\\beta_{2}^{*}\\right) & \\cdots & \\frac{\\partial}{\\partial \\beta} r_{q}\\left(\\beta_{q}^{*}\\right)\n\\end{array}\\right]\n\\]\nSince \\(\\widetilde{\\beta} \\underset{p}{\\vec{p}} \\beta\\) it follows that \\(\\beta_{j}^{*} \\vec{p} \\beta\\), and by the CMT, \\(\\boldsymbol{R}^{*} \\underset{p}{\\rightarrow} \\boldsymbol{R}\\). Stacking the (8.52), we obtain\n\\[\nr(\\widetilde{\\beta})=r(\\beta)+\\boldsymbol{R}^{* \\prime}(\\widetilde{\\beta}-\\beta) .\n\\]\nSince \\(r(\\widetilde{\\beta})=0\\) by construction and \\(r(\\beta)=0\\) by Assumption \\(8.1\\) this implies\n\\[\n0=\\boldsymbol{R}^{* \\prime}(\\widetilde{\\beta}-\\beta) .\n\\]\nThe first-order condition for (8.47) is \\(\\widehat{\\boldsymbol{W}}(\\widehat{\\beta}-\\widetilde{\\beta})=\\widehat{\\boldsymbol{R}} \\widetilde{\\lambda}\\) where \\(\\widehat{\\boldsymbol{R}}\\) is defined in (8.48). Premultiplying by \\(\\boldsymbol{R}^{* \\prime} \\widehat{\\boldsymbol{W}}^{-1}\\), inverting, and using (8.53), we find\n\\[\n\\tilde{\\lambda}=\\left(\\boldsymbol{R}^{* \\prime} \\widehat{\\boldsymbol{W}}^{-1} \\widehat{\\boldsymbol{R}}\\right)^{-1} \\boldsymbol{R}^{* \\prime}(\\widehat{\\beta}-\\widetilde{\\beta})=\\left(\\boldsymbol{R}^{* \\prime} \\widehat{\\boldsymbol{W}}^{-1} \\widehat{\\boldsymbol{R}}\\right)^{-1} \\boldsymbol{R}^{* \\prime}(\\widehat{\\beta}-\\beta) .\n\\]\nThus\n\\[\n\\widetilde{\\beta}-\\beta=\\left(\\boldsymbol{I}_{k}-\\widehat{\\boldsymbol{W}}^{-1} \\widehat{\\boldsymbol{R}}\\left(\\boldsymbol{R}_{n}^{* \\prime} \\widehat{\\boldsymbol{W}}^{-1} \\widehat{\\boldsymbol{R}}\\right)^{-1} \\boldsymbol{R}_{n}^{* \\prime}\\right)(\\widehat{\\beta}-\\beta) .\n\\]\nFrom Theorem \\(7.3\\) and Theorem \\(7.6\\) we find\n\\[\n\\begin{aligned}\n\\sqrt{n}(\\widetilde{\\beta}-\\beta) &=\\left(\\boldsymbol{I}_{k}-\\widehat{\\boldsymbol{W}}^{-1} \\widehat{\\boldsymbol{R}}\\left(\\boldsymbol{R}_{n}^{* \\prime} \\widehat{\\boldsymbol{W}}^{-1} \\widetilde{\\boldsymbol{R}}\\right)^{-1} \\boldsymbol{R}_{n}^{* \\prime}\\right) \\sqrt{n}(\\widehat{\\beta}-\\beta) \\\\\n& \\underset{d}{\\longrightarrow}\\left(\\boldsymbol{I}_{k}-\\boldsymbol{W}^{-1} \\boldsymbol{R}\\left(\\boldsymbol{R}^{\\prime} \\boldsymbol{W}^{-1} \\boldsymbol{R}\\right)^{-1} \\boldsymbol{R}^{\\prime}\\right) \\mathrm{N}\\left(0, \\boldsymbol{V}_{\\beta}\\right) \\\\\n&=\\mathrm{N}\\left(0, \\boldsymbol{V}_{\\beta}(\\boldsymbol{W})\\right)\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "chpt08-restricted-est.html#exercises",
    "href": "chpt08-restricted-est.html#exercises",
    "title": "8  Restricted Estimation",
    "section": "8.17 Exercises",
    "text": "8.17 Exercises\nExercise 8.1 In the model \\(Y=X_{1}^{\\prime} \\beta_{1}+X_{2}^{\\prime} \\beta_{2}+e\\), show directly from definition (8.3) that the CLS estimator of \\(\\beta=\\left(\\beta_{1}, \\beta_{2}\\right)\\) subject to the constraint that \\(\\beta_{2}=0\\) is the OLS regression of \\(Y\\) on \\(X_{1}\\).\nExercise 8.2 In the model \\(Y=X_{1}^{\\prime} \\beta_{1}+X_{2}^{\\prime} \\beta_{2}+e\\), show directly from definition (8.3) that the CLS estimator of \\(\\beta=\\left(\\beta_{1}, \\beta_{2}\\right)\\) subject to the constraint \\(\\beta_{1}=\\boldsymbol{c}\\) (where \\(\\boldsymbol{c}\\) is some given vector) is OLS of \\(Y-X_{1}^{\\prime} \\boldsymbol{c}\\) on \\(X_{2}\\).\nExercise 8.3 In the model \\(Y=X_{1}^{\\prime} \\beta_{1}+X_{2}^{\\prime} \\beta_{2}+e\\), with \\(\\beta_{1}\\) and \\(\\beta_{2}\\) each \\(k \\times 1\\), find the CLS estimator of \\(\\beta=\\left(\\beta_{1}, \\beta_{2}\\right)\\) subject to the constraint that \\(\\beta_{1}=-\\beta_{2}\\).\nExercise 8.4 In the linear projection model \\(Y=\\alpha+X^{\\prime} \\beta+e\\) consider the restriction \\(\\beta=0\\).\n\nFind the CLS estimator of \\(\\alpha\\) under the restriction \\(\\beta=0\\).\nFind an expression for the efficient minimum distance estimator of \\(\\alpha\\) under the restriction \\(\\beta=0\\).\n\nExercise 8.5 Verify that for \\(\\widetilde{\\beta}_{\\mathrm{cls}}\\) defined in (8.8) that \\(\\boldsymbol{R}^{\\prime} \\widetilde{\\beta}_{\\mathrm{cls}}=\\boldsymbol{c}\\).\nExercise 8.6 Prove Theorem 8.1.\nExercise 8.7 Prove Theorem 8.2, that is, \\(\\mathbb{E}\\left[\\widetilde{\\beta}_{\\text {cls }} \\mid \\boldsymbol{X}\\right]=\\beta\\), under the assumptions of the linear regression regression model and (8.1). (Hint: Use Theorem 8.1.) Exercise 8.8 Prove Theorem 8.3.\nExercise 8.9 Prove Theorem 8.4. That is, show \\(\\mathbb{E}\\left[s_{\\mathrm{cls}}^{2} \\mid \\boldsymbol{X}\\right]=\\sigma^{2}\\) under the assumptions of the homoskedastic regression model and (8.1).\nExercise 8.10 Verify (8.22), (8.23), and that the minimum distance estimator \\(\\widetilde{\\beta}_{\\mathrm{md}}\\) with \\(\\widehat{\\boldsymbol{W}}=\\widehat{\\boldsymbol{Q}}_{X X}\\) equals the CLS estimator.\nExercise 8.11 Prove Theorem 8.6.\nExercise 8.12 Prove Theorem 8.7.\nExercise 8.13 Prove Theorem 8.8. (Hint: Use that CLS is a special case of Theorem 8.7.)\nExercise 8.14 Verify that (8.26) is \\(\\boldsymbol{V}_{\\beta}(\\boldsymbol{W})\\) with \\(\\boldsymbol{W}=\\boldsymbol{V}_{\\beta}^{-1}\\).\nExercise 8.15 Prove (8.27). Hint: Use (8.26).\nExercise 8.16 Verify (8.29), (8.30) and (8.31).\nExercise 8.17 Verify (8.32), (8.33), and (8.34).\nExercise 8.18 Suppose you have two independent samples each with \\(n\\) observations which satisfy the models \\(Y_{1}=X_{1}^{\\prime} \\beta_{1}+e_{1}\\) with \\(\\mathbb{E}\\left[X_{1} e_{1}\\right]=0\\) and \\(Y_{2}=X_{2}^{\\prime} \\beta_{2}+e_{2}\\) with \\(\\mathbb{E}\\left[X_{2} e_{2}\\right]=0\\) where \\(\\beta_{1}\\) and \\(\\beta_{2}\\) are both \\(k \\times 1\\). You estimate \\(\\beta_{1}\\) and \\(\\beta_{2}\\) by OLS on each sample, with consistent asymptotic covariance matrix estimators \\(\\widehat{\\boldsymbol{V}}_{\\beta_{1}}\\) and \\(\\widehat{\\boldsymbol{V}}_{\\beta_{2}}\\). Consider efficient minimum distance estimation under the restriction \\(\\beta_{1}=\\beta_{2}\\).\n\nFind the estimator \\(\\widetilde{\\beta}\\) of \\(\\beta=\\beta_{1}=\\beta_{2}\\).\nFind the asymptotic distribution of \\(\\widetilde{\\beta}\\).\nHow would you approach the problem if the sample sizes are different, say \\(n_{1}\\) and \\(n_{2}\\) ?\n\nExercise 8.19 Use the cps09mar dataset and the subsample of white male Hispanics.\n\nEstimate the regression\n\n\\[\n\\begin{aligned}\n& \\widehat{\\log (\\text { wage })}=\\beta_{1} \\text { education }+\\beta_{2} \\text { experience }+\\beta_{3} \\text { experience }^{2} / 100+\\beta_{4} \\text { married }_{1} \\\\\n& +\\beta_{5} \\text { married }_{2}+\\beta_{6} \\text { married }_{3}+\\beta_{7} \\text { widowed }+\\beta_{8} \\text { divorced }+\\beta_{9} \\text { separated }+\\beta_{10}\n\\end{aligned}\n\\]\nwhere married \\(_{1}\\), married \\(_{2}\\), and married \\(_{3}\\) are the first three marital codes listed in Section 3.22.\n\nEstimate the equation by CLS imposing the constraints \\(\\beta_{4}=\\beta_{7}\\) and \\(\\beta_{8}=\\beta_{9}\\). Report the estimates and standard errors.\nEstimate the equation using efficient minimum distance imposing the same constraints. Report the estimates and standard errors.\nUnder what constraint on the coefficients is the wage equation non-decreasing in experience for experience up to 50 ?\nEstimate the equation imposing \\(\\beta_{4}=\\beta_{7}, \\beta_{8}=\\beta_{9}\\), and the inequality from part (d). Exercise 8.20 Take the model\n\n\\[\n\\begin{aligned}\nY &=m(X)+e \\\\\nm(x) &=\\beta_{0}+\\beta_{1} x+\\beta_{2} x^{2}+\\cdots+\\beta_{p} x^{p} \\\\\n\\mathbb{E}\\left[X^{j} e\\right] &=0, \\quad j=0, \\ldots, p \\\\\ng(x) &=\\frac{d}{d x} m(x)\n\\end{aligned}\n\\]\nwith i.i.d. observations \\(\\left(Y_{i}, X_{i}\\right), i=1, \\ldots, n\\). The order of the polynomial \\(p\\) is known.\n\nHow should we interpret the function \\(m(x)\\) given the projection assumption? How should we interpret \\(g(x)\\) ? (Briefly)\nDescribe an estimator \\(\\widehat{g}(x)\\) of \\(g(x)\\).\nFind the asymptotic distribution of \\(\\sqrt{n}(\\widehat{g}(x)-g(x))\\) as \\(n \\rightarrow \\infty\\).\nShow how to construct an asymptotic 95% confidence interval for \\(g(x)\\) (for a single \\(x\\) ).\nAssume \\(p=2\\). Describe how to estimate \\(g(x)\\) imposing the constraint that \\(m(x)\\) is concave.\nAssume \\(p=2\\). Describe how to estimate \\(g(x)\\) imposing the constraint that \\(m(u)\\) is increasing on the region \\(u \\in\\left[x_{L}, x_{U}\\right]\\)\n\nExercise 8.21 Take the linear model with restrictions \\(Y=X^{\\prime} \\beta+e\\) with \\(\\mathbb{E}[X e]=0\\) and \\(\\boldsymbol{R}^{\\prime} \\beta=\\boldsymbol{c}\\). Consider three estimators for \\(\\beta\\) :\n\n\\(\\widehat{\\beta}\\) the unconstrained least squares estimator\n\\(\\widetilde{\\beta}\\) the constrained least squares estimator\n\\(\\bar{\\beta}\\) the constrained efficient minimum distance estimator\n\nFor the three estimator define the residuals \\(\\widehat{e}_{i}=Y_{i}-X_{i}^{\\prime} \\widehat{\\beta}, \\widetilde{e}_{i}=Y_{i}-X_{i}^{\\prime} \\widetilde{\\beta}, \\bar{e}_{i}=Y_{i}-X_{i}^{\\prime} \\bar{\\beta}\\), and variance estimators \\(\\widehat{\\sigma}^{2}=n^{-1} \\sum_{i=1}^{n} \\widehat{e}_{i}^{2}, \\widetilde{\\sigma}^{2}=n^{-1} \\sum_{i=1}^{n} \\widetilde{e}_{i}^{2}\\), and \\(\\bar{\\sigma}^{2}=n^{-1} \\sum_{i=1}^{n} \\bar{e}_{i}^{2}\\).\n\nAs \\(\\bar{\\beta}\\) is the most efficient estimator and \\(\\widehat{\\beta}\\) the least, do you expect \\(\\bar{\\sigma}^{2}<\\widetilde{\\sigma}^{2}<\\widehat{\\sigma}^{2}\\) in large samples?\nConsider the statistic\n\n\\[\nT_{n}=\\widehat{\\sigma}^{-2} \\sum_{i=1}^{n}\\left(\\widehat{e}_{i}-\\widetilde{e}_{i}\\right)^{2} .\n\\]\nFind the asymptotic distribution for \\(T_{n}\\) when \\(\\boldsymbol{R}^{\\prime} \\beta=\\boldsymbol{c}\\) is true.\n\nDoes the result of the previous question simplify when the error \\(e_{i}\\) is homoskedastic?\n\nExercise 8.22 Take the linear model \\(Y=X_{1} \\beta_{1}+X_{2} \\beta_{2}+e\\) with \\(\\mathbb{E}[X e]=0\\). Consider the restriction \\(\\frac{\\beta_{1}}{\\beta_{2}}=2\\).\n\nFind an explicit expression for the CLS estimator \\(\\widetilde{\\beta}=\\left(\\widetilde{\\beta}_{1}, \\widetilde{\\beta}_{2}\\right)\\) of \\(\\beta=\\left(\\beta_{1}, \\beta_{2}\\right)\\) under the restriction. Your answer should be specific to the restriction. It should not be a generic formula for an abstract general restriction.\nDerive the asymptotic distribution of \\(\\widetilde{\\beta}_{1}\\) under the assumption that the restriction is true."
  },
  {
    "objectID": "chpt09-hypothesit-test.html",
    "href": "chpt09-hypothesit-test.html",
    "title": "9  Hypothesis Testing",
    "section": "",
    "text": "In Chapter 5 we briefly introduced hypothesis testing in the context of the normal regression model. In this chapter we explore hypothesis testing in greater detail with a particular emphasis on asymptotic inference. For more detail on the foundations see Chapter 13 of Probability and Statistics for Economists."
  },
  {
    "objectID": "chpt09-hypothesit-test.html#hypotheses",
    "href": "chpt09-hypothesit-test.html#hypotheses",
    "title": "9  Hypothesis Testing",
    "section": "9.1 Hypotheses",
    "text": "9.1 Hypotheses\nIn Chapter 8 we discussed estimation subject to restrictions, including linear restrictions (8.1), nonlinear restrictions (8.44), and inequality restrictions (8.49). In this chapter we discuss tests of such restrictions.\nHypothesis tests attempt to assess whether there is evidence contrary to a proposed restriction. Let \\(\\theta=r(\\beta)\\) be a \\(q \\times 1\\) parameter of interest where \\(r: \\mathbb{R}^{k} \\rightarrow \\Theta \\subset \\mathbb{R}^{q}\\) is some transformation. For example, \\(\\theta\\) may be a single coefficient, e.g. \\(\\theta=\\beta_{j}\\), the difference between two coefficients, e.g. \\(\\theta=\\beta_{j}-\\beta_{\\ell}\\), or the ratio of two coefficients, e.g. \\(\\theta=\\beta_{j} / \\beta_{\\ell}\\).\nA point hypothesis concerning \\(\\theta\\) is a proposed restriction such as\n\\[\n\\theta=\\theta_{0}\n\\]\nwhere \\(\\theta_{0}\\) is a hypothesized (known) value.\nMore generally, letting \\(\\beta \\in B \\subset \\mathbb{R}^{k}\\) be the parameter space, a hypothesis is a restriction \\(\\beta \\in B_{0}\\) where \\(B_{0}\\) is a proper subset of \\(B\\). This specializes to (9.1) by setting \\(B_{0}=\\left\\{\\beta \\in B: r(\\beta)=\\theta_{0}\\right\\}\\).\nIn this chapter we will focus exclusively on point hypotheses of the form (9.1) as they are the most common and relatively simple to handle.\nThe hypothesis to be tested is called the null hypothesis.\nDefinition 9.1 The null hypothesis \\(\\mathbb{M}_{0}\\) is the restriction \\(\\theta=\\theta_{0}\\) or \\(\\beta \\in B_{0}\\).\nWe often write the null hypothesis as \\(\\mathbb{M}_{0}: \\theta=\\theta_{0}\\) or \\(\\mathbb{M}_{0}: r(\\beta)=\\theta_{0}\\).\nThe complement of the null hypothesis (the collection of parameter values which do not satisfy the null hypothesis) is called the alternative hypothesis.\nDefinition 9.2 The alternative hypothesis \\(\\mathbb{M}_{1}\\) is the set \\(\\left\\{\\theta \\in \\Theta: \\theta \\neq \\theta_{0}\\right\\}\\) or \\(\\left\\{\\beta \\in B: \\beta \\notin B_{0}\\right\\}\\) We often write the alternative hypothesis as \\(\\mathbb{M}_{1}: \\theta \\neq \\theta_{0}\\) or \\(\\mathbb{M}_{1}: r(\\beta) \\neq \\theta_{0}\\). For simplicity, we often refer to the hypotheses as “the null” and “the alternative”. Figure 9.1(a) illustrates the division of the parameter space into null and alternative hypotheses.\n\n\nNull and Alternative Hypotheses\n\n\n\nAcceptance and Rejection Regions\n\nFigure 9.1: Hypothesis Testing\nIn hypothesis testing, we assume that there is a true (but unknown) value of \\(\\theta\\) and this value either satisfies \\(\\mathbb{M}_{0}\\) or does not satisfy \\(\\mathbb{M}_{0}\\). The goal of hypothesis testing is to assess whether or not \\(\\mathbb{H}_{0}\\) is true by asking if \\(\\mathbb{M}_{0}\\) is consistent with the observed data.\nTo be specific, take our example of wage determination and consider the question: Does union membership affect wages? We can turn this into a hypothesis test by specifying the null as the restriction that a coefficient on union membership is zero in a wage regression. Consider, for example, the estimates reported in Table 4.1. The coefficient for “Male Union Member” is \\(0.095\\) (a wage premium of \\(9.5 %\\) ) and the coefficient for “Female Union Member” is \\(0.022\\) (a wage premium of \\(2.2 %\\) ). These are estimates, not the true values. The question is: Are the true coefficients zero? To answer this question the testing method asks the question: Are the observed estimates compatible with the hypothesis, in the sense that the deviation from the hypothesis can be reasonably explained by stochastic variation? Or are the observed estimates incompatible with the hypothesis, in the sense that that the observed estimates would be highly unlikely if the hypothesis were true?"
  },
  {
    "objectID": "chpt09-hypothesit-test.html#acceptance-and-rejection",
    "href": "chpt09-hypothesit-test.html#acceptance-and-rejection",
    "title": "9  Hypothesis Testing",
    "section": "9.2 Acceptance and Rejection",
    "text": "9.2 Acceptance and Rejection\nA hypothesis test either accepts the null hypothesis or rejects the null hypothesis in favor of the alternative hypothesis. We can describe these two decisions as “Accept \\(\\mathbb{H}_{0}\\)” and “Reject \\(\\mathbb{H}_{0}\\)”. In the example given in the previous section the decision is either to accept the hypothesis that union membership does not affect wages, or to reject the hypothesis in favor of the alternative that union membership does affect wages.\nThe decision is based on the data and so is a mapping from the sample space to the decision set. This splits the sample space into two regions \\(S_{0}\\) and \\(S_{1}\\) such that if the observed sample falls into \\(S_{0}\\) we accept \\(\\mathbb{M}_{0}\\), while if the sample falls into \\(S_{1}\\) we reject \\(\\mathbb{M}_{0}\\). The set \\(S_{0}\\) is called the acceptance region and the set \\(S_{1}\\) the rejection or critical region.\nIt is convenient to express this mapping as a real-valued function called a test statistic\n\\[\nT=T\\left(\\left(Y_{1}, X_{1}\\right), \\ldots,\\left(Y_{n}, X_{n}\\right)\\right)\n\\]\nrelative to a critical value \\(c\\). The hypothesis test then consists of the decision rule:\n\nAccept \\(\\mathbb{H}_{0}\\) if \\(T \\leq c\\).\nReject \\(\\mathbb{M}_{0}\\) if \\(T>c\\).\n\nFigure 9.1(b) illustrates the division of the sample space into acceptance and rejection regions.\nA test statistic \\(T\\) should be designed so that small values are likely when \\(\\mathbb{H}_{0}\\) is true and large values are likely when \\(\\mathbb{M}_{1}\\) is true. There is a well developed statistical theory concerning the design of optimal tests. We will not review that theory here, but instead refer the reader to Lehmann and Romano (2005). In this chapter we will summarize the main approaches to the design of test statistics.\nThe most commonly used test statistic is the absolute value of the t-statistic\n\\[\nT=\\left|T\\left(\\theta_{0}\\right)\\right|\n\\]\nwhere\n\\[\nT(\\theta)=\\frac{\\widehat{\\theta}-\\theta}{s(\\widehat{\\theta})}\n\\]\nis the t-statistic from (7.33), \\(\\widehat{\\theta}\\) is a point estimator, and \\(s(\\widehat{\\theta})\\) its standard error. \\(T\\) is an appropriate statistic when testing hypotheses on individual coefficients or real-valued parameters \\(\\theta=h(\\beta)\\) and \\(\\theta_{0}\\) is the hypothesized value. Quite typically \\(\\theta_{0}=0\\), as interest focuses on whether or not a coefficient equals zero, but this is not the only possibility. For example, interest may focus on whether an elasticity \\(\\theta\\) equals 1 , in which case we may wish to test \\(\\mathbb{H}_{0}: \\theta=1\\)."
  },
  {
    "objectID": "chpt09-hypothesit-test.html#type-i-error",
    "href": "chpt09-hypothesit-test.html#type-i-error",
    "title": "9  Hypothesis Testing",
    "section": "9.3 Type I Error",
    "text": "9.3 Type I Error\nA false rejection of the null hypothesis \\(\\mathbb{H}_{0}\\) (rejecting \\(\\mathbb{M}_{0}\\) when \\(\\mathbb{H}_{0}\\) is true) is called a Type I error. The probability of a Type I error is called the size of the test.\n\\[\n\\mathbb{P}\\left[\\text { Reject } \\mathbb{H}_{0} \\mid \\mathbb{H}_{0} \\text { true }\\right]=\\mathbb{P}\\left[T>c \\mid \\mathbb{H}_{0} \\text { true }\\right] .\n\\]\nThe uniform size of the test is the supremum of (9.4) across all data distributions which satisfy \\(\\mathbb{H}_{0}\\). A primary goal of test construction is to limit the incidence of Type I error by bounding the size of the test.\nFor the reasons discussed in Chapter 7 , in typical econometric models the exact sampling distributions of estimators and test statistics are unknown and hence we cannot explicitly calculate (9.4). Instead, we typically rely on asymptotic approximations. Suppose that the test statistic has an asymptotic distribution under \\(\\mathbb{H}_{0}\\). That is, when \\(\\mathbb{H}_{0}\\) is true\n\\[\nT \\longrightarrow \\underset{d}{\\xi}\n\\]\nas \\(n \\rightarrow \\infty\\) for some continuously-distributed random variable \\(\\xi\\). This is not a substantive restriction as most conventional econometric tests satisfy (9.5). Let \\(G(u)=\\mathbb{P}[\\xi \\leq u]\\) denote the distribution of \\(\\xi\\). We call \\(\\xi\\) (or \\(G\\) ) the asymptotic null distribution. It is desirable to design test statistics \\(T\\) whose asymptotic null distribution \\(G\\) is known and does not depend on unknown parameters. In this case we say that \\(T\\) is asymptotically pivotal.\nFor example, if the test statistic equals the absolute \\(t\\)-statistic from (9.2), then we know from Theorem \\(7.11\\) that if \\(\\theta=\\theta_{0}\\) (that is, the null hypothesis holds), then \\(T \\underset{d}{\\rightarrow}|Z|\\) as \\(n \\rightarrow \\infty\\) where \\(Z \\sim \\mathrm{N}(0,1)\\). This means that \\(G(u)=\\mathbb{P}[|Z| \\leq u]=2 \\Phi(u)-1\\), the distribution of the absolute value of the standard normal as shown in (7.34). This distribution does not depend on unknowns and is pivotal.\nWe define the asymptotic size of the test as the asymptotic probability of a Type I error:\n\\[\n\\lim _{n \\rightarrow \\infty} \\mathbb{P}\\left[T>c \\mid \\mathbb{M}_{0} \\text { true }\\right]=\\mathbb{P}[\\xi>c]=1-G(c) .\n\\]\nWe see that the asymptotic size of the test is a simple function of the asymptotic null distribution \\(G\\) and the critical value \\(c\\). For example, the asymptotic size of a test based on the absolute t-statistic with critical value \\(c\\) is \\(2(1-\\Phi(c))\\).\nIn the dominant approach to hypothesis testing the researcher pre-selects a significance level \\(\\alpha \\epsilon\\) \\((0,1)\\) and then selects \\(c\\) so the asymptotic size is no larger than \\(\\alpha\\). When the asymptotic null distribution \\(G\\) is pivotal we accomplish this by setting \\(c\\) equal to the \\(1-\\alpha\\) quantile of the distribution \\(G\\). (If the distribution \\(G\\) is not pivotal more complicated methods must be used.) We call \\(c\\) the asymptotic critical value because it has been selected from the asymptotic null distribution. For example, since \\(2(1-\\Phi(1.96))=0.05\\) it follows that the \\(5 %\\) asymptotic critical value for the absolute t-statistic is \\(c=1.96\\). Calculation of normal critical values is done numerically in statistical software. For example, in MATLAB the command is norminv \\((1-\\alpha / 2)\\)."
  },
  {
    "objectID": "chpt09-hypothesit-test.html#t-tests",
    "href": "chpt09-hypothesit-test.html#t-tests",
    "title": "9  Hypothesis Testing",
    "section": "9.4 t tests",
    "text": "9.4 t tests\nAs we mentioned earlier, the most common test of the one-dimensional hypothesis \\(\\mathbb{H}_{0}: \\theta=\\theta_{0} \\in \\mathbb{R}\\) against the alternative \\(\\mathbb{M}_{1}: \\theta \\neq \\theta_{0}\\) is the absolute value of the \\(\\mathrm{t}\\)-statistic (9.3). We now formally state its asymptotic null distribution, which is a simple application of Theorem 7.11.\nTheorem 9.1 Under Assumptions 7.2, 7.3, and \\(\\mathbb{H}_{0}: \\theta=\\theta_{0} \\in \\mathbb{R}, T\\left(\\theta_{0}\\right) \\underset{d}{\\longrightarrow} Z \\sim\\) \\(\\mathrm{N}(0,1)\\). For \\(c\\) satisfying \\(\\alpha=2(1-\\Phi(c)), \\mathbb{P}\\left[\\left|T\\left(\\theta_{0}\\right)\\right|>c \\mid \\mathbb{H}_{0}\\right] \\rightarrow \\alpha\\), and the test “Reject \\(\\mathbb{H}_{0}\\) if \\(\\left|T\\left(\\theta_{0}\\right)\\right|>c\\)” has asymptotic size \\(\\alpha\\).\nTheorem 9.1 shows that asymptotic critical values can be taken from the normal distribution. As in our discussion of asymptotic confidence intervals (Section 7.13) the critical value could alternatively be taken from the student \\(t\\) distribution, which would be the exact test in the normal regression model (Section 5.12). Indeed, \\(t\\) critical values are the default in packages such as Stata. Since the critical values from the student \\(t\\) distribution are (slightly) larger than those from the normal distribution, student \\(t\\) critical values slightly decrease the rejection probability of the test. In practical applications the difference is typically unimportant unless the sample size is quite small (in which case the asymptotic approximation should be questioned as well).\nThe alternative hypothesis \\(\\theta \\neq \\theta_{0}\\) is sometimes called a “two-sided” alternative. In contrast, sometimes we are interested in testing for one-sided alternatives such as \\(\\mathbb{M}_{1}: \\theta>\\theta_{0}\\) or \\(\\mathbb{H}_{1}: \\theta<\\theta_{0}\\). Tests of \\(\\theta=\\theta_{0}\\) against \\(\\theta>\\theta_{0}\\) or \\(\\theta<\\theta_{0}\\) are based on the signed t-statistic \\(T=T\\left(\\theta_{0}\\right)\\). The hypothesis \\(\\theta=\\theta_{0}\\) is rejected in favor of \\(\\theta>\\theta_{0}\\) if \\(T>c\\) where \\(c\\) satisfies \\(\\alpha=1-\\Phi(c)\\). Negative values of \\(T\\) are not taken as evidence against \\(\\mathbb{M}_{0}\\), as point estimates \\(\\widehat{\\theta}\\) less than \\(\\theta_{0}\\) do not point to \\(\\theta>\\theta_{0}\\). Since the critical values are taken from the single tail of the normal distribution they are smaller than for two-sided tests. Specifically, the asymptotic \\(5 %\\) critical value is \\(c=1.645\\). Thus, we reject \\(\\theta=\\theta_{0}\\) in favor of \\(\\theta>\\theta_{0}\\) if \\(T>1.645\\).\nConversely, tests of \\(\\theta=\\theta_{0}\\) against \\(\\theta<\\theta_{0}\\) reject \\(\\mathbb{M}_{0}\\) for negative t-statistics, e.g. if \\(T<-c\\). Large positive values of \\(T\\) are not evidence for \\(\\mathbb{H}_{1}: \\theta<\\theta_{0}\\). An asymptotic \\(5 %\\) test rejects if \\(T<-1.645\\).\nThere seems to be an ambiguity. Should we use the two-sided critical value \\(1.96\\) or the one-sided critical value 1.645? The answer is that in most cases the two-sided critical value is appropriate. We should use the one-sided critical values only when the parameter space is known to satisfy a one-sided restriction such as \\(\\theta \\geq \\theta_{0}\\). This is when the test of \\(\\theta=\\theta_{0}\\) against \\(\\theta>\\theta_{0}\\) makes sense. If the restriction \\(\\theta \\geq \\theta_{0}\\) is not known a priori then imposing this restriction to test \\(\\theta=\\theta_{0}\\) against \\(\\theta>\\theta_{0}\\) does not makes sense. Since linear regression coefficients typically do not have a priori sign restrictions, the standard convention is to use two-sided critical values.\nThis may seem contrary to the way testing is presented in statistical textbooks which often focus on one-sided alternative hypotheses. The latter focus is primarily for pedagogy as the one-sided theoretical problem is cleaner and easier to understand."
  },
  {
    "objectID": "chpt09-hypothesit-test.html#type-ii-error-and-power",
    "href": "chpt09-hypothesit-test.html#type-ii-error-and-power",
    "title": "9  Hypothesis Testing",
    "section": "9.5 Type II Error and Power",
    "text": "9.5 Type II Error and Power\nA false acceptance of the null hypothesis \\(\\mathbb{H}_{0}\\) (accepting \\(\\mathbb{M}_{0}\\) when \\(\\mathbb{H}_{1}\\) is true) is called a Type II error. The rejection probability under the alternative hypothesis is called the power of the test, and equals 1 minus the probability of a Type II error:\n\\[\n\\pi(\\theta)=\\mathbb{P}\\left[\\text { Reject } \\mathbb{H}_{0} \\mid \\mathbb{H}_{1} \\text { true }\\right]=\\mathbb{P}\\left[T>c \\mid \\mathbb{M}_{1} \\text { true }\\right] .\n\\]\nWe call \\(\\pi(\\theta)\\) the power function and is written as a function of \\(\\theta\\) to indicate its dependence on the true value of the parameter \\(\\theta\\).\nIn the dominant approach to hypothesis testing the goal of test construction is to have high power subject to the constraint that the size of the test is lower than the pre-specified significance level. Generally, the power of a test depends on the true value of the parameter \\(\\theta\\), and for a well-behaved test the power is increasing both as \\(\\theta\\) moves away from the null hypothesis \\(\\theta_{0}\\) and as the sample size \\(n\\) increases.\nGiven the two possible states of the world \\(\\left(\\mathbb{M}_{0}\\right.\\) or \\(\\left.\\mathbb{H}_{1}\\right)\\) and the two possible decisions (Accept \\(\\mathbb{M}_{0}\\) or Reject \\(\\mathbb{M}_{0}\\) ) there are four possible pairings of states and decisions as is depicted in Table 9.1.\nTable 9.1: Hypothesis Testing Decisions\n\n\n\n\n\n\n\n\n\n{Accept \\(\\mathbb{H}_{0}\\)\n{Reject \\(\\mathbb{M}_{0}\\)\n\n\n\n\n\\(\\mathbb{M}_{0}\\) true\nCorrect Decision\nType I Error\n\n\n\\(\\mathbb{H}_{1}\\) true\nType II Error\nCorrect Decision\n\n\n\nGiven a test statistic \\(T\\), increasing the critical value \\(c\\) increases the acceptance region \\(S_{0}\\) while decreasing the rejection region \\(S_{1}\\). This decreases the likelihood of a Type I error (decreases the size) but increases the likelihood of a Type II error (decreases the power). Thus the choice of \\(c\\) involves a trade-off between size and the power. This is why the significance level \\(\\alpha\\) of the test cannot be set arbitrarily small. Otherwise the test will not have meaningful power.\nIt is important to consider the power of a test when interpreting hypothesis tests as an overly narrow focus on size can lead to poor decisions. For example, it is easy to design a test which has perfect size yet has trivial power. Specifically, for any hypothesis we can use the following test: Generate a random variable \\(U \\sim U[0,1]\\) and reject \\(\\mathbb{M}_{0}\\) if \\(U<\\alpha\\). This test has exact size of \\(\\alpha\\). Yet the test also has power precisely equal to \\(\\alpha\\). When the power of a test equals the size we say that the test has trivial power. Nothing is learned from such a test."
  },
  {
    "objectID": "chpt09-hypothesit-test.html#statistical-significance",
    "href": "chpt09-hypothesit-test.html#statistical-significance",
    "title": "9  Hypothesis Testing",
    "section": "9.6 Statistical Significance",
    "text": "9.6 Statistical Significance\nTesting requires a pre-selected choice of significance level \\(\\alpha\\) yet there is no objective scientific basis for choice of \\(\\alpha\\). Nevertheless, the common practice is to set \\(\\alpha=0.05\\) (5%). Alternative common values are \\(\\alpha=0.10(10 %)\\) and \\(\\alpha=0.01(1 %)\\). These choices are somewhat the by-product of traditional tables of critical values and statistical software.\nThe informal reasoning behind the \\(5 %\\) critical value is to ensure that Type I errors should be relatively unlikely - that the decision “Reject \\(\\mathbb{H}_{0}\\)” has scientific strength - yet the test retains power against reasonable alternatives. The decision “Reject \\(\\mathbb{M}_{0}\\)” means that the evidence is inconsistent with the null hypothesis in the sense that it is relatively unlikely ( 1 in 20) that data generated by the null hypothesis would yield the observed test result.\nIn contrast, the decision “Accept \\(\\mathbb{H}_{0}\\)” is not a strong statement. It does not mean that the evidence supports \\(\\mathbb{M}_{0}\\), only that there is insufficient evidence to reject \\(\\mathbb{M}_{0}\\). Because of this it is more accurate to use the label “Do not Reject \\(\\mathbb{M}_{0}\\)” instead of “Accept \\(\\mathbb{H}_{0}\\)”.\nWhen a test rejects \\(\\mathbb{M}_{0}\\) at the \\(5 %\\) significance level it is common to say that the statistic is statistically significant and if the test accepts \\(\\mathbb{M}_{0}\\) it is common to say that the statistic is not statistically significant or that it is statistically insignificant. It is helpful to remember that this is simply a compact way of saying “Using the statistic \\(T\\) the hypothesis \\(\\mathbb{H}_{0}\\) can [cannot] be rejected at the asymptotic \\(5 %\\) level.” Furthermore, when the null hypothesis \\(\\mathbb{M}_{0}: \\theta=0\\) is rejected it is common to say that the coefficient \\(\\theta\\) is statistically significant, because the test has rejected the hypothesis that the coefficient is equal to zero.\nLet us return to the example about the union wage premium as measured in Table 4.1. The absolute \\(\\mathrm{t}\\)-statistic for the coefficient on “Male Union Member” is \\(0.095 / 0.020=4.7\\), which is greater than the \\(5 %\\) asymptotic critical value of \\(1.96\\). Therefore we reject the hypothesis that union membership does not affect wages for men. In this case we can say that union membership is statistically significant for men. However, the absolute t-statistic for the coefficient on “Female Union Member” is \\(0.023 / 0.020=1.2\\), which is less than \\(1.96\\) and therefore we do not reject the hypothesis that union membership does not affect wages for women. In this case we find that membership for women is not statistically significant.\nWhen a test accepts a null hypothesis (when a test is not statistically significant) a common misinterpretation is that this is evidence that the null hypothesis is true. This is incorrect. Failure to reject is by itself not evidence. Without an analysis of power we do not know the likelihood of making a Type II error and thus are uncertain. In our wage example it would be a mistake to write that “the regression finds that female union membership has no effect on wages”. This is an incorrect and most unfortunate interpretation. The test has failed to reject the hypothesis that the coefficient is zero but that does not mean that the coefficient is actually zero.\nWhen a test rejects a null hypothesis (when a test is statistically significant) it is strong evidence against the hypothesis (because if the hypothesis were true then rejection is an unlikely event). Rejection should be taken as evidence against the null hypothesis. However, we can never conclude that the null hypothesis is indeed false as we cannot exclude the possibility that we are making a Type I error.\nPerhaps more importantly, there is an important distinction between statistical and economic significance. If we correctly reject the hypothesis \\(\\mathbb{M}_{0}: \\theta=0\\) it means that the true value of \\(\\theta\\) is non-zero. This includes the possibility that \\(\\theta\\) may be non-zero but close to zero in magnitude. This only makes sense if we interpret the parameters in the context of their relevant models. In our wage regression example we might consider wage effects of \\(1 %\\) magnitude or less as being “close to zero”. In a log wage regression this corresponds to a dummy variable with a coefficient less than \\(0.01\\). If the standard error is sufficiently small (less than \\(0.005\\) ) then a coefficient estimate of \\(0.01\\) will be statistically significant but not economically significant. This occurs frequently in applications with very large sample sizes where standard errors can be quite small.\nThe solution is to focus whenever possible on confidence intervals and the economic meaning of the coefficients. For example, if the coefficient estimate is \\(0.005\\) with a standard error of \\(0.002\\) then a \\(95 %\\) confidence interval would be \\([0.001,0.009]\\) indicating that the true effect is likely between \\(0 %\\) and \\(1 %\\), and hence is slightly positive but small. This is much more informative than the misleading statement “the effect is statistically positive”."
  },
  {
    "objectID": "chpt09-hypothesit-test.html#p-values",
    "href": "chpt09-hypothesit-test.html#p-values",
    "title": "9  Hypothesis Testing",
    "section": "9.7 P-Values",
    "text": "9.7 P-Values\nContinuing with the wage regression estimates reported in Table 4.1, consider another question: Does marriage status affect wages? To test the hypothesis that marriage status has no effect on wages, we examine the t-statistics for the coefficients on “Married Male” and “Married Female” in Table 4.1, which are \\(0.211 / 0.010=22\\) and \\(0.016 / 0.010=1.7\\), respectively. The first exceeds the asymptotic \\(5 %\\) critical value of \\(1.96\\) so we reject the hypothesis for men. The second is smaller than \\(1.96\\) so we fail to reject the hypothesis for women. Taking a second look at the statistics we see that the statistic for men (22) is exceptionally high and that for women (1.7) is only slightly below the critical value. Suppose that the \\(\\mathrm{t}\\)-statistic for women were slightly increased to 2.0. This is larger than the critical value so would lead to the decision “Reject \\(\\mathbb{M}_{0}\\)” rather than “Accept \\(\\mathbb{M}_{0}\\)”. Should we really be making a different decision if the \\(\\mathrm{t}\\)-statistic is \\(2.0\\) rather than 1.7? The difference in values is small, shouldn’t the difference in the decision be also small? Thinking through these examples it seems unsatisfactory to simply report “Accept \\(\\mathbb{M}_{0}\\)” or “Reject \\(\\mathbb{H}_{0}\\)”. These two decisions do not summarize the evidence. Instead, the magnitude of the statistic \\(T\\) suggests a “degree of evidence” against \\(\\mathbb{H}_{0}\\). How can we take this into account?\nThe answer is to report what is known as the asymptotic p-value\n\\[\np=1-G(T) .\n\\]\nSince the distribution function \\(G\\) is monotonically increasing, the p-value is a monotonically decreasing function of \\(T\\) and is an equivalent test statistic. Instead of rejecting \\(\\mathbb{R}_{0}\\) at the significance level \\(\\alpha\\) if \\(T>c\\), we can reject \\(\\mathbb{M}_{0}\\) if \\(p<\\alpha\\). Thus it is sufficient to report \\(p\\), and let the reader decide. In practice, the p-value is calculated numerically. For example, in MATLAB the command is \\(2 *(1-\\operatorname{normal} c d f(\\mathrm{abs}(\\mathrm{t})))\\).\nIt is instructive to interpret \\(p\\) as the marginal significance level: the smallest value of \\(\\alpha\\) for which the test \\(T\\) “rejects” the null hypothesis. That is, \\(p=0.11\\) means that \\(T\\) rejects \\(\\mathbb{H}_{0}\\) for all significance levels greater than \\(0.11\\), but fails to reject \\(\\mathbb{M}_{0}\\) for significance levels less than \\(0.11\\).\nFurthermore, the asymptotic p-value has a very convenient asymptotic null distribution. Since \\(T-\\vec{d}\\) \\(\\xi\\) under \\(\\mathbb{M}_{0}\\), then \\(p=1-G(T) \\underset{d}{\\longrightarrow} 1-G(\\xi)\\), which has the distribution\n\\[\n\\begin{aligned}\n\\mathbb{P}[1-G(\\xi) \\leq u] &=\\mathbb{P}[1-u \\leq G(\\xi)] \\\\\n&=1-\\mathbb{P}\\left[\\xi \\leq G^{-1}(1-u)\\right] \\\\\n&=1-G\\left(G^{-1}(1-u)\\right) \\\\\n&=1-(1-u) \\\\\n&=u,\n\\end{aligned}\n\\]\nwhich is the uniform distribution on \\([0,1]\\). (This calculation assumes that \\(G(u)\\) is strictly increasing which is true for conventional asymptotic distributions such as the normal.) Thus \\(p \\underset{d}{\\longrightarrow} U[0,1]\\). This means that the “unusualness” of \\(p\\) is easier to interpret than the “unusualness” of \\(T\\).\nAn important caveat is that the \\(\\mathrm{p}\\)-value \\(p\\) should not be interpreted as the probability that either hypothesis is true. A common mis-interpretation is that \\(p\\) is the probability “that the null hypothesis is true.” This is incorrect. Rather, \\(p\\) is the marginal significance level-a measure of the strength of information against the null hypothesis. For a t-statistic the p-value can be calculated either using the normal distribution or the student \\(t\\) distribution, the latter presented in Section 5.12. p-values calculated using the student \\(t\\) will be slightly larger, though the difference is small when the sample size is large.\nReturning to our empirical example, for the test that the coefficient on “Married Male” is zero the pvalue is \\(0.000\\). This means that it would be nearly impossible to observe a t-statistic as large as 22 when the true value of the coefficient is zero. When presented with such evidence we can say that we “strongly reject” the null hypothesis, that the test is “highly significant”, or that “the test rejects at any conventional critical value”. In contrast, the p-value for the coefficient on “Married Female” is \\(0.094\\). In this context it is typical to say that the test is “close to significant”, meaning that the p-value is larger than \\(0.05\\), but not too much larger.\nA related but inferior empirical practice is to append asterisks \\((*)\\) to coefficient estimates or test statistics to indicate the level of significance. A common practice to to append a single asterisk (\\textit{) for an estimate or test statistic which exceeds the \\(10 %\\) critical value (i.e., is significant at the \\(10 %\\) level), append a double asterisk () for a test which exceeds the \\(5 %\\) critical value, and append a triple asterisk (}) for a test which exceeds the \\(1 %\\) critical value. Such a practice can be better than a table of raw test statistics as the asterisks permit a quick interpretation of significance. On the other hand, asterisks are inferior to p-values, which are also easy and quick to interpret. The goal is essentially the same; it is wiser to report p-values whenever possible and avoid the use of asterisks.\nOur recommendation is that the best empirical practice is to compute and report the asymptotic pvalue \\(p\\) rather than simply the test statistic \\(T\\), the binary decision Accept/Reject, or appending asterisks. The p-value is a simple statistic, easy to interpret, and contains more information than the other choices.\nWe now summarize the main features of hypothesis testing.\n\nSelect a significance level \\(\\alpha\\).\nSelect a test statistic \\(T\\) with asymptotic distribution \\(T \\underset{d}{\\rightarrow} \\xi\\) under \\(\\mathbb{H}_{0}\\).\nSet the asymptotic critical value \\(c\\) so that \\(1-G(c)=\\alpha\\), where \\(G\\) is the distribution function of \\(\\xi\\).\nCalculate the asymptotic p-value \\(p=1-G(T)\\).\nReject \\(\\mathbb{R}_{0}\\) if \\(T>c\\), or equivalently \\(p<\\alpha\\).\nAccept \\(\\mathbb{H}_{0}\\) if \\(T \\leq c\\), or equivalently \\(p \\geq \\alpha\\).\nReport \\(p\\) to summarize the evidence concerning \\(\\mathbb{M}_{0}\\) versus \\(\\mathbb{M}_{1}\\)."
  },
  {
    "objectID": "chpt09-hypothesit-test.html#t-ratios-and-the-abuse-of-testing",
    "href": "chpt09-hypothesit-test.html#t-ratios-and-the-abuse-of-testing",
    "title": "9  Hypothesis Testing",
    "section": "9.8 t-ratios and the Abuse of Testing",
    "text": "9.8 t-ratios and the Abuse of Testing\nIn Section \\(4.19\\) we argued that a good applied practice is to report coefficient estimates \\(\\widehat{\\theta}\\) and standard errors \\(s(\\widehat{\\theta})\\) for all coefficients of interest in estimated models. With \\(\\widehat{\\theta}\\) and \\(s(\\widehat{\\theta})\\) the reader can easily construct confidence intervals \\([\\widehat{\\theta} \\pm 2 s(\\widehat{\\theta})]\\) and t-statistics \\(\\left(\\widehat{\\theta}-\\theta_{0}\\right) / s(\\widehat{\\theta})\\) for hypotheses of interest.\nSome applied papers (especially older ones) report t-ratios \\(T=\\widehat{\\theta} / s(\\widehat{\\theta})\\) instead of standard errors. This is poor econometric practice. While the same information is being reported (you can back out standard errors by division, e.g. \\(s(\\widehat{\\theta})=\\widehat{\\theta} / T)\\), standard errors are generally more helpful to readers than t-ratios. Standard errors help the reader focus on the estimation precision and confidence intervals, while t-ratios focus attention on statistical significance. While statistical significance is important, it is less important that the parameter estimates themselves and their confidence intervals. The focus should be on the meaning of the parameter estimates, their magnitudes, and their interpretation, not on listing which variables have significant (e.g. non-zero) coefficients. In many modern applications sample sizes are very large so standard errors can be very small. Consequently t-ratios can be large even if the coefficient estimates are economically small. In such contexts it may not be interesting to announce “The coefficient is non-zero!” Instead, what is interesting to announce is that “The coefficient estimate is economically interesting!”\nIn particular, some applied papers report coefficient estimates and t-ratios and limit their discussion of the results to describing which variables are “significant” (meaning that their t-ratios exceed 2) and the signs of the coefficient estimates. This is very poor empirical work and should be studiously avoided. It is also a recipe for banishment of your work to lower tier economics journals.\nFundamentally, the common t-ratio is a test for the hypothesis that a coefficient equals zero. This should be reported and discussed when this is an interesting economic hypothesis of interest. But if this is not the case it is distracting.\nOne problem is that standard packages, such as Stata, by default report t-statistics and p-values for every estimated coefficient. While this can be useful (as a user doesn’t need to explicitly ask to test a desired coefficient) it can be misleading as it may unintentionally suggest that the entire list of t-statistics and p-values are important. Instead, a user should focus on tests of scientifically motivated hypotheses.\nIn general, when a coefficient \\(\\theta\\) is of interest it is constructive to focus on the point estimate, its standard error, and its confidence interval. The point estimate gives our “best guess” for the value. The standard error is a measure of precision. The confidence interval gives us the range of values consistent with the data. If the standard error is large then the point estimate is not a good summary about \\(\\theta\\). The endpoints of the confidence interval describe the bounds on the likely possibilities. If the confidence interval embraces too broad a set of values for \\(\\theta\\) then the dataset is not sufficiently informative to render useful inferences about \\(\\theta\\). On the other hand if the confidence interval is tight then the data have produced an accurate estimate and the focus should be on the value and interpretation of this estimate. In contrast, the statement “the t-ratio is highly significant” has little interpretive value.\nThe above discussion requires that the researcher knows what the coefficient \\(\\theta\\) means (in terms of the economic problem) and can interpret values and magnitudes, not just signs. This is critical for good applied econometric practice.\nFor example, consider the question about the effect of marriage status on mean log wages. We had found that the effect is “highly significant” for men and “close to significant” for women. Now, let’s construct asymptotic \\(95 %\\) confidence intervals for the coefficients. The one for men is \\([0.19,0.23]\\) and that for women is \\([-0.00,0.03]\\). This shows that average wages for married men are about \\(19-23 %\\) higher than for unmarried men, which is substantial, while the difference for women is about 0-3%, which is small. These magnitudes are more informative than the results of the hypothesis tests."
  },
  {
    "objectID": "chpt09-hypothesit-test.html#wald-tests",
    "href": "chpt09-hypothesit-test.html#wald-tests",
    "title": "9  Hypothesis Testing",
    "section": "9.9 Wald Tests",
    "text": "9.9 Wald Tests\nThe t-test is appropriate when the null hypothesis is a real-valued restriction. More generally there may be multiple restrictions on the coefficient vector \\(\\beta\\). Suppose that we have \\(q>1\\) restrictions which can be written in the form (9.1). It is natural to estimate \\(\\theta=r(\\beta)\\) by the plug-in estimator \\(\\widehat{\\theta}=r(\\widehat{\\beta})\\). To test \\(\\mathbb{H}_{0}: \\theta=\\theta_{0}\\) against \\(\\mathbb{H}_{1}: \\theta \\neq \\theta_{0}\\) one approach is to measure the magnitude of the discrepancy \\(\\widehat{\\theta}-\\theta_{0}\\). As this is a vector there is more than one measure of its length. One simple measure is the weighted quadratic form known as the Wald statistic. This is (7.37) evaluated at the null hypothesis\n\\[\nW=W\\left(\\theta_{0}\\right)=\\left(\\widehat{\\theta}-\\theta_{0}\\right)^{\\prime} \\widehat{\\boldsymbol{V}}_{\\widehat{\\theta}}^{-1}\\left(\\widehat{\\theta}-\\theta_{0}\\right)\n\\]\nwhere \\(\\widehat{\\boldsymbol{V}}_{\\widehat{\\theta}}=\\widehat{\\boldsymbol{R}}^{\\prime} \\widehat{\\boldsymbol{V}}_{\\widehat{\\beta}} \\widehat{\\boldsymbol{R}}\\) is an estimator of \\(\\boldsymbol{V}_{\\widehat{\\theta}}\\) and \\(\\widehat{\\boldsymbol{R}}=\\frac{\\partial}{\\partial \\beta} r(\\widehat{\\beta})^{\\prime}\\). Notice that we can write \\(W\\) alternatively as\n\\[\nW=n\\left(\\widehat{\\theta}-\\theta_{0}\\right)^{\\prime} \\widehat{\\boldsymbol{V}}_{\\theta}^{-1}\\left(\\widehat{\\theta}-\\theta_{0}\\right)\n\\]\nusing the asymptotic variance estimator \\(\\widehat{\\boldsymbol{V}}_{\\theta}\\), or we can write it directly as a function of \\(\\widehat{\\beta}\\) as\n\\[\nW=\\left(r(\\widehat{\\beta})-\\theta_{0}\\right)^{\\prime}\\left(\\widehat{\\boldsymbol{R}}^{\\prime} \\widehat{\\boldsymbol{V}}_{\\widehat{\\beta}} \\widehat{\\boldsymbol{R}}\\right)^{-1}\\left(r(\\widehat{\\beta})-\\theta_{0}\\right) .\n\\]\nAlso, when \\(r(\\beta)=\\boldsymbol{R}^{\\prime} \\beta\\) is a linear function of \\(\\beta\\), then the Wald statistic simplifies to\n\\[\nW=\\left(\\boldsymbol{R}^{\\prime} \\widehat{\\beta}-\\theta_{0}\\right)^{\\prime}\\left(\\boldsymbol{R}^{\\prime} \\widehat{\\boldsymbol{V}}_{\\widehat{\\beta}} \\boldsymbol{R}\\right)^{-1}\\left(\\boldsymbol{R}^{\\prime} \\widehat{\\beta}-\\theta_{0}\\right) .\n\\]\nThe Wald statistic \\(W\\) is a weighted Euclidean measure of the length of the vector \\(\\widehat{\\theta}-\\theta_{0}\\). When \\(q=1\\) then \\(W=T^{2}\\), the square of the t-statistic, so hypothesis tests based on \\(W\\) and \\(|T|\\) are equivalent. The Wald statistic (9.6) is a generalization of the t-statistic to the case of multiple restrictions. As the Wald statistic is symmetric in the argument \\(\\widehat{\\theta}-\\theta_{0}\\) it treats positive and negative alternatives symmetrically. Thus the inherent alternative is always two-sided.\nAs shown in Theorem 7.13, when \\(\\beta\\) satisfies \\(r(\\beta)=\\theta_{0}\\) then \\(W \\underset{d}{\\rightarrow} \\chi_{q}^{2}\\), a chi-square random variable with \\(q\\) degrees of freedom. Let \\(G_{q}(u)\\) denote the \\(\\chi_{q}^{2}\\) distribution function. For a given significance level \\(\\alpha\\) the asymptotic critical value \\(c\\) satisfies \\(\\alpha=1-G_{q}(c)\\). For example, the \\(5 %\\) critical values for \\(q=1, q=2\\), and \\(q=3\\) are \\(3.84,5.99\\), and \\(7.82\\), respectively, and in general the level \\(\\alpha\\) critical value can be calculated in MATLAB as chi2inv \\((1-\\alpha, q)\\). An asymptotic test rejects \\(\\mathbb{M}_{0}\\) in favor of \\(\\mathbb{M}_{1}\\) if \\(W>c\\). As with t-tests, it is conventional to describe a Wald test as “significant” if \\(W\\) exceeds the \\(5 %\\) asymptotic critical value.\nTheorem 9.2 Under Assumptions 7.2, 7.3, 7.4, and \\(\\mathbb{M}_{0}: \\theta=\\theta_{0} \\in \\mathbb{R}^{q}\\), then \\(W \\vec{d}\\) \\(\\chi_{q}^{2}\\). For \\(c\\) satisfying \\(\\alpha=1-G_{q}(c), \\mathbb{P}\\left(W>c \\mid \\mathbb{H}_{0}\\right) \\longrightarrow \\alpha\\) so the test “Reject \\(\\mathbb{H}_{0}\\) if \\(W>c\\)” has asymptotic size \\(\\alpha\\).\nNotice that the asymptotic distribution in Theorem \\(9.2\\) depends solely on \\(q\\), the number of restrictions being tested. It does not depend on \\(k\\), the number of parameters estimated.\nThe asymptotic p-value for \\(W\\) is \\(p=1-G_{q}(W)\\), and this is particularly useful when testing multiple restrictions. For example, if you write that a Wald test on eight restrictions ( \\(q=8\\) ) has the value \\(W=\\) \\(11.2\\) it is difficult for a reader to assess the magnitude of this statistic unless they have quick access to a statistical table or software. Instead, if you write that the p-value is \\(p=0.19\\) (as is the case for \\(W=11.2\\) and \\(q=8\\) ) then it is simple for a reader to interpret its magnitude as “insignificant”. To calculate the asymptotic p-value for a Wald statistic in MATLAB use the command \\(1-\\operatorname{ch} i 2 c d f(w, q)\\).\nSome packages (including Stata) and papers report \\(F\\) versions of Wald statistics. For any Wald statistic \\(W\\) which tests a \\(q\\)-dimensional restriction, the \\(F\\) version of the test is\n\\[\nF=W / q .\n\\]\nWhen \\(F\\) is reported, it is conventional to use \\(F_{q, n-k}\\) critical values and \\(\\mathrm{p}\\)-values rather than \\(\\chi_{q}^{2}\\) values. The connection between Wald and F statistics is demonstrated in Section \\(9.14\\) where we show that when Wald statistics are calculated using a homoskedastic covariance matrix then \\(F=W / q\\) is identicial to the F statistic of (5.19). While there is no formal justification to using the \\(F_{q, n-k}\\) distribution for nonhomoskedastic covariance matrices, the \\(F_{q, n-k}\\) distribution provides continuity with the exact distribution theory under normality and is a bit more conservative than the \\(\\chi_{q}^{2}\\) distribution. (Furthermore, the difference is small when \\(n-k\\) is moderately large.)\nTo implement a test of zero restrictions in Stata an easy method is to use the command test X1 X2 where X1 and X2 are the names of the variables whose coefficients are hypothesized to equal zero. The \\(F\\) version of the Wald statistic is reported using the covariance matrix calculated by the method specified in the regression command. A p-value is reported, calculated using the \\(F_{q, n-k}\\) distribution.\nTo illustrate, consider the empirical results presented in Table 4.1. The hypothesis “Union membership does not affect wages” is the joint restriction that both coefficients on “Male Union Member” and “Female Union Member” are zero. We calculate the Wald statistic for this joint hypothesis and find \\(W=23\\) (or \\(F=12.5\\) ) with a p-value of \\(p=0.000\\). Thus we reject the null hypothesis in favor of the alternative that at least one of the coefficients is non-zero. This does not mean that both coefficients are non-zero, just that one of the two is non-zero. Therefore examining both the joint Wald statistic and the individual t-statistics is useful for interpretation.\nAs a second example from the same regression, take the hypothesis that married status has no effect on mean wages for women. This is the joint restriction that the coefficients on “Married Female” and “Formerly Married Female” are zero. The Wald statistic for this hypothesis is \\(W=6.4(F=3.2)\\) with a p-value of \\(0.04\\). Such a p-value is typically called “marginally significant” in the sense that it is slightly smaller than \\(0.05\\).\nThe Wald statistic was proposed by Wald (1943)."
  },
  {
    "objectID": "chpt09-hypothesit-test.html#homoskedastic-wald-tests",
    "href": "chpt09-hypothesit-test.html#homoskedastic-wald-tests",
    "title": "9  Hypothesis Testing",
    "section": "9.10 Homoskedastic Wald Tests",
    "text": "9.10 Homoskedastic Wald Tests\nIf the error is known to be homoskedastic then it is appropriate to use the homoskedastic Wald statistic (7.38) which replaces \\(\\widehat{\\boldsymbol{V}}_{\\widehat{\\theta}}\\) with the homoskedastic estimator \\(\\widehat{\\boldsymbol{V}}_{\\widehat{\\theta}}^{0}\\). This statistic equals\n\\[\n\\begin{aligned}\nW^{0} &=\\left(\\widehat{\\theta}-\\theta_{0}\\right)^{\\prime}\\left(\\widehat{\\boldsymbol{V}}_{\\widehat{\\theta}}^{0}\\right)^{-1}\\left(\\widehat{\\theta}-\\theta_{0}\\right) \\\\\n&=\\left(r(\\widehat{\\beta})-\\theta_{0}\\right)^{\\prime}\\left(\\boldsymbol{R}^{\\prime}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\widehat{\\boldsymbol{R}}\\right)^{-1}\\left(r(\\widehat{\\beta})-\\theta_{0}\\right) / s^{2} .\n\\end{aligned}\n\\]\nIn the case of linear hypotheses \\(\\mathbb{M}_{0}: \\boldsymbol{R}^{\\prime} \\beta=\\theta_{0}\\) we can write this as\n\\[\nW^{0}=\\left(\\boldsymbol{R}^{\\prime} \\widehat{\\beta}-\\theta_{0}\\right)^{\\prime}\\left(\\boldsymbol{R}^{\\prime}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\boldsymbol{R}\\right)^{-1}\\left(\\boldsymbol{R}^{\\prime} \\widehat{\\beta}-\\theta_{0}\\right) / s^{2} .\n\\]\nWe call \\(W^{0}\\) a homoskedastic Wald statistic as it is appropriate when the errors are conditionally homoskedastic.\nWhen \\(q=1\\) then \\(W^{0}=T^{2}\\), the square of the t-statistic where the latter is computed with a homoskedastic standard error. Theorem 9.3 Under Assumptions \\(7.2\\) and 7.3, \\(\\mathbb{E}\\left[e^{2} \\mid X\\right]=\\sigma^{2}>0\\), and \\(\\mathbb{M}_{0}: \\theta=\\) \\(\\theta_{0} \\in \\mathbb{R}^{q}\\), then \\(W^{0} \\underset{d}{\\longrightarrow} \\chi_{q}^{2}\\). For \\(c\\) satisfying \\(\\alpha=1-G_{q}(c), \\mathbb{P}\\left[W^{0}>c \\mid \\mathbb{H}_{0}\\right] \\longrightarrow \\alpha\\) so the test “Reject \\(\\mathbb{M}_{0}\\) if \\(W^{0}>c\\)” has asymptotic size \\(\\alpha\\)."
  },
  {
    "objectID": "chpt09-hypothesit-test.html#criterion-based-tests",
    "href": "chpt09-hypothesit-test.html#criterion-based-tests",
    "title": "9  Hypothesis Testing",
    "section": "9.11 Criterion-Based Tests",
    "text": "9.11 Criterion-Based Tests\nThe Wald statistic is based on the length of the vector \\(\\widehat{\\theta}-\\theta_{0}\\) : the discrepancy between the estimator \\(\\widehat{\\theta}=r(\\widehat{\\beta})\\) and the hypothesized value \\(\\theta_{0}\\). An alternative class of tests is based on the discrepancy between the criterion function minimized with and without the restriction.\nCriterion-based testing applies when we have a criterion function, say \\(J(\\beta)\\) with \\(\\beta \\in B\\), which is minimized for estimation, and the goal is to test \\(\\mathbb{M}_{0}: \\beta \\in B_{0}\\) versus \\(\\mathbb{M}_{1}: \\beta \\notin B_{0}\\) where \\(B_{0} \\subset \\beta\\). Minimizing the criterion function over \\(B\\) and \\(B_{0}\\) we obtain the unrestricted and restricted estimators\n\\[\n\\begin{aligned}\n&\\widehat{\\beta}=\\underset{\\beta \\in B}{\\operatorname{argmin}} J(\\beta) \\\\\n&\\widetilde{\\beta}=\\underset{\\beta \\in B_{0}}{\\operatorname{argmin}} J(\\beta) .\n\\end{aligned}\n\\]\nThe criterion-based statistic for \\(\\mathbb{H}_{0}\\) versus \\(\\mathbb{H}_{1}\\) is proportional to\n\\[\nJ=\\min _{\\beta \\in B_{0}} J(\\beta)-\\min _{\\beta \\in B} J(\\beta)=J(\\widetilde{\\beta})-J(\\widehat{\\beta}) .\n\\]\nThe criterion-based statistic \\(J\\) is sometimes called a distance statistic, a minimum-distance statistic, or a likelihood-ratio-like statistic.\nSince \\(B_{0}\\) is a subset of \\(B, J(\\widetilde{\\beta}) \\geq J(\\widehat{\\beta})\\) and thus \\(J \\geq 0\\). The statistic \\(J\\) measures the cost on the criterion of imposing the null restriction \\(\\beta \\in B_{0}\\)."
  },
  {
    "objectID": "chpt09-hypothesit-test.html#minimum-distance-tests",
    "href": "chpt09-hypothesit-test.html#minimum-distance-tests",
    "title": "9  Hypothesis Testing",
    "section": "9.12 Minimum Distance Tests",
    "text": "9.12 Minimum Distance Tests\nThe minimum distance test is based on the minimum distance criterion (8.19)\n\\[\nJ(\\beta)=n(\\widehat{\\beta}-\\beta)^{\\prime} \\widehat{\\boldsymbol{W}}(\\widehat{\\beta}-\\beta)\n\\]\nwith \\(\\widehat{\\beta}\\) the unrestricted least squares estimator. The restricted estimator \\(\\widetilde{\\beta}_{\\text {md }}\\) minimizes (9.8) subject to \\(\\beta \\in B_{0}\\). Observing that \\(J(\\widehat{\\beta})=0\\), the minimum distance statistic simplifies to\n\\[\nJ=J\\left(\\widetilde{\\beta}_{\\mathrm{md}}\\right)=n\\left(\\widehat{\\beta}-\\widetilde{\\beta}_{\\mathrm{md}}\\right)^{\\prime} \\widehat{\\boldsymbol{W}}\\left(\\widehat{\\beta}-\\widetilde{\\beta}_{\\mathrm{md}}\\right) .\n\\]\nThe efficient minimum distance estimator \\(\\widetilde{\\beta}_{\\mathrm{emd}}\\) is obtained by setting \\(\\widehat{\\boldsymbol{W}}=\\widehat{\\boldsymbol{V}}_{\\beta}^{-1}\\) in (9.8) and (9.9). The efficient minimum distance statistic for \\(\\mathbb{H}_{0}: \\beta \\in B_{0}\\) is therefore\n\\[\nJ^{*}=n\\left(\\widehat{\\beta}-\\widetilde{\\beta}_{\\mathrm{emd}}\\right)^{\\prime} \\widehat{\\boldsymbol{V}}_{\\beta}^{-1}\\left(\\widehat{\\beta}-\\widetilde{\\beta}_{\\mathrm{emd}}\\right) .\n\\]\nConsider the class of linear hypotheses \\(\\mathbb{M}_{0}: \\boldsymbol{R}^{\\prime} \\beta=\\theta_{0}\\). In this case we know from (8.25) that the efficient minimum distance estimator \\(\\widetilde{\\beta}_{\\mathrm{emd}}\\) subject to the constraint \\(\\boldsymbol{R}^{\\prime} \\beta=\\theta_{0}\\) is\n\\[\n\\widetilde{\\beta}_{\\mathrm{emd}}=\\widehat{\\beta}-\\widehat{\\boldsymbol{V}}_{\\beta} \\boldsymbol{R}\\left(\\boldsymbol{R}^{\\prime} \\widehat{\\boldsymbol{V}}_{\\beta} \\boldsymbol{R}\\right)^{-1}\\left(\\boldsymbol{R}^{\\prime} \\widehat{\\beta}-\\theta_{0}\\right)\n\\]\nand thus\n\\[\n\\widehat{\\beta}-\\widetilde{\\beta}_{\\mathrm{emd}}=\\widehat{\\boldsymbol{V}}_{\\beta} \\boldsymbol{R}\\left(\\boldsymbol{R}^{\\prime} \\widehat{\\boldsymbol{V}}_{\\beta} \\boldsymbol{R}\\right)^{-1}\\left(\\boldsymbol{R}^{\\prime} \\widehat{\\beta}-\\theta_{0}\\right) .\n\\]\nSubstituting into (9.10) we find\n\\[\n\\begin{aligned}\nJ^{*} &=n\\left(\\boldsymbol{R}^{\\prime} \\widehat{\\beta}-\\theta_{0}\\right)^{\\prime}\\left(\\boldsymbol{R}^{\\prime} \\widehat{\\boldsymbol{V}}_{\\beta} \\boldsymbol{R}\\right)^{-1} \\boldsymbol{R}^{\\prime} \\widehat{\\boldsymbol{V}}_{\\beta} \\widehat{\\boldsymbol{V}}_{\\boldsymbol{\\beta}}^{-1} \\widehat{\\boldsymbol{V}}_{\\beta} \\boldsymbol{R}\\left(\\boldsymbol{R}^{\\prime} \\widehat{\\boldsymbol{V}}_{\\beta} \\boldsymbol{R}\\right)^{-1}\\left(\\boldsymbol{R}^{\\prime} \\widehat{\\beta}-\\theta_{0}\\right) \\\\\n&=n\\left(\\boldsymbol{R}^{\\prime} \\widehat{\\beta}-\\theta_{0}\\right)^{\\prime}\\left(\\boldsymbol{R}^{\\prime} \\widehat{\\boldsymbol{V}}_{\\beta} \\boldsymbol{R}\\right)^{-1}\\left(\\boldsymbol{R}^{\\prime} \\widehat{\\beta}-\\theta_{0}\\right) \\\\\n&=W,\n\\end{aligned}\n\\]\nwhich is the Wald statistic (9.6).\nThus for linear hypotheses \\(\\mathbb{H}_{0}: \\boldsymbol{R}^{\\prime} \\beta=\\theta_{0}\\), the efficient minimum distance statistic \\(J^{*}\\) is identical to the Wald statistic (9.6). For nonlinear hypotheses, however, the Wald and minimum distance statistics are different.\nNewey and West (1987a) established the asymptotic null distribution of \\(J^{*}\\).\nTheorem 9.4 Under Assumptions \\(7.2,7.3,7.4\\), and \\(\\mathbb{H}_{0}: \\theta=\\theta_{0} \\in \\mathbb{R}^{q}, J^{*} \\underset{d}{\\longrightarrow} \\chi_{q}^{2}\\).\nTesting using the minimum distance statistic \\(J^{*}\\) is similar to testing using the Wald statistic \\(W\\). Critical values and p-values are computed using the \\(\\chi_{q}^{2}\\) distribution. \\(\\mathbb{H}_{0}\\) is rejected in favor of \\(\\mathbb{H}_{1}\\) if \\(J^{*}\\) exceeds the level \\(\\alpha\\) critical value, which can be calculated in MATLAB as chi2inv \\((1-\\alpha, q)\\). The asymptotic pvalue is \\(p=1-G_{q}\\left(J^{*}\\right)\\). In MATLAB, use the command \\(1-\\operatorname{chi} 2 \\mathrm{cdf}(\\mathrm{J}, \\mathrm{q})\\).\nWe now demonstrate Theorem 9.4. The conditions of Theorem \\(8.10\\) hold, because \\(\\mathbb{H}_{0}\\) implies Assumption 8.1. From (8.54) with \\(\\widehat{\\boldsymbol{W}}=\\widehat{\\boldsymbol{V}}_{\\beta}\\), we see that\n\\[\n\\begin{aligned}\n\\sqrt{n}\\left(\\widehat{\\beta}-\\widetilde{\\beta}_{\\mathrm{emd}}\\right) &=\\widehat{\\boldsymbol{V}}_{\\beta} \\widehat{\\boldsymbol{R}}\\left(\\boldsymbol{R}_{n}^{* \\prime} \\widehat{\\boldsymbol{V}}_{\\beta} \\widehat{\\boldsymbol{R}}\\right)^{-1} \\boldsymbol{R}_{n}^{* \\prime} \\sqrt{n}(\\widehat{\\beta}-\\beta) \\\\\n& \\underset{d}{\\longrightarrow} \\boldsymbol{V}_{\\beta} \\boldsymbol{R}\\left(\\boldsymbol{R}^{\\prime} \\boldsymbol{V}_{\\beta} \\boldsymbol{R}\\right)^{-1} \\boldsymbol{R}^{\\prime} \\mathrm{N}\\left(0, \\boldsymbol{V}_{\\beta}\\right)=\\boldsymbol{V}_{\\beta} \\boldsymbol{R} Z\n\\end{aligned}\n\\]\nwhere \\(Z \\sim \\mathrm{N}\\left(0,\\left(\\boldsymbol{R}^{\\prime} \\boldsymbol{V}_{\\beta} \\boldsymbol{R}\\right)^{-1}\\right)\\). Thus\n\\[\nJ^{*}=n\\left(\\widehat{\\beta}-\\widetilde{\\beta}_{\\mathrm{emd}}\\right)^{\\prime} \\widehat{\\boldsymbol{V}}_{\\beta}^{-1}\\left(\\widehat{\\beta}-\\widetilde{\\beta}_{\\mathrm{emd}}\\right) \\underset{d}{\\longrightarrow} Z^{\\prime} \\boldsymbol{R}^{\\prime} \\boldsymbol{V}_{\\beta} \\boldsymbol{V}_{\\beta}^{-1} \\boldsymbol{V}_{\\beta} \\boldsymbol{R} Z=Z^{\\prime}\\left(\\boldsymbol{R}^{\\prime} \\boldsymbol{V}_{\\beta} \\boldsymbol{R}\\right) Z=\\chi_{q}^{2}\n\\]\nas claimed."
  },
  {
    "objectID": "chpt09-hypothesit-test.html#minimum-distance-tests-under-homoskedasticity",
    "href": "chpt09-hypothesit-test.html#minimum-distance-tests-under-homoskedasticity",
    "title": "9  Hypothesis Testing",
    "section": "9.13 Minimum Distance Tests Under Homoskedasticity",
    "text": "9.13 Minimum Distance Tests Under Homoskedasticity\nIf we set \\(\\widehat{\\boldsymbol{W}}=\\widehat{\\boldsymbol{Q}}_{X X} / s^{2}\\) in (9.8) we obtain the criterion (8.20)\n\\[\nJ^{0}(\\beta)=n(\\widehat{\\beta}-\\beta)^{\\prime} \\widehat{\\boldsymbol{Q}}_{X X}(\\widehat{\\beta}-\\beta) / s^{2} .\n\\]\nA minimum distance statistic for \\(\\mathbb{\\Perp}_{0}: \\beta \\in B_{0}\\) is\n\\[\nJ^{0}=\\min _{\\beta \\in B_{0}} J^{0}(\\beta) .\n\\]\nEquation (8.21) showed that \\(\\operatorname{SSE}(\\beta)=n \\widehat{\\sigma}^{2}+s^{2} J^{0}(\\beta)\\). So the minimizers of \\(\\operatorname{SSE}(\\beta)\\) and \\(J^{0}(\\beta)\\) are identical. Thus the constrained minimizer of \\(J^{0}(\\beta)\\) is constrained least squares\n\\[\n\\widetilde{\\beta}_{\\text {cls }}=\\underset{\\beta \\in B_{0}}{\\operatorname{argmin}} J^{0}(\\beta)=\\underset{\\beta \\in B_{0}}{\\operatorname{argmin}} \\operatorname{SSE}(\\beta)\n\\]\nand therefore\n\\[\nJ_{n}^{0}=J_{n}^{0}\\left(\\widetilde{\\beta}_{\\mathrm{cls}}\\right)=n\\left(\\widehat{\\beta}-\\widetilde{\\beta}_{\\mathrm{cls}}\\right)^{\\prime} \\widehat{\\boldsymbol{Q}}_{X X}\\left(\\widehat{\\beta}-\\widetilde{\\beta}_{\\mathrm{cls}}\\right) / s^{2} .\n\\]\nIn the special case of linear hypotheses \\(\\mathbb{M}_{0}: \\boldsymbol{R}^{\\prime} \\beta=\\theta_{0}\\), the constrained least squares estimator subject to \\(\\boldsymbol{R}^{\\prime} \\beta=\\theta_{0}\\) has the solution (8.9)\n\\[\n\\widetilde{\\beta}_{\\mathrm{cls}}=\\widehat{\\beta}-\\widehat{\\boldsymbol{Q}}_{X X}^{-1} \\boldsymbol{R}\\left(\\boldsymbol{R}^{\\prime} \\widehat{\\boldsymbol{Q}}_{X X}^{-1} \\boldsymbol{R}\\right)^{-1}\\left(\\boldsymbol{R}^{\\prime} \\widehat{\\beta}-\\theta_{0}\\right)\n\\]\nand solving we find\n\\[\nJ^{0}=n\\left(\\boldsymbol{R}^{\\prime} \\widehat{\\beta}-\\theta_{0}\\right)^{\\prime}\\left(\\boldsymbol{R}^{\\prime} \\widehat{\\boldsymbol{Q}}_{X X}^{-1} \\boldsymbol{R}\\right)^{-1}\\left(\\boldsymbol{R}^{\\prime} \\widehat{\\beta}-\\theta_{0}\\right) / s^{2}=W^{0} .\n\\]\nThis is the homoskedastic Wald statistic (9.7). Thus for testing linear hypotheses, homoskedastic minimum distance and Wald statistics agree.\nFor nonlinear hypotheses they disagree, but have the same null asymptotic distribution.\nTheorem 9.5 Under Assumptions \\(7.2\\) and \\(7.3, \\mathbb{E}\\left[e^{2} \\mid X\\right]=\\sigma^{2}>0\\), and \\(\\mathbb{M}_{0}: \\theta=\\) \\(\\theta_{0} \\in \\mathbb{R}^{q}\\), then \\(J^{0} \\underset{d}{\\longrightarrow} \\chi_{q}^{2}\\)"
  },
  {
    "objectID": "chpt09-hypothesit-test.html#f-tests",
    "href": "chpt09-hypothesit-test.html#f-tests",
    "title": "9  Hypothesis Testing",
    "section": "9.14 F Tests",
    "text": "9.14 F Tests\nIn Section \\(5.13\\) we introduced the \\(F\\) test for exclusion restrictions in the normal regression model. In this section we generalize this test to a broader set of restrictions. Let \\(B_{0} \\subset \\mathbb{R}^{k}\\) be a constrained parameter space which imposes \\(q\\) restrictions on \\(\\beta\\).\nLet \\(\\widehat{\\beta}_{\\text {ols }}\\) be the unrestricted least squares estimator and let \\(\\widehat{\\sigma}^{2}=n^{-1} \\sum_{i=1}^{n}\\left(Y_{i}-X_{i}^{\\prime} \\widehat{\\beta}_{\\text {ols }}\\right)^{2}\\) be the associated estimator of \\(\\sigma^{2}\\). Let \\(\\widetilde{\\beta}_{\\text {cls }}\\) be the CLS estimator (9.11) satisfying \\(\\widetilde{\\beta}_{\\text {cls }} \\in B_{0}\\) and let \\(\\widetilde{\\sigma}^{2}=n^{-1} \\sum_{i=1}^{n}\\left(Y_{i}-X_{i}^{\\prime} \\widetilde{\\beta}_{\\text {cls }}\\right)^{2}\\) be the associated estimator of \\(\\sigma^{2}\\). The \\(F\\) statistic for testing \\(\\mathbb{M}_{0}: \\beta \\in B_{0}\\) is\n\\[\nF=\\frac{\\left(\\tilde{\\sigma}^{2}-\\widehat{\\sigma}^{2}\\right) / q}{\\widehat{\\sigma}^{2} /(n-k)} .\n\\]\nWe can alternatively write\n\\[\nF=\\frac{\\operatorname{SSE}\\left(\\widetilde{\\beta}_{\\mathrm{cls}}\\right)-\\operatorname{SSE}\\left(\\widehat{\\beta}_{\\mathrm{ols}}\\right)}{q s^{2}}\n\\]\nwhere \\(\\operatorname{SSE}(\\beta)=\\sum_{i=1}^{n}\\left(Y_{i}-X_{i}^{\\prime} \\beta\\right)^{2}\\) is the sum-of-squared errors.\nThis shows that \\(F\\) is a criterion-based statistic. Using (8.21) we can also write \\(F=J^{0} / q\\), so the \\(F\\) statistic is identical to the homoskedastic minimum distance statistic divided by the number of restrictions \\(q\\).\nAs we discussed in the previous section, in the special case of linear hypotheses \\(\\mathbb{M}_{0}: \\boldsymbol{R}^{\\prime} \\beta=\\theta_{0}, J^{0}=\\) \\(W^{0}\\). It follows that in this case \\(F=W^{0} / q\\). Thus for linear restrictions the \\(F\\) statistic equals the homoskedastic Wald statistic divided by \\(q\\). It follows that they are equivalent tests for \\(\\mathbb{H}_{0}\\) against \\(\\mathbb{H}_{1}\\). Theorem 9.6 For tests of linear hypotheses \\(\\mathbb{H}_{0}: \\boldsymbol{R}^{\\prime} \\beta=\\theta_{0} \\in \\mathbb{R}^{q}\\), the \\(\\mathrm{F}\\) statistic equals \\(F=W^{0} / q\\) where \\(W^{0}\\) is the homoskedastic Wald statistic. Thus under 7.2, \\(\\mathbb{E}\\left[e^{2} \\mid X\\right]=\\sigma^{2}>0\\), and \\(\\mathbb{M}_{0}: \\theta=\\theta_{0}\\), then \\(F \\underset{d}{\\longrightarrow} \\chi_{q}^{2} / q\\).\nWhen using an \\(F\\) statistic it is conventional to use the \\(F_{q, n-k}\\) distribution for critical values and pvalues. Critical values are given in MATLAB by \\(f\\) inv \\((1-\\alpha, q, n-k)\\) and \\(p\\)-values by \\(1-f c d f(F, q, n-k)\\). Alternatively, the \\(\\chi_{q}^{2} / q\\) distribution can be used, using chi2inv \\((1-\\alpha, q) / q\\) and \\(1-\\operatorname{chi} 2 c d f(F * q, q)\\), respectively. Using the \\(F_{q, n-k}\\) distribution is a prudent small sample adjustment which yields exact answers if the errors are normal and otherwise slightly increasing the critical values and p-values relative to the asymptotic approximation. Once again, if the sample size is small enough that the choice makes a difference then probably we shouldn’t be trusting the asymptotic approximation anyway!\nAn elegant feature about (9.12) or (9.13) is that they are directly computable from the standard output from two simple OLS regressions, as the sum of squared errors (or regression variance) is a typical printed output from statistical packages and is often reported in applied tables. Thus \\(F\\) can be calculated by hand from standard reported statistics even if you don’t have the original data (or if you are sitting in a seminar and listening to a presentation!).\nIf you are presented with an \\(F\\) statistic (or a Wald statistic, as you can just divide by \\(q\\) ) but don’t have access to critical values, a useful rule of thumb is to know that for large \\(n\\) the \\(5 %\\) asymptotic critical value is decreasing as \\(q\\) increases and is less than 2 for \\(q \\geq 7\\).\nA word of warning: In many statistical packages when an OLS regression is estimated an “F-statistic” is automatically reported even though no hypothesis test was requested. What the package is reporting is an \\(F\\) statistic of the hypothesis that all slope coefficients \\({ }^{1}\\) are zero. This was a popular statistic in the early days of econometric reporting when sample sizes were very small and researchers wanted to know if there was “any explanatory power” to their regression. This is rarely an issue today as sample sizes are typically sufficiently large that this \\(F\\) statistic is nearly always highly significant. While there are special cases where this \\(F\\) statistic is useful these cases are not typical. As a general rule there is no reason to report this \\(F\\) statistic."
  },
  {
    "objectID": "chpt09-hypothesit-test.html#hausman-tests",
    "href": "chpt09-hypothesit-test.html#hausman-tests",
    "title": "9  Hypothesis Testing",
    "section": "9.15 Hausman Tests",
    "text": "9.15 Hausman Tests\nHausman (1978) introduced a general idea about how to test a hypothesis \\(\\mathbb{M}_{0}\\). If you have two estimators, one which is efficient under \\(\\mathbb{M}_{0}\\) but inconsistent under \\(\\mathbb{H}_{1}\\), and another which is consistent under \\(\\mathbb{H}_{1}\\), then construct a test as a quadratic form in the differences of the estimators. In the case of testing a hypothesis \\(\\mathbb{M}_{0}: r(\\beta)=\\theta_{0}\\) let \\(\\widehat{\\beta}_{\\text {ols }}\\) denote the unconstrained least squares estimator and let \\(\\widetilde{\\beta}_{\\text {emd }}\\) denote the efficient minimum distance estimator which imposes \\(r(\\beta)=\\theta_{0}\\). Both estimators are consistent under \\(\\mathbb{M}_{0}\\) but \\(\\widetilde{\\beta}_{\\mathrm{emd}}\\) is asymptotically efficient. Under \\(\\mathbb{H}_{1}, \\widehat{\\beta}_{\\mathrm{ols}}\\) is consistent for \\(\\beta\\) but \\(\\widetilde{\\beta}_{\\mathrm{emd}}\\) is inconsistent. The difference has the asymptotic distribution\n\\[\n\\sqrt{n}\\left(\\widehat{\\beta}_{\\mathrm{ols}}-\\widetilde{\\beta}_{\\mathrm{emd}}\\right) \\underset{d}{\\longrightarrow} \\mathrm{N}\\left(0, \\boldsymbol{V}_{\\beta} \\boldsymbol{R}\\left(\\boldsymbol{R}^{\\prime} \\boldsymbol{V}_{\\beta} \\boldsymbol{R}\\right)^{-1} \\boldsymbol{R}^{\\prime} \\boldsymbol{V}_{\\beta}\\right) .\n\\]\nLet \\(\\boldsymbol{A}^{-}\\)denote the Moore-Penrose generalized inverse. The Hausman statistic for \\(\\mathbb{H}_{0}\\) is\n\\[\n\\begin{aligned}\n& H=\\left(\\widehat{\\beta}_{\\mathrm{ols}}-\\widetilde{\\beta}_{\\mathrm{emd}}\\right)^{\\prime} \\widehat{\\operatorname{avar}}\\left(\\widehat{\\beta}_{\\mathrm{ols}}-\\widetilde{\\beta}_{\\mathrm{emd}}\\right)^{-}\\left(\\widehat{\\beta}_{\\mathrm{ols}}-\\widetilde{\\beta}_{\\mathrm{emd}}\\right) \\\\\n& =n\\left(\\widehat{\\beta}_{\\mathrm{ols}}-\\widetilde{\\beta}_{\\mathrm{emd}}\\right)^{\\prime}\\left(\\widehat{\\boldsymbol{V}}_{\\beta} \\widehat{\\boldsymbol{R}}\\left(\\widehat{\\boldsymbol{R}}^{\\prime} \\widehat{\\boldsymbol{V}}_{\\beta} \\widehat{\\boldsymbol{R}}\\right)^{-1} \\widehat{\\boldsymbol{R}}^{\\prime} \\widehat{\\boldsymbol{V}}_{\\beta}\\right)^{-}\\left(\\widehat{\\beta}_{\\mathrm{ols}}-\\widetilde{\\beta}_{\\mathrm{emd}}\\right) .\n\\end{aligned}\n\\]\n\\({ }^{1}\\) All coefficients except the intercept. The matrix \\(\\widehat{\\boldsymbol{V}}_{\\beta}^{1 / 2} \\widehat{\\boldsymbol{R}}\\left(\\widehat{\\boldsymbol{R}}^{\\prime} \\widehat{\\boldsymbol{V}}_{\\beta} \\widehat{\\boldsymbol{R}}\\right)^{-1} \\widehat{\\boldsymbol{R}}^{\\prime} \\widehat{\\boldsymbol{V}}_{\\beta}^{1 / 2}\\) idempotent so its generalized inverse is itself. (See Section A.11.) It follows that\n\\[\n\\begin{aligned}\n& =\\widehat{\\boldsymbol{V}}_{\\beta}^{-1 / 2} \\widehat{\\boldsymbol{V}}_{\\beta}^{1 / 2} \\widehat{\\boldsymbol{R}}\\left(\\widehat{\\boldsymbol{R}}^{\\prime} \\widehat{\\boldsymbol{V}}_{\\beta} \\widehat{\\boldsymbol{R}}\\right)^{-1} \\widehat{\\boldsymbol{R}}^{\\prime} \\widehat{\\boldsymbol{V}}_{\\beta}^{1 / 2} \\widehat{\\boldsymbol{V}}_{\\beta}^{-1 / 2} \\\\\n& =\\widehat{\\boldsymbol{R}}\\left(\\widehat{\\boldsymbol{R}}^{\\prime} \\widehat{\\boldsymbol{V}}_{\\beta} \\widehat{\\boldsymbol{R}}\\right)^{-1} \\widehat{\\boldsymbol{R}}^{\\prime} .\n\\end{aligned}\n\\]\nThus the Hausman statistic is\n\\[\nH=n\\left(\\widehat{\\beta}_{\\mathrm{ols}}-\\widetilde{\\beta}_{\\mathrm{emd}}\\right)^{\\prime} \\widehat{\\boldsymbol{R}}\\left(\\widehat{\\boldsymbol{R}}^{\\prime} \\widehat{\\boldsymbol{V}}_{\\beta} \\widehat{\\boldsymbol{R}}\\right)^{-1} \\widehat{\\boldsymbol{R}}^{\\prime}\\left(\\widehat{\\beta}_{\\mathrm{ols}}-\\widetilde{\\beta}_{\\mathrm{emd}}\\right) .\n\\]\nIn the context of linear restrictions, \\(\\widehat{\\boldsymbol{R}}=\\boldsymbol{R}\\) and \\(\\boldsymbol{R}^{\\prime} \\widetilde{\\beta}=\\theta_{0}\\) so the statistic takes the form\n\\[\nH=n\\left(\\boldsymbol{R}^{\\prime} \\widehat{\\beta}_{\\mathrm{ols}}-\\theta_{0}\\right)^{\\prime} \\widehat{\\boldsymbol{R}}\\left(\\boldsymbol{R}^{\\prime} \\widehat{\\boldsymbol{V}}_{\\beta} \\boldsymbol{R}\\right)^{-1}\\left(\\boldsymbol{R}^{\\prime} \\widehat{\\beta}_{\\mathrm{ols}}-\\theta_{0}\\right),\n\\]\nwhich is precisely the Wald statistic. With nonlinear restrictions \\(W\\) and \\(H\\) can differ.\nIn either case we see that that the asymptotic null distribution of the Hausman statistic \\(H\\) is \\(\\chi_{q}^{2}\\), so the appropriate test is to reject \\(\\mathbb{M}_{0}\\) in favor of \\(\\mathbb{H}_{1}\\) if \\(H>c\\) where \\(c\\) is a critical value taken from the \\(\\chi_{q}^{2}\\) distribution.\nTheorem 9.7 For general hypotheses the Hausman test statistic is\n\\[\nH=n\\left(\\widehat{\\beta}_{\\mathrm{ols}}-\\widetilde{\\beta}_{\\mathrm{emd}}\\right)^{\\prime} \\widehat{\\boldsymbol{R}}\\left(\\widehat{\\boldsymbol{R}}^{\\prime} \\widehat{\\boldsymbol{V}}_{\\beta} \\widehat{\\boldsymbol{R}}\\right)^{-1} \\widehat{\\boldsymbol{R}}^{\\prime}\\left(\\widehat{\\beta}_{\\mathrm{ols}}-\\widetilde{\\beta}_{\\mathrm{emd}}\\right) .\n\\]\nUnder Assumptions \\(7.2,7.3,7.4\\), and \\(\\mathbb{M}_{0}: r(\\beta)=\\theta_{0} \\in \\mathbb{R}^{q}, H \\underset{d}{\\longrightarrow} \\chi_{q}^{2}\\)"
  },
  {
    "objectID": "chpt09-hypothesit-test.html#score-tests",
    "href": "chpt09-hypothesit-test.html#score-tests",
    "title": "9  Hypothesis Testing",
    "section": "9.16 Score Tests",
    "text": "9.16 Score Tests\nScore tests are traditionally derived in likelihood analysis but can more generally be constructed from first-order conditions evaluated at restricted estimates. We focus on the likelihood derivation.\nGiven the log likelihood function \\(\\ell_{n}\\left(\\beta, \\sigma^{2}\\right)\\), a restriction \\(\\mathbb{H}_{0}: r(\\beta)=\\theta_{0}\\), and restricted estimators \\(\\widetilde{\\beta}\\) and \\(\\widetilde{\\sigma}^{2}\\), the score statistic for \\(\\mathbb{H}_{0}\\) is defined as\n\\[\nS=\\left(\\frac{\\partial}{\\partial \\beta} \\ell_{n}\\left(\\widetilde{\\beta}, \\widetilde{\\sigma}^{2}\\right)\\right)^{\\prime}\\left(-\\frac{\\partial^{2}}{\\partial \\beta \\partial \\beta^{\\prime}} \\ell_{n}\\left(\\widetilde{\\beta}, \\widetilde{\\sigma}^{2}\\right)\\right)^{-1}\\left(\\frac{\\partial}{\\partial \\beta} \\ell_{n}\\left(\\widetilde{\\beta}, \\widetilde{\\sigma}^{2}\\right)\\right) .\n\\]\nThe idea is that if the restriction is true then the restricted estimators should be close to the maximum of the log-likelihood where the derivative is zero. However if the restriction is false then the restricted estimators should be distant from the maximum and the derivative should be large. Hence small values of \\(S\\) are expected under \\(\\mathbb{H}_{0}\\) and large values under \\(\\mathbb{H}_{1}\\). Tests of \\(\\mathbb{M}_{0}\\) reject for large values of \\(S\\).\nWe explore the score statistic in the context of the normal regression model and linear hypotheses \\(r(\\beta)=\\boldsymbol{R}^{\\prime} \\beta\\). Recall that in the normal regression log-likelihood function is\n\\[\n\\ell_{n}\\left(\\beta, \\sigma^{2}\\right)=-\\frac{n}{2} \\log \\left(2 \\pi \\sigma^{2}\\right)-\\frac{1}{2 \\sigma^{2}} \\sum_{i=1}^{n}\\left(Y_{i}-X_{i}^{\\prime} \\beta\\right)^{2} .\n\\]\n\nThe constrained MLE under linear hypotheses is constrained least squares\n\\[\n\\begin{aligned}\n\\widetilde{\\beta} &=\\widehat{\\beta}-\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\boldsymbol{R}\\left[\\boldsymbol{R}^{\\prime}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\boldsymbol{R}\\right]^{-1}\\left(\\boldsymbol{R}^{\\prime} \\widehat{\\beta}-\\boldsymbol{c}\\right) \\\\\n\\widetilde{e}_{i} &=Y_{i}-X_{i}^{\\prime} \\widetilde{\\beta} \\\\\n\\widetilde{\\sigma}^{2} &=\\frac{1}{n} \\sum_{i=1}^{n} \\widetilde{e}_{i}^{2}\n\\end{aligned}\n\\]\nWe can calculate that the derivative and Hessian are\n\\[\n\\begin{aligned}\n\\frac{\\partial}{\\partial \\beta} \\ell_{n}\\left(\\widetilde{\\beta}, \\widetilde{\\sigma}^{2}\\right) &=\\frac{1}{\\widetilde{\\sigma}^{2}} \\sum_{i=1}^{n} X_{i}\\left(Y_{i}-X_{i}^{\\prime} \\widetilde{\\beta}\\right)=\\frac{1}{\\widetilde{\\sigma}^{2}} \\boldsymbol{X}^{\\prime} \\widetilde{\\boldsymbol{e}} \\\\\n-\\frac{\\partial^{2}}{\\partial \\beta \\partial \\beta^{\\prime}} \\ell_{n}\\left(\\widetilde{\\beta}, \\widetilde{\\sigma}^{2}\\right) &=\\frac{1}{\\widetilde{\\sigma}^{2}} \\sum_{i=1}^{n} X_{i} X_{i}^{\\prime}=\\frac{1}{\\widetilde{\\sigma}^{2}} \\boldsymbol{X}^{\\prime} \\boldsymbol{X}\n\\end{aligned}\n\\]\nSince \\(\\widetilde{\\boldsymbol{e}}=\\boldsymbol{Y}-\\boldsymbol{X} \\widetilde{\\beta}\\) we can further calculate that\n\\[\n\\begin{aligned}\n\\frac{\\partial}{\\partial \\beta} \\ell_{n}\\left(\\widetilde{\\beta}, \\widetilde{\\sigma}^{2}\\right) &=\\frac{1}{\\widetilde{\\sigma}^{2}}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)\\left(\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\boldsymbol{X}^{\\prime} \\boldsymbol{Y}-\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\boldsymbol{X}^{\\prime} \\boldsymbol{X} \\widetilde{\\beta}\\right) \\\\\n&=\\frac{1}{\\widetilde{\\sigma}^{2}}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)(\\widehat{\\beta}-\\widetilde{\\beta}) \\\\\n&=\\frac{1}{\\widetilde{\\sigma}^{2}} \\boldsymbol{R}\\left[\\boldsymbol{R}^{\\prime}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\boldsymbol{R}\\right]^{-1}\\left(\\boldsymbol{R}^{\\prime} \\widehat{\\beta}-\\boldsymbol{c}\\right) .\n\\end{aligned}\n\\]\nTogether we find that\n\\[\nS=\\left(\\boldsymbol{R}^{\\prime} \\widehat{\\boldsymbol{\\beta}}-\\boldsymbol{c}\\right)^{\\prime}\\left(\\boldsymbol{R}^{\\prime}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\boldsymbol{R}\\right)^{-1}\\left(\\boldsymbol{R}^{\\prime} \\widehat{\\boldsymbol{\\beta}}-\\boldsymbol{c}\\right) / \\widetilde{\\sigma}^{2} .\n\\]\nThis is identical to the homoskedastic Wald statistic with \\(s^{2}\\) replaced by \\(\\widetilde{\\sigma}^{2}\\). We can also write \\(S\\) as a monotonic transformation of the \\(F\\) statistic, as\n\\[\nS=n \\frac{\\left(\\widetilde{\\sigma}^{2}-\\widehat{\\sigma}^{2}\\right)}{\\widetilde{\\sigma}^{2}}=n\\left(1-\\frac{\\widehat{\\sigma}^{2}}{\\widetilde{\\sigma}^{2}}\\right)=n\\left(1-\\frac{1}{1+\\frac{q}{n-k} F}\\right) .\n\\]\nThe test “Reject \\(\\mathbb{M}_{0}\\) for large values of \\(S\\)” is identical to the test “Reject \\(\\mathbb{M}_{0}\\) for large values of \\(F\\)” so they are identical tests. Since for the normal regression model the exact distribution of \\(F\\) is known, it is better to use the \\(F\\) statistic with \\(F\\) p-values.\nIn more complicated settings a potential advantage of score tests is that they are calculated using the restricted parameter estimates \\(\\widetilde{\\beta}\\) rather than the unrestricted estimates \\(\\widehat{\\beta}\\). Thus when \\(\\widetilde{\\beta}\\) is relatively easy to calculate there can be a preference for score statistics. This is not a concern for linear restrictions.\nMore generally, score and score-like statistics can be constructed from first-order conditions evaluated at restricted parameter estimates. Also, when test statistics are constructed using covariance matrix estimators which are calculated using restricted parameter estimates (e.g. restricted residuals) then these are often described as score tests.\nAn example of the latter is the Wald-type statistic\n\\[\nW=\\left(r(\\widehat{\\beta})-\\theta_{0}\\right)^{\\prime}\\left(\\widehat{\\boldsymbol{R}}^{\\prime} \\widetilde{\\boldsymbol{V}}_{\\widehat{\\beta}} \\widehat{\\boldsymbol{R}}\\right)^{-1}\\left(r(\\widehat{\\beta})-\\theta_{0}\\right)\n\\]\nwhere the covariance matrix estimate \\(\\widetilde{\\boldsymbol{V}}_{\\widehat{\\beta}}\\) is calculated using the restricted residuals \\(\\widetilde{e}_{i}=Y_{i}-X_{i}^{\\prime} \\widetilde{\\beta}\\). This may be a good choice when \\(\\beta\\) and \\(\\theta\\) are high-dimensional as in this context there may be worry that the estimator \\(\\widehat{\\boldsymbol{V}}_{\\widehat{\\beta}}\\) is imprecise."
  },
  {
    "objectID": "chpt09-hypothesit-test.html#problems-with-tests-of-nonlinear-hypotheses",
    "href": "chpt09-hypothesit-test.html#problems-with-tests-of-nonlinear-hypotheses",
    "title": "9  Hypothesis Testing",
    "section": "9.17 Problems with Tests of Nonlinear Hypotheses",
    "text": "9.17 Problems with Tests of Nonlinear Hypotheses\nWhile the \\(t\\) and Wald tests work well when the hypothesis is a linear restriction on \\(\\beta\\), they can work quite poorly when the restrictions are nonlinear. This can be seen by a simple example introduced by Lafontaine and White (1986). Take the model \\(Y \\sim \\mathrm{N}\\left(\\beta, \\sigma^{2}\\right.\\) ) and consider the hypothesis \\(\\mathbb{H}_{0}: \\beta=1\\). Let \\(\\widehat{\\beta}\\) and \\(\\widehat{\\sigma}^{2}\\) be the sample mean and variance of \\(Y\\). The standard Wald statistic to test \\(\\mathbb{H}_{0}\\) is\n\\[\nW=n \\frac{(\\widehat{\\beta}-1)^{2}}{\\widehat{\\sigma}^{2}} .\n\\]\nNotice that \\(\\mathbb{M}_{0}\\) is equivalent to the hypothesis \\(\\mathbb{M}_{0}(s): \\beta^{s}=1\\) for any positive integer \\(s\\). Letting \\(r(\\beta)=\\) \\(\\beta^{s}\\), and noting \\(\\boldsymbol{R}=s \\beta^{s-1}\\), we find that the Wald statistic to test \\(\\mathbb{M}_{0}(s)\\) is\n\\[\nW_{s}=n \\frac{\\left(\\widehat{\\beta}^{s}-1\\right)^{2}}{\\widehat{\\sigma}^{2} s^{2} \\widehat{\\beta}^{2 s-2}} .\n\\]\nWhile the hypothesis \\(\\beta^{s}=1\\) is unaffected by the choice of \\(s\\), the statistic \\(W_{s}\\) varies with \\(s\\). This is an unfortunate feature of the Wald statistic.\nTo demonstrate this effect, we have plotted in Figure \\(9.2\\) the Wald statistic \\(W_{s}\\) as a function of \\(s\\), setting \\(n / \\widehat{\\sigma}^{2}=10\\). The increasing line is for the case \\(\\widehat{\\beta}=0.8\\). The decreasing line is for the case \\(\\widehat{\\beta}=1.6\\). It is easy to see that in each case there are values of \\(s\\) for which the test statistic is significant relative to asymptotic critical values, while there are other values of \\(s\\) for which the test statistic is insignificant. This is distressing because the choice of \\(s\\) is arbitrary and irrelevant to the actual hypothesis.\nOur first-order asymptotic theory is not useful to help pick \\(s\\), as \\(W_{s} \\underset{d}{\\longrightarrow} \\chi_{1}^{2}\\) under \\(\\mathbb{H}_{0}\\) for any \\(s\\). This is a context where Monte Carlo simulation can be quite useful as a tool to study and compare the exact distributions of statistical procedures in finite samples. The method uses random simulation to create artificial datasets to which we apply the statistical tools of interest. This produces random draws from the statistic’s sampling distribution. Through repetition, features of this distribution can be calculated.\nIn the present context of the Wald statistic, one feature of importance is the Type I error of the test using the asymptotic \\(5 %\\) critical value \\(3.84\\) - the probability of a false rejection, \\(\\mathbb{P}\\left[W_{s}>3.84 \\mid \\beta=1\\right]\\). Given the simplicity of the model this probability depends only on \\(s, n\\), and \\(\\sigma^{2}\\). In Table \\(9.2\\) we report the results of a Monte Carlo simulation where we vary these three parameters. The value of \\(s\\) is varied from 1 to \\(10, n\\) is varied among 20,100 , and 500 , and \\(\\sigma\\) is varied among 1 and 3 . The table reports the simulation estimate of the Type I error probability from 50,000 random samples. Each row of the table corresponds to a different value of \\(s\\) - and thus corresponds to a particular choice of test statistic. The second through seventh columns contain the Type I error probabilities for different combinations of \\(n\\) and \\(\\sigma\\). These probabilities are calculated as the percentage of the 50,000 simulated Wald statistics \\(W_{s}\\) which are larger than 3.84. The null hypothesis \\(\\beta^{s}=1\\) is true so these probabilities are Type I error.\nTo interpret the table remember that the ideal Type I error probability is \\(5 %(.05)\\) with deviations indicating distortion. Type I error rates between \\(3 %\\) and \\(8 %\\) are considered reasonable. Error rates above \\(10 %\\) are considered excessive. Rates above \\(20 %\\) are unacceptable. When comparing statistical procedures we compare the rates row by row, looking for tests for which rejection rates are close to \\(5 %\\) and rarely fall outside of the \\(3 %-8 %\\) range. For this particular example the only test which meets this criterion is the conventional \\(W=W_{1}\\) test. Any other \\(s\\) leads to a test with unacceptable Type I error probabilities.\nIn Table \\(9.2\\) you can also see the impact of variation in sample size. In each case the Type I error probability improves towards \\(5 %\\) as the sample size \\(n\\) increases. There is, however, no magic choice of \\(n\\) for which all tests perform uniformly well. Test performance deteriorates as \\(s\\) increases which is not surprising given the dependence of \\(W_{s}\\) on \\(s\\) as shown in Figure 9.2.\n\nFigure 9.2: Wald Statistic as a Function of \\(s\\)\nIn this example it is not surprising that the choice \\(s=1\\) yields the best test statistic. Other choices are arbitrary and would not be used in practice. While this is clear in this particular example, in other examples natural choices are not obvious and the best choices may be counter-intuitive.\nThis point can be illustrated through an example based on Gregory and Veall (1985). Take the model\n\\[\n\\begin{aligned}\nY &=\\beta_{0}+X_{1} \\beta_{1}+X_{2} \\beta_{2}+e \\\\\n\\mathbb{E}[X e] &=0\n\\end{aligned}\n\\]\nand the hypothesis \\(\\mathbb{M}_{0}: \\frac{\\beta_{1}}{\\beta_{2}}=\\theta_{0}\\) where \\(\\theta_{0}\\) is a known constant. Equivalently, define \\(\\theta=\\beta_{1} / \\beta_{2}\\) so the hypothesis can be stated as \\(\\mathbb{M}_{0}: \\theta=\\theta_{0}\\).\nLet \\(\\widehat{\\beta}=\\left(\\widehat{\\beta}_{0}, \\widehat{\\beta}_{1}, \\widehat{\\beta}_{2}\\right)\\) be the least squares estimator of \\((9.14)\\), let \\(\\widehat{\\boldsymbol{V}}_{\\widehat{\\beta}}\\) be an estimator of the covariance matrix for \\(\\widehat{\\beta}\\) and set \\(\\widehat{\\theta}=\\widehat{\\beta}_{1} / \\widehat{\\beta}_{2}\\). Define\n\\[\n\\widehat{\\boldsymbol{R}}_{1}=\\left(\\begin{array}{c}\n0 \\\\\n\\frac{1}{\\widehat{\\beta}_{2}} \\\\\n-\\frac{\\widehat{\\beta}_{1}}{\\widehat{\\beta}_{2}^{2}}\n\\end{array}\\right)\n\\]\nTable 9.2: Type I Error Probability of Asymptotic \\(5 % W(s)\\) Test\n\nRejection frequencies from 50,000 simulated random samples.\nso that the standard error for \\(\\widehat{\\theta}\\) is \\(s(\\widehat{\\theta})=\\left(\\widehat{\\boldsymbol{R}}_{1}^{\\prime} \\widehat{\\boldsymbol{V}}_{\\widehat{\\beta}} \\widehat{\\boldsymbol{R}}_{1}\\right)^{1 / 2}\\). In this case a t-statistic for \\(\\mathbb{M}_{0}\\) is\n\\[\nT_{1}=\\frac{\\left(\\frac{\\widehat{\\beta}_{1}}{\\widehat{\\beta}_{2}}-\\theta_{0}\\right)}{s(\\widehat{\\theta})} .\n\\]\nAn alternative statistic can be constructed through reformulating the null hypothesis as\n\\[\n\\mathbb{M}_{0}: \\beta_{1}-\\theta_{0} \\beta_{2}=0 .\n\\]\nA t-statistic based on this formulation of the hypothesis is\n\\[\nT_{2}=\\frac{\\widehat{\\beta}_{1}-\\theta_{0} \\widehat{\\beta}_{2}}{\\left(\\boldsymbol{R}_{2}^{\\prime} \\widehat{\\boldsymbol{V}}_{\\widehat{\\beta}} \\boldsymbol{R}_{2}\\right)^{1 / 2}}\n\\]\nwhere\n\\[\n\\boldsymbol{R}_{2}=\\left(\\begin{array}{c}\n0 \\\\\n1 \\\\\n-\\theta_{0}\n\\end{array}\\right) \\text {. }\n\\]\nTo compare \\(T_{1}\\) and \\(T_{2}\\) we perform another simple Monte Carlo simulation. We let \\(X_{1}\\) and \\(X_{2}\\) be mutually independent \\(\\mathrm{N}(0,1)\\) variables, \\(e\\) be an independent \\(\\mathrm{N}\\left(0, \\sigma^{2}\\right)\\) draw with \\(\\sigma=3\\), and normalize \\(\\beta_{0}=0\\) and \\(\\beta_{1}=1\\). This leaves \\(\\beta_{2}\\) as a free parameter along with sample size \\(n\\). We vary \\(\\beta_{2}\\) among \\(0.1\\), \\(0.25,0.50,0.75\\), and \\(1.0\\) and \\(n\\) among 100 and 500 .\nThe one-sided Type I error probabilities \\(\\mathbb{P}[T<-1.645]\\) and \\(\\mathbb{P}[T>1.645]\\) are calculated from 50,000 simulated samples. The results are presented in Table 9.3. Ideally, the entries in the table should be \\(0.05\\). However, the rejection rates for the \\(T_{1}\\) statistic diverge greatly from this value, especially for small values of \\(\\beta_{2}\\). The left tail probabilities \\(\\mathbb{P}\\left[T_{1}<-1.645\\right]\\) greatly exceed \\(5 %\\), while the right tail probabilities \\(\\mathbb{P}\\left[T_{1}>1.645\\right]\\) are close to zero in most cases. In contrast, the rejection rates for the \\(T_{2}\\) statistic are invariant to the value of \\(\\beta_{2}\\) and equal \\(5 %\\) for both sample sizes. The implication of Table \\(9.3\\) is that the two t-ratios have dramatically different sampling behavior.\nThe common message from both examples is that Wald statistics are sensitive to the algebraic formulation of the null hypothesis. Table 9.3: Type I Error Probability of Asymptotic 5% t-tests\n\nRejection frequencies from 50,000 simulated random samples.\nA simple solution is to use the minimum distance statistic \\(J\\) which equals \\(W\\) with \\(r=1\\) in the first example, and \\(\\left|T_{2}\\right|\\) in the second example. The minimum distance statistic is invariant to the algebraic formulation of the null hypothesis so is immune to this problem. Whenever possible, the Wald statistic should not be used to test nonlinear hypotheses.\nTheoretical investigations of these issues include Park and Phillips (1988) and Dufour (1997)."
  },
  {
    "objectID": "chpt09-hypothesit-test.html#monte-carlo-simulation",
    "href": "chpt09-hypothesit-test.html#monte-carlo-simulation",
    "title": "9  Hypothesis Testing",
    "section": "9.18 Monte Carlo Simulation",
    "text": "9.18 Monte Carlo Simulation\nIn Section \\(9.17\\) we introduced the method of Monte Carlo simulation to illustrate the small sample problems with tests of nonlinear hypotheses. In this section we describe the method in more detail.\nRecall, our data consist of observations \\(\\left(Y_{i}, X_{i}\\right)\\) which are random draws from a population distribution \\(F\\). Let \\(\\theta\\) be a parameter and let \\(T=T\\left(\\left(Y_{1}, X_{1}\\right), \\ldots,\\left(Y_{n}, X_{n}\\right), \\theta\\right)\\) be a statistic of interest, for example an estimator \\(\\widehat{\\theta}\\) or a t-statistic \\((\\widehat{\\theta}-\\theta) / s(\\widehat{\\theta})\\). The exact distribution of \\(T\\) is\n\\[\nG(u, F)=\\mathbb{P}[T \\leq u \\mid F] .\n\\]\nWhile the asymptotic distribution of \\(T\\) might be known, the exact (finite sample) distribution \\(G\\) is generally unknown.\nMonte Carlo simulation uses numerical simulation to compute \\(G(u, F)\\) for selected choices of \\(F\\). This is useful to investigate the performance of the statistic \\(T\\) in reasonable situations and sample sizes. The basic idea is that for any given \\(F\\) the distribution function \\(G(u, F)\\) can be calculated numerically through simulation. The name Monte Carlo derives from the Mediterranean gambling resort where games of chance are played.\nThe method of Monte Carlo is simple to describe. The researcher chooses \\(F\\) (the distribution of the pseudo data) and the sample size \\(n\\). A “true” value of \\(\\theta\\) is implied by this choice, or equivalently the value \\(\\theta\\) is selected directly by the researcher which implies restrictions on \\(F\\).\nThen the following experiment is conducted by computer simulation:\n\n\\(n\\) independent random pairs \\(\\left(Y_{i}^{*}, X_{i}^{*}\\right), i=1, \\ldots, n\\), are drawn from the distribution \\(F\\) using the computer’s random number generator.\nThe statistic \\(T=T\\left(\\left(Y_{1}^{*}, X_{1}^{*}\\right), \\ldots,\\left(Y_{n}^{*}, X_{n}^{*}\\right), \\theta\\right)\\) is calculated on this pseudo data.\n\nFor step 1, computer packages have built-in random number procedures including \\(U[0,1]\\) and \\(N(0,1)\\). From these most random variables can be constructed. (For example, a chi-square can be generated by sums of squares of normals.) For step 2, it is important that the statistic be evaluated at the “true” value of \\(\\theta\\) corresponding to the choice of \\(F\\).\nThe above experiment creates one random draw \\(T\\) from the distribution \\(G(u, F)\\). This is one observation from an unknown distribution. Clearly, from one observation very little can be said. So the researcher repeats the experiment \\(B\\) times where \\(B\\) is a large number. Typically, we set \\(B \\geq 1000\\). We will discuss this choice later.\nNotationally, let the \\(b^{t h}\\) experiment result in the draw \\(T_{b}, b=1, \\ldots, B\\). These results are stored. After all \\(B\\) experiments have been calculated these results constitute a random sample of size \\(B\\) from the distribution of \\(G(u, F)=\\mathbb{P}\\left[T_{b} \\leq u\\right]=\\mathbb{P}[T \\leq u \\mid F]\\).\nFrom a random sample we can estimate any feature of interest using (typically) a method of moments estimator. We now describe some specific examples.\nSuppose we are interested in the bias, mean-squared error (MSE), and/or variance of the distribution of \\(\\widehat{\\theta}-\\theta\\). We then set \\(T=\\widehat{\\theta}-\\theta\\), run the above experiment, and calculate\n\\[\n\\begin{aligned}\n\\widehat{\\operatorname{bias}}[\\widehat{\\theta}] &=\\frac{1}{B} \\sum_{b=1}^{B} T_{b}=\\frac{1}{B} \\sum_{b=1}^{B} \\widehat{\\theta}_{b}-\\theta \\\\\n\\widehat{\\operatorname{mse}}[\\widehat{\\theta}] &=\\frac{1}{B} \\sum_{b=1}^{B}\\left(T_{b}\\right)^{2}=\\frac{1}{B} \\sum_{b=1}^{B}\\left(\\widehat{\\theta}_{b}-\\theta\\right)^{2} \\\\\n\\widehat{\\operatorname{var}}[\\widehat{\\theta}] &=\\widehat{\\operatorname{mse}}[\\widehat{\\theta}]-(\\widehat{\\operatorname{bias}}[\\hat{\\theta}])^{2}\n\\end{aligned}\n\\]\nSuppose we are interested in the Type I error associated with an asymptotic 5% two-sided t-test. We would then set \\(T=|\\widehat{\\theta}-\\theta| / s(\\widehat{\\theta})\\) and calculate\n\\[\n\\widehat{P}=\\frac{1}{B} \\sum_{b=1}^{B} \\mathbb{1}\\left\\{T_{b} \\geq 1.96\\right\\},\n\\]\nthe percentage of the simulated t-ratios which exceed the asymptotic \\(5 %\\) critical value.\nSuppose we are interested in the \\(5 %\\) and \\(95 %\\) quantile of \\(T=\\widehat{\\theta}\\) or \\(T=(\\widehat{\\theta}-\\theta) / s(\\widehat{\\theta})\\). We then compute the \\(5 %\\) and \\(95 %\\) sample quantiles of the sample \\(\\left\\{T_{b}\\right\\}\\). For details on quantile estimation see Section \\(11.13\\) of Probability and Statistics for Economists.\nThe typical purpose of a Monte Carlo simulation is to investigate the performance of a statistical procedure in realistic settings. Generally, the performance will depend on \\(n\\) and \\(F\\). In many cases an estimator or test may perform wonderfully for some values and poorly for others. It is therefore useful to conduct a variety of experiments for a selection of choices of \\(n\\) and \\(F\\).\nAs discussed above the researcher must select the number of experiments \\(B\\). Often this is called the number of replications. Quite simply, a larger \\(B\\) results in more precise estimates of the features of interest of \\(G\\) but requires more computational time. In practice, therefore, the choice of \\(B\\) is often guided by the computational demands of the statistical procedure. Since the results of a Monte Carlo experiment are estimates computed from a random sample of size \\(B\\) it is straightforward to calculate standard errors for any quantity of interest. If the standard error is too large to make a reliable inference then \\(B\\) will have to be increased. A useful rule-of-thumb is to set \\(B=10,000\\) whenever possible.\nIn particular, it is simple to make inferences about rejection probabilities from statistical tests, such as the percentage estimate reported in (9.15). The random variable \\(\\mathbb{1}\\left\\{T_{b} \\geq 1.96\\right\\}\\) is i.i.d. Bernoulli, equalling 1 with probability \\(p=\\mathbb{E}\\left[\\mathbb{1}\\left\\{T_{b} \\geq 1.96\\right\\}\\right]\\). The average (9.15) is therefore an unbiased estimator of \\(p\\) with standard error \\(s(\\widehat{p})=\\sqrt{p(1-p) / B}\\). As \\(p\\) is unknown, this may be approximated by replacing \\(p\\) with \\(\\widehat{p}\\) or with an hypothesized value. For example, if we are assessing an asymptotic \\(5 %\\) test, then we can set \\(s(\\widehat{p})=\\sqrt{(.05)(.95) / B} \\simeq .22 / \\sqrt{B}\\). Hence, standard errors for \\(B=100,1000\\), and 5000, are, respectively, \\(s(\\widehat{p})=.022, .007\\), and \\(.003 .\\) Most papers in econometric methods and some empirical papers include the results of Monte Carlo simulations to illustrate the performance of their methods. When extending existing results it is good practice to start by replicating existing (published) results. This may not be exactly possible in the case of simulation results as they are inherently random. For example suppose a paper investigates a statistical test and reports a simulated rejection probability of \\(0.07\\) based on a simulation with \\(B=100\\) replications. Suppose you attempt to replicate this result and find a rejection probability of \\(0.03\\) (again using \\(B=100\\) simulation replications). Should you conclude that you have failed in your attempt? Absolutely not! Under the hypothesis that both simulations are identical you have two independent estimates, \\(\\widehat{p}_{1}=0.07\\) and \\(\\widehat{p}_{2}=0.03\\), of a common probability \\(p\\). The asymptotic (as \\(B \\rightarrow \\infty\\) ) distribution of their difference is \\(\\sqrt{B}\\left(\\widehat{p}_{1}-\\widehat{p}_{2}\\right) \\underset{d}{\\longrightarrow} \\mathrm{N}(0,2 p(1-p))\\), so a standard error for \\(\\widehat{p}_{1}-\\widehat{p}_{2}=0.04\\) is \\(\\widehat{s}=\\sqrt{2 \\bar{p}(1-\\bar{p}) / B} \\simeq 0.03\\), using the estimate \\(\\bar{p}=\\left(\\widehat{p}_{1}+\\widehat{p}_{2}\\right) / 2\\). Since the t-ratio \\(0.04 / 0.03=1.3\\) is not statistically significant it is incorrect to reject the null hypothesis that the two simulations are identical. The difference between the results \\(\\widehat{p}_{1}=0.07\\) and \\(\\widehat{p}_{2}=0.03\\) is consistent with random variation.\nWhat should be done? The first mistake was to copy the previous paper’s choice of \\(B=100\\). Instead, suppose you set \\(B=10,000\\) and now obtain \\(\\widehat{p}_{2}=0.04\\). Then \\(\\widehat{p}_{1}-\\widehat{p}_{2}=0.03\\) and a standard error is \\(\\widehat{s}=\\) \\(\\sqrt{\\bar{p}(1-\\bar{p})(1 / 100+1 / 10000)} \\simeq 0.02\\). Still we cannot reject the hypothesis that the two simulations are different. Even though the estimates ( \\(0.07\\) and \\(0.04)\\) appear to be quite different, the difficulty is that the original simulation used a very small number of replications \\((B=100)\\) so the reported estimate is quite imprecise. In this case it is appropriate to conclude that your results “replicate” the previous study as there is no statistical evidence to reject the hypothesis that they are equivalent.\nMost journals have policies requiring authors to make available their data sets and computer programs required for empirical results. Most do not have similar policies regarding simulations. Nevertheless, it is good professional practice to make your simulations available. The best practice is to post your simulation code on your webpage. This invites others to build on and use your results, leading to possible collaboration, citation, and/or advancement."
  },
  {
    "objectID": "chpt09-hypothesit-test.html#confidence-intervals-by-test-inversion",
    "href": "chpt09-hypothesit-test.html#confidence-intervals-by-test-inversion",
    "title": "9  Hypothesis Testing",
    "section": "9.19 Confidence Intervals by Test Inversion",
    "text": "9.19 Confidence Intervals by Test Inversion\nThere is a close relationship between hypothesis tests and confidence intervals. We observed in Section \\(7.13\\) that the standard \\(95 %\\) asymptotic confidence interval for a parameter \\(\\theta\\) is\n\\[\n\\widehat{C}=[\\widehat{\\theta}-1.96 \\times s(\\widehat{\\theta}), \\quad \\widehat{\\theta}+1.96 \\times s(\\widehat{\\theta})]=\\{\\theta:|T(\\theta)| \\leq 1.96\\} .\n\\]\nThat is, we can describe \\(\\widehat{C}\\) as “The point estimate plus or minus 2 standard errors” or “The set of parameter values not rejected by a two-sided t-test.” The second definition, known as test statistic inversion, is a general method for finding confidence intervals, and typically produces confidence intervals with excellent properties.\nGiven a test statistic \\(T(\\theta)\\) and critical value \\(c\\), the acceptance region “Accept if \\(T(\\theta) \\leq c\\)” is identical to the confidence interval \\(\\widehat{C}=\\{\\theta: T(\\theta) \\leq c\\}\\). Since the regions are identical the probability of coverage \\(\\mathbb{P}[\\theta \\in \\widehat{C}]\\) equals the probability of correct acceptance \\(\\mathbb{P}[\\) Accept \\(\\mid \\theta]\\) which is exactly 1 minus the Type I error probability. Thus inverting a test with good Type I error probabilities yields a confidence interval with good coverage probabilities.\nNow suppose that the parameter of interest \\(\\theta=r(\\beta)\\) is a nonlinear function of the coefficient vector \\(\\beta\\). In this case the standard confidence interval for \\(\\theta\\) is the set \\(\\widehat{C}\\) as in (9.16) where \\(\\widehat{\\theta}=r(\\widehat{\\beta})\\) is the point estimator and \\(s(\\widehat{\\theta})=\\sqrt{\\widehat{\\boldsymbol{R}}^{\\prime} \\widehat{\\boldsymbol{V}}_{\\widehat{\\beta}} \\widehat{\\boldsymbol{R}}}\\) is the delta method standard error. This confidence interval is inverting the t-test based on the nonlinear hypothesis \\(r(\\beta)=\\theta\\). The trouble is that in Section \\(9.17\\) we learned that there is no unique t-statistic for tests of nonlinear hypotheses and that the choice of parameterization matters greatly. For example, if \\(\\theta=\\beta_{1} / \\beta_{2}\\) then the coverage probability of the standard interval (9.16) is 1 minus the probability of the Type I error, which as shown in Table \\(8.2\\) can be far from the nominal \\(5 %\\).\nIn this example a good solution is the same as discussed in Section \\(9.17\\) - to rewrite the hypothesis as a linear restriction. The hypothesis \\(\\theta=\\beta_{1} / \\beta_{2}\\) is the same as \\(\\theta \\beta_{2}=\\beta_{1}\\). The t-statistic for this restriction is\n\\[\nT(\\theta)=\\frac{\\widehat{\\beta}_{1}-\\widehat{\\beta}_{2} \\theta}{\\left(\\boldsymbol{R}^{\\prime} \\widehat{\\boldsymbol{V}}_{\\widehat{\\beta}} \\boldsymbol{R}\\right)^{1 / 2}}\n\\]\nwhere\n\\[\n\\boldsymbol{R}=\\left(\\begin{array}{c}\n1 \\\\\n-\\theta\n\\end{array}\\right)\n\\]\nand \\(\\widehat{V}_{\\widehat{\\beta}}\\) is the covariance matrix for \\(\\left(\\widehat{\\beta}_{1} \\widehat{\\beta}_{2}\\right)\\). A 95% confidence interval for \\(\\theta=\\beta_{1} / \\beta_{2}\\) is the set of values of \\(\\theta\\) such that \\(|T(\\theta)| \\leq 1.96\\). Since \\(T(\\theta)\\) is a nonlinear function of \\(\\theta\\) one method to find the confidence set is grid search over \\(\\theta\\).\nFor example, in the wage equation\n\\[\n\\log (\\text { wage })=\\beta_{1} \\text { experience }+\\beta_{2} \\text { experience }^{2} / 100+\\cdots\n\\]\nthe highest expected wage occurs at experience \\(=-50 \\beta_{1} / \\beta_{2}\\). From Table \\(4.1\\) we have the point estimate \\(\\widehat{\\theta}=29.8\\) and we can calculate the standard error \\(s(\\widehat{\\theta})=0.022\\) for a 95% confidence interval \\([29.8,29.9]\\). However, if we instead invert the linear form of the test we numerically find the interval \\([29.1,30.6]\\) which is much larger. From the evidence presented in Section \\(9.17\\) we know the first interval can be quite inaccurate and the second interval is greatly preferred."
  },
  {
    "objectID": "chpt09-hypothesit-test.html#multiple-tests-and-bonferroni-corrections",
    "href": "chpt09-hypothesit-test.html#multiple-tests-and-bonferroni-corrections",
    "title": "9  Hypothesis Testing",
    "section": "9.20 Multiple Tests and Bonferroni Corrections",
    "text": "9.20 Multiple Tests and Bonferroni Corrections\nIn most applications economists examine a large number of estimates, test statistics, and p-values. What does it mean (or does it mean anything) if one statistic appears to be “significant” after examining a large number of statistics? This is known as the problem of multiple testing or multiple comparisons.\nTo be specific, suppose we examine a set of \\(k\\) coefficients, standard errors and t-ratios, and consider the “significance” of each statistic. Based on conventional reasoning, for each coefficient we would reject the hypothesis that the coefficient is zero with asymptotic size \\(\\alpha\\) if the absolute t-statistic exceeds the \\(1-\\alpha\\) critical value of the normal distribution, or equivalently if the \\(\\mathrm{p}\\)-value for the t-statistic is smaller than \\(\\alpha\\). If we observe that one of the \\(k\\) statistics is “significant” based on this criterion, that means that one of the p-values is smaller than \\(\\alpha\\), or equivalently, that the smallest p-value is smaller than \\(\\alpha\\). We can then rephrase the question: Under the joint hypothesis that a set of \\(k\\) hypotheses are all true, what is the probability that the smallest \\(\\mathrm{p}\\)-value is smaller than \\(\\alpha\\) ? In general, we cannot provide a precise answer to this quesion, but the Bonferroni correction bounds this probability by \\(\\alpha k\\). The Bonferroni method furthermore suggests that if we want the familywise error probability (the probability that one of the tests falsely rejects) to be bounded below \\(\\alpha\\), then an appropriate rule is to reject only if the smallest p-value is smaller than \\(\\alpha / k\\). Equivalently, the Bonferroni familywise \\(\\mathrm{p}\\)-value is \\(k \\min _{j \\leq k} p_{j}\\).\nFormally, suppose we have \\(k\\) hypotheses \\(\\mathbb{M}_{j}, j=1, \\ldots, k\\). For each we have a test and associated pvalue \\(p_{j}\\) with the property that when \\(\\mathbb{H}_{j}\\) is true \\(\\lim _{n \\rightarrow \\infty} \\mathbb{P}\\left[p_{j}<\\alpha\\right]=\\alpha\\). We then observe that among the \\(k\\) tests, one of the \\(k\\) is “significant” if \\(\\min _{j \\leq k} p_{j}<\\alpha\\). This event can be written as\n\\[\n\\left\\{\\min _{j \\leq k} p_{j}<\\alpha\\right\\}=\\bigcup_{j=1}^{k}\\left\\{p_{j}<\\alpha\\right\\} .\n\\]\nBoole’s inequality states that for any \\(k\\) events \\(A_{j}, \\mathbb{P}\\left[\\bigcup_{j=1}^{k} A_{j}\\right] \\leq \\sum_{j=1}^{k} \\mathbb{P}\\left[A_{k}\\right]\\). Thus\n\\[\n\\mathbb{P}\\left[\\min _{j \\leq k} p_{j}<\\alpha\\right] \\leq \\sum_{j=1}^{k} \\mathbb{P}\\left[p_{j}<\\alpha\\right] \\rightarrow k \\alpha\n\\]\nas stated. This demonstates that the asymptotic familywise rejection probability is at most \\(k\\) times the individual rejection probability.\nFurthermore,\n\\[\n\\mathbb{P}\\left[\\min _{j \\leq k} p_{j}<\\frac{\\alpha}{k}\\right] \\leq \\sum_{j=1}^{k} \\mathbb{P}\\left[p_{j}<\\frac{\\alpha}{k}\\right] \\rightarrow \\alpha .\n\\]\nThis demonstrates that the asymptotic familywise rejection probability can be controlled (bounded below \\(\\alpha\\) ) if each individual test is subjected to the stricter standard that a p-value must be smaller than \\(\\alpha / k\\) to be labeled as “significant”.\nTo illustrate, suppose we have two coefficient estimates with individual p-values \\(0.04\\) and \\(0.15\\). Based on a conventional \\(5 %\\) level the standard individual tests would suggest that the first coefficient estimate is “significant” but not the second. A Bonferroni 5% test, however, does not reject as it would require that the smallest p-value be smaller than \\(0.025\\), which is not the case in this example. Alternatively, the Bonferroni familywise \\(\\mathrm{p}\\)-value is \\(0.04 \\times 2=0.08\\), which is not significant at the \\(5 %\\) level.\nIn contrast, if the two p-values were \\(0.01\\) and \\(0.15\\), then the Bonferroni familywise p-value would be \\(0.01 \\times 2=0.02\\), which is significant at the \\(5 %\\) level."
  },
  {
    "objectID": "chpt09-hypothesit-test.html#power-and-test-consistency",
    "href": "chpt09-hypothesit-test.html#power-and-test-consistency",
    "title": "9  Hypothesis Testing",
    "section": "9.21 Power and Test Consistency",
    "text": "9.21 Power and Test Consistency\nThe power of a test is the probability of rejecting \\(\\mathbb{M}_{0}\\) when \\(\\mathbb{M}_{1}\\) is true.\nFor simplicity suppose that \\(Y_{i}\\) is i.i.d. \\(\\mathrm{N}\\left(\\theta, \\sigma^{2}\\right)\\) with \\(\\sigma^{2}\\) known, consider the t-statistic \\(T(\\theta)=\\sqrt{n}(\\bar{Y}-\\theta) / \\sigma\\), and tests of \\(\\mathbb{M}_{0}: \\theta=0\\) against \\(\\mathbb{M}_{1}: \\theta>0\\). We reject \\(\\mathbb{H}_{0}\\) if \\(T=T(0)>c\\). Note that\n\\[\nT=T(\\theta)+\\sqrt{n} \\theta / \\sigma\n\\]\nand \\(T(\\theta)\\) has an exact \\(\\mathrm{N}(0,1)\\) distribution. This is because \\(T(\\theta)\\) is centered at the true mean \\(\\theta\\), while the test statistic \\(T(0)\\) is centered at the (false) hypothesized mean of 0 .\nThe power of the test is\n\\[\n\\mathbb{P}[T>c \\mid \\theta]=\\mathbb{P}[\\mathrm{Z}+\\sqrt{n} \\theta / \\sigma>c]=1-\\Phi(c-\\sqrt{n} \\theta / \\sigma) .\n\\]\nThis function is monotonically increasing in \\(\\mu\\) and \\(n\\), and decreasing in \\(\\sigma\\) and \\(c\\).\nNotice that for any \\(c\\) and \\(\\theta \\neq 0\\) the power increases to 1 as \\(n \\rightarrow \\infty\\). This means that for \\(\\theta \\in \\mathbb{H}_{1}\\) the test will reject \\(\\mathbb{M}_{0}\\) with probability approaching 1 as the sample size gets large. We call this property test consistency.\nDefinition 9.3 A test of \\(\\mathbb{H}_{0}: \\theta \\in \\Theta_{0}\\) is consistent against fixed alternatives if for all \\(\\theta \\in \\Theta_{1}, \\mathbb{P}\\left[\\right.\\) Reject \\(\\left.\\mathbb{M}_{0} \\mid \\theta\\right] \\rightarrow 1\\) as \\(n \\rightarrow \\infty\\).\nFor tests of the form “Reject \\(\\mathbb{H}_{0}\\) if \\(T>c\\)”, a sufficient condition for test consistency is that the \\(T\\) diverges to positive infinity with probability one for all \\(\\theta \\in \\Theta_{1}\\). Definition 9.4 We say that \\(T \\underset{p}{\\rightarrow}\\) as \\(n \\rightarrow \\infty\\) if for all \\(M<\\infty, \\mathbb{P}[T \\leq M] \\rightarrow 0\\) as \\(n \\rightarrow \\infty\\). Similarly, we say that \\(T \\underset{p}{\\rightarrow}-\\infty\\) as \\(n \\rightarrow \\infty\\) if for all \\(M<\\infty\\), \\(\\mathbb{P}[T \\geq-M] \\rightarrow 0\\) as \\(n \\rightarrow \\infty\\)\nIn general, t-tests and Wald tests are consistent against fixed alternatives. Take a t-statistic for a test of \\(\\mathbb{\\sharp}_{0}: \\theta=\\theta_{0}, T=\\left(\\widehat{\\theta}-\\theta_{0}\\right) / s(\\widehat{\\theta})\\) where \\(\\theta_{0}\\) is a known value and \\(s(\\widehat{\\theta})=\\sqrt{n^{-1} \\widehat{V}_{\\theta}}\\). Note that\n\\[\nT=\\frac{\\widehat{\\theta}-\\theta}{s(\\widehat{\\theta})}+\\frac{\\sqrt{n}\\left(\\theta-\\theta_{0}\\right)}{\\sqrt{\\widehat{V}_{\\theta}}} .\n\\]\nThe first term on the right-hand-side converges in distribution to \\(\\mathrm{N}(0,1)\\). The second term on the righthand-side equals zero if \\(\\theta=\\theta_{0}\\), converges in probability to \\(+\\infty\\) if \\(\\theta>\\theta_{0}\\), and converges in probability to \\(-\\infty\\) if \\(\\theta<\\theta_{0}\\). Thus the two-sided t-test is consistent against \\(\\mathbb{M}_{1}: \\theta \\neq \\theta_{0}\\), and one-sided t-tests are consistent against the alternatives for which they are designed.\nTheorem 9.8 Under Assumptions 7.2, 7.3, and 7.4, for \\(\\theta=r(\\beta) \\neq \\theta_{0}\\) and \\(q=1\\), then \\(|T| \\underset{p}{\\longrightarrow}\\). For any \\(c<\\infty\\) the test “Reject \\(\\mathbb{H}_{0}\\) if \\(|T|>c\\)” is consistent against fixed alternatives.\nThe Wald statistic for \\(\\mathbb{M}_{0}: \\theta=r(\\beta)=\\theta_{0}\\) against \\(\\mathbb{M}_{1}: \\theta \\neq \\theta_{0}\\) is \\(W=n\\left(\\widehat{\\theta}-\\theta_{0}\\right)^{\\prime} \\widehat{\\boldsymbol{V}}_{\\theta}^{-1}\\left(\\widehat{\\theta}-\\theta_{0}\\right)\\). Under \\(\\mathbb{H}_{1}\\), \\(\\widehat{\\theta} \\underset{p}{\\longrightarrow} \\theta \\neq \\theta_{0}\\). Thus \\(\\left(\\widehat{\\theta}-\\theta_{0}\\right)^{\\prime} \\widehat{\\boldsymbol{V}}_{\\theta}^{-1}\\left(\\widehat{\\theta}-\\theta_{0}\\right) \\underset{p}{\\longrightarrow}\\left(\\theta-\\theta_{0}\\right)^{\\prime} \\boldsymbol{V}_{\\theta}^{-1}\\left(\\theta-\\theta_{0}\\right)>0\\). Hence under \\(\\mathbb{H}_{1}, W \\underset{p}{\\longrightarrow}\\). Again, this implies that Wald tests are consistent.\nTheorem 9.9 Under Assumptions 7.2, 7.3, and 7.4, for \\(\\theta=r(\\beta) \\neq \\theta_{0}\\), then \\(W \\underset{p}{\\longrightarrow}\\). For any \\(c<\\infty\\) the test “Reject \\(\\mathbb{M}_{0}\\) if \\(W>c\\)” is consistent against fixed alternatives."
  },
  {
    "objectID": "chpt09-hypothesit-test.html#asymptotic-local-power",
    "href": "chpt09-hypothesit-test.html#asymptotic-local-power",
    "title": "9  Hypothesis Testing",
    "section": "9.22 Asymptotic Local Power",
    "text": "9.22 Asymptotic Local Power\nConsistency is a good property for a test but is does not provided a tool to calculate test power. To approximate the power function we need a distributional approximation.\nThe standard asymptotic method for power analysis uses what are called local alternatives. This is similar to our analysis of restriction estimation under misspecification (Section 8.13). The technique is to index the parameter by sample size so that the asymptotic distribution of the statistic is continuous in a localizing parameter. In this section we consider t-tests on real-valued parameters and in the next section Wald tests. Specifically, we consider parameter vectors \\(\\beta_{n}\\) which are indexed by sample size \\(n\\) and satisfy the real-valued relationship\n\\[\n\\theta_{n}=r\\left(\\beta_{n}\\right)=\\theta_{0}+n^{-1 / 2} h\n\\]\nwhere the scalar \\(h\\) is called a localizing parameter. We index \\(\\beta_{n}\\) and \\(\\theta_{n}\\) by sample size to indicate their dependence on \\(n\\). The way to think of (9.17) is that the true value of the parameters are \\(\\beta_{n}\\) and \\(\\theta_{n}\\). The parameter \\(\\theta_{n}\\) is close to the hypothesized value \\(\\theta_{0}\\), with deviation \\(n^{-1 / 2} h\\).\nThe specification (9.17) states that for any fixed \\(h, \\theta_{n}\\) approaches \\(\\theta_{0}\\) as \\(n\\) gets large. Thus \\(\\theta_{n}\\) is “close” or “local” to \\(\\theta_{0}\\). The concept of a localizing sequence (9.17) might seem odd since in the actual world the sample size cannot mechanically affect the value of the parameter. Thus (9.17) should not be interpreted literally. Instead, it should be interpreted as a technical device which allows the asymptotic distribution to be continuous in the alternative hypothesis.\nTo evaluate the asymptotic distribution of the test statistic we start by examining the scaled estimator centered at the hypothesized value \\(\\theta_{0}\\). Breaking it into a term centered at the true value \\(\\theta_{n}\\) and a remainder we find\n\\[\n\\sqrt{n}\\left(\\widehat{\\theta}-\\theta_{0}\\right)=\\sqrt{n}\\left(\\widehat{\\theta}-\\theta_{n}\\right)+\\sqrt{n}\\left(\\theta_{n}-\\theta_{0}\\right)=\\sqrt{n}\\left(\\widehat{\\theta}-\\theta_{n}\\right)+h\n\\]\nwhere the second equality is (9.17). The first term is asymptotically normal:\n\\[\n\\sqrt{n}\\left(\\widehat{\\theta}-\\theta_{n}\\right) \\underset{d}{\\longrightarrow} \\sqrt{V_{\\theta}} Z\n\\]\nwhere \\(Z \\sim \\mathrm{N}(0,1)\\). Therefore\n\\[\n\\sqrt{n}\\left(\\widehat{\\theta}-\\theta_{0}\\right) \\underset{d}{\\longrightarrow} \\sqrt{V_{\\theta}} Z+h \\sim \\mathrm{N}\\left(h, V_{\\theta}\\right) .\n\\]\nThis asymptotic distribution depends continuously on the localizing parameter \\(h\\).\nApplied to the \\(t\\) statistic we find\n\\[\nT=\\frac{\\widehat{\\theta}-\\theta_{0}}{s(\\widehat{\\theta})} \\underset{d}{\\longrightarrow} \\frac{\\sqrt{V_{\\theta}} Z+h}{\\sqrt{V_{\\theta}}} \\sim Z+\\delta\n\\]\nwhere \\(\\delta=h / \\sqrt{V_{\\theta}}\\). This generalizes Theorem \\(9.1\\) (which assumes \\(\\mathbb{M}_{0}\\) is true) to allow for local alternatives of the form (9.17).\nConsider a t-test of \\(\\mathbb{M}_{0}\\) against the one-sided alternative \\(\\mathbb{M}_{1}: \\theta>\\theta_{0}\\) which rejects \\(\\mathbb{H}_{0}\\) for \\(T>c\\) where \\(\\Phi(c)=1-\\alpha\\). The asymptotic local power of this test is the limit (as the sample size diverges) of the rejection probability under the local alternative (9.17)\n\\[\n\\begin{aligned}\n\\lim _{n \\rightarrow \\infty} \\mathbb{P}\\left[\\text { Reject } \\mathbb{M}_{0}\\right] &=\\lim _{n \\rightarrow \\infty} \\mathbb{P}[T>c] \\\\\n&=\\mathbb{P}[Z+\\delta>c] \\\\\n&=1-\\Phi(c-\\delta) \\\\\n&=\\Phi(\\delta-c) \\\\\n& \\stackrel{\\text { def }}{=} \\pi(\\delta) .\n\\end{aligned}\n\\]\nWe call \\(\\pi(\\delta)\\) the asymptotic local power function.\nIn Figure 9.3(a) we plot the local power function \\(\\pi(\\delta)\\) as a function of \\(\\delta \\in[-1,4]\\) for tests of asymptotic size \\(\\alpha=0.05\\) and \\(\\alpha=0.01 . \\delta=0\\) corresponds to the null hypothesis so \\(\\pi(\\delta)=\\alpha\\). The power functions are monotonically increasing in \\(\\delta\\). Note that the power is lower than \\(\\alpha\\) for \\(\\delta<0\\) due to the one-sided nature of the test.\nWe can see that the power functions are ranked by \\(\\alpha\\) so that the test with \\(\\alpha=0.05\\) has higher power than the test with \\(\\alpha=0.01\\). This is the inherent trade-off between size and power. Decreasing size induces a decrease in power, and conversely.\n\n\nOne-Sided t Test\n\n\n\nVector Case\n\nFigure 9.3: Asymptotic Local Power Function\nThe coefficient \\(\\delta\\) can be interpreted as the parameter deviation measured as a multiple of the standard error \\(s(\\widehat{\\theta})\\). To see this, recall that \\(s(\\widehat{\\theta})=n^{-1 / 2} \\sqrt{\\widehat{V}_{\\theta}} \\simeq n^{-1 / 2} \\sqrt{V_{\\theta}}\\) and then note that\n\\[\n\\delta=\\frac{h}{\\sqrt{V_{\\theta}}} \\simeq \\frac{n^{-1 / 2} h}{s(\\widehat{\\theta})}=\\frac{\\theta_{n}-\\theta_{0}}{s(\\widehat{\\theta})} .\n\\]\nThus \\(\\delta\\) approximately equals the deviation \\(\\theta_{n}-\\theta_{0}\\) expressed as multiples of the standard error \\(s(\\widehat{\\theta})\\). Thus as we examine Figure 9.3(a) we can interpret the power function at \\(\\delta=1\\) (e.g. \\(26 %\\) for a 5% size test) as the power when the parameter \\(\\theta_{n}\\) is one standard error above the hypothesized value. For example, from Table \\(4.2\\) the standard error for the coefficient on “Married Female” is \\(0.010\\). Thus, in this example \\(\\delta=1\\) corresponds to \\(\\theta_{n}=0.010\\) or an \\(1.0 %\\) wage premium for married females. Our calculations show that the asymptotic power of a one-sided \\(5 %\\) test against this alternative is about \\(26 %\\).\nThe difference between power functions can be measured either vertically or horizontally. For example, in Figure 9.3(a) there is a vertical dashed line at \\(\\delta=1\\), showing that the asymptotic local power function \\(\\pi(\\delta)\\) equals \\(26 %\\) for \\(\\alpha=0.0\\), and \\(9 %\\) for \\(\\alpha=0.01\\). This is the difference in power across tests of differing size, holding fixed the parameter in the alternative.\nA horizontal comparison can also be illuminating. To illustrate, in Figure 9.3(a) there is a horizontal dashed line at \\(50 %\\) power. \\(50 %\\) power is a useful benchmark as it is the point where the test has equal odds of rejection and acceptance. The dotted line crosses the two power curves at \\(\\delta=1.65(\\alpha=0.05)\\) and \\(\\delta=2.33(\\alpha=0.01)\\). This means that the parameter \\(\\theta\\) must be at least \\(1.65\\) standard errors above the hypothesized value for a one-sided \\(5 %\\) test to have \\(50 %\\) (approximate) power, and \\(2.33\\) standard errors for a one-sided \\(1 %\\) test.\nThe ratio of these values (e.g. 2.33/1.65 = 1.41) measures the relative parameter magnitude needed to achieve the same power. (Thus, for a \\(1 %\\) size test to achieve \\(50 %\\) power, the parameter must be \\(41 %\\) larger than for a \\(5 %\\) size test.) Even more interesting, the square of this ratio (e.g. \\(1.41^{2}=2\\) ) is the increase in sample size needed to achieve the same power under fixed parameters. That is, to achieve \\(50 %\\) power, a \\(1 %\\) size test needs twice as many observations as a \\(5 %\\) size test. This interpretation follows by the following informal argument. By definition and (9.17) \\(\\delta=h / \\sqrt{V_{\\theta}}=\\sqrt{n}\\left(\\theta_{n}-\\theta_{0}\\right) / \\sqrt{V_{\\theta}}\\). Thus holding \\(\\theta\\) and \\(V_{\\theta}\\) fixed, \\(\\delta^{2}\\) is proportional to \\(n\\).\nThe analysis of a two-sided t test is similar. (9.18) implies that\n\\[\nT=\\left|\\frac{\\widehat{\\theta}-\\theta_{0}}{s(\\widehat{\\theta})}\\right| \\vec{d}|Z+\\delta|\n\\]\nand thus the local power of a two-sided t test is\n\\[\n\\lim _{n \\rightarrow \\infty} \\mathbb{P}\\left[\\text { Reject } \\mathbb{H}_{0}\\right]=\\lim _{n \\rightarrow \\infty} \\mathbb{P}[T>c]=\\mathbb{P}[|Z+\\delta|>c]=\\Phi(\\delta-c)+\\Phi(-\\delta-c)\n\\]\nwhich is monotonically increasing in \\(|\\delta|\\).\nTheorem 9.10 Under Assumptions 7.2, 7.3,7.4, and \\(\\theta_{n}=r\\left(\\beta_{n}\\right)=r_{0}+n^{-1 / 2} h\\), then\n\\[\nT\\left(\\theta_{0}\\right)=\\frac{\\widehat{\\theta}-\\theta_{0}}{s(\\widehat{\\theta})} \\underset{d}{\\longrightarrow} Z+\\delta\n\\]\nwhere \\(Z \\sim \\mathrm{N}(0,1)\\) and \\(\\delta=h / \\sqrt{V_{\\theta}}\\). For \\(c\\) such that \\(\\Phi(c)=1-\\alpha\\),\n\\[\n\\mathbb{P}\\left[T\\left(\\theta_{0}\\right)>c\\right] \\longrightarrow \\Phi(\\delta-c) .\n\\]\nFurthermore, for \\(c\\) such that \\(\\Phi(c)=1-\\alpha / 2\\),\n\\[\n\\mathbb{P}\\left[\\left|T\\left(\\theta_{0}\\right)\\right|>c\\right] \\longrightarrow \\Phi(\\delta-c)+\\Phi(-\\delta-c) .\n\\]"
  },
  {
    "objectID": "chpt09-hypothesit-test.html#asymptotic-local-power-vector-case",
    "href": "chpt09-hypothesit-test.html#asymptotic-local-power-vector-case",
    "title": "9  Hypothesis Testing",
    "section": "9.23 Asymptotic Local Power, Vector Case",
    "text": "9.23 Asymptotic Local Power, Vector Case\nIn this section we extend the local power analysis of the previous section to the case of vector-valued alternatives. We generalize (9.17) to vector-valued \\(\\theta_{n}\\). The local parameterization is\n\\[\n\\theta_{n}=r\\left(\\beta_{n}\\right)=\\theta_{0}+n^{-1 / 2} h\n\\]\nwhere \\(h\\) is \\(q \\times 1\\).\nUnder (9.19),\n\\[\n\\sqrt{n}\\left(\\widehat{\\theta}-\\theta_{0}\\right)=\\sqrt{n}\\left(\\widehat{\\theta}-\\theta_{n}\\right)+h \\underset{d}{\\longrightarrow} Z_{h} \\sim \\mathrm{N}\\left(h, \\boldsymbol{V}_{\\theta}\\right),\n\\]\na normal random vector with mean \\(h\\) and covariance matrix \\(\\boldsymbol{V}_{\\theta}\\).\nApplied to the Wald statistic we find\n\\[\nW=n\\left(\\widehat{\\theta}-\\theta_{0}\\right)^{\\prime} \\widehat{\\boldsymbol{V}}_{\\theta}^{-1}\\left(\\widehat{\\theta}-\\theta_{0}\\right) \\underset{d}{\\longrightarrow} Z_{h}^{\\prime} \\boldsymbol{V}_{\\theta}^{-1} Z_{h} \\sim \\chi_{q}^{2}(\\lambda)\n\\]\nwhere \\(\\lambda=h^{\\prime} \\boldsymbol{V}^{-1} h . \\chi_{q}^{2}(\\lambda)\\) is a non-central chi-square random variable with non-centrality parameter \\(\\lambda\\). (Theorem 5.3.6.)\nThe convergence (9.20) shows that under the local alternatives (9.19), W \\(\\underset{d}{ } \\chi_{q}^{2}(\\lambda)\\). This generalizes the null asymptotic distribution which obtains as the special case \\(\\lambda=0\\). We can use this result to obtain a continuous asymptotic approximation to the power function. For any significance level \\(\\alpha>0\\) set the asymptotic critical value \\(c\\) so that \\(\\mathbb{P}\\left[\\chi_{q}^{2}>c\\right]=\\alpha\\). Then as \\(n \\rightarrow \\infty\\),\n\\[\n\\mathbb{P}[W>c] \\longrightarrow \\mathbb{P}\\left[\\chi_{q}^{2}(\\lambda)>c\\right] \\stackrel{\\text { def }}{=} \\pi(\\lambda) .\n\\]\nThe asymptotic local power function \\(\\pi(\\lambda)\\) depends only on \\(\\alpha, q\\), and \\(\\lambda\\).\nTheorem 9.11 Under Assumptions 7.2, 7.3, 7.4, and \\(\\theta_{n}=r\\left(\\beta_{n}\\right)=\\theta_{0}+n^{-1 / 2} h\\), then \\(W \\underset{d}{\\longrightarrow} \\chi_{q}^{2}(\\lambda)\\) where \\(\\lambda=h^{\\prime} \\boldsymbol{V}_{\\theta}^{-1} h\\). Furthermore, for \\(c\\) such that \\(\\mathbb{P}\\left[\\chi_{q}^{2}>c\\right]=\\) \\(\\alpha, \\mathbb{P}[W>c] \\longrightarrow \\mathbb{P}\\left[\\chi_{q}^{2}(\\lambda)>c\\right]\\)\nFigure 9.3(b) plots \\(\\pi(\\lambda)\\) as a function of \\(\\lambda\\) for \\(q=1, q=2\\), and \\(q=3\\), and \\(\\alpha=0.05\\). The asymptotic power functions are monotonically increasing in \\(\\lambda\\) and asymptote to one.\nFigure 9.3(b) also shows the power loss for fixed non-centrality parameter \\(\\lambda\\) as the dimensionality of the test increases. The power curves shift to the right as \\(q\\) increases, resulting in a decrease in power. This is illustrated by the dashed line at \\(50 %\\) power. The dashed line crosses the three power curves at \\(\\lambda=3.85(q=1), \\lambda=4.96(q=2)\\), and \\(\\lambda=5.77(q=3)\\). The ratio of these \\(\\lambda\\) values correspond to the relative sample sizes needed to obtain the same power. Thus increasing the dimension of the test from \\(q=1\\) to \\(q=2\\) requires a \\(28 %\\) increase in sample size, or an increase from \\(q=1\\) to \\(q=3\\) requires a \\(50 %\\) increase in sample size, to maintain \\(50 %\\) power."
  },
  {
    "objectID": "chpt09-hypothesit-test.html#exercises",
    "href": "chpt09-hypothesit-test.html#exercises",
    "title": "9  Hypothesis Testing",
    "section": "9.24 Exercises",
    "text": "9.24 Exercises\nExercise 9.1 Prove that if an additional regressor \\(\\boldsymbol{X}_{k+1}\\) is added to \\(\\boldsymbol{X}\\), Theil’s adjusted \\(\\bar{R}^{2}\\) increases if and only if \\(\\left|T_{k+1}\\right|>1\\), where \\(T_{k+1}=\\widehat{\\beta}_{k+1} / s\\left(\\widehat{\\beta}_{k+1}\\right)\\) is the t-ratio for \\(\\widehat{\\beta}_{k+1}\\) and\n\\[\ns\\left(\\widehat{\\beta}_{k+1}\\right)=\\left(s^{2}\\left[\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1}\\right]_{k+1, k+1}\\right)^{1 / 2}\n\\]\nis the homoskedasticity-formula standard error.\nExercise 9.2 You have two independent samples \\(\\left(Y_{1 i}, X_{1 i}\\right)\\) and \\(\\left(Y_{2 i}, X_{2 i}\\right)\\) both with sample sizes \\(n\\) which satisfy \\(Y_{1}=X_{1}^{\\prime} \\beta_{1}+e_{1}\\) and \\(Y_{2}=X_{2}^{\\prime} \\beta_{2}+e_{2}\\), where \\(\\mathbb{E}\\left[X_{1} e_{1}\\right]=0\\) and \\(\\mathbb{E}\\left[X_{2} e_{2}\\right]=0\\). Let \\(\\widehat{\\beta}_{1}\\) and \\(\\widehat{\\beta}_{2}\\) be the OLS estimators of \\(\\beta_{1} \\in \\mathbb{R}^{k}\\) and \\(\\beta_{2} \\in \\mathbb{R}^{k}\\).\n\nFind the asymptotic distribution of \\(\\sqrt{n}\\left(\\left(\\widehat{\\beta}_{2}-\\widehat{\\beta}_{1}\\right)-\\left(\\beta_{2}-\\beta_{1}\\right)\\right)\\) as \\(n \\rightarrow \\infty\\).\nFind an appropriate test statistic for \\(\\mathbb{H}_{0}: \\beta_{2}=\\beta_{1}\\).\nFind the asymptotic distribution of this statistic under \\(\\mathbb{H}_{0}\\).\n\nExercise 9.3 Let \\(T\\) be a t-statistic for \\(\\mathbb{H}_{0}: \\theta=0\\) versus \\(\\mathbb{H}_{1}: \\theta \\neq 0\\). Since \\(|T| \\rightarrow{ }_{d}|Z|\\) under \\(\\mathbb{H}_{0}\\), someone suggests the test “Reject \\(\\mathbb{M}_{0}\\) if \\(|T|<c_{1}\\) or \\(|T|>c_{2}\\), where \\(c_{1}\\) is the \\(\\alpha / 2\\) quantile of \\(|Z|\\) and \\(c_{2}\\) is the \\(1-\\alpha / 2\\) quantile of \\(|Z|\\).\n\nShow that the asymptotic size of the test is \\(\\alpha\\). (b) Is this a good test of \\(\\mathbb{M}_{0}\\) versus \\(\\mathbb{M}_{1}\\) ? Why or why not?\n\nExercise 9.4 Let \\(W\\) be a Wald statistic for \\(\\mathbb{M}_{0}: \\theta=0\\) versus \\(\\mathbb{M}_{1}: \\theta \\neq 0\\), where \\(\\theta\\) is \\(q \\times 1\\). Since \\(W \\underset{d}{\\rightarrow} \\chi_{q}^{2}\\) under \\(H_{0}\\), someone suggests the test “Reject \\(\\mathbb{H}_{0}\\) if \\(W<c_{1}\\) or \\(W>c_{2}\\), where \\(c_{1}\\) is the \\(\\alpha / 2\\) quantile of \\(\\chi_{q}^{2}\\) and \\(c_{2}\\) is the \\(1-\\alpha / 2\\) quantile of \\(\\chi_{q}^{2}\\).\n\nShow that the asymptotic size of the test is \\(\\alpha\\).\nIs this a good test of \\(\\mathbb{M}_{0}\\) versus \\(\\mathbb{H}_{1}\\) ? Why or why not?\n\nExercise 9.5 Take the linear model \\(Y=X_{1}^{\\prime} \\beta_{1}+X_{2}^{\\prime} \\beta_{2}+e\\) with \\(\\mathbb{E}[X e]=0\\) where both \\(X_{1}\\) and \\(X_{2}\\) are \\(q \\times 1\\). Show how to test the hypotheses \\(\\mathbb{M}_{0}: \\beta_{1}=\\beta_{2}\\) against \\(\\mathbb{M}_{1}: \\beta_{1} \\neq \\beta_{2}\\).\nExercise 9.6 Suppose a researcher wants to know which of a set of 20 regressors has an effect on a variable testscore. He regresses testscore on the 20 regressors and reports the results. One of the 20 regressors (studytime) has a large t-ratio (about 2.5), while the other t-ratios are insignificant (smaller than 2 in absolute value). He argues that the data show that studytime is the key predictor for testscore. Do you agree with this conclusion? Is there a deficiency in his reasoning?\nExercise 9.7 Take the model \\(Y=X \\beta_{1}+X^{2} \\beta_{2}+e\\) with \\(\\mathbb{E}[e \\mid X]=0\\) where \\(Y\\) is wages (dollars per hour) and \\(X\\) is age. Describe how you would test the hypothesis that the expected wage for a 40 -year-old worker is \\(\\$ 20\\) an hour.\nExercise 9.8 You want to test \\(\\mathbb{H}_{0}: \\beta_{2}=0\\) against \\(\\mathbb{H}_{1}: \\beta_{2} \\neq 0\\) in the model \\(Y=X_{1}^{\\prime} \\beta_{1}+X_{2}^{\\prime} \\beta_{2}+e\\) with \\(\\mathbb{E}[X e]=0\\). You read a paper which estimates the model\n\\[\nY=X_{1}^{\\prime} \\widehat{\\gamma}_{1}+\\left(X_{2}-X_{1}\\right)^{\\prime} \\widehat{\\gamma}_{2}+u\n\\]\nand reports a test of \\(\\mathbb{M}_{0}: \\gamma_{2}=0\\) against \\(\\mathbb{M}_{1}: \\gamma_{2} \\neq 0\\). Is this related to the test you wanted to conduct?\nExercise 9.9 Suppose a researcher uses one dataset to test a specific hypothesis \\(\\mathbb{H}_{0}\\) against \\(\\mathbb{H}_{1}\\) and finds that he can reject \\(\\mathbb{H}_{0}\\). A second researcher gathers a similar but independent dataset, uses similar methods and finds that she cannot reject \\(\\mathbb{M}_{0}\\). How should we (as interested professionals) interpret these mixed results?\nExercise 9.10 In Exercise \\(7.8\\) you showed that \\(\\sqrt{n}\\left(\\widehat{\\sigma}^{2}-\\sigma^{2}\\right) \\underset{d}{\\rightarrow} \\mathrm{N}(0, V)\\) as \\(n \\rightarrow \\infty\\) for some \\(V\\). Let \\(\\widehat{V}\\) be an estimator of \\(V\\).\n\nUsing this result construct a t-statistic for \\(\\mathbb{H}_{0}: \\sigma^{2}=1\\) against \\(\\mathbb{H}_{1}: \\sigma^{2} \\neq 1\\).\nUsing the Delta Method find the asymptotic distribution of \\(\\sqrt{n}(\\widehat{\\sigma}-\\sigma)\\).\nUse the previous result to construct a t-statistic for \\(\\mathbb{M}_{0}: \\sigma=1\\) against \\(\\mathbb{H}_{1}: \\sigma \\neq 1\\).\nAre the null hypotheses in (a) and (c) the same or are they different? Are the tests in (a) and (c) the same or are they different? If they are different, describe a context in which the two tests would give contradictory results.\n\nExercise 9.11 Consider a regression such as Table \\(4.1\\) where both experience and its square are included. A researcher wants to test the hypothesis that experience does not affect mean wages and does this by computing the t-statistic for experience. Is this the correct approach? If not, what is the appropriate testing method? Exercise 9.12 A researcher estimates a regression and computes a test of \\(\\mathbb{H}_{0}\\) against \\(\\mathbb{H}_{1}\\) and finds a pvalue of \\(p=0.08\\), or “not significant”. She says “I need more data. If I had a larger sample the test will have more power and then the test will reject.” Is this interpretation correct?\nExercise 9.13 A common view is that “If the sample size is large enough, any hypothesis will be rejected.” What does this mean? Interpret and comment.\nExercise 9.14 Take the model \\(Y=X^{\\prime} \\beta+e\\) with \\(\\mathbb{E}[X e]=0\\) and parameter of interest \\(\\theta=\\boldsymbol{R}^{\\prime} \\beta\\) with \\(\\boldsymbol{R} k \\times 1\\). Let \\(\\widehat{\\beta}\\) be the least squares estimator and \\(\\widehat{\\boldsymbol{V}}_{\\widehat{\\beta}}\\) its variance estimator.\n\nWrite down \\(\\widehat{C}\\), the \\(95 %\\) asymptotic confidence interval for \\(\\theta\\), in terms of \\(\\widehat{\\beta}, \\widehat{\\boldsymbol{V}} \\widehat{\\widehat{\\beta}}\\), \\(\\boldsymbol{R}\\), and \\(z=1.96\\) (the \\(97.5 %\\) quantile of \\(N(0,1))\\).\nShow that the decision “Reject \\(\\mathbb{M}_{0}\\) if \\(\\theta_{0} \\notin \\widehat{C}\\)” is an asymptotic \\(5 %\\) test of \\(\\mathbb{M}_{0}: \\theta=\\theta_{0}\\).\n\nExercise 9.15 You are at a seminar where a colleague presents a simulation study of a test of a hypothesis \\(\\mathbb{H}_{0}\\) with nominal size \\(5 %\\). Based on \\(B=100\\) simulation replications under \\(\\mathbb{H}_{0}\\) the estimated size is \\(7 %\\). Your colleague says: “Unfortunately the test over-rejects.”\n\nDo you agree or disagree with your colleague? Explain. Hint: Use an asymptotic (large B) approximation.\nSuppose the number of simulation replications were \\(B=1000\\) yet the estimated size is still \\(7 %\\). Does your answer change?\n\nExercise 9.16 Consider two alternative regression models\n\\[\n\\begin{aligned}\nY &=X_{1}^{\\prime} \\beta_{1}+e_{1} \\\\\n\\mathbb{E}\\left[X_{1} e_{1}\\right] &=0 \\\\\nY &=X_{2}^{\\prime} \\beta_{2}+e_{2} \\\\\n\\mathbb{E}\\left[X_{2} e_{2}\\right] &=0\n\\end{aligned}\n\\]\nwhere \\(X_{1}\\) and \\(X_{2}\\) have at least some different regressors. (For example, (9.21) is a wage regression on geographic variables and (2) is a wage regression on personal appearance measurements.) You want to know if model (9.21) or model (9.22) fits the data better. Define \\(\\sigma_{1}^{2}=\\mathbb{E}\\left[e_{1}^{2}\\right]\\) and \\(\\sigma_{2}^{2}=\\mathbb{E}\\left[e_{2}^{2}\\right]\\). You decide that the model with the smaller variance fit (e.g., model (9.21) fits better if \\(\\sigma_{1}^{2}<\\sigma_{2}^{2}\\).) You decide to test for this by testing the hypothesis of equal fit \\(\\mathbb{H}_{0}: \\sigma_{1}^{2}=\\sigma_{2}^{2}\\) against the alternative of unequal fit \\(\\mathbb{H}_{1}: \\sigma_{1}^{2} \\neq \\sigma_{2}^{2}\\). For simplicity, suppose that \\(e_{1 i}\\) and \\(e_{2 i}\\) are observed.\n\nConstruct an estimator \\(\\widehat{\\theta}\\) of \\(\\theta=\\sigma_{1}^{2}-\\sigma_{2}^{2}\\).\nFind the asymptotic distribution of \\(\\sqrt{n}(\\widehat{\\theta}-\\theta)\\) as \\(n \\rightarrow \\infty\\).\nFind an estimator of the asymptotic variance of \\(\\widehat{\\theta}\\).\nPropose a test of asymptotic size \\(\\alpha\\) of \\(\\mathbb{M}_{0}\\) against \\(\\mathbb{M}_{1}\\).\nSuppose the test accepts \\(\\mathbb{M}_{0}\\). Briefly, what is your interpretation? Exercise 9.17 You have two regressors \\(X_{1}\\) and \\(X_{2}\\) and estimate a regression with all quadratic terms included\n\n\\[\nY=\\alpha+\\beta_{1} X_{1}+\\beta_{2} X_{2}+\\beta_{3} X_{1}^{2}+\\beta_{4} X_{2}^{2}+\\beta_{5} X_{1} X_{2}+e .\n\\]\nOne of your advisors asks: Can we exclude the variable \\(X_{2}\\) from this regression?\nHow do you translate this question into a statistical test? When answering these questions, be specific, not general.\n\nWhat is the relevant null and alternative hypotheses?\nWhat is an appropriate test statistic?\nWhat is the appropriate asymptotic distribution for the statistic?\nWhat is the rule for acceptance/rejection of the null hypothesis?\n\nExercise 9.18 The observed data is \\(\\left\\{Y_{i}, X_{i}, Z_{i}\\right\\} \\in \\mathbb{R} \\times \\mathbb{R}^{k} \\times \\mathbb{R}^{\\ell}, k>1\\) and \\(\\ell>1, i=1, \\ldots, n\\). An econometrician first estimates \\(Y_{i}=X_{i}^{\\prime} \\widehat{\\beta}+\\widehat{e}_{i}\\) by least squares. The econometrician next regresses the residual \\(\\widehat{e}_{i}\\) on \\(Z_{i}\\), which can be written as \\(\\widehat{e}_{i}=Z_{i}^{\\prime} \\widetilde{\\gamma}+\\widetilde{u}_{i}\\).\n\nDefine the population parameter \\(\\gamma\\) being estimated in this second regression.\nFind the probability limit for \\(\\widetilde{\\gamma}\\).\nSuppose the econometrician constructs a Wald statistic \\(W\\) for \\(\\mathbb{H}_{0}: \\gamma=0\\) from the second regression, ignoring the two-stage estimation process. Write down the formula for \\(W\\).\nAssume \\(\\mathbb{E}\\left[Z X^{\\prime}\\right]=0\\). Find the asymptotic distribution for \\(W\\) under \\(\\mathbb{M}_{0}: \\gamma=0\\).\nIf \\(\\mathbb{E}\\left[Z X^{\\prime}\\right] \\neq 0\\) will your answer to (d) change?\n\nExercise 9.19 An economist estimates \\(Y=X_{1}^{\\prime} \\beta_{1}+X_{2} \\beta_{2}+e\\) by least squares and tests hypothesis \\(\\mathbb{H}_{0}: \\beta_{2}=0\\) against \\(\\mathbb{H}_{1}: \\beta_{2} \\neq 0\\). Assume \\(\\beta_{1} \\in \\mathbb{R}^{k}\\) and \\(\\beta_{2} \\in \\mathbb{R}\\). She obtains a Wald statistic \\(W=0.34\\). The sample size is \\(n=500\\).\n\nWhat is the correct degrees of freedom for the \\(\\chi^{2}\\) distribution to evaluate the significance of the Wald statistic?\nThe Wald statistic \\(W\\) is very small. Indeed, is it less than the \\(1 %\\) quantile of the appropriate \\(\\chi^{2}\\) distribution? If so, should you reject \\(\\mathbb{H}_{0}\\) ? Explain your reasoning.\n\nExercise 9.20 You are reading a paper, and it reports the results from two nested OLS regressions:\n\\[\n\\begin{aligned}\n&Y_{i}=X_{1 i}^{\\prime} \\widetilde{\\beta}_{1}+\\widetilde{e}_{i} \\\\\n&Y_{i}=X_{1 i}^{\\prime} \\widehat{\\beta}_{1}+X_{2 i}^{\\prime} \\widehat{\\beta}_{2}+\\widehat{e}_{i} .\n\\end{aligned}\n\\]\nSome summary statistics are reported:\n\\[\n\\begin{array}{ll}\n\\text { Short Regression } & \\text { Long Regression } \\\\\nR^{2}=.20 & R^{2}=.26 \\\\\n\\sum_{i=1}^{n} \\widetilde{e}_{i}^{2}=106 & \\sum_{i=1}^{n} \\widehat{e}_{i}^{2}=100 \\\\\n\\# \\text { of coefficients }=5 & \\# \\text { of coefficients }=8 \\\\\nn=50 & n=50\n\\end{array}\n\\]\nYou are curious if the estimate \\(\\widehat{\\beta}_{2}\\) is statistically different from the zero vector. Is there a way to determine an answer from this information? Do you have to make any assumptions (beyond the standard regularity conditions) to justify your answer? Exercise 9.21 Take the model \\(Y=X_{1} \\beta_{1}+X_{2} \\beta_{2}+X_{3} \\beta_{3}+X_{4} \\beta_{4}+e\\) with \\(\\mathbb{E}[X e]=0\\). Describe how to test\n\\[\n\\mathbb{M}_{0}: \\frac{\\beta_{1}}{\\beta_{2}}=\\frac{\\beta_{3}}{\\beta_{4}}\n\\]\nagainst\n\\[\n\\mathbb{M}_{1}: \\frac{\\beta_{1}}{\\beta_{2}} \\neq \\frac{\\beta_{3}}{\\beta_{4}} .\n\\]\nExercise 9.22 You have a random sample from the model \\(Y=X \\beta_{1}+X^{2} \\beta_{2}+e\\) with \\(\\mathbb{E}[e \\mid X]=0\\) where \\(Y\\) is wages (dollars per hour) and \\(X\\) is age. Describe how you would test the hypothesis that the expected wage for a 40 -year-old worker is \\(\\$ 20\\) an hour.\nExercise 9.23 Let \\(T\\) be a test statistic such that under \\(\\mathbb{M}_{0}, T \\underset{d}{\\longrightarrow} \\chi_{3}^{2}\\). Since \\(\\mathbb{P}\\left[\\chi_{3}^{2}>7.815\\right]=0.05\\), an asymptotic \\(5 %\\) test of \\(\\mathbb{H}_{0}\\) rejects when \\(T>7.815\\). An econometrician is interested in the Type I error of this test when \\(n=100\\) and the data structure is well specified. She performs the following Monte Carlo experiment.\n\n\\(B=200\\) samples of size \\(n=100\\) are generated from a distribution satisfying \\(\\mathbb{H}_{0}\\).\nOn each sample, the test statistic \\(T_{b}\\) is calculated.\nShe calculates \\(\\hat{p}=B^{-1} \\sum_{b=1}^{B} \\mathbb{1}\\left\\{T_{b}>7.815\\right\\}=0.070\\).\nThe econometrician concludes that the test \\(T\\) is oversized in this context-it rejects too frequently under \\(\\mathbb{M}_{0}\\).\n\nIs her conclusion correct, incorrect, or incomplete? Be specific in your answer.\nExercise 9.24 Do a Monte Carlo simulation. Take the model \\(Y=\\alpha+X \\beta+e\\) with \\(\\mathbb{E}[X e]=0\\) where the parameter of interest is \\(\\theta=\\exp (\\beta)\\). Your data generating process (DGP) for the simulation is: \\(X\\) is \\(U[0,1]\\), \\(e \\sim \\mathrm{N}(0,1)\\) is independent of \\(X\\), and \\(n=50\\). Set \\(\\alpha=0\\) and \\(\\beta=1\\). Generate \\(B=1000\\) independent samples with \\(\\alpha\\). On each, estimate the regression by least squares, calculate the covariance matrix using a standard (heteroskedasticity-robust) formula, and similarly estimate \\(\\theta\\) and its standard error. For each replication, store \\(\\widehat{\\beta}, \\widehat{\\theta}, T_{\\beta}=(\\widehat{\\beta}-\\beta) / s(\\widehat{\\beta})\\), and \\(T_{\\theta}=(\\widehat{\\theta}-\\theta) / s(\\widehat{\\theta})\\).\n\nDoes the value of \\(\\alpha\\) matter? Explain why the described statistics are invariant to \\(\\alpha\\) and thus setting \\(\\alpha=0\\) is irrelevant.\nFrom the 1000 replications estimate \\(\\mathbb{E}[\\widehat{\\beta}]\\) and \\(\\mathbb{E}[\\widehat{\\theta}]\\). Discuss if you see evidence if either estimator is biased or unbiased.\nFrom the 1000 replications estimate \\(\\mathbb{P}\\left[T_{\\beta}>1.645\\right]\\) and \\(\\mathbb{P}\\left[T_{\\theta}>1.645\\right]\\). What does asymptotic theory predict these probabilities should be in large samples? What do your simulation results indicate?\n\nExercise 9.25 The data set Invest1993 on the textbook website contains data on 1962 U.S. firms extracted from Compustat, assembled by Bronwyn Hall, and used in Hall and Hall (1993).\nThe variables we use in this exercise are in the table below. The flow variables are annual sums. The stock variables are beginning of year.\n\n\n\n\nyear\nyear of the observation\n\n\n\n\n\\(I\\)\ninva\nInvestment to Capital Ratio\n\n\n\\(Q\\)\nvala\nTotal Market Value to Asset Ratio (Tobin’s Q)\n\n\n\\(C\\)\ncfa\nCash Flow to Asset Ratio\n\n\n\\(D\\)\ndebta\nLong Term Debt to Asset Ratio\n\n\n\n\nExtract the sub-sample of observations for 1987. There should be 1028 observations. Estimate a linear regression of \\(I\\) (investment to capital ratio) on the other variables. Calculate appropriate standard errors.\nCalculate asymptotic confidence intervals for the coefficients.\nThis regression is related to Tobin’s \\(q\\) theory of investment, which suggests that investment should be predicted solely by \\(Q\\) (Tobin’s \\(Q\\) ). This theory predicts that the coefficient on \\(Q\\) should be positive and the others should be zero. Test the joint hypothesis that the coefficients on cash flow \\((C)\\) and debt \\((D)\\) are zero. Test the hypothesis that the coefficient on \\(Q\\) is zero. Are the results consistent with the predictions of the theory?\nNow try a nonlinear (quadratic) specification. Regress \\(I\\) on \\(Q, C, D, Q^{2}, C^{2}, D^{2}, Q \\times C, Q \\times D, C \\times D\\). Test the joint hypothesis that the six interaction and quadratic coefficients are zero.\n\nExercise 9.26 In a paper in 1963, Marc Nerlove analyzed a cost function for 145 American electric companies. Nerlov was interested in estimating a cost function: \\(C=f(Q, P L, P F, P K)\\) where the variables are listed in the table below. His data set Nerlove1963 is on the textbook website.\n\n\n\nC\nTotal Cost\n\n\n\n\nQ\nOutput\n\n\nPL\nUnit price of labor\n\n\nPK\nUnit price of capital\n\n\nPF\nUnit price of fuel\n\n\n\n\nFirst, estimate an unrestricted Cobb-Douglass specification\n\n\\[\n\\log C=\\beta_{1}+\\beta_{2} \\log Q+\\beta_{3} \\log P L+\\beta_{4} \\log P K+\\beta_{5} \\log P F+e .\n\\]\nReport parameter estimates and standard errors.\n\nWhat is the economic meaning of the restriction \\(\\mathbb{H}_{0}: \\beta_{3}+\\beta_{4}+\\beta_{5}=1\\) ?\nEstimate (9.23) by constrained least squares imposing \\(\\beta_{3}+\\beta_{4}+\\beta_{5}=1\\). Report your parameter estimates and standard errors.\nEstimate (9.23) by efficient minimum distance imposing \\(\\beta_{3}+\\beta_{4}+\\beta_{5}=1\\). Report your parameter estimates and standard errors.\nTest \\(\\mathbb{H}_{0}: \\beta_{3}+\\beta_{4}+\\beta_{5}=1\\) using a Wald statistic.\nTest \\(\\mathbb{H}_{0}: \\beta_{3}+\\beta_{4}+\\beta_{5}=1\\) using a minimum distance statistic. Exercise 9.27 In Section 8.12 we reported estimates from Mankiw, Romer and Weil (1992). We reported estimation both by unrestricted least squares and by constrained estimation, imposing the constraint that three coefficients ( \\(2^{n d}, 3^{r d}\\) and \\(4^{t h}\\) coefficients) sum to zero as implied by the Solow growth theory. Using the same dataset MRW1992 estimate the unrestricted model and test the hypothesis that the three coefficients sum to zero.\n\nExercise 9.28 Using the cps09mar dataset and the subsample of non-Hispanic Black individuals (race code \\(=2\\) ) test the hypothesis that marriage status does not affect mean wages.\n\nTake the regression reported in Table 4.1. Which variables will need to be omitted to estimate a regression for this subsample?\nExpress the hypothesis “marriage status does not affect mean wages” as a restriction on the coefficients. How many restrictions is this?\nFind the Wald (or F) statistic for this hypothesis. What is the appropriate distribution for the test statistic? Calculate the p-value of the test.\nWhat do you conclude?\n\nExercise 9.29 Using the cps09mar dataset and the subsample of non-Hispanic Black individuals (race code \\(=2\\) ) and white individuals (race code \\(=1\\) ) test the hypothesis that the returns to education is common across groups.\n\nAllow the return to education to vary across the four groups (white male, white female, Black male, Black female) by interacting dummy variables with education. Estimate an appropriate version of the regression reported in Table 4.1.\nFind the Wald (or F) statistic for this hypothessis. What is the appropriate distribution for the test statistic? Calculate the p-value of the test.\nWhat do you conclude?"
  },
  {
    "objectID": "chpt10-resample-method.html",
    "href": "chpt10-resample-method.html",
    "title": "10  Resampling Methods",
    "section": "",
    "text": "So far in this textbook we have discussed two approaches to inference: exact and asymptotic. Both have their strengths and weaknesses. Exact theory provides a useful benchmark but is based on the unrealistic and stringent assumption of the homoskedastic normal regression model. Asymptotic theory provides a more flexible distribution theory but is an approximation with uncertain accuracy.\nIn this chapter we introduce a set of alternative inference methods which are based around the concept of resampling - which means using sampling information extracted from the empirical distribution of the data. These are powerful methods, widely applicable, and often more accurate than exact methods and asymptotic approximations. Two disadvantages, however, are (1) resampling methods typically require more computation power; and (2) the theory is considerably more challenging. A consequence of the computation requirement is that most empirical researchers use asymptotic approximations for routine calculations while resampling approximations are used for final reporting.\nWe will discuss two categories of resampling methods used in statistical and econometric practice: jackknife and bootstrap. Most of our attention will be given to the bootstrap as it is the most commonly used resampling method in econometric practice.\nThe jackknife is the distribution obtained from the \\(n\\) leave-one-out estimators (see Section 3.20). The jackknife is most commonly used for variance estimation.\nThe bootstrap is the distribution obtained by estimation on samples created by i.i.d. sampling with replacement from the dataset. (There are other variants of bootstrap sampling, including parametric sampling and residual sampling.) The bootstrap is commonly used for variance estimation, confidence interval construction, and hypothesis testing.\nThere is a third category of resampling methods known as sub-sampling which we will not cover in this textbook. Sub-sampling is the distribution obtained by estimation on sub-samples (sampling without replacement) of the dataset. Sub-sampling can be used for most of same purposes as the bootstrap. See the excellent monograph by Politis, Romano and Wolf (1999)."
  },
  {
    "objectID": "chpt10-resample-method.html#example",
    "href": "chpt10-resample-method.html#example",
    "title": "10  Resampling Methods",
    "section": "10.2 Example",
    "text": "10.2 Example\nTo motivate our discussion we focus on the application presented in Section 3.7, which is a bivariate regression applied to the CPS subsample of married Black female wage earners with 12 years potential work experience and displayed in Table 3.1. The regression equation is\n\\[\n\\log (\\text { wage })=\\beta_{1} \\text { education }+\\beta_{2}+e .\n\\]\nThe estimates as reported in (4.44) are\n\\[\n\\begin{aligned}\n& \\log (\\text { wage })=0.155 \\text { education }+0.698+\\widehat{e} \\\\\n& \\text { (0.031) } \\quad(0.493) \\\\\n& \\widehat{\\sigma}^{2}=0.144 \\\\\n& \\text { (0.043) } \\\\\n& n=20 \\text {. }\n\\end{aligned}\n\\]\nWe focus on four estimates constructed from this regression. The first two are the coefficient estimates \\(\\widehat{\\beta}_{1}\\) and \\(\\widehat{\\beta}_{2}\\). The third is the variance estimate \\(\\widehat{\\sigma}^{2}\\). The fourth is an estimate of the expected level of wages for an individual with 16 years of education (a college graduate), which turns out to be a nonlinear function of the parameters. Under the simplifying assumption that the error \\(e\\) is independent of the level of education and normally distributed we find that the expected level of wages is\n\\[\n\\begin{aligned}\n\\mu &=\\mathbb{E}[\\text { wage } \\mid \\text { education }=16] \\\\\n&=\\mathbb{E}\\left[\\exp \\left(16 \\beta_{1}+\\beta_{2}+e\\right)\\right] \\\\\n&=\\exp \\left(16 \\beta_{1}+\\beta_{2}\\right) \\mathbb{E}[\\exp (e)] \\\\\n&=\\exp \\left(16 \\beta_{1}+\\beta_{2}+\\sigma^{2} / 2\\right) .\n\\end{aligned}\n\\]\nThe final equality is \\(\\mathbb{E}[\\exp (e)]=\\exp \\left(\\sigma^{2} / 2\\right)\\) which can be obtained from the normal moment generating function. The parameter \\(\\mu\\) is a nonlinear function of the coefficients. The natural estimator of \\(\\mu\\) replaces the unknowns by the point estimators. Thus\n\\[\n\\widehat{\\mu}=\\exp \\left(16 \\widehat{\\beta}_{1}+\\widehat{\\beta}_{2}+\\widehat{\\sigma}^{2} / 2\\right)=25.80\n\\]\nThe standard error for \\(\\widehat{\\mu}\\) can be found by extending Exercise \\(7.8\\) to find the joint asymptotic distribution of \\(\\widehat{\\sigma}^{2}\\) and the slope estimates, and then applying the delta method.\nWe are interested in calculating standard errors and confidence intervals for the four estimates described above."
  },
  {
    "objectID": "chpt10-resample-method.html#jackknife-estimation-of-variance",
    "href": "chpt10-resample-method.html#jackknife-estimation-of-variance",
    "title": "10  Resampling Methods",
    "section": "10.3 Jackknife Estimation of Variance",
    "text": "10.3 Jackknife Estimation of Variance\nThe jackknife estimates moments of estimators using the distribution of the leave-one-out estimators. The jackknife estimators of bias and variance were introduced by Quenouille (1949) and Tukey (1958), respectively. The idea was expanded further in the monographs of Efron (1982) and Shao and Tu (1995).\nLet \\(\\widehat{\\theta}\\) be any estimator of a vector-valued parameter \\(\\theta\\) which is a function of a random sample of size \\(n\\). Let \\(\\boldsymbol{V}_{\\widehat{\\theta}}=\\operatorname{var}[\\widehat{\\theta}]\\) be the variance of \\(\\widehat{\\theta}\\). Define the leave-one-out estimators \\(\\widehat{\\theta}_{(-i)}\\) which are computed using the formula for \\(\\widehat{\\theta}\\) except that observation \\(i\\) is deleted. Tukey’s jackknife estimator for \\(\\boldsymbol{V}_{\\widehat{\\theta}}\\) is defined as a scale of the sample variance of the leave-one-out estimators:\n\\[\n\\widehat{\\boldsymbol{V}}_{\\widehat{\\theta}}^{\\text {jack }}=\\frac{n-1}{n} \\sum_{i=1}^{n}\\left(\\widehat{\\theta}_{(-i)}-\\bar{\\theta}\\right)\\left(\\widehat{\\theta}_{(-i)}-\\bar{\\theta}\\right)^{\\prime}\n\\]\nwhere \\(\\bar{\\theta}\\) is the sample mean of the leave-one-out estimators \\(\\bar{\\theta}=n^{-1} \\sum_{i=1}^{n} \\widehat{\\theta}_{(-i)}\\). For scalar estimators \\(\\widehat{\\theta}\\) the jackknife standard error is the square root of (10.1): \\(s_{\\widehat{\\theta}}^{\\text {jack }}=\\sqrt{\\widehat{V}_{\\widehat{\\theta}}^{\\text {jack }}}\\).\nA convenient feature of the jackknife estimator \\(\\widehat{V}_{\\widehat{\\theta}}^{\\text {jack }}\\) is that the formula (10.1) is quite general and does not require any technical (exact or asymptotic) calculations. A downside is that can require \\(n\\) separate estimations, which in some cases can be computationally costly.\nIn most cases \\(\\widehat{\\boldsymbol{V}}_{\\widehat{\\theta}}^{\\text {jack }}\\) will be similar to a robust asymptotic covariance matrix estimator. The main attractions of the jackknife estimator are that it can be used when an explicit asymptotic variance formula is not available and that it can be used as a check on the reliability of an asymptotic formula.\nThe formula (10.1) is not immediately intuitive so may benefit from some motivation. We start by examining the sample mean \\(\\bar{Y}=\\frac{1}{n} \\sum_{i=1}^{n} Y_{i}\\) for \\(Y \\in \\mathbb{R}^{m}\\). The leave-one-out estimator is\n\\[\n\\bar{Y}_{(-i)}=\\frac{1}{n-1} \\sum_{j \\neq i} Y_{j}=\\frac{n}{n-1} \\bar{Y}-\\frac{1}{n-1} Y_{i} .\n\\]\nThe sample mean of the leave-one-out estimators is\n\\[\n\\frac{1}{n} \\sum_{i=1}^{n} \\bar{Y}_{(-i)}=\\frac{n}{n-1} \\bar{Y}-\\frac{1}{n-1} \\bar{Y}=\\bar{Y}\n\\]\nThe difference is\n\\[\n\\bar{Y}_{(-i)}-\\bar{Y}=\\frac{1}{n-1}\\left(\\bar{Y}-Y_{i}\\right) .\n\\]\nThe jackknife estimate of variance (10.1) is then\n\\[\n\\begin{aligned}\n\\widehat{\\boldsymbol{V}}_{\\bar{Y}}^{\\text {jack }} &=\\frac{n-1}{n} \\sum_{i=1}^{n}\\left(\\frac{1}{n-1}\\right)^{2}\\left(\\bar{Y}-Y_{i}\\right)\\left(\\bar{Y}-Y_{i}\\right)^{\\prime} \\\\\n&=\\frac{1}{n}\\left(\\frac{1}{n-1}\\right) \\sum_{i=1}^{n}\\left(\\bar{Y}-Y_{i}\\right)\\left(\\bar{Y}-Y_{i}\\right)^{\\prime}\n\\end{aligned}\n\\]\nThis is identical to the conventional estimator for the variance of \\(\\bar{Y}\\). Indeed, Tukey proposed the \\((n-1) / n\\) scaling in (10.1) so that \\(\\widehat{V}_{\\bar{Y}}^{\\text {jack }}\\) precisely equals the conventional estimator.\nWe next examine the case of least squares regression coefficient estimator. Recall from (3.43) that the leave-one-out OLS estimator equals\n\\[\n\\widehat{\\beta}_{(-i)}=\\widehat{\\beta}-\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} X_{i} \\widetilde{e}_{i}\n\\]\nwhere \\(\\widetilde{e}_{i}=\\left(1-h_{i i}\\right)^{-1} \\widehat{e}_{i}\\) and \\(h_{i i}=X_{i}^{\\prime}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} X_{i}\\). The sample mean of the leave-one-out estimators is \\(\\bar{\\beta}=\\widehat{\\beta}-\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\widetilde{\\mu}\\) where \\(\\widetilde{\\mu}=n^{-1} \\sum_{i=1}^{n} X_{i} \\widetilde{e}_{i}\\). Thus \\(\\widehat{\\beta}_{(-i)}-\\bar{\\beta}=-\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1}\\left(X_{i} \\widetilde{e}_{i}-\\widetilde{\\mu}\\right)\\). The jackknife estimate of variance for \\(\\widehat{\\beta}\\) is\n\\[\n\\begin{aligned}\n\\widehat{\\boldsymbol{V}}_{\\widehat{\\beta}}^{\\text {jack }} &=\\frac{n-1}{n} \\sum_{i=1}^{n}\\left(\\widehat{\\beta}_{(-i)}-\\bar{\\beta}\\right)\\left(\\widehat{\\beta}_{(-i)}-\\bar{\\beta}\\right)^{\\prime} \\\\\n&=\\frac{n-1}{n}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1}\\left(\\sum_{i=1}^{n} X_{i} X_{i}^{\\prime} \\tilde{e}_{i}^{2}-n \\widetilde{\\mu} \\widetilde{\\mu}^{\\prime}\\right)\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\\\\n&=\\frac{n-1}{n} \\widehat{\\boldsymbol{V}}_{\\widehat{\\beta}}^{\\mathrm{HC} 3}-(n-1)\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\widetilde{\\mu} \\widetilde{\\mu}^{\\prime}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1}\n\\end{aligned}\n\\]\nwhere \\(\\widehat{\\boldsymbol{V}}_{\\widehat{\\beta}}^{\\mathrm{HC}}\\) is the HC3 covariance estimator (4.39) based on prediction errors. The second term in (10.5) is typically quite small since \\(\\widetilde{\\mu}\\) is typically small in magnitude. Thus \\(\\widehat{\\boldsymbol{V}}_{\\widehat{\\beta}}^{\\text {jack }} \\simeq \\widehat{\\boldsymbol{V}}_{\\widehat{\\beta}}^{\\mathrm{HC}}\\). Indeed the HC3 estimator was originally motivated as a simplification of the jackknife estimator. This shows that for regression coefficients the jackknife estimator of variance is similar to a conventional robust estimator. This is accomplished without the user “knowing” the form of the asymptotic covariance matrix. This is further confirmation that the jackknife is making a reasonable calculation.\nThird, we examine the jackknife estimator for a function \\(\\widehat{\\theta}=r(\\widehat{\\beta})\\) of a least squares estimator. The leave-one-out estimator of \\(\\theta\\) is\n\\[\n\\begin{aligned}\n\\widehat{\\theta}_{(-i)} &=r\\left(\\widehat{\\beta}_{(-i)}\\right) \\\\\n&=r\\left(\\widehat{\\beta}-\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} X_{i} \\widetilde{e}_{i}\\right) \\\\\n& \\simeq \\widehat{\\theta}-\\widehat{\\boldsymbol{R}}^{\\prime}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} X_{i} \\widetilde{e}_{i}\n\\end{aligned}\n\\]\nThe second equality is (10.4). The final approximation is obtained by a mean-value expansion, using \\(r(\\widehat{\\beta})=\\widehat{\\theta}\\) and setting \\(\\widehat{\\boldsymbol{R}}=(\\partial / \\partial \\beta) r(\\widehat{\\beta})^{\\prime}\\). This approximation holds in large samples because \\(\\widehat{\\beta}_{(-i)}\\) are uniformly consistent for \\(\\beta\\). The jackknife variance estimator for \\(\\widehat{\\theta}\\) thus equals\n\\[\n\\begin{aligned}\n\\widehat{\\boldsymbol{V}}_{\\widehat{\\theta}}^{\\mathrm{jack}} &=\\frac{n-1}{n} \\sum_{i=1}^{n}\\left(\\widehat{\\theta}_{(-i)}-\\bar{\\theta}\\right)\\left(\\widehat{\\theta}_{(-i)}-\\bar{\\theta}\\right)^{\\prime} \\\\\n& \\simeq \\frac{n-1}{n} \\widehat{\\boldsymbol{R}}^{\\prime}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1}\\left(\\sum_{i=1}^{n} X_{i} X_{i}^{\\prime} \\widehat{e}_{i}^{2}-n \\widetilde{\\mu} \\widetilde{\\mu}^{\\prime}\\right)\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\widehat{\\boldsymbol{R}} \\\\\n&=\\widehat{\\boldsymbol{R}}^{\\prime} \\widehat{\\boldsymbol{V}}_{\\widehat{\\beta}}^{\\mathrm{jack}} \\widehat{\\boldsymbol{R}} \\\\\n& \\simeq \\widehat{\\boldsymbol{R}}^{\\prime} \\widetilde{\\boldsymbol{V}}_{\\widehat{\\beta}} \\widehat{\\boldsymbol{R}} .\n\\end{aligned}\n\\]\nThe final line equals a delta-method estimator for the variance of \\(\\widehat{\\theta}\\) constructed with the covariance estimator (4.39). This shows that the jackknife estimator of variance for \\(\\widehat{\\theta}\\) is approximately an asymptotic delta-method estimator. While this is an asymptotic approximation, it again shows that the jackknife produces an estimator which is asymptotically similar to one produced by asymptotic methods. This is despite the fact that the jackknife estimator is calculated without reference to asymptotic theory and does not require calculation of the derivatives of \\(r(\\beta)\\).\nThis argument extends directly to any “smooth function” estimator. Most of the estimators discussed so far in this textbook take the form \\(\\widehat{\\theta}=g(\\bar{W})\\) where \\(\\bar{W}=n^{-1} \\sum_{i=1}^{n} W_{i}\\) and \\(W_{i}\\) is some vector-valued function of the data. For any such estimator \\(\\widehat{\\theta}\\) the leave-one-out estimator equals \\(\\widehat{\\theta}_{(-i)}=g\\left(\\bar{W}_{(-i)}\\right)\\) and its jackknife estimator of variance is (10.1). Using (10.2) and a mean-value expansion we have the largesample approximation\n\\[\n\\begin{aligned}\n\\widehat{\\theta}_{(-i)} &=g\\left(\\bar{W}_{(-i)}\\right) \\\\\n&=g\\left(\\frac{n}{n-1} \\bar{W}-\\frac{1}{n-1} W_{i}\\right) \\\\\n& \\simeq g(\\bar{W})-\\frac{1}{n-1} \\boldsymbol{G}(\\bar{W})^{\\prime} W_{i}\n\\end{aligned}\n\\]\nwhere \\(\\boldsymbol{G}(x)=(\\partial / \\partial x) g(x)^{\\prime}\\). Thus\n\\[\n\\widehat{\\theta}_{(-i)}-\\bar{\\theta} \\simeq-\\frac{1}{n-1} \\boldsymbol{G}(\\bar{W})^{\\prime}\\left(W_{i}-\\bar{W}\\right)\n\\]\nand the jackknife estimator of the variance of \\(\\widehat{\\theta}\\) approximately equals\n\\[\n\\begin{aligned}\n\\widehat{\\boldsymbol{V}}_{\\widehat{\\theta}}^{\\mathrm{jack}} &=\\frac{n-1}{n} \\sum_{i=1}^{n}\\left(\\widehat{\\theta}_{(-i)}-\\widehat{\\theta}_{(\\cdot)}\\right)\\left(\\widehat{\\theta}_{(-i)}-\\widehat{\\theta}_{(\\cdot)}\\right)^{\\prime} \\\\\n& \\simeq \\frac{n-1}{n} \\boldsymbol{G}(\\bar{W})^{\\prime}\\left(\\frac{1}{(n-1)^{2}} \\sum_{i=1}^{n}\\left(W_{i}-\\bar{W}\\right)\\left(W_{i}-\\bar{W}\\right)^{\\prime}\\right) \\boldsymbol{G}(\\bar{W}) \\\\\n&=\\boldsymbol{G}(\\bar{W})^{\\prime} \\widehat{\\boldsymbol{V}}_{\\bar{W}}^{\\mathrm{jack}} \\boldsymbol{G}(\\bar{W})\n\\end{aligned}\n\\]\nwhere \\(\\widehat{V}_{\\bar{W}}^{\\text {jack }}\\) as defined in (10.3) is the conventional (and jackknife) estimator for the variance of \\(\\bar{W}\\). Thus \\(\\widehat{\\boldsymbol{V}}_{\\widehat{\\theta}}^{\\text {jack }}\\) is approximately the delta-method estimator. Once again, we see that the jackknife estimator automatically calculates what is effectively the delta-method variance estimator, but without requiring the user to explicitly calculate the derivative of \\(g(x)\\)."
  },
  {
    "objectID": "chpt10-resample-method.html#example-1",
    "href": "chpt10-resample-method.html#example-1",
    "title": "10  Resampling Methods",
    "section": "10.4 Example",
    "text": "10.4 Example\nWe illustrate by reporting the asymptotic and jackknife standard errors for the four parameter estimates given earlier. In Table \\(10.1\\) we report the actual values of the leave-one-out estimates for each of the twenty observations in the sample. The jackknife standard errors are calculated as the scaled square roots of the sample variances of these leave-one-out estimates and are reported in the second-to-last row. For comparison the asymptotic standard errors are reported in the final row.\nFor all estimates the jackknife and asymptotic standard errors are quite similar. This reinforces the credibility of both standard error estimates. The largest differences arise for \\(\\widehat{\\beta}_{2}\\) and \\(\\widehat{\\mu}\\), whose jackknife standard errors are about \\(5 %\\) larger than the asymptotic standard errors.\nThe take-away from our presentation is that the jackknife is a simple and flexible method for variance and standard error calculation. Circumventing technical asymptotic and exact calculations, the jackknife produces estimates which in many cases are similar to asymptotic delta-method counterparts. The jackknife is especially appealing in cases where asymptotic standard errors are not available or are difficult to calculate. They can also be used as a double-check on the reasonableness of asymptotic delta-method calculations.\nIn Stata, jackknife standard errors for coefficient estimates in many models are obtained by the vce(jackknife) option. For nonlinear functions of the coefficients or other estimators the jackkn ife command can be combined with any other command to obtain jackknife standard errors.\nTo illustrate, below we list the Stata commands which calculate the jackknife standard errors listed above. The first line is least squares estimation with standard errors calculated by the jackknife. The second line calculates the error variance estimate \\(\\widehat{\\sigma}^{2}\\) with a jackknife standard error. The third line does the same for the estimate \\(\\widehat{\\mu}\\).\n\nTable 10.1: Leave-one-out Estimators and Jackknife Standard Errors\n\n\n\n\n\n\n\n\n\n\nObservation\n\\(\\widehat{\\beta}_{1(-i)}\\)\n\\(\\widehat{\\beta}_{2(-i)}\\)\n\\(\\widehat{\\sigma}_{(-i)}^{2}\\)\n\\(\\widehat{\\mu}_{(-i)}\\)\n\n\n\n\n1\n\\(0.150\\)\n\\(0.764\\)\n\\(0.150\\)\n\\(25.63\\)\n\n\n2\n\\(0.148\\)\n\\(0.798\\)\n\\(0.149\\)\n\\(25.48\\)\n\n\n3\n\\(0.153\\)\n\\(0.739\\)\n\\(0.151\\)\n\\(25.97\\)\n\n\n4\n\\(0.156\\)\n\\(0.695\\)\n\\(0.144\\)\n\\(26.31\\)\n\n\n5\n\\(0.154\\)\n\\(0.701\\)\n\\(0.146\\)\n\\(25.38\\)\n\n\n6\n\\(0.158\\)\n\\(0.655\\)\n\\(0.151\\)\n\\(26.05\\)\n\n\n7\n\\(0.152\\)\n\\(0.705\\)\n\\(0.114\\)\n\\(24.32\\)\n\n\n8\n\\(0.146\\)\n\\(0.822\\)\n\\(0.147\\)\n\\(25.37\\)\n\n\n9\n\\(0.162\\)\n\\(0.588\\)\n\\(0.151\\)\n\\(25.75\\)\n\n\n10\n\\(0.157\\)\n\\(0.693\\)\n\\(0.139\\)\n\\(26.40\\)\n\n\n11\n\\(0.168\\)\n\\(0.510\\)\n\\(0.141\\)\n\\(26.40\\)\n\n\n12\n\\(0.158\\)\n\\(0.691\\)\n\\(0.118\\)\n\\(26.48\\)\n\n\n13\n\\(0.139\\)\n\\(0.974\\)\n\\(0.141\\)\n\\(26.56\\)\n\n\n14\n\\(0.169\\)\n\\(0.451\\)\n\\(0.131\\)\n\\(26.26\\)\n\n\n15\n\\(0.146\\)\n\\(0.852\\)\n\\(0.150\\)\n\\(24.93\\)\n\n\n16\n\\(0.156\\)\n\\(0.696\\)\n\\(0.148\\)\n\\(26.06\\)\n\n\n17\n\\(0.165\\)\n\\(0.513\\)\n\\(0.140\\)\n\\(25.22\\)\n\n\n18\n\\(0.155\\)\n\\(0.698\\)\n\\(0.151\\)\n\\(25.90\\)\n\n\n19\n\\(0.152\\)\n\\(0.742\\)\n\\(0.151\\)\n\\(25.73\\)\n\n\n20\n\\(0.155\\)\n\\(0.697\\)\n\\(0.151\\)\n\\(25.95\\)\n\n\n\\(s^{\\text {jack }}\\)\n\\(0.032\\)\n\\(0.514\\)\n\\(0.046\\)\n\\(2.39\\)\n\n\n\\(s^{\\text {asy }}\\)\n\\(0.031\\)\n\\(0.493\\)\n\\(0.043\\)\n\\(2.29\\)"
  },
  {
    "objectID": "chpt10-resample-method.html#jackknife-for-clustered-observations",
    "href": "chpt10-resample-method.html#jackknife-for-clustered-observations",
    "title": "10  Resampling Methods",
    "section": "10.5 Jackknife for Clustered Observations",
    "text": "10.5 Jackknife for Clustered Observations\nIn Section \\(4.21\\) we introduced the clustered regression model, cluster-robust variance estimators, and cluster-robust standard errors. Jackknife variance estimation can also be used for clustered samples but with some natural modifications. Recall that the least squares estimator in the clustered sample context can be written as\n\\[\n\\widehat{\\beta}=\\left(\\sum_{g=1}^{G} \\boldsymbol{X}_{g}^{\\prime} \\boldsymbol{X}_{g}\\right)^{-1}\\left(\\sum_{g=1}^{G} \\boldsymbol{X}_{g}^{\\prime} \\boldsymbol{Y}_{g}\\right)\n\\]\nwhere \\(g=1, \\ldots, G\\) indexes the cluster. Instead of leave-one-out estimators, it is natural to use deletecluster estimators, which delete one cluster at a time. They take the form (4.58):\n\\[\n\\widehat{\\beta}_{(-g)}=\\widehat{\\beta}-\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\boldsymbol{X}_{g}^{\\prime} \\widetilde{\\boldsymbol{e}}_{g}\n\\]\nwhere\n\\[\n\\begin{aligned}\n&\\widetilde{\\boldsymbol{e}}_{g}=\\left(\\boldsymbol{I}_{n_{g}}-\\boldsymbol{X}_{g}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\boldsymbol{X}_{g}^{\\prime}\\right)^{-1} \\widehat{\\boldsymbol{e}}_{g} \\\\\n&\\widehat{\\boldsymbol{e}}_{g}=\\boldsymbol{Y}_{g}-\\boldsymbol{X}_{g} \\widehat{\\beta}\n\\end{aligned}\n\\]\nThe delete-cluster jackknife estimator of the variance of \\(\\widehat{\\beta}\\) is\n\\[\n\\begin{aligned}\n\\widehat{\\boldsymbol{V}}_{\\widehat{\\beta}}^{\\mathrm{jack}} &=\\frac{G-1}{G} \\sum_{g=1}^{G}\\left(\\widehat{\\beta}_{(-g)}-\\bar{\\beta}\\right)\\left(\\widehat{\\beta}_{(-g)}-\\bar{\\beta}\\right)^{\\prime} \\\\\n\\bar{\\beta} &=\\frac{1}{G} \\sum_{g=1}^{G} \\widehat{\\beta}_{(-g)} .\n\\end{aligned}\n\\]\nWe call \\(\\widehat{V}_{\\widehat{\\beta}}^{\\text {jack }}\\) a cluster-robust jackknife estimator of variance.\nUsing the same approximations as the previous section we can show that the delete-cluster jackknife estimator is asymptotically equivalent to the cluster-robust covariance matrix estimator (4.59) calculated with the delete-cluster prediction errors. This verifies that the delete-cluster jackknife is the appropriate jackknife approach for clustered dependence.\nFor parameters which are functions \\(\\widehat{\\theta}=r(\\widehat{\\beta})\\) of the least squares estimator, the delete-cluster jackknife estimator of the variance of \\(\\widehat{\\theta}\\) is\n\\[\n\\begin{aligned}\n\\widehat{\\boldsymbol{V}}_{\\widehat{\\theta}}^{\\text {jack }} &=\\frac{G-1}{G} \\sum_{g=1}^{G}\\left(\\widehat{\\theta}_{(-g)}-\\bar{\\theta}\\right)\\left(\\widehat{\\theta}_{(-g)}-\\bar{\\theta}\\right)^{\\prime} \\\\\n\\widehat{\\theta}_{(-i)} &=r\\left(\\widehat{\\beta}_{(-g)}\\right) \\\\\n\\bar{\\theta} &=\\frac{1}{G} \\sum_{g=1}^{G} \\widehat{\\theta}_{(-g)} .\n\\end{aligned}\n\\]\nUsing a mean-value expansion we can show that this estimator is asymptotically equivalent to the deltamethod cluster-robust covariance matrix estimator for \\(\\widehat{\\theta}\\). This shows that the jackknife estimator is appropriate for covariance matrix estimation.\nAs in the context of i.i.d. samples, one advantage of the jackknife covariance matrix estimators is that they do not require the user to make a technical calculation of the asymptotic distribution. A downside is an increase in computation cost, as \\(G\\) separate regressions are effectively estimated.\nIn Stata, jackknife standard errors for coefficient estimates with clustered observations are obtained by using the options cluster (id) vce(jackkn ife) where id denotes the cluster variable."
  },
  {
    "objectID": "chpt10-resample-method.html#the-bootstrap-algorithm",
    "href": "chpt10-resample-method.html#the-bootstrap-algorithm",
    "title": "10  Resampling Methods",
    "section": "10.6 The Bootstrap Algorithm",
    "text": "10.6 The Bootstrap Algorithm\nThe bootstrap is a powerful approach to inference and is due to the pioneering work of Efron (1979). There are many textbook and monograph treatments of the bootstrap, including Efron (1982), Hall (1992), Efron and Tibshirani (1993), Shao and Tu (1995), and Davison and Hinkley (1997). Reviews for econometricians are provided by Hall (1994) and Horowitz (2001)\nThere are several ways to describe or define the bootstrap and there are several forms of the bootstrap. We start in this section by describing the basic nonparametric bootstrap algorithm. In subsequent sections we give more formal definitions of the bootstrap as well as theoretical justifications.\nBriefly, the bootstrap distribution is obtained by estimation on independent samples created by i.i.d. sampling (sampling with replacement) from the original dataset.\nTo understand this it is useful to start with the concept of sampling with replacement from the dataset. To continue the empirical example used earlier in the chapter we focus on the dataset displayed in Table 3.1, which has \\(n=20\\) observations. Sampling from this distribution means randomly selecting one row from this table. Mathematically this is the same as randomly selecting an integer from the set \\(\\{1,2, \\ldots, 20\\}\\). To illustrate, MATLAB has a random integer generator (the function randi). Using the random number seed of 13 (an arbitrary choice) we obtain the random draw 16 . This means that we draw observation number 16 from Table 3.1. Examining the table we can see that this is an individual with wage \\(\\$ 18.75\\) and education of 16 years. We repeat by drawing another random integer on the set \\(\\{1,2, \\ldots, 20\\}\\) and this time obtain 5 . This means we take observation 5 from Table 3.1, which is an individual with wage \\(\\$ 33.17\\) and education of 16 years. We continue until we have \\(n=20\\) such draws. This random set of observations are \\(\\{16,5,17,20,20,10,13,16,13,15,1,6,2,18,8,14,6,7,1,8\\}\\). We call this the bootstrap sample.\nNotice that the observations \\(1,6,8,13,16,20\\) each appear twice in the bootstrap sample, and the observations \\(3,4,9,11,12,19\\) do not appear at all. That is okay. In fact, it is necessary for the bootstrap to work. This is because we are drawing with replacement. (If we instead made draws without replacement then the constructed dataset would have exactly the same observations as in Table 3.1, only in different order.) We can also ask the question “What is the probability that an individual observation will appear at least once in the bootstrap sample?” The answer is\n\\[\n\\begin{aligned}\n\\mathbb{P}[\\text { Observation in Bootstrap Sample }] &=1-\\left(1-\\frac{1}{n}\\right)^{n} \\\\\n& \\rightarrow 1-e^{-1} \\simeq 0.632 .\n\\end{aligned}\n\\]\nThe limit holds as \\(n \\rightarrow \\infty\\). The approximation \\(0.632\\) is excellent even for small \\(n\\). For example, when \\(n=20\\) the probability (10.6) is \\(0.641\\). These calculations show that an individual observation is in the bootstrap sample with probability near \\(2 / 3\\).\nOnce again, the bootstrap sample is the constructed dataset with the 20 observations drawn randomly from the original sample. Notationally, we write the \\(i^{\\text {th }}\\) bootstrap observation as \\(\\left(Y_{i}^{*}, X_{i}^{*}\\right)\\) and the bootstrap sample as \\(\\left\\{\\left(Y_{1}^{*}, X_{1}^{*}\\right), \\ldots,\\left(Y_{n}^{*}, X_{n}^{*}\\right)\\right\\}\\). In our present example with \\(Y\\) denoting the log wage the bootstrap sample is\n\\[\n\\left\\{\\left(Y_{1}^{*}, X_{1}^{*}\\right), \\ldots,\\left(Y_{n}^{*}, X_{n}^{*}\\right)\\right\\}=\\{(2.93,16),(3.50,16) \\ldots,(3.76,18)\\}\n\\]\nThe bootstrap estimate \\(\\widehat{\\beta}^{*}\\) is obtained by applying the least squares estimation formula to the bootstrap sample. Thus we regress \\(Y^{*}\\) on \\(X^{*}\\). The other bootstrap estimates, in our example \\(\\widehat{\\sigma}^{2 *}\\) and \\(\\widehat{\\mu}^{*}\\), are obtained by applying their estimation formulae to the bootstrap sample as well. Writing \\(\\widehat{\\theta}^{*}=\\) \\(\\left(\\widehat{\\beta}_{1}^{*}, \\widehat{\\beta}_{2}^{*}, \\widehat{\\sigma}^{* 2}, \\widehat{\\mu}^{*}\\right)^{\\prime}\\) we have the bootstrap estimate of the parameter vector \\(\\theta=\\left(\\beta_{1}, \\beta_{2}, \\sigma^{2}, \\mu\\right)^{\\prime}\\). In our example (the bootstrap sample described above) \\(\\widehat{\\theta}^{*}=(0.195,0.113,0.107,26.7)^{\\prime}\\). This is one draw from the bootstrap distribution of the estimates.\nThe estimate \\(\\widehat{\\theta}^{*}\\) as described is one random draw from the distribution of estimates obtained by i.i.d. sampling from the original data. With one draw we can say relatively little. But we can repeat this exercise to obtain multiple draws from this bootstrap distribution. To distinguish between these draws we index the bootstrap samples by \\(b=1, \\ldots, B\\), and write the bootstrap estimates as \\(\\widehat{\\theta}_{b}^{*}\\) or \\(\\widehat{\\theta}^{*}(b)\\).\nTo continue our illustration we draw 20 more random integers \\(\\{19,5,7,19,1,2,13,18,1,15,17,2\\), \\(14,11,10,20,1,5,15,7\\}\\) and construct a second bootstrap sample. On this sample we again estimate the parameters and obtain \\(\\widehat{\\theta}^{*}(2)=(0.175,0.52,0.124,29.3)^{\\prime}\\). This is a second random draw from the distribution of \\(\\widehat{\\theta}^{*}\\). We repeat this \\(B\\) times, storing the parameter estimates \\(\\widehat{\\theta}^{*}(b)\\). We have thus created a new dataset of bootstrap draws \\(\\left\\{\\widehat{\\theta}^{*}(b): b=1, \\ldots, B\\right\\}\\). By construction the draws are independent across \\(b\\) and identically distributed.\nThe number of bootstrap draws, \\(B\\), is often called the “number of bootstrap replications”. Typical choices for \\(B\\) are 1000,5000 , and 10,000. We discuss selecting \\(B\\) later, but roughly speaking, larger \\(B\\) results in a more precise estimate at an increased computation cost. For our application we set \\(B=\\) 10,000 . To illustrate, Figure \\(13.1\\) displays the densities of the distributions of the bootstrap estimates \\(\\widehat{\\beta}_{1}^{*}\\) and \\(\\widehat{\\mu}^{*}\\) across 10,000 draws. The dashed lines show the point estimate. You can notice that the density for \\(\\widehat{\\beta}_{1}^{*}\\) is slightly skewed to the left.\\\n\nFigure 10.1: Bootstrap Distributions of \\(\\widehat{\\beta}_{1}^{*}\\) and \\(\\widehat{\\mu}^{*}\\)"
  },
  {
    "objectID": "chpt10-resample-method.html#bootstrap-variance-and-standard-errors",
    "href": "chpt10-resample-method.html#bootstrap-variance-and-standard-errors",
    "title": "10  Resampling Methods",
    "section": "10.7 Bootstrap Variance and Standard Errors",
    "text": "10.7 Bootstrap Variance and Standard Errors\nGiven the bootstrap draws we can estimate features of the bootstrap distribution. The bootstrap estimator of variance of an estimator \\(\\widehat{\\theta}\\) is the sample variance across the bootstrap draws \\(\\widehat{\\theta}^{*}(b)\\). It equals\n\\[\n\\begin{aligned}\n\\widehat{\\boldsymbol{V}}_{\\widehat{\\theta}}^{\\text {boot }} &=\\frac{1}{B-1} \\sum_{b=1}^{B}\\left(\\widehat{\\theta}^{*}(b)-\\bar{\\theta}^{*}\\right)\\left(\\widehat{\\theta}^{*}(b)-\\bar{\\theta}^{*}\\right)^{\\prime} \\\\\n\\bar{\\theta}^{*} &=\\frac{1}{B} \\sum_{b=1}^{B} \\widehat{\\theta}^{*}(b)\n\\end{aligned}\n\\]\nFor a scalar estimator \\(\\hat{\\theta}\\) the bootstrap standard error is the square root of the bootstrap estimator of variance:\n\\[\ns_{\\widehat{\\widehat{\\theta}}}^{\\text {boot }}=\\sqrt{\\widehat{\\boldsymbol{V}}_{\\widehat{\\theta}}^{\\text {boot }}} .\n\\]\nThis is a very simple statistic to calculate and is the most common use of the bootstrap in applied econometric practice. A caveat (discussed in more detail in Section 10.15) is that in many cases it is better to use a trimmed estimator.\nStandard errors are conventionally reported to convey the precision of the estimator. They are also commonly used to construct confidence intervals. Bootstrap standard errors can be used for this purpose. The normal-approximation bootstrap confidence interval is\n\\[\nC^{\\mathrm{nb}}=\\left[\\widehat{\\theta}-z_{1-\\alpha / 2} s_{\\widehat{\\theta}}^{\\text {boot }}, \\quad \\widehat{\\theta}+z_{1-\\alpha / 2} s_{\\widehat{\\theta}}^{\\text {boot }}\\right]\n\\]\nwhere \\(z_{1-\\alpha / 2}\\) is the \\(1-\\alpha / 2\\) quantile of the \\(\\mathrm{N}(0,1)\\) distribution. This interval \\(C^{\\mathrm{nb}}\\) is identical in format to an asymptotic confidence interval, but with the bootstrap standard error replacing the asymptotic standard error. \\(C^{\\mathrm{nb}}\\) is the default confidence interval reported by Stata when the bootstrap has been used to calculate standard errors. However, the normal-approximation interval is in general a poor choice for confidence interval construction as it relies on the normal approximation to the t-ratio which can be inaccurate in finite samples. There are other methods - such as the bias-corrected percentile method to be discussed in Section \\(10.17\\) - which are just as simple to compute but have better performance. In general, bootstrap standard errors should be used as estimates of precision rather than as tools to construct confidence intervals.\nSince \\(B\\) is finite, all bootstrap statistics, such as \\(\\widehat{\\boldsymbol{V}}_{\\widehat{\\theta}}^{\\text {boot }}\\), are estimates and hence random. Their values will vary across different choices for \\(B\\) and simulation runs (depending on how the simulation seed is set). Thus you should not expect to obtain the exact same bootstrap standard errors as other researchers when replicating their results. They should be similar (up to simulation sampling error) but not precisely the same.\nIn Table \\(10.2\\) we report the four parameter estimates introduced in Section \\(10.2\\) along with asymptotic, jackknife and bootstrap standard errors. We also report four bootstrap confidence intervals which will be introduced in subsequent sections.\nFor these four estimators we can see that the bootstrap standard errors are quite similar to the asymptotic and jackknife standard errors. The most noticable difference arises for \\(\\widehat{\\beta}_{2}\\), where the bootstrap standard error is about \\(10 %\\) larger than the asymptotic standard error.\nTable 10.2: Comparison of Methods\n\n\n\n\n\n\n\n\n\n\n\n\\(\\widehat{\\beta}_{1}\\)\n\\(\\widehat{\\beta}_{2}\\)\n\\(\\widehat{\\sigma}^{2}\\)\n\\(\\widehat{\\mu}\\)\n\n\n\n\nEstimate\n\\(0.155\\)\n\\(0.698\\)\n\\(0.144\\)\n\\(25.80\\)\n\n\nAsymptotic s.e.\n\\((0.031)\\)\n\\((0.493)\\)\n\\((0.043)\\)\n\\((2.29)\\)\n\n\nJackknife s.e.\n\\((0.032)\\)\n\\((0.514)\\)\n\\((0.046)\\)\n\\((2.39)\\)\n\n\nBootstrap s.e.\n\\((0.034)\\)\n\\((0.548)\\)\n\\((0.041)\\)\n\\((2.38)\\)\n\n\n\\(95 %\\) Percentile Interval\n\\([0.08,0.21]\\)\n\\([-0.27,1.91]\\)\n\\([0.06,0.22]\\)\n\\([21.4,30.7]\\)\n\n\n\\(95 %\\) BC Percentile Interval\n\\([0.08,0.21]\\)\n\\([-0.25,1.93]\\)\n\\([0.09,0.28]\\)\n\\([22.0,31.5]\\)\n\n\n\\(95 %\\) BC\n\n\n\n\n\n\n\nIn Stata, bootstrap standard errors for coefficient estimates in many models are obtained by the vce(bootstrap, reps(#)) option, where # is the number of bootstrap replications. For nonlinear functions of the coefficients or other estimators the bootstrap command can be combined with any other command to obtain bootstrap standard errors. Synonyms for bootstrap are bstrap and bs.\nTo illustrate, below we list the Stata commands which will calculate \\({ }^{1}\\) the bootstrap standard errors listed above.\n\\({ }^{1}\\) They will not precisely replicate the standard errors since those in Table \\(10.2\\) were produced in Matlab which uses a different random number sequence.\nStata Commands reg wage education if \\(\\operatorname{mbf} 12==1\\), vce(bootstrap, reps \\((10000))\\)\nbs (e(rss)/e(N)), reps(10000): reg wage education if \\(\\mathrm{mbf} 12==1\\)\nbs ( \\(\\exp \\left(16^{*}\\right.\\) bb[education]+_b[_cons] \\(\\left.\\left.+\\mathrm{e}(\\mathrm{rss}) / \\mathrm{e}(\\mathrm{N}) / 2\\right)\\right)\\), reps(10000): ///\nreg wage education if \\(\\operatorname{mbf} 12==1\\)"
  },
  {
    "objectID": "chpt10-resample-method.html#percentile-interval",
    "href": "chpt10-resample-method.html#percentile-interval",
    "title": "10  Resampling Methods",
    "section": "10.8 Percentile Interval",
    "text": "10.8 Percentile Interval\nThe second most common use of bootstrap methods is for confidence intervals. There are multiple bootstrap methods to form confidence intervals. A popular and simple method is called the percentile interval. It is based on the quantiles of the bootstrap distribution.\nIn Section \\(10.6\\) we described the bootstrap algorithm which creates an i.i.d. sample of bootstrap estimates \\(\\left\\{\\widehat{\\theta}_{1}^{*}, \\widehat{\\theta}_{2}^{*}, \\ldots, \\widehat{\\theta}_{B}^{*}\\right\\}\\) corresponding to an estimator \\(\\widehat{\\theta}\\) of a parameter \\(\\theta\\). We focus on the case of a scalar parameter \\(\\theta\\).\nFor any \\(0<\\alpha<1\\) we can calculate the empirical quantile \\(q_{\\alpha}^{*}\\) of these bootstrap estimates. This is the number such that \\(n \\alpha\\) bootstrap estimates are smaller than \\(q_{\\alpha}^{*}\\), and is typically calculated by taking the \\(n \\alpha^{t h}\\) order statistic of the \\(\\widehat{\\theta}_{b}^{*}\\). See Section \\(11.13\\) of Probability and Statistics for Economists for a precise discussion of empirical quantiles and common quantile estimators.\nThe percentile bootstrap \\(100(1-\\alpha) %\\) confidence interval is\n\\[\nC^{\\mathrm{pc}}=\\left[q_{\\alpha / 2}^{*}, q_{1-\\alpha / 2}^{*}\\right] .\n\\]\nFor example, if \\(B=1000, \\alpha=0.05\\), and the empirical quantile estimator is used, then \\(C^{\\mathrm{pc}}=\\left[\\widehat{\\theta}_{(25)}^{*}, \\widehat{\\theta}_{(975)}^{*}\\right]\\).\nTo illustrate, the \\(0.025\\) and \\(0.975\\) quantiles of the bootstrap distributions of \\(\\widehat{\\beta}_{1}^{*}\\) and \\(\\widehat{\\mu}^{*}\\) are indicated in Figure \\(13.1\\) by the arrows. The intervals between the arrows are the \\(95 %\\) percentile intervals.\nThe percentile interval has the convenience that it does not require calculation of a standard error. This is particularly convenient in contexts where asymptotic standard error calculation is complicated, burdensome, or unknown. \\(C^{\\mathrm{pc}}\\) is a simple by-product of the bootstrap algorithm and does not require meaningful computational cost above that required to calculate the bootstrap standard error.\nThe percentile interval has the useful property that it is transformation-respecting. Take a monotone parameter transformation \\(m(\\theta)\\). The percentile interval for \\(m(\\theta)\\) is simply the percentile interval for \\(\\theta\\) mapped by \\(m(\\theta)\\). That is, if \\(\\left[q_{\\alpha / 2}^{*}, q_{1-\\alpha / 2}^{*}\\right]\\) is the percentile interval for \\(\\theta\\), then \\(\\left[m\\left(q_{\\alpha / 2}^{*}\\right), m\\left(q_{1-\\alpha / 2}^{*}\\right)\\right]\\) is the percentile interval for \\(m(\\theta)\\). This property follows directly from the equivariance property of sample quantiles. Many confidence-interval methods, such as the delta-method asymptotic interval and the normal-approximation interval \\(C^{\\mathrm{nb}}\\), do not share this property.\nTo illustrate the usefulness of the transformation-respecting property consider the variance \\(\\sigma^{2}\\). In some cases it is useful to report the variance \\(\\sigma^{2}\\) and in other cases it is useful to report the standard deviation \\(\\sigma\\). Thus we may be interested in confidence intervals for \\(\\sigma^{2}\\) or \\(\\sigma\\). To illustrate, the asymptotic \\(95 %\\) normal confidence interval for \\(\\sigma^{2}\\) which we calculate from Table \\(13.2\\) is \\([0.060,0.228]\\). Taking square roots we obtain an interval for \\(\\sigma\\) of [0.244,0.477]. Alternatively, the delta method standard error for \\(\\widehat{\\sigma}=0.379\\) is \\(0.057\\), leading to an asymptotic \\(95 %\\) confidence interval for \\(\\sigma\\) of \\([0.265,0.493]\\) which is different. This shows that the delta method is not transformation-respecting. In contrast, the \\(95 %\\) percentile interval for \\(\\sigma^{2}\\) is \\([0.062,0.220]\\) and that for \\(\\sigma\\) is \\([0.249,0.469]\\) which is identical to the square roots of the interval for \\(\\sigma^{2}\\).\nThe bootstrap percentile intervals for the four estimators are reported in Table 13.2. In Stata, percentile confidence intervals can be obtained by using the command estat bootstrap, percentile or the command estat bootstrap, all after an estimation command which calculates standard errors via the bootstrap."
  },
  {
    "objectID": "chpt10-resample-method.html#the-bootstrap-distribution",
    "href": "chpt10-resample-method.html#the-bootstrap-distribution",
    "title": "10  Resampling Methods",
    "section": "10.9 The Bootstrap Distribution",
    "text": "10.9 The Bootstrap Distribution\nFor applications it is often sufficient if one understands the bootstrap as an algorithm. However, for theory it is more useful to view the bootstrap as a specific estimator of the sampling distribution. For this it is useful to introduce some additional notation.\nThe key is that the distribution of any estimator or statistic is determined by the distribution of the data. While the latter is unknown it can be estimated by the empirical distribution of the data. This is what the bootstrap does.\nTo fix notation, let \\(F\\) denote the distribution of an individual observation \\(W\\). (In regression, \\(W\\) is the \\(\\operatorname{pair}(Y, X)\\).) Let \\(G_{n}(u, F)\\) denote the distribution of an estimator \\(\\widehat{\\theta}\\). That is,\n\\[\nG_{n}(u, F)=\\mathbb{P}[\\widehat{\\theta} \\leq u \\mid F] .\n\\]\nWe write the distribution \\(G_{n}\\) as a function of \\(n\\) and \\(F\\) since the latter (generally) affect the distribution of \\(\\widehat{\\theta}\\). We are interested in the distribution \\(G_{n}\\). For example, we want to know its variance to calculate a standard error or its quantiles to calculate a percentile interval.\nIn principle, if we knew the distribution \\(F\\) we should be able to determine the distribution \\(G_{n}\\). In practice there are two barriers to implementation. The first barrier is that the calculation of \\(G_{n}(u, F)\\) is generally infeasible except in certain special cases such as the normal regression model. The second barrier is that in general we do not know \\(F\\).\nThe bootstrap simultaneously circumvents these two barriers by two clever ideas. First, the bootstrap proposes estimation of \\(F\\) by the empirical distribution function (EDF) \\(F_{n}\\), which is the simplest nonparametric estimator of the joint distribution of the observations. The EDF is \\(F_{n}(w)=n^{-1} \\sum_{i=1}^{n} \\mathbb{1}\\left\\{W_{i} \\leq w\\right\\}\\). (See Section \\(11.2\\) of Probability and Statistics for Economists for details and properties.) Replacing \\(F\\) with \\(F_{n}\\) we obtain the idealized bootstrap estimator of the distribution of \\(\\widehat{\\theta}\\)\n\\[\nG_{n}^{*}(u)=G_{n}\\left(u, F_{n}\\right) .\n\\]\nThe bootstrap’s second clever idea is to estimate \\(G_{n}^{*}\\) by simulation. This is the bootstrap algorithm described in the previous sections. The essential idea is that simulation from \\(F_{n}\\) is sampling with replacement from the original data, which is computationally simple. Applying the estimation formula for \\(\\hat{\\theta}\\) we obtain i.i.d. draws from the distribution \\(G_{n}^{*}(u)\\). By making a large number \\(B\\) of such draws we can estimate any feature of \\(G_{n}^{*}\\) of interest. The bootstrap combines these two ideas: (1) estimate \\(G_{n}(u, F)\\) by \\(G_{n}\\left(u, F_{n}\\right)\\); (2) estimate \\(G_{n}\\left(u, F_{n}\\right)\\) by simulation. These ideas are intertwined. Only by considering these steps together do we obtain a feasible method.\nThe way to think about the connection between \\(G_{n}\\) and \\(G_{n}^{*}\\) is as follows. \\(G_{n}\\) is the distribution of the estimator \\(\\widehat{\\theta}\\) obtained when the observations are sampled i.i.d. from the population distribution \\(F\\). \\(G_{n}^{*}\\) is the distribution of the same statistic, denoted \\(\\widehat{\\theta}^{*}\\), obtained when the observations are sampled i.i.d. from the empirical distribution \\(F_{n}\\). It is useful to conceptualize the “universe” which separately generates the dataset and the bootstrap sample. The “sampling universe” is the population distribution \\(F\\). In this universe the true parameter is \\(\\theta\\). The “bootstrap universe” is the empircal distribution \\(F_{n}\\). When drawing from the bootstrap universe we are treating \\(F_{n}\\) as if it is the true distribution. Thus anything which is true about \\(F_{n}\\) should be treated as true in the bootstrap universe. In the bootstrap universe the “true” value of the parameter \\(\\theta\\) is the value determined by the EDF \\(F_{n}\\). In most cases this is the estimate \\(\\widehat{\\theta}\\). It is the true value of the coefficient when the true distribution is \\(F_{n}\\). We now carefully explain the connection with the bootstrap algorithm as previously described.\nFirst, observe that sampling with replacement from the sample \\(\\left\\{Y_{1}, \\ldots, Y_{n}\\right\\}\\) is identical to sampling from the EDF \\(F_{n}\\). This is because the EDF is the probability distribution which puts probability mass \\(1 / n\\) on each observation. Thus sampling from \\(F_{n}\\) means sampling an observation with probability \\(1 / n\\), which is sampling with replacement.\nSecond, observe that the bootstrap estimator \\(\\widehat{\\theta}^{*}\\) described here is identical to the bootstrap algorithm described in Section 10.6. That is, \\(\\widehat{\\theta}^{*}\\) is the random vector generated by applying the estimator formula \\(\\widehat{\\theta}\\) to samples obtained by random sampling from \\(F_{n}\\).\nThird, observe that the distribution of these bootstrap estimators is the bootstrap distribution (10.9). This is a precise equality. That is, the bootstrap algorithm generates i.i.d. samples from \\(F_{n}\\), and when the estimators are applied we obtain random variables \\(\\widehat{\\theta}^{*}\\) with the distribution \\(G_{n}^{*}\\).\nFourth, observe that the bootstrap statistics described earlier - bootstrap variance, standard error, and quantiles - are estimators of the corresponding features of the bootstrap distribution \\(G_{n}^{*}\\).\nThis discussion is meant to carefully describe why the notation \\(G_{n}^{*}(u)\\) is useful to help understand the properties of the bootstrap algorithm. Since \\(F_{n}\\) is the natural nonparametric estimator of the unknown distribution \\(F, G_{n}^{*}(u)=G_{n}\\left(u, F_{n}\\right)\\) is the natural plug-in estimator of the unknown \\(G_{n}(u, F)\\). Furthermore, because \\(F_{n}\\) is uniformly consistent for \\(F\\) by the Glivenko-Cantelli Lemma (Theorem \\(18.8\\) in Probability and Statistics for Economists) we also can expect \\(G_{n}^{*}(u)\\) to be consistent for \\(G_{n}(u)\\). Making this precise is a bit challenging since \\(F_{n}\\) and \\(G_{n}\\) are functions. In the next several sections we develop an asymptotic distribution theory for the bootstrap distribution based on extending asymptotic theory to the case of conditional distributions."
  },
  {
    "objectID": "chpt10-resample-method.html#the-distribution-of-the-bootstrap-observations",
    "href": "chpt10-resample-method.html#the-distribution-of-the-bootstrap-observations",
    "title": "10  Resampling Methods",
    "section": "10.10 The Distribution of the Bootstrap Observations",
    "text": "10.10 The Distribution of the Bootstrap Observations\nLet \\(Y^{*}\\) be a random draw from the sample \\(\\left\\{Y_{1}, \\ldots, Y_{n}\\right\\}\\). What is the distribution of \\(Y^{*}\\) ?\nSince we are fixing the observations, the correct question is: What is the conditional distribution of \\(Y^{*}\\), conditional on the observed data? The empirical distribution function \\(F_{n}\\) summarizes the information in the sample, so equivalently we are talking about the distribution conditional on \\(F_{n}\\). Consequently we will write the bootstrap probability function and expectation as\n\\[\n\\begin{aligned}\n\\mathbb{P}^{*}\\left[Y^{*} \\leq x\\right] &=\\mathbb{P}\\left[Y^{*} \\leq x \\mid F_{n}\\right] \\\\\n\\mathbb{E}^{*}\\left[Y^{*}\\right] &=\\mathbb{E}\\left[Y^{*} \\mid F_{n}\\right] .\n\\end{aligned}\n\\]\nNotationally, the starred distribution and expectation are conditional given the data.\nThe (conditional) distribution of \\(Y^{*}\\) is the empirical distribution function \\(F_{n}\\), which is a discrete distribution with mass points \\(1 / n\\) on each observation \\(Y_{i}\\). Thus even if the original data come from a continuous distribution, the bootstrap data distribution is discrete.\nThe (conditional) mean and variance of \\(Y^{*}\\) are calculated from the EDF, and equal the sample mean and variance of the data. The mean is\n\\[\n\\mathbb{E}^{*}\\left[Y^{*}\\right]=\\sum_{i=1}^{n} Y_{i} \\mathbb{P}^{*}\\left[Y^{*}=Y_{i}\\right]=\\sum_{i=1}^{n} Y_{i} \\frac{1}{n}=\\bar{Y}\n\\]\nand the variance is\n\\[\n\\begin{aligned}\n\\operatorname{var}^{*}\\left[Y^{*}\\right] &=\\mathbb{E}^{*}\\left[Y^{*} Y^{* \\prime}\\right]-\\left(\\mathbb{E}^{*}\\left[Y^{*}\\right]\\right)\\left(\\mathbb{E}^{*}\\left[Y^{*}\\right]\\right)^{\\prime} \\\\\n&=\\sum_{i=1}^{n} Y_{i} Y_{i}^{\\prime} \\mathbb{P}^{*}\\left[Y^{*}=Y_{i}\\right]-\\bar{Y} \\bar{Y}^{\\prime} \\\\\n&=\\sum_{i=1}^{n} Y_{i} Y_{i}^{\\prime} \\frac{1}{n}-\\bar{Y} \\bar{Y}^{\\prime} \\\\\n&=\\widehat{\\Sigma}\n\\end{aligned}\n\\]\nTo summarize, the conditional distribution of \\(Y^{*}\\), given \\(F_{n}\\), is the discrete distribution on \\(\\left\\{Y_{1}, \\ldots, Y_{n}\\right\\}\\) with mean \\(\\bar{Y}\\) and covariance matrix \\(\\widehat{\\Sigma}\\).\nWe can extend this analysis to any integer moment \\(r\\). Assume \\(Y\\) is scalar. The \\(r^{t h}\\) moment of \\(Y^{*}\\) is\n\\[\n\\mu_{r}^{* \\prime}=\\mathbb{E}^{*}\\left[Y^{* r}\\right]=\\sum_{i=1}^{n} Y_{i}^{r} \\mathbb{P}^{*}\\left[Y^{*}=Y_{i}\\right]=\\frac{1}{n} \\sum_{i=1}^{n} Y_{i}^{r}=\\widehat{\\mu}_{r}^{\\prime},\n\\]\nthe \\(r^{t h}\\) sample moment. The \\(r^{t h}\\) central moment of \\(Y^{*}\\) is\n\\[\n\\mu_{r}^{*}=\\mathbb{E}^{*}\\left[\\left(Y^{*}-\\bar{Y}\\right)^{r}\\right]=\\frac{1}{n} \\sum_{i=1}^{n}\\left(Y_{i}-\\bar{Y}\\right)^{r}=\\widehat{\\mu}_{r},\n\\]\nthe \\(r^{t h}\\) central sample moment. Similarly, the \\(r^{t h}\\) cumulant of \\(Y^{*}\\) is \\(\\kappa_{r}^{*}=\\widehat{\\kappa}_{r}\\), the \\(r^{t h}\\) sample cumulant."
  },
  {
    "objectID": "chpt10-resample-method.html#the-distribution-of-the-bootstrap-sample-mean",
    "href": "chpt10-resample-method.html#the-distribution-of-the-bootstrap-sample-mean",
    "title": "10  Resampling Methods",
    "section": "10.11 The Distribution of the Bootstrap Sample Mean",
    "text": "10.11 The Distribution of the Bootstrap Sample Mean\nThe bootstrap sample mean is\n\\[\n\\bar{Y}^{*}=\\frac{1}{n} \\sum_{i=1}^{n} Y_{i}^{*} .\n\\]\nWe can calculate its (conditional) mean and variance. The mean is\n\\[\n\\mathbb{E}^{*}\\left[\\bar{Y}^{*}\\right]=\\mathbb{E}^{*}\\left[\\frac{1}{n} \\sum_{i=1}^{n} Y_{i}^{*}\\right]=\\frac{1}{n} \\sum_{i=1}^{n} \\mathbb{E}^{*}\\left[Y_{i}^{*}\\right]=\\frac{1}{n} \\sum_{i=1}^{n} \\bar{Y}=\\bar{Y}\n\\]\nusing (10.10). Thus the bootstrap sample mean \\(\\bar{Y}^{*}\\) has a distribution centered at the sample mean \\(\\bar{Y}\\). This is because the bootstrap observations \\(Y_{i}^{*}\\) are drawn from the bootstrap universe, which treats the EDF as the truth, and the mean of the latter distribution is \\(\\bar{Y}\\).\nThe (conditional) variance of the bootstrap sample mean is\n\\[\n\\operatorname{var}^{*}\\left[\\bar{Y}^{*}\\right]=\\operatorname{var}^{*}\\left[\\frac{1}{n} \\sum_{i=1}^{n} Y_{i}^{*}\\right]=\\frac{1}{n^{2}} \\sum_{i=1}^{n} \\operatorname{var}^{*}\\left[Y_{i}^{*}\\right]=\\frac{1}{n^{2}} \\sum_{i=1}^{n} \\widehat{\\Sigma}=\\frac{1}{n} \\widehat{\\Sigma}\n\\]\nusing (10.11). In the scalar case, \\(\\operatorname{var}^{*}\\left[\\bar{Y}^{*}\\right]=\\widehat{\\sigma}^{2} / n\\). This shows that the bootstrap variance of \\(\\bar{Y}^{*}\\) is precisely described by the sample variance of the original observations. Again, this is because the bootstrap observations \\(Y_{i}^{*}\\) are drawn from the bootstrap universe.\nWe can extend this to any integer moment \\(r\\). Assume \\(Y\\) is scalar. Define the normalized bootstrap sample mean \\(Z_{n}^{*}=\\sqrt{n}\\left(\\bar{Y}^{*}-\\bar{Y}\\right)\\). Using expressions from Section \\(6.17\\) of Probability and Statistics for Economists, the \\(3^{r d}\\) through \\(6^{\\text {th }}\\) conditional moments of \\(Z_{n}^{*}\\) are\n\\[\n\\begin{aligned}\n&\\mathbb{E}^{*}\\left[Z_{n}^{* 3}\\right]=\\widehat{\\kappa}_{3} / n^{1 / 2} \\\\\n&\\mathbb{E}^{*}\\left[Z_{n}^{* 4}\\right]=\\widehat{\\kappa}_{4} / n+3 \\widehat{\\kappa}_{2}^{2} \\\\\n&\\mathbb{E}^{*}\\left[Z_{n}^{* 5}\\right]=\\widehat{\\kappa}_{5} / n^{3 / 2}+10 \\widehat{\\kappa}_{3} \\widehat{\\kappa}_{2} / n^{1 / 2} \\\\\n&\\mathbb{E}^{*}\\left[Z_{n}^{* 6}\\right]=\\widehat{\\kappa}_{6} / n^{2}+\\left(15 \\widehat{\\kappa}_{4} \\kappa_{2}+10 \\widehat{\\kappa}_{3}^{2}\\right) / n+15 \\widehat{\\kappa}_{2}^{3}\n\\end{aligned}\n\\]\nwhere \\(\\widehat{\\kappa}_{r}\\) is the \\(r^{t h}\\) sample cumulant. Similar expressions can be derived for higher moments. The moments (10.14) are exact, not approximations."
  },
  {
    "objectID": "chpt10-resample-method.html#bootstrap-asymptotics",
    "href": "chpt10-resample-method.html#bootstrap-asymptotics",
    "title": "10  Resampling Methods",
    "section": "10.12 Bootstrap Asymptotics",
    "text": "10.12 Bootstrap Asymptotics\nThe bootstrap mean \\(\\bar{Y}^{*}\\) is a sample average over \\(n\\) i.i.d. random variables, so we might expect it to converge in probability to its expectation. Indeed, this is the case, but we have to be a bit careful since the bootstrap mean has a conditional distribution (given the data) so we need to define convergence in probability for conditional distributions.\nDefinition \\(10.1\\) We say that a random vector \\(Z_{n}^{*}\\) converges in bootstrap probability to \\(Z\\) as \\(n \\rightarrow \\infty\\), denoted \\(Z_{n}^{*} \\underset{p^{*}}{\\longrightarrow} Z\\), if for all \\(\\epsilon>0\\)\n\\[\n\\mathbb{P}^{*}\\left[\\left\\|Z_{n}^{*}-Z\\right\\|>\\epsilon\\right] \\underset{p}{\\longrightarrow} 0\n\\]\nTo understand this definition recall that conventional convergence in probability \\(Z_{n} \\underset{p}{\\longrightarrow}\\) means that for a sufficiently large sample size \\(n\\), the probability is high that \\(Z_{n}\\) is arbitrarily close to its limit \\(Z\\). In contrast, Definition \\(10.1\\) says \\(Z_{n}^{*} \\underset{p^{*}}{ } Z\\) means that for a sufficiently large \\(n\\), the probability is high that the conditional probability that \\(Z_{n}^{*}\\) is close to its limit \\(Z\\) is high. Note that there are two uses of probability - both unconditional and conditional.\nOur label “convergence in bootstrap probability” is a bit unusual. The label used in much of the statistical literature is “convergence in probability, in probability” but that seems like a mouthful. That literature more often focuses on the related concept of “convergence in probability, almost surely” which holds if we replace the ” \\(\\underset{p}{\\text { \" }}\\) convergence with almost sure convergence. We do not use this concept in this chapter as it is an unnecessary complication.\nWhile we have stated Definition \\(10.1\\) for the specific conditional probability distribution \\(\\mathbb{P}^{*}\\), the idea is more general and can be used for any conditional distribution and any sequence of random vectors.\nThe following may seem obvious but it is useful to state for clarity. Its proof is given in Section \\(10.31 .\\)\nTheorem \\(10.1\\) If \\(Z_{n} \\underset{p}{\\longrightarrow} Z\\) as \\(n \\rightarrow \\infty\\) then \\(Z_{n} \\underset{p^{*}}{ } Z\\).\nGiven Definition 10.1, we can establish a law of large numbers for the bootstrap sample mean. Theorem \\(10.2\\) Bootstrap WLLN. If \\(Y_{i}\\) are independent and uniformly integrable then \\(\\bar{Y}^{*}-\\bar{Y} \\underset{p^{*}}{\\longrightarrow} 0\\) and \\(\\bar{Y}^{*} \\underset{p^{*}}{\\longrightarrow} \\mu=\\mathbb{E}[Y]\\) as \\(n \\rightarrow \\infty\\).\nThe proof (presented in Section 10.31) is somewhat different from the classical case as it is based on the Marcinkiewicz WLLN (Theorem 10.20, presented in Section 10.31).\nNotice that the conditions for the bootstrap WLLN are the same for the conventional WLLN. Notice as well that we state two related but slightly different results. The first is that the difference between the bootstrap sample mean \\(\\bar{Y}^{*}\\) and the sample mean \\(\\bar{Y}\\) diminishes as the sample size diverges. The second result is that the bootstrap sample mean converges to the population mean \\(\\mu\\). The latter is not surprising (since the sample mean \\(\\bar{Y}\\) converges in probability to \\(\\mu\\) ) but it is constructive to be precise since we are dealing with a new convergence concept.\nTheorem 10.3 Bootstrap Continuous Mapping Theorem. If \\(Z_{n}^{*} \\underset{p^{*}}{ } c\\) as \\(n \\rightarrow\\) \\(\\infty\\) and \\(g(\\cdot)\\) is continuous at \\(c\\), then \\(g\\left(Z_{n}^{*}\\right) \\underset{p^{*}}{ } g(c)\\) as \\(n \\rightarrow \\infty\\).\nThe proof is essentially identical to that of Theorem \\(6.6\\) so is omitted.\nWe next would like to show that the bootstrap sample mean is asymptotically normally distributed, but for that we need a definition of convergence for conditional distributions.\nDefinition \\(10.2\\) Let \\(Z_{n}^{*}\\) be a sequence of random vectors with conditional distributions \\(G_{n}^{*}(x)=\\mathbb{P}^{*}\\left[Z_{n}^{*} \\leq x\\right]\\). We say that \\(Z_{n}^{*}\\) converges in bootstrap distribution to \\(Z\\) as \\(n \\rightarrow \\infty\\), denoted \\(Z_{n}^{*} \\underset{d^{*}}{\\longrightarrow}\\), if for all \\(x\\) at which \\(G(x)=\\mathbb{P}[Z \\leq x]\\) is continuous, \\(G_{n}^{*}(x) \\underset{p}{\\longrightarrow} G(x)\\) as \\(n \\rightarrow \\infty\\).\nThe difference with the conventional definition is that Definition \\(10.2\\) treats the conditional distribution as random. An alternative label for Definition \\(10.2\\) is “convergence in distribution, in probability”.\nWe now state a CLT for the bootstrap sample mean, with a proof given in Section 10.31.\nTheorem 10.4 Bootstrap CLT. If \\(Y_{i}\\) are i.i.d., \\(\\mathbb{E}\\|Y\\|^{2}<\\infty\\), and \\(\\Sigma=\\operatorname{var}[Y]>0\\), then as \\(n \\rightarrow \\infty, \\sqrt{n}\\left(\\bar{Y}^{*}-\\bar{Y}\\right) \\underset{d^{*}}{\\longrightarrow} \\mathrm{N}(0, \\Sigma)\\).\nTheorem \\(10.4\\) shows that the normalized bootstrap sample mean has the same asymptotic distribution as the sample mean. Thus the bootstrap distribution is asymptotically the same as the sampling distribution. A notable difference, however, is that the bootstrap sample mean is normalized by centering at the sample mean, not at the population mean. This is because \\(\\bar{Y}\\) is the true mean in the bootstrap universe.\nWe next state the distributional form of the continuous mapping theorem for bootstrap distributions and the Bootstrap Delta Method. Theorem 10.5 Bootstrap Continuous Mapping Theorem\nIf \\(Z_{n}^{*} \\underset{d^{*}}{ } Z\\) as \\(n \\rightarrow \\infty\\) and \\(g: \\mathbb{R}^{m} \\rightarrow \\mathbb{R}^{k}\\) has the set of discontinuity points \\(D_{g}\\) such that \\(\\mathbb{P}^{*}\\left[Z^{*} \\in D_{g}\\right]=0\\), then \\(g\\left(Z_{n}^{*}\\right) \\underset{d^{*}}{\\rightarrow} g(Z)\\) as \\(n \\rightarrow \\infty\\).\nTheorem 10.6 Bootstrap Delta Method: If \\(\\widehat{\\mu} \\underset{p}{\\longrightarrow} \\mu, \\sqrt{n}\\left(\\widehat{\\mu}^{*}-\\widehat{\\mu}\\right) \\underset{d^{*}}{\\longrightarrow} \\xi\\), and \\(g(u)\\) is continuously differentiable in a neighborhood of \\(\\mu\\), then as \\(n \\rightarrow \\infty\\)\n\\[\n\\sqrt{n}\\left(g\\left(\\widehat{\\mu}^{*}\\right)-g(\\widehat{\\mu})\\right) \\underset{d^{*}}{\\longrightarrow} \\boldsymbol{G}^{\\prime} \\xi\n\\]\nwhere \\(\\boldsymbol{G}(x)=\\frac{\\partial}{\\partial x} g(x)^{\\prime}\\) and \\(\\boldsymbol{G}=\\boldsymbol{G}(\\mu)\\). In particular, if \\(\\xi \\sim \\mathrm{N}(0, \\boldsymbol{V})\\) then as \\(n \\rightarrow \\infty\\)\n\\[\n\\sqrt{n}\\left(g\\left(\\widehat{\\mu}^{*}\\right)-g(\\widehat{\\mu})\\right) \\underset{d^{*}}{\\longrightarrow} \\mathrm{N}\\left(0, \\boldsymbol{G}^{\\prime} \\boldsymbol{V} \\boldsymbol{G}\\right) .\n\\]\nFor a proof, see Exercise 10.7.\nWe state an analog of Theorem 6.10, which presented the asymptotic distribution for general smooth functions of sample means, which covers most econometric estimators.\nTheorem 10.7 Under the assumptions of Theorem 6.10, that is, if \\(Y_{i}\\) is i.i.d., \\(\\mu=\\mathbb{E}[h(Y)], \\theta=g(\\mu), \\mathbb{E}\\|h(Y)\\|^{2}<\\infty\\), and \\(\\boldsymbol{G}(x)=\\frac{\\partial}{\\partial x} g(x)^{\\prime}\\) is continuous in a neighborhood of \\(\\mu\\), for \\(\\widehat{\\theta}=g(\\widehat{\\mu})\\) with \\(\\widehat{\\mu}=\\frac{1}{n} \\sum_{i=1}^{n} h\\left(Y_{i}\\right)\\) and \\(\\widehat{\\theta}^{*}=g\\left(\\widehat{\\mu}^{*}\\right)\\) with \\(\\widehat{\\mu}^{*}=\\frac{1}{n} \\sum_{i=1}^{n} h\\left(Y_{i}^{*}\\right)\\), as \\(n \\rightarrow \\infty\\)\n\\[\n\\sqrt{n}\\left(\\widehat{\\theta}^{*}-\\widehat{\\theta}\\right) \\underset{d^{*}}{\\longrightarrow} \\mathrm{N}\\left(0, \\boldsymbol{V}_{\\theta}\\right)\n\\]\nwhere \\(\\boldsymbol{V}_{\\theta}=\\boldsymbol{G}^{\\prime} \\boldsymbol{V} \\boldsymbol{G}, \\boldsymbol{V}=\\mathbb{E}\\left[(h(Y)-\\mu)(h(Y)-\\mu)^{\\prime}\\right]\\) and \\(\\boldsymbol{G}=\\boldsymbol{G}(\\mu)\\).\nFor a proof, see Exercise 10.8.\nTheorem \\(10.7\\) shows that the asymptotic distribution of the bootstrap estimator \\(\\widehat{\\theta}^{*}\\) is identical to that of the sample estimator \\(\\widehat{\\theta}\\). This means that we can learn the distribution of \\(\\widehat{\\theta}\\) from the bootstrap distribution, and hence perform asymptotically correct inference.\nFor some bootstrap applications we use bootstrap estimates of variance. The plug-in estimator of \\(\\boldsymbol{V}_{\\boldsymbol{\\theta}}\\) is \\(\\widehat{\\boldsymbol{V}}_{\\theta}=\\widehat{\\boldsymbol{G}}^{\\prime} \\widehat{\\boldsymbol{V}} \\widehat{\\boldsymbol{G}}\\) where \\(\\widehat{\\boldsymbol{G}}=\\boldsymbol{G}(\\widehat{\\mu})\\) and\n\\[\n\\widehat{\\boldsymbol{V}}=\\frac{1}{n} \\sum_{i=1}^{n}\\left(h\\left(Y_{i}\\right)-\\widehat{\\mu}\\right)\\left(h\\left(Y_{i}\\right)-\\widehat{\\mu}\\right)^{\\prime} .\n\\]\nThe bootstrap version is\n\\[\n\\begin{aligned}\n&\\widehat{\\boldsymbol{V}}_{\\theta}^{*}=\\widehat{\\boldsymbol{G}}^{* \\prime} \\widehat{\\boldsymbol{V}}^{*} \\widehat{\\boldsymbol{G}}^{*} \\\\\n&\\widehat{\\boldsymbol{G}}^{*}=\\boldsymbol{G}\\left(\\widehat{\\mu}^{*}\\right) \\\\\n&\\widehat{\\boldsymbol{V}}^{*}=\\frac{1}{n} \\sum_{i=1}^{n}\\left(h\\left(Y_{i}^{*}\\right)-\\widehat{\\mu}^{*}\\right)\\left(h\\left(Y_{i}^{*}\\right)-\\widehat{\\mu}^{*}\\right)^{\\prime} .\n\\end{aligned}\n\\]\nApplication of the bootstrap WLLN and bootstrap CMT show that \\(\\widehat{\\boldsymbol{V}}_{\\theta}^{*}\\) is consistent for \\(\\boldsymbol{V}_{\\theta}\\).\nTheorem \\(10.8\\) Under the assumptions of Theorem 10.7, \\(\\widehat{\\boldsymbol{V}}_{\\theta}^{*} \\underset{p^{*}}{\\longrightarrow} \\boldsymbol{V}_{\\theta}\\) as \\(n \\rightarrow \\infty\\).\nFor a proof, see Exercise 10.9."
  },
  {
    "objectID": "chpt10-resample-method.html#consistency-of-the-bootstrap-estimate-of-variance",
    "href": "chpt10-resample-method.html#consistency-of-the-bootstrap-estimate-of-variance",
    "title": "10  Resampling Methods",
    "section": "10.13 Consistency of the Bootstrap Estimate of Variance",
    "text": "10.13 Consistency of the Bootstrap Estimate of Variance\nRecall the definition (10.7) of the bootstrap estimator of variance \\(\\widehat{\\boldsymbol{V}}_{\\widehat{\\theta}}^{\\text {boot }}\\) of an estimator \\(\\widehat{\\theta}\\). In this section we explore conditions under which \\(\\widehat{\\boldsymbol{V}}_{\\widehat{\\theta}}^{\\text {boot }}\\) is consistent for the asymptotic variance of \\(\\widehat{\\theta}\\).\nTo do so it is useful to focus on a normalized version of the estimator so that the asymptotic variance is not degenerate. Suppose that for some sequence \\(a_{n}\\) we have\n\\[\nZ_{n}=a_{n}(\\widehat{\\theta}-\\theta) \\underset{d}{\\longrightarrow} \\xi\n\\]\nand\n\\[\nZ_{n}^{*}=a_{n}\\left(\\widehat{\\theta}^{*}-\\widehat{\\theta}\\right) \\underset{d^{*}}{\\longrightarrow} \\xi\n\\]\nfor some limit distribution \\(\\xi\\). That is, for some normalization, both \\(\\hat{\\theta}\\) and \\(\\widehat{\\theta}^{*}\\) have the same asymptotic distribution. This is quite general as it includes the smooth function model. The conventional bootstrap estimator of the variance of \\(Z_{n}\\) is the sample variance of the bootstrap draws \\(\\left\\{Z_{n}^{*}(b): b=1, \\ldots, B\\right\\}\\). This equals the estimator (10.7) multiplied by \\(a_{n}^{2}\\). Thus it is equivalent (up to scale) whether we discuss estimating the variance of \\(\\widehat{\\theta}\\) or \\(Z_{n}\\).\nThe bootstrap estimator of variance of \\(Z_{n}\\) is\n\\[\n\\begin{aligned}\n\\widehat{\\boldsymbol{V}}_{\\theta}^{\\text {boot,B }} &=\\frac{1}{B-1} \\sum_{b=1}^{B}\\left(Z_{n}^{*}(b)-Z_{n}^{*}\\right)\\left(Z_{n}^{*}(b)-Z_{n}^{*}\\right)^{\\prime} \\\\\n\\bar{Z}_{n}^{*} &=\\frac{1}{B} \\sum_{b=1}^{B} Z_{n}^{*}(b)\n\\end{aligned}\n\\]\nNotice that we index the estimator by the number of bootstrap replications \\(B\\).\nSince \\(Z_{n}^{*}\\) converges in bootstrap distribution to the same asymptotic distribution as \\(Z_{n}\\), it seems reasonable to guess that the variance of \\(Z_{n}^{*}\\) will converge to that of \\(\\xi\\). However, convergence in distribution is not sufficient for convergence in moments. For the variance to converge it is also necessary for the sequence \\(Z_{n}^{*}\\) to be uniformly square integrable. Theorem \\(10.9\\) If (10.15) and (10.16) hold for some sequence \\(a_{n}\\) and \\(\\left\\|Z_{n}^{*}\\right\\|^{2}\\) is uniformly integrable, then as \\(B \\rightarrow \\infty\\)\n\\[\n\\widehat{\\boldsymbol{V}}_{\\theta}^{\\mathrm{boot}, \\mathrm{B}} \\underset{p^{*}}{\\longrightarrow} \\widehat{\\boldsymbol{V}}_{\\theta}^{\\text {boot }}=\\operatorname{var}\\left[Z_{n}^{*}\\right] \\text {, }\n\\]\nand as \\(n \\rightarrow \\infty\\)\n\\[\n\\widehat{\\boldsymbol{V}}_{\\theta}^{\\text {boot }} \\underset{p^{*}}{\\longrightarrow} \\boldsymbol{V}_{\\theta}=\\operatorname{var}[\\xi] .\n\\]\nThis raises the question: Is the normalized sequence \\(Z_{n}\\) uniformly integrable? We spend the remainder of this section exploring this question and turn in the next section to trimmed variance estimators which do not require uniform integrability.\nThis condition is reasonably straightforward to verify for the case of a scalar sample mean with a finite variance. That is, suppose \\(Z_{n}^{*}=\\sqrt{n}\\left(\\bar{Y}^{*}-\\bar{Y}\\right)\\) and \\(\\mathbb{E}\\left[Y^{2}\\right]<\\infty\\). In (10.14) we calculated the exact fourth central moment of \\(Z_{n}^{*}\\) :\n\\[\n\\mathbb{E}^{*}\\left[Z_{n}^{* 4}\\right]=\\frac{\\widehat{\\kappa}_{4}}{n}+3 \\widehat{\\sigma}^{4}=\\frac{\\widehat{\\mu}_{4}-3 \\widehat{\\sigma}^{4}}{n}+3 \\widehat{\\sigma}^{4}\n\\]\nwhere \\(\\widehat{\\sigma}^{2}=n^{-1} \\sum_{i=1}^{n}\\left(Y_{i}-\\bar{Y}\\right)^{2}\\) and \\(\\widehat{\\mu}_{4}=n^{-1} \\sum_{i=1}^{n}\\left(Y_{i}-\\bar{Y}\\right)^{4}\\). The assumption \\(\\mathbb{E}\\left[Y^{2}\\right]<\\infty\\) implies that \\(\\mathbb{E}\\left[\\widehat{\\sigma}^{2}\\right]=O(1)\\) so \\(\\widehat{\\sigma}^{2}=O_{p}(1)\\). Furthermore, \\(n^{-1} \\widehat{\\mu}_{4}=n^{-2} \\sum_{i=1}^{n}\\left(Y_{i}-\\bar{Y}\\right)^{4}=o_{p}(1)\\) by the Marcinkiewicz WLLN (Theorem 10.20). It follows that\n\\[\n\\mathbb{E}^{*}\\left[Z_{n}^{* 4}\\right]=n^{2} \\mathbb{E}^{*}\\left[\\left(\\bar{Y}^{*}-\\bar{Y}\\right)^{4}\\right]=O_{p}(1) .\n\\]\nTheorem \\(6.13\\) shows that this implies that \\(Z_{n}^{* 2}\\) is uniformly integrable. Thus if \\(Y\\) has a finite variance the normalized bootstrap sample mean is uniformly square integrable and the bootstrap estimate of variance is consistent by Theorem \\(10.9\\).\nNow consider the smooth function model of Theorem 10.7. We can establish the following result.\nTheorem 10.10 In the smooth function model of Theorem 10.7, if for some \\(p \\geq 1\\) the \\(p^{t h}\\)-order derivatives of \\(g(x)\\) are bounded, then \\(Z_{n}^{*}=\\sqrt{n}\\left(\\widehat{\\theta}^{*}-\\widehat{\\theta}\\right)\\) is uniformly square integrable and the bootstrap estimator of variance is consistent as in Theorem 10.9.\nFor a proof see Section \\(10.31\\).\nThis shows that the bootstrap estimate of variance is consistent for a reasonably broad class of estimators. The class of functions \\(g(x)\\) covered by this result includes all \\(p^{t h}\\)-order polynomials."
  },
  {
    "objectID": "chpt10-resample-method.html#trimmed-estimator-of-bootstrap-variance",
    "href": "chpt10-resample-method.html#trimmed-estimator-of-bootstrap-variance",
    "title": "10  Resampling Methods",
    "section": "10.14 Trimmed Estimator of Bootstrap Variance",
    "text": "10.14 Trimmed Estimator of Bootstrap Variance\nTheorem \\(10.10\\) showed that the bootstrap estimator of variance is consistent for smooth functions with a bounded \\(p^{t h}\\) order derivative. This is a fairly broad class but excludes many important applications. An example is \\(\\theta=\\mu_{1} / \\mu_{2}\\) where \\(\\mu_{1}=\\mathbb{E}\\left[Y_{1}\\right]\\) and \\(\\mu_{2}=\\mathbb{E}\\left[Y_{2}\\right]\\). This function does not have a bounded derivative (unless \\(\\mu_{2}\\) is bounded away from zero) so is not covered by Theorem 10.10. This is more than a technical issue. When \\(\\left(Y_{1}, Y_{2}\\right)\\) are jointly normally distributed then it is known that \\(\\widehat{\\theta}=\\bar{Y}_{1} / \\bar{Y}_{2}\\) does not possess a finite variance. Consequently we cannot expect the bootstrap estimator of variance to perform well. (It is attempting to estimate the variance of \\(\\widehat{\\theta}\\), which is infinity.)\nIn these cases it is preferred to use a trimmed estimator of bootstrap variance. Let \\(\\tau_{n} \\rightarrow \\infty\\) be a sequence of positive trimming numbers satisfying \\(\\tau_{n}=O\\left(e^{n / 8}\\right)\\). Define the trimmed statistic\n\\[\nZ_{n}^{* *}=Z_{n}^{*} \\mathbb{1}\\left\\{\\left\\|Z_{n}^{*}\\right\\| \\leq \\tau_{n}\\right\\} .\n\\]\nThe trimmed bootstrap estimator of variance is\n\\[\n\\begin{aligned}\n\\widehat{\\boldsymbol{V}}_{\\theta}^{\\text {boot, }, \\tau} &=\\frac{1}{B-1} \\sum_{b=1}^{B}\\left(Z_{n}^{* *}(b)-Z_{n}^{* *}\\right)\\left(Z_{n}^{* *}(b)-Z_{n}^{* *}\\right)^{\\prime} \\\\\nZ_{n}^{* *} &=\\frac{1}{B} \\sum_{b=1}^{B} Z_{n}^{* *}(b) .\n\\end{aligned}\n\\]\nWe first examine the behavior of \\(\\widehat{\\boldsymbol{V}}_{\\theta}^{\\text {boot, } \\mathrm{B}}\\) as the number of bootstrap replications \\(B\\) grows to infinity. It is a sample variance of independent bounded random vectors. Thus by the bootstrap WLLN (Theorem 10.2) \\(\\widehat{\\boldsymbol{V}}_{\\theta}^{\\mathrm{boot}, \\mathrm{B}, \\tau}\\) converges in bootstrap probability to the variance of \\(Z_{n}^{* *}\\).\n\nWe next examine the behavior of the bootstrap estimator \\(\\widehat{\\boldsymbol{V}}_{\\theta}^{\\text {boot, } \\tau}\\) as \\(n\\) grows to infinity. We focus on the smooth function model of Theorem 10.7, which showed that \\(Z_{n}^{*}=\\sqrt{n}\\left(\\widehat{\\theta}^{*}-\\widehat{\\theta}\\right) \\underset{d^{*}}{\\longrightarrow} \\sim \\mathrm{N}\\left(0, \\boldsymbol{V}_{\\theta}\\right)\\). Since the trimming is asymptotically negligible, it follows that \\(Z_{n}^{* *} \\underset{d^{*}}{\\longrightarrow}\\). If we can show that \\(Z_{n}^{* *}\\) is uniformly square integrable, Theorem \\(10.9\\) shows that \\(\\operatorname{var}\\left[Z_{n}^{* *}\\right] \\rightarrow \\operatorname{var}[Z]=\\boldsymbol{V}_{\\theta}\\) as \\(n \\rightarrow \\infty\\). This is shown in the following result, whose proof is presented in Section 10.31.\nTheorem \\(10.12\\) Under the assumptions of Theorem 10.7, \\(\\widehat{\\boldsymbol{V}}_{\\theta}^{\\mathrm{boot}, \\tau} \\underset{p^{*}}{\\longrightarrow} \\boldsymbol{V}_{\\theta} .\\)\nTheorems \\(10.11\\) and \\(10.12\\) show that the trimmed bootstrap estimator of variance is consistent for the asymptotic variance in the smooth function model, which includes most econometric estimators. This justifies bootstrap standard errors as consistent estimators for the asymptotic distribution.\nAn important caveat is that these results critically rely on the trimmed variance estimator. This is a critical caveat as conventional statistical packages (e.g. Stata) calculate bootstrap standard errors using the untrimmed estimator (10.7). Thus there is no guarantee that the reported standard errors are consistent. The untrimmed variance estimator works in the context of Theorem \\(10.10\\) and whenever the bootstrap statistic is uniformly square integrable, but not necessarily in general applications.\nIn practice, it may be difficult to know how to select the trimming sequence \\(\\tau_{n}\\). The rule \\(\\tau_{n}=O\\left(e^{n / 8}\\right)\\) does not provide practical guidance. Instead, it may be useful to think about trimming in terms of percentages of the bootstrap draws. Thus we can set \\(\\tau_{n}\\) so that a given small percentage \\(\\gamma_{n}\\) is trimmed. For theoretical interpretation we would set \\(\\gamma_{n} \\rightarrow 0\\) as \\(n \\rightarrow \\infty\\). In practice we might set \\(\\gamma_{n}=1 %\\)."
  },
  {
    "objectID": "chpt10-resample-method.html#unreliability-of-untrimmed-bootstrap-standard-errors",
    "href": "chpt10-resample-method.html#unreliability-of-untrimmed-bootstrap-standard-errors",
    "title": "10  Resampling Methods",
    "section": "10.15 Unreliability of Untrimmed Bootstrap Standard Errors",
    "text": "10.15 Unreliability of Untrimmed Bootstrap Standard Errors\nIn the previous section we presented a trimmed bootstrap variance estimator which should be used to form bootstrap standard errors for nonlinear estimators. Otherwise, the untrimmed estimator is potentially unreliable.\nThis is an unfortunate situation, because reporting of bootstrap standard errors is commonplace in contemporary applied econometric practice, and standard applications (including Stata) use the untrimmed estimator.\nTo illustrate the seriousness of the problem we use the simple wage regression (7.31) which we repeat here. This is the subsample of married Black women with 982 observations. The point estimates and standard errors are\n\nWe are interested in the experience level which maximizes expected log wages \\(\\theta_{3}=-50 \\beta_{2} / \\beta_{3}\\). The point estimate and standard errors calculated with different methods are reported in Table \\(10.3\\) below.\nThe point estimate of the experience level with maximum earnings is \\(\\widehat{\\theta}_{3}=35\\). The asymptotic and jackknife standard errors are about 7 . The bootstrap standard error, however, is 825 ! Confused by this unusual value we rerun the bootstrap and obtain a standard error of 544 . Each was computed with 10,000 bootstrap replications. The fact that the two bootstrap standard errors are considerably different when recomputed (with different starting seeds) is indicative of moment failure. When there is an enormous discrepancy like this between the asymptotic and bootstrap standard error, and between bootstrap runs, it is a signal that there may be moment failure and consequently bootstrap standard errors are unreliable.\nA trimmed bootstrap with \\(\\tau=25\\) (set to slightly exceed three asymptotic standard errors) produces a more reasonable standard error of \\(10 .\\)\nOne message from this application is that when different methods produce very different standard errors we should be cautious about trusting any single method. The large discrepancies indicate poor asymptotic approximations, rendering all methods inaccurate. Another message is to be cautious about reporting conventional bootstrap standard errors. Trimmed versions are preferred, especially for nonlinear functions of estimated coefficients.\nTable 10.3: Experience Level Which Maximizes Expected log Wages\n\n\n\nEstimate\n\\(35.2\\)\n\n\n\n\nAsymptotic s.e.\n\\((7.0)\\)\n\n\nJackknife s.e.\n\\((7.0)\\)\n\n\nBootstrap s.e. (standard)\n\\((825)\\)\n\n\nBootstrap s.e. (repeat)\n\\((544)\\)\n\n\nBootstrap s.e. (trimmed)\n\\((10.1)\\)"
  },
  {
    "objectID": "chpt10-resample-method.html#consistency-of-the-percentile-interval",
    "href": "chpt10-resample-method.html#consistency-of-the-percentile-interval",
    "title": "10  Resampling Methods",
    "section": "10.16 Consistency of the Percentile Interval",
    "text": "10.16 Consistency of the Percentile Interval\nRecall the percentile interval (10.8). We now provide conditions under which it has asymptotically correct coverage. Theorem \\(10.13\\) Assume that for some sequence \\(a_{n}\\)\n\\[\na_{n}(\\widehat{\\theta}-\\theta) \\underset{d}{\\longrightarrow} \\xi\n\\]\nand\n\\[\na_{n}\\left(\\widehat{\\theta}^{*}-\\widehat{\\theta}\\right) \\underset{d^{*}}{\\longrightarrow} \\xi\n\\]\nwhere \\(\\xi\\) is continuously distributed and symmetric about zero. Then \\(\\mathbb{P}\\left[\\theta \\in C^{\\mathrm{pc}}\\right] \\rightarrow 1-\\alpha\\) as \\(n \\rightarrow \\infty\\)\nThe assumptions (10.18)-(10.19) hold for the smooth function model of Theorem 10.7, so this result incorporates many applications. The beauty of Theorem \\(10.13\\) is that the simple confidence interval \\(C^{\\mathrm{pc}}\\) - which does not require technical calculation of asymptotic standard errors - has asymptotically valid coverage for any estimator which falls in the smooth function class, as well as any other estimator satisfying the convergence results (10.18)-(10.19) with \\(\\xi\\) symmetrically distributed. The conditions are weaker than those required for consistent bootstrap variance estimation (and normal-approximation confidence intervals) because it is not necessary to verify that \\(\\widehat{\\theta}^{*}\\) is uniformly integrable, nor necessary to employ trimming.\nThe proof of Theorem \\(10.7\\) is not difficult. The convergence assumption (10.19) implies that the \\(\\alpha^{t h}\\) quantile of \\(a_{n}\\left(\\widehat{\\theta}^{*}-\\widehat{\\theta}\\right)\\), which is \\(a_{n}\\left(q_{\\alpha}^{*}-\\widehat{\\theta}\\right)\\) by quantile equivariance, converges in probability to the \\(\\alpha^{t h}\\) quantile of \\(\\xi\\), which we can denote as \\(\\bar{q}_{\\alpha}\\). Thus\n\\[\na_{n}\\left(q_{\\alpha}^{*}-\\widehat{\\theta}\\right) \\underset{p}{\\longrightarrow} \\bar{q}_{\\alpha} .\n\\]\nLet \\(H(x)=\\mathbb{P}[\\xi \\leq x]\\) be the distribution function of \\(\\xi\\). The assumption of symmetry implies \\(H(-x)=\\) \\(1-H(x)\\). Then the percentile interval has coverage\n\\[\n\\begin{aligned}\n\\mathbb{P}\\left[\\theta \\in C^{\\mathrm{pc}}\\right] &=\\mathbb{P}\\left[q_{\\alpha / 2}^{*} \\leq \\theta \\leq q_{1-\\alpha / 2}^{*}\\right] \\\\\n&=\\mathbb{P}\\left[-a_{n}\\left(q_{\\alpha / 2}^{*}-\\widehat{\\theta}\\right) \\geq a_{n}(\\widehat{\\theta}-\\theta) \\geq-a_{n}\\left(q_{1-\\alpha / 2}^{*}-\\widehat{\\theta}\\right)\\right] \\\\\n& \\rightarrow \\mathbb{P}\\left[-\\bar{q}_{\\alpha / 2} \\geq \\xi \\geq-\\bar{q}_{1-\\alpha / 2}\\right] \\\\\n&=H\\left(-\\bar{q}_{\\alpha / 2}\\right)-H\\left(-\\bar{q}_{1-\\alpha / 2}\\right) \\\\\n&=H\\left(\\bar{q}_{1-\\alpha / 2}\\right)-H\\left(\\bar{q}_{\\alpha / 2}\\right) \\\\\n&=1-\\alpha .\n\\end{aligned}\n\\]\nThe convergence holds by (10.18) and (10.20). The following equality uses the definition of \\(H\\), the nextto-last is the symmetry of \\(H\\), and the final equality is the definition of \\(\\bar{q}_{\\alpha}\\). This establishes Theorem \\(10.13 .\\)\nTheorem \\(10.13\\) seems quite general, but it critically rests on the assumption that the asymptotic distribution \\(\\xi\\) is symmetrically distributed about zero. This may seem innocuous since conventional asymptotic distributions are normal and hence symmetric, but it deserves further scrutiny. It is not merely a technical assumption - an examination of the steps in the preceeding argument isolate quite clearly that if the symmetry assumption is violated then the asymptotic coverage will not be \\(1-\\alpha\\). While Theorem \\(10.13\\) does show that the percentile interval is asymptotically valid for a conventional asymptotically normal estimator, the reliance on symmetry in the argument suggests that the percentile method will work poorly when the finite sample distribution is asymmetric. This turns out to be the case and leads us to consider alternative methods in the following sections. It is also worthwhile to investigate a finite sample justification for the percentile interval based on a heuristic analogy due to Efron.\nAssume that there exists an unknown but strictly increasing transformation \\(\\psi(\\theta)\\) such that \\(\\psi(\\widehat{\\theta})-\\) \\(\\psi(\\theta)\\) has a pivotal distribution \\(H(u)\\) (does not vary with \\(\\theta\\) ) which is symmetric about zero. For example, if \\(\\widehat{\\theta} \\sim \\mathrm{N}\\left(\\theta, \\sigma^{2}\\right)\\) we can set \\(\\psi(\\theta)=\\theta / \\sigma\\). Alternatively, if \\(\\widehat{\\theta}=\\exp (\\widehat{\\mu})\\) and \\(\\widehat{\\mu} \\sim \\mathrm{N}\\left(\\mu, \\sigma^{2}\\right)\\) then we can set \\(\\psi(\\theta)=\\) \\(\\log (\\theta) / \\sigma\\)\nTo assess the coverage of the percentile interval, observe that since the distribution \\(H\\) is pivotal the bootstrap distribution \\(\\psi\\left(\\widehat{\\theta}^{*}\\right)-\\psi(\\widehat{\\theta})\\) also has distribution \\(H(u)\\). Let \\(\\bar{q}_{\\alpha}\\) be the \\(\\alpha^{\\text {th }}\\) quantile of the distribution \\(H\\). Since \\(q_{\\alpha}^{*}\\) is the \\(\\alpha^{t h}\\) quantile of the distribution of \\(\\widehat{\\theta}^{*}\\) and \\(\\psi\\left(\\widehat{\\theta}^{*}\\right)-\\psi(\\widehat{\\theta})\\) is a monotonic transformation of \\(\\widehat{\\theta}^{*}\\), by the quantile equivariance property we deduce that \\(\\bar{q}_{\\alpha}+\\psi(\\widehat{\\theta})=\\psi\\left(q_{\\alpha}^{*}\\right)\\). The percentile interval has coverage\n\\[\n\\begin{aligned}\n\\mathbb{P}\\left[\\theta \\in C^{\\mathrm{pc}}\\right] &=\\mathbb{P}\\left[q_{\\alpha / 2}^{*} \\leq \\theta \\leq q_{1-\\alpha / 2}^{*}\\right] \\\\\n&=\\mathbb{P}\\left[\\psi\\left(q_{\\alpha / 2}^{*}\\right) \\leq \\psi(\\theta) \\leq \\psi\\left(q_{1-\\alpha / 2}^{*}\\right)\\right] \\\\\n&=\\mathbb{P}\\left[\\psi(\\widehat{\\theta})-\\psi\\left(q_{\\alpha / 2}^{*}\\right) \\geq \\psi(\\widehat{\\theta})-\\psi(\\theta) \\geq \\psi(\\widehat{\\theta})-\\psi\\left(q_{1-\\alpha / 2}^{*}\\right)\\right] \\\\\n&=\\mathbb{P}\\left[-\\bar{q}_{\\alpha / 2} \\geq \\psi(\\widehat{\\theta})-\\psi(\\theta) \\geq-\\bar{q}_{1-\\alpha / 2}\\right] \\\\\n&=H\\left(-\\bar{q}_{\\alpha / 2}\\right)-H\\left(-\\bar{q}_{1-\\alpha / 2}\\right) \\\\\n&=H\\left(\\bar{q}_{1-\\alpha / 2}\\right)-H\\left(\\bar{q}_{\\alpha / 2}\\right) \\\\\n&=1-\\alpha .\n\\end{aligned}\n\\]\nThe second equality applies the monotonic transformation \\(\\psi(u)\\) to all elements. The fourth uses the relationship \\(\\bar{q}_{\\alpha}+\\psi(\\widehat{\\theta})=\\psi\\left(q_{\\alpha}^{*}\\right)\\). The fifth uses the defintion of \\(H\\). The sixth uses the symmetry property of \\(H\\), and the final is by the definition of \\(\\bar{q}_{\\alpha}\\) as the \\(\\alpha^{t h}\\) quantile of \\(H\\).\nThis calculation shows that under these assumptions the percentile interval has exact coverage \\(1-\\alpha\\). The nice thing about this argument is the introduction of the unknown transformation \\(\\psi(u)\\) for which the percentile interval automatically adapts. The unpleasant feature is the assumption of symmetry. Similar to the asymptotic argument the calculation strongly relies on the symmetry of distribution \\(H(x)\\). Without symmetry the coverage will be incorrect.\nIntuitively, we expect that when the assumptions are approximately true then the percentile interval will have approximately correct coverage. Thus so long as there is a transformation \\(\\psi(u)\\) such that \\(\\psi(\\widehat{\\theta})-\\) \\(\\psi(\\theta)\\) is approximately pivotal and symmetric about zero, then the percentile interval should work well.\nThis argument has the following application. Suppose that the parameter of interest is \\(\\theta=\\exp (\\mu)\\) where \\(\\mu=\\mathbb{E}[Y]\\) and suppose \\(Y\\) has a pivotal symmetric distribution about \\(\\mu\\). Then even though \\(\\widehat{\\theta}=\\) \\(\\exp (\\bar{Y})\\) does not have a symmetric distribution, the percentile interval applied to \\(\\widehat{\\theta}\\) will have the correct coverage, because the monotonic transformation \\(\\log (\\widehat{\\theta})\\) has a pivotal symmetric distribution."
  },
  {
    "objectID": "chpt10-resample-method.html#bias-corrected-percentile-interval",
    "href": "chpt10-resample-method.html#bias-corrected-percentile-interval",
    "title": "10  Resampling Methods",
    "section": "10.17 Bias-Corrected Percentile Interval",
    "text": "10.17 Bias-Corrected Percentile Interval\nThe accuracy of the percentile interval depends critically upon the assumption that the sampling distribution is approximately symmetrically distributed. This excludes finite sample bias, for an estimator which is biased cannot be symmetrically distributed. Many contexts in which we want to apply bootstrap methods (rather than asymptotic) are when the parameter of interest is a nonlinear function of the model parameters, and nonlinearity typically induces estimation bias. Consequently it is difficult to expect the percentile method to generally have accurate coverage.\nTo reduce the bias problem Efron (1982) introduced the bias-corrected (BC) percentile interval. The justification is heuristic but there is considerable evidence that the bias-corrected method is an important improvement on the percentile interval. The construction is based on the assumption is that there is a an unknown but strictly increasing transformation \\(\\psi(\\theta)\\) and unknown constant \\(z_{0}\\) such that\n\\[\nZ=\\psi(\\widehat{\\theta})-\\psi(\\theta)+z_{0} \\sim \\mathrm{N}(0,1) .\n\\]\n(The assumption that \\(Z\\) is normal is not critical. It could be replaced by any known symmetric and invertible distribution.) Let \\(\\Phi(x)\\) denote the normal distribution function, \\(\\Phi^{-1}(p)\\) its quantile function, and \\(z_{\\alpha}=\\Phi^{-1}(\\alpha)\\) the normal critical values. Then the BC interval can be constructed from the bootstrap estimators \\(\\widehat{\\theta}_{b}^{*}\\) and bootstrap quantiles \\(q_{\\alpha}^{*}\\) as follows. Set\n\\[\np^{*}=\\frac{1}{B} \\sum_{b=1}^{B} \\mathbb{1}\\left\\{\\widehat{\\theta}_{b}^{*} \\leq \\widehat{\\theta}\\right\\}\n\\]\nand\n\\[\nz_{0}=\\Phi^{-1}\\left(p^{*}\\right) .\n\\]\n\\(p^{*}\\) is a measure of median bias, and \\(z_{0}\\) is \\(p^{*}\\) transformed into normal units. If the bias of \\(\\widehat{\\theta}\\) is zero then \\(p^{*}=0.5\\) and \\(z_{0}=0\\). If \\(\\widehat{\\theta}\\) is upwards biased then \\(p^{*}<0.5\\) and \\(z_{0}<0\\). Conversely if \\(\\widehat{\\theta}\\) is dowward biased then \\(p^{*}>0.5\\) and \\(z_{0}>0\\). Define for any \\(\\alpha\\) an adjusted version\n\\[\nx(\\alpha)=\\Phi\\left(z_{\\alpha}+2 z_{0}\\right) .\n\\]\nIf \\(z_{0}=0\\) then \\(x(\\alpha)=\\alpha\\). If \\(z_{0}>0\\) then \\(x(\\alpha)>\\alpha\\), and conversely when \\(x(\\alpha)<0\\). The BC interval is\n\\[\nC^{\\mathrm{bc}}=\\left[q_{x(\\alpha / 2)}^{*}, q_{x(1-\\alpha / 2)}^{*}\\right] .\n\\]\nEssentially, rather than going from the \\(2.5 %\\) to \\(97.5 %\\) quantile, the BC interval uses adjusted quantiles, with the degree of adjustment depending on the extent of the bias.\nThe construction of the BC interval is not intuitive. We now show that assumption (10.21) implies that the BC interval has exact coverage. (10.21) implies that\n\\[\n\\mathbb{P}\\left[\\psi(\\widehat{\\theta})-\\psi(\\theta)+z_{0} \\leq x\\right]=\\Phi(x) .\n\\]\nSince the distribution is pivotal the result carries over to the bootstrap distribution\n\\[\n\\mathbb{P}^{*}\\left[\\psi\\left(\\widehat{\\theta}^{*}\\right)-\\psi(\\widehat{\\theta})+z_{0} \\leq x\\right]=\\Phi(x) .\n\\]\nEvaluating (10.26) at \\(x=z_{0}\\) we find \\(\\mathbb{P}^{*}\\left[\\psi\\left(\\widehat{\\theta}^{*}\\right)-\\psi(\\widehat{\\theta}) \\leq 0\\right]=\\Phi\\left(z_{0}\\right)\\) which implies \\(\\mathbb{P}^{*}\\left[\\widehat{\\theta}^{*} \\leq \\widehat{\\theta}\\right]=\\Phi\\left(z_{0}\\right)\\). Inverting, we obtain\n\\[\nz_{0}=\\Phi^{-1}\\left(\\mathbb{P}^{*}\\left[\\widehat{\\theta}^{*} \\leq \\widehat{\\theta}\\right]\\right)\n\\]\nwhich is the probability limit of (10.23) as \\(B \\rightarrow \\infty\\). Thus the unknown \\(z_{0}\\) is recoved by (10.23), and we can treat \\(z_{0}\\) as if it were known.\nFrom (10.26) we deduce that\n\\[\n\\begin{aligned}\nx(\\alpha) &=\\Phi\\left(z_{\\alpha}+2 z_{0}\\right) \\\\\n&\\left.=\\mathbb{P}^{*}\\left[\\psi\\left(\\widehat{\\theta}^{*}\\right)-\\psi(\\widehat{\\theta}) \\leq z_{\\alpha}+z_{0}\\right)\\right] \\\\\n&=\\mathbb{P}^{*}\\left[\\widehat{\\theta}^{*} \\leq \\psi^{-1}\\left(\\psi(\\widehat{\\theta})+z_{0}+z_{\\alpha}\\right)\\right] .\n\\end{aligned}\n\\]\nThis equation shows that \\(\\psi^{-1}\\left(\\psi(\\widehat{\\theta})+z_{0}+z_{\\alpha}\\right)\\) equals the \\(x(\\alpha)^{t h}\\) bootstrap quantile. That is, \\(q_{x(\\alpha)}^{*}=\\) \\(\\psi^{-1}\\left(\\psi(\\widehat{\\theta})+z_{0}+z_{\\alpha}\\right)\\). Hence we can write (10.25) as\n\\[\nC^{\\mathrm{bc}}=\\left[\\psi^{-1}\\left(\\psi(\\widehat{\\theta})+z_{0}+z_{\\alpha / 2}\\right), \\psi^{-1}\\left(\\psi(\\widehat{\\theta})+z_{0}+z_{1-\\alpha / 2}\\right)\\right] .\n\\]\nIt has coverage probability\n\\[\n\\begin{aligned}\n\\mathbb{P}\\left[\\theta \\in C^{\\mathrm{bc}}\\right] &=\\mathbb{P}\\left[\\psi^{-1}\\left(\\psi(\\widehat{\\theta})+z_{0}+z_{\\alpha / 2}\\right) \\leq \\theta \\leq \\psi^{-1}\\left(\\psi(\\widehat{\\theta})+z_{0}+z_{1-\\alpha / 2}\\right)\\right] \\\\\n&=\\mathbb{P}\\left[\\psi(\\widehat{\\theta})+z_{0}+z_{\\alpha / 2} \\leq \\psi(\\theta) \\leq \\psi(\\widehat{\\theta})+z_{0}+z_{1-\\alpha / 2}\\right] \\\\\n&=\\mathbb{P}\\left[-z_{\\alpha / 2} \\geq \\psi(\\widehat{\\theta})-\\psi(\\theta)+z_{0} \\geq-z_{1-\\alpha / 2}\\right] \\\\\n&=\\mathbb{P}\\left[z_{1-\\alpha / 2} \\geq Z \\geq z_{\\alpha / 2}\\right] \\\\\n&=\\Phi\\left(z_{1-\\alpha / 2}\\right)-\\Phi\\left(z_{\\alpha / 2}\\right) \\\\\n&=1-\\alpha .\n\\end{aligned}\n\\]\nThe second equality applies the transformation \\(\\psi(\\theta)\\). The fourth equality uses the model (10.21) and the fact \\(z_{\\alpha}=-z_{1-\\alpha}\\). This shows that the BC interval (10.25) has exact coverage under the assumption (10.21).\nFurthermore, under the assumptions of Theorem 10.13, the \\(\\mathrm{BC}\\) interval has asymptotic coverage probability \\(1-\\alpha\\), since the bias correction is asymptotically negligible.\nAn important property of the BC percentile interval is that it is transformation-respecting (like the percentile interval). To see this, observe that \\(p^{*}\\) is invariant to transformations because it is a probability, and thus \\(z_{0}^{*}\\) and \\(x(\\alpha)\\) are invariant. Since the interval is constructed from the \\(x(\\alpha / 2)\\) and \\(x(1-\\alpha / 2)\\) quantiles, the quantile equivariance property shows that the interval is transformation-respecting.\nThe bootstrap BC percentile intervals for the four estimators are reported in Table 13.2. They are generally similar to the percentile intervals, though the intervals for \\(\\sigma^{2}\\) and \\(\\mu\\) are somewhat shifted to the right.\nIn Stata, BC percentile confidence intervals can be obtained by using the command estat bootstrap after an estimation command which calculates standard errors via the bootstrap."
  },
  {
    "objectID": "chpt10-resample-method.html#mathrmbc_a-percentile-interval",
    "href": "chpt10-resample-method.html#mathrmbc_a-percentile-interval",
    "title": "10  Resampling Methods",
    "section": "10.18 \\(\\mathrm{BC}_{a}\\) Percentile Interval",
    "text": "10.18 \\(\\mathrm{BC}_{a}\\) Percentile Interval\nA further improvement on the BC interval was made by Efron (1987) to account for the skewness in the sampling distribution, which can be modeled by specifying that the variance of the estimator depends on the parameter. The resulting bootstrap accelerated bias-corrected percentile interval \\(\\left(\\mathrm{BC}_{a}\\right)\\) has improved performance on the BC interval, but requires a bit more computation and is less intuitive to understand.\nThe construction is a generalization of that for the BC intervals. The assumption is that there is an unknown but strictly increasing transformation \\(\\psi(\\theta)\\) and unknown constants \\(a\\) and \\(z_{0}\\) such that\n\\[\nZ=\\frac{\\psi(\\widehat{\\theta})-\\psi(\\theta)}{1+a \\psi(\\theta)}+z_{0} \\sim \\mathrm{N}(0,1) .\n\\]\n(As before, the assumption that \\(Z\\) is normal could be replaced by any known symmetric and invertible distribution.)\nThe constant \\(z_{0}\\) is estimated by (10.23) just as for the BC interval. There are several possible estimators of \\(a\\). Efron’s suggestion is a scaled jackknife estimator of the skewness of \\(\\widehat{\\theta}\\) :\n\\[\n\\begin{aligned}\n&\\widehat{a}=\\frac{\\sum_{i=1}^{n}\\left(\\bar{\\theta}-\\widehat{\\theta}_{(-i)}\\right)^{3}}{6\\left(\\sum_{i=1}^{n}\\left(\\bar{\\theta}-\\widehat{\\theta}_{(-i)}\\right)^{2}\\right)^{3 / 2}} \\\\\n&\\bar{\\theta}=\\frac{1}{n} \\sum_{i=1}^{n} \\widehat{\\theta}_{(-i)} .\n\\end{aligned}\n\\]\nThe jackknife estimator of \\(\\widehat{a}\\) makes the \\(\\mathrm{BC}_{a}\\) interval more computationally costly than other intervals.\nDefine for any \\(\\alpha\\) the adjusted version\n\\[\nx(\\alpha)=\\Phi\\left(z_{0}+\\frac{z_{\\alpha}+z_{0}}{1-a\\left(z_{\\alpha}+z_{0}\\right)}\\right) .\n\\]\nThe \\(\\mathrm{BC}_{a}\\) percentile interval is\n\\[\nC^{\\mathrm{bca}}=\\left[q_{x(\\alpha / 2)}^{*}, q_{x(1-\\alpha / 2)}^{*}\\right] .\n\\]\nNote that \\(x(\\alpha)\\) simplifies to (10.24) and \\(C^{\\text {bca }}\\) simplies to \\(C^{\\text {bc }}\\) when \\(a=0\\). While \\(C^{\\text {bc }}\\) improves on \\(C^{\\text {pc }}\\) by correcting the median bias, \\(C^{\\text {bca }}\\) makes a further correction for skewness.\nThe \\(\\mathrm{BC}_{a}\\) interval is only well-defined for values of \\(\\alpha\\) such that \\(a\\left(z_{\\alpha}+z_{0}\\right)<1\\). (Or equivalently, if \\(\\alpha<\\Phi\\left(a^{-1}-z_{0}\\right)\\) for \\(a>0\\) and \\(\\alpha>\\Phi\\left(a^{-1}-z_{0}\\right)\\) for \\(a<0\\).)\nThe \\(\\mathrm{BC}_{a}\\) interval, like the \\(\\mathrm{BC}\\) and percentile intervals, is transformation-respecting. Thus if \\(\\left[q_{x(\\alpha / 2)}^{*}, q_{x(1-\\alpha / 2)}^{*}\\right]\\) is the \\(\\mathrm{BC}_{a}\\) interval for \\(\\theta\\), then \\(\\left[m\\left(q_{x(\\alpha / 2)}^{*}\\right), m\\left(q_{x(1-\\alpha / 2)}^{*}\\right)\\right]\\) is the \\(\\mathrm{BC}_{\\alpha}\\) interval for \\(\\phi=m(\\theta)\\) when \\(m(\\theta)\\) is monotone.\nWe now give a justification for the \\(\\mathrm{BC}_{a}\\) interval. The most difficult feature to understand is the estimator \\(\\widehat{a}\\) for \\(a\\). This involves higher-order approximations which are too advanced for our treatment, so we instead refer readers to Chapter \\(4.1 .4\\) of Shao and Tu (1995) and simply assume that \\(a\\) is known.\nWe now show that assumption (10.28) with \\(a\\) known implies that \\(C^{\\text {bca }}\\) has exact coverage. The argument is essentially the same as that given in the previous section. Assumption (10.28) implies that the bootstrap distribution satisfies\n\\[\n\\mathbb{P}^{*}\\left[\\frac{\\psi\\left(\\widehat{\\theta}^{*}\\right)-\\psi(\\widehat{\\theta})}{1+a \\psi(\\widehat{\\theta})}+z_{0} \\leq x\\right]=\\Phi(x) .\n\\]\nEvaluating at \\(x=z_{0}\\) and inverting we obtain (10.27) which is the same as for the BC interval. Thus the estimator (10.23) is consistent as \\(B \\rightarrow \\infty\\) and we can treat \\(z_{0}\\) as if it were known.\nFrom (10.29) we deduce that\n\\[\n\\begin{aligned}\nx(\\alpha) &=\\mathbb{P}^{*}\\left[\\frac{\\psi\\left(\\widehat{\\theta}^{*}\\right)-\\psi(\\widehat{\\theta})}{1+a \\psi(\\widehat{\\theta})} \\leq \\frac{z_{\\alpha}+z_{0}}{1-a\\left(z_{\\alpha}+z_{0}\\right)}\\right] \\\\\n&=\\mathbb{P}^{*}\\left[\\widehat{\\theta}^{*} \\leq \\psi^{-1}\\left(\\frac{\\psi(\\widehat{\\theta})+z_{\\alpha}+z_{0}}{1-a\\left(z_{\\alpha}+z_{0}\\right)}\\right)\\right] .\n\\end{aligned}\n\\]\nThis shows that \\(\\psi^{-1}\\left(\\frac{\\psi(\\widehat{\\theta})+z_{\\alpha}+z_{0}}{1-a\\left(z_{\\alpha}+z_{0}\\right)}\\right)\\) equals the \\(x(\\alpha)^{t h}\\) bootstrap quantile. Hence we can write \\(C^{\\text {bca }}\\) as\n\\[\nC^{\\mathrm{bca}}=\\left[\\psi^{-1}\\left(\\frac{\\psi(\\widehat{\\theta})+z_{\\alpha / 2}+z_{0}}{1-a\\left(z_{\\alpha / 2}+z_{0}\\right)}\\right), \\quad \\psi^{-1}\\left(\\frac{\\psi(\\widehat{\\theta})+z_{1-\\alpha / 2}+z_{0}}{1-a\\left(z_{1-\\alpha / 2}+z_{0}\\right)}\\right)\\right] .\n\\]\nIt has coverage probability\n\\[\n\\begin{aligned}\n\\mathbb{P}\\left[\\theta \\in C^{\\mathrm{bca}}\\right] &=\\mathbb{P}\\left[\\psi^{-1}\\left(\\frac{\\psi(\\widehat{\\theta})+z_{\\alpha / 2}+z_{0}}{1-a\\left(z_{\\alpha / 2}+z_{0}\\right)}\\right) \\leq \\theta \\leq \\psi^{-1}\\left(\\frac{\\psi(\\widehat{\\theta})+z_{1-\\alpha / 2}+z_{0}}{1-a\\left(z_{1-\\alpha / 2}+z_{0}\\right)}\\right)\\right] \\\\\n&=\\mathbb{P}\\left[\\frac{\\psi(\\widehat{\\theta})+z_{\\alpha / 2}+z_{0}}{1-a\\left(z_{\\alpha / 2}+z_{0}\\right)} \\leq \\psi(\\theta) \\leq \\frac{\\psi(\\widehat{\\theta})+z_{1-\\alpha / 2}+z_{0}}{1-a\\left(z_{1-\\alpha / 2}+z_{0}\\right)}\\right] \\\\\n&=\\mathbb{P}\\left[-z_{\\alpha / 2} \\geq \\frac{\\psi(\\widehat{\\theta})-\\psi(\\theta)}{1+a \\psi(\\theta)}+z_{0} \\geq-z_{1-\\alpha / 2}\\right] \\\\\n&=\\mathbb{P}\\left[z_{1-\\alpha / 2} \\geq Z \\geq z_{\\alpha / 2}\\right] \\\\\n&=1-\\alpha .\n\\end{aligned}\n\\]\nThe second equality applies the transformation \\(\\psi(\\theta)\\). The fourth equality uses the model (10.28) and the fact \\(z_{\\alpha}=-z_{1-\\alpha}\\). This shows that the \\(\\mathrm{BC}_{a}\\) interval \\(C^{\\text {bca }}\\) has exact coverage under the assumption (10.28) with \\(a\\) known.\nThe bootstrap \\(\\mathrm{BC}_{a}\\) percentile intervals for the four estimators are reported in Table 13.2. They are generally similar to the BC intervals, though the intervals for \\(\\sigma^{2}\\) and \\(\\mu\\) are slightly shifted to the right.\nIn Stata, \\(\\mathrm{BC}_{a}\\) intervals can be obtained by using the command estat bootstrap, bca or the command estat bootstrap, all after an estimation command which calculates standard errors via the bootstrap using the bca option."
  },
  {
    "objectID": "chpt10-resample-method.html#percentile-t-interval",
    "href": "chpt10-resample-method.html#percentile-t-interval",
    "title": "10  Resampling Methods",
    "section": "10.19 Percentile-t Interval",
    "text": "10.19 Percentile-t Interval\nIn many cases we can obtain improvement in accuracy by bootstrapping a studentized statistic such as a t-ratio. Let \\(\\widehat{\\theta}\\) be an estimator of a scalar parameter \\(\\theta\\) and \\(s(\\widehat{\\theta})\\) a standard error. The sample t-ratio is\n\\[\nT=\\frac{\\widehat{\\theta}-\\theta}{s(\\widehat{\\theta})} .\n\\]\nThe bootstrap t-ratio is\n\\[\nT^{*}=\\frac{\\widehat{\\theta}^{*}-\\widehat{\\theta}}{s\\left(\\widehat{\\theta}^{*}\\right)}\n\\]\nwhere \\(s\\left(\\widehat{\\theta}^{*}\\right)\\) is the standard error calculated on the bootstrap sample. Notice that the bootstrap t-ratio is centered at the parameter estimator \\(\\widehat{\\theta}\\). This is because \\(\\widehat{\\theta}\\) is the “true value” in the bootstrap universe.\nThe percentile-t interval is formed using the distribution of \\(T^{*}\\). This can be calculated via the bootstrap algorithm. On each bootstrap sample the estimator \\(\\widehat{\\theta}^{*}\\) and its standard error \\(s\\left(\\widehat{\\theta}^{*}\\right)\\) are calculated, and the t-ratio \\(T^{*}=\\left(\\widehat{\\theta}^{*}-\\widehat{\\theta}\\right) / s\\left(\\widehat{\\theta}^{*}\\right)\\) calculated and stored. This is repeated \\(B\\) times. The \\(\\alpha^{t h}\\) quantile \\(q_{\\alpha}^{*}\\) is estimated by the \\(\\alpha^{t h}\\) empirical quantile (or any quantile estimator) from the \\(B\\) bootstrap draws of \\(T^{*}\\).\nThe bootstrap percentile-t confidence interval is defined as\n\\[\nC^{\\mathrm{pt}}=\\left[\\widehat{\\theta}-s(\\widehat{\\theta}) q_{1-\\alpha / 2}^{*}, \\widehat{\\theta}-s(\\widehat{\\theta}) q_{\\alpha / 2}^{*}\\right] .\n\\]\nThe form may appear unusual when compared with the percentile interval. The left endpoint is determined by the upper quantile of the distribution of \\(T^{*}\\), and the right endpoint is determined by the lower quantile. As we show below, this construction is important for the interval to have correct coverage when the distribution is not symmetric.\nWhen the estimator is asymptotically normal and the standard error a reliable estimator of the standard deviation of the distribution we would expect the t-ratio \\(T\\) to be roughly approximated by the normal distribution. In this case we would expect \\(q_{0.975}^{*} \\approx-q_{0.025}^{*} \\approx 2\\). Departures from this baseline occur as the distribution becomes skewed or fat-tailed. If the bootstrap quantiles depart substantially from this baseline it is evidence of substantial departure from normality. (It may also indicate a programming error, so in these cases it is wise to triple-check!)\nThe percentile-t has the following advantages. First, when the standard error \\(s(\\widehat{\\theta})\\) is reasonably reliable, the percentile-t bootstrap makes use of the information in the standard error, thereby reducing the role of the bootstrap. This can improve the precision of the method relative to other methods. Second, as we show later, the percentile-t intervals achieve higher-order accuracy than the percentile and BC percentile intervals. Third, the percentile-t intervals correspond to the set of parameter values “not rejected” by one-sided t-tests using bootstrap critical values (bootstrap tests are presented in Section 10.21).\nThe percentile-t interval has the following disadvantages. First, they may be infeasible when standard error formula are unknown. Second, they may be practically infeasible when standard error calculations are computationally costly (since the standard error calculation needs to be performed on each bootstrap sample). Third, the percentile-t may be unreliable if the standard errors \\(s(\\widehat{\\theta})\\) are unreliable and thus add more noise than clarity. Fourth, the percentile-t interval is not translation preserving, unlike the percentile, \\(\\mathrm{BC}\\) percentile, and \\(\\mathrm{BC}_{a}\\) percentile intervals.\nIt is typical to calculate percentile-t intervals with t-ratios constructed with conventional asymptotic standard errors. But this is not the only possible implementation. The percentile-t interval can be constructed with any data-dependent measure of scale. For example, if \\(\\widehat{\\theta}\\) is a two-step estimator for which it is unclear how to construct a correct asymptotic standard error, but we know how to calculate a standard error \\(s(\\widehat{\\theta})\\) appropriate for the second step alone, then \\(s(\\widehat{\\theta})\\) can be used for a percentile-t-type interval as described above. It will not possess the higher-order accuracy properties of the following section, but it will satisfy the conditions for first-order validity.\nFurthermore, percentile-t intervals can be constructed using bootstrap standard errors. That is, the statistics \\(T\\) and \\(T^{*}\\) can be computed using bootstrap standard errors \\(s_{\\widehat{\\theta}}^{\\text {boot }}\\). This is computationally costly as it requires what we call a “nested bootstrap”. Specifically, for each bootstrap replication, a random sample is drawn, the bootstrap estimate \\(\\widehat{\\theta}^{*}\\) computed, and then \\(B\\) additional bootstrap sub-samples drawn from the bootstrap sample to compute the bootstrap standard error for the bootstrap estimate \\(\\widehat{\\theta}^{*}\\). Effectively \\(B^{2}\\) bootstrap samples are drawn and estimated, which increases the computational requirement by an order of magnitude.\nWe now describe the distribution theory for first-order validity of the percentile-t bootstrap.\nFirst, consider the smooth function model, where \\(\\widehat{\\theta}=g(\\widehat{\\mu})\\) and \\(s(\\widehat{\\theta})=\\sqrt{\\frac{1}{n} \\widehat{\\boldsymbol{G}}^{\\prime} \\widehat{\\boldsymbol{V}} \\widehat{\\boldsymbol{G}}}\\) with bootstrap analogs \\(\\widehat{\\theta}^{*}=g\\left(\\widehat{\\mu}^{*}\\right)\\) and \\(s\\left(\\widehat{\\theta}^{*}\\right)=\\sqrt{\\frac{1}{n} \\widehat{\\boldsymbol{G}}^{* \\prime} \\widehat{\\boldsymbol{V}}^{*} \\widehat{\\boldsymbol{G}}^{*}}\\). From Theorems \\(6.10,10.7\\), and \\(10.8\\)\n\\[\nT=\\frac{\\sqrt{n}(\\widehat{\\theta}-\\theta)}{\\sqrt{\\widehat{\\boldsymbol{G}}^{\\prime} \\widehat{\\boldsymbol{V}} \\widehat{\\boldsymbol{G}}}} \\underset{d}{\\longrightarrow}\n\\]\nand\n\\[\nT^{*}=\\frac{\\sqrt{n}\\left(\\widehat{\\theta}^{*}-\\widehat{\\theta}\\right)}{\\sqrt{\\widehat{\\boldsymbol{G}}^{* \\prime} \\widehat{\\boldsymbol{V}}^{*} \\widehat{\\boldsymbol{G}}^{*}}} \\overrightarrow{d^{*}} Z\n\\]\nwhere \\(Z \\sim \\mathrm{N}(0,1)\\). This shows that the sample and bootstrap t-ratios have the same asymptotic distribution.\nThis motivates considering the broader situation where the sample and bootstrap t-ratios have the same asymptotic distribution but not necessarily normal. Thus assume that\n\\[\n\\begin{gathered}\nT \\underset{d}{\\longrightarrow} \\xi \\\\\nT^{*} \\underset{d^{*}}{\\longrightarrow} \\xi\n\\end{gathered}\n\\]\nfor some continuous distribution \\(\\xi\\). (10.31) implies that the quantiles of \\(T^{*}\\) converge in probability to those of \\(\\xi\\), that is \\(q_{\\alpha}^{*} \\longrightarrow \\underset{p}{\\longrightarrow} q_{\\alpha}\\) where \\(q_{\\alpha}\\) is the \\(\\alpha^{t h}\\) quantile of \\(\\xi\\). This and (10.30) imply\n\\[\n\\begin{aligned}\n\\mathbb{P}\\left[\\theta \\in C^{\\mathrm{pt}}\\right] &=\\mathbb{P}\\left[\\widehat{\\theta}-s(\\widehat{\\theta}) q_{1-\\alpha / 2}^{*} \\leq \\theta \\leq \\hat{\\theta}-s(\\widehat{\\theta}) q_{\\alpha / 2}^{*}\\right] \\\\\n&=\\mathbb{P}\\left[q_{\\alpha / 2}^{*} \\leq T \\leq q_{1-\\alpha / 2}^{*}\\right] \\\\\n& \\rightarrow \\mathbb{P}\\left[q_{\\alpha / 2} \\leq \\xi \\leq q_{1-\\alpha / 2}\\right] \\\\\n&=1-\\alpha .\n\\end{aligned}\n\\]\nThus the percentile-t is asymptotically valid. Theorem \\(10.14\\) If (10.30) and (10.31) hold where \\(\\xi\\) is continuously distributed, then \\(\\mathbb{P}\\left[\\theta \\in C^{\\mathrm{pt}}\\right] \\rightarrow 1-\\alpha\\) as \\(n \\rightarrow \\infty\\)\nThe bootstrap percentile-t intervals for the four estimators are reported in Table 13.2. They are similar but somewhat different from the percentile-type intervals, and generally wider. The largest difference arises with the interval for \\(\\sigma^{2}\\) which is noticably wider than the other intervals."
  },
  {
    "objectID": "chpt10-resample-method.html#percentile-t-asymptotic-refinement",
    "href": "chpt10-resample-method.html#percentile-t-asymptotic-refinement",
    "title": "10  Resampling Methods",
    "section": "10.20 Percentile-t Asymptotic Refinement",
    "text": "10.20 Percentile-t Asymptotic Refinement\nThis section uses the theory of Edgeworth and Cornish-Fisher expansions introduced in Chapter 9.8-9.10 of Probability and Statistics for Economists. This theory will not be familiar to most students. If you are interested in the following refinement theory it is advisable to start by reading these sections of Probability and Statistics for Economists.\nThe percentile-t interval can be viewed as the intersection of two one-sided confidence intervals. In our discussion of Edgeworth expansions for the coverage probability of one-sided asymptotic confidence intervals (following Theorem \\(7.15\\) in the context of functions of regression coefficients) we found that one-sided asymptotic confidence intervals have accuracy to order \\(O\\left(n^{-1 / 2}\\right)\\). We now show that the percentile-t interval has improved accuracy.\nTheorem \\(9.13\\) of Probability and Statistics for Economists showed that the Cornish-Fisher expansion for the quantile \\(q_{\\alpha}\\) of a t-ratio \\(T\\) in the smooth function model takes the form\n\\[\nq_{\\alpha}=z_{\\alpha}+n^{-1 / 2} p_{11}\\left(z_{\\alpha}\\right)+O\\left(n^{-1}\\right)\n\\]\nwhere \\(p_{11}(x)\\) is an even polynomial of order 2 with coefficients depending on the moments up to order 8. The bootstrap quantile \\(q_{\\alpha}^{*}\\) has a similar Cornish-Fisher expansion\n\\[\nq_{\\alpha}^{*}=z_{\\alpha}+n^{-1 / 2} p_{11}^{*}\\left(z_{\\alpha}\\right)+O_{p}\\left(n^{-1}\\right)\n\\]\nwhere \\(p_{11}^{*}(x)\\) is the same as \\(p_{11}(x)\\) except that the population moments are replaced by the corresponding sample moments. Sample moments are estimated at the rate \\(n^{-1 / 2}\\). Thus we can replace \\(p_{11}^{*}\\) with \\(p_{11}\\) without affecting the order of this expansion:\n\\[\nq_{\\alpha}^{*}=z_{\\alpha}+n^{-1 / 2} p_{11}\\left(z_{\\alpha}\\right)+O_{p}\\left(n^{-1}\\right)=q_{\\alpha}+O_{p}\\left(n^{-1}\\right) .\n\\]\nThis shows that the bootstrap quantiles \\(q_{\\alpha}^{*}\\) of the studentized t-ratio are within \\(O_{p}\\left(n^{-1}\\right)\\) of the exact quantiles \\(q_{\\alpha}\\).\nBy the Edgeworth expansion Delta method (Theorem \\(9.12\\) of Probability and Statistics for Economists), \\(T\\) and \\(T+\\left(q_{\\alpha}-q_{\\alpha}^{*}\\right)=T+O_{p}\\left(n^{-1}\\right)\\) have the same Edgeworth expansion to order \\(O\\left(n^{-1}\\right)\\). Thus\n\\[\n\\begin{aligned}\n\\mathbb{P}\\left[T \\leq q_{\\alpha}^{*}\\right] &=\\mathbb{P}\\left[T+\\left(q_{\\alpha}-q_{\\alpha}^{*}\\right) \\leq q_{\\alpha}\\right] \\\\\n&=\\mathbb{P}\\left[T \\leq q_{\\alpha}\\right]+O\\left(n^{-1}\\right) \\\\\n&=\\alpha+O\\left(n^{-1}\\right) .\n\\end{aligned}\n\\]\nThus the coverage of the percentile-t interval is\n\\[\n\\begin{aligned}\n\\mathbb{P}\\left[\\theta \\in C^{\\mathrm{pt}}\\right] &=\\mathbb{P}\\left[q_{\\alpha / 2}^{*} \\leq T \\leq q_{1-\\alpha / 2}^{*}\\right] \\\\\n&=\\mathbb{P}\\left[q_{\\alpha / 2} \\leq T \\leq q_{1-\\alpha / 2}\\right]+O\\left(n^{-1}\\right) \\\\\n&=1-\\alpha+O\\left(n^{-1}\\right) .\n\\end{aligned}\n\\]\nThis is an improved rate of convergence relative to the one-sided asymptotic confidence interval. Theorem \\(10.15\\) Under the assumptions of Theorem \\(9.11\\) of Probability and Statistics for Economists, \\(\\mathbb{P}\\left[\\theta \\in C^{\\mathrm{pt}}\\right]=1-\\alpha+O\\left(n^{-1}\\right)\\).\nThe following definition of the accuracy of a confidence interval is useful.\nDefinition 10.3 A confidence set \\(C\\) for \\(\\theta\\) is \\(k^{t h}\\)-order accurate if\n\\[\n\\mathbb{P}[\\theta \\in C]=1-\\alpha+O\\left(n^{-k / 2}\\right) .\n\\]\nExamining our results we find that one-sided asymptotic confidence intervals are first-order accurate but percentile-t intervals are second-order accurate. When a bootstrap confidence interval (or test) achieves high-order accuracy than the analogous asymptotic interval (or test), we say that the bootstrap method achieves an asymptotic refinement. Here, we have shown that the percentile-t interval achieves an asymptotic refinement.\nIn order to achieve this asymptotic refinement it is important that the t-ratio \\(T\\) (and its bootstrap counter-part \\(T^{*}\\) ) are constructed with asymptotically valid standard errors. This is because the first term in the Edgeworth expansion is the standard normal distribution and this requires that the t-ratio is asymptotically normal. This also has the practical finite-sample implication that the accuracy of the percentile-t interval in practice depends on the accuracy of the standard errors used to construct the t-ratio.\nWe do not go through the details, but normal-approximation bootstrap intervals, percentile bootstrap intervals, and bias-corrected percentile bootstrap intervals are all first-order accurate and do not achieve an asymptotic refinement.\nThe \\(\\mathrm{BC}_{a}\\) interval, however, can be shown to be asymptotically equivalent to the percentile-t interval, and thus achieves an asymptotic refinement. We do not make this demonstration here as it is advanced. See Section 3.10.4 of Hall (1992).\n\n\n\n\n\n\nPeter Hall\n\n\n\n\nPeter Gavin Hall (1951-2016) of Australia was one of the most influential and\n\n\nprolific theoretical statisticians in history. He made wide-ranging contributions.\n\n\nSome of the most relevant for econometrics are theoretical investigations of\n\n\nbootstrap methods and nonparametric kernel methods."
  },
  {
    "objectID": "chpt10-resample-method.html#bootstrap-hypothesis-tests",
    "href": "chpt10-resample-method.html#bootstrap-hypothesis-tests",
    "title": "10  Resampling Methods",
    "section": "10.21 Bootstrap Hypothesis Tests",
    "text": "10.21 Bootstrap Hypothesis Tests\nTo test the hypothesis \\(\\mathbb{M}_{0}: \\theta=\\theta_{0}\\) against \\(\\mathbb{M}_{1}: \\theta \\neq \\theta_{0}\\) the most common approach is a t-test. We reject \\(\\mathbb{H}_{0}\\) in favor of \\(\\mathbb{H}_{1}\\) for large absolute values of the t-statistic \\(T=\\left(\\widehat{\\theta}-\\theta_{0}\\right) / s(\\hat{\\theta})\\) where \\(\\widehat{\\theta}\\) is an estimator of \\(\\theta\\) and \\(s(\\widehat{\\theta})\\) is a standard error for \\(\\widehat{\\theta}\\). For a bootstrap test we use the bootstrap algorithm to calculate the critical value. The bootstrap algorithm samples with replacement from the dataset. Given a bootstrap sample the bootstrap estimator \\(\\widehat{\\theta}^{*}\\) and standard error \\(s\\left(\\widehat{\\theta}^{*}\\right)\\) are calculated. Given these values the bootstrap \\(\\mathrm{t}\\)-statistic is \\(T^{*}=\\left(\\widehat{\\theta}^{*}-\\widehat{\\theta}\\right) / s\\left(\\widehat{\\theta}^{*}\\right)\\). There are two important features about the bootstrap t-statistic. First, \\(T^{*}\\) is centered at the sample estimate \\(\\widehat{\\theta}\\), not at the hypothesized value \\(\\theta_{0}\\). This is done because \\(\\widehat{\\theta}\\) is the true value in the bootstrap universe, and the distribution of the t-statistic must be centered at the true value within the bootstrap sampling framework. Second, \\(T^{*}\\) is calculated using the bootstrap standard error \\(s\\left(\\widehat{\\theta}^{*}\\right)\\). This allows the bootstrap to incorporate the randomness in standard error estimation.\nThe failure to properly center the bootstrap statistic at \\(\\widehat{\\theta}\\) is a common error in applications. Often this is because the hypothesis to be tested is \\(\\mathbb{H}_{0}: \\theta=0\\), so the test statistic is \\(T=\\widehat{\\theta} / s(\\widehat{\\theta})\\). This intuitively suggests the bootstrap statistic \\(T^{*}=\\widehat{\\theta}^{*} / s\\left(\\widehat{\\theta}^{*}\\right)\\), but this is wrong. The correct bootstrap statistic is \\(T^{*}=\\) \\(\\left(\\widehat{\\theta}^{*}-\\widehat{\\theta}\\right) / s\\left(\\widehat{\\theta}^{*}\\right)\\)\nThe bootstrap algorithm creates \\(B\\) draws \\(T^{*}(b)=\\left(\\widehat{\\theta}^{*}(b)-\\widehat{\\theta}\\right) / s\\left(\\widehat{\\theta}^{*}(b)\\right), b=1, \\ldots, B\\). The bootstrap \\(100 \\alpha %\\) critical value is \\(q_{1-\\alpha}^{*}\\), where \\(q_{\\alpha}^{*}\\) is the \\(\\alpha^{\\text {th }}\\) quantile of the absolute values of the bootstrap t-ratios \\(\\left|T^{*}(b)\\right|\\). For a \\(100 \\alpha %\\) test we reject \\(\\mathbb{M}_{0}: \\theta=\\theta_{0}\\) in favor of \\(\\mathbb{H}_{1}: \\theta \\neq \\theta_{0}\\) if \\(|T|>q_{1-\\alpha}^{*}\\) and fail to reject if \\(|T| \\leq q_{1-\\alpha}^{*}\\).\nIt is generally better to report p-values rather than critical values. Recall that a p-value is \\(p=1-\\) \\(G_{n}(|T|)\\) where \\(G_{n}(u)\\) is the null distribution of the statistic \\(|T|\\). The bootstrap p-value is defined as \\(p^{*}=1-G_{n}^{*}(|T|)\\), where \\(G_{n}^{*}(u)\\) is the bootstrap distribution of \\(\\left|T^{*}\\right|\\). This is estimated from the bootstrap algorithm as\n\\[\np^{*}=\\frac{1}{B} \\sum_{b=1}^{B} \\mathbb{1}\\left\\{\\left|T^{*}(b)\\right|>|T|\\right\\},\n\\]\nthe percentage of bootstrap t-statistics that are larger than the observed t-statistic. Intuitively, we want to know how “unusual” is the observed statistic \\(T\\) when the null hypothesis is true. The bootstrap algorithm generates a large number of independent draws from the distribution \\(T^{*}\\) (which is an approximation to the unknown distribution of \\(T\\) ). If the percentage of the \\(\\left|T^{*}\\right|\\) that exceed \\(|T|\\) is very small (say \\(1 %\\) ) this tells us that \\(|T|\\) is an unusually large value. However, if the percentage is larger, say \\(15 %\\), then we cannot interpret \\(|T|\\) as unusually large.\nIf desired, the bootstrap test can be implemented as a one-sided test. In this case the statistic is the signed version of the t-ratio, and bootstrap critical values are calculated from the upper tail of the distribution for the alternative \\(\\mathbb{H}_{1}: \\theta>\\theta_{0}\\), and from the lower tail for the alternative \\(\\mathbb{H}_{1}: \\theta<\\theta_{0}\\). There is a connection between the one-sided tests and the percentile-t confidence interval. The latter is the set of parameter values \\(\\theta\\) which are not rejected by either one-sided \\(100 \\alpha / 2 %\\) bootstrap t-test.\nBootstrap tests can also be conducted with other statistics. When standard errors are not available or are not reliable we can use the non-studentized statistic \\(T=\\widehat{\\theta}-\\theta_{0}\\). The bootstrap version is \\(T^{*}=\\widehat{\\theta}^{*}-\\widehat{\\theta}\\). Let \\(q_{\\alpha}^{*}\\) be the \\(\\alpha^{\\text {th }}\\) quantile of the bootstrap statistics \\(\\left|\\widehat{\\theta}^{*}(b)-\\widehat{\\theta}\\right|\\). A bootstrap \\(100 \\alpha %\\) test rejects \\(\\mathbb{H}_{0}: \\theta=\\theta_{0}\\) if \\(\\left|\\widehat{\\theta}-\\theta_{0}\\right|>q_{1-\\alpha}^{*}\\). The bootstrap p-value is\n\\[\np^{*}=\\frac{1}{B} \\sum_{b=1}^{B} \\mathbb{1}\\left\\{\\left|\\widehat{\\theta}^{*}(b)-\\widehat{\\theta}\\right|>\\left|\\widehat{\\theta}-\\theta_{0}\\right|\\right\\} .\n\\]\nTheorem \\(10.16\\) If (10.30) and (10.31) hold where \\(\\xi\\) is continuously distributed, then the bootstrap critical value satisfies \\(q_{1-\\alpha}^{*} \\underset{p}{\\longrightarrow} q_{1-\\alpha}\\) where \\(q_{1-\\alpha}\\) is the \\(1-\\alpha^{t h}\\) quantile of \\(|\\xi|\\). The bootstrap test “Reject \\(\\mathbb{M}_{0}\\) in favor of \\(\\mathbb{M}_{1}\\) if \\(|T|>q_{1-\\alpha}^{*}\\)” has asymptotic size \\(\\alpha: \\mathbb{P}\\left[|T|>q_{1-\\alpha}^{*} \\mid \\mathbb{H}_{0}\\right] \\longrightarrow \\alpha\\) as \\(n \\rightarrow \\infty\\). In the smooth function model the t-test (with correct standard errors) has the following performance.\nTheorem \\(10.17\\) Under the assumptions of Theorem \\(9.11\\) of Probability and Statistics for Economists,\n\\[\nq_{1-\\alpha}^{*}=\\bar{z}_{1-\\alpha}+o_{p}\\left(n^{-1}\\right)\n\\]\nwhere \\(\\bar{z}_{\\alpha}=\\Phi^{-1}((1+\\alpha) / 2)\\) is the \\(\\alpha^{t h}\\) quantile of \\(|Z|\\). The asymptotic test “Reject \\(\\mathbb{M}_{0}\\) in favor of \\(\\mathbb{M}_{1}\\) if \\(|T|>\\bar{z}_{1-\\alpha}\\)” has accuracy\n\\[\n\\mathbb{P}\\left[|T|>\\bar{z}_{1-\\alpha} \\mid \\mathbb{H}_{0}\\right]=1-\\alpha+O\\left(n^{-1}\\right)\n\\]\nand the bootstrap test “Reject \\(\\mathbb{M}_{0}\\) in favor of \\(\\mathbb{M}_{1}\\) if \\(|T|>q_{1-\\alpha}^{*}\\)” has accuracy\n\\[\n\\mathbb{P}\\left[|T|>q_{1-\\alpha}^{*} \\mid \\mathbb{M}_{0}\\right]=1-\\alpha+o\\left(n^{-1}\\right) .\n\\]\nThis shows that the bootstrap test achieves a refinement relative to the asymptotic test.\nThe reasoning is as follows. We have shown that the Edgeworth expansion for the absolute t-ratio takes the form\n\\[\n\\mathbb{P}[|T| \\leq x]=2 \\Phi(x)-1+n^{-1} 2 p_{2}(x)+o\\left(n^{-1}\\right) .\n\\]\nThis means the asymptotic test has accuracy of order \\(O\\left(n^{-1}\\right)\\).\nGiven the Edgeworth expansion, the Cornish-Fisher expansion for the \\(\\alpha^{t h}\\) quantile \\(q_{\\alpha}\\) of the distribution of \\(|T|\\) takes the form\n\\[\nq_{\\alpha}=\\bar{z}_{\\alpha}+n^{-1} p_{21}\\left(\\bar{z}_{\\alpha}\\right)+o\\left(n^{-1}\\right) .\n\\]\nThe bootstrap quantile \\(q_{\\alpha}^{*}\\) has the Cornish-Fisher expansion\n\\[\n\\begin{aligned}\nq_{\\alpha}^{*} &=\\bar{z}_{\\alpha}+n^{-1} p_{21}^{*}\\left(\\bar{z}_{\\alpha}\\right)+o\\left(n^{-1}\\right) \\\\\n&=\\bar{z}_{\\alpha}+n^{-1} p_{21}\\left(\\bar{z}_{\\alpha}\\right)+o_{p}\\left(n^{-1}\\right) \\\\\n&=q_{\\alpha}+o_{p}\\left(n^{-1}\\right)\n\\end{aligned}\n\\]\nwhere \\(p_{21}^{*}(x)\\) is the same as \\(p_{21}(x)\\) except that the population moments are replaced by the corresponding sample moments. The bootstrap test has rejection probability, using the Edgeworth expansion Delta method (Theorem \\(11.12\\) of of Probability and Statistics for Economists)\n\\[\n\\begin{aligned}\n\\mathbb{P}\\left[|T|>q_{1-\\alpha}^{*} \\mid \\mathbb{B}_{0}\\right] &=\\mathbb{P}\\left[|T|+\\left(q_{1-\\alpha}-q_{1-\\alpha}^{*}\\right)>q_{1-\\alpha}\\right] \\\\\n&=\\mathbb{P}\\left[|T|>q_{1-\\alpha}\\right]+o\\left(n^{-1}\\right) \\\\\n&=1-\\alpha+o\\left(n^{-1}\\right)\n\\end{aligned}\n\\]\nas claimed."
  },
  {
    "objectID": "chpt10-resample-method.html#wald-type-bootstrap-tests",
    "href": "chpt10-resample-method.html#wald-type-bootstrap-tests",
    "title": "10  Resampling Methods",
    "section": "10.22 Wald-Type Bootstrap Tests",
    "text": "10.22 Wald-Type Bootstrap Tests\nIf \\(\\theta\\) is a vector then to test \\(\\mathbb{M}_{0}: \\theta=\\theta_{0}\\) against \\(\\mathbb{M}_{1}: \\theta \\neq \\theta_{0}\\) at size \\(\\alpha\\), a common test is based on the Wald statistic \\(W=\\left(\\widehat{\\theta}-\\theta_{0}\\right)^{\\prime} \\widehat{\\boldsymbol{V}}_{\\widehat{\\theta}}^{-1}\\left(\\widehat{\\theta}-\\theta_{0}\\right)\\) where \\(\\widehat{\\theta}\\) is an estimator of \\(\\theta\\) and \\(\\widehat{\\boldsymbol{V}}_{\\widehat{\\theta}}\\) is a covariance matrix estimator. For a bootstrap test we use the bootstrap algorithm to calculate the critical value. The bootstrap algorithm samples with replacement from the dataset. Given a bootstrap sample the bootstrap estimator \\(\\widehat{\\theta}^{*}\\) and covariance matrix estimator \\(\\widehat{\\boldsymbol{V}}_{\\widehat{\\theta}}^{*}\\) are calculated. Given these values the bootstrap Wald statistic is\n\\[\nW^{*}=\\left(\\widehat{\\theta}^{*}-\\widehat{\\theta}\\right)^{\\prime} \\widehat{\\boldsymbol{V}}_{\\widehat{\\theta}}^{*-1}\\left(\\widehat{\\theta}^{*}-\\widehat{\\theta}\\right) .\n\\]\nAs for the t-test it is essential that the bootstrap Wald statistic \\(W^{*}\\) is centered at the sample estimator \\(\\widehat{\\theta}\\) instead of the hypothesized value \\(\\theta_{0}\\). This is because \\(\\widehat{\\theta}\\) is the true value in the bootstrap universe.\nBased on \\(B\\) bootstrap replications we calculate the \\(\\alpha^{t h}\\) quantile \\(q_{\\alpha}^{*}\\) of the distribution of the bootstrap Wald statistics \\(W^{*}\\). The bootstrap test rejects \\(\\mathbb{M}_{0}\\) in favor of \\(\\mathbb{H}_{1}\\) if \\(W>q_{1-\\alpha}^{*}\\). More commonly, we calculate a bootstrap p-value. This is\n\\[\np^{*}=\\frac{1}{B} \\sum_{b=1}^{B} \\mathbb{1}\\left\\{W^{*}(b)>W\\right\\} .\n\\]\nThe asymptotic performance of the Wald test mimics that of the t-test. In general, the bootstrap Wald test is first-order correct (achieves the correct size asymptotically) and under conditions for which an Edgeworth expansion exists, has accuracy\n\\[\n\\mathbb{P}\\left[W>q_{1-\\alpha}^{*} \\mid \\mathbb{H}_{0}\\right]=1-\\alpha+o\\left(n^{-1}\\right)\n\\]\nand thus achieves a refinement relative to the asymptotic Wald test.\nIf a reliable covariance matrix estimator \\(\\widehat{\\boldsymbol{V}}_{\\widehat{\\theta}}\\) is not available a Wald-type test can be implemented with any positive-definite weight matrix instead of \\(\\widehat{\\boldsymbol{V}}_{\\widehat{\\theta}}\\). This includes simple choices such as the identity matrix. The bootstrap algorithm can be used to calculate critical values and \\(\\mathrm{p}\\)-values for the test. So long as the estimator \\(\\hat{\\theta}\\) has an asymptotic distribution this bootstrap test will be asymptotically firstorder valid. The test will not achieve an asymptotic refinement but provides a simple method to test hypotheses when covariance matrix estimates are not available."
  },
  {
    "objectID": "chpt10-resample-method.html#criterion-based-bootstrap-tests",
    "href": "chpt10-resample-method.html#criterion-based-bootstrap-tests",
    "title": "10  Resampling Methods",
    "section": "10.23 Criterion-Based Bootstrap Tests",
    "text": "10.23 Criterion-Based Bootstrap Tests\nA criterion-based estimator takes the form\n\\[\n\\widehat{\\beta}=\\underset{\\beta}{\\operatorname{argmin}} J(\\beta)\n\\]\nfor some criterion function \\(J(\\beta)\\). This includes least squares, maximum likelihood, and minimum distance. Given a hypothesis \\(\\mathbb{M}_{0}: \\theta=\\theta_{0}\\) where \\(\\theta=r(\\beta)\\), the restricted estimator which satisfies \\(\\mathbb{H}_{0}\\) is\n\\[\n\\widetilde{\\beta}=\\underset{r(\\beta)=\\theta_{0}}{\\operatorname{argmin}} J(\\beta) .\n\\]\nA criterion-based statistic to test \\(\\mathbb{H}_{0}\\) is\n\\[\nJ=\\min _{r(\\beta)=\\theta_{0}} J(\\beta)-\\min _{\\beta} J(\\beta)=J(\\widetilde{\\beta})-J(\\widehat{\\beta}) .\n\\]\nA criterion-based test rejects \\(\\mathbb{H}_{0}\\) for large values of \\(J\\). A bootstrap test uses the bootstrap algorithm to calculate the critical value.\nIn this context we need to be a bit thoughtful about how to construct bootstrap versions of \\(J\\). It might seem natural to construct the exact same statistic on the bootstrap samples as on the original sample, but this is incorrect. It makes the same error as calculating a t-ratio or Wald statistic centered at the hypothesized value. In the bootstrap universe, the true value of \\(\\theta\\) is not \\(\\theta_{0}\\), rather it is \\(\\widehat{\\theta}=r(\\widehat{\\beta})\\). Thus when using the nonparametric bootstrap, we want to impose the constraint \\(r(\\beta)=r(\\widehat{\\beta})=\\widehat{\\theta}\\) to obtain the bootstrap version of \\(J\\).\nThus, the correct way to calculate a bootstrap version of \\(J\\) is as follows. Generate a bootstrap sample by random sampling from the dataset. Let \\(J^{*}(\\beta)\\) be the the bootstrap version of the criterion. On a bootstrap sample calculate the unrestricted estimator \\(\\widehat{\\beta}^{*}=\\underset{\\beta}{\\operatorname{argmin}} J^{*}(\\beta)\\) and the restricted version \\(\\widetilde{\\beta}^{*}=\\) \\(\\operatorname{argmin} J^{*}(\\beta)\\) where \\(\\widehat{\\theta}=r(\\widehat{\\beta})\\). The bootstrap statistic is \\(r(\\beta)=\\hat{\\theta}\\)\n\\[\nJ^{*}=\\min _{r(\\beta)=\\widehat{\\theta}} J^{*}(\\beta)-\\min _{\\beta} J^{*}(\\beta)=J^{*}\\left(\\widetilde{\\beta}^{*}\\right)-J^{*}\\left(\\widehat{\\beta}^{*}\\right) .\n\\]\nCalculate \\(J^{*}\\) on each bootstrap sample. Take the \\(1-\\alpha^{\\text {th }}\\) quantile \\(q_{1-\\alpha}^{*}\\). The bootstrap test rejects \\(\\mathbb{H}_{0}\\) in favor of \\(\\mathbb{H}_{1}\\) if \\(J>q_{1-\\alpha}^{*}\\). The bootstrap p-value is\n\\[\np^{*}=\\frac{1}{B} \\sum_{b=1}^{B} \\mathbb{1}\\left\\{J^{*}(b)>J\\right\\} .\n\\]\nSpecial cases of criterion-based tests are minimum distance tests, \\(F\\) tests, and likelihood ratio tests. Take the F test for a linear hypothesis \\(\\boldsymbol{R}^{\\prime} \\beta=\\theta_{0}\\). The \\(F\\) statistic is\n\\[\n\\mathrm{F}=\\frac{\\left(\\widetilde{\\sigma}^{2}-\\widehat{\\sigma}^{2}\\right) / q}{\\widehat{\\sigma}^{2} /(n-k)}\n\\]\nwhere \\(\\widehat{\\sigma}^{2}\\) is the unrestricted estimator of the error variance, \\(\\widetilde{\\sigma}^{2}\\) is the restricted estimator, \\(q\\) is the number of restrictions and \\(k\\) is the number of estimated coefficients. The bootstrap version of the \\(F\\) statistic is\n\\[\n\\mathrm{F}^{*}=\\frac{\\left(\\widetilde{\\sigma}^{* 2}-\\widehat{\\sigma}^{* 2}\\right) / q}{\\widehat{\\sigma}^{* 2} /(n-k)}\n\\]\nwhere \\(\\widehat{\\sigma}^{* 2}\\) is the unrestricted estimator on the bootstrap sample, and \\(\\widetilde{\\sigma}^{* 2}\\) is the restricted estimator which imposes the restriction \\(\\widehat{\\theta}=\\boldsymbol{R}^{\\prime} \\widehat{\\beta}\\).\nTake the likelihood ratio (LR) test for the hypothesis \\(r(\\beta)=\\theta_{0}\\). The LR test statistic is\n\\[\n\\mathrm{LR}=2\\left(\\ell_{n}(\\widehat{\\beta})-\\ell_{n}(\\widetilde{\\beta})\\right)\n\\]\nwhere \\(\\widehat{\\beta}\\) is the unrestricted MLE and \\(\\widetilde{\\beta}\\) is the restricted MLE (imposing \\(r(\\beta)=\\theta_{0}\\) ). The bootstrap version is\n\\[\n\\operatorname{LR}^{*}=2\\left(\\ell_{n}^{*}\\left(\\widehat{\\beta}^{*}\\right)-\\ell_{n}^{*}\\left(\\widetilde{\\beta}^{*}\\right)\\right)\n\\]\nwhere \\(\\ell_{n}^{*}(\\beta)\\) is the log-likelihood function calculated on the bootstrap sample, \\(\\widehat{\\beta}^{*}\\) is the unrestricted maximizer, and \\(\\widetilde{\\beta}^{*}\\) is the restricted maximizer imposing the restriction \\(r(\\beta)=r(\\widehat{\\beta})\\)."
  },
  {
    "objectID": "chpt10-resample-method.html#parametric-bootstrap",
    "href": "chpt10-resample-method.html#parametric-bootstrap",
    "title": "10  Resampling Methods",
    "section": "10.24 Parametric Bootstrap",
    "text": "10.24 Parametric Bootstrap\nThroughout this chapter we have described the most popular form of the bootstrap known as the nonparametric bootstrap. However there are other forms of the bootstrap algorithm including the parametric bootstrap. This is appropriate when there is a full parametric model for the distribution as in likelihood estimation.\nFirst, consider the context where the model specifies the full distribution of the random vector \\(Y\\), e.g. \\(Y \\sim F(y \\mid \\beta)\\) where the distribution function \\(F\\) is known but the parameter \\(\\beta\\) is unknown. Let \\(\\widehat{\\beta}\\) be an estimator of \\(\\beta\\) such as the maximum likelihood estimator. The parametric bootstrap algorithm generates bootstrap observations \\(Y_{i}^{*}\\) by drawing random vectors from the distribution function \\(F(y \\mid \\widehat{\\beta})\\). When this is done, the true value of \\(\\beta\\) in the bootstrap universe is \\(\\widehat{\\beta}\\). Everything which has been discussed in the chapter can be applied using this bootstrap algorithm.\nSecond, consider the context where the model specifies the conditional distribution of the random vector \\(Y\\) given the random vector \\(X\\), e.g. \\(Y \\mid X \\sim F(y \\mid X, \\beta)\\). An example is the normal linear regression model, where \\(Y \\mid X \\sim \\mathrm{N}\\left(X^{\\prime} \\beta, \\sigma^{2}\\right)\\). In this context we can hold the regressors \\(X_{i}\\) fixed and then draw the bootstrap observations \\(Y_{i}^{*}\\) from the conditional distribution \\(F\\left(y \\mid X_{i}, \\widehat{\\beta}\\right)\\). In the example of the normal regression model this is equivalent to drawing a normal error \\(e_{i}^{*} \\sim \\mathrm{N}\\left(0, \\widehat{\\sigma}^{2}\\right)\\) and then setting \\(Y_{i}^{*}=X_{i}^{\\prime} \\widehat{\\beta}+\\) \\(e_{i}^{*}\\). Again, in this algorithm the true value of \\(\\beta\\) is \\(\\widehat{\\beta}\\) and everything which is discussed in this chapter can be applied as before.\nThird, consider tests of the hypothesis \\(r(\\beta)=\\theta_{0}\\). In this context we can also construct a restricted estimator \\(\\widetilde{\\beta}\\) (for example the restricted MLE) which satisfies the hypothesis \\(r(\\widetilde{\\beta})=\\theta_{0}\\). Then we can generate bootstrap samples by simulating from the distribution \\(Y_{i}^{*} \\sim F(y \\mid \\widetilde{\\beta})\\), or in the conditional context from \\(Y_{i}^{*} \\sim F\\left(y \\mid X_{i}, \\widetilde{\\beta}\\right)\\). When this is done the true value of \\(\\beta\\) in the bootstrap is \\(\\widetilde{\\beta}\\) which satisfies the hypothesis. So in this context the correct values of the bootstrap statistics are\n\\[\n\\begin{gathered}\nT^{*}=\\frac{\\widehat{\\theta}^{*}-\\theta_{0}}{s\\left(\\widehat{\\theta}^{*}\\right)} \\\\\nW^{*}=\\left(\\widehat{\\theta}^{*}-\\theta_{0}\\right)^{\\prime} \\widehat{\\boldsymbol{V}}_{\\widehat{\\theta}}^{*-1}\\left(\\widehat{\\theta}^{*}-\\theta_{0}\\right) \\\\\nJ^{*}=\\min _{r(\\beta)=\\theta_{0}} J^{*}(\\beta)-\\min _{\\beta} J^{*}(\\beta) \\\\\n\\mathrm{LR}^{*}=2\\left(\\max _{\\beta} \\ell_{n}^{*}(\\beta)-\\max _{r(\\beta)=\\theta_{0}} \\ell_{n}^{*}(\\beta)\\right)\n\\end{gathered}\n\\]\nand\n\\[\n\\mathrm{F}^{*}=\\frac{\\left(\\widetilde{\\sigma}^{* 2}-\\widehat{\\sigma}^{* 2}\\right) / q}{\\widehat{\\sigma}^{* 2} /(n-k)}\n\\]\nwhere \\(\\widehat{\\sigma}^{* 2}\\) is the unrestricted estimator on the bootstrap sample and \\(\\widetilde{\\sigma}^{* 2}\\) is the restricted estimator which imposes the restriction \\(\\boldsymbol{R}^{\\prime} \\beta=\\theta_{0}\\).\nThe primary advantage of the parametric bootstrap (relative to the nonparametric bootstrap) is that it will be more accurate when the parametric model is correct. This may be quite important in small samples. The primary disadvantage of the parametric bootstrap is that it can be inaccurate when the parametric model is incorrect."
  },
  {
    "objectID": "chpt10-resample-method.html#how-many-bootstrap-replications",
    "href": "chpt10-resample-method.html#how-many-bootstrap-replications",
    "title": "10  Resampling Methods",
    "section": "10.25 How Many Bootstrap Replications?",
    "text": "10.25 How Many Bootstrap Replications?\nHow many bootstrap replications should be used? There is no universally correct answer as there is a trade-off between accuracy and computation cost. Computation cost is essentially linear in \\(B\\). Accuracy (either standard errors or \\(p\\)-values) is proportional to \\(B^{-1 / 2}\\). Improved accuracy can be obtained but only at a higher computational cost.\nIn most empirical research, most calculations are quick and investigatory, not requiring full accuracy. But final results (those going into the final version of the paper) should be accurate. Thus it seems reasonable to use asymptotic and/or bootstrap methods with a modest number of replications for daily calculations, but use a much larger \\(B\\) for the final version.\nIn particular, for final calculations, \\(B=10,000\\) is desired, with \\(B=1000\\) a minimal choice. In contrast, for daily quick calculations values as low as \\(B=100\\) may be sufficient for rough estimates. A useful way to think about the accuracy of bootstrap methods stems from the calculation of pvalues. The bootstrap p-value \\(p^{*}\\) is an average of \\(B\\) Bernoulli draws. The variance of the simulation estimator of \\(p^{*}\\) is \\(p^{*}\\left(1-p^{*}\\right) / B\\), which is bounded above by \\(1 / 4 B\\). To calculate the \\(\\mathrm{p}\\)-value within, say, \\(0.01\\) of the true value with \\(95 %\\) probability requires a standard error below \\(0.005\\). This is ensured if \\(B \\geq 10,000\\).\nStata by default sets \\(B=50\\). This is useful for verification that a program runs but is a poor choice for empirical reporting. Make sure that you set \\(B\\) to the value you want."
  },
  {
    "objectID": "chpt10-resample-method.html#setting-the-bootstrap-seed",
    "href": "chpt10-resample-method.html#setting-the-bootstrap-seed",
    "title": "10  Resampling Methods",
    "section": "10.26 Setting the Bootstrap Seed",
    "text": "10.26 Setting the Bootstrap Seed\nComputers do not generate true random numbers but rather pseudo-random numbers generated by a deterministic algorithm. The algorithms generate sequences which are indistinguishable from random sequences so this is not a worry for bootstrap applications.\nThe methods, however, necessarily require a starting value known as a “seed”. Some packages (including Stata and MATLAB) implement this with a default seed which is reset each time the statistical package is started. This means if you start the package fresh, run a bootstrap program (e.g. a “do” file in Stata), exit the package, restart the package and then rerun the bootstrap program, you should obtain exactly the same results. If you instead run the bootstrap program (e.g. “do” file) twice sequentially without restarting the package, the seed is not reset so a different set of pseudo-random numbers will be generated and the results from the two runs will be different.\nThe R package has a different implementation. When \\(\\mathrm{R}\\) is loaded the random number seed is generated based on the computer’s clock (which results in an essentially random starting seed). Therefore if you run a bootstrap program in R, exit, restart, and rerun, you will obtain a different set of random draws and therefore a different bootstrap result.\nPackages allow users to set their own seed. (In Stata, the command is set seed #. In MATLAB the command is \\(r n g(\\#)\\). In \\(R\\) the command is set. seed (#).) If the seed is set to a specific number at the start of a file then the exact same pseudo-random numbers will be generated each time the program is run. If this is the case, the results of a bootstrap calculation (standard error or test) will be identical across computer runs.\nThe fact that the bootstrap results can be fixed by setting the seed in the replication file has motivated many researchers to follow this choice. They set the seed at the start of the replication file so that repeated executions result in the same numerical findings.\nFixing seeds, however, should be done cautiously. It may be a wise choice for a final calculation (when a paper is finished) but is an unwise choice for daily calculations. If you use a small number of replications in your preliminary work, say \\(B=100\\), the bootstrap calculations will be inaccurate. But as you run your results again and again (as is typical in empirical projects) you will obtain the same numerical standard errors and test results, giving you a false sense of stability and accuracy. If instead a different seed is used each time the program is run then the bootstrap results will vary across runs, and you will observe that the results vary across these runs, giving you important and meaningful information about the (lack of) accuracy in your results. One way to ensure this is to set the seed according to the current clock. In MATLAB use the command rng(‘shuffle’). In R use set. seed (seed=NULL). Stata does not have this option.\nThese considerations lead to a recommended hybrid approach. For daily empirical investigations do not fix the bootstrap seed in your program unless you have it set by the clock. For your final calculations set the seed to a specific arbitrary choice, and set \\(B=10,000\\) so that the results are insensitive to the seed."
  },
  {
    "objectID": "chpt10-resample-method.html#bootstrap-regression",
    "href": "chpt10-resample-method.html#bootstrap-regression",
    "title": "10  Resampling Methods",
    "section": "10.27 Bootstrap Regression",
    "text": "10.27 Bootstrap Regression\nA major focus of this textbook has been on the least squares estimator \\(\\widehat{\\beta}\\) in the projection model. The bootstrap can be used to calculate standard errors and confidence intervals for smooth functions of the coefficient estimates.\nThe nonparametric bootstrap algorithm, as described before, samples observations randomly with replacement from the dataset, creating the bootstrap sample \\(\\left\\{\\left(Y_{1}^{*}, X_{1}^{*}\\right), \\ldots,\\left(Y_{n}^{*}, X_{n}^{*}\\right)\\right\\}\\), or in matrix notation \\(\\left(\\boldsymbol{Y}^{*}, \\boldsymbol{X}^{*}\\right)\\) It is important to recognize that entire observations (pairs of \\(Y_{i}\\) and \\(X_{i}\\) ) are sampled. This is often called the pairs bootstrap.\nGiven this bootstrap sample, we calculate the regression estimator\n\\[\n\\widehat{\\beta}^{*}=\\left(\\boldsymbol{X}^{* \\prime} \\boldsymbol{X}^{*}\\right)^{-1}\\left(\\boldsymbol{X}^{* \\prime} \\boldsymbol{Y}^{*}\\right) .\n\\]\nThis is repeated \\(B\\) times. The bootstrap standard errors are the standard deviations across the draws and confidence intervals are constructed from the empirical quantiles across the draws.\nWhat is the nature of the bootstrap distribution of \\(\\widehat{\\beta}^{*}\\) ? It is useful to start with the distribution of the bootstrap observations \\(\\left(Y_{i}^{*}, X_{i}^{*}\\right)\\), which is the discrete distribution which puts mass \\(1 / n\\) on each observation pair \\(\\left(Y_{i}, X_{i}\\right)\\). The bootstrap universe can be thought of as the empirical scatter plot of the observations. The true value of the projection coefficient in this bootstrap universe is\n\\[\n\\left(\\mathbb{E}^{*}\\left[X_{i}^{*} X_{i}^{* \\prime}\\right]\\right)^{-1}\\left(\\mathbb{E}^{*}\\left[X_{i}^{*} Y_{i}^{*}\\right]\\right)=\\left(\\frac{1}{n} \\sum_{i=1}^{n} X_{i} X_{i}^{\\prime}\\right)^{-1}\\left(\\frac{1}{n} \\sum_{i=1}^{n} X_{i} Y_{i}\\right)=\\widehat{\\beta}\n\\]\nWe see that the true value in the bootstrap distribution is the least squares estimator \\(\\widehat{\\beta}\\).\nThe bootstrap observations satisfy the projection equation\n\\[\n\\begin{aligned}\nY_{i}^{*} &=X_{i}^{* \\prime} \\widehat{\\beta}+e_{i}^{*} \\\\\n\\mathbb{E}^{*}\\left[X_{i}^{*} e_{i}^{*}\\right] &=0 .\n\\end{aligned}\n\\]\nFor each bootstrap pair \\(\\left(Y_{i}^{*}, X_{i}^{*}\\right)=\\left(Y_{j}, X_{j}\\right)\\) the true error \\(e_{i}^{*}=\\widehat{e}_{j}\\) equals the least squares residual from the original dataset. This is because each bootstrap pair corresponds to an actual observation.\nA technical problem (which is typically ignored) is that it is possible for \\(\\boldsymbol{X}^{* \\prime} \\boldsymbol{X}^{*}\\) to be singular in a simulated bootstrap sample, in which case the least squares estimator \\(\\widehat{\\beta}^{*}\\) is not uniquely defined. Indeed, the probability is positive that \\(\\boldsymbol{X}^{* \\prime} \\boldsymbol{X}^{*}\\) is singular. For example, the probability that a bootstrap sample consists entirely of one observation repeated \\(n\\) times is \\(n^{-(n-1)}\\). This is a small probability, but positive. A more significant example is sparse dummy variable designs where it is possible to draw an entire sample with only one observed value for the dummy variable. For example, if a sample has \\(n=20\\) observations with a dummy variable with treatment (equals 1) for only three of the 20 observations, the probability is \\(4 %\\) that a bootstrap sample contains entirely non-treated values (all 0’s). \\(4 %\\) is quite high!\nThe standard approach to circumvent this problem is to compute \\(\\widehat{\\beta}^{*}\\) only if \\(\\boldsymbol{X}^{* \\prime} \\boldsymbol{X}^{*}\\) is non-singular as defined by a conventional numerical tolerance and treat it as missing otherwise. A better solution is to define a tolerance which bounds \\(\\boldsymbol{X}^{* \\prime} \\boldsymbol{X}^{*}\\) away from non-singularity. Define the ratio of the smallest eigenvalue of the bootstrap design matrix to that of the data design matrix\n\\[\n\\lambda^{*}=\\frac{\\lambda_{\\min }\\left(\\boldsymbol{X}^{* \\prime} \\boldsymbol{X}^{*}\\right)}{\\lambda_{\\min }\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)} .\n\\]\nIf, in a given bootstrap replication, \\(\\lambda^{*}<\\tau\\) is smaller than a given tolerance (Shao and Tu \\((1995, \\mathrm{p} .291)\\) recommend \\(\\tau=1 / 2\\) ) then the estimator can be treated as missing, or we can define the trimming rule\n\\[\n\\widehat{\\beta}^{*}=\\left\\{\\begin{array}{cc}\n\\widehat{\\beta}^{*} & \\text { if } \\lambda^{*} \\geq \\tau \\\\\n\\widehat{\\beta} & \\text { if } \\lambda^{*}<\\tau\n\\end{array}\\right.\n\\]\nThis ensures that the bootstrap estimator \\(\\widehat{\\beta}^{*}\\) will be well behaved."
  },
  {
    "objectID": "chpt10-resample-method.html#bootstrap-regression-asymptotic-theory",
    "href": "chpt10-resample-method.html#bootstrap-regression-asymptotic-theory",
    "title": "10  Resampling Methods",
    "section": "10.28 Bootstrap Regression Asymptotic Theory",
    "text": "10.28 Bootstrap Regression Asymptotic Theory\nDefine the least squares estimator \\(\\widehat{\\beta}\\), its bootstrap version \\(\\widehat{\\beta}^{*}\\) as in (10.32), and the transformations \\(\\widehat{\\theta}=g(\\widehat{\\beta})\\) and \\(\\widehat{\\theta}^{*}=r\\left(\\widehat{\\beta}^{*}\\right)\\) for some smooth transformation \\(r\\). Let \\(\\widehat{\\boldsymbol{V}}_{\\beta}\\) and \\(\\widehat{\\boldsymbol{V}}_{\\theta}\\) denote heteroskedasticityrobust covariance matrix estimators for \\(\\widehat{\\beta}\\) and \\(\\widehat{\\theta}\\), and let \\(\\widehat{V}_{\\beta}^{*}\\) and \\(\\widehat{\\boldsymbol{V}}_{\\theta}^{*}\\) be their bootstrap versions. When \\(\\theta\\) is scalar define the standard errors \\(s(\\widehat{\\theta})=\\sqrt{n^{-1} \\widehat{\\boldsymbol{V}}_{\\theta}}\\) and \\(s\\left(\\widehat{\\theta}^{*}\\right)=\\sqrt{n^{-1} \\widehat{\\boldsymbol{V}}_{\\theta^{*}}}\\). Define the t-ratios \\(T=\\) \\((\\widehat{\\theta}-\\theta) / s(\\widehat{\\theta})\\) and bootstrap version \\(T^{*}=\\left(\\widehat{\\theta}^{*}-\\widehat{\\theta}\\right) / s\\left(\\widehat{\\theta}^{*}\\right)\\). We are interested in the asymptotic distributions of \\(\\widehat{\\beta}^{*}, \\widehat{\\theta}^{*}\\) and \\(T^{*}\\)\nSince the bootstrap observations satisfy the model (10.33), we see by standard calculations that\n\\[\n\\sqrt{n}\\left(\\widehat{\\beta}^{*}-\\widehat{\\beta}\\right)=\\left(\\frac{1}{n} \\sum_{i=1}^{n} X_{i}^{*} X_{i}^{* \\prime}\\right)^{-1}\\left(\\frac{1}{\\sqrt{n}} \\sum_{i=1}^{n} X_{i}^{*} e_{i}^{*}\\right)\n\\]\nBy the bootstrap WLLN\n\\[\n\\frac{1}{n} \\sum_{i=1}^{n} X_{i}^{*} X_{i}^{* \\prime} \\underset{p^{*}}{\\longrightarrow} \\mathbb{E}\\left[X_{i} X_{i}^{\\prime}\\right]=\\boldsymbol{Q}\n\\]\nand by the bootstrap CLT\n\\[\n\\frac{1}{\\sqrt{n}} \\sum_{i=1}^{n} X_{i}^{*} e_{i}^{*} \\underset{d^{*}}{\\longrightarrow} \\mathrm{N}(0, \\Omega)\n\\]\nwhere \\(\\Omega=\\mathbb{E}\\left[X X^{\\prime} e^{2}\\right]\\). Again applying the bootstrap WLLN we obtain\n\\[\n\\widehat{\\boldsymbol{V}}_{\\beta} \\underset{p^{*}}{ } \\boldsymbol{V}_{\\beta}=\\boldsymbol{Q}^{-1} \\Omega \\boldsymbol{Q}^{-1}\n\\]\nand\n\\[\n\\widehat{\\boldsymbol{V}}_{\\theta} \\underset{p^{*}}{\\longrightarrow} \\boldsymbol{V}_{\\theta}=\\boldsymbol{R}^{\\prime} \\boldsymbol{V}_{\\beta} \\boldsymbol{R}\n\\]\nwhere \\(\\boldsymbol{R}=\\boldsymbol{R}(\\beta)\\).\nCombining with the bootstrap CMT and delta method we establish the asymptotic distribution of the bootstrap regression estimator.\nTheorem \\(10.18\\) Under Assumption 7.2, as \\(n \\rightarrow \\infty\\)\n\\[\n\\sqrt{n}\\left(\\widehat{\\theta}^{*}-\\widehat{\\theta}\\right) \\underset{d^{*}}{\\longrightarrow} \\mathrm{N}\\left(0, \\boldsymbol{V}_{\\beta}\\right) .\n\\]\nIf Assumption \\(7.3\\) also holds then\n\\[\n\\sqrt{n}\\left(\\widehat{\\theta}^{*}-\\widehat{\\theta}\\right) \\underset{d^{*}}{\\longrightarrow} \\mathrm{N}\\left(0, \\boldsymbol{V}_{\\theta}\\right) .\n\\]\nIf Assumption \\(7.4\\) also holds then\n\\[\nT^{*} \\underset{d^{*}}{\\longrightarrow} \\mathrm{N}(0,1) .\n\\]\nThis means that the bootstrap confidence interval and testing methods all apply for inference on \\(\\beta\\) and \\(\\theta\\). This includes the percentile, \\(\\mathrm{BC}\\) percentile, \\(\\mathrm{BC}_{a}\\), and percentile-t intervals, and hypothesis tests based on t-tests, Wald tests, MD tests, LR tests and F tests.\nTo justify bootstrap standard errors we also need to verify the uniform square integrability of \\(\\widehat{\\beta}^{*}\\) and \\(\\widehat{\\theta}^{*}\\). This is technically challenging because the least squares estimator involves matrix inversion which is not globally continuous. A partial solution is to use the trimmed estimator (10.34). This bounds the moments of \\(\\widehat{\\beta}^{*}\\) by those of \\(n^{-1} \\sum_{i=1}^{n} X_{i}^{*} e_{i}^{*}\\). Since this is a sample mean, Theorem \\(10.10\\) applies and \\(\\widehat{\\boldsymbol{V}}_{\\beta}^{*}\\) is bootstrap consistent for \\(\\boldsymbol{V}_{\\beta}\\). However, this does not ensure that \\(\\widehat{\\boldsymbol{V}}_{\\theta}^{*}\\) will be consistent for \\(\\widehat{\\boldsymbol{V}}_{\\theta}\\) unless the function \\(r(x)\\) satisfies the conditions of Theorem 10.10. For general applications use a trimmed estimator for the bootstrap variance. For some \\(\\tau_{n}=O\\left(e^{n / 8}\\right)\\) define\n\\[\n\\begin{aligned}\nZ_{n}^{*} &=\\sqrt{n}\\left(\\widehat{\\theta}^{*}-\\widehat{\\theta}\\right) \\\\\nZ^{* *} &=z^{*} \\mathbb{1}\\left\\{\\left\\|Z_{n}^{*}\\right\\| \\leq \\tau_{n}\\right\\} \\\\\nZ^{* *} &=\\frac{1}{B} \\sum_{b=1}^{B} Z^{* *}(b) \\\\\n\\widehat{\\mathbf{V}}_{\\theta}^{\\text {boot }, \\tau} &=\\frac{1}{B-1} \\sum_{b=1}^{B}\\left(Z^{* *}(b)-Z^{* *}\\right)\\left(Z^{* *}(b)-Z^{* *}\\right)^{\\prime} .\n\\end{aligned}\n\\]\nThe matrix \\(\\widehat{\\boldsymbol{V}}_{\\theta}^{\\text {boot }}\\) is a trimmed bootstrap estimator of the variance of \\(Z_{n}=\\sqrt{n}(\\widehat{\\theta}-\\theta)\\). The associated bootstrap standard error for \\(\\widehat{\\theta}\\) (in the scalar case) is \\(s(\\widehat{\\theta})=\\sqrt{n^{-1} \\widehat{\\boldsymbol{V}}_{\\theta}^{\\text {boot }}}\\).\nBy an application of Theorems \\(10.11\\) and 10.12, we find that this estimator \\(\\widehat{\\boldsymbol{V}}_{\\theta}^{\\text {boot }}\\) is consistent for the asymptotic variance.\nTheorem 10.19 Under Assumption \\(7.2\\) and \\(7.3\\), as \\(n \\rightarrow \\infty, \\widehat{\\boldsymbol{V}}_{\\theta}^{\\mathrm{boot}, \\tau} \\underset{p^{*}}{\\longrightarrow} \\boldsymbol{V}_{\\theta}\\)\nPrograms such as Stata use the untrimmed estimator \\(\\widehat{\\boldsymbol{V}}_{\\theta}^{\\text {boot }}\\) rather than the trimmed estimator \\(\\widehat{\\boldsymbol{V}}_{\\theta}^{\\text {boot }, \\tau}\\). This means that we should be cautious about interpreting reported bootstrap standard errors especially for nonlinear functions such as ratios."
  },
  {
    "objectID": "chpt10-resample-method.html#wild-bootstrap",
    "href": "chpt10-resample-method.html#wild-bootstrap",
    "title": "10  Resampling Methods",
    "section": "10.29 Wild Bootstrap",
    "text": "10.29 Wild Bootstrap\nTake the linear regression model\n\\[\n\\begin{aligned}\nY &=X^{\\prime} \\beta+e \\\\\n\\mathbb{E}[e \\mid X] &=0 .\n\\end{aligned}\n\\]\nWhat is special about this model is the conditional mean restriction. The nonparametric bootstrap (which samples the pairs \\(\\left(Y_{i}^{*}, X_{i}^{*}\\right)\\) i.i.d. from the original observations) does not make use of this restriction. Consequently the bootstrap distribution for \\(\\left(Y^{*}, X^{*}\\right)\\) does not satisfy the conditional mean restriction and therefore does not satisfy the linear regression assumption. To improve precision it seems reasonable to impose the conditional mean restriction on the bootstrap distribution.\nA natural approach is to hold the regressors \\(X_{i}\\) fixed and then draw the errors \\(e_{i}^{*}\\) in some way which imposes a conditional mean of zero. The simplest approach is to draw the errors independent from the regressors, perhaps from the empirical distribution of the residuals. This procedure is known as the residual bootstrap. However, this imposes independence of the errors from the regressors which is much stronger than the conditional mean assumption. This is generally undesirable.\nA method which imposes the conditional mean restriction while allowing general heteroskedasticity is the wild bootstrap. It was proposed by Liu (1988) and extended by Mammon (1993). The method uses auxiliary random variables \\(\\xi^{*}\\) which are i.i.d., mean zero, and variance 1 . The bootstrap observations are then generated as \\(Y_{i}^{*}=X_{i}^{\\prime} \\widehat{\\beta}+e_{i}^{*}\\) with \\(e_{i}^{*}=\\widehat{e}_{i} \\xi_{i}^{*}\\), where the regressors \\(X_{i}\\) are held fixed at their sample values, \\(\\widehat{\\beta}\\) is the sample least squares estimator, and \\(\\widehat{e}_{i}\\) are the least squares residuals, which are also held fixed at their sample values.\nThis algorithm generates bootstrap errors \\(e_{i}^{*}\\) which are conditionally mean zero. Thus the bootstrap pairs \\(\\left(Y_{i}^{*}, X_{i}\\right)\\) satisfy a linear regression with the “true” coefficient of \\(\\widehat{\\beta}\\). The conditional variance of the wild bootstrap errors \\(e_{i}^{*}\\) are \\(\\mathbb{E}^{*}\\left[e_{i}^{* 2} \\mid X_{i}\\right]=\\widehat{e}_{i}^{2}\\). This means that the conditional variance of the bootstrap estimator \\(\\widehat{\\beta}^{*}\\) is\n\\[\n\\mathbb{E}^{*}\\left[\\left(\\widehat{\\beta}^{*}-\\widehat{\\beta}\\right)\\left(\\widehat{\\beta}^{*}-\\widehat{\\beta}\\right)^{\\prime} \\mid \\boldsymbol{X}\\right]=\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1}\\left(\\sum_{i=1}^{n} X_{i} X_{i}^{\\prime} \\widehat{e}_{i}^{2}\\right)\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1}\n\\]\nwhich is the White estimator of the variance of \\(\\widehat{\\beta}\\). Thus the wild bootstrap replicates the appropriate first and second moments of the distribution.\nTwo distributions have been proposed for the auxilary variables \\(\\xi_{i}^{*}\\) both of which are two-point discrete distributions. The first are Rademacher random variables which satisfy \\(\\mathbb{P}\\left[\\xi^{*}=1\\right]=\\frac{1}{2}\\) and \\(\\mathbb{P}\\left[\\xi^{*}=-1\\right]=\\) \\(\\frac{1}{2}\\). The second is the Mammen (1993) two-point distribution\n\\[\n\\begin{aligned}\n&\\mathbb{P}\\left[\\xi^{*}=\\frac{1+\\sqrt{5}}{2}\\right]=\\frac{\\sqrt{5}-1}{2 \\sqrt{5}} \\\\\n&\\mathbb{P}\\left[\\xi^{*}=\\frac{1-\\sqrt{5}}{2}\\right]=\\frac{\\sqrt{5}+1}{2 \\sqrt{5}}\n\\end{aligned}\n\\]\nThe reasoning behind the Mammen distribution is that this choice implies \\(\\mathbb{E}\\left[\\xi^{* 3}\\right]=1\\), which implies that the third central moment of \\(\\widehat{\\beta}^{*}\\) matches the natural nonparametric estimator of the third central moment of \\(\\widehat{\\beta}\\). Since the wild bootstrap matches the first three moments, the percentile-t interval and one-sided t-tests can be shown to achieve asymptotic refinements.\nThe reasoning behind the Rademacher distribution is that this choice implies \\(\\mathbb{E}\\left[\\xi^{* 4}\\right]=1\\), which implies that the fourth central moment of \\(\\widehat{\\beta}^{*}\\) matches the natural nonparametric estimator of the fourth central moment of \\(\\widehat{\\beta}\\). If the regression errors \\(e\\) are symmetrically distributed (so the third moment is zero) then the first four moments are matched. In this case the wild bootstrap should have even better performance, and additionally two-sided t-tests can be shown to achieve an asymptotic refinement. When the regression error is not symmetrically distributed these asymptotic refinements are not achieved. Limited simulation evidence for one-sided t-tests presented in Davidson and Flachaire (2008) suggests that the Rademacher distribution (used with the restricted wild bootstrap) has better performance and is their recommendation.\nFor hypothesis testing improved precision can be obtained by the restricted wild bootstrap. Consider tests of the hypothesis \\(\\mathbb{H}_{0}: r(\\beta)=0\\). Let \\(\\widetilde{\\beta}\\) be a CLS or EMD estimator of \\(\\beta\\) subject to the restriction \\(r(\\widetilde{\\beta})=0\\). Let \\(\\widetilde{e}_{i}=Y_{i}-X_{i}^{\\prime} \\widetilde{\\beta}\\) be the constrained residuals. The restricted wild bootstrap algorithm generates observations as \\(Y_{i}^{*}=X_{i}^{\\prime} \\widetilde{\\beta}+e_{i}^{*}\\) with \\(e_{i}^{*}=\\widetilde{e}_{i} \\xi_{i}^{*}\\). With this modification \\(\\widetilde{\\beta}\\) is the true value in the bootstrap universe so the null hypothesis \\(\\mathbb{M}_{0}\\) holds. Thus bootstrap tests are constructed the same as for the parametric bootstrap using a restricted parameter estimator."
  },
  {
    "objectID": "chpt10-resample-method.html#bootstrap-for-clustered-observations",
    "href": "chpt10-resample-method.html#bootstrap-for-clustered-observations",
    "title": "10  Resampling Methods",
    "section": "10.30 Bootstrap for Clustered Observations",
    "text": "10.30 Bootstrap for Clustered Observations\nBootstrap methods can also be applied in to clustered samples though the methodological literature is relatively thin. Here we review methods discussed in Cameron, Gelbach and Miller (2008).\nLet \\(\\boldsymbol{Y}_{g}=\\left(Y_{1 g}, \\ldots, Y_{n_{g} g}\\right)^{\\prime}\\) and \\(\\boldsymbol{X}_{g}=\\left(X_{1 g}, \\ldots, X_{n_{g} g}\\right)^{\\prime}\\) denote the \\(n_{g} \\times 1\\) vector of dependent variables and \\(n_{g} \\times k\\) matrix of regressors for the \\(g^{t h}\\) cluster. A linear regression model using cluster notation is \\(\\boldsymbol{Y}_{g}=\\) \\(\\boldsymbol{X}_{g} \\beta+\\boldsymbol{e}_{g}\\) where \\(\\boldsymbol{e}_{g}=\\left(e_{1 g}, \\ldots, e_{n_{g} g}\\right)^{\\prime}\\) is an \\(n_{g} \\times 1\\) error vector. The sample has \\(G\\) cluster pairs \\(\\left(\\boldsymbol{Y}_{g}, \\boldsymbol{X}_{g}\\right)\\).\nThe pairs cluster bootstrap samples \\(G\\) cluster pairs \\(\\left(\\boldsymbol{Y}_{g}, \\boldsymbol{X}_{g}\\right)\\) to create the bootstrap sample. Least squares is applied to the bootstrap sample to obtain the coefficient estimators. By repeating \\(B\\) times bootstrap standard errors for coefficients estimates, or functions of the coefficient estimates, can be calculated. Percentile, \\(\\mathrm{BC}\\) percentile, and \\(\\mathrm{BC}_{a}\\) confidence intervals can be calculated.\nThe \\(\\mathrm{BC}_{a}\\) interval requires an estimator of the acceleration coefficient \\(a\\) which is a scaled jackknife estimate of the third moment of the estimator. In the context of clustered observations the delete-cluster jackknife should be used for estimation of \\(a\\).\nFurthermore, on each bootstrap sample the cluster-robust standard errors can be calculated and used to compute bootstrap t-ratios, from which percentile-t confidence intervals can be calculated. tions as\nThe wild cluster bootstrap fixes the clusters and regressors, and generates the bootstrap observa-\n\\[\n\\begin{aligned}\n\\boldsymbol{Y}_{g}^{*} &=\\boldsymbol{X}_{g} \\widehat{\\beta}+\\boldsymbol{e}_{g}^{*} \\\\\n\\boldsymbol{e}_{g}^{*} &=\\widehat{\\boldsymbol{e}}_{i} \\xi_{g}^{*}\n\\end{aligned}\n\\]\nwhere \\(\\xi_{g}^{*}\\) is a scalar auxilary random variable as described in the previous section. Notice that \\(\\xi_{g}^{*}\\) is interacted with the entire vector of residuals from cluster \\(g\\). Cameron, Gelbach and Miller (2008) follow the recommendation of Davidson and Flachaire (2008) and use Rademacher random variables for \\(\\xi_{g}^{*}\\).\nFor hypothesis testing, Cameron, Gelbach and Miller (2008) recommend the restricted wild cluster bootstrap. For tests of \\(\\mathbb{M}_{0}: r(\\beta)=0\\) let \\(\\widetilde{\\beta}\\) be a CLS or EMD estimator of \\(\\beta\\) subject to the restriction \\(r(\\widetilde{\\beta})=\\) 0. Let \\(\\widetilde{\\boldsymbol{e}}_{g}=\\boldsymbol{Y}_{g}-\\boldsymbol{X}_{g} \\widetilde{\\beta}\\) be the constrained cluster-level residuals. The restricted wild cluster bootstrap algorithm generates observations as\n\\[\n\\begin{aligned}\n\\boldsymbol{Y}_{g}^{*} &=\\boldsymbol{X}_{g} \\widetilde{\\beta}+\\boldsymbol{e}_{g}^{*} \\\\\n\\boldsymbol{e}_{g}^{*} &=\\widetilde{\\boldsymbol{e}}_{i} \\xi_{g}^{*}\n\\end{aligned}\n\\]\nOn each bootstrap sample the test statistic for \\(\\mathbb{M}_{0}\\) (t-ratio, Wald, LR, or F) is applied. Since the bootstrap algorithm satisfies \\(\\mathbb{M}_{0}\\) these statistics are centered at the hypothesized value. p-values are then calculated conventionally and used to assess the significance of the test statistic.\nThere are several reasons why conventional asymptotic approximations may work poorly with clustered observations. First, while the sample size \\(n\\) may be large, the effective sample size is the number of clusters \\(G\\). This is because when the dependence structure within each cluster is unconstrained the central limit theorem effectively treats each cluster as a single observation. Thus, if \\(G\\) is small we should treat inference as a small sample problem. Second, cluster-robust covariance matrix estimation explicitly treats each cluster as a single observation. Consequently the accuracy of normal approximations to tratios and Wald statistics is more accurately viewed as a small sample distribution problem. Third, when cluster sizes \\(n_{g}\\) are heterogeneous this means that the estimation problems just described also involve heterogeneous variances. Specifically, heterogeneous cluster sizes induces a high degree of effective heteroskedasticity (since the variance of a within-cluster sum is proportional to \\(n_{g}\\) ). When \\(G\\) is small this means that cluster-robust inference is similar to finite-sample inference with a small heteroskedastic sample. Fourth, interest often concerns treatment which is applied at the level of a cluster (such as the effect of tracking discussed in Section 4.21). If the number of treated clusters is small this is equivalent to estimation with a highly sparse dummy variable design in which case cluster-robust covariance matrix estimation can be unreliable.\nThese concerns suggest that conventional normal approximations may be poor in the context of clustered observations with a small number of groups \\(G\\), motivating the use of bootstrap methods. However, these concerns also can cause challenges with the accuracy of bootstrap approximations. When the number of clusters \\(G\\) is small, the cluster sizes \\(n_{g}\\) heterogeneous, or the number of treated clusters small, bootstrap methods may be inaccurate. In such cases inference should proceed cautiously.\nTo illustrate the use of the pairs cluster bootstrap, Table \\(10.4\\) reports the estimates of the example from Section \\(4.21\\) of the effect of tracking on testscores from Duflo, Dupas, and Kremer (2011). In addition to the asymptotic cluster standard error we report the cluster jackknife and cluster bootstrap standard errors as well as three percentile-type confidence intervals. We use 10,000 bootstrap replications. In this example the asymptotic, jackknife, and cluster bootstrap standard errors are identical, which reflects the good balance of this particular regression design.\nTable 10.4: Comparison of Methods for Estimate of Effect of Tracking\n\n\n\nCoefficient on Tracking\n\\(0.138\\)\n\n\n\n\nAsymptotic cluster s.e.\n\\((0.078)\\)\n\n\nJackknife cluster s.e.\n\\((0.078)\\)\n\n\nCluster Bootstrap s.e.\n\\((0.078)\\)\n\n\n\\(95 %\\) Percentile Interval\n\\([-0.013,0.291]\\)\n\n\n\\(95 % \\mathrm{BC}\\) Percentile Interval\n\\([-0.015,0.289]\\)\n\n\n\\(95 % \\mathrm{BC}_{a}\\) Percentile Interval\n\\([-0.018,0.286]\\)\n\n\n\nIn Stata, to obtain cluster bootstrap standard errors and confidence intervals use the options cluster (id) vce(bootstrap, reps \\(\\#\\) )) , where id is the cluster variable and # is the number of replications."
  },
  {
    "objectID": "chpt10-resample-method.html#technical-proofs",
    "href": "chpt10-resample-method.html#technical-proofs",
    "title": "10  Resampling Methods",
    "section": "10.31 Technical Proofs*",
    "text": "10.31 Technical Proofs*\nSome of the asymptotic results are facilitated by the following convergence result.\nTheorem 10.20 Marcinkiewicz WLLN If \\(u_{i}\\) are independent and uniformly integrable, then for any \\(r>\\) 1 , as \\(n \\rightarrow \\infty, n^{-r} \\sum_{i=1}^{n}\\left|u_{i}\\right|^{r} \\underset{p}{\\longrightarrow} 0\\).\nProof of Theorem \\(10.20\\)\n\\[\nn^{-r} \\sum_{i=1}^{n}\\left|u_{i}\\right|^{r} \\leq\\left(n^{-1} \\max _{1 \\leq i \\leq n}\\left|u_{i}\\right|\\right)^{r-1} \\frac{1}{n} \\sum_{i=1}^{n}\\left|u_{i}\\right| \\underset{p}{\\longrightarrow} 0\n\\]\nby the WLLN, Theorem \\(6.15\\), and \\(r>1\\).\nProof of Theorem 10.1 Fix \\(\\epsilon>0\\). Since \\(Z_{n} \\underset{p}{\\longrightarrow} Z\\) there is an \\(n\\) sufficiently large such that\n\\[\n\\mathbb{P}\\left[\\left\\|Z_{n}-Z\\right\\|>\\epsilon\\right]<\\epsilon .\n\\]\nSince the event \\(\\left\\|Z_{n}-Z\\right\\|>\\epsilon\\) is non-random under the conditional probability \\(\\mathbb{P}^{*}\\), for such \\(n\\),\n\\[\n\\mathbb{P}^{*}\\left[\\left\\|Z_{n}-Z\\right\\|>\\epsilon\\right]=\\left\\{\\begin{array}{cc}\n0 & \\text { with probability exceeding } 1-\\epsilon \\\\\n1 & \\text { with probability less than } \\epsilon .\n\\end{array}\\right.\n\\]\nSince \\(\\varepsilon\\) is arbitrary we conclude \\(\\mathbb{P}^{*}\\left[\\left\\|Z_{n}-Z\\right\\|>\\epsilon\\right] \\underset{p}{\\longrightarrow} 0\\) as required.\nProof of Theorem 10.2 Fix \\(\\epsilon>0\\). By Markov’s inequality (B.36), the facts (10.12) and (10.13), and finally the Marcinkiewicz WLLN (Theorem 10.20) with \\(r=2\\) and \\(u_{i}=\\left\\|Y_{i}\\right\\|\\),\n\\[\n\\begin{aligned}\n\\mathbb{P}^{*}\\left[\\left\\|\\bar{Y}^{*}-\\bar{Y}\\right\\|>\\epsilon\\right] & \\leq \\epsilon^{-2} \\mathbb{E}^{*}\\left\\|\\bar{Y}^{*}-\\bar{Y}\\right\\|^{2} \\\\\n&=\\epsilon^{-2} \\operatorname{tr}\\left(\\operatorname{var}^{*}\\left[\\bar{Y}^{*}\\right]\\right) \\\\\n&=\\epsilon^{-2} \\operatorname{tr}\\left(\\frac{1}{n} \\widehat{\\Sigma}\\right) \\\\\n& \\leq \\epsilon^{-2} n^{-2} \\sum_{i=1}^{n} Y_{i}^{\\prime} Y_{i} \\\\\n& \\underset{p}{ } 0\n\\end{aligned}\n\\]\nThis establishes that \\(\\bar{Y}^{*}-\\bar{Y} \\underset{p^{*}}{\\longrightarrow} 0\\).\nSince \\(\\bar{Y}-\\mu \\underset{p}{\\longrightarrow} 0\\) by the WLLN, \\(\\bar{Y}-\\mu \\underset{p^{*}}{\\longrightarrow} 0\\) by Theorem 10.1. Since \\(\\bar{Y}^{*}-\\mu=\\bar{Y}^{*}-\\bar{Y}+\\bar{Y}-\\mu\\), we deduce that \\(\\bar{Y}^{*}-\\mu \\underset{p^{*}}{ } 0\\).\nProof of Theorem 10.4 We verify conditions for the multivariate Lindeberg CLT (Theorem 6.4). (We cannot use the Lindeberg-Lévy CLT because the conditional distribution depends on \\(n\\).) Conditional on \\(F_{n}\\), the bootstrap draws \\(Y_{i}^{*}-\\bar{Y}\\) are i.i.d. with mean 0 and covariance matrix \\(\\widehat{\\Sigma}\\). Set \\(v_{n}^{2}=\\lambda_{\\min }(\\widehat{\\Sigma})\\). Note that by the WLLN, \\(v_{n}^{2} \\underset{p}{\\rightarrow} v^{2}=\\lambda_{\\min }(\\Sigma)>0\\). Thus for \\(n\\) sufficiently large, \\(v_{n}^{2}>0\\) with high probability. Fix \\(\\epsilon>0\\). Equation (6.2) equals\n\\[\n\\begin{aligned}\n\\frac{1}{n v_{n}^{2}} \\sum_{i=1}^{n} \\mathbb{E}^{*}\\left[\\left\\|Y_{i}^{*}-\\bar{Y}\\right\\|^{2} \\mathbb{1}\\left\\{\\left\\|Y_{i}^{*}-\\bar{Y}\\right\\|^{2} \\geq \\epsilon n v_{n}^{2}\\right\\}\\right] &=\\frac{1}{v_{n}^{2}} \\mathbb{E}^{*}\\left[\\left\\|Y_{i}^{*}-\\bar{Y}\\right\\|^{2} \\mathbb{1}\\left\\{\\left\\|Y_{i}^{*}-\\bar{Y}\\right\\|^{2} \\geq \\epsilon n v_{n}^{2}\\right\\}\\right] \\\\\n& \\leq \\frac{1}{\\epsilon n v_{n}^{4}} \\mathbb{E}^{*}\\left\\|Y_{i}^{*}-\\bar{Y}\\right\\|^{4} \\\\\n& \\leq \\frac{2^{4}}{\\epsilon n v_{n}^{4}} \\mathbb{E}^{*}\\left\\|Y_{i}^{*}\\right\\|^{4} \\\\\n&=\\frac{2^{4}}{\\epsilon n^{2} v_{n}^{4}} \\sum_{i=1}^{n}\\left\\|Y_{i}\\right\\|^{4} \\\\\n& \\longrightarrow 0 .\n\\end{aligned}\n\\]\nThe second inequality uses Minkowski’s inequality (B.34), Liapunov’s inequality (B.35), and the \\(c_{r}\\) inequality (B.6). The following equality is \\(\\mathbb{E}^{*}\\left\\|Y_{i}^{*}\\right\\|^{4}=n^{-1} \\sum_{i=1}^{n}\\left\\|Y_{i}\\right\\|^{4}\\), which is similar to (10.10). The final convergence holds by the Marcinkiewicz WLLN (Theorem 10.20) with \\(r=2\\) and \\(u_{i}=\\left\\|Y_{i}\\right\\|^{2}\\). The conditions for Theorem \\(6.4\\) hold and we conclude\n\\[\n\\widehat{\\Sigma}^{-1 / 2} \\sqrt{n}\\left(\\bar{Y}^{*}-\\bar{Y}\\right) \\underset{d^{*}}{\\longrightarrow} \\mathrm{N}(0, \\boldsymbol{I}) .\n\\]\nSince \\(\\widehat{\\Sigma} \\underset{p^{*}}{\\longrightarrow} \\Sigma\\) we deduce that \\(\\sqrt{n}\\left(\\bar{Y}^{*}-\\bar{Y}\\right) \\underset{d^{*}}{\\longrightarrow} \\mathrm{N}(0, \\Sigma)\\) as claimed.\nProof of Theorem \\(10.10\\) For notational simplicity assume \\(\\theta\\) and \\(\\mu\\) are scalar. Set \\(h_{i}=h\\left(Y_{i}\\right)\\). The assumption that the \\(p^{t h}\\) derivative of \\(g(u)\\) is bounded implies \\(\\left|g^{(p)}(u)\\right| \\leq C\\) for some \\(C<\\infty\\). Taking a \\(p^{\\text {th }}\\) order Taylor series expansion\n\\[\n\\widehat{\\theta}^{*}-\\widehat{\\theta}=g\\left(\\bar{h}^{*}\\right)-g(\\bar{h})=\\sum_{j=1}^{p-1} \\frac{g^{(j)}(\\bar{h})}{j !}\\left(\\bar{h}^{*}-\\bar{h}\\right)^{j}+\\frac{g^{(p)}\\left(\\zeta_{n}^{*}\\right)}{p !}\\left(\\bar{h}^{*}-\\bar{h}\\right)^{p}\n\\]\nwhere \\(\\zeta_{n}^{*}\\) lies between \\(\\bar{h}^{*}\\) and \\(\\bar{h}\\). This implies\n\\[\n\\left|z_{n}^{*}\\right|=\\sqrt{n}\\left|\\widehat{\\theta}^{*}-\\widehat{\\theta}\\right| \\leq \\sqrt{n} \\sum_{j=1}^{p} c_{j} \\mid \\bar{h}^{*}-\\bar{h}^{j}\n\\]\nwhere \\(c_{j}=\\left|g^{(j)}(\\bar{h})\\right| / j\\) ! for \\(j<p\\) and \\(c_{p}=C / p\\) !. We find that the fourth central moment of the normalized bootstrap estimator \\(Z_{n}^{*}=\\sqrt{n}\\left(\\widehat{\\theta}^{*}-\\widehat{\\theta}\\right)\\) satisfies the bound\n\\[\n\\mathbb{E}^{*}\\left[Z_{n}^{* 4}\\right] \\leq \\sum_{r=4}^{4 p} a_{r} n^{2} \\mathbb{E}^{*}\\left|\\bar{h}^{*}-\\bar{h}\\right|^{r}\n\\]\nwhere the coefficients \\(a_{r}\\) are products of the coefficients \\(c_{j}\\) and hence each \\(O_{p}(1)\\). We see that \\(\\mathbb{E}^{*}\\left[Z_{n}^{* 4}\\right]=\\) \\(O_{p}(1)\\) if \\(n^{2} \\mathbb{E}^{*}\\left|\\bar{h}^{*}-\\bar{h}\\right|^{r}=O_{p}(1)\\) for \\(r=4, \\ldots, 4 p\\).\nWe show this holds for any \\(r \\geq 4\\) using Rosenthal’s inequality (B.50), which states that for each \\(r\\) there is a constant \\(R_{r}<\\infty\\) such that\n\\[\n\\begin{aligned}\nn^{2} \\mathbb{E}^{*}\\left|\\bar{h}^{*}-\\bar{h}\\right|^{r} &=n^{2-r_{\\mathbb{E}}}\\left|\\sum_{i=1}^{n}\\left(h_{i}^{*}-\\bar{h}\\right)\\right|^{r} \\\\\n& \\leq n^{2-r} R_{r}\\left\\{\\left(n \\mathbb{E}^{*}\\left(h_{i}^{*}-\\bar{h}\\right)^{2}\\right)^{r / 2}+n \\mathbb{E}^{*}\\left|h_{i}^{*}-\\bar{h}\\right|^{r}\\right\\} \\\\\n&=R_{r}\\left\\{n^{2-r / 2} \\widehat{\\sigma}^{r}+\\frac{1}{n^{r-2}} \\sum_{i=1}^{n}\\left|h_{i}-\\bar{h}\\right|^{r}\\right\\}\n\\end{aligned}\n\\]\nSince \\(\\mathbb{E}\\left[h_{i}^{2}\\right]<\\infty, \\widehat{\\sigma}^{2}=O_{p}(1)\\), so the first term in (10.36) is \\(O_{p}(1)\\). Also, by the Marcinkiewicz WLLN (Theorem 10.20), \\(n^{-r / 2} \\sum_{i=1}^{n}\\left|h_{i}-\\bar{h}\\right|^{r}=o_{p}\\) (1) for any \\(r \\geq 1\\), so the second term in (10.36) is \\(o_{p}(1)\\) for \\(r \\geq 4\\). Thus for all \\(r \\geq 4,(10.36)\\) is \\(O_{p}(1)\\) and thus (10.35) is \\(O_{p}(1)\\). We deduce that \\(Z_{n}^{*}\\) is uniformly square integrable, and the bootstrap estimate of variance is consistent.\nThis argument can be extended to vector-valued means and estimates.\nProof of Theorem 10.12 We show that \\(\\mathbb{E}^{*}\\left\\|Z_{n}^{* *}\\right\\|^{4}=O_{p}(1)\\). Theorem \\(6.13\\) shows that \\(Z_{n}^{* *}\\) is uniformly square integrable. Since \\(Z_{n}^{* *} \\underset{d^{*}}{\\longrightarrow} Z\\), Theorem \\(6.14\\) implies that \\(\\operatorname{var}\\left[Z_{n}^{* *}\\right] \\rightarrow \\operatorname{var}[Z]=V_{\\beta}\\) as stated.\nSet \\(h_{i}=h\\left(Y_{i}\\right)\\). Since \\(\\boldsymbol{G}(x)=\\frac{\\partial}{\\partial x} g(x)^{\\prime}\\) is continuous in a neighborhood of \\(\\mu\\), there exists \\(\\eta>0\\) and \\(M<\\infty\\) such that \\(\\|x-\\mu\\| \\leq 2 \\eta\\) implies \\(\\operatorname{tr}\\left(\\boldsymbol{G}(x)^{\\prime} \\boldsymbol{G}(x)\\right) \\leq M\\). By the WLLN and bootstrap WLLN there is an \\(n\\) sufficiently large such that \\(\\left\\|\\bar{h}_{n}-\\mu\\right\\| \\leq \\eta\\) and \\(\\left\\|\\bar{h}_{n}^{*}-\\bar{h}_{n}\\right\\| \\leq \\eta\\) with probability exceeding \\(1-\\eta\\). On this event, \\(\\left\\|x-\\bar{h}_{n}\\right\\| \\leq \\eta\\) implies \\(\\operatorname{tr}\\left(\\boldsymbol{G}(x)^{\\prime} \\boldsymbol{G}(x)\\right) \\leq M\\). Using the mean-value theorem at a point \\(\\zeta_{n}^{*}\\) intermediate between \\(\\bar{h}_{n}^{*}\\) and \\(\\bar{h}_{n}\\)\n\\[\n\\begin{aligned}\n\\left\\|Z_{n}^{* *}\\right\\|^{4} \\mathbb{1}\\left\\{\\left\\|\\bar{h}_{n}^{*}-\\bar{h}_{n}\\right\\| \\leq \\eta\\right\\} & \\leq n^{2}\\left\\|g\\left(\\bar{h}_{n}^{*}\\right)-g\\left(\\bar{h}_{n}\\right)\\right\\|^{4} \\mathbb{1}\\left\\{\\left\\|\\bar{h}_{n}^{*}-\\bar{h}_{n}\\right\\| \\leq \\eta\\right\\} \\\\\n& \\leq n^{2}\\left\\|\\boldsymbol{G}\\left(\\zeta_{n}^{*}\\right)^{\\prime}\\left(\\bar{h}_{n}^{*}-\\bar{h}_{n}\\right)\\right\\|^{4} \\\\\n& \\leq M^{2} n^{2}\\left\\|\\bar{h}_{n}^{*}-\\bar{h}_{n}\\right\\|^{4} .\n\\end{aligned}\n\\]\nThen\n\\[\n\\begin{aligned}\n\\mathbb{E}^{*}\\left\\|Z_{n}^{* *}\\right\\|^{4} & \\leq \\mathbb{E}^{*}\\left[\\left\\|Z_{n}^{* *}\\right\\|^{4} \\mathbb{1}\\left\\{\\left\\|\\bar{h}_{n}^{*}-\\bar{h}_{n}\\right\\| \\leq \\eta\\right\\}\\right]+\\tau_{n}^{4} \\mathbb{E}^{*}\\left[\\mathbb{1}\\left\\{\\left\\|\\bar{h}_{n}^{*}-\\bar{h}_{n}\\right\\|>\\eta\\right\\}\\right] \\\\\n& \\leq M^{2} n^{2} \\mathbb{E}^{*}\\left\\|\\bar{h}_{n}^{*}-\\bar{h}_{n}\\right\\|^{4}+\\tau_{n}^{4} \\mathbb{P}^{*}\\left(\\left\\|\\bar{h}_{n}^{*}-\\bar{h}_{n}\\right\\|>\\eta\\right) .\n\\end{aligned}\n\\]\nIn (10.17) we showed that the first term in (10.37) is \\(O_{p}(1)\\) in the scalar case. The vector case follows by element-by-element expansion.\nNow take the second term in (10.37). We apply Bernstein’s inequality for vectors (B.41). Note that \\(\\bar{h}_{n}^{*}-\\bar{h}_{n}=n^{-1} \\sum_{i=1}^{n} u_{i}^{*}\\) with \\(u_{i}^{*}=h_{i}^{*}-\\bar{h}_{n}\\) and \\(j^{t h}\\) element \\(u_{j i}^{*}=h_{j i}^{*}-\\bar{h}_{j n}\\). The \\(u_{i}^{*}\\) are i.i.d., mean zero, \\(\\mathbb{E}^{*}\\left[u_{j i}^{* 2}\\right]=\\widehat{\\sigma}_{j}^{2}=O_{p}(1)\\), and satisfy the bound \\(\\left|u_{j i}^{*}\\right| \\leq 2 \\max _{i, j}\\left|h_{j i}\\right|=B_{n}\\), say. Bernstein’s inequality states that\n\\[\n\\mathbb{P}^{*}\\left[\\left\\|\\bar{h}_{n}^{*}-\\bar{h}_{n}\\right\\|>\\eta\\right] \\leq 2 m \\exp \\left(-n^{1 / 2} \\frac{\\eta^{2}}{2 m^{2} n^{-1 / 2} \\max _{j} \\widehat{\\sigma}_{j}^{2}+2 m n^{-1 / 2} B_{n} \\eta / 3}\\right) .\n\\]\nTheorem \\(6.15\\) shows that \\(n^{-1 / 2} B_{n}=o_{p}(1)\\). Thus the expression in the denominator of the parentheses in (10.38) is \\(o_{p}\\) (1) as \\(n \\rightarrow \\infty\\), . It follows that for \\(n\\) sufficiently large (10.38) is \\(O_{p}\\left(\\exp \\left(-n^{1 / 2}\\right)\\right)\\). Hence the second term in (10.37) is \\(O_{p}\\left(\\exp \\left(-n^{1 / 2}\\right)\\right) o_{p}\\left(\\exp \\left(-n^{1 / 2}\\right)\\right)=o_{p}(1)\\) by the assumption on \\(\\tau_{n}\\).\nWe have shown that the two terms in (10.37) are each \\(O_{p}(1)\\). This completes the proof."
  },
  {
    "objectID": "chpt10-resample-method.html#exercises",
    "href": "chpt10-resample-method.html#exercises",
    "title": "10  Resampling Methods",
    "section": "10.32 Exercises",
    "text": "10.32 Exercises\nExercise 10.1 Find the jackknife estimator of variance of the estimator \\(\\widehat{\\mu}_{r}=n^{-1} \\sum_{i=1}^{n} Y_{i}^{r}\\) for \\(\\mu_{r}=\\mathbb{E}\\left[Y_{i}^{r}\\right]\\).\nExercise 10.2 Show that if the jackknife estimator of variance of \\(\\widehat{\\beta}\\) is \\(\\widehat{V}_{\\widehat{\\beta}}^{\\text {jack }}\\), then the jackknife estimator of variance of \\(\\widehat{\\theta}=\\boldsymbol{a}+\\boldsymbol{C} \\widehat{\\beta}\\) is \\(\\widehat{\\boldsymbol{V}}_{\\widehat{\\theta}}^{\\mathrm{jack}}=\\boldsymbol{C} \\widehat{\\boldsymbol{V}}_{\\widehat{\\beta}}^{\\mathrm{jack}} \\boldsymbol{C}^{\\prime}\\).\nExercise 10.3 A two-step estimator such as (12.49) is \\(\\widehat{\\beta}=\\left(\\sum_{i=1}^{n} \\widehat{W}_{i} \\widehat{W}_{i}^{\\prime}\\right)^{-1}\\left(\\sum_{i=1}^{n} \\widehat{W}_{i} Y_{i}\\right)\\) where \\(\\widehat{W}_{i}=\\widehat{A}^{\\prime} Z_{i}\\) and \\(\\widehat{\\boldsymbol{A}}=\\left(\\boldsymbol{Z}^{\\prime} \\boldsymbol{Z}\\right)^{-1} \\boldsymbol{Z}^{\\prime} \\boldsymbol{X}\\). Describe how to construct the jackknife estimator of variance of \\(\\widehat{\\beta}\\).\nExercise 10.4 Show that if the bootstrap estimator of variance of \\(\\widehat{\\beta}\\) is \\(\\widehat{V}_{\\widehat{\\beta}}^{\\text {boot }}\\), then the bootstrap estimator of variance of \\(\\widehat{\\theta}=\\boldsymbol{a}+\\boldsymbol{C} \\widehat{\\beta}\\) is \\(\\widehat{\\boldsymbol{V}}_{\\widehat{\\theta}}^{\\text {boot }}=\\boldsymbol{C} \\widehat{\\boldsymbol{V}}_{\\widehat{\\beta}}^{\\text {boot }} \\boldsymbol{C}^{\\prime}\\).\nExercise \\(10.5\\) Show that if the percentile interval for \\(\\beta\\) is \\([L, U]\\) then the percentile interval for \\(a+c \\beta\\) is \\([a+c L, a+c U]\\).\nExercise \\(10.6\\) Consider the following bootstrap procedure. Using the nonparametric bootstrap, generate bootstrap samples, calculate the estimate \\(\\widehat{\\theta}^{*}\\) on these samples and then calculate\n\\[\nT^{*}=\\left(\\widehat{\\theta}^{*}-\\widehat{\\theta}\\right) / s(\\widehat{\\theta}),\n\\]\nwhere \\(s(\\hat{\\theta})\\) is the standard error in the original data. Let \\(q_{\\alpha / 2}^{*}\\) and \\(q_{1-\\alpha / 2}^{*}\\) denote the \\(\\alpha / 2^{t h}\\) and \\(1-\\alpha / 2^{t h}\\) quantiles of \\(T^{*}\\), and define the bootstrap confidence interval\n\\[\nC=\\left[\\widehat{\\theta}+s(\\widehat{\\theta}) q_{\\alpha / 2}^{*}, \\quad \\widehat{\\theta}+s(\\widehat{\\theta}) q_{1-\\alpha / 2}^{*}\\right] .\n\\]\nShow that \\(C\\) exactly equals the percentile interval. Exercise \\(10.7\\) Prove Theorem 10.6.\nExercise \\(10.8\\) Prove Theorem 10.7.\nExercise \\(10.9\\) Prove Theorem 10.8.\nExercise \\(10.10\\) Let \\(Y_{i}\\) be i.i.d., \\(\\mu=\\mathbb{E}[Y]>0\\), and \\(\\theta=\\mu^{-1}\\). Let \\(\\widehat{\\mu}=\\bar{Y}_{n}\\) be the sample mean and \\(\\widehat{\\theta}=\\widehat{\\mu}^{-1}\\).\n\nIs \\(\\hat{\\theta}\\) unbiased for \\(\\theta\\) ?\nIf \\(\\widehat{\\theta}\\) is biased, can you determine the direction of the bias \\(\\mathbb{E}[\\widehat{\\theta}-\\theta]\\) (up or down)?\nIs the percentile interval appropriate in this context for confidence interval construction?\n\nExercise \\(10.11\\) Consider the following bootstrap procedure for a regression of \\(Y\\) on \\(X\\). Let \\(\\widehat{\\beta}\\) denote the OLS estimator and \\(\\widehat{e}_{i}=Y_{i}-X_{i}^{\\prime} \\widehat{\\beta}\\) the OLS residuals.\n\nDraw a random vector \\(\\left(X^{*}, e^{*}\\right)\\) from the pair \\(\\left\\{\\left(X_{i}, \\widehat{e}_{i}\\right): i=1, \\ldots, n\\right\\}\\). That is, draw a random integer \\(i^{\\prime}\\) from \\([1,2, \\ldots, n]\\), and set \\(X^{*}=X_{i^{\\prime}}\\) and \\(e^{*}=\\widehat{e}_{i^{\\prime}}\\). Set \\(Y^{*}=X^{* \\prime} \\widehat{\\beta}+e^{*}\\). Draw (with replacement) \\(n\\) such vectors, creating a random bootstrap data set \\(\\left(\\boldsymbol{Y}^{*}, \\boldsymbol{X}^{*}\\right)\\).\nRegress \\(\\boldsymbol{Y}^{*}\\) on \\(\\boldsymbol{X}^{*}\\), yielding OLS estimator \\(\\widehat{\\beta}^{*}\\) and any other statistic of interest.\n\nShow that this bootstrap procedure is (numerically) identical to the nonparametric bootstrap.\nExercise \\(10.12\\) Take \\(p^{*}\\) as defined in (10.22) for the BC percentile interval. Show that it is invariant to replacing \\(\\theta\\) with \\(g(\\theta)\\) for any strictly monotonically increasing transformation \\(g(\\theta)\\). Does this extend to \\(z_{0}^{*}\\) as defined in (10.23)?\nExercise \\(10.13\\) Show that if the percentile-t interval for \\(\\beta\\) is \\([L, U]\\) then the percentile-t interval for \\(a+c \\beta\\) is \\([a+b L, a+b U]\\).\nExercise 10.14 You want to test \\(\\mathbb{M}_{0}: \\theta=0\\) against \\(\\mathbb{M}_{1}: \\theta>0\\). The test for \\(\\mathbb{M}_{0}\\) is to reject if \\(T_{n}=\\widehat{\\theta} / s(\\widehat{\\theta})>c\\) where \\(c\\) is picked so that Type I error is \\(\\alpha\\). You do this as follows. Using the nonparametric bootstrap, you generate bootstrap samples, calculate the estimates \\(\\widehat{\\theta}^{*}\\) on these samples and then calculate \\(T^{*}=\\) \\(\\widehat{\\theta}^{*} / s\\left(\\widehat{\\theta}^{*}\\right)\\). Let \\(q_{1-\\alpha}^{*}\\) denote the \\(1-\\alpha^{t h}\\) quantile of \\(T^{*}\\). You replace \\(c\\) with \\(q_{1-\\alpha}^{*}\\), and thus reject \\(\\mathbb{H}_{0}\\) if \\(T_{n}=\\widehat{\\theta} / s(\\widehat{\\theta})>q_{1-\\alpha}^{*}\\). What is wrong with this procedure?\nExercise 10.15 Suppose that in an application, \\(\\widehat{\\theta}=1.2\\) and \\(s(\\widehat{\\theta})=0.2\\). Using the nonparametric bootstrap, 1000 samples are generated from the bootstrap distribution, and \\(\\widehat{\\theta}^{*}\\) is calculated on each sample. The \\(\\widehat{\\theta}^{*}\\) are sorted, and the \\(0.025^{t h}\\) and \\(0.975^{t h}\\) quantiles of the \\(\\widehat{\\theta}^{*}\\) are \\(.75\\) and \\(1.3\\), respectively.\n\nReport the \\(95 %\\) percentile interval for \\(\\theta\\).\nWith the given information, can you calculate the 95% BC percentile interval or percentile-t interval for \\(\\theta\\) ?\n\nExercise 10.16 Take the normal regression model \\(Y=X^{\\prime} \\beta+e\\) with \\(e \\mid X \\sim \\mathrm{N}\\left(0, \\sigma^{2}\\right)\\) where we know the MLE equals the least squares estimators \\(\\widehat{\\beta}\\) and \\(\\widehat{\\sigma}^{2}\\).\n\nDescribe the parametric regression bootstrap for this model. Show that the conditional distribution of the bootstrap observations is \\(Y_{i}^{*} \\mid F_{n} \\sim \\mathrm{N}\\left(X_{i}^{\\prime} \\widehat{\\beta}, \\widehat{\\sigma}^{2}\\right)\\). (b) Show that the distribution of the bootstrap least squares estimator is \\(\\widehat{\\beta}^{*} \\mid F_{n} \\sim \\mathrm{N}\\left(\\widehat{\\beta},\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\widehat{\\sigma}^{2}\\right)\\).\nShow that the distribution of the bootstrap t-ratio with a homoskedastic standard error is \\(T^{*} \\sim\\) \\(t_{n-k}\\).\n\nExercise \\(10.17\\) Consider the model \\(Y=X^{\\prime} \\beta+e\\) with \\(\\mathbb{E}[e \\mid X]=0, Y\\) scalar, and \\(X\\) a \\(k\\) vector. You have a random sample \\(\\left(Y_{i}, X_{i}: i=1, \\ldots, n\\right)\\). You are interested in estimating the regression function \\(m(x)=\\) \\(\\mathbb{E}[Y \\mid X=x]\\) at a fixed vector \\(x\\) and constructing a \\(95 %\\) confidence interval.\n\nWrite down the standard estimator and asymptotic confidence interval for \\(m(x)\\).\nDescribe the percentile bootstrap confidence interval for \\(m(x)\\).\nDescribe the percentile-t bootstrap confidence interval for \\(m(x)\\).\n\nExercise 10.18 The observed data is \\(\\left\\{Y_{i}, X_{i}\\right\\} \\in \\mathbb{R} \\times \\mathbb{R}^{k}, k>1, i=1, \\ldots, n\\). Take the model \\(Y=X^{\\prime} \\beta+e\\) with \\(\\mathbb{E}[X e]=0 .\\)\n\nWrite down an estimator for \\(\\mu_{3}=\\mathbb{E}\\left[e^{3}\\right]\\).\nExplain how to use the percentile method to construct a 90% confidence interval for \\(\\mu_{3}\\) in this specific model.\n\nExercise \\(10.19\\) Take the model \\(Y=X^{\\prime} \\beta+e\\) with \\(\\mathbb{E}[X e]=0\\). Describe the bootstrap percentile confidence interval for \\(\\sigma^{2}=\\mathbb{E}\\left[e^{2}\\right]\\).\nExercise 10.20 The model is \\(Y=X_{1}^{\\prime} \\beta_{1}+X_{2}^{\\prime} \\beta_{2}+e\\) with \\(\\mathbb{E}[X e]=0\\) and \\(X_{2}\\) scalar. Describe how to test \\(\\mathbb{H}_{0}: \\beta_{2}=0\\) against \\(\\mathbb{H}_{1}: \\beta_{2} \\neq 0\\) using the nonparametric bootstrap.\nExercise 10.21 The model is \\(Y=X_{1}^{\\prime} \\beta_{1}+X_{2}^{\\prime} \\beta_{2}+e\\) with \\(\\mathbb{E}[X e]=0\\), and both \\(X_{1}\\) and \\(X_{2} k \\times 1\\). Describe how to test \\(\\mathbb{M}_{0}: \\beta_{1}=\\beta_{2}\\) against \\(\\mathbb{M}_{1}: \\beta_{1} \\neq \\beta_{2}\\) using the nonparametric bootstrap.\nExercise 10.22 Suppose a Ph.D. student has a sample \\(\\left(Y_{i}, X_{i}, Z_{i}: i=1, \\ldots, n\\right)\\) and estimates by OLS the equation \\(Y=Z \\alpha+X^{\\prime} \\beta+e\\) where \\(\\alpha\\) is the coefficient of interest. She is interested in testing \\(\\mathbb{H}_{0}: \\alpha=0\\) against \\(\\mathbb{H}_{1}: \\alpha \\neq 0\\). She obtains \\(\\widehat{\\alpha}=2.0\\) with standard error \\(s(\\widehat{\\alpha})=1.0\\) so the value of the t-ratio for \\(\\mathbb{H}_{0}\\) is \\(T=\\widehat{\\alpha} / s(\\widehat{\\alpha})=2.0\\). To assess significance, the student decides to use the bootstrap. She uses the following algorithm\n\nSamples \\(\\left(Y_{i}^{*}, X_{i}^{*}, Z_{i}^{*}\\right)\\) randomly from the observations. (Random sampling with replacement). Creates a random sample with \\(n\\) observations.\nOn this pseudo-sample, estimates the equation \\(Y_{i}^{*}=Z_{i}^{*} \\alpha+X_{i}^{* \\prime} \\beta+e_{i}^{*}\\) by OLS and computes standard errors, including \\(s\\left(\\widehat{\\alpha}^{*}\\right)\\). The t-ratio for \\(\\mathbb{H}_{0}, T^{*}=\\widehat{\\alpha}^{*} / s\\left(\\widehat{\\alpha}^{*}\\right)\\) is computed and stored.\nThis is repeated \\(B=10,000\\) times.\nThe \\(0.95^{t h}\\) empirical quantile \\(q_{.95}^{*}=3.5\\) of the bootstrap absolute t-ratios \\(\\left|T^{*}\\right|\\) is computed.\nThe student notes that while \\(|T|=2>1.96\\) (and thus an asymptotic \\(5 %\\) size test rejects \\(\\mathbb{M}_{0}\\) ), \\(|T|=\\) \\(2<q_{.95}^{*}=3.5\\) and thus the bootstrap test does not reject \\(\\mathbb{M}_{0}\\). As the bootstrap is more reliable, the student concludes that \\(\\mathbb{M}_{0}\\) cannot be rejected in favor of \\(\\mathbb{H}_{1}\\). Question: Do you agree with the student’s method and reasoning? Do you see an error in her method?\n\nExercise 10.23 Take the model \\(Y=X_{1} \\beta_{1}+X_{2} \\beta_{2}+e\\) with \\(\\mathbb{E}[X e]=0\\) and scalar \\(X_{1}\\) and \\(X_{2}\\). The parameter of interest is \\(\\theta=\\beta_{1} \\beta_{2}\\). Show how to construct a confidence interval for \\(\\theta\\) using the following three methods.\n\nAsymptotic Theory.\nPercentile Bootstrap.\nPercentile-t Bootstrap.\n\nYour answer should be specific to this problem, not general.\nExercise 10.24 Take the model \\(Y=X_{1} \\beta_{1}+X_{2} \\beta_{2}+e\\) with i.i.d observations, \\(\\mathbb{E}[X e]=0\\) and scalar \\(X_{1}\\) and \\(X_{2}\\). Describe how you would construct the percentile-t bootstrap confidence interval for \\(\\theta=\\beta_{1} / \\beta_{2}\\).\nExercise 10.25 The model is i.i.d. data, \\(i=1, \\ldots, n, Y=X^{\\prime} \\beta+e\\) and \\(\\mathbb{E}[e \\mid X]=0\\). Does the presence of conditional heteroskedasticity invalidate the application of the nonparametric bootstrap? Explain.\nExercise 10.26 The RESET specification test for nonlinearity in a random sample (due to Ramsey (1969)) is the following. The null hypothesis is a linear regression \\(Y=X^{\\prime} \\beta+e\\) with \\(\\mathbb{E}[e \\mid X]=0\\). The parameter \\(\\beta\\) is estimated by OLS yielding predicted values \\(\\widehat{Y}_{i}\\). Then a second-stage least squares regression is estimated including both \\(X_{i}\\) and \\(\\widehat{Y}_{i}\\)\n\\[\nY_{i}=X_{i}^{\\prime} \\widetilde{\\beta}+\\left(\\widehat{Y}_{i}\\right)^{2} \\widetilde{\\gamma}+\\widetilde{e}_{i}\n\\]\nThe RESET test statistic \\(R\\) is the squared t-ratio on \\(\\widetilde{\\gamma}\\).\nA colleague suggests obtaining the critical value for the test using the bootstrap. He proposes the following bootstrap implementation.\n\nDraw \\(n\\) observations \\(\\left(Y_{i}^{*}, X_{i}^{*}\\right)\\) randomly from the observed sample pairs \\(\\left(Y_{i}, X_{i}\\right)\\) to create a bootstrap sample.\nCompute the statistic \\(R^{*}\\) on this bootstrap sample as described above.\nRepeat this \\(B\\) times. Sort the bootstrap statistics \\(R^{*}\\), take the \\(0.95^{t h}\\) quantile and use this as the critical value.\nReject the null hypothesis if \\(R\\) exceeds this critical value, otherwise do not reject.\n\nIs this procedure a correct implementation of the bootstrap in this context? If not, propose a modification.\nExercise 10.27 The model is \\(Y=X^{\\prime} \\beta+e\\) with \\(\\mathbb{E}[X e] \\neq 0\\). We know that in this case, the least squares estimator may be biased for the parameter \\(\\beta\\). We also know that the nonparametric BC percentile interval is (generally) a good method for confidence interval construction in the presence of bias. Explain whether or not you expect the BC percentile interval applied to the least squares estimator will have accurate coverage in this context.\nExercise 10.28 In Exercise 9.26 you estimated a cost function for 145 electric companies and tested the restriction \\(\\theta=\\beta_{3}+\\beta_{4}+\\beta_{5}=1\\). (a) Estimate the regression by unrestricted least squares and report standard errors calculated by asymptotic, jackknife and the bootstrap.\n\nEstimate \\(\\theta=\\beta_{3}+\\beta_{4}+\\beta_{5}\\) and report standard errors calculated by asymptotic, jackknife and the bootstrap.\nReport confidence intervals for \\(\\theta\\) using the percentile and \\(\\mathrm{BC}_{a}\\) methods.\n\nExercise 10.29 In Exercise 9.27 you estimated the Mankiw, Romer, and Weil (1992) unrestricted regression. Let \\(\\theta\\) be the sum of the second, third, and fourth coefficients.\n\nEstimate the regression by unrestricted least squares and report standard errors calculated by asymptotic, jackknife and the bootstrap.\nEstimate \\(\\theta\\) and report standard errors calculated by asymptotic, jackknife and the bootstrap.\nReport confidence intervals for \\(\\theta\\) using the percentile and BC methods.\n\nExercise 10.30 In Exercise \\(7.28\\) you estimated a wage regression with the cps09mar dataset and the subsample of white Male Hispanics. Further restrict the sample to those never-married and live in the Midwest region. (This sample has 99 observations.) As in subquestion (b) let \\(\\theta\\) be the ratio of the return to one year of education to the return of one year of experience.\n\nEstimate \\(\\theta\\) and report standard errors calculated by asymptotic, jackknife and the bootstrap.\nExplain the discrepancy between the standard errors.\nReport confidence intervals for \\(\\theta\\) using the BC percentile method.\n\nExercise 10.31 In Exercise \\(4.26\\) you extended the work from Duflo, Dupas, and Kremer (2011). Repeat that regression, now calculating the standard error by cluster bootstrap. Report a \\(\\mathrm{BC}_{a}\\) confidence interval for each coefficient."
  },
  {
    "objectID": "chpt11-multi-reg.html",
    "href": "chpt11-multi-reg.html",
    "title": "11  Multivariate Regression",
    "section": "",
    "text": "Multivariate regression is a system of regression equations. Multivariate regression is used as reduced form models for instrumental variable estimation (Chaper 12), vector autoregressions (Chapter 15), demand systems (demand for multiple goods), and other contexts.\nMultivariate regression is also called by the name systems of regression equations. Closely related is the method of Seemingly Unrelated Regressions (SUR) introduced in Section 11.7.\nMost of the tools of single equation regression generalize to multivariate regression. A major difference is a new set of notation to handle matrix estimators."
  },
  {
    "objectID": "chpt11-multi-reg.html#regression-systems",
    "href": "chpt11-multi-reg.html#regression-systems",
    "title": "11  Multivariate Regression",
    "section": "11.2 Regression Systems",
    "text": "11.2 Regression Systems\nA univariate linear regression equation equals \\(Y=X^{\\prime} \\beta+e\\) where \\(Y\\) is scalar and \\(X\\) is a vector. Multivariate regression is a system of \\(m\\) linear regressions, and equals\n\\[\nY_{j}=X_{j}^{\\prime} \\beta_{j}+e_{j}\n\\]\nfor \\(j=1, \\ldots, m\\). Here we use the subscript \\(j\\) to denote the \\(j^{t h}\\) dependent variable, not the \\(i^{t h}\\) individual. As an example, \\(Y_{j}\\) could be expenditures by a household on good category \\(j\\) (e.g., food, housing, transportation, clothing, recreation). The regressor vectors \\(X_{j}\\) are \\(k_{j} \\times 1\\) and \\(e_{j}\\) is an error. The coefficient vectors \\(\\beta_{j}\\) are \\(k_{j} \\times 1\\). The total number of coefficients are \\(\\bar{k}=\\sum_{j=1}^{m} k_{j}\\). The regressors can be common across \\(j\\) or can vary across \\(j\\). In the household expenditure example the regressors \\(X_{j}\\) are typically common across \\(j\\), and include variables such as household income, number and ages of family members, and demographic characteristics. The regression system specializes to univariate regression when \\(m=1\\).\nDefine the \\(m \\times 1\\) error vector \\(e=\\left(e_{1}, \\ldots, e_{m}\\right)^{\\prime}\\) and its \\(m \\times m\\) covariance matrix \\(\\Sigma=\\mathbb{E}\\left[e e^{\\prime}\\right]\\). The diagonal elements are the variances of the errors \\(e_{j}\\) and the off-diagonals are the covariances across variables.\nWe can group the \\(m\\) equations (11.1) into a single equation as follows. Let \\(Y=\\left(Y_{1}, \\ldots, Y_{m}\\right)^{\\prime}\\) be the \\(m \\times 1\\) vector of dependent variables. Define the \\(m \\times \\bar{k}\\) matrix of regressors\n\\[\n\\bar{X}=\\left(\\begin{array}{cccc}\nX_{1}^{\\prime} & 0 & \\cdots & 0 \\\\\n\\vdots & X_{2}^{\\prime} & & \\vdots \\\\\n0 & 0 & \\cdots & X_{m}^{\\prime}\n\\end{array}\\right)\n\\]\nand the \\(\\bar{k} \\times 1\\) stacked coefficient vector\n\\[\n\\beta=\\left(\\begin{array}{c}\n\\beta_{1} \\\\\n\\vdots \\\\\n\\beta_{m}\n\\end{array}\\right)\n\\]\nThe \\(m\\) regression equations can be jointly written as\n\\[\nY=\\bar{X} \\beta+e .\n\\]\nThis is a system of \\(m\\) equations.\nFor \\(n\\) observations the joint system can be written in matrix notation by stacking. Define\n\\[\n\\boldsymbol{Y}=\\left(\\begin{array}{c}\nY_{1} \\\\\n\\vdots \\\\\nY_{n}\n\\end{array}\\right), \\quad \\boldsymbol{e}=\\left(\\begin{array}{c}\ne_{1} \\\\\n\\vdots \\\\\ne_{n}\n\\end{array}\\right), \\quad \\overline{\\boldsymbol{X}}=\\left(\\begin{array}{c}\n\\bar{X}_{1} \\\\\n\\vdots \\\\\n\\bar{X}_{n}\n\\end{array}\\right)\n\\]\nwhich are \\(m n \\times 1, m n \\times 1\\), and \\(m n \\times \\bar{k}\\), respectively. The system can be written as \\(\\boldsymbol{Y}=\\overline{\\boldsymbol{X}} \\beta+\\boldsymbol{e}\\).\nIn many applications the regressor vectors \\(X_{j}\\) are common across the variables \\(j\\), so \\(X_{j}=X\\) and \\(k_{j}=k\\). By this we mean that the same variables enter each equation with no exclusion restrictions. Several important simplifications occur in this context. One is that we can write (11.2) using the notation\n\\[\nY=\\boldsymbol{B}^{\\prime} X+e\n\\]\nwhere \\(\\boldsymbol{B}=\\left(\\beta_{1}, \\beta_{2}, \\cdots, \\beta_{m}\\right)\\) is \\(k \\times m\\). Another is that we can write the joint system of observations in the \\(n \\times m\\) matrix notation \\(\\boldsymbol{Y}=\\boldsymbol{X} \\boldsymbol{B}+\\boldsymbol{E}\\) where\n\\[\n\\boldsymbol{Y}=\\left(\\begin{array}{c}\nY_{1}^{\\prime} \\\\\n\\vdots \\\\\nY_{n}^{\\prime}\n\\end{array}\\right), \\quad \\boldsymbol{E}=\\left(\\begin{array}{c}\ne_{1}^{\\prime} \\\\\n\\vdots \\\\\ne_{n}^{\\prime}\n\\end{array}\\right), \\quad \\boldsymbol{X}=\\left(\\begin{array}{c}\nX_{1}^{\\prime} \\\\\n\\vdots \\\\\nX_{n}^{\\prime}\n\\end{array}\\right)\n\\]\nAnother convenient implication of common regressors is that we have the simplification\n\\[\n\\bar{X}=\\left(\\begin{array}{cccc}\nX^{\\prime} & 0 & \\cdots & 0 \\\\\n0 & X^{\\prime} & & 0 \\\\\n\\vdots & \\vdots & & \\vdots \\\\\n0 & 0 & \\cdots & X^{\\prime}\n\\end{array}\\right)=\\boldsymbol{I}_{m} \\otimes X^{\\prime}\n\\]\nwhere \\(\\otimes\\) is the Kronecker product (see Appendix A.21)."
  },
  {
    "objectID": "chpt11-multi-reg.html#least-squares-estimator",
    "href": "chpt11-multi-reg.html#least-squares-estimator",
    "title": "11  Multivariate Regression",
    "section": "11.3 Least Squares Estimator",
    "text": "11.3 Least Squares Estimator\nThe equations (11.1) can be estimated by least squares. This takes the form\n\\[\n\\widehat{\\beta}_{j}=\\left(\\sum_{i=1}^{n} X_{j i} X_{j i}^{\\prime}\\right)^{-1}\\left(\\sum_{i=1}^{n} X_{j i} Y_{j i}\\right) .\n\\]\nAn estimator of \\(\\beta\\) is the stacked vector\n\\[\n\\widehat{\\beta}=\\left(\\begin{array}{c}\n\\widehat{\\beta}_{1} \\\\\n\\vdots \\\\\n\\widehat{\\beta}_{m}\n\\end{array}\\right) .\n\\]\nWe can alternatively write this estimator using the systems notation\n\\[\n\\widehat{\\beta}=\\left(\\overline{\\boldsymbol{X}}^{\\prime} \\overline{\\boldsymbol{X}}\\right)^{-1}\\left(\\overline{\\boldsymbol{X}}^{\\prime} \\boldsymbol{Y}\\right)=\\left(\\sum_{i=1}^{n} \\bar{X}_{i}^{\\prime} \\bar{X}_{i}\\right)^{-1}\\left(\\sum_{i=1}^{n} \\bar{X}_{i}^{\\prime} Y_{i}\\right)\n\\]\nTo see this, observe that\n\\[\n\\begin{aligned}\n\\overline{\\boldsymbol{X}}^{\\prime} \\overline{\\boldsymbol{X}} &=\\left(\\begin{array}{ccc}\n\\bar{X}_{1}^{\\prime} & \\cdots & \\bar{X}_{n}^{\\prime}\n\\end{array}\\right)\\left(\\begin{array}{c}\n\\bar{X}_{1} \\\\\n\\vdots \\\\\n\\bar{X}_{n}\n\\end{array}\\right) \\\\\n&=\\sum_{i=1}^{n} \\bar{X}_{i}^{\\prime} \\bar{X}_{i} \\\\\n&=\\sum_{i=1}^{n}\\left(\\begin{array}{cccc}\nX_{1 i} & 0 & \\cdots & 0 \\\\\n\\vdots & X_{2 i} & & \\vdots \\\\\n0 & 0 & \\cdots & X_{m i}\n\\end{array}\\right)\\left(\\begin{array}{ccccc}\nX_{1 i}^{\\prime} & 0 & \\cdots & 0 \\\\\n\\vdots & X_{2 i}^{\\prime} & & \\vdots \\\\\n0 & 0 & \\cdots & X_{m i}^{\\prime}\n\\end{array}\\right) \\\\\n&=\\left(\\begin{array}{cccccc}\n\\sum_{i=1}^{n} X_{1 i} X_{1 i}^{\\prime} & & 0 \\\\\n\\vdots & & \\sum_{i=1}^{n} X_{2 i} X_{2 i}^{\\prime} & & & \\\\\n0 & & 0 & \\cdots & \\sum_{i=1}^{n} X_{m i} X_{m i}^{\\prime}\n\\end{array}\\right)\n\\end{aligned}\n\\]\nand\n\\[\n\\begin{aligned}\n\\overline{\\boldsymbol{X}}^{\\prime} \\boldsymbol{Y} &=\\left(\\begin{array}{ccc}\n\\bar{X}_{1}^{\\prime} & \\cdots & \\bar{X}_{n}^{\\prime}\n\\end{array}\\right)\\left(\\begin{array}{c}\nY_{1} \\\\\n\\vdots \\\\\nY_{n}\n\\end{array}\\right) \\\\\n&=\\sum_{i=1}^{n} \\bar{X}_{i}^{\\prime} Y_{i} \\\\\n&=\\sum_{i=1}^{n}\\left(\\begin{array}{cccc}\nX_{1 i} & 0 & \\cdots & 0 \\\\\n\\vdots & X_{2 i} & & \\vdots \\\\\n0 & 0 & \\cdots & X_{m i}\n\\end{array}\\right)\\left(\\begin{array}{c}\nY_{1 i} \\\\\n\\vdots \\\\\nY_{m i}\n\\end{array}\\right) \\\\\n&=\\left(\\begin{array}{c}\n\\sum_{i=1}^{n} X_{1 i} Y_{1 i} \\\\\n\\vdots \\\\\n\\sum_{i=1}^{n} X_{m i} Y_{m i}\n\\end{array}\\right) .\n\\end{aligned}\n\\]\nHence\n\\[\n\\begin{aligned}\n\\left(\\overline{\\boldsymbol{X}}^{\\prime} \\overline{\\boldsymbol{X}}\\right)^{-1}\\left(\\overline{\\boldsymbol{X}}^{\\prime} \\boldsymbol{Y}\\right) &=\\left(\\sum_{i=1}^{n} \\bar{X}_{i} \\bar{X}_{i}^{\\prime}\\right)^{-1}\\left(\\sum_{i=1}^{n} \\bar{X}_{i} Y_{i}\\right) \\\\\n&=\\left(\\begin{array}{c}\n\\left(\\sum_{i=1}^{n} X_{1 i} X_{1 i}^{\\prime}\\right)^{-1}\\left(\\sum_{i=1}^{n} X_{1 i} Y_{1 i}\\right) \\\\\n\\vdots \\\\\n\\left(\\sum_{i=1}^{n} X_{m i} X_{m i}^{\\prime}\\right)^{-1}\\left(\\sum_{i=1}^{n} X_{m i} Y_{m i}\\right)\n\\end{array}\\right) \\\\\n&=\\widehat{\\beta}\n\\end{aligned}\n\\]\nas claimed. The \\(m \\times 1\\) residual vector for the \\(i^{t h}\\) observation is \\(\\widehat{e}_{i}=Y_{i}-\\overline{\\boldsymbol{X}}_{i}^{\\prime} \\widehat{\\beta}\\). The least squares estimator of the \\(m \\times m\\) error covariance matrix is\n\\[\n\\widehat{\\Sigma}=\\frac{1}{n} \\sum_{i=1}^{n} \\widehat{e}_{i} \\widehat{e}_{i}^{\\prime} .\n\\]\nIn the case of common regressors, the least squares coefficients can be written as\n\\[\n\\widehat{\\beta}_{j}=\\left(\\sum_{i=1}^{n} X_{i} X_{i}^{\\prime}\\right)^{-1}\\left(\\sum_{i=1}^{n} X_{i} Y_{j i}\\right)\n\\]\nand\n\\[\n\\widehat{\\boldsymbol{B}}=\\left(\\widehat{\\beta}_{1}, \\widehat{\\beta}_{2}, \\cdots, \\widehat{\\beta}_{m}\\right)=\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{Y}\\right) .\n\\]\nIn Stata, multivariate regression can be implemented using the mvreg command."
  },
  {
    "objectID": "chpt11-multi-reg.html#expectation-and-variance-of-systems-least-squares",
    "href": "chpt11-multi-reg.html#expectation-and-variance-of-systems-least-squares",
    "title": "11  Multivariate Regression",
    "section": "11.4 Expectation and Variance of Systems Least Squares",
    "text": "11.4 Expectation and Variance of Systems Least Squares\nWe can calculate the finite-sample expectation and variance of \\(\\widehat{\\beta}\\) under the conditional expectation assumption\n\\[\n\\mathbb{E}[e \\mid X]=0\n\\]\nwhere \\(X\\) is the union of the regressors \\(X_{j}\\). Equation (11.7) is equivalent to \\(\\mathbb{E}\\left\\lfloor Y_{j} \\mid X\\right\\rfloor=X_{j}^{\\prime} \\beta_{j}\\), which means that the regression model is correctly specified.\nWe can center the estimator as\n\\[\n\\widehat{\\beta}-\\beta=\\left(\\overline{\\boldsymbol{X}}^{\\prime} \\overline{\\boldsymbol{X}}\\right)^{-1}\\left(\\overline{\\boldsymbol{X}}^{\\prime} \\boldsymbol{e}\\right)=\\left(\\sum_{i=1}^{n} \\bar{X}_{i}^{\\prime} \\bar{X}_{i}\\right)^{-1}\\left(\\sum_{i=1}^{n} \\bar{X}_{i}^{\\prime} e_{i}\\right)\n\\]\nTaking conditional expectations we find \\(\\mathbb{E}[\\widehat{\\beta} \\mid \\boldsymbol{X}]=\\beta\\). Consequently, systems least squares is unbiased under correct specification.\nTo compute the variance of the estimator, define the conditional covariance matrix of the errors of the \\(i^{t h}\\) observation \\(\\mathbb{E}\\left[e_{i} e_{i}^{\\prime} \\mid X_{i}\\right]=\\Sigma_{i}\\) which in general is a function of \\(X_{i}\\). If the observations are mutually independent then\n\\[\n\\mathbb{E}\\left[\\boldsymbol{e} \\boldsymbol{e}^{\\prime} \\mid \\boldsymbol{X}\\right]=\\mathbb{E}\\left[\\left(\\begin{array}{cccc}\ne_{1} e_{1}^{\\prime} & e_{1} e_{2}^{\\prime} & \\cdots & e_{1} e_{n}^{\\prime} \\\\\n\\vdots & \\ddots & & \\vdots \\\\\ne_{n} e_{1}^{\\prime} & e_{n} e_{2}^{\\prime} & \\cdots & e_{n} e_{n}^{\\prime}\n\\end{array}\\right) \\mid \\boldsymbol{X}\\right]=\\left(\\begin{array}{cccc}\n\\Sigma_{1} & 0 & \\cdots & 0 \\\\\n\\vdots & \\ddots & & \\vdots \\\\\n0 & 0 & \\cdots & \\Sigma_{n}\n\\end{array}\\right) \\text {. }\n\\]\nAlso, by independence across observations,\n\\[\n\\operatorname{var}\\left[\\sum_{i=1}^{n} \\bar{X}_{i}^{\\prime} e_{i} \\mid \\boldsymbol{X}\\right]=\\sum_{i=1}^{n} \\operatorname{var}\\left[\\bar{X}_{i}^{\\prime} e_{i} \\mid X_{i}\\right]=\\sum_{i=1}^{n} \\bar{X}_{i}^{\\prime} \\Sigma_{i} \\bar{X}_{i} .\n\\]\nIt follows that\n\\[\n\\operatorname{var}[\\widehat{\\beta} \\mid \\boldsymbol{X}]=\\left(\\overline{\\boldsymbol{X}}^{\\prime} \\overline{\\boldsymbol{X}}\\right)^{-1}\\left(\\sum_{i=1}^{n} \\bar{X}_{i}^{\\prime} \\Sigma_{i} \\bar{X}_{i}\\right)\\left(\\overline{\\boldsymbol{X}}^{\\prime} \\overline{\\boldsymbol{X}}\\right)^{-1} .\n\\]\nWhen the regressors are common so that \\(\\bar{X}_{i}=\\boldsymbol{I}_{m} \\otimes X_{i}^{\\prime}\\) then the covariance matrix can be written as\n\\[\n\\operatorname{var}[\\widehat{\\beta} \\mid \\boldsymbol{X}]=\\left(\\boldsymbol{I}_{m} \\otimes\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1}\\right)\\left(\\sum_{i=1}^{n}\\left(\\Sigma_{i} \\otimes X_{i} X_{i}^{\\prime}\\right)\\right)\\left(\\boldsymbol{I}_{m} \\otimes\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1}\\right)\n\\]\nIf the errors are conditionally homoskedastic\n\\[\n\\mathbb{E}\\left[e e^{\\prime} \\mid X\\right]=\\Sigma\n\\]\nthen the covariance matrix simplifies to\n\\[\n\\operatorname{var}[\\widehat{\\beta} \\mid \\boldsymbol{X}]=\\left(\\overline{\\boldsymbol{X}}^{\\prime} \\overline{\\boldsymbol{X}}\\right)^{-1}\\left(\\sum_{i=1}^{n} \\bar{X}_{i}^{\\prime} \\Sigma \\bar{X}_{i}\\right)\\left(\\overline{\\boldsymbol{X}}^{\\prime} \\overline{\\boldsymbol{X}}\\right)^{-1}\n\\]\nIf both simplifications (common regressors and conditional homoskedasticity) hold then we have the considerable simplication\n\\[\n\\operatorname{var}[\\widehat{\\beta} \\mid \\boldsymbol{X}]=\\Sigma \\otimes\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} .\n\\]"
  },
  {
    "objectID": "chpt11-multi-reg.html#asymptotic-distribution",
    "href": "chpt11-multi-reg.html#asymptotic-distribution",
    "title": "11  Multivariate Regression",
    "section": "11.5 Asymptotic Distribution",
    "text": "11.5 Asymptotic Distribution\nFor an asymptotic distribution it is sufficient to consider the equation-by-equation projection model in which case\n\\[\n\\mathbb{E}\\left[X_{j} e_{j}\\right]=0 .\n\\]\nFirst, consider consistency. Since \\(\\widehat{\\beta}_{j}\\) are the standard least squares estimators, they are consistent for the projection coefficients \\(\\beta_{j}\\).\nSecond, consider the asymptotic distribution. Our single equation theory implies that the \\(\\widehat{\\beta}_{j}\\) are asymptotically normal. But this theory does not provide a joint distribution of the \\(\\widehat{\\beta}_{j}\\) across \\(j\\), which we now derive. Since the vector\n\\[\n\\bar{X}_{i}^{\\prime} e_{i}=\\left(\\begin{array}{c}\nX_{1 i} e_{1 i} \\\\\n\\vdots \\\\\nX_{m i} e_{m i}\n\\end{array}\\right)\n\\]\nis i.i.d. across \\(i\\) and mean zero under (11.9), the central limit theorem implies\n\\[\n\\frac{1}{\\sqrt{n}} \\sum_{i=1}^{n} \\bar{X}_{i}^{\\prime} e_{i} \\underset{d}{\\longrightarrow} \\mathrm{N}(0, \\Omega)\n\\]\nwhere\n\\[\n\\Omega=\\mathbb{E}\\left[\\bar{X}_{i}^{\\prime} e_{i} e_{i}^{\\prime} \\bar{X}_{i}\\right]=\\mathbb{E}\\left[\\bar{X}_{i}^{\\prime} \\Sigma_{i} \\bar{X}_{i}\\right] .\n\\]\nThe matrix \\(\\Omega\\) is the covariance matrix of the variables \\(X_{j i} e_{j i}\\) across equations. Under conditional homoskedasticity (11.8) the matrix \\(\\Omega\\) simplifies to\n\\[\n\\Omega=\\mathbb{E}\\left[\\bar{X}_{i}^{\\prime} \\Sigma \\bar{X}_{i}\\right]\n\\]\n(see Exercise 11.1). When the regressors are common it simplies to\n\\[\n\\Omega=\\mathbb{E}\\left[e e^{\\prime} \\otimes X X^{\\prime}\\right]\n\\]\n(see Exercise 11.2). Under both conditions (homoskedasticity and common regressors) it simplifies to\n\\[\n\\Omega=\\Sigma \\otimes \\mathbb{E}\\left[X X^{\\prime}\\right]\n\\]\n(see Exercise 11.3).\nApplied to the centered and normalized estimator we obtain the asymptotic distribution. Theorem 11.1 Under Assumption 7.2, \\(\\sqrt{n}(\\widehat{\\beta}-\\beta) \\underset{d}{\\longrightarrow} \\mathrm{N}\\left(0, \\boldsymbol{V}_{\\beta}\\right)\\) where \\(\\boldsymbol{V}_{\\beta}=\\) \\(Q^{-1} \\Omega Q^{-1}\\) and\n\\[\n\\boldsymbol{Q}=\\mathbb{E}\\left[\\bar{X}^{\\prime} \\bar{X}\\right]=\\left(\\begin{array}{cccc}\n\\mathbb{E}\\left[X_{1} X_{1}^{\\prime}\\right] & 0 & \\cdots & 0 \\\\\n\\vdots & \\ddots & & \\vdots \\\\\n0 & 0 & \\cdots & \\mathbb{E}\\left[X_{m} X_{m}^{\\prime}\\right]\n\\end{array}\\right)\n\\]\nFor a proof, see Exercise 11.4.\nWhen the regressors are common the matrix \\(\\boldsymbol{Q}\\) simplifies as\n\\[\n\\boldsymbol{Q}=\\boldsymbol{I}_{m} \\otimes \\mathbb{E}\\left[X X^{\\prime}\\right]\n\\]\n(See Exercise 11.5).\nIf both the regressors are common and the errors are conditionally homoskedastic (11.8) then we have the simplification\n\\[\n\\boldsymbol{V}_{\\beta}=\\Sigma \\otimes\\left(\\mathbb{E}\\left[X X^{\\prime}\\right]\\right)^{-1}\n\\]\n(see Exercise 11.6).\nSometimes we are interested in parameters \\(\\theta=r\\left(\\beta_{1}, \\ldots, \\beta_{m}\\right)=r(\\beta)\\) which are functions of the coefficients from multiple equations. In this case the least squares estimator of \\(\\theta\\) is \\(\\widehat{\\theta}=r(\\widehat{\\beta})\\). The asymptotic distribution of \\(\\widehat{\\theta}\\) can be obtained from Theorem \\(11.1\\) by the delta method.\nTheorem 11.2 Under Assumptions \\(7.2\\) and \\(7.3, \\sqrt{n}(\\widehat{\\theta}-\\theta) \\underset{d}{\\longrightarrow} \\mathrm{N}\\left(0, \\boldsymbol{V}_{\\theta}\\right)\\) where \\(\\boldsymbol{V}_{\\theta}=\\boldsymbol{R}^{\\prime} \\boldsymbol{V}_{\\beta} \\boldsymbol{R}\\) and \\(\\boldsymbol{R}=\\frac{\\partial}{\\partial \\beta} r(\\beta)^{\\prime} .\\)\nFor a proof, see Exercise 11.7.\nTheorem \\(11.2\\) is an example where multivariate regression is fundamentally distinct from univariate regression. Only by treating least squares as a joint estimator can we obtain a distributional theory for a function of multiple equations. We can thereby construct standard errors, confidence intervals, and hypothesis tests."
  },
  {
    "objectID": "chpt11-multi-reg.html#covariance-matrix-estimation",
    "href": "chpt11-multi-reg.html#covariance-matrix-estimation",
    "title": "11  Multivariate Regression",
    "section": "11.6 Covariance Matrix Estimation",
    "text": "11.6 Covariance Matrix Estimation\nFrom the finite sample and asymptotic theory we can construct appropriate estimators for the variance of \\(\\widehat{\\beta}\\). In the general case we have\n\\[\n\\widehat{\\boldsymbol{V}}_{\\widehat{\\beta}}=\\left(\\overline{\\boldsymbol{X}}^{\\prime} \\overline{\\boldsymbol{X}}\\right)^{-1}\\left(\\sum_{i=1}^{n} \\bar{X}_{i}^{\\prime} \\widehat{e}_{i} \\widehat{e}_{i}^{\\prime} \\bar{X}_{i}\\right)\\left(\\overline{\\boldsymbol{X}}^{\\prime} \\overline{\\boldsymbol{X}}\\right)^{-1} .\n\\]\nUnder conditional homoskedasticity (11.8) an appropriate estimator is\n\\[\n\\widehat{\\boldsymbol{V}}_{\\widehat{\\beta}}^{0}=\\left(\\overline{\\boldsymbol{X}}^{\\prime} \\overline{\\boldsymbol{X}}\\right)^{-1}\\left(\\sum_{i=1}^{n} \\bar{X}_{i}^{\\prime} \\widehat{\\Sigma}_{i}\\right)\\left(\\overline{\\boldsymbol{X}}^{\\prime} \\overline{\\boldsymbol{X}}\\right)^{-1} .\n\\]\nWhen the regressors are common then these estimators equal\n\\[\n\\widehat{\\boldsymbol{V}}_{\\widehat{\\beta}}=\\left(\\boldsymbol{I}_{m} \\otimes\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1}\\right)\\left(\\sum_{i=1}^{n}\\left(\\widehat{e}_{i} \\widehat{e}_{i}^{\\prime} \\otimes X_{i} X_{i}^{\\prime}\\right)\\right)\\left(\\boldsymbol{I}_{m} \\otimes\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1}\\right)\n\\]\nand \\(\\widehat{\\boldsymbol{V}}_{\\widehat{\\beta}}^{0}=\\widehat{\\Sigma} \\otimes\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1}\\), respectively.\nCovariance matrix estimators for \\(\\widehat{\\theta}\\) are found as\n\\[\n\\begin{aligned}\n\\widehat{\\boldsymbol{V}}_{\\widehat{\\theta}} &=\\widehat{\\boldsymbol{R}}^{\\prime} \\widehat{\\boldsymbol{V}}_{\\widehat{\\beta}} \\widehat{\\boldsymbol{R}} \\\\\n\\widehat{\\boldsymbol{V}}_{\\widehat{\\theta}}^{0} &=\\widehat{\\boldsymbol{R}}^{\\prime} \\widehat{\\boldsymbol{V}}_{\\widehat{\\beta}}^{0} \\widehat{\\boldsymbol{R}} \\\\\n\\widehat{\\boldsymbol{R}} &=\\frac{\\partial}{\\partial \\beta} r(\\widehat{\\beta})^{\\prime} .\n\\end{aligned}\n\\]\nTheorem 11.3 Under Assumption 7.2, \\(n \\widehat{\\boldsymbol{V}}_{\\widehat{\\beta}} \\underset{p}{\\rightarrow} \\boldsymbol{V}_{\\beta}\\) and \\(n \\widehat{\\boldsymbol{V}}_{\\widehat{\\beta}}^{0} \\vec{p}^{0} \\boldsymbol{V}_{\\beta}^{0}\\)\nFor a proof, see Exercise 11.8."
  },
  {
    "objectID": "chpt11-multi-reg.html#seemingly-unrelated-regression",
    "href": "chpt11-multi-reg.html#seemingly-unrelated-regression",
    "title": "11  Multivariate Regression",
    "section": "11.7 Seemingly Unrelated Regression",
    "text": "11.7 Seemingly Unrelated Regression\nConsider the systems regression model under the conditional expectation and homoskedasticity assumptions\n\\[\n\\begin{aligned}\nY &=\\bar{X} \\beta+e \\\\\n\\mathbb{E}[e \\mid X] &=0 \\\\\n\\mathbb{E}\\left[e e^{\\prime} \\mid X\\right] &=\\Sigma .\n\\end{aligned}\n\\]\nSince the errors are correlated across equations we consider estimation by Generalized Least Squares (GLS). To derive the estimator, premultiply (11.15) by \\(\\Sigma^{-1 / 2}\\) so that the transformed error vector is i.i.d. with covariance matrix \\(\\boldsymbol{I}_{m}\\). Then apply least squares and rearrange to find\n\\[\n\\widehat{\\beta}_{\\mathrm{gls}}=\\left(\\sum_{i=1}^{n} \\bar{X}_{i}^{\\prime} \\Sigma^{-1} \\bar{X}_{i}\\right)^{-1}\\left(\\sum_{i=1}^{n} \\bar{X}_{i}^{\\prime} \\Sigma^{-1} Y_{i}\\right)\n\\]\n(see Exercise 11.9). Another approach is to take the vector representation\n\\[\n\\boldsymbol{Y}=\\overline{\\boldsymbol{X}} \\beta+\\boldsymbol{e}\n\\]\nand calculate that the equation error \\(\\boldsymbol{e}\\) has variance \\(\\mathbb{E}\\left[\\boldsymbol{e} \\boldsymbol{e}^{\\prime}\\right]=\\boldsymbol{I}_{n} \\otimes \\Sigma\\). Premultiply the equation by \\(\\boldsymbol{I}_{n} \\otimes\\) \\(\\Sigma^{-1 / 2}\\) so that the transformed error has covariance matrix \\(\\boldsymbol{I}_{n m}\\) and then apply least squares to find\n\\[\n\\widehat{\\beta}_{\\mathrm{gls}}=\\left(\\overline{\\boldsymbol{X}}^{\\prime}\\left(\\boldsymbol{I}_{n} \\otimes \\Sigma^{-1}\\right) \\overline{\\boldsymbol{X}}\\right)^{-1}\\left(\\overline{\\boldsymbol{X}}^{\\prime}\\left(\\boldsymbol{I}_{n} \\otimes \\Sigma^{-1}\\right) \\boldsymbol{Y}\\right)\n\\]\n(see Exercise 11.10). Expressions (11.16) and (11.17) are algebraically equivalent. To see the equivalence, observe that\n\\[\n\\begin{aligned}\n\\overline{\\boldsymbol{X}}^{\\prime}\\left(\\boldsymbol{I}_{n} \\otimes \\Sigma^{-1}\\right) \\overline{\\boldsymbol{X}} &=\\left(\\begin{array}{lll}\n\\bar{X}_{1}^{\\prime} & \\cdots & \\bar{X}_{n}^{\\prime}\n\\end{array}\\right)\\left(\\begin{array}{cccc}\n\\Sigma^{-1} & 0 & \\cdots & 0 \\\\\n\\vdots & \\Sigma^{-1} & & \\vdots \\\\\n0 & 0 & \\cdots & \\Sigma^{-1}\n\\end{array}\\right)\\left(\\begin{array}{c}\n\\bar{X}_{1} \\\\\n\\vdots \\\\\n\\bar{X}_{n}\n\\end{array}\\right) \\\\\n&=\\sum_{i=1}^{n} \\bar{X}_{i}^{\\prime} \\Sigma^{-1} \\bar{X}_{i}\n\\end{aligned}\n\\]\nand\n\\[\n\\begin{aligned}\n\\overline{\\boldsymbol{X}}^{\\prime}\\left(\\boldsymbol{I}_{n} \\otimes \\Sigma^{-1}\\right) \\boldsymbol{Y} &=\\left(\\begin{array}{lll}\n\\bar{X}_{1}^{\\prime} & \\cdots & \\bar{X}_{n}^{\\prime}\n\\end{array}\\right)\\left(\\begin{array}{cccc}\n\\Sigma^{-1} & 0 & \\cdots & 0 \\\\\n\\vdots & \\Sigma^{-1} & & \\vdots \\\\\n0 & 0 & \\cdots & 0^{-1}\n\\end{array}\\right)\\left(\\begin{array}{c}\nY_{1} \\\\\n\\vdots \\\\\nY_{n}\n\\end{array}\\right) \\\\\n&=\\sum_{i=1}^{n} \\bar{X}_{i}^{\\prime} \\Sigma^{-1} Y_{i} .\n\\end{aligned}\n\\]\nSince \\(\\Sigma\\) is unknown it must be replaced by an estimator. Using \\(\\widehat{\\Sigma}\\) from (11.5) we obtain a feasible GLS estimator.\n\\[\n\\begin{aligned}\n\\widehat{\\beta}_{\\mathrm{sur}} &=\\left(\\sum_{i=1}^{n} \\bar{X}_{i}^{\\prime} \\widehat{\\Sigma}^{-1} \\bar{X}_{i}\\right)^{-1}\\left(\\sum_{i=1}^{n} \\bar{X}_{i}^{\\prime} \\widehat{\\Sigma}^{-1} Y_{i}\\right) \\\\\n&=\\left(\\overline{\\boldsymbol{X}}^{\\prime}\\left(\\boldsymbol{I}_{n} \\otimes \\widehat{\\Sigma}^{-1}\\right) \\overline{\\boldsymbol{X}}\\right)^{-1}\\left(\\overline{\\boldsymbol{X}}^{\\prime}\\left(\\boldsymbol{I}_{n} \\otimes \\widehat{\\Sigma}^{-1}\\right) \\boldsymbol{Y}\\right) .\n\\end{aligned}\n\\]\nThis is the Seemingly Unrelated Regression (SUR) estimator as introduced by Zellner (1962).\nThe estimator \\(\\widehat{\\Sigma}\\) can be updated by calculating the SUR residuals \\(\\widehat{e}_{i}=Y_{i}-\\bar{X}_{i}^{\\prime} \\widehat{\\beta}_{\\text {sur }}\\) and the covariance matrix estimator \\(\\widehat{\\Sigma}=\\frac{1}{n} \\sum_{i=1}^{n} \\widehat{e}_{i} \\widehat{e}_{i}^{\\prime}\\). Substituted into (11.18) we obtain an iterated SUR estimator. This can be iterated until convergence.\nUnder conditional homoskedasticity (11.8) we can derive its asymptotic distribution.\nTheorem 11.4 Under Assumption \\(7.2\\) and (11.8)\n\\[\n\\sqrt{n}\\left(\\widehat{\\beta}_{\\text {sur }}-\\beta\\right) \\underset{d}{\\longrightarrow} \\mathrm{N}\\left(0, V_{\\beta}^{*}\\right)\n\\]\nwhere \\(\\boldsymbol{V}_{\\beta}^{*}=\\left(\\mathbb{E}\\left[\\bar{X}^{\\prime} \\Sigma^{-1} \\bar{X}\\right]\\right)^{-1}\\).\nFor a proof, see Exercise 11.11.\nUnder these assumptions, SUR is more efficient than least squares.\nTheorem 11.5 Under Assumption \\(7.2\\) and (11.8)\n\\[\n\\boldsymbol{V}_{\\beta}^{*}=\\left(\\mathbb{E}\\left[\\bar{X}^{\\prime} \\Sigma^{-1} \\bar{X}\\right]\\right)^{-1} \\leq\\left(\\mathbb{E}\\left[\\bar{X}^{\\prime} \\bar{X}\\right]\\right)^{-1} \\mathbb{E}\\left[\\bar{X}^{\\prime} \\Sigma \\bar{X}\\right]\\left(\\mathbb{E}\\left[\\bar{X}^{\\prime} \\bar{X}\\right]\\right)^{-1}=\\boldsymbol{V}_{\\beta}\n\\]\nand thus \\(\\widehat{\\beta}_{\\text {sur }}\\) is asymptotically more efficient than \\(\\widehat{\\beta}_{\\text {ols. }}\\). For a proof, see Exercise 11.12.\nAn appropriate estimator of the variance of \\(\\widehat{\\beta}_{\\text {sur }}\\) is\n\\[\n\\widehat{\\boldsymbol{V}}_{\\widehat{\\beta}}=\\left(\\sum_{i=1}^{n} \\bar{X}_{i}^{\\prime} \\widehat{\\Sigma}^{-1} \\bar{X}_{i}\\right)^{-1}\n\\]\nTheorem 11.6 Under Assumption \\(7.2\\) and (11.8) \\(n \\widehat{\\boldsymbol{V}}_{\\widehat{\\beta}} \\underset{p}{\\longrightarrow} \\boldsymbol{V}_{\\beta}\\).\nFor a proof, see Exercise 11.13.\nIn Stata, the seemingly unrelated regressions estimator is implemented using the sureg command."
  },
  {
    "objectID": "chpt11-multi-reg.html#equivalence-of-sur-and-least-squares",
    "href": "chpt11-multi-reg.html#equivalence-of-sur-and-least-squares",
    "title": "11  Multivariate Regression",
    "section": "11.8 Equivalence of SUR and Least Squares",
    "text": "11.8 Equivalence of SUR and Least Squares\nWhen the regressors are common across equations \\(X_{j}=X\\) it turns out that the SUR estimator simplifies to least squares.\nTo see this, recall that when regressors are common this implies that \\(\\bar{X}=\\boldsymbol{I}_{m} \\otimes X^{\\prime}\\). Then\n\\[\n\\begin{aligned}\n\\bar{X}_{i}^{\\prime} \\widehat{\\Sigma}^{-1} &=\\left(\\boldsymbol{I}_{m} \\otimes X_{i}\\right) \\widehat{\\Sigma}^{-1} \\\\\n&=\\widehat{\\Sigma}^{-1} \\otimes X_{i} \\\\\n&=\\left(\\widehat{\\Sigma}^{-1} \\otimes \\boldsymbol{I}_{k}\\right)\\left(\\boldsymbol{I}_{m} \\otimes X_{i}\\right) \\\\\n&=\\left(\\widehat{\\Sigma}^{-1} \\otimes \\boldsymbol{I}_{k}\\right) \\bar{X}_{i}^{\\prime} .\n\\end{aligned}\n\\]\nThus\n\\[\n\\begin{aligned}\n\\widehat{\\beta}_{\\mathrm{sur}} &=\\left(\\sum_{i=1}^{n} \\bar{X}_{i}^{\\prime} \\widehat{\\Sigma}^{-1} \\bar{X}_{i}\\right)^{-1}\\left(\\sum_{i=1}^{n} \\bar{X}_{i}^{\\prime} \\widehat{\\Sigma}^{-1} Y_{i}\\right) \\\\\n&=\\left(\\left(\\widehat{\\Sigma}^{-1} \\otimes \\boldsymbol{I}_{k}\\right) \\sum_{i=1}^{n} \\bar{X}_{i}^{\\prime} \\bar{X}_{i}\\right)^{-1}\\left(\\left(\\widehat{\\Sigma}^{-1} \\otimes \\boldsymbol{I}_{k}\\right) \\sum_{i=1}^{n} \\bar{X}_{i}^{\\prime} Y_{i}\\right) \\\\\n&=\\left(\\sum_{i=1}^{n} \\bar{X}_{i}^{\\prime} \\bar{X}_{i}\\right)^{-1}\\left(\\sum_{i=1}^{n} \\bar{X}_{i}^{\\prime} Y_{i}\\right)=\\widehat{\\beta}_{\\mathrm{ols}}\n\\end{aligned}\n\\]\nA model where regressors are not common across equations is nested within a model with the union of all regressors included in all equations. Thus the model with regressors common across equations is a fully unrestricted model, and a model where the regressors differ across equations is a restricted model. Thus the above result shows that the SUR estimator reduces to least squares in the absence of restrictions, but SUR can differ from least squares otherwise.\nAnother context where SUR=OLS is when the variance matrix is diagonal, \\(\\Sigma=\\operatorname{diag}\\left\\{\\sigma_{1}^{2}, \\ldots, \\sigma_{m}^{2}\\right\\}\\). In this case \\(\\Sigma^{-1 / 2} \\bar{X}_{i}=\\bar{X}_{i} \\operatorname{diag}\\left\\{\\boldsymbol{I}_{k_{1}} \\sigma_{1}^{-1 / 2}, \\ldots, \\boldsymbol{I}_{k_{m}} \\sigma_{m}^{-1 / 2}\\right\\}\\) from which you can calculate that \\(\\widehat{\\beta}_{\\text {sur }}=\\widehat{\\beta}_{\\text {ols }}\\). The intuition is that there is no difference in systems estimation when the equations are uncorrelated, which occurs when \\(\\Sigma\\) is diagonal."
  },
  {
    "objectID": "chpt11-multi-reg.html#maximum-likelihood-estimator",
    "href": "chpt11-multi-reg.html#maximum-likelihood-estimator",
    "title": "11  Multivariate Regression",
    "section": "11.9 Maximum Likelihood Estimator",
    "text": "11.9 Maximum Likelihood Estimator\nTake the linear model under the assumption that the error is independent of the regressors and multivariate normally distributed. Thus \\(Y=\\bar{X} \\beta+e\\) with \\(e \\sim \\mathrm{N}(0, \\Sigma)\\). In this case we can consider the maximum likelihood estimator (MLE) of the coefficients.\nIt is convenient to reparameterize the covariance matrix in terms of its inverse \\(S=\\Sigma^{-1}\\). With this reparameterization the conditional density of \\(Y\\) given \\(X=x\\) equals\n\\[\nf(y \\mid x)=\\frac{\\operatorname{det}(\\boldsymbol{S})^{1 / 2}}{(2 \\pi)^{m / 2}} \\exp \\left(-\\frac{1}{2}(y-x \\beta)^{\\prime} \\boldsymbol{S}(y-x \\beta)\\right) .\n\\]\nThe log-likelihood function for the sample is\n\\[\n\\ell_{n}(\\beta, \\boldsymbol{S})=-\\frac{n m}{2} \\log (2 \\pi)+\\frac{n}{2} \\log (\\operatorname{det}(\\boldsymbol{S}))-\\frac{1}{2} \\sum_{i=1}^{n}\\left(Y_{i}-\\bar{X}_{i} \\beta\\right)^{\\prime} S\\left(Y_{i}-\\bar{X}_{i} \\beta\\right) .\n\\]\nThe maximum likelihood estimator \\(\\left(\\widehat{\\beta}_{\\text {mle }}, \\widehat{S}_{\\text {mle }}\\right)\\) maximizes the log-likelihood function. The first order conditions are\n\\[\n0=\\left.\\frac{\\partial}{\\partial \\beta} \\ell_{n}(\\beta, \\boldsymbol{S})\\right|_{\\beta=\\widehat{\\beta}, \\boldsymbol{S}=\\widehat{\\boldsymbol{S}}}=\\sum_{i=1}^{n} \\bar{X}_{i} \\widehat{\\boldsymbol{S}}\\left(Y_{i}-\\bar{X}_{i} \\widehat{\\beta}\\right)\n\\]\nand\n\\[\n0=\\left.\\frac{\\partial}{\\partial \\boldsymbol{S}} \\ell_{n}(\\beta, \\Sigma)\\right|_{\\beta=\\widehat{\\beta}, \\boldsymbol{S}=\\widehat{\\boldsymbol{S}}}=\\frac{n}{2} \\widehat{\\boldsymbol{S}}^{-1}-\\frac{1}{2} \\operatorname{tr}\\left(\\sum_{i=1}^{n}\\left(Y_{i}-\\bar{X}_{i} \\widehat{\\beta}\\right)\\left(Y_{i}-\\bar{X}_{i} \\widehat{\\beta}\\right)^{\\prime}\\right)\n\\]\nThe second equation uses the matrix results \\(\\frac{\\partial}{\\partial S} \\log (\\operatorname{det}(\\boldsymbol{S}))=\\boldsymbol{S}^{-1}\\) and \\(\\frac{\\partial}{\\partial \\boldsymbol{B}} \\operatorname{tr}(\\boldsymbol{A B})=\\boldsymbol{A}^{\\prime}\\) from Appendix A.20.\nSolving and making the substitution \\(\\widehat{\\Sigma}=\\widehat{\\boldsymbol{S}}^{-1}\\) we obtain\n\\[\n\\begin{gathered}\n\\widehat{\\beta}_{\\mathrm{mle}}=\\left(\\sum_{i=1}^{n} \\bar{X}_{i}^{\\prime} \\widehat{\\Sigma}^{-1} \\bar{X}_{i}\\right)^{-1}\\left(\\sum_{i=1}^{n} \\bar{X}_{i}^{\\prime} \\widehat{\\Sigma}^{-1} Y_{i}\\right) \\\\\n\\widehat{\\Sigma}_{\\mathrm{mle}}=\\frac{1}{n} \\sum_{i=1}^{n}\\left(Y_{i}-\\bar{X}_{i} \\widehat{\\beta}\\right)\\left(Y_{i}-\\bar{X}_{i} \\widehat{\\beta}\\right)^{\\prime} .\n\\end{gathered}\n\\]\nNotice that each equation refers to the other. Hence these are not closed-form expressions but can be solved via iteration. The solution is identical to the iterated SUR estimator. Thus the iterated SUR estimator is identical to MLE under normality.\nRecall that the SUR estimator simplifies to OLS when the regressors are common across equations. The same occurs for the MLE. Thus when \\(\\bar{X}_{i}=\\boldsymbol{I}_{m} \\otimes X_{i}^{\\prime}\\) we find that \\(\\widehat{\\beta}_{\\mathrm{mle}}=\\widehat{\\beta}_{\\text {ols }}\\) and \\(\\widehat{\\Sigma}_{\\text {mle }}=\\widehat{\\Sigma}_{\\text {ols }}\\)."
  },
  {
    "objectID": "chpt11-multi-reg.html#restricted-estimation",
    "href": "chpt11-multi-reg.html#restricted-estimation",
    "title": "11  Multivariate Regression",
    "section": "11.10 Restricted Estimation",
    "text": "11.10 Restricted Estimation\nIn many multivariate regression applications it is desired to impose restrictions on the coefficients. In particular, cross-equation restrictions (for example, imposing Slutsky symmetry on a demand system) can be quite important and can only be imposed by a multivariate estimation method. Estimation subject to restrictions can be done by minimum distance, maximum likelihood, or the generalized method of moments.\nMinimum distance is a straightforward application of the methods of Chapter 8 to the estimators presented in this chapter, as such methods apply to any asymptotically normal estimator.\nImposing restrictions on maximum likelihood is also straightforward. The likelihood is maximized subject to the imposed restrictions. One important example is explored in detail in the following section.\nGeneralized method of moments estimation of multivariate regression subject to restrictions will be explored in Section 13.18. This is a particularly simple and straightforward way to estimate restricted multivariate regression models and is our generally preferred approach."
  },
  {
    "objectID": "chpt11-multi-reg.html#reduced-rank-regression",
    "href": "chpt11-multi-reg.html#reduced-rank-regression",
    "title": "11  Multivariate Regression",
    "section": "11.11 Reduced Rank Regression",
    "text": "11.11 Reduced Rank Regression\nOne context where systems estimation is important is when it is desired to impose or test restrictions across equations. Restricted systems are commonly estimated by maximum likelihood under normality. In this section we explore one important special case of restricted multivariate regression known as reduced rank regression. The model was originally proposed by Anderson (1951) and extended by Johansen (1995).\nThe unrestricted model is\n\\[\n\\begin{aligned}\nY &=\\boldsymbol{B}^{\\prime} X+\\boldsymbol{C}^{\\prime} Z+e \\\\\n\\mathbb{E}\\left[e e^{\\prime} \\mid X, Z\\right] &=\\Sigma\n\\end{aligned}\n\\]\nwhere \\(\\boldsymbol{B}\\) is \\(k \\times m, \\boldsymbol{C}\\) is \\(\\ell \\times m, Y \\in \\mathbb{R}^{m}, X \\in \\mathbb{R}^{k}\\), and \\(Z \\in \\mathbb{R}^{\\ell}\\). We separate the regressors as \\(X\\) and \\(Z\\) because the coefficient matrix \\(\\boldsymbol{B}\\) will be restricted while \\(\\boldsymbol{C}\\) will be unrestricted.\nThe matrix \\(\\boldsymbol{B}\\) is full rank if\n\\[\n\\operatorname{rank}(\\boldsymbol{B})=\\min (k, m) .\n\\]\nThe reduced rank restriction is \\(\\operatorname{rank}(\\boldsymbol{B})=r<\\min (k, m)\\) for some known \\(r\\).\nThe reduced rank restriction implies that we can write the coefficient matrix \\(\\boldsymbol{B}\\) in the factored form \\(\\boldsymbol{B}=\\boldsymbol{G} \\boldsymbol{A}^{\\prime}\\) where \\(\\boldsymbol{A}\\) is \\(m \\times r\\) and \\(\\boldsymbol{G}\\) is \\(k \\times r\\). This representation is not unique as we can replace \\(\\boldsymbol{G}\\) with \\(\\boldsymbol{G} \\boldsymbol{Q}\\) and \\(\\boldsymbol{A}\\) with \\(\\boldsymbol{A} \\boldsymbol{Q}^{-1 \\prime}\\) for any invertible \\(\\boldsymbol{Q}\\) and the same relation holds. Identification therefore requires a normalization of the coefficients. A conventional normalization is \\(\\boldsymbol{G}^{\\prime} \\boldsymbol{D} \\boldsymbol{G}=\\boldsymbol{I}_{r}\\) for given \\(\\boldsymbol{D}\\).\nEquivalently, the reduced rank restriction can be imposed by requiring that \\(\\boldsymbol{B}\\) satisfy the restriction \\(\\boldsymbol{B} \\boldsymbol{A}_{\\perp}=\\boldsymbol{G} \\boldsymbol{A}^{\\prime} \\boldsymbol{A}_{\\perp}=0\\) for some \\(m \\times(m-r)\\) coefficient matrix \\(\\boldsymbol{A}_{\\perp}\\). Since \\(\\boldsymbol{G}\\) is full rank this requires that \\(\\boldsymbol{A}^{\\prime} \\boldsymbol{A}_{\\perp}=0\\), hence \\(\\boldsymbol{A}_{\\perp}\\) is the orthogonal complement of \\(\\boldsymbol{A}\\). Note that \\(\\boldsymbol{A}_{\\perp}\\) is not unique as it can be replaced by \\(\\boldsymbol{A}_{\\perp} \\boldsymbol{Q}\\) for any \\((m-r) \\times(m-r)\\) invertible \\(\\boldsymbol{Q}\\). Thus if \\(\\boldsymbol{A}_{\\perp}\\) is to be estimated it requires a normalization.\nWe discuss methods for estimation of \\(\\boldsymbol{G}, \\boldsymbol{A}, \\Sigma, \\boldsymbol{C}\\), and \\(\\boldsymbol{A}_{\\perp}\\). The standard approach is maximum likelihood under the assumption that \\(e \\sim \\mathrm{N}(0, \\Sigma)\\). The log-likelihood function for the sample is\n\\[\n\\begin{aligned}\n\\ell_{n}(\\boldsymbol{G}, \\boldsymbol{A}, \\boldsymbol{C}, \\Sigma) &=-\\frac{n m}{2} \\log (2 \\pi)-\\frac{n}{2} \\log (\\operatorname{det}(\\Sigma)) \\\\\n&-\\frac{1}{2} \\sum_{i=1}^{n}\\left(Y_{i}-\\boldsymbol{A} \\boldsymbol{G}^{\\prime} X_{i}-\\boldsymbol{C}^{\\prime} Z_{i}\\right)^{\\prime} \\Sigma^{-1}\\left(Y_{i}-\\boldsymbol{A} \\boldsymbol{G}^{\\prime} X_{i}-\\boldsymbol{C}^{\\prime} Z_{i}\\right) .\n\\end{aligned}\n\\]\nAnderson (1951) derived the MLE by imposing the constraint \\(\\boldsymbol{B} \\boldsymbol{A}_{\\perp}=0\\) via the method of Lagrange multipliers. This turns out to be algebraically cumbersome.\nJohansen (1995) instead proposed the following straightforward concentration method. Treating \\(\\boldsymbol{G}\\) as if it is known, maximize the log-likelihood with respect to the other parameters. Resubstituting these estimators we obtain the concentrated log-likelihood function with respect to \\(\\boldsymbol{G}\\). This can be maximized to find the MLE for \\(\\boldsymbol{G}\\). The other parameter estimators are then obtain by substitution. We now describe these steps in detail.\nGiven \\(\\boldsymbol{G}\\) the likelihood is a normal multivariate regression in the variables \\(\\boldsymbol{G}^{\\prime} X\\) and \\(Z\\), so the MLE for \\(\\boldsymbol{A}, \\boldsymbol{C}\\) and \\(\\Sigma\\) are least squares. In particular, using the Frisch-Waugh-Lovell residual regression formula we can write the estimators for \\(\\boldsymbol{A}\\) and \\(\\Sigma\\) as\n\\[\n\\widehat{\\boldsymbol{A}}(\\boldsymbol{G})=\\left(\\widetilde{\\boldsymbol{Y}}^{\\prime} \\widetilde{\\boldsymbol{X}} \\boldsymbol{G}\\right)\\left(\\boldsymbol{G}^{\\prime} \\widetilde{\\boldsymbol{X}}^{\\prime} \\widetilde{\\boldsymbol{X}} \\boldsymbol{G}\\right)^{-1}\n\\]\nand\n\\[\n\\widehat{\\Sigma}(\\boldsymbol{G})=\\frac{1}{n}\\left(\\widetilde{\\boldsymbol{Y}}^{\\prime} \\tilde{\\boldsymbol{Y}}-\\widetilde{\\boldsymbol{Y}}^{\\prime} \\widetilde{\\boldsymbol{X}} \\boldsymbol{G}\\left(\\boldsymbol{G}^{\\prime} \\widetilde{\\boldsymbol{X}}^{\\prime} \\widetilde{\\boldsymbol{X}} \\boldsymbol{G}\\right)^{-1} \\boldsymbol{G}^{\\prime} \\widetilde{\\boldsymbol{X}}^{\\prime} \\widetilde{\\boldsymbol{Y}}\\right)\n\\]\nwhere \\(\\tilde{\\boldsymbol{Y}}=\\boldsymbol{Y}-\\boldsymbol{Z}\\left(\\boldsymbol{Z}^{\\prime} \\boldsymbol{Z}\\right)^{-1} \\boldsymbol{Z}^{\\prime} \\boldsymbol{Y}\\) and \\(\\widetilde{\\boldsymbol{X}}=\\boldsymbol{X}-\\boldsymbol{Z}\\left(\\boldsymbol{Z}^{\\prime} \\boldsymbol{Z}\\right)^{-1} \\boldsymbol{Z}^{\\prime} \\boldsymbol{X}\\).\nSubstituting these estimators into the log-likelihood function we obtain the concentrated likelihood function, which is a function of \\(G\\) only.\n\\[\n\\begin{aligned}\n\\widetilde{\\ell}_{n}(\\boldsymbol{G}) &=\\ell_{n}(\\boldsymbol{G}, \\widehat{\\boldsymbol{A}}(\\boldsymbol{G}), \\widehat{\\boldsymbol{C}}(\\boldsymbol{G}), \\widehat{\\Sigma}(\\boldsymbol{G})) \\\\\n&=\\frac{m}{2}(n \\log (2 \\pi)-1)-\\frac{n}{2} \\log \\left[\\operatorname{det}\\left(\\widetilde{\\boldsymbol{Y}}^{\\prime} \\widetilde{\\boldsymbol{Y}}-\\widetilde{\\boldsymbol{Y}}^{\\prime} \\widetilde{\\boldsymbol{X}} \\boldsymbol{G}\\left(\\boldsymbol{G}^{\\prime} \\widetilde{\\boldsymbol{X}}^{\\prime} \\widetilde{\\boldsymbol{X}} \\boldsymbol{G}\\right)^{-1} \\boldsymbol{G}^{\\prime} \\widetilde{\\boldsymbol{X}}^{\\prime} \\widetilde{\\boldsymbol{Y}}\\right)\\right] \\\\\n&=\\frac{m}{2}(n \\log (2 \\pi)-1)-\\frac{n}{2} \\log \\left(\\operatorname{det}\\left(\\widetilde{\\boldsymbol{Y}}^{\\prime} \\widetilde{\\boldsymbol{Y}}\\right)\\right)-\\frac{n}{2} \\log \\left[\\frac{\\operatorname{det}\\left(\\boldsymbol{G}^{\\prime}\\left(\\widetilde{\\boldsymbol{X}}^{\\prime} \\widetilde{\\boldsymbol{X}}-\\widetilde{\\boldsymbol{X}}^{\\prime} \\tilde{\\boldsymbol{Y}}\\left(\\widetilde{\\boldsymbol{Y}}^{\\prime} \\widetilde{\\boldsymbol{Y}}\\right)^{-1} \\boldsymbol{Y}^{\\prime} \\widetilde{\\boldsymbol{X}}\\right) \\boldsymbol{G}\\right)}{\\operatorname{det}\\left(\\boldsymbol{G}^{\\prime} \\widetilde{\\boldsymbol{X}}^{\\prime} \\widetilde{\\boldsymbol{X}} \\boldsymbol{G}\\right)} .\\right.\n\\end{aligned}\n\\]\nThe third equality uses Theorem A.1.8. The MLE \\(\\widehat{\\boldsymbol{G}}\\) for \\(\\boldsymbol{G}\\) is the maximizer of \\(\\widetilde{\\ell}_{n}(\\boldsymbol{G})\\), or equivalently equals\n\\[\n\\begin{aligned}\n\\widehat{\\boldsymbol{G}} &=\\underset{\\boldsymbol{G}}{\\operatorname{argmin}} \\frac{\\operatorname{det}\\left(\\boldsymbol{G}^{\\prime}\\left(\\widetilde{\\boldsymbol{X}}^{\\prime} \\widetilde{\\boldsymbol{X}}-\\widetilde{\\boldsymbol{X}}^{\\prime} \\widetilde{\\boldsymbol{Y}}\\left(\\tilde{\\boldsymbol{Y}}^{\\prime} \\widetilde{\\boldsymbol{Y}}\\right)^{-1} \\boldsymbol{Y}^{\\prime} \\tilde{\\boldsymbol{X}}\\right) \\boldsymbol{G}\\right)}{\\operatorname{det}\\left(\\boldsymbol{G}^{\\prime} \\widetilde{\\boldsymbol{X}}^{\\prime} \\widetilde{\\boldsymbol{X}} \\boldsymbol{G}\\right)} \\\\\n&=\\underset{\\boldsymbol{G}}{\\operatorname{argmax}} \\frac{\\operatorname{det}\\left(\\boldsymbol{G}^{\\prime} \\widetilde{\\boldsymbol{X}}^{\\prime} \\widetilde{\\boldsymbol{Y}}\\left(\\widetilde{\\boldsymbol{Y}}^{\\prime} \\widetilde{\\boldsymbol{Y}}\\right)^{-1} \\boldsymbol{Y}^{\\prime} \\widetilde{\\boldsymbol{X}} \\boldsymbol{G}\\right)}{\\operatorname{det}\\left(\\boldsymbol{G}^{\\prime} \\widetilde{\\boldsymbol{X}}^{\\prime} \\widetilde{\\boldsymbol{X}} \\boldsymbol{G}\\right)} \\\\\n&=\\left\\{v_{1}, \\ldots, v_{r}\\right\\}\n\\end{aligned}\n\\]\nwhich are the generalized eigenvectors of \\(\\widetilde{\\boldsymbol{X}}^{\\prime} \\tilde{\\boldsymbol{Y}}\\left(\\tilde{\\boldsymbol{Y}}^{\\prime} \\tilde{\\boldsymbol{Y}}\\right)^{-1} \\boldsymbol{Y}^{\\prime} \\tilde{\\boldsymbol{X}}\\) with respect to \\(\\widetilde{\\boldsymbol{X}}^{\\prime} \\widetilde{\\boldsymbol{X}}\\) corresponding to the \\(r\\) largest generalized eigenvalues. (Generalized eigenvalues and eigenvectors are discussed in Section A.14.) The estimator satisfies the normalization \\(\\widehat{\\boldsymbol{G}}^{\\prime} \\widetilde{\\boldsymbol{X}}^{\\prime} \\widetilde{\\boldsymbol{X}} \\widehat{\\boldsymbol{G}}=\\boldsymbol{I}_{r}\\). Letting \\(v_{j}^{*}\\) denote the eigenvectors of (11.20) we can also express \\(\\widehat{\\boldsymbol{G}}=\\left\\{v_{m}^{*}, \\ldots, v_{m-r+1}^{*}\\right\\}\\).\nThis is computationally straightforward. In MATLAB, for example, the generalized eigenvalues and eigenvectors of a matrix \\(\\boldsymbol{A}\\) with respect to \\(\\boldsymbol{B}\\) are found using the command eig \\((\\mathrm{A}, \\mathrm{B})\\).\nGiven \\(\\widehat{\\boldsymbol{G}}\\), the MLE \\(\\widehat{\\boldsymbol{A}}, \\widehat{\\boldsymbol{C}}, \\widehat{\\Sigma}\\) are found by least squares regression of \\(Y\\) on \\(\\widehat{\\boldsymbol{G}}^{\\prime} X\\) and \\(Z\\). In particular, \\(\\widehat{\\boldsymbol{A}}=\\widehat{\\boldsymbol{G}}^{\\prime} \\widetilde{\\boldsymbol{X}}^{\\prime} \\tilde{\\boldsymbol{Y}}\\) because \\(\\widehat{\\boldsymbol{G}}^{\\prime} \\widetilde{\\boldsymbol{X}}^{\\prime} \\widetilde{\\boldsymbol{X}} \\widehat{\\boldsymbol{G}}=\\boldsymbol{I}_{r}\\) We now discuss the estimator \\(\\widehat{\\boldsymbol{A}}_{\\perp}\\) of \\(\\boldsymbol{A}_{\\perp}\\). It turns out that\n\\[\n\\begin{aligned}\n\\widehat{\\boldsymbol{A}}_{\\perp} &=\\underset{\\boldsymbol{A}}{\\operatorname{argmax}} \\frac{\\operatorname{det}\\left(\\boldsymbol{A}^{\\prime}\\left(\\tilde{\\boldsymbol{Y}}^{\\prime} \\tilde{\\boldsymbol{Y}}-\\tilde{\\boldsymbol{Y}}^{\\prime} \\tilde{\\boldsymbol{X}}\\left(\\tilde{\\boldsymbol{X}}^{\\prime} \\tilde{\\boldsymbol{X}}\\right)^{-1} \\widetilde{\\boldsymbol{X}}^{\\prime} \\tilde{\\boldsymbol{Y}}\\right) \\boldsymbol{A}\\right)}{\\operatorname{det}\\left(\\boldsymbol{A}^{\\prime} \\tilde{\\boldsymbol{Y}}^{\\prime} \\tilde{\\boldsymbol{Y}} \\boldsymbol{A}\\right)} \\\\\n&=\\left\\{w_{1}, \\ldots, w_{m-r}\\right\\}\n\\end{aligned}\n\\]\nthe eigenvectors of \\(\\tilde{\\boldsymbol{Y}}^{\\prime} \\tilde{\\boldsymbol{Y}}-\\tilde{\\boldsymbol{Y}}^{\\prime} \\tilde{\\boldsymbol{X}}\\left(\\tilde{\\boldsymbol{X}}^{\\prime} \\tilde{\\boldsymbol{X}}\\right)^{-1} \\tilde{\\boldsymbol{X}}^{\\prime} \\tilde{\\boldsymbol{Y}}\\) with respect to \\(\\tilde{\\boldsymbol{Y}}^{\\prime} \\tilde{\\boldsymbol{Y}}\\) associated with the largest \\(m-r\\) eigenvalues.\nBy the dual eigenvalue relation (Theorem A.5), equations (11.20) and (11.21) have the same non-zero eigenvalues \\(\\lambda_{j}\\) and the associated eigenvectors \\(v_{j}^{*}\\) and \\(w_{j}\\) satisfy the relationship\n\\[\nw_{j}=\\lambda_{j}^{-1 / 2}\\left(\\tilde{\\boldsymbol{Y}}^{\\prime} \\tilde{\\boldsymbol{Y}}\\right)^{-1} \\tilde{\\boldsymbol{Y}}^{\\prime} \\widetilde{\\boldsymbol{X}} v_{j}^{*}\n\\]\nLetting \\(\\Lambda=\\operatorname{diag}\\left\\{\\lambda_{m}, \\ldots, \\lambda_{m-r+1}\\right\\}\\) this implies\n\\[\n\\left\\{w_{m}, \\ldots, w_{m-r+1}\\right\\}=\\left(\\widetilde{\\boldsymbol{Y}}^{\\prime} \\widetilde{\\boldsymbol{Y}}\\right)^{-1} \\tilde{\\boldsymbol{Y}}^{\\prime} \\widetilde{\\boldsymbol{X}}\\left\\{v_{m}^{*}, \\ldots, v_{m-r+1}^{*}\\right\\} \\Lambda=\\left(\\tilde{\\boldsymbol{Y}}^{\\prime} \\widetilde{\\boldsymbol{Y}}\\right)^{-1} \\widehat{\\boldsymbol{A}} \\Lambda .\n\\]\nThe second equality holds because \\(\\widehat{\\boldsymbol{G}}=\\left\\{v_{m}^{*}, \\ldots, v_{m-r+1}^{*}\\right\\}\\) and \\(\\widehat{\\boldsymbol{A}}=\\widetilde{\\boldsymbol{Y}}^{\\prime} \\widetilde{\\boldsymbol{X}} \\widehat{\\boldsymbol{G}}\\). Since the eigenvectors \\(w_{j}\\) satisfy the orthogonality property \\(w_{j}^{\\prime} \\widetilde{\\boldsymbol{Y}}^{\\prime} \\widetilde{\\boldsymbol{Y}} w_{\\ell}=0\\) for \\(j \\neq \\ell\\), it follows that\n\\[\n0=\\widehat{\\boldsymbol{A}}_{\\perp}^{\\prime} \\tilde{\\boldsymbol{Y}}^{\\prime} \\tilde{\\boldsymbol{Y}}\\left\\{w_{m}, \\ldots, w_{m-r+1}\\right\\}=\\widehat{\\boldsymbol{A}}_{\\perp}^{\\prime} \\widehat{\\boldsymbol{A}} \\Lambda .\n\\]\nSince \\(\\Lambda>0\\) we conclude that \\(\\widehat{A}_{\\perp}^{\\prime} \\widehat{A}=0\\) as desired.\nThe solution \\(\\widehat{A}_{\\perp}\\) in (11.21) can be represented several ways. One which is computationally convenient is to observe that\n\\[\n\\tilde{\\boldsymbol{Y}}^{\\prime} \\widetilde{\\boldsymbol{Y}}-\\widetilde{\\boldsymbol{Y}}^{\\prime} \\widetilde{\\boldsymbol{X}}\\left(\\widetilde{\\boldsymbol{X}}^{\\prime} \\widetilde{\\boldsymbol{X}}\\right)^{-1} \\widetilde{\\boldsymbol{Y}}^{\\prime} \\widetilde{\\boldsymbol{X}}=\\boldsymbol{Y}^{\\prime} \\boldsymbol{M}_{\\boldsymbol{X}, \\boldsymbol{Z}} \\boldsymbol{Y}=\\widetilde{\\boldsymbol{E}}^{\\prime} \\widetilde{\\boldsymbol{E}}\n\\]\nwhere \\(\\boldsymbol{M}_{\\boldsymbol{X}, \\boldsymbol{Z}}=\\boldsymbol{I}_{n}-(\\boldsymbol{X}, \\boldsymbol{Z})\\left((\\boldsymbol{X}, \\boldsymbol{Z})^{\\prime}(\\boldsymbol{X}, \\boldsymbol{Z})\\right)^{-1}(\\boldsymbol{X}, \\boldsymbol{Z})^{\\prime}\\) and \\(\\widetilde{\\boldsymbol{E}}=\\boldsymbol{M}_{\\boldsymbol{X}, \\boldsymbol{Z}} \\boldsymbol{Y}\\) is the residual matrix from the unrestricted multivariate least squares regression of \\(Y\\) on \\(X\\) and \\(\\boldsymbol{Z}\\). The first equality follows by the FrischWaugh-Lovell theorem. This shows that \\(\\widehat{\\boldsymbol{A}}_{\\perp}\\) are the generalized eigenvectors of \\(\\widetilde{\\boldsymbol{E}}^{\\prime} \\widetilde{\\boldsymbol{E}}\\) with respect to \\(\\widetilde{\\boldsymbol{Y}}^{\\prime} \\tilde{\\boldsymbol{Y}}\\) corresponding to the \\(m-r\\) largest eigenvalues. In MATLAB, for example, these can be computed using the eig \\((\\mathrm{A}, \\mathrm{B})\\) command.\nAnother representation is to write \\(\\boldsymbol{M}_{Z}=\\boldsymbol{I}_{n}-\\boldsymbol{Z}\\left(\\boldsymbol{Z}^{\\prime} \\boldsymbol{Z}\\right)^{-1} \\boldsymbol{Z}^{\\prime}\\) so that\n\\[\n\\widehat{A}_{\\perp}=\\underset{A}{\\operatorname{argmax}} \\frac{\\operatorname{det}\\left(\\boldsymbol{A}^{\\prime} \\boldsymbol{Y}^{\\prime} \\boldsymbol{M}_{\\boldsymbol{X}, \\boldsymbol{Z}} \\boldsymbol{Y} \\boldsymbol{A}\\right)}{\\operatorname{det}\\left(\\boldsymbol{A}^{\\prime} \\boldsymbol{Y}^{\\prime} \\boldsymbol{M}_{\\boldsymbol{Z}} \\boldsymbol{Y} \\boldsymbol{A}\\right)}=\\underset{\\boldsymbol{A}}{\\operatorname{argmin}} \\frac{\\operatorname{det}\\left(\\boldsymbol{A}^{\\prime} \\boldsymbol{Y}^{\\prime} \\boldsymbol{M}_{\\boldsymbol{Z}} \\boldsymbol{Y} \\boldsymbol{A}\\right)}{\\operatorname{det}\\left(\\boldsymbol{A}^{\\prime} \\boldsymbol{Y}^{\\prime} \\boldsymbol{M}_{\\boldsymbol{X}, \\boldsymbol{Z}} \\boldsymbol{Y} \\boldsymbol{A}\\right)} .\n\\]\nWe summarize our findings. Theorem 11.7 The MLE for the reduced rank model (11.19) under \\(e \\sim \\mathrm{N}(0, \\Sigma)\\) is given as follows. Let \\(\\tilde{\\boldsymbol{Y}}\\) and \\(\\widetilde{\\boldsymbol{X}}\\) be the residual matrices from multivariate regression of \\(\\boldsymbol{Y}\\) and \\(\\boldsymbol{X}\\) on \\(\\boldsymbol{Z}\\), respectively. Then \\(\\widehat{\\boldsymbol{G}}_{\\mathrm{mle}}=\\left\\{v_{1}, \\ldots, v_{r}\\right\\}\\), the generalized eigenvectors of \\(\\widetilde{\\boldsymbol{X}}^{\\prime} \\tilde{\\boldsymbol{Y}}\\left(\\widetilde{\\boldsymbol{Y}}^{\\prime} \\tilde{\\boldsymbol{Y}}\\right)^{-1} \\boldsymbol{Y}^{\\prime} \\widetilde{\\boldsymbol{X}}\\) with respect to \\(\\widetilde{\\boldsymbol{X}}^{\\prime} \\widetilde{\\boldsymbol{X}}\\) corresponding to the \\(r\\) largest eigenvalues \\(\\widehat{\\lambda}_{j} . \\widehat{\\boldsymbol{A}}_{\\text {mle }}, \\widehat{\\boldsymbol{C}}_{\\text {mle }}\\) and \\(\\widehat{\\Sigma}_{\\text {mle }}\\) are obtained by the least squares regression\n\\[\n\\begin{aligned}\nY_{i} &=\\widehat{\\boldsymbol{A}}_{\\mathrm{mle}} \\widehat{\\boldsymbol{G}}_{\\mathrm{mle}}^{\\prime} X_{i}+\\widehat{\\boldsymbol{C}}_{\\mathrm{mle}}^{\\prime} Z_{i}+\\widehat{e}_{i} \\\\\n\\widehat{\\Sigma}_{\\mathrm{mle}} &=\\frac{1}{n} \\sum_{i=1}^{n} \\widehat{e}_{i} \\widehat{e}_{i}^{\\prime}\n\\end{aligned}\n\\]\nLet \\(\\widetilde{\\boldsymbol{E}}\\) be the residual matrix from a multivariate regression of \\(\\boldsymbol{Y}\\) on \\(\\boldsymbol{X}\\) and \\(\\boldsymbol{Z}\\). Then \\(\\widehat{\\boldsymbol{A}}_{\\perp}\\) equals the generalized eigenvectors of \\(\\widetilde{\\boldsymbol{E}} \\widetilde{\\boldsymbol{E}}^{\\prime}\\) with respect to \\(\\widetilde{\\boldsymbol{Y}}^{\\prime} \\widetilde{\\boldsymbol{Y}}\\) corresponding to the \\(m-r\\) smallest eigenvalues. The maximized likelihood equals\n\\[\n\\ell_{n}=\\frac{m}{2}(n \\log (2 \\pi)-1)-\\frac{n}{2} \\log \\left(\\operatorname{det}\\left(\\widetilde{\\boldsymbol{Y}}^{\\prime} \\tilde{\\boldsymbol{Y}}\\right)\\right)-\\frac{n}{2} \\sum_{j=1}^{r} \\log \\left(1-\\widehat{\\lambda}_{j}\\right) .\n\\]\nAn R package for reduced rank regression is “RRR”. I am unaware of a Stata command."
  },
  {
    "objectID": "chpt11-multi-reg.html#principal-component-analysis",
    "href": "chpt11-multi-reg.html#principal-component-analysis",
    "title": "11  Multivariate Regression",
    "section": "11.12 Principal Component Analysis",
    "text": "11.12 Principal Component Analysis\nIn Section \\(4.21\\) we described the Duflo, Dupas, and Kremer (2011) dataset which is a sample of Kenyan first grade test scores. Following the authors we focused on the variable totalscore which is each student’s composite test score. If you examine the data file you will find other pieces of information about the students’ performance, including each student’s score on separate sections of the test, with the labels wordscore (word recognition), sentscore (sentence recognition), letterscore (letter recognition), spellscore (spelling), additions_score (addition), substractions_score (subtraction), multiplications_score (multiplication). The “total” score sums the scores from the individual sections. Perhaps there is more information in the section scores. How can we learn about this from the data?\nPrincipal component analysis (PCA) addresses this issue by ordering linear combinations by their contribution to variance. Definition \\(11.1\\) Let \\(X\\) be a \\(k \\times 1\\) random vector.\nThe first principal component is \\(U_{1}=h_{1}^{\\prime} X\\) where \\(h_{1}\\) satisfies\n\\[\nh_{1}=\\underset{h^{\\prime} h=1}{\\operatorname{argmax}} \\operatorname{var}\\left[h^{\\prime} X\\right] .\n\\]\nThe second principal component is \\(U_{2}=h_{2}^{\\prime} X\\) where\n\\[\nh_{2}=\\underset{h^{\\prime} h=1, h^{\\prime} h_{1}=0}{\\operatorname{argmax}} \\operatorname{var}\\left[h^{\\prime} X\\right] .\n\\]\nIn general, the \\(j^{t h}\\) principal component is \\(U_{j}=h_{j}^{\\prime} X\\) where\n\\[\nh_{j}=\\underset{h^{\\prime} h=1, h^{\\prime} h_{1}=0, \\ldots, h^{\\prime} h_{j-1}=0}{\\operatorname{argmax}} \\operatorname{var}\\left[h^{\\prime} X\\right] .\n\\]\nThe principal components of \\(X\\) are linear combinations \\(h^{\\prime} X\\) ranked by contribution to variance. By the properties of quadratic forms (Section A.15) the weight vectors \\(h_{j}\\) are the eigenvectors of \\(\\Sigma=\\operatorname{var}[X]\\).\nTheorem 11.8 The principal components of \\(X\\) are \\(U_{j}=h_{j}^{\\prime} X\\), where \\(h_{j}\\) is the eigenvector of \\(\\Sigma\\) associated with the \\(j^{\\text {th }}\\) ordered eigenvalue \\(\\lambda_{j}\\) of \\(\\Sigma\\).\nAnother way to see the PCA construction is as follows. Since \\(\\Sigma\\) is symmetric the spectral decomposition (Theorem A.3) states that \\(\\Sigma=\\boldsymbol{H} \\boldsymbol{D} \\boldsymbol{H}^{\\prime}\\) where \\(\\boldsymbol{H}=\\left[h_{1}, \\ldots, h_{k}\\right]\\) and \\(\\boldsymbol{D}=\\operatorname{diag}\\left(d_{1}, \\ldots, d_{k}\\right)\\) are the eigenvectors and eigenvalues of \\(\\Sigma\\). Since \\(\\Sigma\\) is positive semi-definite the eigenvalues are real, non-negative, and ordered \\(d_{1} \\geq d_{2} \\geq \\cdots \\geq d_{k}\\). Let \\(U=\\left(U_{1}, \\ldots, U_{k}\\right)\\) be the principal components of \\(X\\). By Theorem 11.8, \\(U=\\boldsymbol{H}^{\\prime} X\\). The covariance matrix of \\(U\\) is\n\\[\n\\operatorname{var}[U]=\\operatorname{var}\\left[\\boldsymbol{H}^{\\prime} X\\right]=\\boldsymbol{H}^{\\prime} \\Sigma \\boldsymbol{H}=\\boldsymbol{D}\n\\]\nwhich is diagonal. This shows that \\(\\operatorname{var}\\left[U_{j}\\right]=d_{j}\\) and the principal components are mutually uncorrelated. The relative variance contribution of the \\(j^{t h}\\) principal component is \\(d_{j} / \\operatorname{tr}(\\Sigma)\\).\nPrincipal components are sensitive to the scaling of \\(X\\). Consequently, it is recommended to first scale each element of \\(X\\) to have mean zero and unit variance. In this case \\(\\Sigma\\) is a correlation matrix.\nThe sample principal components are obtained by replacing the unknowns by sample estimators. Let \\(\\widehat{\\Sigma}\\) be the sample covariance or correlation matrix and \\(\\widehat{h}_{1}, \\widehat{h}_{2}, \\ldots, \\widehat{h}_{k}\\) its ordered eigenvectors. The sample principal components are \\(\\widehat{h}_{j}^{\\prime} X_{i}\\).\nTo illustrate we use the Duflo, Dupas, and Kremer (2011) dataset. In Table \\(11.1\\) we display the seven eigenvalues of the sample correlation matrix for the seven test scores described above. The seven eigenvalues sum to seven because we have applied PCA to the correlation matrix. The first eigenvalue is \\(4.0\\), implying that the first principal component explains \\(57 %\\) of the variance of the seven test scores. The second eigenvalue is \\(1.0\\), implying that the second principal component explains \\(15 %\\) of the variance. Together the first two components explain \\(72 %\\) of the variance of the seven test scores.\nIn Table \\(11.2\\) we display the weight vectors (eigenvectors) for the first two principal components. The weights for the first component are all positive and similar in magnitude. This means that the first Table 11.1: Eigenvalue Decomposition of Sample Correlation Matrix\n|Eigenvalue|Proportion|\n|:|———-|———-| |1| \\(4.02\\) | \\(0.57\\) | |2| \\(1.04\\) | \\(0.15\\) | |3| \\(0.57\\) | \\(0.08\\) | |4| \\(0.52\\) | \\(0.08\\) | |5| \\(0.37\\) | \\(0.05\\) | |6| \\(0.29\\) | \\(0.04\\) | |7| \\(0.19\\) | \\(0.03\\) |\nTable 11.2: Principal Component Weight Vectors\n\n\n\n\nFirst\nSecond\n\n\n\n\nwords\n\\(0.41\\)\n\\(-0.32\\)\n\n\nsentences\n\\(0.32\\)\n\\(-0.49\\)\n\n\nletters\n\\(0.40\\)\n\\(-0.13\\)\n\n\nspelling\n\\(0.43\\)\n\\(-0.28\\)\n\n\naddition\n\\(0.38\\)\n\\(0.41\\)\n\n\nsubtraction\n\\(0.35\\)\n\\(0.52\\)\n\n\nmultiplication\n\\(0.33\\)\n\\(0.36\\)\n\n\n\nprincipal component is similar to a simple average of the seven test scores. This is quite fascinating. This is consistent with our intuition that a simple average (e.g. the variable totalscore) captures most of the information contained in the seven test scores. The weights for the second component have a different pattern. The four literacy scores receive negative weight and the three math scores receive positive weight with similar magnitudes. This means that the second principal component is similar to the difference between a student’s math and verbal test scores. Taken together, the information in the first two principal components is equivalent to “average verbal” and “average math” test scores. What this shows is that \\(57 %\\) of the variation in the seven section test scores can be explained by a simple average (e.g. totalscore), and \\(72 %\\) can be explained by averages for the verbal and math halves of the test.\nIn Stata, principal components analysis can be implemented with the pca command. In \\(\\mathrm{R}\\) use prcomp or princomp. All three can be applied to either covariance matrices (unscaled data) or correlation matrices (normalized data) but they have different default settings. The Stata pca command by default normalizes the observations. The R commands by default do not normalize the observations."
  },
  {
    "objectID": "chpt11-multi-reg.html#factor-models",
    "href": "chpt11-multi-reg.html#factor-models",
    "title": "11  Multivariate Regression",
    "section": "11.13 Factor Models",
    "text": "11.13 Factor Models\nClosely related to principal components are factor models. These are statistical models which decompose random vectors into common factors and idiosyncratic errors. Factor models are popular throughout the social sciences. Consequently a variety of estimation methods have been developed. In the next few sections we focus on methods which are popular among economists.\nLet \\(X=\\left(X_{1}, \\ldots, X_{k}\\right)^{\\prime}\\) be a \\(k \\times 1\\) random vector (for example the seven test scores described in the previous section). Assume that the elements of \\(X\\) are scaled to have mean zero and unit variance.\nA single factor model for \\(X\\) is\n\\[\nX=\\lambda F+u\n\\]\nwhere \\(\\lambda \\in \\mathbb{R}^{k}\\) are factor loadings, \\(F \\in \\mathbb{R}\\) is a common factor, and \\(u \\in \\mathbb{R}^{k}\\) is a random error. The factor \\(F\\) is individual-specific while the coefficient \\(\\lambda\\) is common across individuals. The model (11.22) specifies that correlation between the elements of \\(X\\) is due to the common factor \\(F\\). In the student test score example it is intuitive to think of \\(F\\) as a student’s scholastic “aptitude”; in this case the vector \\(\\lambda\\) describes how scholastic aptitude affects the seven subject scores.\nA multiple factor model has \\(r<k\\) factors. We write the model as\n\\[\nX=\\Lambda F+u\n\\]\nwhere \\(\\Lambda\\) is a \\(k \\times r\\) matrix of factor loadings and \\(F=\\left(F_{1}, \\ldots, F_{r}\\right)^{\\prime}\\) is an \\(r \\times 1\\) vector of factors. In the student test score example possible factors could be “math aptitude”, “language skills”, “social skills”, “artistic ability”, “creativity”, etc. The factor loading matrix \\(\\Lambda\\) indicates the effect of each factor on each test score. The number of factors \\(r\\) is taken as known. We discuss selection of \\(r\\) later.\nThe error vector \\(u\\) is assumed to be mean zero, uncorrelated with \\(F\\), and (under correct specification) to have mutually uncorrelated elements. We write its covariance matrix as \\(\\Psi=\\mathbb{E}\\left[u u^{\\prime}\\right]\\). The factor vector \\(F\\) can either be treated as random or as a regressor. In this section we treat \\(F\\) as random; in the next we treat \\(F\\) as regressors. The random factors \\(F\\) are assumed mean zero and are normalized so that \\(\\mathbb{E}\\left[F F^{\\prime}\\right]=\\) \\(\\boldsymbol{I}_{r}\\)\nThe assumptions imply that the correlation matrix \\(\\Sigma=\\mathbb{E}\\left[X X^{\\prime}\\right]\\) equals\n\\[\n\\Sigma=\\Lambda \\Lambda^{\\prime}+\\Psi .\n\\]\nThe factor analysis literature often describes \\(\\Lambda \\Lambda^{\\prime}\\) as the communality and the idiosyncratic error matrix \\(\\Psi\\) as the uniqueness. The former is the portion of the variance which is explained by the factor model and the latter is the unexplained portion of the variance.\nThe model is often \\({ }^{1}\\) estimated by maximum likelihood. Under joint normality of \\((F, u)\\) the distribution of \\(X\\) is \\(\\mathrm{N}\\left(0, \\Lambda \\Lambda^{\\prime}+\\Psi\\right)\\). The parameters are \\(\\Lambda\\) and \\(\\Psi=\\operatorname{diag}\\left(\\psi_{1}, \\ldots, \\psi_{k}\\right)\\). The log-likelihood function of a random sample \\(\\left(X_{1}, \\ldots, X_{n}\\right)\\) is\n\\[\n\\ell_{n}(\\Lambda, \\Psi)=-\\frac{n k}{2} \\log (2 \\pi)-\\frac{n}{2} \\log \\operatorname{det}\\left(\\Lambda \\Lambda^{\\prime}+\\Psi\\right)-\\frac{n}{2} \\operatorname{tr}\\left(\\left(\\Lambda \\Lambda^{\\prime}+\\Psi\\right)^{-1} \\widehat{\\Sigma}\\right) .\n\\]\nThe \\(\\operatorname{MLE}(\\widehat{\\Lambda}, \\widehat{\\Psi})\\) maximizes \\(\\ell_{n}(\\Lambda, \\Psi)\\). There is not an algebraic solution so the estimator is found using numerical methods. Fortunately, computational algorithms are available in standard packages. A detailed description and analysis can be found in Anderson (2003, Chapter 14).\nThe form of the log-likelihood is intriguing. Notice that the log-likelihood is only a function of the observations through its correlation matrix \\(\\widehat{\\Sigma}\\), and only a function of the parameters through the population correlation matrix \\(\\Lambda \\Lambda^{\\prime}+\\Psi\\). The final term in (11.25) is a measure of the match between \\(\\widehat{\\Sigma}\\) and \\(\\Lambda \\Lambda^{\\prime}+\\Psi\\). Together, we see that the Gaussian log-likelihood is essentially a measure of the fit of the model and sample correlation matrices. It is therefore not reliant on the normality assumption.\nIt is often of interest to estimate the factors \\(F_{i}\\). Given \\(\\Lambda\\) the equation \\(X_{i}=\\Lambda F_{i}+u_{i}\\) can be viewed as a regression with coefficient \\(F_{i}\\). Its least squares estimator is \\(\\widehat{F}_{i}=\\left(\\Lambda^{\\prime} \\Lambda\\right)^{-1} \\Lambda^{\\prime} X_{i}\\). The GLS estimator (taking into account the covariance matrix of \\(\\left.u_{i}\\right)\\) is \\(\\widehat{F}_{i}=\\left(\\Lambda^{\\prime} \\Psi^{-1} \\Lambda\\right)^{-1} \\Lambda^{\\prime} \\Psi^{-1} X_{i}\\). This motivates the Bartlett scoring estimator\n\\[\n\\widetilde{F}_{i}=\\left(\\widehat{\\Lambda}^{\\prime} \\widehat{\\Psi}^{-1} \\widehat{\\Lambda}\\right)^{-1} \\widehat{\\Lambda}^{\\prime} \\widehat{\\Psi}^{-1} X_{i} .\n\\]\nThe idealized version satisfies\n\\[\n\\widehat{F}_{i}=\\left(\\Lambda^{\\prime} \\Psi^{-1} \\Lambda\\right)^{-1} \\Lambda^{\\prime} \\Psi^{-1}\\left(\\Lambda F_{i}+u_{i}\\right)=F_{i}+\\left(\\Lambda^{\\prime} \\Psi^{-1} \\Lambda\\right)^{-1} \\Lambda^{\\prime} \\Psi^{-1} u_{i}\n\\]\n\\({ }^{1}\\) There are other estimators used in applied factor analysis. However there is little reason to consider estimators beyond the MLE of this section and the principal components estimator of the next section. which is unbiased for \\(F_{i}\\) and has variance \\(\\left(\\Lambda^{\\prime} \\Psi^{-1} \\Lambda\\right)^{-1}\\). Thus the Barlett scoring estimator is typically described as “unbiased” though this is actually a property of its idealized version \\(\\widehat{F}_{i}\\).\nA second estimator for the factors can be constructed from the multivariate linear projection of \\(F\\) on \\(X\\). This is \\(F=A X+\\xi\\) where the coefficient matrix \\(\\boldsymbol{A}\\) is \\(r \\times k\\). The coefficient matrix equals\n\\[\n\\boldsymbol{A}=\\mathbb{E}\\left[F X^{\\prime}\\right] \\mathbb{E}\\left[X X^{\\prime}\\right]^{-1}=\\Lambda^{\\prime} \\Sigma^{-1},\n\\]\nthe second equation using \\(\\mathbb{E}\\left[F X^{\\prime}\\right]=\\mathbb{E}\\left[F(\\Lambda F+u)^{\\prime}\\right]=\\mathbb{E}\\left[F F^{\\prime}\\right] \\Lambda^{\\prime}+\\mathbb{E}\\left[F u^{\\prime}\\right]=\\Lambda^{\\prime}\\). The predicted value of \\(F_{i}\\) is \\(F_{i}^{*}=\\boldsymbol{A} X_{i}=\\Lambda^{\\prime} \\Sigma^{-1} X_{i}\\). This motivates the regression scoring estimator\n\\[\n\\bar{F}_{i}=\\widehat{\\Lambda}^{\\prime} \\widehat{\\Sigma}^{-1} X_{i} .\n\\]\nThe idealized version \\(F_{i}^{*}\\) has conditional expectation \\(\\Lambda^{\\prime} \\Sigma^{-1} \\Lambda F_{i}\\) and is thus biased for \\(F_{i}\\). Hence the regression scoring estimator \\(\\bar{F}_{i}\\) is often described as “biased”. Some algebraic manipulations reveal that \\(F_{i}^{*}\\) has MSE \\(\\boldsymbol{I}_{r}-\\Lambda^{\\prime}\\left(\\Lambda^{\\prime} \\Lambda+\\Psi\\right)^{-1} \\Lambda\\) which is smaller (in a positive definite sense) than the MSE of the idealized Bartlett estimator \\(\\widehat{F}_{i}\\).\nWhich estimator is preferred, Bartlett or regression scoring? The differences diminish when \\(k\\) is large so the choice is most relevant for small to moderate \\(k\\). The regression scoring estimator has lower approximate MSE, meaning that it is a more precise estimator. Thus based on estimation precision this is our recommended choice.\nThe factor loadings \\(\\Lambda\\) and factors \\(F\\) are not separately identified. To see this, notice that if you replace \\((\\Lambda, F)\\) with \\(\\Lambda^{*}=\\Lambda \\boldsymbol{G}\\) and \\(F^{*}=\\boldsymbol{G}^{\\prime} F\\) where \\(\\boldsymbol{G}\\) is \\(r \\times r\\) and orthonormal then the regression model is identical. Such replacements are called “rotations” in the factor analysis literature. Any orthogonal rotation of the factor loadings is an equally valid representation. The default MLE outputs are one specific rotation; others can be obtained by a variety of algorithms (which we do not review here). Consequently it is unwise to attribute meaning to the individual factor loading estimates.\nAnother important and tricky issue is selection of the number of factors \\(r\\). There is no clear guideline. One approach is to examine the principal component decomposition, look for a division between the “large” and “small eigenvalues, and set \\(r\\) to equal to the number of”large” eigenvalues. Another approach is based on testing. As a by-product of the MLE (and standard package implementations) we obtain the LR test for the null hypothesis of \\(r\\) factors against the alternative hypothesis of \\(k\\) factors. If the LR test rejects (has a small p-value) this is evidence that the given \\(r\\) may be too small.\nIn Stata, the \\(\\operatorname{MLE}(\\widehat{\\Lambda}, \\widehat{\\Psi})\\) can be calculated with the factor, ml factors (r) command. The factor estimates \\(\\widetilde{F}_{i}\\) and \\(\\bar{F}_{i}\\) can be calculated by the predict command with either the barlett or regression option, respectively. In \\(R\\), the command factanal ( \\(X\\), factors=r, rotation=“none”) calculates the \\(\\operatorname{MLE}(\\widehat{\\Lambda}, \\widehat{\\Psi})\\) and also calculates the factor estimates \\(\\widetilde{F}_{i}\\) and/or \\(\\bar{F}_{i}\\) using the scores option."
  },
  {
    "objectID": "chpt11-multi-reg.html#approximate-factor-models",
    "href": "chpt11-multi-reg.html#approximate-factor-models",
    "title": "11  Multivariate Regression",
    "section": "11.14 Approximate Factor Models",
    "text": "11.14 Approximate Factor Models\nThe MLE of the previous section is a good choice for factor estimation when the number of variables \\(k\\) is small and the factor model is believed to be correctly specified. In many economic applications of factor analysis, however, the number of variables is \\(k\\) is large. In such contexts the MLE can be computationally costly and/or unstable. Furthermore it is typically not credible to believe that the model is correctly specified; rather it is more reasonable to view the factor model as a useful approximation. In this section we explore an approach known as the approximate factor model with estimation by principal components. The estimation method is justified by an asymptotic framework where the number of variables \\(k \\rightarrow \\infty\\) The approximate factor model was introduced by Chamberlain and Rothschild (1983). It is the same as (11.23) but relaxes the assumption on the idiosyncratic error \\(u\\) so that the covariance matrix \\(\\Psi=\\) \\(\\mathbb{E}\\left[u u^{\\prime}\\right]\\) is left unrestricted. In this context the Gaussian MLE of the previous section is misspecified.\nChamberlain and Rothschild (and the literature which followed) proposed estimation by least squares. The idea is to treat the factors as unknown regressors and simultaneously estimate the factors \\(F_{i}\\) and factor loadings \\(\\Lambda\\). We first describe the estimation method.\nLet \\(\\left(X_{1}, \\ldots, X_{n}\\right)\\) be a sample centered at sample means. The least squares criterion is\n\\[\n\\frac{1}{n} \\sum_{i=1}^{n}\\left(X_{i}-\\Lambda F_{i}\\right)^{\\prime}\\left(X_{i}-\\Lambda F_{i}\\right) .\n\\]\nLet \\(\\left(\\widehat{\\Lambda}, \\widehat{F}_{1}, \\ldots, \\widehat{F}_{n}\\right)\\) be the joint minimizers. As \\(\\Lambda\\) and \\(F_{i}\\) are not separately identified a normalization is needed. For compatibility with the notation of the previous section we use \\(n^{-1} \\sum_{i=1}^{n} \\widehat{F}_{i} \\widehat{F}_{i}^{\\prime}=\\boldsymbol{I}_{r}\\).\nWe use a concentration argument to find the solution. As described in the previous section, each observation satisfies the multivariate equation \\(X_{i}=\\Lambda F_{i}+u_{i}\\). For fixed \\(\\Lambda\\) this is a set of \\(k\\) equations with \\(r\\) unknowns \\(F_{i}\\). The least squares solution is \\(\\widehat{F}_{i}(\\Lambda)=\\left(\\Lambda^{\\prime} \\Lambda\\right)^{-1} \\Lambda^{\\prime} X_{i}\\). Substituting this expression into the least squares criterion the concentrated least squares criterion for \\(\\Lambda\\) is\n\\[\n\\begin{aligned}\n\\frac{1}{n} \\sum_{i=1}^{n}\\left(X_{i}-\\Lambda \\widehat{F}_{i}(\\Lambda)\\right)^{\\prime}\\left(X_{i}-\\Lambda \\widehat{F}_{i}(\\Lambda)\\right) &=\\frac{1}{n} \\sum_{i=1}^{n}\\left(X_{i}-\\Lambda\\left(\\Lambda^{\\prime} \\Lambda\\right)^{-1} \\Lambda^{\\prime} X_{i}\\right)^{\\prime}\\left(X_{i}-\\Lambda\\left(\\Lambda^{\\prime} \\Lambda\\right)^{-1} \\Lambda^{\\prime} X_{i}\\right) \\\\\n&=\\frac{1}{n} \\sum_{i=1}^{n}\\left(X_{i}^{\\prime} X_{i}-X_{i}^{\\prime} \\Lambda\\left(\\Lambda^{\\prime} \\Lambda\\right)^{-1} \\Lambda^{\\prime} X_{i}\\right) \\\\\n&=\\operatorname{tr}[\\widehat{\\Sigma}]-\\operatorname{tr}\\left[\\left(\\Lambda^{\\prime} \\Lambda\\right)^{-1} \\Lambda^{\\prime} \\widehat{\\Sigma} \\Lambda\\right]\n\\end{aligned}\n\\]\nwhere \\(\\widehat{\\Sigma}=n^{-1} \\sum_{i=1}^{n} X_{i} X_{i}^{\\prime}\\) is the sample covariance matrix. The least squares estimator \\(\\widehat{\\Lambda}\\) minimizes this criterion. Let \\(\\widehat{\\boldsymbol{D}}\\) and \\(\\widehat{\\boldsymbol{H}}\\) be first \\(r\\) eigenvalues and eigenvectors of \\(\\widehat{\\Sigma}\\). Using the normalization \\(\\Lambda^{\\prime} \\Lambda=\\boldsymbol{I}_{r}\\), from the extrema results of Section A.15 the minimizer of the least squares criterion is \\(\\widehat{\\Lambda}=\\widehat{\\boldsymbol{H}}\\). More broadly any rotation of \\(\\widehat{\\boldsymbol{H}}\\) is valid. Consider \\(\\widehat{\\Lambda}=\\widehat{\\boldsymbol{H}} \\widehat{\\boldsymbol{D}}^{1 / 2}\\). Recall the expression for the factors \\(\\widehat{F}_{i}(\\Lambda)=\\) \\(\\left(\\Lambda^{\\prime} \\Lambda\\right)^{-1} \\Lambda^{\\prime} X_{i}\\). We find that the estimated factors are\n\\[\n\\widehat{F}_{i}=\\left(\\widehat{\\boldsymbol{D}}^{1 / 2} \\widehat{\\boldsymbol{H}}^{\\prime} \\widehat{\\boldsymbol{H}} \\widehat{\\boldsymbol{D}}^{1 / 2}\\right)^{-1} \\widehat{\\boldsymbol{D}}^{1 / 2} \\widehat{\\boldsymbol{H}}^{\\prime} X_{i}=\\widehat{\\boldsymbol{D}}^{-1 / 2} \\widehat{\\boldsymbol{H}}^{\\prime} X_{i}\n\\]\nWe calculate that\n\\[\nn^{-1} \\sum_{i=1}^{n} \\widehat{F}_{i} \\widehat{F}_{i}^{\\prime}=\\widehat{\\boldsymbol{D}}^{-1 / 2} \\widehat{\\boldsymbol{H}}^{\\prime} \\widehat{\\Sigma} \\widehat{\\boldsymbol{H}} \\widehat{\\boldsymbol{D}}^{-1 / 2 \\prime}=\\widehat{\\boldsymbol{D}}^{-1 / 2} \\widehat{\\boldsymbol{D}} \\widehat{\\boldsymbol{D}}^{-1 / 2 \\prime}=\\boldsymbol{I}_{r}\n\\]\nwhich is the desired normalization. This shows that the rotation \\(\\widehat{\\Lambda}=\\widehat{\\boldsymbol{H}} \\widehat{\\boldsymbol{D}}^{1 / 2}\\) produces factor estimates satisfying this normalization.\nWe have proven the following result.\nTheorem 11.9 The least squares estimator of the factor model (11.23) under the normalization \\(n^{-1} \\sum_{i=1}^{n} \\widehat{F}_{i} \\widehat{F}_{i}^{\\prime}=\\boldsymbol{I}_{r}\\) has the following solution:\n\nLet \\(\\widehat{\\boldsymbol{D}}=\\operatorname{diag}\\left[\\widehat{d}_{1}, \\ldots, \\widehat{d}_{r}\\right]\\) and \\(\\widehat{\\boldsymbol{H}}=\\left[\\widehat{h}_{1}, \\ldots, \\widehat{h}_{r}\\right]\\) be the first \\(r\\) eigenvalues and eigenvectors of the sample covariance matrix \\(\\widehat{\\Sigma}\\).\n\\(\\widehat{\\Lambda}=\\widehat{\\boldsymbol{H}} \\widehat{\\boldsymbol{D}}^{1 / 2}\\).\n\\(\\widehat{F}_{i}=\\widehat{\\boldsymbol{D}}^{-1 / 2} \\widehat{\\boldsymbol{H}}^{\\prime} X_{i}\\). Theorem \\(11.9\\) shows that the least squares estimator is based on an eigenvalue decomposition of the covariance matrix. This is computationally stable even in high dimensions.\n\nThe factor estimates are the principal components scaled by the eigenvalues of \\(\\widehat{\\Sigma}\\). Specifically, the \\(j^{t h}\\) factor estimate is \\(\\widehat{F}_{j i}=\\widehat{d}_{j}^{-1 / 2} \\widehat{h}_{j}^{\\prime} X\\). Consequently many authors call this estimator the “principalcomponent method”.\nUnfortunately, \\(\\widehat{\\Lambda}\\) is inconsistent for \\(\\Lambda\\) if \\(k\\) is fixed, as we now show. By the WLLN and CMT, \\(\\widehat{\\Sigma} \\underset{p}{\\longrightarrow}\\) and \\(\\widehat{\\boldsymbol{H}} \\underset{p}{\\longrightarrow} \\boldsymbol{H}\\), the first \\(r\\) eigenvectors of \\(\\Sigma\\). When \\(\\Psi\\) is diagonal, the eigenvectors of \\(\\Sigma=\\Lambda \\Lambda^{\\prime}+\\Psi\\) do not lie in the range space of \\(\\Lambda\\) except in the special case \\(\\Psi=\\sigma^{2} \\boldsymbol{I}_{k}\\). Consequently the estimator \\(\\widehat{\\Lambda}\\) is inconsistent.\nThis inconsistency should not be viewed as surprising. The sample has a total of \\(n k\\) observations and the model has a total of \\(n r+k r-r(r+1) / 2\\) parameters. Since the number of estimated pararameters is proportional to sample size we should not expect estimator consistency.\nAs first recognized by Chamberlain and Rothschild, this deficiency diminishes as \\(k\\) increases. Specifically, assume that \\(k \\rightarrow \\infty\\) as \\(n \\rightarrow \\infty\\). One implication is that the number of observations \\(n k\\) increase at a rate faster than \\(n\\), while the number of parameters increase at a rate proportional to \\(n\\). Another implication is that as \\(k\\) increases there is increasing information about the factors.\nTo make this precise we add the following assumption. Let \\(\\lambda_{\\min }(\\boldsymbol{A})\\) and \\(\\lambda_{\\max }(\\boldsymbol{A})\\) denote the smallest and largest eigenvalues of a positive semi-definite matrix \\(\\boldsymbol{A}\\).\nAssumption \\(11.1\\) As \\(k \\rightarrow \\infty\\)\n\n\\(\\lambda_{\\max }(\\Psi) \\leq B<\\infty\\).\n\\(\\lambda_{\\min }\\left(\\Lambda^{\\prime} \\Lambda\\right) \\rightarrow \\infty\\) as \\(k \\rightarrow \\infty\\).\n\nAssumption 11.1.1 bounds the covariance matrix of the idiosyncratic errors. When \\(\\Psi=\\operatorname{diag}\\left(\\sigma_{1}^{2}, \\ldots, \\sigma_{k}^{2}\\right)\\) this is the same as bounding the individual variances. Effectively Assumption 11.1.1 means that while the elements of \\(u\\) can be correlated they cannot have a correlation structure similar to that of a factor model. Assumption 11.1.2 requires the factor loading matrix to increase in magnitude as the number of variables increases. This is a fairly mild requirement. When the factor loadings are of similar magnitude across variables, \\(\\lambda_{\\min }\\left(\\Lambda^{\\prime} \\Lambda\\right) \\sim k \\rightarrow \\infty\\). Conceptually, Assumption 11.1.2 requires additional variables to add information about the unobserved factors.\nAssumption \\(11.1\\) implies that in the covariance matrix factorization \\(\\Sigma=\\Lambda \\Lambda^{\\prime}+\\Psi\\) the component \\(\\Lambda \\Lambda^{\\prime}\\) dominates as \\(k\\) increases. This means that for large \\(k\\) the first \\(r\\) eigenvectors of \\(\\Sigma\\) are equivalent to those of \\(\\Lambda \\Lambda^{\\prime}\\), which are in the range space of \\(\\Lambda\\). This observation led Chamberlain and Rothschild (1983) to deduce that the principal components estimator is an asymptotic (large \\(k\\) ) analog estimator for the factor loadings and factors. Bai (2003) demonstrated that the estimator is consistent as \\(n, k \\rightarrow \\infty\\) jointly. The conditions and proofs are technical so are not reviewed here.\nNow consider the estimated factors\n\\[\n\\widehat{F}_{i}=\\boldsymbol{D}^{-1 / 2} \\boldsymbol{H}^{\\prime} X_{i}=\\boldsymbol{D}^{-1} \\Lambda^{\\prime} X_{i}\n\\]\nwhere for simplicity we ignore estimation error. Since \\(X_{i}=\\Lambda F_{i}+u_{i}\\) and \\(\\Lambda^{\\prime} \\Lambda=\\boldsymbol{D}\\) we can write this as\n\\[\n\\widehat{F}_{i}=F_{i}+\\boldsymbol{D}^{-1} \\Lambda^{\\prime} u_{i} .\n\\]\nThis shows that \\(\\widehat{F}_{i}\\) is an unbiased estimator for \\(F_{i}\\) and has variance \\(\\operatorname{var}\\left[\\widehat{F}_{i}\\right]=\\boldsymbol{D}^{-1} \\Lambda^{\\prime} \\Psi \\Lambda \\boldsymbol{D}^{-1}\\). Under Assumption 11.1, \\(\\left\\|\\operatorname{var}\\left[\\widehat{F}_{i}\\right]\\right\\| \\leq B / \\lambda_{\\min }\\left(\\Lambda^{\\prime} \\Lambda\\right) \\rightarrow 0\\). Thus \\(\\widehat{F}_{i}\\) is consistent for \\(F_{i}\\) as \\(k \\rightarrow \\infty\\). Bai (2003) shows that this extends to the feasible estimator as \\(n, k \\rightarrow \\infty\\).\nIn Stata, the least squares estimator \\(\\widehat{\\Lambda}\\) and factors \\(\\widehat{F}_{i}\\) can be calculated with the factor, pcf factors (r) command followed by predict. In \\(\\mathrm{R}\\) a feasible estimation approach is to calculate the factors by eigenvalue decomposition."
  },
  {
    "objectID": "chpt11-multi-reg.html#factor-models-with-additional-regressors",
    "href": "chpt11-multi-reg.html#factor-models-with-additional-regressors",
    "title": "11  Multivariate Regression",
    "section": "11.15 Factor Models with Additional Regressors",
    "text": "11.15 Factor Models with Additional Regressors\nConsider the model\n\\[\nX=\\Lambda F+B Z+e\n\\]\nwhere \\(X\\) and \\(e\\) are \\(k \\times 1, \\Lambda\\) is \\(k \\times r, F\\) is \\(r \\times 1, B\\) is \\(k \\times \\ell\\), and \\(Z\\) is \\(\\ell \\times 1\\).\nThe coefficients \\(\\Lambda\\) and \\(\\boldsymbol{B}\\) can be estimated by a combination of factor regression (either MLE or principal components) and least squares. The key is the following two observations:\n\nGiven \\(\\boldsymbol{B}\\), the coefficient \\(\\Lambda\\) can be estimated by factor regression applied to \\(X-\\boldsymbol{B} Z\\).\nGiven the factors \\(F\\), the coefficients \\(\\Lambda\\) and \\(\\boldsymbol{B}\\) can be estimated by multivariate least squares of \\(X\\) on \\(F\\) and \\(Z\\).\n\nEstimation iterates between these two steps. Start with a preliminary estimator of \\(\\boldsymbol{B}\\) obtained by multivariate least squares of \\(X\\) on \\(Z\\). Then apply the above two steps and iterate under convergence."
  },
  {
    "objectID": "chpt11-multi-reg.html#factor-augmented-regression",
    "href": "chpt11-multi-reg.html#factor-augmented-regression",
    "title": "11  Multivariate Regression",
    "section": "11.16 Factor-Augmented Regression",
    "text": "11.16 Factor-Augmented Regression\nIn the previous sections we considered factor models which decompose a set of variables into common factors and idiosyncratic errors. In this section we consider factor-augmented regression, which uses such common factors as regressors for dimension reduction.\nSuppose we have the variables \\((Y, Z, X)\\) where \\(Y \\in \\mathbb{R}, Z \\in \\mathbb{R}^{\\ell}\\), and \\(X \\in \\mathbb{R}^{k}\\). In practice, \\(k\\) may be large and the elements of \\(X\\) may be highly correlated. The factor-augmented regression model is\n\\[\n\\begin{aligned}\nY &=F^{\\prime} \\beta+Z^{\\prime} \\gamma+e \\\\\nX &=\\Lambda F+u \\\\\n\\mathbb{E}[F e] &=0 \\\\\n\\mathbb{E}[Z e] &=0 \\\\\n\\mathbb{E}\\left[F u^{\\prime}\\right] &=0 \\\\\n\\mathbb{E}[u e] &=0,\n\\end{aligned}\n\\]\nThe random variables are \\(e \\in \\mathbb{R}, F \\in \\mathbb{R}^{r}\\), and \\(u \\in \\mathbb{R}^{k}\\). The regression coefficients are \\(\\beta \\in \\mathbb{R}^{k}\\) and \\(\\gamma \\in \\mathbb{R}^{\\ell}\\). The matrix \\(\\Lambda\\) are the factor loadings.\nThis model specifies that the influence of \\(X\\) on \\(Y\\) is through the common factors \\(F\\). The idea is that the variation in the regressors is mostly captured by the variation in the factors, so the influence of the regressors can be captured through these factors. This can be viewed as a dimension-reduction technique as we have reduced the \\(k\\)-dimensional \\(X\\) to the \\(r\\)-dimensional \\(F\\). Interest typically focuses on the regressors \\(Z\\) and its coefficients \\(\\gamma\\). The factors \\(F\\) are included in the regression as “controls” and its coefficient \\(\\beta\\) is less typically of interest. Since it is difficult to interpret the factors \\(F\\) only their range space is identified it is generally prudent to avoid intrepreting the coefficients \\(\\beta\\). The model is typically estimated in multiple steps. First, the factor loadings \\(\\Lambda\\) and factors \\(F_{i}\\) are estimated by factor regression. In the case of principal-components estimation the factor estimates are the scaled \\({ }^{2}\\) principal components \\(\\widehat{F}_{i}=\\widehat{\\boldsymbol{D}}^{-1} \\widehat{\\Lambda}^{\\prime} X_{i}\\). Second, \\(Y\\) is regressed on the estimated factors and the other regressors to obtain the estimator of \\(\\beta\\) and \\(\\gamma\\). This second-step estimator equals (for simplicity assume there is no \\(Z\\) )\n\\[\n\\begin{aligned}\n\\widehat{\\beta} &=\\left(\\sum_{i=1}^{n} \\widehat{F}_{i} \\widehat{F}_{i}^{\\prime}\\right)^{-1}\\left(\\sum_{i=1}^{n} \\widehat{F}_{i} Y_{i}\\right) \\\\\n&=\\left(\\widehat{\\boldsymbol{D}}^{-1} \\widehat{\\Lambda}^{\\prime} \\frac{1}{n} \\sum_{i=1}^{n} X_{i} X_{i}^{\\prime} \\widehat{\\Lambda}^{-1}\\right)^{-1}\\left(\\widehat{\\boldsymbol{D}}^{-1} \\widehat{\\Lambda}^{\\prime} \\frac{1}{n} \\sum_{i=1}^{n} X_{i} Y_{i}\\right) .\n\\end{aligned}\n\\]\nNow let’s investigate its asymptotic behavior. As \\(n \\rightarrow \\infty, \\widehat{\\Lambda} \\underset{p}{\\rightarrow} \\Lambda\\) and \\(\\widehat{\\boldsymbol{D}} \\underset{p}{\\rightarrow} \\boldsymbol{D}\\) so\n\\[\n\\widehat{\\beta} \\underset{p}{\\longrightarrow} \\beta^{*}=\\left(\\boldsymbol{D}^{-1} \\Lambda^{\\prime} \\mathbb{E}\\left[X X^{\\prime}\\right] \\Lambda \\boldsymbol{D}^{-1}\\right)^{-1}\\left(\\boldsymbol{D}^{-1} \\Lambda^{\\prime} \\mathbb{E}[X Y]\\right) .\n\\]\nRecall \\(\\mathbb{E}\\left[X X^{\\prime}\\right]=\\Lambda \\Lambda^{\\prime}+\\Psi\\) and \\(\\Lambda^{\\prime} \\Lambda=\\boldsymbol{D}\\). We calculate that\n\\[\n\\mathbb{E}[X Y]=\\mathbb{E}\\left[(\\Lambda F+u)\\left(F^{\\prime} \\beta+e\\right)\\right]=\\Lambda \\beta .\n\\]\nWe find that the right-hand-side of (11.26) equals\n\\[\n\\beta^{*}=\\left(D^{-1} \\Lambda^{\\prime}\\left(\\Lambda \\Lambda^{\\prime}+\\Psi\\right) \\Lambda \\boldsymbol{D}^{-1}\\right)^{-1}\\left(\\boldsymbol{D}^{-1} \\Lambda^{\\prime} \\Lambda \\beta\\right)=\\left(\\boldsymbol{I}_{r}+\\boldsymbol{D}^{-1} \\Lambda^{\\prime} \\Psi \\Lambda \\boldsymbol{D}^{-1}\\right)^{-1} \\beta\n\\]\nwhich does not equal \\(\\beta\\). Thus \\(\\widehat{\\beta}\\) has a probability limit but is inconsistent for \\(\\beta\\) as \\(n \\rightarrow \\infty\\).\nThis deficiency diminishes as \\(k \\rightarrow \\infty\\). Indeed,\n\\[\n\\left\\|\\boldsymbol{D}^{-1} \\Lambda^{\\prime} \\Psi \\Lambda \\boldsymbol{D}^{-1}\\right\\| \\leq B\\left\\|\\boldsymbol{D}^{-1}\\right\\| \\rightarrow 0\n\\]\nas \\(k \\rightarrow \\infty\\). This implies \\(\\beta^{*} \\rightarrow \\beta\\). Hence, if we take the sequential asymptotic limit \\(n \\rightarrow \\infty\\) followed by \\(k \\rightarrow\\) \\(\\infty\\), we find \\(\\widehat{\\beta} \\underset{p}{\\longrightarrow} \\beta\\). This implies that the estimator is consistent. Bai (2003) demonstrated consistency under the more rigorous but technically challenging setting where \\(n, k \\rightarrow \\infty\\) jointly. The implication of this result is that factor augmented regression is consistent if both the sample size and dimension of \\(X\\) are large.\nFor asymptotic normality of \\(\\widehat{\\beta}\\) it turns out that we need to strengthen Assumption 11.1.2. The relevant condition is \\(n^{-1 / 2} \\lambda_{\\min }\\left(\\Lambda^{\\prime} \\Lambda\\right) \\rightarrow \\infty\\). This is similar to the condition that \\(k^{2} / n \\rightarrow \\infty\\). This is technical but can be interpreted as meaning that \\(k\\) is large relative to \\(\\sqrt{n}\\). Intuitively, this requires that dimension of \\(X\\) is larger than sample size \\(n\\).\nIn Stata, estimation takes the following steps. First, the factor command is used to estimate the factor model. Either MLE or principal components estimation can be used. Second, the predict command is used to estimate the factors, either by Barlett or regression scoring. Third, the factors are treated as regressors in an estimated regression."
  },
  {
    "objectID": "chpt11-multi-reg.html#multivariate-normal",
    "href": "chpt11-multi-reg.html#multivariate-normal",
    "title": "11  Multivariate Regression",
    "section": "11.17 Multivariate Normal*",
    "text": "11.17 Multivariate Normal*\nSome interesting sampling results hold for matrix-valued normal variates. Let \\(\\boldsymbol{Y}\\) be an \\(n \\times m\\) matrix whose rows are independent and distributed \\(\\mathrm{N}(\\mu, \\Sigma)\\). We say that \\(\\boldsymbol{Y}\\) is multivariate matrix normal, and\n\\({ }^{2}\\) The unscaled principal components can equivalently be used if the coefficients \\(\\widehat{\\beta}\\) are not reported. The coefficient estimates \\(\\hat{\\gamma}\\) are unaffected by the choice of factor scaling. write \\(Y \\sim \\mathrm{N}\\left(\\bar{\\mu}, I_{n} \\otimes \\Sigma\\right)\\), where \\(\\bar{\\mu}\\) is \\(n \\times m\\) with each row equal to \\(\\mu^{\\prime}\\). The notation is due to the fact that \\(\\operatorname{vec}\\left((\\boldsymbol{Y}-\\mu)^{\\prime}\\right) \\sim \\mathrm{N}\\left(0, \\boldsymbol{I}_{n} \\otimes \\Sigma\\right)\\)\nDefinition 11.2 If \\(n \\times m Y \\sim \\mathrm{N}\\left(\\bar{\\mu}, I_{n} \\otimes \\Sigma\\right)\\) then \\(W=Y^{\\prime} Y\\) is distributed Wishart with \\(n\\) degress of freedom and covariance matrix \\(\\Sigma\\), and is written as \\(W \\sim\\) \\(W_{m}(n, \\Sigma)\\).\nThe Wishart is a multivariate generalization of the chi-square. If \\(W \\sim W_{1}\\left(n, \\sigma^{2}\\right)\\) then \\(W \\sim \\sigma^{2} \\chi_{n}^{2}\\).\nThe Wishart arises as the exact distribution of a sample covariance matrix in the normal sampling model. The bias-corrected estimator of \\(\\Sigma\\) is\n\\[\n\\widehat{\\Sigma}=\\frac{1}{n-1} \\sum_{i=1}^{n}\\left(Y_{i}-\\bar{Y}\\right)\\left(Y_{i}-\\bar{Y}\\right)^{\\prime} .\n\\]\nTheorem 11.10 If \\(Y_{i} \\sim \\mathrm{N}(\\mu, \\Sigma)\\) are independent then \\(\\widehat{\\Sigma} \\sim W_{m}\\left(n-1, \\frac{1}{n-1} \\Sigma\\right)\\).\nThe following manipulation is useful.\nTheorem 11.11 If \\(W \\sim W_{m}(n, \\Sigma)\\) then for \\(m \\times 1 \\alpha,\\left(\\alpha^{\\prime} W^{-1} \\alpha\\right)^{-1} \\sim \\frac{\\chi_{n-m+1}^{2}}{\\alpha^{\\prime} \\Sigma^{-1} \\alpha}\\)\nTo prove this, note that without loss of generality we can take \\(\\Sigma=\\boldsymbol{I}_{m}\\) and \\(\\alpha^{\\prime} \\alpha=1\\). Let \\(\\boldsymbol{H}\\) be \\(m \\times m\\) orthonormal with first row equal to \\(\\alpha\\). so that \\(\\boldsymbol{H} \\alpha=\\left(\\begin{array}{l}1 \\\\ 0\\end{array}\\right)\\). Since the distribution of \\(\\boldsymbol{Y}\\) and \\(\\boldsymbol{Y} \\boldsymbol{H}\\) are identical we can without loss of generality set \\(\\alpha=\\left(\\begin{array}{c}1 \\\\ 0\\end{array}\\right)\\). Partition \\(\\boldsymbol{Y}=\\left[\\boldsymbol{Y}_{1}, \\boldsymbol{Y}_{2}\\right]\\) where \\(\\boldsymbol{Y}_{1}\\) is \\(n \\times 1, \\boldsymbol{Y}_{2}\\) is \\(n \\times(m-1)\\), and they are independent. Then\n\\[\n\\begin{aligned}\n\\left(\\alpha^{\\prime} W^{-1} \\alpha\\right)^{-1} &=\\left(\\left(\\begin{array}{ll}\n1 & 0\n\\end{array}\\right)\\left(\\begin{array}{cc}\n\\boldsymbol{Y}_{1}^{\\prime} \\boldsymbol{Y}_{1} & \\boldsymbol{Y}_{1}^{\\prime} \\boldsymbol{Y}_{2} \\\\\n\\boldsymbol{Y}_{2}^{\\prime} \\boldsymbol{Y}_{1} & \\boldsymbol{Y}_{2}^{\\prime} \\boldsymbol{Y}_{2}\n\\end{array}\\right)^{-1}\\left(\\begin{array}{l}\n1 \\\\\n0\n\\end{array}\\right)\\right)^{-1} \\\\\n&=\\boldsymbol{Y}_{1}^{\\prime} \\boldsymbol{Y}_{1}-\\boldsymbol{Y}_{1}^{\\prime} \\boldsymbol{Y}_{2}\\left(\\boldsymbol{Y}_{2}^{\\prime} \\boldsymbol{Y}_{2}\\right)^{-1} \\boldsymbol{Y}_{2}^{\\prime} \\boldsymbol{Y}_{1} \\\\\n&=\\boldsymbol{Y}_{1}^{\\prime} \\boldsymbol{M}_{2} \\boldsymbol{Y}_{1} \\sim \\chi_{n-(m-1)}^{2}\n\\end{aligned}\n\\]\nwhere \\(\\boldsymbol{M}_{2}=\\boldsymbol{I}_{m-1}-\\boldsymbol{Y}_{2}\\left(\\boldsymbol{Y}_{2}^{\\prime} \\boldsymbol{Y}_{2}\\right)^{-1} \\boldsymbol{Y}_{2}^{\\prime}\\). The final distributional equality holds conditional on \\(\\boldsymbol{Y}_{2}\\) by the same argument in the proof of Theorem 5.7. Since this does not depend on \\(\\boldsymbol{Y}_{2}\\) it is the unconditional distribution as well. This establishes the stated result.\nTo test hypotheses about \\(\\mu\\) a classical statistic is known as Hotelling’s \\(T^{2}\\) :\n\\[\nT^{2}=n(\\bar{Y}-\\mu)^{\\prime} \\widehat{\\Sigma}^{-1}(\\bar{Y}-\\mu) .\n\\]\nTheorem 11.12 If \\(Y \\sim \\mathrm{N}(\\mu, \\Sigma)\\) then\n\\[\nT^{2} \\sim \\frac{m}{(n-m)(n-1)} F(m, n-m)\n\\]\na scaled F distribution.\nTo prove this recall that \\(\\bar{Y}\\) is independent of \\(\\widehat{\\Sigma}\\). Apply Theorem \\(11.11\\) with \\(\\alpha=\\bar{Y}-\\mu\\). Conditional on \\(\\bar{Y}\\) and using the fact that \\(\\widehat{\\Sigma} \\sim W_{m}\\left(n-1, \\frac{1}{n-1} \\Sigma\\right)\\),\n\\[\n\\begin{aligned}\n\\frac{n}{T^{2}} &=\\left((\\bar{Y}-\\Sigma)^{\\prime} \\widehat{\\Sigma}^{-1}(\\bar{Y}-\\Sigma)\\right)^{-1} \\\\\n& \\sim \\frac{\\chi_{n-1-m+1}^{2}}{(\\bar{Y}-\\mu)^{\\prime}\\left(\\frac{1}{n-1} \\Sigma\\right)^{-1}(\\bar{Y}-\\mu)} \\\\\n& \\sim n(n-1) \\frac{\\chi_{n-m}^{2}}{\\chi_{m}^{2}}\n\\end{aligned}\n\\]\nSince the two chi-square variables are independent, this is the stated result.\nA very interesting property of this result is that the \\(T^{2}\\) statistic is a multivariate quadratric form in normal random variables, yet it has the exact \\(F\\) distribution."
  },
  {
    "objectID": "chpt11-multi-reg.html#exercises",
    "href": "chpt11-multi-reg.html#exercises",
    "title": "11  Multivariate Regression",
    "section": "11.18 Exercises",
    "text": "11.18 Exercises\nExercise 11.1 Show (11.10) when the errors are conditionally homoskedastic (11.8).\nExercise 11.2 Show (11.11) when the regressors are common across equations \\(X_{j}=X\\).\nExercise 11.3 Show (11.12) when the regressors are common across equations \\(X_{j}=X\\) and the errors are conditionally homoskedastic (11.8).\nExercise 11.4 Prove Theorem 11.1.\nExercise 11.5 Show (11.13) when the regressors are common across equations \\(X_{j}=X\\).\nExercise 11.6 Show (11.14) when the regressors are common across equations \\(X_{j}=X\\) and the errors are conditionally homoskedastic (11.8).\nExercise 11.7 Prove Theorem 11.2.\nExercise 11.8 Prove Theorem 11.3.\nExercise \\(11.9\\) Show that (11.16) follows from the steps described.\nExercise 11.10 Show that (11.17) follows from the steps described.\nExercise \\(11.11\\) Prove Theorem 11.4. Exercise 11.12 Prove Theorem 11.5.\nHint: First, show that it is sufficient to show that\n\\[\n\\mathbb{E}\\left[\\bar{X}^{\\prime} \\bar{X}\\right]\\left(\\mathbb{E}\\left[\\bar{X}^{\\prime} \\Sigma^{-1} \\bar{X}\\right]\\right)^{-1} \\mathbb{E}\\left[\\bar{X}^{\\prime} \\bar{X}\\right] \\leq \\mathbb{E}\\left[\\bar{X}^{\\prime} \\Sigma \\bar{X}\\right] .\n\\]\nSecond, rewrite this equation using the transformations \\(U=\\Sigma^{1 / 2} \\bar{X}\\) and \\(V=\\Sigma^{1 / 2} \\bar{X}\\), and then apply the matrix Cauchy-Schwarz inequality (B.33).\nExercise 11.13 Prove Theorem 11.6.\nExercise \\(11.14\\) Take the model\n\\[\n\\begin{aligned}\nY &=\\pi^{\\prime} \\beta+e \\\\\n\\pi &=\\mathbb{E}[X \\mid Z]=\\Gamma^{\\prime} Z \\\\\n\\mathbb{E}[e \\mid Z] &=0\n\\end{aligned}\n\\]\nwhere \\(Y\\) is scalar, \\(X\\) is a \\(k\\) vector and \\(Z\\) is an \\(\\ell\\) vector. \\(\\beta\\) and \\(\\pi\\) are \\(k \\times 1\\) and \\(\\Gamma\\) is \\(\\ell \\times k\\). The sample is \\(\\left(Y_{i}, X_{i}, Z_{i}: i=1, \\ldots, n\\right)\\) with \\(\\pi_{i}\\) unobserved.\nConsider the estimator \\(\\widehat{\\beta}\\) for \\(\\beta\\) by OLS of \\(Y\\) on \\(\\widehat{\\pi}=\\widehat{\\Gamma}^{\\prime} Z\\) where \\(\\widehat{\\Gamma}\\) is the OLS coefficient from the multivariate regression of \\(X\\) on \\(Z\\).\n\nShow that \\(\\widehat{\\beta}\\) is consistent for \\(\\beta\\).\nFind the asymptotic distribution \\(\\sqrt{n}(\\widehat{\\beta}-\\beta)\\) as \\(n \\rightarrow \\infty\\) assuming that \\(\\beta=0\\).\nWhy is the assumption \\(\\beta=0\\) an important simplifying condition in part (b)?\nUsing the result in (c) construct an appropriate asymptotic test for the hypothesis \\(\\mathbb{H}_{0}: \\beta=0\\).\n\nExercise \\(11.15\\) The observations are i.i.d., \\(\\left(Y_{1 i}, Y_{2 i}, X_{i}: i=1, \\ldots, n\\right)\\). The dependent variables \\(Y_{1}\\) and \\(Y_{2}\\) are real-valued. The regressor \\(X\\) is a \\(k\\)-vector. The model is the two-equation system\n\\[\n\\begin{aligned}\nY_{1} &=X^{\\prime} \\beta_{1}+e_{1} \\\\\n\\mathbb{E}\\left[X e_{1}\\right] &=0 \\\\\nY_{2} &=X^{\\prime} \\beta_{2}+e_{2} \\\\\n\\mathbb{E}\\left[X e_{2}\\right] &=0 .\n\\end{aligned}\n\\]\n\nWhat are the appropriate estimators \\(\\widehat{\\beta}_{1}\\) and \\(\\widehat{\\beta}_{2}\\) for \\(\\beta_{1}\\) and \\(\\beta_{2}\\) ?\nFind the joint asymptotic distribution of \\(\\widehat{\\beta}_{1}\\) and \\(\\widehat{\\beta}_{2}\\).\nDescribe a test for \\(\\mathbb{H}_{0}: \\beta_{1}=\\beta_{2}\\)."
  },
  {
    "objectID": "chpt12-iv.html",
    "href": "chpt12-iv.html",
    "title": "12  Instrumental Variables",
    "section": "",
    "text": "The concepts of endogeneity and instrumental variable are fundamental to econometrics, and mark a substantial departure from other branches of statistics. The ideas of endogeneity arise naturally in economics from models of simultaneous equations, most notably the classic supply/demand model of price determination.\nThe identification problem in simultaneous equations dates back to Philip Wright (1915) and Working (1927). The method of instrumental variables first appears in an Appendix of a 1928 book by Philip Wright, though the authorship is sometimes credited to his son Sewell Wright. The label “instrumental variables” was introduced by Reiersøl (1945). An excellent review of the history of instrumental variables is Stock and Trebbi (2003)."
  },
  {
    "objectID": "chpt12-iv.html#overview",
    "href": "chpt12-iv.html#overview",
    "title": "12  Instrumental Variables",
    "section": "12.2 Overview",
    "text": "12.2 Overview\nWe say that there is endogeneity in the linear model\n\\[\nY=X^{\\prime} \\beta+e\n\\]\nif \\(\\beta\\) is the parameter of interest and\n\\[\n\\mathbb{E}[X e] \\neq 0 \\text {. }\n\\]\nThis is a core problem in econometrics and largely differentiates the field from statistics. To distinguish (12.1) from the regression and projection models, we will call (12.1) a structural equation and \\(\\beta\\) a structural parameter. When (12.2) holds, it is typical to say that \\(X\\) is endogenous for \\(\\beta\\).\nEndogeneity cannot happen if the coefficient is defined by linear projection. Indeed, we can define the linear projection coefficient \\(\\beta^{*}=\\mathbb{E}\\left[X X^{\\prime}\\right]^{-1} \\mathbb{E}[X Y]\\) and linear projection equation\n\\[\n\\begin{aligned}\nY &=X^{\\prime} \\beta^{*}+e^{*} \\\\\n\\mathbb{E}\\left[X e^{*}\\right] &=0 .\n\\end{aligned}\n\\]\nHowever, under endogeneity (12.2) the projection coefficient \\(\\beta^{*}\\) does not equal the structural parameter \\(\\beta\\). Indeed,\n\\[\n\\begin{aligned}\n\\beta^{*} &=\\left(\\mathbb{E}\\left[X X^{\\prime}\\right]\\right)^{-1} \\mathbb{E}[X Y] \\\\\n&=\\left(\\mathbb{E}\\left[X X^{\\prime}\\right]\\right)^{-1} \\mathbb{E}\\left[X\\left(X^{\\prime} \\beta+e\\right)\\right] \\\\\n&=\\beta+\\left(\\mathbb{E}\\left[X X^{\\prime}\\right]\\right)^{-1} \\mathbb{E}[X e] \\neq \\beta\n\\end{aligned}\n\\]\nthe final relation because \\(\\mathbb{E}[X e] \\neq 0\\).\nThus endogeneity requires that the coefficient be defined differently than projection. We describe such definitions as structural. We will present three examples in the following section.\nEndogeneity implies that the least squares estimator is inconsistent for the structural parameter. Indeed, under i.i.d. sampling, least squares is consistent for the projection coefficient.\n\\[\n\\widehat{\\beta} \\underset{p}{\\longrightarrow}\\left(\\mathbb{E}\\left[X X^{\\prime}\\right]\\right)^{-1} \\mathbb{E}[X Y]=\\beta^{*} \\neq \\beta .\n\\]\nThe inconsistency of least squares is typically referred to as endogeneity bias or estimation bias due to endogeneity. This is an imperfect label as the actual issue is inconsistency, not bias.\nAs the structural parameter \\(\\beta\\) is the parameter of interest, endogeneity requires the development of alternative estimation methods. We discuss those in later sections."
  },
  {
    "objectID": "chpt12-iv.html#examples",
    "href": "chpt12-iv.html#examples",
    "title": "12  Instrumental Variables",
    "section": "12.3 Examples",
    "text": "12.3 Examples\nThe concept of endogeneity may be easiest to understand by example. We discuss three. In each case it is important to see how the structural parameter \\(\\beta\\) is defined independently from the linear projection model.\nExample: Measurement error in the regressor. Suppose that \\((Y, Z)\\) are joint random variables, \\(\\mathbb{E}[Y \\mid Z]=Z^{\\prime} \\beta\\) is linear, and \\(\\beta\\) is the structural parameter. \\(Z\\) is not observed. Instead we observe \\(X=Z+u\\) where \\(u\\) is a \\(k \\times 1\\) measurement error, independent of \\(e\\) and \\(Z\\). This is an example of a latent variable model, where “latent” refers to an unobserved structural variable.\nThe model \\(X=Z+u\\) with \\(Z\\) and \\(u\\) independent and \\(\\mathbb{E}[u]=0\\) is known as classical measurement error. This means that \\(X\\) is a noisy but unbiased measure of \\(Z\\).\nBy substitution we can express \\(Y\\) as a function of the observed variable \\(X\\).\n\\[\nY=Z^{\\prime} \\beta+e=(X-u)^{\\prime} \\beta+e=X^{\\prime} \\beta+v\n\\]\nwhere \\(v=e-u^{\\prime} \\beta\\). This means that \\((Y, X)\\) satisfy the linear equation\n\\[\nY=X^{\\prime} \\beta+v\n\\]\nwith an error \\(v\\). But this error is not a projection error. Indeed,\n\\[\n\\mathbb{E}[X v]=\\mathbb{E}\\left[(Z+u)\\left(e-u^{\\prime} \\beta\\right)\\right]=-\\mathbb{E}\\left[u u^{\\prime}\\right] \\beta \\neq 0\n\\]\nif \\(\\beta \\neq 0\\) and \\(\\mathbb{E}\\left[u u^{\\prime}\\right] \\neq 0\\). As we learned in the previous section, if \\(\\mathbb{E}[X \\nu] \\neq 0\\) then least squares estimation will be inconsistent.\nWe can calculate the form of the projection coefficient (which is consistently estimated by least squares). For simplicity suppose that \\(k=1\\). We find\n\\[\n\\beta^{*}=\\beta+\\frac{\\mathbb{E}[X \\nu]}{\\mathbb{E}\\left[X^{2}\\right]}=\\beta\\left(1-\\frac{\\mathbb{E}\\left[u^{2}\\right]}{\\mathbb{E}\\left[X^{2}\\right]}\\right) .\n\\]\nSince \\(\\mathbb{E}\\left[u^{2}\\right] / \\mathbb{E}\\left[X^{2}\\right]<1\\) the projection coefficient shrinks the structural parameter \\(\\beta\\) towards zero. This is called measurement error bias or attenuation bias.\nTo illustrate, Figure 12.1(a) displays the impact of measurement error on the regression line. The three solid points are pairs \\((Y, Z)\\) which are measured without error. The regression function drawn through these three points is marked as “No Measurement Error”. The six open circles mark pairs \\((Y, X)\\) where \\(X=Z+u\\) with \\(u=\\{+1,-1\\}\\). Thus \\(X\\) is a mis-measured version of \\(Z\\). The six open circles spread the joint distribution along the \\(\\mathrm{x}\\)-axis, but not along the \\(\\mathrm{y}\\)-axis. The regression line drawn for these six points is marked as “With Measurement Error”. You can see that the latter regression line is flattened relative to the original regression function. This is the attenuation bias due to measurement error.\n\n\nMeasurement Error\n\n\n\nSupply and Demand\n\nFigure 12.1: Examples of Endogeneity\nExample: Supply and Demand. The variables \\(Q\\) and \\(P\\) (quantity and price) are determined jointly by the demand equation\n\\[\nQ=-\\beta_{1} P+e_{1}\n\\]\nand the supply equation\n\\[\nQ=\\beta_{2} P+e_{2} \\text {. }\n\\]\nAssume that \\(e=\\left(e_{1}, e_{2}\\right)\\) satisfies \\(\\mathbb{E}[e]=0\\) and \\(\\mathbb{E}\\left[e e^{\\prime}\\right]=\\boldsymbol{I}_{2}\\) (the latter for simplicity). The question is: if we regress \\(Q\\) on \\(P\\), what happens?\nIt is helpful to solve for \\(Q\\) and \\(P\\) in terms of the errors. In matrix notation,\n\\[\n\\left[\\begin{array}{cc}\n1 & \\beta_{1} \\\\\n1 & -\\beta_{2}\n\\end{array}\\right]\\left(\\begin{array}{l}\nQ \\\\\nP\n\\end{array}\\right)=\\left(\\begin{array}{l}\ne_{1} \\\\\ne_{2}\n\\end{array}\\right)\n\\]\nso\n\\[\n\\begin{aligned}\n\\left(\\begin{array}{l}\nQ \\\\\nP\n\\end{array}\\right) &=\\left[\\begin{array}{cc}\n1 & \\beta_{1} \\\\\n1 & -\\beta_{2}\n\\end{array}\\right]^{-1}\\left(\\begin{array}{c}\ne_{1} \\\\\ne_{2}\n\\end{array}\\right) \\\\\n&=\\left[\\begin{array}{cc}\n\\beta_{2} & \\beta_{1} \\\\\n1 & -1\n\\end{array}\\right]\\left(\\begin{array}{l}\ne_{1} \\\\\ne_{2}\n\\end{array}\\right)\\left(\\frac{1}{\\beta_{1}+\\beta_{2}}\\right) \\\\\n&=\\left(\\begin{array}{c}\n\\left(\\beta_{2} e_{1}+\\beta_{1} e_{2}\\right) /\\left(\\beta_{1}+\\beta_{2}\\right) \\\\\n\\left(e_{1}-e_{2}\\right) /\\left(\\beta_{1}+\\beta_{2}\\right)\n\\end{array}\\right) .\n\\end{aligned}\n\\]\nThe projection of \\(Q\\) on \\(P\\) yields \\(Q=\\beta^{*} P+e^{*}\\) with \\(\\mathbb{E}\\left[P e^{*}\\right]=0\\) and the projection coefficient is\n\\[\n\\beta^{*}=\\frac{\\mathbb{E}[P Q]}{\\mathbb{E}\\left[P^{2}\\right]}=\\frac{\\beta_{2}-\\beta_{1}}{2} .\n\\]\nThe projection coefficient \\(\\beta^{*}\\) equals neither the demand slope \\(\\beta_{1}\\) nor the supply slope \\(\\beta_{2}\\), but equals an average of the two. (The fact that it is a simple average is an artifact of the covariance structure.)\nThe OLS estimator satisfies \\(\\widehat{\\beta} \\underset{p}{\\rightarrow} \\beta^{*}\\) and the limit does not equal either \\(\\beta_{1}\\) or \\(\\beta_{2}\\). This is called simultaneous equations bias. This occurs generally when \\(Y\\) and \\(X\\) are jointly determined, as in a market equilibrium.\nGenerally, when both the dependent variable and a regressor are simultaneously determined then the regressor should be treated as endogenous.\nTo illustrate, Figure 12.1(b) draws a supply/demand model with Quantity on the y-axis and Price on the \\(\\mathrm{x}\\)-axis. The supply and demand equations are \\(Q=P+\\varepsilon_{1}\\) and \\(Q=4-P-\\varepsilon_{2}\\), respectively. Suppose that the errors each have the Rademacher distribution \\(\\varepsilon \\in\\{-1,+1\\}\\). This model has four equilibrium outcomes, marked by the four points in the figure. The regression line through these four points has a slope of zero and is marked as “Regression”. This is what would be measured by a least squares regression of observed quantity on observed price. This is endogeneity bias due to simultaneity.\nExample: Choice Variables as Regressors. Take the classic wage equation\n\\[\n\\log (\\text { wage })=\\beta \\text { education }+e\n\\]\nwith \\(\\beta\\) the average causal effect of education on wages. If wages are affected by unobserved ability, and individuals with high ability self-select into higher education, then \\(e\\) contains unobserved ability, so education and \\(e\\) will be positively correlated. Hence education is endogenous. The positive correlation means that the linear projection coefficient \\(\\beta^{*}\\) will be upward biased relative to the structural coefficient \\(\\beta\\). Thus least squares (which is estimating the projection coefficient) will tend to over-estimate the causal effect of education on wages.\nThis type of endogeneity occurs generally when \\(Y\\) and \\(X\\) are both choices made by an economic agent, even if they are made at different points in time.\nGenerally, when both the dependent variable and a regressor are choice variables made by the same agent, the variables should be treated as endogenous.\nThis example was illustrated back in Figure \\(2.8\\) which displayed the joint distribution of wages and education of the population of Jennifers and Georges. In Figure 2.8, the plotted Average Causal Effect is the structural impact (on average in the population) of college education on wages. The plotted regression line has a larger slope, as it adds the endogeneity bias due to the fact that education is a choice variable."
  },
  {
    "objectID": "chpt12-iv.html#endogenous-regressors",
    "href": "chpt12-iv.html#endogenous-regressors",
    "title": "12  Instrumental Variables",
    "section": "12.4 Endogenous Regressors",
    "text": "12.4 Endogenous Regressors\nWe have defined endogeneity as the context where a regressor is correlated with the equation error. The converse of endogeneity is exogeneity. That is, we say a regressor \\(X\\) is exogenous for \\(\\beta\\) if \\(\\mathbb{E}[X e]=\\) 0 . In general the distinction in an economic model is that a regressor \\(X\\) is endogenous if it is jointly determined with \\(Y\\), while a regressor \\(X\\) is exogenous if it is determined separately from \\(Y\\).\nIn most applications only a subset of the regressors are treated as endogenous. Partition \\(X=\\left(X_{1}, X_{2}\\right)\\) with dimensions \\(\\left(k_{1}, k_{2}\\right)\\) so that \\(X_{1}\\) contains the exogenous regressors and \\(X_{2}\\) contains the endogenous regressors. As the dependent variable \\(Y\\) is also endogenous, we sometimes differentiate \\(X_{2}\\) by calling it the endogenous right-hand-side variable. Similarly partition \\(\\beta=\\left(\\beta_{1}, \\beta_{2}\\right)\\). With this notation the structural equation is\n\\[\nY=X_{1}^{\\prime} \\beta_{1}+X_{2}^{\\prime} \\beta_{2}+e .\n\\]\nAn alternative notation is as follows. Let \\(Y_{2}=X_{2}\\) be the endogenous regressors and rename the dependent variable \\(Y\\) as \\(Y_{1}\\). Then the structural equation is\n\\[\nY_{1}=X_{1}^{\\prime} \\beta_{1}+Y_{2}^{\\prime} \\beta_{2}+e .\n\\]\nThis is especially useful so that the notation clarifies which variables are endogenous and which exogenous. We also write \\(\\vec{Y}=\\left(Y_{1}, Y_{2}\\right)\\) as the set of endogenous variables. We use the notation \\(\\vec{Y}\\) so that there is no confusion with \\(Y\\) as defined in (12.3).\nThe assumptions regarding the regressors and regression error are\n\\[\n\\begin{aligned}\n&\\mathbb{E}\\left[X_{1} e\\right]=0 \\\\\n&\\mathbb{E}\\left[Y_{2} e\\right] \\neq 0 .\n\\end{aligned}\n\\]\nThe endogenous regressors \\(Y_{2}\\) are the critical variables discussed in the examples of the previous section - simultaneous variables, choice variables, mis-measured regressors - that are potentially correlated with the equation error \\(e\\). In many applications \\(k_{2}\\) is small (1 or 2 ). The exogenous variables \\(X_{1}\\) are the remaining regressors (including the equation intercept) and can be low or high dimensional."
  },
  {
    "objectID": "chpt12-iv.html#instruments",
    "href": "chpt12-iv.html#instruments",
    "title": "12  Instrumental Variables",
    "section": "12.5 Instruments",
    "text": "12.5 Instruments\nTo consistently estimate \\(\\beta\\) we require additional information. One type of information which is commonly used in economic applications are what we call instruments.\nDefinition \\(12.1\\) The \\(\\ell \\times 1\\) random vector \\(Z\\) is an instrumental variable for (12.3) if\n\\[\n\\begin{aligned}\n\\mathbb{E}[Z e] &=0 \\\\\n\\mathbb{E}\\left[Z Z^{\\prime}\\right] &>0 \\\\\n\\operatorname{rank}\\left(\\mathbb{E}\\left[Z X^{\\prime}\\right]\\right) &=k .\n\\end{aligned}\n\\]\nThere are three components to the definition as given. The first (12.5) is that the instruments are uncorrelated with the regression error. The second (12.6) is a normalization which excludes linearly redundant instruments. The third (12.7) is often called the relevance condition and is essential for the identification of the model, as we discuss later. A necessary condition for (12.7) is that \\(\\ell \\geq k\\).\nCondition (12.5) - that the instruments are uncorrelated with the equation error - is often described as that they are exogenous in the sense that they are determined outside the model for \\(Y\\).\nNotice that the regressors \\(X_{1}\\) satisfy condition (12.5) and thus should be included as instrumental variables. They are therefore a subset of the variables \\(Z\\). Notationally we make the partition\n\\[\nZ=\\left(\\begin{array}{l}\nZ_{1} \\\\\nZ_{2}\n\\end{array}\\right)=\\left(\\begin{array}{c}\nX_{1} \\\\\nZ_{2}\n\\end{array}\\right) \\begin{aligned}\n&k_{1} \\\\\n&\\ell_{2}\n\\end{aligned} .\n\\]\nHere, \\(X_{1}=Z_{1}\\) are the included exogenous variables and \\(Z_{2}\\) are the excluded exogenous variables. That is, \\(Z_{2}\\) are variables which could be included in the equation for \\(Y\\) (in the sense that they are uncorrelated with \\(e\\) ) yet can be excluded as they have true zero coefficients in the equation. With this notation we can also write the structural equation (12.4) as\n\\[\nY_{1}=Z_{1}^{\\prime} \\beta_{1}+Y_{2}^{\\prime} \\beta_{2}+e .\n\\]\nThis is useful notation as it clarifies that the variable \\(Z_{1}\\) is exogenous and the variable \\(Y_{2}\\) is endogenous.\nMany authors describe \\(Z_{1}\\) as the “exogenous variables”, \\(Y_{2}\\) as the “endogenous variables”, and \\(Z_{2}\\) as the “instrumental variables”.\nWe say that the model is just-identified if \\(\\ell=k\\) and over-identified if \\(\\ell>k\\).\nWhat variables can be used as instrumental variables? From the definition \\(\\mathbb{E}[Z e]=0\\) the instrument must be uncorrelated with the equation error, meaning that it is excluded from the structural equation as mentioned above. From the rank condition (12.7) it is also important that the instrumental variables be correlated with the endogenous variables \\(Y_{2}\\) after controlling for the other exogenous variables \\(Z_{1}\\). These two requirements are typically interpreted as requiring that the instruments be determined outside the system for \\(\\vec{Y}\\), causally determine \\(Y_{2}\\), but do not causally determine \\(Y_{1}\\) except through \\(Y_{2}\\).\nLet’s take the three examples given above.\nMeasurement error in the regressor. When \\(X\\) is a mis-measured version of \\(Z\\) a common choice for an instrument \\(Z_{2}\\) is an alternative measurement of \\(Z\\). For this \\(Z_{2}\\) to satisfy the property of an instrumental variable the measurement error in \\(Z_{2}\\) must be independent of that in \\(X\\).\nSupply and Demand. An appropriate instrument for price \\(P\\) in a demand equation is a variable \\(Z_{2}\\) which influences supply but not demand. Such a variable affects the equilibrium values of \\(P\\) and \\(Q\\) but does not directly affect price except through quantity. Variables which affect supply but not demand are typically related to production costs.\nAn appropriate instrument for price in a supply equation is a variable which influences demand but not supply. Such a variable affects the equilibrium values of price and quantity but only affects price through quantity.\nChoice Variable as Regressor. An ideal instrument affects the choice of the regressor (education) but does not directly influence the dependent variable (wages) except through the indirect effect on the regressor. We will discuss an example in the next section."
  },
  {
    "objectID": "chpt12-iv.html#example-college-proximity",
    "href": "chpt12-iv.html#example-college-proximity",
    "title": "12  Instrumental Variables",
    "section": "12.6 Example: College Proximity",
    "text": "12.6 Example: College Proximity\nIn a influential paper David Card (1995) suggested if a potential student lives close to a college this reduces the cost of attendence and thereby raises the likelihood that the student will attend college. However, college proximity does not directly affect a student’s skills or abilities so should not have a direct effect on his or her market wage. These considerations suggest that college proximity can be used as an instrument for education in a wage regression. We use the simplest model reported in Card’s paper to illustrate the concepts of instrumental variables throughout the chapter.\nCard used data from the National Longitudinal Survey of Young Men (NLSYM) for 1976. A baseline least squares wage regression for his data set is reported in the first column of Table 12.1. The dependent variable is the log of weekly earnings. The regressors are education (years of schooling), experience (years of work experience, calculated as age (years) less education \\(+6\\) ), experience \\({ }^{2} / 100\\), Black, south (an indicator for residence in the southern region of the U.S.), and urban (an indicator for residence in a standard metropolitan statistical area). We drop observations for which wage is missing. The remaining sample has 3,010 observations. His data is the file Card1995 on the textbook website. The point estimate obtained by least squares suggests an \\(7 %\\) increase in earnings for each year of education.\nTable 12.1: Instrumental Variable Wage Regressions\n\n\n\n\n\n\n\n\n\n\n\n\neducation\nOLS\nIV(a)\nIV(b)\n2SLS(a)\n2SLS(b)\nLIML\n\n\n\n\n\n\\(0.074\\)\n\\(0.132\\)\n\\(0.133\\)\n\\(0.161\\)\n\\(0.160\\)\n\\(0.164\\)\n\n\n\n\\((0.004)\\)\n\\((0.049)\\)\n\\((0.051)\\)\n\\((0.040)\\)\n\\((0.041)\\)\n\\((0.042)\\)\n\n\n\n\\(0.084\\)\n\\(0.107\\)\n\\(0.056\\)\n\\(0.119\\)\n\\(0.047\\)\n\\(0.120\\)\n\n\nexperience \\(2 / 100\\)\n\\(-0.224\\)\n\\(-0.228\\)\n\\(-0.080\\)\n\\(-0.231\\)\n\\(-0.032\\)\n\\(-0.231\\)\n\n\n\n\\((0.032)\\)\n\\((0.035)\\)\n\\((0.133)\\)\n\\((0.037)\\)\n\\((0.127)\\)\n\\((0.037)\\)\n\n\nBlack\n\\(-0.190\\)\n\\(-0.131\\)\n\\(-0.103\\)\n\\(-0.102\\)\n\\(-0.064\\)\n\\(-0.099\\)\n\n\n\n\\((0.017)\\)\n\\((0.051)\\)\n\\((0.075)\\)\n\\((0.044)\\)\n\\((0.061)\\)\n\\((0.045)\\)\n\n\nsouth\n\\(-0.125\\)\n\\(-0.105\\)\n\\(-0.098\\)\n\\(-0.095\\)\n\\(-0.086\\)\n\\(-0.094\\)\n\n\n\n\\((0.015)\\)\n\\((0.023)\\)\n\\((0.0284)\\)\n\\((0.022)\\)\n\\((0.026)\\)\n\\((0.022)\\)\n\n\nurban\n\\(0.161\\)\n\\(0.131\\)\n\\(0.108\\)\n\\(0.116\\)\n\\(0.083\\)\n\\(0.115\\)\n\n\n\n\\((0.015)\\)\n\\((0.030)\\)\n\\((0.049)\\)\n\\((0.026)\\)\n\\((0.041)\\)\n\\((0.027)\\)\n\n\nSargan\n\n\n\n\\(0.82\\)\n\\(0.52\\)\n\\(0.82\\)\n\n\np-value\n\n\n\n\\(0.37\\)\n\\(0.47\\)\n\\(0.37\\)\n\n\n\nNotes:\n\nIV(a) uses college as an instrument for education.\nIV(b) uses college, age, and age \\(^{2} / 100\\) as instruments for education, experience, and experience \\({ }^{2} / 100\\).\n2SLS(a) uses public and private as instruments for education.\n\\(2 \\mathrm{SLS}(\\mathrm{b})\\) uses public, private, age, and age \\({ }^{2}\\) as instruments for education, experience, and experience \\(^{2} / 100\\).\nLIML uses public and private as instruments for education.\n\nAs discussed in the previous sections it is reasonable to view years of education as a choice made by an individual and thus is likely endogenous for the structural return to education. This means that least squares is an estimate of a linear projection but is inconsistent for coefficient of a structural equation representing the causal impact of years of education on expected wages. Labor economics predicts that ability, education, and wages will be positively correlated. This suggests that the population projection coefficient estimated by least squares will be higher than the structural parameter (and hence upwards biased). However, the sign of the bias is uncertain because there are multiple regressors and there are other potential sources of endogeneity.\nTo instrument for the endogeneity of education, Card suggested that a reasonable instrument is a dummy variable indicating if the individual grew up near a college. We will consider three measures:\ncollege Grew up in same county as a 4-year college\npublic Grew up in same county as a 4-year public college\nprivate Grew up in same county as a 4-year private college."
  },
  {
    "objectID": "chpt12-iv.html#reduced-form",
    "href": "chpt12-iv.html#reduced-form",
    "title": "12  Instrumental Variables",
    "section": "12.7 Reduced Form",
    "text": "12.7 Reduced Form\nThe reduced form is the relationship between the endogenous regressors \\(Y_{2}\\) and the instruments \\(Z\\). A linear reduced form model for \\(Y_{2}\\) is\n\\[\nY_{2}=\\Gamma^{\\prime} Z+u_{2}=\\Gamma_{12}^{\\prime} Z_{1}+\\Gamma_{22}^{\\prime} Z_{2}+u_{2}\n\\]\nThis is a multivariate regression as introduced in Chapter 11 . The \\(\\ell \\times k_{2}\\) coefficient matrix \\(\\Gamma\\) is defined by linear projection:\n\\[\n\\Gamma=\\mathbb{E}\\left[Z Z^{\\prime}\\right]^{-1} \\mathbb{E}\\left[Z Y_{2}^{\\prime}\\right]\n\\]\nThis implies \\(\\mathbb{E}\\left[Z u_{2}^{\\prime}\\right]=0\\). The projection coefficient (12.11) is well defined and unique under (12.6).\nWe also construct the reduced form for \\(Y_{1}\\). Substitute (12.10) into (12.9) to obtain\n\\[\n\\begin{aligned}\nY_{1} &=Z_{1}^{\\prime} \\beta_{1}+\\left(\\Gamma_{12}^{\\prime} Z_{1}+\\Gamma_{22}^{\\prime} Z_{2}+u_{2}\\right)^{\\prime} \\beta_{2}+e \\\\\n&=Z_{1}^{\\prime} \\lambda_{1}+Z_{2}^{\\prime} \\lambda_{2}+u_{1} \\\\\n&=Z^{\\prime} \\lambda+u_{1}\n\\end{aligned}\n\\]\nwhere\n\\[\n\\begin{aligned}\n&\\lambda_{1}=\\beta_{1}+\\Gamma_{12} \\beta_{2} \\\\\n&\\lambda_{2}=\\Gamma_{22} \\beta_{2} \\\\\n&u_{1}=u_{2}^{\\prime} \\beta_{2}+e .\n\\end{aligned}\n\\]\nWe can also write\n\\[\n\\lambda=\\bar{\\Gamma} \\beta\n\\]\nwhere\n\\[\n\\bar{\\Gamma}=\\left[\\begin{array}{cc}\n\\boldsymbol{I}_{k_{1}} & \\Gamma_{12} \\\\\n0 & \\Gamma_{22}\n\\end{array}\\right]=\\left[\\begin{array}{cc}\n\\boldsymbol{I}_{k_{1}} & \\Gamma \\\\\n0 &\n\\end{array}\\right] .\n\\]\nTogether, the reduced form equations for the system are\n\\[\n\\begin{aligned}\n&Y_{1}=\\lambda^{\\prime} Z+u_{1} \\\\\n&Y_{2}=\\Gamma^{\\prime} Z+u_{2} .\n\\end{aligned}\n\\]\nor\n\\[\n\\vec{Y}=\\left[\\begin{array}{cc}\n\\lambda_{1}^{\\prime} & \\lambda_{2}^{\\prime} \\\\\n\\Gamma_{12}^{\\prime} & \\Gamma_{22}^{\\prime}\n\\end{array}\\right] Z+u\n\\]\nwhere \\(u=\\left(u_{1}, u_{2}\\right)\\).\nThe relationships (12.14)-(12.16) are critically important for understanding the identification of the structural parameters \\(\\beta_{1}\\) and \\(\\beta_{2}\\), as we discuss below. These equations show the tight relationship between the structural parameters \\(\\left(\\beta_{1}\\right.\\) and \\(\\left.\\beta_{2}\\right)\\) and the reduced form parameters \\((\\Gamma\\) and \\(\\lambda)\\).\nThe reduced form equations are projections so the coefficients may be estimated by least squares (see Chapter 11). The least squares estimators of (12.11) and (12.13) are\n\\[\n\\begin{aligned}\n&\\widehat{\\Gamma}=\\left(\\sum_{i=1}^{n} Z_{i} Z_{i}^{\\prime}\\right)^{-1}\\left(\\sum_{i=1}^{n} Z_{i} Y_{2 i}^{\\prime}\\right) \\\\\n&\\widehat{\\lambda}=\\left(\\sum_{i=1}^{n} Z_{i} Z_{i}^{\\prime}\\right)^{-1}\\left(\\sum_{i=1}^{n} Z_{i} Y_{1 i}\\right) .\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "chpt12-iv.html#identification",
    "href": "chpt12-iv.html#identification",
    "title": "12  Instrumental Variables",
    "section": "12.8 Identification",
    "text": "12.8 Identification\nA parameter is identified if it is a unique function of the probability distribution of the observables. One way to show that a parameter is identified is to write it as an explicit function of population moments. For example, the reduced form coefficient matrices \\(\\Gamma\\) and \\(\\lambda\\) are identified because they can be written as explicit functions of the moments of the variables \\((Y, X, Z)\\). That is,\n\\[\n\\begin{aligned}\n&\\Gamma=\\mathbb{E}\\left[Z Z^{\\prime}\\right]^{-1} \\mathbb{E}\\left[Z Y_{2}^{\\prime}\\right] \\\\\n&\\lambda=\\mathbb{E}\\left[Z Z^{\\prime}\\right]^{-1} \\mathbb{E}\\left[Z Y_{1}\\right] .\n\\end{aligned}\n\\]\nThese are uniquely determined by the probability distribution of \\(\\left(Y_{1}, Y_{2}, Z\\right)\\) if Definition \\(12.1\\) holds, because this includes the requirement that \\(\\mathbb{E}\\left[Z Z^{\\prime}\\right]\\) is invertible.\nWe are interested in the structural parameter \\(\\beta\\). It relates to \\((\\lambda, \\Gamma)\\) through (12.16). \\(\\beta\\) is identified if it uniquely determined by this relation. This is a set of \\(\\ell\\) equations with \\(k\\) unknowns with \\(\\ell \\geq k\\). From linear algebra we know that there is a unique solution if and only if \\(\\bar{\\Gamma}\\) has full rank \\(k\\).\n\\[\n\\operatorname{rank}(\\bar{\\Gamma})=k .\n\\]\nUnder (12.22) \\(\\beta\\) can be uniquely solved from (12.16). If (12.22) fails then (12.16) has fewer equations than coefficients so there is not a unique solution.\nWe can write \\(\\bar{\\Gamma}=\\mathbb{E}\\left[Z Z^{\\prime}\\right]^{-1} \\mathbb{E}\\left[Z X^{\\prime}\\right]\\). Combining this with (12.16) we obtain\n\\[\n\\mathbb{E}\\left[Z Z^{\\prime}\\right]^{-1} \\mathbb{E}\\left[Z Y_{1}\\right]=\\mathbb{E}\\left[Z Z^{\\prime}\\right]^{-1} \\mathbb{E}\\left[Z X^{\\prime}\\right] \\beta\n\\]\nor\n\\[\n\\mathbb{E}\\left[Z Y_{1}\\right]=\\mathbb{E}\\left[Z X^{\\prime}\\right] \\beta\n\\]\nwhich is a set of \\(\\ell\\) equations with \\(k\\) unknowns. This has a unique solution if (and only if)\n\\[\n\\operatorname{rank}\\left(\\mathbb{E}\\left[Z X^{\\prime}\\right]\\right)=k\n\\]\nwhich was listed in (12.7) as a condition of Definition 12.1. (Indeed, this is why it was listed as part of the definition.) We can also see that (12.22) and (12.23) are equivalent ways of expressing the same requirement. If this condition fails then \\(\\beta\\) will not be identified. The condition (12.22)-(12.23) is called the relevance condition.\nIt is useful to have explicit expressions for the solution \\(\\beta\\). The easiest case is when \\(\\ell=k\\). Then (12.22) implies \\(\\bar{\\Gamma}\\) is invertible so the structural parameter equals \\(\\beta=\\bar{\\Gamma}^{-1} \\lambda\\). It is a unique solution because \\(\\bar{\\Gamma}\\) and \\(\\lambda\\) are unique and \\(\\bar{\\Gamma}\\) is invertible.\nWhen \\(\\ell>k\\) we can solve for \\(\\beta\\) by applying least squares to the system of equations \\(\\lambda=\\bar{\\Gamma} \\beta\\). This is \\(\\ell\\) equations with \\(k\\) unknowns and no error. The least squares solution is \\(\\beta=\\left(\\bar{\\Gamma}^{\\prime} \\bar{\\Gamma}\\right)^{-1} \\bar{\\Gamma}^{\\prime} \\lambda\\). Under (12.22) the matrix \\(\\bar{\\Gamma}^{\\prime} \\bar{\\Gamma}\\) is invertible so the solution is unique.\n\\(\\beta\\) is identified if \\(\\operatorname{rank}(\\bar{\\Gamma})=k\\), which is true if and only if \\(\\operatorname{rank}\\left(\\Gamma_{22}\\right)=k_{2}\\) (by the upper-diagonal structure of \\(\\bar{\\Gamma})\\). Thus the key to identification of the model rests on the \\(\\ell_{2} \\times k_{2}\\) matrix \\(\\Gamma_{22}\\) in (12.10). To see this, recall the reduced form relationships (12.14)-(12.15). We can see that \\(\\beta_{2}\\) is identified from (12.15) alone, and the necessary and sufficient condition is \\(\\operatorname{rank}\\left(\\Gamma_{22}\\right)=k_{2}\\). If this is satisfied then the solution equals \\(\\beta_{2}=\\left(\\Gamma_{22}^{\\prime} \\Gamma_{22}\\right)^{-1} \\Gamma_{22}^{\\prime} \\lambda_{2} \\cdot \\beta_{1}\\) is identified from this and (12.14), with the explicit solution \\(\\beta_{1}=\\lambda_{1}-\\Gamma_{12}\\left(\\Gamma_{22}^{\\prime} \\Gamma_{22}\\right)^{-1} \\Gamma_{22}^{\\prime} \\lambda_{2}\\). In the just-identified case \\(\\left(\\ell_{2}=k_{2}\\right)\\) these equations simplify as \\(\\beta_{2}=\\Gamma_{22}^{-1} \\lambda_{2}\\) and \\(\\beta_{1}=\\lambda_{1}-\\Gamma_{12} \\Gamma_{22}^{-1} \\lambda_{2}\\)"
  },
  {
    "objectID": "chpt12-iv.html#instrumental-variables-estimator",
    "href": "chpt12-iv.html#instrumental-variables-estimator",
    "title": "12  Instrumental Variables",
    "section": "12.9 Instrumental Variables Estimator",
    "text": "12.9 Instrumental Variables Estimator\nIn this section we consider the special case where the model is just-identified so that \\(\\ell=k\\).\nThe assumption that \\(Z\\) is an instrumental variable implies that \\(\\mathbb{E}[Z e]=0\\). Making the substitution \\(e=Y_{1}-X^{\\prime} \\beta\\) we find \\(\\mathbb{E}\\left[Z\\left(Y_{1}-X^{\\prime} \\beta\\right)\\right]=0\\). Expanding,\n\\[\n\\mathbb{E}\\left[Z Y_{1}\\right]-\\mathbb{E}\\left[Z X^{\\prime}\\right] \\beta=0 .\n\\]\nThis is a system of \\(\\ell=k\\) equations and \\(k\\) unknowns. Solving for \\(\\beta\\) we find\n\\[\n\\beta=\\left(\\mathbb{E}\\left[Z X^{\\prime}\\right]\\right)^{-1} \\mathbb{E}\\left[Z Y_{1}\\right] .\n\\]\nThis requires that the matrix \\(\\mathbb{E}\\left[Z X^{\\prime}\\right]\\) is invertible, which holds under (12.7) or equivalently (12.23).\nThe instrumental variables (IV) estimator \\(\\beta\\) replaces population by sample moments. We find\n\\[\n\\begin{aligned}\n\\widehat{\\beta}_{\\mathrm{iv}} &=\\left(\\frac{1}{n} \\sum_{i=1}^{n} Z_{i} X_{i}^{\\prime}\\right)^{-1}\\left(\\frac{1}{n} \\sum_{i=1}^{n} Z_{i} Y_{1 i}\\right) \\\\\n&=\\left(\\sum_{i=1}^{n} Z_{i} X_{i}^{\\prime}\\right)^{-1}\\left(\\sum_{i=1}^{n} Z_{i} Y_{1 i}\\right) .\n\\end{aligned}\n\\]\nMore generally, given any variable \\(W \\in \\mathbb{R}^{k}\\) it is common to refer to the estimator\n\\[\n\\widehat{\\beta}_{\\mathrm{iv}}=\\left(\\sum_{i=1}^{n} W_{i} X_{i}^{\\prime}\\right)^{-1}\\left(\\sum_{i=1}^{n} W_{i} Y_{1 i}\\right)\n\\]\nas the IV estimator for \\(\\beta\\) using the instrument \\(W\\).\nAlternatively, recall that when \\(\\ell=k\\) the structural parameter can be written as a function of the reduced form parameters as \\(\\beta=\\bar{\\Gamma}^{-1} \\lambda\\). Replacing \\(\\bar{\\Gamma}\\) and \\(\\lambda\\) by their least squares estimators (12.18)-(12.19) we can construct what is called the Indirect Least Squares (ILS) estimator. Using the matrix algebra representations\n\\[\n\\begin{aligned}\n\\widehat{\\beta}_{\\mathrm{ils}} &=\\widehat{\\bar{\\Gamma}}^{-1} \\widehat{\\lambda} \\\\\n&=\\left(\\left(\\boldsymbol{Z}^{\\prime} \\boldsymbol{Z}\\right)^{-1}\\left(\\boldsymbol{Z}^{\\prime} \\boldsymbol{X}\\right)\\right)^{-1}\\left(\\left(\\boldsymbol{Z}^{\\prime} \\boldsymbol{Z}\\right)^{-1}\\left(\\boldsymbol{Z}^{\\prime} \\boldsymbol{Y}_{1}\\right)\\right) \\\\\n&=\\left(\\boldsymbol{Z}^{\\prime} \\boldsymbol{X}\\right)^{-1}\\left(\\boldsymbol{Z}^{\\prime} \\boldsymbol{Z}\\right)\\left(\\boldsymbol{Z}^{\\prime} \\boldsymbol{Z}\\right)^{-1}\\left(\\boldsymbol{Z}^{\\prime} \\boldsymbol{Y}_{1}\\right) \\\\\n&=\\left(\\boldsymbol{Z}^{\\prime} \\boldsymbol{X}\\right)^{-1}\\left(\\boldsymbol{Z}^{\\prime} \\boldsymbol{Y}_{1}\\right) .\n\\end{aligned}\n\\]\nWe see that this equals the IV estimator (12.24). Thus the ILS and IV estimators are identical.\nGiven the IV estimator we define the residual \\(\\widehat{e}_{i}=Y_{1 i}-X_{i}^{\\prime} \\widehat{\\beta}_{\\mathrm{iv}}\\). It satisfies\n\\[\n\\boldsymbol{Z}^{\\prime} \\widehat{\\boldsymbol{e}}=\\boldsymbol{Z}^{\\prime} \\boldsymbol{Y}_{1}-\\boldsymbol{Z}^{\\prime} \\boldsymbol{X}\\left(\\boldsymbol{Z}^{\\prime} \\boldsymbol{X}\\right)^{-1}\\left(\\boldsymbol{Z}^{\\prime} \\boldsymbol{Y}_{1}\\right)=0\n\\]\nSince \\(Z\\) includes an intercept this means that the residuals sum to zero and are uncorrelated with the included and excluded instruments.\nTo illustrate IV regression we estimate the reduced form equations, treating education as endogenous and using college as an instrumental variable. The reduced form equations for log(wage) and education are reported in the first and second columns of Table 12.2. Table 12.2: Reduced Form Regressions\n\nOf particular interest is the equation for the endogenous regressor education, and the coefficients for the excluded instruments - in this case college. The estimated coefficient equals \\(0.337\\) with a small standard error. This implies that growing up near a 4-year college increases average educational attainment by \\(0.3\\) years. This seems to be a reasonable magnitude.\nSince the structural equation is just-identified with one right-hand-side endogenous variable the ILS/IV estimate for the education coefficient is the ratio of the coefficient estimates for the instrument college in the two equations, e.g. \\(0.045 / 0.337=0.13\\), implying a \\(13 %\\) return to each year of education. This is substantially greater than the \\(7 %\\) least squares estimate from the first column of Table 12.1. The IV estimates of the full equation are reported in the second column of Table 12.1. One first reaction is surprise that the IV estimate is larger than the OLS estimate. The endogeneity of educational choice should lead to upward bias in the OLS estimator, which predicts that the IV estimate should have been smaller than the OLS estimator. An alternative explanation may be needed. One possibility is heterogeneous education effects (when the education coefficient \\(\\beta\\) is heterogenous across individuals). In Section \\(12.34\\) we show that in this context the IV estimator picks up this treatment effect for a subset of the population, and this may explain why IV estimation results in a larger estimated coefficient.\nCard (1995) also points out that if education is endogenous then so is our measure of experience as it is calculated by subtracting education from age. He suggests that we can use the variables age and age \\({ }^{2}\\) as instruments for experience and experience \\({ }^{2}\\). The age variables are exogenous (not choice variables) yet highly correlated with experience and experience \\({ }^{2}\\). Notice that this approach treats experience \\({ }^{2}\\) as a variable separate from experience. Indeed, this is the correct approach.\nFollowing this recommendation we now have three endogenous regressors and three instruments. We present the three reduced form equations for the three endogenous regressors in the third through fifth columns of Table 12.2. It is interesting to compare the equations for education and experience. The two sets of coefficients are simply the sign change of the other with the exception of the coefficient on age. Indeed this must be the case because the three variables are linearly related. Does this cause a problem for 2SLS? Fortunately, no. The fact that the coefficient on age is not simply a sign change means that the equations are not linearly singular. Hence Assumption (12.22) is not violated.\nThe IV estimates using the three instruments college, age, and age \\({ }^{2}\\) for the endogenous regressors education, experience, and experience \\({ }^{2}\\) is presented in the third column of Table 12.1. The estimate of the returns to schooling is not affected by this change in the instrument set, but the estimated return to experience profile flattens (the quadratic effect diminishes).\nThe IV estimator may be calculated in Stata using the ivregress 2 sls command."
  },
  {
    "objectID": "chpt12-iv.html#demeaned-representation",
    "href": "chpt12-iv.html#demeaned-representation",
    "title": "12  Instrumental Variables",
    "section": "12.10 Demeaned Representation",
    "text": "12.10 Demeaned Representation\nDoes the well-known demeaned representation for linear regression (3.18) carry over to the IV estimator? To see, write the linear projection equation in the format \\(Y_{1}=X^{\\prime} \\beta+\\alpha+e\\) where \\(\\alpha\\) is the intercept and \\(X\\) does not contain a constant. Similarly, partition the instrument as \\((1, Z)\\) where \\(Z\\) does not contain a constant. We can write the IV estimator for the \\(i^{t h}\\) equation as\n\\[\nY_{1 i}=X_{i}^{\\prime} \\widehat{\\beta}_{\\mathrm{iv}}+\\widehat{\\alpha}_{\\mathrm{iv}}+\\widehat{e}_{i} .\n\\]\nThe orthogonality (12.25) implies the two-equation system\n\\[\n\\begin{aligned}\n&\\sum_{i=1}^{n}\\left(Y_{1 i}-X_{i}^{\\prime} \\widehat{\\beta}_{\\mathrm{iv}}-\\widehat{\\alpha}_{\\mathrm{iv}}\\right)=0 \\\\\n&\\sum_{i=1}^{n} Z_{i}\\left(Y_{1 i}-X_{i}^{\\prime} \\widehat{\\beta}_{\\mathrm{iv}}-\\widehat{\\alpha}_{\\mathrm{iv}}\\right)=0 .\n\\end{aligned}\n\\]\nThe first equation implies \\(\\widehat{\\alpha}_{\\mathrm{iv}}=\\overline{Y_{1}}-\\bar{X}^{\\prime} \\widehat{\\beta}_{\\mathrm{iv}}\\). Substituting into the second equation\n\\[\n\\sum_{i=1}^{n} Z_{i}\\left(\\left(Y_{1 i}-\\overline{Y_{1}}\\right)-\\left(X_{i}-\\bar{X}\\right)^{\\prime} \\widehat{\\beta}_{\\mathrm{iv}}\\right)\n\\]\nand solving for \\(\\widehat{\\beta}_{\\text {iv }}\\) we find\n\\[\n\\begin{aligned}\n\\widehat{\\beta}_{\\mathrm{iv}} &=\\left(\\sum_{i=1}^{n} Z_{i}\\left(X_{i}-\\bar{X}\\right)^{\\prime}\\right)^{-1}\\left(\\sum_{i=1}^{n} Z_{i}\\left(Y_{1 i}-\\bar{Y}_{1}\\right)\\right) \\\\\n&=\\left(\\sum_{i=1}^{n}\\left(Z_{i}-\\bar{Z}\\right)\\left(X_{i}-\\bar{X}\\right)^{\\prime}\\right)^{-1}\\left(\\sum_{i=1}^{n}\\left(Z_{i}-\\bar{Z}\\right)\\left(Y_{1 i}-\\bar{Y}_{1}\\right)\\right) .\n\\end{aligned}\n\\]\nThus the demeaning equations for least squares carry over to the IV estimator. The coefficient estimator \\(\\widehat{\\beta}_{\\text {iv }}\\) is a function only of the demeaned data."
  },
  {
    "objectID": "chpt12-iv.html#wald-estimator",
    "href": "chpt12-iv.html#wald-estimator",
    "title": "12  Instrumental Variables",
    "section": "12.11 Wald Estimator",
    "text": "12.11 Wald Estimator\nIn many cases including the Card proximity example the excluded instrument is a binary (dummy) variable. Let’s focus on that case and suppose that the model has just one endogenous regressor and no other regressors beyond the intercept. The model can be written as \\(Y=X \\beta+\\alpha+e\\) with \\(\\mathbb{E}[e \\mid Z]=0\\) and \\(Z\\) binary. Take expectations of the structural equation given \\(Z=1\\) and \\(Z=0\\), respectively. We obtain\n\\[\n\\begin{aligned}\n&\\mathbb{E}[Y \\mid Z=1]=\\mathbb{E}[X \\mid Z=1] \\beta+\\alpha \\\\\n&\\mathbb{E}[Y \\mid Z=0]=\\mathbb{E}[X \\mid Z=0] \\beta+\\alpha .\n\\end{aligned}\n\\]\nSubtracting and dividing we obtain an expression for the slope coefficient\n\\[\n\\beta=\\frac{\\mathbb{E}[Y \\mid Z=1]-\\mathbb{E}[Y \\mid Z=0]}{\\mathbb{E}[X \\mid Z=1]-\\mathbb{E}[X \\mid Z=0]} .\n\\]\nThe natural moment estimator replaces the expectations by the averages within the “grouped data” where \\(Z_{i}=1\\) and \\(Z_{i}=0\\), respectively. That is, define the group means\n\\[\n\\begin{array}{ll}\n\\bar{Y}_{1}=\\frac{\\sum_{i=1}^{n} Z_{i} Y_{i}}{\\sum_{i=1}^{n} Z_{i}}, & \\bar{Y}_{0}=\\frac{\\sum_{i=1}^{n}\\left(1-Z_{i}\\right) Y_{i}}{\\sum_{i=1}^{n}\\left(1-Z_{i}\\right)} \\\\\n\\bar{X}_{1}=\\frac{\\sum_{i=1}^{n} Z_{i} X_{i}}{\\sum_{i=1}^{n} Z_{i}}, & \\bar{X}_{0}=\\frac{\\sum_{i=1}^{n}\\left(1-Z_{i}\\right) X_{i}}{\\sum_{i=1}^{n}\\left(1-Z_{i}\\right)}\n\\end{array}\n\\]\nand the moment estimator\n\\[\n\\widehat{\\beta}=\\frac{\\bar{Y}_{1}-\\bar{Y}_{0}}{\\bar{X}_{1}-\\bar{X}_{0}} .\n\\]\nThis is the “Wald estimator” of Wald (1940).\nThese expressions are rather insightful. (12.27) shows that the structural slope coefficient is the expected change in \\(Y\\) due to changing the instrument divided by the expected change in \\(X\\) due to changing the instrument. Informally, it is the change in \\(Y\\) (due to \\(Z\\) ) over the change in \\(X\\) (due to \\(Z\\) ). Equation (12.28) shows that the slope coefficient can be estimated by the ratio of differences in means.\nThe expression (12.28) may appear like a distinct estimator from the IV estimator \\(\\widehat{\\beta}_{\\text {iv }}\\) but it turns out that they are the same. That is, \\(\\widehat{\\beta}=\\widehat{\\beta}_{\\mathrm{iv}}\\). To see this, use (12.26) to find\n\\[\n\\widehat{\\beta}_{\\mathrm{iv}}=\\frac{\\sum_{i=1}^{n} Z_{i}\\left(Y_{i}-\\bar{Y}\\right)}{\\sum_{i=1}^{n} Z_{i}\\left(X_{i}-\\bar{X}\\right)}=\\frac{\\bar{Y}_{1}-\\bar{Y}}{\\bar{X}_{1}-\\bar{X}} .\n\\]\nThen notice\n\\[\n\\bar{Y}_{1}-\\bar{Y}=\\bar{Y}_{1}-\\left(\\frac{1}{n} \\sum_{i=1}^{n} Z_{i} \\bar{Y}_{1}+\\frac{1}{n} \\sum_{i=1}^{n}\\left(1-Z_{i}\\right) \\bar{Y}_{0}\\right)=(1-\\bar{Z})\\left(\\bar{Y}_{1}-\\bar{Y}_{0}\\right)\n\\]\nand similarly\n\\[\n\\bar{X}_{1}-\\bar{X}=(1-\\bar{Z})\\left(\\bar{X}_{1}-\\bar{X}_{0}\\right)\n\\]\nand hence\n\\[\n\\widehat{\\beta}_{\\mathrm{iv}}=\\frac{(1-\\bar{Z})\\left(\\bar{Y}_{1}-\\bar{Y}_{0}\\right)}{(1-\\bar{Z})\\left(\\bar{X}_{1}-\\bar{X}_{0}\\right)}=\\widehat{\\beta}\n\\]\nas defined in (12.28). Thus the Wald estimator equals the IV estimator.\nWe can illustrate using the Card proximity example. If we estimate a simple IV model with no covariates we obtain the estimate \\(\\widehat{\\beta}_{\\text {iv }}=0.19\\). If we estimate the group-mean of log wages and education based on the instrument college we find\n\n\n\n\nnear college\nnot near college\ndifference\n\n\n\n\n\\(\\log (\\) wage)\n\\(6.311\\)\n\\(6.156\\)\n\\(0.155\\)\n\n\neducation\n\\(13.527\\)\n\\(12.698\\)\n\\(0.829\\)\n\n\nratio\n\n\n\\(0.19\\)\n\n\n\nBased on these estimates the Wald estimator of the slope coefficient is \\((6.311-6.156) /(13.527-12.698)=\\) \\(0.155 / 0.829=0.19\\), the same as the IV estimator."
  },
  {
    "objectID": "chpt12-iv.html#two-stage-least-squares",
    "href": "chpt12-iv.html#two-stage-least-squares",
    "title": "12  Instrumental Variables",
    "section": "12.12 Two-Stage Least Squares",
    "text": "12.12 Two-Stage Least Squares\nThe IV estimator described in the previous section presumed \\(\\ell=k\\). Now we allow the general case of \\(\\ell \\geq k\\). Examining the reduced-form equation (12.13) we see\n\\[\n\\begin{aligned}\nY_{1} &=Z^{\\prime} \\bar{\\Gamma} \\beta+u_{1} \\\\\n\\mathbb{E}\\left[Z u_{1}\\right] &=0 .\n\\end{aligned}\n\\]\nDefining \\(W=\\bar{\\Gamma}^{\\prime} Z\\) we can write this as\n\\[\n\\begin{aligned}\nY_{1} &=W^{\\prime} \\beta+u_{1} \\\\\n\\mathbb{E}\\left[W u_{1}\\right] &=0 .\n\\end{aligned}\n\\]\nOne way of thinking about this is that \\(Z\\) is set of candidate instruments. The instrument vector \\(W=\\bar{\\Gamma}^{\\prime} Z\\) is a \\(k\\)-dimentional set of linear combinations.\nSuppose that \\(\\bar{\\Gamma}\\) were known. Then we would estimate \\(\\beta\\) by least squares of \\(Y_{1}\\) on \\(W=\\bar{\\Gamma}^{\\prime} Z\\)\n\\[\n\\widehat{\\beta}=\\left(\\boldsymbol{W}^{\\prime} \\boldsymbol{W}\\right)^{-1}\\left(\\boldsymbol{W}^{\\prime} \\boldsymbol{Y}\\right)=\\left(\\bar{\\Gamma}^{\\prime} \\boldsymbol{Z}^{\\prime} \\boldsymbol{Z} \\bar{\\Gamma}\\right)^{-1}\\left(\\bar{\\Gamma}^{\\prime} \\boldsymbol{Z}^{\\prime} \\boldsymbol{Y}_{1}\\right) .\n\\]\nWhile this is infeasible we can estimate \\(\\bar{\\Gamma}\\) from the reduced form regression. Replacing \\(\\bar{\\Gamma}\\) with its estimator \\(\\widehat{\\Gamma}=\\left(\\boldsymbol{Z}^{\\prime} \\boldsymbol{Z}\\right)^{-1}\\left(\\boldsymbol{Z}^{\\prime} \\boldsymbol{X}\\right)\\) we obtain\n\\[\n\\begin{aligned}\n\\widehat{\\beta}_{2 \\text { sls }} &=\\left(\\widehat{\\Gamma}^{\\prime} \\boldsymbol{Z}^{\\prime} \\boldsymbol{Z} \\widehat{\\Gamma}\\right)^{-1}\\left(\\widehat{\\Gamma}^{\\prime} \\boldsymbol{Z}^{\\prime} \\boldsymbol{Y}_{1}\\right) \\\\\n&=\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{Z}\\left(\\boldsymbol{Z}^{\\prime} \\boldsymbol{Z}\\right)^{-1} \\boldsymbol{Z}^{\\prime} \\boldsymbol{Z}\\left(\\boldsymbol{Z}^{\\prime} \\boldsymbol{Z}\\right)^{-\\mathbf{1}} \\boldsymbol{Z}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\boldsymbol{X}^{\\prime} \\boldsymbol{Z}\\left(\\boldsymbol{Z}^{\\prime} \\boldsymbol{Z}\\right)^{-1} \\boldsymbol{Z}^{\\prime} \\boldsymbol{Y}_{1} \\\\\n&=\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{Z}\\left(\\boldsymbol{Z}^{\\prime} \\boldsymbol{Z}\\right)^{-1} \\boldsymbol{Z}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\boldsymbol{X}^{\\prime} \\boldsymbol{Z}\\left(\\boldsymbol{Z}^{\\prime} \\boldsymbol{Z}\\right)^{-1} \\boldsymbol{Z}^{\\prime} \\boldsymbol{Y}_{1}\n\\end{aligned}\n\\]\nThis is called the two-stage-least squares (2SLS) estimator. It was originally proposed by Theil (1953) and Basmann (1957) and is a standard estimator for linear equations with instruments.\nIf the model is just-identified, so that \\(k=\\ell\\), then 2SLS simplifies to the IV estimator of the previous section. Since the matrices \\(\\boldsymbol{X}^{\\prime} \\boldsymbol{Z}\\) and \\(\\boldsymbol{Z}^{\\prime} \\boldsymbol{X}\\) are square we can factor\n\\[\n\\begin{aligned}\n\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{Z}\\left(\\boldsymbol{Z}^{\\prime} \\boldsymbol{Z}\\right)^{-1} \\boldsymbol{Z}^{\\prime} \\boldsymbol{X}\\right)^{-1} &=\\left(\\boldsymbol{Z}^{\\prime} \\boldsymbol{X}\\right)^{-1}\\left(\\left(\\boldsymbol{Z}^{\\prime} \\boldsymbol{Z}\\right)^{-1}\\right)^{-1}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{Z}\\right)^{-1} \\\\\n&=\\left(\\boldsymbol{Z}^{\\prime} \\boldsymbol{X}\\right)^{-1}\\left(\\boldsymbol{Z}^{\\prime} \\boldsymbol{Z}\\right)\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{Z}\\right)^{-1}\n\\end{aligned}\n\\]\n(Once again, this only works when \\(k=\\ell\\).) Then\n\\[\n\\begin{aligned}\n\\widehat{\\beta}_{2 \\text { sls }} &=\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{Z}\\left(\\boldsymbol{Z}^{\\prime} \\boldsymbol{Z}\\right)^{-1} \\boldsymbol{Z}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\boldsymbol{X}^{\\prime} \\boldsymbol{Z}\\left(\\boldsymbol{Z}^{\\prime} \\boldsymbol{Z}\\right)^{-1} \\boldsymbol{Z}^{\\prime} \\boldsymbol{Y}_{1} \\\\\n&=\\left(\\boldsymbol{Z}^{\\prime} \\boldsymbol{X}\\right)^{-1}\\left(\\boldsymbol{Z}^{\\prime} \\boldsymbol{Z}\\right)\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{Z}\\right)^{-1} \\boldsymbol{X}^{\\prime} \\boldsymbol{Z}\\left(\\boldsymbol{Z}^{\\prime} \\boldsymbol{Z}\\right)^{-1} \\boldsymbol{Z}^{\\prime} \\boldsymbol{Y}_{1} \\\\\n&=\\left(\\boldsymbol{Z}^{\\prime} \\boldsymbol{X}\\right)^{-1}\\left(\\boldsymbol{Z}^{\\prime} \\boldsymbol{Z}\\right)\\left(\\boldsymbol{Z}^{\\prime} \\boldsymbol{Z}\\right)^{-1} \\boldsymbol{Z}^{\\prime} \\boldsymbol{Y}_{1} \\\\\n&=\\left(\\boldsymbol{Z}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\boldsymbol{Z}^{\\prime} \\boldsymbol{Y}_{1}=\\widehat{\\beta}_{\\mathrm{iv}}\n\\end{aligned}\n\\]\nas claimed. This shows that the 2SLS estimator as defined in (12.29) is a generalization of the IV estimator defined in (12.24).\nThere are several alternative representations of the 2SLS estimator which we now describe. First, defining the projection matrix\n\\[\n\\boldsymbol{P}_{\\boldsymbol{Z}}=\\boldsymbol{Z}\\left(\\boldsymbol{Z}^{\\prime} \\boldsymbol{Z}\\right)^{-1} \\boldsymbol{Z}^{\\prime}\n\\]\nwe can write the 2SLS estimator more compactly as\n\\[\n\\widehat{\\beta}_{2 \\text { sls }}=\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{P}_{\\boldsymbol{Z}} \\boldsymbol{X}\\right)^{-1} \\boldsymbol{X}^{\\prime} \\boldsymbol{P}_{\\boldsymbol{Z}} \\boldsymbol{Y}_{1} .\n\\]\nThis is useful for representation and derivations but is not useful for computation as the \\(n \\times n\\) matrix \\(\\boldsymbol{P}_{\\boldsymbol{Z}}\\) is too large to compute when \\(n\\) is large.\nSecond, define the fitted values for \\(\\boldsymbol{X}\\) from the reduced form \\(\\widehat{\\boldsymbol{X}}=\\boldsymbol{P}_{\\boldsymbol{Z}} \\boldsymbol{X}=\\boldsymbol{Z} \\widehat{\\Gamma}\\). Then the 2SLS estimator can be written as\n\\[\n\\widehat{\\beta}_{2 \\text { sls }}=\\left(\\widehat{\\boldsymbol{X}}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\widehat{\\boldsymbol{X}}^{\\prime} \\boldsymbol{Y}_{1}\n\\]\nThis is an IV estimator as defined in the previous section using \\(\\widehat{X}\\) as an instrument for \\(X\\).\nThird, because \\(\\boldsymbol{P}_{Z}\\) is idempotent we can also write the 2SLS estimator as\n\\[\n\\widehat{\\beta}_{2 \\text { sls }}=\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{P}_{\\boldsymbol{Z}} \\boldsymbol{P}_{\\boldsymbol{Z}} \\boldsymbol{X}\\right)^{-1} \\boldsymbol{X}^{\\prime} \\boldsymbol{P}_{\\boldsymbol{Z}} \\boldsymbol{Y}_{1}=\\left(\\widehat{\\boldsymbol{X}}^{\\prime} \\widehat{\\boldsymbol{X}}\\right)^{-1} \\widehat{\\boldsymbol{X}}^{\\prime} \\boldsymbol{Y}_{1}\n\\]\nwhich is the least squares estimator obtained by regressing \\(Y_{1}\\) on the fitted values \\(\\widehat{X}\\).\nThis is the source of the “two-stage” name as it can be computed as follows.\n\nRegress \\(X\\) on \\(Z\\) to obtain the fitted \\(\\widehat{X}: \\widehat{\\Gamma}=\\left(\\boldsymbol{Z}^{\\prime} \\boldsymbol{Z}\\right)^{-1}\\left(\\boldsymbol{Z}^{\\prime} \\boldsymbol{X}\\right)\\) and \\(\\widehat{\\boldsymbol{X}}=\\boldsymbol{Z} \\widehat{\\Gamma}=\\boldsymbol{P}_{\\boldsymbol{Z}} \\boldsymbol{X}\\).\nRegress \\(Y_{1}\\) on \\(\\widehat{X}: \\widehat{\\beta}_{2 s l s}=\\left(\\widehat{\\boldsymbol{X}}^{\\prime} \\widehat{\\boldsymbol{X}}\\right)^{-1} \\widehat{\\boldsymbol{X}}^{\\prime} \\boldsymbol{Y}_{1}\\)\n\nIt is useful to scrutinize the projection \\(\\widehat{\\boldsymbol{X}}\\). Recall, \\(\\boldsymbol{X}=\\left[\\boldsymbol{Z}_{1}, \\boldsymbol{Y}_{2}\\right]\\) and \\(\\boldsymbol{Z}=\\left[\\boldsymbol{Z}_{1}, \\boldsymbol{Z}_{2}\\right]\\). Notice \\(\\widehat{\\boldsymbol{X}}_{1}=\\boldsymbol{P}_{\\boldsymbol{Z}} \\boldsymbol{Z}_{1}=\\) \\(Z_{1}\\) because \\(Z_{1}\\) lies in the span of \\(\\boldsymbol{Z}\\). Then \\(\\widehat{\\boldsymbol{X}}=\\left[\\widehat{\\boldsymbol{X}}_{1}, \\widehat{\\boldsymbol{Y}}_{2}\\right]=\\left[\\boldsymbol{Z}_{1}, \\widehat{\\boldsymbol{Y}}_{2}\\right]\\). This shows that in the second stage we regress \\(Y_{1}\\) on \\(Z_{1}\\) and \\(\\widehat{Y}_{2}\\). This means that only the endogenous variables \\(Y_{2}\\) are replaced by their fitted values, \\(\\widehat{Y}_{2}=\\widehat{\\Gamma}_{12}^{\\prime} Z_{1}+\\widehat{\\Gamma}_{22}^{\\prime} Z_{2}\\).\nA fourth representation of 2SLS can be obtained using the FWL Theorem. The third representation and following discussion showed that 2SLS is obtained as least squares of \\(Y_{1}\\) on the fitted values \\(\\left(Z_{1}, \\widehat{Y}_{2}\\right)\\). Hence the coefficient \\(\\widehat{\\beta}_{2}\\) on the endogenous variables can be found by residual regression. Set \\(\\boldsymbol{P}_{1}=\\) \\(Z_{1}\\left(Z_{1}^{\\prime} Z_{1}\\right)^{-1} Z_{1}^{\\prime}\\). Applying the FWL theorem we obtain\n\\[\n\\begin{aligned}\n\\widehat{\\beta}_{2} &=\\left(\\widehat{\\boldsymbol{Y}}_{2}^{\\prime}\\left(\\boldsymbol{I}_{n}-\\boldsymbol{P}_{1}\\right) \\widehat{\\boldsymbol{Y}}_{2}\\right)^{-1}\\left(\\widehat{\\boldsymbol{Y}}_{2}^{\\prime}\\left(\\boldsymbol{I}_{n}-\\boldsymbol{P}_{1}\\right) \\boldsymbol{Y}_{1}\\right) \\\\\n&=\\left(\\boldsymbol{Y}_{2}^{\\prime} \\boldsymbol{P}_{\\boldsymbol{Z}}\\left(\\boldsymbol{I}_{n}-\\boldsymbol{P}_{1}\\right) \\boldsymbol{P}_{\\boldsymbol{Z}} \\boldsymbol{Y}_{2}\\right)^{-1}\\left(\\boldsymbol{Y}_{2}^{\\prime} \\boldsymbol{P}_{\\boldsymbol{Z}}\\left(\\boldsymbol{I}_{n}-\\boldsymbol{P}_{1}\\right) \\boldsymbol{Y}_{1}\\right) \\\\\n&=\\left(\\boldsymbol{Y}_{2}^{\\prime}\\left(\\boldsymbol{P}_{\\boldsymbol{Z}}-\\boldsymbol{P}_{1}\\right) \\boldsymbol{Y}_{2}\\right)^{-1}\\left(\\boldsymbol{Y}_{2}^{\\prime}\\left(\\boldsymbol{P}_{\\boldsymbol{Z}}-\\boldsymbol{P}_{1}\\right) \\boldsymbol{Y}_{1}\\right)\n\\end{aligned}\n\\]\nbecause \\(\\boldsymbol{P}_{Z} \\boldsymbol{P}_{1}=\\boldsymbol{P}_{1}\\).\nA fifth representation can be obtained by a further projection. The projection matrix \\(\\boldsymbol{P}_{\\boldsymbol{Z}}\\) can be replaced by the projection onto the pair \\(\\left[\\boldsymbol{Z}_{1}, \\widetilde{\\boldsymbol{Z}}_{2}\\right.\\) ] where \\(\\widetilde{\\boldsymbol{Z}}_{2}=\\left(\\boldsymbol{I}_{n}-\\boldsymbol{P}_{1}\\right) \\boldsymbol{Z}_{2}\\) is \\(\\boldsymbol{Z}_{2}\\) projected orthogonal to \\(\\boldsymbol{Z}_{1}\\). Since \\(\\boldsymbol{Z}_{1}\\) and \\(\\widetilde{\\boldsymbol{Z}}_{2}\\) are orthogonal, \\(\\boldsymbol{P}_{\\boldsymbol{Z}}=\\boldsymbol{P}_{1}+\\boldsymbol{P}_{2}\\) where \\(\\boldsymbol{P}_{2}=\\widetilde{\\boldsymbol{Z}}_{2}\\left(\\widetilde{\\boldsymbol{Z}}_{2}^{\\prime} \\widetilde{\\boldsymbol{Z}}_{2}\\right)^{-1} \\widetilde{\\boldsymbol{Z}}_{2}^{\\prime}\\). Thus \\(\\boldsymbol{P}_{\\boldsymbol{Z}}-\\boldsymbol{P}_{1}=\\boldsymbol{P}_{2}\\) and\n\\[\n\\begin{aligned}\n\\widehat{\\beta}_{2} &=\\left(\\boldsymbol{Y}_{2}^{\\prime} \\boldsymbol{P}_{2} \\boldsymbol{Y}_{2}\\right)^{-1}\\left(\\boldsymbol{Y}_{2}^{\\prime} \\boldsymbol{P}_{2} \\boldsymbol{Y}_{1}\\right) \\\\\n&=\\left(\\boldsymbol{Y}_{2}^{\\prime} \\widetilde{\\boldsymbol{Z}}_{2}\\left(\\widetilde{\\boldsymbol{Z}}_{2}^{\\prime} \\widetilde{\\boldsymbol{Z}}_{2}\\right)^{-1} \\widetilde{\\boldsymbol{Z}}_{2}^{\\prime} \\boldsymbol{Y}_{2}\\right)^{-1}\\left(\\boldsymbol{Y}_{2}^{\\prime} \\widetilde{\\boldsymbol{Z}}_{2}\\left(\\widetilde{\\boldsymbol{Z}}_{2}^{\\prime} \\widetilde{\\boldsymbol{Z}}_{2}\\right)^{-1} \\widetilde{\\boldsymbol{Z}}_{2}^{\\prime} \\boldsymbol{Y}_{1}\\right) .\n\\end{aligned}\n\\]\nGiven the 2SLS estimator we define the residual \\(\\widehat{e}_{i}=Y_{1 i}-X_{i}^{\\prime} \\widehat{\\beta}_{2 s l s}\\). When the model is overidentified the instruments and residuals are not orthogonal. That is, \\(\\boldsymbol{Z}^{\\prime} \\widehat{\\boldsymbol{e}} \\neq 0\\). It does, however, satisfy\n\\[\n\\begin{aligned}\n\\widehat{\\boldsymbol{X}}^{\\prime} \\widehat{\\boldsymbol{e}} &=\\widehat{\\boldsymbol{\\Gamma}}^{\\prime} \\boldsymbol{Z}^{\\prime} \\widehat{\\boldsymbol{e}} \\\\\n&=\\boldsymbol{X}^{\\prime} \\boldsymbol{Z}\\left(\\boldsymbol{Z}^{\\prime} \\boldsymbol{Z}\\right)^{-1} \\boldsymbol{Z}^{\\prime} \\widehat{\\boldsymbol{e}} \\\\\n&=\\boldsymbol{X}^{\\prime} \\boldsymbol{Z}\\left(\\boldsymbol{Z}^{\\prime} \\boldsymbol{Z}\\right)^{-1} \\boldsymbol{Z}^{\\prime} \\boldsymbol{Y}_{1}-\\boldsymbol{X}^{\\prime} \\boldsymbol{Z}\\left(\\boldsymbol{Z}^{\\prime} \\boldsymbol{Z}\\right)^{-1} \\boldsymbol{Z}^{\\prime} \\boldsymbol{X} \\widehat{\\beta}_{2 \\text { sls }}=0 .\n\\end{aligned}\n\\]\nReturning to Card’s college proximity example suppose that we treat experience as exogeneous but that instead of using the single instrument college (grew up near a 4-year college) we use the two instruments (public, private) (grew up near a public/private 4-year college, respectively). In this case we have one endogenous variable (education) and two instruments (public, private). The estimated reduced form equation for education is presented in the sixth column of Table 12.2. In this specification the coefficient on public - growing up near a public 4-year college - is somewhat larger than that found for the variable college in the previous specification (column 2). Furthermore, the coefficient on private-growing up near a private 4-year college - is much smaller. This indicates that the key impact of proximity on education is via public colleges rather than private colleges.\nThe 2SLS estimates obtained using these two instruments are presented in the fourth column of Table 12.1. The coefficient on education increases to \\(0.161\\), indicating a \\(16 %\\) return to a year of education. This is roughly twice as large as the estimate obtained by least squares in the first column.\nAdditionally, if we follow Card and treat experience as endogenous and use age as an instrument we now have three endogenous variables (education, experience, experience \\({ }^{2} / 100\\) ) and four instruments (public, private, age, \\(a g e^{2}\\) ). We present the 2SLS estimates using this specification in the fifth column of Table 12.1. The estimate of the return to education remains \\(16 %\\) and the return to experience flattens.\nYou might wonder if we could use all three instruments - college, public, and private. The answer is no. This is because college \\(=\\) public \\(+\\) private so the three variables are colinear. Since the instruments are linearly related the three together would violate the full-rank condition (12.6).\nThe 2SLS estimator may be calculated in Stata using the ivregress 2 sls command."
  },
  {
    "objectID": "chpt12-iv.html#limited-information-maximum-likelihood",
    "href": "chpt12-iv.html#limited-information-maximum-likelihood",
    "title": "12  Instrumental Variables",
    "section": "12.13 Limited Information Maximum Likelihood",
    "text": "12.13 Limited Information Maximum Likelihood\nAn alternative method to estimate the parameters of the structural equation is by maximum likelihood. Anderson and Rubin (1949) derived the maximum likelihood estimator for the joint distribution of \\(\\vec{Y}=\\left(Y_{1}, Y_{2}\\right)\\). The estimator is known as limited information maximum likelihood (LIML).\nThis estimator is called “limited information” because it is based on the structural equation for \\(Y\\) combined with the reduced form equation for \\(X_{2}\\). If maximum likelihood is derived based on a structural equation for \\(X_{2}\\) as well this leads to what is known as full information maximum likelihood (FIML). The advantage of LIML relative to FIML is that the former does not require a structural model for \\(X_{2}\\) and thus allows the researcher to focus on the structural equation of interest - that for \\(Y\\). We do not describe the FIML estimator as it is not commonly used in applied econometrics.\nWhile the LIML estimator is less widely used among economists than 2SLS it has received a resurgence of attention from econometric theorists.\nTo derive the LIML estimator recall the definition \\(\\vec{Y}=\\left(Y_{1}, Y_{2}\\right)\\) and the reduced form (12.17)\n\\[\n\\begin{aligned}\n\\vec{Y} &=\\left[\\begin{array}{cc}\n\\lambda_{1}^{\\prime} & \\lambda_{2} \\\\\n\\Gamma_{12}^{\\prime} & \\Gamma_{22}^{\\prime}\n\\end{array}\\right]\\left(\\begin{array}{l}\nZ_{1} \\\\\nZ_{2}\n\\end{array}\\right)+u \\\\\n&=\\Pi_{1}^{\\prime} Z_{1}+\\Pi_{2}^{\\prime} Z_{2}+u\n\\end{aligned}\n\\]\nwhere \\(\\Pi_{1}=\\left[\\begin{array}{cc}\\lambda_{1} & \\Gamma_{12}\\end{array}\\right]\\) and \\(\\Pi_{2}=\\left[\\begin{array}{cc}\\lambda_{2} & \\Gamma_{22}\\end{array}\\right]\\). The LIML estimator is derived under the assumption that \\(u\\) is multivariate normal.\nDefine \\(\\gamma^{\\prime}=\\left[\\begin{array}{ll}1 & -\\beta_{2}^{\\prime}\\end{array}\\right]\\). From (12.15) we find\n\\[\n\\Pi_{2} \\gamma=\\lambda_{2}-\\Gamma_{22} \\beta_{2}=0 .\n\\]\nThus the \\(\\ell_{2} \\times\\left(k_{2}+1\\right)\\) coefficient matrix \\(\\Pi_{2}\\) in (12.33) has deficient rank. Indeed, its rank must be \\(k_{2}\\) because \\(\\Gamma_{22}\\) has full rank.\nThis means that the model (12.33) is precisely the reduced rank regression model of Section \\(11.11 .\\) Theorem \\(11.7\\) presents the maximum likelihood estimators for the reduced rank parameters. In particular, the MLE for \\(\\gamma\\) is\n\\[\n\\widehat{\\gamma}=\\underset{\\gamma}{\\operatorname{argmin}} \\frac{\\gamma^{\\prime} \\overrightarrow{\\boldsymbol{Y}}^{\\prime} \\boldsymbol{M}_{1} \\overrightarrow{\\boldsymbol{Y}} \\gamma}{\\gamma^{\\prime} \\overrightarrow{\\boldsymbol{Y}}^{\\prime} \\boldsymbol{M}_{\\boldsymbol{Z}} \\overrightarrow{\\boldsymbol{Y}} \\gamma}\n\\]\nwhere \\(\\boldsymbol{M}_{1}=\\boldsymbol{I}_{n}-\\boldsymbol{Z}_{1}\\left(\\boldsymbol{Z}_{1}^{\\prime} \\boldsymbol{Z}_{1}\\right)^{-1} \\boldsymbol{Z}_{1}^{\\prime}\\) and \\(\\boldsymbol{M}_{\\boldsymbol{Z}}=\\boldsymbol{I}_{n}-\\boldsymbol{Z}\\left(\\boldsymbol{Z}^{\\prime} \\boldsymbol{Z}\\right)^{-1} \\boldsymbol{Z}^{\\prime}\\). The minimization (12.34) is sometimes called the “least variance ratio” problem.\nThe minimization problem (12.34) is invariant to the scale of \\(\\gamma\\) (that is, \\(\\widehat{\\gamma} c\\) is equivalently the argmin for any c) so normalization is required. A convenient choice is \\(\\gamma^{\\prime} \\overrightarrow{\\boldsymbol{Y}}^{\\prime} \\boldsymbol{M}_{Z} \\overrightarrow{\\boldsymbol{Y}} \\gamma=1\\). Using this normalization and the theory of the minimum of quadratic forms (Section A.15) \\(\\widehat{\\gamma}\\) is the generalized eigenvector of \\(\\overrightarrow{\\boldsymbol{Y}}^{\\prime} \\boldsymbol{M}_{1} \\overrightarrow{\\boldsymbol{Y}}\\) with respect to \\(\\overrightarrow{\\boldsymbol{Y}}^{\\prime} \\boldsymbol{M}_{Z} \\overrightarrow{\\boldsymbol{Y}}\\) associated with the smallest generalized eigenvalue. (See Section A.14 for the definition of generalized eigenvalues and eigenvectors.) Computationally this is straightforward. For example, in MATLAB the generalized eigenvalues and eigenvectors of the matrix \\(\\boldsymbol{A}\\) with respect to \\(\\boldsymbol{B}\\) is found by the command eig \\((\\boldsymbol{A}, \\boldsymbol{B})\\). Once this \\(\\widehat{\\gamma}\\) is found any other normalization can be obtained by rescaling. For example, to obtain the MLE for \\(\\beta_{2}\\) make the partition \\(\\widehat{\\gamma}^{\\prime}=\\left[\\begin{array}{cc}\\widehat{\\gamma}_{1} & \\widehat{\\gamma}_{2}^{\\prime}\\end{array}\\right]\\) and set \\(\\widehat{\\beta}_{2}=-\\widehat{\\gamma}_{2} / \\widehat{\\gamma}_{1}\\).\nTo obtain the MLE for \\(\\beta_{1}\\) recall the structural equation \\(Y_{1}=Z_{1}^{\\prime} \\beta_{1}+Y_{2}^{\\prime} \\beta_{2}+e\\). Replace \\(\\beta_{2}\\) with the MLE \\(\\widehat{\\beta}_{2}\\) and apply regression. This yields\n\\[\n\\widehat{\\beta}_{1}=\\left(Z_{1}^{\\prime} Z_{1}\\right)^{-1} Z_{1}^{\\prime}\\left(Y_{1}-Y_{2} \\widehat{\\beta}_{2}\\right) .\n\\]\nThese solutions are the MLE for the structural parameters \\(\\beta_{1}\\) and \\(\\beta_{2}\\).\nPrevious econometrics textbooks did not present a derivation of the LIML estimator as the original derivation by Anderson and Rubin (1949) is lengthy and not particularly insightful. In contrast the derivation given here based on reduced rank regression is simple.\nThere is an alternative (and traditional) expression for the LIML estimator. Define the minimum obtained in (12.34)\n\\[\n\\widehat{\\boldsymbol{\\kappa}}=\\min _{\\gamma} \\frac{\\gamma^{\\prime} \\overrightarrow{\\boldsymbol{Y}}^{\\prime} \\boldsymbol{M}_{1} \\overrightarrow{\\boldsymbol{Y}} \\gamma}{\\gamma^{\\prime} \\overrightarrow{\\boldsymbol{Y}}^{\\prime} \\boldsymbol{M}_{\\boldsymbol{Z}} \\overrightarrow{\\boldsymbol{Y}} \\gamma}\n\\]\nwhich is the smallest generalized eigenvalue of \\(\\overrightarrow{\\boldsymbol{Y}}^{\\prime} \\boldsymbol{M}_{1} \\overrightarrow{\\boldsymbol{Y}}\\) with respect to \\(\\overrightarrow{\\boldsymbol{Y}}^{\\prime} \\boldsymbol{M}_{\\boldsymbol{Z}} \\overrightarrow{\\boldsymbol{Y}}\\). The LIML estimator can be written as\n\\[\n\\widehat{\\beta}_{\\text {liml }}=\\left(\\boldsymbol{X}^{\\prime}\\left(\\boldsymbol{I}_{n}-\\widehat{\\kappa} \\boldsymbol{M}_{\\boldsymbol{Z}}\\right) \\boldsymbol{X}\\right)^{-1}\\left(\\boldsymbol{X}^{\\prime}\\left(\\boldsymbol{I}_{n}-\\widehat{\\kappa} \\boldsymbol{M}_{\\boldsymbol{Z}}\\right) \\boldsymbol{Y}_{1}\\right) .\n\\]\nWe defer the derivation of (12.37) until the end of this section. Expression (12.37) does not simplify computation (because \\(\\widehat{\\kappa}\\) requires solving the same eigenvector problem that yields \\(\\widehat{\\beta}_{2}\\) ). However (12.37) is important for the distribution theory. It also helps reveal the algebraic connection between LIML, least squares, and 2SLS.\nThe estimator (12.37) with arbitrary \\(\\kappa\\) is known as a k-class estimator of \\(\\beta\\). While the LIML estimator obtains by setting \\(\\kappa=\\widehat{\\kappa}\\), the least squares estimator is obtained by setting \\(\\kappa=0\\) and 2SLS is obtained by setting \\(\\kappa=1\\). It is worth observing that the LIML solution satisfies \\(\\widehat{\\kappa} \\geq 1\\). When the model is just-identified the LIML estimator is identical to the IV and 2SLS estimators. They are only different in the over-identified setting. (One corollary is that under just-identification and normal errors the IV estimator is MLE.)\nFor inference it is useful to observe that (12.37) shows that \\(\\widehat{\\beta}_{\\mathrm{liml}}\\) can be written as an IV estimator\n\\[\n\\widehat{\\beta}_{\\mathrm{liml}}=\\left(\\widetilde{\\boldsymbol{X}}^{\\prime} \\boldsymbol{X}\\right)^{-1}\\left(\\widetilde{\\boldsymbol{X}}^{\\prime} \\boldsymbol{Y}_{1}\\right)\n\\]\nusing the instrument\n\\[\n\\widetilde{\\boldsymbol{X}}=\\left(\\boldsymbol{I}_{n}-\\widehat{\\kappa} \\boldsymbol{M}_{\\boldsymbol{Z}}\\right) \\boldsymbol{X}=\\left(\\begin{array}{c}\n\\boldsymbol{X}_{1} \\\\\n\\boldsymbol{X}_{2}-\\widehat{\\kappa} \\widehat{\\boldsymbol{U}}_{2}\n\\end{array}\\right)\n\\]\nwhere \\(\\widehat{\\boldsymbol{U}}_{2}=\\boldsymbol{M}_{\\boldsymbol{Z}} \\boldsymbol{X}_{2}\\) are the reduced-form residuals from the multivariate regression of the endogenous regressors \\(Y_{2}\\) on the instruments \\(Z\\). Expressing LIML using this IV formula is useful for variance estimation.\nThe LIML estimator has the same asymptotic distribution as 2SLS. However, they have quite different behaviors in finite samples. There is considerable evidence that the LIML estimator has reduced finite sample bias relative to 2 SLS when there are many instruments or the reduced form is weak. (We review these cases in the following sections.) However, on the other hand LIML has wider finite sample dispersion.\nWe now derive the expression (12.37). Use the normalization \\(\\gamma^{\\prime}=\\left[\\begin{array}{ll}1 & -\\beta_{2}^{\\prime}\\end{array}\\right]\\) to write (12.34) as\n\\[\n\\widehat{\\beta}_{2}=\\underset{\\beta_{2}}{\\operatorname{argmin}} \\frac{\\left(\\boldsymbol{Y}_{1}-\\boldsymbol{Y}_{2} \\beta_{2}\\right)^{\\prime} \\boldsymbol{M}_{1}\\left(\\boldsymbol{Y}_{1}-\\boldsymbol{Y}_{2} \\beta_{2}\\right)}{\\left(\\boldsymbol{Y}_{1}-\\boldsymbol{Y} \\beta_{2}\\right)^{\\prime} \\boldsymbol{M}_{\\boldsymbol{Z}}\\left(\\boldsymbol{Y}_{1}-\\boldsymbol{Y}_{2} \\beta_{2}\\right)} .\n\\]\nThe first-order-condition for minimization is \\(2 /\\left(\\boldsymbol{Y}_{1}-\\boldsymbol{Y}_{2} \\widehat{\\beta}_{2}\\right)^{\\prime} \\boldsymbol{M}_{\\boldsymbol{Z}}\\left(\\boldsymbol{Y}_{1}-\\boldsymbol{Y}_{2} \\widehat{\\beta}_{2}\\right)\\) times\n\\[\n\\begin{aligned}\n0 &=\\boldsymbol{Y}_{2}^{\\prime} \\boldsymbol{M}_{1}\\left(\\boldsymbol{Y}_{1}-\\boldsymbol{Y}_{2} \\widehat{\\beta}_{2}\\right)-\\frac{\\left(\\boldsymbol{Y}_{1}-\\boldsymbol{Y}_{2} \\widehat{\\beta}_{2}\\right)^{\\prime} \\boldsymbol{M}_{1}\\left(\\boldsymbol{Y}_{1}-\\boldsymbol{Y}_{2} \\widehat{\\beta}_{2}\\right)}{\\left(\\boldsymbol{Y}_{1}-\\boldsymbol{Y}_{2} \\widehat{\\beta}_{2}\\right)^{\\prime} \\boldsymbol{M}_{\\boldsymbol{Z}}\\left(\\boldsymbol{Y}_{1}-\\boldsymbol{Y}_{2} \\widehat{\\beta}_{2}\\right)} \\boldsymbol{X}_{2}^{\\prime} \\boldsymbol{M}_{\\boldsymbol{Z}}\\left(\\boldsymbol{Y}_{1}-\\boldsymbol{Y}_{2} \\widehat{\\beta}_{2}\\right) \\\\\n&=\\boldsymbol{Y}_{2}^{\\prime} \\boldsymbol{M}_{1}\\left(\\boldsymbol{Y}_{1}-\\boldsymbol{Y}_{2} \\widehat{\\beta}_{2}\\right)-\\widehat{\\kappa} \\boldsymbol{X}_{2}^{\\prime} \\boldsymbol{M}_{\\boldsymbol{Z}}\\left(\\boldsymbol{Y}_{1}-\\boldsymbol{Y}_{2} \\widehat{\\beta}_{2}\\right)\n\\end{aligned}\n\\]\nusing definition (12.36). Rewriting,\n\\[\n\\boldsymbol{Y}_{2}^{\\prime}\\left(\\boldsymbol{M}_{1}-\\widehat{\\kappa} \\boldsymbol{M}_{\\boldsymbol{Z}}\\right) \\boldsymbol{X}_{2} \\widehat{\\beta}_{2}=\\boldsymbol{X}_{2}^{\\prime}\\left(\\boldsymbol{M}_{1}-\\widehat{\\kappa} \\boldsymbol{M}_{\\boldsymbol{Z}}\\right) \\boldsymbol{Y}_{1} .\n\\]\nEquation (12.37) is the same as the two equation system\n\\[\n\\begin{aligned}\n\\boldsymbol{Z}_{1}^{\\prime} \\boldsymbol{Z}_{1} \\widehat{\\beta}_{1}+\\boldsymbol{Z}_{1}^{\\prime} \\boldsymbol{Y}_{2} \\widehat{\\beta}_{2} &=\\boldsymbol{Z}_{1}^{\\prime} \\boldsymbol{Y}_{1} \\\\\n\\boldsymbol{Y}_{2}^{\\prime} \\boldsymbol{Z}_{1} \\widehat{\\beta}_{1}+\\left(\\boldsymbol{Y}_{2}^{\\prime}\\left(\\boldsymbol{I}_{n}-\\widehat{\\kappa} \\boldsymbol{M}_{\\boldsymbol{Z}}\\right) \\boldsymbol{Y}_{2}\\right) \\widehat{\\beta}_{2} &=\\boldsymbol{Y}_{2}^{\\prime}\\left(\\boldsymbol{I}_{n}-\\widehat{\\kappa} \\boldsymbol{M}_{\\boldsymbol{Z}}\\right) \\boldsymbol{Y}_{1} .\n\\end{aligned}\n\\]\nThe first equation is (12.35). Using (12.35), the second is\n\\[\n\\boldsymbol{Y}_{2}^{\\prime} \\boldsymbol{Z}_{1}\\left(\\boldsymbol{Z}_{1}^{\\prime} \\boldsymbol{Z}_{1}\\right)^{-1} \\boldsymbol{Z}_{1}^{\\prime}\\left(\\boldsymbol{Y}_{1}-\\boldsymbol{Y}_{2} \\widehat{\\beta}_{2}\\right)+\\left(\\boldsymbol{Y}_{2}^{\\prime}\\left(\\boldsymbol{I}_{n}-\\widehat{\\kappa} \\boldsymbol{M}_{\\boldsymbol{Z}}\\right) \\boldsymbol{Y}_{2}\\right) \\widehat{\\beta}_{2}=\\boldsymbol{Y}_{2}^{\\prime}\\left(\\boldsymbol{I}_{n}-\\widehat{\\kappa} \\boldsymbol{M}_{\\boldsymbol{Z}}\\right) \\boldsymbol{Y}_{1}\n\\]\nwhich is (12.38) when rearranged. We have thus shown that (12.37) is equivalent to (12.35) and (12.38) and is thus a valid expression for the LIML estimator.\nReturning to the Card college proximity example we now present the LIML estimates of the equation with the two instruments (public, private). They are reported in the final column of Table 12.1. They are quite similar to the 2SLS estimates.\nThe LIML estimator may be calculated in Stata using the ivregress liml command.\n\n\n\n\n\n\nTheodore Anderson\n\n\n\n\nTheodore (Ted) Anderson (1918-2016) was a American statistician and econo-\n\n\nmetrician, who made fundamental contributions to multivariate statistical the-\n\n\nory. Important contributions include the Anderson-Darling distribution test, the\n\n\nAnderson-Rubin statistic, the method of reduced rank regression, and his most\n\n\nfamous econometrics contribution - the LIML estimator. He continued working\n\n\nthroughout his long life, even publishing theoretical work at the age of 97 !"
  },
  {
    "objectID": "chpt12-iv.html#split-sample-iv-and-jive",
    "href": "chpt12-iv.html#split-sample-iv-and-jive",
    "title": "12  Instrumental Variables",
    "section": "12.14 Split-Sample IV and JIVE",
    "text": "12.14 Split-Sample IV and JIVE\nThe ideal instrument for estimation of \\(\\beta\\) is \\(W=\\Gamma^{\\prime} Z\\). We can write the ideal IV estimator as\n\\[\n\\widehat{\\beta}_{\\text {ideal }}=\\left(\\sum_{i=1}^{n} W_{i} X_{i}^{\\prime}\\right)^{-1}\\left(\\sum_{i=1}^{n} W_{i} Y_{i}\\right) .\n\\]\nThis estimator is not feasible since \\(\\Gamma\\) is unknown. The 2SLS estimator replaces \\(\\Gamma\\) with the multivariate least squares estimator \\(\\widehat{\\Gamma}\\) and \\(W_{i}\\) with \\(\\widehat{W}_{i}=\\widehat{\\Gamma}^{\\prime} Z_{i}\\) leading to the following representation for 2SLS\n\\[\n\\widehat{\\beta}_{2 \\text { sls }}=\\left(\\sum_{i=1}^{n} \\widehat{W}_{i} X_{i}^{\\prime}\\right)^{-1}\\left(\\sum_{i=1}^{n} \\widehat{W}_{i} Y_{i}\\right) .\n\\]\nSince \\(\\widehat{\\Gamma}\\) is estimated on the full sample including observation \\(i\\) it is a function of the reduced form error \\(u\\) which is correlated with the structural error \\(e\\). It follows that \\(\\widehat{W}\\) and \\(e\\) are correlated, which means that \\(\\widehat{\\beta}_{2 s l s}\\) is biased for \\(\\beta\\). This correlation and bias disappears asymptotically but it can be important in applications.\nA possible solution to this problem is to replace \\(\\widehat{W}\\) with a predicted value which is uncorrelated with the error \\(e\\). One method is the split-sample IV (SSIV) estimator of Angrist and Krueger (1995). Divide the sample randomly into two independent halves \\(A\\) and \\(B\\). Use \\(A\\) to estimate the reduced form and \\(B\\) to estimate the structural coefficient. Specifically, use sample \\(A\\) to construct \\(\\widehat{\\Gamma}_{A}=\\left(\\boldsymbol{Z}_{A}^{\\prime} \\boldsymbol{Z}_{A}\\right)^{-1}\\left(\\boldsymbol{Z}_{A}^{\\prime} \\boldsymbol{X}_{A}\\right)\\). Combine this with sample \\(B\\) to create the predicted values \\(\\widehat{\\boldsymbol{W}}_{B}=Z_{B} \\widehat{\\Gamma}_{A}\\). The SSIV estimator is \\(\\widehat{\\beta}_{\\text {ssiv }}=\\) \\(\\left(\\widehat{\\boldsymbol{W}}_{B}^{\\prime} \\boldsymbol{X}_{B}\\right)^{-1}\\left(\\widehat{\\boldsymbol{W}}_{B}^{\\prime} \\boldsymbol{Y}_{B}\\right)\\). This has lower bias than \\(\\widehat{\\beta}_{2 \\text { sls. }}\\)\nA limitation of SSIV is that the results will be sensitive to the sample spliting. One split will produce one estimator; another split will produce a different estimator. Any specific split is arbitrary, so the estimator depends on the specific random sorting of the observations into the samples \\(A\\) and \\(B\\). A second limitation of SSIV is that it is unlikely to work well when the sample size \\(n\\) is small.\nA much better solution is obtained by a leave-one-out estimator for \\(\\Gamma\\). Specifically, let\n\\[\n\\widehat{\\Gamma}_{(-i)}=\\left(\\boldsymbol{Z}^{\\prime} \\boldsymbol{Z}-Z_{i} Z_{i}^{\\prime}\\right)^{-1}\\left(\\boldsymbol{Z}^{\\prime} \\boldsymbol{X}-Z_{i} X_{i}^{\\prime}\\right)\n\\]\nbe the least squares leave-one-out estimator of the reduced form matrix \\(\\Gamma\\), and let \\(\\widehat{W}_{i}=\\widehat{\\Gamma}_{(-i)}^{\\prime} Z_{i}\\) be the reduced form predicted values. Using \\(\\widehat{W}_{i}=\\widehat{\\Gamma}_{(-i)}^{\\prime} Z_{i}\\) as an instrument we obtain the estimator\n\\[\n\\widehat{\\beta}_{\\mathrm{jive1}}=\\left(\\sum_{i=1}^{n} \\widehat{W}_{i} X_{i}^{\\prime}\\right)^{-1}\\left(\\sum_{i=1}^{n} \\widehat{W}_{i} Y_{i}\\right)=\\left(\\sum_{i=1}^{n} \\widehat{\\Gamma}_{(-i)}^{\\prime} Z_{i} X_{i}^{\\prime}\\right)^{-1}\\left(\\sum_{i=1}^{n} \\widehat{\\Gamma}_{(-i)}^{\\prime} Z_{i} Y_{i}\\right) .\n\\]\nThis was called the jackknife instrumental variables (JIVE1) estimator by Angrist, Imbens, and Krueger (1999). It first appeared in Phillips and Hale (1977).\nAngrist, Imbens, and Krueger (1999) pointed out that a somewhat simpler adjustment also removes the correlation and bias. Define the estimator and predicted value\n\\[\n\\begin{aligned}\n\\widetilde{\\Gamma}_{(-i)} &=\\left(\\boldsymbol{Z}^{\\prime} \\boldsymbol{Z}\\right)^{-1}\\left(\\boldsymbol{Z}^{\\prime} \\boldsymbol{X}-Z_{i} X_{i}^{\\prime}\\right) \\\\\n\\widetilde{W}_{i} &=\\widetilde{\\Gamma}_{(-i)}^{\\prime} Z_{i}\n\\end{aligned}\n\\]\nwhich only adjusts the \\(\\boldsymbol{Z}^{\\prime} \\boldsymbol{X}\\) component. Their JIVE2 estimator is\n\\[\n\\widehat{\\beta}_{\\mathrm{jive} 2}=\\left(\\sum_{i=1}^{n} \\widetilde{W}_{i} X_{i}^{\\prime}\\right)^{-1}\\left(\\sum_{i=1}^{n} \\widetilde{W}_{i} Y_{i}\\right)=\\left(\\sum_{i=1}^{n} \\widetilde{\\Gamma}_{(-i)}^{\\prime} Z_{i} X_{i}^{\\prime}\\right)^{-1}\\left(\\sum_{i=1}^{n} \\widetilde{\\Gamma}_{(-i)}^{\\prime} Z_{i} Y_{i}\\right) .\n\\]\nUsing the formula for leave-one-out estimators (Theorem 3.7), the JIVE1 and JIVE2 estimators use two linear operations: the first to create the predicted values \\(\\widehat{W}_{i}\\) or \\(\\widetilde{W}_{i}\\), and the second to calculate the IV estimator. Thus the estimators do not require significantly more computation than 2SLS.\nAn asymptotic distribution theory for JIVE1 and JIVE2 was developed by Chao, Swanson, Hausman, Newey, and Woutersen (2012).\nThe JIVE1 and JIVE2 estimators may be calculated in Stata using the \\(j\\) ive command. It is not a part of the standard package but can be easily added."
  },
  {
    "objectID": "chpt12-iv.html#consistency-of-2sls",
    "href": "chpt12-iv.html#consistency-of-2sls",
    "title": "12  Instrumental Variables",
    "section": "12.15 Consistency of 2SLS",
    "text": "12.15 Consistency of 2SLS\nWe now demonstrate the consistency of the 2SLS estimator for the structural parameter. The following is a set of regularity conditions.\nAssumption $12.1\n\nThe variables \\(\\left(Y_{1 i}, X_{i}, Z_{i}\\right), i=1, \\ldots, n\\), are independent and identically distributed.\n\\(\\mathbb{E}\\left[Y_{1}^{2}\\right]<\\infty\\).\n\\(\\mathbb{E}\\|X\\|^{2}<\\infty\\).\n\\(\\mathbb{E}\\|Z\\|^{2}<\\infty\\)\n\\(\\mathbb{E}\\left[Z Z^{\\prime}\\right]\\) is positive definite.\n\\(\\mathbb{E}\\left[Z X^{\\prime}\\right]\\) has full rank \\(k\\).\n\\(\\mathbb{E}[Z e]=0\\).\n\nAssumptions 12.1.2-4 state that all variables have finite variances. Assumption 12.1.5 states that the instrument vector has an invertible design matrix, which is identical to the core assumption about regressors in the linear regression model. This excludes linearly redundant instruments. Assumptions 12.1.6 and 12.1.7 are the key identification conditions for instrumental variables. Assumption 12.1.6 states that the instruments and regressors have a full-rank cross-moment matrix. This is often called the relevance condition. Assumption 12.1.7 states that the instrumental variables and structural error are uncorrelated. Assumptions 12.1.5-7 are identical to Definition 12.1.\nTheorem 12.1 Under Assumption 12.1, \\(\\widehat{\\beta}_{2 s l s} \\underset{p}{\\longrightarrow} \\beta\\) as \\(n \\rightarrow \\infty\\).\nThe proof of the theorem is provided below.\nThis theorem shows that the 2SLS estimator is consistent for the structural coefficient \\(\\beta\\) under similar moment conditions as the least squares estimator. The key differences are the instrumental variables assumption \\(\\mathbb{E}[Z e]=0\\) and the relevance condition \\(\\operatorname{rank}\\left(\\mathbb{E}\\left[Z X^{\\prime}\\right]\\right)=k\\).\nThe result includes the IV estimator (when \\(\\ell=k\\) ) as a special case.\nThe proof of this consistency result is similar to that for least squares. Take the structural equation \\(\\boldsymbol{Y}=\\boldsymbol{X} \\beta+\\boldsymbol{e}\\) in matrix format and substitute it into the expression for the estimator. We obtain\n\\[\n\\begin{aligned}\n\\widehat{\\beta}_{2 \\text { sls }} &=\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{Z}\\left(\\boldsymbol{Z}^{\\prime} \\boldsymbol{Z}\\right)^{-1} \\boldsymbol{Z}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\boldsymbol{X}^{\\prime} \\boldsymbol{Z}\\left(\\boldsymbol{Z}^{\\prime} \\boldsymbol{Z}\\right)^{-1} \\boldsymbol{Z}^{\\prime}(\\boldsymbol{X} \\beta+\\boldsymbol{e}) \\\\\n&=\\beta+\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{Z}\\left(\\boldsymbol{Z}^{\\prime} \\boldsymbol{Z}\\right)^{-1} \\boldsymbol{Z}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\boldsymbol{X}^{\\prime} \\boldsymbol{Z}\\left(\\boldsymbol{Z}^{\\prime} \\boldsymbol{Z}\\right)^{-1} \\boldsymbol{Z}^{\\prime} \\boldsymbol{e} .\n\\end{aligned}\n\\]\nThis separates out the stochastic component. Re-writing and applying the WLLN and CMT\n\\[\n\\begin{aligned}\n\\widehat{\\beta}_{2 s l s}-\\beta &=\\left(\\left(\\frac{1}{n} \\boldsymbol{X}^{\\prime} \\boldsymbol{Z}\\right)\\left(\\frac{1}{n} \\boldsymbol{Z}^{\\prime} \\boldsymbol{Z}\\right)^{-1}\\left(\\frac{1}{n} \\boldsymbol{Z}^{\\prime} \\boldsymbol{X}\\right)\\right)^{-1} \\\\\n& \\times\\left(\\frac{1}{n} \\boldsymbol{X}^{\\prime} \\boldsymbol{Z}\\right)\\left(\\frac{1}{n} \\boldsymbol{Z}^{\\prime} \\boldsymbol{Z}\\right)^{-1}\\left(\\frac{1}{n} \\boldsymbol{Z}^{\\prime} \\boldsymbol{e}\\right) \\\\\n\\underset{p}{\\rightarrow}\\left(\\boldsymbol{Q}_{X Z} \\boldsymbol{Q}_{Z Z}^{-1} \\boldsymbol{Q}_{Z X}\\right)^{-1} \\boldsymbol{Q}_{X Z} \\boldsymbol{Q}_{Z Z}^{-1} \\mathbb{E}[Z e]=0\n\\end{aligned}\n\\]\nwhere\n\\[\n\\begin{aligned}\n&\\boldsymbol{Q}_{X Z}=\\mathbb{E}\\left[X Z^{\\prime}\\right] \\\\\n&\\boldsymbol{Q}_{Z Z}=\\mathbb{E}\\left[Z Z^{\\prime}\\right] \\\\\n&\\boldsymbol{Q}_{Z X}=\\mathbb{E}\\left[Z X^{\\prime}\\right] .\n\\end{aligned}\n\\]\nThe WLLN holds under Assumptions 12.1.1 and 12.1.2-4. The continuous mapping theorem applies if the matrices \\(\\boldsymbol{Q}_{Z Z}\\) and \\(\\boldsymbol{Q}_{X Z} \\boldsymbol{Q}_{Z Z}^{-1} \\boldsymbol{Q}_{Z X}\\) are invertible, which hold under Assumptions 12.1.5 and 12.1.6. The final equality uses Assumption 12.1.7."
  },
  {
    "objectID": "chpt12-iv.html#asymptotic-distribution-of-2sls",
    "href": "chpt12-iv.html#asymptotic-distribution-of-2sls",
    "title": "12  Instrumental Variables",
    "section": "12.16 Asymptotic Distribution of 2SLS",
    "text": "12.16 Asymptotic Distribution of 2SLS\nWe now show that the 2SLS estimator satisfies a central limit theorem. We first state a set of sufficient regularity conditions. Assumption 12.2 In addition to Assumption 12.1,\n\n\\(\\mathbb{E}\\left[Y_{1}^{4}\\right]<\\infty\\).\n\\(\\mathbb{E}\\|X\\|^{4}<\\infty\\).\n\\(\\mathbb{E}\\|Z\\|^{4}<\\infty\\).\n\\(\\Omega=\\mathbb{E}\\left[Z Z^{\\prime} e^{2}\\right]\\) is positive definite.\n\nAssumption \\(12.2\\) strengthens Assumption \\(12.1\\) by requiring that the dependent variable and instruments have finite fourth moments. This is used to establish the central limit theorem.\nTheorem 12.2 Under Assumption 12.2, as \\(n \\rightarrow \\infty\\).\n\\[\n\\sqrt{n}\\left(\\widehat{\\beta}_{2 \\text { sls }}-\\beta\\right) \\underset{d}{\\longrightarrow} \\mathrm{N}\\left(0, V_{\\beta}\\right)\n\\]\nwhere\n\\[\n\\boldsymbol{V}_{\\beta}=\\left(\\boldsymbol{Q}_{X Z} \\boldsymbol{Q}_{Z Z}^{-1} \\boldsymbol{Q}_{Z X}\\right)^{-1}\\left(\\boldsymbol{Q}_{X Z} \\boldsymbol{Q}_{Z Z}^{-1} \\Omega \\boldsymbol{Q}_{Z Z}^{-1} \\boldsymbol{Q}_{Z X}\\right)\\left(\\boldsymbol{Q}_{X Z} \\boldsymbol{Q}_{Z Z}^{-1} \\boldsymbol{Q}_{Z X}\\right)^{-1}\n\\]\nThis shows that the 2 SLS estimator converges at a \\(\\sqrt{n}\\) rate to a normal random vector. It shows as well the form of the covariance matrix. The latter takes a substantially more complicated form than the least squares estimator.\nAs in the case of least squares estimation the asymptotic variance simplifies under a conditional homoskedasticity condition. For 2SLS the simplification occurs when \\(\\mathbb{E}\\left[e^{2} \\mid Z\\right]=\\sigma^{2}\\). This holds when \\(Z\\) and \\(e\\) are independent. It may be reasonable in some contexts to conceive that the error \\(e\\) is independent of the excluded instruments \\(Z_{2}\\), since by assumption the impact of \\(Z_{2}\\) on \\(Y\\) is only through \\(X\\), but there is no reason to expect \\(e\\) to be independent of the included exogenous variables \\(X_{1}\\). Hence heteroskedasticity should be equally expected in 2SLS and least squares regression. Nevertheless, under homoskedasticity we have the simplifications \\(\\Omega=\\boldsymbol{Q}_{Z Z} \\sigma^{2}\\) and \\(\\boldsymbol{V}_{\\beta}=\\boldsymbol{V}_{\\beta}^{0} \\stackrel{\\text { def }}{=}\\left(\\boldsymbol{Q}_{X Z} \\boldsymbol{Q}_{Z Z}^{-1} \\boldsymbol{Q}_{Z X}\\right)^{-1} \\sigma^{2}\\).\nThe derivation of the asymptotic distribution builds on the proof of consistency. Using equation (12.39) we have\n\\[\n\\sqrt{n}\\left(\\widehat{\\beta}_{2 \\mathrm{sls}}-\\beta\\right)=\\left(\\left(\\frac{1}{n} \\boldsymbol{X}^{\\prime} \\boldsymbol{Z}\\right)\\left(\\frac{1}{n} \\boldsymbol{Z}^{\\prime} \\boldsymbol{Z}\\right)^{-1}\\left(\\frac{1}{n} \\boldsymbol{Z}^{\\prime} \\boldsymbol{X}\\right)\\right)^{-1}\\left(\\frac{1}{n} \\boldsymbol{X}^{\\prime} \\boldsymbol{Z}\\right)\\left(\\frac{1}{n} \\boldsymbol{Z}^{\\prime} \\boldsymbol{Z}\\right)^{-1}\\left(\\frac{1}{\\sqrt{n}} \\boldsymbol{Z}^{\\prime} \\boldsymbol{e}\\right) \\text {. }\n\\]\nWe apply the WLLN and CMT for the moment matrices involving \\(X\\) and \\(\\boldsymbol{Z}\\) the same as in the proof of consistency. In addition, by the CLT for i.i.d. observations\n\\[\n\\frac{1}{\\sqrt{n}} \\boldsymbol{Z}^{\\prime} \\boldsymbol{e}=\\frac{1}{\\sqrt{n}} \\sum_{i=1}^{n} Z_{i} e_{i} \\underset{d}{\\longrightarrow} \\mathrm{N}(0, \\Omega)\n\\]\nbecause the vector \\(Z_{i} e_{i}\\) is i.i.d. and mean zero under Assumptions 12.1.1 and 12.1.7, and has a finite second moment as we verify below. We obtain\n\\[\n\\begin{aligned}\n\\sqrt{n}\\left(\\widehat{\\beta}_{2 \\text { sls }}-\\beta\\right) &=\\left(\\left(\\frac{1}{n} \\boldsymbol{X}^{\\prime} \\boldsymbol{Z}\\right)\\left(\\frac{1}{n} \\boldsymbol{Z}^{\\prime} \\boldsymbol{Z}\\right)^{-1}\\left(\\frac{1}{n} \\boldsymbol{Z}^{\\prime} \\boldsymbol{X}\\right)\\right)^{-1}\\left(\\frac{1}{n} \\boldsymbol{X}^{\\prime} \\boldsymbol{Z}\\right)\\left(\\frac{1}{n} \\boldsymbol{Z}^{\\prime} \\boldsymbol{Z}\\right)^{-1}\\left(\\frac{1}{\\sqrt{n}} \\boldsymbol{Z}^{\\prime} \\boldsymbol{e}\\right) \\\\\n& \\underset{d}{\\rightarrow}\\left(\\boldsymbol{Q}_{X Z} \\boldsymbol{Q}_{Z Z}^{-1} \\boldsymbol{Q}_{Z X}\\right)^{-1} \\boldsymbol{Q}_{X Z} \\boldsymbol{Q}_{Z Z}^{-1} \\mathrm{~N}(0, \\Omega)=\\mathrm{N}\\left(0, \\boldsymbol{V}_{\\beta}\\right)\n\\end{aligned}\n\\]\nas stated.\nTo complete the proof we demonstrate that \\(Z e\\) has a finite second moment under Assumption 12.2. To see this, note that by Minkowski’s inequality (B.34)\n\\[\n\\left(\\mathbb{E}\\left[e^{4}\\right]\\right)^{1 / 4}=\\left(\\mathbb{E}\\left[\\left(Y_{1}-X^{\\prime} \\beta\\right)^{4}\\right]\\right)^{1 / 4} \\leq\\left(\\mathbb{E}\\left[Y_{1}^{4}\\right]\\right)^{1 / 4}+\\|\\beta\\|\\left(\\mathbb{E}\\|X\\|^{4}\\right)^{1 / 4}<\\infty\n\\]\nunder Assumptions 12.2.1 and 12.2.2. Then by the Cauchy-Schwarz inequality (B.32)\n\\[\n\\mathbb{E}\\|Z e\\|^{2} \\leq\\left(\\mathbb{E}\\|Z\\|^{4}\\right)^{1 / 2}\\left(\\mathbb{E}\\left[e^{4}\\right]\\right)^{1 / 2}<\\infty\n\\]\nusing Assumptions 12.2.3."
  },
  {
    "objectID": "chpt12-iv.html#determinants-of-2-sls-variance",
    "href": "chpt12-iv.html#determinants-of-2-sls-variance",
    "title": "12  Instrumental Variables",
    "section": "12.17 Determinants of 2 SLS Variance",
    "text": "12.17 Determinants of 2 SLS Variance\nIt is instructive to examine the asymptotic variance of the 2SLS estimator to understand the factors which determine the precision (or lack thereof) of the estimator. As in the least squares case it is more transparent to examine the variance under the assumption of homoskedasticity. In this case the asymptotic variance takes the form\n\\[\n\\begin{aligned}\n\\boldsymbol{V}_{\\beta}^{0} &=\\left(\\boldsymbol{Q}_{X Z} \\boldsymbol{Q}_{Z Z}^{-1} \\boldsymbol{Q}_{Z X}\\right)^{-1} \\sigma^{2} \\\\\n&=\\left(\\mathbb{E}\\left[X Z^{\\prime}\\right]\\left(\\mathbb{E}\\left[Z Z^{\\prime}\\right]\\right)^{-1} \\mathbb{E}\\left[Z X^{\\prime}\\right]\\right)^{-1} \\mathbb{E}\\left[e^{2}\\right] .\n\\end{aligned}\n\\]\nAs in the least squares case we can see that the variance of \\(\\widehat{\\beta}_{2 \\text { sls }}\\) is increasing in the variance of the error \\(e\\) and decreasing in the variance of \\(X\\). What is different is that the variance is decreasing in the (matrixvalued) correlation between \\(X\\) and \\(Z\\).\nIt is also useful to observe that the variance expression is not affected by the variance structure of \\(Z\\). Indeed, \\(\\boldsymbol{V}_{\\beta}^{0}\\) is invariant to rotations of \\(Z\\) (if you replace \\(Z\\) with \\(\\boldsymbol{C Z}\\) for invertible \\(\\boldsymbol{C}\\) the expression does not change). This means that the variance expression is not affected by the scaling of \\(Z\\) and is not directly affected by correlation among the \\(Z\\).\nWe can also use this expression to examine the impact of increasing the instrument set. Suppose we partition \\(Z=\\left(Z_{a}, Z_{b}\\right)\\) where \\(\\operatorname{dim}\\left(Z_{a}\\right) \\geq k\\) so we can construct a 2SLS estimator using \\(Z_{a}\\) alone. Let \\(\\widehat{\\beta}_{a}\\) and \\(\\widehat{\\beta}\\) denote the 2SLS estimators constructed using the instrument sets \\(Z_{a}\\) and \\(\\left(Z_{a}, Z_{b}\\right)\\), respectively. Without loss of generality we can assume that \\(Z_{a}\\) and \\(Z_{b}\\) are uncorrelated (if not, replace \\(Z_{b}\\) with the projection error after projecting onto \\(Z_{a}\\) ). In this case both \\(\\mathbb{E}\\left[Z Z^{\\prime}\\right]\\) and \\(\\left(\\mathbb{E}\\left[Z Z^{\\prime}\\right]\\right)^{-1}\\) are block diagonal so\n\\[\n\\begin{aligned}\n\\operatorname{avar}[\\widehat{\\beta}] &=\\left(\\mathbb{E}\\left[X Z^{\\prime}\\right]\\left(\\mathbb{E}\\left[Z Z^{\\prime}\\right]\\right)^{-1} \\mathbb{E}\\left[Z X^{\\prime}\\right]\\right)^{-1} \\sigma^{2} \\\\\n&=\\left(\\mathbb{E}\\left[X Z_{a}^{\\prime}\\right]\\left(\\mathbb{E}\\left[Z_{a} Z_{a}^{\\prime}\\right]\\right)^{-1} \\mathbb{E}\\left[Z_{a} X^{\\prime}\\right]+\\mathbb{E}\\left[X Z_{b}^{\\prime}\\right]\\left(\\mathbb{E}\\left[Z_{b} Z_{b}^{\\prime}\\right]\\right)^{-1} \\mathbb{E}\\left[Z_{b} X^{\\prime}\\right]\\right)^{-1} \\sigma^{2} \\\\\n& \\leq\\left(\\mathbb{E}\\left[X Z_{a}^{\\prime}\\right]\\left(\\mathbb{E}\\left[Z_{a} Z_{a}^{\\prime}\\right]\\right)^{-1} \\mathbb{E}\\left[Z_{a} X^{\\prime}\\right]\\right)^{-1} \\sigma^{2} \\\\\n&=\\operatorname{avar}\\left[\\widehat{\\beta}_{a}\\right]\n\\end{aligned}\n\\]\nwith strict inequality if \\(\\mathbb{E}\\left[X Z_{b}^{\\prime}\\right] \\neq 0\\). Thus the 2SLS estimator with the full instrument set has a smaller asymptotic variance than the estimator with the smaller instrument set.\nWhat we have shown is that the asymptotic variance of the 2SLS estimator is decreasing as the number of instruments increases. From the viewpoint of asymptotic efficiency this means that it is better to use more instruments (when they are available and are all known to be valid instruments).\nUnfortunately there is a catch. It turns out that the finite sample bias of the 2SLS estimator (which cannot be calculated exactly but can be approximated using asymptotic expansions) is generically increasing linearily as the number of instruments increases. We will see some calculations illustrating this phenomenon in Section 12.37. Thus the choice of instruments in practice induces a trade-off between bias and variance."
  },
  {
    "objectID": "chpt12-iv.html#covariance-matrix-estimation",
    "href": "chpt12-iv.html#covariance-matrix-estimation",
    "title": "12  Instrumental Variables",
    "section": "12.18 Covariance Matrix Estimation",
    "text": "12.18 Covariance Matrix Estimation\nEstimation of the asymptotic covariance matrix \\(\\boldsymbol{V}_{\\beta}\\) is done using similar techniques as for least squares estimation. The estimator is constructed by replacing the population moment matrices by sample counterparts. Thus\n\\[\n\\widehat{\\boldsymbol{V}}_{\\beta}=\\left(\\widehat{\\mathbf{Q}}_{X Z} \\widehat{\\mathbf{Q}}_{Z Z}^{-1} \\widehat{\\mathbf{Q}}_{Z X}\\right)^{-1}\\left(\\widehat{\\mathbf{Q}}_{X Z} \\widehat{\\mathbf{Q}}_{Z Z}^{-1} \\widehat{\\Omega} \\widehat{\\mathbf{Q}}_{Z Z}^{-1} \\widehat{\\mathbf{Q}}_{Z X}\\right)\\left(\\widehat{\\mathbf{Q}}_{X Z} \\widehat{\\mathbf{Q}}_{Z Z}^{-1} \\widehat{\\mathbf{Q}}_{Z X}\\right)^{-1}\n\\]\nwhere\n\\[\n\\begin{aligned}\n\\widehat{\\boldsymbol{Q}}_{Z Z} &=\\frac{1}{n} \\sum_{i=1}^{n} Z_{i} Z_{i}^{\\prime}=\\frac{1}{n} \\boldsymbol{Z}^{\\prime} \\boldsymbol{Z} \\\\\n\\widehat{\\boldsymbol{Q}}_{X Z} &=\\frac{1}{n} \\sum_{i=1}^{n} X_{i} Z_{i}^{\\prime}=\\frac{1}{n} \\boldsymbol{X}^{\\prime} \\boldsymbol{Z} \\\\\n\\widehat{\\Omega} &=\\frac{1}{n} \\sum_{i=1}^{n} Z_{i} Z_{i}^{\\prime} \\widehat{e}_{i}^{2} \\\\\n\\widehat{e}_{i} &=Y_{i}-X_{i}^{\\prime} \\widehat{\\beta}_{2 s l s}\n\\end{aligned}\n\\]\nThe homoskedastic covariance matrix can be estimated by\n\\[\n\\begin{aligned}\n\\widehat{\\boldsymbol{V}}_{\\beta}^{0} &=\\left(\\widehat{\\boldsymbol{Q}}_{X Z} \\widehat{\\boldsymbol{Q}}_{Z Z}^{-1} \\widehat{\\boldsymbol{Q}}_{Z X}\\right)^{-1} \\widehat{\\sigma}^{2} \\\\\n\\widehat{\\sigma}^{2} &=\\frac{1}{n} \\sum_{i=1}^{n} \\widehat{e}_{i}^{2}\n\\end{aligned}\n\\]\nStandard errors for the coefficients are obtained as the square roots of the diagonal elements of \\(n^{-1} \\widehat{\\boldsymbol{V}}_{\\beta}\\). Confidence intervals, t-tests, and Wald tests may all be constructed from the coefficient and covariance matrix estimates exactly as for least squares regression.\nIn Stata the ivregress command by default calculates the covariance matrix estimator using the homoskedastic covariance matrix. To obtain covariance matrix estimation and standard errors with the robust estimator \\(\\widehat{\\boldsymbol{V}}_{\\beta}\\), use the “,r” option.\nTheorem 12.3 Under Assumption 12.2, as \\(n \\rightarrow \\infty, \\widehat{\\boldsymbol{V}}_{\\beta}^{0}{\\underset{p}{\\longrightarrow}}^{\\boldsymbol{V}_{\\beta}^{0}}\\) and \\(\\widehat{\\boldsymbol{V}}_{\\beta} \\vec{p}^{\\boldsymbol{V}_{\\beta}}\\) To prove Theorem \\(12.3\\) the key is to show \\(\\widehat{\\Omega} \\vec{p} \\Omega\\) as the other convergence results were established in the proof of consistency. We defer this to Exercise 12.6.\nIt is important that the covariance matrix be constructed using the correct residual formula \\(\\widehat{e}_{i}=Y_{i}-\\) \\(X_{i}^{\\prime} \\widehat{\\beta}_{2 \\text { sls }}\\). This is different than what would be obtained if the “two-stage” computation method were used. To see this let’s walk through the two-stage method. First, we estimate the reduced form \\(X_{i}=\\widehat{\\Gamma}^{\\prime} Z_{i}+\\widehat{u}_{i}\\) to obtain the predicted values \\(\\widehat{X}_{i}=\\widehat{\\Gamma}^{\\prime} Z_{i}\\). Second, we regress \\(Y\\) on \\(\\widehat{X}\\) to obtain the 2SLS estimator \\(\\widehat{\\beta}_{2 \\text { sls }}\\). This latter regression takes the form\n\\[\nY_{i}=\\widehat{X}_{i}^{\\prime} \\widehat{\\beta}_{2 \\mathrm{sls}}+\\widehat{v}_{i}\n\\]\nwhere \\(\\widehat{v}_{i}\\) are least squares residuals. The covariance matrix (and standard errors) reported by this regression are constructed using the residual \\(\\widehat{v}_{i}\\). For example, the homoskedastic formula is\n\\[\n\\begin{aligned}\n\\widehat{\\boldsymbol{V}}_{\\beta} &=\\left(\\frac{1}{n} \\widehat{\\boldsymbol{X}}^{\\prime} \\widehat{\\boldsymbol{X}}\\right)^{-1} \\widehat{\\sigma}_{v}^{2}=\\left(\\widehat{\\boldsymbol{Q}}_{X Z} \\widehat{\\boldsymbol{Q}}_{Z Z}^{-1} \\widehat{\\mathbf{Q}}_{Z X}\\right)^{-1} \\widehat{\\sigma}_{v}^{2} \\\\\n\\widehat{\\sigma}_{v}^{2} &=\\frac{1}{n} \\sum_{i=1}^{n} \\widehat{v}_{i}^{2}\n\\end{aligned}\n\\]\nwhich is proportional to the variance estimator \\(\\widehat{\\sigma}_{v}^{2}\\) rather than \\(\\widehat{\\sigma}^{2}\\). This is important because the residual \\(\\widehat{v}\\) differs from \\(\\widehat{e}\\). We can see this because the regression (12.41) uses the regressor \\(\\widehat{X}\\) rather than \\(X\\). Indeed, we calculate that\n\\[\n\\widehat{v}_{i}=Y_{i}-X_{i}^{\\prime} \\widehat{\\beta}_{2 \\mathrm{sls}}+\\left(X_{i}-\\widehat{X}_{i}\\right)^{\\prime} \\widehat{\\beta}_{2 \\mathrm{sls}}=\\widehat{e}_{i}+\\widehat{u}_{i}^{\\prime} \\widehat{\\beta}_{2 \\mathrm{sls}} \\neq \\widehat{e}_{i} \\text {. }\n\\]\nThis means that standard errors reported by the regression (12.41) will be incorrect.\nThis problem is avoided if the 2SLS estimator is constructed directly and the standard errors calculated with the correct formula rather than taking the “two-step” shortcut."
  },
  {
    "objectID": "chpt12-iv.html#liml-asymptotic-distribution",
    "href": "chpt12-iv.html#liml-asymptotic-distribution",
    "title": "12  Instrumental Variables",
    "section": "12.19 LIML Asymptotic Distribution",
    "text": "12.19 LIML Asymptotic Distribution\nIn this section we show that the LIML estimator is asymptotically equivalent to the 2SLS estimator. We recommend, however, a different covariance matrix estimator based on the IV representation.\nWe start by deriving the asymptotic distribution. Recall that the LIML estimator has several representations including\n\\[\n\\widehat{\\beta}_{\\mathrm{liml}}=\\left(\\boldsymbol{X}^{\\prime}\\left(\\boldsymbol{I}_{n}-\\widehat{\\kappa} \\boldsymbol{M}_{\\boldsymbol{Z}}\\right) \\boldsymbol{X}\\right)^{-1}\\left(\\boldsymbol{X}^{\\prime}\\left(\\boldsymbol{I}_{n}-\\widehat{\\kappa} \\boldsymbol{M}_{\\boldsymbol{Z}}\\right) \\boldsymbol{Y}_{1}\\right)\n\\]\nwhere\n\\[\n\\widehat{\\boldsymbol{\\kappa}}=\\min _{\\gamma} \\frac{\\gamma^{\\prime} \\overrightarrow{\\boldsymbol{Y}}^{\\prime} \\boldsymbol{M}_{1} \\overrightarrow{\\boldsymbol{Y}} \\gamma}{\\gamma^{\\prime} \\overrightarrow{\\boldsymbol{Y}}^{\\prime} \\boldsymbol{M}_{\\boldsymbol{Z}} \\overrightarrow{\\boldsymbol{Y}} \\gamma}\n\\]\nand \\(\\gamma=\\left(1,-\\beta_{2}^{\\prime}\\right)^{\\prime}\\). For the distribution theory it is useful to rewrite the slope coefficient as\n\\[\n\\widehat{\\beta}_{\\mathrm{liml}}=\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{P}_{Z} \\boldsymbol{X}-\\widehat{\\mu} \\boldsymbol{X}^{\\prime} \\boldsymbol{M}_{Z} \\boldsymbol{X}\\right)^{-1}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{P}_{\\boldsymbol{Z}} \\boldsymbol{Y}_{1}-\\widehat{\\mu} \\boldsymbol{X}^{\\prime} \\boldsymbol{M}_{Z} \\boldsymbol{Y}_{1}\\right)\n\\]\nwhere\n\\[\n\\widehat{\\mu}=\\widehat{\\kappa}-1=\\min _{\\gamma} \\frac{\\gamma^{\\prime} \\overrightarrow{\\boldsymbol{Y}}^{\\prime} \\boldsymbol{M}_{1} \\boldsymbol{Z}_{2}\\left(\\boldsymbol{Z}_{2}^{\\prime} \\boldsymbol{M}_{1} \\boldsymbol{Z}_{2}\\right)^{-1} \\boldsymbol{Z}_{2}^{\\prime} \\boldsymbol{M}_{1} \\overrightarrow{\\boldsymbol{Y}} \\gamma}{\\gamma^{\\prime} \\overrightarrow{\\boldsymbol{Y}}^{\\prime} \\boldsymbol{M}_{\\boldsymbol{Z}} \\overrightarrow{\\boldsymbol{Y}} \\gamma} \\text {. }\n\\]\nThis second equality holds because the span of \\(\\boldsymbol{Z}=\\left[\\boldsymbol{Z}_{1}, \\boldsymbol{Z}_{2}\\right]\\) equals the span of \\(\\left[\\boldsymbol{Z}_{1}, \\boldsymbol{M}_{1} \\boldsymbol{Z}_{2}\\right]\\). This implies\n\\[\n\\boldsymbol{P}_{\\boldsymbol{Z}}=\\boldsymbol{Z}\\left(\\boldsymbol{Z}^{\\prime} \\boldsymbol{Z}\\right)^{-1} \\boldsymbol{Z}^{\\prime}=\\boldsymbol{Z}_{1}\\left(\\boldsymbol{Z}_{1}^{\\prime} \\boldsymbol{Z}_{1}\\right)^{-1} \\boldsymbol{Z}_{1}^{\\prime}+\\boldsymbol{M}_{1} \\boldsymbol{Z}_{2}\\left(\\boldsymbol{Z}_{2}^{\\prime} \\boldsymbol{M}_{1} \\boldsymbol{Z}_{2}\\right)^{-1} \\boldsymbol{Z}_{2}^{\\prime} \\boldsymbol{M}_{1}\n\\]\nWe now show that \\(n \\widehat{\\mu}=O_{p}(1)\\). The reduced form (12.33) implies that\n\\[\n\\boldsymbol{Y}=\\boldsymbol{Z}_{1} \\Pi_{1}+\\boldsymbol{Z}_{2} \\Pi_{2}+\\boldsymbol{e} .\n\\]\nIt will be important to note that\n\\[\n\\Pi_{2}=\\left[\\lambda_{2}, \\Gamma_{22}\\right]=\\left[\\Gamma_{22} \\beta_{2}, \\Gamma_{22}\\right]\n\\]\nusing (12.15). It follows that \\(\\Pi_{2} \\gamma=0\\). Note \\(\\boldsymbol{U} \\gamma=\\boldsymbol{e}\\). Then \\(\\boldsymbol{M}_{\\boldsymbol{Z}} \\boldsymbol{Y} \\gamma=\\boldsymbol{M}_{\\boldsymbol{Z}} \\boldsymbol{e}\\) and \\(\\boldsymbol{M}_{1} \\boldsymbol{Y} \\gamma=\\boldsymbol{M}_{1} \\boldsymbol{e}\\). Hence\n\\[\n\\begin{aligned}\nn \\widehat{\\mu} &=\\min _{\\gamma} \\frac{\\gamma^{\\prime} \\overrightarrow{\\boldsymbol{Y}}^{\\prime} \\boldsymbol{M}_{1} \\boldsymbol{Z}_{2}\\left(\\boldsymbol{Z}_{2}^{\\prime} \\boldsymbol{M}_{1} \\boldsymbol{Z}_{2}\\right)^{-1} \\boldsymbol{Z}_{2}^{\\prime} \\boldsymbol{M}_{1} \\overrightarrow{\\boldsymbol{Y}} \\gamma}{\\gamma^{\\prime} \\frac{1}{n} \\overrightarrow{\\boldsymbol{Y}}^{\\prime} \\boldsymbol{M}_{\\boldsymbol{Z}} \\overrightarrow{\\boldsymbol{Y}} \\gamma} \\\\\n& \\leq \\frac{\\left(\\frac{1}{\\sqrt{n}} \\boldsymbol{e}^{\\prime} \\boldsymbol{M}_{1} \\boldsymbol{Z}_{2}\\right)\\left(\\frac{1}{n} \\boldsymbol{Z}_{2}^{\\prime} \\boldsymbol{M}_{1} \\boldsymbol{Z}_{2}\\right)^{-1}\\left(\\frac{1}{\\sqrt{n}} \\boldsymbol{Z}_{2}^{\\prime} \\boldsymbol{M}_{1} \\boldsymbol{e}\\right)}{\\frac{1}{n} \\boldsymbol{e}^{\\prime} \\boldsymbol{M}_{\\boldsymbol{Z}} \\boldsymbol{e}} \\\\\n&=O_{p}(1) .\n\\end{aligned}\n\\]\nIt follows that\n\\[\n\\begin{aligned}\n\\sqrt{n}\\left(\\widehat{\\beta}_{\\mathrm{liml}}-\\beta\\right) &=\\left(\\frac{1}{n} \\boldsymbol{X}^{\\prime} \\boldsymbol{P}_{Z} \\boldsymbol{X}-\\widehat{\\mu} \\frac{1}{n} \\boldsymbol{X}^{\\prime} \\boldsymbol{M}_{\\boldsymbol{Z}} \\boldsymbol{X}\\right)^{-1}\\left(\\frac{1}{\\sqrt{n}} \\boldsymbol{X}^{\\prime} \\boldsymbol{P}_{Z} \\boldsymbol{e}-\\sqrt{n} \\widehat{\\mu} \\frac{1}{n} \\boldsymbol{X}^{\\prime} \\boldsymbol{M}_{Z} \\boldsymbol{e}\\right) \\\\\n&=\\left(\\frac{1}{n} \\boldsymbol{X}^{\\prime} \\boldsymbol{P}_{Z} \\boldsymbol{X}-o_{p}(1)\\right)^{-1}\\left(\\frac{1}{\\sqrt{n}} \\boldsymbol{X}^{\\prime} \\boldsymbol{P}_{Z} \\boldsymbol{e}-o_{p}(1)\\right) \\\\\n&=\\sqrt{n}\\left(\\widehat{\\beta}_{2 \\mathrm{sls}}-\\beta\\right)+o_{p}(1)\n\\end{aligned}\n\\]\nwhich means that LIML and 2SLS have the same asymptotic distribution. This holds under the same assumptions as for 2SLS.\nConsequently, one method to obtain an asymptotically valid covariance estimator for LIML is to use the 2SLS formula. However, this is not the best choice. Rather, consider the IV representation for LIML\n\\[\n\\widehat{\\beta}_{\\mathrm{liml}}=\\left(\\widetilde{\\boldsymbol{X}}^{\\prime} \\boldsymbol{X}\\right)^{-1}\\left(\\widetilde{\\boldsymbol{X}}^{\\prime} \\boldsymbol{Y}_{1}\\right)\n\\]\nwhere\n\\[\n\\widetilde{\\boldsymbol{X}}=\\left(\\begin{array}{c}\n\\boldsymbol{X}_{1} \\\\\n\\boldsymbol{X}_{2}-\\widehat{\\boldsymbol{K}}_{2}\n\\end{array}\\right)\n\\]\nand \\(\\widehat{\\boldsymbol{U}}_{2}=\\boldsymbol{M}_{\\boldsymbol{Z}} \\boldsymbol{X}_{2}\\). The asymptotic covariance matrix formula for an IV estimator is\n\\[\n\\widehat{\\boldsymbol{V}}_{\\beta}=\\left(\\frac{1}{n} \\widetilde{\\boldsymbol{X}}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\widehat{\\Omega}\\left(\\frac{1}{n} \\boldsymbol{X}^{\\prime} \\tilde{\\boldsymbol{X}}\\right)^{-1}\n\\]\nwhere\n\\[\n\\begin{aligned}\n\\widehat{\\Omega} &=\\frac{1}{n} \\sum_{i=1}^{n} \\widetilde{X}_{i} \\widetilde{X}_{i} \\widehat{e}_{i}^{2} \\\\\n\\widehat{e}_{i} &=Y_{1 i}-X_{i}^{\\prime} \\widehat{\\beta}_{\\text {liml }} .\n\\end{aligned}\n\\]\nThis simplifies to the 2SLS formula when \\(\\widehat{\\kappa}=1\\) but otherwise differs. The estimator (12.42) is a better choice than the 2SLS formula for covariance matrix estimation as it takes advantage of the LIML estimator structure."
  },
  {
    "objectID": "chpt12-iv.html#functions-of-parameters",
    "href": "chpt12-iv.html#functions-of-parameters",
    "title": "12  Instrumental Variables",
    "section": "12.20 Functions of Parameters",
    "text": "12.20 Functions of Parameters\nGiven the distribution theory in Theorems \\(12.2\\) and \\(12.3\\) it is straightforward to derive the asymptotic distribution of smooth nonlinear functions of the coefficient estimators.\nSpecifically, given a function \\(r(\\beta): \\mathbb{R}^{k} \\rightarrow \\Theta \\subset \\mathbb{R}^{q}\\) we define the parameter \\(\\theta=r(\\beta)\\). Given \\(\\widehat{\\beta}_{2 \\text { sls }}\\) a natural estimator of \\(\\theta\\) is \\(\\widehat{\\theta}_{2 \\text { sls }}=r\\left(\\widehat{\\beta}_{2 \\text { sls }}\\right)\\).\nConsistency follows from Theorem \\(12.1\\) and the continuous mapping theorem.\nTheorem 12.4 Under Assumptions \\(12.1\\) and 7.3, as \\(n \\rightarrow \\infty, \\widehat{\\theta}_{2 s l s} \\underset{p}{\\longrightarrow} \\theta\\)\nIf \\(r(\\beta)\\) is differentiable then an estimator of the asymptotic covariance matrix for \\(\\widehat{\\theta}_{2 \\text { sls }}\\) is\n\\[\n\\begin{aligned}\n\\widehat{\\boldsymbol{V}}_{\\theta} &=\\widehat{\\boldsymbol{R}}^{\\prime} \\widehat{\\boldsymbol{V}}_{\\beta} \\widehat{\\boldsymbol{R}} \\\\\n\\widehat{\\boldsymbol{R}} &=\\frac{\\partial}{\\partial \\beta} r\\left(\\widehat{\\beta}_{2 s l s}\\right)^{\\prime} .\n\\end{aligned}\n\\]\nWe similarly define the homoskedastic variance estimator as \\(\\widehat{\\boldsymbol{V}}_{\\theta}^{0}=\\widehat{\\boldsymbol{R}}^{\\prime} \\widehat{\\boldsymbol{V}}_{\\beta}^{0} \\widehat{\\boldsymbol{R}}\\).\nThe asymptotic distribution theory follows from Theorems \\(12.2\\) and \\(12.3\\) and the delta method.\nTheorem 12.5 Under Assumptions \\(12.2\\) and \\(7.3\\), as \\(n \\rightarrow \\infty\\),\n\\[\n\\sqrt{n}\\left(\\widehat{\\theta}_{2 s l s}-\\theta\\right) \\underset{d}{\\longrightarrow} \\mathrm{N}\\left(0, \\boldsymbol{V}_{\\theta}\\right)\n\\]\nand \\(\\widehat{\\boldsymbol{V}}_{\\theta} \\underset{p}{\\longrightarrow} \\boldsymbol{V}_{\\theta}\\) where \\(\\boldsymbol{V}_{\\theta}=\\boldsymbol{R}^{\\prime} \\boldsymbol{V}_{\\beta} \\boldsymbol{R}\\) and \\(\\boldsymbol{R}=\\frac{\\partial}{\\partial \\beta} \\boldsymbol{r}(\\beta)^{\\prime}\\)\nWhen \\(q=1\\), a standard error for \\(\\widehat{\\theta}_{2 \\text { sls }}\\) is \\(s\\left(\\widehat{\\theta}_{2 \\text { sls }}\\right)=\\sqrt{n^{-1} \\widehat{\\boldsymbol{V}}_{\\theta}}\\).\nFor example, let’s take the parameter estimates from the fifth column of Table 12.1, which are the 2SLS estimates with three endogenous regressors and four excluded instruments. Suppose we are interested in the return to experience, which depends on the level of experience. The estimated return at experience \\(=10\\) is \\(0.047-0.032 \\times 2 \\times 10 / 100=0.041\\) and its standard error is \\(0.003\\). This implies a \\(4 %\\) increase in wages per year of experience and is precisely estimated. Or suppose we are interested in the level of experience at which the function maximizes. The estimate is \\(50 \\times 0.047 / 0.032=73\\). This has a standard error of 249 . The large standard error implies that the estimate (73 years of experience) is without precision and is thus uninformative."
  },
  {
    "objectID": "chpt12-iv.html#hypothesis-tests",
    "href": "chpt12-iv.html#hypothesis-tests",
    "title": "12  Instrumental Variables",
    "section": "12.21 Hypothesis Tests",
    "text": "12.21 Hypothesis Tests\nAs in the previous section, for a given function \\(r(\\beta): \\mathbb{R}^{k} \\rightarrow \\Theta \\subset \\mathbb{R}^{q}\\) we define the parameter \\(\\theta=r(\\beta)\\) and consider tests of hypotheses of the form \\(\\mathbb{M}_{0}: \\theta=\\theta_{0}\\) against \\(\\mathbb{H}_{1}: \\theta \\neq \\theta_{0}\\). The Wald statistic for \\(\\mathbb{M}_{0}\\) is\n\\[\nW=n\\left(\\widehat{\\theta}-\\theta_{0}\\right)^{\\prime} \\widehat{\\boldsymbol{V}}_{\\widehat{\\theta}}^{-1}\\left(\\widehat{\\theta}-\\theta_{0}\\right) .\n\\]\nFrom Theorem \\(12.5\\) we deduce that \\(W\\) is asymptotically chi-square distributed. Let \\(G_{q}(u)\\) denote the \\(\\chi_{q}^{2}\\) distribution function.\nTheorem 12.6 Under Assumption 12.2, Assumption 7.3, and \\(\\mathbb{H}_{0}\\), then as \\(n \\rightarrow\\) \\(\\infty, W \\underset{d}{\\rightarrow} \\chi_{q}^{2}\\). For \\(c\\) satisfying \\(\\alpha=1-G_{q}(c), \\mathbb{P}\\left[W>c \\mid \\mathbb{M}_{0}\\right] \\longrightarrow \\alpha\\) so the test “Reject \\(\\mathbb{M}_{0}\\) if \\(W>c\\)” has asymptotic size \\(\\alpha\\).\nIn linear regression we often report the \\(F\\) version of the Wald statistic (by dividing by degrees of freedom) and use the \\(F\\) distribution for inference as this is justified in the normal sampling model. For 2SLS estimation, however, this is not done as there is no finite sample \\(F\\) justification for the \\(F\\) version of the Wald statistic.\nTo illustrate, once again let’s take the parameter estimates from the fifth column of Table \\(12.1\\) and again consider the return to experience which is determined by the coefficients on experience and experience \\(^{2} / 100\\). Neither coefficient is statisticially significant at the \\(5 %\\) level and it is unclear if the overall effect is statistically significant. We can assess this by testing the joint hypothesis that both coefficients are zero. The Wald statistic for this hypothesis is \\(W=244\\) which is highly significant with an asymptotic p-value of \\(0.0000\\). Thus by examining the joint test in contrast to the individual tests is quite clear that experience has a non-zero effect."
  },
  {
    "objectID": "chpt12-iv.html#finite-sample-theory",
    "href": "chpt12-iv.html#finite-sample-theory",
    "title": "12  Instrumental Variables",
    "section": "12.22 Finite Sample Theory",
    "text": "12.22 Finite Sample Theory\nIn Chapter 5 we reviewed the rich exact distribution available for the linear regression model under the assumption of normal innovations. There is a similarly rich literature in econometrics for IV, 2SLS and LIML estimators. An excellent review of the theory, mostly developed in the 1970s and early 1980s, is provided by Peter Phillips (1983).\nThis theory was developed under the assumption that the structural error vector \\(e\\) and reduced form error \\(u_{2}\\) are multivariate normally distributed. Even though the errors are normal, IV-type estimators are nonlinear functions of these errors and are thus non-normally distributed. Formulae for the exact distributions have been derived but are unfortunately functions of model parameters and hence are not directly useful for finite sample inference.\nOne important implication of this literature is that even in this optimal context of exact normal innovations the finite sample distributions of the IV estimators are non-normal and the finite sample distributions of test statistics are not chi-squared. The normal and chi-squared approximations hold asymptotically but there is no reason to expect these approximations to be accurate in finite samples.\nA second important result is that under the assumption of normal errors most of the estimators do not have finite moments in any finite sample. A clean statement concerning the existence of moments for the 2SLS estimator was obtained by Kinal (1980) for the case of joint normality. Let \\(\\widehat{\\beta}_{2 s l s, 2}\\) be the 2SLS estimators of the coefficients on the endogeneous regressors.\nTheorem \\(12.7\\) If \\((Y, X, Z)\\) are jointly normal, then for any \\(r, \\mathbb{E}\\left\\|\\widehat{\\beta}_{2 s l s, 2}\\right\\|^{r}<\\infty\\) if and only if \\(r<\\ell_{2}-k_{2}+1\\). This result states that in the just-identified case the IV estimator does not have any finite order integer moments. In the over-identified case the number of finite moments corresponds to the number of overidentifying restrictions \\(\\left(\\ell_{2}-k_{2}\\right)\\). Thus if there is one over-identifying restriction 2 SLS has a finite expectation and if there are two over-identifying restrictions then the 2SLS estimator has a finite variance.\nThe LIML estimator has a more severe moment problem as it has no finite integer moments (Mariano, 1982) regardless of the number of over-identifying restrictions. Due to this lack of moments Fuller (1977) proposed the following modification of LIML. His estimator is\n\\[\n\\begin{aligned}\n\\widehat{\\beta}_{\\text {Fuller }} &=\\left(\\boldsymbol{X}^{\\prime}\\left(\\boldsymbol{I}_{n}-K \\boldsymbol{M}_{\\boldsymbol{Z}}\\right) \\boldsymbol{X}\\right)^{-1}\\left(\\boldsymbol{X}^{\\prime}\\left(\\boldsymbol{I}_{n}-K \\boldsymbol{M}_{\\boldsymbol{Z}}\\right) \\boldsymbol{Y}_{1}\\right) \\\\\nK &=\\widehat{\\kappa}-\\frac{C}{n-k}\n\\end{aligned}\n\\]\nfor some \\(C \\geq 1\\). Fuller showed that his estimator has all moments finite under suitable conditions.\nHausman, Newey, Woutersen, Chao and Swanson (2012) propose an estimator they call HFUL which combines the ideas of JIVE and Fuller which has excellent finite sample properties."
  },
  {
    "objectID": "chpt12-iv.html#bootstrap-for-2sls",
    "href": "chpt12-iv.html#bootstrap-for-2sls",
    "title": "12  Instrumental Variables",
    "section": "12.23 Bootstrap for 2SLS",
    "text": "12.23 Bootstrap for 2SLS\nThe standard bootstrap algorithm for IV, 2SLS, and GMM generates bootstrap samples by sampling the triplets \\(\\left(Y_{1 i}^{*}, X_{i}^{*}, Z_{i}^{*}\\right)\\) independently and with replacement from the original sample \\(\\left\\{\\left(Y_{1 i}, X_{i}, Z_{i}\\right): i=\\right.\\) \\(1, \\ldots, n\\}\\). Sampling \\(n\\) such observations and stacking into observation matrices \\(\\left(\\boldsymbol{Y}_{1}^{*}, \\boldsymbol{X}^{*}, \\boldsymbol{Z}^{*}\\right)\\), the bootstrap 2SLS estimator is\n\\[\n\\widehat{\\beta}_{2 \\mathrm{sls}}^{*}=\\left(\\boldsymbol{X}^{* \\prime} \\boldsymbol{Z}^{*}\\left(\\boldsymbol{Z}^{* \\prime} \\boldsymbol{Z}^{*}\\right)^{-1} \\boldsymbol{Z}^{* \\prime} \\boldsymbol{X}^{*}\\right)^{-1} \\boldsymbol{X}^{* \\prime} \\boldsymbol{Z}^{*}\\left(\\boldsymbol{Z}^{* \\prime} \\boldsymbol{Z}^{*}\\right)^{-1} \\boldsymbol{Z}^{* \\prime} \\boldsymbol{Y}_{1}^{*}\n\\]\nThis is repeated \\(B\\) times to create a sample of \\(B\\) bootstrap draws. Given these draws bootstrap statistics can be calculated. This includes the bootstrap estimate of variance, standard errors, and confidence intervals, including percentile, \\(\\mathrm{BC}\\) percentile, \\(\\mathrm{BC}_{a}\\) and percentile-t.\nWe now show that the bootstrap estimator has the same asymptotic distribution as the sample estimator. For overidentified cases this demonstration requires a bit of extra care. This was first shown by Hahn (1996).\nThe sample observations satisfy the model \\(Y_{1}=X^{\\prime} \\beta+e\\) with \\(\\mathbb{E}[Z e]=0\\). The true value of \\(\\beta\\) in the population can be written as\n\\[\n\\beta=\\left(\\mathbb{E}\\left[X Z^{\\prime}\\right] \\mathbb{E}\\left[Z Z^{\\prime}\\right]^{-1} \\mathbb{E}\\left[Z X^{\\prime}\\right]\\right)^{-1} \\mathbb{E}\\left[X Z^{\\prime}\\right] \\mathbb{E}\\left[Z Z^{\\prime}\\right]^{-1} \\mathbb{E}\\left[Z Y_{1}\\right]\n\\]\nThe true value in the bootstrap universe is obtained by replacing the population moments by the sample moments, which equals the 2SLS estimator\n\\[\n\\begin{aligned}\n&\\left(\\mathbb{E}^{*}\\left[X^{*} Z^{* \\prime}\\right] \\mathbb{E}^{*}\\left[Z^{*} Z^{* \\prime}\\right]^{-1} \\mathbb{E}^{*}\\left[Z^{*} X^{* \\prime}\\right]\\right)^{-1} \\mathbb{E}^{*}\\left[X^{*} Z^{* \\prime}\\right] \\mathbb{E}^{*}\\left[Z^{*} Z^{* \\prime}\\right]^{-1} \\mathbb{E}^{*}\\left[Z^{*} Y_{1}^{*}\\right] \\\\\n&=\\left(\\left(\\frac{1}{n} \\boldsymbol{X}^{\\prime} \\boldsymbol{Z}\\right)\\left(\\frac{1}{n} \\boldsymbol{Z}^{\\prime} \\boldsymbol{Z}\\right)^{-1}\\left(\\frac{1}{n} \\boldsymbol{Z}^{\\prime} \\boldsymbol{X}\\right)\\right)^{-1}\\left(\\frac{1}{n} \\boldsymbol{X}^{\\prime} \\boldsymbol{Z}\\right)\\left(\\frac{1}{n} \\boldsymbol{Z}^{\\prime} \\boldsymbol{Z}\\right)^{-1}\\left[\\frac{1}{n} \\boldsymbol{Z}^{\\prime} \\boldsymbol{Y}_{1}\\right] \\\\\n&=\\widehat{\\beta}_{2 \\text { sls }} .\n\\end{aligned}\n\\]\nThe bootstrap observations thus satisfy the equation \\(Y_{1 i}^{*}=X_{i}^{* \\prime} \\widehat{\\beta}_{2 s l s}+e_{i}^{*}\\). In matrix notation for the sample this is\n\\[\n\\boldsymbol{Y}_{1}^{*}=\\boldsymbol{X}^{* \\prime} \\widehat{\\beta}_{2 \\mathrm{sls}}+\\boldsymbol{e}^{*} .\n\\]\nGiven a bootstrap triple \\(\\left(Y_{1 i}^{*}, X_{i}^{*}, Z_{i}^{*}\\right)=\\left(Y_{1 j}, X_{j}, Z_{j}\\right)\\) for some observation \\(j\\) the true bootstrap error is\n\\[\ne_{i}^{*}=Y_{1 j}-X_{j}^{\\prime} \\widehat{\\beta}_{2 s l s}=\\widehat{e}_{j} .\n\\]\nIt follows that\n\\[\n\\mathbb{E}^{*}\\left[Z^{*} e^{*}\\right]=n^{-1} \\boldsymbol{Z}^{\\prime} \\widehat{\\boldsymbol{e}} .\n\\]\nThis is generally not equal to zero in the over-identified case.\nThis an an important complication. In over-identified models the true observations satisfy the population condition \\(\\mathbb{E}[Z e]=0\\) but in the bootstrap sample \\(\\mathbb{E}^{*}\\left[Z^{*} e^{*}\\right] \\neq 0\\). This means that to apply the central limit theorem to the bootstrap estimator we first have to recenter the moment condition. That is, (12.44) and the bootstrap CLT imply\n\\[\n\\frac{1}{\\sqrt{n}}\\left(\\boldsymbol{Z}^{* \\prime} \\boldsymbol{e}^{*}-\\boldsymbol{Z}^{\\prime} \\widehat{\\boldsymbol{e}}\\right)=\\frac{1}{\\sqrt{n}} \\sum_{i=1}^{n}\\left(Z_{i}^{*} e_{i}^{*}-\\mathbb{E}^{*}\\left[Z^{*} e^{*}\\right]\\right) \\underset{d^{*}}{\\longrightarrow} \\mathrm{N}(0, \\Omega)\n\\]\nwhere\n\\[\n\\Omega=\\mathbb{E}\\left[Z Z^{\\prime} e^{2}\\right] .\n\\]\nUsing (12.43) we can normalize the bootstrap estimator as\n\\[\n\\begin{aligned}\n\\sqrt{n}\\left(\\widehat{\\boldsymbol{\\beta}}_{2 \\mathrm{sls}}^{*}-\\widehat{\\boldsymbol{\\beta}}_{2 \\mathrm{sls}}\\right) &=\\sqrt{n}\\left(\\boldsymbol{X}^{* \\prime} \\boldsymbol{Z}^{*}\\left(\\boldsymbol{Z}^{* \\prime} \\boldsymbol{Z}^{*}\\right)^{-1} \\boldsymbol{Z}^{* \\prime} \\boldsymbol{X}^{*}\\right)^{-1} \\boldsymbol{X}^{* \\prime} \\boldsymbol{Z}^{*}\\left(\\boldsymbol{Z}^{* \\prime} \\boldsymbol{Z}^{*}\\right)^{-1} \\boldsymbol{Z}^{* \\prime} \\boldsymbol{e}^{*} \\\\\n&=\\left(\\left(\\frac{1}{n} \\boldsymbol{X}^{* \\prime} \\boldsymbol{Z}^{*}\\right)\\left(\\frac{1}{n} \\boldsymbol{Z}^{* \\prime} \\boldsymbol{Z}^{*}\\right)^{-1}\\left(\\frac{1}{n} \\boldsymbol{Z}^{* \\prime} \\boldsymbol{X}^{*}\\right)\\right)^{-1} \\\\\n& \\times\\left(\\frac{1}{n} \\boldsymbol{X}^{* \\prime} \\boldsymbol{Z}^{*}\\right)\\left(\\frac{1}{n} \\boldsymbol{Z}^{* \\prime} \\boldsymbol{Z}^{*}\\right)^{-1} \\frac{1}{\\sqrt{n}}\\left(\\boldsymbol{Z}^{* \\prime} \\boldsymbol{e}^{*}-\\boldsymbol{Z}^{\\prime} \\widehat{\\boldsymbol{e}}\\right) \\\\\n&+\\left(\\left(\\frac{1}{n} \\boldsymbol{X}^{* \\prime} \\boldsymbol{Z}^{*}\\right)\\left(\\frac{1}{n} \\boldsymbol{Z}^{* \\prime} \\boldsymbol{Z}^{*}\\right)^{-1}\\left(\\frac{1}{n} \\boldsymbol{Z}^{* \\prime} \\boldsymbol{X}^{*}\\right)\\right)^{-1} \\\\\n& \\times\\left(\\frac{1}{n} \\boldsymbol{X}^{* \\prime} \\boldsymbol{Z}^{*}\\right)\\left(\\frac{1}{n} \\boldsymbol{Z}^{* \\prime} \\boldsymbol{Z}^{*}\\right)^{-1}\\left(\\frac{1}{\\sqrt{n}} \\boldsymbol{Z}^{\\prime} \\widehat{\\boldsymbol{e}}\\right) .\n\\end{aligned}\n\\]\nUsing the bootstrap WLLN,\n\\[\n\\begin{aligned}\n\\frac{1}{n} \\boldsymbol{X}^{* \\prime} \\boldsymbol{Z}^{*} &=\\frac{1}{n} \\boldsymbol{X}^{\\prime} \\boldsymbol{Z}+o_{p}(1) \\\\\n\\frac{1}{n} \\boldsymbol{Z}^{* \\prime} \\boldsymbol{Z}^{*} &=\\frac{1}{n} \\boldsymbol{Z}^{\\prime} \\boldsymbol{Z}+o_{p}(1) .\n\\end{aligned}\n\\]\nThis implies (12.47) is equal to\n\\[\n\\sqrt{n}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{Z}\\left(\\boldsymbol{Z}^{\\prime} \\boldsymbol{Z}\\right)^{-1}\\left(\\boldsymbol{Z}^{\\prime} \\boldsymbol{X}\\right)\\right)^{-1} \\boldsymbol{X}^{\\prime} \\boldsymbol{Z}\\left(\\boldsymbol{Z}^{\\prime} \\boldsymbol{Z}\\right)^{-1} \\boldsymbol{Z}^{\\prime} \\widehat{\\boldsymbol{e}}+o_{p}(1)=0+o_{p}(1)\n\\]\nThe equality holds because the 2SLS first-order condition implies \\(\\boldsymbol{X}^{\\prime} \\boldsymbol{Z}\\left(\\boldsymbol{Z}^{\\prime} \\boldsymbol{Z}\\right)^{-1} \\boldsymbol{Z}^{\\prime} \\widehat{\\boldsymbol{e}}=0\\). Also, combined with (12.45) we see that (12.46) converges in bootstrap distribution to\n\\[\n\\left(\\boldsymbol{Q}_{X Z} \\boldsymbol{Q}_{Z Z}^{-1} \\boldsymbol{Q}_{Z X}\\right)^{-1} \\boldsymbol{Q}_{X Z} \\boldsymbol{Q}_{Z Z}^{-1} \\mathrm{~N}(0, \\Omega)=\\mathrm{N}\\left(0, \\boldsymbol{V}_{\\beta}\\right)\n\\]\nwhere \\(\\boldsymbol{V}_{\\beta}\\) is the 2SLS asymptotic variance from Theorem 12.2. This is the asymptotic distribution of \\(\\sqrt{n}\\left(\\widehat{\\beta}_{2 s l s}^{*}-\\widehat{\\beta}_{2 s l s}\\right)\\)\nBy standard calculations we can also show that bootstrap t-ratios are asymptotically normal. Theorem 12.8 Under Assumption 12.2, as \\(n \\rightarrow \\infty\\)\n\\[\n\\sqrt{n}\\left(\\widehat{\\beta}_{2 s l s}^{*}-\\widehat{\\beta}_{2 s l s}\\right) \\underset{d^{*}}{\\longrightarrow} \\mathrm{N}\\left(0, \\boldsymbol{V}_{\\beta}\\right)\n\\]\nwhere \\(\\boldsymbol{V}_{\\beta}\\) is the \\(2 \\mathrm{SLS}\\) asymptotic variance from Theorem 12.2. Furthermore,\n\\[\nT^{*}=\\frac{\\sqrt{n}\\left(\\widehat{\\beta}_{2 s l s}^{*}-\\widehat{\\beta}_{2 s l s}\\right)}{s\\left(\\widehat{\\beta}_{2 \\text { sls }}^{*}\\right)} \\underset{d^{*}}{\\longrightarrow} \\mathrm{N}(0,1) .\n\\]\nThis shows that percentile-type and percentile-t confidence intervals are asymptotically valid.\nOne might expect that the asymptotic refinement arguments extend to the \\(\\mathrm{BC}_{a}\\) and percentile-t methods but this does not appear to be the case. While \\(\\sqrt{n}\\left(\\widehat{\\beta}_{2 \\text { sls }}^{*}-\\widehat{\\beta}_{2 s l s}\\right)\\) and \\(\\sqrt{n}\\left(\\widehat{\\beta}_{2 s l s}-\\beta\\right)\\) have the same asymptotic distribution they differ in finite samples by an \\(O_{p}\\left(n^{-1 / 2}\\right)\\) term. This means that they have distinct Edgeworth expansions. Consequently, unadjusted bootstrap methods will not achieve an asymptotic refinement.\nAn alternative suggested by Hall and Horowitz (1996) is to recenter the bootstrap 2SLS estimator so that it satisfies the correct orthogonality condition. Define\n\\[\n\\widehat{\\beta}_{2 \\text { sls }}^{* *}=\\left(\\boldsymbol{X}^{* \\prime} \\boldsymbol{Z}^{*}\\left(\\boldsymbol{Z}^{* \\prime} \\boldsymbol{Z}^{*}\\right)^{-1} \\boldsymbol{Z}^{* \\prime} \\boldsymbol{X}^{*}\\right)^{-1} \\boldsymbol{X}^{* \\prime} \\boldsymbol{Z}^{*}\\left(\\boldsymbol{Z}^{* \\prime} \\boldsymbol{Z}^{*}\\right)^{-1}\\left(\\boldsymbol{Z}^{* \\prime} \\boldsymbol{Y}_{1}^{*}-\\boldsymbol{Z}^{\\prime} \\widehat{\\boldsymbol{e}}\\right) .\n\\]\nWe can see that\n\\[\n\\begin{aligned}\n\\sqrt{n}\\left(\\widehat{\\beta}_{2 \\text { sls }}^{* *}-\\widehat{\\beta}_{2 \\mathrm{sls}}\\right) &=\\left(\\frac{1}{n} \\boldsymbol{X}^{* \\prime} \\boldsymbol{Z}^{*}\\left(\\frac{1}{n} \\boldsymbol{Z}^{* \\prime} \\boldsymbol{Z}^{*}\\right)^{-1} \\frac{1}{n} \\boldsymbol{Z}^{* \\prime} \\boldsymbol{X}^{*}\\right)^{-1} \\\\\n& \\times\\left(\\frac{1}{n} \\boldsymbol{X}^{* \\prime} \\boldsymbol{Z}^{*}\\right)\\left(\\frac{1}{n} \\boldsymbol{Z}^{* \\prime} \\boldsymbol{Z}^{*}\\right)^{-1}\\left(\\frac{1}{\\sqrt{n}} \\sum_{i=1}^{n}\\left(Z_{i}^{*} e_{i}^{*}-\\mathbb{E}^{*}\\left[Z^{*} e^{*}\\right]\\right)\\right)\n\\end{aligned}\n\\]\nwhich converges to the \\(\\mathrm{N}\\left(0, \\boldsymbol{V}_{\\beta}\\right)\\) distribution without special handling. Hall and Horowitz (1996) show that percentile-t methods applied to \\(\\widehat{\\beta}_{2 \\text { sls }}^{* *}\\) achieve an asymptotic refinement and are thus preferred to the unadjusted bootstrap estimator.\nThis recentered estimator, however, is not the standard implementation of the bootstrap for 2SLS as used in empirical practice."
  },
  {
    "objectID": "chpt12-iv.html#the-peril-of-bootstrap-2sls-standard-errors",
    "href": "chpt12-iv.html#the-peril-of-bootstrap-2sls-standard-errors",
    "title": "12  Instrumental Variables",
    "section": "12.24 The Peril of Bootstrap 2SLS Standard Errors",
    "text": "12.24 The Peril of Bootstrap 2SLS Standard Errors\nIt is tempting to use the bootstrap algorithm to estimate variance matrices and standard errors for the 2SLS estimator. In fact this is one of the most common uses of bootstrap methods in current econometric practice. Unfortunately this is an unjustified and ill-conceived idea and should not be done. In finite samples the 2SLS estimator may not have a finite second moment, meaning that bootstrap variance estimates are unstable and unreliable.\nTheorem \\(12.7\\) shows that under joint normality the 2SLS estimator will have a finite variance if and only if the number of overidentifying restrictions is two or larger. Thus for just-identified IV, and 2SLS with one degree of overidentification, the finite sample variance is infinite. The bootstrap will be attempting to estimate this value - infinity - and will yield nonsensical answers. When the observations are not jointly normal there is no finite sample theory (so it is possible that the finite sample variance is actually finite) but this is unknown and unverifiable. In overidentified settings when the number of overidentifying restrictions is two or larger the bootstrap can be applied for standard error estimation. However this is not the most common application of IV methods in econometric practice and thus should be viewed as the exception rather than the norm.\nTo understand what is going on consider the simplest case of a just-identified model with a single endogenous regressor and no included exogenous regressors. In this case the estimator can be written as a ratio of means\n\\[\n\\widehat{\\beta}_{\\mathrm{iv}}-\\beta=\\frac{\\sum_{i=1}^{n} Z_{i} e_{i}}{\\sum_{i=1}^{n} Z_{i} X_{i}} .\n\\]\nUnder joint normality of \\((e, X)\\) this has a Cauchy-like distribution which does not possess any finite integer moments. The trouble is that the denominator can be either positive or negative, and arbitrarily close to zero. This means that the ratio can take arbitrarily large values.\nTo illustrate let us return to the basic Card IV wage regression from column 2 of Table \\(12.1\\) which uses college as an instrument for education. We estimate this equation for the subsample of Black men which has \\(n=703\\) observations, and focus on the coefficient for the return to education. The coefficient estimate is reported in Table 12.3, along with asymptotic, jackknife, and two bootstrap standard errors each calculated with 10,000 bootstrap replications.\nTable 12.3: Instrumental Variable Return to Education for Black Men\n\n\n\nEstimate\n\\(0.11\\)\n\n\n\n\nAsymptotic s.e.\n\\((0.11)\\)\n\n\nJackknife s.e.\n\\((0.11)\\)\n\n\nBootstrap s.e. (standard)\n\\((1.42)\\)\n\n\nBootstrap s.e. (repeat)\n\\((4.79)\\)\n\n\n\nThe bootstrap standard errors are an order of magnitude larger than the asymptotic standard errors, and vary substantially across the bootstrap runs despite using 10,000 bootstrap replications. This indicates moment failure and unreliability of the bootstrap standard errors.\nThis is a strong message that bootstrap standard errors should not be computed for IV estimators. Instead, report percentile-type confidence intervals."
  },
  {
    "objectID": "chpt12-iv.html#clustered-dependence",
    "href": "chpt12-iv.html#clustered-dependence",
    "title": "12  Instrumental Variables",
    "section": "12.25 Clustered Dependence",
    "text": "12.25 Clustered Dependence\nIn Section \\(4.21\\) we introduced clustered dependence. We can also use the methods of clustered dependence for 2SLS estimation. Recall, the \\(g^{t h}\\) cluster has the observations \\(\\boldsymbol{Y}_{g}=\\left(Y_{1 g}, \\ldots, Y_{n_{g} g}\\right)^{\\prime}, \\boldsymbol{X}_{g}=\\) \\(\\left(X_{1 g}, \\ldots, X_{n_{g} g}\\right)^{\\prime}\\), and \\(Z_{g}=\\left(Z_{1 g}, \\ldots, Z_{n_{g} g}\\right)^{\\prime}\\). The structural equation for the \\(g^{t h}\\) cluster can be written as the matrix system \\(\\boldsymbol{Y}_{g}=\\boldsymbol{X}_{g} \\beta+\\boldsymbol{e}_{g}\\). Using this notation the centered 2SLS estimator can be written as\n\\[\n\\begin{aligned}\n\\widehat{\\beta}_{2 \\text { sls }}-\\beta &=\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{Z}\\left(\\boldsymbol{Z}^{\\prime} \\boldsymbol{Z}\\right)^{-1} \\boldsymbol{Z}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\boldsymbol{X}^{\\prime} \\boldsymbol{Z}\\left(\\boldsymbol{Z}^{\\prime} \\boldsymbol{Z}\\right)^{-1} \\boldsymbol{Z}^{\\prime} \\boldsymbol{e} \\\\\n&=\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{Z}\\left(\\boldsymbol{Z}^{\\prime} \\boldsymbol{Z}\\right)^{-1} \\boldsymbol{Z}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\boldsymbol{X}^{\\prime} \\boldsymbol{Z}\\left(\\boldsymbol{Z}^{\\prime} \\boldsymbol{Z}\\right)^{-1}\\left(\\sum_{g=1}^{G} \\boldsymbol{Z}_{g}^{\\prime} \\boldsymbol{e}_{g}\\right)\n\\end{aligned}\n\\]\nThe cluster-robust covariance matrix estimator for \\(\\widehat{\\beta}_{2 s l s}\\) thus takes the form\n\\[\n\\widehat{\\boldsymbol{V}}_{\\beta}=\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{Z}\\left(\\boldsymbol{Z}^{\\prime} \\boldsymbol{Z}\\right)^{-1} \\boldsymbol{Z}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\boldsymbol{X}^{\\prime} \\boldsymbol{Z}\\left(\\boldsymbol{Z}^{\\prime} \\boldsymbol{Z}\\right)^{-1} \\widehat{\\boldsymbol{S}}\\left(\\boldsymbol{Z}^{\\prime} \\boldsymbol{Z}\\right)^{-1} \\boldsymbol{Z}^{\\prime} \\boldsymbol{X}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{Z}\\left(\\boldsymbol{Z}^{\\prime} \\boldsymbol{Z}\\right)^{-1} \\boldsymbol{Z}^{\\prime} \\boldsymbol{X}\\right)^{-1}\n\\]\nwith\n\\[\n\\widehat{\\boldsymbol{S}}=\\sum_{g=1}^{G} \\boldsymbol{Z}_{g}^{\\prime} \\widehat{\\boldsymbol{e}}_{g} \\widehat{\\boldsymbol{e}}_{g}^{\\prime} \\boldsymbol{Z}_{g}\n\\]\nand the clustered residuals \\(\\widehat{\\boldsymbol{e}}_{g}=\\boldsymbol{Y}_{g}-\\boldsymbol{X}_{g} \\widehat{\\beta}_{2 \\text { sls }}\\)\nThe difference between the heteroskedasticity-robust estimator and the cluster-robust estimator is the covariance estimator \\(\\widehat{\\boldsymbol{S}}\\)."
  },
  {
    "objectID": "chpt12-iv.html#generated-regressors",
    "href": "chpt12-iv.html#generated-regressors",
    "title": "12  Instrumental Variables",
    "section": "12.26 Generated Regressors",
    "text": "12.26 Generated Regressors\nThe “two-stage” form of the 2SLS estimator is an example of what is called “estimation with generated regressors”. We say a regressor is a generated if it is an estimate of an idealized regressor or if it is a function of estimated parameters. Typically, a generated regressor \\(\\widehat{W}\\) is an estimate of an unobserved ideal regressor \\(W\\). As an estimate, \\(\\widehat{W}_{i}\\) is a function of the full sample not just observation \\(i\\). Hence it is not “i.i.d.” as it is dependent across observations which invalidates the conventional regression assumptions. Consequently, the sampling distribution of regression estimates is affected. Unless this is incorporated into our inference methods, covariance matrix estimates and standard errors will be incorrect.\nThe econometric theory of generated regressors was developed by Pagan (1984) for linear models and extended to nonlinear models and more general two-step estimators by Pagan (1986). Independently, similar results were obtained by Murphy and Topel (1985). Here we focus on the linear model:\n\\[\n\\begin{aligned}\nY &=W^{\\prime} \\beta+v \\\\\nW &=\\boldsymbol{A}^{\\prime} Z \\\\\n\\mathbb{E}[Z v] &=0 .\n\\end{aligned}\n\\]\nThe observables are \\((Y, Z)\\). We also have an estimate \\(\\widehat{\\boldsymbol{A}}\\) of \\(\\boldsymbol{A}\\).\nGiven \\(\\widehat{A}\\) we construct the estimate \\(\\widehat{W}_{i}=\\widehat{A}^{\\prime} Z_{i}\\) of \\(W_{i}\\), replace \\(W_{i}\\) in (12.48) with \\(\\widehat{W}_{i}\\), and then estimate \\(\\beta\\) by least squares, resulting in the estimator\n\\[\n\\widehat{\\beta}=\\left(\\sum_{i=1}^{n} \\widehat{W}_{i} \\widehat{W}_{i}^{\\prime}\\right)^{-1}\\left(\\sum_{i=1}^{n} \\widehat{W}_{i} Y_{i}\\right)\n\\]\nThe regressors \\(\\widehat{W}_{i}\\) are called generated regressors. The properties of \\(\\widehat{\\beta}\\) are different than least squares with i.i.d. observations because the generated regressors are themselves estimates.\nThis framework includes 2SLS as well as other common estimators. The 2SLS model can be written as (12.48) by looking at the reduced form equation (12.13), with \\(W=\\Gamma^{\\prime} Z, A=\\Gamma\\), and \\(\\widehat{A}=\\widehat{\\Gamma}\\).\nThe examples which motivated Pagan (1984) and Murphy and Topel (1985) emerged from the macroeconomics literature, in particular the work of Barro (1977) which examined the impact of inflation expectations and expectation errors on economic output. Let \\(\\pi\\) denote realized inflation and \\(Z\\) be variables available to economic agents. A model of inflation expectations sets \\(W=\\mathbb{E}[\\pi \\mid Z]=\\gamma^{\\prime} Z\\) and a model of expectation error sets \\(W=\\pi-\\mathbb{E}[\\pi \\mid Z]=\\pi-\\gamma^{\\prime} Z\\). Since expectations and errors are not observed they are replaced in applications with the fitted values \\(\\widehat{W}_{i}=\\widehat{\\gamma}^{\\prime} Z_{i}\\) and residuals \\(\\widehat{W}_{i}=\\pi_{i}-\\widehat{\\gamma}^{\\prime} Z_{i}\\) where \\(\\widehat{\\gamma}\\) is the coefficient from a regression of \\(\\pi\\) on \\(Z\\).\nThe generated regressor framework includes all of these examples.\nThe goal is to obtain a distributional approximation for \\(\\widehat{\\beta}\\) in order to construct standard errors, confidence intervals, and tests. Start by substituting equation (12.48) into (12.49). We obtain\n\\[\n\\widehat{\\beta}=\\left(\\sum_{i=1}^{n} \\widehat{W}_{i} \\widehat{W}_{i}^{\\prime}\\right)^{-1}\\left(\\sum_{i=1}^{n} \\widehat{W}_{i}\\left(W_{i}^{\\prime} \\beta+v_{i}\\right)\\right) .\n\\]\nNext, substitute \\(W_{i}^{\\prime} \\beta=\\widehat{W}_{i}^{\\prime} \\beta+\\left(W_{i}-\\widehat{W}_{i}\\right)^{\\prime} \\beta\\). We obtain\n\\[\n\\widehat{\\beta}-\\beta=\\left(\\sum_{i=1}^{n} \\widehat{W}_{i} \\widehat{W}_{i}^{\\prime}\\right)^{-1}\\left(\\sum_{i=1}^{n} \\widehat{W}_{i}\\left(\\left(W_{i}-\\widehat{W}_{i}\\right)^{\\prime} \\beta+v_{i}\\right)\\right) .\n\\]\nEffectively, this shows that the distribution of \\(\\widehat{\\beta}-\\beta\\) has two random components, one due to conventional regression component and the second due to the generated regressor. Conventional variance estimators do not address this second component and thus will be biased.\nInterestingly, the distribution in (12.50) dramatically simplifies in the special case that the “generated regressor term” \\(\\left(W_{i}-\\widehat{W}_{i}\\right)^{\\prime} \\beta\\) disappears. This occurs when the slope coefficients on the generated regressors are zero. To be specific, partition \\(W_{i}=\\left(W_{1 i}, W_{2 i}\\right), \\widehat{W}_{i}=\\left(W_{1 i}, \\widehat{W}_{2 i}\\right)\\), and \\(\\beta=\\left(\\beta_{1}, \\beta_{2}\\right)\\) so that \\(W_{1 i}\\) are the conventional observed regressors and \\(\\widehat{W}_{2 i}\\) are the generated regressors. Then \\(\\left(W_{i}-\\widehat{W}_{i}\\right)^{\\prime} \\beta=\\) \\(\\left(W_{2 i}-\\widehat{W}_{2 i}\\right)^{\\prime} \\beta_{2}\\). Thus if \\(\\beta_{2}=0\\) this term disappears. In this case (12.50) equals\n\\[\n\\widehat{\\beta}-\\widehat{\\beta}=\\left(\\sum_{i=1}^{n} \\widehat{W}_{i} \\widehat{W}_{i}^{\\prime}\\right)^{-1}\\left(\\sum_{i=1}^{n} \\widehat{W}_{i} v_{i}\\right) .\n\\]\nThis is a dramatic simplification.\nFurthermore, since \\(\\widehat{W}_{i}=\\widehat{A}^{\\prime} Z_{i}\\) we can write the estimator as a function of sample moments:\n\\[\n\\sqrt{n}(\\widehat{\\beta}-\\beta)=\\left(\\widehat{\\boldsymbol{A}}^{\\prime}\\left(\\frac{1}{n} \\sum_{i=1}^{n} Z_{i} Z_{i}^{\\prime}\\right) \\widehat{\\boldsymbol{A}}\\right)^{-1} \\widehat{\\boldsymbol{A}}^{\\prime}\\left(\\frac{1}{\\sqrt{n}} \\sum_{i=1}^{n} Z_{i} v_{i}\\right)\n\\]\nIf \\(\\widehat{\\boldsymbol{A}} \\underset{p}{\\longrightarrow} \\boldsymbol{A}\\) we find from standard manipulations that \\(\\sqrt{n}(\\widehat{\\beta}-\\beta) \\underset{d}{\\longrightarrow} \\mathrm{N}\\left(0, \\boldsymbol{V}_{\\beta}\\right)\\) where\n\\[\n\\boldsymbol{V}_{\\boldsymbol{\\beta}}=\\left(\\boldsymbol{A}^{\\prime} \\mathbb{E}\\left[Z Z^{\\prime}\\right] \\boldsymbol{A}\\right)^{-1}\\left(\\boldsymbol{A}^{\\prime} \\mathbb{E}\\left[Z Z^{\\prime} v^{2}\\right] \\boldsymbol{A}\\right)\\left(\\boldsymbol{A}^{\\prime} \\mathbb{E}\\left[Z Z^{\\prime}\\right] \\boldsymbol{A}\\right)^{-1} .\n\\]\nThe conventional asymptotic covariance matrix estimator for \\(\\widehat{\\beta}\\) takes the form\n\\[\n\\widehat{\\boldsymbol{V}}_{\\beta}=\\left(\\frac{1}{n} \\sum_{i=1}^{n} \\widehat{W}_{i} \\widehat{W}_{i}^{\\prime}\\right)^{-1}\\left(\\frac{1}{n} \\sum_{i=1}^{n} \\widehat{W}_{i} \\widehat{W}_{i}^{\\prime} \\widehat{v}_{i}^{2}\\right)\\left(\\frac{1}{n} \\sum_{i=1}^{n} \\widehat{W}_{i} \\widehat{W}_{i}^{\\prime}\\right)^{-1}\n\\]\nwhere \\(\\widehat{v}_{i}=Y_{i}-\\widehat{W}_{i}^{\\prime} \\widehat{\\beta}\\). Under the given assumptions, \\(\\widehat{\\boldsymbol{V}}_{\\beta} \\underset{p}{\\longrightarrow} \\boldsymbol{V}_{\\beta}\\). Thus inference using \\(\\widehat{\\boldsymbol{V}}_{\\beta}\\) is asymptotically valid. This is useful when we are interested in tests of \\(\\beta_{2}=0\\). Often this is of major interest in applications.\nTo test \\(\\mathbb{M}_{0}: \\beta_{2}=0\\) we partition \\(\\widehat{\\beta}=\\left(\\widehat{\\beta}_{1}, \\widehat{\\beta}_{2}\\right)\\) and construct a conventional Wald statistic\n\\[\nW=n \\widehat{\\beta}_{2}^{\\prime}\\left(\\left[\\widehat{\\boldsymbol{V}}_{\\beta}\\right]_{22}\\right)^{-1} \\widehat{\\beta}_{2} .\n\\]\nTheorem 12.9 Take model (12.48) with \\(\\mathbb{E}\\left[Y^{4}\\right]<\\infty, \\mathbb{E}\\|Z\\|^{4}<\\infty, A^{\\prime} \\mathbb{E}\\left[Z Z^{\\prime}\\right] \\boldsymbol{A}>\\) \\(0, \\widehat{\\boldsymbol{A}} \\underset{p}{\\longrightarrow} \\boldsymbol{A}\\), and \\(\\widehat{W}_{i}=\\left(W_{1 i}, \\widehat{W}_{2 i}\\right)\\). Under \\(\\mathbb{H}_{0}: \\beta_{2}=0\\), as \\(n \\rightarrow \\infty, \\sqrt{n}(\\widehat{\\beta}-\\beta) \\underset{d}{\\longrightarrow}\\) \\(\\mathrm{N}\\left(0, \\boldsymbol{V}_{\\beta}\\right)\\) where \\(\\boldsymbol{V}_{\\beta}\\) is given in (12.51). For \\(\\widehat{\\boldsymbol{V}}_{\\beta}\\) given in (12.52), \\(\\widehat{\\boldsymbol{V}}_{\\beta} \\underset{p}{\\longrightarrow} \\boldsymbol{V}_{\\beta}\\). Furthermore, \\(W \\underset{d}{\\longrightarrow} \\chi_{q}^{2}\\) where \\(q=\\operatorname{dim}\\left(\\beta_{2}\\right)\\). For \\(c\\) satisfying \\(\\alpha=1-G_{q}(c)\\), \\(\\mathbb{P}\\left[W>c \\mid \\mathbb{M}_{0}\\right] \\rightarrow \\alpha\\), so the test “Reject \\(\\mathbb{M}_{0}\\) if \\(W>c\\)” has asymptotic size \\(\\alpha\\). In the special case that \\(\\widehat{\\boldsymbol{A}}=\\boldsymbol{A}(\\boldsymbol{X}, \\boldsymbol{Z})\\) and \\(v \\mid X, Z \\sim \\mathrm{N}\\left(0, \\sigma^{2}\\right)\\) there is a finite sample version of the previous result. Let \\(W^{0}\\) be the Wald statistic constructed with a homoskedastic covariance matrix estimator, and let\n\\[\nF=W / q\n\\]\nbe the the \\(F\\) statistic, where \\(q=\\operatorname{dim}\\left(\\beta_{2}\\right)\\)\nTheorem 12.10 Take model (12.48) with \\(\\widehat{\\boldsymbol{A}}=\\boldsymbol{A}(\\boldsymbol{X}, \\boldsymbol{Z}), v \\mid X, Z \\sim \\mathrm{N}\\left(0, \\sigma^{2}\\right)\\) and \\(\\widehat{W}=\\left(W_{1}, \\widehat{W}_{2}\\right)\\). Under \\(\\mathbb{M}_{0}: \\beta_{2}=0\\), t-statistics have exact \\(\\mathrm{N}(0,1)\\) distributions, and the \\(F\\) statistic (12.53) has an exact \\(F_{q, n-k}\\) distribution where \\(q=\\operatorname{dim}\\left(\\beta_{2}\\right)\\) and \\(k=\\operatorname{dim}(\\beta)\\)\nTo summarize, in the model \\(Y=W_{1}^{\\prime} \\beta_{1}+W_{2}^{\\prime} \\beta_{2}+v\\) where \\(W_{2}\\) is not observed but replaced with an estimate \\(\\widehat{W}_{2}\\), conventional significance tests for \\(\\mathbb{M}_{0}: \\beta_{2}=0\\) are asymptotically valid without adjustment.\nWhile this theory allows tests of \\(\\mathbb{M}_{0}: \\beta_{2}=0\\) it unfortunately does not justify conventional standard errors or confidence intervals. For this, we need to work out the distribution without imposing the simplification \\(\\beta_{2}=0\\). This often needs to be worked out case-by-case or by using methods based on the generalized method of moments to be introduced in Chapter 13. However, in one important set of examples it is straightforward to work out the asymptotic distribution.\nFor the remainder of this section we examine the setting where the estimators \\(\\widehat{A}\\) take a least squares form so for some \\(\\boldsymbol{X}\\) can be written as \\(\\widehat{\\boldsymbol{A}}=\\left(\\boldsymbol{Z}^{\\prime} \\boldsymbol{Z}\\right)^{-1}\\left(\\boldsymbol{Z}^{\\prime} \\boldsymbol{X}\\right)\\). Such estimators correspond to the multivariate projection model\n\\[\n\\begin{aligned}\nX &=\\boldsymbol{A}^{\\prime} Z+u \\\\\n\\mathbb{E}\\left[Z u^{\\prime}\\right] &=0 .\n\\end{aligned}\n\\]\nThis class of estimators includes 2SLS and the expectation model described above. We can write the matrix of generated regressors as \\(\\widehat{W}=Z \\widehat{A}\\) and then (12.50) as\n\\[\n\\begin{aligned}\n\\widehat{\\beta}-\\beta &=\\left(\\widehat{\\boldsymbol{W}}^{\\prime} \\widehat{\\boldsymbol{W}}\\right)^{-1}\\left(\\widehat{\\boldsymbol{W}}^{\\prime}((\\boldsymbol{W}-\\widehat{\\boldsymbol{W}}) \\beta+\\boldsymbol{v})\\right) \\\\\n&=\\left(\\widehat{\\boldsymbol{A}}^{\\prime} \\boldsymbol{Z}^{\\prime} \\boldsymbol{Z} \\widehat{\\boldsymbol{A}}\\right)^{-1}\\left(\\widehat{\\boldsymbol{A}}^{\\prime} \\boldsymbol{Z}^{\\prime}\\left(-\\boldsymbol{Z}\\left(\\boldsymbol{Z}^{\\prime} \\boldsymbol{Z}\\right)^{-1}\\left(\\boldsymbol{Z}^{\\prime} \\boldsymbol{U}\\right) \\beta+\\boldsymbol{v}\\right)\\right) \\\\\n&=\\left(\\widehat{\\boldsymbol{A}}^{\\prime} \\boldsymbol{Z}^{\\prime} \\boldsymbol{Z} \\widehat{\\boldsymbol{A}}\\right)^{-1}\\left(\\widehat{\\boldsymbol{A}}^{\\prime} \\boldsymbol{Z}^{\\prime}(-\\boldsymbol{U} \\beta+\\boldsymbol{v})\\right) \\\\\n&=\\left(\\widehat{\\boldsymbol{A}}^{\\prime} \\boldsymbol{Z}^{\\prime} \\boldsymbol{Z} \\widehat{\\boldsymbol{A}}\\right)^{-1}\\left(\\widehat{\\boldsymbol{A}}^{\\prime} \\boldsymbol{Z}^{\\prime} \\boldsymbol{e}\\right)\n\\end{aligned}\n\\]\nwhere\n\\[\ne=v-u^{\\prime} \\beta=Y-X^{\\prime} \\beta .\n\\]\nThis estimator has the asymptotic distribution \\(\\sqrt{n}(\\widehat{\\beta}-\\beta) \\underset{d}{\\longrightarrow} \\mathrm{N}\\left(0, V_{\\beta}\\right)\\) where\n\\[\n\\boldsymbol{V}_{\\beta}=\\left(\\boldsymbol{A}^{\\prime} \\mathbb{E}\\left[Z Z^{\\prime}\\right] \\boldsymbol{A}\\right)^{-1}\\left(\\boldsymbol{A}^{\\prime} \\mathbb{E}\\left[Z Z^{\\prime} e^{2}\\right] \\boldsymbol{A}\\right)\\left(\\boldsymbol{A}^{\\prime} \\mathbb{E}\\left[Z Z^{\\prime}\\right] \\boldsymbol{A}\\right)^{-1} .\n\\]\nUnder conditional homoskedasticity the covariance matrix simplifies to\n\\[\n\\boldsymbol{V}_{\\boldsymbol{\\beta}}=\\left(\\boldsymbol{A}^{\\prime} \\mathbb{E}\\left[Z Z^{\\prime}\\right] \\boldsymbol{A}\\right)^{-1} \\mathbb{E}\\left[e^{2}\\right] .\n\\]\nAn appropriate estimator of \\(\\boldsymbol{V}_{\\beta}\\) is\n\\[\n\\begin{aligned}\n\\widehat{\\boldsymbol{V}}_{\\beta} &=\\left(\\frac{1}{n} \\widehat{\\boldsymbol{W}}^{\\prime} \\widehat{\\boldsymbol{W}}\\right)^{-1}\\left(\\frac{1}{n} \\sum_{i=1}^{n} \\widehat{W}_{i} \\widehat{W}_{i}^{\\prime} \\widehat{e}_{i}^{2}\\right)\\left(\\frac{1}{n} \\widehat{\\boldsymbol{W}}^{\\prime} \\widehat{\\boldsymbol{W}}\\right)^{-1} \\\\\n\\widehat{e}_{i} &=Y_{i}-X_{i}^{\\prime} \\widehat{\\beta}\n\\end{aligned}\n\\]\nUnder the assumption of conditional homoskedasticity this can be simplified as usual.\nThis appears to be the usual covariance matrix estimator, but it is not because the least squares residuals \\(\\widehat{v}_{i}=Y_{i}-\\widehat{W_{i}^{\\prime}} \\widehat{\\beta}\\) have been replaced with \\(\\widehat{e}_{i}\\). This is exactly the substitution made by the 2SLS covariance matrix formula. Indeed, the covariance matrix estimator \\(\\widehat{\\boldsymbol{V}}_{\\beta}\\) precisely equals (12.40).\nTheorem 12.11 Take model (12.48) and (12.54) with \\(\\mathbb{E}\\left[Y^{4}\\right]<\\infty, \\mathbb{E}\\|Z\\|^{4}<\\infty\\), \\(\\boldsymbol{A}^{\\prime} \\mathbb{E}\\left[Z Z^{\\prime}\\right] \\boldsymbol{A}>0\\), and \\(\\widehat{\\boldsymbol{A}}=\\left(\\boldsymbol{Z}^{\\prime} \\boldsymbol{Z}\\right)^{-1}\\left(\\boldsymbol{Z}^{\\prime} \\boldsymbol{X}\\right)\\). As \\(n \\rightarrow \\infty, \\sqrt{n}(\\widehat{\\beta}-\\beta) \\underset{d}{\\rightarrow} \\mathrm{N}\\left(0, \\boldsymbol{V}_{\\beta}\\right)\\) where \\(\\boldsymbol{V}_{\\beta}\\) is given in (12.56) with \\(e\\) defined in (12.55). For \\(\\widehat{\\boldsymbol{V}}_{\\beta}\\) given in (12.57), \\(\\widehat{\\boldsymbol{V}}_{\\beta} \\underset{p}{\\longrightarrow} \\boldsymbol{V}_{\\beta}\\).\nSince the parameter estimators are asymptotically normal and the covariance matrix is consistently estimated, standard errors and test statistics constructed from \\(\\widehat{\\boldsymbol{V}}_{\\beta}\\) are asymptotically valid with conventional interpretations.\nWe now summarize the results of this section. In general, care needs to be exercised when estimating models with generated regressors. As a general rule, generated regressors and two-step estimation affect sampling distributions and variance matrices. An important simplication occurs for tests that the generated regressors have zero slopes. In this case conventional tests have conventional distributions, both asymptotically and in finite samples. Another important special case occurs when the generated regressors are least squares fitted values. In this case the asymptotic distribution takes a conventional form but the conventional residual needs to be replaced by one constructed with the forecasted variable. With this one modification asymptotic inference using the generated regressors is conventional."
  },
  {
    "objectID": "chpt12-iv.html#regression-with-expectation-errors",
    "href": "chpt12-iv.html#regression-with-expectation-errors",
    "title": "12  Instrumental Variables",
    "section": "12.27 Regression with Expectation Errors",
    "text": "12.27 Regression with Expectation Errors\nIn this section we examine a generated regressor model which includes expectation errors in the regression. This is an important class of generated regressor models and is relatively straightforward to characterize. The model is\n\\[\n\\begin{aligned}\nY &=X^{\\prime} \\beta+u^{\\prime} \\alpha+v \\\\\nW &=\\boldsymbol{A}^{\\prime} Z \\\\\nX &=W+u \\\\\n\\mathbb{E}[Z v] &=0 \\\\\n\\mathbb{E}[u v] &=0 \\\\\n\\mathbb{E}\\left[Z u^{\\prime}\\right] &=0 .\n\\end{aligned}\n\\]\nThe observables are \\((Y, X, Z)\\). This model states that \\(W\\) is the expectation of \\(X\\) (or more generally, the projection of \\(X\\) on \\(Z\\) ) and \\(u\\) is its expectation error. The model allows for exogenous regressors as in the standard IV model if they are listed in \\(W, X\\), and \\(Z\\). This model is used, for example, to decompose the effect of expectations from expectation errors. In some cases it is desired to include only the expectation error \\(u\\), not the expectation \\(W\\). This does not change the results described here.\nThe model is estimated as follows. First, \\(\\boldsymbol{A}\\) is estimated by multivariate least squares of \\(X\\) on \\(Z\\), \\(\\widehat{\\boldsymbol{A}}=\\left(\\boldsymbol{Z}^{\\prime} \\boldsymbol{Z}\\right)^{-1}\\left(\\boldsymbol{Z}^{\\prime} \\boldsymbol{X}\\right)\\), which yields as by-products the fitted values \\(\\widehat{W}_{i}=\\widehat{\\boldsymbol{A}}^{\\prime} Z_{i}\\) and residuals \\(\\widehat{u}_{i}=\\widehat{X}_{i}-\\widehat{W}_{i}\\). Second, the coefficients are estimated by least squares of \\(Y\\) on the fitted values \\(\\widehat{W}\\) and residuals \\(\\widehat{u}\\)\n\\[\nY_{i}=\\widehat{W}_{i}^{\\prime} \\widehat{\\beta}+\\widehat{u}_{i}^{\\prime} \\widehat{\\alpha}+\\widehat{v}_{i} .\n\\]\nWe now examine the asymptotic distributions of these estimators.\nBy the first-step regression \\(\\boldsymbol{Z}^{\\prime} \\widehat{\\boldsymbol{U}}=0, \\widehat{\\boldsymbol{W}}^{\\prime} \\widehat{\\boldsymbol{U}}=0\\) and \\(\\boldsymbol{W}^{\\prime} \\widehat{\\boldsymbol{U}}=0\\). This means that \\(\\widehat{\\beta}\\) and \\(\\widehat{\\alpha}\\) can be computed separately. Notice that\n\\[\n\\widehat{\\beta}=\\left(\\widehat{\\boldsymbol{W}}^{\\prime} \\widehat{\\boldsymbol{W}}\\right)^{-1} \\widehat{\\boldsymbol{W}}^{\\prime} \\boldsymbol{Y}\n\\]\nand\n\\[\n\\boldsymbol{Y}=\\widehat{\\boldsymbol{W}} \\beta+\\boldsymbol{U} \\alpha+(\\boldsymbol{W}-\\widehat{\\boldsymbol{W}}) \\beta+\\boldsymbol{v} .\n\\]\nSubstituting, using \\(\\widehat{\\boldsymbol{W}}^{\\prime} \\widehat{\\boldsymbol{U}}=0\\) and \\(\\boldsymbol{W}-\\widehat{\\boldsymbol{W}}=-\\boldsymbol{Z}\\left(\\boldsymbol{Z}^{\\prime} \\boldsymbol{Z}\\right)^{-1} \\boldsymbol{Z}^{\\prime} \\boldsymbol{U}\\) we find\n\\[\n\\begin{aligned}\n\\widehat{\\beta}-\\beta &=\\left(\\widehat{\\boldsymbol{W}}^{\\prime} \\widehat{\\boldsymbol{W}}\\right)^{-1} \\widehat{\\boldsymbol{W}}^{\\prime}(\\boldsymbol{U} \\alpha+(\\boldsymbol{W}-\\widehat{\\boldsymbol{W}}) \\beta+\\boldsymbol{v}) \\\\\n&=\\left(\\widehat{\\boldsymbol{A}}^{\\prime} \\boldsymbol{Z}^{\\prime} \\boldsymbol{Z} \\widehat{\\boldsymbol{A}}\\right)^{-1} \\widehat{\\boldsymbol{A}}^{\\prime} \\boldsymbol{Z}^{\\prime}(\\boldsymbol{U} \\alpha-\\boldsymbol{U} \\beta+\\boldsymbol{v}) \\\\\n&=\\left(\\widehat{\\boldsymbol{A}}^{\\prime} \\boldsymbol{Z}^{\\prime} \\boldsymbol{Z} \\widehat{\\boldsymbol{A}}\\right)^{-1} \\widehat{\\boldsymbol{A}}^{\\prime} \\boldsymbol{Z}^{\\prime} \\boldsymbol{e}\n\\end{aligned}\n\\]\nwhere\n\\[\ne_{i}=v_{i}+u_{i}^{\\prime}(\\alpha-\\beta)=Y_{i}-X_{i}^{\\prime} \\beta .\n\\]\nWe also find\n\\[\n\\widehat{\\alpha}=\\left(\\widehat{\\boldsymbol{U}}^{\\prime} \\widehat{\\boldsymbol{U}}\\right)^{-1} \\widehat{\\boldsymbol{U}}^{\\prime} \\boldsymbol{Y} .\n\\]\nSince \\(\\widehat{\\boldsymbol{U}}^{\\prime} \\boldsymbol{W}=0, \\boldsymbol{U}-\\widehat{\\boldsymbol{U}}=\\boldsymbol{Z}\\left(\\boldsymbol{Z}^{\\prime} \\boldsymbol{Z}\\right)^{-1} \\boldsymbol{Z}^{\\prime} \\boldsymbol{U}\\) and \\(\\widehat{\\boldsymbol{U}}^{\\prime} \\boldsymbol{Z}=0\\) then\n\\[\n\\begin{aligned}\n\\widehat{\\alpha}-\\alpha &=\\left(\\widehat{\\boldsymbol{U}}^{\\prime} \\widehat{\\boldsymbol{U}}\\right)^{-1} \\widehat{\\boldsymbol{U}}^{\\prime}(\\boldsymbol{W} \\beta+(\\boldsymbol{U}-\\widehat{\\boldsymbol{U}}) \\alpha+\\boldsymbol{v}) \\\\\n&=\\left(\\widehat{\\boldsymbol{U}}^{\\prime} \\widehat{\\boldsymbol{U}}\\right)^{-1} \\widehat{\\boldsymbol{U}}^{\\prime} \\boldsymbol{v}\n\\end{aligned}\n\\]\nTogether, we establish the following distributional result. Theorem 12.12 For the model and estimators described in this section, with \\(\\mathbb{E}\\left[Y^{4}\\right]<\\infty, \\mathbb{E}\\|Z\\|^{4}<\\infty, \\mathbb{E}\\|X\\|^{4}<\\infty, A^{\\prime} \\mathbb{E}\\left[Z Z^{\\prime}\\right] A>0\\), and \\(\\mathbb{E}\\left[u u^{\\prime}\\right]>0\\), as \\(n \\rightarrow \\infty\\)\n\\[\n\\sqrt{n}\\left(\\begin{array}{c}\n\\widehat{\\beta}-\\beta \\\\\n\\widehat{\\alpha}-\\alpha\n\\end{array}\\right) \\underset{d}{\\longrightarrow} \\mathrm{N}(0, \\boldsymbol{V})\n\\]\nwhere\n\\[\n\\boldsymbol{V}=\\left(\\begin{array}{ll}\n\\boldsymbol{V}_{\\beta \\beta} & \\boldsymbol{V}_{\\beta \\alpha} \\\\\n\\boldsymbol{V}_{\\alpha \\beta} & \\boldsymbol{V}_{\\alpha \\alpha}\n\\end{array}\\right)\n\\]\nand\n\\[\n\\begin{aligned}\n\\boldsymbol{V}_{\\beta \\beta} &=\\left(\\boldsymbol{A}^{\\prime} \\mathbb{E}\\left[Z Z^{\\prime}\\right] \\boldsymbol{A}\\right)^{-1}\\left(\\boldsymbol{A}^{\\prime} \\mathbb{E}\\left[Z Z^{\\prime} e^{2}\\right] \\boldsymbol{A}\\right)\\left(\\boldsymbol{A}^{\\prime} \\mathbb{E}\\left[Z Z^{\\prime}\\right] \\boldsymbol{A}\\right)^{-1} \\\\\n\\boldsymbol{V}_{\\alpha \\beta} &=\\left(\\mathbb{E}\\left[u u^{\\prime}\\right]\\right)^{-1}\\left(\\mathbb{E}\\left[u Z^{\\prime} e v\\right] \\boldsymbol{A}\\right)\\left(\\boldsymbol{A}^{\\prime} \\mathbb{E}\\left[Z Z^{\\prime}\\right] \\boldsymbol{A}\\right)^{-1} \\\\\n\\boldsymbol{V}_{\\alpha \\alpha} &=\\left(\\mathbb{E}\\left[u u^{\\prime}\\right]\\right)^{-1} \\mathbb{E}\\left[u u^{\\prime} v^{2}\\right]\\left(\\mathbb{E}\\left[u u^{\\prime}\\right]\\right)^{-1}\n\\end{aligned}\n\\]\nThe asymptotic covariance matrix is estimated by\n\\[\n\\begin{aligned}\n&\\widehat{\\boldsymbol{V}}_{\\beta \\beta}=\\left(\\frac{1}{n} \\widehat{\\boldsymbol{W}}^{\\prime} \\widehat{\\boldsymbol{W}}\\right)^{-1}\\left(\\frac{1}{n} \\sum_{i=1}^{n} \\widehat{W}_{i} \\widehat{W}_{i}^{\\prime} \\widehat{e}_{i}^{2}\\right)\\left(\\frac{1}{n} \\widehat{\\boldsymbol{W}}^{\\prime} \\widehat{\\boldsymbol{W}}\\right)^{-1} \\\\\n&\\widehat{\\boldsymbol{V}}_{\\alpha \\beta}=\\left(\\frac{1}{n} \\widehat{\\boldsymbol{U}}^{\\prime} \\widehat{\\boldsymbol{U}}\\right)^{-1}\\left(\\frac{1}{n} \\sum_{i=1}^{n} \\widehat{u}_{i} \\widehat{W}_{i}^{\\prime} \\widehat{e}_{i} \\widehat{v}_{i}\\right)\\left(\\frac{1}{n} \\widehat{\\boldsymbol{W}}^{\\prime} \\widehat{\\boldsymbol{W}}\\right)^{-1} \\\\\n&\\widehat{\\boldsymbol{V}}_{\\alpha \\alpha}=\\left(\\frac{1}{n} \\widehat{\\boldsymbol{U}}^{\\prime} \\widehat{\\boldsymbol{U}}\\right)^{-1}\\left(\\frac{1}{n} \\sum_{i=1}^{n} \\widehat{U}_{i} \\widehat{U}_{i}^{\\prime} \\widehat{v}_{i}^{2}\\right)\\left(\\frac{1}{n} \\widehat{\\boldsymbol{U}}^{\\prime} \\widehat{\\boldsymbol{U}}\\right)^{-1}\n\\end{aligned}\n\\]\nwhere\n\\[\n\\begin{aligned}\n\\widehat{W}_{i} &=\\widehat{A}^{\\prime} Z_{i} \\\\\n\\widehat{u}_{i} &=\\widehat{X}_{i}-\\widehat{W}_{i} \\\\\n\\widehat{e}_{i} &=Y_{i}-X_{i}^{\\prime} \\widehat{\\beta} \\\\\n\\widehat{v}_{i} &=Y_{i}-\\widehat{W}_{i}^{\\prime} \\widehat{\\beta}-\\widehat{u}_{i}^{\\prime} \\widehat{\\alpha} .\n\\end{aligned}\n\\]\nUnder conditional homoskedasticity, specifically\n\\[\n\\mathbb{E}\\left[\\left(\\begin{array}{cc}\ne_{i}^{2} & e_{i} v_{i} \\\\\ne_{i} v_{i} & v_{i}^{2}\n\\end{array}\\right) \\mid Z_{i}\\right]=\\boldsymbol{C}\n\\]\nthen \\(\\boldsymbol{V}_{\\alpha \\beta}=0\\) and the coefficient estimates \\(\\widehat{\\beta}\\) and \\(\\widehat{\\alpha}\\) are asymptotically independent. The variance components also simplify to\n\\[\n\\begin{aligned}\n\\boldsymbol{V}_{\\beta \\beta} &=\\left(\\boldsymbol{A}^{\\prime} \\mathbb{E}\\left[Z Z^{\\prime}\\right] \\boldsymbol{A}\\right)^{-1} \\mathbb{E}\\left[e_{i}^{2}\\right] \\\\\n\\boldsymbol{V}_{\\alpha \\alpha} &=\\left(\\mathbb{E}\\left[u u^{\\prime}\\right]\\right)^{-1} \\mathbb{E}\\left[v^{2}\\right] .\n\\end{aligned}\n\\]\nIn this case we have the covariance matrix estimators\n\\[\n\\begin{aligned}\n&\\widehat{\\boldsymbol{V}}_{\\beta \\beta}^{0}=\\left(\\frac{1}{n} \\widehat{\\boldsymbol{W}}^{\\prime} \\widehat{\\boldsymbol{W}}\\right)^{-1}\\left(\\frac{1}{n} \\sum_{i=1}^{n} \\widehat{e}_{i}^{2}\\right) \\\\\n&\\widehat{\\boldsymbol{V}}_{\\alpha \\alpha}^{0}=\\left(\\frac{1}{n} \\widehat{\\boldsymbol{U}}^{\\prime} \\widehat{\\boldsymbol{U}}\\right)^{-1}\\left(\\frac{1}{n} \\sum_{i=1}^{n} \\widehat{v}_{i}^{2}\\right)\n\\end{aligned}\n\\]\nand \\(\\widehat{\\boldsymbol{V}}_{\\alpha \\beta}^{0}=0\\)"
  },
  {
    "objectID": "chpt12-iv.html#control-function-regression",
    "href": "chpt12-iv.html#control-function-regression",
    "title": "12  Instrumental Variables",
    "section": "12.28 Control Function Regression",
    "text": "12.28 Control Function Regression\nIn this section we present an alternative way of computing the 2SLS estimator by least squares. It is useful in nonlinear contexts, and also in the linear model to construct tests for endogeneity.\nThe structural and reduced form equations for the standard IV model are\n\\[\n\\begin{aligned}\nY &=X_{1}^{\\prime} \\beta_{1}+X_{2}^{\\prime} \\beta_{2}+e \\\\\nX_{2} &=\\Gamma_{12}^{\\prime} Z_{1}+\\Gamma_{22}^{\\prime} Z_{2}+u_{2} .\n\\end{aligned}\n\\]\nSince the instrumental variable assumption specifies that \\(\\mathbb{E}[Z e]=0, X_{2}\\) is endogenous (correlated with \\(e)\\) if \\(u_{2}\\) and \\(e\\) are correlated. We can therefore consider the linear projection of \\(e\\) on \\(u_{2}\\)\n\\[\n\\begin{aligned}\ne &=u_{2}^{\\prime} \\alpha+v \\\\\n\\alpha &=\\left(\\mathbb{E}\\left[u_{2} u_{2}^{\\prime}\\right]\\right)^{-1} \\mathbb{E}\\left[u_{2} e\\right] \\\\\n\\mathbb{E}\\left[u_{2} v\\right] &=0 .\n\\end{aligned}\n\\]\nSubstituting this into the structural form equation we find\n\\[\n\\begin{aligned}\nY &=X_{1}^{\\prime} \\beta_{1}+X_{2}^{\\prime} \\beta_{2}+u_{2}^{\\prime} \\alpha+v \\\\\n\\mathbb{E}\\left[X_{1} v\\right] &=0 \\\\\n\\mathbb{E}\\left[X_{2} v\\right] &=0 \\\\\n\\mathbb{E}\\left[u_{2} v\\right] &=0 .\n\\end{aligned}\n\\]\nNotice that \\(X_{2}\\) is uncorrelated with \\(v\\). This is because \\(X_{2}\\) is correlated with \\(e\\) only through \\(u_{2}\\), and \\(v\\) is the error after \\(e\\) has been projected orthogonal to \\(u_{2}\\).\nIf \\(u_{2}\\) were observed we could then estimate (12.59) by least squares. Since it is not observed we estimate it by the reduced-form residual \\(\\widehat{u}_{2 i}=X_{2 i}-\\widehat{\\Gamma}_{12}^{\\prime} Z_{1 i}-\\widehat{\\Gamma}_{22}^{\\prime} Z_{2 i}\\). Then the coefficients \\(\\left(\\beta_{1}, \\beta_{2}, \\alpha\\right)\\) can be estimated by least squares of \\(Y\\) on \\(\\left(X_{1}, X_{2}, \\widehat{u}_{2}\\right)\\). We can write this as\n\\[\nY_{i}=X_{i}^{\\prime} \\widehat{\\beta}+\\widehat{u}_{2 i}^{\\prime} \\widehat{\\alpha}+\\widehat{v}_{i}\n\\]\nor in matrix notation as\n\\[\n\\boldsymbol{Y}=\\boldsymbol{X} \\widehat{\\beta}+\\widehat{\\boldsymbol{U}}_{2} \\widehat{\\alpha}+\\widehat{\\boldsymbol{v}} .\n\\]\nThis turns out to be an alternative algebraic expression for the 2SLS estimator.\nIndeed, we now show that \\(\\widehat{\\beta}=\\widehat{\\beta}_{2 s l s}\\). First, note that the reduced form residual can be written as\n\\[\n\\widehat{\\boldsymbol{U}}_{2}=\\left(\\boldsymbol{I}_{n}-\\boldsymbol{P}_{\\boldsymbol{Z}}\\right) \\boldsymbol{X}_{2}\n\\]\nwhere \\(\\boldsymbol{P}_{\\boldsymbol{Z}}\\) is defined in (12.30). By the FWL representation\n\\[\n\\widehat{\\beta}=\\left(\\widetilde{\\boldsymbol{X}}^{\\prime} \\widetilde{\\boldsymbol{X}}\\right)^{-1}\\left(\\widetilde{\\boldsymbol{X}}^{\\prime} \\boldsymbol{Y}\\right)\n\\]\nwhere \\(\\widetilde{\\boldsymbol{X}}=\\left[\\widetilde{\\boldsymbol{X}}_{1}, \\widetilde{\\boldsymbol{X}}_{2}\\right]\\) with\n\\[\n\\widetilde{\\boldsymbol{X}}_{1}=\\boldsymbol{X}_{1}-\\widehat{\\boldsymbol{U}}_{2}\\left(\\widehat{\\boldsymbol{U}}_{2}^{\\prime} \\widehat{\\boldsymbol{U}}_{2}\\right)^{-1} \\widehat{\\boldsymbol{U}}_{2}^{\\prime} \\boldsymbol{X}_{1}=\\boldsymbol{X}_{1}\n\\]\n(since \\(\\left.\\widehat{\\boldsymbol{U}}_{2}^{\\prime} \\boldsymbol{X}_{1}=0\\right)\\) and\n\\[\n\\begin{aligned}\n\\widetilde{\\boldsymbol{X}}_{2} &=\\boldsymbol{X}_{2}-\\widehat{\\boldsymbol{U}}_{2}\\left(\\widehat{\\boldsymbol{U}}_{2}^{\\prime} \\widehat{\\boldsymbol{U}}_{2}\\right)^{-1} \\widehat{\\boldsymbol{U}}_{2}^{\\prime} \\boldsymbol{X}_{2} \\\\\n&=\\boldsymbol{X}_{2}-\\widehat{\\boldsymbol{U}}_{2}\\left(\\boldsymbol{X}_{2}^{\\prime}\\left(\\boldsymbol{I}_{n}-\\boldsymbol{P}_{\\boldsymbol{Z}}\\right) \\boldsymbol{X}_{2}\\right)^{-1} \\boldsymbol{X}_{2}^{\\prime}\\left(\\boldsymbol{I}_{n}-\\boldsymbol{P}_{\\boldsymbol{Z}}\\right) \\boldsymbol{X}_{2} \\\\\n&=\\boldsymbol{X}_{2}-\\widehat{\\boldsymbol{U}}_{2} \\\\\n&=\\boldsymbol{P}_{\\boldsymbol{Z}} \\boldsymbol{X}_{2} .\n\\end{aligned}\n\\]\nThus \\(\\tilde{\\boldsymbol{X}}=\\left[\\boldsymbol{X}_{1}, \\boldsymbol{P}_{\\boldsymbol{Z}} \\boldsymbol{X}_{2}\\right]=\\boldsymbol{P}_{\\boldsymbol{Z}} \\boldsymbol{X}\\). Substituted into (12.61) we find\n\\[\n\\widehat{\\beta}=\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{P}_{\\boldsymbol{Z}} \\boldsymbol{X}\\right)^{-1}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{P}_{\\boldsymbol{Z}} \\boldsymbol{Y}\\right)=\\widehat{\\beta}_{2 \\text { sls }}\n\\]\nwhich is (12.31) as claimed.\nAgain, what we have found is that OLS estimation of equation (12.60) yields algebraically the 2SLS estimator \\(\\widehat{\\beta}_{2 \\text { sls }}\\).\nWe now consider the distribution of the control function estimator \\((\\widehat{\\beta}, \\widehat{\\alpha})\\). It is a generated regression model, and in fact is covered by the model examined in Section \\(12.27\\) after a slight reparametrization. Let \\(W=\\bar{\\Gamma}^{\\prime} Z\\). Note \\(u=X-W\\). Then the main equation (12.59) can be written as \\(Y=W^{\\prime} \\beta+u_{2}^{\\prime} \\gamma+v\\) where \\(\\gamma=\\alpha+\\beta_{2}\\). This is the model in Section 12.27.\nSet \\(\\widehat{\\gamma}=\\widehat{\\alpha}+\\widehat{\\beta}_{2}\\). It follows from (12.58) that as \\(n \\rightarrow \\infty\\) we have the joint distribution\n\\[\n\\sqrt{n}\\left(\\begin{array}{c}\n\\widehat{\\beta}_{2}-\\beta_{2} \\\\\n\\widehat{\\gamma}-\\gamma\n\\end{array}\\right) \\vec{d} \\mathrm{~N}(0, \\boldsymbol{V})\n\\]\nwhere\n\\[\n\\begin{aligned}\n\\boldsymbol{V}=\\left(\\begin{array}{ll}\n\\boldsymbol{V}_{22} & \\boldsymbol{V}_{2 \\gamma} \\\\\n\\boldsymbol{V}_{\\gamma 2} & \\boldsymbol{V}_{\\gamma \\gamma}\n\\end{array}\\right) \\\\\n\\boldsymbol{V}_{22} &=\\left[\\left(\\bar{\\Gamma}^{\\prime} \\mathbb{E}\\left[Z Z^{\\prime}\\right] \\bar{\\Gamma}\\right)^{-1} \\bar{\\Gamma}^{\\prime} \\mathbb{E}\\left[Z Z^{\\prime} e^{2}\\right] \\bar{\\Gamma}\\left(\\bar{\\Gamma}^{\\prime} \\mathbb{E}\\left[Z Z^{\\prime}\\right] \\bar{\\Gamma}\\right)^{-1}\\right]_{22} \\\\\n\\boldsymbol{V}_{\\gamma 2} &=\\left[\\left(\\mathbb{E}\\left[u_{2} u_{2}^{\\prime}\\right]\\right)^{-1} \\mathbb{E}\\left[u Z^{\\prime} e v\\right] \\bar{\\Gamma}\\left(\\bar{\\Gamma}^{\\prime} \\mathbb{E}\\left[Z Z^{\\prime}\\right] \\bar{\\Gamma}\\right)^{-1}\\right]_{\\cdot 2} \\\\\n\\boldsymbol{V}_{\\gamma \\gamma} &=\\left(\\mathbb{E}\\left[u_{2} u_{2}^{\\prime}\\right]\\right)^{-1} \\mathbb{E}\\left[u_{2} u_{2}^{\\prime} v^{2}\\right]\\left(\\mathbb{E}\\left[u_{2} u_{2}^{\\prime}\\right]\\right)^{-1} \\\\\ne &=Y-X^{\\prime} \\beta .\n\\end{aligned}\n\\]\nThe asymptotic distribution of \\(\\widehat{\\gamma}=\\widehat{\\alpha}-\\widehat{\\beta}_{2}\\) can be deduced.\nTheorem 12.13 If \\(\\mathbb{E}\\left[Y^{4}\\right]<\\infty, \\mathbb{E}\\|Z\\|^{4}<\\infty, \\mathbb{E}\\|X\\|^{4}<\\infty, A^{\\prime} \\mathbb{E}\\left[Z Z^{\\prime}\\right] A>0\\), and \\(\\mathbb{E}\\left[u u^{\\prime}\\right]>0\\), as \\(n \\rightarrow \\infty\\)\n\\[\n\\sqrt{n}(\\widehat{\\alpha}-\\alpha) \\underset{d}{\\longrightarrow} \\mathrm{N}\\left(0, \\boldsymbol{V}_{\\alpha}\\right)\n\\]\nwhere\n\\[\n\\boldsymbol{V}_{\\alpha}=\\boldsymbol{V}_{22}+\\boldsymbol{V}_{\\gamma \\gamma}-\\boldsymbol{V}_{\\gamma 2}-\\boldsymbol{V}_{\\gamma 2}^{\\prime} .\n\\]\nUnder conditional homoskedasticity we have the important simplifications\n\\[\n\\begin{aligned}\n\\boldsymbol{V}_{22} &=\\left[\\left(\\bar{\\Gamma}^{\\prime} \\mathbb{E}\\left[Z Z^{\\prime}\\right] \\bar{\\Gamma}\\right)^{-1}\\right]_{22} \\mathbb{E}\\left[e^{2}\\right] \\\\\n\\boldsymbol{V}_{\\gamma \\gamma} &=\\left(\\mathbb{E}\\left[u_{2} u_{2}^{\\prime}\\right]\\right)^{-1} \\mathbb{E}\\left[v^{2}\\right] \\\\\n\\boldsymbol{V}_{\\gamma 2} &=0 \\\\\n\\boldsymbol{V}_{\\alpha} &=\\boldsymbol{V}_{22}+\\boldsymbol{V}_{\\gamma \\gamma} .\n\\end{aligned}\n\\]\nAn estimator for \\(\\boldsymbol{V}_{\\alpha}\\) in the general case is\n\\[\n\\widehat{\\boldsymbol{V}}_{\\alpha}=\\widehat{\\boldsymbol{V}}_{22}+\\widehat{\\boldsymbol{V}}_{\\gamma \\gamma}-\\widehat{\\boldsymbol{V}}_{\\gamma 2}-\\widehat{\\boldsymbol{V}}_{\\gamma 2}^{\\prime}\n\\]\nwhere\n\\[\n\\begin{aligned}\n\\widehat{\\boldsymbol{V}}_{22} &=\\left[\\frac{1}{n}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{P}_{\\boldsymbol{Z}} \\boldsymbol{X}\\right)^{-1} \\boldsymbol{X}^{\\prime} \\boldsymbol{Z}\\left(\\boldsymbol{Z}^{\\prime} \\boldsymbol{Z}\\right)^{-1}\\left(\\sum_{i=1}^{n} Z_{i} Z_{i}^{\\prime} \\widehat{e}_{i}^{2}\\right)\\left(\\boldsymbol{Z}^{\\prime} \\boldsymbol{Z}\\right)^{-1} \\boldsymbol{Z}^{\\prime} \\boldsymbol{X}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{P}_{\\boldsymbol{Z}} \\boldsymbol{X}\\right)^{-1}\\right]_{22} \\\\\n\\widehat{\\boldsymbol{V}}_{\\gamma 2} &=\\left[\\frac{1}{n}\\left(\\widehat{\\boldsymbol{U}}^{\\prime} \\widehat{\\boldsymbol{U}}\\right)^{-1}\\left(\\sum_{i=1}^{n} \\widehat{u}_{i} \\widehat{W}_{i}^{\\prime} \\widehat{e}_{i} \\widehat{v}_{i}\\right)\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{P}_{\\boldsymbol{Z}} \\boldsymbol{X}\\right)^{-1}\\right]_{-2} \\\\\n\\widehat{e}_{i} &=Y_{i}-X_{i}^{\\prime} \\widehat{\\beta} \\\\\n\\widehat{v}_{i} &=Y_{i}-X_{i}^{\\prime} \\widehat{\\beta}-\\widehat{u}_{2 i}^{\\prime} \\widehat{\\gamma}\n\\end{aligned}\n\\]\nUnder the assumption of conditional homoskedasticity we have the estimator\n\\[\n\\begin{aligned}\n\\widehat{\\boldsymbol{V}}_{\\alpha}^{0} &=\\widehat{\\boldsymbol{V}}_{\\beta \\beta}^{0}+\\widehat{\\boldsymbol{V}}_{\\gamma \\gamma}^{0} \\\\\n\\widehat{\\boldsymbol{V}}_{\\beta \\beta} &=\\left[\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{P}_{\\boldsymbol{Z}} \\boldsymbol{X}\\right)^{-1}\\right]_{22}\\left(\\sum_{i=1}^{n} \\widehat{e}_{i}^{2}\\right) \\\\\n\\widehat{\\boldsymbol{V}}_{\\gamma \\gamma} &=\\left(\\widehat{\\boldsymbol{U}}^{\\prime} \\widehat{\\boldsymbol{U}}\\right)^{-1}\\left(\\sum_{i=1}^{n} \\widehat{v}_{i}^{2}\\right) .\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "chpt12-iv.html#endogeneity-tests",
    "href": "chpt12-iv.html#endogeneity-tests",
    "title": "12  Instrumental Variables",
    "section": "12.29 Endogeneity Tests",
    "text": "12.29 Endogeneity Tests\nThe 2SLS estimator allows the regressor \\(X_{2}\\) to be endogenous, meaning that \\(X_{2}\\) is correlated with the structural error \\(e\\). If this correlation is zero then \\(X_{2}\\) is exogenous and the structural equation can be estimated by least squares. This is a testable restriction. Effectively, the null hypothesis is\n\\[\n\\mathbb{H}_{0}: \\mathbb{E}\\left[X_{2} e\\right]=0\n\\]\nwith the alternative\n\\[\n\\mathbb{M}_{1}: \\mathbb{E}\\left[X_{2} e\\right] \\neq 0 .\n\\]\nThe maintained hypothesis is \\(\\mathbb{E}[Z e]=0\\). Since \\(X_{1}\\) is a component of \\(Z\\) this implies \\(\\mathbb{E}\\left[X_{1} e\\right]=0\\). Consequently we could alternatively write the null as \\(\\mathbb{H}_{0}: \\mathbb{E}[X e]=0\\) (and some authors do so).\nRecall the control function regression (12.59)\n\\[\n\\begin{aligned}\n&Y=X_{1}^{\\prime} \\beta_{1}+X_{2}^{\\prime} \\beta_{2}+u_{2}^{\\prime} \\alpha+v \\\\\n&\\alpha=\\left(\\mathbb{E}\\left[u_{2} u_{2}^{\\prime}\\right]\\right)^{-1} \\mathbb{E}\\left[u_{2} e\\right]\n\\end{aligned}\n\\]\nNotice that \\(\\mathbb{E}\\left[X_{2} e\\right]=0\\) if and only if \\(\\mathbb{E}\\left[u_{2} e\\right]=0\\), so the hypothesis can be restated as \\(\\mathbb{H}_{0}: \\alpha=0\\) against \\(\\mathbb{H}_{1}: \\alpha \\neq 0\\). Thus a natural test is based on the Wald statistic \\(W\\) for \\(\\alpha=0\\) in the control function regression (12.28). Under Theorem 12.9, Theorem \\(12.10\\), and \\(\\mathbb{M}_{0}, W\\) is asymptotically chi-square with \\(k_{2}\\) degrees of freedom. In addition, under the normal regression assumption the \\(F\\) statistic has an exact \\(F\\left(k_{2}, n-\\right.\\) \\(k_{1}-2 k_{2}\\) ) distribution. We accept the null hypothesis that \\(X_{2}\\) is exogenous if \\(W\\) (or F) is smaller than the critical value, and reject in favor of the hypothesis that \\(X_{2}\\) is endogenous if the statistic is larger than the critical value.\nSpecifically, estimate the reduced form by least squares\n\\[\nX_{2 i}=\\widehat{\\Gamma}_{12}^{\\prime} Z_{1 i}+\\widehat{\\Gamma}_{22}^{\\prime} Z_{2 i}+\\widehat{u}_{2 i}\n\\]\nto obtain the residuals. Then estimate the control function by least squares\n\\[\nY_{i}=X_{i}^{\\prime} \\widehat{\\beta}+\\widehat{u}_{2 i}^{\\prime} \\widehat{\\alpha}+\\widehat{v}_{i} .\n\\]\nLet \\(W, W^{0}\\) and \\(F=W^{0} / k_{2}\\) denote the Wald, homoskedastic Wald, and \\(F\\) statistics for \\(\\alpha=0\\).\nTheorem 12.14 Under \\(\\mathbb{M}_{0}, W \\underset{d}{\\longrightarrow} \\chi_{k_{2}}^{2}\\). Let \\(c_{1-\\alpha}\\) solve \\(\\mathbb{P}\\left[\\chi_{k_{2}}^{2} \\leq c_{1-\\alpha}\\right]=1-\\alpha\\). The test “Reject \\(\\mathbb{M}_{0}\\) if \\(W>c_{1-\\alpha}\\)” has asymptotic size \\(\\alpha\\).\nTheorem 12.15 Suppose \\(e \\mid X, Z \\sim \\mathrm{N}\\left(0, \\sigma^{2}\\right)\\). Under \\(\\mathbb{H}_{0}, \\mathrm{~F} \\sim F\\left(k_{2}, n-k_{1}-2 k_{2}\\right)\\). Let \\(c_{1-\\alpha}\\) solve \\(\\mathbb{P}\\left[F\\left(k_{2}, n-k_{1}-2 k_{2}\\right) \\leq c_{1-\\alpha}\\right]=1-\\alpha\\). The test “Reject \\(\\mathbb{H}_{0}\\) if \\(\\mathrm{F}>\\) \\(c_{1-\\alpha}\\)” has exact size \\(\\alpha\\).\nSince in general we do not want to impose homoskedasticity these results suggest that the most appropriate test is the Wald statistic constructed with the robust heteroskedastic covariance matrix. This can be computed in Stata using the command estat endogenous after ivregress when the latter uses a robust covariance option. Stata reports the Wald statistic in \\(F\\) form (and thus uses the \\(F\\) distribution to calculate the p-value) as “Robust regression F”. Using the \\(F\\) rather than the \\(\\chi^{2}\\) is not formally justified but is a reasonable finite sample adjustment. If the command estat endogenous is applied after ivregress without a robust covariance option Stata reports the \\(F\\) statistic as “Wu-Hausman F”.\nThere is an alternative (and traditional) way to derive a test for endogeneity. Under \\(\\mathbb{M}_{0}\\), both OLS and 2 SLS are consistent estimators. But under \\(\\mathbb{M}_{1}\\) they converge to different values. Thus the difference between the OLS and 2SLS estimators is a valid test statistic for endogeneity. It also measures what we often care most about - the impact of endogeneity on the parameter estimates. This literature was developed under the assumption of conditional homoskedasticity (and it is important for these results) so we assume this condition for the development of the statistic.\nLet \\(\\widehat{\\beta}=\\left(\\widehat{\\beta}_{1}, \\widehat{\\beta}_{2}\\right)\\) be the OLS estimator and let \\(\\widetilde{\\beta}=\\left(\\widetilde{\\beta}_{1}, \\widetilde{\\beta}_{2}\\right)\\) be the 2SLS estimator. Under \\(\\mathbb{H}_{0}\\) and homoskedasticity the OLS estimator is Gauss-Markov efficient so by the Hausman equality\n\\[\n\\begin{aligned}\n\\operatorname{var}\\left[\\widehat{\\beta}_{2}-\\widetilde{\\beta}_{2}\\right] &=\\operatorname{var}\\left[\\widetilde{\\beta}_{2}\\right]-\\operatorname{var}\\left[\\widehat{\\beta}_{2}\\right] \\\\\n&=\\left(\\left(\\boldsymbol{X}_{2}^{\\prime}\\left(\\boldsymbol{P}_{\\boldsymbol{Z}}-\\boldsymbol{P}_{1}\\right) \\boldsymbol{X}_{2}\\right)^{-1}-\\left(\\boldsymbol{X}_{2}^{\\prime} \\boldsymbol{M}_{1} \\boldsymbol{X}_{2}\\right)^{-1}\\right) \\sigma^{2}\n\\end{aligned}\n\\]\nwhere \\(\\boldsymbol{P}_{\\boldsymbol{Z}}=\\boldsymbol{Z}\\left(\\boldsymbol{Z}^{\\prime} \\boldsymbol{Z}\\right)^{-1} \\boldsymbol{Z}^{\\prime}, \\boldsymbol{P}_{1}=\\boldsymbol{X}_{1}\\left(\\boldsymbol{X}_{1}^{\\prime} \\boldsymbol{X}_{1}\\right)^{-1} \\boldsymbol{X}_{1}^{\\prime}\\), and \\(\\boldsymbol{M}_{1}=\\boldsymbol{I}_{n}-\\boldsymbol{P}_{1}\\). Thus a valid test statistic for \\(\\mathbb{H}_{0}\\) is\n\\[\nT=\\frac{\\left(\\widehat{\\beta}_{2}-\\widetilde{\\beta}_{2}\\right)^{\\prime}\\left(\\left(\\boldsymbol{X}_{2}^{\\prime}\\left(\\boldsymbol{P}_{\\boldsymbol{Z}}-\\boldsymbol{P}_{1}\\right) \\boldsymbol{X}_{2}\\right)^{-1}-\\left(\\boldsymbol{X}_{2}^{\\prime} \\boldsymbol{M}_{1} \\boldsymbol{X}_{2}\\right)^{-1}\\right)^{-1}\\left(\\widehat{\\beta}_{2}-\\widetilde{\\beta}_{2}\\right)}{\\widehat{\\sigma}^{2}}\n\\]\nfor some estimator \\(\\widehat{\\sigma}^{2}\\) of \\(\\sigma^{2}\\). Durbin (1954) first proposed \\(T\\) as a test for endogeneity in the context of IV estimation setting \\(\\widehat{\\sigma}^{2}\\) to be the least squares estimator of \\(\\sigma^{2}\\). Wu (1973) proposed \\(T\\) as a test for endogeneity in the context of 2SLS estimation, considering a set of possible estimators \\(\\widehat{\\sigma}^{2}\\) including the regression estimator from (12.63). Hausman (1978) proposed a version of \\(T\\) based on the full contrast \\(\\widehat{\\beta}-\\widetilde{\\beta}\\), and observed that it equals the regression Wald statistic \\(W^{0}\\) described earlier. In fact, when \\(\\widehat{\\sigma}^{2}\\) is the regression estimator from (12.63) the statistic (12.64) algebraically equals both \\(W^{0}\\) and the version of (12.64) based on the full contrast \\(\\widehat{\\beta}-\\widetilde{\\beta}\\). We show these equalities below. Thus these three approaches yield exactly the same statistic except for possible differences regarding the choice of \\(\\widehat{\\sigma}^{2}\\). Since the regression \\(F\\) test described earlier has an exact \\(F\\) distribution in the normal sampling model and thus can exactly control test size, this is the preferred version of the test. The general class of tests are called Durbin-Wu-Hausman tests, Wu-Hausman tests, or Hausman tests, depending on the author.\nWhen \\(k_{2}=1\\) (there is one right-hand-side endogenous variable), which is quite common in applications, the endogeneity test can be equivalently expressed at the t-statistic for \\(\\widehat{\\alpha}\\) in the estimated control function. Thus it is sufficient to estimate the control function regression and check the t-statistic for \\(\\widehat{\\alpha}\\). If \\(|\\widehat{\\alpha}|>2\\) then we can reject the hypothesis that \\(X_{2}\\) is exogenous for \\(\\beta\\).\nWe illustrate using the Card proximity example using the two instruments public and private. We first estimate the reduced form for education, obtain the residual, and then estimate the control function regression. The residual has a coefficient \\(-0.088\\) with a standard error of \\(0.037\\) and a t-statistic of 2.4. Since the latter exceeds the \\(5 %\\) critical value (its p-value is \\(0.017\\) ) we reject exogeneity. This means that the 2SLS estimates are statistically different from the least squares estimates of the structural equation and supports our decision to treat education as an endogenous variable. (Alternatively, the \\(F\\) statistic is \\(2.4^{2}=5.7\\) with the same p-value).\nWe now show the equality of the various statistics.\nWe first show that the statistic (12.64) is not altered if based on the full contrast \\(\\widehat{\\beta}-\\widetilde{\\beta}\\). Indeed, \\(\\widehat{\\beta}_{1}-\\widetilde{\\beta}_{1}\\) is a linear function of \\(\\widehat{\\beta}_{2}-\\widetilde{\\beta}_{2}\\), so there is no extra information in the full contrast. To see this, observe that given \\(\\widehat{\\beta}_{2}\\) we can solve by least squares to find\n\\[\n\\widehat{\\beta}_{1}=\\left(\\boldsymbol{X}_{1}^{\\prime} \\boldsymbol{X}_{1}\\right)^{-1}\\left(\\boldsymbol{X}_{1}^{\\prime}\\left(\\boldsymbol{Y}-\\boldsymbol{X}_{2} \\widehat{\\beta}_{2}\\right)\\right)\n\\]\nand similarly\n\\[\n\\widetilde{\\beta}_{1}=\\left(\\boldsymbol{X}_{1}^{\\prime} \\boldsymbol{X}_{1}\\right)^{-1}\\left(\\boldsymbol{X}_{1}^{\\prime}\\left(\\boldsymbol{Y}-\\boldsymbol{P}_{\\boldsymbol{Z}} \\boldsymbol{X}_{2} \\widetilde{\\beta}\\right)\\right)=\\left(\\boldsymbol{X}_{1}^{\\prime} \\boldsymbol{X}_{1}\\right)^{-1}\\left(\\boldsymbol{X}_{1}^{\\prime}\\left(\\boldsymbol{Y}-\\boldsymbol{X}_{2} \\widetilde{\\beta}\\right)\\right)\n\\]\nthe second equality because \\(\\boldsymbol{P}_{\\boldsymbol{Z}} \\boldsymbol{X}_{1}=\\boldsymbol{X}_{1}\\). Thus\n\\[\n\\begin{aligned}\n\\widehat{\\beta}_{1}-\\widetilde{\\beta}_{1} &=\\left(\\boldsymbol{X}_{1}^{\\prime} \\boldsymbol{X}_{1}\\right)^{-1} \\boldsymbol{X}_{1}^{\\prime}\\left(\\boldsymbol{Y}-\\boldsymbol{X}_{2} \\widehat{\\beta}_{2}\\right)-\\left(\\boldsymbol{X}_{1}^{\\prime} \\boldsymbol{X}_{1}\\right)^{-1} \\boldsymbol{X}_{1}^{\\prime}\\left(\\boldsymbol{Y}-\\boldsymbol{P}_{\\boldsymbol{Z}} \\boldsymbol{X}_{2} \\widetilde{\\beta}\\right) \\\\\n&=\\left(\\boldsymbol{X}_{1}^{\\prime} \\boldsymbol{X}_{1}\\right)^{-1} \\boldsymbol{X}_{1}^{\\prime} \\boldsymbol{X}_{2}\\left(\\widetilde{\\beta}_{2}-\\widehat{\\beta}_{2}\\right)\n\\end{aligned}\n\\]\nas claimed.\nWe next show that \\(T\\) in (12.64) equals the homoskedastic Wald statistic \\(W^{0}\\) for \\(\\widehat{\\alpha}\\) from the regression (12.63). Consider the latter regression. Since \\(\\boldsymbol{X}_{2}\\) is contained in \\(\\boldsymbol{X}\\) the coefficient estimate \\(\\widehat{\\alpha}\\) is invariant to replacing \\(\\widehat{\\boldsymbol{U}}_{2}=\\boldsymbol{X}_{2}-\\widehat{\\boldsymbol{X}}_{2}\\) with \\(-\\widehat{\\boldsymbol{X}}_{2}=-\\boldsymbol{P}_{\\boldsymbol{Z}} \\boldsymbol{X}_{2}\\). By the FWL representation, setting \\(\\boldsymbol{M}_{\\boldsymbol{X}}=\\) \\(\\boldsymbol{I}_{n}-\\boldsymbol{X}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\boldsymbol{X}^{\\prime}\\)\n\\[\n\\widehat{\\alpha}=-\\left(\\widehat{\\boldsymbol{X}}_{2}^{\\prime} \\boldsymbol{M}_{\\boldsymbol{X}} \\widehat{\\boldsymbol{X}}_{2}\\right)^{-1} \\widehat{\\boldsymbol{X}}_{2}^{\\prime} \\boldsymbol{M}_{\\boldsymbol{X}} \\boldsymbol{Y}=-\\left(\\boldsymbol{X}_{2}^{\\prime} \\boldsymbol{P}_{\\boldsymbol{Z}} \\boldsymbol{M}_{\\boldsymbol{X}} \\boldsymbol{P}_{\\boldsymbol{Z}} \\boldsymbol{X}_{2}\\right)^{-1} \\boldsymbol{X}_{2}^{\\prime} \\boldsymbol{P}_{\\boldsymbol{Z}} \\boldsymbol{M}_{\\boldsymbol{X}} \\boldsymbol{Y}\n\\]\nIt follows that\n\\[\nW^{0}=\\frac{\\boldsymbol{Y}^{\\prime} \\boldsymbol{M}_{\\boldsymbol{X}} \\boldsymbol{P}_{\\boldsymbol{Z}} \\boldsymbol{X}_{2}\\left(\\boldsymbol{X}_{2}^{\\prime} \\boldsymbol{P}_{\\boldsymbol{Z}} \\boldsymbol{M}_{\\boldsymbol{X}} \\boldsymbol{P}_{\\boldsymbol{Z}} \\boldsymbol{X}_{2}\\right)^{-1} \\boldsymbol{X}_{2}^{\\prime} \\boldsymbol{P}_{\\boldsymbol{Z}} \\boldsymbol{M}_{\\boldsymbol{X}} \\boldsymbol{Y}}{\\widehat{\\sigma}^{2}} .\n\\]\nOur goal is to show that \\(T=W^{0}\\). Define \\(\\widetilde{\\boldsymbol{X}}_{2}=\\left(\\boldsymbol{I}_{n}-\\boldsymbol{P}_{1}\\right) \\boldsymbol{X}_{2}\\) so \\(\\widehat{\\beta}_{2}=\\left(\\widetilde{\\boldsymbol{X}}_{2}^{\\prime} \\widetilde{\\boldsymbol{X}}_{2}\\right)^{-1} \\widetilde{\\boldsymbol{X}}_{2}^{\\prime} \\boldsymbol{Y}\\). Then using \\(\\left(\\boldsymbol{P}_{\\boldsymbol{Z}}-\\boldsymbol{P}_{1}\\right)\\left(\\boldsymbol{I}_{n}-\\boldsymbol{P}_{1}\\right)=\\left(\\boldsymbol{P}_{\\boldsymbol{Z}}-\\boldsymbol{P}_{1}\\right)\\) and defining \\(\\boldsymbol{Q}=\\widetilde{\\boldsymbol{X}}_{2}\\left(\\widetilde{\\boldsymbol{X}}_{2}^{\\prime} \\tilde{\\boldsymbol{X}}_{2}\\right)^{-1} \\widetilde{\\boldsymbol{X}}_{2}^{\\prime}\\) we find\n\\[\n\\begin{aligned}\n&\\boldsymbol{\\Delta} \\stackrel{\\text { def }}{=}\\left(\\boldsymbol{X}_{2}^{\\prime}\\left(\\boldsymbol{P}_{\\boldsymbol{Z}}-\\boldsymbol{P}_{1}\\right) \\boldsymbol{X}_{2}\\right)\\left(\\widetilde{\\beta}_{2}-\\widehat{\\beta}_{2}\\right) \\\\\n&=\\boldsymbol{X}_{2}^{\\prime}\\left(\\boldsymbol{P}_{\\boldsymbol{Z}}-\\boldsymbol{P}_{1}\\right) \\boldsymbol{Y}-\\left(\\boldsymbol{X}_{2}^{\\prime}\\left(\\boldsymbol{P}_{\\boldsymbol{Z}}-\\boldsymbol{P}_{1}\\right) \\boldsymbol{X}_{2}\\right)\\left(\\widetilde{\\boldsymbol{X}}_{2}^{\\prime} \\widetilde{\\boldsymbol{X}}_{2}\\right)^{-1} \\widetilde{\\boldsymbol{X}}_{2}^{\\prime} \\boldsymbol{Y} \\\\\n&=\\boldsymbol{X}_{2}^{\\prime}\\left(\\boldsymbol{P}_{\\boldsymbol{Z}}-\\boldsymbol{P}_{1}\\right)\\left(\\boldsymbol{I}_{n}-\\boldsymbol{Q}\\right) \\boldsymbol{Y} \\\\\n&=\\boldsymbol{X}_{2}^{\\prime}\\left(\\boldsymbol{P}_{\\boldsymbol{Z}}-\\boldsymbol{P}_{1}-\\boldsymbol{P}_{\\boldsymbol{Z}} \\boldsymbol{Q}\\right) \\boldsymbol{Y} \\\\\n&=\\boldsymbol{X}_{2}^{\\prime} \\boldsymbol{P}_{\\boldsymbol{Z}}\\left(\\boldsymbol{I}_{n}-\\boldsymbol{P}_{1}-\\boldsymbol{Q}\\right) \\boldsymbol{Y} \\\\\n&=\\boldsymbol{X}_{2}^{\\prime} \\boldsymbol{P}_{\\boldsymbol{Z}} \\boldsymbol{M}_{\\boldsymbol{X}} \\boldsymbol{Y} .\n\\end{aligned}\n\\]\nThe third-to-last equality is \\(\\boldsymbol{P}_{1} \\boldsymbol{Q}=0\\) and the final uses \\(\\boldsymbol{M}_{\\boldsymbol{X}}=\\boldsymbol{I}_{n}-\\boldsymbol{P}_{1}-\\boldsymbol{Q}\\). We also calculate that\n\\[\n\\begin{aligned}\n&\\boldsymbol{Q}^{*} \\stackrel{\\text { def }}{=}\\left(\\boldsymbol{X}_{2}^{\\prime}\\left(\\boldsymbol{P}_{\\boldsymbol{Z}}-\\boldsymbol{P}_{1}\\right) \\boldsymbol{X}_{2}\\right)\\left(\\left(\\boldsymbol{X}_{2}^{\\prime}\\left(\\boldsymbol{P}_{\\boldsymbol{Z}}-\\boldsymbol{P}_{1}\\right) \\boldsymbol{X}_{2}\\right)^{-1}-\\left(\\boldsymbol{X}_{2}^{\\prime} \\boldsymbol{M}_{1} \\boldsymbol{X}_{2}\\right)^{-1}\\right)\\left(\\boldsymbol{X}_{2}^{\\prime}\\left(\\boldsymbol{P}_{\\boldsymbol{Z}}-\\boldsymbol{P}_{1}\\right) \\boldsymbol{X}_{2}\\right) \\\\\n&=\\boldsymbol{X}_{2}^{\\prime}\\left(\\boldsymbol{P}_{\\boldsymbol{Z}}-\\boldsymbol{P}_{1}-\\left(\\boldsymbol{P}_{\\boldsymbol{Z}}-\\boldsymbol{P}_{1}\\right) \\boldsymbol{Q}\\left(\\boldsymbol{P}_{\\boldsymbol{Z}}-\\boldsymbol{P}_{1}\\right)\\right) \\boldsymbol{X}_{2} \\\\\n&=\\boldsymbol{X}_{2}^{\\prime}\\left(\\boldsymbol{P}_{\\boldsymbol{Z}}-\\boldsymbol{P}_{1}-\\boldsymbol{P}_{\\boldsymbol{Z}} \\boldsymbol{Q} \\boldsymbol{P}_{\\boldsymbol{Z}}\\right) \\boldsymbol{X}_{2} \\\\\n&=\\boldsymbol{X}_{2}^{\\prime} \\boldsymbol{P}_{\\boldsymbol{Z}} \\boldsymbol{M}_{\\boldsymbol{X}} \\boldsymbol{P}_{\\boldsymbol{Z}} \\boldsymbol{X}_{2} .\n\\end{aligned}\n\\]\nThus\n\\[\n\\begin{aligned}\nT &=\\frac{\\boldsymbol{\\Delta}^{\\prime} \\boldsymbol{Q}^{*-1} \\boldsymbol{\\Delta}}{\\widehat{\\sigma}^{2}} \\\\\n&=\\frac{\\boldsymbol{Y}^{\\prime} \\boldsymbol{M}_{\\boldsymbol{X}} \\boldsymbol{P}_{\\boldsymbol{Z}} \\boldsymbol{X}_{2}\\left(\\boldsymbol{X}_{2}^{\\prime} \\boldsymbol{P}_{\\boldsymbol{Z}} \\boldsymbol{M}_{\\boldsymbol{X}} \\boldsymbol{P}_{\\boldsymbol{Z}} \\boldsymbol{X}_{2}\\right)^{-1} \\boldsymbol{X}_{2}^{\\prime} \\boldsymbol{P}_{\\boldsymbol{Z}} \\boldsymbol{M}_{\\boldsymbol{X}} \\boldsymbol{Y}}{\\widehat{\\sigma}^{2}} \\\\\n&=W^{0}\n\\end{aligned}\n\\]\nas claimed."
  },
  {
    "objectID": "chpt12-iv.html#subset-endogeneity-tests",
    "href": "chpt12-iv.html#subset-endogeneity-tests",
    "title": "12  Instrumental Variables",
    "section": "12.30 Subset Endogeneity Tests",
    "text": "12.30 Subset Endogeneity Tests\nIn some cases we may only wish to test the endogeneity of a subset of the variables. In the Card proximity example we may wish test the exogeneity of education separately from experience and its square. To execute a subset endogeneity test it is useful to partition the regressors into three groups so that the structural model is\n\\[\n\\begin{aligned}\nY &=X_{1}^{\\prime} \\beta_{1}+X_{2}^{\\prime} \\beta_{2}+X_{3}^{\\prime} \\beta_{3}+e \\\\\n\\mathbb{E}[Z e] &=0 .\n\\end{aligned}\n\\]\nAs before, the instrument vector \\(Z\\) includes \\(X_{1}\\). The vector \\(X_{3}\\) is treated as endogenous and \\(X_{2}\\) is treated as potentially endogenous. The hypothesis to test is that \\(X_{2}\\) is exogenous, or \\(\\mathbb{H}_{0}: \\mathbb{E}\\left[X_{2} e\\right]=0\\) against \\(\\mathbb{H}_{1}: \\mathbb{E}\\left[X_{2} e\\right] \\neq 0\\)\nUnder homoskedasticity a straightfoward test can be constructed by the Durbin-Wu-Hausman principle. Under \\(\\mathbb{M}_{0}\\) the appropriate estimator is \\(2 \\mathrm{SLS}\\) using the instruments \\(\\left(Z, X_{2}\\right)\\). Let this estimator of \\(\\beta_{2}\\) be denoted \\(\\widehat{\\beta}_{2}\\). Under \\(\\mathbb{H}_{1}\\) the appropriate estimator is 2SLS using the smaller instrument set \\(Z\\). Let this estimator of \\(\\beta_{2}\\) be denoted \\(\\widetilde{\\beta}_{2}\\). A Durbin-Wu-Hausman statistic for \\(\\mathbb{H}_{0}\\) against \\(\\mathbb{H}_{1}\\) is\n\\[\nT=\\left(\\widehat{\\beta}_{2}-\\widetilde{\\beta}_{2}\\right)^{\\prime}\\left(\\widehat{\\operatorname{var}}\\left[\\widetilde{\\beta}_{2}\\right]-\\widehat{\\operatorname{var}}\\left[\\widehat{\\beta}_{2}\\right]\\right)^{-1}\\left(\\widehat{\\beta}_{2}-\\widetilde{\\beta}_{2}\\right) .\n\\]\nThe asymptotic distribution under \\(\\mathbb{H}_{0}\\) is \\(\\chi_{k_{2}}^{2}\\) where \\(k_{2}=\\operatorname{dim}\\left(X_{2}\\right)\\), so we reject the hypothesis that the variables \\(X_{2}\\) are exogenous if \\(T\\) exceeds an upper critical value from the \\(\\chi_{k_{2}}^{2}\\) distribution.\nInstead of using the Wald statistic one could use the \\(F\\) version of the test by dividing by \\(k_{2}\\) and using the \\(F\\) distribution for critical values. There is no finite sample justification for this modification, however, since \\(X_{3}\\) is endogenous under the null hypothesis.\nIn Stata, the command estat endogenous (adding the variable name to specify which variable to test for exogeneity) after ivregress without a robust covariance option reports the \\(F\\) version of this statistic as “Wu-Hausman F”. For example, in the Card proximity example using the four instruments public, private, age, and \\(a g e^{2}\\), if we estimate the equation by 2SLS with a non-robust covariance matrix and then compute the endogeneity test for education we find \\(F=272\\) with a p-value of \\(0.0000\\), but if we compute the test for experience and its square we find \\(F=2.98\\) with a p-value of \\(0.051\\). In this model, the assumption of exogeneity with homogenous coefficients is rejected for education but the result for experience is unclear.\nA heteroskedasticity or cluster-robust test cannot be constructed easily by the Durbin-Wu-Hausman approach since the covariance matrix does not take a simple form. To allow for non-homoskedastic errors it is recommended to use GMM estimation. See Section 13.24."
  },
  {
    "objectID": "chpt12-iv.html#overidentification-tests",
    "href": "chpt12-iv.html#overidentification-tests",
    "title": "12  Instrumental Variables",
    "section": "12.31 OverIdentification Tests",
    "text": "12.31 OverIdentification Tests\nWhen \\(\\ell>k\\) the model is overidentified meaning that there are more moments than free parameters. This is a restriction and is testable. Such tests are called overidentification tests.\nThe instrumental variables model specifies \\(\\mathbb{E}[Z e]=0\\). Equivalently, since \\(e=Y-X^{\\prime} \\beta\\) this is\n\\[\n\\mathbb{E}[Z Y]-\\mathbb{E}\\left[Z X^{\\prime}\\right] \\beta=0 .\n\\]\nThis is an \\(\\ell \\times 1\\) vector of restrictions on the moment matrices \\(\\mathbb{E}[Z Y]\\) and \\(\\mathbb{E}\\left[Z X^{\\prime}\\right]\\). Yet since \\(\\beta\\) is of dimension \\(k\\) which is less than \\(\\ell\\) it is not certain if indeed such a \\(\\beta\\) exists.\nTo make things a bit more concrete, suppose there is a single endogenous regressor \\(X_{2}\\), no \\(X_{1}\\), and two instruments \\(Z_{1}\\) and \\(Z_{2}\\). Then the model specifies that\n\\[\n\\mathbb{E}\\left(\\left[Z_{1} Y\\right]=\\mathbb{E}\\left[Z_{1} X_{2}\\right] \\beta\\right.\n\\]\nand\n\\[\n\\mathbb{E}\\left[Z_{2} Y\\right]=\\mathbb{E}\\left[Z_{2} X_{2}\\right] \\beta .\n\\]\nThus \\(\\beta\\) solves both equations. This is rather special.\nAnother way of thinking about this is we could solve for \\(\\beta\\) using either one equation or the other. In terms of estimation this is equivalent to estimating by IV using just the instrument \\(Z_{1}\\) or instead just using the instrument \\(Z_{2}\\). These two estimators (in finite samples) are different. If the overidentification hypothesis is correct both are estimating the same parameter and both are consistent for \\(\\beta\\). In contrast, if the overidentification hypothesis is false then the two estimators will converge to different probability limits and it is unclear if either probability limit is interesting.\nFor example, take the 2SLS estimates in the fourth column of Table \\(12.1\\) which use public and private as instruments for education. Suppose we instead estimate by IV using just public as an instrument and then repeat using private. The IV coefficient for education in the first case is \\(0.16\\) and in the second case 0.27. These appear to be quite different. However, the second estimate has a large standard error (0.16) so the difference may be sampling variation. An overidentification test addresses this question.\nFor a general overidentification test the null and alternative hypotheses are \\(\\mathbb{H}_{0}: \\mathbb{E}[Z e]=0\\) against \\(\\mathbb{H}_{1}: \\mathbb{E}[Z e] \\neq 0\\). We will also add the conditional homoskedasticity assumption\n\\[\n\\mathbb{E}\\left[e^{2} \\mid Z\\right]=\\sigma^{2} .\n\\]\nTo avoid (12.65) it is best to take a GMM approach which we defer until Chapter \\(13 .\\)\nTo implement a test of \\(\\mathbb{M}_{0}\\) consider a linear regression of the error \\(e\\) on the instruments \\(Z\\)\n\\[\ne=Z^{\\prime} \\alpha+v\n\\]\nwith \\(\\alpha=\\left(\\mathbb{E}\\left[Z Z^{\\prime}\\right]\\right)^{-1} \\mathbb{E}[Z e]\\). We can rewrite \\(\\mathbb{H}_{0}\\) as \\(\\alpha=0\\). While \\(e\\) is not observed we can replace it with the 2SLS residual \\(\\widehat{e}_{i}\\) and estimate \\(\\alpha\\) by least squares regression, e.g. \\(\\widehat{\\alpha}=\\left(Z^{\\prime} \\boldsymbol{Z}\\right)^{-1} Z^{\\prime} \\widehat{\\boldsymbol{e}}\\). Sargan (1958) proposed testing \\(\\mathbb{M}_{0}\\) via a score test, which equals\n\\[\nS=\\widehat{\\alpha}^{\\prime}(\\widehat{\\operatorname{var}}[\\widehat{\\alpha}])^{-} \\widehat{\\alpha}=\\frac{\\widehat{\\boldsymbol{e}}^{\\prime} \\boldsymbol{Z}\\left(\\boldsymbol{Z}^{\\prime} \\boldsymbol{Z}\\right)^{-1} \\boldsymbol{Z}^{\\prime} \\widehat{\\boldsymbol{e}}}{\\widehat{\\sigma}^{2}} .\n\\]\nwhere \\(\\widehat{\\sigma}^{2}=\\frac{1}{n} \\widehat{\\boldsymbol{e}} \\widehat{\\boldsymbol{e}}\\). Basmann (1960) independently proposed a Wald statistic for \\(\\mathbb{H}_{0}\\), which is \\(S\\) with \\(\\widehat{\\sigma}^{2}\\) replaced with \\(\\widetilde{\\sigma}^{2}=n^{-1} \\widehat{\\boldsymbol{v}} ' \\widehat{\\boldsymbol{v}}\\) where \\(\\widehat{\\boldsymbol{v}}=\\widehat{\\boldsymbol{e}}-\\boldsymbol{Z} \\widehat{\\alpha}\\). By the equivalence of homoskedastic score and Wald tests (see Section 9.16) Basmann’s statistic is a monotonic function of Sargan’s statistic and hence they yield equivalent tests. Sargan’s version is more typically reported.\nThe Sargan test rejects \\(\\mathbb{H}_{0}\\) in favor of \\(\\mathbb{M}_{1}\\) if \\(S>c\\) for some critical value \\(c\\). An asymptotic test sets \\(c\\) as the \\(1-\\alpha\\) quantile of the \\(\\chi_{\\ell-k}^{2}\\) distribution. This is justified by the asymptotic null distribution of \\(S\\) which we now derive.\nTheorem 12.16 Under Assumption \\(12.2\\) and \\(\\mathbb{E}\\left[e^{2} \\mid Z\\right]=\\sigma^{2}\\), then as \\(n \\rightarrow \\infty\\), \\(S \\underset{d}{\\longrightarrow} \\chi_{\\ell-k}^{2}\\). For \\(c\\) satisfying \\(\\alpha=1-G_{\\ell-k}(c), \\mathbb{P}\\left[S>c \\mid \\mathbb{H}_{0}\\right] \\rightarrow \\alpha\\) so the test “Reject\n\\(\\mathbb{M}_{0}\\) if \\(S>c\\) ” has asymptotic size \\(\\alpha\\).\nWe prove Theorem \\(12.16\\) below.\nThe Sargan statistic \\(S\\) is an asymptotic test of the overidentifying restrictions under the assumption of conditional homoskedasticity. It has some limitations. First, it is an asymptotic test and does not have a finite sample (e.g. F) counterpart. Simulation evidence suggests that the test can be oversized (reject too frequently) in small and moderate sample sizes. Consequently, p-values should be interpreted cautiously. Second, the assumption of conditional homoskedasticity is unrealistic in applications. The best way to generalize the Sargan statistic to allow heteroskedasticity is to use the GMM overidentification statistic - which we will examine in Chapter 13. For 2SLS, Wooldrige (1995) suggested a robust score test, but Baum, Schaffer and Stillman (2003) point out that it is numerically equivalent to the GMM overidentification statistic. Hence the bottom line appears to be that to allow heteroskedasticity or clustering it is best to use a GMM approach.\nIn overidentified applications it is always prudent to report an overidentification test. If the test is insignificant it means that the overidentifying restrictions are not rejected, supporting the estimated model. If the overidentifying test statistic is highly significant (if the p-value is very small) this is evidence that the overidentifying restrictions are violated. In this case we should be concerned that the model is misspecified and interpreting the parameter estimates should be done cautiously.\nWhen reporting the results of an overidentification test it seems reasonable to focus on very small significance levels such as \\(1 %\\). This means that we should only treat a model as “rejected” if the Sargan p-value is very small, e.g. less than \\(0.01\\). The reason to focus on very small significance levels is because it is very difficult to interpret the result “The model is rejected”. Stepping back a bit it does not seem credible that any overidentified model is literally true; rather what seems potentially credible is that an overidentified model is a reasonable approximation. A test is asking the question “Is there evidence that a model is not true” when we really want to know the answer to “Is there evidence that the model is a poor approximation”. Consequently it seems reasonable to require strong evidence to lead to the conclusion “Let’s reject this model”. The recommendation is that mild rejections ( \\(\\mathrm{p}\\)-values between \\(1 %\\) and 5%) should be viewed as mildly worrisome but not critical evidence against a model. The results of an overidentification test should be integrated with other information before making a strong decision.\nWe illustrate the methods with the Card college proximity example. We have estimated two overidentified models by 2SLS in columns 4 & 5 of Table 12.1. In each case the number of overidentifying restrictions is 1 . We report the Sargan statistic and its asymptotic \\(p\\)-value (calculated using the \\(\\chi_{1}^{2}\\) distribution) in the table. Both p-values (0.37 and \\(0.47)\\) are far from significant indicating that there is no evidence that the models are misspecified.\nWe now prove Theorem 12.16. The statistic \\(S\\) is invariant to rotations of \\(\\boldsymbol{Z}\\) (replacing \\(\\boldsymbol{Z}\\) with \\(\\boldsymbol{Z} \\boldsymbol{C}\\) ) so without loss of generality we assume \\(\\mathbb{E}\\left[Z Z^{\\prime}\\right]=\\boldsymbol{I}_{\\ell}\\). As \\(n \\rightarrow \\infty, n^{-1 / 2} \\boldsymbol{Z}^{\\prime} \\boldsymbol{e} \\underset{d}{\\rightarrow} Z\\) where \\(Z \\sim \\mathrm{N}\\left(0, \\boldsymbol{I}_{\\ell}\\right)\\). Also \\(\\frac{1}{n} \\boldsymbol{Z}^{\\prime} \\boldsymbol{Z} \\underset{p}{\\longrightarrow} \\boldsymbol{I}_{\\ell}\\) and \\(\\frac{1}{n} \\boldsymbol{Z}^{\\prime} \\boldsymbol{X} \\underset{p}{\\longrightarrow} \\boldsymbol{Q}\\), say. Then\n\\[\n\\begin{aligned}\nn^{-1 / 2} \\boldsymbol{Z}^{\\prime} \\widehat{\\boldsymbol{e}} &=\\left(\\boldsymbol{I}_{\\ell}-\\left(\\frac{1}{n} \\boldsymbol{Z}^{\\prime} \\boldsymbol{X}\\right)\\left(\\frac{1}{n} \\boldsymbol{X}^{\\prime} \\boldsymbol{P}_{\\boldsymbol{Z}} \\boldsymbol{X}\\right)^{-1}\\left(\\frac{1}{n} \\boldsymbol{X}^{\\prime} \\boldsymbol{Z}\\right)\\left(\\frac{1}{n} \\boldsymbol{Z}^{\\prime} \\boldsymbol{Z}\\right)^{-1}\\right) n^{-1 / 2} \\boldsymbol{Z}^{\\prime} \\boldsymbol{e} \\\\\n& \\underset{d}{\\rightarrow} \\sigma\\left(\\boldsymbol{I}_{\\ell}-\\boldsymbol{Q}\\left(\\boldsymbol{Q}^{\\prime} \\boldsymbol{Q}\\right)^{-1} \\boldsymbol{Q}^{\\prime}\\right) Z\n\\end{aligned}\n\\]\nSince \\(\\widehat{\\sigma}^{2} \\underset{p}{\\rightarrow} \\sigma^{2}\\) it follows that\n\\[\nS \\underset{d}{\\rightarrow} Z^{\\prime}\\left(\\boldsymbol{I}_{\\ell}-\\boldsymbol{Q}\\left(\\boldsymbol{Q}^{\\prime} \\boldsymbol{Q}\\right)^{-1} \\boldsymbol{Q}^{\\prime}\\right) \\mathrm{Z} \\sim \\chi_{\\ell-k}^{2} .\n\\]\nThe distribution is \\(\\chi_{\\ell-k}^{2}\\) because \\(\\boldsymbol{I}_{\\ell}-\\boldsymbol{Q}\\left(\\boldsymbol{Q}^{\\prime} \\boldsymbol{Q}\\right)^{-1} \\boldsymbol{Q}^{\\prime}\\) is idempotent with rank \\(\\ell-k\\).\nThe Sargan statistic test can be implemented in Stata using the command estat overid after ivregress 2sls or ivregres liml if a standard (non-robust) covariance matrix has been specified (that is, without the ‘, r’ option), or otherwise by the command estat overid, forcenonrobust."
  },
  {
    "objectID": "chpt12-iv.html#subset-overidentification-tests",
    "href": "chpt12-iv.html#subset-overidentification-tests",
    "title": "12  Instrumental Variables",
    "section": "12.32 Subset OverIdentification Tests",
    "text": "12.32 Subset OverIdentification Tests\nTests of \\(\\mathbb{H}_{0}: \\mathbb{E}[Z e]=0\\) are typically interpreted as tests of model specification. The alternative \\(\\mathbb{H}_{1}\\) : \\(\\mathbb{E}[Z e] \\neq 0\\) means that at least one element of \\(Z\\) is correlated with the error \\(e\\) and is thus an invalid instrumental variable. In some cases it may be reasonable to test only a subset of the moment conditions.\nAs in the previous section we restrict attention to the homoskedastic case \\(\\mathbb{E}\\left[e^{2} \\mid Z\\right]=\\sigma^{2}\\).\nPartition \\(Z=\\left(Z_{a}, Z_{b}\\right)\\) with dimensions \\(\\ell_{a}\\) and \\(\\ell_{b}\\), respectively, where \\(Z_{a}\\) contains the instruments which are believed to be uncorrelated with \\(e\\) and \\(Z_{b}\\) contains the instruments which may be correlated with \\(e\\). It is necessary to select this partition so that \\(\\ell_{a}>k\\), or equivalently \\(\\ell_{b}<\\ell-k\\). This means that the model with just the instruments \\(Z_{a}\\) is over-identified, or that \\(\\ell_{b}\\) is smaller than the number of overidentifying restrictions. (If \\(\\ell_{a}=k\\) then the tests described here exist but reduce to the Sargan test so are not interesting.) Hence the tests require that \\(\\ell-k>1\\), that the number of overidentifying restrictions exceeds one.\nGiven this partition the maintained hypothesis is \\(\\mathbb{E}\\left[Z_{a} e\\right]=0\\). The null and alternative hypotheses are \\(\\mathbb{H}_{0}: \\mathbb{E}\\left[Z_{b} e\\right]=0\\) against \\(\\mathbb{H}_{1}: \\mathbb{E}\\left[Z_{b} e\\right] \\neq 0\\). That is, the null hypothesis is that the full set of moment conditions are valid while the alternative hypothesis is that the instrument subset \\(Z_{b}\\) is correlated with \\(e\\) and thus an invalid instrument. Rejection of \\(\\mathbb{H}_{0}\\) in favor of \\(\\mathbb{M}_{1}\\) is then interpreted as evidence that \\(Z_{b}\\) is misspecified as an instrument.\nBased on the same reasoning as described in the previous section, to test \\(\\mathbb{H}_{0}\\) against \\(\\mathbb{H}_{1}\\) we consider a partitioned version of the regression (12.66)\n\\[\ne=Z_{a}^{\\prime} \\alpha_{a}+Z_{b}^{\\prime} \\alpha_{b}+v\n\\]\nbut now focus on the coefficient \\(\\alpha_{b}\\). Given \\(\\mathbb{E}\\left[Z_{a} e\\right]=0, \\mathbb{H}_{0}\\) is equivalent to \\(\\alpha_{b}=0\\). The equation is estimated by least squares replacing the unobserved \\(e_{i}\\) with the 2 SLS residual \\(\\widehat{e}_{i}\\). The estimate of \\(\\alpha_{b}\\) is\n\\[\n\\widehat{\\alpha}_{b}=\\left(\\boldsymbol{Z}_{b}^{\\prime} \\boldsymbol{M}_{a} \\boldsymbol{Z}_{b}\\right)^{-1} \\boldsymbol{Z}_{b}^{\\prime} \\boldsymbol{M}_{a} \\widehat{\\boldsymbol{e}}\n\\]\nwhere \\(\\boldsymbol{M}_{a}=\\boldsymbol{I}_{n}-\\boldsymbol{Z}_{a}\\left(\\boldsymbol{Z}_{a}^{\\prime} \\boldsymbol{Z}_{a}\\right)^{-1} \\boldsymbol{Z}_{a}^{\\prime}\\). Newey (1985) showed that an optimal (asymptotically most powerful) test of \\(\\mathbb{M}_{0}\\) against \\(\\mathbb{M}_{1}\\) is to reject for large values of the score statistic\n\\[\nN=\\widehat{\\alpha}_{b}^{\\prime}\\left(\\widehat{\\operatorname{var}}\\left[\\widehat{\\alpha}_{b}\\right]\\right)^{-} \\widehat{\\alpha}_{b}=\\frac{\\widehat{\\boldsymbol{e}}^{\\prime} \\boldsymbol{R}\\left(\\boldsymbol{R}^{\\prime} \\boldsymbol{R}-\\boldsymbol{R}^{\\prime} \\widehat{\\boldsymbol{X}}\\left(\\widehat{\\boldsymbol{X}}^{\\prime} \\widehat{\\boldsymbol{X}}\\right)^{-1} \\widehat{\\boldsymbol{X}}^{\\prime} \\boldsymbol{R}\\right)^{-1} \\boldsymbol{R}^{\\prime} \\widehat{\\boldsymbol{e}}}{\\widehat{\\sigma}^{2}}\n\\]\nwhere \\(\\widehat{\\boldsymbol{X}}=\\boldsymbol{P} \\boldsymbol{X}, \\boldsymbol{P}=\\boldsymbol{Z}\\left(\\boldsymbol{Z}^{\\prime} \\boldsymbol{Z}\\right)^{-1} \\boldsymbol{Z}^{\\prime}, \\boldsymbol{R}=\\boldsymbol{M}_{a} \\boldsymbol{Z}_{b}\\), and \\(\\widehat{\\sigma}^{2}=\\frac{1}{n} \\widehat{\\boldsymbol{e}}^{\\prime} \\widehat{\\boldsymbol{e}}\\).\nIndependently from Newey (1985), Eichenbaum, L. Hansen, and Singleton (1988) proposed a test based on the difference of Sargan statistics. Let \\(S\\) be the Sargan test statistic (12.67) based on the full instrument set and \\(S_{a}\\) be the Sargan statistic based on the instrument set \\(Z_{a}\\). The Sargan difference statistic is \\(C=S-S_{a}\\). Specifically, let \\(\\widetilde{\\beta}_{2 \\text { sls }}\\) be the 2SLS estimator using the instruments \\(Z_{a}\\) only, set \\(\\widetilde{e}_{i}=Y_{i}-X_{i}^{\\prime} \\widetilde{\\beta}_{2 s l s}\\), and set \\(\\widetilde{\\sigma}^{2}=\\frac{1}{n} \\widetilde{\\boldsymbol{e}}^{\\prime} \\widetilde{\\boldsymbol{e}}\\). Then\n\\[\nS_{a}=\\frac{\\widetilde{\\boldsymbol{e}}^{\\prime} \\boldsymbol{Z}_{a}\\left(\\boldsymbol{Z}_{a}^{\\prime} \\boldsymbol{Z}_{a}\\right)^{-1} \\boldsymbol{Z}_{a}^{\\prime} \\widetilde{\\boldsymbol{e}}}{\\widetilde{\\sigma}^{2}}\n\\]\nAn advantage of the \\(C\\) statistic is that it is quite simple to calculate from the standard regression output.\nAt this point it is useful to reflect on our stated requirement that \\(\\ell_{a}>k\\). Indeed, if \\(\\ell_{a}<k\\) then \\(Z_{a}\\) fails the order condition for identification and \\(\\widetilde{\\beta}_{2 \\text { sls }}\\) cannot be calculated. Thus \\(\\ell_{a} \\geq k\\) is necessary to compute \\(S_{a}\\) and hence \\(S\\). Furthermore, if \\(\\ell_{a}=k\\) then model \\(a\\) is just identified so while \\(\\widetilde{\\beta}_{2 \\text { sls }}\\) can be calculated, the statistic \\(S_{a}=0\\) so \\(C=S\\). Thus when \\(\\ell_{a}=k\\) the subset test equals the full overidentification test so there is no gain from considering subset tests.\nThe \\(C\\) statistic \\(S_{a}\\) is asymptotically equivalent to replacing \\(\\widetilde{\\sigma}^{2}\\) in \\(S_{a}\\) with \\(\\widehat{\\sigma}^{2}\\), yielding the statistic\n\\[\nC^{*}=\\frac{\\widehat{\\boldsymbol{e}}^{\\prime} \\boldsymbol{Z}\\left(\\boldsymbol{Z}^{\\prime} \\boldsymbol{Z}\\right)^{-1} \\boldsymbol{Z}^{\\prime} \\widehat{\\boldsymbol{e}}}{\\widehat{\\sigma}^{2}}-\\frac{\\widetilde{\\boldsymbol{e}}^{\\prime} \\boldsymbol{Z}_{a}\\left(\\boldsymbol{Z}_{a}^{\\prime} \\boldsymbol{Z}_{a}\\right)^{-1} \\boldsymbol{Z}_{a}^{\\prime} \\widetilde{\\boldsymbol{e}}}{\\widehat{\\sigma}^{2}} .\n\\]\nIt turns out that this is Newey’s statistic \\(N\\). These tests have chi-square asymptotic distributions.\nLet \\(c\\) satisfy \\(\\alpha=1-G_{\\ell_{b}}(c)\\).\nTheorem 12.17 Algebraically, \\(N=C^{*}\\). Under Assumption \\(12.2\\) and \\(\\mathbb{E}\\left[e^{2} \\mid Z\\right]=\\) \\(\\sigma^{2}\\), as \\(n \\rightarrow \\infty, N \\underset{d}{\\longrightarrow} \\chi_{\\ell_{b}}^{2}\\) and \\(C \\underset{d}{\\longrightarrow} \\chi_{\\ell_{b}}^{2}\\). Thus the tests “Reject \\(\\mathbb{H}_{0}\\) if \\(N>c\\)” and\n“Reject \\(\\mathbb{H}_{0}\\) if \\(C>c\\)” are asymptotically equivalent and have asymptotic size \\(\\alpha\\).\nTheorem \\(12.17\\) shows that \\(N\\) and \\(C^{*}\\) are identical and are near equivalents to the convenient statistic C. The appropriate asymptotic distribution is \\(\\chi_{\\ell_{b}}^{2}\\). Computationally, the easiest method to implement a subset overidentification test is to estimate the model twice by 2SLS, first using the full instrument set \\(Z\\) and the second using the partial instrument set \\(Z_{a}\\). Compute the Sargan statistics for both 2SLS regressions and compute \\(C\\) as the difference in the Sargan statistics. In Stata, for example, this is simple to implement with a few lines of code.\nWe illustrate using the Card college proximity example. Our reported 2SLS estimates have \\(\\ell-k=1\\) so there is no role for a subset overidentification test. (Recall, the number of overidentifying restrictions must exceed one.) To illustrate we add extra instruments to the estimates in column 5 of Table \\(12.1\\) (the 2SLS estimates using public, private, age, and age \\({ }^{2}\\) as instruments for education, experience, and experience \\(\\left.{ }^{2} / 100\\right)\\). We add two instruments: the years of education of the father and the mother of the worker. These variables had been used in the earlier labor economics literature as instruments but Card did not. (He used them as regression controls in some specifications.) The motivation for using parent’s education as instruments is the hypothesis that parental education influences children’s educational attainment but does not directly influence their ability. The more modern labor economics literature has disputed this idea, arguing that children are educated in part at home and thus parent’s education has a direct impact on the skill attainment of children (and not just an indirect impact via educational attainment). The older view was that parent’s education is a valid instrument, the modern view is that it is not valid. We can test this dispute using a overidentification subset test.\nWe do this by estimating the wage equation by 2SLS using public, private, age, age \\(^{2}\\), father, and \\(^{2}\\) mother, as instruments for education, experience, and experience \\(\\left.{ }^{2} / 100\\right)\\). We do not report the parameter estimates here but observe that this model is overidentified with 3 overidentifying restrictions. We calculate the Sargan overidentification statistic. It is \\(7.9\\) with an asymptotic p-value (calculated using \\(\\chi_{3}^{2}\\) ) of \\(0.048\\). This is a mild rejection of the null hypothesis of correct specification. As we argued in the previous section this by itself is not reason to reject the model. Now we consider a subset overidentification test. We are interested in testing the validity of the two instruments father and mother, not the instruments public, private, age, \\(a g e^{2}\\). To test the hypothesis that these two instruments are uncorrelated with the structural error we compute the difference in Sargan statistic, \\(C=7.9-0.5=7.4\\), which has a p-value (calculated using \\(\\chi_{2}^{2}\\) ) of \\(0.025\\). This is marginally statistically significant, meaning that there is evidence that father and mother are not valid instruments for the wage equation. Since the \\(\\mathrm{p}\\)-value is not smaller than \\(1 %\\) it is not overwhelming evidence but it still supports Card’s decision to not use parental education as instruments for the wage equation. We now prove the results in Theorem 12.17.\nWe first show that \\(N=C^{*}\\). Define \\(\\boldsymbol{P}_{a}=\\boldsymbol{Z}_{a}\\left(\\boldsymbol{Z}_{a}^{\\prime} \\boldsymbol{Z}_{a}\\right)^{-1} \\boldsymbol{Z}_{a}^{\\prime}\\) and \\(\\boldsymbol{P}_{\\boldsymbol{R}}=\\boldsymbol{R}\\left(\\boldsymbol{R}^{\\prime} \\boldsymbol{R}\\right)^{-1} \\boldsymbol{R}^{\\prime}\\). Since \\(\\left[\\boldsymbol{Z}_{a}, \\boldsymbol{R}\\right]\\) span \\(\\boldsymbol{Z}\\) we find \\(\\boldsymbol{P}=\\boldsymbol{P}_{\\boldsymbol{R}}+\\boldsymbol{P}_{a}\\) and \\(\\boldsymbol{P}_{\\boldsymbol{R}} \\boldsymbol{P}_{a}=0\\). It will be useful to note that\n\\[\n\\begin{aligned}\n\\boldsymbol{P}_{R} \\widehat{\\boldsymbol{X}} &=\\boldsymbol{P}_{\\boldsymbol{R}} \\boldsymbol{P} \\boldsymbol{X}=\\boldsymbol{P}_{\\boldsymbol{R}} \\boldsymbol{X} \\\\\n\\widehat{\\boldsymbol{X}}^{\\prime} \\widehat{\\boldsymbol{X}}-\\widehat{\\boldsymbol{X}}^{\\prime} \\boldsymbol{P}_{\\boldsymbol{R}} \\widehat{\\boldsymbol{X}} &=\\boldsymbol{X}^{\\prime}\\left(\\boldsymbol{P}-\\boldsymbol{P}_{\\boldsymbol{R}}\\right) \\boldsymbol{X}=\\boldsymbol{X}^{\\prime} \\boldsymbol{P}_{a} \\boldsymbol{X}\n\\end{aligned}\n\\]\nThe fact that \\(\\boldsymbol{X}^{\\prime} \\boldsymbol{P} \\widehat{\\boldsymbol{e}}=\\widehat{\\boldsymbol{X}}^{\\prime} \\widehat{\\boldsymbol{e}}=0\\) implies \\(\\boldsymbol{X}^{\\prime} \\boldsymbol{P}_{\\boldsymbol{R}} \\widehat{\\boldsymbol{e}}=-\\boldsymbol{X}^{\\prime} \\boldsymbol{P}_{a} \\widehat{\\boldsymbol{e}}\\). Finally, since \\(\\boldsymbol{Y}=\\boldsymbol{X} \\widehat{\\boldsymbol{\\beta}}+\\widehat{\\boldsymbol{e}}\\),\n\\[\n\\widetilde{\\boldsymbol{e}}=\\left(\\boldsymbol{I}_{n}-\\boldsymbol{X}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{P}_{a} \\boldsymbol{X}\\right)^{-1} \\boldsymbol{X}^{\\prime} \\boldsymbol{P}_{a}\\right) \\widehat{\\boldsymbol{e}}\n\\]\nso\n\\[\n\\widetilde{\\boldsymbol{e}}^{\\prime} \\boldsymbol{P}_{a} \\widetilde{\\boldsymbol{e}}=\\widehat{\\boldsymbol{e}}^{\\prime}\\left(\\boldsymbol{P}_{a}-\\boldsymbol{P}_{a} \\boldsymbol{X}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{P}_{a} \\boldsymbol{X}\\right)^{-1} \\boldsymbol{X}^{\\prime} \\boldsymbol{P}_{a}\\right) \\widehat{\\boldsymbol{e}} .\n\\]\nApplying the Woodbury matrix equality to the definition of \\(N\\) and the above algebraic relationships,\n\\[\n\\begin{aligned}\nN &=\\frac{\\widehat{\\boldsymbol{e}}^{\\prime} \\boldsymbol{P}_{\\boldsymbol{R}} \\widehat{\\boldsymbol{e}}+\\widehat{\\boldsymbol{e}}^{\\prime} \\boldsymbol{P}_{\\boldsymbol{R}} \\widehat{\\boldsymbol{X}}\\left(\\widehat{\\boldsymbol{X}}^{\\prime} \\widehat{\\boldsymbol{X}}-\\widehat{\\boldsymbol{X}}^{\\prime} \\boldsymbol{P}_{\\boldsymbol{R}} \\widehat{\\boldsymbol{X}}\\right)^{-1} \\widehat{\\boldsymbol{X}}^{\\prime} \\boldsymbol{P}_{\\boldsymbol{R}} \\widehat{\\boldsymbol{e}}}{\\widehat{\\sigma}^{2}} \\\\\n&=\\frac{\\widehat{\\boldsymbol{e}}^{\\prime} \\boldsymbol{P} \\widehat{\\boldsymbol{e}}-\\widehat{\\boldsymbol{e}}^{\\prime} \\boldsymbol{P}_{a} \\widehat{\\boldsymbol{e}}+\\widehat{\\boldsymbol{e}}^{\\prime} \\boldsymbol{P}_{a} \\boldsymbol{X}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{P}_{a} \\boldsymbol{X}\\right)^{-1} \\boldsymbol{X}^{\\prime} \\boldsymbol{P}_{a} \\widehat{\\boldsymbol{e}}}{\\widehat{\\sigma}^{2}} \\\\\n&=\\frac{\\widehat{\\boldsymbol{e}}^{\\prime} \\boldsymbol{P} \\widehat{\\boldsymbol{e}}-\\widetilde{\\boldsymbol{e}}^{\\prime} \\boldsymbol{P}_{a} \\widetilde{\\boldsymbol{e}}}{\\widehat{\\sigma}^{2}} \\\\\n&=C^{*}\n\\end{aligned}\n\\]\nas claimed.\nWe next establish the asymptotic distribution. Since \\(\\boldsymbol{Z}_{a}\\) is a subset of \\(\\boldsymbol{Z}, \\boldsymbol{P}_{a}=\\boldsymbol{M}_{a} \\boldsymbol{P}\\), thus \\(\\boldsymbol{P} \\boldsymbol{R}=\\boldsymbol{R}\\) and \\(\\boldsymbol{R}^{\\prime} \\boldsymbol{X}=\\boldsymbol{R}^{\\prime} \\widehat{\\boldsymbol{X}}\\). Consequently\n\\[\n\\begin{aligned}\n\\frac{1}{\\sqrt{n}} \\boldsymbol{R}^{\\prime} \\widehat{\\boldsymbol{e}} &=\\frac{1}{\\sqrt{n}} \\boldsymbol{R}^{\\prime}(\\boldsymbol{Y}-\\boldsymbol{X} \\widehat{\\boldsymbol{\\beta}}) \\\\\n&=\\frac{1}{\\sqrt{n}} \\boldsymbol{R}^{\\prime}\\left(\\boldsymbol{I}_{n}-\\boldsymbol{X}\\left(\\widehat{\\boldsymbol{X}}^{\\prime} \\widehat{\\boldsymbol{X}}\\right)^{-1} \\widehat{\\boldsymbol{X}}^{\\prime}\\right) \\boldsymbol{e} \\\\\n&=\\frac{1}{\\sqrt{n}} \\boldsymbol{R}^{\\prime}\\left(\\boldsymbol{I}_{n}-\\widehat{\\boldsymbol{X}}\\left(\\widehat{\\boldsymbol{X}}^{\\prime} \\widehat{\\boldsymbol{X}}\\right)^{-1} \\widehat{\\boldsymbol{X}}^{\\prime}\\right) \\boldsymbol{e} \\\\\n& \\underset{d}{\\longrightarrow} \\mathrm{N}\\left(0, \\boldsymbol{V}_{2}\\right)\n\\end{aligned}\n\\]\nwhere\n\\[\n\\boldsymbol{V}_{2}=\\operatorname{plim}_{n \\rightarrow \\infty}\\left(\\frac{1}{n} \\boldsymbol{R}^{\\prime} \\boldsymbol{R}-\\frac{1}{n} \\boldsymbol{R}^{\\prime} \\widehat{\\boldsymbol{X}}\\left(\\frac{1}{n} \\widehat{\\boldsymbol{X}}^{\\prime} \\widehat{\\boldsymbol{X}}\\right)^{-1} \\frac{1}{n} \\widehat{\\boldsymbol{X}}^{\\prime} \\boldsymbol{R}\\right) .\n\\]\nIt follows that \\(N=C^{*} \\underset{d}{\\longrightarrow} \\chi_{\\ell_{b}}^{2}\\) as claimed. Since \\(C=C^{*}+o_{p}(1)\\) it has the same limiting distribution."
  },
  {
    "objectID": "chpt12-iv.html#bootstrap-overidentification-tests",
    "href": "chpt12-iv.html#bootstrap-overidentification-tests",
    "title": "12  Instrumental Variables",
    "section": "12.33 Bootstrap Overidentification Tests",
    "text": "12.33 Bootstrap Overidentification Tests\nIn small to moderate sample sizes the overidentification tests are not well approximated by the asymptotic chi-square distributions. For improved accuracy it is advised to use bootstrap critical values. The bootstrap for 2SLS (Section 12.23) can be used for this purpose but the bootstrap version of the overidentification statistic must be adjusted. This is because in the bootstrap universe the overidentified moment conditions are not satisfied. One solution is to center the moment conditions. For the 2SLS estimator the standard overidentification test is based on the Sargan statistic\n\\[\n\\begin{aligned}\n&S=n \\frac{\\widehat{\\boldsymbol{e}}^{\\prime} \\boldsymbol{Z}\\left(\\boldsymbol{Z}^{\\prime} \\boldsymbol{Z}\\right)^{-1} \\boldsymbol{Z}^{\\prime} \\widehat{\\boldsymbol{e}}}{\\widehat{\\boldsymbol{e}}^{\\prime} \\widehat{\\boldsymbol{e}}} \\\\\n&\\widehat{\\boldsymbol{e}}=\\boldsymbol{Y}-\\boldsymbol{X} \\widehat{\\beta}_{2 s l s}\n\\end{aligned}\n\\]\nThe recentered bootstrap analog is\n\\[\n\\begin{aligned}\nS^{* *} &=n \\frac{\\left(\\widehat{\\boldsymbol{e}}^{* \\prime} \\boldsymbol{Z}^{*}-\\boldsymbol{Z}^{\\prime} \\widehat{\\boldsymbol{e}}\\right)\\left(\\boldsymbol{Z}^{* \\prime} \\boldsymbol{Z}^{*}\\right)^{-1}\\left(\\boldsymbol{Z}^{* \\prime} \\widehat{\\boldsymbol{e}}^{*}-\\boldsymbol{Z}^{\\prime} \\widehat{\\boldsymbol{e}}\\right)}{\\widehat{\\boldsymbol{e}}^{*} \\widehat{\\boldsymbol{e}}^{*}} \\\\\n\\widehat{\\boldsymbol{e}}^{*} &=\\boldsymbol{Y}^{*}-\\boldsymbol{X}^{*} \\widehat{\\beta}_{2 \\mathrm{sls}}^{*}\n\\end{aligned}\n\\]\nOn each bootstrap sample \\(S^{* *}(b)\\) is calculated and stored. The bootstrap p-value is\n\\[\np^{*}=\\frac{1}{B} \\sum_{b=1}^{B} \\mathbb{1}\\left\\{S^{* *}(b)>S\\right\\} .\n\\]\nThis bootstrap \\(\\mathrm{p}\\)-value is valid because the statistic \\(S^{* *}\\) satisfies the overidentified moment conditions."
  },
  {
    "objectID": "chpt12-iv.html#local-average-treatment-effects",
    "href": "chpt12-iv.html#local-average-treatment-effects",
    "title": "12  Instrumental Variables",
    "section": "12.34 Local Average Treatment Effects",
    "text": "12.34 Local Average Treatment Effects\nIn a pair of influential papers, Imbens and Angrist (1994) and Angrist, Imbens and Rubin (1996) proposed an new interpretation of the instrumental variables estimator using the potential outcomes model introduced in Section 2.30.\nWe will restrict attention to the case that the endogenous regressor \\(X\\) and excluded instrument \\(Z\\) are binary variables. We write the model as a pair of potential outcome functions. The dependent variable \\(Y\\) is a function of the regressor and an unobservable vector \\(U, Y=h(X, U)\\), and the endogenous regressor \\(X\\) is a function of the instrument \\(Z\\) and \\(U, X=g(Z, U)\\). By specifying \\(U\\) as a vector there is no loss of generality in letting both equations depend on \\(U\\).\nIn this framework the outcomes are determined by the random vector \\(U\\) and the exogenous instrument \\(Z\\). This determines \\(X\\) which determines \\(Y\\). To put this in the context of the college proximity example the variable \\(U\\) is everything specific about an individual. Given college proximity \\(Z\\) the person decides to attend college or not. The person’s wage is determined by the individual attributes \\(U\\) as well as college attendence \\(X\\) but is not directly affected by college proximity \\(Z\\).\nWe can omit the random variable \\(U\\) from the notation as follows. An individual has a realization \\(U\\). We then set \\(Y(x)=h(x, U)\\) and \\(X(z)=g(z, U)\\). Also, given a realization \\(Z\\) the observables are \\(X=X(Z)\\) and \\(Y=Y(X)\\).\nIn this model the causal effect of college for an individual is \\(C=Y(1)-Y(0)\\). As discussed in Section \\(2.30\\), this is individual-specific and random.\nWe would like to learn about the distribution of the causal effects, or at least features of the distribution. A common feature of interest is the average treatment effect (ATE)\n\\[\n\\operatorname{ATE}=\\mathbb{E}[C]=\\mathbb{E}[Y(1)-Y(0)] .\n\\]\nThis, however, it typically not feasible to estimate allowing for endogenous \\(X\\) without strong assumptions (such as that the causal effect \\(C\\) is constant across individuals). The treatment effect literature has explored what features of the distribution of \\(C\\) can be estimated. One particular feature of interest emphasized by Imbens and Angrist (1994) is the local average treatment effect (LATE). Roughly, this is the average effect upon those effected by the instrumental variable. To understand LATE, consider the college proximity example. In the potential outcomes framework each person is fully characterized by their individual unobservable \\(U\\). Given \\(U\\), their decision to attend college is a function of the proximity indicator \\(Z\\). For some students, proximity has no effect on their decision. For other students, it has an effect in the specific sense that given \\(Z=1\\) they choose to attend college while if \\(Z=0\\) they choose to not attend. We can summarize the possibilites with the following chart which is based on labels developed by Angrist, Imbens and Rubin (1996).\n\\[\n\\begin{array}{ccc}\n& X(0)=0 & X(0)=1 \\\\\nX(1)=0 & \\text { Never Takers } & \\text { Defiers } \\\\\nX(1)=1 & \\text { Compliers } & \\text { Always Takers }\n\\end{array}\n\\]\nThe columns indicate the college attendence decision given \\(Z=0\\) (not close to a college). The rows indicate the college attendence decision given \\(Z=1\\) (close to a college). The four entries are labels for the four types of individuals based on these decisions. The upper-left entry are the individuals who do not attend college regardless of \\(Z\\). They are called “Never Takers”. The lower-right entry are the individuals who conversely attend college regardless of \\(Z\\). They are called “Always Takers”. The bottom left are the individuals who only attend college if they live close to one. They are called “Compliers”. The upper right entry is a bit of a challenge. These are individuals who attend college only if they do not live close to one. They are called “Dediers”. Imbens and Angrist discovered that to identify the parameters of interest we need to assume that there are no Dediers, or equivalently that \\(X(1) \\geq X(0)\\). They call this a “monotonicity” condition - increasing the instrument does not decrease \\(X\\) for any individual.\nAs another example, suppose we are interested in the effect of wearing a face mask \\(X\\) on health \\(Y\\) during a virus pandemic. Wearing a face mask is a choice made by the individual so should be viewed as endogenous. For an instrument \\(Z\\) consider a government policy that requires face masks to be worn in public. The “Compliers” are those who wear a face mask if there is a policy but otherwise do not. The “Deniers” are those who do the converse. That is, these individuals would have worn a face mask based on the evidence of a pandemic but rebel against a government policy. Once again, identification requires that there are no Deniers.\nWe can distinguish the types in the table by the relative values of \\(X(1)-X(0)\\). For Never-Takers and Always-Takers \\(X(1)-X(0)=0\\), while for Compliers \\(X(1)-X(0)=1\\).\nWe are interested in the causal effect \\(C=h(1, U)-h(0, U)\\) of college on wages. The average causal effect (ACE) is its expectation \\(\\mathbb{E}[Y(1)-Y(0)]\\). To estimate the ACE we need observations of both \\(Y(0)\\) and \\(Y\\) (1) which means we need to observe some individuals who attend college and some who do not attend college. Consider the group “Never-Takers”. They never attend college so we only observe \\(Y(0)\\). It is thus impossible to estimate the ACE of college for this group. Similarly consider the group “Always-Takers”. They always attend college so we only observe \\(Y(1)\\) and again we cannot estimate the ACE of college for this group. The group for which we can estimate the ACE are the “Compliers”. The ACE for this group is\n\\[\n\\text { LATE }=\\mathbb{E}[Y(1)-Y(0) \\mid X(1)>X(0)] .\n\\]\nImbens and Angrist call this the local average treatment effect (LATE) as it is the average treatment effect for the sub-population whose endogenous regressor is affected by the instrument. Examining the definition, the LATE is the average causal effect of college attendence on wages for the sub-sample of individuals who choose to attend college if (and only if) they live close to one.\nInterestingly, we show below that\n\\[\n\\text { LATE }=\\frac{\\mathbb{E}[Y \\mid Z=1]-\\mathbb{E}[Y \\mid Z=0]}{\\mathbb{E}[X \\mid Z=1]-\\mathbb{E}[X \\mid Z=0]} .\n\\]\nThat is, LATE equals the Wald expression (12.27) for the slope coefficient in the IV regression model. This means that the standard IV estimator is an estimator of LATE. Thus when treatment effects are potentially heterogeneous we can interpret IV as an estimator of LATE. The equality (12.68) occurs under the following conditions.\nAssumption 12.3 \\(U\\) and \\(Z\\) are independent and \\(\\mathbb{P}[X(1)-X(0)<0]=0 .\\)\nOne interesting feature about LATE is that its value can depend on the instrument \\(Z\\) and the distribution of causal effects \\(C\\) in the population. To make this concrete suppose that instead of the Card proximity instrument we consider an instrument based on the financial cost of local college attendence. It is reasonable to expect that while the set of students affected by these two instruments are similar the two sets of students will not be the same. That is, some students may be responsive to proximity but not finances, and conversely. If the causal effect \\(C\\) has a different average in these two groups of students then LATE will be different when calculated with these two instruments. Thus LATE can vary by the choice of instrument.\nHow can that be? How can a well-defined parameter depend on the choice of instrument? Doesn’t this contradict the basic IV regression model? The answer is that the basic IV regression model is restrictive - it specifies that the causal effect \\(\\beta\\) is common across all individuals. Its value is the same regardless of the choice of specific instrument (so long as it satisfies the instrumental variables assumptions). In contrast, the potential outcomes framework is more general allowing for the causal effect to vary across individuals. What this analysis shows us is that in this context is quite possible for the LATE coefficient to vary by instrument. This occurs when causal effects are heterogeneous.\nOne implication of the LATE framework is that IV estimates should be interpreted as causal effects only for the population of compliers. Interpretation should focus on the population of potential compliers and extension to other populations should be done with caution. For example, in the Card proximity model the IV estimates of the causal return to schooling presented in Table \\(12.1\\) should be interpreted as applying to the population of students who are incentivized to attend college by the presence of a college within their home county. The estimates should not be applied to other students.\nFormally, the analysis of this section examined the case of a binary instrument and endogenous regressor. How does this generalize? Suppose that the regressor \\(X\\) is discrete, taking \\(J+1\\) discrete values. We can then rewrite the model as one with \\(J\\) binary endogenous regressors. If we then have \\(J\\) binary instruments we are back in the Imbens-Angrist framework (assuming the instruments have a monotonic impact on the endogenous regressors). A benefit is that with a larger set of instruments it is plausible that the set of compliers in the population is expanded.\nWe close this section by showing (12.68) under Assumption 12.3. The realized value of \\(X\\) can be written as\n\\[\nX=(1-Z) X(0)+Z X(1)=X(0)+Z(X(1)-X(0))\n\\]\nSimilarly\n\\[\nY=Y(0)+X(Y(1)-Y(0))=Y(0)+X C .\n\\]\nCombining,\n\\[\nY=Y(0)+X(0) C+Z(X(1)-Y(0)) C .\n\\]\nThe independence of \\(u\\) and \\(Z\\) implies independence of \\((Y(0), Y(1), X(0), X(1), C)\\) and \\(Z\\). Thus\n\\[\n\\mathbb{E}[Y \\mid Z=1]=\\mathbb{E}[Y(0)]+\\mathbb{E}[X(0) C]+\\mathbb{E}[(X(1)-X(0)) C]\n\\]\nand\n\\[\n\\mathbb{E}[Y \\mid Z=0]=\\mathbb{E}[Y(0)]+\\mathbb{E}[X(0) C] .\n\\]\nSubtracting we obtain\n\\[\n\\begin{aligned}\n\\mathbb{E}[Y \\mid Z=1]-\\mathbb{E}[Y \\mid Z=0] &=\\mathbb{E}[(X(1)-X(0)) C] \\\\\n&=1 \\times \\mathbb{E}[C \\mid X(1)-X(0)=1] \\mathbb{P}[X(1)-X(0)=1] \\\\\n&+0 \\times \\mathbb{E}[C \\mid X(1)-X(0)=0] \\mathbb{P}[X(1)-X(0)=0] \\\\\n&+(-1) \\times \\mathbb{E}[C \\mid X(1)-X(0)=-1] \\mathbb{P}[X(1)-X(0)=-1] \\\\\n&=\\mathbb{E}[C \\mid X(1)-X(0)=1](\\mathbb{E}[X \\mid X=1]-\\mathbb{E}[X \\mid Z=0])\n\\end{aligned}\n\\]\nwhere the final equality uses \\(\\mathbb{P}[X(1)-X(0)<0]=0\\) and\n\\[\n\\mathbb{P}[X(1)-X(0)=1]=\\mathbb{E}[X(1)-X(0)]=\\mathbb{E}[X \\mid Z=1]-\\mathbb{E}[X \\mid Z=0] .\n\\]\nRearranging\n\\[\n\\mathrm{LATE}=\\mathbb{E}[C \\mid X(1)-X(0)=1]=\\frac{\\mathbb{E}[Y \\mid Z=1]-\\mathbb{E}[Y \\mid Z=0]}{\\mathbb{E}[X \\mid Z=1]-\\mathbb{E}[X \\mid Z=0]}\n\\]\nas claimed."
  },
  {
    "objectID": "chpt12-iv.html#identification-failure",
    "href": "chpt12-iv.html#identification-failure",
    "title": "12  Instrumental Variables",
    "section": "12.35 Identification Failure",
    "text": "12.35 Identification Failure\nRecall the reduced form equation\n\\[\nX_{2}=\\Gamma_{12}^{\\prime} Z_{1}+\\Gamma_{22}^{\\prime} Z_{2}+u_{2} .\n\\]\nThe parameter \\(\\beta\\) fails to be identified if \\(\\Gamma_{22}\\) has deficient rank. The consequences of identification failure for inference are quite severe.\nTake the simplest case where \\(k_{1}=0\\) and \\(k_{2}=\\ell_{2}=1\\). Then the model may be written as\n\\[\n\\begin{aligned}\n&Y=X \\beta+e \\\\\n&X=Z \\gamma+u\n\\end{aligned}\n\\]\nand \\(\\Gamma_{22}=\\gamma=\\mathbb{E}[Z X] / \\mathbb{E}\\left[Z^{2}\\right]\\). We see that \\(\\beta\\) is identified if and only if \\(\\gamma \\neq 0\\), which occurs when \\(\\mathbb{E}[X Z] \\neq 0\\). Thus identification hinges on the existence of correlation between the excluded exogenous variable and the included endogenous variable.\nSuppose this condition fails. In this case \\(\\gamma=0\\) and \\(\\mathbb{E}[X Z]=0\\). We now analyze the distribution of the least squares and IV estimators of \\(\\beta\\). For simplicity we assume conditional homoskedasticity and normalize the variances of \\(e, u\\), and \\(Z\\) to unity. Thus\n\\[\n\\operatorname{var}\\left[\\left(\\begin{array}{c}\ne \\\\\nu\n\\end{array}\\right) \\mid Z\\right]=\\left(\\begin{array}{ll}\n1 & \\rho \\\\\n\\rho & 1\n\\end{array}\\right) .\n\\]\nThe errors have non-zero correlation \\(\\rho \\neq 0\\) when the variables are endogenous.\nBy the CLT we have the joint convergence\n\\[\n\\frac{1}{\\sqrt{n}} \\sum_{i=1}^{n}\\left(\\begin{array}{c}\nZ_{i} e_{i} \\\\\nZ_{i} u_{i}\n\\end{array}\\right) \\underset{d}{ }\\left(\\begin{array}{l}\n\\xi_{1} \\\\\n\\xi_{2}\n\\end{array}\\right) \\sim \\mathrm{N}\\left(0,\\left(\\begin{array}{cc}\n1 & \\rho \\\\\n\\rho & 1\n\\end{array}\\right)\\right) .\n\\]\nIt is convenient to define \\(\\xi_{0}=\\xi_{1}-\\rho \\xi_{2}\\) which is normal and independent of \\(\\xi_{2}\\). As a benchmark it is useful to observe that the least squares estimator of \\(\\beta\\) satisfies\n\\[\n\\widehat{\\beta}_{\\mathrm{ols}}-\\beta=\\frac{n^{-1} \\sum_{i=1}^{n} u_{i} e_{i}}{n^{-1} \\sum_{i=1}^{n} u_{i}^{2}} \\underset{p}{\\longrightarrow} \\rho \\neq 0\n\\]\nso endogeneity causes \\(\\widehat{\\beta}_{\\text {ols }}\\) to be inconsistent for \\(\\beta\\).\nUnder identification failure \\(\\gamma=0\\) the asymptotic distribution of the IV estimator is\n\\[\n\\widehat{\\beta}_{\\mathrm{iv}}-\\beta=\\frac{\\frac{1}{\\sqrt{n}} \\sum_{i=1}^{n} Z_{i} e_{i}}{\\frac{1}{\\sqrt{n}} \\sum_{i=1}^{n} Z_{i} X_{i}} \\underset{\\mathrm{d}}{\\xi_{2}}=\\rho+\\frac{\\xi_{0}}{\\xi_{2}} .\n\\]\nThis asymptotic convergence result uses the continuous mapping theorem which applies since the function \\(\\xi_{1} / \\xi_{2}\\) is continuous everywhere except at \\(\\xi_{2}=0\\), which occurs with probability equal to zero.\nThis limiting distribution has several notable features.\nFirst, \\(\\widehat{\\beta}_{\\mathrm{iv}}\\) does not converge in probability to a limit, rather it converges in distribution to a random variable. Thus the IV estimator is inconsistent. Indeed, it is not possible to consistently estimate an unidentified parameter and \\(\\beta\\) is not identified when \\(\\gamma=0\\).\nSecond, the ratio \\(\\xi_{0} / \\xi_{2}\\) is symmetrically distributed about zero so the median of the limiting distribution of \\(\\widehat{\\beta}_{\\text {iv }}\\) is \\(\\beta+\\rho\\). This means that the IV estimator is median biased under endogeneity. Thus under identification failure the IV estimator does not correct the centering (median bias) of least squares.\nThird, the ratio \\(\\xi_{0} / \\xi_{2}\\) of two independent normal random variables is Cauchy distributed. This is particularly nasty as the Cauchy distribution does not have a finite mean. The distribution has thick tails meaning that extreme values occur with higher frequency than the normal. Inferences based on the normal distribution can be quite incorrect.\nTogether, these results show that \\(\\gamma=0\\) renders the IV estimator particularly poorly behaved - it is inconsistent, median biased, and non-normally distributed.\nWe can also examine the behavior of the t-statistic. For simplicity consider the classical (homoskedastic) t-statistic. The error variance estimate has the asymptotic distribution\n\\[\n\\begin{aligned}\n& \\widehat{\\sigma}^{2}=\\frac{1}{n} \\sum_{i=1}^{n}\\left(Y_{i}-X_{i} \\widehat{\\beta}_{\\mathrm{iv}}\\right)^{2} \\\\\n& =\\frac{1}{n} \\sum_{i=1}^{n} e_{i}^{2}-\\frac{2}{n} \\sum_{i=1}^{n} e_{i} X_{i}\\left(\\widehat{\\beta}_{\\mathrm{iv}}-\\beta\\right)+\\frac{1}{n} \\sum_{i=1}^{n} X_{i}^{2}\\left(\\widehat{\\beta}_{\\mathrm{iv}}-\\beta\\right)^{2} \\\\\n& \\underset{d}{\\longrightarrow} 1-2 \\rho \\frac{\\xi_{1}}{\\xi_{2}}+\\left(\\frac{\\xi_{1}}{\\xi_{2}}\\right)^{2} \\text {. }\n\\end{aligned}\n\\]\nThus the t-statistic has the asymptotic distribution\n\nThe limiting distribution is non-normal, meaning that inference using the normal distribution will be (considerably) incorrect. This distribution depends on the correlation \\(\\rho\\). The distortion is increasing in \\(\\rho\\). Indeed as \\(\\rho \\rightarrow 1\\) we have \\(\\xi_{1} / \\xi_{2} \\rightarrow p 1\\) and the unexpected finding \\(\\widehat{\\sigma}^{2} \\rightarrow{ }_{p} 0\\). The latter means that the conventional standard error \\(s\\left(\\widehat{\\beta}_{\\text {iv }}\\right)\\) for \\(\\widehat{\\beta}_{\\text {iv }}\\) also converges in probability to zero. This implies that the t-statistic diverges in the sense \\(|T| \\rightarrow p \\infty\\). In this situations users may incorrectly interpret estimates as precise despite the fact that they are highly imprecise."
  },
  {
    "objectID": "chpt12-iv.html#weak-instruments",
    "href": "chpt12-iv.html#weak-instruments",
    "title": "12  Instrumental Variables",
    "section": "12.36 Weak Instruments",
    "text": "12.36 Weak Instruments\nIn the previous section we examined the extreme consequences of full identification failure. Similar problems occur when identification is weak in the sense that the reduced form coefficients are of small magnitude. In this section we derive the asymptotic distribution of the OLS, 2SLS, and LIML estimators when the reduced form coefficients are treated as weak. We show that the estimators are inconsistent and the 2SLS and LIML estimators remain random in large samples.\nTo simplify the exposition we assume that there are no included exogenous variables (no \\(X_{1}\\) ) so we write \\(X_{2}, Z_{2}\\), and \\(\\beta_{2}\\) simply as \\(X, Z\\), and \\(\\beta\\). The model is\n\\[\n\\begin{aligned}\n&Y=X^{\\prime} \\beta+e \\\\\n&X=\\Gamma^{\\prime} Z+u_{2} .\n\\end{aligned}\n\\]\nRecall the reduced form error vector \\(u=\\left(u_{1}, u_{2}\\right)\\) and its covariance matrix\n\\[\n\\mathbb{E}\\left[u u^{\\prime}\\right]=\\Sigma=\\left[\\begin{array}{cc}\n\\Sigma_{11} & \\Sigma_{12} \\\\\n\\Sigma_{21} & \\Sigma_{22}\n\\end{array}\\right] .\n\\]\nRecall that the structural error is \\(e=u_{1}-\\beta^{\\prime} u_{2}=\\gamma^{\\prime} u\\) where \\(\\gamma=(1,-\\beta)\\) which has variance \\(\\mathbb{E}\\left[e^{2} \\mid Z\\right]=\\) \\(\\gamma^{\\prime} \\Sigma \\gamma\\). Also define the covariance \\(\\Sigma_{2 e}=\\mathbb{E}\\left[u_{2} e \\mid Z\\right]=\\Sigma_{21}-\\Sigma_{22} \\beta\\).\nIn Section \\(12.35\\) we assumed complete identification failure in the sense that \\(\\Gamma=0\\). We now want to assume that identification does not completely fail but is weak in the sense that \\(\\Gamma\\) is small. A rich asymptotic distribution theory has been developed to understand this setting by modeling \\(\\Gamma\\) as “localto-zero”. The seminal contribution is Staiger and Stock (1997). The theory was extended to nonlinear GMM estimation by Stock and Wright (2000).\nThe technical device introduced by Staiger and Stock (1997) is to assume that the reduced form parameter is local-to-zero, specifically\n\\[\n\\Gamma=n^{-1 / 2} \\boldsymbol{C}\n\\]\nwhere \\(\\boldsymbol{C}\\) is a free matrix. The \\(n^{-1 / 2}\\) scaling is picked because it provides just the right balance to allow a useful distribution theory. The local-to-zero assumption (12.71) is not meant to be taken literally but rather is meant to be a useful distributional approximation. The parameter \\(\\boldsymbol{C}\\) indexes the degree of identification. Larger \\(\\|\\boldsymbol{C}\\|\\) implies stronger identification; smaller \\(\\|\\boldsymbol{C}\\|\\) implies weaker identification.\nWe now derive the asymptotic distribution of the least squares, 2SLS, and LIML estimators under the local-to-unity assumption (12.71).\nThe least squares estimator satisfies\n\\[\n\\begin{aligned}\n\\widehat{\\beta}_{\\mathrm{ols}}-\\beta &=\\left(n^{-1} \\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1}\\left(n^{-1} \\boldsymbol{X}^{\\prime} \\boldsymbol{e}\\right) \\\\\n&=\\left(n^{-1} \\boldsymbol{U}_{2}^{\\prime} \\boldsymbol{U}_{2}\\right)^{-1}\\left(n^{-1} \\boldsymbol{U}_{2}^{\\prime} \\boldsymbol{e}\\right)+o_{p}(1) \\\\\n& \\longrightarrow \\underset{22}{-1} \\Sigma_{2 e} .\n\\end{aligned}\n\\]\nThus the least squares estimator is inconsistent for \\(\\beta\\).\nTo examine the 2SLS estimator, by the central limit theorem\n\\[\n\\frac{1}{\\sqrt{n}} \\sum_{i=1}^{n} Z_{i} u_{i}^{\\prime} \\underset{d}{\\longrightarrow} \\xi=\\left[\\xi_{1}, \\xi_{2}\\right]\n\\]\nwhere\n\\[\n\\operatorname{vec}(\\xi) \\sim \\mathrm{N}\\left(0, \\mathbb{E}\\left[u u^{\\prime} \\otimes Z Z^{\\prime}\\right]\\right)\n\\]\nThis implies\n\\[\n\\frac{1}{\\sqrt{n}} \\boldsymbol{Z}^{\\prime} \\boldsymbol{e} \\longrightarrow \\underset{d}{\\xi_{e}}=\\xi \\gamma\n\\]\nWe also find that\n\\[\n\\frac{1}{\\sqrt{n}} \\boldsymbol{Z}^{\\prime} \\boldsymbol{X}=\\frac{1}{n} \\boldsymbol{Z}^{\\prime} \\boldsymbol{Z} \\boldsymbol{C}+\\frac{1}{\\sqrt{n}} \\boldsymbol{Z}^{\\prime} \\boldsymbol{U}_{2} \\underset{d}{\\longrightarrow} \\boldsymbol{Q}_{Z} \\boldsymbol{C}+\\xi_{2} .\n\\]\nThus\n\\[\n\\boldsymbol{X}^{\\prime} \\boldsymbol{P}_{\\boldsymbol{Z}} \\boldsymbol{X}=\\left(\\frac{1}{\\sqrt{n}} \\boldsymbol{X}^{\\prime} \\boldsymbol{Z}\\right)\\left(\\frac{1}{n} \\boldsymbol{Z}^{\\prime} \\boldsymbol{Z}\\right)^{-1}\\left(\\frac{1}{\\sqrt{n}} \\boldsymbol{Z}^{\\prime} \\boldsymbol{X}\\right) \\underset{d}{\\longrightarrow}\\left(\\boldsymbol{Q}_{Z} \\boldsymbol{C}+\\xi_{2}\\right)^{\\prime} \\boldsymbol{Q}_{Z}^{-1}\\left(\\boldsymbol{Q}_{Z} \\boldsymbol{C}+\\xi_{2}\\right)\n\\]\nand\n\\[\n\\boldsymbol{X}^{\\prime} \\boldsymbol{P}_{\\boldsymbol{Z}} \\boldsymbol{e}=\\left(\\frac{1}{\\sqrt{n}} \\boldsymbol{X}^{\\prime} \\boldsymbol{Z}\\right)\\left(\\frac{1}{n} \\boldsymbol{Z}^{\\prime} \\boldsymbol{Z}\\right)^{-1}\\left(\\frac{1}{\\sqrt{n}} \\boldsymbol{Z}^{\\prime} \\boldsymbol{e}\\right) \\underset{d}{\\longrightarrow}\\left(\\boldsymbol{Q}_{Z} \\boldsymbol{C}+\\xi_{2}\\right)^{\\prime} \\boldsymbol{Q}_{Z}^{-1} \\xi_{e}\n\\]\nWe find that the 2SLS estimator has the asymptotic distribution\n\\[\n\\begin{aligned}\n\\widehat{\\beta}_{2 \\text { sls }}-\\beta &=\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{P}_{\\boldsymbol{Z}} \\boldsymbol{X}\\right)^{-1}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{P}_{\\boldsymbol{Z}} \\boldsymbol{e}\\right) \\\\\n& \\longrightarrow\\left(\\left(\\boldsymbol{Q}_{Z} \\boldsymbol{C}+\\xi_{2}\\right)^{\\prime} \\boldsymbol{Q}_{Z}^{-1}\\left(\\boldsymbol{Q}_{Z} \\boldsymbol{C}+\\xi_{2}\\right)\\right)^{-1}\\left(\\boldsymbol{Q}_{Z} \\boldsymbol{C}+\\xi_{2}\\right)^{\\prime} \\boldsymbol{Q}_{Z}^{-1} \\xi_{e} .\n\\end{aligned}\n\\]\nAs in the case of complete identification failure we find that \\(\\widehat{\\beta}_{2 s l s}\\) is inconsistent for \\(\\beta\\), it is asymptotically random, and its asymptotic distribution is non-normal. The distortion is affected by the coefficient \\(\\boldsymbol{C}\\). As \\(\\|\\boldsymbol{C}\\| \\rightarrow \\infty\\) the distribution in (12.72) converges in probability to zero suggesting that \\(\\widehat{\\beta}_{2 \\text { sls }}\\) is consistent for \\(\\beta\\). This corresponds to the classic “strong identification” context.\nNow consider the LIML estimator. The reduced form is \\(\\overrightarrow{\\boldsymbol{Y}}=\\boldsymbol{Z \\Pi}+\\boldsymbol{U}\\). This implies \\(\\boldsymbol{M}_{Z} \\overrightarrow{\\boldsymbol{Y}}=\\boldsymbol{M}_{Z} \\boldsymbol{U}\\) and by standard asymptotic theory\n\\[\n\\frac{1}{n} \\overrightarrow{\\boldsymbol{Y}}^{\\prime} \\boldsymbol{M}_{\\boldsymbol{Z}} \\overrightarrow{\\boldsymbol{Y}}=\\frac{1}{n} \\boldsymbol{U}^{\\prime} \\boldsymbol{M}_{\\boldsymbol{Z}} \\boldsymbol{U} \\underset{p}{\\longrightarrow}=\\mathbb{E}\\left[u u^{\\prime}\\right] .\n\\]\nDefine \\(\\bar{\\beta}=\\left[\\beta, \\boldsymbol{I}_{k}\\right]\\) so that the reduced form coefficients equal \\(\\Pi=[\\boldsymbol{\\Gamma} \\beta, \\boldsymbol{\\Gamma}]=n^{-1 / 2} \\boldsymbol{C} \\bar{\\beta}\\). Then\n\\[\n\\frac{1}{\\sqrt{n}} \\boldsymbol{Z}^{\\prime} \\overrightarrow{\\boldsymbol{Y}}=\\frac{1}{n} \\boldsymbol{Z}^{\\prime} \\boldsymbol{Z} \\boldsymbol{C} \\bar{\\beta}+\\frac{1}{\\sqrt{n}} \\boldsymbol{Z}^{\\prime} \\boldsymbol{U} \\underset{d}{\\longrightarrow} \\boldsymbol{Q}_{Z} \\boldsymbol{C} \\bar{\\beta}+\\xi\n\\]\nand\n\\[\n\\overrightarrow{\\boldsymbol{Y}}^{\\prime} \\boldsymbol{Z}\\left(\\boldsymbol{Z}^{\\prime} \\boldsymbol{Z}\\right)^{-1} \\boldsymbol{Z}^{\\prime} \\overrightarrow{\\boldsymbol{Y}} \\underset{d}{\\longrightarrow}\\left(\\boldsymbol{Q}_{Z} \\boldsymbol{C} \\bar{\\beta}+\\xi\\right)^{\\prime} \\boldsymbol{Q}_{Z}^{-1}\\left(\\boldsymbol{Q}_{Z} \\boldsymbol{C} \\bar{\\beta}+\\xi\\right) .\n\\]\nThis allows us to calculate that by the continuous mapping theorem\n\\[\n\\begin{aligned}\nn \\widehat{\\mu} &=\\min _{\\gamma} \\frac{\\gamma^{\\prime} \\overrightarrow{\\boldsymbol{Y}}^{\\prime} \\boldsymbol{Z}\\left(\\boldsymbol{Z}^{\\prime} \\boldsymbol{Z}\\right)^{-1} \\boldsymbol{Z}^{\\prime} \\overrightarrow{\\boldsymbol{Y}} \\gamma}{\\gamma^{\\prime} \\frac{1}{n} \\overrightarrow{\\boldsymbol{Y}}^{\\prime} \\boldsymbol{M}_{\\boldsymbol{Z}} \\overrightarrow{\\boldsymbol{Y}} \\gamma} \\\\\n& \\underset{d}{\\longrightarrow} \\min _{\\gamma} \\frac{\\gamma^{\\prime}\\left(\\boldsymbol{Q}_{Z} \\boldsymbol{C} \\bar{\\beta}+\\xi\\right)^{\\prime} \\boldsymbol{Q}_{Z}^{-1}\\left(\\boldsymbol{Q}_{Z} \\boldsymbol{C} \\bar{\\beta}+\\xi\\right) \\gamma}{\\gamma^{\\prime} \\Sigma \\gamma} \\\\\n&=\\mu^{*}\n\\end{aligned}\n\\]\nsay, which is a function of \\(\\xi\\) and thus random. We deduce that the asymptotic distribution of the LIML estimator is\n\\[\n\\begin{aligned}\n\\widehat{\\beta}_{\\mathrm{liml}}-\\beta=&\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{P}_{\\boldsymbol{Z}} \\boldsymbol{X}-n \\widehat{\\mu} \\frac{1}{n} \\boldsymbol{X}^{\\prime} \\boldsymbol{M}_{\\boldsymbol{Z}} \\boldsymbol{X}\\right)^{-1}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{P}_{\\boldsymbol{Z}} \\boldsymbol{e}-n \\widehat{\\mu} \\frac{1}{n} \\boldsymbol{X}^{\\prime} \\boldsymbol{M}_{\\boldsymbol{Z}} \\boldsymbol{e}\\right) \\\\\n\\underset{d}{\\longrightarrow}\\left(\\left(\\boldsymbol{Q}_{Z} \\boldsymbol{C}+\\xi_{2}\\right)^{\\prime} \\boldsymbol{Q}_{Z}^{-1}\\left(\\boldsymbol{Q}_{Z} \\boldsymbol{C}+\\xi_{2}\\right)-\\mu^{*} \\Sigma_{22}\\right)^{-1}\\left(\\left(\\boldsymbol{Q}_{Z} \\boldsymbol{C}+\\xi_{2}\\right)^{\\prime} \\boldsymbol{Q}_{Z}^{-1} \\xi_{e}-\\mu^{*} \\Sigma_{2 e}\\right) .\n\\end{aligned}\n\\]\nSimilarly to 2SLS, the LIML estimator is inconsistent for \\(\\beta\\), is asymptotically random, and non-normally distributed.\nWe summarize.\nTheorem 12.18 Under (12.71),\n\\[\n\\begin{gathered}\n\\widehat{\\beta}_{\\mathrm{ols}}-\\beta \\underset{p}{\\longrightarrow} \\Sigma_{22}^{-1} \\Sigma_{2 e} \\\\\n\\widehat{\\beta}_{2 \\mathrm{sls}}-\\beta \\underset{d}{\\longrightarrow}\\left(\\left(\\boldsymbol{Q}_{Z} \\boldsymbol{C}+\\xi_{2}\\right)^{\\prime} \\boldsymbol{Q}_{Z}^{-1}\\left(\\boldsymbol{Q}_{Z} \\boldsymbol{C}+\\xi_{2}\\right)\\right)^{-1}\\left(\\boldsymbol{Q}_{Z} \\boldsymbol{C}+\\xi_{2}\\right)^{\\prime} \\boldsymbol{Q}_{Z}^{-1} \\xi_{e}\n\\end{gathered}\n\\]\nand\n\\[\n\\begin{aligned}\n&\\widehat{\\beta}_{\\mathrm{liml}}-\\beta \\underset{d}{\\longrightarrow}\\left(\\left(\\boldsymbol{Q}_{Z} \\boldsymbol{C}+\\xi_{2}\\right)^{\\prime} \\boldsymbol{Q}_{Z}^{-1}\\left(\\boldsymbol{Q}_{Z} \\boldsymbol{C}+\\xi_{2}\\right)-\\mu^{*} \\Sigma_{22}\\right)^{-1} \\\\\n&\\times\\left(\\left(\\boldsymbol{Q}_{Z} \\boldsymbol{C}+\\xi_{2}\\right)^{\\prime} \\boldsymbol{Q}_{Z}^{-1} \\xi_{e}-\\mu^{*} \\boldsymbol{\\Sigma}_{2 e}\\right)\n\\end{aligned}\n\\]\nwhere\n\\[\n\\mu^{*}=\\min _{\\gamma} \\frac{\\gamma^{\\prime}\\left(\\boldsymbol{Q}_{Z} \\boldsymbol{C} \\bar{\\beta}+\\xi\\right)^{\\prime} \\boldsymbol{Q}_{Z}^{-1}\\left(\\boldsymbol{Q}_{Z} \\boldsymbol{C} \\bar{\\beta}+\\xi\\right) \\gamma}{\\gamma^{\\prime} \\Sigma \\gamma}\n\\]\nand \\(\\bar{\\beta}=\\left[\\beta, I_{k}\\right]\\)\nAll three estimators are inconsistent. The 2SLS and LIML estimators are asymptotically random with non-standard distributions, similar to the asymptotic distribution of the IV estimator under complete identification failure explored in the previous section. The difference under weak identification is the presence of the coefficient matrix \\(\\boldsymbol{C}\\)."
  },
  {
    "objectID": "chpt12-iv.html#many-instruments",
    "href": "chpt12-iv.html#many-instruments",
    "title": "12  Instrumental Variables",
    "section": "12.37 Many Instruments",
    "text": "12.37 Many Instruments\nSome applications have available a large number \\(\\ell\\) of instruments. If they are all valid, using a large number should reduce the asymptotic variance relative to estimation with a smaller number of instruments. Is it then good practice to use many instruments? Or is there a cost to this practice? Bekker (1994) initiated a large literature investigating this question by formalizing the idea of “many instruments”. Bekker proposed an asymptotic approximation which treats the number of instruments \\(\\ell\\) as proportional to the sample size, that is \\(\\ell=\\alpha n\\), or equivalently that \\(\\ell / n \\rightarrow \\alpha \\in[0,1)\\). The distributional theory obtained is similar in many respects to the weak instrument theory outlined in the previous section. Consequently the impact of “weak” and “many” instruments is similar.\nAgain for simplicity we assume that there are no included exogenous regressors so that the model is\n\\[\n\\begin{aligned}\n&Y=X^{\\prime} \\beta+e \\\\\n&X=\\Gamma^{\\prime} Z+u_{2}\n\\end{aligned}\n\\]\nwith \\(Z \\ell \\times 1\\). We also make the simplifying assumption that the reduced form errors are conditionally homoskedastic. Specifically,\n\\[\n\\mathbb{E}\\left[u u^{\\prime} \\mid Z\\right]=\\Sigma=\\left[\\begin{array}{cc}\n\\Sigma_{11} & \\Sigma_{12} \\\\\n\\Sigma_{21} & \\Sigma_{22}\n\\end{array}\\right] .\n\\]\nIn addition we assume that the conditional fourth moments are bounded\n\\[\n\\mathbb{E}\\left[\\|u\\|^{4} \\mid Z\\right] \\leq B<\\infty .\n\\]\nThe idea that there are “many instruments” is formalized by the assumption that the number of instruments is increasing proportionately with the sample size\n\\[\n\\frac{\\ell}{n} \\longrightarrow \\alpha .\n\\]\nThe best way to think about this is to view \\(\\alpha\\) as the ratio of \\(\\ell\\) to \\(n\\) in a given sample. Thus if an application has \\(n=100\\) observations and \\(\\ell=10\\) instruments, then we should treat \\(\\alpha=0.10\\).\nSuppose that there is a single endogenous regressor \\(X\\). Calculate its variance using the reduced form: \\(\\operatorname{var}[X]=\\operatorname{var}\\left[Z^{\\prime} \\Gamma\\right]+\\operatorname{var}[u]\\). Suppose as well that \\(\\operatorname{var}[X]\\) and \\(\\operatorname{var}[u]\\) are unchanging as \\(\\ell\\) increases. This implies that \\(\\operatorname{var}\\left[Z^{\\prime} \\Gamma\\right]\\) is unchanging even though the dimension \\(\\ell\\) is increasing. This is a useful assumption as it implies that the population \\(R^{2}\\) of the reduced form is not changing with \\(\\ell\\). We don’t need this exact condition, rather we simply assume that the sample version converges in probability to a fixed constant. Specifically, we assume that\n\\[\n\\frac{1}{n} \\sum_{i=1}^{n} \\Gamma^{\\prime} Z_{i} Z_{i}^{\\prime} \\Gamma \\underset{p}{\\longrightarrow} \\boldsymbol{H}\n\\]\nfor some matrix \\(\\boldsymbol{H}>0\\). Again, this essentially implies that the \\(R^{2}\\) of the reduced form regressions for each component of \\(X\\) converge to constants.\nAs a baseline it is useful to examine the behavior of the least squares estimator of \\(\\beta\\). First, observe that the variance of \\(\\operatorname{vec}\\left(n^{-1} \\sum_{i=1}^{n} \\Gamma^{\\prime} Z_{i} u_{i}^{\\prime}\\right)\\), conditional on \\(Z\\), is\n\\[\n\\Sigma \\otimes n^{-2} \\sum_{i=1}^{n} \\Gamma^{\\prime} Z_{i} Z_{i}^{\\prime} \\Gamma \\underset{p}{\\longrightarrow} 0\n\\]\nby (12.77). Thus it converges in probability to zero:\n\\[\nn^{-1} \\sum_{i=1}^{n} \\Gamma^{\\prime} Z_{i} u_{i}^{\\prime} \\underset{p}{\\longrightarrow} 0 .\n\\]\nCombined with (12.77) and the WLLN we find\n\\[\n\\frac{1}{n} \\sum_{i=1}^{n} X_{i} e_{i}=\\frac{1}{n} \\sum_{i=1}^{n} \\Gamma^{\\prime} Z_{i} e_{i}+\\frac{1}{n} \\sum_{i=1}^{n} u_{2 i} e_{i} \\underset{p}{\\longrightarrow} \\Sigma_{2 e}\n\\]\nand\n\\[\n\\frac{1}{n} \\sum_{i=1}^{n} X_{i} X_{i}^{\\prime}=\\frac{1}{n} \\sum_{i=1}^{n} \\Gamma^{\\prime} Z_{i} Z_{i}^{\\prime} \\Gamma+\\frac{1}{n} \\sum_{i=1}^{n} \\Gamma^{\\prime} Z_{i} u_{2 i}^{\\prime}+\\frac{1}{n} \\sum_{i=1}^{n} u_{2 i} Z_{i}^{\\prime} \\Gamma+\\frac{1}{n} \\sum_{i=1}^{n} u_{2 i} u_{2 i}^{\\prime} \\underset{p}{\\rightarrow} \\boldsymbol{H}+\\Sigma_{22}\n\\]\nHence\n\\[\n\\widehat{\\beta}_{\\mathrm{ols}}=\\beta+\\left(\\frac{1}{n} \\sum_{i=1}^{n} X_{i} X_{i}^{\\prime}\\right)^{-1}\\left(\\frac{1}{n} \\sum_{i=1}^{n} X_{i} e_{i}\\right) \\underset{p}{\\longrightarrow} \\beta+\\left(\\boldsymbol{H}+\\Sigma_{22}\\right)^{-1} \\Sigma_{2 e}\n\\]\nThus least squares is inconsistent for \\(\\beta\\).\nNow consider the 2SLS estimator. In matrix notation, setting \\(\\boldsymbol{P}_{Z}=\\boldsymbol{Z}\\left(\\boldsymbol{Z}^{\\prime} \\boldsymbol{Z}\\right)^{-1} \\boldsymbol{Z}^{\\prime}\\),\n\\[\n\\begin{aligned}\n\\widehat{\\beta}_{2 \\mathrm{sls}}-\\beta &=\\left(\\frac{1}{n} \\boldsymbol{X}^{\\prime} \\boldsymbol{P}_{\\boldsymbol{Z}} \\boldsymbol{X}\\right)^{-1}\\left(\\frac{1}{n} \\boldsymbol{X}^{\\prime} \\boldsymbol{P}_{\\boldsymbol{Z}} \\boldsymbol{e}\\right) \\\\\n&=\\left(\\frac{1}{n} \\bar{\\Gamma}^{\\prime} \\boldsymbol{Z}^{\\prime} \\boldsymbol{Z} \\bar{\\Gamma}+\\frac{1}{n} \\bar{\\Gamma}^{\\prime} \\boldsymbol{Z}^{\\prime} \\boldsymbol{u}_{2}+\\frac{1}{n} \\boldsymbol{u}_{2}^{\\prime} \\boldsymbol{Z} \\bar{\\Gamma}+\\frac{1}{n} \\boldsymbol{u}_{2}^{\\prime} \\boldsymbol{P}_{Z} \\boldsymbol{u}_{2}\\right)^{-1}\\left(\\frac{1}{n} \\Gamma^{\\prime} \\boldsymbol{Z}^{\\prime} \\boldsymbol{e}+\\frac{1}{n} \\boldsymbol{u}_{2}^{\\prime} \\boldsymbol{P}_{Z} \\boldsymbol{e}\\right)\n\\end{aligned}\n\\]\nIn the expression on the right-side of (12.79) several of the components have been examined in (12.77) and (12.78). We now examine the remaining components \\(\\frac{1}{n} \\boldsymbol{u}_{2}^{\\prime} \\boldsymbol{P}_{\\boldsymbol{Z}} \\boldsymbol{e}\\) and \\(\\frac{1}{n} \\boldsymbol{u}_{2}^{\\prime} \\boldsymbol{P}_{\\boldsymbol{Z}} \\boldsymbol{u}_{2}\\) which are sub-components of the matrix \\(\\frac{1}{n} \\boldsymbol{u}^{\\prime} \\boldsymbol{P}_{\\boldsymbol{Z}} \\boldsymbol{u}\\). Take the \\(j k^{t h}\\) element \\(\\frac{1}{n} \\boldsymbol{u}_{j}^{\\prime} \\boldsymbol{P}_{\\boldsymbol{Z}} \\boldsymbol{u}_{k}\\).\nFirst, take its expectation. We have (given under the conditional homoskedasticity assumption (12.74))\n\\[\n\\mathbb{E}\\left[\\frac{1}{n} \\boldsymbol{u}_{j}^{\\prime} \\boldsymbol{P}_{\\boldsymbol{Z}} \\boldsymbol{u}_{k} \\mid \\boldsymbol{Z}\\right]=\\frac{1}{n} \\operatorname{tr}\\left(\\mathbb{E}\\left[\\boldsymbol{P}_{\\boldsymbol{Z}} \\boldsymbol{u}_{k} \\boldsymbol{u}_{j}^{\\prime} \\mid \\boldsymbol{Z}\\right]\\right)=\\frac{1}{n} \\operatorname{tr}\\left(\\boldsymbol{P}_{\\boldsymbol{Z}}\\right) \\Sigma_{j k}=\\frac{\\ell}{n} \\Sigma_{j k} \\rightarrow \\alpha \\Sigma_{j k}\n\\]\nusing \\(\\operatorname{tr}\\left(\\boldsymbol{P}_{Z}\\right)=\\ell\\).\nSecond, we calculate its variance which is a more cumbersome exercise. Let \\(P_{i m}=Z_{i}^{\\prime}\\left(\\boldsymbol{Z}^{\\prime} \\boldsymbol{Z}\\right)^{-1} Z_{m}\\) be the \\(i m^{t h}\\) element of \\(\\boldsymbol{P}_{\\boldsymbol{Z}}\\). Then \\(\\boldsymbol{u}_{j}^{\\prime} \\boldsymbol{P}_{\\boldsymbol{Z}} \\boldsymbol{u}_{k}=\\sum_{i=1}^{n} \\sum_{m=1}^{n} u_{j i} u_{k m} P_{i m}\\). The matrix \\(\\boldsymbol{P}_{\\boldsymbol{Z}}\\) is idempotent. It therefore has the properties \\(\\sum_{i=1}^{n} P_{i i}=\\operatorname{tr}\\left(\\boldsymbol{P}_{Z}\\right)=\\ell\\) and \\(0 \\leq P_{i i} \\leq 1\\). The property \\(\\boldsymbol{P}_{Z} \\boldsymbol{P}_{Z}=\\boldsymbol{P}_{Z}\\) also implies \\(\\sum_{m=1}^{n} P_{i m}^{2}=P_{i i}\\). Then\n\\[\n\\begin{aligned}\n\\operatorname{var}\\left[\\frac{1}{n} \\boldsymbol{u}_{j}^{\\prime} \\boldsymbol{P}_{\\boldsymbol{Z}} \\boldsymbol{u}_{k} \\mid \\boldsymbol{Z}\\right] &=\\frac{1}{n^{2}} \\mathbb{E}\\left[\\sum_{i=1}^{n} \\sum_{m=1}^{n}\\left(u_{j i} u_{k m}-\\mathbb{E}\\left[u_{j i} u_{k m}\\right] \\mathbb{1}\\{i=m\\}\\right) P_{i m} \\mid \\boldsymbol{Z}\\right]^{2} \\\\\n&=\\frac{1}{n^{2}} \\mathbb{E}\\left[\\sum_{i=1}^{n} \\sum_{m=1}^{n} \\sum_{q=1}^{n} \\sum_{r=1}^{n}\\left(u_{j i} u_{k m}-\\Sigma_{j k} \\mathbb{1}\\{i=m\\}\\right) P_{i m}\\left(u_{j q} u_{k r}-\\Sigma_{j k} \\mathbb{1}\\{q=r\\}\\right) P_{q r}\\right] \\\\\n&=\\frac{1}{n^{2}} \\sum_{i=1}^{n} \\mathbb{E}\\left[\\left(u_{j i} u_{k i}-\\Sigma_{j k}\\right)^{2}\\right] P_{i i}^{2} \\\\\n&+\\frac{1}{n^{2}} \\sum_{i=1}^{n} \\sum_{m \\neq i} \\mathbb{E}\\left[u_{j i}^{2} u_{k m}^{2}\\right] P_{i m}^{2}+\\frac{1}{n^{2}} \\sum_{i=1}^{n} \\sum_{m \\neq i} \\mathbb{E}\\left[u_{j i} u_{k m} u_{j m} u_{k i}\\right] P_{i m}^{2} \\\\\n& \\leq \\frac{B}{n^{2}}\\left(\\sum_{i=1}^{n} P_{i i}^{2}+2 \\sum_{i=1}^{n} \\sum_{m=1}^{n} P_{i m}^{2}\\right) \\\\\n& \\leq \\frac{3 B}{n^{2}} \\sum_{i=1}^{n} P_{i i} \\\\\n&=3 B \\frac{\\ell}{n^{2}} \\rightarrow 0 .\n\\end{aligned}\n\\]\nThe third equality holds because the remaining cross-products have zero expectation as the observations are independent and the errors have zero mean. The first inequality is (12.75). The second uses \\(P_{i i}^{2} \\leq P_{i i}\\) and \\(\\sum_{m=1}^{n} P_{i m}^{2}=P_{i i}\\). The final equality is \\(\\sum_{i=1}^{n} P_{i i}=\\ell\\).\nUsing (12.76), (12.80), Markov’s inequality (B.36), and combining across all \\(j\\) and \\(k\\) we deduce that\n\\[\n\\frac{1}{n} \\boldsymbol{u}^{\\prime} \\boldsymbol{P}_{\\boldsymbol{Z}} \\boldsymbol{u} \\underset{p}{\\longrightarrow} \\alpha \\Sigma .\n\\]\nReturning to the 2SLS estimator (12.79) and combining (12.77), (12.78), and (12.81), we find\n\\[\n\\widehat{\\beta}_{2 \\text { sls }}-\\beta \\underset{p}{\\longrightarrow}\\left(\\boldsymbol{H}+\\alpha \\Sigma_{22}\\right)^{-1} \\alpha \\Sigma_{2 e} .\n\\]\nThus 2SLS is also inconsistent for \\(\\beta\\). The limit, however, depends on the magnitude of \\(\\alpha\\).\nWe finally examine the LIML estimator. (12.81) implies\n\\[\n\\frac{1}{n} \\boldsymbol{Y}^{\\prime} \\boldsymbol{M}_{Z} \\boldsymbol{Y}=\\frac{1}{n} \\boldsymbol{u}^{\\prime} \\boldsymbol{u}-\\frac{1}{n} \\boldsymbol{u}^{\\prime} \\boldsymbol{P}_{\\boldsymbol{Z}} \\boldsymbol{u} \\underset{p}{\\longrightarrow}(1-\\alpha) \\Sigma .\n\\]\nSimilarly\n\\[\n\\begin{aligned}\n\\frac{1}{n} \\boldsymbol{Y}^{\\prime} \\boldsymbol{Z}\\left(\\boldsymbol{Z}^{\\prime} \\boldsymbol{Z}\\right)^{-1} \\boldsymbol{Z}^{\\prime} \\boldsymbol{Y} &=\\bar{\\beta}^{\\prime} \\Gamma^{\\prime}\\left(\\frac{1}{n} \\boldsymbol{Z}^{\\prime} \\boldsymbol{Z}\\right) \\Gamma \\bar{\\beta}+\\bar{\\beta}^{\\prime} \\Gamma^{\\prime}\\left(\\frac{1}{n} \\boldsymbol{Z}^{\\prime} \\boldsymbol{u}\\right)+\\left(\\frac{1}{n} \\boldsymbol{u}^{\\prime} \\boldsymbol{Z}\\right) \\Gamma \\bar{\\beta}+\\frac{1}{n} \\boldsymbol{u}^{\\prime} \\boldsymbol{P}_{\\boldsymbol{Z}} \\boldsymbol{u} \\\\\n& \\underset{d}{\\longrightarrow} \\bar{\\beta}^{\\prime} \\boldsymbol{H} \\bar{\\beta}+\\alpha \\Sigma .\n\\end{aligned}\n\\]\nHence\n\\[\n\\widehat{\\mu}=\\min _{\\gamma} \\frac{\\gamma^{\\prime} \\boldsymbol{Y}^{\\prime} \\boldsymbol{Z}\\left(\\boldsymbol{Z}^{\\prime} \\boldsymbol{Z}\\right)^{-1} \\boldsymbol{Z}^{\\prime} \\boldsymbol{Y} \\gamma}{\\gamma^{\\prime} \\boldsymbol{Y}^{\\prime} \\boldsymbol{M}_{\\boldsymbol{Z}} \\boldsymbol{Y} \\gamma} \\underset{d}{\\longrightarrow} \\min _{\\gamma} \\frac{\\gamma^{\\prime}\\left(\\bar{\\beta}^{\\prime} \\boldsymbol{H} \\bar{\\beta}+\\alpha \\Sigma\\right) \\gamma}{\\gamma^{\\prime}(1-\\alpha) \\Sigma \\gamma}=\\frac{\\alpha}{1-\\alpha}\n\\]\nand\n\\[\n\\begin{aligned}\n\\widehat{\\beta}_{\\mathrm{liml}}-\\beta &=\\left(\\frac{1}{n} \\boldsymbol{X}^{\\prime} \\boldsymbol{P}_{\\boldsymbol{Z}} \\boldsymbol{X}-\\widehat{\\mu} \\frac{1}{n} \\boldsymbol{X}^{\\prime} \\boldsymbol{M}_{\\boldsymbol{Z}} \\boldsymbol{X}\\right)^{-1}\\left(\\frac{1}{n} \\boldsymbol{X}^{\\prime} \\boldsymbol{P}_{Z} \\boldsymbol{e}-\\widehat{\\mu} \\frac{1}{n} \\boldsymbol{X}^{\\prime} \\boldsymbol{M}_{\\boldsymbol{Z}} \\boldsymbol{e}\\right) \\\\\n& \\underset{d}{\\longrightarrow}\\left(\\boldsymbol{H}+\\alpha \\Sigma_{22}-\\frac{\\alpha}{1-\\alpha}(1-\\alpha) \\Sigma_{22}\\right)^{-1}\\left(\\alpha \\Sigma_{2 e}-\\frac{\\alpha}{1-\\alpha}(1-\\alpha) \\Sigma_{2 e}\\right) \\\\\n&=\\boldsymbol{H}^{-1} 0 \\\\\n&=0 .\n\\end{aligned}\n\\]\nThus LIML is consistent for \\(\\beta\\), unlike 2SLS.\nWe state these results formally.\nTheorem 12.19 In model (12.73), under assumptions (12.74), (12.75) and (12.76), then as \\(n \\rightarrow \\infty\\).\n\\[\n\\begin{aligned}\n&\\widehat{\\beta}_{\\text {ols }} \\underset{p}{\\longrightarrow} \\beta+\\left(\\boldsymbol{H}+\\Sigma_{22}\\right)^{-1} \\Sigma_{2 e} \\\\\n&\\widehat{\\beta}_{2 \\text { sls }} \\underset{p}{\\longrightarrow} \\beta+\\left(\\boldsymbol{H}+\\alpha \\Sigma_{22}\\right)^{-1} \\alpha \\Sigma_{2 e} \\\\\n&\\widehat{\\beta}_{\\text {liml }} \\underset{p}{\\longrightarrow} \\beta .\n\\end{aligned}\n\\]\nThis result is quite insightful. It shows that while endogeneity \\(\\left(\\Sigma_{2 e} \\neq 0\\right)\\) renders the least squares estimator inconsistent, the 2SLS estimator is also inconsistent if the number of instruments diverges proportionately with \\(n\\). The limit in Theorem \\(12.19\\) shows a continuity between least squares and 2 SLS. The probability limit of the 2SLS estimator is continuous in \\(\\alpha\\), with the extreme case \\((\\alpha=1)\\) implying that 2SLS and least squares have the same probability limit. The general implication is that the inconsistency of 2 SLS is increasing in \\(\\alpha\\).\nThe theorem also shows that unlike 2SLS the LIML estimator is consistent under the many instruments assumption. Effectively, LIML makes a bias-correction.\nTheorems \\(12.18\\) (weak instruments) and \\(12.19\\) (many instruments) tell a cautionary tale. They show that when instruments are weak and/or many the 2SLS estimator is inconsistent. The degree of inconsistency depends on the weakness of the instruments (the magnitude of the matrix \\(\\boldsymbol{C}\\) in Theorem 12.18) and the degree of overidentification (the ratio \\(\\alpha\\) in Theorem 12.19). The Theorems also show that the LIML estimator is inconsistent under the weak instrument assumption but with a bias-correction, and is consistent under the many instrument assumption. This suggests that LIML is more robust than 2SLS to weak and many instruments.\nAn important limitation of the results in Theorem \\(12.19\\) is the assumption of conditional homoskedasticity. It appears likely that the consistency of LIML fails in the many instrument setting if the errors are heteroskedastic.\nIn applications users should be aware of the potential consequences of the many instrument framework. It is useful to calculate the “many instrument ratio” \\(\\alpha=\\ell / n\\). While there is no specific rule-ofthumb for \\(\\alpha\\) which leads to acceptable inference a minimum criterion is that if \\(\\alpha \\geq 0.05\\) you should be seriously concerned about the many-instrument problem. In general, when \\(\\alpha\\) is large it seems preferable to use LIML instead of 2SLS."
  },
  {
    "objectID": "chpt12-iv.html#testing-for-weak-instruments",
    "href": "chpt12-iv.html#testing-for-weak-instruments",
    "title": "12  Instrumental Variables",
    "section": "12.38 Testing for Weak Instruments",
    "text": "12.38 Testing for Weak Instruments\nIn the previous sections we found that weak instruments results in non-standard asymptotic distributions for the 2SLS and LIML estimators. In practice how do we know if this is a problem? Is there a way to check if the instruments are weak?\nThis question was addressed in an influential paper by Stock and Yogo (2005) as an extension of Staiger and Stock (1997). Stock-Yogo focus on two implications of weak instruments: (1) estimation bias and (2) inference distortion. They show how to test the hypothesis that these distortions are not “too big”. They propose \\(F\\) tests for the excluded instruments in the reduced form regressions with non-standard critical values. In particular, when there is one endogenous regressor and a single instrument the StockYogo test rejects the null of weak instruments when this \\(F\\) statistic exceeds 10 . While Stock and Yogo explore two types of distortions, we focus exclusively on inference as that is the more challenging problem. In this section we describe the Stock-Yogo theory and tests for the case of a single endogenous regressor \\(\\left(k_{2}=1\\right)\\). In the following section we describe their method for the case of multiple endogeneous regressors.\nWhile the theory in Stock and Yogo allows for an arbitrary number of exogenous regressors and instruments, for the sake of clear exposition we will focus on the very simple case of no included exogenous variables \\(\\left(k_{1}=0\\right)\\) and just one exogenous instrument \\(\\left(\\ell_{2}=1\\right)\\) which is model (12.69) from Section 12.35.\n\\[\n\\begin{aligned}\n&Y=X \\beta+e \\\\\n&X=Z \\Gamma+u .\n\\end{aligned}\n\\]\nFurthermore, as in Section \\(12.35\\) we assume conditional homoskedasticity and normalize the variances as in (12.70). Since the model is just-identified the 2SLS, LIML, and IV estimators are all equivalent.\nThe question of primary interest is to determine conditions on the reduced form under which the IV estimator of the structural equation is well behaved, and secondly, what statistical tests can be used to learn if these conditions are satisfied. As in Section \\(12.36\\) we assume that the reduced form coefficient \\(\\Gamma\\) is local-to-zero, specifically \\(\\Gamma=n^{-1 / 2} \\mu\\). The asymptotic distribution of the IV estimator is presented in Theorem 12.18. Given the simplifying assumptions the result is\n\\[\n\\widehat{\\beta}_{\\mathrm{iv}}-\\beta \\underset{d}{\\longrightarrow} \\frac{\\xi_{e}}{\\mu+\\xi_{2}}\n\\]\nwhere \\(\\left(\\xi_{e}, \\xi_{2}\\right)\\) are bivariate normal. For inference we also examine the behavior of the classical (ho- moskedastic) t-statistic for the IV estimator. Note\n\\[\n\\begin{aligned}\n\\widehat{\\sigma}^{2} &=\\frac{1}{n} \\sum_{i=1}^{n}\\left(Y_{i}-X_{i} \\widehat{\\beta}_{\\mathrm{iv}}\\right)^{2} \\\\\n&=\\frac{1}{n} \\sum_{i=1}^{n} e_{i}^{2}-\\frac{2}{n} \\sum_{i=1}^{n} e_{i} X_{i}\\left(\\widehat{\\beta}_{\\mathrm{iv}}-\\beta\\right)+\\frac{1}{n} \\sum_{i=1}^{n} X_{i}^{2}\\left(\\widehat{\\beta}_{\\mathrm{iv}}-\\beta\\right)^{2} \\\\\n& \\underset{d}{\\longrightarrow} 1-2 \\rho \\frac{\\xi_{e}}{\\mu+\\xi_{2}}+\\left(\\frac{\\xi_{e}}{\\mu+\\xi_{2}}\\right)^{2} .\n\\end{aligned}\n\\]\nThus\n\nIn general, \\(S\\) is non-normal and its distribution depends on the parameters \\(\\rho\\) and \\(\\mu\\).\nCan we use the distribution \\(S\\) for inference on \\(\\beta\\) ? The distribution depends on two unknown parameters and neither is consistently estimable. This means we cannot use the distribution in (12.82) with \\(\\rho\\) and \\(\\mu\\) replaced with estimates. To eliminate the dependence on \\(\\rho\\) one possibility is to use the “worst case” value which turns out to be \\(\\rho=1\\). By worst-case we mean the value which causes the greatest distortion away from normal critical values. Setting \\(\\rho=1\\) we have the considerable simplification\n\\[\nS=S_{1}=\\xi\\left|1+\\frac{\\xi}{\\mu}\\right|\n\\]\nwhere \\(\\xi \\sim \\mathrm{N}(0,1)\\). When the model is strongly identified (so \\(|\\mu|\\) is very large) then \\(S_{1} \\approx \\xi\\) is standard normal, consistent with classical theory. However when \\(|\\mu|\\) is very small (but non-zero) \\(\\left|S_{1}\\right| \\approx \\xi^{2} / \\mu\\) (in the sense that this term dominates), which is a scaled \\(\\chi_{1}^{2}\\) and quite far from normal. As \\(|\\mu| \\rightarrow 0\\) we find the extreme case \\(\\left|S_{1}\\right| \\rightarrow p \\infty\\).\nWhile (12.83) is a convenient simplification it does not yield a useful approximation for inference as the distribution in (12.83) is highly dependent on the unknown \\(\\mu\\). If we take the worst-case value of \\(\\mu\\), which is \\(\\mu=0\\), we find that \\(\\left|S_{1}\\right|\\) diverges and all distributional approximations fail.\nTo break this impasse Stock and Yogo (2005) recommended a constructive alternative. Rather than using the worst-case \\(\\mu\\) they suggested finding a threshold such that if \\(\\mu\\) exceeds this threshold then the distribution (12.83) is not “too badly” distorted from the normal distribution.\nSpecifically, the Stock-Yogo recommendation can be summarized by two steps. First, the distribution result (12.83) can be used to find a threshold value \\(\\tau^{2}\\) such that if \\(\\mu^{2} \\geq \\tau^{2}\\) then the size of the nominal \\({ }^{1}\\) 5% test “Reject if \\(|T| \\geq 1.96\\)” has asymptotic size \\(\\mathbb{P}\\left[\\left|S_{1}\\right| \\geq 1.96\\right] \\leq 0.15\\). This means that while the goal is to obtain a test with size \\(5 %\\), we recognize that there may be size distortion due to weak instruments and are willing to tolerate a specific distortion. For example, a \\(10 %\\) distortion means we allow the actual size to be up to \\(15 %\\). Second, they use the asymptotic distribution of the reduced-form (first stage) \\(F\\) statistic to test if the actual unknown value of \\(\\mu^{2}\\) exceeds the threshold \\(\\tau^{2}\\). These two steps together give rise to the rule-of-thumb that the first-stage \\(F\\) statistic should exceed 10 in order to achieve reliable IV inference. (This is for the case of one instrumental variable. If there is more than one instrument then the rule-of-thumb changes.) We now describe the steps behind this reasoning in more detail.\nThe first step is to use the distribution (12.82) to determine the threshold \\(\\tau^{2}\\). Formally, the goal is to find the value of \\(\\tau^{2}=\\mu^{2}\\) at which the asymptotic size of a nominal \\(5 %\\) test is actually a given \\(r\\) (e.g.\n\\({ }^{1}\\) The term “nominal size” of a test is the official intended size - the size which would obtain under ideal circumstances. In this context the test “Reject if \\(|T| \\geq 1.96\\)” has nominal size \\(0.05\\) as this would be the asymptotic rejection probability in the ideal context of strong instruments. \\(r=0.15)\\), thus \\(\\mathbb{P}\\left[\\left|S_{1}\\right| \\geq 1.96\\right] \\leq r\\). By some algebra and the quadratic formula the event \\(|\\xi(1+\\xi / \\mu)|<x\\) is the same as\n\\[\n\\frac{\\mu^{2}}{4}-x \\mu<\\left(\\xi+\\frac{\\mu}{2}\\right)^{2}<\\frac{\\mu^{2}}{4}+x \\mu .\n\\]\nThe random variable between the inequalities is distributed \\(\\chi_{1}^{2}\\left(\\mu^{2} / 4\\right)\\), a noncentral chi-square with one degree of freedom and noncentrality parameter \\(\\mu^{2} / 4\\). Thus\n\\[\n\\begin{aligned}\n\\mathbb{P}\\left[\\left|S_{1}\\right| \\geq x\\right] &=\\mathbb{P}\\left[\\chi_{1}^{2}\\left(\\frac{\\mu^{2}}{4}\\right) \\geq \\frac{\\mu^{2}}{4}+x \\mu\\right]+\\mathbb{P}\\left[\\chi_{1}^{2}\\left(\\frac{\\mu^{2}}{4}\\right) \\leq \\frac{\\mu^{2}}{4}-x \\mu\\right] \\\\\n&=1-G\\left(\\frac{\\mu^{2}}{4}+x \\mu, \\frac{\\mu^{2}}{4}\\right)+G\\left(\\frac{\\mu^{2}}{4}-x \\mu, \\frac{\\mu^{2}}{4}\\right)\n\\end{aligned}\n\\]\nwhere \\(G(u, \\lambda)\\) is the distribution function of \\(\\chi_{1}^{2}(\\lambda)\\). Hence the desired threshold \\(\\tau^{2}\\) solves\n\\[\n1-G\\left(\\frac{\\tau^{2}}{4}+1.96 \\tau, \\frac{\\tau^{2}}{4}\\right)+G\\left(\\frac{\\tau^{2}}{4}-1.96 \\tau, \\frac{\\tau^{2}}{4}\\right)=r\n\\]\nor effectively\n\\[\nG\\left(\\frac{\\tau^{2}}{4}+1.96 \\tau, \\frac{\\tau^{2}}{4}\\right)=1-r\n\\]\nbecause \\(\\tau^{2} / 4-1.96 \\tau<0\\) for relevant values of \\(\\tau\\). The numerical solution (computed with the non-central chi-square distribution function, e.g. ncx \\(2 c d f\\) in MATLAB) is \\(\\tau^{2}=1.70\\) when \\(r=0.15\\). (That is, the command\n\\[\n\\operatorname{ncx} 2 \\mathrm{cdf}(1.7 / 4+1.96 * \\operatorname{sqrt}(1.7), 1,1.7 / 4)\n\\]\nyields the answer \\(0.8500\\). Stock and Yogo (2005) approximate the same calculation using simulation methods and report \\(\\tau^{2}=1.82\\).)\nThis calculation means that if the reduced form satisfies \\(\\mu^{2} \\geq 1.7\\), or equivalently if \\(\\Gamma^{2} \\geq 1.7 / n\\), then the asymptotic size of a nominal \\(5 %\\) test on the structural parameter is no larger than \\(15 %\\).\nTo summarize the Stock-Yogo first step, we calculate the minimum value \\(\\tau^{2}\\) for \\(\\mu^{2}\\) sufficient to ensure that the asymptotic size of a nominal 5% t-test does not exceed \\(r\\), and find that \\(\\tau^{2}=1.70\\) for \\(r=0.15\\).\nThe Stock-Yogo second step is to find a critical value for the first-stage \\(F\\) statistic sufficient to reject the hypothesis that \\(\\mathbb{M}_{0}: \\mu^{2}=\\tau^{2}\\) against \\(\\mathbb{M}_{1}: \\mu^{2}>\\tau^{2}\\). We now describe this procedure.\nThey suggest testing \\(\\mathbb{M}_{0}: \\mu^{2}=\\tau^{2}\\) at the \\(5 %\\) size using the first stage \\(F\\) statistic. If the \\(F\\) statistic is small so that the test does not reject then we should be worried that the true value of \\(\\mu^{2}\\) is small and there is a weak instrument problem. On the other hand if the \\(F\\) statistic is large so that the test rejects then we can have some confidence that the true value of \\(\\mu^{2}\\) is sufficiently large that the weak instrument problem is not too severe.\nTo implement the test we need to calculate an appropriate critical value. It should be calculated under the null hypothesis \\(\\mathbb{H}_{0}: \\mu^{2}=\\tau^{2}\\). This is different from a conventional \\(F\\) test which is calculated under \\(\\mathbb{M}_{0}: \\mu^{2}=0\\).\nWe start by calculating the asymptotic distribution of \\(\\mathrm{F}\\). Since there is one regressor and one instrument in our simplified setting the first-stage \\(F\\) statistic is the squared t-statistic from the reduced form. Given our previous calculations it has the asymptotic distribution\n\\[\n\\mathrm{F}=\\frac{\\widehat{\\gamma}^{2}}{s(\\widehat{\\gamma})^{2}}=\\frac{\\left(\\sum_{i=1}^{n} Z_{i} X_{i}\\right)^{2}}{\\left(\\sum_{i=1}^{n} X_{i}^{2}\\right) \\widehat{\\sigma}_{u}^{2}} \\underset{d}{ }\\left(\\mu+\\xi_{2}\\right)^{2} \\sim \\chi_{1}^{2}\\left(\\mu^{2}\\right) .\n\\]\nThis is a non-central chi-square distribution \\(G\\left(u, \\mu^{2}\\right)\\) with one degree of freedom and non-centrality parameter \\(\\mu^{2}\\). To test \\(\\mathbb{M}_{0}: \\mu^{2}=\\tau^{2}\\) against \\(\\mathbb{M}_{1}: \\mu^{2}>\\tau^{2}\\) we reject for \\(\\mathrm{F} \\geq c\\) where \\(c\\) is selected so that the asymptotic rejection probability satisfies\n\\[\n\\mathbb{P}\\left[\\mathrm{F} \\geq c \\mid \\mu^{2}=\\tau^{2}\\right] \\rightarrow \\mathbb{P}\\left[\\chi_{1}^{2}\\left(\\tau^{2}\\right) \\geq c\\right]=1-G\\left(c, \\tau^{2}\\right)=0.05\n\\]\nfor \\(\\tau^{2}=1.70\\), or equivalently \\(G(c, 1.7)=0.95\\). This is found by inverting the non-central chi-square quantile function, e.g. the function \\(Q(p, d)\\) which solves \\(G(Q(p, d), d)=p\\). We find that \\(c=Q(0.95,1.7)=8.7\\). In MATLAB, this can be computed by ncx2inv \\((.95,1.7\\) ). Stock and Yogo (2005) report \\(c=9.0\\) because they used \\(\\tau^{2}=1.82\\).\nThis means that if \\(\\mathrm{F}>8.7\\) we can reject \\(\\mathbb{M}_{0}: \\mu^{2}=1.7\\) against \\(\\mathbb{H}_{1}: \\mu^{2}>1.7\\) with an asymptotic \\(5 %\\) test. In this context we should expect the IV estimator and tests to be reasonably well behaved. However, if \\(\\mathrm{F}<8.7\\) then we should be cautious about the IV estimator, confidence intervals, and tests. This finding led Staiger and Stock (1997) to propose the informal “rule of thumb” that the first stage \\(F\\) statistic should exceed 10. Notice that \\(\\mathrm{F}\\) exceeding \\(8.7\\) (or 10) is equivalent to the reduced form t-statistic exceeding \\(2.94\\) (or 3.16), which is considerably larger than a conventional check if the t-statistic is “significant”. Equivalently, the recommended rule-of-thumb for the case of a single instrument is to estimate the reduced form and verify that the t-statistic for exclusion of the instrumental variable exceeds 3 in absolute value.\nDoes the proposed procedure control the asymptotic size of a 2SLS test? The first step has asymptotic size bounded below \\(r\\) (e.g. 15%). The second step has asymptotic size 5%. By the Bonferroni bound (see Section 9.20) the two steps together have asymptotic size bounded below \\(r+0.05\\) (e.g. 20%). We can thus call the Stock-Yogo procedure a rigorous test with asymptotic size \\(r+0.05\\) (or 20%).\nOur analysis has been confined to the case \\(k_{2}=\\ell_{2}=1\\). Stock and Yogo (2005) also examine the case \\(\\ell_{2}>1\\) (which requires numerical simulation to solve) and both the 2SLS and LIML estimators. They show that the \\(F\\) statistic critical values depend on the number of instruments \\(\\ell_{2}\\) as well as the estimator. Their critical values (calculated by simulation) are in their paper and posted on Motohiro Yogo’s webpage. We report a subset in Table 12.4.\nTable 12.4: 5% Critical Value for Weak Instruments, \\(k_{2}=1\\)\n|\\(\\ell_{2}\\)|\\(0.10\\)|\\(0.15\\)|\\(0.20\\)|\\(0.25\\)||\\(0.10\\)|\\(0.15\\)|\\(0.20\\)|\\(0.25\\)| |———:|—–:|—–:|—–:|—–:||—–:|—–:|—–:|—–:| | 1|\\(16.4\\)| \\(9.0\\)| \\(6.7\\)| \\(5.5\\)||\\(16.4\\)| \\(9.0\\)| \\(6.7\\)| \\(5.5\\)| | 2|\\(19.9\\)|\\(11.6\\)| \\(8.7\\)| \\(7.2\\)|| \\(8.7\\)| \\(5.3\\)| \\(4.4\\)| \\(3.9\\)| | 3|\\(22.3\\)|\\(12.8\\)| \\(9.5\\)| \\(7.8\\)|| \\(6.5\\)| \\(4.4\\)| \\(3.7\\)| \\(3.3\\)| | 4|\\(24.6\\)|\\(14.0\\)|\\(10.3\\)| \\(8.3\\)|| \\(5.4\\)| \\(3.9\\)| \\(3.3\\)| \\(3.0\\)| | 5|\\(26.9\\)|\\(15.1\\)|\\(11.0\\)| \\(8.8\\)|| \\(4.8\\)| \\(3.6\\)| \\(3.0\\)| \\(2.8\\)| | 6|\\(29.2\\)|\\(16.2\\)|\\(11.7\\)| \\(9.4\\)|| \\(4.4\\)| \\(3.3\\)| \\(2.9\\)| \\(2.6\\)| | 7|\\(31.5\\)|\\(17.4\\)|\\(12.5\\)| \\(9.9\\)|| \\(4.2\\)| \\(3.2\\)| \\(2.7\\)| \\(2.5\\)| | 8|\\(33.8\\)|\\(18.5\\)|\\(13.2\\)|\\(10.5\\)|| \\(4.0\\)| \\(3.0\\)| \\(2.6\\)| \\(2.4\\)| | 9|\\(36.2\\)|\\(19.7\\)|\\(14.0\\)|\\(11.1\\)|| \\(3.8\\)| \\(2.9\\)| \\(2.5\\)| \\(2.3\\)| | 10|\\(38.5\\)|\\(20.9\\)|\\(14.8\\)|\\(11.6\\)|| \\(3.7\\)| \\(2.8\\)| \\(2.5\\)| \\(2.2\\)| | 15|\\(50.4\\)|\\(26.8\\)|\\(18.7\\)|\\(12.2\\)|| \\(3.3\\)| \\(2.5\\)| \\(2.2\\)| \\(2.0\\)| | 20|\\(62.3\\)|\\(32.8\\)|\\(22.7\\)|\\(17.6\\)|| \\(3.2\\)| \\(2.3\\)| \\(2.1\\)| \\(1.9\\)| | 25|\\(74.2\\)|\\(38.8\\)|\\(26.7\\)|\\(20.6\\)|| \\(3.8\\)| \\(2.2\\)| \\(2.0\\)| \\(1.8\\)| | 30|\\(86.2\\)|\\(44.8\\)|\\(30.7\\)|\\(23.6\\)|| \\(3.9\\)| \\(2.2\\)| \\(1.9\\)| \\(1.7\\)|\nSource: . One striking feature about these critical values is that those for the 2SLS estimator are strongly increasing in \\(\\ell_{2}\\) while those for the LIML estimator are decreasing in \\(\\ell_{2}\\). This means that when the number of instruments \\(\\ell_{2}\\) is large, 2SLS requires a much stronger reduced form (larger \\(\\mu^{2}\\) ) in order for inference to be reliable, but this is not the case for LIML. This is direct evidence that LIML inference is less sensitive to weak instruments than 2SLS. This makes a strong case for LIML over 2SLS, especially when \\(\\ell_{2}\\) is large or the instruments are potentially weak.\nWe now summarize the recommended Staiger-Stock/Stock-Yogo procedure for \\(k_{1} \\geq 1, k_{2}=1\\), and \\(\\ell_{2} \\geq 1\\). The structural equation and reduced form equations are\n\\[\n\\begin{aligned}\n&Y_{1}=Z_{1}^{\\prime} \\beta_{1}+Y_{2} \\beta_{2}+e \\\\\n&Y_{2}=Z_{1}^{\\prime} \\gamma_{1}+Z_{2}^{\\prime} \\gamma_{2}+u .\n\\end{aligned}\n\\]\nThe structural equation is estimated by either 2 SLS or LIML. Let \\(\\mathrm{F}\\) be the \\(F\\) statistic for \\(\\mathbb{H}_{0}: \\gamma_{2}=0\\) in the reduced form equation. Let \\(s\\left(\\widehat{\\beta}_{2}\\right)\\) be a standard error for \\(\\beta_{2}\\) in the structural equation. The procedure is:\n\nCompare \\(F\\) with the critical values \\(c\\) in Table \\(12.4\\) with the row selected to match the number of excluded instruments \\(\\ell_{2}\\) and the columns to match the estimation method (2SLS or LIML) and the desired size \\(r\\).\nIf \\(F>c\\) then report the 2 SLS or LIML estimates with conventional inference.\n\nThe Stock-Yogo test can be implemented in Stata using the command estat firststage after ivregress 2 sls or ivregres liml if a standard (non-robust) covariance matrix has been specified (that is, without the ‘, r’ option).\nThere are possible extensions to the Stock-Yogo procedure.\nOne modest extension is to use the information to convey the degree of confidence in the accuracy of a confidence interval. Suppose in an application you have \\(\\ell_{2}=5\\) excluded instruments and have estimated your equation by 2SLS. Now suppose that your reduced form \\(F\\) statistic equals 12 . You check Table \\(12.4\\) and find that \\(\\mathrm{F}=12\\) is significant with \\(r=0.20\\). Thus we can interpret the conventional 2SLS confidence interval as having coverage of \\(80 %\\) (or \\(75 %\\) if we make the Bonferroni correction). On the other hand if \\(\\mathrm{F}=27\\) we would conclude that the test for weak instruments is significant with \\(r=0.10\\), meaning that the conventional 2SLS confidence interval can be interpreted as having coverage of \\(90 %\\) (or \\(85 %\\) after Bonferroni correction). Thus the value of the \\(\\mathrm{F}\\) statistic can be used to calibrate the coverage accuracy.\nA more substantive extension, which we now discuss, reverses the steps. Unfortunately this discussion will be limited to the case \\(\\ell_{2}=1\\). First, use the reduced form \\(F\\) statistic to find a one-sided confidence interval for \\(\\mu^{2}\\) of the form \\(\\left[\\mu_{L}^{2}, \\infty\\right)\\). Second, use the lower bound \\(\\mu_{L}^{2}\\) to calculate a critical value \\(c\\) for \\(S_{1}\\) such that the 2SLS test has asymptotic size bounded below \\(0.05\\). This produces better size control than the Stock-Yogo procedure and produces more informative confidence intervals for \\(\\beta_{2}\\). We now describe the steps in detail.\nThe first goal is to find a one-sided confidence interval for \\(\\mu^{2}\\). This is found by test inversion. As we described earlier, for any \\(\\tau^{2}\\) we reject \\(\\mathbb{M}_{0}: \\mu^{2}=\\tau^{2}\\) in favor of \\(\\mathbb{H}_{1}: \\mu^{2}>\\tau^{2}\\) if \\(\\mathrm{F}>c\\) where \\(G\\left(c, \\tau^{2}\\right)=0.95\\). Equivalently, we reject if \\(G\\left(\\mathrm{~F}, \\tau^{2}\\right)>0.95\\). By the test inversion principle an asymptotic \\(95 %\\) confidence interval \\(\\left[\\mu_{L}^{2}, \\infty\\right)\\) is the set of all values of \\(\\tau^{2}\\) which are not rejected. Since \\(G\\left(\\mathrm{~F}, \\tau^{2}\\right) \\geq 0.95\\) for all \\(\\tau^{2}\\) in this set, the lower bound \\(\\mu_{L}^{2}\\) satisfies \\(G\\left(\\mathrm{~F}, \\mu_{L}^{2}\\right)=0.95\\), and is found numerically. In MATLAB, the solution is mu2 when \\(n c x 2 c d f(F, 1, m u 2)\\) returns \\(0.95 .\\)\nThe second goal is to find the critical value \\(c\\) such that \\(\\mathbb{P}\\left(\\left|S_{1}\\right| \\geq c\\right)=0.05\\) when \\(\\mu^{2}=\\mu_{L}^{2}\\). From (12.84) this is achieved when\n\\[\n1-G\\left(\\frac{\\mu_{L}^{2}}{4}+c \\mu_{L}, \\frac{\\mu_{L}^{2}}{4}\\right)+G\\left(\\frac{\\mu_{L}^{2}}{4}-c \\mu_{L}, \\frac{\\mu_{L}^{2}}{4}\\right)=0.05 .\n\\]\nThis can be solved as\n\\[\nG\\left(\\frac{\\mu_{L}^{2}}{4}+c \\mu_{L}, \\frac{\\mu_{L}^{2}}{4}\\right)=0.95 \\text {. }\n\\]\n(The third term on the left-hand-side of (12.85) is zero for all solutions so can be ignored.) Using the non-central chi-square quantile function \\(Q(p, d)\\), this \\(C\\) equals\n\\[\nc=\\frac{Q\\left(0.95, \\frac{\\mu_{L}^{2}}{4}\\right)-\\frac{\\mu_{L}^{2}}{4}}{\\mu_{L}} .\n\\]\nFor example, in MATLAB this is found as \\(c=(n c x 2 i n v ~(.95,1, \\mathrm{mu} 2 / 4)-\\mathrm{mu} 2 / 4) / \\mathrm{sqrt}(\\mathrm{mu} 2) .95 %\\) confidence intervals for \\(\\beta_{2}\\) are then calculated as \\(\\widehat{\\beta}_{\\mathrm{iv}} \\pm c s\\left(\\widehat{\\beta}_{\\mathrm{iv}}\\right)\\).\nWe can also calculate a p-value for the t-statistic \\(T\\) for \\(\\beta_{2}\\). This is\n\\[\np=1-G\\left(\\frac{\\mu_{L}^{2}}{4}+|T| \\mu_{L}, \\frac{\\mu_{L}^{2}}{4}\\right)+G\\left(\\frac{\\mu_{L}^{2}}{4}-|T| \\mu_{L}, \\frac{\\mu_{L}^{2}}{4}\\right)\n\\]\nwhere the third term equals zero if \\(|T| \\geq \\mu_{L} / 4\\). In MATLAB, for example, this can be calculated by the commands\n\\(\\mathrm{T} 1=\\mathrm{mu} 2 / 4+\\operatorname{abs}(\\mathrm{T}) * \\operatorname{sqrt}(\\mathrm{mu} 2)\\)\n\\(\\mathrm{T} 2=\\mathrm{mu} 2 / 4-\\operatorname{abs}(\\mathrm{T}) * \\operatorname{sqrt}(\\mathrm{mu} 2) ;\\)\n\\(\\mathrm{p}=-\\mathrm{ncx} 2 \\mathrm{cdf}(\\mathrm{T} 1,1, \\mathrm{mu} 2 / 4)+\\mathrm{ncx} 2 \\mathrm{cdf}(\\mathrm{T} 2,1, \\mathrm{mu} 2 / 4)\\);\nThese confidence intervals and p-values will be larger than the conventional intervals and p-values, reflecting the incorporation of information about the strength of the instruments through the first-stage \\(F\\) statistic. Also, by the Bonferroni bound these tests have asymptotic size bounded below \\(10 %\\) and the confidence intervals have asymptotic converage exceeding \\(90 %\\), unlike the Stock-Yogo method which has size of \\(20 %\\) and coverage of \\(80 %\\).\nThe augmented procedure suggested here, only for the \\(\\ell_{2}=1\\) case, is\n\nFind \\(\\mu_{L}^{2}\\) which solves \\(G\\left(\\mathrm{~F}, \\mu_{L}^{2}\\right)=0.95\\). In MATLAB, the solution is mu2 when \\(\\operatorname{cx} 2 \\operatorname{cdf}(\\mathrm{F}, 1, \\operatorname{mu} 2)\\) returns \\(0.95 .\\)\nFind \\(c\\) which solves \\(G\\left(\\mu_{L}^{2} / 4+c \\mu_{L}, \\mu_{L}^{2} / 4\\right)=0.95\\). In MATLAB, the command is \\(c=(n c x 2 \\operatorname{inv}(.95,1, \\mathrm{mu} 2 / 4)-\\mathrm{mu} 2 / 4) / \\mathrm{sqrt}(\\mathrm{mu} 2)\\)\nReport the confidence interval \\(\\widehat{\\beta}_{2} \\pm c s\\left(\\widehat{\\beta}_{2}\\right)\\) for \\(\\beta_{2}\\).\nFor the \\(\\mathrm{t}\\) statistic \\(T=\\left(\\widehat{\\beta}_{2}-\\beta_{2}\\right) / s\\left(\\widehat{\\beta}_{2}\\right)\\) the asymptotic \\(\\mathrm{p}\\)-value is\n\n\\[\np=1-G\\left(\\frac{\\mu_{L}^{2}}{4}+|T| \\mu_{L}, \\frac{\\mu_{L}^{2}}{4}\\right)+G\\left(\\frac{\\mu_{L}^{2}}{4}-|T| \\mu_{L}, \\frac{\\mu_{L}^{2}}{4}\\right)\n\\]\nwhich is computed in MATLAB by \\(\\mathrm{T} 1=\\mathrm{mu} 2 / 4+\\mathrm{abs}(\\mathrm{T}) * \\operatorname{sqrt}(\\mathrm{mu} 2) ; \\mathrm{T} 2=\\mathrm{mu} 2 / 4-\\mathrm{abs}(\\mathrm{T}) * \\mathrm{sqrt}(\\mathrm{mu} 2)\\); and \\(\\mathrm{p}=1-\\mathrm{ncx} 2 \\mathrm{cdf}(\\mathrm{T} 1,1, \\mathrm{mu} 2 / 4)+\\mathrm{ncx} 2 \\mathrm{cdf}(\\mathrm{T} 2,1, \\mathrm{mu} 2 / 4)\\).\nWe have described an extension to the Stock-Yogo procedure for the case of one instrumental variable \\(\\ell_{2}=1\\). This restriction was due to the use of the analytic formula (12.85) for the asymptotic distribution which is only available when \\(\\ell_{2}=1\\). In principle the procedure could be extended using simulation or bootstrap methods but this has not been done to my knowledge.\nTo illustrate the Stock-Yogo and extended procedures let us return to the Card proximity example. Take the IV estimates reported in the second column of Table \\(12.1\\) which used college proximity as a single instrument. The reduced form estimates for the endogenous variable education are reported in the second column of Table 12.2. The excluded instrument college has a t-ratio of \\(4.2\\) which implies an \\(F\\) statistic of 17.8. The \\(F\\) statistic exceeds the rule-of thumb of 10 so the structural estimates pass the Stock-Yogo threshold. Based on their recommendation this means that we can interpret the estimates conventionally. However, the conventional confidence interval, e.g. for the returns to education \\(0.132 \\pm\\) \\(0.049 \\times 1.96=[0.04,0.23]\\), has an asymptotic coverage of \\(80 %\\) rather than the nominal \\(95 %\\) rate.\nNow consider the extended procedure. Given \\(\\mathrm{F}=17.8\\) we calculate the lower bound \\(\\mu_{L}^{2}=6.6\\). This implies a critical value of \\(C=2.7\\). Hence an improved confidence interval for the returns to education in this equation is \\(0.132 \\pm 0.049 \\times 2.7=[0.01,0.26]\\). This is a wider confidence interval but has improved asymptotic coverage of \\(90 %\\). The p-value for \\(\\beta_{2}=0\\) is \\(p=0.012\\).\nNext, take the 2SLS estimates reported in the fourth column of Table \\(11.1\\) which use the two instruments public and private. The reduced form equation is reported in column six of Table 12.2. An \\(F\\) statistic for exclusion of the two instruments is \\(F=13.9\\) which exceeds the \\(15 %\\) size threshold for 2SLS and all thresholds for LIML, indicating that the structural estimates pass the Stock-Yogo threshold test and can be interpreted conventionally.\nThe weak instrument methods described here are important for applied econometrics as they discipline researchers to assess the quality of their reduced form relationships before reporting structural estimates. The theory, however, has limitations and shortcomings, in particular the strong assumption of conditional homoskedasticity. Despite this limitation, in practice researchers apply the Stock-Yogo recommendations to estimates computed with heteroskedasticity-robust standard errors. This is an active area of research so the recommended methods may change in the years ahead."
  },
  {
    "objectID": "chpt12-iv.html#weak-instruments-with-k_21",
    "href": "chpt12-iv.html#weak-instruments-with-k_21",
    "title": "12  Instrumental Variables",
    "section": "12.39 Weak Instruments with \\(k_{2}>1\\)",
    "text": "12.39 Weak Instruments with \\(k_{2}>1\\)\nWhen there is more than one endogenous regressor \\(\\left(k_{2}>1\\right)\\) it is better to examine the reduced form as a system. Staiger and Stock (1997) and Stock and Yogo (2005) provided an analysis of this case and constructed a test for weak instruments. The theory is considerably more involved than the \\(k_{2}=1\\) case so we briefly summarize it here excluding many details, emphasizing their suggested methods.\nThe structural equation and reduced form equations are\n\\[\n\\begin{aligned}\n&Y_{1}=Z_{1}^{\\prime} \\beta_{1}+Y_{2}^{\\prime} \\beta_{2}+e \\\\\n&Y_{2}=\\Gamma_{12}^{\\prime} Z_{1}+\\Gamma_{22}^{\\prime} Z_{2}+u_{2} .\n\\end{aligned}\n\\]\nAs in the previous section we assume that the errors are conditionally homoskedastic.\nIdentification of \\(\\beta_{2}\\) requires the matrix \\(\\Gamma_{22}\\) to be full rank. A necessary condition is that each row of \\(\\Gamma_{22}^{\\prime}\\) is non-zero but this is not sufficient.\nWe focus on the size performance of the homoskedastic Wald statistic for the 2SLS estimator of \\(\\beta_{2}\\). For simplicity assume that the variance of \\(e\\) is known and normalized to one. Using representation (12.32), the Wald statistic can be written as\n\\[\nW=\\boldsymbol{e}^{\\prime} \\widetilde{\\boldsymbol{Z}}_{2}\\left(\\widetilde{\\boldsymbol{Z}}_{2}^{\\prime} \\widetilde{\\boldsymbol{Z}}_{2}\\right)^{-1} \\widetilde{\\boldsymbol{Z}}_{2}^{\\prime} \\boldsymbol{Y}_{2}\\left(\\boldsymbol{Y}_{2}^{\\prime} \\widetilde{\\boldsymbol{Z}}_{2}\\left(\\widetilde{\\boldsymbol{Z}}_{2}^{\\prime} \\widetilde{\\boldsymbol{Z}}_{2}\\right)^{-1} \\widetilde{\\boldsymbol{Z}}_{2}^{\\prime} \\boldsymbol{Y}_{2}\\right)^{-1}\\left(\\boldsymbol{Y}_{2}^{\\prime} \\widetilde{\\boldsymbol{Z}}_{2}\\left(\\widetilde{\\boldsymbol{Z}}_{2}^{\\prime} \\widetilde{\\boldsymbol{Z}}_{2}\\right)^{-1} \\widetilde{\\boldsymbol{Z}}_{2}^{\\prime} \\boldsymbol{e}\\right)\n\\]\nwhere \\(\\widetilde{\\boldsymbol{Z}}_{2}=\\left(\\boldsymbol{I}_{n}-\\boldsymbol{P}_{1}\\right) \\boldsymbol{Z}_{2}\\) and \\(\\boldsymbol{P}_{1}=\\boldsymbol{Z}_{1}\\left(\\boldsymbol{Z}_{1}^{\\prime} \\boldsymbol{Z}_{1}\\right)^{-1} \\boldsymbol{Z}_{1}^{\\prime}\\).\nRecall from Section \\(12.36\\) that Stock and Staiger model the excluded instruments \\(Z_{2}\\) as weak by setting \\(\\Gamma_{22}=n^{-1 / 2} \\boldsymbol{C}\\) for some matrix \\(\\boldsymbol{C}\\). In this framework we have the asymptotic distribution results\n\\[\n\\frac{1}{n} \\widetilde{\\boldsymbol{Z}}_{2}^{\\prime} \\widetilde{\\boldsymbol{Z}}_{2} \\underset{p}{\\longrightarrow} \\boldsymbol{Q}=\\mathbb{E}\\left[Z_{2} Z_{2}^{\\prime}\\right]-\\mathbb{E}\\left[Z_{2} Z_{1}^{\\prime}\\right]\\left(\\mathbb{E}\\left[Z_{1} Z_{1}^{\\prime}\\right]\\right)^{-1} \\mathbb{E}\\left[Z_{1} Z_{2}^{\\prime}\\right]\n\\]\nand\n\\[\n\\frac{1}{\\sqrt{n}} \\widetilde{\\boldsymbol{Z}}_{2}^{\\prime} \\boldsymbol{e} \\underset{d}{\\longrightarrow} \\boldsymbol{Q}^{1 / 2} \\xi_{0}\n\\]\nwhere \\(\\xi_{0}\\) is a matrix normal variate whose columns are independent \\(\\mathrm{N}(0, \\boldsymbol{I})\\). Furthermore, setting \\(\\Sigma=\\) \\(\\mathbb{E}\\left[u_{2} u_{2}^{\\prime}\\right]\\) and \\(\\overline{\\boldsymbol{C}}=\\boldsymbol{Q}^{1 / 2} \\boldsymbol{C} \\Sigma^{-1 / 2}\\),\n\\[\n\\frac{1}{\\sqrt{n}} \\widetilde{\\boldsymbol{Z}}_{2}^{\\prime} \\boldsymbol{Y}_{2}=\\frac{1}{n} \\widetilde{\\boldsymbol{Z}}_{2}^{\\prime} \\widetilde{\\boldsymbol{Z}}_{2} \\boldsymbol{C}+\\frac{1}{\\sqrt{n}} \\widetilde{\\boldsymbol{Z}}_{2}^{\\prime} \\boldsymbol{U}_{2} \\underset{d}{\\longrightarrow} \\boldsymbol{Q}^{1 / 2} \\overline{\\boldsymbol{C}} \\Sigma^{1 / 2}+\\boldsymbol{Q}^{1 / 2} \\xi_{2} \\Sigma^{1 / 2}\n\\]\nwhere \\(\\xi_{2}\\) is a matrix normal variate whose columns are independent \\(\\mathrm{N}(0, \\boldsymbol{I})\\). The variables \\(\\xi_{0}\\) and \\(\\xi_{2}\\) are correlated. Together we obtain the asymptotic distribution of the Wald statistic\n\\[\nW \\underset{d}{\\longrightarrow} S=\\xi_{0}^{\\prime}\\left(\\overline{\\boldsymbol{C}}+\\xi_{2}\\right)\\left(\\overline{\\boldsymbol{C}}^{\\prime} \\overline{\\boldsymbol{C}}\\right)^{-1}\\left(\\overline{\\boldsymbol{C}}+\\xi_{2}\\right)^{\\prime} \\xi_{0}\n\\]\nUsing the spectral decomposition, \\(\\overline{\\boldsymbol{C}}^{\\prime} \\overline{\\boldsymbol{C}}=\\boldsymbol{H}^{\\prime} \\Lambda \\boldsymbol{H}\\) where \\(\\boldsymbol{H}^{\\prime} \\boldsymbol{H}=\\boldsymbol{I}\\) and \\(\\Lambda\\) is diagonal. Thus we can write \\(S=\\xi_{0}^{\\prime} \\bar{\\xi}_{2} \\Lambda^{-1} \\bar{\\xi}_{2}^{\\prime} \\xi_{0}\\) where \\(\\bar{\\xi}_{2}=\\overline{\\boldsymbol{C}} \\boldsymbol{H}^{\\prime}+\\xi_{2} \\boldsymbol{H}^{\\prime}\\). The matrix \\(\\xi^{*}=\\left(\\xi_{0}, \\bar{\\xi}_{2}\\right)\\) is multivariate normal, so \\(\\xi^{* \\prime} \\xi^{*}\\) has what is called a non-central Wishart distribution. It only depends on the matrix \\(\\overline{\\boldsymbol{C}}\\) through \\(\\boldsymbol{H} \\overline{\\boldsymbol{C}}^{\\prime} \\overline{\\boldsymbol{C}} \\boldsymbol{H}^{\\prime}=\\Lambda\\) which are the eigenvalues of \\(\\overline{\\boldsymbol{C}}^{\\prime} \\overline{\\boldsymbol{C}}\\). Since \\(S\\) is a function of \\(\\xi^{*}\\) only through \\(\\bar{\\xi}_{2}^{\\prime} \\xi_{0}\\) we conclude that \\(S\\) is a function of \\(\\overline{\\boldsymbol{C}}\\) only through these eigenvalues.\nThis is a very quick derivation of a rather involved derivation but the conclusion drawn by Stock and Yogo is that the asymptotic distribution of the Wald statistic is non-standard and a function of the model parameters only through the eigenvalues of \\(\\overline{\\boldsymbol{C}} \\overline{\\bar{C}}\\) and the correlations between the normal variates \\(\\xi_{0}\\) and \\(\\bar{\\xi}_{2}\\). The worst-case can be summarized by the maximal correlation between \\(\\xi_{0}\\) and \\(\\bar{\\xi}_{2}\\) and the smallest eigenvalue of \\(\\overline{\\boldsymbol{C}}^{\\prime} \\overline{\\boldsymbol{C}}\\). For convenience they rescale the latter by dividing by the number of endogenous variables. Define\n\\[\n\\boldsymbol{G}=\\overline{\\boldsymbol{C}}^{\\prime} \\overline{\\boldsymbol{C}} / k_{2}=\\Sigma^{-1 / 2} \\boldsymbol{C}^{\\prime} \\boldsymbol{Q} \\boldsymbol{C} \\Sigma^{-1 / 2} / k_{2}\n\\]\nand\n\\[\ng=\\lambda_{\\min }(\\boldsymbol{G})=\\lambda_{\\min }\\left(\\Sigma^{-1 / 2} \\boldsymbol{C}^{\\prime} \\boldsymbol{Q} \\boldsymbol{C} \\Sigma^{-1 / 2}\\right) / k_{2} .\n\\]\nThis can be estimated from the reduced-form regression\n\\[\nX_{2 i}=\\widehat{\\Gamma}_{12}^{\\prime} Z_{1 i}+\\widehat{\\Gamma}_{22}^{\\prime} Z_{2 i}+\\widehat{u}_{2 i} .\n\\]\nThe estimator is\n\\[\n\\begin{aligned}\n\\widehat{\\boldsymbol{G}} &=\\widehat{\\Sigma}^{-1 / 2} \\widehat{\\Gamma}_{22}^{\\prime}\\left(\\widetilde{\\boldsymbol{Z}}_{2}^{\\prime} \\widetilde{\\boldsymbol{Z}}_{2}\\right) \\widehat{\\Gamma}_{22} \\widehat{\\Sigma}^{-1 / 2} / k_{2}=\\widehat{\\Sigma}^{-1 / 2}\\left(\\boldsymbol{X}_{2}^{\\prime} \\widetilde{\\boldsymbol{Z}}_{2}\\left(\\widetilde{\\boldsymbol{Z}}_{2}^{\\prime} \\widetilde{\\boldsymbol{Z}}_{2}\\right)^{-1} \\widetilde{\\boldsymbol{Z}}_{2}^{\\prime} \\boldsymbol{X}_{2}\\right) \\widehat{\\Sigma}^{-1 / 2} / k_{2} \\\\\n\\widehat{\\Sigma} &=\\frac{1}{n-k} \\sum_{i=1}^{n} \\widehat{u}_{2 i} \\widehat{u}_{2 i}^{\\prime} \\\\\n\\widehat{g} &=\\lambda_{\\min }(\\widehat{\\boldsymbol{G}})\n\\end{aligned}\n\\]\n\\(\\widehat{\\boldsymbol{G}}\\) is a matrix \\(F\\)-type statistic for the coefficient matrix \\(\\widehat{\\Gamma}_{22}\\).\nThe statistic \\(\\widehat{g}\\) was proposed by Cragg and Donald (1993) as a test for underidentification. Stock and Yogo (2005) use it as a test for weak instruments. Using simulation methods they determined critical values for \\(\\widehat{g}\\) similar to those for \\(k_{2}=1\\). For given size \\(r>0.05\\) there is a critical value \\(c\\) (reported in the table below) such that if \\(\\widehat{g}>c\\) then the 2SLS (or LIML) Wald statistic \\(W\\) for \\(\\widehat{\\beta}_{2}\\) has asymptotic size bounded below \\(r\\). On the other hand, if \\(\\widehat{g} \\leq c\\) then we cannot bound the asymptotic size below \\(r\\) and we cannot reject the hypothesis of weak instruments. Critical values (calculated by simulation) are reported in their paper and posted on Motohiro Yogo’s webpage. We report a subset for the case \\(k_{2}=2\\) in Table 12.5. The methods and theory applies to the cases \\(k_{2}>2\\) as well but those critical values have not been calculated. As for the \\(k_{2}=1\\) case the critical values for 2 SLS are dramatically increasing in \\(\\ell_{2}\\). Thus when the model is over-identified, we need a large value of \\(\\widehat{g}\\) to reject the hypothesis of weak instruments. This is a strong cautionary message to check the \\(\\widehat{g}\\) statistic in applications. Furthermore, the critical values for LIML are generally decreasing in \\(\\ell_{2}\\) (except for \\(r=0.10\\) where the critical values are increasing for large \\(\\ell_{2}\\) ). This means that for over-identified models LIML inference is less sensitive to weak instruments than 2SLS and may be the preferred estimation method.\nThe Stock-Yogo test can be implemented in Stata using the command estat firststage after ivregress 2sls or ivregres \\(\\operatorname{liml}\\) if a standard (non-robust) covariance matrix has been specified (that is, without the ‘, r’ option). Critical values which control for size are only available for \\(k_{2} \\leq 2\\). For for \\(k_{2}>2\\) critical values which control for relative bias are reported.\nRobust versions of the test have been proposed by Kleibergen and Paap (2006). These can be implemented in Stata using the downloadable command ivreg2.\nTable 12.5: 5% Critical Value for Weak Instruments, \\(k_{2}=2\\)\n|\\(\\ell_{2}\\)|\\(0.10\\)|\\(0.15\\)|\\(0.20\\)|\\(0.25\\)||\\(0.10\\)|\\(0.15\\)|\\(0.20\\)|\\(0.25\\)| |———:|—–:|—–:|—–:|—–:||—–:|—–:|—–:|—–:| | 2| \\(7.0\\)| \\(4.6\\)| \\(3.9\\)| \\(3.6\\)|| \\(7.0\\)| \\(4.6\\)| \\(3.9\\)| \\(3.6\\)| | 3|\\(13.4\\)| \\(8.2\\)| \\(6.4\\)| \\(5.4\\)|| \\(5.4\\)| \\(3.8\\)| \\(3.3\\)| \\(3.1\\)| | 4|\\(16.9\\)| \\(9.9\\)| \\(7.5\\)| \\(6.3\\)|| \\(4.7\\)| \\(3.4\\)| \\(3.0\\)| \\(2.8\\)| | 5|\\(19.4\\)|\\(11.2\\)| \\(8.4\\)| \\(6.9\\)|| \\(4.3\\)| \\(3.1\\)| \\(2.8\\)| \\(2.6\\)| | 6|\\(21.7\\)|\\(12.3\\)| \\(9.1\\)| \\(7.4\\)|| \\(4.1\\)| \\(2.9\\)| \\(2.6\\)| \\(2.5\\)| | 7|\\(23.7\\)|\\(13.3\\)| \\(9.8\\)| \\(7.9\\)|| \\(3.9\\)| \\(2.8\\)| \\(2.5\\)| \\(2.4\\)| | 8|\\(25.6\\)|\\(14.3\\)|\\(10.4\\)| \\(8.4\\)|| \\(3.8\\)| \\(2.7\\)| \\(2.4\\)| \\(2.3\\)| | 9|\\(27.5\\)|\\(15.2\\)|\\(11.0\\)| \\(8.8\\)|| \\(3.7\\)| \\(2.7\\)| \\(2.4\\)| \\(2.2\\)| | 10|\\(29.3\\)|\\(16.2\\)|\\(11.6\\)| \\(9.3\\)|| \\(3.6\\)| \\(2.6\\)| \\(2.3\\)| \\(2.1\\)| | 15|\\(38.0\\)|\\(20.6\\)|\\(14.6\\)|\\(11.6\\)|| \\(3.5\\)| \\(2.4\\)| \\(2.1\\)| \\(2.0\\)| | 20|\\(46.6\\)|\\(25.0\\)|\\(17.6\\)|\\(13.8\\)|| \\(3.6\\)| \\(2.4\\)| \\(2.0\\)| \\(1.9\\)| | 25|\\(55.1\\)|\\(29.3\\)|\\(20.6\\)|\\(16.1\\)|| \\(3.6\\)| \\(2.4\\)|\\(1.97\\)| \\(1.8\\)| | 30|\\(63.5\\)|\\(33.6\\)|\\(23.5\\)|\\(18.3\\)|| \\(4.1\\)| \\(2.4\\)|\\(1.95\\)| \\(1.7\\)|\nSource: ."
  },
  {
    "objectID": "chpt12-iv.html#example-acemoglu-johnson-and-robinson-2001",
    "href": "chpt12-iv.html#example-acemoglu-johnson-and-robinson-2001",
    "title": "12  Instrumental Variables",
    "section": "12.40 Example: Acemoglu, Johnson, and Robinson (2001)",
    "text": "12.40 Example: Acemoglu, Johnson, and Robinson (2001)\nOne particularly well-cited instrumental variable regression is in Acemoglu, Johnson, and Robinson (2001) with additional details published in (2012). They are interested in the effect of political institutions on economic performance. The theory is that good institutions (rule-of-law, property rights) should result in a country having higher long-term economic output than if the same country had poor institutions. To investigate this question they focus on a sample of 64 former European colonies. Their data is in the file AJR2001 on the textbook website.\nThe authors’ premise is that modern political institutions have been influenced by colonization. In particular they argue that colonizing countries tended to set up colonies as either an “extractive state” or as a “migrant colony”. An extractive state was used by the colonizer to extract resources for the colonizing country but was not largely settled by the European colonists. In this case the colonists had no incentive to set up good political institutions. In contrast, if a colony was set up as a “migrant colony” then large numbers of European settlers migrated to the colony to live. These settlers desired institutions similar to those in their home country and hence had an incentive to set up good political institutions. The nature of institutions is quite persistent over time so these \\(19^{t h}\\)-century foundations affect the nature of modern institutions. The authors conclude that the \\(19^{t h}\\)-century nature of the colony is predictive of the nature of modern institutions and hence modern economic growth.\nTo start the investigation they report an OLS regression of log GDP per capita in 1995 on a measure of political institutions they call risk which is a measure of legal protection against expropriation. This variable ranges from 0 to 10 , with 0 the lowest protection against appropriation and 10 the highest. For each country the authors take the average value of the index over 1985 to 1995 (the mean is \\(6.5\\) with a standard deviation of 1.5). Their reported OLS estimates (intercept omitted) are\n\nThese estimates imply a \\(52 %\\) difference in GDP between countries with a 1 -unit difference in risk.\nThe authors argue that the risk is endogenous since economic output influences political institutions and because the variable risk is undoubtedly measured with error. These issues induce least-square bias in different directions and thus the overall bias effect is unclear.\nTo correct for endogeneity bias the authors argue the need for an instrumental variable which does not directly affect economic performance yet is associated with political institutions. Their innovative suggestion was to use the mortality rate which faced potential European settlers in the \\(19^{t h}\\) century. Colonies with high expected mortality were less attractive to European settlers resulting in lower levels of European migrants. As a consequence the authors expect such colonies to be more likely structured as an extractive state rather than a migrant colony. To measure the expected mortality rate the authors use estimates provided by historical research of the annualized deaths per 1000 soldiers, labeled mortality. (They used military mortality rates as the military maintained high-quality records.) The first-stage regression is\n\\[\n\\text { risk }=\\underset{(0.13)}{-0.61} \\log (\\text { mortality })+\\widehat{u} .\n\\]\nThese estimates confirm that \\(19^{t h}\\)-century high mortality rates are associated with lower quality modern institutions. Using \\(\\log\\) (mortality) as an instrument for risk, they estimate the structural equation using 2SLS and report\n\\[\n\\log (\\text { GDP per Capita })=\\begin{gathered}\n0.94 \\text { risk. } \\\\\n(0.16)\n\\end{gathered}\n\\]\nThis estimate is much higher than the OLS estimate from (12.86). The estimate is consistent with a near doubling of GDP due to a 1-unit difference in the risk index.\nThese are simple regressions involving just one right-hand-side variable. The authors considered a range of other models. Included in these results are a reversal of a traditional finding. In a conventional least squares regression two relevant variables for output are latitude (distance from the equator) and africa (a dummy variable for countries from Africa) both of which are difficult to interpret causally. But in the proposed instrumental variables regression the variables latitude and africa have much smaller and statistically insignificant - coefficients. To assess the specification we can use the Stock-Yogo and endogeneity tests. The Stock-Yogo test is from the reduced form (12.87). The instrument has a t-ratio of \\(4.8\\) (or \\(F=23\\) ) which exceeds the StockYogo critical value and hence can be treated as strong. For an endogeneity test we take the least squares residual \\(\\widehat{u}\\) from this equation and include it in the structural equation and estimate by least squares. We find a coefficient on \\(\\widehat{u}\\) of \\(-0.57\\) with a t-ratio of \\(4.7\\) which is highly significant. We conclude that the least squares and 2SLS estimates are statistically different and reject the hypothesis that the variable risk is exogenous for the GDP structural equation.\nIn Exercise \\(12.22\\) you will replicate and extend these results using the authors’ data.\nThis paper is a creative and careful use of instrumental variables. The creativity stems from the historical analysis which lead to the focus on mortality as a potential predictor of migration choices. The care comes in the implementation as the authors needed to gather country-level data on political institutions and mortality from distinct sources. Putting these pieces together is the art of the project."
  },
  {
    "objectID": "chpt12-iv.html#example-angrist-and-krueger-1991",
    "href": "chpt12-iv.html#example-angrist-and-krueger-1991",
    "title": "12  Instrumental Variables",
    "section": "12.41 Example: Angrist and Krueger (1991)",
    "text": "12.41 Example: Angrist and Krueger (1991)\nAnother influential instrument variable regression is Angrist and Krueger (1991). Their concern, similar to Card (1995), is estimation of the structural returns to education while treating educational attainment as endogenous. Like Card, their goal is to find an instrument which is exogenous for wages yet has an impact on education. A subset of their data in the file AK1991 on the textbook website.\nTheir creative suggestion was to focus on compulsory school attendance policies and their interaction with birthdates. Compulsory schooling laws vary across states in the United States, but typically require that youth remain in school until their sixteenth or seventeenth birthday. Angrist and Krueger argue that compulsory schooling has a causal effect on wages - youth who would have chosen to drop out of school stay in school for more years - and thus have more education which causally impacts their earnings as adults.\nAngrist and Krueger observe that these policies have differential impact on youth who are born early or late in the school year. Students who are born early in the calendar year are typically older when they enter school. Consequently when they attain the legal dropout age they have attended less school than those born near the end of the year. This means that birthdate (early in the calendar year versus late) exogenously impacts educational attainment and thus wages through education. Yet birthdate must be exogenous for the structural wage equation as there is no reason to believe that birthdate itself has a causal impact on a person’s ability or wages. These considerations together suggest that birthdate is a valid instrumental variable for education in a causal wage equation.\nTypical wage datasets include age but not birthdates. To obtain information on birthdate, Angrist and Krueger used U.S. Census data which includes an individual’s quarter of birth (January-March, AprilJune, etc.). They use this variable to construct 2SLS estimates of the return to education.\nTheir paper carefully documents that educational attainment varies by quarter of birth (as predicted by the above discussion), and reports a large set of least squares and 2SLS estimates. We focus on two estimates at the core of their analysis, reported in column (6) of their Tables \\(\\mathrm{V}\\) and VII. This involves data from the 1980 census with men born in 1930-1939, with 329,509 observations. The first equation is\n\nwhere \\(e d u\\) is years of education and Black, urban, and married are dummy variables indicating race (1 if Black, 0 otherwise), lives in a metropolitan area, and if married. In addition to the reported coefficients the equation also includes as regressors nine year-of-birth dummies and eight region-of-residence dummies. The equation is estimated by 2 SLS. The instrumental variables are the 30 interactions of three quarter-of-birth times ten year-of-birth dummy variables.\nThis equation indicates an \\(8 %\\) increase in wages due to each year of education.\nAngrist and Krueger observe that the effect of compulsory education laws are likely to vary across states, so expand the instrument set to include interactions with state-of-birth. They estimate the following equation by 2 SLS\n\nThis equation also adds fifty state-of-birth dummy variables as regressors. The instrumental variables are the 180 interactions of quarter-of-birth times year-of-birth dummy variables, plus quarter-of-birth times state-of-birth interactions.\nThis equation shows a similar estimated causal effect of education on wages as in (12.89). More notably, the standard error is smaller in (12.90) suggesting improved precision by the expanded instrumental variable set.\nHowever, these estimates seem excellent candidates for weak instruments and many instruments. Indeed, this paper (published in 1991) helped spark these two literatures. We can use the Stock-Yogo tools to explore the instrument strength and the implications for the Angrist-Krueger estimates.\nWe first take equation (12.89). Using the original Angrist-Krueger data we estimate the corresponding reduced form and calculate the \\(F\\) statistic for the 30 excluded instruments. We find \\(F=4.8\\). It has an asymptotic p-value of \\(0.000\\) suggesting that we can reject (at any significance level) the hypothesis that the coefficients on the excluded instruments are zero. Thus Angrist and Krueger appear to be correct that quarter of birth helps to explain educational attainment and are thus a valid instrumental variable set. However, using the Stock-Yogo test, \\(F=4.8\\) is not high enough to reject the hypothesis that the instruments are weak. Specifically, for \\(\\ell_{2}=30\\) and \\(15 %\\) size the critical value for the \\(F\\) statistic is 45 . The actual value of \\(4.8\\) is far below 45. Since we cannot reject that the instruments are weak this indicates that we cannot interpret the 2SLS estimates and test statistics in (12.89) as reliable.\nSecond, take (12.90) with the expanded regressor and instrument set. Estimating the corresponding reduced form we find the \\(F\\) statistic for the 180 excluded instruments is \\(\\mathrm{F}=2.43\\) which also has an asymptotic p-value of \\(0.000\\) indicating that we can reject at any significance level the hypothesis that the excluded instruments have no effect on educational attainment. However, using the Stock-Yogo test we also cannot reject the hypothesis that the instruments are weak. While Stock and Yogo did not calculate the critical values for \\(\\ell_{2}=180\\), the 2 SLS critical values are increasing in \\(\\ell_{2}\\) so we can use those for \\(\\ell_{2}=30\\) as a lower bound. The observed value of \\(\\mathrm{F}=2.43\\) is far below the level needed for significance. Consequently the results in (12.90) cannot be viewed as reliable. In particular, the observation that the standard errors in (12.90) are smaller than those in (12.89) should not be interpreted as evidence of greater precision. Rather, they should be viewed as evidence of unreliability due to weak instruments.\nWhen instruments are weak one constructive suggestion is to use LIML estimation rather than 2SLS. Another constructive suggestion is to alter the instrument set. While Angrist and Krueger used a large number of instrumental variables we can consider a smaller set. Take equation (12.89). Rather than estimating it using the 30 interaction instruments consider using only the three quarter-of-birth dummy variables. We report the reduced form estimates here:\n\nwhere \\(Q_{2}, Q_{3}\\), and \\(Q_{4}\\) are dummy variables for birth in the \\(2^{n d}, 3^{r d}\\), and \\(4^{t h}\\) quarter. The regression also includes nine year-of-birth and eight region-of-residence dummy variables.\nThe reduced form coefficients in (12.91) on the quarter-of-birth dummies are instructive. The coefficients are positive and increasing, consistent with the Angrist-Krueger hypothesis that individuals born later in the year achieve higher average education. Focusing on the weak instrument problem the \\(F\\) test for exclusion of these three variables is \\(\\mathrm{F}=31\\). The Stock-Yogo critical value is \\(12.8\\) for \\(\\ell_{2}=3\\) and a size of \\(15 %\\), and is \\(22.3\\) for a size of \\(10 %\\). Since \\(F=31\\) exceeds both these thresholds we can reject the hypothesis that this reduced form is weak. Estimating the model by 2SLS with these three instruments we find\n\nThese estimates indicate a slightly larger (10%) causal impact of education on wages but with a larger standard error. The Stock-Yogo analysis indicates that we can interpret the confidence intervals from these estimates as having asymptotic coverage \\(85 %\\).\nWhile the original Angrist-Krueger estimates suffer due to weak instruments their paper is a very creative and thoughtful application of the natural experiment methodology. They discovered a completely exogenous variation present in the world - birthdate - and showed how this has a small but measurable effect on educational attainment and thereby on earnings. Their crafting of this natural experiment regression is clever and demonstrates a style of analysis which can successfully underlie an effective instrumental variables empirical analysis."
  },
  {
    "objectID": "chpt12-iv.html#programming",
    "href": "chpt12-iv.html#programming",
    "title": "12  Instrumental Variables",
    "section": "12.42 Programming",
    "text": "12.42 Programming\nWe now present Stata code for some of the empirical work reported in this chapter.\nStata do File for Card Example use Card1995.dta, clear\nset more off\ngen exp = age \\(76-\\) ed \\(76-6\\)\ngen \\(\\exp 2=\\left(\\exp ^{\\wedge} 2\\right) / 100\\)\n\nDrop observations with missing wage\n\ndrop if lwage \\(76==.\\)\n\nTable \\(12.1\\) regressions\n\nreg lwage76 ed76 exp exp2 black reg76r smsa76r, \\(r\\)\nivregress 2 sls lwage76 exp exp2 black reg76r smsa76r (ed76=nearc4), r\nivregress 2 sls lwage76 black reg76r smsa76r (ed76 exp exp2 \\(=\\) nearc4 age76 age2), r perfect\nivregress 2sls lwage76 exp exp2 black reg76r smsa76r (ed76=nearc4a nearc4b), \\(\\mathrm{r}\\)\nivregress 2 sls lwage76 black reg76r smsa76r (ed76 exp exp2 \\(=\\) nearc4a nearc4b age76 age2), \\(r\\) perfect\nivregress liml lwage76 exp exp2 black reg76r smsa76r (ed76=nearc4a nearc4b), \\(r\\)\n\nTable \\(12.2\\) regressions\n\nreg lwage76 exp exp2 black reg76r smsa76r nearc4, \\(r\\)\nreg ed76 exp exp2 black reg76r smsa76r nearc4, \\(r\\)\nreg ed76 black reg76r smsa76r nearc4 age76 age2, \\(r\\)\nreg exp black reg76r smsa76r nearc4 age76 age2, \\(r\\)\nreg exp2 black reg76r smsa76r nearc4 age76 age2, \\(r\\)\nreg ed76 exp exp2 black reg76r smsa76r nearc4a nearc4b, \\(r\\)\nreg lwage76 ed76 exp exp2 smsa76r reg76r, \\(r\\)\nreg lwage76 nearc4 exp exp2 smsa76r reg76r, \\(r\\)\nreg ed76 nearc4 exp exp2 smsa76r reg76r, \\(r\\)\nStata do File for Acemoglu-Johnson-Robinson Example use AJR2001.dta, clear\nreg loggdp risk\nreg risk logmort0\npredict \\(u\\), residual\nivregress 2sls loggdp (risk=logmort0)\nreg loggdp risk \\(u\\)\n\n\n\n\n\n\nStata do File for Angrist-Krueger Example\n\n\n\n\nuse AK1991.dta, clear\n\n\nivregress 2sls logwage black smsa married i.yob i.region (edu = i.qob#i.yob)\n\n\nivregress 2sls logwage black smsa married i.yob i.region i.state (edu \\(=\\)\n\n\ni.qob#i.yob i.qob#i.state)\n\n\nreg edu black smsa married i.yob i.region i.qob#i.yob\n\n\ntestparm i.qob#i.yob\n\n\nreg edu black smsa married i.yob i.region i.state i.qob#i.yob i.qob#i.state\n\n\ntestparm i.qob#i.yob i.qob#i.state\n\n\nreg edu black smsa married i.yob i.region i.qob\n\n\ntestparm i.qob\n\n\nivregress 2sls logwage black smsa married i.yob i.region (edu = i.qob)"
  },
  {
    "objectID": "chpt12-iv.html#exercises",
    "href": "chpt12-iv.html#exercises",
    "title": "12  Instrumental Variables",
    "section": "12.43 Exercises",
    "text": "12.43 Exercises\nExercise 12.1 Consider the single equation model \\(Y=Z \\beta+e\\) where \\(Y\\) and \\(Z\\) are both real-valued \\((1 \\times 1)\\). Let \\(\\widehat{\\beta}\\) denote the IV estimator of \\(\\beta\\) using as an instrument a dummy variable \\(D\\) (takes only the values 0 and 1). Find a simple expression for the IV estimator in this context.\nExercise 12.2 Take the linear model \\(Y=X^{\\prime} \\beta+e\\) with \\(\\mathbb{E}[e \\mid X]=0\\). Suppose \\(\\sigma^{2}(x)=\\mathbb{E}\\left[e^{2} \\mid X=x\\right]\\) is known. Show that the GLS estimator of \\(\\beta\\) can be written as an IV estimator using some instrument \\(Z\\). (Find an expression for \\(Z\\).)\nExercise 12.3 Take the linear model \\(Y=X^{\\prime} \\beta+e\\). Let the OLS estimator for \\(\\beta\\) be \\(\\widehat{\\beta}\\) with OLS residual \\(\\widehat{e}_{i}\\). Let the IV estimator for \\(\\beta\\) using some instrument \\(Z\\) be \\(\\widetilde{\\beta}\\) with IV residual \\(\\widetilde{e}_{i}=Y_{i}-X_{i}^{\\prime} \\widetilde{\\beta}\\). If \\(X\\) is indeed endogenous, will IV “fit” better than OLS in the sense that \\(\\sum_{i=1}^{n} \\widetilde{e}_{i}^{2}<\\sum_{i=1}^{n} \\widehat{e}_{i}^{2}\\), at least in large samples?\nExercise 12.4 The reduced form between the regressors \\(X\\) and instruments \\(Z\\) takes the form \\(X=\\Gamma^{\\prime} Z+u\\) where \\(X\\) is \\(k \\times 1, Z\\) is \\(\\ell \\times 1\\), and \\(\\Gamma\\) is \\(\\ell \\times k\\). The parameter \\(\\Gamma\\) is defined by the population moment condition \\(\\mathbb{E}\\left[Z u^{\\prime}\\right]=0\\). Show that the method of moments estimator for \\(\\Gamma\\) is \\(\\widehat{\\Gamma}=\\left(\\boldsymbol{Z}^{\\prime} \\boldsymbol{Z}\\right)^{-1}\\left(\\boldsymbol{Z}^{\\prime} \\boldsymbol{X}\\right)\\).\nExercise 12.5 In the structural model \\(Y=X^{\\prime} \\beta+e\\) with \\(X=\\Gamma^{\\prime} Z+u\\) and \\(\\Gamma \\ell \\times k, \\ell \\geq k\\), we claim that a necessary condition for \\(\\beta\\) to be identified (can be recovered from the reduced form) is \\(\\operatorname{rank}(\\Gamma)=k\\). Explain why this is true. That is, show that if \\(\\operatorname{rank}(\\Gamma)<k\\) then \\(\\beta\\) is not identified.\nExercise 12.6 For Theorem \\(12.3\\) establish that \\(\\widehat{\\boldsymbol{V}}_{\\beta} \\underset{p}{\\longrightarrow} \\boldsymbol{V}_{\\beta}\\)\nExercise 12.7 Take the linear model \\(Y=X^{\\prime} \\beta+e\\) with \\(\\mathbb{E}[e \\mid X]=0\\) where \\(X\\) and \\(\\beta\\) are \\(1 \\times 1\\).\n\nShow that \\(\\mathbb{E}[X e]=0\\) and \\(\\mathbb{E}\\left[X^{2} e\\right]=0\\). Is \\(Z=\\left(\\begin{array}{ll}X & X^{2}\\end{array}\\right)^{\\prime}\\) a valid instrument for estimation of \\(\\beta\\) ?\nDefine the 2SLS estimator of \\(\\beta\\) using \\(Z\\) as an instrument for \\(X\\). How does this differ from OLS? Exercise 12.8 Suppose that price and quantity are determined by the intersection of the linear demand and supply curves\n\n\\[\n\\begin{aligned}\n\\text { Demand: } & Q=a_{0}+a_{1} P+a_{2} Y+e_{1} \\\\\n\\text { Supply: } & Q=b_{0}+b_{1} P+b_{2} W+e_{2}\n\\end{aligned}\n\\]\nwhere income \\((Y)\\) and wage \\((W)\\) are determined outside the market. In this model are the parameters identified?\nExercise 12.9 Consider the model \\(Y=X^{\\prime} \\beta+e\\) with \\(\\mathbb{E}[e \\mid Z]=0\\) with \\(Y\\) scalar and \\(X\\) and \\(Z\\) each a \\(k\\) vector. You have a random sample \\(\\left(Y_{i}, X_{i}, Z_{i}: i=1, \\ldots, n\\right)\\).\n\nAssume that \\(X\\) is exogenous in the sense that \\(\\mathbb{E}[e \\mid Z, X]=0\\). Is the IV estimator \\(\\widehat{\\beta}_{\\mathrm{iv}}\\) unbiased?\nContinuing to assume that \\(X\\) is exogenous, find the conditional covariance matrix \\(\\operatorname{var}\\left[\\widehat{\\beta}_{\\text {iv }} \\mid \\boldsymbol{X}, \\boldsymbol{Z}\\right]\\).\n\nExercise 12.10 Consider the model\n\\[\n\\begin{aligned}\nY &=X^{\\prime} \\beta+e \\\\\nX &=\\Gamma^{\\prime} Z+u \\\\\n\\mathbb{E}[Z e] &=0 \\\\\n\\mathbb{E}\\left[Z u^{\\prime}\\right] &=0\n\\end{aligned}\n\\]\nwith \\(Y\\) scalar and \\(X\\) and \\(Z\\) each a \\(k\\) vector. You have a random sample \\(\\left(Y_{i}, X_{i}, Z_{i}: i=1, \\ldots, n\\right)\\). Take the control function equation \\(e=u^{\\prime} \\gamma+v\\) with \\(\\mathbb{E}[u v]=0\\) and assume for simplicity that \\(u\\) is observed. Inserting into the structural equation we find \\(Y=Z^{\\prime} \\beta+u^{\\prime} \\gamma+v\\). The control function estimator \\((\\widehat{\\beta}, \\widehat{\\gamma})\\) is OLS estimation of this equation.\n\nShow that \\(\\mathbb{E}[X v]=0\\) (algebraically).\nDerive the asymptotic distribution of \\((\\widehat{\\beta}, \\widehat{\\gamma})\\).\n\nExercise 12.11 Consider the structural equation\n\\[\nY=\\beta_{0}+\\beta_{1} X+\\beta_{2} X^{2}+e\n\\]\nwith \\(X \\in \\mathbb{R}\\) treated as endogenous so that \\(\\mathbb{E}[X e] \\neq 0\\). We have an instrument \\(Z \\in \\mathbb{R}\\) which satisfies \\(\\mathbb{E}[e \\mid Z]=0\\) so in particular \\(\\mathbb{E}[e]=0, \\mathbb{E}[Z e]=0\\) and \\(\\mathbb{E}\\left[Z^{2} e\\right]=0\\).\n\nShould \\(X^{2}\\) be treated as endogenous or exogenous?\nSuppose we have a scalar instrument \\(Z\\) which satisfies\n\n\\[\nX=\\gamma_{0}+\\gamma_{1} Z+u\n\\]\nwith \\(u\\) independent of \\(Z\\) and mean zero.\nConsider using \\(\\left(1, Z, Z^{2}\\right.\\) ) as instruments. Is this a sufficient number of instruments? Is (12.93) just-identified, over-identified, or under-identified?\n\nWrite out the reduced form equation for \\(X^{2}\\). Under what condition on the reduced form parameters (12.94) are the parameters in (12.93) identified? Exercise 12.12 Consider the structural equation and reduced form\n\n\\[\n\\begin{aligned}\nY &=\\beta X^{2}+e \\\\\nX &=\\gamma Z+u \\\\\n\\mathbb{E}[Z e] &=0 \\\\\n\\mathbb{E}[Z u] &=0\n\\end{aligned}\n\\]\nwith \\(X^{2}\\) treated as endogenous so that \\(\\mathbb{E}\\left[X^{2} e\\right] \\neq 0\\). For simplicity assume no intercepts. \\(Y, Z\\), and \\(X\\) are scalar. Assume \\(\\gamma \\neq 0\\). Consider the following estimator. First, estimate \\(\\gamma\\) by OLS of \\(X\\) on \\(Z\\) and construct the fitted values \\(\\widehat{X}_{i}=\\widehat{\\gamma} Z_{i}\\). Second, estimate \\(\\beta\\) by OLS of \\(Y_{i}\\) on \\(\\left(\\widehat{X}_{i}\\right)^{2}\\).\n\nWrite out this estimator \\(\\widehat{\\beta}\\) explicitly as a function of the sample.\nFind its probability limit as \\(n \\rightarrow \\infty\\).\nIn general, is \\(\\widehat{\\beta}\\) consistent for \\(\\beta\\) ? Is there a reasonable condition under which \\(\\widehat{\\beta}\\) is consistent?\n\nExercise 12.13 Consider the structural equation \\(Y_{1}=Z_{1}^{\\prime} \\beta_{1}+Y_{2}^{\\prime} \\beta_{2}+e\\) with \\(\\mathbb{E}[Z e]=0\\) where \\(Y_{2}\\) is \\(k_{2} \\times 1\\) and treated as endogenous. The variables \\(Z=\\left(Z_{1}, Z_{2}\\right)\\) are treated as exogenous where \\(Z_{2}\\) is \\(\\ell_{2} \\times 1\\) and \\(\\ell_{2} \\geq k_{2}\\). You are interested in testing the hypothesis \\(\\mathbb{H}_{0}: \\beta_{2}=0\\).\nConsider the reduced form equation for \\(Y_{1}\\)\n\\[\nY_{1}=Z_{1}^{\\prime} \\lambda_{1}+Z_{2}^{\\prime} \\lambda_{2}+u_{1} .\n\\]\nShow how to test \\(\\mathbb{M}_{0}\\) using only the OLS estimates of (12.95).\nHint: This will require an analysis of the reduced form equations and their relation to the structural equation.\nExercise 12.14 Take the linear instrumental variables equation \\(Y_{1}=Z_{1}^{\\prime} \\beta_{1}+Y_{2}^{\\prime} \\beta_{2}+e\\) with \\(\\mathbb{E}[Z e]=0\\) where \\(Z_{1}\\) is \\(k_{1} \\times 1, Y_{2}\\) is \\(k_{2} \\times 1\\), and \\(Z\\) is \\(\\ell \\times 1\\), with \\(\\ell \\geq k=k_{1}+k_{2}\\). The sample size is \\(n\\). Assume that \\(\\boldsymbol{Q}_{Z Z}=\\) \\(\\mathbb{E}\\left[Z Z^{\\prime}\\right]>0\\) and \\(Q_{Z X}=\\mathbb{E}\\left[Z X^{\\prime}\\right]\\) has full rank \\(k\\).\nSuppose that only \\(\\left(Y_{1}, Z_{1}, Z_{2}\\right)\\) are available and \\(Y_{2}\\) is missing from the dataset.\nConsider the 2SLS estimator \\(\\widehat{\\beta}_{1}\\) of \\(\\beta_{1}\\) obtained from the misspecified IV regression of \\(Y_{1}\\) on \\(Z_{1}\\) only, using \\(Z_{2}\\) as an instrument for \\(Z_{1}\\).\n\nFind a stochastic decomposition \\(\\widehat{\\beta}_{1}=\\beta_{1}+b_{1 n}+r_{1 n}\\) where \\(r_{1 n}\\) depends on the error \\(e\\) and \\(b_{1 n}\\) does not depend on the error \\(e\\).\nShow that \\(r_{1 n} \\rightarrow p 0\\) as \\(n \\rightarrow \\infty\\).\nFind the probability limit of \\(b_{1 n}\\) and \\(\\widehat{\\beta}_{1}\\) as \\(n \\rightarrow \\infty\\).\nDoes \\(\\widehat{\\beta}_{1}\\) suffer from “omitted variables bias”? Explain. Under what conditions is there no omitted variables bias?\nFind the asymptotic distribution as \\(n \\rightarrow \\infty\\) of \\(\\sqrt{n}\\left(\\widehat{\\beta}_{1}-\\beta_{1}-b_{1 n}\\right)\\).\n\nExercise 12.15 Take the linear instrumental variables equation \\(Y_{1}=Z \\beta_{1}+Y_{2} \\beta_{2}+e\\) with \\(\\mathbb{E}[e \\mid Z]=0\\) where both \\(X\\) and \\(Z\\) are scalar \\(1 \\times 1\\).\n\nCan the coefficients \\(\\left(\\beta_{1}, \\beta_{2}\\right)\\) be estimated by 2 SLS using \\(Z\\) as an instrument for \\(Y_{2}\\) ?\n\nWhy or why not? (b) Can the coefficients \\(\\left(\\beta_{1}, \\beta_{2}\\right)\\) be estimated by 2SLS using \\(Z\\) and \\(Z^{2}\\) as instruments?\n\nFor the 2SLS estimator suggested in (b), what is the implicit exclusion restriction?\nIn (b) what is the implicit assumption about instrument relevance?\n\n[Hint: Write down the implied reduced form equation for \\(Y_{2}\\).]\n\nIn a generic application would you be comfortable with the assumptions in (c) and (d)?\n\nExercise 12.16 Take a linear equation with endogeneity and a just-identified linear reduced form \\(Y=\\) \\(X \\beta+e\\) with \\(X=\\gamma Z+u_{2}\\) where both \\(X\\) and \\(Z\\) are scalar \\(1 \\times 1\\). Assume that \\(\\mathbb{E}[Z e]=0\\) and \\(\\mathbb{E}\\left[Z u_{2}\\right]=0\\).\n\nDerive the reduced form equation \\(Y=Z \\lambda+u_{1}\\). Show that \\(\\beta=\\lambda / \\gamma\\) if \\(\\gamma \\neq 0\\), and that \\(\\mathbb{E}[Z u]=0\\).\nLet \\(\\widehat{\\lambda}\\) denote the OLS estimate from linear regression of \\(Y\\) on \\(Z\\), and let \\(\\widehat{\\gamma}\\) denote the OLS estimate from linear regression of \\(X\\) on \\(Z\\). Write \\(\\theta=(\\lambda, \\gamma)^{\\prime}\\) and let \\(\\widehat{\\theta}=(\\widehat{\\lambda}, \\widehat{\\gamma})^{\\prime}\\). Define \\(u=\\left(u_{1}, u_{2}\\right)\\). Write \\(\\sqrt{n}(\\widehat{\\theta}-\\theta)\\) using a single expression as a function of the error \\(u\\).\nShow that \\(\\mathbb{E}[Z u]=0\\).\nDerive the joint asymptotic distribution of \\(\\sqrt{n}(\\widehat{\\theta}-\\theta)\\) as \\(n \\rightarrow \\infty\\). Hint: Define \\(\\Omega_{u}=\\mathbb{E}\\left[Z^{2} u u^{\\prime}\\right]\\).\nUsing the previous result and the Delta Method find the asymptotic distribution of the Indirect Least Squares estimator \\(\\widehat{\\beta}=\\widehat{\\lambda} / \\widehat{\\gamma}\\).\nIs the answer in (e) the same as the asymptotic distribution of the 2SLS estimator in Theorem 12.2? Hint: Show that \\(\\left(\\begin{array}{ll}1 & -\\beta\\end{array}\\right) u=e\\) and \\(\\left(\\begin{array}{cc}1 & -\\beta\\end{array}\\right) \\Omega_{u}\\left(\\begin{array}{c}1 \\\\ -\\beta\\end{array}\\right)=\\mathbb{E}\\left[Z^{2} e^{2}\\right]\\).\n\nExercise 12.17 Take the model \\(Y=X^{\\prime} \\beta+e\\) with \\(\\mathbb{E}[Z e]=0\\) and consider the two-stage least squares estimator. The first-stage estimate is least squares of \\(X\\) on \\(Z\\) with least squares fitted values \\(\\widehat{X}\\). The second-stage is least squares of \\(Y\\) on \\(\\widehat{X}\\) with coefficient estimator \\(\\widehat{\\beta}\\) and least squares residuals \\(\\widehat{e}_{i}=\\) \\(Y_{i}-\\widehat{X}_{i} \\widehat{\\beta}\\). Consider \\(\\widehat{\\sigma}^{2}=\\frac{1}{n} \\sum_{i=1}^{n} \\widehat{e}_{i}^{2}\\) as an estimator for \\(\\sigma^{2}=\\mathbb{E}\\left[e_{i}^{2}\\right]\\). Is this appropriate? If not, propose an alternative estimator.\nExercise 12.18 You have two independent i.i.d. samples \\(\\left(Y_{1 i}, X_{1 i}, Z_{1 i}: i=1, \\ldots, n\\right)\\) and \\(\\left(Y_{2 i}, X_{2 i}, Z_{2 i}: i=\\right.\\) \\(1, \\ldots, n\\) ). The dependent variables \\(Y_{1}\\) and \\(Y_{2}\\) are real-valued. The regressors \\(X_{1}\\) and \\(X_{2}\\) and instruments \\(Z_{1}\\) and \\(Z_{2}\\) are \\(k\\)-vectors. The model is standard just-identified linear instrumental variables\n\\[\n\\begin{aligned}\nY_{1} &=X_{1}^{\\prime} \\beta_{1}+e_{1} \\\\\n\\mathbb{E}\\left[Z_{1} e_{1}\\right] &=0 \\\\\nY_{2} &=X_{2}^{\\prime} \\beta_{2}+e_{2} \\\\\n\\mathbb{E}\\left[Z_{2} e_{2}\\right] &=0 .\n\\end{aligned}\n\\]\nFor concreteness, sample 1 are women and sample 2 are men. You want to test \\(\\mathbb{M}_{0}: \\beta_{1}=\\beta_{2}\\), that the two samples have the same coefficients.\n\nDevelop a test statistic for \\(\\mathbb{H}_{0}\\).\nDerive the asymptotic distribution of the test statistic. (c) Describe (in brief) the testing procedure.\n\nExercise 12.19 You want to use household data to estimate \\(\\beta\\) in the model \\(Y=X \\beta+e\\) with \\(X\\) scalar and endogenous, using as an instrument the state of residence.\n\nWhat are the assumptions needed to justify this choice of instrument?\nIs the model just identified or overidentified?\n\nExercise 12.20 The model is \\(Y=X^{\\prime} \\beta+e\\) with \\(\\mathbb{E}[Z e]=0\\). An economist wants to obtain the 2 SLS estimates and standard errors for \\(\\beta\\). He uses the following steps\n\nRegresses \\(X\\) on \\(Z\\), obtains the predicted values \\(\\widehat{X}\\).\nRegresses \\(Y\\) on \\(\\widehat{X}\\), obtains the coefficient estimate \\(\\widehat{\\beta}\\) and standard error \\(s(\\widehat{\\beta})\\) from this regression. Is this correct? Does this produce the 2SLS estimates and standard errors?\n\nExercise 12.21 In the linear model \\(Y=X \\beta+e\\) with \\(X \\in \\mathbb{R}\\) suppose \\(\\sigma^{2}(x)=\\mathbb{E}\\left[e^{2} \\mid X=x\\right]\\) is known. Show that the GLS estimator of \\(\\beta\\) can be written as an instrumental variables estimator using some instrument \\(Z\\). (Find an expression for \\(Z\\).)\nExercise 12.22 You will replicate and extend the work reported in Acemoglu, Johnson, and Robinson (2001). The authors provided an expanded set of controls when they published their 2012 extension and posted the data on the AER website. This dataset is A JR2001 on the textbook website.\n\nEstimate the OLS regression (12.86), the reduced form regression (12.87), and the 2SLS regression (12.88). (Which point estimate is different by \\(0.01\\) from the reported values? This is a common phenomenon in empirical replication).\nFor the above estimates calculate both homoskedastic and heteroskedastic-robust standard errors. Which were used by the authors (as reported in (12.86)-(12.87)-(12.88)?)\nCalculate the 2SLS estimates by the Indirect Least Squares formula. Are they the same?\nCalculate the 2SLS estimates by the two-stage approach. Are they the same?\nCalculate the 2SLS estimates by the control variable approach. Are they the same?\nAcemoglu, Johnson, and Robinson (2001) reported many specifications including alternative regressor controls, for example latitude and africa. Estimate by least squares the equation for logGDP adding latitude and africa as regressors. Does this regression suggest that latitude and africa are predictive of the level of GDP?\nNow estimate the same equation as in (f) but by 2SLS using log(mortality) as an instrument for risk. How does the interpretation of the effect of latitude and africa change?\nReturn to our baseline model (without including latitude and africa). The authors’ reduced form equation uses \\(\\log\\) (mortality) as the instrument, rather than, say, the level of mortality. Estimate the reduced form for risk with mortality as the instrument. (This variable is not provided in the dataset so you need to take the exponential of \\(\\log\\) (mortality).) Can you explain why the authors preferred the equation with \\(\\log (\\) mortality) ? (i) Try an alternative reduced form including both \\(\\log\\) (mortality) and the square of \\(\\log (\\) mortality). Interpret the results. Re-estimate the structural equation by 2 SLS using both \\(\\log (\\) mortality) and its square as instruments. How do the results change?\nFor the estimates in (i) are the instruments strong or weak using the Stock-Yogo test?\nCalculate and interpret a test for exogeneity of the instruments.\nEstimate the equation by LIML using the instruments \\(\\log (\\) mortality) and the square of \\(\\log (\\) mortality).\n\nExercise 12.23 In Exercise 12.22 you extended the work reported in Acemoglu, Johnson, and Robinson (2001). Consider the 2SLS regression (12.88). Compute the standard errors both by the asymptotic formula and by the bootstrap using a large number \\((10,000)\\) of bootstrap replications. Re-calculate the bootstrap standard errors. Comment on the reliability of bootstrap standard errors for IV regression.\nExercise 12.24 You will replicate and extend the work reported in the chapter relating to Card (1995). The data is from the author’s website and is posted as Card1995. The model we focus on is labeled 2SLS(a) in Table \\(12.1\\) which uses public and private as instruments for edu. The variables you will need for this exercise include lwage76, ed76, age76, smsa76r, reg76r, nearc2, nearc4, nearc4a, nearc4b. See the description file for definitions. Experience is not in the dataset, so needs to be generated as age-edu-6.\n\nFirst, replicate the reduced form regression presented in the final column of Table 12.2, and the 2SLS regression described above (using public and private as instruments for \\(e d u\\) ) to verify that you have the same variable defintions.\nTry a different reduced form model. The variable nearc2 means “grew up near a 2-year college”. See if adding it to the reduced form equation is useful.\nTry more interactions in the reduced form. Create the interactions nearc \\(4 a^{*}\\) age 76 and nearc \\(4 a^{*}\\) age \\(76^{2} / 100\\), and add them to the reduced form equation. Estimate this by least squares. Interpret the coefficients on the two new variables.\nEstimate the structural equation by 2SLS using the expanded instrument set \\(\\left\\{\\right.\\) nearc \\(4 a\\), nearc \\(4 b\\), nearc \\(4 a^{*}\\) age 76 , nearc \\(4 a^{*}\\) age \\(\\left.76^{2} / 100\\right\\}\\).\n\nWhat is the impact on the structural estimate of the return to schooling?\n\nUsing the Stock-Yogo test are the instruments strong or weak?\nTest the hypothesis that \\(e d u\\) is exogenous for the structural return to schooling.\nRe-estimate the last equation by LIML. Do the results change meaningfully?\n\nExercise 12.25 In Exercise 12.24 you extended the work reported in Card (1995). Now, estimate the IV equation corresponding to the IV(a) column of Table 12.1 which is the baseline specification considered in Card. Use the bootstrap to calculate a BC percentile confidence interval. In this example should we also report the bootstrap standard error?\nExercise 12.26 You will extend Angrist and Krueger (1991) using the data file AK1991 on the textbook website.. Their Table VIII reports estimates of an analog of (12.90) for the subsample of 26,913 Black men. Use this sub-sample for the following analysis. (a) Estimate an equation which is identical in form to (12.90) with the same additional regressors (year-of-birth, region-of-residence, and state-of-birth dummy variables) and 180 excluded instrumental variables (the interactions of quarter-of-birth times year-of-birth dummy variables and quarter-of-birth times state-of-birth interactions) but use the subsample of Black men. One regressor must be omitted to achieve identification. Which variable is this?\n\nEstimate the reduced form for the above equation by least squares. Calculate the \\(F\\) statistic for the excluded instruments. What do you conclude about the strength of the instruments?\nRepeat, estimating the reduced form for the analog of (12.89) which has 30 excluded instrumental variables and does not include the state-of-birth dummy variables in the regression. What do you conclude about the strength of the instruments?\nRepeat, estimating the reduced form for the analog of (12.92) which has only 3 excluded instrumental variables. Are the instruments sufficiently strong for 2SLS estimation? For LIML estimation?\nEstimate the structural wage equation using what you believe is the most appropriate set of regressors, instruments, and the most appropriate estimation method. What is the estimated return to education (for the subsample of Black men) and its standard error? Without doing a formal hypothesis test, do these results (or in which way?) appear meaningfully different from the results for the full sample?\n\nExercise 12.27 In Exercise 12.26 you extended the work reported in Angrist and Krueger (1991) by estimating wage equations for the subsample of Black men. Re-estimate equation (12.92) for this group using as instruments only the three quarter-of-birth dummy variables. Calculate the standard error for the return to education by asymptotic and bootstrap methods. Calculate a BC percentile interval. In this application of 2SLS is it appropriate to report the bootstrap standard error?"
  },
  {
    "objectID": "chpt13-gmm.html",
    "href": "chpt13-gmm.html",
    "title": "13  Generalized Method of Moments",
    "section": "",
    "text": "One of the most popular estimation methods in applied econometrics is the Generalized Method of Moments (GMM). GMM generalizes classical method of moments by allowing for more equations than unknown parameters (so are overidentified) and by allowing general nonlinear functions of the observations and parameters. Together this allows for a fairly rich and flexible estimation framework. GMM includes as special cases OLS, IV, multivariate regression, and 2SLS. It includes both linear and nonlinear models. In this chapter we focus primarily on linear models.\nThe GMM label and methods were introduced to econometrics in a seminal paper by Lars Hansen (1982). The ideas and methods build on the work of Amemiya \\((1974,1977)\\), Gallant (1977), and Gallant and Jorgenson (1979). The ideas are closely related to the contemporeneous work of Halbert White (1980, 1982) and White and Domowitz (1984). The methods are also related to what are called estimating equations in the statistics literature. For a review of the latter see Godambe (1991)."
  },
  {
    "objectID": "chpt13-gmm.html#moment-equation-models",
    "href": "chpt13-gmm.html#moment-equation-models",
    "title": "13  Generalized Method of Moments",
    "section": "13.2 Moment Equation Models",
    "text": "13.2 Moment Equation Models\nAll of the models that have been introduced so far can be written as moment equation models where the population parameters solve a system of moment equations. Moment equation models are broader than the models so far considered and understanding their common structure opens up straightforward techniques to handle new econometric models.\nMoment equation models take the following form. Let \\(g_{i}(\\beta)\\) be a known \\(\\ell \\times 1\\) function of the \\(i^{\\text {th }}\\) observation and a \\(k \\times 1\\) parameter \\(\\beta\\). A moment equation model is summarized by the moment equations\n\\[\n\\mathbb{E}\\left[g_{i}(\\beta)\\right]=0\n\\]\nand a parameter space \\(\\beta \\in B\\). For example, in the instrumental variables model \\(g_{i}(\\beta)=Z_{i}\\left(Y_{i}-X_{i}^{\\prime} \\beta\\right)\\).\nIn general, we say that a parameter \\(\\beta\\) is identified if there is a unique mapping from the data distribution to \\(\\beta\\). In the context of the model (13.1) this means that there is a unique \\(\\beta\\) satisfying (13.1). Since (13.1) is a system of \\(\\ell\\) equations with \\(k\\) unknowns, then it is necessary that \\(\\ell \\geq k\\) for there to be a unique solution. If \\(\\ell=k\\) we say that the model is just identified, meaning that there is just enough information to identify the parameters. If \\(\\ell>k\\) we say that the model is overidentified, meaning that there is excess information. If \\(\\ell<k\\) we say that the model is underidentified, meaning that there is insufficient information to identify the parameters. In general, we assume that \\(\\ell \\geq k\\) so the model is either just identified or overidentified."
  },
  {
    "objectID": "chpt13-gmm.html#method-of-moments-estimators",
    "href": "chpt13-gmm.html#method-of-moments-estimators",
    "title": "13  Generalized Method of Moments",
    "section": "13.3 Method of Moments Estimators",
    "text": "13.3 Method of Moments Estimators\nIn this section we consider the just-identified case \\(\\ell=k\\).\nDefine the sample analog of (13.5)\n\\[\n\\bar{g}_{n}(\\beta)=\\frac{1}{n} \\sum_{i=1}^{n} g_{i}(\\beta) .\n\\]\nThe method of moments estimator (MME) \\(\\widehat{\\beta}_{\\mathrm{mm}}\\) is the parameter value which sets \\(\\bar{g}_{n}(\\beta)=0\\). Thus\n\\[\n\\bar{g}_{n}\\left(\\widehat{\\beta}_{\\mathrm{mm}}\\right)=\\frac{1}{n} \\sum_{i=1}^{n} g_{i}\\left(\\widehat{\\beta}_{\\mathrm{mm}}\\right)=0 .\n\\]\nThe equations (13.3) are known as the estimating equations as they are the equations which determine the estimator \\(\\widehat{\\beta}_{\\mathrm{mm}}\\).\nIn some contexts (such as those discussed in the examples below) there is an explicit solution for \\(\\widehat{\\beta}_{\\mathrm{mm}}\\). In other cases the solution must be found numerically.\nWe now show how most of the estimators discussed so far in the textbook can be written as method of moments estimators.\nMean: Set \\(g_{i}(\\mu)=Y_{i}-\\mu\\). The MME is \\(\\widehat{\\mu}=\\frac{1}{n} \\sum_{i=1}^{n} Y_{i}\\).\nMean and Variance: Set\n\\[\ng_{i}\\left(\\mu, \\sigma^{2}\\right)=\\left(\\begin{array}{c}\nY_{i}-\\mu \\\\\n\\left(Y_{i}-\\mu\\right)^{2}-\\sigma^{2}\n\\end{array}\\right) .\n\\]\nThe MME are \\(\\widehat{\\mu}=\\frac{1}{n} \\sum_{i=1}^{n} Y_{i}\\) and \\(\\widehat{\\sigma}^{2}=\\frac{1}{n} \\sum_{i=1}^{n}\\left(Y_{i}-\\widehat{\\mu}\\right)^{2}\\).\nOLS: Set \\(g_{i}(\\beta)=X_{i}\\left(Y_{i}-X_{i}^{\\prime} \\beta\\right)\\). The MME is \\(\\widehat{\\beta}=\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{Y}\\right)\\).\nOLS and Variance: Set\n\\[\ng_{i}\\left(\\beta, \\sigma^{2}\\right)=\\left(\\begin{array}{c}\nX_{i}\\left(Y_{i}-X_{i}^{\\prime} \\beta\\right) \\\\\n\\left(Y_{i}-X_{i}^{\\prime} \\beta\\right)^{2}-\\sigma^{2}\n\\end{array}\\right) \\text {. }\n\\]\nThe MME is \\(\\widehat{\\beta}=\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{Y}\\right)\\) and \\(\\widehat{\\sigma}^{2}=\\frac{1}{n} \\sum_{i=1}^{n}\\left(Y_{i}-X_{i}^{\\prime} \\widehat{\\beta}\\right)^{2}\\).\nMultivariate Least Squares, vector form: \\(\\operatorname{Set} g_{i}(\\beta)=\\bar{X}_{i}^{\\prime}\\left(Y_{i}-\\bar{X}_{i} \\beta\\right)\\). The MME is \\(\\widehat{\\beta}=\\left(\\sum_{i=1}^{n} \\bar{X}_{i}^{\\prime} \\bar{X}_{i}\\right)^{-1}\\left(\\sum_{i=1}^{n} \\bar{X}_{i} Y_{i}\\right)\\) which is (11.4).\nMultivariate Least Squares, matrix form: Set \\(g_{i}(\\boldsymbol{B})=\\operatorname{vec}\\left(X_{i}\\left(Y_{i}^{\\prime}-X_{i}^{\\prime} \\boldsymbol{B}\\right)\\right)\\). The MME is \\(\\widehat{\\boldsymbol{B}}=\\left(\\sum_{i=1}^{n} X_{i} X_{i}^{\\prime}\\right)^{-1}\\left(\\sum_{i=1}^{n} X_{i} Y_{i}^{\\prime}\\right)\\) which is (11.6).\nSeemingly Unrelated Regression: Set\n\\[\ng_{i}(\\beta, \\Sigma)=\\left(\\begin{array}{c}\n\\bar{X}_{i} \\Sigma^{-1}\\left(Y_{i}-\\bar{X}_{i}^{\\prime} \\beta\\right) \\\\\n\\operatorname{vec}\\left(\\Sigma-\\left(Y_{i}-\\bar{X}_{i}^{\\prime} \\beta\\right)\\left(Y_{i}-\\bar{X}_{i}^{\\prime} \\beta\\right)^{\\prime}\\right)\n\\end{array}\\right)\n\\]\nThe MME is \\(\\widehat{\\beta}=\\left(\\sum_{i=1}^{n} \\bar{X}_{i} \\widehat{\\Sigma}^{-1} \\bar{X}_{i}^{\\prime}\\right)^{-1}\\left(\\sum_{i=1}^{n} \\bar{X}_{i} \\widehat{\\Sigma}^{-1} Y_{i}\\right)\\) and \\(\\widehat{\\Sigma}=n^{-1} \\sum_{i=1}^{n}\\left(Y_{i}-\\bar{X}_{i}^{\\prime} \\widehat{\\beta}\\right)\\left(Y_{i}-\\bar{X}_{i}^{\\prime} \\widehat{\\beta}\\right)^{\\prime}\\).\nIV: Set \\(g_{i}(\\beta)=Z_{i}\\left(Y_{i}-X_{i}^{\\prime} \\beta\\right)\\). The MME is \\(\\widehat{\\beta}=\\left(\\boldsymbol{Z}^{\\prime} \\boldsymbol{X}\\right)^{-1}\\left(\\boldsymbol{Z}^{\\prime} \\boldsymbol{Y}\\right)\\). Generated Regressors: Set\n\\[\ng_{i}(\\beta, \\boldsymbol{A})=\\left(\\begin{array}{c}\n\\boldsymbol{A}^{\\prime} Z_{i}\\left(Y_{i}-Z_{i}^{\\prime} \\boldsymbol{A} \\beta\\right) \\\\\n\\operatorname{vec}\\left(Z_{i}\\left(X_{i}^{\\prime}-Z_{i}^{\\prime} \\boldsymbol{A}\\right)\\right)\n\\end{array}\\right)\n\\]\nThe MME is \\(\\widehat{\\boldsymbol{A}}=\\left(\\sum_{i=1}^{n} Z_{i} Z_{i}^{\\prime}\\right)^{-1}\\left(\\sum_{i=1}^{n} Z_{i} X_{i}^{\\prime}\\right)\\) and \\(\\widehat{\\beta}=\\left(\\widehat{\\boldsymbol{A}}^{\\prime} \\boldsymbol{Z}^{\\prime} \\boldsymbol{Z} \\widehat{\\boldsymbol{A}}\\right)^{-1}\\left(\\widehat{\\boldsymbol{A}}^{\\prime} \\boldsymbol{Z}^{\\prime} \\boldsymbol{Y}\\right)\\).\nA common feature of these examples is that the estimator can be written as the solution to a set of estimating equations (13.3). This provides a common framework which enables a convenient development of a unified distribution theory."
  },
  {
    "objectID": "chpt13-gmm.html#overidentified-moment-equations",
    "href": "chpt13-gmm.html#overidentified-moment-equations",
    "title": "13  Generalized Method of Moments",
    "section": "13.4 Overidentified Moment Equations",
    "text": "13.4 Overidentified Moment Equations\nIn the instrumental variables model \\(g_{i}(\\beta)=Z_{i}\\left(Y_{i}-X_{i}^{\\prime} \\beta\\right)\\). Thus (13.2) is\n\\[\n\\bar{g}_{n}(\\beta)=\\frac{1}{n} \\sum_{i=1}^{n} g_{i}(\\beta)=\\frac{1}{n} \\sum_{i=1}^{n} Z_{i}\\left(Y_{i}-X_{i}^{\\prime} \\beta\\right)=\\frac{1}{n}\\left(\\boldsymbol{Z}^{\\prime} \\boldsymbol{Y}-\\boldsymbol{Z}^{\\prime} \\boldsymbol{X} \\beta\\right) .\n\\]\nWe have defined the method of moments estimator for \\(\\beta\\) as the parameter value which sets \\(\\bar{g}_{n}(\\beta)=\\) 0 . However, when the model is overidentified (if \\(\\ell>k\\) ) this is generally impossible as there are more equations than free parameters. Equivalently, there is no choice of \\(\\beta\\) which sets (13.4) to zero. Thus the method of moments estimator is not defined for the overidentified case.\nWhile we cannot find an estimator which sets \\(\\bar{g}_{n}(\\beta)\\) equal to zero we can try to find an estimator which makes \\(\\bar{g}_{n}(\\beta)\\) as close to zero as possible.\nOne way to think about this is to define the vector \\(\\mu=\\boldsymbol{Z}^{\\prime} \\boldsymbol{Y}\\), the matrix \\(\\boldsymbol{G}=\\boldsymbol{Z}^{\\prime} \\boldsymbol{X}\\) and the “error” \\(\\eta=\\mu-\\boldsymbol{G} \\beta\\). Then we can write (13.4) as \\(\\mu=\\boldsymbol{G} \\beta+\\eta\\). This looks like a regression equation with the \\(\\ell \\times 1\\) dependent variable \\(\\mu\\), the \\(\\ell \\times k\\) regressor matrix \\(\\boldsymbol{G}\\), and the \\(\\ell \\times 1\\) error vector \\(\\eta\\). The goal is to make the error vector \\(\\eta\\) as small as possible. Recalling our knowledge about least squares we deduct that a simple method is to regress \\(\\mu\\) on \\(\\boldsymbol{G}\\), obtaining \\(\\widehat{\\beta}=\\left(\\boldsymbol{G}^{\\prime} \\boldsymbol{G}\\right)^{-1}\\left(\\boldsymbol{G}^{\\prime} \\mu\\right)\\). This minimizes the sum-of-squares \\(\\eta^{\\prime} \\eta\\). This is certainly one way to make \\(\\eta\\) “small”.\nMore generally we know that when errors are non-homogeneous it can be more efficient to estimate by weighted least squares. Thus for some weight matrix \\(\\boldsymbol{W}\\) consider the estimator\n\\[\n\\widehat{\\beta}=\\left(\\boldsymbol{G}^{\\prime} \\boldsymbol{W} \\boldsymbol{G}\\right)^{-1}\\left(\\boldsymbol{G}^{\\prime} \\boldsymbol{W} \\boldsymbol{\\mu}\\right)=\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{Z} \\boldsymbol{W} \\boldsymbol{Z}^{\\prime} \\boldsymbol{X}\\right)^{-1}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{Z} \\boldsymbol{W} \\boldsymbol{Z}^{\\prime} \\boldsymbol{Y}\\right) .\n\\]\nThis minimizes the weighted sum of squares \\(\\eta^{\\prime} \\boldsymbol{W} \\eta\\). This solution is known as the generalized method of moments (GMM).\nThe estimator is typically defined as follows. Given a set of moment equations (13.2) and an \\(\\ell \\times \\ell\\) weight matrix \\(\\boldsymbol{W}>0\\) the GMM criterion function is defined as\n\\[\nJ(\\beta)=n \\bar{g}_{n}(\\beta)^{\\prime} \\boldsymbol{W} \\bar{g}_{n}(\\beta) .\n\\]\nThe factor ” \\(n\\) ” is not important for the definition of the estimator but is convenient for the distribution theory. The criterion \\(J(\\beta)\\) is the weighted sum of squared moment equation errors. When \\(\\boldsymbol{W}=\\boldsymbol{I}_{\\ell}\\) then \\(J(\\beta)=n \\bar{g}_{n}(\\beta)^{\\prime} \\bar{g}_{n}(\\beta)=n\\left\\|\\bar{g}_{n}(\\beta)\\right\\|^{2}\\), the square of the Euclidean length. Since we restrict attention to positive definite weight matrices \\(\\boldsymbol{W}\\) the criterion \\(J(\\beta)\\) is non-negative.\nDefinition 13.1 The Generalized Method of Moments (GMM) estimator is\n\\[\n\\widehat{\\beta}_{\\mathrm{gmm}}=\\underset{\\beta}{\\operatorname{argmin}} J(\\beta) .\n\\]\nRecall that in the just-identified case \\(k=\\ell\\) the method of moments estimator \\(\\widehat{\\beta}_{\\mathrm{mm}}\\) solves \\(\\bar{g}_{n}\\left(\\widehat{\\beta}_{\\mathrm{mm}}\\right)=\\) 0 . Hence in this case \\(J\\left(\\widehat{\\beta}_{\\mathrm{mm}}\\right)=0\\) which means that \\(\\widehat{\\beta}_{\\mathrm{mm}}\\) minimizes \\(J(\\beta)\\) and equals \\(\\widehat{\\beta}_{\\mathrm{gmm}}=\\widehat{\\beta}_{\\mathrm{mm}}\\). This means that GMM includes MME as a special case. This implies that all of our results for GMM apply to any method of moments estimator.\nIn the over-identified case the GMM estimator depends on the choice of weight matrix \\(\\boldsymbol{W}\\) and so this is an important focus of the theory. In the just-identified case the GMM estimator simplifies to the MME which does not depend on \\(\\boldsymbol{W}\\).\nThe method and theory of the generalized method of moments was developed in an influential paper by Lars Hansen (1982). This paper introduced the method, its asymptotic distribution, the form of the efficient weight matrix, and tests for overidentification."
  },
  {
    "objectID": "chpt13-gmm.html#linear-moment-models",
    "href": "chpt13-gmm.html#linear-moment-models",
    "title": "13  Generalized Method of Moments",
    "section": "13.5 Linear Moment Models",
    "text": "13.5 Linear Moment Models\nOne of the great advantages of the moment equation framework is that it allows both linear and nonlinear models. However, when the moment equations are linear in the parameters then we have explicit solutions for the estimates and a straightforward asymptotic distribution theory. Hence we start by confining attention to linear moment equations and return to nonlinear moment equations later. In the examples listed earlier the estimators which have linear moment equations include the sample mean, OLS, multivariate least squares, IV, and 2SLS. The estimates which have nonlinear moment equations include the sample variance, SUR, and generated regressors.\nIn particular, we focus on the overidentified IV model with moment equations\n\\[\ng_{i}(\\beta)=Z_{i}\\left(Y_{i}-X_{i}^{\\prime} \\beta\\right)\n\\]\nwhere \\(Z_{i}\\) is \\(\\ell \\times 1\\) and \\(X_{i}\\) is \\(k \\times 1\\)."
  },
  {
    "objectID": "chpt13-gmm.html#gmm-estimator",
    "href": "chpt13-gmm.html#gmm-estimator",
    "title": "13  Generalized Method of Moments",
    "section": "13.6 GMM Estimator",
    "text": "13.6 GMM Estimator\nGiven (13.5) the sample moment equations are (13.4). The GMM criterion can be written as\n\\[\nJ(\\beta)=n\\left(\\boldsymbol{Z}^{\\prime} \\boldsymbol{Y}-\\boldsymbol{Z}^{\\prime} \\boldsymbol{X} \\beta\\right)^{\\prime} \\boldsymbol{W}\\left(\\boldsymbol{Z}^{\\prime} \\boldsymbol{Y}-\\boldsymbol{Z}^{\\prime} \\boldsymbol{X} \\beta\\right) .\n\\]\nThe GMM estimator minimizes \\(J(\\beta)\\). The first order conditions are\n\\[\n\\begin{aligned}\n0 &=\\frac{\\partial}{\\partial \\beta} J(\\widehat{\\beta}) \\\\\n&=2 \\frac{\\partial}{\\partial \\beta} \\bar{g}_{n}(\\widehat{\\beta})^{\\prime} \\boldsymbol{W} \\bar{g}_{n}(\\widehat{\\beta}) \\\\\n&=-2\\left(\\frac{1}{n} \\boldsymbol{X}^{\\prime} \\boldsymbol{Z}\\right) \\boldsymbol{W}\\left(\\frac{1}{n} \\boldsymbol{Z}^{\\prime}(\\boldsymbol{Y}-\\boldsymbol{X} \\widehat{\\beta})\\right) .\n\\end{aligned}\n\\]\nThe solution is given as follows.\nTheorem 13.1 For the overidentified IV model\n\\[\n\\widehat{\\beta}_{\\mathrm{gmm}}=\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{Z} \\boldsymbol{W} \\boldsymbol{Z}^{\\prime} \\boldsymbol{X}\\right)^{-1}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{Z} \\boldsymbol{W} \\boldsymbol{Z}^{\\prime} \\boldsymbol{Y}\\right) .\n\\]\nWhile the estimator depends on \\(\\boldsymbol{W}\\) the dependence is only up to scale. This is because if \\(\\boldsymbol{W}\\) is replaced by \\(c W\\) for some \\(c>0, \\widehat{\\beta}_{\\text {gmm }}\\) does not change. When \\(W\\) is fixed by the user we call \\(\\widehat{\\beta}_{\\text {gmm }}\\) ane-step GMM estimator. The formula (13.6) applies for the over-identified \\((\\ell>k)\\) and the just-identified \\((\\ell=k)\\) case. When the model is just-identified then \\(\\boldsymbol{X}^{\\prime} \\boldsymbol{Z}\\) is \\(k \\times k\\) so expression (13.6) simplifies to\n\\[\n\\widehat{\\beta}_{\\mathrm{gmm}}=\\left(\\boldsymbol{Z}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\boldsymbol{W}^{-1}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{Z}\\right)^{-1}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{Z} \\boldsymbol{W} \\boldsymbol{Z}^{\\prime} \\boldsymbol{Y}\\right)=\\left(\\boldsymbol{Z}^{\\prime} \\boldsymbol{X}\\right)^{-1}\\left(\\boldsymbol{Z}^{\\prime} \\boldsymbol{Y}\\right)=\\widehat{\\beta}_{\\mathrm{iv}}\n\\]\nthe IV estimator.\nThe GMM estimator (13.6) resembles the 2SLS estimator (12.29). In fact they are equal when \\(\\boldsymbol{W}=\\) \\(\\left(\\boldsymbol{Z}^{\\prime} \\boldsymbol{Z}\\right)^{-1}\\). This means that the 2SLS estimator is a one-step GMM estimator for the linear model.\nTheorem 13.2 If \\(\\boldsymbol{W}=\\left(\\boldsymbol{Z}^{\\prime} \\boldsymbol{Z}\\right)^{-1}\\) then \\(\\widehat{\\beta}_{\\mathrm{gmm}}=\\widehat{\\beta}_{2 \\text { sls. }}\\) Furthermore, if \\(k=\\ell\\) then \\(\\widehat{\\beta}_{\\mathrm{gmm}}=\\widehat{\\beta}_{\\mathrm{iv}}\\)"
  },
  {
    "objectID": "chpt13-gmm.html#distribution-of-gmm-estimator",
    "href": "chpt13-gmm.html#distribution-of-gmm-estimator",
    "title": "13  Generalized Method of Moments",
    "section": "13.7 Distribution of GMM Estimator",
    "text": "13.7 Distribution of GMM Estimator\nLet \\(\\boldsymbol{Q}=\\mathbb{E}\\left[Z X^{\\prime}\\right]\\) and \\(\\Omega=\\mathbb{E}\\left[Z Z^{\\prime} e^{2}\\right]\\). Then\n\\[\n\\left(\\frac{1}{n} \\boldsymbol{X}^{\\prime} \\boldsymbol{Z}\\right) \\boldsymbol{W}\\left(\\frac{1}{n} \\boldsymbol{Z}^{\\prime} \\boldsymbol{X}\\right) \\underset{p}{\\longrightarrow} \\boldsymbol{Q}^{\\prime} \\boldsymbol{W} \\boldsymbol{Q}\n\\]\nand\n\\[\n\\left(\\frac{1}{n} \\boldsymbol{X}^{\\prime} \\boldsymbol{Z}\\right) \\boldsymbol{W}\\left(\\frac{1}{\\sqrt{n}} \\boldsymbol{Z}^{\\prime} \\boldsymbol{e}\\right) \\underset{d}{\\longrightarrow} \\boldsymbol{Q}^{\\prime} \\boldsymbol{W} \\mathrm{N}(0, \\Omega)\n\\]\nWe conclude:\nTheorem 13.3 Asymptotic Distribution of GMM Estimator. Under Assumption 12.2, as \\(n \\rightarrow \\infty, \\sqrt{n}\\left(\\widehat{\\beta}_{\\mathrm{gmm}}-\\beta\\right) \\underset{d}{\\longrightarrow} \\mathrm{N}\\left(0, \\boldsymbol{V}_{\\beta}\\right)\\) where\n\\[\n\\boldsymbol{V}_{\\boldsymbol{\\beta}}=\\left(\\boldsymbol{Q}^{\\prime} \\boldsymbol{W} \\boldsymbol{Q}\\right)^{-1}\\left(\\boldsymbol{Q}^{\\prime} \\boldsymbol{W} \\Omega \\boldsymbol{W} \\boldsymbol{Q}\\right)\\left(\\boldsymbol{Q}^{\\prime} \\boldsymbol{W} \\boldsymbol{Q}\\right)^{-1} .\n\\]\nThe GMM estimator is asymptotically normal with a “sandwich form” asymptotic variance.\nOur derivation treated the weight matrix \\(W\\) as if it is non-random but Theorem \\(13.3\\) applies to the random weight matrix case so long as \\(\\widehat{\\boldsymbol{W}}\\) converges in probability to a positive definite limit \\(\\boldsymbol{W}\\). This may require scaling the weight matrix, for example replacing \\(\\widehat{\\boldsymbol{W}}=\\left(\\boldsymbol{Z}^{\\prime} \\boldsymbol{Z}\\right)^{-1}\\) with \\(\\widehat{\\boldsymbol{W}}=\\left(n^{-1} \\boldsymbol{Z}^{\\prime} \\boldsymbol{Z}\\right)^{-1}\\). Since rescaling the weight matrix does not affect the estimator this is ignored in implementation."
  },
  {
    "objectID": "chpt13-gmm.html#efficient-gmm",
    "href": "chpt13-gmm.html#efficient-gmm",
    "title": "13  Generalized Method of Moments",
    "section": "13.8 Efficient GMM",
    "text": "13.8 Efficient GMM\nThe asymptotic distribution of the GMM estimator \\(\\widehat{\\beta}_{\\mathrm{gmm}}\\) depends on the weight matrix \\(\\boldsymbol{W}\\) through the asymptotic variance \\(\\boldsymbol{V}_{\\beta}\\). The asymptotically optimal weight matrix \\(\\boldsymbol{W}_{0}\\) is that which minimizes \\(\\boldsymbol{V}_{\\beta}\\). This turns out to be \\(\\boldsymbol{W}_{0}=\\Omega^{-1}\\). The proof is left to Exercise 13.4.\nWhen the GMM estimator \\(\\widehat{\\beta}\\) is constructed with \\(\\boldsymbol{W}=\\boldsymbol{W}_{0}=\\Omega^{-1}\\) (or a weight matrix which is a consistent estimator of \\(\\boldsymbol{W}_{0}\\) ) we call it the Efficient GMM estimator:\n\\[\n\\widehat{\\beta}_{\\mathrm{gmm}}=\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{Z} \\Omega^{-1} \\boldsymbol{Z}^{\\prime} \\boldsymbol{X}\\right)^{-1}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{Z} \\Omega^{-1} \\boldsymbol{Z}^{\\prime} \\boldsymbol{Y}\\right) .\n\\]\nIts asymptotic distribution takes a simpler form than in Theorem 13.3. By substituting \\(\\boldsymbol{W}=\\boldsymbol{W}_{0}=\\Omega^{-1}\\) into (13.7) we find\n\\[\n\\boldsymbol{V}_{\\beta}=\\left(\\boldsymbol{Q}^{\\prime} \\Omega^{-1} \\boldsymbol{Q}\\right)^{-1}\\left(\\boldsymbol{Q}^{\\prime} \\Omega^{-1} \\Omega \\Omega^{-1} \\boldsymbol{Q}\\right)\\left(\\boldsymbol{Q}^{\\prime} \\Omega^{-1} \\boldsymbol{Q}\\right)^{-1}=\\left(\\boldsymbol{Q}^{\\prime} \\Omega^{-1} \\boldsymbol{Q}\\right)^{-1} .\n\\]\nThis is the asymptotic variance of the efficient GMM estimator.\nTheorem 13.4 Asymptotic Distribution of GMM with Efficient Weight Ma-\\ trix. Under Assumption \\(12.2\\) and \\(\\boldsymbol{W}=\\Omega^{-1}\\), as \\(n \\rightarrow \\infty, \\sqrt{n}\\left(\\widehat{\\beta}_{\\mathrm{gmm}}-\\beta\\right) \\underset{d}{\\mathrm{~d}}\\)\\ \\(\\mathrm{~N}\\left(0, \\boldsymbol{V}_{\\beta}\\right)\\) where \\(\\boldsymbol{V}_{\\beta}=\\left(\\boldsymbol{Q}^{\\prime} \\Omega^{-1} \\boldsymbol{Q}\\right)^{-1}\\).\nTheorem 13.5 Efficient GMM. Under Assumption 12.2, for any \\(\\boldsymbol{W}>0\\),\n\\[\n\\left(\\boldsymbol{Q}^{\\prime} \\boldsymbol{W} \\boldsymbol{Q}\\right)^{-1}\\left(\\boldsymbol{Q}^{\\prime} \\boldsymbol{W} \\Omega \\boldsymbol{W} \\boldsymbol{Q}\\right)\\left(\\boldsymbol{Q}^{\\prime} \\boldsymbol{W} \\boldsymbol{Q}\\right)^{-1}-\\left(\\boldsymbol{Q}^{\\prime} \\Omega^{-1} \\boldsymbol{Q}\\right)^{-1} \\geq 0\n\\]\nThe inequality ” \\(\\geq\\) ” can be replaced with ” \\(>\\) ” if \\(W \\neq \\Omega^{-1}\\). Thus if \\(\\widehat{\\beta}_{\\mathrm{gmm}}\\) is the efficient GMM estimator and \\(\\widetilde{\\beta}_{\\text {gmm }}\\) is another GMM estimator, then\n\\[\n\\operatorname{avar}\\left[\\widehat{\\beta}_{\\mathrm{gmm}}\\right] \\leq \\operatorname{avar}\\left[\\widetilde{\\beta}_{\\mathrm{gmm}}\\right] .\n\\]\nFor a proof see Exercise 13.4.\nThis means that the smallest possible GMM covariance matrix (in the positive definite sense) is achieved by the efficient GMM weight matrix.\n\\(\\boldsymbol{W}_{0}=\\Omega^{-1}\\) is not known in practice but it can be estimated consistently as we discuss in Section \\(13.10 .\\) For any \\(\\widehat{\\boldsymbol{W}} \\underset{p}{\\rightarrow} \\boldsymbol{W}_{0}\\) the asymptotic distribution in Theorem \\(13.4\\) is unaffected. Consequently we call any \\(\\widehat{\\beta}_{\\mathrm{gmm}}\\) constructed with an estimate of the efficient weight matrix an efficient GMM estimator.\nBy “efficient” we mean that this estimator has the smallest asymptotic variance in the class of GMM estimators with this set of moment conditions. This is a weak concept of optimality as we are only considering alternative weight matrices \\(\\widehat{\\boldsymbol{W}}\\). However, it turns out that the GMM estimator is semiparametrically efficient as shown by Gary Chamberlain (1987). If it is known that \\(\\mathbb{E}\\left[g_{i}(\\beta)\\right]=0\\) and this is all that is known this is a semi-parametric problem as the distribution of the data is unknown. Chamberlain showed that in this context no semiparametric estimator (one which is consistent globally for the class of models considered) can have a smaller asymptotic variance than \\(\\left(\\boldsymbol{G}^{\\prime} \\Omega^{-1} \\boldsymbol{G}\\right)^{-1}\\) where \\(\\boldsymbol{G}=\\mathbb{E}\\left[\\frac{\\partial}{\\partial \\beta^{\\prime}} g_{i}(\\beta)\\right]\\). Since the GMM estimator has this asymptotic variance it is semiparametrically efficient.\nThe results in this section show that in the linear model no estimator has better asymptotic efficiency than the efficient linear GMM estimator. No estimator can do better (in this first-order asymptotic sense) without imposing additional assumptions."
  },
  {
    "objectID": "chpt13-gmm.html#efficient-gmm-versus-2sls",
    "href": "chpt13-gmm.html#efficient-gmm-versus-2sls",
    "title": "13  Generalized Method of Moments",
    "section": "13.9 Efficient GMM versus 2SLS",
    "text": "13.9 Efficient GMM versus 2SLS\nFor the linear model we introduced 2SLS as a standard estimator for \\(\\beta\\). Now we have introduced GMM which includes 2SLS as a special case. Is there a context where 2SLS is efficient?\nTo answer this question recall that 2SLS is GMM given the weight matrix \\(\\widehat{\\boldsymbol{W}}=\\left(\\boldsymbol{Z}^{\\prime} \\boldsymbol{Z}\\right)^{-1}\\) or equivalently \\(\\widehat{\\boldsymbol{W}}=\\left(n^{-1} \\boldsymbol{Z}^{\\prime} \\boldsymbol{Z}\\right)^{-1}\\) since scaling doesn’t matter. Since \\(\\widehat{\\boldsymbol{W}} \\underset{p}{\\longrightarrow}\\left(\\mathbb{E}\\left[Z Z^{\\prime}\\right]\\right)^{-1}\\) this is asymptotically equivalent to the weight matrix \\(\\boldsymbol{W}=\\left(\\mathbb{E}\\left[Z Z^{\\prime}\\right]\\right)^{-1}\\). In contrast, the efficient weight matrix takes the form \\(\\left(\\mathbb{E}\\left[Z Z^{\\prime} e^{2}\\right]\\right)^{-1}\\). Now suppose that the structural equation error \\(e\\) is conditionally homoskedastic in the sense that \\(\\mathbb{E}\\left[e^{2} \\mid Z\\right]=\\sigma^{2}\\). Then the efficient weight matrix equals \\(\\boldsymbol{W}=\\left(\\mathbb{E}\\left[Z Z^{\\prime}\\right]\\right)^{-1} \\sigma^{-2}\\) or equivalently \\(W=\\left(\\mathbb{E}\\left[Z Z^{\\prime}\\right]\\right)^{-1}\\) since scaling doesn’t matter. The latter weight matrix is the same as the 2SLS asymptotic weight matrix. This shows that the 2SLS weight matrix is the efficient weight matrix under conditional homoskedasticity.\nTheorem 13.6 Under Assumption \\(12.2\\) and \\(\\mathbb{E}\\left[e^{2} \\mid Z\\right]=\\sigma^{2}, \\widehat{\\beta}_{2 \\text { sls }}\\) is efficient GMM.\nThis shows that 2SLS is efficient under homoskedasticity. When homoskedasticity holds there is no reason to use efficient GMM over 2SLS. More broadly, when homoskedasticity is a reasonable approximation then 2SLS will be a reasonable estimator. However, this result also shows that in the general case where the error is conditionally heteroskedastic, 2SLS is inefficient relative to efficient GMM."
  },
  {
    "objectID": "chpt13-gmm.html#estimation-of-the-efficient-weight-matrix",
    "href": "chpt13-gmm.html#estimation-of-the-efficient-weight-matrix",
    "title": "13  Generalized Method of Moments",
    "section": "13.10 Estimation of the Efficient Weight Matrix",
    "text": "13.10 Estimation of the Efficient Weight Matrix\nTo construct the efficient GMM estimator we need a consistent estimator \\(\\widehat{\\boldsymbol{W}}\\) of \\(\\boldsymbol{W}_{0}=\\Omega^{-1}\\). The convention is to form an estimator \\(\\widehat{\\Omega}\\) of \\(\\Omega\\) and then set \\(\\widehat{\\boldsymbol{W}}=\\widehat{\\Omega}^{-1}\\).\nThe two-step GMM estimator proceeds by using a one-step consistent estimator of \\(\\beta\\) to construct the weight matrix estimator \\(\\widehat{\\boldsymbol{W}}\\). In the linear model the natural one-step estimator for \\(\\beta\\) is 2 SLS. Set \\(\\widetilde{e}_{i}=Y_{i}-X_{i}^{\\prime} \\widehat{\\beta}_{2 s l s}, \\widetilde{g}_{i}=g_{i}(\\widetilde{\\beta})=Z_{i} \\widetilde{e}_{i}\\), and \\(\\bar{g}_{n}=n^{-1} \\sum_{i=1}^{n} \\widetilde{g}_{i}\\). Two moment estimators of \\(\\Omega\\) are\n\\[\n\\widehat{\\Omega}=\\frac{1}{n} \\sum_{i=1}^{n} \\widetilde{g}_{i} \\widetilde{g}_{i}^{\\prime}\n\\]\nand\n\\[\n\\widehat{\\Omega}^{*}=\\frac{1}{n} \\sum_{i=1}^{n}\\left(\\widetilde{g}_{i}-\\bar{g}_{n}\\right)\\left(\\widetilde{g}_{i}-\\bar{g}_{n}\\right)^{\\prime} .\n\\]\nThe estimator (13.8) is an uncentered covariance matrix estimator while the estimator (13.9) is a centered version. Either is consistent when \\(\\mathbb{E}[Z e]=0\\) which holds under correct specification. However under misspecification we may have \\(\\mathbb{E}[Z e] \\neq 0\\). In the latter context \\(\\widehat{\\Omega}^{*}\\) remains an estimator of var \\([Z e]\\) while \\(\\widehat{\\Omega}\\) is an estimator of \\(\\mathbb{E}\\left[Z Z^{\\prime} e^{2}\\right]\\). In this sense \\(\\widehat{\\Omega}^{*}\\) is a robust variance estimator. For some testing problems it turns out to be preferable to use a covariance matrix estimator which is robust to the alternative hypothesis. For these reasons estimator (13.9) is generally preferred. The uncentered estimator (13.8) is more commonly seen in practice since it is the default choice by most packages. It is also worth observing that when the model is just identified then \\(\\bar{g}_{n}=0\\) so the two are algebraically identical. The choice of weight matrix may also impact covariance matrix estimation as discussed in Section 13.12.\nGiven the choice of covariance matrix estimator we set \\(\\widehat{W}=\\widehat{\\Omega}^{-1}\\) or \\(\\widehat{W}=\\widehat{\\Omega}^{*-1}\\). Given this weight matrix we construct the two-step GMM estimator as (13.6) using the weight matrix \\(\\widehat{\\boldsymbol{W}}\\).\nSince the 2SLS estimator is consistent for \\(\\beta\\), by arguments nearly identical to those used for covariance matrix estimation we can show that \\(\\widehat{\\Omega}\\) and \\(\\widehat{\\Omega}^{*}\\) are consistent for \\(\\Omega\\) and thus \\(\\widehat{\\boldsymbol{W}}\\) is consistent for \\(\\Omega^{-1}\\). See Exercise 13.3.\nThis also means that the two-step GMM estimator satisfies the conditions for Theorem 13.4.\nTheorem \\(13.7\\) Under Assumption \\(12.2\\) and \\(\\Omega>0\\), if \\(\\widehat{W}=\\widehat{\\Omega}^{-1}\\) or \\(\\widehat{W}=\\) \\(\\widehat{\\Omega}^{*-1}\\) where the latter are defined in (13.8) and (13.9) then as \\(n \\rightarrow \\infty\\), \\(\\sqrt{n}\\left(\\widehat{\\beta}_{\\mathrm{gmm}}-\\beta\\right) \\underset{d}{\\longrightarrow} \\mathrm{N}\\left(0, \\boldsymbol{V}_{\\beta}\\right)\\) where \\(\\boldsymbol{V}_{\\beta}=\\left(\\boldsymbol{Q}^{\\prime} \\Omega^{-1} \\boldsymbol{Q}\\right)^{-1}\\).\nThis shows that the two-step GMM estimator is asymptotically efficient.\nThe two-step GMM estimator of the IV regression equation can be computed in Stata using the ivregress gmm command. By default it uses formula (13.8). The centered version (13.9) may be selected using the center option."
  },
  {
    "objectID": "chpt13-gmm.html#iterated-gmm",
    "href": "chpt13-gmm.html#iterated-gmm",
    "title": "13  Generalized Method of Moments",
    "section": "13.11 Iterated GMM",
    "text": "13.11 Iterated GMM\nThe asymptotic distribution of the two-step GMM estimator does not depend on the choice of the preliminary one-step estimator. However, the actual value of the estimator depends on this choice and so will the finite sample distribution. This is undesirable and likely inefficient. To remove this dependence we can iterate the estimation sequence. Specifically, given \\(\\widehat{\\beta}_{\\mathrm{gmm}}\\) we can construct an updated weight matrix estimate \\(\\widehat{\\boldsymbol{W}}\\) and then re-estimate \\(\\widehat{\\beta}_{\\mathrm{gmm}}\\). This updating can be iterated until convergence \\({ }^{1}\\). The result is called the iterated GMM estimator and is a common implementation of efficient GMM.\nInterestingly, B. E. Hansen and Lee (2021) show that the iterated GMM estimator is unaffected if the weight matrix is computed with or without centering. Standard errors and test statistics, however, will be affected by the choice.\nThe iterated GMM estimator of the IV regression equation can be computed in Stata using the ivregress gmm command using the igmm option."
  },
  {
    "objectID": "chpt13-gmm.html#covariance-matrix-estimation",
    "href": "chpt13-gmm.html#covariance-matrix-estimation",
    "title": "13  Generalized Method of Moments",
    "section": "13.12 Covariance Matrix Estimation",
    "text": "13.12 Covariance Matrix Estimation\nAn estimator of the asymptotic variance of \\(\\widehat{\\beta}_{\\mathrm{gmm}}\\) can be obtained by replacing the matrices in the asymptotic variance formula by consistent estimators.\n\\({ }^{1}\\) In practice, “convergence” obtains when the difference between the estimates at subsequent steps is smaller than a prespecified tolerance. A sufficient condition for convergence is that the sequence is a contraction mapping. Indeed, B. Hansen and Lee (2021) have shown that the iterated GMM estimator generally satisfies this condition in large samples. For the one-step or two-step GMM estimator the covariance matrix estimator is\n\\[\n\\widehat{\\boldsymbol{V}}_{\\beta}=\\left(\\widehat{\\boldsymbol{Q}}^{\\prime} \\widehat{\\boldsymbol{W}} \\widehat{\\boldsymbol{Q}}\\right)^{-1}\\left(\\widehat{\\boldsymbol{Q}}^{\\prime} \\widehat{\\boldsymbol{W}} \\widehat{\\Omega} \\widehat{\\boldsymbol{W}} \\widehat{\\boldsymbol{Q}}\\right)\\left(\\widehat{\\boldsymbol{Q}}^{\\prime} \\widehat{\\boldsymbol{W}} \\widehat{\\boldsymbol{Q}}\\right)^{-1}\n\\]\nwhere \\(\\widehat{\\boldsymbol{Q}}=\\frac{1}{n} \\sum_{i=1}^{n} Z_{i} X_{i}^{\\prime}\\). The weight matrix is constructed using either the uncentered estimator (13.8) or centered estimator (13.9) with the residuals \\(\\widehat{e}_{i}=Y_{i}-X_{i}^{\\prime} \\widehat{\\beta}_{\\mathrm{gmm}}\\).\nFor the efficient iterated GMM estimator the covariance matrix estimator is\n\\[\n\\widehat{\\boldsymbol{V}}_{\\beta}=\\left(\\widehat{\\boldsymbol{Q}}^{\\prime} \\widehat{\\Omega}^{-1} \\widehat{\\boldsymbol{Q}}\\right)^{-1}=\\left(\\left(\\frac{1}{n} \\boldsymbol{X}^{\\prime} \\boldsymbol{Z}\\right) \\widehat{\\Omega}^{-1}\\left(\\frac{1}{n} \\boldsymbol{Z}^{\\prime} \\boldsymbol{X}\\right)\\right)^{-1} .\n\\]\n\\(\\widehat{\\Omega}\\) can be computed using either the uncentered estimator (13.8) or centered estimator (13.9). Based on the asymptotic approximation the estimator (13.11) can be used as well for the two-step estimator but should use the final residuals \\(\\widehat{e}_{i}=Y_{i}-X_{i}^{\\prime} \\widehat{\\beta}_{\\mathrm{gmm}}\\).\nAsymptotic standard errors are given by the square roots of the diagonal elements of \\(n^{-1} \\widehat{\\boldsymbol{V}}_{\\beta}\\).\nIt is unclear if it is preferred to use the covariance matrix estimator based on the centered or uncentered estimator of \\(\\Omega\\) to construct the covariance matrix estimator. Using the centered estimator results in a smaller covariance matrix and standard errors and thus more “significant” tests based on asymptotic critical values. In contrast the uncentered estimator of \\(\\Omega\\) will result in larger standard errors and will thus be more “conservative”.\nIn Stata, the default covariance matrix estimation method is determined by the choice of weight matrix. Thus if the centered estimator (13.9) is used for the weight matrix it is also used for the covariance matrix estimator."
  },
  {
    "objectID": "chpt13-gmm.html#clustered-dependence",
    "href": "chpt13-gmm.html#clustered-dependence",
    "title": "13  Generalized Method of Moments",
    "section": "13.13 Clustered Dependence",
    "text": "13.13 Clustered Dependence\nIn Section \\(4.21\\) we introduced clustered dependence and in Section \\(12.25\\) described covariance matrix estimation for 2SLS. The methods extend naturally to GMM but with the additional complication of potentially altering weight matrix calculation.\nThe structural equation for the \\(g^{t h}\\) cluster can be written as the matrix system \\(\\boldsymbol{Y}_{g}=\\boldsymbol{X}_{g} \\beta+\\boldsymbol{e}_{g}\\). Using this notation the centered GMM estimator with weight matrix \\(\\boldsymbol{W}\\) can be written as\n\\[\n\\widehat{\\beta}_{\\mathrm{gmm}}-\\beta=\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{Z} \\boldsymbol{W} \\boldsymbol{Z}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\boldsymbol{X}^{\\prime} \\boldsymbol{Z} \\boldsymbol{W}\\left(\\sum_{g=1}^{G} \\boldsymbol{Z}_{g}^{\\prime} \\boldsymbol{e}_{g}\\right)\n\\]\nThe cluster-robust covariance matrix estimator for \\(\\widehat{\\beta}_{\\mathrm{gmm}}\\) is\n\\[\n\\widehat{\\boldsymbol{V}}_{\\beta}=\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{Z} \\boldsymbol{W} \\boldsymbol{Z}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\boldsymbol{X}^{\\prime} \\boldsymbol{Z} \\boldsymbol{W} \\widehat{\\boldsymbol{S}} \\boldsymbol{W} \\boldsymbol{Z}^{\\prime} \\boldsymbol{X}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{Z} \\boldsymbol{W} \\boldsymbol{Z}^{\\prime} \\boldsymbol{X}\\right)^{-1}\n\\]\nwith\n\\[\n\\widehat{\\boldsymbol{S}}=\\sum_{g=1}^{G} \\boldsymbol{Z}_{g}^{\\prime} \\widehat{\\boldsymbol{e}}_{g} \\widehat{\\boldsymbol{e}}_{g}^{\\prime} \\boldsymbol{Z}_{g}\n\\]\nand the clustered residuals\n\\[\n\\widehat{\\boldsymbol{e}}_{g}=\\boldsymbol{Y}_{g}-\\boldsymbol{X}_{g} \\widehat{\\beta}_{\\mathrm{gmm}} .\n\\]\nThe cluster-robust estimator (13.12) is appropriate for the one-step or two-step GMM estimator. It is also appropriate for the iterated estimator when the latter uses a conventional (non-clustered) efficient weight matrix. However in the clustering context it is more natural to use a cluster-robust weight matrix such as \\(\\boldsymbol{W}=\\widehat{\\boldsymbol{S}}^{-1}\\) where \\(\\widehat{\\boldsymbol{S}}\\) is a cluster-robust covariance estimator as in (13.13) based on a one-step or iterated residual. This gives rise to the cluster-robust GMM estimator\n\\[\n\\widehat{\\beta}_{\\mathrm{gmm}}=\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{Z} \\widehat{\\boldsymbol{S}}^{-1} \\boldsymbol{Z}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\boldsymbol{X}^{\\prime} \\boldsymbol{Z} \\widehat{\\boldsymbol{S}}^{-1} \\boldsymbol{Z}^{\\prime} \\boldsymbol{Y}\n\\]\nAn appropriate cluster-robust covariance matrix estimator is\n\\[\n\\widehat{\\boldsymbol{V}}_{\\beta}=\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{Z} \\widehat{\\boldsymbol{S}}^{-1} \\boldsymbol{Z}^{\\prime} \\boldsymbol{X}\\right)^{-1}\n\\]\nwhere \\(\\widehat{S}\\) is calculated using the final residuals.\nTo implement a cluster-robust weight matrix use the 2SLS estimator for first step. Compute the cluster residuals (13.14) and covariance matrix (13.13). Then (13.15) is the two-step GMM estimator. Iterating the residuals and covariance matrix until convergence we obtain the iterated GMM estimator.\nIn Stata, using the ivregress gmm command with the cluster option implements the two-step GMM estimator using the cluster-robust weight matrix and cluster-robust covariance matrix estimator. To use the centered covariance matrix use the center option and to implement the iterated GMM estimator use the igmm option. Alternatively, you can use the wmatrix and vce options to separately specify the weight matrix and covariance matrix estimation methods."
  },
  {
    "objectID": "chpt13-gmm.html#wald-test",
    "href": "chpt13-gmm.html#wald-test",
    "title": "13  Generalized Method of Moments",
    "section": "13.14 Wald Test",
    "text": "13.14 Wald Test\nFor a given function \\(r(\\beta): \\mathbb{R}^{k} \\rightarrow \\Theta \\subset \\mathbb{R}^{q}\\) we define the parameter \\(\\theta=r(\\beta)\\). The GMM estimator of \\(\\theta\\) is \\(\\widehat{\\theta}_{\\mathrm{gmm}}=r\\left(\\widehat{\\beta}_{\\mathrm{gmm}}\\right)\\). By the delta method it is asymptotically normal with covariance matrix \\(\\boldsymbol{V}_{\\theta}=\\) \\(\\boldsymbol{R}^{\\prime} \\boldsymbol{V}_{\\beta} \\boldsymbol{R}\\) where \\(\\boldsymbol{R}=\\frac{\\partial}{\\partial \\beta} r(\\beta)^{\\prime}\\). An estimator of the asymptotic covariance matrix is \\(\\widehat{\\boldsymbol{V}}_{\\theta}=\\widehat{\\boldsymbol{R}}^{\\prime} \\widehat{\\boldsymbol{V}}_{\\beta} \\widehat{\\boldsymbol{R}}^{\\text {where }}\\) \\(\\widehat{\\boldsymbol{R}}=\\frac{\\partial}{\\partial \\beta} r\\left(\\widehat{\\beta}_{\\mathrm{gmm}}\\right)^{\\prime}\\). When \\(\\theta\\) is scalar then an asymptotic standard error for \\(\\widehat{\\theta}_{\\mathrm{gmm}}\\) is formed as \\(\\sqrt{n^{-1} \\widehat{\\boldsymbol{V}}_{\\theta}} .\\)\nA standard test of the hypothesis \\(\\mathbb{M}_{0}: \\theta=\\theta_{0}\\) against \\(\\mathbb{M}_{1}: \\theta \\neq \\theta_{0}\\) is based on the Wald statistic\n\\[\nW=n\\left(\\widehat{\\theta}-\\theta_{0}\\right)^{\\prime} \\widehat{\\boldsymbol{V}}_{\\widehat{\\theta}}^{-1}\\left(\\widehat{\\theta}-\\theta_{0}\\right) .\n\\]\nLet \\(G_{q}(u)\\) denote the \\(\\chi_{q}^{2}\\) distribution function.\nTheorem \\(13.8\\) Under Assumption 12.2, Assumption 7.3, and \\(\\mathbb{H}_{0}\\), as \\(n \\rightarrow \\infty\\), \\(W \\underset{d}{\\longrightarrow} \\chi_{q}^{2}\\). For \\(c\\) satisfying \\(\\alpha=1-G_{q}(c), \\mathbb{P}\\left[W>c \\mid \\mathbb{H}_{0}\\right] \\longrightarrow \\alpha\\) so the test “Reject \\(\\mathbb{H}_{0}\\) if \\(W>c\\)” has asymptotic size \\(\\alpha\\).\nFor a proof see Exercise 13.5.\nIn Stata, the commands test and testparm can be used after ivregress gmm to implement Wald tests of linear hypotheses. The commands nlcom and testnl can be used after ivregress gmm to implement Wald tests of nonlinear hypotheses."
  },
  {
    "objectID": "chpt13-gmm.html#restricted-gmm",
    "href": "chpt13-gmm.html#restricted-gmm",
    "title": "13  Generalized Method of Moments",
    "section": "13.15 Restricted GMM",
    "text": "13.15 Restricted GMM\nIt is often desirable to impose restrictions on the coefficients. In this section we consider estimation subject to the linear constraints \\(\\boldsymbol{R}^{\\prime} \\beta=\\boldsymbol{c}\\). In the following section we consider nonlinear constraints.\nThe constrained GMM estimator minimizes the GMM criterion subject to the constraint. It is\n\\[\n\\widehat{\\beta}_{\\mathrm{cgmm}}=\\underset{\\boldsymbol{R}^{\\prime} \\beta=\\boldsymbol{c}}{\\operatorname{argmin}} J(\\beta) .\n\\]\nThis is the parameter vector which makes the estimating equations as close to zero as possible with respect to the weighted quadratic distance while imposing the restriction on the parameters.\nSuppose the weight matrix \\(\\boldsymbol{W}\\) is fixed. Using the methods of Chapter 8 it is straightforward to derive that the constrained GMM estimator is\n\\[\n\\widehat{\\beta}_{\\mathrm{cgmm}}=\\widehat{\\beta}_{\\mathrm{gmm}}-\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{Z} \\boldsymbol{W} \\boldsymbol{Z}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\boldsymbol{R}\\left(\\boldsymbol{R}^{\\prime}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{Z} \\boldsymbol{W} \\boldsymbol{Z}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\boldsymbol{R}\\right)^{-1}\\left(\\boldsymbol{R}^{\\prime} \\widehat{\\beta}_{\\mathrm{gmm}}-\\boldsymbol{c}\\right) .\n\\]\n(For details, see Exercise 13.6.)\nWe derive the asymptotic distribution under the assumption that the restriction is true. Make the substitution \\(\\boldsymbol{c}=\\boldsymbol{R}^{\\prime} \\beta\\) in (13.16) and reorganize to find\n\\[\n\\sqrt{n}\\left(\\widehat{\\beta}_{\\mathrm{cgmm}}-\\beta\\right)=\\left(\\boldsymbol{I}_{k}-\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{Z} \\boldsymbol{W} \\boldsymbol{Z}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\boldsymbol{R}\\left(\\boldsymbol{R}^{\\prime}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{Z} \\boldsymbol{W} \\boldsymbol{Z}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\boldsymbol{R}\\right)^{-1} \\boldsymbol{R}^{\\prime}\\right) \\sqrt{n}\\left(\\widehat{\\beta}_{\\mathrm{gmm}}-\\beta\\right)\n\\]\nThis is a linear function of \\(\\sqrt{n}\\left(\\widehat{\\beta}_{\\mathrm{gmm}}-\\beta\\right)\\). Since the asymptotic distribution of the latter is known the asymptotic distribution of \\(\\sqrt{n}\\left(\\widehat{\\beta}_{\\mathrm{cgmm}}-\\beta\\right)\\) is a linear function of the former.\n\\[\n\\begin{aligned}\n&\\text { Theorem 13.9 Under Assumptions } 12.2 \\text { and 8.3, for the constrained GMM es- } \\\\\n&\\text { timator (13.16), } \\sqrt{n}\\left(\\widehat{\\beta}_{\\mathrm{cgmm}}-\\beta\\right) \\underset{d}{\\longrightarrow} \\mathrm{N}\\left(0, \\boldsymbol{V}_{\\mathrm{cgmm}}\\right) \\text { as } n \\rightarrow \\infty \\text {, where } \\\\\n&\\boldsymbol{V}_{\\mathrm{cgmm}}=\\boldsymbol{V}_{\\beta}-\\left(\\boldsymbol{Q}^{\\prime} \\boldsymbol{W} \\boldsymbol{Q}\\right)^{-1} \\boldsymbol{R}\\left(\\boldsymbol{R}^{\\prime}\\left(\\boldsymbol{Q}^{\\prime} \\boldsymbol{W} \\boldsymbol{Q}\\right)^{-1} \\boldsymbol{R}\\right)^{-1} \\boldsymbol{R}^{\\prime} \\boldsymbol{V}_{\\beta} \\\\\n&\\quad-\\boldsymbol{V}_{\\beta} \\boldsymbol{R}\\left(\\boldsymbol{R}^{\\prime}\\left(\\boldsymbol{Q}^{\\prime} \\boldsymbol{W} \\boldsymbol{Q}\\right)^{-1} \\boldsymbol{R}\\right)^{-1} \\boldsymbol{R}^{\\prime}\\left(\\boldsymbol{Q}^{\\prime} \\boldsymbol{W} \\boldsymbol{Q}\\right)^{-1} \\\\\n& \\\\\n&+\\left(\\boldsymbol{Q}^{\\prime} \\boldsymbol{W} \\boldsymbol{Q}\\right)^{-1} \\boldsymbol{R}\\left(\\boldsymbol{R}^{\\prime}\\left(\\boldsymbol{Q}^{\\prime} \\boldsymbol{W} \\boldsymbol{Q}\\right)^{-1} \\boldsymbol{R}\\right)^{-1} \\boldsymbol{R}^{\\prime} \\boldsymbol{V}_{\\beta} \\boldsymbol{R}\\left(\\boldsymbol{R}^{\\prime}\\left(\\boldsymbol{Q}^{\\prime} \\boldsymbol{W} \\boldsymbol{Q}\\right)^{-1} \\boldsymbol{R}\\right)^{-1} \\boldsymbol{R}^{\\prime}\\left(\\boldsymbol{Q}^{\\prime} \\boldsymbol{W} \\boldsymbol{Q}\\right)^{-1}\n\\end{aligned}\n\\]\nFor a proof, see Exercise 13.8. Unfortunately the asymptotic covariance matrix formula (13.18) is quite tedious!\nNow suppose that the the weight matrix is set as \\(W=\\widehat{\\Omega}^{-1}\\), the efficient weight matrix from unconstrained estimation. In this case the constrained GMM estimator can be written as\n\\[\n\\widehat{\\beta}_{\\mathrm{cgmm}}=\\widehat{\\beta}_{\\mathrm{gmm}}-\\widehat{\\boldsymbol{V}}_{\\beta} \\boldsymbol{R}\\left(\\boldsymbol{R}^{\\prime} \\widehat{\\boldsymbol{V}}_{\\boldsymbol{\\beta}} \\boldsymbol{R}\\right)^{-1}\\left(\\boldsymbol{R}^{\\prime} \\widehat{\\beta}_{\\mathrm{gmm}}-\\boldsymbol{c}\\right)\n\\]\nwhich is the same formula (8.25) as efficient minimum distance. (For details, see Exercise 13.7.) We find that the asymptotic covariance matrix simplifies considerably. Theorem 13.10 Under Assumptions \\(12.2\\) and 8.3, for the efficient constrained GMM estimator (13.19), \\(\\sqrt{n}\\left(\\widehat{\\beta}_{\\mathrm{cgmm}}-\\beta\\right) \\underset{d}{\\longrightarrow} \\mathrm{N}\\left(0, \\boldsymbol{V}_{\\mathrm{cgmm}}\\right)\\) as \\(n \\rightarrow \\infty\\), where\n\\[\n\\boldsymbol{V}_{\\mathrm{cgmm}}=\\boldsymbol{V}_{\\beta}-\\boldsymbol{V}_{\\beta} \\boldsymbol{R}\\left(\\boldsymbol{R}^{\\prime} \\boldsymbol{V}_{\\beta} \\boldsymbol{R}\\right)^{-1} \\boldsymbol{R}^{\\prime} \\boldsymbol{V}_{\\beta} .\n\\]\nFor a proof, see Exercise 13.9.\nThe asymptotic covariance matrix (13.20) can be estimated by\n\\[\n\\begin{aligned}\n\\widehat{\\boldsymbol{V}}_{\\mathrm{cgmm}} &=\\widetilde{\\boldsymbol{V}}_{\\beta}-\\widetilde{\\boldsymbol{V}}_{\\beta} \\boldsymbol{R}\\left(\\boldsymbol{R}^{\\prime} \\widetilde{\\boldsymbol{V}}_{\\beta} \\boldsymbol{R}\\right)^{-1} \\boldsymbol{R}^{\\prime} \\widetilde{\\boldsymbol{V}}_{\\beta} . \\\\\n\\widetilde{\\boldsymbol{V}}_{\\beta} &=\\left(\\widehat{\\boldsymbol{Q}}^{\\prime} \\widetilde{\\Omega}^{-1} \\widehat{\\boldsymbol{Q}}\\right)^{-1} \\\\\n\\widetilde{\\Omega} &=\\frac{1}{n} \\sum_{i=1}^{n} Z_{i} Z_{i}^{\\prime} \\widetilde{e}_{i}^{2} \\\\\n\\widetilde{e}_{i} &=Y_{i}-X_{i}^{\\prime} \\widehat{\\beta}_{\\mathrm{cgmm}} .\n\\end{aligned}\n\\]\nThe covariance matrix (13.18) can be estimated similarly, though using (13.10) to estimate \\(\\boldsymbol{V}_{\\beta}\\). The covariance matrix estimator \\(\\widetilde{\\Omega}\\) can also be replaced with a centered version.\nA constrained iterated GMM estimator can be implemented by setting \\(\\boldsymbol{W}=\\widetilde{\\Omega}^{-1}\\) where \\(\\widetilde{\\Omega}\\) is defined in (13.22) and then iterating until convergence. This is a natural estimator as it is the appropriate implementation of iterated GMM.\nSince both \\(\\widehat{\\Omega}\\) and \\(\\widetilde{\\Omega}\\) converge to the same limit \\(\\Omega\\) under the assumption that the constraint is true the constrained iterated GMM estimator has the asymptotic distribution given in Theorem 13.10."
  },
  {
    "objectID": "chpt13-gmm.html#nonlinear-restricted-gmm",
    "href": "chpt13-gmm.html#nonlinear-restricted-gmm",
    "title": "13  Generalized Method of Moments",
    "section": "13.16 Nonlinear Restricted GMM",
    "text": "13.16 Nonlinear Restricted GMM\nNonlinear constraints on the parameters can be written as \\(r(\\beta)=0\\) for some function \\(r: \\mathbb{R}^{k} \\rightarrow \\mathbb{R}^{q}\\). The constraint is nonlinear if \\(r(\\beta)\\) cannot be written as a linear function of \\(\\beta\\). Least squares estimation subject to nonlinear constraints was explored in Section 8.14. In this section we introduce GMM estimation subject to nonlinear constraints.\nThe constrained GMM estimator minimizes the GMM criterion subject to the constraint. It is\n\\[\n\\widehat{\\beta}_{\\mathrm{cgmm}}=\\underset{r(\\beta)=0}{\\operatorname{argmin}} J(\\beta) .\n\\]\nThis is the parameter vector which makes the estimating equations as close to zero as possible with respect to the weighted quadratic distance while imposing the restriction on the parameters.\nIn general there is no explicit solution for \\(\\widehat{\\beta}_{\\mathrm{cgmm}}\\). Instead the solution is found numerically. Fortunately there are excellent nonlinear constrained optimization solvers implemented in standard software packages.\nFor the asymptotic distribution assume that the restriction \\(r(\\beta)=0\\) is true. Using the same methods as in the proof of Theorem \\(8.10\\) we can show that (13.17) approximately holds in the sense that\n\\[\n\\sqrt{n}\\left(\\widehat{\\beta}_{\\mathrm{cgmm}}-\\beta\\right)=\\left(\\boldsymbol{I}_{k}-\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{Z} \\boldsymbol{W} \\boldsymbol{Z}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\boldsymbol{R}\\left(\\boldsymbol{R}^{\\prime}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{Z} \\boldsymbol{W} \\boldsymbol{Z}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\boldsymbol{R}\\right)^{-1} \\boldsymbol{R}^{\\prime}\\right) \\sqrt{n}\\left(\\widehat{\\beta}_{\\mathrm{gmm}}-\\beta\\right)+o_{p}(1)\n\\]\nwhere \\(\\boldsymbol{R}=\\frac{\\partial}{\\partial \\beta} r(\\beta)^{\\prime}\\). Thus the asymptotic distribution of the constrained estimator takes the same form as in the linear case. Theorem \\(13.11\\) Under Assumptions \\(12.2\\) and 8.3, for the constrained GMM estimator (13.23), \\(\\sqrt{n}\\left(\\widehat{\\beta}_{\\mathrm{cgmm}}-\\beta\\right) \\underset{d}{\\longrightarrow} \\mathrm{N}\\left(0, V_{\\mathrm{cgmm}}\\right)\\) as \\(n \\rightarrow \\infty\\), where \\(\\boldsymbol{V}_{\\mathrm{cgmm}}\\) equals (13.18). If \\(W=\\widehat{\\Omega}^{-1}\\), then \\(V_{\\text {cgmm }}\\) equals (13.20).\nThe asymptotic covariance matrix in the efficient case is estimated by (13.21) with \\(\\boldsymbol{R}\\) replaced with \\(\\widehat{\\boldsymbol{R}}=\\frac{\\partial}{\\partial \\beta} r\\left(\\widehat{\\beta}_{\\text {cgmm }}\\right)^{\\prime}\\). The asymptotic covariance matrix (13.18) in the general case is estimated similarly.\nTo implement an iterated restricted GMM estimator the weight matrix may be set as \\(\\boldsymbol{W}=\\widetilde{\\Omega}^{-1}\\) where \\(\\widetilde{\\Omega}\\) is defined in (13.22), and then iterated until convergence."
  },
  {
    "objectID": "chpt13-gmm.html#constrained-regression",
    "href": "chpt13-gmm.html#constrained-regression",
    "title": "13  Generalized Method of Moments",
    "section": "13.17 Constrained Regression",
    "text": "13.17 Constrained Regression\nTake the conventional projection model \\(Y=X^{\\prime} \\beta+e\\) with \\(\\mathbb{E}[X e]=0\\). This is a special case of GMM as it is model (13.5) with \\(Z=X\\). The just-identified GMM estimator equals least squares \\(\\widehat{\\beta}_{\\mathrm{gmm}}=\\widehat{\\beta}_{\\mathrm{ols}}\\).\nIn Chapter 8 we discussed estimation of the projection model subject to linear constraints \\(\\boldsymbol{R}^{\\prime} \\beta=\\boldsymbol{c}\\), which includes exclusion restrictions. Since the projection model is a special case of GMM the constrained projection model is also constrained GMM. From the results of Section \\(13.15\\) we find that the efficient constrained GMM estimator is\n\\[\n\\widehat{\\beta}_{\\mathrm{cgmm}}=\\widehat{\\beta}_{\\mathrm{ols}}-\\widehat{\\boldsymbol{V}}_{\\beta} \\boldsymbol{R}\\left(\\boldsymbol{R}^{\\prime} \\widehat{\\boldsymbol{V}}_{\\beta} \\boldsymbol{R}\\right)^{-1}\\left(\\boldsymbol{R}^{\\prime} \\widehat{\\beta}_{\\mathrm{ols}}-\\boldsymbol{c}\\right)=\\widehat{\\beta}_{\\mathrm{emd}},\n\\]\nthe efficient minimum distance estimator. Thus for linear constraints on the linear projection model efficient GMM equals efficient minimum distance. Thus one convenient method to implement efficient minimum distance is GMM."
  },
  {
    "objectID": "chpt13-gmm.html#multivariate-regression",
    "href": "chpt13-gmm.html#multivariate-regression",
    "title": "13  Generalized Method of Moments",
    "section": "13.18 Multivariate Regression",
    "text": "13.18 Multivariate Regression\nGMM methods can simplify estimation and inference for multivariate regressions such as those introduced in Chapter \\(11 .\\)\nThe general multivariate regression (projection) model is\n\\[\n\\begin{aligned}\nY_{j} &=X_{j}^{\\prime} \\beta_{j}+e_{j} \\\\\n\\mathbb{E}\\left[X_{j} e_{j}\\right] &=0\n\\end{aligned}\n\\]\nfor \\(j=1, \\ldots, m\\). Using the notation from Section \\(11.2\\) the equations can be written jointly as \\(Y=\\bar{X} \\beta+e\\) and for the full sample as \\(\\boldsymbol{Y}=\\overline{\\boldsymbol{X}} \\beta+\\boldsymbol{e}\\). The \\(\\bar{k}\\) moment conditions are\n\\[\n\\mathbb{E}\\left[\\bar{X}^{\\prime}(Y-\\bar{X} \\beta)\\right]=0 .\n\\]\nGiven a \\(\\bar{k} \\times \\bar{k}\\) weight matrix \\(\\boldsymbol{W}\\) the GMM criterion is\n\\[\nJ(\\beta)=n(\\boldsymbol{Y}-\\overline{\\boldsymbol{X}} \\beta)^{\\prime} \\overline{\\boldsymbol{X}} \\boldsymbol{W} \\overline{\\boldsymbol{X}}^{\\prime}(\\boldsymbol{Y}-\\overline{\\boldsymbol{X}} \\beta) .\n\\]\nThe GMM estimator \\(\\widehat{\\beta}_{\\text {gmm }}\\) minimizes \\(J(\\beta)\\). Since this is a just-identified model the estimator solves the sample equations\n\\[\n\\overline{\\boldsymbol{X}}^{\\prime}\\left(\\boldsymbol{Y}-\\overline{\\boldsymbol{X}} \\widehat{\\beta}_{\\mathrm{gmm}}\\right)=0\n\\]\nThe solution is\n\\[\n\\widehat{\\beta}_{\\mathrm{gmm}}=\\left(\\sum_{i=1}^{n} \\bar{X}_{i}^{\\prime} \\bar{X}_{i}\\right)^{-1}\\left(\\sum_{i=1}^{n} \\bar{X}_{i}^{\\prime} Y_{i}\\right)=\\left(\\overline{\\boldsymbol{X}}^{\\prime} \\overline{\\boldsymbol{X}}\\right)^{-1}\\left(\\overline{\\boldsymbol{X}}^{\\prime} \\boldsymbol{Y}\\right)=\\widehat{\\beta}_{\\mathrm{ols}}\n\\]\nthe multivariate least squares estimator.\nThus the unconstrained GMM estimator of the multivariate regression model is least squares. The estimator does not depend on the weight matrix since the model is just-identified.\nA important advantage of the GMM framework is the ability to incorporate cross-equation constraints. Consider the class of restrictions \\(\\boldsymbol{R}^{\\prime} \\beta=\\boldsymbol{c}\\). Minimization of the GMM criterion subject to this restriction has solutions as described in (13.15). The restricted GMM estimator is\n\\[\n\\widehat{\\beta}_{\\mathrm{gmm}}=\\widehat{\\beta}_{\\mathrm{ols}}-\\left(\\overline{\\boldsymbol{X}}^{\\prime} \\overline{\\boldsymbol{X}} \\boldsymbol{W} \\overline{\\boldsymbol{X}}^{\\prime} \\overline{\\boldsymbol{X}}\\right)^{-1} \\boldsymbol{R}\\left(\\boldsymbol{R}^{\\prime}\\left(\\overline{\\boldsymbol{X}}^{\\prime} \\overline{\\boldsymbol{X}} \\boldsymbol{W} \\overline{\\boldsymbol{X}}^{\\prime} \\overline{\\boldsymbol{X}}\\right)^{-1} \\boldsymbol{R}\\right)^{-1}\\left(\\boldsymbol{R}^{\\prime} \\widehat{\\beta}_{\\mathrm{ols}}-\\boldsymbol{c}\\right)\n\\]\nThis estimator depends on the weight matrix because it is over-identified.\nA simple choice for weight matrix is \\(\\boldsymbol{W}=\\overline{\\boldsymbol{X}}^{\\prime} \\overline{\\boldsymbol{X}}\\). This leads to the one-step estimator\n\\[\n\\widehat{\\beta}_{1}=\\widehat{\\beta}_{\\mathrm{ols}}-\\left(\\overline{\\boldsymbol{X}}^{\\prime} \\overline{\\boldsymbol{X}}\\right)^{-1} \\boldsymbol{R}\\left(\\boldsymbol{R}^{\\prime}\\left(\\overline{\\boldsymbol{X}}^{\\prime} \\overline{\\boldsymbol{X}}\\right)^{-1} \\boldsymbol{R}\\right)^{-1}\\left(\\boldsymbol{R}^{\\prime} \\widehat{\\beta}_{\\mathrm{ols}}-\\boldsymbol{c}\\right) .\n\\]\nThe asymptotically efficient choice sets \\(\\boldsymbol{W}=\\widehat{\\Omega}^{-1}\\) where \\(\\widehat{\\Omega}=n^{-1} \\sum_{i=1}^{n} \\bar{X}_{i}^{\\prime} \\widehat{e}_{i} \\widehat{e}_{i}^{\\prime} \\bar{X}_{i}\\) and \\(\\widehat{e}_{i}=Y_{i}-\\bar{X}_{i} \\widehat{\\beta}_{1}\\). This leads to the two-step estimator\n\\[\n\\widehat{\\beta}_{2}=\\widehat{\\beta}_{\\text {ols }}-\\left(\\overline{\\boldsymbol{X}}^{\\prime} \\overline{\\boldsymbol{X}} \\widehat{\\Omega}^{-1} \\overline{\\boldsymbol{X}}^{\\prime} \\overline{\\boldsymbol{X}}\\right)^{-1} \\boldsymbol{R}\\left(\\boldsymbol{R}^{\\prime}\\left(\\overline{\\boldsymbol{X}}^{\\prime} \\overline{\\boldsymbol{X}} \\widehat{\\Omega}^{-1} \\overline{\\boldsymbol{X}}^{\\prime} \\overline{\\boldsymbol{X}}\\right)^{-1} \\boldsymbol{R}\\right)^{-1}\\left(\\boldsymbol{R}^{\\prime} \\widehat{\\beta}_{\\mathrm{ols}}-\\boldsymbol{c}\\right)\n\\]\nWhen the regressors \\(X\\) are common across all equations the multivariate regression model can be written conveniently as in (11.3): \\(Y=\\boldsymbol{B}^{\\prime} X+e\\) with \\(\\mathbb{E}\\left[X e^{\\prime}\\right]=0\\). The moment restrictions can be written as the matrix system \\(\\mathbb{E}\\left[X\\left(Y^{\\prime}-X^{\\prime} \\boldsymbol{B}\\right)\\right]=0\\). Written as a vector system this is (13.24) and leads to the same restricted GMM estimators.\nThese are general formula for imposing restrictions. In specific cases (such as an exclusion restriction) direct methods may be more convenient. In all cases the solution is found by minimization of the GMM criterion \\(J(\\beta)\\) subject to the restriction."
  },
  {
    "objectID": "chpt13-gmm.html#distance-test",
    "href": "chpt13-gmm.html#distance-test",
    "title": "13  Generalized Method of Moments",
    "section": "13.19 Distance Test",
    "text": "13.19 Distance Test\nIn Section \\(13.14\\) we introduced Wald tests of the hypothesis \\(\\mathbb{M}_{0}: \\theta=\\theta_{0}\\) where \\(\\theta=r(\\beta)\\) for a given function \\(r(\\beta): \\mathbb{R}^{k} \\rightarrow \\Theta \\subset \\mathbb{R}^{q}\\). When \\(r(\\beta)\\) is nonlinear an alternative is to use a criterion-based statistic. This is sometimes called the GMM Distance statistic and sometimes called a LR-like statistic (the LR is for likelihood-ratio). The idea was first put forward by Newey and West (1987a).\nThe idea is to compare the unrestricted and restricted estimators by contrasting the criterion functions. The unrestricted estimator takes the form\n\\[\n\\widehat{\\beta}_{\\mathrm{gmm}}=\\underset{\\beta}{\\operatorname{argmin}} \\widehat{J}(\\beta)\n\\]\nwhere\n\\[\n\\widehat{J}(\\beta)=n \\bar{g}_{n}(\\beta)^{\\prime} \\widehat{\\Omega}^{-1} \\bar{g}_{n}(\\beta)\n\\]\nis the unrestricted GMM criterion with an efficient weight matrix estimate \\(\\widehat{\\Omega}\\). The minimized value of the criterion is \\(\\widehat{J}=\\widehat{J}\\left(\\widehat{\\beta}_{\\mathrm{gmm}}\\right)\\). As in Section 13.15, the estimator subject to \\(r(\\beta)=\\theta_{0}\\) is\n\\[\n\\widehat{\\beta}_{\\mathrm{cgmm}}=\\underset{r(\\beta)=\\theta_{0}}{\\operatorname{argmin}} \\widetilde{J}(\\beta)\n\\]\nwhere\n\\[\n\\widetilde{J}(\\beta)=n \\bar{g}_{n}(\\beta)^{\\prime} \\widetilde{\\Omega}^{-1} \\bar{g}_{n}(\\beta)\n\\]\nwhich depends on an efficient weight matrix estimator, either \\(\\widehat{\\Omega}\\) (the same as the unrestricted estimator) or \\(\\widetilde{\\Omega}\\) (the iterated weight matrix from constrained estimation). The minimized value of the criterion is \\(\\widetilde{J}=\\widetilde{J}\\left(\\widehat{\\beta}_{\\operatorname{cgmm}}\\right)\\)\nThe GMM distance (or LR-like) statistic is the difference in the criterion functions: \\(D=\\widetilde{J}-\\widehat{J}\\). The distance test shares the useful feature of LR tests in that it is a natural by-product of the computation of alternative models.\nThe test has the following large sample distribution.\nTheorem \\(13.12\\) Under Assumption 12.2, Assumption 7.3, and \\(\\mathbb{H}_{0}\\), then as \\(n \\rightarrow\\) \\(\\infty, D \\longrightarrow \\chi_{q}^{2}\\). For \\(c\\) satisfying \\(\\alpha=1-G_{q}(c), \\mathbb{P}\\left[D>c \\mid \\mathbb{H}_{0}\\right] \\longrightarrow \\alpha\\). The test “Reject \\(\\mathbb{H}_{0}\\) if \\(D>c\\)” has asymptotic size \\(\\alpha\\).\nThe proof is given in Section 13.28.\nTheorem \\(13.12\\) shows that the distance statistic has the same asymptotic distribution as Wald and likelihood ratio statistics and can be interpreted similarly. Small values of \\(D\\) mean that imposing the restriction does not result in a large value of the moment equations. Hence the restriction appears to be compatible with the data. On the other hand, large values of \\(D\\) mean that imposing the restriction results in a much larger value of the moment equations, implying that the restriction is not compatible with the data. The finding that the asymptotic distribution is chi-squared allows the calculation of asymptotic critical values and p-values.\nWe now discuss the choice of weight matrix. As mentioned above one simple choice is to set \\(\\widetilde{\\Omega}=\\widehat{\\Omega}\\). In this case we have the following result.\nTheorem 13.13 If \\(\\widetilde{\\Omega}=\\widehat{\\Omega}\\) then \\(D \\geq 0\\). Furthermore, if \\(r\\) is linear in \\(\\beta\\) then \\(D\\) equals the Wald statistic.\nThe statement that \\(\\widetilde{\\Omega}=\\widehat{\\Omega}\\) implies \\(D \\geq 0\\) follows from the fact that in this case the criterion functions \\(\\widehat{J}(\\beta)=\\widetilde{J}(\\beta)\\) are identical so the constrained minimum cannot be smaller than the unconstrained. The statement that linear hypotheses and \\(\\widetilde{\\Omega}=\\widehat{\\Omega}\\) implies \\(D=W\\) follows from applying the expression for the constrained GMM estimator (13.19) and using the covariance matrix formula (13.11).\nThe fact that \\(D \\geq 0\\) when \\(\\widetilde{\\Omega}=\\widehat{\\Omega}\\) motivated Newey and West (1987a) to recommend this choice. However, \\(\\widetilde{\\Omega}=\\widehat{\\Omega}\\) is not necessary. Instead, setting \\(\\widetilde{\\Omega}\\) to equal the constrained efficient weight matrix is natural for efficient estimation of \\(\\widehat{\\beta}_{\\mathrm{cgmm}}\\). In the event that \\(D<0\\) the test simply fails to reject \\(\\mathbb{H}_{0}\\) at any significance level.\nAs discussed in Section \\(9.17\\) for tests of nonlinear hypotheses the Wald statistic can work quite poorly. In particular, the Wald statistic is affected by how the hypothesis \\(r(\\beta)\\) is formulated. In contrast, the distance statistic \\(D\\) is not affected by the algebraic formulation of the hypothesis. Current evidence suggests that the \\(D\\) statistic appears to have good sampling properties, and is a preferred test statistic relative to the Wald statistic for nonlinear hypotheses. (See B. E. Hansen (2006).)\nIn Stata the command estat overid after ivregress gmm can be used to report the value of the GMM criterion \\(J\\). By estimating the two nested GMM regressions the values \\(\\widehat{J}\\) and \\(\\widetilde{J}\\) can be obtained and \\(D\\) computed."
  },
  {
    "objectID": "chpt13-gmm.html#continuously-updated-gmm",
    "href": "chpt13-gmm.html#continuously-updated-gmm",
    "title": "13  Generalized Method of Moments",
    "section": "13.20 Continuously-Updated GMM",
    "text": "13.20 Continuously-Updated GMM\nAn alternative to the two-step GMM estimator can be constructed by letting the weight matrix be an explicit function of \\(\\beta\\). These leads to the criterion function\n\\[\nJ(\\beta)=n \\bar{g}_{n}(\\beta)^{\\prime}\\left(\\frac{1}{n} \\sum_{i=1}^{n} g_{i}(\\beta) g_{i}(\\beta)^{\\prime}\\right)^{-1} \\bar{g}_{n}(\\beta) .\n\\]\nThe \\(\\widehat{\\beta}\\) which minimizes this function is called the continuously-updated GMM (CU-GMM) estimator and was introduced by L. Hansen, Heaton and Yaron (1996).\nA complication is that the continuously-updated criterion \\(J(\\beta)\\) is not quadratic in \\(\\beta\\). This means that minimization requires numerical methods. It may appear that the CU-GMM estimator is the same as the iterated GMM estimator but this is not the case at all. They solve distinct first-order conditions and can be quite different in applications.\nRelative to traditional GMM the CU-GMM estimator has lower bias but thicker distributional tails. While it has received considerable theoretical attention it is not used commonly in applications."
  },
  {
    "objectID": "chpt13-gmm.html#overidentification-test",
    "href": "chpt13-gmm.html#overidentification-test",
    "title": "13  Generalized Method of Moments",
    "section": "13.21 OverIdentification Test",
    "text": "13.21 OverIdentification Test\nIn Section \\(12.31\\) we introduced the Sargan (1958) overidentification test for the 2SLS estimator under the assumption of homoskedasticity. L. Hansen (1982) generalized the test to cover the GMM estimator allowing for general heteroskedasticity.\nRecall, overidentified models \\((\\ell>k)\\) are special in the sense that there may not be a parameter value \\(\\beta\\) such that the moment condition \\(\\mathbb{H}_{0}: \\mathbb{E}[Z e]=0\\) holds. Thus the model-the overidentifying restrictions - are testable.\nFor example, take the linear model \\(Y=\\beta_{1}^{\\prime} X_{1}+\\beta_{2}^{\\prime} X_{2}+e\\) with \\(\\mathbb{E}\\left[X_{1} e\\right]=0\\) and \\(\\mathbb{E}\\left[X_{2} e\\right]=0\\). It is possible that \\(\\beta_{2}=0\\) so that the linear equation may be written as \\(Y=\\beta_{1}^{\\prime} X_{1}+e\\). However, it is possible that \\(\\beta_{2} \\neq 0\\). In this case it is impossible to find a value of \\(\\beta_{1}\\) such that both \\(\\mathbb{E}\\left[X_{1}\\left(Y-X_{1}^{\\prime} \\beta_{1}\\right)\\right]=0\\) and \\(\\mathbb{E}\\left[X_{2}\\left(Y-X_{1}^{\\prime} \\beta_{1}\\right)\\right]=0\\) hold simultaneously. In this sense an exclusion restriction can be seen as an overidentifying restriction.\nNote that \\(\\bar{g}_{n} \\underset{p}{\\mathbb{E}}[Z e]\\) and thus \\(\\bar{g}_{n}\\) can be used to assess the hypothesis \\(\\mathbb{E}[Z e]=0\\). Assuming that an efficient weight matrix estimator is used the criterion function at the parameter estimator is \\(J=\\) \\(J\\left(\\widehat{\\beta}_{\\mathrm{gmm}}\\right)=n \\bar{g}_{n}^{\\prime} \\widehat{\\Omega}^{-1} \\bar{g}_{n}\\). This is a quadratic form in \\(\\bar{g}_{n}\\) and is thus a natural test statistic for \\(\\mathbb{H}_{0}: \\mathbb{E}[Z e]=0\\). Note that we assume that the criterion function is constructed with an efficient weight matrix estimator. This is important for the distribution theory.\nTheorem 13.14 Under Assumption \\(12.2\\) then as \\(n \\rightarrow \\infty, J=J\\left(\\widehat{\\beta}_{\\mathrm{gmm}}\\right) \\underset{d}{\\rightarrow} \\chi_{\\ell-k}^{2} \\cdot\\) For \\(c\\) satisfying \\(\\alpha=1-G_{\\ell-k}(c), \\mathbb{P}\\left[J>c \\mid \\mathbb{M}_{0}\\right] \\longrightarrow \\alpha\\) so the test “Reject \\(\\mathbb{H}_{0}\\) if \\(J>c\\)” has asymptotic size \\(\\alpha\\). The proof of the theorem is left to Exercise 13.13.\nThe degrees of freedom of the asymptotic distribution are the number of overidentifying restrictions. If the statistic \\(J\\) exceeds the chi-square critical value we can reject the model. Based on this information alone it is unclear what is wrong but it is typically cause for concern. The GMM overidentification test is a useful by-product of the GMM methodology and it is advisable to report the statistic \\(J\\) whenever GMM is the estimation method. When over-identified models are estimated by GMM it is customary to report the \\(J\\) statistic as a general test of model adequacy.\nIn Stata the command estat overid afer ivregress gmm can be used to implement the overidentification test. The GMM criterion \\(J\\) and its asymptotic \\(\\mathrm{p}\\)-value using the \\(\\chi_{\\ell-k}^{2}\\) distribution are reported."
  },
  {
    "objectID": "chpt13-gmm.html#subset-overidentification-tests",
    "href": "chpt13-gmm.html#subset-overidentification-tests",
    "title": "13  Generalized Method of Moments",
    "section": "13.22 Subset OverIdentification Tests",
    "text": "13.22 Subset OverIdentification Tests\nIn Section \\(12.32\\) we introduced subset overidentification tests for the 2SLS estimator under the assumption of homoskedasticity. In this section we describe how to construct analogous tests for the GMM estimator under general heteroskedasticity.\nRecall, subset overidentification tests are used when it is desired to focus attention on a subset of instruments whose validity is questioned. Partition \\(Z=\\left(Z_{a}, Z_{b}\\right)\\) with dimensions \\(\\ell_{a}\\) and \\(\\ell_{b}\\), respectively, where \\(Z_{a}\\) contains the instruments which are believed to be uncorrelated with \\(e\\) and \\(Z_{b}\\) contains the instruments which may be correlated with \\(e\\). It is necessary to select this partition so that \\(\\ell_{a}>k\\), so that the instruments \\(Z_{a}\\) alone identify the parameters.\nGiven this partition the maintained hypothesis is \\(\\mathbb{E}\\left[Z_{a} e\\right]=0\\). The null and alternative hypotheses are \\(\\mathbb{H}_{0}: \\mathbb{E}\\left[Z_{b} e\\right]=0\\) and \\(\\mathbb{M}_{1}: \\mathbb{E}\\left[Z_{b} e\\right] \\neq 0\\). The GMM test is constructed as follows. First, estimate the model by efficient GMM with only the smaller set \\(Z_{a}\\) of instruments. Let \\(\\widetilde{J}\\) denote the resulting GMM criterion. Second, estimate the model by efficient GMM with the full set \\(Z=\\left(Z_{a}, Z_{b}\\right)\\) of instruments. Let \\(\\widehat{J}\\) denote the resulting GMM criterion. The test statistic is the difference in the criterion functions: \\(C=\\widehat{J}-\\widetilde{J}\\). This is similar to the GMM distance statistic presented in Section 13.19. The difference is that the distance statistic compares models which differ based on the parameter restrictions while the \\(C\\) statistic compares models based on different instrument sets.\nTypically \\(C \\geq 0\\). However, this is not necessary and \\(C<0\\) can arise. If this occurs it leads to a nonrejection of \\(\\mathbb{H}_{0}\\).\nIf the smaller instrument set \\(Z_{a}\\) is just-identified so that \\(\\ell_{a}=k\\) then \\(\\widetilde{J}=0\\) so \\(C=\\widehat{J}\\) is simply the standard overidentification test. This is why we have restricted attention to the case \\(\\ell_{a}>k\\).\nThe test has the following large sample distribution.\nTheorem 13.15 Under Assumption \\(12.2\\) and \\(\\mathbb{E}\\left[Z_{a} X^{\\prime}\\right]\\) has full rank \\(k\\), then as \\(n \\rightarrow \\infty, C \\rightarrow \\underset{d}{\\rightarrow} \\chi_{\\ell_{b}}^{2} .\\) For \\(c\\) satisfying \\(\\alpha=1-G_{\\ell_{b}}(c), \\mathbb{P}\\left[C>c \\mid \\mathbb{H}_{0}\\right] \\longrightarrow \\alpha .\\) The test “Reject \\(\\mathbb{H}_{0}\\) if \\(C>c\\)” has asymptotic size \\(\\alpha\\).\nThe proof of Theorem \\(13.15\\) is presented in Section 13.28.\nIn Stata the command estat overid zb afer ivregress gmm can be used to implement a subset overidentification test where \\(\\mathrm{zb}\\) is the name(s) of the instruments(s) tested for validity. The statistic \\(C\\) and its asymptotic \\(p\\)-value using the \\(\\chi_{\\ell_{2}}^{2}\\) distribution are reported."
  },
  {
    "objectID": "chpt13-gmm.html#endogeneity-test",
    "href": "chpt13-gmm.html#endogeneity-test",
    "title": "13  Generalized Method of Moments",
    "section": "13.23 Endogeneity Test",
    "text": "13.23 Endogeneity Test\nIn Section \\(12.29\\) we introduced tests for endogeneity in the context of 2SLS estimation. Endogeneity tests are simple to implement in the GMM framework as a subset overidentification test. The model is \\(Y=Z_{1}^{\\prime} \\beta_{1}+Y_{2}^{\\prime} \\beta_{2}+e\\) where the maintained assumption is that the regressors \\(Z_{1}\\) and excluded instruments \\(Z_{2}\\) are exogenous so that \\(\\mathbb{E}\\left[Z_{1} e\\right]=0\\) and \\(\\mathbb{E}\\left[Z_{2} e\\right]=0\\). The question is whether or not \\(Y_{2}\\) is endogenous. The null hypothesis is \\(\\mathbb{M}_{0}: \\mathbb{E}\\left[Y_{2} e\\right]=0\\) with the alternative \\(\\mathbb{H}_{1}: \\mathbb{E}\\left[Y_{2} e\\right] \\neq 0\\).\nThe GMM test is constructed as follows. First, estimate the model by efficient GMM using \\(\\left(Z_{1}, Z_{2}\\right)\\) as instruments for \\(\\left(Z_{1}, Y_{2}\\right)\\). Let \\(\\widetilde{J}\\) denote the resulting GMM criterion. Second, estimate the model by efficient \\(\\mathrm{GMM}^{2}\\) using \\(\\left(Z_{1}, Z_{2}, Y_{2}\\right)\\) as instruments for \\(\\left(Z_{1}, Y_{2}\\right)\\). Let \\(\\widehat{J}\\) denote the resulting GMM criterion. The test statistic is the difference in the criterion functions: \\(C=\\widehat{J}-\\widetilde{J}\\).\nThe distribution theory for the test is a special case of overidentification testing.\nTheorem \\(13.16\\) Under Assumption \\(12.2\\) and \\(\\mathbb{E}\\left[Z_{2} Y_{2}^{\\prime}\\right]\\) has full rank \\(k_{2}\\), then as \\(n \\rightarrow \\infty, C \\underset{d}{\\rightarrow} \\chi_{k_{2}}^{2}\\). For \\(c\\) satisfying \\(\\alpha=1-G_{k_{2}}(c), \\mathbb{P}\\left[C>c \\mid \\mathbb{H}_{0}\\right] \\rightarrow \\alpha\\). The test “Reject \\(\\mathbb{H}_{0}\\) if \\(C>c\\)” has asymptotic size \\(\\alpha\\).\nIn Stata the command estat endogenous afer ivregress gmm can be used to implement the test for endogeneity. The statistic \\(C\\) and its asymptotic \\(p\\)-value using the \\(\\chi_{k_{2}}^{2}\\) distribution are reported."
  },
  {
    "objectID": "chpt13-gmm.html#subset-endogeneity-test",
    "href": "chpt13-gmm.html#subset-endogeneity-test",
    "title": "13  Generalized Method of Moments",
    "section": "13.24 Subset Endogeneity Test",
    "text": "13.24 Subset Endogeneity Test\nIn Section \\(12.30\\) we introduced subset endogeneity tests for 2SLS estimation. GMM tests are simple to implement as subset overidentification tests. The model is \\(Y=Z_{1}^{\\prime} \\beta_{1}+Y_{2}^{\\prime} \\beta_{2}+Y_{3}^{\\prime} \\beta_{3}+e\\) with \\(\\mathbb{E}[Z e]=0\\) where the instrument vector is \\(Z=\\left(Z_{1}, Z_{2}\\right)\\). The \\(k_{3} \\times 1\\) variables \\(Y_{3}\\) are treated as endogenous and the \\(k_{2} \\times 1\\) variables \\(Y_{2}\\) are treated as potentially endogenous. The hypothesis to test is that \\(Y_{2}\\) is exogenous, or \\(\\mathbb{M}_{0}: \\mathbb{E}\\left[Y_{2} e\\right]=0\\) against \\(\\mathbb{H}_{1}: \\mathbb{E}\\left[Y_{2} e\\right] \\neq 0\\). The test requires that \\(\\ell_{2} \\geq\\left(k_{2}+k_{3}\\right)\\) so that the model can be estimated under \\(\\mathbb{H}_{1}\\).\nThe GMM test is constructed as follows. First, estimate the model by efficient GMM using \\(\\left(Z_{1}, Z_{2}\\right.\\) ) as instruments for \\(\\left(Z_{1}, Y_{2}, Y_{3}\\right)\\). Let \\(\\widetilde{J}\\) denote the resulting GMM criterion. Second, estimate the model by efficient GMM using \\(\\left(Z_{1}, Z_{2}, Y_{2}\\right)\\) as instruments for \\(\\left(Z_{1}, Y_{2}, Y_{3}\\right)\\). Let \\(\\widehat{J}\\) denote the resulting GMM criterion. The test statistic is the difference in the criterion functions: \\(C=\\widehat{J}-\\widetilde{J}\\).\nThe distribution theory for the test is a special case of the theory of overidentification testing.\nTheorem \\(13.17\\) Under Assumption \\(12.2\\) and \\(\\mathbb{E}\\left[Z_{2}\\left(Y_{2}^{\\prime}, Y_{3}^{\\prime}\\right)\\right]\\) has full rank \\(k_{2}+k_{3}\\), then as \\(n \\rightarrow \\infty, C \\underset{d}{\\longrightarrow} \\chi_{k_{2}}^{2}\\). For \\(c\\) satisfying \\(\\alpha=1-G_{k_{2}}(c), \\mathbb{P}\\left[C>c \\mid \\mathbb{H}_{0}\\right] \\longrightarrow \\alpha\\). The test “Reject \\(\\mathbb{H}_{0}\\) if \\(C>c\\)” has asymptotic size \\(\\alpha\\).\nIn Stata, the command estat endogenous \\(\\mathrm{x} 2\\) afer ivregress gmm can be used to implement the test for endogeneity where \\(\\mathrm{x} 2\\) is the name(s) of the variable(s) tested for endogeneity. The statistic \\(C\\) and its asymptotic \\(\\mathrm{p}\\)-value using the \\(\\chi_{k_{2}}^{2}\\) distribution are reported.\n\\({ }^{2}\\) If the homoskedastic weight matrix is used this GMM estimator equals least squares, but when the weight matrix allows for heteroskedasticity the efficient GMM estimator does not equal least squares as the model is overidentified."
  },
  {
    "objectID": "chpt13-gmm.html#nonlinear-gmm",
    "href": "chpt13-gmm.html#nonlinear-gmm",
    "title": "13  Generalized Method of Moments",
    "section": "13.25 Nonlinear GMM",
    "text": "13.25 Nonlinear GMM\nGMM applies whenever an economic or statistical model implies the \\(\\ell \\times 1\\) moment condition\n\\[\n\\mathbb{E}\\left[g_{i}(\\beta)\\right]=0 .\n\\]\nwhere \\(g_{i}(\\beta)\\) is a possibly nonlinear function of the parameters \\(\\beta\\). Often, this is all that is known. Identification requires \\(\\ell \\geq k=\\operatorname{dim}(\\beta)\\). The GMM estimator minimizes\n\\[\nJ(\\beta)=n \\bar{g}_{n}(\\beta)^{\\prime} \\widehat{\\boldsymbol{W}} \\bar{g}_{n}(\\beta)\n\\]\nfor some weight matrix \\(\\widehat{\\boldsymbol{W}}\\) where\n\\[\n\\bar{g}_{n}(\\beta)=\\frac{1}{n} \\sum_{i=1}^{n} g_{i}(\\beta) .\n\\]\nThe efficient GMM estimator can be constructed by setting\n\\[\n\\widehat{\\boldsymbol{W}}=\\left(\\frac{1}{n} \\sum_{i=1}^{n} \\widehat{g}_{i} \\widehat{g}_{i}^{\\prime}-\\bar{g}_{n} \\bar{g}_{n}^{\\prime}\\right)^{-1},\n\\]\nwith \\(\\widehat{g}_{i}=g_{i}(\\widetilde{\\beta})\\) constructed using a preliminary consistent estimator \\(\\widetilde{\\beta}\\), perhaps obtained with \\(\\widehat{\\boldsymbol{W}}=\\boldsymbol{I}_{\\ell}\\). As in the case of the linear model the weight matrix can be iterated until convergence to obtain the iterated GMM estimator.\nProposition 13.1 Distribution of Nonlinear GMM Estimator Under general regularity conditions, \\(\\sqrt{n}\\left(\\widehat{\\beta}_{\\mathrm{gmm}}-\\beta\\right) \\underset{d}{\\longrightarrow} \\mathrm{N}\\left(0, \\boldsymbol{V}_{\\beta}\\right)\\) where\n\\[\n\\boldsymbol{V}_{\\beta}=\\left(\\boldsymbol{Q}^{\\prime} \\boldsymbol{W} \\boldsymbol{Q}\\right)^{-1}\\left(\\boldsymbol{Q}^{\\prime} \\boldsymbol{W} \\Omega \\boldsymbol{W} \\boldsymbol{Q}\\right)\\left(\\boldsymbol{Q}^{\\prime} \\boldsymbol{W} \\boldsymbol{Q}\\right)^{-1}\n\\]\nwith \\(\\Omega=\\mathbb{E}\\left[g_{i} g_{i}^{\\prime}\\right]\\) and\n\\[\n\\boldsymbol{Q}=\\mathbb{E}\\left[\\frac{\\partial}{\\partial \\beta^{\\prime}} g_{i}(\\beta)\\right]\n\\]\nIf the efficient weight matrix is used then \\(\\boldsymbol{V}_{\\beta}=\\left(\\boldsymbol{Q}^{\\prime} \\Omega^{-1} \\boldsymbol{Q}\\right)^{-1}\\).\nThe proof of this result is omitted as it uses more advanced techniques.\nThe asymptotic covariance matrices can be estimated by sample counterparts of the population matrices. For the case of a general weight matrix,\n\\[\n\\widehat{\\boldsymbol{V}}_{\\beta}=\\left(\\widehat{\\boldsymbol{Q}}^{\\prime} \\widehat{\\boldsymbol{W}} \\widehat{\\boldsymbol{Q}}\\right)^{-1}\\left(\\widehat{\\boldsymbol{Q}}^{\\prime} \\widehat{\\boldsymbol{W}} \\widehat{\\Omega} \\widehat{\\boldsymbol{W}} \\widehat{\\boldsymbol{Q}}\\right)\\left(\\widehat{\\boldsymbol{Q}}^{\\prime} \\widehat{\\boldsymbol{W}} \\widehat{\\boldsymbol{Q}}\\right)^{-1}\n\\]\nwhere\n\\[\n\\begin{gathered}\n\\widehat{\\Omega}=\\frac{1}{n} \\sum_{i=1}^{n}\\left(g_{i}(\\widehat{\\beta})-\\bar{g}\\right)\\left(g_{i}(\\widehat{\\beta})-\\bar{g}\\right)^{\\prime} \\\\\n\\bar{g}=n^{-1} \\sum_{i=1}^{n} g_{i}(\\widehat{\\beta})\n\\end{gathered}\n\\]\nand\n\\[\n\\widehat{\\boldsymbol{Q}}=\\frac{1}{n} \\sum_{i=1}^{n} \\frac{\\partial}{\\partial \\beta^{\\prime}} g_{i}(\\widehat{\\beta}) .\n\\]\nFor the case of the iterated efficient weight matrix,\n\\[\n\\widehat{\\boldsymbol{V}}_{\\beta}=\\left(\\widehat{\\boldsymbol{Q}}^{\\prime} \\widehat{\\Omega}^{-1} \\widehat{\\boldsymbol{Q}}\\right)^{-1} .\n\\]\nAll of the methods discussed in this chapter - Wald tests, constrained estimation, distance tests, overidentification tests, endogeneity tests - apply similarly to the nonlinear GMM estimator."
  },
  {
    "objectID": "chpt13-gmm.html#bootstrap-for-gmm",
    "href": "chpt13-gmm.html#bootstrap-for-gmm",
    "title": "13  Generalized Method of Moments",
    "section": "13.26 Bootstrap for GMM",
    "text": "13.26 Bootstrap for GMM\nThe bootstrap for 2SLS (Section 12.23) can be used for GMM estimation. The standard bootstrap algorithm generates bootstrap samples by sampling the triplets \\(\\left(Y_{i}^{*}, X_{i}^{*}, Z_{i}^{*}\\right)\\) independently and with replacement from the original sample. The GMM estimator is applied to the bootstrap sample to obtain the bootstrap estimates \\(\\widehat{\\beta}_{\\mathrm{gmm}}^{*}\\). This is repeated \\(B\\) times to create a sample of \\(B\\) bootstrap draws. Given these draws, bootstrap confidence intervals, including percentile, \\(\\mathrm{BC}\\) percentile, \\(\\mathrm{BC}_{a}\\) and percentile-t, are calculated conventionally.\nFor variance and standard error estimation the same cautions apply as for 2SLS. It is difficult to know if the GMM estimator has a finite variance in a given application. It is best to avoid using the bootstrap to calculate standard errors. Instead, use the bootstrap for percentile and percentile-t confidence intervals.\nWhen the model is overidentified, as discussed for 2SLS, bootstrap GMM inference will not achieve an asymptotic refinement unless the bootstrap estimator is recentered to satisfy the orthogonality condition. We now describe the recentering recommended by Hall and Horowitz (1996).\nFor linear GMM wth weight matrix \\(\\boldsymbol{W}\\) the recentered GMM bootstrap estimator is\n\\[\n\\widehat{\\beta}_{\\mathrm{gmm}}^{* *}=\\left(\\boldsymbol{X}^{* \\prime} \\boldsymbol{Z}^{*} \\boldsymbol{W}^{*} \\boldsymbol{Z}^{* \\prime} \\boldsymbol{X}^{*}\\right)^{-1}\\left(\\boldsymbol{X}^{* \\prime} \\boldsymbol{Z}^{*} \\boldsymbol{W}^{*}\\left(\\boldsymbol{Z}^{* \\prime} \\boldsymbol{Y}^{*}-\\boldsymbol{Z}^{\\prime} \\widehat{\\boldsymbol{e}}\\right)\\right)\n\\]\nwhere \\(\\boldsymbol{W}^{*}\\) is the bootstrap version of \\(\\boldsymbol{W}\\) and \\(\\widehat{\\boldsymbol{e}}=\\boldsymbol{Y}-\\boldsymbol{X} \\widehat{\\beta}_{\\mathrm{gmm}}\\). For efficient GMM,\n\\[\n\\boldsymbol{W}^{*}=\\left(\\frac{1}{n} \\sum_{i=1}^{n} Z_{i}^{*} Z_{i}^{* \\prime}\\left(Y_{i}^{*}-X_{i}^{* \\prime} \\widetilde{\\beta}^{*}\\right)^{2}\\right)^{-1}\n\\]\nfor preliminary estimator \\(\\widetilde{\\beta}^{*}\\).\nFor nonlinear GMM (Section 13.25) the bootstrap criterion function is modified. The recentered bootstrap criterion is\n\\[\n\\begin{aligned}\n&J^{* *}(\\beta)=n\\left(\\bar{g}_{n}^{*}(\\beta)-\\bar{g}_{n}\\left(\\widehat{\\beta}_{\\mathrm{gmm}}\\right)\\right)^{\\prime} \\boldsymbol{W}^{*}\\left(\\bar{g}_{n}^{*}(\\beta)-\\bar{g}_{n}\\left(\\widehat{\\beta}_{\\mathrm{gmm}}\\right)\\right) \\\\\n&\\bar{g}_{n}^{*}(\\beta)=\\frac{1}{n} \\sum_{i=1}^{n} g_{i}^{*}(\\beta)\n\\end{aligned}\n\\]\nwhere \\(\\bar{g}_{n}\\left(\\widehat{\\beta}_{\\mathrm{gmm}}\\right)\\) is from the sample not from the bootstrap data. The bootstrap estimator is\n\\[\n\\widehat{\\beta}_{\\mathrm{gmm}}^{* *}=\\operatorname{argmin} J^{* *}(\\beta) .\n\\]\nThe bootstrap can be used to calculate the p-value of the GMM overidentification test. For the GMM estimator with an efficient weight matrix the standard overidentification test is the Hansen \\(J\\) statistic\n\\[\nJ=n \\bar{g}_{n}\\left(\\widehat{\\beta}_{\\mathrm{gmm}}\\right)^{\\prime} \\widehat{\\Omega}^{-1} \\bar{g}_{n}\\left(\\widehat{\\beta}_{\\mathrm{gmm}}\\right) .\n\\]\nThe recentered bootstrap analog is\n\\[\nJ^{* *}=n\\left(\\bar{g}_{n}^{*}\\left(\\widehat{\\beta}_{\\mathrm{gmm}}^{* *}\\right)-\\bar{g}_{n}\\left(\\widehat{\\beta}_{\\mathrm{gmm}}\\right)\\right)^{\\prime} \\widehat{\\Omega}^{*-1}\\left(\\bar{g}_{n}^{*}\\left(\\widehat{\\beta}_{\\mathrm{gmm}}^{* *}\\right)-\\bar{g}_{n}\\left(\\widehat{\\beta}_{\\mathrm{gmm}}\\right)\\right)\n\\]\nOn each bootstrap sample \\(J^{* *}(b)\\) is calculated and stored. The bootstrap p-value is\n\\[\np^{*}=\\frac{1}{B} \\sum_{b=1}^{B} \\mathbb{1}\\left\\{J^{* *}(b)>S\\right\\} .\n\\]\nThis bootstrap p-value is asymptotically valid since \\(J^{* *}\\) satisfies the overidentified moment conditions."
  },
  {
    "objectID": "chpt13-gmm.html#conditional-moment-equation-models",
    "href": "chpt13-gmm.html#conditional-moment-equation-models",
    "title": "13  Generalized Method of Moments",
    "section": "13.27 Conditional Moment Equation Models",
    "text": "13.27 Conditional Moment Equation Models\nIn many contexts, an economic model implies conditional moment restriction of the form\n\\[\n\\mathbb{E}\\left[e_{i}(\\beta) \\mid Z_{i}\\right]=0\n\\]\nwhere \\(e_{i}(\\beta)\\) is some \\(s \\times 1\\) function of the observation and the parameters. In many cases \\(s=1\\). It turns out that this conditional moment restriction is more powerful than the unconditional moment equation model discussed throughout this chapter.\nFor example, the linear model \\(Y=X^{\\prime} \\beta+e\\) with instruments \\(Z\\) falls into this class under the assumption \\(\\mathbb{E}[e \\mid Z]=0\\). In this case \\(e_{i}(\\beta)=Y_{i}-X_{i}^{\\prime} \\beta\\).\nIt is also helpful to realize that conventional regression models also fall into this class except that in this case \\(X=Z\\). For example, in linear regression \\(e_{i}(\\beta)=Y_{i}-X_{i}^{\\prime} \\beta\\), while in a nonlinear regression model \\(e_{i}(\\beta)=Y_{i}-m\\left(X_{i}, \\beta\\right)\\). In a joint model of the conditional expectation \\(\\mathbb{E}[Y \\mid X=x]=x^{\\prime} \\beta\\) and variance \\(\\operatorname{var}[Y \\mid X=x]=f(x)^{\\prime} \\gamma\\), then\n\\[\ne_{i}(\\beta, \\gamma)=\\left\\{\\begin{array}{c}\nY_{i}-X_{i}^{\\prime} \\beta \\\\\n\\left(Y_{i}-X_{i}^{\\prime} \\beta\\right)^{2}-f\\left(X_{i}\\right)^{\\prime} \\gamma\n\\end{array} .\\right.\n\\]\nHere \\(s=2\\).\nGiven a conditional moment restriction an unconditional moment restriction can always be constructed. That is for any \\(\\ell \\times 1\\) function \\(\\phi(Z, \\beta)\\) we can set \\(g_{i}(\\beta)=\\phi\\left(Z_{i}, \\beta\\right) e_{i}(\\beta)\\) which satisfies \\(\\mathbb{E}\\left[g_{i}(\\beta)\\right]=\\) 0 and hence defines an unconditional moment equation model. The obvious problem is that the class of functions \\(\\phi\\) is infinite. Which should be selected?\nThis is equivalent to the problem of selection of the best instruments. If \\(Z \\in \\mathbb{R}\\) is a valid instrument satisfying \\(\\mathbb{E}[e \\mid Z]=0\\), then \\(Z, Z^{2}, Z^{3}\\),…, etc., are all valid instruments. Which should be used?\nOne solution is to construct an infinite list of potent instruments and then use the first \\(\\ell\\). How is \\(\\ell\\) to be determined? This is an area of theory still under development. One study of this problem is Donald and Newey (2001).\nAnother approach is to construct the optimal instrument which minimizes the asymptotic variance. The form was uncovered by Chamberlain (1987). Take the case \\(s=1\\). Let\n\\[\nR_{i}=\\mathbb{E}\\left[\\frac{\\partial}{\\partial \\beta} e_{i}(\\beta) \\mid Z_{i}\\right]\n\\]\nand \\(\\sigma_{i}^{2}=\\mathbb{E}\\left[e_{i}(\\beta)^{2} \\mid Z_{i}\\right]\\). Then the optimal instrument is \\(A_{i}=-\\sigma_{i}^{-2} R_{i}\\). The optimal moment is \\(g_{i}(\\beta)=\\) \\(A_{i} e_{i}(\\beta)\\). Setting \\(g_{i}(\\beta)\\) to be this choice (which is \\(k \\times 1\\), so is just-identified) yields the GMM estimator with lowest asymptotic variance. In practice \\(A_{i}\\) is unknown, but its form helps us think about construction of good instruments. In the linear model \\(e_{i}(\\beta)=Y_{i}-X_{i}^{\\prime} \\beta\\) note that \\(R_{i}=-\\mathbb{E}\\left[X_{i} \\mid Z_{i}\\right]\\) and \\(\\sigma_{i}^{2}=\\mathbb{E}\\left[e_{i}^{2} \\mid Z_{i}\\right]\\). This means the optimal instrument is \\(A_{i}=\\sigma_{i}^{-2} \\mathbb{E}\\left[X_{i} \\mid Z_{i}\\right]\\). In the case of linear regression \\(X_{i}=Z_{i}\\) so \\(A_{i}=\\sigma_{i}^{-2} Z_{i}\\). Hence efficient GMM is equivalent to GLS!\nIn the case of endogenous variables note that the efficient instrument \\(A_{i}\\) involves the estimation of the conditional mean of \\(X\\) given \\(Z\\). In other words, to get the best instrument for \\(X\\) we need the best conditional mean model for \\(X\\) given \\(Z\\) not just an arbitrary linear projection. The efficient instrument is also inversely proportional to the conditional variance of \\(e\\). This is the same as the GLS estimator; namely that improved efficiency can be obtained if the observations are weighted inversely to the conditional variance of the errors."
  },
  {
    "objectID": "chpt13-gmm.html#technical-proofs",
    "href": "chpt13-gmm.html#technical-proofs",
    "title": "13  Generalized Method of Moments",
    "section": "13.28 Technical Proofs*",
    "text": "13.28 Technical Proofs*\nProof of Theorem 13.12 Set \\(\\widetilde{e}_{i}=Y_{i}-X_{i}^{\\prime} \\widehat{\\beta}_{\\mathrm{cgmm}}\\) and \\(\\widehat{e}_{i}=Y_{i}-X_{i}^{\\prime} \\widehat{\\beta}_{\\mathrm{gmm}}\\). By standard covariance matrix analysis \\(\\widehat{\\Omega} \\vec{p} \\Omega\\) and \\(\\widetilde{\\Omega} \\vec{p} \\Omega\\). Thus we can replace \\(\\widehat{\\Omega}\\) and \\(\\widetilde{\\Omega}\\) in the criteria without affecting the asymptotic distribution. In particular\n\\[\n\\begin{aligned}\n\\widetilde{J}\\left(\\widehat{\\beta}_{\\mathrm{cgmm}}\\right) &=\\frac{1}{n} \\widetilde{\\boldsymbol{e}}^{\\prime} \\boldsymbol{Z} \\widetilde{\\Omega}^{-1} \\boldsymbol{Z}^{\\prime} \\widetilde{\\boldsymbol{e}} \\\\\n&=\\frac{1}{n} \\widetilde{\\boldsymbol{e}}^{\\prime} \\boldsymbol{Z} \\widehat{\\Omega}^{-1} \\boldsymbol{Z}^{\\prime} \\widetilde{\\boldsymbol{e}}+o_{p}(1) .\n\\end{aligned}\n\\]\nNow observe that\n\\[\n\\boldsymbol{Z}^{\\prime} \\widetilde{\\boldsymbol{e}}=\\boldsymbol{Z}^{\\prime} \\widehat{\\boldsymbol{e}}-\\boldsymbol{Z}^{\\prime} \\boldsymbol{X}\\left(\\widehat{\\beta}_{\\mathrm{cgmm}}-\\widehat{\\beta}_{\\mathrm{gmm}}\\right) .\n\\]\nThus\n\\[\n\\begin{aligned}\n\\frac{1}{n} \\widetilde{\\boldsymbol{e}}^{\\prime} \\boldsymbol{Z} \\widehat{\\Omega}^{-1} \\boldsymbol{Z}^{\\prime} \\widetilde{\\boldsymbol{e}} &=\\frac{1}{n} \\widehat{\\boldsymbol{e}}^{\\prime} \\boldsymbol{Z} \\widehat{\\Omega}^{-1} \\boldsymbol{Z}^{\\prime} \\widehat{\\boldsymbol{e}}-\\frac{2}{n}\\left(\\widehat{\\beta}_{\\mathrm{cgmm}}-\\widehat{\\beta}_{\\mathrm{gmm}}\\right)^{\\prime} \\boldsymbol{X}^{\\prime} \\boldsymbol{Z} \\widehat{\\Omega}^{-1} \\boldsymbol{Z}^{\\prime} \\widehat{\\boldsymbol{e}} \\\\\n&+\\frac{1}{n}\\left(\\widehat{\\beta}_{\\mathrm{cgmm}}-\\widehat{\\beta}_{\\mathrm{gmm}}\\right)^{\\prime} \\boldsymbol{X}^{\\prime} \\boldsymbol{Z} \\widehat{\\Omega}^{-1} \\boldsymbol{Z}^{\\prime} \\boldsymbol{X}\\left(\\widehat{\\beta}_{\\mathrm{cgmm}}-\\widehat{\\beta}_{\\mathrm{gmm}}\\right) \\\\\n&=\\widehat{J}\\left(\\widehat{\\beta}_{\\mathrm{gmm}}\\right)+\\frac{1}{n}\\left(\\widehat{\\beta}_{\\mathrm{cgmm}}-\\widehat{\\beta}_{\\mathrm{gmm}}\\right)^{\\prime} \\boldsymbol{X}^{\\prime} \\boldsymbol{Z} \\widehat{\\Omega}^{-1} \\boldsymbol{Z}^{\\prime} \\boldsymbol{X}\\left(\\widehat{\\beta}_{\\mathrm{cgmm}}-\\widehat{\\beta}_{\\mathrm{gmm}}\\right)\n\\end{aligned}\n\\]\nwhere the second equality holds because \\(\\boldsymbol{X}^{\\prime} \\boldsymbol{Z} \\widehat{\\Omega}^{-1} Z^{\\prime} \\widehat{\\boldsymbol{e}}=0\\) is the first-order condition for \\(\\widehat{\\beta}_{\\mathrm{gmm}}\\). By (13.16) and Theorem 13.4, under \\(\\mathbb{M}_{0}\\)\n\\[\n\\begin{aligned}\n\\sqrt{n}\\left(\\widehat{\\beta}_{\\mathrm{cgmm}}-\\widehat{\\beta}_{\\mathrm{gmm}}\\right) &=-\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{Z} \\Omega^{-1} \\boldsymbol{Z}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\boldsymbol{R}\\left(\\boldsymbol{R}^{\\prime}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{Z} \\Omega^{-1} \\boldsymbol{Z}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\boldsymbol{R}\\right)^{-1} \\boldsymbol{R}^{\\prime} \\sqrt{n}\\left(\\widehat{\\beta}_{\\mathrm{gmm}}-\\beta\\right)+o_{p}(1) \\\\\n& \\underset{d}{\\longrightarrow}\\left(\\boldsymbol{Q}^{\\prime} \\Omega^{-1} \\boldsymbol{Q}\\right)^{-1} \\boldsymbol{R} Z\n\\end{aligned}\n\\]\nwhere\n\\[\n\\begin{aligned}\nZ & \\sim \\mathrm{N}\\left(0, \\boldsymbol{V}_{\\boldsymbol{R}}\\right) \\\\\n\\boldsymbol{V}_{\\boldsymbol{R}} &=\\left(\\boldsymbol{R} \\boldsymbol{V}^{\\prime}\\left(\\boldsymbol{Q}^{\\prime} \\Omega^{-1} \\boldsymbol{Q}\\right)^{-1} \\boldsymbol{R}\\right)^{-1} .\n\\end{aligned}\n\\]\nPutting together (13.25), (13.26), (13.27) and (13.28),\n\\[\n\\begin{aligned}\nD &=\\widetilde{J}\\left(\\widehat{\\beta}_{\\mathrm{cgmm}}\\right)-\\widehat{J}\\left(\\widehat{\\beta}_{\\mathrm{gmm}}\\right) \\\\\n&=\\sqrt{n}\\left(\\widehat{\\beta}_{\\mathrm{cgmm}}-\\widehat{\\beta}_{\\mathrm{gmm}}\\right)^{\\prime} \\frac{1}{n} \\boldsymbol{X}^{\\prime} \\boldsymbol{Z} \\widehat{\\Omega}^{-1} \\frac{1}{n} \\boldsymbol{Z}^{\\prime} \\boldsymbol{X} \\sqrt{n}\\left(\\widehat{\\beta}_{\\mathrm{cgmm}}-\\widehat{\\beta}_{\\mathrm{gmm}}\\right) \\\\\n& \\underset{d}{\\longrightarrow} Z^{\\prime} \\boldsymbol{V}_{\\boldsymbol{R}}^{-1} Z \\sim \\chi_{q}^{2}\n\\end{aligned}\n\\]\nbecause \\(V_{R}>0\\) and \\(\\mathrm{Z}\\) is \\(q \\times 1\\).\nProof of Theorem 13.15 Let \\(\\widetilde{\\beta}\\) denote the GMM estimator obtained with the instrument set \\(Z_{a}\\) and let \\(\\widehat{\\beta}\\) denote the GMM estimator obtained with the instrument set \\(Z\\). Set \\(\\widetilde{e}_{i}=Y_{i}-X_{i}^{\\prime} \\widetilde{\\beta}, \\widehat{e}{ }_{i}=Y_{i}-X_{i}^{\\prime} \\widehat{\\beta}\\),\n\\[\n\\begin{aligned}\n&\\widetilde{\\Omega}=n^{-1} \\sum_{i=1}^{n} Z_{a i} Z_{a i}^{\\prime} \\widetilde{e}_{i}^{2} \\\\\n&\\widehat{\\Omega}=n^{-1} \\sum_{i=1}^{n} Z_{i} Z_{i}^{\\prime} \\widehat{e}_{i}^{2}\n\\end{aligned}\n\\]\nLet \\(\\boldsymbol{R}\\) be the \\(\\ell \\times \\ell_{a}\\) selector matrix so that \\(Z_{a}=\\boldsymbol{R}^{\\prime} Z\\). Note that\n\\[\n\\widetilde{\\Omega}=\\boldsymbol{R}^{\\prime} n^{-1} \\sum_{i=1}^{n} Z_{i} Z_{i}^{\\prime} \\widetilde{e}_{i}^{2} \\boldsymbol{R} .\n\\]\nBy standard covariance matrix analysis, \\(\\widehat{\\Omega} \\underset{p}{\\rightarrow} \\Omega\\) and \\(\\widetilde{\\Omega} \\underset{p}{\\rightarrow} \\boldsymbol{R}^{\\prime} \\Omega \\boldsymbol{R}\\). Also, \\(\\frac{1}{n} \\boldsymbol{Z}^{\\prime} \\boldsymbol{X} \\underset{p}{\\rightarrow} \\boldsymbol{Q}\\), say. By the CLT, \\(n^{-1 / 2} \\boldsymbol{Z}^{\\prime} \\boldsymbol{e} \\underset{d}{\\longrightarrow} Z\\) where \\(Z \\sim \\mathrm{N}(0, \\Omega)\\). Then\n\\[\n\\begin{aligned}\nn^{-1 / 2} \\boldsymbol{Z}^{\\prime} \\widehat{\\boldsymbol{e}} &=\\left(\\boldsymbol{I}_{\\ell}-\\left(\\frac{1}{n} \\boldsymbol{Z}^{\\prime} \\boldsymbol{X}\\right)\\left(\\frac{1}{n} \\boldsymbol{X}^{\\prime} \\boldsymbol{Z} \\widehat{\\Omega}^{-1} \\frac{1}{n} \\boldsymbol{Z}^{\\prime} \\boldsymbol{X}\\right)^{-1}\\left(\\frac{1}{n} \\boldsymbol{X}^{\\prime} \\boldsymbol{Z}\\right) \\widehat{\\Omega}^{-1}\\right) n^{-1 / 2} \\boldsymbol{Z}^{\\prime} \\boldsymbol{e} \\\\\n& \\rightarrow\\left(\\boldsymbol{I}_{\\ell}-\\boldsymbol{Q}\\left(\\boldsymbol{Q}^{\\prime} \\Omega^{-1} \\boldsymbol{Q}\\right)^{-1} \\boldsymbol{Q}^{\\prime} \\Omega^{-1}\\right) Z\n\\end{aligned}\n\\]\nand\n\\[\n\\begin{aligned}\nn^{-1 / 2} \\boldsymbol{Z}_{a}^{\\prime} \\widetilde{\\boldsymbol{e}} &=\\boldsymbol{R}^{\\prime}\\left(\\boldsymbol{I}_{\\ell}-\\left(\\frac{1}{n} \\boldsymbol{Z}^{\\prime} \\boldsymbol{X}\\right)\\left(\\frac{1}{n} \\boldsymbol{X}^{\\prime} \\boldsymbol{Z} \\boldsymbol{R}^{-1} \\boldsymbol{R}^{\\prime} \\frac{1}{n} \\boldsymbol{Z}^{\\prime} \\boldsymbol{X}\\right)^{-1}\\left(\\frac{1}{n} \\boldsymbol{X}^{\\prime} \\boldsymbol{Z}\\right) \\boldsymbol{R}^{-1} \\boldsymbol{R}^{\\prime}\\right) n^{-1 / 2} \\boldsymbol{Z}^{\\prime} \\boldsymbol{e} \\\\\n& \\underset{d}{\\longrightarrow} \\boldsymbol{R}^{\\prime}\\left(\\boldsymbol{I}_{\\ell}-\\boldsymbol{Q}\\left(\\boldsymbol{Q}^{\\prime} \\boldsymbol{R}\\left(\\boldsymbol{R}^{\\prime} \\Omega \\boldsymbol{R}\\right)^{-1} \\boldsymbol{R}^{\\prime} \\boldsymbol{Q}\\right)^{-1} \\boldsymbol{Q}^{\\prime} \\boldsymbol{R}\\left(\\boldsymbol{R}^{\\prime} \\Omega \\boldsymbol{R}\\right)^{-1} \\boldsymbol{R}^{\\prime}\\right) Z\n\\end{aligned}\n\\]\njointly.\nBy linear rotations of \\(Z\\) and \\(\\boldsymbol{R}\\) we can set \\(\\Omega=\\boldsymbol{I}_{\\ell}\\) to simplify the notation. Thus setting \\(\\boldsymbol{P}_{\\boldsymbol{Q}}=\\boldsymbol{Q}\\left(\\boldsymbol{Q}^{\\prime} \\boldsymbol{Q}\\right)^{-1} \\boldsymbol{Q}^{\\prime}\\), \\(\\boldsymbol{P}_{\\boldsymbol{R}}=\\boldsymbol{R}\\left(\\boldsymbol{R}^{\\prime} \\boldsymbol{R}\\right)^{-1} \\boldsymbol{R}^{\\prime}\\) and \\(Z \\sim \\mathrm{N}\\left(0, \\boldsymbol{I}_{\\ell}\\right)\\) we have\n\\[\n\\widehat{J} \\underset{d}{\\longrightarrow} Z^{\\prime}\\left(I_{\\ell}-\\boldsymbol{P}_{\\mathbf{Q}}\\right) Z\n\\]\nand\n\\[\n\\widetilde{J} \\underset{d}{\\rightarrow} Z^{\\prime}\\left(\\boldsymbol{P}_{\\boldsymbol{R}}-\\boldsymbol{P}_{\\boldsymbol{R}} \\boldsymbol{Q}\\left(\\boldsymbol{Q}^{\\prime} \\boldsymbol{P}_{R} \\boldsymbol{Q}\\right)^{-1} \\boldsymbol{Q}^{\\prime} \\boldsymbol{P}_{\\boldsymbol{R}}\\right) Z\n\\]\nIt follows that\n\\[\nC=\\widehat{J}-\\widetilde{J} \\underset{d}{\\longrightarrow} \\mathrm{Z}^{\\prime} A \\mathrm{Z}\n\\]\nwhere\n\\[\n\\boldsymbol{A}=\\left(\\boldsymbol{I}_{\\ell}-\\boldsymbol{P}_{Q}-\\boldsymbol{P}_{\\boldsymbol{R}}+\\boldsymbol{P}_{R} \\boldsymbol{Q}\\left(\\boldsymbol{Q}^{\\prime} \\boldsymbol{P}_{R} \\boldsymbol{Q}\\right)^{-1} \\boldsymbol{Q}^{\\prime} \\boldsymbol{P}_{R}\\right) .\n\\]\nThis is a quadratic form in a standard normal vector and the matrix \\(\\boldsymbol{A}\\) is idempotent (this is straightforward to check). \\(Z^{\\prime} A Z\\) is thus distributed \\(\\chi_{d}^{2}\\) with degrees of freedom \\(d\\) equal to\n\\[\n\\begin{aligned}\n\\operatorname{rank}(\\boldsymbol{A}) &=\\operatorname{tr}\\left(\\boldsymbol{I}_{\\ell}-\\boldsymbol{P}_{\\boldsymbol{Q}}-\\boldsymbol{P}_{\\boldsymbol{R}}+\\boldsymbol{P}_{\\boldsymbol{R}} \\boldsymbol{Q}\\left(\\boldsymbol{Q}^{\\prime} \\boldsymbol{P}_{\\boldsymbol{R}} \\boldsymbol{Q}\\right)^{-1} \\boldsymbol{Q}^{\\prime} \\boldsymbol{P}_{\\boldsymbol{R}}\\right) \\\\\n&=\\ell-k-\\ell_{a}+k=\\ell_{b}\n\\end{aligned}\n\\]\nThus the asymptotic distribution of \\(C\\) is \\(\\chi_{\\ell_{b}}^{2}\\) as claimed."
  },
  {
    "objectID": "chpt13-gmm.html#exercises",
    "href": "chpt13-gmm.html#exercises",
    "title": "13  Generalized Method of Moments",
    "section": "13.29 Exercises",
    "text": "13.29 Exercises\nExercise 13.1 Take the model\n\\[\n\\begin{aligned}\nY &=X^{\\prime} \\beta+e \\\\\n\\mathbb{E}[X e] &=0 \\\\\ne^{2} &=Z^{\\prime} \\gamma+\\eta \\\\\n\\mathbb{E}[Z \\eta] &=0 .\n\\end{aligned}\n\\]\nFind the method of moments estimators \\((\\widehat{\\beta}, \\widehat{\\gamma})\\) for \\((\\beta, \\gamma)\\)\nExercise 13.2 Take the model \\(Y=X^{\\prime} \\beta+e\\) with \\(\\mathbb{E}[e \\mid Z]=0\\). Let \\(\\widehat{\\beta}_{\\mathrm{gmm}}\\) be the GMM estimator using the weight matrix \\(\\boldsymbol{W}_{n}=\\left(\\boldsymbol{Z}^{\\prime} \\boldsymbol{Z}\\right)^{-1}\\). Under the assumption \\(\\mathbb{E}\\left[e^{2} \\mid Z\\right]=\\sigma^{2}\\) show that\n\\[\n\\sqrt{n}(\\widehat{\\beta}-\\beta) \\underset{d}{\\longrightarrow} \\mathrm{N}\\left(0, \\sigma^{2}\\left(\\boldsymbol{Q}^{\\prime} \\boldsymbol{M}^{-1} \\boldsymbol{Q}\\right)^{-1}\\right)\n\\]\nwhere \\(\\boldsymbol{Q}=\\mathbb{E}\\left[Z X^{\\prime}\\right]\\) and \\(\\boldsymbol{M}=\\mathbb{E}\\left[Z Z^{\\prime}\\right]\\)\nExercise 13.3 Take the model \\(Y=X^{\\prime} \\beta+e\\) with \\(\\mathbb{E}[Z e]=0\\). Let \\(\\widetilde{e}_{i}=Y_{i}-X_{i}^{\\prime} \\widetilde{\\beta}\\) where \\(\\widetilde{\\beta}\\) is consistent for \\(\\beta\\) (e.g. a GMM estimator with some weight matrix). An estimator of the optimal GMM weight matrix is\n\\[\n\\widehat{\\boldsymbol{W}}=\\left(\\frac{1}{n} \\sum_{i=1}^{n} Z_{i} Z_{i}^{\\prime} \\widetilde{e}_{i}^{2}\\right)^{-1} .\n\\]\nShow that \\(\\widehat{\\boldsymbol{W}} \\underset{p}{\\longrightarrow} \\Omega^{-1}\\) where \\(\\Omega=\\mathbb{E}\\left[Z Z^{\\prime} e^{2}\\right]\\).\nExercise \\(13.4\\) In the linear model estimated by GMM with general weight matrix \\(\\boldsymbol{W}\\) the asymptotic variance of \\(\\widehat{\\beta}_{\\mathrm{gmm}}\\) is\n\\[\n\\boldsymbol{V}=\\left(\\boldsymbol{Q}^{\\prime} \\boldsymbol{W} \\boldsymbol{Q}\\right)^{-1} \\boldsymbol{Q}^{\\prime} \\boldsymbol{W} \\Omega \\boldsymbol{W} \\boldsymbol{Q}\\left(\\boldsymbol{Q}^{\\prime} \\boldsymbol{W} \\boldsymbol{Q}\\right)^{-1}\n\\]\n\nLet \\(\\boldsymbol{V}_{0}\\) be this matrix when \\(\\boldsymbol{W}=\\Omega^{-1}\\). Show that \\(\\boldsymbol{V}_{0}=\\left(\\boldsymbol{Q}^{\\prime} \\Omega^{-1} \\boldsymbol{Q}\\right)^{-1}\\).\nWe want to show that for any \\(\\boldsymbol{W}, \\boldsymbol{V}-\\boldsymbol{V}_{0}\\) is positive semi-definite (for then \\(\\boldsymbol{V}_{0}\\) is the smaller possible covariance matrix and \\(W=\\Omega^{-1}\\) is the efficient weight matrix). To do this start by finding matrices \\(\\boldsymbol{A}\\) and \\(\\boldsymbol{B}\\) such that \\(\\boldsymbol{V}=\\boldsymbol{A}^{\\prime} \\Omega \\boldsymbol{A}\\) and \\(\\boldsymbol{V}_{0}=\\boldsymbol{B}^{\\prime} \\Omega \\boldsymbol{B}\\).\nShow that \\(\\boldsymbol{B}^{\\prime} \\Omega \\boldsymbol{A}=\\boldsymbol{B}^{\\prime} \\Omega \\boldsymbol{B}\\) and therefore that \\(\\boldsymbol{B}^{\\prime} \\Omega(\\boldsymbol{A}-\\boldsymbol{B})=0\\).\nUse the expressions \\(\\boldsymbol{V}=\\boldsymbol{A}^{\\prime} \\mathbf{\\Omega} \\boldsymbol{A}, \\boldsymbol{A}=\\boldsymbol{B}+(\\boldsymbol{A}-\\boldsymbol{B})\\), and \\(\\boldsymbol{B}^{\\prime} \\boldsymbol{\\Omega}(\\boldsymbol{A}-\\boldsymbol{B})=0\\) to show that \\(\\boldsymbol{V} \\geq \\boldsymbol{V}_{0}\\).\n\nExercise \\(13.5\\) Prove Theorem 13.8.\nExercise 13.6 Derive the constrained GMM estimator (13.16).\nExercise 13.7 Show that the constrained GMM estimator (13.16) with the efficient weight matrix is (13.19).\nExercise \\(13.8\\) Prove Theorem 13.9.\nExercise 13.9 Prove Theorem 13.10. Exercise \\(13.10\\) The equation of interest is \\(Y=m(X, \\beta)+e\\) with \\(\\mathbb{E}[Z e]=0\\) where \\(m(x, \\beta)\\) is a known function, \\(\\beta\\) is \\(k \\times 1\\) and \\(Z\\) is \\(\\ell \\times 1\\). Show how to construct an efficient GMM estimator for \\(\\beta\\).\nExercise 13.11 As a continuation of Exercise \\(12.7\\) derive the efficient GMM estimator using the instrument \\(Z=\\left(\\begin{array}{ll}X & X^{2}\\end{array}\\right)^{\\prime}\\). Does this differ from 2SLS and/or OLS?\nExercise 13.12 In the linear model \\(Y=X^{\\prime} \\beta+e\\) with \\(\\mathbb{E}[X e]=0\\) the GMM criterion function for \\(\\beta\\) is\n\\[\nJ(\\beta)=\\frac{1}{n}(\\boldsymbol{Y}-\\boldsymbol{X} \\beta)^{\\prime} \\boldsymbol{X} \\widehat{\\Omega}^{-1} \\boldsymbol{X}^{\\prime}(\\boldsymbol{Y}-\\boldsymbol{X} \\beta)\n\\]\nwhere \\(\\widehat{\\Omega}=n^{-1} \\sum_{i=1}^{n} X_{i} X_{i}^{\\prime} \\widehat{e}_{i}^{2}, \\widehat{e}_{i}=Y_{i}-X_{i}^{\\prime} \\widehat{\\beta}\\) are the OLS residuals, and \\(\\widehat{\\beta}=\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\boldsymbol{X}^{\\prime} \\boldsymbol{Y}\\) is least squares. The GMM estimator of \\(\\beta\\) subject to the restriction \\(r(\\beta)=0\\) is\n\\[\n\\widetilde{\\beta}=\\underset{r(\\beta)=0}{\\operatorname{argmin}} J_{n}(\\beta) .\n\\]\nThe GMM test statistic (the distance statistic) of the hypothesis \\(r(\\beta)=0\\) is\n\\[\nD=J(\\tilde{\\beta})=\\min _{r(\\beta)=0} J(\\beta) .\n\\]\n\nShow that you can rewrite \\(J(\\beta)\\) in (13.29) as\n\n\\[\nJ(\\beta)=n(\\beta-\\widehat{\\beta})^{\\prime} \\widehat{\\boldsymbol{V}}_{\\beta}^{-1}(\\beta-\\widehat{\\beta})\n\\]\nand thus \\(\\widetilde{\\beta}\\) is the same as the minimum distance estimator.\n\nShow that under linear hypotheses the distance statistic \\(D\\) in (13.30) equals the Wald statistic.\n\nExercise 13.13 Take the linear model \\(Y=X^{\\prime} \\beta+e\\) with \\(\\mathbb{E}[Z e]=0\\). Consider the GMM estimator \\(\\widehat{\\beta}\\) of \\(\\beta\\). Let \\(J=n \\bar{g}_{n}(\\widehat{\\beta})^{\\prime} \\widehat{\\Omega}^{-1} \\bar{g}_{n}(\\widehat{\\beta})\\) denote the test of overidentifying restrictions. Show that \\(J \\underset{d}{\\longrightarrow} \\chi_{\\ell-k}^{2}\\) as \\(n \\rightarrow \\infty\\) by demonstrating each of the following.\n\nSince \\(\\Omega>0\\), we can write \\(\\Omega^{-1}=\\boldsymbol{C} \\boldsymbol{C}^{\\prime}\\) and \\(\\Omega=\\boldsymbol{C}^{\\prime-1} \\boldsymbol{C}^{-1}\\) for some matrix \\(\\boldsymbol{C}\\).\n\\(J=n\\left(\\boldsymbol{C}^{\\prime} \\bar{g}_{n}(\\widehat{\\beta})\\right)^{\\prime}\\left(\\boldsymbol{C}^{\\prime} \\widehat{\\Omega} \\boldsymbol{C}\\right)^{-1} \\boldsymbol{C}^{\\prime} \\bar{g}_{n}(\\widehat{\\beta})\\).\n\\(\\boldsymbol{C}^{\\prime} \\bar{g}_{n}(\\widehat{\\beta})=\\boldsymbol{D}_{n} \\boldsymbol{C}^{\\prime} \\bar{g}_{n}(\\beta)\\) where \\(\\bar{g}_{n}(\\beta)=\\frac{1}{n} \\boldsymbol{Z}^{\\prime} \\boldsymbol{e}\\) and\n\n\\[\n\\boldsymbol{D}_{n}=\\boldsymbol{I}_{\\ell}-\\boldsymbol{C}^{\\prime}\\left(\\frac{1}{n} \\boldsymbol{Z}^{\\prime} \\boldsymbol{X}\\right)\\left(\\left(\\frac{1}{n} \\boldsymbol{X}^{\\prime} \\boldsymbol{Z}\\right) \\widehat{\\Omega}^{-1}\\left(\\frac{1}{n} \\boldsymbol{Z}^{\\prime} \\boldsymbol{X}\\right)\\right)^{-1}\\left(\\frac{1}{n} \\boldsymbol{X}^{\\prime} \\boldsymbol{Z}\\right) \\widehat{\\Omega}^{-1} \\boldsymbol{C}^{\\prime-1}\n\\]\n\n\\(\\boldsymbol{D}_{n} \\underset{p}{\\longrightarrow} \\boldsymbol{I}_{\\ell}-\\boldsymbol{R}\\left(\\boldsymbol{R}^{\\prime} \\boldsymbol{R}\\right)^{-1} \\boldsymbol{R}^{\\prime}\\) where \\(\\boldsymbol{R}=\\boldsymbol{C}^{\\prime} \\mathbb{E}\\left[Z X^{\\prime}\\right]\\).\n\\(n^{1 / 2} \\boldsymbol{C}^{\\prime} \\bar{g}_{n}(\\beta) \\underset{d}{\\longrightarrow} u \\sim \\mathrm{N}\\left(0, \\boldsymbol{I}_{\\ell}\\right)\\).\n\\(J \\underset{d}{\\longrightarrow} u^{\\prime}\\left(I_{\\ell}-\\boldsymbol{R}\\left(\\boldsymbol{R}^{\\prime} \\boldsymbol{R}\\right)^{-1} \\boldsymbol{R}^{\\prime}\\right) u\\).\n\\(u^{\\prime}\\left(\\boldsymbol{I}_{\\ell}-\\boldsymbol{R}\\left(\\boldsymbol{R}^{\\prime} \\boldsymbol{R}\\right)^{-1} \\boldsymbol{R}^{\\prime}\\right) u \\sim \\chi_{\\ell-k}^{2}\\).\n\nHint: \\(\\boldsymbol{I}_{\\ell}-\\boldsymbol{R}\\left(\\boldsymbol{R}^{\\prime} \\boldsymbol{R}\\right)^{-1} \\boldsymbol{R}^{\\prime}\\) is a projection matrix. Exercise 13.14 Take the model \\(Y=X^{\\prime} \\beta+e\\) with \\(\\mathbb{E}[Z e]=0, Y \\in \\mathbb{R}, X \\in \\mathbb{R}^{k}, Z \\in \\mathbb{R}^{\\ell}, \\ell \\geq k\\). Consider the statistic\n\\[\n\\begin{aligned}\nJ(\\beta) &=n \\bar{m}_{n}(\\beta)^{\\prime} \\boldsymbol{W} \\bar{m}_{n}(\\beta) \\\\\n\\bar{m}_{n}(\\beta) &=\\frac{1}{n} \\sum_{i=1}^{n} Z_{i}\\left(Y_{i}-X_{i}^{\\prime} \\beta\\right)\n\\end{aligned}\n\\]\nfor some weight matrix \\(W>0\\).\n\nTake the hypothesis \\(\\mathbb{I}_{0}: \\beta=\\beta_{0}\\). Derive the asymptotic distribution of \\(J\\left(\\beta_{0}\\right)\\) under \\(\\mathbb{H}_{0}\\) as \\(n \\rightarrow \\infty\\).\nWhat choice for \\(W\\) yields a known asymptotic distribution in part (a)? (Be specific about degrees of freedom.)\nWrite down an appropriate estimator \\(\\widehat{\\boldsymbol{W}}\\) for \\(W\\) which takes advantage of \\(\\mathbb{M}_{0}\\). (You do not need to demonstrate consistency or unbiasedness.)\nDescribe an asymptotic test of \\(\\mathbb{H}_{0}\\) against \\(\\mathbb{M}_{1}: \\beta \\neq \\beta_{0}\\) based on this statistic.\nUse the result in part (d) to construct a confidence region for \\(\\beta\\). What can you say about the form of this region? For example, does the confidence region take the form of an ellipse, similar to conventional confidence regions?\n\nExercise 13.15 Consider the model \\(Y=X^{\\prime} \\beta+e\\) with \\(\\mathbb{E}[Z e]=0\\) and\n\\[\n\\boldsymbol{R}^{\\prime} \\beta=0\n\\]\nwith \\(Y \\in \\mathbb{R}, X \\in \\mathbb{R}^{k}, Z \\in \\mathbb{R}^{\\ell}, \\ell>k\\). The matrix \\(\\boldsymbol{R}\\) is \\(k \\times q\\) with \\(1 \\leq q<k\\). You have a random sample \\(\\left(Y_{i}, X_{i}, Z_{i}: i=1, \\ldots, n\\right)\\).\nFor simplicity, assume the efficient weight matrix \\(\\boldsymbol{W}=\\left(\\mathbb{E}\\left[Z Z^{\\prime} e^{2}\\right]\\right)^{-1}\\) is known.\n\nWrite out the GMM estimator \\(\\widehat{\\beta}\\) ignoring constraint (13.31).\nWrite out the GMM estimator \\(\\widetilde{\\beta}\\) adding the constraint (13.31).\nFind the asymptotic distribution of \\(\\sqrt{n}(\\widetilde{\\beta}-\\beta)\\) as \\(n \\rightarrow \\infty\\) under Assumption (13.31).\n\nExercise \\(13.16\\) The observed data is \\(\\left\\{Y_{i}, X_{i}, Z_{i}\\right\\} \\in \\mathbb{R} \\times \\mathbb{R}^{k} \\times \\mathbb{R}^{\\ell}, k>1\\) and \\(\\ell>k>1, i=1, \\ldots, n\\). The model is \\(Y=X^{\\prime} \\beta+e\\) with \\(\\mathbb{E}[Z e]=0\\).\n\nGiven a weight matrix \\(\\boldsymbol{W}>0\\) write down the GMM estimator \\(\\widehat{\\beta}\\) for \\(\\beta\\).\nSuppose the model is misspecified. Specifically, assume that for some \\(\\delta \\neq 0\\),\n\n\\[\n\\begin{aligned}\ne &=\\delta n^{-1 / 2}+u \\\\\n\\mathbb{E}[u \\mid Z] &=0\n\\end{aligned}\n\\]\nwith \\(\\mu_{Z}=\\mathbb{E}[Z] \\neq 0\\). Show that (13.32) implies that \\(\\mathbb{E}[Z e] \\neq 0\\).\n\nExpress \\(\\sqrt{n}(\\widehat{\\beta}-\\beta)\\) as a function of \\(\\boldsymbol{W}, n, \\delta\\), and the variables \\(\\left(X_{i}, Z_{i}, u_{i}\\right)\\).\nFind the asymptotic distribution of \\(\\sqrt{n}(\\widehat{\\beta}-\\beta)\\) under Assumption (13.32). Exercise \\(13.17\\) The model is \\(Y=Z \\beta+X \\gamma+e\\) with \\(\\mathbb{E}[e \\mid Z]=0, X \\in \\mathbb{R}\\) and \\(Z \\in \\mathbb{R}\\). \\(X\\) is potentially endogenous and \\(Z\\) is exogenous. Someone suggests estimating \\((\\beta, \\gamma)\\) by GMM using the pair \\(\\left(Z, Z^{2}\\right)\\) as instruments. Is this feasible? Under what conditions is this a valid estimator?\n\nExercise \\(13.18\\) The observations are i.i.d., \\(\\left(Y_{i}, X_{i}, Q_{i}: i=1, \\ldots, n\\right)\\), where \\(X\\) is \\(k \\times 1\\) and \\(Q\\) is \\(m \\times 1\\). The model is \\(Y=X^{\\prime} \\beta+e\\) with \\(\\mathbb{E}[X e]=0\\) and \\(\\mathbb{E}[Q e]=0\\). Find the efficient GMM estimator for \\(\\beta\\).\nExercise 13.19 You want to estimate \\(\\mu=\\mathbb{E}[Y]\\) under the assumption that \\(\\mathbb{E}[X]=0\\), where \\(Y\\) and \\(X\\) are scalar and observed from a random sample. Find an efficient GMM estimator for \\(\\mu\\).\nExercise 13.20 Consider the model \\(Y=X^{\\prime} \\beta+e\\) given \\(\\mathbb{E}[Z e]=0\\) and \\(\\boldsymbol{R}^{\\prime} \\beta=0\\). The dimensions are \\(X \\in R^{k}\\) and \\(Z \\in R^{\\ell}\\) with \\(\\ell>k\\). The matrix \\(\\boldsymbol{R}\\) is \\(k \\times q, 1 \\leq q<k\\). Derive an efficient GMM estimator for \\(\\beta\\).\nExercise 13.21 Take the linear equation \\(Y=X^{\\prime} \\beta+e\\) and consider the following estimators of \\(\\beta\\).\n\n\\(\\widehat{\\beta}\\) : 2SLS using the instruments \\(Z_{1}\\).\n\\(\\widetilde{\\beta}: 2\\) SLS using the instruments \\(Z_{2}\\).\n\\(\\bar{\\beta}\\) : GMM using the instruments \\(Z=\\left(Z_{1}, Z_{2}\\right)\\) and the weight matrix\n\n\\[\n\\boldsymbol{W}=\\left(\\begin{array}{cc}\n\\left(\\boldsymbol{Z}_{1}^{\\prime} \\boldsymbol{Z}_{1}\\right)^{-1} \\lambda & 0 \\\\\n0 & \\left(\\boldsymbol{Z}_{2}^{\\prime} \\boldsymbol{Z}_{2}\\right)^{-1}(1-\\lambda)\n\\end{array}\\right)\n\\]\nfor \\(\\lambda \\in(0,1)\\).\nFind an expression for \\(\\bar{\\beta}\\) which shows that it is a specific weighted average of \\(\\widehat{\\beta}\\) and \\(\\widetilde{\\beta}\\).\nExercise 13.22 Consider the just-identified model \\(Y=X_{1}^{\\prime} \\beta_{1}+X_{2}^{\\prime} \\beta_{2}+e\\) with \\(\\mathbb{E}[Z e]=0\\) where \\(X=\\left(X_{1}^{\\prime}\\right.\\) \\(\\left.X_{2}^{\\prime}\\right)^{\\prime} \\in \\mathbb{R}^{k}\\) and \\(Z \\in \\mathbb{R}^{k}\\). We want to test \\(\\mathbb{H}_{0}: \\beta_{1}=0\\). Three econometricians are called for advice.\n\nEconometrician 1 proposes testing \\(\\mathbb{M}_{0}\\) by a Wald statistic.\nEconometrician 2 suggests testing \\(\\mathbb{M}_{0}\\) by the GMM Distance Statistic.\nEconometrician 3 suggests testing \\(\\mathbb{M}_{0}\\) using the test of overidentifying restrictions.\n\nYou are asked to settle this dispute. Explain the advantages and/or disadvantages of the different procedures in this specific context.\nExercise 13.23 Take the model \\(Y=X^{\\prime} \\beta+e\\) with \\(\\mathbb{E}[X e]=0\\) and \\(\\beta=\\boldsymbol{Q} \\theta\\), where \\(\\beta\\) is \\(k \\times 1, \\boldsymbol{Q}\\) is \\(k \\times m\\) with \\(m<k, \\boldsymbol{Q}\\) is known, and \\(\\theta\\) is \\(m \\times 1\\). The observations \\(\\left(Y_{i}, X_{i}\\right)\\) are i.i.d. across \\(i=1, \\ldots, n\\).\nUnder these assumptions what is the efficient estimator of \\(\\theta\\) ?\nExercise 13.24 Take the model \\(Y=\\theta+e\\) with \\(\\mathbb{E}[X e]=0, Y \\in \\mathbb{R}, X \\in \\mathbb{R}^{k}\\) and \\(\\left(Y_{i}, X_{i}\\right)\\) a random sample.\n\nFind the efficient GMM estimator of \\(\\theta\\).\nIs this model over-identified or just-identified?\nFind the GMM test statistic for over-identification. Exercise 13.25 Take the model \\(Y=X^{\\prime} \\beta+e\\) with \\(\\mathbb{E}[X e]=0\\) where \\(X\\) contains an intercept so \\(\\mathbb{E}[e]=0\\). An enterprising econometrician notices that this implies the \\(n\\) moment conditions\n\n\\[\n\\mathbb{E}\\left[e_{i}\\right]=0, i=1, \\ldots, n .\n\\]\nGiven an \\(n \\times n\\) weight matrix \\(\\boldsymbol{W}\\), this implies a GMM criterion\n\\[\nJ(\\beta)=(\\boldsymbol{Y}-\\boldsymbol{X} \\beta)^{\\prime} \\boldsymbol{W}(\\boldsymbol{Y}-\\boldsymbol{X} \\beta) .\n\\]\n\nUnder i.i.d. sampling, show that the efficient weight matrix is \\(\\boldsymbol{W}=\\sigma^{-2} \\boldsymbol{I}_{n}\\) where \\(\\sigma^{2}=\\mathbb{E}\\left[e^{2}\\right]\\).\nUsing the weight matrix \\(\\boldsymbol{W}=\\sigma^{-2} \\boldsymbol{I}_{n}\\) find the GMM estimator \\(\\widehat{\\beta}\\) that minimizes \\(J(\\beta)\\).\nFind a simple expression for the minimized criteria \\(J(\\widehat{\\beta})\\).\nTheorem \\(13.14\\) says that criterion such as \\(J(\\widehat{\\beta})\\) are asymptotically \\(\\chi_{\\ell-k}^{2}\\) where \\(\\ell\\) is the number of moments. While the assumptions of Theorem \\(13.14\\) do not apply to this context, what is \\(\\ell\\) here? That is, which \\(\\chi^{2}\\) distribution is the asserted asymptotic distribution?\nDoes the answer in (d) make sense? Explain your reasoning.\n\nExercise 13.26 Take the model \\(Y=X^{\\prime} \\beta+e\\) with \\(\\mathbb{E}[e \\mid X]=0\\) and \\(\\mathbb{E}\\left[e^{2} \\mid X\\right]=\\sigma^{2}\\). An econometrician more enterprising than the one in previous question notices that this implies the \\(n k\\) moment conditions\n\\[\n\\mathbb{E}\\left[X_{i} e_{i}\\right]=0, i=1, \\ldots, n .\n\\]\nWe can write the moments using matrix notation as \\(\\mathbb{E}\\left[\\bar{X}^{\\prime}(\\boldsymbol{Y}-\\boldsymbol{X} \\beta)\\right]\\) where\n\\[\n\\overline{\\boldsymbol{X}}=\\left(\\begin{array}{cccc}\nX_{1}^{\\prime} & 0 & \\cdots & 0 \\\\\n0 & X_{2}^{\\prime} & & 0 \\\\\n\\vdots & \\vdots & & \\vdots \\\\\n0 & 0 & \\cdots & X_{n}^{\\prime}\n\\end{array}\\right) \\text {. }\n\\]\nGiven an \\(n k \\times n k\\) weight matrix \\(\\boldsymbol{W}\\) this implies a GMM criterion\n\\[\nJ(\\beta)=(\\boldsymbol{Y}-\\boldsymbol{X} \\beta)^{\\prime} \\overline{\\boldsymbol{X}} \\boldsymbol{W} \\overline{\\boldsymbol{X}}^{\\prime}(\\boldsymbol{Y}-\\boldsymbol{X} \\beta) .\n\\]\n\nCalculate \\(\\Omega=\\mathbb{E}\\left[\\overline{\\boldsymbol{X}}^{\\prime} \\boldsymbol{e} \\boldsymbol{e}^{\\prime} \\overline{\\boldsymbol{X}}\\right]\\).\nThe econometrician decides to set \\(\\boldsymbol{W}=\\Omega^{-}\\), the Moore-Penrose generalized inverse of \\(\\Omega\\). (See Section A.6.) Note: A useful fact is that for a vector \\(\\boldsymbol{a},\\left(\\boldsymbol{a} \\boldsymbol{a}^{\\prime}\\right)^{-}=\\boldsymbol{a} \\boldsymbol{a}^{\\prime}\\left(\\boldsymbol{a}^{\\prime} \\boldsymbol{a}\\right)^{-2}\\).\nFind the GMM estimator \\(\\widehat{\\beta}\\) that minimizes \\(J(\\beta)\\).\nFind a simple expression for the minimized criterion \\(J(\\widehat{\\beta})\\).\nComment on whether the \\(\\chi^{2}\\) approximation from Theorem \\(13.14\\) is appropriate for \\(J(\\widehat{\\beta})\\).\n\nExercise 13.27 Continuation of Exercise 12.22, based on the empirical work reported in Acemoglu, Johnson, and Robinson (2001).\n\nRe-estimate the model estimated in part (j) by efficient GMM. Use the 2SLS estimates as the firststep for the weight matrix and then calculate the GMM estimator using this weight matrix without further iteration. Report the estimates and standard errors. (b) Calculate and report the \\(J\\) statistic for overidentification.\nCompare the GMM and 2SLS estimates. Discuss your findings.\n\nExercise 13.28 Continuation of Exercise 12.24, which involved estimation of a wage equation by 2 SLS.\n\nRe-estimate the model in part (a) by efficient GMM. Do the results change meaningfully?\nRe-estimate the model in part (d) by efficient GMM. Do the results change meaningfully?\nReport the \\(J\\) statistic for overidentification."
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "Summary",
    "section": "",
    "text": "In summary, this book has no content whatsoever."
  },
  {
    "objectID": "summary.html#section-depth2",
    "href": "summary.html#section-depth2",
    "title": "Summary",
    "section": "section depth2",
    "text": "section depth2"
  }
]
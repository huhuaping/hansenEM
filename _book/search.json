[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Hansen中高级计量体系",
    "section": "",
    "text": "前言\n手稿未出版，盛名已远扬。计量泰斗Hansen Bruce个人官网的传奇大作《Econometrics》，2022年终于横空出世，正式出版了。\n\nHansen B. Econometrics[M]. Princeton: Princeton University Press, 2022.\n\n作为硕士和博士的中高级计量讲义，如果能将之引入中国，传播给国内学子，绝对是大功一件。\n本项目志在如此，希望能与同道之士携手努力，大家共献绵薄，各展所长，日进寸功，添砖加瓦，展示其中的精奥！\n\n本项目的在线网址 https://hansenem.netlify.app.\n本项目完全开源，代码公开于两个托管平台（完全同步）：gihub仓库https://github.com/huhuaping/hansenEM，以及gitee仓库https://gitee.com/kevinhhp/hansenEM\n本项目将基于R编程语言生态，并使用全新的出版工具Quarto进行书稿展示。\n\n如果有兴趣参与本项目，请联系huhuaping01[at]qq.com"
  },
  {
    "objectID": "participate.html#sec-hardware",
    "href": "participate.html#sec-hardware",
    "title": "如何参与？",
    "section": "准备硬件环境",
    "text": "准备硬件环境\n\n建议使用Window操作系统"
  },
  {
    "objectID": "participate.html#sec-software",
    "href": "participate.html#sec-software",
    "title": "如何参与？",
    "section": "准备软件环境",
    "text": "准备软件环境\n\n必要软件\n以下为必备安装软件：\n\n安装R语言底层环境：R V4.2版本及以上（免费下载地址https://cran.r-project.org/）\n安装R语言编译环境（IDE）：Rstudio V2022.07版本及以上（免费下载地址https://www.rstudio.com/products/rstudio/download/）\n\n\n\n推荐软件\n以下为建议安装软件：\n\n使用版本控制工具：安装gitbash（Windows操作系统用户，免费下载地址https://gitforwindows.org/）。主要用于项目（project）的内容维护和成员工作协同。\n\n\n\n互联网服务\n为了更好地团队协作和在线办公，建议使用如下相关互联网产品/服务：\n\n建议注册代码托管平台的账号服务：一是国际码农最大社区github（https://github.com/，国内访问速度慢，偶尔会抽风）；二是国内代码托管平台gitee（https://gitee.com/）。提示：注册账号时最好使用高校邮箱（形如xxx\\@xxx.edu.cn），可以享受更多教育服务和优惠。本项目的所有代码和资料同步托管于上述两个平台，大家根据自身情况任意选用。"
  },
  {
    "objectID": "participate.html#主要参与方式",
    "href": "participate.html#主要参与方式",
    "title": "如何参与？",
    "section": "主要参与方式",
    "text": "主要参与方式\n首先需要声明的是：所有关注本项目的任何贡献和努力都是受到欢迎和支持的。\n当然，根据参与人个人知识和技能的差异，我们支持如下的松散参与和紧密参与两种方式，简单地：\n\n松散参与：参与者仅需对项目工作感兴趣（中高级计量分析），并不要求具备其他太多的相关知识和技能（这里先略过不提）。只需一台电脑，自行独立工作，通过传统互动（面谈/Email/QQ/微信等）提交自己的任何工作贡献。\n紧密参与：参与者对项目工作感兴趣（中高级计量分析），且具备其他初步相关知识和技能（一种或多种），例如：统计编程语言（R/python/stata等）、现代标记语言（markdown/Rmarkdown）、版本控制语言（git）等。需要在自己电脑上开展工作，与项目其他成员通过代码托管平台（见前面 Section 2.3）在线互动（github或gitee平台等），提交自己的任何工作贡献。\n\n\n松散参与：本地贡献\n主要过程如下：\n\n电脑安装基本的必要软件R和Rstudio（见前面 Section 2.1 ）。\n从托管平台（见前面 Section 2.3 ）拷贝项目到自己的本地电脑，然后解压缩项目文件。本项目完全开源，代码公开于两个托管平台（完全同步）：\n\ngihub仓库https://github.com/huhuaping/hansenEM\ngitee仓库https://gitee.com/kevinhhp/hansenEM\n\n使用编译工具Rstudio软件，打开项目文件hansenEM.Rproj\n在编译工具Rstudio软件下开展相关工作（后面会详谈如何开展自己的工作）。\n自行独立工作，通过传统互动（面谈/Email/QQ/微信等）提交自己的任何工作贡献。\n\n\n\n紧密参与：在线协作\n\n电脑安装基本的必要软件R和Rstudio（见前面 Section 2.1）以及版本控制软件gitbash（见前面 Section 2.2），同时注册代码托管平台账号（github或gitee平台，见前面 Section 2.3）。\n从托管平台（见前面 Section 2.3 ）拷贝项目到自己的本地电脑，然后解压缩项目文件。本项目完全开源，代码公开于两个托管平台（完全同步）：\n\ngihub仓库https://github.com/huhuaping/hansenEM\ngitee仓库https://gitee.com/kevinhhp/hansenEM\n\n使用编译工具Rstudio软件，打开项目文件hansenEM.Rproj\n在编译工具Rstudio软件下开展相关工作（后面会详谈如何开展自己的工作）。\n在自己电脑上开展工作，并与项目其他成员通过代码托管平台（见前面 Section 2.3）在线互动（github或gitee平台），在线提交自己的任何工作贡献。"
  },
  {
    "objectID": "chpt01-intro.html#what-is-econometrics",
    "href": "chpt01-intro.html#what-is-econometrics",
    "title": "1  Introduction",
    "section": "1.1 What is Econometrics?",
    "text": "1.1 What is Econometrics?\nThe term “econometrics” is believed to have been crafted by Ragnar Frisch (1895-1973) of Norway, one of the three principal founders of the Econometric Society, first editor of the journal Econometrica, and co-winner of the first Nobel Memorial Prize in Economic Sciences in 1969. It is therefore fitting that we turn to Frisch’s own words in the introduction to the first issue of Econometrica to describe the discipline.\nA word of explanation regarding the term econometrics may be in order. Its definition is implied in the statement of the scope of the [Econometric] Society, in Section I of the Constitution, which reads: “The Econometric Society is an international society for the advancement of economic theory in its relation to statistics and mathematics…. Its main object shall be to promote studies that aim at a unification of the theoretical-quantitative and the empirical-quantitative approach to economic problems….”\nBut there are several aspects of the quantitative approach to economics, and no single one of these aspects, taken by itself, should be confounded with econometrics. Thus, econometrics is by no means the same as economic statistics. Nor is it identical with what we call general economic theory, although a considerable portion of this theory has a defininitely quantitative character. Nor should econometrics be taken as synonomous with the application of mathematics to economics. Experience has shown that each of these three viewpoints, that of statistics, economic theory, and mathematics, is a necessary, but not by itself a sufficient, condition for a real understanding of the quantitative relations in modern economic life. It is the unification of all three that is powerful. And it is this unification that constitutes econometrics.\nRagnar Frisch, Econometrica, (1933), 1, pp. 1-2.\nThis definition remains valid today, although some terms have evolved somewhat in their usage. Today, we would say that econometrics is the unified study of economic models, mathematical statistics, and economic data.\nWithin the field of econometrics there are sub-divisions and specializations. Econometric theory concerns the development of tools and methods, and the study of the properties of econometric methods. Applied econometrics is a term describing the development of quantitative economic models and the application of econometric methods to these models using economic data."
  },
  {
    "objectID": "chpt01-intro.html#the-probability-approach-to-econometrics",
    "href": "chpt01-intro.html#the-probability-approach-to-econometrics",
    "title": "1  Introduction",
    "section": "1.2 The Probability Approach to Econometrics",
    "text": "1.2 The Probability Approach to Econometrics\nThe unifying methodology of modern econometrics was articulated by Trygve Haavelmo (1911-1999) of Norway, winner of the 1989 Nobel Memorial Prize in Economic Sciences, in his seminal paper “The probability approach in econometrics” (1944). Haavelmo argued that quantitative economic models must necessarily be probability models (by which today we would mean stochastic). Deterministic models are blatently inconsistent with observed economic quantities, and it is incoherent to apply deterministic models to non-deterministic data. Economic models should be explicitly designed to incorporate randomness; stochastic errors should not be simply added to deterministic models to make them random. Once we acknowledge that an economic model is a probability model, it follows naturally that an appropriate tool way to quantify, estimate, and conduct inferences about the economy is through the powerful theory of mathematical statistics. The appropriate method for a quantitative economic analysis follows from the probabilistic construction of the economic model.\nHaavelmo’s probability approach was quickly embraced by the economics profession. Today no quantitative work in economics shuns its fundamental vision.\nWhile all economists embrace the probability approach, there has been some evolution in its implementation.\nThe structural approach is the closest to Haavelmo’s original idea. A probabilistic economic model is specified, and the quantitative analysis performed under the assumption that the economic model is correctly specified. Researchers often describe this as “taking their model seriously”. The structural approach typically leads to likelihood-based analysis, including maximum likelihood and Bayesian estimation.\nA criticism of the structural approach is that it is misleading to treat an economic model as correctly specified. Rather, it is more accurate to view a model as a useful abstraction or approximation. In this case, how should we interpret structural econometric analysis? The quasi-structural approach to inference views a structural economic model as an approximation rather than the truth. This theory has led to the concepts of the pseudo-true value (the parameter value defined by the estimation problem), the quasi-likelihood function, quasi-MLE, and quasi-likelihood inference.\nClosely related is the semiparametric approach. A probabilistic economic model is partially specified but some features are left unspecified. This approach typically leads to estimation methods such as least squares and the generalized method of moments. The semiparametric approach dominates contemporary econometrics, and is the main focus of this textbook.\nAnother branch of quantitative structural economics is the calibration approach. Similar to the quasi-structural approach, the calibration approach interprets structural models as approximations and hence inherently false. The difference is that the calibrationist literature rejects mathematical statistics (deeming classical theory as inappropriate for approximate models) and instead selects parameters by matching model and data moments using non-statistical \\(a d h o c^{1}\\) methods."
  },
  {
    "objectID": "chpt01-intro.html#trygve-haavelmo",
    "href": "chpt01-intro.html#trygve-haavelmo",
    "title": "1  Introduction",
    "section": "1.3 Trygve Haavelmo",
    "text": "1.3 Trygve Haavelmo\nThe founding ideas of the field of econometrics are largely due to the Norweigen econometrician Trygve Haavelmo (1911-1999). His advocacy of probability models revolutionized the field, and his use of formal mathematical reasoning laid the foundation for subsequent generations. He was awarded the Nobel Memorial Prize in Economic Sciences in \\(1989 .\\)\n\\({ }^{1}\\) Ad hoc means “for this purpose” - a method designed for a specific problem - and not based on a generalizable principle."
  },
  {
    "objectID": "chpt01-intro.html#econometric-terms",
    "href": "chpt01-intro.html#econometric-terms",
    "title": "1  Introduction",
    "section": "1.4 Econometric Terms",
    "text": "1.4 Econometric Terms\nIn a typical application, an econometrician has a set of repeated measurements on a set of variables. For example, in a labor application the variables could include weekly earnings, educational attainment, age, and other descriptive characteristics. We call this information the data, dataset, or sample.\nWe use the term observations to refer to distinct repeated measurements on the variables. An individual observation often corresponds to a specific economic unit, such as a person, household, corporation, firm, organization, country, state, city or other geographical region. An individual observation could also be a measurement at a point in time, such as quarterly GDP or a daily interest rate.\nEconomists typically denote variables by the italicized roman characters \\(Y, X\\), and/or \\(Z\\). The convention in econometrics is to use the character \\(Y\\) to denote the variable to be explained, while the characters \\(X\\) and \\(Z\\) are used to denote the conditioning (explaining) variables. Following mathematical practice, random variables and vectors are denoted by upper case roman characters such as \\(Y\\) and \\(X\\). We make an exception for equation errors which we typically denote by the lower case letters \\(e, u\\), or \\(v\\).\nReal numbers (elements of the real line \\(\\mathbb{R}\\), also called scalars) are written using lower case italics such as \\(x\\). Vectors (elements of \\(\\mathbb{R}^{k}\\) ) are typically also written using lower case italics such as \\(x\\), or using lower case bold italics such as \\(\\boldsymbol{x}\\). We use bold in matrix algebraic expressions for compatibility with matrix notation.\nMatrices are written using upper case bold italics such as \\(\\boldsymbol{X}\\). Our notation will not make a distinction between random and non-random matrices. Typically we use \\(\\boldsymbol{U}, \\boldsymbol{V}, \\boldsymbol{X}, \\boldsymbol{Y}, \\boldsymbol{Z}\\) to denote random matrices and use \\(\\boldsymbol{A}, \\boldsymbol{B}, \\boldsymbol{C}, \\boldsymbol{W}\\) to denote non-random matrices.\nWe denote the number of observations by the natural number \\(n\\), and subscript the variables by the index \\(i\\) to denote the individual observation, e.g. \\(Y_{i}\\). In some contexts we use indices other than \\(i\\), such as in time series applications where the index \\(t\\) is common. In panel studies we typically use the double index \\(i t\\) to refer to individual \\(i\\) at a time period \\(t\\).\nWe typically use Greek letters such as \\(\\beta, \\theta\\), and \\(\\sigma^{2}\\) to denote unknown parameters (scalar or vectors). Parameter matrices are written using upper case Latin boldface, e.g. \\(\\boldsymbol{A}\\). Estimators are typically denoted by putting a hat ” \\(\\wedge\\) “, tilde” \\(\"\\), or bar “-” over the corresponding letter, e.g. \\(\\widehat{\\beta}\\) and \\(\\widetilde{\\beta}\\) are estimators of \\(\\beta\\), and \\(\\widehat{A}\\) is an estimator of \\(\\boldsymbol{A}\\).\nThe covariance matrix of an econometric estimator will typically be written using the upper case boldface \\(\\boldsymbol{V}\\), often with a subscript to denote the estimator, e.g. \\(\\boldsymbol{V}_{\\widehat{\\beta}}=\\operatorname{var}[\\widehat{\\beta}]\\) as the covariance matrix for \\(\\widehat{\\beta}\\). Hopefully without causing confusion, we will use the notation \\(\\boldsymbol{V}_{\\beta}=\\) avar \\([\\widehat{\\beta}]\\) to denote the asymptotic covariance matrix of \\(\\sqrt{n}(\\widehat{\\beta}-\\beta)\\) (the variance of the asymptotic distribution). Covariance matrix estimators will be denoted by appending hats or tildes, e.g. \\(\\widehat{V}_{\\beta}\\) is an estimator of \\(\\boldsymbol{V}_{\\beta}\\)."
  },
  {
    "objectID": "chpt01-intro.html#observational-data",
    "href": "chpt01-intro.html#observational-data",
    "title": "1  Introduction",
    "section": "1.5 Observational Data",
    "text": "1.5 Observational Data\nA common econometric question is to quantify the causal impact of one set of variables on another variable. For example, a concern in labor economics is the returns to schooling - the change in earnings induced by increasing a worker’s education, holding other variables constant. Another issue of interest is the earnings gap between men and women.\nIdeally, we would use experimental data to answer these questions. To measure the returns to schooling, an experiment might randomly divide children into groups, mandate different levels of education to the different groups, and then follow the children’s wage path after they mature and enter the labor force. The differences between the groups would be direct measurements of the effects of different levels of education. However, experiments such as this would be widely condemned as immoral! Consequently, in economics experimental data sets are typically narrow in scope. Instead, most economic data is observational. To continue the above example, through data collection we can record the level of a person’s education and their wage. With such data we can measure the joint distribution of these variables and assess their joint dependence. But from observational data it is difficult to infer causality as we are not able to manipulate one variable to see the direct effect on the other. For example, a person’s level of education is (at least partially) determined by that person’s choices. These factors are likely to be affected by their personal abilities and attitudes towards work. The fact that a person is highly educated suggests a high level of ability, which suggests a high relative wage. This is an alternative explanation for an observed positive correlation between educational levels and wages. High ability individuals do better in school, and therefore choose to attain higher levels of education, and their high ability is the fundamental reason for their high wages. The point is that multiple explanations are consistent with a positive correlation between schooling levels and education. Knowledge of the joint distribution alone may not be able to distinguish between these explanations.\nMost economic data sets are observational, not experimental. This means that all variables must be treated as random and possibly jointly determined.\nThis discussion means that it is difficult to infer causality from observational data alone. Causal inference requires identification, and this is based on strong assumptions. We will discuss these issues on occasion throughout the text."
  },
  {
    "objectID": "chpt01-intro.html#standard-data-structures",
    "href": "chpt01-intro.html#standard-data-structures",
    "title": "1  Introduction",
    "section": "1.6 Standard Data Structures",
    "text": "1.6 Standard Data Structures\nThere are five major types of economic data sets: cross-sectional, time series, panel, clustered, and spatial. They are distinguished by the dependence structure across observations.\nCross-sectional data sets have one observation per individual. Surveys and administrative records are a typical source for cross-sectional data. In typical applications, the individuals surveyed are persons, households, firms, or other economic agents. In many contemporary econometric cross-section studies the sample size \\(n\\) is quite large. It is conventional to assume that cross-sectional observations are mutually independent. Most of this text is devoted to the study of cross-section data.\nTime series data are indexed by time. Typical examples include macroeconomic aggregates, prices, and interest rates. This type of data is characterized by serial dependence. Most aggregate economic data is only available at a low frequency (annual, quarterly, or monthly) so the sample size is typically much smaller than in cross-section studies. An exception is financial data where data are available at a high frequency (daily, hourly, or by transaction) so sample sizes can be quite large.\nPanel data combines elements of cross-section and time series. These data sets consist of a set of individuals (typically persons, households, or corporations) measured repeatedly over time. The common modeling assumption is that the individuals are mutually independent of one another, but a given individual’s observations are mutually dependent. In some panel data contexts the number of time series observations \\(T\\) per individual is small while the number of individuals \\(n\\) is large. In other panel data contexts (for example when countries or states are taken as the unit of measurement) the number of individuals \\(n\\) can be small while the number of time series observations \\(T\\) can be moderately large. An important issue in econometric panel data is the treatment of error components.\nClustered samples are increasing popular in applied economics and are related to panel data. In clustered sampling the observations are grouped into “clusters” which are treated as mutually independent yet allowed to be dependent within the cluster. The major difference with panel data is that clustered sampling typically does not explicitly model error component structures, nor the dependence within clusters, but rather is concerned with inference which is robust to arbitrary forms of within-cluster correlation.\nSpatial dependence is another model of interdependence. The observations are treated as mutually dependent according to a spatial measure (for example, geographic proximity). Unlike clustering, spatial models allow all observations to be mutually dependent, and typically rely on explicit modeling of the dependence relationships. Spatial dependence can also be viewed as a generalization of time series dependence."
  },
  {
    "objectID": "chpt01-intro.html#data-structures",
    "href": "chpt01-intro.html#data-structures",
    "title": "1  Introduction",
    "section": "1.7 Data Structures",
    "text": "1.7 Data Structures\n\nCross-section\nTime-series\nPanel\nClustered\nSpatial\n\nAs we mentioned above, most of this text will be devoted to cross-sectional data under the assumption of mutually independent observations. By mutual independence we mean that the \\(i^{t h}\\) observation \\(\\left(Y_{i}, X_{i}\\right)\\) is independent of the \\(j^{t h}\\) observation \\(\\left(Y_{j}, X_{j}\\right)\\) for \\(i \\neq j\\). In this case we say that the data are independently distributed. (Sometimes the label “independent” is misconstrued. It is a statement about the relationship between observations \\(i\\) and \\(j\\), not a statement about the relationship between \\(Y_{i}\\) and \\(X_{i}\\).)\nFurthermore, if the data is randomly gathered, it is reasonable to model each observation as a draw from the same probability distribution. In this case we say that the data are identically distributed. If the observations are mutually independent and identically distributed, we say that the observations are independent and identically distributed, i.i.d., or a random sample. For most of this text we will assume that our observations come from a random sample.\nDefinition 1.1 The variables \\(\\left(Y_{i}, X_{i}\\right)\\) are a sample from the distribution \\(F\\) if they are identically distributed with distribution \\(F\\).\nDefinition 1.2 The variables \\(\\left(Y_{i}, X_{i}\\right)\\) are a random sample if they are mutually independent and identically distributed (i.i.d.) across \\(i=1, \\ldots, n\\).\nIn the random sampling framework, we think of an individual observation \\(\\left(Y_{i}, X_{i}\\right)\\) as a realization from a joint probability distribution \\(F(y, x)\\) which we call the population. This “population” is infinitely large. This abstraction can be a source of confusion as it does not correspond to a physical population in the real world. It is an abstraction because the distribution \\(F\\) is unknown, and the goal of statistical inference is to learn about features of \\(F\\) from the sample. The assumption of random sampling provides the mathematical foundation for treating economic statistics with the tools of mathematical statistics.\nThe random sampling framework was a major intellectual breakthrough of the late 19th century, allowing the application of mathematical statistics to the social sciences. Before this conceptual development, methods from mathematical statistics had not been applied to economic data as the latter was viewed as non-random. The random sampling framework enabled economic samples to be treated as random, a necessary precondition for the application of statistical methods."
  },
  {
    "objectID": "chpt01-intro.html#econometric-software",
    "href": "chpt01-intro.html#econometric-software",
    "title": "1  Introduction",
    "section": "1.8 Econometric Software",
    "text": "1.8 Econometric Software\nEconomists use a variety of econometric, statistical, and programming software.\nStata is a powerful statistical program with a broad set of pre-programmed econometric and statistical tools. It is quite popular among economists, and is continuously being updated with new methods. It is an excellent package for most econometric analysis, but is limited when you want to use new or lesscommon econometric methods which have not yet been programed. At many points in this textbook specific Stata estimation methods and commands are described. These commands are valid for Stata version \\(16 .\\)\nMATLAB, GAUSS, and OxMetrics are high-level matrix programming languages with a wide variety of built-in statistical functions. Many econometric methods have been programed in these languages and are available on the web. The advantage of these packages is that you are in complete control of your analysis, and it is easier to program new methods than in Stata. Some disadvantages are that you have to do much of the programming yourself, programming complicated procedures takes significant time, and programming errors are hard to prevent and difficult to detect and eliminate. Of these languages, GAUSS used to be quite popular among econometricians, but currently MATLAB is more popular.\nAn intermediate choice is R. R has the capabilities of the above high-level matrix programming languages, but also has many built-in statistical environments which can replicate much of the functionality of Stata. R is the dominant programming language in the statistics field, so methods developed in that arena are most commonly available in R. Uniquely, R is open-source, user-contributed, and best of all, completely free! A growing group of econometricians are enthusiastic fans of \\(R\\).\nFor highly-intensive computational tasks, some economists write their programs in a standard programming language such as Fortran or C. This can lead to major gains in computational speed, at the cost of increased time in programming and debugging.\nThere are many other packages which are used by econometricians, include Eviews, Gretl, PcGive, Python, Julia, RATS, and SAS.\nAs the packages described above have distinct advantages many empirical economists use multiple packages. As a student of econometrics you will learn at least one of these packages and probably more than one. My advice is that all students of econometrics should develop a basic level of familiarity with Stata, MATLAB, and R."
  },
  {
    "objectID": "chpt01-intro.html#replication",
    "href": "chpt01-intro.html#replication",
    "title": "1  Introduction",
    "section": "1.9 Replication",
    "text": "1.9 Replication\nScientific research needs to be documented and replicable. For social science research using observational data this requires careful documentation and archiving of the research methods, data manipulations, and coding. The best practice is as follows. Accompanying each published paper an author should create a complete replication package (set of data files, documentation, and program code files). This package should contain the source (raw) data used for analysis, and code which executes the empirical analysis and other numerical work reported in the paper. In most cases this is a set of programs which may need to be executed sequentially. (For example, there may be an initial program which “cleans” and manipulates the data, and then a second set of programs which estimate the reported models.) The ideal is full documentation and clarity. This package should be posted on the author(s) website, and posted at the journal website when that is an option.\nA complicating factor is that many current economic data sets have restricted access and cannot be shared without permission. In these cases the data cannot be posted nor shared. The computed code, however, can and should be posted.\nMost journals in economics require authors of published papers to make their datasets generally available. For example:"
  },
  {
    "objectID": "chpt01-intro.html#econometrica-states",
    "href": "chpt01-intro.html#econometrica-states",
    "title": "1  Introduction",
    "section": "1.10 Econometrica states:",
    "text": "1.10 Econometrica states:\nEconometrica has the policy that all empirical, experimental and simulation results must be replicable. Therefore, authors of accepted papers must submit data sets, programs, and information on empirical analysis, experiments and simulations that are needed for replication and some limited sensitivity analysis.\nThe American Economic Review states:\nIt is the policy of the American Economic Association to publish papers only if the data and code used in the analysis are clearly and precisely documented and access to the data and code is non-exclusive to the authors. Authors of accepted papers that contain empirical work, simulations, or experimental work must provide, prior to acceptance, information about the data, programs, and other details of the computations sufficient to permit replication, as well as information about access to data and programs.\nThe Journal of Political Economy states:\nIt is the policy of the Journal of Political Economy to publish papers only if the data used in the analysis are clearly and precisely documented and are readily available to any researcher for purposes of replication.\nIf you are interested in using the data from a published paper, first check the journal’s website, as many journals archive data and replication programs online. Second, check the website(s) of the paper’s author(s). Most academic economists maintain webpages, and some make available replication files complete with data and programs. If these investigations fail, email the author(s), politely requesting the data. You may need to be persistent.\nAs a matter of professional etiquette, all authors absolutely have the obligation to make their data and programs available. Unfortunately, many fail to do so, and typically for poor reasons. The irony of the situation is that it is typically in the best interests of a scholar to make as much of their work (including all data and programs) freely available, as this only increases the likelihood of their work being cited and having an impact.\nKeep this in mind as you start your own empirical project. Remember that as part of your end product, you will need (and want) to provide all data and programs to the community of scholars. The greatest form of flattery is to learn that another scholar has read your paper, wants to extend your work, or wants to use your empirical methods. In addition, public openness provides a healthy incentive for transparency and integrity in empirical analysis."
  },
  {
    "objectID": "chpt01-intro.html#data-files-for-textbook",
    "href": "chpt01-intro.html#data-files-for-textbook",
    "title": "1  Introduction",
    "section": "1.11 Data Files for Textbook",
    "text": "1.11 Data Files for Textbook\nOn the textbook webpage bhansen/econometrics/ there are posted a number of files containing data sets which are used in this textbook both for illustration and for end-ofchapter empirical exercises. For most of the data sets there are four files: (1) Description (pdf format); (2) Excel data file; (3) Text data file; (4) Stata data file. The three data files are identical in content: the observations and variables are listed in the same order in each, and all have variable labels.\nFor example, the text makes frequent reference to a wage data set extracted from the Current Population Survey. This data set is named cps09mar, and is represented by the files cps09mar_description.pdf, cps09mar.xlsx, cps09mar .txt, and cps09mar. dta.\nThe data sets currently included are\n\nAB1991\nData file from Arellano and Bond (1991)\nAJR2001\nData file from Acemoglu, Johnson, and Robinson (2001)\nAK1991\nData file from Angrist and Krueger (1991)\nAL1999\nData file from Angrist and Lavy (1999)\nBMN2016\nData file from Bernheim, Meer and Novarro (2016)\ncps09mar\nhousehold survey data extracted from the March 2009 Current Population Survey\nCard1995\nData file from Card (1995)\nCHJ2004\nData file from Cox, B. E. Hansen and Jimenez (2004)\nCK1994\nData file from Card and Krueger (1994)\nCMR2008\nDate file from Card, Mas, and Rothstein (2008) - DDK2011\nData file from Duflo, Dupas, and Kremer (2011)\nDS2004\nData file from DiTella and Schargrodsky (2004)\nFRED-MD and FRED-QD\nU.S. monthly and quarterly macroeconomic databases from McCracken and Ng (2015)\nInvest1993\nData file from Hall and Hall (1993)\nLM2007\nData file from Ludwig and Miller (2007) and Cattaneo, Titiunik, and Vazquez-Bare (2017)\nKilian2009\nData file from Kilian (2009)\nKoppelman\nData file from Forinash and Koppelman (1993), Koppelman and Wen (2000) and Wen and Koppelman (2001)\nMRW1992\nData file from Mankiw, Romer, and Weil (1992)\nNerlove1963\nData file from Nerlov (1963)\nPSS2017\nData file from Papageorgiou, Saam, and Schulte (2017)\nRR2010\nData file from Reinhard and Rogoff (2010)"
  },
  {
    "objectID": "chpt01-intro.html#reading-the-manuscript",
    "href": "chpt01-intro.html#reading-the-manuscript",
    "title": "1  Introduction",
    "section": "1.12 Reading the Manuscript",
    "text": "1.12 Reading the Manuscript\nI have endeavored to use a unified notation and nomenclature. The development of the material is cumulative, with later chapters building on the earlier ones. Nevertheless, every attempt has been made to make each chapter self-contained so readers can pick and choose topics according to their interests.\nTo fully understand econometric methods it is necessary to have a mathematical understanding of its mechanics, and this includes the mathematical proofs of the main results. Consequently, this text is selfcontained with nearly all results proved with full mathematical rigor. The mathematical development and proofs aim at brevity and conciseness (sometimes described as mathematical elegance), but also at pedagogy. To understand a mathematical proof it is not sufficient to simply read the proof, you need to follow it and re-create it for yourself.\nNevertheless, many readers will not be interested in each mathematical detail, explanation, or proof. This is okay. To use a method it may not be necessary to understand the mathematical details. Accordingly I have placed the more technical mathematical proofs and details in chapter appendices. These appendices and other technical sections are marked with an asterisk \\(\\left(^{*}\\right)\\). These sections can be skipped without any loss in exposition.\nKey concepts in matrix algebra and a set of useful inequalities are reviewed in Appendices A & B. It may be useful to read or review Appendix A.1-A.11 before starting Chapter 3, and review Appendix B before Chapter 6 . It is not necessary to understand all the material in the appendices. They are intended to be reference material and some of the results are not used in this textbook."
  },
  {
    "objectID": "chpt01-intro-chn.html#什么是计量经济学",
    "href": "chpt01-intro-chn.html#什么是计量经济学",
    "title": "介绍",
    "section": "什么是计量经济学？",
    "text": "什么是计量经济学？\n“计量经济学”一词被认为是由挪威的 Ragnar Frisch (1895-1973) 创造的，他是计量经济学会的三位主要创始人之一，《计量经济学》杂志的第一任编辑，也是第一届诺贝尔纪念奖的共同获得者1969 年获得经济学博士学位。因此，我们在《计量经济学》第一期的导言中用弗里施自己的话来描述这门学科是合适的。\n关于计量经济学一词的解释可能是有序的。它的定义隐含在[计量经济学]社会范围的声明中，在宪法第一节中写道：“计量经济学会是一个国际社会，旨在促进经济理论与统计和数学的关系…… .. 其主要目的是促进旨在统一理论-定量和经验-定量方法来解决经济问题的研究….”\n但是，经济学的定量方法有几个方面，其中任何一个方面，就其本身而言，不应与计量经济学相混淆。因此，计量经济学绝不等同于经济统计。它也不等同于我们所谓的一般经济理论，尽管该理论的相当一部分具有明确的数量特征。计量经济学也不应被视为将数学应用于经济学的同义词。经验表明，统计学、经济理论和数学这三种观点中的每一种都是真正理解现代经济生活中的数量关系的必要条件，但其本身并不是充分条件。这三者的统一才是强大的。正是这种统一构成了计量经济学。\nRagnar Frisch, Econometrica, (1933), 1, pp. 1-2。\n这个定义在今天仍然有效，尽管一些术语在使用上有所演变。今天，我们会说计量经济学是对经济模型、数理统计和经济数据的统一研究。\n在计量经济学领域内有细分和专业。计量经济学理论涉及工具和方法的发展，以及对计量经济学方法性质的研究。应用计量经济学是描述定量经济模型的发展以及使用经济数据将计量经济学方法应用于这些模型的术语。"
  },
  {
    "objectID": "chpt01-intro-chn.html#计量经济学的概率方法",
    "href": "chpt01-intro-chn.html#计量经济学的概率方法",
    "title": "介绍",
    "section": "计量经济学的概率方法",
    "text": "计量经济学的概率方法\n现代计量经济学的统一方法论由挪威的 Trygve Haavelmo (1911-1999) 提出，他是 1989 年诺贝尔经济学奖得主，在他的开创性论文“计量经济学中的概率方法”（1944 年）中。 Haavelmo 认为定量经济模型必须是概率模型（今天我们指的是随机模型）。确定性模型与观察到的经济量明显不一致，将确定性模型应用于非确定性数据是不连贯的。应明确设计经济模型以纳入随机性；不应该简单地将随机误差添加到确定性模型中以使其随机化。一旦我们承认一个经济模型是一个概率模型，很自然地，对经济进行量化、估计和进行推断的适当工具方法就是通过强大的数理统计理论。定量经济分析的适当方法来自经济模型的概率构建。\nHaavelmo 的概率方法很快被经济学界所接受。今天，经济学中的任何量化工作都不会回避其基本愿景。\n虽然所有经济学家都接受概率方法，但它的实施已经发生了一些演变。\n结构方法最接近 Haavelmo 最初的想法。指定概率经济模型，并在正确指定经济模型的假设下进行定量分析。研究人员经常将其描述为“认真对待他们的模型”。结构方法通常导致基于似然的分析，包括最大似然和贝叶斯估计。\n对结构性方法的批评是，将经济模型视为正确指定是一种误导。相反，将模型视为有用的抽象或近似更为准确。在这种情况下，我们应该如何解释结构计量经济分析？推理的准结构方法将结构经济模型视为近似值而不是事实。该理论导致了伪真值（由估计问题定义的参数值）、拟似然函数、拟 MLE 和拟似然推理的概念。\n密切相关的是半参数方法。概率经济模型已部分指定，但某些特征未指定。这种方法通常会导致估计方法，例如最小二乘法和广义矩量法。半参数方法在当代计量经济学中占主导地位，是这本教科书的主要焦点。\n数量结构经济学的另一个分支是校准方法。与准结构方法类似，校准方法将结构模型解释为近似值，因此本质上是错误的。不同之处在于校准主义文献拒绝数学统计（认为经典理论不适用于近似模型），而是使用非统计的 \\(a d h o c^{1}\\) 方法通过匹配模型和数据矩来选择参数。"
  },
  {
    "objectID": "chpt01-intro-chn.html#特里格维哈维尔莫",
    "href": "chpt01-intro-chn.html#特里格维哈维尔莫",
    "title": "介绍",
    "section": "特里格维·哈维尔莫",
    "text": "特里格维·哈维尔莫\n计量经济学领域的创始思想很大程度上归功于挪威计量经济学家 Trygve Haavelmo (1911-1999)。他对概率模型的倡导彻底改变了该领域，他对形式数学推理的使用为后代奠定了基础。他在 \\(1989 .\\) 获得诺贝尔经济学奖\n\\({ }^{1}\\) Ad hoc 的意思是“为此目的”——一种为特定问题设计的方法——而不是基于可概括的原则。"
  },
  {
    "objectID": "chpt01-intro-chn.html#计量经济学术语",
    "href": "chpt01-intro-chn.html#计量经济学术语",
    "title": "介绍",
    "section": "计量经济学术语",
    "text": "计量经济学术语\n在典型应用中，计量经济学家对一组变量进行一组重复测量。例如，在劳工申请中，变量可能包括周收入、教育程度、年龄和其他描述性特征。我们称这些信息为数据、数据集或样本。\n我们使用术语观察来指代对变量的不同重复测量。个人观察通常对应于特定的经济单位，例如个人、家庭、公司、公司、组织、国家、州、城市或其他地理区域。个人观察也可以是某个时间点的衡量指标，例如季度 GDP 或每日利率。\n经济学家通常用斜体罗马字符 \\(Y, X\\) 和/或 \\(Z\\) 来表示变量。计量经济学的惯例是使用字符 \\(Y\\) 来表示要解释的变量，而字符 \\(X\\) 和 \\(Z\\) 用于表示条件（解释）变量。按照数学实践，随机变量和向量用大写罗马字符表示，例如 \\(Y\\) 和 \\(X\\)。我们对通常用小写字母 \\(e, u\\) 或 \\(v\\) 表示的方程错误例外。\n实数（实线 \\(\\mathbb{R}\\) 的元素，也称为标量）使用小写斜体书写，例如 \\(x\\)。向量（\\(\\mathbb{R}^{k}\\) 的元素）通常也使用小写斜体，例如 \\(x\\)，或使用小写粗斜体，例如 \\(\\boldsymbol{x}\\)。我们在矩阵代数表达式中使用粗体来与矩阵表示法兼容。\n矩阵使用大写粗斜体书写，例如 \\(\\boldsymbol{X}\\)。我们的符号不会区分随机矩阵和非随机矩阵。通常我们使用 \\(\\boldsymbol{U}, \\boldsymbol{V}, \\boldsymbol{X}, \\boldsymbol{Y}, \\boldsymbol{Z}\\) 来表示随机矩阵并使用 \\(\\boldsymbol{A}, \\boldsymbol{B}, \\boldsymbol{C}, \\boldsymbol{W}\\) 来表示非随机矩阵。\n我们用自然数 \\(n\\) 表示观察的数量，并用索引 \\(i\\) 为变量下标以表示单个观察，例如\\(Y_{i}\\)。在某些情况下，我们使用 \\(i\\) 以外的索引，例如在索引 \\(t\\) 很常见的时间序列应用程序中。在小组研究中，我们通常使用双重索引 \\(i t\\) 来指代某个时间段 \\(t\\) 的个体 \\(i\\)。\n我们通常使用希腊字母，例如 \\(\\beta, \\theta\\) 和 \\(\\sigma^{2}\\) 来表示未知参数（标量或向量）。参数矩阵使用大写拉丁黑体书写，例如\\(\\boldsymbol{A}\\)。估计量通常用帽子“\\(\\wedge\\)”、波浪号“\\(\"\\)”或横杠“-”来表示，例如，\\(\\widehat{\\beta}\\) 和 \\(\\widetilde{\\beta}\\) 是 \\(\\beta\\) 的估计量，\\(\\widehat{A}\\) 是\\(\\beta, \\theta\\) 的估计量。\n计量经济学估计量的协方差矩阵通常使用大写粗体 \\(\\boldsymbol{V}\\) 编写，通常带有下标来表示估计量，例如\\(\\boldsymbol{V}_{\\widehat{\\beta}}=\\operatorname{var}[\\widehat{\\beta}]\\) 作为 \\(\\widehat{\\beta}\\) 的协方差矩阵。希望不会引起混淆，我们将使用符号 \\(\\boldsymbol{V}_{\\beta}=\\) avar \\([\\widehat{\\beta}]\\) 来表示 \\(\\sqrt{n}(\\widehat{\\beta}-\\beta)\\) 的渐近协方差矩阵（渐近分布的方差）。协方差矩阵估计器将通过附加帽子或波浪线来表示，例如\\(\\widehat{V}_{\\beta}\\) 是 \\(\\boldsymbol{V}_{\\beta}\\) 的估计量。"
  },
  {
    "objectID": "chpt01-intro-chn.html#观测数据",
    "href": "chpt01-intro-chn.html#观测数据",
    "title": "介绍",
    "section": "观测数据",
    "text": "观测数据\n一个常见的计量经济学问题是量化一组变量对另一变量的因果影响。例如，劳动经济学的一个关注点是学校教育的回报——在保持其他变量不变的情况下，增加工人的教育所引起的收入变化。另一个有趣的问题是男女之间的收入差距。\n理想情况下，我们会使用实验数据来回答这些问题。为了衡量学校教育的回报，一项实验可能会将儿童随机分组，要求不同的群体接受不同程度的教育，然后在儿童成年并进入劳动力市场后遵循他们的工资路径。各组之间的差异将直接衡量不同教育水平的影响。然而，像这样的实验会被广泛谴责为不道德的！因此，在经济学中，实验数据集的范围通常很窄。相反，大多数经济数据都是观察性的。继续上面的例子，通过数据收集我们可以记录一个人的教育水平和工资。有了这些数据，我们可以测量这些变量的联合分布并评估它们的联合依赖性。但是从观察数据很难推断因果关系，因为我们无法操纵一个变量来查看对另一个变量的直接影响。例如，一个人的教育水平（至少部分地）由该人的选择决定。这些因素很可能会受到个人能力和工作态度的影响。一个人受过高等教育的事实表明能力水平高，这表明相对工资较高。这是对观察到的教育水平和工资之间正相关的另一种解释。能力强的人在学校表现更好，因此选择接受更高层次的教育，能力强是他们获得高工资的根本原因。关键是，多种解释与受教育程度与教育之间的正相关是一致的。仅对联合分布的了解可能无法区分这些解释。\n大多数经济数据集是观察性的，而不是实验性的。这意味着必须将所有变量视为随机变量，并可能共同确定。\n这种讨论意味着仅从观测数据很难推断出因果关系。因果推理需要识别，这是基于强有力的假设。我们将在整本书中不时讨论这些问题。"
  },
  {
    "objectID": "chpt01-intro-chn.html#标准数据结构",
    "href": "chpt01-intro-chn.html#标准数据结构",
    "title": "介绍",
    "section": "标准数据结构",
    "text": "标准数据结构\n经济数据集有五种主要类型：横截面、时间序列、面板、集群和空间。它们的区别在于观察之间的依赖结构。\n横截面数据集每个人有一个观察结果。调查和行政记录是横截面数据的典型来源。在典型应用中，被调查的个人是个人、家庭、公司或其他经济主体。在许多当代计量经济学横截面研究中，样本量 \\(n\\) 非常大。通常假设横截面观察是相互独立的。本书的大部分内容都致力于研究横截面数据。\n时间序列数据按时间索引。典型的例子包括宏观经济总量、价格和利率。这种类型的数据的特点是串行依赖。大多数综合经济数据只能以低频率（每年、每季度或每月）获得，因此样本量通常比横截面研究小得多。一个例外是财务数据，其中数据的可用频率很高（每天、每小时或按交易），因此样本量可能非常大。\n面板数据结合了横截面和时间序列的元素。这些数据集由一组随时间重复测量的个人（通常是个人、家庭或公司）组成。常见的建模假设是个体相互独立，但给定个体的观察是相互依赖的。在某些面板数据上下文中，每个个体的时间序列观察 \\(T\\) 的数量很少，而个体 \\(n\\) 的数量很大。在其他面板数据环境中（例如，当以国家或州作为衡量单位时），个体数量 \\(n\\) 可能很小，而时间序列观测值 \\(T\\) 的数量可能适中。计量经济学面板数据中的一个重要问题是误差分量的处理。\n聚类样本在应用经济学中越来越流行，并且与面板数据相关。在集群抽样中，观察被分组为“集群”，这些集群被视为相互独立但允许在集群内依赖。与面板数据的主要区别在于，聚类抽样通常不会显式地对误差分量结构进行建模，也不会对聚类内的依赖性进行建模，而是关注对任意形式的聚类内相关性具有鲁棒性的推理。\n空间依赖是相互依赖的另一种模式。根据空间度量（例如，地理接近度），观测被视为相互依赖。与聚类不同，空间模型允许所有观察结果相互依赖，并且通常依赖于依赖关系的显式建模。空间依赖也可以看作是时间序列依赖的概括。"
  },
  {
    "objectID": "chpt01-intro-chn.html#数据结构",
    "href": "chpt01-intro-chn.html#数据结构",
    "title": "介绍",
    "section": "数据结构",
    "text": "数据结构\n\n横截面\n时间序列\n控制板\n集群\n空间\n\n正如我们上面提到的，本文的大部分内容将在相互独立的观察假设下专门讨论横截面数据。通过相互独立，我们的意思是 \\(i^{t h}\\) 观察 \\(\\left(Y_{i}, X_{i}\\right)\\) 独立于 \\(i \\neq j\\) 的 \\(j^{t h}\\) 观察 \\(\\left(Y_{j}, X_{j}\\right)\\)。在这种情况下，我们说数据是独立分布的。 （有时“独立”这个标签会被误解。它是关于观察 \\(i\\) 和 \\(j\\) 之间关系的陈述，而不是关于 \\(Y_{i}\\) 和 \\(X_{i}\\) 之间关系的陈述。）\n此外，如果数据是随机收集的，则将每个观察值建模为来自相同概率分布的抽取是合理的。在这种情况下，我们说数据是同分布的。如果观察是相互独立且同分布的，我们称这些观察是独立同分布的，i.i.d.，或随机样本。对于本文的大部分内容，我们将假设我们的观察来自随机样本。\n定义 1.1 如果变量 \\(\\left(Y_{i}, X_{i}\\right)\\) 与分布 \\(F\\) 同分布，则变量 \\(\\left(Y_{i}, X_{i}\\right)\\) 是分布 \\(F\\) 的样本。\n定义 1.2 如果变量 \\(\\left(Y_{i}, X_{i}\\right)\\) 在 \\(i=1, \\ldots, n\\) 上相互独立且同分布（i.i.d.），则它们是随机样本。\n在随机抽样框架中，我们将单个观察 \\(\\left(Y_{i}, X_{i}\\right)\\) 视为来自我们称为总体的联合概率分布 \\(F(y, x)\\) 的实现。这个“人口”是无限大的。这种抽象可能会造成混淆，因为它与现实世界中的物理人口不对应。这是一个抽象，因为分布 \\(F\\) 是未知的，而统计推断的目标是从样本中了解 \\(F\\) 的特征。随机抽样的假设为用数理统计工具处理经济统计提供了数学基础。\n随机抽样框架是 19 世纪后期的一项重大智力突破，允许将数理统计应用于社会科学。在这一概念发展之前，数理统计方法并未应用于经济数据，因为后者被视为非随机的。随机抽样框架使经济样本能够被视为随机样本，这是应用统计方法的必要前提。"
  },
  {
    "objectID": "chpt01-intro-chn.html#计量经济学软件",
    "href": "chpt01-intro-chn.html#计量经济学软件",
    "title": "介绍",
    "section": "计量经济学软件",
    "text": "计量经济学软件\n经济学家使用各种计量、统计和编程软件。\nStata 是一个强大的统计程序，具有广泛的预编程计量经济学和统计工具。它在经济学家中颇受欢迎，并不断更新新方法。对于大多数计量经济分析来说，它是一个出色的软件包，但是当您想要使用尚未编程的新的或不太常见的计量经济学方法时，它会受到限制。在这本教科书的许多地方都描述了特定的 Stata 估计方法和命令。这些命令对 Stata 版本 \\(16 .\\) 有效\nMATLAB、GAUSS 和 OxMetrics 是具有多种内置统计函数的高级矩阵编程语言。许多计量经济学方法已经用这些语言编写，并且可以在网上获得。这些软件包的优点是您可以完全控制您的分析，并且比在 Stata 中编写新方法更容易。一些缺点是您必须自己进行大量编程，编写复杂的程序需要大量时间，并且编程错误难以预防，难以检测和消除。在这些语言中，GAUSS 曾经在计量经济学家中非常流行，但目前 MATLAB 更流行。\n中间的选择是 R。R 具有上述高级矩阵编程语言的功能，但也有许多内置的统计环境，可以复制 Stata 的大部分功能。 R 是统计领域的主要编程语言，因此在该领域开发的方法在 R 中最常见。独特的是，R 是开源的、用户贡献的，最重要的是，完全免费！越来越多的计量经济学家是 \\(R\\) 的狂热粉丝。\n对于高度密集的计算任务，一些经济学家使用标准编程语言（如 Fortran 或 C）编写程序。这可能会大大提高计算速度，但会增加编程和调试的时间。\n计量经济学家还使用了许多其他软件包，包括 Eviews、Gretl、PcGive、Python、Julia、RATS 和 SAS。\n由于上述软件包具有明显的优势，许多经验经济学家使用多个软件包。作为计量经济学的学生，您将至少学习这些软件包中的一个，并且可能不止一个。我的建议是所有计量经济学的学生都应该对 Stata、MATLAB 和 R 有一定的了解。"
  },
  {
    "objectID": "chpt01-intro-chn.html#复制",
    "href": "chpt01-intro-chn.html#复制",
    "title": "介绍",
    "section": "复制",
    "text": "复制\n科学研究需要记录和可复制。对于使用观察数据的社会科学研究，这需要仔细记录和归档研究方法、数据操作和编码。最佳实践如下。伴随每篇发表的论文，作者应创建一个完整的复制包（一组数据文件、文档和程序代码文件）。这个包应该包含用于分析的源（原始）数据，以及执行经验分析和论文中报告的其他数值工作的代码。在大多数情况下，这是一组可能需要按顺序执行的程序。 （例如，可能有一个“清理”和操作数据的初始程序，然后是估计报告模型的第二组程序。）理想的是完整的文档和清晰性。该文件包应发布在作者网站上，并在可以选择的情况下发布在期刊网站上。\n一个复杂的因素是，当前许多经济数据集的访问受到限制，未经许可不得共享。在这些情况下，无法发布或共享数据。然而，计算出的代码可以而且应该发布。\n大多数经济学期刊都要求已发表论文的作者公开其数据集。例如："
  },
  {
    "objectID": "chpt01-intro-chn.html#计量经济学指出",
    "href": "chpt01-intro-chn.html#计量经济学指出",
    "title": "介绍",
    "section": "计量经济学指出：",
    "text": "计量经济学指出：\nEconometrica 的政策是所有经验、实验和模拟结果都必须是可复制的。因此，被接受论文的作者必须提交数据集、程序和有关复制所需的经验分析、实验和模拟的信息以及一些有限的敏感性分析。\n美国经济评论指出：\n美国经济学会的政策是，只有在分析中使用的数据和代码被清晰准确地记录并且对数据和代码的访问对作者来说是非专有的时才发表论文。包含实证工作、模拟或实验工作的已接受论文的作者必须在接受之前提供有关数据、程序和足以允许复制的其他计算细节的信息，以及有关访问数据和程序的信息。\n《政治经济学杂志》指出：\n《政治经济学杂志》的政策是，只有在分析中使用的数据得到清晰准确的记录并且任何研究人员都可以随时复制以供复制时，才发表论文。\n如果您对使用已发表论文中的数据感兴趣，请首先查看该期刊的网站，因为许多期刊在线存档数据和复制程序。其次，查看论文作者的网站。大多数学术经济学家维护网页，有些人提供包含数据和程序的完整复制文件。如果这些调查失败，请给作者发电子邮件，礼貌地索取数据。你可能需要坚持不懈。\n作为职业礼仪，所有作者绝对有义务提供他们的数据和程序。不幸的是，许多人没有这样做，而且通常是出于糟糕的原因。具有讽刺意味的是，将尽可能多的工作（包括所有数据和程序）免费提供通常符合学者的最大利益，因为这只会增加他们的工作被引用和产生影响的可能性。\n当您开始自己的经验项目时，请记住这一点。请记住，作为最终产品的一部分，您将需要（并且希望）向学者社区提供所有数据和程序。奉承的最大形式是得知另一位学者已经阅读了您的论文，想要扩展您的工作，或者想要使用您的经验方法。此外，公共开放性为实证分析的透明度和完整性提供了健康的激励。"
  },
  {
    "objectID": "chpt01-intro-chn.html#教科书数据文件",
    "href": "chpt01-intro-chn.html#教科书数据文件",
    "title": "介绍",
    "section": "教科书数据文件",
    "text": "教科书数据文件\n在教科书网页 bhansen/econometrics/ 上发布了许多包含数据集的文件，这些文件是在本书中用于说明和章末的经验练习。大多数数据集有四个文件：（1）描述（pdf格式）； (2) Excel数据文件； (3) 文本数据文件； (4) 状态数据文件。这三个数据文件的内容是相同的：观察值和变量在每个中都以相同的顺序列出，并且都有变量标签。\n例如，文本经常引用从当前人口调查中提取的工资数据集。该数据集名为 cps09mar，由文件 cps09mar_description.pdf、cps09mar.xlsx、cps09mar .txt 和 cps09mar 表示。数据。\n目前包含的数据集是\n\nAB1991\nArellano 和 Bond (1991) 的数据文件\nAJR2001\n来自 Acemoglu、Johnson 和 Robinson 的数据文件 (2001)\nAK1991\n来自 Angrist 和 Krueger 的数据文件 (1991)\nAL1999\n来自 Angrist 和 Lavy (1999) 的数据文件\nBMN2016\n来自 Bernheim、Meer 和 Novarro 的数据文件（2016 年）\ncps09mar\n从 2009 年 3 月当前人口调查中提取的住户调查数据\n卡1995\nCard 的数据文件 (1995)\nCHJ2004\n来自 Cox、B. E. Hansen 和 Jimenez 的数据文件（2004 年）\nCK1994\nCard and Krueger (1994) 的数据文件\nCMR2008\nCard、Mas 和 Rothstein 的日期文件 (2008) - DDK2011\n来自 Duflo、Dupas 和 Kremer 的数据文件 (2011)\nDS2004\nDiTella 和 Schargrodsky 的数据文件 (2004)\nFRED-MD 和 FRED-QD\n来自 McCracken 和 Ng（2015 年）的美国月度和季度宏观经济数据库\n投资1993\nHall and Hall (1993) 的数据文件\nLM2007\n来自 Ludwig 和 Miller (2007) 以及 Cattaneo、Titiunik 和 Vazquez-Bare (2017) 的数据文件\nKilian2009\nKilian 的数据文件 (2009)\n情侣男人\n数据文件来自 Forinash 和 Koppelman (1993)、Koppelman 和 Wen (2000) 以及 Wen 和 Koppelman (2001)\n\n*MRW1992\n\nMankiw、Romer 和 Weil 的数据文件 (1992)\n内洛夫1963\nNerlov (1963) 的数据文件\nPSS2017\nPapageorgiou、Saam 和 Schulte 的数据文件 (2017)\nRR2010\n来自 Reinhard 和 Rogoff 的数据文件 (2010)"
  },
  {
    "objectID": "chpt01-intro-chn.html#阅读手稿",
    "href": "chpt01-intro-chn.html#阅读手稿",
    "title": "介绍",
    "section": "阅读手稿",
    "text": "阅读手稿\n我努力使用统一的符号和命名法。材料的发展是累积的，后面的章节建立在前面的章节之上。尽管如此，我们已尽一切努力使每一章都自成一体，以便读者可以根据自己的兴趣选择主题。\n为了充分理解计量经济学方法，有必要对其力学有数学上的理解，这包括主要结果的数学证明。因此，该文本是自包含的，几乎所有结果都以完全数学严谨性证明。数学发展和证明旨在简洁明了（有时被描述为数学优雅），但也着眼于教学法。要理解数学证明，仅仅阅读证明是不够的，您需要遵循它并为自己重新创建它。\n然而，许多读者不会对每个数学细节、解释或证明感兴趣。这没关系。要使用一种方法，可能不需要了解数学细节。因此，我将更多技术性的数学证明和细节放在章节附录中。这些附录和其他技术部分标有星号 \\(\\left(^{*}\\right)\\)。可以跳过这些部分而不会丢失任何说明。\n矩阵代数中的关键概念和一组有用的不等式在附录 A 和 B 中进行了回顾。在开始第 3 章之前阅读或回顾附录 A.1-A.11 并在第 6 章之前回顾附录 B 可能会很有用。不必了解附录中的所有材料。它们旨在作为参考资料，其中一些结果未在本教科书中使用。"
  },
  {
    "objectID": "chpt02-ce.html#introduction",
    "href": "chpt02-ce.html#introduction",
    "title": "2  Conditional Expectation and Projection",
    "section": "2.1 Introduction",
    "text": "2.1 Introduction\nThe most commonly applied econometric tool is least squares estimation, also known as regression. Least squares is a tool to estimate the conditional mean of one variable (the dependent variable) given another set of variables (the regressors, conditioning variables, or covariates).\nIn this chapter we abstract from estimation and focus on the probabilistic foundation of the conditional expectation model and its projection approximation. This includes a review of probability theory. For a background in intermediate probability theory see Chapters 1-5 of Probability and Statistics for Economists."
  },
  {
    "objectID": "chpt02-ce.html#the-distribution-of-wages",
    "href": "chpt02-ce.html#the-distribution-of-wages",
    "title": "2  Conditional Expectation and Projection",
    "section": "2.2 The Distribution of Wages",
    "text": "2.2 The Distribution of Wages\nSuppose that we are interested in wage rates in the United States. Since wage rates vary across workers we cannot describe wage rates by a single number. Instead, we can describe wages using a probability distribution. Formally, we view the wage of an individual worker as a random variable wage with the probability distribution\n\\[\nF(y)=\\mathbb{P}[\\text { wage } \\leq y] .\n\\]\nWhen we say that a person’s wage is random we mean that we do not know their wage before it is measured, and we treat observed wage rates as realizations from the distribution \\(F\\). Treating unobserved wages as random variables and observed wages as realizations is a powerful mathematical abstraction which allows us to use the tools of mathematical probability.\nA useful thought experiment is to imagine dialing a telephone number selected at random, and then asking the person who responds to tell us their wage rate. (Assume for simplicity that all workers have equal access to telephones and that the person who answers your call will answer honestly.) In this thought experiment, the wage of the person you have called is a single draw from the distribution \\(F\\) of wages in the population. By making many such phone calls we can learn the full distribution.\nWhen a distribution function \\(F\\) is differentiable we define the probability density function\n\\[\nf(y)=\\frac{d}{d y} F(y) .\n\\]\nThe density contains the same information as the distribution function, but the density is typically easier to visually interpret.\n\n\nWage Density\n\n\n\nLog Wage Density\n\nFigure 2.1: Density of Wages and Log Wages\nIn Figure 2.1(a) we display an estimate \\({ }^{1}\\) of the probability density function of U.S. wage rates in \\(2009 .\\) We see that the density is peaked around \\(\\$ 15\\), and most of the probability mass appears to lie between \\(\\$ 10\\) and \\(\\$ 40\\). These are ranges for typical wage rates in the U.S. population.\nImportant measures of central tendency are the median and the mean. The median \\(m\\) of a continuous distribution \\(F\\) is the unique solution to\n\\[\nF(m)=\\frac{1}{2} .\n\\]\nThe median U.S. wage is \\(\\$ 19.23\\). The median is a robust \\({ }^{2}\\) measure of central tendency, but it is tricky to use for many calculations as it is not a linear operator.\nThe mean or expectation of a random variable \\(Y\\) with discrete support is\n\\[\n\\mu=\\mathbb{E}[Y]=\\sum_{j=1}^{\\infty} \\tau_{j} \\mathbb{P}\\left[Y=\\tau_{j}\\right] .\n\\]\nFor a continuous random variable with density \\(f(y)\\) the expectation is\n\\[\n\\mu=\\mathbb{E}[Y]=\\int_{-\\infty}^{\\infty} y f(y) d y .\n\\]\nHere we have used the common and convenient convention of using the single character \\(Y\\) to denote a random variable, rather than the more cumbersome label wage. An alternative notation which includes both discrete and continuous random variables as special cases is to write the integral as \\(\\int_{-\\infty}^{\\infty} y d F(y)\\).\nThe expectation is a convenient measure of central tendency because it is a linear operator and arises naturally in many economic models. A disadvantage of the expectation is that it is not robust \\({ }^{3}\\) especially\n\\({ }^{1}\\) The distribution and density are estimated nonparametrically from the sample of 50,742 full-time non-military wageearners reported in the March 2009 Current Population Survey. The wage rate is constructed as annual individual wage and salary earnings divided by hours worked.\n\\({ }^{2}\\) The median is not sensitive to pertubations in the tails of the distribution.\n\\({ }^{3}\\) The expectation is sensitive to pertubations in the tails of the distribution. in the presence of substantial skewness or thick tails, both which are features of the wage distribution as can be seen in Figure 2.1(a). Another way of viewing this is that \\(64 %\\) of workers earn less than the mean wage of \\(\\$ 23.90\\), suggesting that it is incorrect to describe the mean \\(\\$ 23.90\\) as a “typical” wage rate.\nIn this context it is useful to transform the data by taking the natural logarithm” \\({ }^{4}\\). Figure \\(2.1\\) (b) shows the density of \\(\\log\\) hourly wages \\(\\log (\\) wage \\()\\) for the same population. The density of log wages is less skewed and fat-tailed than the density of the level of wages, so its mean\n\\[\n\\mathbb{E}[\\log (\\text { wage })]=2.95\n\\]\nis a better (more robust) measure \\({ }^{5}\\) of central tendency of the distribution. For this reason, wage regressions typically use log wages as a dependent variable rather than the level of wages.\nAnother useful way to summarize the probability distribution \\(F(y)\\) is in terms of its quantiles. For any \\(\\alpha \\in(0,1)\\), the \\(\\alpha^{t h}\\) quantile of the continuous \\({ }^{6}\\) distribution \\(F\\) is the real number \\(q_{\\alpha}\\) which satisfies \\(F\\left(q_{\\alpha}\\right)=\\alpha\\). The quantile function \\(q_{\\alpha}\\), viewed as a function of \\(\\alpha\\), is the inverse of the distribution function \\(F\\). The most commonly used quantile is the median, that is, \\(q_{0.5}=m\\). We sometimes refer to quantiles by the percentile representation of \\(\\alpha\\) and in this case they are called percentiles. E.g. the median is the \\(50^{t h}\\) percentile."
  },
  {
    "objectID": "chpt02-ce.html#conditional-expectation",
    "href": "chpt02-ce.html#conditional-expectation",
    "title": "2  Conditional Expectation and Projection",
    "section": "2.3 Conditional Expectation",
    "text": "2.3 Conditional Expectation\nWe saw in Figure 2.1(b) the density of log wages. Is this distribution the same for all workers, or does the wage distribution vary across subpopulations? To answer this question, we can compare wage distributions for different groups - for example, men and women. To investigate, we plot in Figure \\(2.2\\) (a) the densities of log wages for U.S. men and women. We can see that the two wage densities take similar shapes but the density for men is somewhat shifted to the right.\nThe values \\(3.05\\) and \\(2.81\\) are the mean log wages in the subpopulations of men and women workers. They are called the conditional expectation (or conditional mean) of log wages given gender. We can write their specific values as\n\\[\n\\begin{gathered}\n\\mathbb{E}[\\log (\\text { wage }) \\mid \\text { gender }=\\text { man }]=3.05 \\\\\n\\mathbb{E}[\\log (\\text { wage }) \\mid \\text { gender }=\\text { woman }]=2.81 .\n\\end{gathered}\n\\]\nWe call these expectations “conditional” as they are conditioning on a fixed value of the variable gender. While you might not think of a person’s gender as a random variable, it is random from the viewpoint of econometric analysis. If you randomly select an individual, the gender of the individual is unknown and thus random. (In the population of U.S. workers, the probability that a worker is a woman happens to be \\(43 %\\).) In observational data, it is most appropriate to view all measurements as random variables, and the means of subpopulations are then conditional means.\nIt is important to mention at this point that we in no way attribute causality or interpretation to the difference in the conditional expectation of log wages between men and women. There are multiple potential explanations.\nAs the two densities in Figure 2.2(a) appear similar, a hasty inference might be that there is not a meaningful difference between the wage distributions of men and women. Before jumping to this conclusion let us examine the differences in the distributions more carefully. As we mentioned above, the\n\\({ }^{4}\\) Throughout the text, we will use \\(\\log (y)\\) or \\(\\log y\\) to denote the natural logarithm of \\(y\\).\n\\({ }^{5}\\) More precisely, the geometric mean \\(\\exp (\\mathbb{E}[\\log W])=\\$ 19.11\\) is a robust measure of central tendency.\n\\({ }^{6}\\) If \\(F\\) is not continuous the definition is \\(q_{\\alpha}=\\inf \\{y: F(y) \\geq \\alpha\\}\\)\n\n\nWomen and Men\n\n\n\nBy Gender and Race\n\nFigure 2.2: Log Wage Density by Gender and Race\nprimary difference between the two densities appears to be their means. This difference equals\n\\[\n\\begin{aligned}\n\\mathbb{E}[\\log (\\text { wage }) \\mid \\text { gender }=\\text { man }]-\\mathbb{E}[\\log (\\text { wage }) \\mid \\text { gender }=\\text { woman }] &=3.05-2.81 \\\\\n&=0.24 .\n\\end{aligned}\n\\]\nA difference in expected log wages of \\(0.24\\) is often interpreted as an average \\(24 %\\) difference between the wages of men and women, which is quite substantial. (For a more complete explanation see Section 2.4.)\nConsider further splitting the male and female subpopulations by race, dividing the population into whites, Blacks, and other races. We display the log wage density functions of four of these groups in Figure \\(2.2\\) (b). Again we see that the primary difference between the four density functions is their central tendency.\nFocusing on the means of these distributions, Table \\(2.1\\) reports the mean log wage for each of the six sub-populations.\nTable 2.1: Mean Log Wages by Gender and Race\n\n\n\n\nmen\nwomen\n\n\n\n\nwhite\n\\(3.07\\)\n\\(2.82\\)\n\n\nBlack\n\\(2.86\\)\n\\(2.73\\)\n\n\nother\n\\(3.03\\)\n\\(2.86\\)\n\n\n\nOnce again we stress that we in no way attribute causality or interpretation to the differences across the entries of the table. The reason why we use these particular sub-populations to illustrate conditional expectation is because differences in economic outcomes between gender and racial groups in the United States (and elsewhere) are widely discussed; part of the role of social science is to carefully document such patterns, and part of its role is to craft models and explanations. Conditional expectations (by themselves) can help in the documentation and description; conditional expectations by themselves are neither a model nor an explanation.\nThe entries in Table \\(2.1\\) are the conditional means of \\(\\log (\\) wage \\()\\) given gender and race. For example\n\\[\n\\mathbb{E}[\\log (\\text { wage }) \\mid \\text { gender }=\\text { man, race }=\\text { white }]=3.07\n\\]\nand\n\\[\n\\mathbb{E}[\\log (\\text { wage }) \\mid \\text { gender }=\\text { woman, race }=\\text { Black }]=2.73 \\text {. }\n\\]\nOne benefit of focusing on conditional means is that they reduce complicated distributions to a single summary measure, and thereby facilitate comparisons across groups. Because of this simplifying property, conditional means are the primary interest of regression analysis and are a major focus in econometrics.\nTable \\(2.1\\) allows us to easily calculate average wage differences between groups. For example, we can see that the wage gap between men and women continues after disaggregation by race, as the average gap between white men and white women is \\(25 %\\), and that between Black men and Black women is \\(13 %\\). We also can see that there is a race gap, as the average wages of Blacks are substantially less than the other race categories. In particular, the average wage gap between white men and Black men is \\(21 %\\), and that between white women and Black women is \\(9 %\\)."
  },
  {
    "objectID": "chpt02-ce.html#logs-and-percentages",
    "href": "chpt02-ce.html#logs-and-percentages",
    "title": "2  Conditional Expectation and Projection",
    "section": "2.4 Logs and Percentages",
    "text": "2.4 Logs and Percentages\nIn this section we want to motivate and clarify the use of the logarithm in regression analysis by making two observations. First, when applied to numbers the difference of logarithms approximately equals the percentage difference. Second, when applied to averages the difference in logarithms approximately equals the percentage difference in the geometric mean. We now explain these ideas and the nature of the approximations involved.\nTake two positive numbers \\(a\\) and \\(b\\). The percentage difference between \\(a\\) and \\(b\\) is\n\\[\np=100\\left(\\frac{a-b}{b}\\right) .\n\\]\nRewriting,\n\\[\n\\frac{a}{b}=1+\\frac{p}{100}\n\\]\nTaking natural logarithms,\n\\[\n\\log a-\\log b=\\log \\left(1+\\frac{p}{100}\\right) .\n\\]\nA useful approximation for small \\(x\\) is\n\\[\n\\log (1+x) \\simeq x .\n\\]\nThis can be derived from the infinite series expansion of \\(\\log (1+x)\\) :\n\\[\n\\log (1+x)=x-\\frac{x^{2}}{2}+\\frac{x^{3}}{3}-\\frac{x^{4}}{4}+\\cdots=x+O\\left(x^{2}\\right) .\n\\]\nThe symbol \\(O\\left(x^{2}\\right.\\) ) means that the remainder is bounded by \\(A x^{2}\\) as \\(x \\rightarrow 0\\) for some \\(A<\\infty\\). Numerically, the approximation \\(\\log (1+x) \\simeq x\\) is within \\(0.001\\) for \\(|x| \\leq 0.1\\), and the approximation error increases with \\(|x|\\)\nApplying (2.3) to (2.2) and multiplying by 100 we find\n\\[\np \\simeq 100(\\log a)-\\log b) .\n\\]\nThis shows that 100 multiplied by the difference in logarithms is approximately the percentage difference. Numerically, the approximation error is less than \\(0.1\\) percentage points for \\(|p| \\leq 10\\).\nNow consider the difference in the expectation of log transformed random variables. Take two random variables \\(X_{1}, X_{2}>0\\). Define their geometric means \\(\\theta_{1}=\\exp \\left(\\mathbb{E}\\left[\\log X_{1}\\right]\\right)\\) and \\(\\theta_{2}=\\exp \\left(\\mathbb{E}\\left[\\log X_{2}\\right]\\right)\\) and their percentage difference\n\\[\np=100\\left(\\frac{\\theta_{2}-\\theta_{1}}{\\theta_{1}}\\right) .\n\\]\nThe difference in the expectation of the log transforms (multiplied by 100) is\n\\[\n100\\left(\\mathbb{E}\\left[\\log X_{2}\\right]-\\mathbb{E}\\left[\\log X_{1}\\right]\\right)=100\\left(\\log \\theta_{2}-\\log \\theta_{1}\\right) \\simeq p\n\\]\nthe percentage difference between \\(\\theta_{2}\\) and \\(\\theta_{1}\\). In words, the difference between the average of the log transformed variables is (approximately) the percentage difference in the geometric means.\nThe reason why this latter observation is important is because many econometric equations take the semi-log form\n\\[\n\\begin{aligned}\n&\\mathbb{E}[\\log Y \\mid \\operatorname{group}=1]=\\mu_{1} \\\\\n&\\mathbb{E}[\\log Y \\mid \\operatorname{group}=2]=\\mu_{2}\n\\end{aligned}\n\\]\nand considerable attention is given to the difference \\(\\mu_{1}-\\mu_{2}\\). For example, in the previous section we compared the average log wages for men and women and found that the difference is \\(0.24\\). In that section we stated that this difference is often interpreted as the average percentage difference. This is not quite right, but is not quite wrong either. What the above calculation shows is that this difference is approximately the percentage difference in the geometric mean. So \\(\\mu_{1}-\\mu_{2}\\) is an average percentage difference, where “average” refers to geometric rather than arithmetic mean.\nTo compare different measures of percentage difference see Table 2.2. In the first two columns we report average wages for men and women in the CPS population using four “averages”: arithmetic mean, median, geometric mean, and mean log. For both groups the arithmetic mean is higher than the median and geometric mean, and the latter two are similar to one another. This is a common feature of skewed distributions such as the wage distribution. The third column reports the percentage difference between the first two columns (using men’s wages as the base). For example, the first entry of \\(34 %\\) states that the mean wage for men is \\(34 %\\) higher than the mean wage for women. The next entries show that the median and geometric mean for men is \\(26 %\\) higher than those for women. The final entry in this column is 100 times the simple difference between the mean log wage, which is \\(24 %\\). As shown above, the difference in the mean of the log transformation is approximately the percentage difference in the geometric mean, and this approximation is excellent for differences under \\(10 %\\).\nLet’s summarize this analysis. It is common to take logarithms of variables and make comparisons between conditional means. We have shown that these differences are measures of the percentage difference in the geometric mean. Thus the common description that the difference between expected log transforms (such as the \\(0.24\\) difference between those for men and women’s wages) is an approximate percentage difference (e.g. a 24% difference in men’s wages relative to women’s) is correct, so long as we realize that we are implicitly comparing geometric means."
  },
  {
    "objectID": "chpt02-ce.html#conditional-expectation-function",
    "href": "chpt02-ce.html#conditional-expectation-function",
    "title": "2  Conditional Expectation and Projection",
    "section": "2.5 Conditional Expectation Function",
    "text": "2.5 Conditional Expectation Function\nAn important determinant of wages is education. In many empirical studies economists measure educational attainment by the number of years \\({ }^{7}\\) of schooling. We will write this variable as education.\n\\({ }^{7}\\) Here, education is defined as years of schooling beyond kindergarten. A high school graduate has education=12, a college graduate has education=16, a Master’s degree has education=18, and a professional degree (medical, law or PhD) has educa- Table 2.2: Average Wages and Percentage Differences\n\n\n\n\nmen\nwomen\n% Difference\n\n\n\n\nArithmetic Mean\n\\(\\$ 26.80\\)\n\\(\\$ 20.00\\)\n\\(34 %\\)\n\n\nMedian\n\\(\\$ 21.14\\)\n\\(\\$ 16.83\\)\n\\(26 %\\)\n\n\nGeometric Mean\n\\(\\$ 21.03\\)\n\\(\\$ 16.64\\)\n\\(26 %\\)\n\n\nMean log Wage\n\\(3.05\\)\n\\(2.81\\)\n\\(24 %\\)\n\n\n\nThe conditional expectation of \\(\\log (\\) wage \\()\\) given gender, race, and education is a single number for each category. For example\n\\[\n\\mathbb{E}[\\log (\\text { wage }) \\mid \\text { gender }=\\text { man, race }=\\text { white, education }=12]=2.84 .\n\\]\nWe display in Figure \\(2.3\\) the conditional expectation of \\(\\log\\) (wage) as a function of education, separately for (white) men and women. The plot is quite revealing. We see that the conditional expectation is increasing in years of education, but at a different rate for schooling levels above and below nine years. Another striking feature of Figure \\(2.3\\) is that the gap between men and women is roughly constant for all education levels. As the variables are measured in logs this implies a constant average percentage gap between men and women regardless of educational attainment.\n\nFigure 2.3: Expected Log Wage as a Function of Education tion=20. In many cases it is convenient to simplify the notation by writing variables using single characters, typically \\(Y, X\\), and/or \\(Z\\). It is conventional in econometrics to denote the dependent variable (e.g. \\(\\log (\\) wage \\()\\) ) by the letter \\(Y\\), a conditioning variable (such as gender) by the letter \\(X\\), and multiple conditioning variables (such as race, education and gender) by the subscripted letters \\(X_{1}, X_{2}, \\ldots, X_{k}\\).\nConditional expectations can be written with the generic notation\n\\[\n\\mathbb{E}\\left[Y \\mid X_{1}=x_{1}, X_{2}=x_{2}, \\ldots, X_{k}=x_{k}\\right]=m\\left(x_{1}, x_{2}, \\ldots, x_{k}\\right) \\text {. }\n\\]\nWe call this the conditional expectation function (CEF). The CEF is a function of \\(\\left(x_{1}, x_{2}, \\ldots, x_{k}\\right)\\) as it varies with the variables. For example, the conditional expectation of \\(Y=\\log (\\) wage \\()\\) given \\(\\left(X_{1}, X_{2}\\right)=(g e n d e r\\), race) is given by the six entries of Table \\(2.1 .\\)\nFor greater compactness we typically write the conditioning variables as a vector in \\(\\mathbb{R}^{k}\\) :\n\\[\nX=\\left(\\begin{array}{c}\nX_{1} \\\\\nX_{2} \\\\\n\\vdots \\\\\nX_{k}\n\\end{array}\\right)\n\\]\nGiven this notation, the CEF can be compactly written as\n\\[\n\\mathbb{E}[Y \\mid X=x]=m(x) .\n\\]\nThe CEF \\(m(x)=\\mathbb{E}[Y \\mid X=x]\\) is a function of \\(x \\in \\mathbb{R}^{k}\\). It says: “When \\(X\\) takes the value \\(x\\) then the average value of \\(Y\\) is \\(m(x)\\).” Sometimes it is useful to view the CEF as a function of the random variable \\(X\\). In this case we evaluate the function \\(m(x)\\) at \\(X\\), and write \\(m(X)\\) or \\(\\mathbb{E}[Y \\mid X]\\). This is random as it is a function of the random variable \\(X\\)."
  },
  {
    "objectID": "chpt02-ce.html#continuous-variables",
    "href": "chpt02-ce.html#continuous-variables",
    "title": "2  Conditional Expectation and Projection",
    "section": "2.6 Continuous Variables",
    "text": "2.6 Continuous Variables\nIn the previous sections we implicitly assumed that the conditioning variables are discrete. However, many conditioning variables are continuous. In this section, we take up this case and assume that the variables \\((Y, X)\\) are continuously distributed with a joint density function \\(f(y, x)\\).\nAs an example, take \\(Y=\\log (\\) wage \\()\\) and \\(X=\\) experience, the latter the number of years of potential labor market experience \\({ }^{8}\\). The contours of their joint density are plotted in Figure \\(2.4\\) (a) for the population of white men with 12 years of education.\nGiven the joint density \\(f(y, x)\\) the variable \\(x\\) has the marginal density\n\\[\nf_{X}(x)=\\int_{-\\infty}^{\\infty} f(y, x) d y .\n\\]\nFor any \\(x\\) such that \\(f_{X}(x)>0\\) the conditional density of \\(Y\\) given \\(X\\) is defined as\n\\[\nf_{Y \\mid X}(y \\mid x)=\\frac{f(y, x)}{f_{X}(x)} .\n\\]\nThe conditional density is a renormalized slice of the joint density \\(f(y, x)\\) holding \\(x\\) fixed. The slice is renormalized (divided by \\(f_{X}(x)\\) so that it integrates to one) and is thus a density. We can visualize this by slicing the joint density function at a specific value of \\(x\\) parallel with the \\(y\\)-axis. For example, take the density contours in Figure 2.4(a) and slice through the contour plot at a specific value of experience, and\n\\({ }^{8}\\) As there is no direct measure for experience, we instead define experience as age-education-6\n\n\nJoint Density of Log Wage and Experience\n\n\n\nConditional Density of Log Wage given Experience\n\nFigure 2.4: Log Wage and Experience\nthen renormalize the slice so that it is a proper density. This gives us the conditional density of log(wage) for white men with 12 years of education and this level of experience. We do this for three levels of experience \\((5,10\\), and 25 years), and plot these densities in Figure \\(2.4\\) (b). We can see that the distribution of wages shifts to the right and becomes more diffuse as experience increases.\nThe CEF of \\(Y\\) given \\(X=x\\) is the expectation of the conditional density (2.5)\n\\[\nm(x)=\\mathbb{E}[Y \\mid X=x]=\\int_{-\\infty}^{\\infty} y f_{Y \\mid X}(y \\mid x) d y .\n\\]\nIntuitively, \\(m(x)\\) is the expectation of \\(Y\\) for the idealized subpopulation where the conditioning variables are fixed at \\(x\\). When \\(X\\) is continuously distributed this subpopulation is infinitely small.\nThis definition (2.6) is appropriate when the conditional density (2.5) is well defined. However, Theorem \\(2.13\\) in Section \\(2.31\\) will show that \\(m(x)\\) can be defined for any random variables \\((Y, X)\\) so long as \\(\\mathbb{E}|Y|<\\infty\\)\nIn Figure 2.4(a) the CEF of \\(\\log\\) (wage) given experience is plotted as the solid line. We can see that the CEF is a smooth but nonlinear function. The CEF is initially increasing in experience, flattens out around experience \\(=30\\), and then decreases for high levels of experience."
  },
  {
    "objectID": "chpt02-ce.html#law-of-iterated-expectations",
    "href": "chpt02-ce.html#law-of-iterated-expectations",
    "title": "2  Conditional Expectation and Projection",
    "section": "2.7 Law of Iterated Expectations",
    "text": "2.7 Law of Iterated Expectations\nAn extremely useful tool from probability theory is the law of iterated expectations. An important special case is known as the Simple Law. Theorem 2.1 Simple Law of Iterated Expectations\nIf \\(\\mathbb{E}|Y|<\\infty\\) then for any random vector \\(X\\),\n\\[\n\\mathbb{E}[\\mathbb{E}[Y \\mid X]]=\\mathbb{E}[Y] .\n\\]\nThis states that the expectation of the conditional expectation is the unconditional expectation. In other words the average of the conditional averages is the unconditional average. For discrete \\(X\\)\n\\[\n\\mathbb{E}[\\mathbb{E}[Y \\mid X]]=\\sum_{j=1}^{\\infty} \\mathbb{E}\\left[Y \\mid X=x_{j}\\right] \\mathbb{P}\\left[X=x_{j}\\right] .\n\\]\nFor continuous \\(X\\)\n\\[\n\\mathbb{E}[\\mathbb{E}[Y \\mid X]]=\\int_{\\mathbb{R}^{k}} \\mathbb{E}[Y \\mid X=x] f_{X}(x) d x .\n\\]\nGoing back to our investigation of average log wages for men and women, the simple law states that\n\\[\n\\begin{aligned}\n&\\mathbb{E}[\\log (\\text { wage }) \\mid \\text { gender }=\\text { man }] \\mathbb{P}[\\text { gender }=\\text { man }] \\\\\n&+\\mathbb{E}[\\log (\\text { wage }) \\mid \\text { gender }=\\text { woman }] \\mathbb{P}[\\text { gender }=\\text { woman }] \\\\\n&=\\mathbb{E}[\\log (\\text { wage })]\n\\end{aligned}\n\\]\nOr numerically,\n\\[\n3.05 \\times 0.57+2.81 \\times 0.43=2.95 \\text {. }\n\\]\nThe general law of iterated expectations allows two sets of conditioning variables.\nTheorem 2.2 Law of Iterated Expectations If \\(\\mathbb{E}|Y|<\\infty\\) then for any random vectors \\(X_{1}\\) and \\(X_{2}\\),\n\\[\n\\mathbb{E}\\left[\\mathbb{E}\\left[Y \\mid X_{1}, X_{2}\\right] \\mid X_{1}\\right]=\\mathbb{E}\\left[Y \\mid X_{1}\\right] .\n\\]\nNotice the way the law is applied. The inner expectation conditions on \\(X_{1}\\) and \\(X_{2}\\), while the outer expectation conditions only on \\(X_{1}\\). The iterated expectation yields the simple answer \\(\\mathbb{E}\\left[Y \\mid X_{1}\\right]\\), the expectation conditional on \\(X_{1}\\) alone. Sometimes we phrase this as: “The smaller information set wins.”\nAs an example\n\\[\n\\begin{aligned}\n&\\mathbb{E}[\\log (\\text { wage }) \\mid \\text { gender }=\\text { man, race }=\\text { white }] \\mathbb{P}[\\text { race }=\\text { white } \\mid \\text { gender }=\\text { man }] \\\\\n&+\\mathbb{E}[\\log (\\text { wage }) \\mid \\text { gender }=\\text { man, race }=\\text { Black }] \\mathbb{P}[\\text { race }=\\text { Black } \\mid \\text { gender }=\\text { man }] \\\\\n&+\\mathbb{E}[\\log (\\text { wage }) \\mid \\text { gender }=\\text { man, race }=\\text { other }] \\mathbb{P}[\\text { race }=\\text { other } \\mid \\text { gender }=\\text { man }] \\\\\n&=\\mathbb{E}[\\log (\\text { wage }) \\mid \\text { gender }=\\text { man }]\n\\end{aligned}\n\\]\nor numerically\n\\[\n3.07 \\times 0.84+2.86 \\times 0.08+3.03 \\times 0.08=3.05 \\text {. }\n\\]\nA property of conditional expectations is that when you condition on a random vector \\(X\\) you can effectively treat it as if it is constant. For example, \\(\\mathbb{E}[X \\mid X]=X\\) and \\(\\mathbb{E}[g(X) \\mid X]=g(X)\\) for any function \\(g(\\cdot)\\). The general property is known as the Conditioning Theorem.\nTheorem 2.3 Conditioning Theorem If \\(\\mathbb{E}|Y|<\\infty\\) then\n\\[\n\\mathbb{E}[g(X) Y \\mid X]=g(X) \\mathbb{E}[Y \\mid X] .\n\\]\nIf in addition \\(\\mathbb{E}|g(X)|<\\infty\\) then\n\\[\n\\mathbb{E}[g(X) Y]=\\mathbb{E}[g(X) \\mathbb{E}[Y \\mid X]] .\n\\]\nThe proofs of Theorems 2.1, \\(2.2\\) and \\(2.3\\) are given in Section \\(2.33 .\\)"
  },
  {
    "objectID": "chpt02-ce.html#cef-error",
    "href": "chpt02-ce.html#cef-error",
    "title": "2  Conditional Expectation and Projection",
    "section": "2.8 CEF Error",
    "text": "2.8 CEF Error\nThe CEF error \\(e\\) is defined as the difference between \\(Y\\) and the CEF evaluated at \\(X\\) :\n\\[\ne=Y-m(X) .\n\\]\nBy construction, this yields the formula\n\\[\nY=m(X)+e .\n\\]\nIn (2.9) it is useful to understand that the error \\(e\\) is derived from the joint distribution of \\((Y, X)\\), and so its properties are derived from this construction.\nMany authors in econometrics denote the CEF error using the Greek letter \\(\\varepsilon\\). I do not follow this convention because the error \\(e\\) is a random variable similar to \\(Y\\) and \\(X\\), and it is typical to use Latin characters for random variables.\nA key property of the CEF error is that it has a conditional expectation of zero. To see this, by the linearity of expectations, the definition \\(m(X)=\\mathbb{E}[Y \\mid X]\\), and the Conditioning Theorem\n\\[\n\\begin{aligned}\n\\mathbb{E}[e \\mid X] &=\\mathbb{E}[(Y-m(X)) \\mid X] \\\\\n&=\\mathbb{E}[Y \\mid X]-\\mathbb{E}[m(X) \\mid X] \\\\\n&=m(X)-m(X)=0 .\n\\end{aligned}\n\\]\nThis fact can be combined with the law of iterated expectations to show that the unconditional expectation is also zero.\n\\[\n\\mathbb{E}[e]=\\mathbb{E}[\\mathbb{E}[e \\mid X]]=\\mathbb{E}[0]=0 .\n\\]\nWe state this and some other results formally.\nTheorem 2.4 Properties of the CEF error\nIf \\(\\mathbb{E}|Y|<\\infty\\) then\n\n\\(\\mathbb{E}[e \\mid X]=0\\).\n\\(\\mathbb{E}[e]=0\\).\nIf \\(\\mathbb{E}|Y|^{r}<\\infty\\) for \\(r \\geq 1\\) then \\(\\mathbb{E}|e|^{r}<\\infty\\).\nFor any function \\(h(x)\\) such that \\(\\mathbb{E}|h(X) e|<\\infty\\) then \\(\\mathbb{E}[h(X) e]=0\\). The proof of the third result is deferred to Section 2.33. The fourth result, whose proof is left to Exercise 2.3, implies that \\(e\\) is uncorrelated with any function of the regressors.\n\nThe equations\n\\[\n\\begin{aligned}\nY &=m(X)+e \\\\\n\\mathbb{E}[e \\mid X] &=0\n\\end{aligned}\n\\]\ntogether imply that \\(m(X)\\) is the CEF of \\(Y\\) given \\(X\\). It is important to understand that this is not a restriction. These equations hold true by definition.\nThe condition \\(\\mathbb{E}[e \\mid X]=0\\) is implied by the definition of \\(e\\) as the difference between \\(Y\\) and the CEF \\(m(X)\\). The equation \\(\\mathbb{E}[e \\mid X]=0\\) is sometimes called a conditional mean restriction, because the conditional mean of the error \\(e\\) is restricted to equal zero. The property is also sometimes called mean independence, for the conditional mean of \\(e\\) is 0 and thus independent of \\(X\\). However, it does not imply that the distribution of \\(e\\) is independent of \\(X\\). Sometimes the assumption ” \\(e\\) is independent of \\(X\\) ” is added as a convenient simplification, but it is not generic feature of the conditional mean. Typically and generally, \\(e\\) and \\(X\\) are jointly dependent even though the conditional mean of \\(e\\) is zero.\nAs an example, the contours of the joint density of the regression error \\(e\\) and experience are plotted in Figure \\(2.5\\) for the same population as Figure 2.4. Notice that the shape of the conditional distribution varies with the level of experience.\n\nLabor Market Experience (Years)\nFigure 2.5: Joint Density of Regression Error and Experience\nAs a simple example of a case where \\(X\\) and \\(e\\) are mean independent yet dependent let \\(e=X u\\) where \\(X\\) and \\(u\\) are independent \\(\\mathrm{N}(0,1)\\). Then conditional on \\(X\\) the error \\(e\\) has the distribution \\(\\mathrm{N}\\left(0, X^{2}\\right)\\). Thus \\(\\mathbb{E}[e \\mid X]=0\\) and \\(e\\) is mean independent of \\(X\\), yet \\(e\\) is not fully independent of \\(X\\). Mean independence does not imply full independence."
  },
  {
    "objectID": "chpt02-ce.html#intercept-only-model",
    "href": "chpt02-ce.html#intercept-only-model",
    "title": "2  Conditional Expectation and Projection",
    "section": "2.9 Intercept-Only Model",
    "text": "2.9 Intercept-Only Model\nA special case of the regression model is when there are no regressors \\(X\\). In this case \\(m(X)=\\mathbb{E}[Y]=\\mu\\), the unconditional expectation of \\(Y\\). We can still write an equation for \\(Y\\) in the regression format:\n\\[\n\\begin{aligned}\nY &=\\mu+e \\\\\n\\mathbb{E}[e] &=0 .\n\\end{aligned}\n\\]\nThis is useful for it unifies the notation."
  },
  {
    "objectID": "chpt02-ce.html#regression-variance",
    "href": "chpt02-ce.html#regression-variance",
    "title": "2  Conditional Expectation and Projection",
    "section": "2.10 Regression Variance",
    "text": "2.10 Regression Variance\nAn important measure of the dispersion about the CEF function is the unconditional variance of the CEF error \\(e\\). We write this as\n\\[\n\\sigma^{2}=\\operatorname{var}[e]=\\mathbb{E}\\left[(e-\\mathbb{E}[e])^{2}\\right]=\\mathbb{E}\\left[e^{2}\\right] .\n\\]\nTheorem 2.4.3 implies the following simple but useful result.\nTheorem 2.5 If \\(\\mathbb{E}\\left[Y^{2}\\right]<\\infty\\) then \\(\\sigma^{2}<\\infty\\).\nWe can call \\(\\sigma^{2}\\) the regression variance or the variance of the regression error. The magnitude of \\(\\sigma^{2}\\) measures the amount of variation in \\(Y\\) which is not “explained” or accounted for in the conditional expectation \\(\\mathbb{E}[Y \\mid X]\\).\nThe regression variance depends on the regressors \\(X\\). Consider two regressions\n\\[\n\\begin{aligned}\n&Y=\\mathbb{E}\\left[Y \\mid X_{1}\\right]+e_{1} \\\\\n&Y=\\mathbb{E}\\left[Y \\mid X_{1}, X_{2}\\right]+e_{2} .\n\\end{aligned}\n\\]\nWe write the two errors distinctly as \\(e_{1}\\) and \\(e_{2}\\) as they are different - changing the conditioning information changes the conditional expectation and therefore the regression error as well.\nIn our discussion of iterated expectations we have seen that by increasing the conditioning set the conditional expectation reveals greater detail about the distribution of \\(Y\\). What is the implication for the regression error?\nIt turns out that there is a simple relationship. We can think of the conditional expectation \\(\\mathbb{E}[Y \\mid X]\\) as the “explained portion” of \\(Y\\). The remainder \\(e=Y-\\mathbb{E}[Y \\mid X]\\) is the “unexplained portion”. The simple relationship we now derive shows that the variance of this unexplained portion decreases when we condition on more variables. This relationship is monotonic in the sense that increasing the amount of information always decreases the variance of the unexplained portion.\nTheorem 2.6 If \\(\\mathbb{E}\\left[Y^{2}\\right]<\\infty\\) then\n\\[\n\\operatorname{var}[Y] \\geq \\operatorname{var}\\left[Y-\\mathbb{E}\\left[Y \\mid X_{1}\\right]\\right] \\geq \\operatorname{var}\\left[Y-\\mathbb{E}\\left[Y \\mid X_{1}, X_{2}\\right]\\right] .\n\\]\nTheorem \\(2.6\\) says that the variance of the difference between \\(Y\\) and its conditional expectation (weakly) decreases whenever an additional variable is added to the conditioning information.\nThe proof of Theorem \\(2.6\\) is given in Section 2.33."
  },
  {
    "objectID": "chpt02-ce.html#best-predictor",
    "href": "chpt02-ce.html#best-predictor",
    "title": "2  Conditional Expectation and Projection",
    "section": "2.11 Best Predictor",
    "text": "2.11 Best Predictor\nSuppose that given a random vector \\(X\\) we want to predict or forecast \\(Y\\). We can write any predictor as a function \\(g(X)\\) of \\(X\\). The (ex-post) prediction error is the realized difference \\(Y-g(X)\\). A non-stochastic measure of the magnitude of the prediction error is the expectation of its square\n\\[\n\\mathbb{E}\\left[(Y-g(X))^{2}\\right] .\n\\]\nWe can define the best predictor as the function \\(g(X)\\) which minimizes (2.10). What function is the best predictor? It turns out that the answer is the CEF \\(m(X)\\). This holds regardless of the joint distribution of \\((Y, X)\\).\nTo see this, note that the mean squared error of a predictor \\(g(X)\\) is\n\\[\n\\begin{aligned}\n\\mathbb{E}\\left[(Y-g(X))^{2}\\right] &=\\mathbb{E}\\left[(e+m(X)-g(X))^{2}\\right] \\\\\n&=\\mathbb{E}\\left[e^{2}\\right]+2 \\mathbb{E}[e(m(X)-g(X))]+\\mathbb{E}\\left[(m(X)-g(X))^{2}\\right] \\\\\n&=\\mathbb{E}\\left[e^{2}\\right]+\\mathbb{E}\\left[(m(X)-g(X))^{2}\\right] \\\\\n& \\geq \\mathbb{E}\\left[e^{2}\\right] \\\\\n&=\\mathbb{E}\\left[(Y-m(X))^{2}\\right] .\n\\end{aligned}\n\\]\nThe first equality makes the substitution \\(Y=m(X)+e\\) and the third equality uses Theorem 2.4.4. The right-hand-side after the third equality is minimized by setting \\(g(X)=m(X)\\), yielding the inequality in the fourth line. The minimum is finite under the assumption \\(\\mathbb{E}\\left[Y^{2}\\right]<\\infty\\) as shown by Theorem \\(2.5\\).\nWe state this formally in the following result.\nTheorem 2.7 Conditional Expectation as Best Predictor If \\(\\mathbb{E}\\left[Y^{2}\\right]<\\infty\\), then for any predictor \\(g(X)\\),\n\\[\n\\mathbb{E}\\left[(Y-g(X))^{2}\\right] \\geq \\mathbb{E}\\left[(Y-m(X))^{2}\\right]\n\\]\nwhere \\(m(X)=\\mathbb{E}[Y \\mid X]\\)\nIt may be helpful to consider this result in the context of the intercept-only model\n\\[\n\\begin{aligned}\nY &=\\mu+e \\\\\n\\mathbb{E}[e] &=0 .\n\\end{aligned}\n\\]\nTheorem \\(2.7\\) shows that the best predictor for \\(Y\\) (in the class of constants) is the unconditional mean \\(\\mu=\\mathbb{E}[Y]\\) in the sense that the mean minimizes the mean squared prediction error."
  },
  {
    "objectID": "chpt02-ce.html#conditional-variance",
    "href": "chpt02-ce.html#conditional-variance",
    "title": "2  Conditional Expectation and Projection",
    "section": "2.12 Conditional Variance",
    "text": "2.12 Conditional Variance\nWhile the conditional mean is a good measure of the location of a conditional distribution it does not provide information about the spread of the distribution. A common measure of the dispersion is the conditional variance. We first give the general definition of the conditional variance of a random variable \\(Y\\).\nDefinition 2.1 If \\(\\mathbb{E}\\left[Y^{2}\\right]<\\infty\\), the conditional variance of \\(Y\\) given \\(X=x\\) is\n\\[\n\\sigma^{2}(x)=\\operatorname{var}[Y \\mid X=x]=\\mathbb{E}\\left[(Y-\\mathbb{E}[Y \\mid X=x])^{2} \\mid X=x\\right] .\n\\]\nThe conditional variance treated as a random variable is \\(\\operatorname{var}[Y \\mid X]=\\sigma^{2}(X)\\).\nThe conditional variance is distinct from the unconditional variance var \\([Y]\\). The difference is that the conditional variance is a function of the conditioning variables. Notice that the conditional variance is the conditional second moment, centered around the conditional first moment.\nGiven this definition we define the conditional variance of the regression error.\nDefinition 2.2 If \\(\\mathbb{E}\\left[e^{2}\\right]<\\infty\\), the conditional variance of the regression error \\(e\\) given \\(X=x\\) is\n\\[\n\\sigma^{2}(x)=\\operatorname{var}[e \\mid X=x]=\\mathbb{E}\\left[e^{2} \\mid X=x\\right] .\n\\]\nThe conditional variance of \\(e\\) treated as a random variable is \\(\\operatorname{var}[e \\mid X]=\\sigma^{2}(X)\\).\nAgain, the conditional variance \\(\\sigma^{2}(x)\\) is distinct from the unconditional variance \\(\\sigma^{2}\\). The conditional variance is a function of the regressors, the unconditional variance is not. Generally, \\(\\sigma^{2}(x)\\) is a non-trivial function of \\(x\\) and can take any form subject to the restriction that it is non-negative. One way to think about \\(\\sigma^{2}(x)\\) is that it is the conditional mean of \\(e^{2}\\) given \\(X\\). Notice as well that \\(\\sigma^{2}(x)=\\operatorname{var}[Y \\mid X=x]\\) so it is equivalently the conditional variance of the dependent variable.\nThe variance of \\(Y\\) is in a different unit of measurement than \\(Y\\). To convert the variance to the same unit of measure we define the conditional standard deviation as its square root \\(\\sigma(x)=\\sqrt{\\sigma^{2}(x)}\\).\nAs an example of how the conditional variance depends on observables, compare the conditional log wage densities for men and women displayed in Figure 2.2. The difference between the densities is not purely a location shift but is also a difference in spread. Specifically, we can see that the density for men’s log wages is somewhat more spread out than that for women, while the density for women’s wages is somewhat more peaked. Indeed, the conditional standard deviation for men’s wages is \\(3.05\\) and that for women is \\(2.81\\). So while men have higher average wages they are also somewhat more dispersed.\nThe unconditional variance is related to the conditional variance by the following identity.\nTheorem 2.8 If \\(\\mathbb{E}\\left[Y^{2}\\right]<\\infty\\) then\n\\[\n\\operatorname{var}[Y]=\\mathbb{E}[\\operatorname{var}[Y \\mid X]]+\\operatorname{var}[\\mathbb{E}[Y \\mid X]] .\n\\]\nSee Theorem \\(4.14\\) of Probability and Statistics for Economists. Theorem \\(2.8\\) decomposes the unconditional variance into what are sometimes called the “within group variance” and the “across group variance”. For example, if \\(X\\) is education level, then the first term is the expected variance of the conditional expectation by education level. The second term is the variance after controlling for education.\nThe regression error has a conditional mean of zero, so its unconditional error variance equals the expected conditional variance, or equivalently can be found by the law of iterated expectations.\n\\[\n\\sigma^{2}=\\mathbb{E}\\left[e^{2}\\right]=\\mathbb{E}\\left[\\mathbb{E}\\left[e^{2} \\mid X\\right]\\right]=\\mathbb{E}\\left[\\sigma^{2}(X)\\right] .\n\\]\nThat is, the unconditional error variance is the average conditional variance.\nGiven the conditional variance we can define a rescaled error\n\\[\nu=\\frac{e}{\\sigma(X)} \\text {. }\n\\]\nWe calculate that since \\(\\sigma(X)\\) is a function of \\(X\\)\n\\[\n\\mathbb{E}[u \\mid X]=\\mathbb{E}\\left[\\frac{e}{\\sigma(X)} \\mid X\\right]=\\frac{1}{\\sigma(X)} \\mathbb{E}[e \\mid X]=0\n\\]\nand\n\\[\n\\operatorname{var}[u \\mid X]=\\mathbb{E}\\left[u^{2} \\mid X\\right]=\\mathbb{E}\\left[\\frac{e^{2}}{\\sigma^{2}(X)} \\mid X\\right]=\\frac{1}{\\sigma^{2}(X)} \\mathbb{E}\\left[e^{2} \\mid X\\right]=\\frac{\\sigma^{2}(X)}{\\sigma^{2}(X)}=1 .\n\\]\nThus \\(u\\) has a conditional expectation of zero and a conditional variance of 1 .\nNotice that (2.11) can be rewritten as\n\\[\ne=\\sigma(X) u .\n\\]\nand substituting this for \\(e\\) in the CEF equation (2.9), we find that\n\\[\nY=m(X)+\\sigma(X) u .\n\\]\nThis is an alternative (mean-variance) representation of the CEF equation.\nMany econometric studies focus on the conditional expectation \\(m(x)\\) and either ignore the conditional variance \\(\\sigma^{2}(x)\\), treat it as a constant \\(\\sigma^{2}(x)=\\sigma^{2}\\), or treat it as a nuisance parameter (a parameter not of primary interest). This is appropriate when the primary variation in the conditional distribution is in the mean but can be short-sighted in other cases. Dispersion is relevant to many economic topics, including income and wealth distribution, economic inequality, and price dispersion. Conditional dispersion (variance) can be a fruitful subject for investigation.\nThe perverse consequences of a narrow-minded focus on the mean is parodied in a classic joke:\nAn economist was standing with one foot in a bucket of boiling water and the other foot in a bucket of ice. When asked how he felt, he replied, “On average I feel just fine.”\nClearly, the economist in question ignored variance!"
  },
  {
    "objectID": "chpt02-ce.html#homoskedasticity-and-heteroskedasticity",
    "href": "chpt02-ce.html#homoskedasticity-and-heteroskedasticity",
    "title": "2  Conditional Expectation and Projection",
    "section": "2.13 Homoskedasticity and Heteroskedasticity",
    "text": "2.13 Homoskedasticity and Heteroskedasticity\nAn important special case obtains when the conditional variance \\(\\sigma^{2}(x)\\) is a constant and independent of \\(x\\). This is called homoskedasticity.\nDefinition 2.3 The error is homoskedastic if \\(\\sigma^{2}(x)=\\sigma^{2}\\) does not depend on \\(x\\).\nIn the general case where \\(\\sigma^{2}(x)\\) depends on \\(x\\) we say that the error \\(e\\) is heteroskedastic.\nDefinition 2.4 The error is heteroskedastic if \\(\\sigma^{2}(x)\\) depends on \\(x\\).\nIt is helpful to understand that the concepts homoskedasticity and heteroskedasticity concern the conditional variance, not the unconditional variance. By definition, the unconditional variance \\(\\sigma^{2}\\) is a constant and independent of the regressors \\(X\\). So when we talk about the variance as a function of the regressors we are talking about the conditional variance \\(\\sigma^{2}(x)\\).\nSome older or introductory textbooks describe heteroskedasticity as the case where “the variance of \\(e\\) varies across observations”. This is a poor and confusing definition. It is more constructive to understand that heteroskedasticity means that the conditional variance \\(\\sigma^{2}(x)\\) depends on observables.\nOlder textbooks also tend to describe homoskedasticity as a component of a correct regression specification and describe heteroskedasticity as an exception or deviance. This description has influenced many generations of economists but it is unfortunately backwards. The correct view is that heteroskedasticity is generic and “standard”, while homoskedasticity is unusual and exceptional. The default in empirical work should be to assume that the errors are heteroskedastic, not the converse.\nIn apparent contradiction to the above statement we will still frequently impose the homoskedasticity assumption when making theoretical investigations into the properties of estimation and inference methods. The reason is that in many cases homoskedasticity greatly simplifies the theoretical calculations and it is therefore quite advantageous for teaching and learning. It should always be remembered, however, that homoskedasticity is never imposed because it is believed to be a correct feature of an empirical model but rather because of its simplicity."
  },
  {
    "objectID": "chpt02-ce.html#heteroskedastic-or-heteroscedastic",
    "href": "chpt02-ce.html#heteroskedastic-or-heteroscedastic",
    "title": "2  Conditional Expectation and Projection",
    "section": "2.14 Heteroskedastic or Heteroscedastic?",
    "text": "2.14 Heteroskedastic or Heteroscedastic?\nThe spelling of the words homoskedastic and heteroskedastic have been somewhat controversial. Early econometrics textbooks were split, with some using a “c” as in heteroscedastic and some ” \\(\\mathrm{k}\\) ” as in heteroskedastic. McCulloch (1985) pointed out that the word is derived from Greek roots.\n\\ means “to scatter”. Since the proper transliteration of the Greek letter \\(\\kappa\\) in \\(\\sigma \\kappa \\varepsilon \\delta \\alpha v v v \\mu \\iota\\) is ” \\(\\mathrm{k}\\) “, this implies that the correct English spelling of the two words is with a” \\(\\mathrm{k}\\) ” as in homoskedastic and heteroskedastic."
  },
  {
    "objectID": "chpt02-ce.html#regression-derivative",
    "href": "chpt02-ce.html#regression-derivative",
    "title": "2  Conditional Expectation and Projection",
    "section": "2.15 Regression Derivative",
    "text": "2.15 Regression Derivative\nOne way to interpret the CEF \\(m(x)=\\mathbb{E}[Y \\mid X=x]\\) is in terms of how marginal changes in the regressors \\(X\\) imply changes in the conditional expectation of the response variable \\(Y\\). It is typical to consider marginal changes in a single regressor, say \\(X_{1}\\), holding the remainder fixed. When a regressor \\(X_{1}\\) is continuously distributed, we define the marginal effect of a change in \\(X_{1}\\), holding the variables \\(X_{2}, \\ldots, X_{k}\\) fixed, as the partial derivative of the CEF\n\\[\n\\frac{\\partial}{\\partial x_{1}} m\\left(x_{1}, \\ldots, x_{k}\\right)\n\\]\nWhen \\(X_{1}\\) is discrete we define the marginal effect as a discrete difference. For example, if \\(X_{1}\\) is binary, then the marginal effect of \\(X_{1}\\) on the CEF is\n\\[\nm\\left(1, x_{2}, \\ldots, x_{k}\\right)-m\\left(0, x_{2}, \\ldots, x_{k}\\right)\n\\]\nWe can unify the continuous and discrete cases with the notation\n\\[\n\\nabla_{1} m(x)=\\left\\{\\begin{array}{cc}\n\\frac{\\partial}{\\partial x_{1}} m\\left(x_{1}, \\ldots, x_{k}\\right), & \\text { if } X_{1} \\text { is continuous } \\\\\nm\\left(1, x_{2}, \\ldots, x_{k}\\right)-m\\left(0, x_{2}, \\ldots, x_{k}\\right), & \\text { if } X_{1} \\text { is binary. }\n\\end{array}\\right.\n\\]\nCollecting the \\(k\\) effects into one \\(k \\times 1\\) vector, we define the regression derivative with respect to \\(X\\) :\n\\[\n\\nabla m(x)=\\left[\\begin{array}{c}\n\\nabla_{1} m(x) \\\\\n\\nabla_{2} m(x) \\\\\n\\vdots \\\\\n\\nabla_{k} m(x)\n\\end{array}\\right]\n\\]\nWhen all elements of \\(X\\) are continuous, then we have the simplification \\(\\nabla m(x)=\\frac{\\partial}{\\partial x} m(x)\\), the vector of partial derivatives.\nThere are two important points to remember concerning our definition of the regression derivative. First, the effect of each variable is calculated holding the other variables constant. This is the ceteris paribus concept commonly used in economics. But in the case of a regression derivative, the conditional expectation does not literally hold all else constant. It only holds constant the variables included in the conditional expectation. This means that the regression derivative depends on which regressors are included. For example, in a regression of wages on education, experience, race and gender, the regression derivative with respect to education shows the marginal effect of education on expected wages, holding constant experience, race, and gender. But it does not hold constant an individual’s unobservable characteristics (such as ability), nor variables not included in the regression (such as the quality of education).\nSecond, the regression derivative is the change in the conditional expectation of \\(Y\\), not the change in the actual value of \\(Y\\) for an individual. It is tempting to think of the regression derivative as the change in the actual value of \\(Y\\), but this is not a correct interpretation. The regression derivative \\(\\nabla m(x)\\) is the change in the actual value of \\(Y\\) only if the error \\(e\\) is unaffected by the change in the regressor \\(X\\). We return to a discussion of causal effects in Section 2.30."
  },
  {
    "objectID": "chpt02-ce.html#linear-cef",
    "href": "chpt02-ce.html#linear-cef",
    "title": "2  Conditional Expectation and Projection",
    "section": "2.16 Linear CEF",
    "text": "2.16 Linear CEF\nAn important special case is when the CEF \\(m(x)=\\mathbb{E}[Y \\mid X=x]\\) is linear in \\(x\\). In this case we can write the mean equation as\n\\[\nm(x)=x_{1} \\beta_{1}+x_{2} \\beta_{2}+\\cdots+x_{k} \\beta_{k}+\\beta_{k+1} .\n\\]\nNotationally it is convenient to write this as a simple function of the vector \\(x\\). An easy way to do so is to augment the regressor vector \\(X\\) by listing the number ” 1 ” as an element. We call this the “constant” and the corresponding coefficient is called the “intercept”. Equivalently, specify that the final element \\({ }^{9}\\) of the vector \\(x\\) is \\(x_{k}=1\\). Thus (2.4) has been redefined as the \\(k \\times 1\\) vector\n\\[\nX=\\left(\\begin{array}{c}\nX_{1} \\\\\nX_{2} \\\\\n\\vdots \\\\\nX_{k-1} \\\\\n1\n\\end{array}\\right)\n\\]\nWith this redefinition, the CEF is\n\\[\nm(x)=x_{1} \\beta_{1}+x_{2} \\beta_{2}+\\cdots+\\beta_{k}=x^{\\prime} \\beta\n\\]\nwhere\n\\[\n\\beta=\\left(\\begin{array}{c}\n\\beta_{1} \\\\\n\\vdots \\\\\n\\beta_{k}\n\\end{array}\\right)\n\\]\nis a \\(k \\times 1\\) coefficient vector. This is the linear CEF model. It is also often called the linear regression model, or the regression of \\(Y\\) on \\(X\\).\nIn the linear CEF model the regression derivative is simply the coefficient vector. That is \\(\\nabla m(x)=\\beta\\). This is one of the appealing features of the linear CEF model. The coefficients have simple and natural interpretations as the marginal effects of changing one variable, holding the others constant.\n\\[\n\\begin{aligned}\n&\\text { Linear CEF Model } \\\\\n&\\begin{aligned}\nY &=X^{\\prime} \\beta+e \\\\\n\\mathbb{E}[e \\mid X] &=0\n\\end{aligned}\n\\end{aligned}\n\\]\nIf in addition the error is homoskedastic we call this the homoskedastic linear CEF model."
  },
  {
    "objectID": "chpt02-ce.html#homoskedastic-linear-cef-model",
    "href": "chpt02-ce.html#homoskedastic-linear-cef-model",
    "title": "2  Conditional Expectation and Projection",
    "section": "2.17 Homoskedastic Linear CEF Model",
    "text": "2.17 Homoskedastic Linear CEF Model\n\\[\n\\begin{aligned}\nY &=X^{\\prime} \\beta+e \\\\\n\\mathbb{E}[e \\mid X] &=0 \\\\\n\\mathbb{E}\\left[e^{2} \\mid X\\right] &=\\sigma^{2}\n\\end{aligned}\n\\]\n\\({ }^{9}\\) The order doesn’t matter. It could be any element."
  },
  {
    "objectID": "chpt02-ce.html#linear-cef-with-nonlinear-effects",
    "href": "chpt02-ce.html#linear-cef-with-nonlinear-effects",
    "title": "2  Conditional Expectation and Projection",
    "section": "2.18 Linear CEF with Nonlinear Effects",
    "text": "2.18 Linear CEF with Nonlinear Effects\nThe linear CEF model of the previous section is less restrictive than it might appear, as we can include as regressors nonlinear transformations of the original variables. In this sense, the linear CEF framework is flexible and can capture many nonlinear effects.\nFor example, suppose we have two scalar variables \\(X_{1}\\) and \\(X_{2}\\). The CEF could take the quadratic form\n\\[\nm\\left(x_{1}, x_{2}\\right)=x_{1} \\beta_{1}+x_{2} \\beta_{2}+x_{1}^{2} \\beta_{3}+x_{2}^{2} \\beta_{4}+x_{1} x_{2} \\beta_{5}+\\beta_{6} .\n\\]\nThis equation is quadratic in the regressors \\(\\left(x_{1}, x_{2}\\right)\\) yet linear in the coefficients \\(\\beta=\\left(\\beta_{1}, \\ldots, \\beta_{6}\\right)^{\\prime}\\). We still call (2.14) a linear CEF because it is a linear function of the coefficients. At the same time, it has nonlinear effects because it is nonlinear in the underlying variables \\(x_{1}\\) and \\(x_{2}\\). The key is to understand that (2.14) is quadratic in the variables \\(\\left(x_{1}, x_{2}\\right)\\) yet linear in the coefficients \\(\\beta\\).\nTo simplify the expression we define the transformations \\(x_{3}=x_{1}^{2}, x_{4}=x_{2}^{2}, x_{5}=x_{1} x_{2}\\), and \\(x_{6}=1\\), and redefine the regressor vector as \\(x=\\left(x_{1}, \\ldots, x_{6}\\right)^{\\prime}\\). With this redefinition, \\(m\\left(x_{1}, x_{2}\\right)=x^{\\prime} \\beta\\) which is linear in \\(\\beta\\). For most econometric purposes (estimation and inference on \\(\\beta\\) ) the linearity in \\(\\beta\\) is all that is important.\nAn exception is in the analysis of regression derivatives. In nonlinear equations such as (2.14) the regression derivative should be defined with respect to the original variables not with respect to the transformed variables. Thus\n\\[\n\\begin{aligned}\n&\\frac{\\partial}{\\partial x_{1}} m\\left(x_{1}, x_{2}\\right)=\\beta_{1}+2 x_{1} \\beta_{3}+x_{2} \\beta_{5} \\\\\n&\\frac{\\partial}{\\partial x_{2}} m\\left(x_{1}, x_{2}\\right)=\\beta_{2}+2 x_{2} \\beta_{4}+x_{1} \\beta_{5} .\n\\end{aligned}\n\\]\nWe see that in the model (2.14), the regression derivatives are not a simple coefficient, but are functions of several coefficients plus the levels of \\(\\left(x_{1}, x_{2}\\right)\\). Consequently it is difficult to interpret the coefficients individually. It is more useful to interpret them as a group.\nWe typically call \\(\\beta_{5}\\) the interaction effect. Notice that it appears in both regression derivative equations and has a symmetric interpretation in each. If \\(\\beta_{5}>0\\) then the regression derivative with respect to \\(x_{1}\\) is increasing in the level of \\(x_{2}\\) (and the regression derivative with respect to \\(x_{2}\\) is increasing in the level of \\(x_{1}\\) ), while if \\(\\beta_{5}<0\\) the reverse is true."
  },
  {
    "objectID": "chpt02-ce.html#linear-cef-with-dummy-variables",
    "href": "chpt02-ce.html#linear-cef-with-dummy-variables",
    "title": "2  Conditional Expectation and Projection",
    "section": "2.19 Linear CEF with Dummy Variables",
    "text": "2.19 Linear CEF with Dummy Variables\nWhen all regressors take a finite set of values it turns out the CEF can be written as a linear function of regressors.\nThis simplest example is a binary variable which takes only two distinct values. For example, in traditional data sets the variable gender takes only the values man and woman (or male and female). Binary variables are extremely common in econometric applications and are alternatively called dummy variables or indicator variables.\nConsider the simple case of a single binary regressor. In this case the conditional expectation can only take two distinct values. For example,\n\\[\n\\mathbb{E}[Y \\mid \\text { gender }]=\\left\\{\\begin{array}{llc}\n\\mu_{0} & \\text { if } \\quad \\text { gender }=\\text { man } \\\\\n\\mu_{1} & \\text { if gender }=\\text { woman. }\n\\end{array}\\right.\n\\]\nTo facilitate a mathematical treatment we record dummy variables with the values \\(\\{0,1\\}\\). For example\n\\[\nX_{1}=\\left\\{\\begin{array}{llc}\n0 & \\text { if } & \\text { gender }=\\text { man } \\\\\n1 & \\text { if } & \\text { gender }=\\text { woman } .\n\\end{array}\\right.\n\\]\nGiven this notation we write the conditional expectation as a linear function of the dummy variable \\(X_{1}\\). Thus \\(\\mathbb{E}\\left[Y \\mid X_{1}\\right]=\\beta_{1} X_{1}+\\beta_{2}\\) where \\(\\beta_{1}=\\mu_{1}-\\mu_{0}\\) and \\(\\beta_{2}=\\mu_{0}\\). In this simple regression equation the intercept \\(\\beta_{2}\\) is equal to the conditional expectation of \\(Y\\) for the \\(X_{1}=0\\) subpopulation (men) and the slope \\(\\beta_{1}\\) is equal to the difference in the conditional expectations between the two subpopulations.\nAlternatively, we could have defined \\(X_{1}\\) as\n\\[\nX_{1}= \\begin{cases}1 & \\text { if } \\quad \\text { gender }=\\text { man } \\\\ 0 & \\text { if } \\quad \\text { gender }=\\text { woman } .\\end{cases}\n\\]\nIn this case, the regression intercept is the expectation for women (rather than for men) and the regression slope has switched signs. The two regressions are equivalent but the interpretation of the coefficients has changed. Therefore it is always important to understand the precise definitions of the variables, and illuminating labels are helpful. For example, labelling \\(X_{1}\\) as “gender” does not help distinguish between definitions (2.15) and (2.16). Instead, it is better to label \\(X_{1}\\) as “women” or “female” if definition (2.15) is used, or as “men” or “male” if (2.16) is used.\nNow suppose we have two dummy variables \\(X_{1}\\) and \\(X_{2}\\). For example, \\(X_{2}=1\\) if the person is married, else \\(X_{2}=0\\). The conditional expectation given \\(X_{1}\\) and \\(X_{2}\\) takes at most four possible values:\n\nIn this case we can write the conditional mean as a linear function of \\(X, X_{2}\\) and their product \\(X_{1} X_{2}\\) :\n\\[\n\\mathbb{E}\\left[Y \\mid X_{1}, X_{2}\\right]=\\beta_{1} X_{1}+\\beta_{2} X_{2}+\\beta_{3} X_{1} X_{2}+\\beta_{4}\n\\]\nwhere \\(\\beta_{1}=\\mu_{10}-\\mu_{00}, \\beta_{2}=\\mu_{01}-\\mu_{00}, \\beta_{3}=\\mu_{11}-\\mu_{10}-\\mu_{01}+\\mu_{00}\\), and \\(\\beta_{4}=\\mu_{00}\\).\nWe can view the coefficient \\(\\beta_{1}\\) as the effect of gender on expected log wages for unmarried wage earners, the coefficient \\(\\beta_{2}\\) as the effect of marriage on expected log wages for men wage earners, and the coefficient \\(\\beta_{3}\\) as the difference between the effects of marriage on expected log wages among women and among men. Alternatively, it can also be interpreted as the difference between the effects of gender on expected log wages among married and non-married wage earners. Both interpretations are equally valid. We often describe \\(\\beta_{3}\\) as measuring the interaction between the two dummy variables, or the interaction effect, and describe \\(\\beta_{3}=0\\) as the case when the interaction effect is zero.\nIn this setting we can see that the CEF is linear in the three variables \\(\\left(X_{1}, X_{2}, X_{1} X_{2}\\right)\\). To put the model in the framework of Section \\(2.15\\) we define the regressor \\(X_{3}=X_{1} X_{2}\\) and the regressor vector as\n\\[\nX=\\left(\\begin{array}{c}\nX_{1} \\\\\nX_{2} \\\\\nX_{3} \\\\\n1\n\\end{array}\\right) .\n\\]\nSo while we started with two dummy variables, the number of regressors (including the intercept) is four.\nIf there are three dummy variables \\(X_{1}, X_{2}, X_{3}\\), then \\(\\mathbb{E}\\left[Y \\mid X_{1}, X_{2}, X_{3}\\right]\\) takes at most \\(2^{3}=8\\) distinct values and can be written as the linear function\n\\[\n\\mathbb{E}\\left[Y \\mid X_{1}, X_{2}, X_{3}\\right]=\\beta_{1} X_{1}+\\beta_{2} X_{2}+\\beta_{3} X_{3}+\\beta_{4} X_{1} X_{2}+\\beta_{5} X_{1} X_{3}+\\beta_{6} X_{2} X_{3}+\\beta_{7 X 1} X_{2} X_{3}+\\beta_{8}\n\\]\nwhich has eight regressors including the intercept. In general, if there are \\(p\\) dummy variables \\(X_{1}, \\ldots, X_{p}\\) then the CEF \\(\\mathbb{E}\\left[Y \\mid X_{1}, X_{2}, \\ldots, X_{p}\\right]\\) takes at most \\(2^{p}\\) distinct values and can be written as a linear function of the \\(2^{p}\\) regressors including \\(X_{1}, X_{2}, \\ldots, X_{p}\\) and all cross-products. A linear regression model which includes all \\(2^{p}\\) binary interactions is called a saturated dummy variable regression model. It is a complete model of the conditional expectation. In contrast, a model with no interactions equals\n\\[\n\\mathbb{E}\\left[Y \\mid X_{1}, X_{2}, \\ldots, X_{p}\\right]=\\beta_{1} X_{1}+\\beta_{2} X_{2}+\\cdots+\\beta_{p} X_{p}+\\beta_{p} .\n\\]\nThis has \\(p+1\\) coefficients instead of \\(2^{p}\\).\nWe started this section by saying that the conditional expectation is linear whenever all regressors take only a finite number of possible values. How can we see this? Take a categorical variable, such as race. For example, we earlier divided race into three categories. We can record categorical variables using numbers to indicate each category, for example\n\\[\nX_{3}=\\left\\{\\begin{array}{lll}\n1 & \\text { if } & \\text { white } \\\\\n2 & \\text { if } & \\text { Black } \\\\\n3 & \\text { if } & \\text { other. }\n\\end{array}\\right.\n\\]\nWhen doing so, the values of \\(X_{3}\\) have no meaning in terms of magnitude, they simply indicate the relevant category.\nWhen the regressor is categorical the conditional expectation of \\(Y\\) given \\(X_{3}\\) takes a distinct value for each possibility:\n\\[\n\\mathbb{E}\\left[Y \\mid X_{3}\\right]=\\left\\{\\begin{array}{lll}\n\\mu_{1} & \\text { if } & X_{3}=1 \\\\\n\\mu_{2} & \\text { if } & X_{3}=2 \\\\\n\\mu_{3} & \\text { if } & X_{3}=3 .\n\\end{array}\\right.\n\\]\nThis is not a linear function of \\(X_{3}\\) itself, but it can be made a linear function by constructing dummy variables for two of the three categories. For example\n\\[\n\\begin{aligned}\n&X_{4}=\\left\\{\\begin{array}{llc}\n1 & \\text { if } & \\text { Black } \\\\\n0 & \\text { if } & \\text { not Black }\n\\end{array}\\right. \\\\\n&X_{5}=\\left\\{\\begin{array}{lll}\n1 & \\text { if } & \\text { other } \\\\\n0 & \\text { if } & \\text { not other. }\n\\end{array}\\right.\n\\end{aligned}\n\\]\nIn this case, the categorical variable \\(X_{3}\\) is equivalent to the pair of dummy variables \\(\\left(X_{4}, X_{5}\\right)\\). The explicit relationship is\n\\[\nX_{3}=\\left\\{\\begin{array}{lll}\n1 & \\text { if } & X_{4}=0 \\text { and } X_{5}=0 \\\\\n2 & \\text { if } & X_{4}=1 \\text { and } X_{5}=0 \\\\\n3 & \\text { if } & X_{4}=0 \\text { and } X_{5}=1\n\\end{array}\\right.\n\\]\nGiven these transformations, we can write the conditional expectation of \\(Y\\) as a linear function of \\(X_{4}\\) and \\(X_{5}\\)\n\\[\n\\mathbb{E}\\left[Y \\mid X_{3}\\right]=\\mathbb{E}\\left[Y \\mid X_{4}, X_{5}\\right]=\\beta_{1} X_{4}+\\beta_{2} X_{5}+\\beta_{3} .\n\\]\nWe can write the CEF as either \\(\\mathbb{E}\\left[Y \\mid X_{3}\\right]\\) or \\(\\mathbb{E}\\left[Y \\mid X_{4}, X_{5}\\right]\\) (they are equivalent), but it is only linear as a function of \\(X_{4}\\) and \\(X_{5}\\).\nThis setting is similar to the case of two dummy variables, with the difference that we have not included the interaction term \\(X_{4} X_{5}\\). This is because the event \\(\\left\\{X_{4}=1\\right.\\) and \\(\\left.X_{5}=1\\right\\}\\) is empty by construction, so \\(X_{4} X_{5}=0\\) by definition."
  },
  {
    "objectID": "chpt02-ce.html#best-linear-predictor",
    "href": "chpt02-ce.html#best-linear-predictor",
    "title": "2  Conditional Expectation and Projection",
    "section": "2.20 Best Linear Predictor",
    "text": "2.20 Best Linear Predictor\nWhile the conditional expectation \\(m(X)=\\mathbb{E}[Y \\mid X]\\) is the best predictor of \\(Y\\) among all functions of \\(X\\), its functional form is typically unknown. In particular, the linear CEF model is empirically unlikely to be accurate unless \\(X\\) is discrete and low-dimensional so all interactions are included. Consequently, in most cases it is more realistic to view the linear specification (2.13) as an approximation. In this section we derive a specific approximation with a simple interpretation.\nTheorem \\(2.7\\) showed that the conditional expectation \\(m(X)\\) is the best predictor in the sense that it has the lowest mean squared error among all predictors. By extension, we can define an approximation to the CEF by the linear function with the lowest mean squared error among all linear predictors.\nFor this derivation we require the following regularity condition.\nAssumption \\(2.1\\)\n\n\\(\\mathbb{E}\\left[Y^{2}\\right]<\\infty\\)\n\\(\\mathbb{E}\\|X\\|^{2}<\\infty\\)\n\\(\\boldsymbol{Q}_{X X}=\\mathbb{E}\\left[X X^{\\prime}\\right]\\) is positive definite.\n\nIn Assumption 2.1.2 we use \\(\\|x\\|=\\left(x^{\\prime} x\\right)^{1 / 2}\\) to denote the Euclidean length of the vector \\(x\\).\nThe first two parts of Assumption \\(2.1\\) imply that the variables \\(Y\\) and \\(X\\) have finite means, variances, and covariances. The third part of the assumption is more technical, and its role will become apparent shortly. It is equivalent to imposing that the columns of the matrix \\(\\boldsymbol{Q}_{X X}=\\mathbb{E}\\left[X X^{\\prime}\\right]\\) are linearly independent and that the matrix is invertible.\nA linear predictor for \\(Y\\) is a function \\(X^{\\prime} \\beta\\) for some \\(\\beta \\in \\mathbb{R}^{k}\\). The mean squared prediction error is\n\\[\nS(\\beta)=\\mathbb{E}\\left[\\left(Y-X^{\\prime} \\beta\\right)^{2}\\right] .\n\\]\nThe best linear predictor of \\(Y\\) given \\(X\\), written \\(\\mathscr{P}[Y \\mid X]\\), is found by selecting the \\(\\beta\\) which minimizes \\(S(\\beta)\\).\nDefinition 2.5 The Best Linear Predictor of \\(Y\\) given \\(X\\) is\n\\[\n\\mathscr{P}[Y \\mid X]=X^{\\prime} \\beta\n\\]\nwhere \\(\\beta\\) minimizes the mean squared prediction error\n\\[\nS(\\beta)=\\mathbb{E}\\left[\\left(Y-X^{\\prime} \\beta\\right)^{2}\\right] .\n\\]\nThe minimizer\n\\[\n\\beta=\\underset{b \\in \\mathbb{R}^{k}}{\\operatorname{argmin}} S(b)\n\\]\nis called the Linear Projection Coefficient. We now calculate an explicit expression for its value. The mean squared prediction error (2.17) can be written out as a quadratic function of \\(\\beta\\) :\n\\[\nS(\\beta)=\\mathbb{E}\\left[Y^{2}\\right]-2 \\beta^{\\prime} \\mathbb{E}[X Y]+\\beta^{\\prime} \\mathbb{E}\\left[X X^{\\prime}\\right] \\beta .\n\\]\nThe quadratic structure of \\(S(\\beta)\\) means that we can solve explicitly for the minimizer. The first-order condition for minimization (from Appendix A.20) is\n\\[\n0=\\frac{\\partial}{\\partial \\beta} S(\\beta)=-2 \\mathbb{E}[X Y]+2 \\mathbb{E}\\left[X X^{\\prime}\\right] \\beta .\n\\]\nRewriting \\((2.20)\\) as\n\\[\n2 \\mathbb{E}[X Y]=2 \\mathbb{E}\\left[X X^{\\prime}\\right] \\beta\n\\]\nand dividing by 2 , this equation takes the form\n\\[\n\\boldsymbol{Q}_{X Y}=\\boldsymbol{Q}_{X X} \\beta\n\\]\nwhere \\(\\boldsymbol{Q}_{X Y}=\\mathbb{E}[X Y]\\) is \\(k \\times 1\\) and \\(\\boldsymbol{Q}_{X X}=\\mathbb{E}\\left[X X^{\\prime}\\right]\\) is \\(k \\times k\\). The solution is found by inverting the matrix \\(\\boldsymbol{Q}_{X X}\\), and is written\n\\[\n\\beta=\\boldsymbol{Q}_{X X}^{-1} \\boldsymbol{Q}_{X Y}\n\\]\nor\n\\[\n\\beta=\\left(\\mathbb{E}\\left[X X^{\\prime}\\right]\\right)^{-1} \\mathbb{E}[X Y] .\n\\]\nIt is worth taking the time to understand the notation involved in the expression (2.22). \\(\\boldsymbol{Q}_{X X}\\) is a \\(k \\times k\\) matrix and \\(\\boldsymbol{Q}_{X Y}\\) is a \\(k \\times 1\\) column vector. Therefore, alternative expressions such as \\(\\frac{\\mathbb{E}[X Y]}{\\mathbb{E}\\left[X X^{\\prime}\\right]}\\) or \\(\\mathbb{E}[X Y]\\left(\\mathbb{E}\\left[X X^{\\prime}\\right]\\right)^{-1}\\) are incoherent and incorrect. We also can now see the role of Assumption 2.1.3. It is equivalent to assuming that \\(\\boldsymbol{Q}_{X X}\\) has an inverse \\(\\boldsymbol{Q}_{X X}^{-1}\\) which is necessary for the solution to the normal equations (2.21) to be unique, and equivalently for \\((2.22)\\) to be uniquely defined. In the absence of Assumption \\(2.1 .3\\) there could be multiple solutions to the equation (2.21).\nWe now have an explicit expression for the best linear predictor:\n\\[\n\\mathscr{P}[Y \\mid X]=X^{\\prime}\\left(\\mathbb{E}\\left[X X^{\\prime}\\right]\\right)^{-1} \\mathbb{E}[X Y] .\n\\]\nThis expression is also referred to as the linear projection of \\(Y\\) on \\(X\\).\nThe projection error is\n\\[\ne=Y-X^{\\prime} \\beta .\n\\]\nThis equals the error (2.9) from the regression equation when (and only when) the conditional expectation is linear in \\(X\\), otherwise they are distinct.\nRewriting, we obtain a decomposition of \\(Y\\) into linear predictor and error\n\\[\nY=X^{\\prime} \\beta+e .\n\\]\nIn general, we call equation (2.24) or \\(X^{\\prime} \\beta\\) the best linear predictor of \\(Y\\) given \\(X\\), or the linear projection of \\(Y\\) on \\(X\\). Equation (2.24) is also often called the regression of \\(Y\\) on \\(X\\) but this can sometimes be confusing as economists use the term “regression” in many contexts. (Recall that we said in Section \\(2.15\\) that the linear CEF model is also called the linear regression model.)\nAn important property of the projection error \\(e\\) is\n\\[\n\\mathbb{E}[X e]=0 .\n\\]\nTo see this, using the definitions (2.23) and (2.22) and the matrix properties \\(\\boldsymbol{A} \\boldsymbol{A}^{-1}=\\boldsymbol{I}\\) and \\(\\boldsymbol{I} \\boldsymbol{a}=\\boldsymbol{a}\\),\n\\[\n\\begin{aligned}\n\\mathbb{E}[X e] &=\\mathbb{E}\\left[X\\left(Y-X^{\\prime} \\beta\\right)\\right] \\\\\n&=\\mathbb{E}[X Y]-\\mathbb{E}\\left[X X^{\\prime}\\right]\\left(\\mathbb{E}\\left[X X^{\\prime}\\right]\\right)^{-1} \\mathbb{E}[X Y] \\\\\n&=0\n\\end{aligned}\n\\]\nas claimed.\nEquation (2.25) is a set of \\(k\\) equations, one for each regressor. In other words, (2.25) is equivalent to\n\\[\n\\mathbb{E}\\left[X_{j} e\\right]=0\n\\]\nfor \\(j=1, \\ldots, k\\). As in (2.12), the regressor vector \\(X\\) typically contains a constant, e.g. \\(X_{k}=1\\). In this case (2.27) for \\(j=k\\) is the same as\n\\[\n\\mathbb{E}[e]=0 .\n\\]\nThus the projection error has a mean of zero when the regressor vector contains a constant. (When \\(X\\) does not have a constant (2.28) is not guaranteed. As it is desirable for \\(e\\) to have a zero mean this is a good reason to always include a constant in any regression model.)\nIt is also useful to observe that because \\(\\operatorname{cov}\\left(X_{j}, e\\right)=\\mathbb{E}\\left[X_{j} e\\right]-\\mathbb{E}\\left[X_{j}\\right] \\mathbb{E}[e]\\), then (2.27)-(2.28) together imply that the variables \\(X_{j}\\) and \\(e\\) are uncorrelated.\nThis completes the derivation of the model. We summarize some of the most important properties.\nTheorem 2.9 Properties of Linear Projection Model Under Assumption 2.1,\n\nThe moments \\(\\mathbb{E}\\left[X X^{\\prime}\\right]\\) and \\(\\mathbb{E}[X Y]\\) exist with finite elements.\nThe linear projection coefficient (2.18) exists, is unique, and equals\n\n\\[\n\\beta=\\left(\\mathbb{E}\\left[X X^{\\prime}\\right]\\right)^{-1} \\mathbb{E}[X Y] .\n\\]\n 1. The best linear predictor of \\(Y\\) given \\(X\\) is\n\\[\n\\mathscr{P}(Y \\mid X)=X^{\\prime}\\left(\\mathbb{E}\\left[X X^{\\prime}\\right]\\right)^{-1} \\mathbb{E}[X Y] .\n\\]\n 1. The projection error \\(e=Y-X^{\\prime} \\beta\\) exists. It satisfies \\(\\mathbb{E}\\left[e^{2}\\right]<\\infty\\) and \\(\\mathbb{E}[X e]=0\\).\n\nIf \\(X\\) contains an constant, then \\(\\mathbb{E}[e]=0\\).\nIf \\(\\mathbb{E}|Y|^{r}<\\infty\\) and \\(\\mathbb{E}\\|X\\|^{r}<\\infty\\) for \\(r \\geq 2\\) then \\(\\mathbb{E}|e|^{r}<\\infty\\).\n\nA complete proof of Theorem \\(2.9\\) is given in Section 2.33.\nIt is useful to reflect on the generality of Theorem 2.9. The only restriction is Assumption 2.1. Thus for any random variables \\((Y, X)\\) with finite variances we can define a linear equation (2.24) with the properties listed in Theorem 2.9. Stronger assumptions (such as the linear CEF model) are not necessary. In this sense the linear model (2.24) exists quite generally. However, it is important not to misinterpret the generality of this statement. The linear equation (2.24) is defined as the best linear predictor. It is not necessarily a conditional mean, nor a parameter of a structural or causal economic model. Linear Projection Model\n\\[\n\\begin{aligned}\nY &=X^{\\prime} \\beta+e \\\\\n\\mathbb{E}[X e] &=0 \\\\\n\\beta &=\\left(\\mathbb{E}\\left[X X^{\\prime}\\right]\\right)^{-1} \\mathbb{E}[X Y]\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "chpt02-ce.html#invertibility-and-identification",
    "href": "chpt02-ce.html#invertibility-and-identification",
    "title": "2  Conditional Expectation and Projection",
    "section": "2.21 Invertibility and Identification",
    "text": "2.21 Invertibility and Identification\nThe linear projection coefficient \\(\\beta=\\left(\\mathbb{E}\\left[X X^{\\prime}\\right]\\right)^{-1} \\mathbb{E}[X Y]\\) exists and is unique as long as the \\(k \\times k\\) matrix \\(\\boldsymbol{Q}_{X X}=\\mathbb{E}\\left[X X^{\\prime}\\right]\\) is invertible. The matrix \\(\\boldsymbol{Q}_{X X}\\) is often called the design matrix as in experimental settings the researcher is able to control \\(\\boldsymbol{Q}_{X X}\\) by manipulating the distribution of the regressors \\(X\\).\nObserve that for any non-zero \\(\\alpha \\in \\mathbb{R}^{k}\\),\n\\[\n\\alpha^{\\prime} \\boldsymbol{Q}_{X X} \\alpha=\\mathbb{E}\\left[\\alpha^{\\prime} X X^{\\prime} \\alpha\\right]=\\mathbb{E}\\left[\\left(\\alpha^{\\prime} X\\right)^{2}\\right] \\geq 0\n\\]\nso \\(\\boldsymbol{Q}_{X X}\\) by construction is positive semi-definite, conventionally written as \\(\\boldsymbol{Q}_{X X} \\geq 0\\). The assumption that it is positive definite means that this is a strict inequality, \\(\\mathbb{E}\\left[\\left(\\alpha^{\\prime} X\\right)^{2}\\right]>0\\). This is conventionally written as \\(\\boldsymbol{Q}_{X X}>0\\). This condition means that there is no non-zero vector \\(\\alpha\\) such that \\(\\alpha^{\\prime} X=0\\) identically. Positive definite matrices are invertible. Thus when \\(\\boldsymbol{Q}_{X X}>0\\) then \\(\\beta=\\left(\\mathbb{E}\\left[X X^{\\prime}\\right]\\right)^{-1} \\mathbb{E}[X Y]\\) exists and is uniquely defined. In other words, if we can exclude the possibility that a linear function of \\(X\\) is degenerate, then \\(\\beta\\) is uniquely defined.\nTheorem \\(2.5\\) shows that the linear projection coefficient \\(\\beta\\) is identified (uniquely determined) under Assumption 2.1. The key is invertibility of \\(\\boldsymbol{Q}_{X X}\\). Otherwise, there is no unique solution to the equation\n\\[\n\\boldsymbol{Q}_{X X} \\beta=\\boldsymbol{Q}_{X Y} .\n\\]\nWhen \\(\\boldsymbol{Q}_{X X}\\) is not invertible there are multiple solutions to (2.29). In this case the coefficient \\(\\beta\\) is not identified as it does not have a unique value."
  },
  {
    "objectID": "chpt02-ce.html#minimization",
    "href": "chpt02-ce.html#minimization",
    "title": "2  Conditional Expectation and Projection",
    "section": "2.22 Minimization",
    "text": "2.22 Minimization\nThe mean squared prediction error (2.19) is a function with vector argument of the form\n\\[\nf(x)=a-2 b^{\\prime} x+x^{\\prime} \\boldsymbol{C} x\n\\]\nwhere \\(\\boldsymbol{C}>0\\). For any function of this form, the unique minimizer is\n\\[\nx=\\boldsymbol{C}^{-1} b .\n\\]\nTo see that this is the unique minimizer we present two proofs. The first uses matrix calculus. From Appendix A.20\n\\[\n\\begin{gathered}\n\\frac{\\partial}{\\partial x}\\left(b^{\\prime} x\\right)=b \\\\\n\\frac{\\partial}{\\partial x}\\left(x^{\\prime} \\boldsymbol{C} x\\right)=2 \\boldsymbol{C} x \\\\\n\\frac{\\partial^{2}}{\\partial x \\partial x^{\\prime}}\\left(x^{\\prime} \\boldsymbol{C} x\\right)=2 \\boldsymbol{C} .\n\\end{gathered}\n\\]\nUsing (2.31) and (2.32), we find\n\\[\n\\frac{\\partial}{\\partial x} f(x)=-2 b+2 \\boldsymbol{C} x .\n\\]\nThe first-order condition for minimization sets this derivative equal to zero. Thus the solution satisfies \\(-2 b+2 \\boldsymbol{C} x=0\\). Solving for \\(x\\) we find (2.30). Using (2.33) we also find\n\\[\n\\frac{\\partial^{2}}{\\partial x \\partial x^{\\prime}} f(x)=2 \\boldsymbol{C}>0\n\\]\nwhich is the second-order condition for minimization. This shows that (2.30) is the unique minimizer of \\(f(x)\\).\nOur second proof is algebraic. Re-write \\(f(x)\\) as\n\\[\nf(x)=\\left(a-b^{\\prime} \\boldsymbol{C}^{-1} b\\right)+\\left(x-\\boldsymbol{C}^{-1} b\\right)^{\\prime} \\boldsymbol{C}\\left(x-\\boldsymbol{C}^{-1} b\\right) .\n\\]\nThe first term does not depend on \\(x\\) so does not affect the minimizer. The second term is a quadratic form in a positive definite matrix. This means that for any non-zero \\(\\alpha, \\alpha^{\\prime} \\boldsymbol{C} \\alpha>0\\). Thus for \\(x \\neq C^{-1} b\\), the second-term is strictly positive, yet for \\(x=C^{-1} b\\) this term equals zero. It is therefore minimized at \\(x=C^{-1} b\\) as claimed."
  },
  {
    "objectID": "chpt02-ce.html#illustrations-of-best-linear-predictor",
    "href": "chpt02-ce.html#illustrations-of-best-linear-predictor",
    "title": "2  Conditional Expectation and Projection",
    "section": "2.23 Illustrations of Best Linear Predictor",
    "text": "2.23 Illustrations of Best Linear Predictor\nWe illustrate the best linear predictor (projection) using three log wage equations introduced in earlier sections.\nFor our first example, we consider a model with the two dummy variables for gender and race similar to Table 2.1. As we learned in Section 2.17, the entries in this table can be equivalently expressed by a linear CEF. For simplicity, let’s consider the CEF of \\(\\log (\\) wage \\()\\) as a function of Black and female.\n\\[\n\\mathbb{E}[\\log (\\text { wage }) \\mid \\text { Black, female }]=-0.20 \\text { Black }-0.24 \\text { female }+0.10 \\text { Black } \\times \\text { female }+3.06 \\text {. }\n\\]\nThis is a CEF as the variables are binary and all interactions are included.\nNow consider a simpler model omitting the interaction effect. This is the linear projection on the variables Black and female\n\\[\n\\mathscr{P}[\\log (\\text { wage }) \\mid \\text { Black, female }]=-0.15 \\text { Black }-0.23 \\text { female }+3.06 .\n\\]\nWhat is the difference? The full CEF (2.34) shows that the race gap is differentiated by gender: it is \\(20 %\\) for Black men (relative to non-Black men) and \\(10 %\\) for Black women (relative to non-Black women). The projection model (2.35) simplifies this analysis, calculating an average \\(15 %\\) wage gap for Black wage earners, ignoring the role of gender. Notice that this is despite the fact that gender is included in (2.35).\n\n\nProjections onto Education\n\n\n\nProjections onto Experience\n\nFigure 2.6: Projections of Log Wage onto Education and Experience\nFor our second example we consider the CEF of log wages as a function of years of education for white men which was illustrated in Figure \\(2.3\\) and is repeated in Figure 2.6(a). Superimposed on the figure are two projections. The first (given by the dashed line) is the linear projection of log wages on years of education\n\\[\n\\mathscr{P}[\\log (\\text { wage }) \\mid \\text { education }]=0.11 \\text { education }+1.5 \\text {. }\n\\]\nThis simple equation indicates an average \\(11 %\\) increase in wages for every year of education. An inspection of the Figure shows that this approximation works well for education \\(\\geq 9\\), but under-predicts for individuals with lower levels of education. To correct this imbalance we use a linear spline equation which allows different rates of return above and below 9 years of education:\n\\[\n\\begin{aligned}\n&\\mathscr{P}[\\log (\\text { wage }) \\mid \\text { education, }(\\text { education }-9) \\times \\mathbb{1} \\text { education }>9\\}] \\\\\n&=0.02 \\text { education }+0.10 \\times(\\text { education }-9) \\times \\mathbb{1} \\text { education }>9\\}+2.3 .\n\\end{aligned}\n\\]\nThis equation is displayed in Figure 2.6(a) using the solid line, and appears to fit much better. It indicates a \\(2 %\\) increase in mean wages for every year of education below 9 , and a \\(12 %\\) increase in mean wages for every year of education above 9 . It is still an approximation to the conditional mean but it appears to be fairly reasonable.\nFor our third example we take the CEF of log wages as a function of years of experience for white men with 12 years of education, which was illustrated in Figure \\(2.4\\) and is repeated as the solid line in Figure 2.6(b). Superimposed on the figure are two projections. The first (given by the dot-dashed line) is the linear projection on experience\n\\[\n\\mathscr{P}[\\log (\\text { wage }) \\mid \\text { experience }]=0.011 \\text { experience }+2.5\n\\]\nand the second (given by the dashed line) is the linear projection on experience and its square\n\\[\n\\mathscr{P}[\\log (\\text { wage }) \\mid \\text { experience }]=0.046 \\text { experience }-0.0007 \\text { experience }^{2}+2.3 \\text {. }\n\\]\nIt is fairly clear from an examination of Figure \\(2.6(\\mathrm{~b})\\) that the first linear projection is a poor approximation. It over-predicts wages for young and old workers, under-predicts for the rest, and misses the strong downturn in expected wages for older wage-earners. The second projection fits much better. We can call this equation a quadratic projection because the function is quadratic in experience."
  },
  {
    "objectID": "chpt02-ce.html#linear-predictor-error-variance",
    "href": "chpt02-ce.html#linear-predictor-error-variance",
    "title": "2  Conditional Expectation and Projection",
    "section": "2.24 Linear Predictor Error Variance",
    "text": "2.24 Linear Predictor Error Variance\nAs in the CEF model, we define the error variance as \\(\\sigma^{2}=\\mathbb{E}\\left[e^{2}\\right]\\). Setting \\(Q_{Y Y}=\\mathbb{E}\\left[Y^{2}\\right]\\) and \\(\\boldsymbol{Q}_{Y X}=\\) \\(\\mathbb{E}\\left[Y X^{\\prime}\\right]\\) we can write \\(\\sigma^{2}\\) as\n\\[\n\\begin{aligned}\n\\sigma^{2} &=\\mathbb{E}\\left[\\left(Y-X^{\\prime} \\beta\\right)^{2}\\right] \\\\\n&=\\mathbb{E}\\left[Y^{2}\\right]-2 \\mathbb{E}\\left[Y X^{\\prime}\\right] \\beta+\\beta^{\\prime} \\mathbb{E}\\left[X X^{\\prime}\\right] \\beta \\\\\n&=Q_{Y Y}-2 \\boldsymbol{Q}_{Y X} \\boldsymbol{Q}_{X X}^{-1} \\boldsymbol{Q}_{X Y}+\\boldsymbol{Q}_{Y X} \\boldsymbol{Q}_{X X}^{-1} \\boldsymbol{Q}_{X X} \\boldsymbol{Q}_{X X}^{-1} \\boldsymbol{Q}_{X Y} \\\\\n&=Q_{Y Y}-\\boldsymbol{Q}_{Y X} \\boldsymbol{Q}_{X X}^{-1} \\boldsymbol{Q}_{X Y} \\\\\n& \\stackrel{\\text { def }}{=} Q_{Y Y \\cdot X} .\n\\end{aligned}\n\\]\nOne useful feature of this formula is that it shows that \\(Q_{Y Y \\cdot X}=Q_{Y Y}-\\boldsymbol{Q}_{Y X} \\boldsymbol{Q}_{X X}^{-1} \\boldsymbol{Q}_{X Y}\\) equals the variance of the error from the linear projection of \\(Y\\) on \\(X\\)."
  },
  {
    "objectID": "chpt02-ce.html#regression-coefficients",
    "href": "chpt02-ce.html#regression-coefficients",
    "title": "2  Conditional Expectation and Projection",
    "section": "2.25 Regression Coefficients",
    "text": "2.25 Regression Coefficients\nSometimes it is useful to separate the constant from the other regressors and write the linear projection equation in the format\n\\[\nY=X^{\\prime} \\beta+\\alpha+e\n\\]\nwhere \\(\\alpha\\) is the intercept and \\(X\\) does not contain a constant.\nTaking expectations of this equation, we find\n\\[\n\\mathbb{E}[Y]=\\mathbb{E}\\left[X^{\\prime} \\beta\\right]+\\mathbb{E}[\\alpha]+\\mathbb{E}[e]\n\\]\nor \\(\\mu_{Y}=\\mu_{X}^{\\prime} \\beta+\\alpha\\) where \\(\\mu_{Y}=\\mathbb{E}[Y]\\) and \\(\\mu_{X}=\\mathbb{E}[X]\\), since \\(\\mathbb{E}[e]=0\\) from (2.28). (While \\(X\\) does not contain a constant, the equation does so (2.28) still applies.) Rearranging, we find \\(\\alpha=\\mu_{Y}-\\mu_{X}^{\\prime} \\beta\\). Subtracting this equation from (2.37) we find\n\\[\nY-\\mu_{Y}=\\left(X-\\mu_{X}\\right)^{\\prime} \\beta+e,\n\\]\na linear equation between the centered variables \\(Y-\\mu_{Y}\\) and \\(X-\\mu_{X}\\). (They are centered at their means so are mean-zero random variables.) Because \\(X-\\mu_{X}\\) is uncorrelated with \\(e\\), (2.38) is also a linear projection. Thus by the formula for the linear projection model,\n\\[\n\\begin{aligned}\n\\beta &=\\left(\\mathbb{E}\\left[\\left(X-\\mu_{X}\\right)\\left(X-\\mu_{X}\\right)^{\\prime}\\right]\\right)^{-1} \\mathbb{E}\\left[\\left(X-\\mu_{X}\\right)\\left(Y-\\mu_{Y}\\right)\\right] \\\\\n&=\\operatorname{var}[X]^{-1} \\operatorname{cov}(X, Y)\n\\end{aligned}\n\\]\na function only of the covariances \\({ }^{10}\\) of \\(X\\) and \\(Y\\).\nTheorem 2.10 In the linear projection model \\(Y=X^{\\prime} \\beta+\\alpha+e\\),\n\\[\n\\alpha=\\mu_{Y}-\\mu_{X}^{\\prime} \\beta\n\\]\nand\n\\[\n\\beta=\\operatorname{var}[X]^{-1} \\operatorname{cov}(X, Y) .\n\\]"
  },
  {
    "objectID": "chpt02-ce.html#regression-sub-vectors",
    "href": "chpt02-ce.html#regression-sub-vectors",
    "title": "2  Conditional Expectation and Projection",
    "section": "2.26 Regression Sub-Vectors",
    "text": "2.26 Regression Sub-Vectors\nLet the regressors be partitioned as\n\\[\nX=\\left(\\begin{array}{l}\nX_{1} \\\\\nX_{2}\n\\end{array}\\right)\n\\]\nWe can write the projection of \\(Y\\) on \\(X\\) as\n\\[\n\\begin{aligned}\nY &=X^{\\prime} \\beta+e \\\\\n&=X_{1}^{\\prime} \\beta_{1}+X_{2}^{\\prime} \\beta_{2}+e \\\\\n\\mathbb{E}[X e] &=0 .\n\\end{aligned}\n\\]\nIn this section we derive formulae for the sub-vectors \\(\\beta_{1}\\) and \\(\\beta_{2}\\).\nPartition \\(\\boldsymbol{Q}_{X X}\\) conformably with \\(X\\)\n\\[\n\\boldsymbol{Q}_{X X}=\\left[\\begin{array}{ll}\n\\boldsymbol{Q}_{11} & \\boldsymbol{Q}_{12} \\\\\n\\boldsymbol{Q}_{21} & \\boldsymbol{Q}_{22}\n\\end{array}\\right]=\\left[\\begin{array}{ll}\n\\mathbb{E}\\left[X_{1} X_{1}^{\\prime}\\right] & \\mathbb{E}\\left[X_{1} X_{2}^{\\prime}\\right] \\\\\n\\mathbb{E}\\left[X_{2} X_{1}^{\\prime}\\right] & \\mathbb{E}\\left[X_{2} X_{2}^{\\prime}\\right]\n\\end{array}\\right]\n\\]\nand similarly\n\\[\n\\boldsymbol{Q}_{X Y}=\\left[\\begin{array}{l}\n\\boldsymbol{Q}_{1 Y} \\\\\n\\boldsymbol{Q}_{2 Y}\n\\end{array}\\right]=\\left[\\begin{array}{c}\n\\mathbb{E}\\left[X_{1} Y\\right] \\\\\n\\mathbb{E}\\left[X_{2} Y\\right]\n\\end{array}\\right] .\n\\]\nBy the partitioned matrix inversion formula (A.3)\n\\[\n\\boldsymbol{Q}_{X X}^{-1}=\\left[\\begin{array}{ll}\n\\boldsymbol{Q}_{11} & \\boldsymbol{Q}_{12} \\\\\n\\boldsymbol{Q}_{21} & \\boldsymbol{Q}_{22}\n\\end{array}\\right]^{-1} \\stackrel{\\operatorname{def}}{=}\\left[\\begin{array}{ll}\n\\boldsymbol{Q}^{11} & \\boldsymbol{Q}^{12} \\\\\n\\boldsymbol{Q}^{21} & \\boldsymbol{Q}^{22}\n\\end{array}\\right]=\\left[\\begin{array}{cc}\n\\boldsymbol{Q}_{11 \\cdot 2}^{-1} & -\\boldsymbol{Q}_{11 \\cdot 2}^{-1} \\boldsymbol{Q}_{12} \\boldsymbol{Q}_{22}^{-1} \\\\\n-\\boldsymbol{Q}_{22 \\cdot 1}^{-1} \\boldsymbol{Q}_{21} \\boldsymbol{Q}_{11}^{-1} & \\boldsymbol{Q}_{22 \\cdot 1}^{-1}\n\\end{array}\\right]\n\\]\n\\({ }^{10}\\) The covariance matrix between vectors \\(X\\) and \\(Z\\) is \\(\\operatorname{cov}(X, Z)=\\mathbb{E}\\left[(X-\\mathbb{E}[X])(Z-\\mathbb{E}[Z])^{\\prime}\\right]\\). The covariance matrix of the \\(\\operatorname{vector} X\\) is \\(\\operatorname{var}[X]=\\operatorname{cov}(X, X)=\\mathbb{E}\\left[(X-\\mathbb{E}[X])(X-\\mathbb{E}[X])^{\\prime}\\right]\\). where \\(\\boldsymbol{Q}_{11 \\cdot 2} \\stackrel{\\text { def }}{=} \\boldsymbol{Q}_{11}-\\boldsymbol{Q}_{12} \\boldsymbol{Q}_{22}^{-1} \\boldsymbol{Q}_{21}\\) and \\(\\boldsymbol{Q}_{22 \\cdot 1} \\stackrel{\\text { def }}{=} \\boldsymbol{Q}_{22}-\\boldsymbol{Q}_{21} \\boldsymbol{Q}_{11}^{-1} \\boldsymbol{Q}_{12}\\). Thus\n\\[\n\\begin{aligned}\n\\beta &=\\left(\\begin{array}{l}\n\\beta_{1} \\\\\n\\beta_{2}\n\\end{array}\\right) \\\\\n&=\\left[\\begin{array}{cc}\n\\boldsymbol{Q}_{11 \\cdot 2}^{-1} & -\\boldsymbol{Q}_{11 \\cdot 2}^{-1} \\boldsymbol{Q}_{12} \\boldsymbol{Q}_{22}^{-1} \\\\\n-\\boldsymbol{Q}_{22 \\cdot 1}^{-1} \\boldsymbol{Q}_{21} \\boldsymbol{Q}_{11}^{-1} & \\boldsymbol{Q}_{22 \\cdot 1}^{-1}\n\\end{array}\\right]\\left[\\begin{array}{l}\n\\boldsymbol{Q}_{1 Y} \\\\\n\\boldsymbol{Q}_{2 Y}\n\\end{array}\\right] \\\\\n&=\\left(\\begin{array}{c}\n\\boldsymbol{Q}_{11 \\cdot 2}^{-1}\\left(\\boldsymbol{Q}_{1 y}-\\boldsymbol{Q}_{12} \\boldsymbol{Q}_{22}^{-1} \\boldsymbol{Q}_{2 Y}\\right) \\\\\n\\boldsymbol{Q}_{22 \\cdot 1}^{-1}\\left(\\boldsymbol{Q}_{2 y}-\\boldsymbol{Q}_{21} \\boldsymbol{Q}_{11}^{-1} \\boldsymbol{Q}_{1 Y}\\right)\n\\end{array}\\right) \\\\\n&=\\left(\\begin{array}{c}\n\\boldsymbol{Q}_{11 \\cdot 2}^{-1} \\boldsymbol{Q}_{1 Y \\cdot 2} \\\\\n\\boldsymbol{Q}_{22 \\cdot 1}^{-1} \\boldsymbol{Q}_{2 Y \\cdot 1}\n\\end{array}\\right)\n\\end{aligned}\n\\]\nWe have shown that \\(\\beta_{1}=\\boldsymbol{Q}_{11 \\cdot 2}^{-1} \\boldsymbol{Q}_{1 Y \\cdot 2}\\) and \\(\\beta_{2}=\\boldsymbol{Q}_{22 \\cdot 1}^{-1} \\boldsymbol{Q}_{2 Y \\cdot 1}\\)."
  },
  {
    "objectID": "chpt02-ce.html#coefficient-decomposition",
    "href": "chpt02-ce.html#coefficient-decomposition",
    "title": "2  Conditional Expectation and Projection",
    "section": "2.27 Coefficient Decomposition",
    "text": "2.27 Coefficient Decomposition\nIn the previous section we derived formulae for the coefficient sub-vectors \\(\\beta_{1}\\) and \\(\\beta_{2}\\). We now use these formulae to give a useful interpretation of the coefficients in terms of an iterated projection.\nTake equation (2.42) for the case \\(\\operatorname{dim}\\left(X_{1}\\right)=1\\) so that \\(\\beta_{1} \\in \\mathbb{R}\\).\n\\[\nY=X_{1} \\beta_{1}+X_{2}^{\\prime} \\beta_{2}+e .\n\\]\nNow consider the projection of \\(X_{1}\\) on \\(X_{2}\\) :\n\\[\n\\begin{aligned}\nX_{1} &=X_{2}^{\\prime} \\gamma_{2}+u_{1} \\\\\n\\mathbb{E}\\left[X_{2} u_{1}\\right] &=0 .\n\\end{aligned}\n\\]\nFrom (2.22) and (2.36), \\(\\gamma_{2}=\\boldsymbol{Q}_{22}^{-1} \\boldsymbol{Q}_{21}\\) and \\(\\mathbb{E}\\left[u_{1}^{2}\\right]=\\boldsymbol{Q}_{11 \\cdot 2}=\\boldsymbol{Q}_{11}-\\boldsymbol{Q}_{12} \\boldsymbol{Q}_{22}^{-1} \\boldsymbol{Q}_{21}\\). We can also calculate that\n\\[\n\\mathbb{E}\\left[u_{1} Y\\right]=\\mathbb{E}\\left[\\left(X_{1}-\\gamma_{2}^{\\prime} X_{2}\\right) Y\\right]=\\mathbb{E}\\left[X_{1} Y\\right]-\\gamma_{2}^{\\prime} \\mathbb{E}\\left[X_{2} Y\\right]=\\boldsymbol{Q}_{1 Y}-\\boldsymbol{Q}_{12} \\boldsymbol{Q}_{22}^{-1} \\boldsymbol{Q}_{2 Y}=\\boldsymbol{Q}_{1 Y \\cdot 2} .\n\\]\nWe have found that\n\\[\n\\beta_{1}=\\boldsymbol{Q}_{11 \\cdot 2}^{-1} \\boldsymbol{Q}_{1 Y \\cdot 2}=\\frac{\\mathbb{E}\\left[u_{1} Y\\right]}{\\mathbb{E}\\left[u_{1}^{2}\\right]}\n\\]\nthe coefficient from the simple regression of \\(Y\\) on \\(u_{1}\\).\nWhat this means is that in the multivariate projection equation (2.44), the coefficient \\(\\beta_{1}\\) equals the projection coefficient from a regression of \\(Y\\) on \\(u_{1}\\), the error from a projection of \\(X_{1}\\) on the other regressors \\(X_{2}\\). The error \\(u_{1}\\) can be thought of as the component of \\(X_{1}\\) which is not linearly explained by the other regressors. Thus the coefficient \\(\\beta_{1}\\) equals the linear effect of \\(X_{1}\\) on \\(Y\\) after stripping out the effects of the other variables.\nThere was nothing special in the choice of the variable \\(X_{1}\\). This derivation applies symmetrically to all coefficients in a linear projection. Each coefficient equals the simple regression of \\(Y\\) on the error from a projection of that regressor on all the other regressors. Each coefficient equals the linear effect of that variable on \\(Y\\) after linearly controlling for all the other regressors."
  },
  {
    "objectID": "chpt02-ce.html#omitted-variable-bias",
    "href": "chpt02-ce.html#omitted-variable-bias",
    "title": "2  Conditional Expectation and Projection",
    "section": "2.28 Omitted Variable Bias",
    "text": "2.28 Omitted Variable Bias\nAgain, let the regressors be partitioned as in (2.41). Consider the projection of \\(Y\\) on \\(X_{1}\\) only. Perhaps this is done because the variables \\(X_{2}\\) are not observed. This is the equation\n\\[\n\\begin{aligned}\nY &=X_{1}^{\\prime} \\gamma_{1}+u \\\\\n\\mathbb{E}\\left[X_{1} u\\right] &=0 .\n\\end{aligned}\n\\]\nNotice that we have written the coefficient as \\(\\gamma_{1}\\) rather than \\(\\beta_{1}\\) and the error as \\(u\\) rather than \\(e\\). This is because (2.45) is different than (2.42). Goldberger (1991) introduced the catchy labels long regression for (2.42) and short regression for (2.45) to emphasize the distinction.\nTypically, \\(\\beta_{1} \\neq \\gamma_{1}\\), except in special cases. To see this, we calculate\n\\[\n\\begin{aligned}\n\\gamma_{1} &=\\left(\\mathbb{E}\\left[X_{1} X_{1}^{\\prime}\\right]\\right)^{-1} \\mathbb{E}\\left[X_{1} Y\\right] \\\\\n&=\\left(\\mathbb{E}\\left[X_{1} X_{1}^{\\prime}\\right]\\right)^{-1} \\mathbb{E}\\left[X_{1}\\left(X_{1}^{\\prime} \\beta_{1}+X_{2}^{\\prime} \\beta_{2}+e\\right)\\right] \\\\\n&=\\beta_{1}+\\left(\\mathbb{E}\\left[X_{1} X_{1}^{\\prime}\\right]\\right)^{-1} \\mathbb{E}\\left[X_{1} X_{2}^{\\prime}\\right] \\beta_{2} \\\\\n&=\\beta_{1}+\\Gamma_{12} \\beta_{2}\n\\end{aligned}\n\\]\nwhere \\(\\Gamma_{12}=\\boldsymbol{Q}_{11}^{-1} \\boldsymbol{Q}_{12}\\) is the coefficient matrix from a projection of \\(X_{2}\\) on \\(X_{1}\\) where we use the notation from Section \\(2.22\\).\nObserve that \\(\\gamma_{1}=\\beta_{1}+\\Gamma_{12} \\beta_{2} \\neq \\beta_{1}\\) unless \\(\\Gamma_{12}=0\\) or \\(\\beta_{2}=0\\). Thus the short and long regressions have different coefficients. They are the same only under one of two conditions. First, if the projection of \\(X_{2}\\) on \\(X_{1}\\) yields a set of zero coefficients (they are uncorrelated), or second, if the coefficient on \\(X_{2}\\) in (2.42) is zero. The difference \\(\\Gamma_{12} \\beta_{2}\\) between \\(\\gamma_{1}\\) and \\(\\beta_{1}\\) is known as omitted variable bias. It is the consequence of omission of a relevant correlated variable.\nTo avoid omitted variables bias the standard advice is to include all potentially relevant variables in estimated models. By construction, the general model will be free of such bias. Unfortunately in many cases it is not feasible to completely follow this advice as many desired variables are not observed. In this case, the possibility of omitted variables bias should be acknowledged and discussed in the course of an empirical investigation.\nFor example, suppose \\(Y\\) is log wages, \\(X_{1}\\) is education, and \\(X_{2}\\) is intellectual ability. It seems reasonable to suppose that education and intellectual ability are positively correlated (highly able individuals attain higher levels of education) which means \\(\\Gamma_{12}>0\\). It also seems reasonable to suppose that conditional on education, individuals with higher intelligence will earn higher wages on average, so that \\(\\beta_{2}>0\\). This implies that \\(\\Gamma_{12} \\beta_{2}>0\\) and \\(\\gamma_{1}=\\beta_{1}+\\Gamma_{12} \\beta_{2}>\\beta_{1}\\). Therefore, it seems reasonable to expect that in a regression of wages on education with intelligence omitted (as the latter is not measured), the coefficient on education is higher than in a regression where intelligence is included. In other words, in this context the omitted variable biases the regression coefficient upwards. It is possible, for example, that \\(\\beta_{1}=0\\) so that education has no direct effect on wages yet \\(\\gamma_{1}=\\Gamma_{12} \\beta_{2}>0\\) meaning that the regression coefficient on education alone is positive, but is a consequence of the unmodeled correlation between education and intellectual ability.\nUnfortunately, the above simple characterization of omitted variable bias does not immediately carry over to more complicated settings, as discovered by Luca, Magnus, and Peracchi (2018). For example, suppose we compare three nested projections\n\\[\n\\begin{aligned}\n&Y=X_{1}^{\\prime} \\gamma_{1}+u_{1} \\\\\n&Y=X_{1}^{\\prime} \\delta_{1}+X_{2}^{\\prime} \\delta_{2}+u_{2} \\\\\n&Y=X_{1}^{\\prime} \\beta_{1}+X_{2}^{\\prime} \\beta_{2}+X_{3}^{\\prime} \\beta_{3}+e .\n\\end{aligned}\n\\]\nWe can call them short, medium, and long regressions. Suppose that the parameter of interest is \\(\\beta_{1}\\) in the long regression. We are interested in the consequences of omitting \\(X_{3}\\) when estimating the medium regression, and of omitting both \\(X_{2}\\) and \\(X_{3}\\) when estimating the short regression. In particular we are interested in the question: Is it better to estimate the short or medium regression, given that both omit \\(X_{3}\\) ? Intuition suggests that the medium regression should be “less biased” but it is worth investigating in greater detail. By similar calculations to those above, we find that\n\\[\n\\begin{aligned}\n&\\gamma_{1}=\\beta_{1}+\\Gamma_{12} \\beta_{2}+\\Gamma_{13} \\beta_{3} \\\\\n&\\delta_{1}=\\beta_{1}+\\Gamma_{13 \\cdot 2} \\beta_{3}\n\\end{aligned}\n\\]\nwhere \\(\\Gamma_{13 \\cdot 2}=\\boldsymbol{Q}_{11 \\cdot 2}^{-1} \\boldsymbol{Q}_{13 \\cdot 2}\\) using the notation from Section \\(2.22\\).\nWe see that the bias in the short regression coefficient is \\(\\Gamma_{12} \\beta_{2}+\\Gamma_{13} \\beta_{3}\\) which depends on both \\(\\beta_{2}\\) and \\(\\beta_{3}\\), while that for the medium regression coefficient is \\(\\Gamma_{13 \\cdot 2} \\beta_{3}\\) which only depends on \\(\\beta_{3}\\). So the bias for the medium regression is less complicated and intuitively seems more likely to be smaller than that of the short regression. However it is impossible to strictly rank the two. It is quite possible that \\(\\gamma_{1}\\) is less biased than \\(\\delta_{1}\\). Thus as a general rule it is unknown if estimation of the medium regression will be less biased than estimation of the short regression."
  },
  {
    "objectID": "chpt02-ce.html#best-linear-approximation",
    "href": "chpt02-ce.html#best-linear-approximation",
    "title": "2  Conditional Expectation and Projection",
    "section": "2.29 Best Linear Approximation",
    "text": "2.29 Best Linear Approximation\nThere are alternative ways we could construct a linear approximation \\(X^{\\prime} \\beta\\) to the conditional expectation \\(m(X)\\). In this section we show that one alternative approach turns out to yield the same answer as the best linear predictor.\nWe start by defining the mean-square approximation error of \\(X^{\\prime} \\beta\\) to \\(m(X)\\) as the expected squared difference between \\(X^{\\prime} \\beta\\) and the conditional expectation \\(m(X)\\)\n\\[\nd(\\beta)=\\mathbb{E}\\left[\\left(m(X)-X^{\\prime} \\beta\\right)^{2}\\right] .\n\\]\nThe function \\(d(\\beta)\\) is a measure of the deviation of \\(X^{\\prime} \\beta\\) from \\(m(X)\\). If the two functions are identical then \\(d(\\beta)=0\\), otherwise \\(d(\\beta)>0\\). We can also view the mean-square difference \\(d(\\beta)\\) as a density-weighted average of the function \\(\\left(m(X)-X^{\\prime} \\beta\\right)^{2}\\) since\n\\[\nd(\\beta)=\\int_{\\mathbb{R}^{k}}\\left(m(x)-x^{\\prime} \\beta\\right)^{2} f_{X}(x) d x\n\\]\nwhere \\(f_{X}(x)\\) is the marginal density of \\(X\\).\nWe can then define the best linear approximation to the conditional \\(m(X)\\) as the function \\(X^{\\prime} \\beta\\) obtained by selecting \\(\\beta\\) to minimize \\(d(\\beta)\\) :\n\\[\n\\beta=\\underset{b \\in \\mathbb{R}^{k}}{\\operatorname{argmin}} d(b) .\n\\]\nSimilar to the best linear predictor we are measuring accuracy by expected squared error. The difference is that the best linear predictor (2.18) selects \\(\\beta\\) to minimize the expected squared prediction error, while the best linear approximation (2.46) selects \\(\\beta\\) to minimize the expected squared approximation error.\nDespite the different definitions, it turns out that the best linear predictor and the best linear approximation are identical. By the same steps as in (2.18) plus an application of conditional expectations we can find that\n\\[\n\\begin{aligned}\n\\beta &=\\left(\\mathbb{E}\\left[X X^{\\prime}\\right]\\right)^{-1} \\mathbb{E}[X m(X)] \\\\\n&=\\left(\\mathbb{E}\\left[X X^{\\prime}\\right]\\right)^{-1} \\mathbb{E}[X Y]\n\\end{aligned}\n\\]\n(see Exercise 2.19). Thus (2.46) equals (2.18). We conclude that the definition (2.46) can be viewed as an alternative motivation for the linear projection coefficient."
  },
  {
    "objectID": "chpt02-ce.html#regression-to-the-mean",
    "href": "chpt02-ce.html#regression-to-the-mean",
    "title": "2  Conditional Expectation and Projection",
    "section": "2.30 Regression to the Mean",
    "text": "2.30 Regression to the Mean\nThe term regression originated in an influential paper by Francis Galton (1886) where he examined the joint distribution of the stature (height) of parents and children. Effectively, he was estimating the conditional expectation of children’s height given their parent’s height. Galton discovered that this conditional expectation was approximately linear with a slope of \\(2 / 3\\). This implies that on average a child’s height is more mediocre (average) than his or her parent’s height. Galton called this phenomenon regression to the mean, and the label regression has stuck to this day to describe most conditional relationships.\nOne of Galton’s fundamental insights was to recognize that if the marginal distributions of \\(Y\\) and \\(X\\) are the same (e.g. the heights of children and parents in a stable environment) then the regression slope in a linear projection is always less than one.\nTo be more precise, take the simple linear projection\n\\[\nY=X \\beta+\\alpha+e\n\\]\nwhere \\(Y\\) equals the height of the child and \\(X\\) equals the height of the parent. Assume that \\(Y\\) and \\(X\\) have the same expectation so that \\(\\mu_{Y}=\\mu_{X}=\\mu\\). Then from (2.39) \\(\\alpha=(1-\\beta) \\mu\\) so we can write the linear projection (2.49) as\n\\[\n\\mathscr{P}(Y \\mid X)=(1-\\beta) \\mu+X \\beta .\n\\]\nThis shows that the projected height of the child is a weighted average of the population expectation \\(\\mu\\) and the parent’s height \\(X\\) with weights \\(\\beta\\) and \\(1-\\beta\\). When the height distribution is stable across generations so that \\(\\operatorname{var}[Y]=\\operatorname{var}[X]\\), then this slope is the simple correlation of \\(Y\\) and \\(X\\). Using (2.40)\n\\[\n\\beta=\\frac{\\operatorname{cov}(X, Y)}{\\operatorname{var}[X]}=\\operatorname{corr}(X, Y) .\n\\]\nBy the Cauchy-Schwarz inequality (B.32), \\(-1 \\leq \\operatorname{corr}(X, Y) \\leq 1\\), with \\(\\operatorname{corr}(X, Y)=1\\) only in the degenerate case \\(Y=X\\). Thus if we exclude degeneracy, \\(\\beta\\) is strictly less than 1 .\nThis means that on average, a child’s height is more mediocre (closer to the population average) than the parent’s.\nA common error - known as the regression fallacy - is to infer from \\(\\beta<1\\) that the population is converging, meaning that its variance is declining towards zero. This is a fallacy because we derived the implication \\(\\beta<1\\) under the assumption of constant means and variances. So certainly \\(\\beta<1\\) does not imply that the variance \\(Y\\) is less than than the variance of \\(X\\).\nAnother way of seeing this is to examine the conditions for convergence in the context of equation (2.49). Since \\(X\\) and \\(e\\) are uncorrelated, it follows that\n\\[\n\\operatorname{var}[Y]=\\beta^{2} \\operatorname{var}[X]+\\operatorname{var}[e] .\n\\]\nThen \\(\\operatorname{var}[Y]<\\operatorname{var}[X]\\) if and only if\n\\[\n\\beta^{2}<1-\\frac{\\operatorname{var}[e]}{\\operatorname{var}[X]}\n\\]\nwhich is not implied by the simple condition \\(|\\beta|<1\\).\nThe regression fallacy arises in related empirical situations. Suppose you sort families into groups by the heights of the parents, and then plot the average heights of each subsequent generation over time. If the population is stable, the regression property implies that the plots lines will converge-children’s height will be more average than their parents. The regression fallacy is to incorrectly conclude that the population is converging. A message to be learned from this example is that such plots are misleading for inferences about convergence. The regression fallacy is subtle. It is easy for intelligent economists to succumb to its temptation. A famous example is The Triumph of Mediocrity in Business by Horace Secrist published in 1933. In this book, Secrist carefully and with great detail documented that in a sample of department stores over 19201930, when he divided the stores into groups based on 1920-1921 profits, and plotted the average profits of these groups for the subsequent 10 years, he found clear and persuasive evidence for convergence “toward mediocrity”. Of course, there was no discovery - regression to the mean is a necessary feature of stable distributions."
  },
  {
    "objectID": "chpt02-ce.html#reverse-regression",
    "href": "chpt02-ce.html#reverse-regression",
    "title": "2  Conditional Expectation and Projection",
    "section": "2.31 Reverse Regression",
    "text": "2.31 Reverse Regression\nGalton noticed another interesting feature of the bivariate distribution. There is nothing special about a regression of \\(Y\\) on \\(X\\). We can also regress \\(X\\) on \\(Y\\). (In his heredity example this is the best linear predictor of the height of parents given the height of their children.) This regression takes the form\n\\[\nX=Y \\beta^{*}+\\alpha^{*}+e^{*} .\n\\]\nThis is sometimes called the reverse regression. In this equation, the coefficients \\(\\alpha^{*}, \\beta^{*}\\) and error \\(e^{*}\\) are defined by linear projection. In a stable population we find that\n\\[\n\\begin{gathered}\n\\beta^{*}=\\operatorname{corr}(X, Y)=\\beta \\\\\n\\alpha^{*}=(1-\\beta) \\mu=\\alpha\n\\end{gathered}\n\\]\nwhich are exactly the same as in the projection of \\(Y\\) on \\(X\\) ! The intercept and slope have exactly the same values in the forward and reverse projections! [This equality is not particularly imporant; it is an artifact of the assumption that \\(X\\) and \\(Y\\) have the same variances.]\nWhile this algebraic discovery is quite simple, it is counter-intuitive. Instead, a common yet mistaken guess for the form of the reverse regression is to take the equation (2.49), divide through by \\(\\beta\\) and rewrite to find the equation\n\\[\nX=Y \\frac{1}{\\beta}-\\frac{\\alpha}{\\beta}-\\frac{1}{\\beta} e\n\\]\nsuggesting that the projection of \\(X\\) on \\(Y\\) should have a slope coefficient of \\(1 / \\beta\\) instead of \\(\\beta\\), and intercept of \\(-\\alpha / \\beta\\) rather than \\(\\alpha\\). What went wrong? Equation (2.51) is perfectly valid because it is a simple manipulation of the valid equation (2.49). The trouble is that (2.51) is neither a CEF nor a linear projection. Inverting a projection (or CEF) does not yield a projection (or CEF). Instead, (2.50) is a valid projection, not (2.51).\nIn any event, Galton’s finding was that when the variables are standardized, the slope in both projections ( \\(Y\\) on \\(X\\), and \\(X\\) on \\(Y\\) ) equals the correlation and both equations exhibit regression to the mean. It is not a causal relation, but a natural feature of joint distributions."
  },
  {
    "objectID": "chpt02-ce.html#limitations-of-the-best-linear-projection",
    "href": "chpt02-ce.html#limitations-of-the-best-linear-projection",
    "title": "2  Conditional Expectation and Projection",
    "section": "2.32 Limitations of the Best Linear Projection",
    "text": "2.32 Limitations of the Best Linear Projection\nLet’s compare the linear projection and linear CEF models.\nFrom Theorem 2.4.4 we know that the CEF error has the property \\(\\mathbb{E}[X e]=0\\). Thus a linear CEF is the best linear projection. However, the converse is not true as the projection error does not necessarily satisfy \\(\\mathbb{E}[e \\mid X]=0\\). Furthermore, the linear projection may be a poor approximation to the CEF.\nTo see these points in a simple example, suppose that the true process is \\(Y=X+X^{2}\\) with \\(X \\sim \\mathrm{N}(0,1)\\). In this case the true CEF is \\(m(x)=x+x^{2}\\) and there is no error. Now consider the linear projection of \\(Y\\) on \\(X\\) and a constant, namely the model \\(Y=\\beta X+\\alpha+e\\). Since \\(X \\sim \\mathrm{N}(0,1)\\) then \\(X\\) and \\(X^{2}\\) are uncorrelated and the linear projection takes the form \\(\\mathscr{P}[Y \\mid X]=X+1\\). This is quite different from the true CEF \\(m(X)=\\) \\(X+X^{2}\\). The projection error equals \\(e=X^{2}-1\\) which is a deterministic function of \\(X\\) yet is uncorrelated with \\(X\\). We see in this example that a projection error need not be a CEF error and a linear projection can be a poor approximation to the CEF.\n\nFigure 2.7: Conditional Expectation and Two Linear Projections\nAnother defect of linear projection is that it is sensitive to the marginal distribution of the regressors when the conditional mean is nonlinear. We illustrate the issue in Figure \\(2.7\\) for a constructed \\({ }^{11}\\) joint distribution of \\(Y\\) and \\(X\\). The thick line is the nonlinear CEF of \\(Y\\) given \\(X\\). The data are divided in two groups - Group 1 and Group 2 - which have different marginal distributions for the regressor \\(X\\), and Group 1 has a lower mean value of \\(X\\) than Group 2. The separate linear projections of \\(Y\\) on \\(X\\) for these two groups are displayed in the figure by the thin lines. These two projections are distinct approximations to the CEF. A defect with linear projection is that it leads to the incorrect conclusion that the effect of \\(X\\) on \\(Y\\) is different for individuals in the two groups. This conclusion is incorrect because in fact there is no difference in the conditional expectation function. The apparent difference is a by-product of linear approximations to a nonlinear expectation combined with different marginal distributions for the conditioning variables.\n\\({ }^{11}\\) The \\(X\\) in Group 1 are \\(\\mathrm{N}(2,1)\\), those in Group 2 are \\(\\mathrm{N}(4,1)\\), and the conditional distribution of \\(Y\\) given \\(X\\) is \\(\\mathrm{N}(m(X), 1)\\) where \\(m(x)=2 x-x^{2} / 6\\). The functions are plotted over \\(0 \\leq x \\leq 6\\)."
  },
  {
    "objectID": "chpt02-ce.html#random-coefficient-model",
    "href": "chpt02-ce.html#random-coefficient-model",
    "title": "2  Conditional Expectation and Projection",
    "section": "2.33 Random Coefficient Model",
    "text": "2.33 Random Coefficient Model\nA model which is notationally similar to but conceptually distinct from the linear CEF model is the linear random coefficient model. It takes the form \\(Y=X^{\\prime} \\eta\\) where the individual-specific coefficient \\(\\eta\\) is random and independent of \\(X\\). For example, if \\(X\\) is years of schooling and \\(Y\\) is log wages, then \\(\\eta\\) is the individual-specific returns to schooling. If a person obtains an extra year of schooling, \\(\\eta\\) is the actual change in their wage. The random coefficient model allows the returns to schooling to vary in the population. Some individuals might have a high return to education (a high \\(\\eta\\) ) and others a low return, possibly 0 , or even negative.\nIn the linear CEF model the regressor coefficient equals the regression derivative - the change in the conditional expectation due to a change in the regressors, \\(\\beta=\\nabla m(X)\\). This is not the effect on a given individual, it is the effect on the population average. In contrast, in the random coefficient model the random vector \\(\\eta=\\nabla\\left(X^{\\prime} \\eta\\right)\\) is the true causal effect - the change in the response variable \\(Y\\) itself due to a change in the regressors.\nIt is interesting, however, to discover that the linear random coefficient model implies a linear CEF. To see this, let \\(\\beta=\\mathbb{E}[\\eta]\\) and \\(\\Sigma=\\operatorname{var}[\\eta]\\) denote the mean and covariance matrix of \\(\\eta\\) and then decompose the random coefficient as \\(\\eta=\\beta+u\\) where \\(u\\) is distributed independently of \\(X\\) with mean zero and covariance matrix \\(\\Sigma\\). Then we can write\n\\[\n\\mathbb{E}[Y \\mid X]=X^{\\prime} \\mathbb{E}[\\eta \\mid X]=X^{\\prime} \\mathbb{E}[\\eta]=X^{\\prime} \\beta\n\\]\nso the CEF is linear in \\(X\\), and the coefficient \\(\\beta\\) equals the expectation of the random coefficient \\(\\eta\\).\nWe can thus write the equation as a linear CEF \\(Y=X^{\\prime} \\beta+e\\) where \\(e=X^{\\prime} u\\) and \\(u=\\eta-\\beta\\). The error is conditionally mean zero: \\(\\mathbb{E}[e \\mid X]=0\\). Furthermore\n\\[\n\\operatorname{var}[e \\mid X]=X^{\\prime} \\operatorname{var}[\\eta] X=X^{\\prime} \\Sigma X\n\\]\nso the error is conditionally heteroskedastic with its variance a quadratic function of \\(X\\).\nTheorem 2.11 In the linear random coefficient model \\(Y=X^{\\prime} \\eta\\) with \\(\\eta\\) independent of \\(X, \\mathbb{E}\\|X\\|^{2}<\\infty\\), and \\(\\mathbb{E}\\|\\eta\\|^{2}<\\infty\\), then\n\\[\n\\begin{aligned}\n\\mathbb{E}[Y \\mid X] &=X^{\\prime} \\beta \\\\\n\\operatorname{var}[Y \\mid X] &=X^{\\prime} \\Sigma X\n\\end{aligned}\n\\]\nwhere \\(\\beta=\\mathbb{E}[\\eta]\\) and \\(\\Sigma=\\operatorname{var}[\\eta]\\)"
  },
  {
    "objectID": "chpt02-ce.html#causal-effects",
    "href": "chpt02-ce.html#causal-effects",
    "title": "2  Conditional Expectation and Projection",
    "section": "2.34 Causal Effects",
    "text": "2.34 Causal Effects\nSo far we have avoided the concept of causality, yet often the underlying goal of an econometric analysis is to measure a causal relationship between variables. It is often of great interest to understand the causes and effects of decisions, actions, and policies. For example, we may be interested in the effect of class sizes on test scores, police expenditures on crime rates, climate change on economic activity, years of schooling on wages, institutional structure on growth, the effectiveness of rewards on behavior, the consequences of medical procedures for health outcomes, or any variety of possible causal relationships. In each case the goal is to understand what is the actual effect on the outcome due to a change in an input. We are not just interested in the conditional expectation or linear projection, we would like to know the actual change.\nTwo inherent barriers are: (1) the causal effect is typically specific to an individual; and (2) the causal effect is typically unobserved.\nConsider the effect of schooling on wages. The causal effect is the actual difference a person would receive in wages if we could change their level of education holding all else constant. This is specific to each individual as their employment outcomes in these two distinct situations are individual. The causal effect is unobserved because the most we can observe is their actual level of education and their actual wage, but not the counterfactual wage if their education had been different.\nTo be concrete suppose that there are two individuals, Jennifer and George, and both have the possibility of being high-school graduates or college graduates, and both would have received different wages given their choices. For example, suppose that Jennifer would have earned \\(\\$ 10\\) an hour as a high-school graduate and \\(\\$ 20\\) an hour as a college graduate while George would have earned \\(\\$ 8\\) as a high-school graduate and \\(\\$ 12\\) as a college graduate. In this example the causal effect of schooling is \\(\\$ 10\\) a hour for Jennifer and \\(\\$ 4\\) an hour for George. The causal effects are specific to the individual and neither causal effect is observed.\nRubin (1974) developed the potential outcomes framework (also known as the Rubin causal model) to clarify the issues. Let \\(Y\\) be a scalar outcome (for example, wages) and \\(D\\) be a binary treatment (for example, college attendence). The specification of treatment as binary is not essential but simplifies the notation. A flexible model describing the impact of the treatment on the outcome is\n\\[\nY=h(D, U)\n\\]\nwhere \\(U\\) is an \\(\\ell \\times 1\\) unobserved random factor and \\(h\\) is a functional relationship. It is also common to use the simplified notation \\(Y(0)=h(0, U)\\) and \\(Y(1)=h(1, U)\\) for the potential outcomes associated with non-treatment and treatment, respectively. The notation implicitly holds \\(U\\) fixed. The potential outcomes are specific to each individual as they depend on \\(U\\). For example, if \\(Y\\) is an individual’s wage, the unobservables \\(U\\) could include characteristics such as the individual’s abilities, skills, work ethic, interpersonal connections, and preferences, all of which potentially influence their wage. In our example these factors are summarized by the labels “Jennifer” and “George”.\nRubin described the effect as causal when we vary \\(D\\) while holding \\(U\\) constant. In our example this means changing an individual’s education while holding constant their other attributes.\nDefinition 2.6 In the model (2.52) the causal effect of \\(D\\) on \\(Y\\) is\n\\[\nC(U)=Y(1)-Y(0)=h(1, U)-h(0, U),\n\\]\nthe change in \\(Y\\) due to treatment while holding \\(U\\) constant.\nIt may be helpful to understand that (2.53) is a definition and does not necessarily describe causality in a fundamental or experimental sense. Perhaps it would be more appropriate to label (2.53) as a structural effect (the effect within the structural model).\nThe causal effect of treatment \\(C(U)\\) defined in (2.53) is heterogeneous and random as the potential outcomes \\(Y(0)\\) and \\(Y(1)\\) vary across individuals. Also, we do not observe both \\(Y(0)\\) and \\(Y(1)\\) for a given individual, but rather only the realized value\n\\[\nY=\\left\\{\\begin{array}{lll}\nY(0) & \\text { if } & D=0 \\\\\nY(1) & \\text { if } & D=1 .\n\\end{array}\\right.\n\\]\nTable 2.3: Example Distribution\n|College Graduate|0|0|6|10|\\(\\$ 17.00\\)| |:—————|:|:|:|-:|———:| |Difference | | | | | \\(\\$ 8.25\\)|\nConsequently the causal effect \\(C(U)\\) is unobserved.\nRubin’s goal was to learn features of the distribution of \\(C(U)\\) including its expected value which he called the average causal effect. He defined it as follows.\nDefinition 2.7 In the model (2.52) the average causal effect of \\(D\\) on \\(Y\\) is\n\\[\n\\mathrm{ACE}=\\mathbb{E}[C(U)]=\\int_{\\mathbb{R}^{\\ell}} C(u) f(u) d u\n\\]\nwhere \\(f(u)\\) is the density of \\(U\\).\nThe ACE is the population average of the causal effect. Extending our Jennifer & George example, suppose that half of the population are like Jennifer and the other half are like George. Then the average causal effect of college on wages is \\((10+4) / 2=\\$ 7\\) an hour.\nTo estimate the ACE a reasonable starting place is to compare average \\(Y\\) for treated and untreated individuals. In our example this is the difference between the average wage among college graduates and high school graduates. This is the same as the coefficient in a regression of the outcome \\(Y\\) on the treatment \\(D\\). Does this equal the ACE?\nThe answer depends on the relationship between treatment \\(D\\) and the unobserved component \\(U\\). If \\(D\\) is randomly assigned as in an experiment then \\(D\\) and \\(U\\) are independent and the regression coefficient equals the ACE. However, if \\(D\\) and \\(U\\) are dependent then the regression coefficient and ACE are different. To see this, observe that the difference between the average outcomes of the treated and untreated populations are\n\\[\n\\mathbb{E}[Y \\mid D=1]-\\mathbb{E}[Y \\mid D=0]=\\int_{\\mathbb{R}^{\\ell}} h(1, u) f(u \\mid D=1) d u-\\int_{\\mathbb{R}^{\\ell}} h(1, u) f(u \\mid D=0) d u\n\\]\nwhere \\(f(u \\mid D)\\) is the conditional density of \\(U\\) given \\(D\\). If \\(U\\) is independent of \\(D\\) then \\(f(u \\mid D)=f(u)\\) and the above expression equals \\(\\int_{\\mathbb{R}^{\\ell}}(h(1, u)-h(0, u)) f(u) d u=\\) ACE. However, if \\(U\\) and \\(D\\) are dependent this equality fails.\nTo illustrate, let’s return to our example of Jennifer and George. Suppose that all high school students take an aptitude test. If a student gets a high \\((\\mathrm{H})\\) score they go to college with probability \\(3 / 4\\), and if a student gets a low (L) score they go to college with probability \\(1 / 4\\). Suppose further that Jennifer gets an aptitude score of \\(\\mathrm{H}\\) with probability 3/4, while George gets a score of \\(\\mathrm{H}\\) with probability \\(1 / 4\\). Given this situation, \\(62.5 %\\) of Jennifer’s will go to college \\({ }^{12}\\) while \\(37.5 %\\) of George’s will go to college \\({ }^{13}\\).\nAn econometrician who randomly samples 32 individuals and collects data on educational attainment and wages will find the wage distribution displayed in Table 2.3.\n\\(12 \\mathbb{P}[\\) college \\(\\mid\\) Jennifer \\(]=\\mathbb{P}[\\) college \\(\\mid H] \\mathbb{P}[H \\mid\\) Jennifer \\(]+\\mathbb{P}[\\) college \\(\\mid L] \\mathbb{P}[L \\mid\\) Jennifer \\(]=(3 / 4)^{2}+(1 / 4)^{2} .\\)\n\\(13 \\mathbb{P}[\\) college \\(\\mid\\) George \\(]=\\mathbb{P}[\\) college \\(\\mid H] \\mathbb{P}[H \\mid\\) George \\(]+\\mathbb{P}[\\) college \\(\\mid L] \\mathbb{P}[L \\mid\\) George \\(]=(3 / 4)(1 / 4)+(1 / 4)(3 / 4)\\). Our econometrician finds that the average wage among high school graduates is \\(\\$ 8.75\\) while the average wage among college graduates is \\(\\$ 17.00\\). The difference of \\(\\$ 8.25\\) is the econometrician’s regression coefficient for the effect of college on wages. But \\(\\$ 8.25\\) overstates the true ACE of \\(\\$ 7\\). The reason is that college attendence is determined by an aptitude test which is correlated with an individual’s causal effect. Jennifer has both a high causal effect and is more likely to attend college, so the observed difference in wages overstates the causal effect of college.\nTo visualize Table \\(2.3\\) examine Figure 2.8. The four points are the four education/wage pairs from the table, with the size of the points calibrated to the wage distribution. The two lines are the econometrician’s regression line and the average causal effect. The Jennifer’s in the population correspond to the points above the two lines, the George’s in the population correspond to the points below the two lines. Because most Jennifer’s go to College, and most George’s do not, the regression line is tilted away from the average causal effect towards the two large points.\n\nFigure 2.8: Average Causal Effect vs Regression\nOur first lesson from this analysis is that we need to be cautious about interpreting regression coefficients as causal effects. Unless the regressors (e.g. education attainment) can be interpreted as randomly assigned it is inappropriate to interpret the regression coefficients causally.\nOur second lesson will be that a causal interpretation can be obtained if we condition on a sufficiently rich set of covariates. We now explore this issue.\nSuppose that the observables include a set of covariates \\(X\\) in addition to the outcome \\(Y\\) and treatment \\(D\\). We extend the potential outcomes model (2.52) to include \\(X\\) :\n\\[\nY=h(D, X, U) .\n\\]\nWe also extend the definition of a causal effect to allow conditioning on \\(X\\).\nDefinition \\(2.8\\) In the model (2.54) the causal effect of \\(D\\) on \\(Y\\) is\n\\[\nC(X, U)=h(1, X, U)-h(0, X, U),\n\\]\nthe change in \\(Y\\) due to treatment holding \\(X\\) and \\(U\\) constant.\nThe conditional average causal effect of \\(D\\) on \\(Y\\) conditional on \\(X=x\\) is\n\\[\n\\operatorname{ACE}(x)=\\mathbb{E}[C(X, U) \\mid X=x]=\\int_{\\mathbb{R}^{\\ell}} C(x, u) f(u \\mid x) d u\n\\]\nwhere \\(f(u \\mid x)\\) is the conditional density of \\(U\\) given \\(X\\).\nThe unconditional average causal effect of \\(D\\) on \\(Y\\) is\n\\[\n\\mathrm{ACE}=\\mathbb{E}[C(X, U)]=\\int \\operatorname{ACE}(x) f(x) d x\n\\]\nwhere \\(f(x)\\) is the density of \\(X\\).\nThe conditional average causal effect \\(\\operatorname{ACE}(x)\\) is the ACE for the sub-population with characteristics \\(X=x\\). Given observations on \\((Y, D, X)\\) we want to measure the causal effect of \\(D\\) on \\(Y\\), and are interested if this can be obtained by a regression of \\(Y\\) on \\((D, X)\\). We would like to interpret the coefficient on \\(D\\) as a causal effect. Is this appropriate?\nOur previous analysis showed that a causal interpretation obtains when \\(U\\) is independent of the regressors. While this is sufficient it is stronger than necessary. Instead, the following is sufficient.\nDefinition 2.9 Conditional Independence Assumption (CIA). Conditional on \\(X\\), the random variables \\(D\\) and \\(U\\) are statistically independent.\nThe CIA implies that the conditional density of \\(U\\) given \\((D, X)\\) only depends on \\(X\\), thus \\(f(u \\mid D, X)=\\) \\(f(u \\mid X)\\). This implies that the regression of \\(Y\\) on \\((D, X)\\) equals\n\\[\n\\begin{aligned}\nm(d, x) &=\\mathbb{E}[Y \\mid D=d, X=x] \\\\\n&=\\mathbb{E}[h(d, x, U) \\mid D=d, X=x] \\\\\n&=\\int h(d, x, u) f(u \\mid x) d u .\n\\end{aligned}\n\\]\nUnder the CIA the treatment effect measured by the regression is\n\\[\n\\begin{aligned}\n\\nabla m(d, x) &=m(1, x)-m(0, x) \\\\\n&=\\int h(1, x, u) f(u \\mid x) d u-\\int h(0, x, u) f(u \\mid x) d u \\\\\n&=\\int C(x, u) f(u \\mid x) d u \\\\\n&=\\operatorname{ACE}(x) .\n\\end{aligned}\n\\]\nThis is the conditional ACE. Thus under the CIA the regression coefficient equals the ACE.\nWe deduce that the regression of \\(Y\\) on \\((D, X)\\) reveals the causal impact of treatment when the CIA holds. This means that regression analysis can be interpreted causally when we can make the case that the regressors \\(X\\) are sufficient to control for factors which are correlated with treatment.\nTheorem 2.12 In the structural model (2.54), the Conditional Independence Assumption implies \\(\\nabla m(d, x)=\\operatorname{ACE}(x)\\), that the regression derivative with respect to treatment equals the conditional ACE.\nThis is a fascinating result. It shows that whenever the unobservable is independent of the treatment variable after conditioning on appropriate regressors, the regression derivative equals the conditional causal effect. This means the CEF has causal economic meaning, giving strong justification to estimation of the CEF.\nIt is important to understand the critical role of the CIA. If CIA fails then the equality (2.55) of the regression derivative and the ACE fails. The CIA states that conditional on \\(X\\) the variables \\(U\\) and \\(D\\) are independent. This means that treatment \\(D\\) is not affected by the unobserved individual factors \\(U\\) and is effectively random. It is a strong assumption. In the wage/education example it means that education is not selected by individuals based on their unobserved characteristics.\nHowever, it is also helpful to understand that the CIA is weaker than full independence of \\(U\\) from the regressors \\((D, X)\\). What is required is only that \\(U\\) and \\(D\\) are independent after conditioning on \\(X\\). If \\(X\\) is sufficiently rich this may not be restrictive.\nReturning to our example, we require a variable \\(X\\) which breaks the dependence between \\(D\\) and \\(U\\). In our example, this variable is the aptitude test score, because the decision to attend college was based on the test score. It follows that educational attainment and type are independent once we condition on the test score.\nTo see this, observe that if a student’s test score is \\(\\mathrm{H}\\) the probability they go to college \\((D=1)\\) is \\(3 / 4\\) for both Jennifers and Georges. Similarly, if their test score is \\(\\mathrm{L}\\) the probability they go to college is \\(1 / 4\\) for both types. This means that college attendence is independent of type, conditional on the aptitude test score.\nThe conditional ACE depends on the test score. Among students who receive a high test score, \\(3 / 4\\) are Jennifers and \\(1 / 4\\) are Georges. Thus the conditional ACE for students with a score of \\(\\mathrm{H}\\) is \\((3 / 4) \\times 10+\\) \\((1 / 4) \\times 4=\\$ 8.50\\). Among students who receive a low test score, \\(1 / 4\\) are Jennifers and \\(3 / 4\\) are Georges. Thus the ACE for students with a score of \\(\\mathrm{L}\\) is \\((1 / 4) \\times 10+(3 / 4) \\times 4=\\$ 5.50\\). The unconditional ACE is the average, \\(\\mathrm{ACE}=(8.50+5.50) / 2=\\$ 7\\), because \\(50 %\\) of students each receive scores of \\(\\mathrm{H}\\) and \\(\\mathrm{L}\\).\nTheorem \\(2.12\\) shows that the conditional ACE is revealed by a regression which includes test scores. To see this in the wage distribution, suppose that the econometrician collects data on the aptitude test score as well as education and wages. Given a random sample of 32 individuals we would expect to find the wage distribution in Table \\(2.4\\).\nDefine a dummy highscore to indicate students who received a high test score. The regression of wages on college attendance and test scores with their interaction is\n\\[\n\\mathbb{E}[\\text { wage } \\mid \\text { college, highscore }]=1.00 \\text { highscore }+5.50 \\text { college }+3.00 \\text { highscore } \\times \\text { college }+8.50 \\text {. }\n\\]\nThe coefficient on college, \\(\\$ 5.50\\), is the regression derivative of college attendance for those with low test scores, and the sum of this coefficient with the interaction coefficient \\(\\$ 3.00\\) equals \\(\\$ 8.50\\) which is the Table 2.4: Example Distribution 2\n\n\n\n\n\n\n\n\n\n\n\n\n\\(\\$ 8\\)\n\\(\\$ 10\\)\n\\(\\$ 12\\)\n\\(\\$ 20\\)\nMean\n\n\n\n\nHigh-School Graduate + High Test Score\n1\n3\n0\n0\n\\(\\$ 9.50\\)\n\n\nCollege Graduate + High Test Score\n0\n0\n3\n9\n\\(\\$ 18.00\\)\n\n\nHigh-School Graduate + Low Test Score\n9\n3\n0\n0\n\\(\\$ 8.50\\)\n\n\nCollege Graduate + Low Test Score\n0\n0\n3\n1\n\\(\\$ 14.00\\)\n\n\n\nregression derivative for college attendance for those with high test scores. \\(\\$ 5.50\\) and \\(\\$ 8.50\\) equal the conditional causal effects as calculated above.\nThis shows that from the regression (2.56) an econometrician will find that the effect of college on wages is \\(\\$ 8.50\\) for those with high test scores and \\(\\$ 5.50\\) for those with low test scores with an average effect of \\(\\$ 7\\) (because \\(50 %\\) of students receive high and low test scores). This is the true average causal effect of college on wages. Thus the regression coefficient on college in (2.56) can be interpreted causally, while a regression omitting the aptitude test score does not reveal the causal effect of education.\nTo summarize our findings, we have shown how it is possible that a simple regression will give a false measurement of a causal effect, but a more careful regression can reveal the true causal effect. The key is to condition on a suitably rich set of covariates such that the remaining unobserved factors affecting the outcome are independent of the treatment variable."
  },
  {
    "objectID": "chpt02-ce.html#existence-and-uniqueness-of-the-conditional-expectation",
    "href": "chpt02-ce.html#existence-and-uniqueness-of-the-conditional-expectation",
    "title": "2  Conditional Expectation and Projection",
    "section": "2.35 Existence and Uniqueness of the Conditional Expectation*",
    "text": "2.35 Existence and Uniqueness of the Conditional Expectation*\nIn Sections \\(2.3\\) and \\(2.6\\) we defined the conditional expectation when the conditioning variables \\(X\\) are discrete and when the variables \\((Y, X)\\) have a joint density. We have explored these cases because these are the situations where the conditional mean is easiest to describe and understand. However, the conditional mean exists quite generally without appealing to the properties of either discrete or continuous random variables.\nTo justify this claim we now present a deep result from probability theory. What it says is that the conditional mean exists for all joint distributions \\((Y, X)\\) for which \\(Y\\) has a finite mean.\nTheorem 2.13 Existence of the Conditional Expectation If \\(\\mathbb{E}|Y|<\\infty\\) then there exists a function \\(m(x)\\) such that for all sets \\(\\mathscr{X}\\) for which \\(\\mathbb{P}[X \\in \\mathscr{X}]\\) is defined,\n\\[\n\\mathbb{E}[\\mathbb{1}\\{X \\in \\mathscr{X}\\} Y]=\\mathbb{E}[\\mathbb{1}\\{X \\in \\mathscr{X}\\} m(X)]\n\\]\nThe function \\(m(X)\\) is almost everywhere unique, in the sense that if \\(h(x)\\) satisfies (2.57), then there is a set \\(S\\) such that \\(\\mathbb{P}[S]=1\\) and \\(m(x)=h(x)\\) for \\(x \\in S\\). The function \\(m(x)\\) is called the conditional expectation and is written \\(m(x)=\\mathbb{E}[Y \\mid X=x]\\)\nSee, for example, Ash (1972), Theorem 6.3.3.\nThe conditional expectation \\(m(x)\\) defined by (2.57) specializes to (2.6) when \\((Y, X)\\) have a joint density. The usefulness of definition (2.57) is that Theorem \\(2.13\\) shows that the conditional expectation \\(m(X)\\) exists for all finite-mean distributions. This definition allows \\(Y\\) to be discrete or continuous, for \\(X\\) to be scalar or vector-valued, and for the components of \\(X\\) to be discrete or continuously distributed.\nYou may have noticed that Theorem \\(2.13\\) applies only to sets \\(\\mathscr{X}\\) for which \\(\\mathbb{P}[X \\in \\mathscr{X}]\\) is defined. This is a technical issue - measurability - which we largely side-step in this textbook. Formal probability theory only applies to sets which are measurable - for which probabilities are defined - as it turns out that not all sets satisfy measurability. This is not a practical concern for applications, so we defer such distinctions for formal theoretical treatments."
  },
  {
    "objectID": "chpt02-ce.html#identification",
    "href": "chpt02-ce.html#identification",
    "title": "2  Conditional Expectation and Projection",
    "section": "2.36 Identification*",
    "text": "2.36 Identification*\nA critical and important issue in structural econometric modeling is identification, meaning that a parameter is uniquely determined by the distribution of the observed variables. It is relatively straightforward in the context of the unconditional and conditional expectation, but it is worthwhile to introduce and explore the concept at this point for clarity.\nLet \\(F\\) denote the distribution of the observed data, for example the distribution of the pair \\((Y, X)\\). Let \\(\\mathscr{F}\\) be a collection of distributions \\(F\\). Let \\(\\theta\\) be a parameter of interest (for example, the expectation \\(\\mathbb{E}[Y])\\).\nDefinition 2.10 A parameter \\(\\theta \\in \\mathbb{R}\\) is identified on \\(\\mathscr{F}\\) if for all \\(F \\in \\mathscr{F}\\), there is a uniquely determined value of \\(\\theta\\).\nEquivalently, \\(\\theta\\) is identified if we can write it as a mapping \\(\\theta=g(F)\\) on the set \\(\\mathscr{F}\\). The restriction to the set \\(\\mathscr{F}\\) is important. Most parameters are identified only on a strict subset of the space of all distributions.\nTake, for example, the expectation \\(\\mu=\\mathbb{E}[Y]\\). It is uniquely determined if \\(\\mathbb{E}|Y|<\\infty\\), so \\(\\mu\\) is identified for the set \\(\\mathscr{F}=\\{F: \\mathbb{E}|Y|<\\infty\\}\\).\nNext, consider the conditional expectation. Theorem \\(2.13\\) demonstrates that \\(\\mathbb{E}|Y|<\\infty\\) is a sufficient condition for identification.\nTheorem 2.14 Identification of the Conditional Expectation If \\(\\mathbb{E}|Y|<\\infty\\), the conditional expectation \\(m(x)=\\mathbb{E}[Y \\mid X=x]\\) is identified almost everywhere.\nIt might seem as if identification is a general property for parameters so long as we exclude degenerate cases. This is true for moments of observed data, but not necessarily for more complicated models. As a case in point, consider the context of censoring. Let \\(Y\\) be a random variable with distribution \\(F\\). Instead of observing \\(Y\\), we observe \\(Y^{*}\\) defined by the censoring rule\n\\[\nY^{*}=\\left\\{\\begin{array}{cc}\nY & \\text { if } Y \\leq \\tau \\\\\n\\tau & \\text { if } Y>\\tau\n\\end{array}\\right.\n\\]\nThat is, \\(Y^{*}\\) is capped at the value \\(\\tau\\). A common example is income surveys, where income responses are “top-coded” meaning that incomes above the top code \\(\\tau\\) are recorded as the top code. The observed variable \\(Y^{*}\\) has distribution\n\\[\nF^{*}(u)=\\left\\{\\begin{array}{cc}\nF(u) & \\text { for } u \\leq \\tau \\\\\n1 & \\text { for } u \\geq \\tau .\n\\end{array}\\right.\n\\]\nWe are interested in features of the distribution \\(F\\) not the censored distribution \\(F^{*}\\). For example, we are interested in the expected wage \\(\\mu=\\mathbb{E}[Y]\\). The difficulty is that we cannot calculate \\(\\mu\\) from \\(F^{*}\\) except in the trivial case where there is no censoring \\(\\mathbb{P}[Y \\geq \\tau]=0\\). Thus the expectation \\(\\mu\\) is not generically identified from the censored distribution.\nA typical solution to the identification problem is to assume a parametric distribution. For example, let \\(\\mathscr{F}\\) be the set of normal distributions \\(Y \\sim \\mathrm{N}\\left(\\mu, \\sigma^{2}\\right)\\). It is possible to show that the parameters \\(\\left(\\mu, \\sigma^{2}\\right)\\) are identified for all \\(F \\in \\mathscr{F}\\). That is, if we know that the uncensored distribution is normal we can uniquely determine the parameters from the censored distribution. This is often called parametric identification as identification is restricted to a parametric class of distributions. In modern econometrics this is generally viewed as a second-best solution as identification has been achieved only through the use of an arbitrary and unverifiable parametric assumption.\nA pessimistic conclusion might be that it is impossible to identify parameters of interest from censored data without parametric assumptions. Interestingly, this pessimism is unwarranted. It turns out that we can identify the quantiles \\(q_{\\alpha}\\) of \\(F\\) for \\(\\alpha \\leq \\mathbb{P}[Y \\leq \\tau]\\). For example, if \\(20 %\\) of the distribution is censored we can identify all quantiles for \\(\\alpha \\in(0,0.8)\\). This is often called nonparametric identification as the parameters are identified without restriction to a parametric class.\nWhat we have learned from this little exercise is that in the context of censored data moments can only be parametrically identified while non-censored quantiles are nonparametrically identified. Part of the message is that a study of identification can help focus attention on what can be learned from the data distributions available."
  },
  {
    "objectID": "chpt02-ce.html#technical-proofs",
    "href": "chpt02-ce.html#technical-proofs",
    "title": "2  Conditional Expectation and Projection",
    "section": "2.37 Technical Proofs*",
    "text": "2.37 Technical Proofs*\nProof of Theorem 2.1 For convenience, assume that the variables have a joint density \\(f(y, x)\\). Since \\(\\mathbb{E}[Y \\mid X]\\) is a function of the random vector \\(X\\) only, to calculate its expectation we integrate with respect to the density \\(f_{X}(x)\\) of \\(X\\), that is\n\\[\n\\mathbb{E}[\\mathbb{E}[Y \\mid X]]=\\int_{\\mathbb{R}^{k}} \\mathbb{E}[Y \\mid X] f_{X}(x) d x .\n\\]\nSubstituting in (2.6) and noting that \\(f_{Y \\mid X}(y \\mid x) f_{X}(x)=f(y, x)\\), we find that the above expression equals\n\\[\n\\int_{\\mathbb{R}^{k}}\\left(\\int_{\\mathbb{R}} y f_{Y \\mid X}(y \\mid x) d y\\right) f_{X}(x) d x=\\int_{\\mathbb{R}^{k}} \\int_{\\mathbb{R}} y f(y, x) d y d x=\\mathbb{E}[Y]\n\\]\nthe unconditional expectation of \\(Y\\).\nProof of Theorem 2.2 Again assume that the variables have a joint density. It is useful to observe that\n\\[\nf\\left(y \\mid x_{1}, x_{2}\\right) f\\left(x_{2} \\mid x_{1}\\right)=\\frac{f\\left(y, x_{1}, x_{2}\\right)}{f\\left(x_{1}, x_{2}\\right)} \\frac{f\\left(x_{1}, x_{2}\\right)}{f\\left(x_{1}\\right)}=f\\left(y, x_{2} \\mid x_{1}\\right)\n\\]\nthe density of \\(\\left(Y, X_{2}\\right)\\) given \\(X_{1}\\). Here, we have abused notation and used a single symbol \\(f\\) to denote the various unconditional and conditional densities to reduce notational clutter.\nNote that\n\\[\n\\mathbb{E}\\left[Y \\mid X_{1}=x_{1}, X_{2}=x_{2}\\right]=\\int_{\\mathbb{R}} y f\\left(y \\mid x_{1}, x_{2}\\right) d y .\n\\]\nIntegrating (2.59) with respect to the conditional density of \\(X_{2}\\) given \\(X_{1}\\), and applying (2.58) we find that\n\\[\n\\begin{aligned}\n\\mathbb{E}\\left[\\mathbb{E}\\left[Y \\mid X_{1}, X_{2}\\right] \\mid X_{1}=x_{1}\\right] &=\\int_{\\mathbb{R}^{k_{2}}} \\mathbb{E}\\left[Y \\mid X_{1}=x_{1}, X_{2}=x_{2}\\right] f\\left(x_{2} \\mid x_{1}\\right) d x_{2} \\\\\n&=\\int_{\\mathbb{R}^{k_{2}}}\\left(\\int_{\\mathbb{R}} y f\\left(y \\mid x_{1}, x_{2}\\right) d y\\right) f\\left(x_{2} \\mid x_{1}\\right) d x_{2} \\\\\n&=\\int_{\\mathbb{R}^{k_{2}}} \\int_{\\mathbb{R}} y f\\left(y \\mid x_{1}, x_{2}\\right) f\\left(x_{2} \\mid x_{1}\\right) d y d x_{2} \\\\\n&=\\int_{\\mathbb{R}^{k_{2}}} \\int_{\\mathbb{R}} y f\\left(y, x_{2} \\mid x_{1}\\right) d y d x_{2} \\\\\n&=\\mathbb{E}\\left[Y \\mid X_{1}=x_{1}\\right] .\n\\end{aligned}\n\\]\nThis implies \\(\\mathbb{E}\\left[\\mathbb{E}\\left[Y \\mid X_{1}, X_{2}\\right] \\mid X_{1}\\right]=\\mathbb{E}\\left[Y \\mid X_{1}\\right]\\) as stated.\nProof of Theorem 2.3\n\\[\n\\mathbb{E}[g(X) Y \\mid X=x]=\\int_{\\mathbb{R}} g(x) y f_{Y \\mid X}(y \\mid x) d y=g(x) \\int_{\\mathbb{R}} y f_{Y \\mid X}(y \\mid x) d y=g(x) \\mathbb{E}[Y \\mid X=x]\n\\]\nThis implies \\(\\mathbb{E}[g(X) Y \\mid X]=g(X) \\mathbb{E}[Y \\mid X]\\) which is (2.7). Equation (2.8) follows by applying the simple law of iterated expectations (Theorem 2.1) to (2.7).\nProof of Theorem 2.4 Applying Minkowski’s inequality (B.34) to \\(e=Y-m(X)\\),\n\\[\n\\left(\\mathbb{E}|e|^{r}\\right)^{1 / r}=\\left(\\mathbb{E}|Y-m(X)|^{r}\\right)^{1 / r} \\leq\\left(\\mathbb{E}|Y|^{r}\\right)^{1 / r}+\\left(\\mathbb{E}|m(X)|^{r}\\right)^{1 / r}<\\infty,\n\\]\nwhere the two parts on the right-hand-side are finite because \\(\\mathbb{E}|Y|^{r}<\\infty\\) by assumption and \\(\\mathbb{E}|m(X)|^{r}<\\) \\(\\infty\\) by the conditional expectation inequality (B.29). The fact that \\(\\left(\\mathbb{E}|e|^{r}\\right)^{1 / r}<\\infty\\) implies \\(\\mathbb{E}|e|^{r}<\\infty\\).\nProof of Theorem 2.6 The assumption that \\(\\mathbb{E}\\left[Y^{2}\\right]<\\infty\\) implies that all the conditional expectations below exist.\nUsing the law of iterated expectations (Theorem 2.2) \\(\\mathbb{E}\\left[Y \\mid X_{1}\\right]=\\mathbb{E}\\left(\\mathbb{E}\\left[Y \\mid X_{1}, X_{2}\\right] \\mid X_{1}\\right)\\) and the conditional Jensen’s inequality (B.28),\n\\[\n\\left(\\mathbb{E}\\left[Y \\mid X_{1}\\right]\\right)^{2}=\\left(\\mathbb{E}\\left(\\mathbb{E}\\left[Y \\mid X_{1}, X_{2}\\right] \\mid X_{1}\\right)\\right)^{2} \\leq \\mathbb{E}\\left[\\left(\\mathbb{E}\\left[Y \\mid X_{1}, X_{2}\\right]\\right)^{2} \\mid X_{1}\\right] .\n\\]\nTaking unconditional expectations, this implies\n\\[\n\\mathbb{E}\\left[\\left(\\mathbb{E}\\left[Y \\mid X_{1}\\right]\\right)^{2}\\right] \\leq \\mathbb{E}\\left[\\left(\\mathbb{E}\\left[Y \\mid X_{1}, X_{2}\\right]\\right)^{2}\\right] .\n\\]\nSimilarly,\n\\[\n(\\mathbb{E}[Y])^{2} \\leq \\mathbb{E}\\left[\\left(\\mathbb{E}\\left[Y \\mid X_{1}\\right]\\right)^{2}\\right] \\leq \\mathbb{E}\\left[\\left(\\mathbb{E}\\left[Y \\mid X_{1}, X_{2}\\right]\\right)^{2}\\right] .\n\\]\nThe variables \\(Y, \\mathbb{E}\\left[Y \\mid X_{1}\\right]\\), and \\(\\mathbb{E}\\left[Y \\mid X_{1}, X_{2}\\right]\\) all have the same expectation \\(\\mathbb{E}[Y]\\), so the inequality (2.60) implies that the variances are ranked monotonically:\n\\[\n0 \\leq \\operatorname{var}\\left(\\mathbb{E}\\left[Y \\mid X_{1}\\right]\\right) \\leq \\operatorname{var}\\left(\\mathbb{E}\\left[Y \\mid X_{1}, X_{2}\\right]\\right) .\n\\]\nDefine \\(e=Y-\\mathbb{E}[Y \\mid X]\\) and \\(u=\\mathbb{E}[Y \\mid X]-\\mu\\) so that we have the decomposition \\(Y-\\mu=e+u\\). Notice \\(\\mathbb{E}[e \\mid X]=0\\) and \\(u\\) is a function of \\(X\\). Thus by the conditioning theorem (Theorem 2.3), \\(\\mathbb{E}[e u]=0\\) so \\(e\\) and \\(u\\) are uncorrelated. It follows that\n\\[\n\\operatorname{var}[Y]=\\operatorname{var}[e]+\\operatorname{var}[u]=\\operatorname{var}[Y-\\mathbb{E}[Y \\mid X]]+\\operatorname{var}[\\mathbb{E}[Y \\mid X]]\n\\]\nThe monotonicity of the variances of the conditional expectation (2.61) applied to the variance decomposition (2.62) implies the reverse monotonicity of the variances of the differences, completing the proof.\nProof of Theorem 2.9 For part 1, by the expectation inequality (B.30), (A.17) and Assumption 2.1,\n\\[\n\\left\\|\\mathbb{E}\\left[X X^{\\prime}\\right]\\right\\| \\leq \\mathbb{E}\\left\\|X X^{\\prime}\\right\\|=\\mathbb{E}\\|X\\|^{2}<\\infty .\n\\]\nSimilarly, using the expectation inequality (B.30), the Cauchy-Schwarz inequality (B.32), and Assumption 2.1,\n\\[\n\\|\\mathbb{E}[X Y]\\| \\leq \\mathbb{E}\\|X Y\\| \\leq\\left(\\mathbb{E}\\|X\\|^{2}\\right)^{1 / 2}\\left(\\mathbb{E}\\left[Y^{2}\\right]\\right)^{1 / 2}<\\infty .\n\\]\nThus the moments \\(\\mathbb{E}[X Y]\\) and \\(\\mathbb{E}\\left[X X^{\\prime}\\right]\\) are finite and well defined.\nFor part 2, the coefficient \\(\\beta=\\left(\\mathbb{E}\\left[X X^{\\prime}\\right]\\right)^{-1} \\mathbb{E}[X Y]\\) is well defined because \\(\\left(\\mathbb{E}\\left[X X^{\\prime}\\right]\\right)^{-1}\\) exists under Assumption 2.1.\nPart 3 follows from Definition \\(2.5\\) and part \\(2 .\\)\nFor part 4, first note that\n\\[\n\\begin{aligned}\n\\mathbb{E}\\left[e^{2}\\right] &=\\mathbb{E}\\left[\\left(Y-X^{\\prime} \\beta\\right)^{2}\\right] \\\\\n&=\\mathbb{E}\\left[Y^{2}\\right]-2 \\mathbb{E}\\left[Y X^{\\prime}\\right] \\beta+\\beta^{\\prime} \\mathbb{E}\\left[X X^{\\prime}\\right] \\beta \\\\\n&=\\mathbb{E}\\left[Y^{2}\\right]-\\mathbb{E}\\left[Y X^{\\prime}\\right]\\left(\\mathbb{E}\\left[X X^{\\prime}\\right]\\right)^{-1} \\mathbb{E}[X Y] \\\\\n& \\leq \\mathbb{E}\\left[Y^{2}\\right]<\\infty .\n\\end{aligned}\n\\]\nThe first inequality holds because \\(\\mathbb{E}\\left[Y X^{\\prime}\\right]\\left(\\mathbb{E}\\left[X X^{\\prime}\\right]\\right)^{-1} \\mathbb{E}[X Y]\\) is a quadratic form and therefore necessarily non-negative. Second, by the expectation inequality (B.30), the Cauchy-Schwarz inequality (B.32), and Assumption 2.1,\n\\[\n\\|\\mathbb{E}[X e]\\| \\leq \\mathbb{E}\\|X e\\|=\\left(\\mathbb{E}\\|X\\|^{2}\\right)^{1 / 2}\\left(\\mathbb{E}\\left[e^{2}\\right]\\right)^{1 / 2}<\\infty .\n\\]\nIt follows that the expectation \\(\\mathbb{E}[X e]\\) is finite, and is zero by the calculation (2.26).\nFor part 6, applying Minkowski’s inequality (B.34) to \\(e=Y-X^{\\prime} \\beta\\),\n\\[\n\\begin{aligned}\n\\left(\\mathbb{E}|e|^{r}\\right)^{1 / r} &=\\left(\\mathbb{E}\\left|Y-X^{\\prime} \\beta\\right|^{r}\\right)^{1 / r} \\\\\n& \\leq\\left(\\mathbb{E}|Y|^{r}\\right)^{1 / r}+\\left(\\mathbb{E}\\left|X^{\\prime} \\beta\\right|^{r}\\right)^{1 / r} \\\\\n& \\leq\\left(\\mathbb{E}|Y|^{r}\\right)^{1 / r}+\\left(\\mathbb{E}\\|X\\|^{r}\\right)^{1 / r}\\|\\beta\\|<\\infty,\n\\end{aligned}\n\\]\nthe final inequality by assumption."
  },
  {
    "objectID": "chpt02-ce.html#exercises",
    "href": "chpt02-ce.html#exercises",
    "title": "2  Conditional Expectation and Projection",
    "section": "2.38 Exercises",
    "text": "2.38 Exercises\nExercise 2.1 Find \\(\\mathbb{E}\\left[\\mathbb{E}\\left[\\mathbb{E}\\left[Y \\mid X_{1}, X_{2}, X_{3}\\right] \\mid X_{1}, X_{2}\\right] \\mid X_{1}\\right]\\)\nExercise 2.2 If \\(\\mathbb{E}[Y \\mid X]=a+b X\\), find \\(\\mathbb{E}[Y X]\\) as a function of moments of \\(X\\).\nExercise 2.3 Prove Theorem 2.4.4 using the law of iterated expectations. Exercise 2.4 Suppose that the random variables \\(Y\\) and \\(X\\) only take the values 0 and 1 , and have the following joint probability distribution\n\n\n\n\n\\(X=0\\)\n\\(X=1\\)\n\n\n\n\n\\(Y=0\\)\n\\(.1\\)\n\\(.2\\)\n\n\n\\(Y=1\\)\n\\(.4\\)\n\\(.3\\)\n\n\n\nFind \\(\\mathbb{E}[Y \\mid X], \\mathbb{E}\\left[Y^{2} \\mid X\\right]\\), and \\(\\operatorname{var}[Y \\mid X]\\) for \\(X=0\\) and \\(X=1\\)\nExercise 2.5 Show that \\(\\sigma^{2}(X)\\) is the best predictor of \\(e^{2}\\) given \\(X\\) :\n\nWrite down the mean-squared error of a predictor \\(h(X)\\) for \\(e^{2}\\).\nWhat does it mean to be predicting \\(e^{2}\\) ?\nShow that \\(\\sigma^{2}(X)\\) minimizes the mean-squared error and is thus the best predictor.\n\nExercise 2.6 Use \\(Y=m(X)+e\\) to show that \\(\\operatorname{var}[Y]=\\operatorname{var}[m(X)]+\\sigma^{2}\\)\nExercise 2.7 Show that the conditional variance can be written as \\(\\sigma^{2}(X)=\\mathbb{E}\\left[Y^{2} \\mid X\\right]-(\\mathbb{E}[Y \\mid X])^{2}\\).\nExercise 2.8 Suppose that \\(Y\\) is discrete-valued, taking values only on the non-negative integers, and the conditional distribution of \\(Y\\) given \\(X=x\\) is Poisson:\n\\[\n\\mathbb{P}[Y=j \\mid X=x]=\\frac{\\exp \\left(-x^{\\prime} \\beta\\right)\\left(x^{\\prime} \\beta\\right)^{j}}{j !}, \\quad j=0,1,2, \\ldots\n\\]\nCompute \\(\\mathbb{E}[Y \\mid X]\\) and \\(\\operatorname{var}[Y \\mid X]\\). Does this justify a linear regression model of the form \\(Y=X^{\\prime} \\beta+e\\) ?\n\\[\n\\text { Hint: If } \\mathbb{P}[Y=j]=\\frac{\\exp (-\\lambda) \\lambda^{j}}{j !} \\text { then } \\mathbb{E}[Y]=\\lambda \\text { and } \\operatorname{var}[Y]=\\lambda \\text {. }\n\\]\nExercise 2.9 Suppose you have two regressors: \\(X_{1}\\) is binary (takes values 0 and 1) and \\(X_{2}\\) is categorical with 3 categories \\((A, B, C)\\). Write \\(\\mathbb{E}\\left[Y \\mid X_{1}, X_{2}\\right]\\) as a linear regression.\nExercise 2.10 True or False. If \\(Y=X \\beta+e, X \\in \\mathbb{R}\\), and \\(\\mathbb{E}[e \\mid X]=0\\), then \\(\\mathbb{E}\\left[X^{2} e\\right]=0\\).\nExercise 2.11 True or False. If \\(Y=X \\beta+e, X \\in \\mathbb{R}\\), and \\(\\mathbb{E}[X e]=0\\), then \\(\\mathbb{E}\\left[X^{2} e\\right]=0\\).\nExercise 2.12 True or False. If \\(Y=X^{\\prime} \\beta+e\\) and \\(\\mathbb{E}[e \\mid X]=0\\), then \\(e\\) is independent of \\(X\\).\nExercise 2.13 True or False. If \\(Y=X^{\\prime} \\beta+e\\) and \\(\\mathbb{E}[X e]=0\\), then \\(\\mathbb{E}[e \\mid X]=0\\).\nExercise 2.14 True or False. If \\(Y=X^{\\prime} \\beta+e, \\mathbb{E}[e \\mid X]=0\\), and \\(\\mathbb{E}\\left[e^{2} \\mid X\\right]=\\sigma^{2}\\), then \\(e\\) is independent of \\(X\\).\nExercise 2.15 Consider the intercept-only model \\(Y=\\alpha+e\\) with \\(\\alpha\\) the best linear predictor. Show that \\(\\alpha=\\mathbb{E}[Y]\\)\nExercise 2.16 Let \\(X\\) and \\(Y\\) have the joint density \\(f(x, y)=\\frac{3}{2}\\left(x^{2}+y^{2}\\right)\\) on \\(0 \\leq x \\leq 1,0 \\leq y \\leq 1\\). Compute the coefficients of the best linear predictor \\(Y=\\alpha+\\beta X+e\\). Compute the conditional expectation \\(m(x)=\\) \\(\\mathbb{E}[Y \\mid X=x]\\). Are the best linear predictor and conditional expectation different? Exercise 2.17 Let \\(X\\) be a random variable with \\(\\mu=\\mathbb{E}[X]\\) and \\(\\sigma^{2}=\\operatorname{var}[X]\\). Define\n\\[\ng\\left(x, \\mu, \\sigma^{2}\\right)=\\left(\\begin{array}{c}\nx-\\mu \\\\\n(x-\\mu)^{2}-\\sigma^{2}\n\\end{array}\\right) .\n\\]\nShow that \\(\\mathbb{E}[g(X, m, s)]=0\\) if and only if \\(m=\\mu\\) and \\(s=\\sigma^{2}\\).\nExercise 2.18 Suppose that \\(X=\\left(1, X_{2}, X_{3}\\right)\\) where \\(X_{3}=\\alpha_{1}+\\alpha_{2} X_{2}\\) is a linear function of \\(X_{2}\\).\n\nShow that \\(\\boldsymbol{Q}_{X X}=\\mathbb{E}\\left[X X^{\\prime}\\right]\\) is not invertible.\nUse a linear transformation of \\(X\\) to find an expression for the best linear predictor of \\(Y\\) given \\(X\\). (Be explicit, do not just use the generalized inverse formula.)\n\nExercise 2.19 Show (2.47)-(2.48), namely that for\n\\[\nd(\\beta)=\\mathbb{E}\\left[\\left(m(X)-X^{\\prime} \\beta\\right)^{2}\\right]\n\\]\nthen\n\\[\n\\beta=\\underset{b \\in \\mathbb{R}^{k}}{\\operatorname{argmin}} d(b)=\\left(\\mathbb{E}\\left[X X^{\\prime}\\right]\\right)^{-1} \\mathbb{E}[X m(X)]=\\left(\\mathbb{E}\\left[X X^{\\prime}\\right]\\right)^{-1} \\mathbb{E}[X Y] .\n\\]\nHint: To show \\(\\mathbb{E}[X m(X)]=\\mathbb{E}[X Y]\\) use the law of iterated expectations.\nExercise 2.20 Verify that (2.57) holds with \\(m(X)\\) defined in (2.6) when \\((Y, X)\\) have a joint density \\(f(y, x)\\).\nExercise 2.21 Consider the short and long projections\n\\[\n\\begin{gathered}\nY=X \\gamma_{1}+e \\\\\nY=X \\beta_{1}+X^{2} \\beta_{2}+u\n\\end{gathered}\n\\]\n\nUnder what condition does \\(\\gamma_{1}=\\beta_{1}\\) ?\nTake the long projection is \\(Y=X \\theta_{1}+X^{3} \\theta_{2}+v\\). Is there a condition under which \\(\\gamma_{1}=\\theta_{1}\\) ?\n\nExercise 2.22 Take the homoskedastic model\n\\[\n\\begin{aligned}\nY &=X_{1}^{\\prime} \\beta_{1}+X_{2}^{\\prime} \\beta_{2}+e \\\\\n\\mathbb{E}\\left[e \\mid X_{1}, X_{2}\\right] &=0 \\\\\n\\mathbb{E}\\left[e^{2} \\mid X_{1}, X_{2}\\right] &=\\sigma^{2} \\\\\n\\mathbb{E}\\left[X_{2} \\mid X_{1}\\right] &=\\Gamma X_{1} .\n\\end{aligned}\n\\]\nAssume \\(\\Gamma \\neq 0\\). Suppose the parameter \\(\\beta_{1}\\) is of interest. We know that the exclusion of \\(X_{2}\\) creates omited variable bias in the projection coefficient on \\(X_{2}\\). It also changes the equation error. Our question is: what is the effect on the homoskedasticity property of the induced equation error? Does the exclusion of \\(X_{2}\\) induce heteroskedasticity or not? Be specific."
  },
  {
    "objectID": "chpt02-ce-chn.html#介绍",
    "href": "chpt02-ce-chn.html#介绍",
    "title": "条件预期和预测",
    "section": "介绍",
    "text": "介绍\n最常用的计量经济学工具是最小二乘估计，也称为回归。最小二乘法是一种工具，用于在给定另一组变量（回归变量、条件变量或协变量）的情况下估计一个变量（因变量）的条件均值。\n\n\nThe most commonly applied econometric tool is least squares estimation, also known as regression. Least squares is a tool to estimate the conditional mean of one variable (the dependent variable) given another set of variables (the regressors, conditioning variables, or covariates).\n在本章中，我们从估计中抽象出来，重点讨论条件期望模型的概率基础及其投影近似。这包括对概率论的回顾。有关中级概率论的背景知识，请参阅《经济学家的概率与统计》第 1-5 章。\n\n\nIn this chapter we abstract from estimation and focus on the probabilistic foundation of the conditional expectation model and its projection approximation. This includes a review of probability theory. For a background in intermediate probability theory see Chapters 1-5 of Probability and Statistics for Economists."
  },
  {
    "objectID": "chpt02-ce-chn.html#工资分配",
    "href": "chpt02-ce-chn.html#工资分配",
    "title": "条件预期和预测",
    "section": "工资分配",
    "text": "工资分配\n假设我们对美国的工资率感兴趣。由于工人的工资率各不相同，我们不能用一个数字来描述工资率。相反，我们可以使用概率分布来描述工资。形式上，我们将单个工人的工资视为具有概率分布的随机变量工资\n\n\nSuppose that we are interested in wage rates in the United States. Since wage rates vary across workers we cannot describe wage rates by a single number. Instead, we can describe wages using a probability distribution. Formally, we view the wage of an individual worker as a random variable wage with the probability distribution\n\\[\nF(y)=\\mathbb{P}[\\text { wage } \\leq y] .\n\\]\n当我们说一个人的工资是随机的时，我们的意思是在测量之前我们不知道他们的工资，我们将观察到的工资率视为分布 \\(F\\) 的实现。将未观察到的工资视为随机变量，将观察到的工资视为实现是一种强大的数学抽象，它允许我们使用数学概率工具。\n一个有用的思想实验是想象拨打一个随机选择的电话号码，然后让接电话的人告诉我们他们的工资率。 （为简单起见，假设所有工人都可以平等地使用电话，并且接听电话的人会诚实接听。）在这个思想实验中，您所拨打电话的人的工资是工资分布 \\(F\\) 的一次抽取在人口中。通过拨打许多这样的电话，我们可以了解完整的分布。\n当分布函数 \\(F\\) 可微时，我们定义概率密度函数\n\\[\nf(y)=\\frac{d}{d y} F(y) .\n\\]\n密度包含与分布函数相同的信息，但密度通常更易于直观解释。\n\n\n工资密度\n\n\n\n对数工资密度\n\n图 2.1：工资密度和对数工资\n在图 2.1(a) 中，我们展示了 \\(2009 .\\) 中美国工资率概率密度函数的估计 \\({ }^{1}\\) 我们看到密度在 \\(\\$ 15\\), and most of the probability mass appears to lie between \\(\\$ 10\\) 和 \\(\\$ 40 附近达到峰值\\). These are ranges for typical wage rates in the U.S. population.\n集中趋势的重要度量是中位数和平均值。连续分布 \\(F\\) 的中位数 \\(m\\) 是\n\\[\nF(m)=\\frac{1}{2} .\n\\]\n美国工资中位数为 \\(\\$ 19.23\\). The median is a robust \\({ }^{2}\\) measure of central tendency, but it is tricky to use for many calculations as it is not a linear operator.\n具有离散支持的随机变量 \\(Y\\) 的均值或期望是\n\\[\n\\mu=\\mathbb{E}[Y]=\\sum_{j=1}^{\\infty} \\tau_{j} \\mathbb{P}\\left[Y=\\tau_{j}\\right] .\n\\]\n对于密度为 \\(f(y)\\) 的连续随机变量，期望为\n\\[\n\\mu=\\mathbb{E}[Y]=\\int_{-\\infty}^{\\infty} y f(y) d y .\n\\]\n在这里，我们使用了使用单个字符 \\(Y\\) 来表示随机变量的常见且方便的约定，而不是更繁琐的标签工资。包含离散和连续随机变量作为特殊情况的另一种表示法是将积分写为 \\(\\int_{-\\infty}^{\\infty} y d F(y)\\)。\n期望是集中趋势的一种方便度量，因为它是一个线性算子，并且在许多经济模型中自然出现。期望的一个缺点是它不是健壮的 \\({ }^{3}\\) 尤其是\n\\({ }^{1}\\) 分布和密度是根据 2009 年 3 月当前人口调查中报告的 50,742 名全职非军事工薪者样本非参数估计的。工资率由个人年度工资和薪金收入除以工作时间构成。\n\\({ }^{2}\\) 中位数对分布尾部的扰动不敏感。\n\\({ }^{3}\\) 期望对分布尾部的扰动很敏感。如图 2.1(a) 所示，存在明显偏斜或厚尾，这两者都是工资分布的特征。另一种看法是，\\(64 %\\) 工人的平均工资低于 \\(\\$ 23.90\\), suggesting that it is incorrect to describe the mean \\(\\$ 23.90\\) 作为“典型”工资率。\n在这种情况下，通过取自然对数“\\({ }^{4}\\)”来转换数据是有用的。图 \\(2.1\\) (b) 显示了相同人口的 \\(\\log\\) 小时工资 \\(\\log (\\) 工资 \\()\\) 的密度。密度对数工资的偏斜和肥尾比工资水平的密度要小，所以它的均值\n\\[\n\\mathbb{E}[\\log (\\text { wage })]=2.95\n\\]\n是分布集中趋势的更好（更稳健）的度量 \\({ }^{5}\\)。出于这个原因，工资回归通常使用对数工资作为因变量，而不是工资水平。\n总结概率分布 \\(F(y)\\) 的另一种有用方法是根据其分位数。对于任何 \\(\\alpha \\in(0,1)\\)，连续 \\({ }^{6}\\) 分布 \\(F\\) 的 \\(\\alpha^{t h}\\) 分位数是满足 \\(F\\left(q_{\\alpha}\\right)=\\alpha\\) 的实数 \\(q_{\\alpha}\\)。分位数函数 \\(q_{\\alpha}\\)，被视为 \\(\\alpha\\) 的函数，是分布函数 \\(F\\) 的倒数。最常用的分位数是中位数，即\\(q_{0.5}=m\\)。我们有时通过 \\(\\alpha\\) 的百分位数表示来指代分位数，在这种情况下，它们被称为百分位数。例如。中位数是 \\(50^{t h}\\) 百分位数。"
  },
  {
    "objectID": "chpt02-ce-chn.html#有条件的期望",
    "href": "chpt02-ce-chn.html#有条件的期望",
    "title": "条件预期和预测",
    "section": "有条件的期望",
    "text": "有条件的期望\n我们在图 2.1(b) 中看到了对数工资的密度。这种分布对所有工人来说都是一样的，还是工资分布在不同的子人群中有所不同？为了回答这个问题，我们可以比较不同群体的工资分布——例如，男性和女性。为了进行调查，我们在图 \\(2.2\\) (a) 中绘制了美国男性和女性的对数工资密度。我们可以看到这两种工资密度具有相似的形状，但男性的密度稍微向右移动。\n值 \\(3.05\\) 和 \\(2.81\\) 是男性和女性工人亚群的平均对数工资。它们被称为给定性别的对数工资的条件期望（或条件均值）。我们可以将它们的具体值写为\n\\[\n\\begin{gathered}\n\\mathbb{E}[\\log (\\text { wage }) \\mid \\text { gender }=\\text { man }]=3.05 \\\\\n\\mathbb{E}[\\log (\\text { wage }) \\mid \\text { gender }=\\text { woman }]=2.81 .\n\\end{gathered}\n\\]\n我们将这些期望称为“有条件的”，因为它们以可变性别的固定值为条件。虽然您可能不会将一个人的性别视为随机变量，但从计量经济学分析的角度来看，它是随机的。如果你随机选择一个人，这个人的性别是未知的，因此是随机的。 （在美国工人群体中，工人是女性的概率恰好是 \\(43 %\\)。）在观察数据中，将所有测量值视为随机变量是最合适的，然后子群体的平均值就是条件平均值。\n在这一点上重要的是要提到，我们绝不会将因果关系或解释归因于男女之间对数工资的条件期望差异。有多种可能的解释。\n由于图 2.2(a) 中的两个密度看起来相似，因此可以草率地推断男性和女性的工资分布之间没有有意义的差异。在得出这个结论之前，让我们更仔细地检查分布的差异。正如我们上面提到的，\n\\({ }^{4}\\) 在整本书中，我们将使用 \\(\\log (y)\\) 或 \\(\\log y\\) 来表示 \\(y\\) 的自然对数。\n\\({ }^{5}\\) 更准确地说，几何平均值 \\(\\exp (\\mathbb{E}[\\log W])=\\$ 19.11\\) is a robust measure of central tendency.\n\\({ }^{6}\\) 如果 \\(F\\) 不连续，则定义为 \\(q_{\\alpha}=\\inf \\{y: F(y) \\geq \\alpha\\}\\)\n\n\n妇女和男子\n\n\n\n按性别和种族\n\n图 2.2：按性别和种族划分的对数工资密度\n两种密度之间的主要区别似乎是它们的平均值。这个差等于\n\\[\n\\begin{aligned}\n\\mathbb{E}[\\log (\\text { wage }) \\mid \\text { gender }=\\text { man }]-\\mathbb{E}[\\log (\\text { wage }) \\mid \\text { gender }=\\text { woman }] &=3.05-2.81 \\\\\n&=0.24 .\n\\end{aligned}\n\\]\n\\(0.24\\) 的预期对数工资差异通常被解释为男性和女性工资之间的平均 \\(24 %\\) 差异，这是相当大的。 （有关更完整的解释，请参见第 2.4 节。）\n考虑进一步按种族划分男性和女性亚群，将人口分为白人、黑人和其他种族。我们在图 \\(2.2\\) (b) 中显示了其中四个组的对数工资密度函数。我们再次看到四个密度函数之间的主要区别在于它们的集中趋势。\n关注这些分布的均值，表 \\(2.1\\) 报告了六个子群体中每一个的平均对数工资。\n表 2.1：按性别和种族划分的平均对数工资\n\n\n\n\nmen\nwomen\n\n\n\n\nwhite\n\\(3.07\\)\n\\(2.82\\)\n\n\nBlack\n\\(2.86\\)\n\\(2.73\\)\n\n\nother\n\\(3.03\\)\n\\(2.86\\)\n\n\n\n我们再次强调，我们绝不会将因果关系或解释归因于表格条目之间的差异。我们之所以使用这些特定的亚群来说明条件期望，是因为美国（和其他地方）性别和种族群体之间的经济结果差异已被广泛讨论；社会科学的部分作用是仔细记录这些模式，其部分作用是制作模型和解释。有条件的期望（自己）可以帮助文档和描述；条件期望本身既不是模型也不是解释。\n表 \\(2.1\\) 中的条目是给定性别和种族的 \\(\\log (\\) 工资 \\()\\) 的条件均值。例如\n\\[\n\\mathbb{E}[\\log (\\text { wage }) \\mid \\text { gender }=\\text { man, race }=\\text { white }]=3.07\n\\]\n和\n\\[\n\\mathbb{E}[\\log (\\text { wage }) \\mid \\text { gender }=\\text { woman, race }=\\text { Black }]=2.73 \\text {. }\n\\]\n关注条件均值的一个好处是它们可以将复杂的分布简化为单一的汇总度量，从而促进跨组的比较。由于这种简化特性，条件均值是回归分析的主要兴趣点，也是计量经济学的主要关注点。\n表 \\(2.1\\) 使我们能够轻松计算各组之间的平均工资差异。例如，我们可以看到男女之间的工资差距在按种族分类后继续存在，因为白人男性和白人女性之间的平均差距是 \\(25 %\\)，而黑人男性和黑人女性之间的平均差距是 \\(13 %\\)。我们还可以看到存在种族差距，因为黑人的平均工资大大低于其他种族类别。特别是，白人男性和黑人男性之间的平均工资差距是 \\(21 %\\)，白人女性和黑人女性之间的平均工资差距是 \\(9 %\\)。"
  },
  {
    "objectID": "chpt02-ce-chn.html#日志和百分比",
    "href": "chpt02-ce-chn.html#日志和百分比",
    "title": "条件预期和预测",
    "section": "日志和百分比",
    "text": "日志和百分比\n在本节中，我们希望通过两个观察来激发和阐明回归分析中对数的使用。首先，当应用于数字时，对数的差异大约等于百分比差异。其次，当应用于平均值时，对数差异大约等于几何平均值的百分比差异。我们现在解释这些想法和所涉及的近似值的性质。\n取两个正数 \\(a\\) 和 \\(b\\)。 \\(a\\) 和 \\(b\\) 之间的百分比差异是\n\\[\np=100\\left(\\frac{a-b}{b}\\right) .\n\\]\n重写，\n\\[\n\\frac{a}{b}=1+\\frac{p}{100}\n\\]\n取自然对数，\n\\[\n\\log a-\\log b=\\log \\left(1+\\frac{p}{100}\\right) .\n\\]\n小 \\(x\\) 的有用近似值是\n\\[\n\\log (1+x) \\simeq x .\n\\]\n这可以从 \\(\\log (1+x)\\) 的无限级数展开推导出来：\n\\[\n\\log (1+x)=x-\\frac{x^{2}}{2}+\\frac{x^{3}}{3}-\\frac{x^{4}}{4}+\\cdots=x+O\\left(x^{2}\\right) .\n\\]\n符号 \\(O\\left(x^{2}\\right.\\) ) 表示余数以 \\(A x^{2}\\) 为界，对于某些 \\(A<\\infty\\)，则为 \\(x \\rightarrow 0\\)。在数值上，对于 \\(|x| \\leq 0.1\\)，近似值 \\(\\log (1+x) \\simeq x\\) 在 \\(0.001\\) 之内，并且近似误差随着 \\(|x|\\) 的增加而增加\n将 (2.3) 应用于 (2.2) 并乘以 100，我们发现\n\\[\np \\simeq 100(\\log a)-\\log b) .\n\\]\n这表明 100 乘以对数差值大约是百分比差值。在数值上，\\(|p| \\leq 10\\) 的近似误差小于 \\(0.1\\) 个百分点。\n现在考虑对数转换随机变量的期望差异。取两个随机变量 \\(X_{1}, X_{2}>0\\)。定义它们的几何平均值 \\(\\theta_{1}=\\exp \\left(\\mathbb{E}\\left[\\log X_{1}\\right]\\right)\\) 和 \\(\\theta_{2}=\\exp \\left(\\mathbb{E}\\left[\\log X_{2}\\right]\\right)\\) 以及它们的百分比差异\n\\[\np=100\\left(\\frac{\\theta_{2}-\\theta_{1}}{\\theta_{1}}\\right) .\n\\]\n对数变换的期望差异（乘以 100）为\n\\[\n100\\left(\\mathbb{E}\\left[\\log X_{2}\\right]-\\mathbb{E}\\left[\\log X_{1}\\right]\\right)=100\\left(\\log \\theta_{2}-\\log \\theta_{1}\\right) \\simeq p\n\\]\n\\(\\theta_{2}\\) 和 \\(\\theta_{1}\\) 之间的百分比差异。换句话说，对数变换变量的平均值之间的差异（大约）是几何平均值的百分比差异。\n后一种观察之所以重要，是因为许多计量经济学方程采用半对数形式\n\\[\n\\begin{aligned}\n&\\mathbb{E}[\\log Y \\mid \\operatorname{group}=1]=\\mu_{1} \\\\\n&\\mathbb{E}[\\log Y \\mid \\operatorname{group}=2]=\\mu_{2}\n\\end{aligned}\n\\]\n并且相当注意差异\\(\\mu_{1}-\\mu_{2}\\)。例如，在上一节中，我们比较了男性和女性的平均对数工资，发现差异为 \\(0.24\\)。在该部分中，我们指出这种差异通常被解释为平均百分比差异。这并不完全正确，但也不是完全错误。上面的计算表明，这个差异大约是几何平均值的百分比差异。所以 \\(\\mu_{1}-\\mu_{2}\\) 是平均百分比差异，其中“平均”是指几何平均值而不是算术平均值。\n要比较不同的百分比差异度量，请参见表 2.2。在前两列中，我们使用四个“平均值”报告 CPS 人群中男性和女性的平均工资：算术平均值、中位数、几何平均值和对数平均值。两组的算术平均值均高于中位数和几何平均值，后两者相似。这是偏态分布（例如工资分布）的共同特征。第三列报告前两列之间的百分比差异（以男性工资为基数）。例如，\\(34 %\\) 的第一个条目表明男性的平均工资比女性的平均工资高 \\(34 %\\)。下一个条目显示男性的中位数和几何平均值比女性高 \\(26 %\\)。此列中的最后一项是平均对数工资之间的简单差值的 100 倍，即 \\(24 %\\)。如上所示，对数变换均值的差异近似为几何均值的百分比差异，这种近似对于 \\(10 %\\) 以下的差异非常有用。\n让我们总结一下这个分析。通常取变量的对数并在条件均值之间进行比较。我们已经表明，这些差异是几何平均值百分比差异的度量。因此，预期对数转换之间的差异（例如男性和女性工资之间的 \\(0.24\\) 差异）是近似百分比差异（例如男性工资相对于女性工资的 24% 差异）的常见描述是正确的，只要当我们意识到我们隐含地比较几何平均值时。"
  },
  {
    "objectID": "chpt02-ce-chn.html#条件期望函数",
    "href": "chpt02-ce-chn.html#条件期望函数",
    "title": "条件预期和预测",
    "section": "条件期望函数",
    "text": "条件期望函数\n工资的一个重要决定因素是教育。在许多实证研究中，经济学家通过受教育年数 \\({ }^{7}\\) 来衡量教育程度。我们将把这个变量写成教育。\n\\({ }^{7}\\) 在这里，教育被定义为幼儿园之后的教育年限。高中毕业生的教育=12，大学毕业生的教育=16，硕士学位的教育=18，专业学位（医学、法律或博士）的教育= 表2.2：平均工资和百分比差异\n\n\n\n\nmen\nwomen\n% Difference\n\n\n\n\nArithmetic Mean\n\\(\\$ 26.80\\)\n\\(\\$ 20.00\\)\n\\(34 %\\)\n\n\nMedian\n\\(\\$ 21.14\\)\n\\(\\$ 16.83\\)\n\\(26 %\\)\n\n\nGeometric Mean\n\\(\\$ 21.03\\)\n\\(\\$ 16.64\\)\n\\(26 %\\)\n\n\nMean log Wage\n\\(3.05\\)\n\\(2.81\\)\n\\(24 %\\)\n\n\n\n在给定性别、种族和教育的情况下，\\(\\log (\\) 工资 \\()\\) 的条件期望是每个类别的单个数字。例如\n\\[\n\\mathbb{E}[\\log (\\text { wage }) \\mid \\text { gender }=\\text { man, race }=\\text { white, education }=12]=2.84 .\n\\]\n我们在图 \\(2.3\\) 中显示了 \\(\\log\\)（工资）作为教育函数的条件期望，分别针对（白人）男性和女性。剧情相当有内涵。我们看到，有条件的期望在受教育年限中增加，但在九年以上和以下的学校教育水平上以不同的速度增长。图 \\(2.3\\) 的另一个显着特点是男女之间的差距在所有教育水平上大致保持不变。由于变量是用对数来衡量的，这意味着无论受教育程度如何，男性和女性之间的平均百分比差距都是恒定的。\n\n图 2.3：预期对数工资与教育 tion=20 的函数关系。在许多情况下，通过使用单个字符（通常是 \\(Y, X\\) 和/或 \\(Z\\)）编写变量来简化符号是很方便的。计量经济学习惯用字母 \\(Y\\) 表示因变量（例如 \\(\\log (\\) 工资 \\()\\) ），用字母 \\(X\\) 表示条件变量（例如性别），以及多个条件变量（例如种族，教育和性别）由下标字母 \\(X_{1}, X_{2}, \\ldots, X_{k}\\)。\n条件期望可以用通用符号来写\n\\[\n\\mathbb{E}\\left[Y \\mid X_{1}=x_{1}, X_{2}=x_{2}, \\ldots, X_{k}=x_{k}\\right]=m\\left(x_{1}, x_{2}, \\ldots, x_{k}\\right) \\text {. }\n\\]\n我们称之为条件期望函数（CEF）。 CEF 是 \\(\\left(x_{1}, x_{2}, \\ldots, x_{k}\\right)\\) 的函数，因为它随变量而变化。例如，\\(Y=\\log (\\) 工资 \\()\\) 给定 \\(\\left(X_{1}, X_{2}\\right)=(g e n d e r\\),race) 的条件期望由表 \\(2.1 .\\) 的六个条目给出\n为了更紧凑，我们通常将条件变量写为 \\(\\mathbb{R}^{k}\\) 中的向量：\n\\[\nX=\\left(\\begin{array}{c}\nX_{1} \\\\\nX_{2} \\\\\n\\vdots \\\\\nX_{k}\n\\end{array}\\right)\n\\]\n给定这个符号，CEF 可以紧凑地写为\n\\[\n\\mathbb{E}[Y \\mid X=x]=m(x) .\n\\]\nCEF \\(m(x)=\\mathbb{E}[Y \\mid X=x]\\) 是 \\(x \\in \\mathbb{R}^{k}\\) 的函数。它说：“当 \\(X\\) 取 \\(x\\) 的值时，\\(Y\\) 的平均值是 \\(m(x)\\)。”有时将 CEF 视为随机变量 \\(X\\) 的函数很有用。在这种情况下，我们在 \\(X\\) 处评估函数 \\(m(x)\\)，并写成 \\(m(X)\\) 或 \\(\\mathbb{E}[Y \\mid X]\\)。这是随机的，因为它是随机变量 \\(X\\) 的函数。"
  },
  {
    "objectID": "chpt02-ce-chn.html#连续变量",
    "href": "chpt02-ce-chn.html#连续变量",
    "title": "条件预期和预测",
    "section": "连续变量",
    "text": "连续变量\n在前面的部分中，我们隐含地假设条件变量是离散的。然而，许多条件变量是连续的。在本节中，我们将采用这种情况并假设变量 \\((Y, X)\\) 以联合密度函数 \\(f(y, x)\\) 连续分布。\n以 \\(Y=\\log (\\) 工资 \\()\\) 和 \\(X=\\) 经验为例，后者是潜在劳动力市场经验的年数 \\({ }^{8}\\)。对于受过 12 年教育的白人男性人口，他们的联合密度等值线绘制在图 \\(2.4\\) (a) 中。\n给定联合密度 \\(f(y, x)\\) 变量 \\(x\\) 具有边际密度\n\\[\nf_{X}(x)=\\int_{-\\infty}^{\\infty} f(y, x) d y .\n\\]\n对于任何 \\(x\\) 使得 \\(f_{X}(x)>0\\) 给定 \\(X\\) 的 \\(Y\\) 的条件密度定义为\n\\[\nf_{Y \\mid X}(y \\mid x)=\\frac{f(y, x)}{f_{X}(x)} .\n\\]\n条件密度是保持 \\(x\\) 固定的联合密度 \\(f(y, x)\\) 的重新归一化切片。切片被重新归一化（除以 \\(f_{X}(x)\\) 以便积分为一），因此是一个密度。我们可以通过在与 \\(y\\) 轴平行的特定值 \\(x\\) 处切片联合密度函数来可视化这一点。例如，取图 2.4(a) 中的密度等值线，并在特定的经验值处切开等值线图，并且\n\\({ }^{8}\\) 由于没有直接衡量经验，我们将经验定义为 age-education-6\n\n\n对数工资和经验的联合密度\n\n\n\n给定经验的对数工资的条件密度\n\n图 2.4：日志工资和经验\n然后重新归一化切片，使其具有适当的密度。这为我们提供了具有 12 年教育和这种经验水平的白人男性对数（工资）的条件密度。我们对三个级别的经验 \\((5,10\\) 和 25 年执行此操作，并将这些密度绘制在图 \\(2.4\\) (b) 中。我们可以看到，随着经验的增加，工资分布向右移动并变得更加分散。\n给定 \\(X=x\\) 的 \\(Y\\) 的 CEF 是条件密度 (2.5) 的期望\n\\[\nm(x)=\\mathbb{E}[Y \\mid X=x]=\\int_{-\\infty}^{\\infty} y f_{Y \\mid X}(y \\mid x) d y .\n\\]\n直观地说，\\(m(x)\\) 是条件变量固定在 \\(x\\) 的理想化子群体的 \\(Y\\) 的期望值。当 \\(X\\) 连续分布时，这个子群体无限小。\n当条件密度（2.5）定义明确时，这个定义（2.6）是合适的。然而，\\(2.31\\) 部分中的定理 \\(2.13\\) 将表明 \\(m(x)\\) 可以定义为任何随机变量 \\((Y, X)\\)，只要 \\(\\mathbb{E}|Y|<\\infty\\)\n在图 2.4(a) 中，给定经验的 \\(\\log\\)（工资）的 CEF 绘制为实线。我们可以看到 CEF 是一个平滑但非线性的函数。 CEF 最初在经验上增加，在经验 \\(=30\\) 附近趋于平缓，然后随着经验水平的提高而降低。"
  },
  {
    "objectID": "chpt02-ce-chn.html#迭代期望定律",
    "href": "chpt02-ce-chn.html#迭代期望定律",
    "title": "条件预期和预测",
    "section": "迭代期望定律",
    "text": "迭代期望定律\n概率论中一个非常有用的工具是迭代期望定律。一个重要的特殊情况被称为简单定律。定理 2.1 迭代期望的简单定律\n如果 \\(\\mathbb{E}|Y|<\\infty\\) 那么对于任何随机向量 \\(X\\)，\n\\[\n\\mathbb{E}[\\mathbb{E}[Y \\mid X]]=\\mathbb{E}[Y] .\n\\]\n这表明有条件期望的期望是无条件期望。换句话说，条件平均值的平均值是无条件平均值。对于离散 \\(X\\)\n\\[\n\\mathbb{E}[\\mathbb{E}[Y \\mid X]]=\\sum_{j=1}^{\\infty} \\mathbb{E}\\left[Y \\mid X=x_{j}\\right] \\mathbb{P}\\left[X=x_{j}\\right] .\n\\]\n对于连续 \\(X\\)\n\\[\n\\mathbb{E}[\\mathbb{E}[Y \\mid X]]=\\int_{\\mathbb{R}^{k}} \\mathbb{E}[Y \\mid X=x] f_{X}(x) d x .\n\\]\n回到我们对男性和女性平均原木工资的调查，简单的法律规定：\n\\[\n\\begin{aligned}\n&\\mathbb{E}[\\log (\\text { wage }) \\mid \\text { gender }=\\text { man }] \\mathbb{P}[\\text { gender }=\\text { man }] \\\\\n&+\\mathbb{E}[\\log (\\text { wage }) \\mid \\text { gender }=\\text { woman }] \\mathbb{P}[\\text { gender }=\\text { woman }] \\\\\n&=\\mathbb{E}[\\log (\\text { wage })]\n\\end{aligned}\n\\]\n或者在数字上，\n\\[\n3.05 \\times 0.57+2.81 \\times 0.43=2.95 \\text {. }\n\\]\n迭代期望的一般规律允许两组条件变量。\n定理 2.2 迭代期望定律 如果 \\(\\mathbb{E}|Y|<\\infty\\) 那么对于任何随机向量 \\(X_{1}\\) 和 \\(X_{2}\\)，\n\\[\n\\mathbb{E}\\left[\\mathbb{E}\\left[Y \\mid X_{1}, X_{2}\\right] \\mid X_{1}\\right]=\\mathbb{E}\\left[Y \\mid X_{1}\\right] .\n\\]\n注意法律的适用方式。 \\(X_{1}\\) 和 \\(X_{2}\\) 上的内部期望条件，而外部期望仅在 \\(X_{1}\\) 上条件。迭代的期望产生简单的答案 \\(\\mathbb{E}\\left[Y \\mid X_{1}\\right]\\)，期望仅以 \\(X_{1}\\) 为条件。有时我们将其表述为：“较小的信息集获胜。”\n举个例子\n\\[\n\\begin{aligned}\n&\\mathbb{E}[\\log (\\text { wage }) \\mid \\text { gender }=\\text { man, race }=\\text { white }] \\mathbb{P}[\\text { race }=\\text { white } \\mid \\text { gender }=\\text { man }] \\\\\n&+\\mathbb{E}[\\log (\\text { wage }) \\mid \\text { gender }=\\text { man, race }=\\text { Black }] \\mathbb{P}[\\text { race }=\\text { Black } \\mid \\text { gender }=\\text { man }] \\\\\n&+\\mathbb{E}[\\log (\\text { wage }) \\mid \\text { gender }=\\text { man, race }=\\text { other }] \\mathbb{P}[\\text { race }=\\text { other } \\mid \\text { gender }=\\text { man }] \\\\\n&=\\mathbb{E}[\\log (\\text { wage }) \\mid \\text { gender }=\\text { man }]\n\\end{aligned}\n\\]\n或数字上\n\\[\n3.07 \\times 0.84+2.86 \\times 0.08+3.03 \\times 0.08=3.05 \\text {. }\n\\]\n条件期望的一个属性是，当您以随机向量 \\(X\\) 为条件时，您可以有效地将其视为常数。例如，对于任何函数 \\(g(\\cdot)\\)，\\(\\mathbb{E}[X \\mid X]=X\\) 和 \\(\\mathbb{E}[g(X) \\mid X]=g(X)\\)。一般性质称为条件定理。\n定理 2.3 条件定理 如果 \\(\\mathbb{E}|Y|<\\infty\\) 那么\n\\[\n\\mathbb{E}[g(X) Y \\mid X]=g(X) \\mathbb{E}[Y \\mid X] .\n\\]\n如果另外 \\(\\mathbb{E}|g(X)|<\\infty\\) 那么\n\\[\n\\mathbb{E}[g(X) Y]=\\mathbb{E}[g(X) \\mathbb{E}[Y \\mid X]] .\n\\]\n定理 2.1、\\(2.2\\) 和 \\(2.3\\) 的证明在 \\(2.33 .\\) 部分给出"
  },
  {
    "objectID": "chpt02-ce-chn.html#cef-错误",
    "href": "chpt02-ce-chn.html#cef-错误",
    "title": "条件预期和预测",
    "section": "CEF 错误",
    "text": "CEF 错误\nCEF 错误 \\(e\\) 定义为 \\(Y\\) 与在 \\(X\\) 评估的 CEF 之间的差异：\n\\[\ne=Y-m(X) .\n\\]\n通过构造，这产生了公式\n\\[\nY=m(X)+e .\n\\]\n在 (2.9) 中，理解误差 \\(e\\) 是从 \\((Y, X)\\) 的联合分布导出的，因此它的性质是从这个构造导出的，这很有用。\n许多计量经济学作者使用希腊字母 \\(\\varepsilon\\) 来表示 CEF 错误。我不遵循这个约定，因为错误 \\(e\\) 是一个类似于 \\(Y\\) 和 \\(X\\) 的随机变量，并且通常使用拉丁字符作为随机变量。\nCEF 错误的一个关键特性是它的条件期望为零。要看到这一点，通过期望的线性、\\(m(X)=\\mathbb{E}[Y \\mid X]\\) 的定义和条件定理\n\\[\n\\begin{aligned}\n\\mathbb{E}[e \\mid X] &=\\mathbb{E}[(Y-m(X)) \\mid X] \\\\\n&=\\mathbb{E}[Y \\mid X]-\\mathbb{E}[m(X) \\mid X] \\\\\n&=m(X)-m(X)=0 .\n\\end{aligned}\n\\]\n这个事实可以结合迭代期望定律来证明无条件期望也是零。\n\\[\n\\mathbb{E}[e]=\\mathbb{E}[\\mathbb{E}[e \\mid X]]=\\mathbb{E}[0]=0 .\n\\]\n我们正式陈述这一点和其他一些结果。\n定理 2.4 CEF 误差的性质\n如果 \\(\\mathbb{E}|Y|<\\infty\\) 那么\n\n\\(\\mathbb{E}[e \\mid X]=0\\)。\n\\(\\mathbb{E}[e]=0\\)。\n如果 \\(\\mathbb{E}|Y|^{r}<\\infty\\) 对应 \\(r \\geq 1\\) 那么 \\(\\mathbb{E}|e|^{r}<\\infty\\)。\n对于任何函数 \\(h(x)\\) 使得 \\(\\mathbb{E}|h(X) e|<\\infty\\) 然后 \\(\\mathbb{E}[h(X) e]=0\\)。第三个结果的证明推迟到 2.33 节。第四个结果的证明留给练习 2.3，这意味着 \\(e\\) 与回归量的任何函数都不相关。\n\n方程\n\\[\n\\begin{aligned}\nY &=m(X)+e \\\\\n\\mathbb{E}[e \\mid X] &=0\n\\end{aligned}\n\\]\n一起暗示 \\(m(X)\\) 是给定 \\(X\\) 的 \\(Y\\) 的 CEF。重要的是要了解这不是限制。根据定义，这些等式成立。\n条件 \\(\\mathbb{E}[e \\mid X]=0\\) 被 \\(e\\) 的定义隐含为 \\(Y\\) 和 CEF \\(m(X)\\) 之间的差。方程 \\(\\mathbb{E}[e \\mid X]=0\\) 有时称为条件均值限制，因为误差 \\(e\\) 的条件均值被限制为零。该属性有时也称为平均独立性，因为 \\(e\\) 的条件平均值为 0，因此独立于 \\(X\\)。但是，这并不意味着 \\(e\\) 的分布独立于 \\(X\\)。有时添加假设“\\(e\\) 独立于 \\(X\\)”是为了方便简化，但它不是条件均值的通用特征。通常，即使 \\(e\\) 的条件均值为零，\\(e\\) 和 \\(X\\) 也是共同依赖的。\n例如，对于与图 2.4 相同的总体，回归误差 \\(e\\) 和经验的联合密度等值线绘制在图 \\(2.5\\) 中。请注意，条件分布的形状随经验水平而变化。\n\n劳动力市场经验（年）\n图 2.5：回归误差和经验的联合密度\n作为一个简单的例子，其中 \\(X\\) 和 \\(e\\) 均值独立但相互依赖，让 \\(e=X u\\) 其中 \\(X\\) 和 \\(u\\) 是独立的 \\(\\mathrm{N}(0,1)\\)。然后以 \\(X\\) 为条件，错误 \\(e\\) 具有分布 \\(\\mathrm{N}\\left(0, X^{2}\\right)\\)。因此 \\(\\mathbb{E}[e \\mid X]=0\\) 和 \\(e\\) 均值独立于 \\(X\\)，但 \\(e\\) 并不完全独立于 \\(X\\)。平均独立并不意味着完全独立。"
  },
  {
    "objectID": "chpt02-ce-chn.html#仅拦截模型",
    "href": "chpt02-ce-chn.html#仅拦截模型",
    "title": "条件预期和预测",
    "section": "仅拦截模型",
    "text": "仅拦截模型\n回归模型的一个特例是没有回归变量 \\(X\\)。在这种情况下 \\(m(X)=\\mathbb{E}[Y]=\\mu\\)，\\(Y\\) 的无条件期望。我们仍然可以为 \\(Y\\) 写出回归格式的方程：\n\\[\n\\begin{aligned}\nY &=\\mu+e \\\\\n\\mathbb{E}[e] &=0 .\n\\end{aligned}\n\\]\n这对于统一符号很有用。"
  },
  {
    "objectID": "chpt02-ce-chn.html#回归方差",
    "href": "chpt02-ce-chn.html#回归方差",
    "title": "条件预期和预测",
    "section": "回归方差",
    "text": "回归方差\nCEF 函数离散度的一个重要度量是 CEF 误差 \\(e\\) 的无条件方差。我们把它写成\n\\[\n\\sigma^{2}=\\operatorname{var}[e]=\\mathbb{E}\\left[(e-\\mathbb{E}[e])^{2}\\right]=\\mathbb{E}\\left[e^{2}\\right] .\n\\]\n定理 2.4.3 暗示了以下简单但有用的结果。\n定理 2.5 如果 \\(\\mathbb{E}\\left[Y^{2}\\right]<\\infty\\) 则 \\(\\sigma^{2}<\\infty\\)。\n我们可以将 \\(\\sigma^{2}\\) 称为回归方差或回归误差的方差。 \\(\\sigma^{2}\\) 的大小衡量 \\(Y\\) 中的变化量，这在条件期望 \\(\\mathbb{E}[Y \\mid X]\\) 中没有“解释”或解释。\n回归方差取决于回归量 \\(X\\)。考虑两个回归\n\\[\n\\begin{aligned}\n&Y=\\mathbb{E}\\left[Y \\mid X_{1}\\right]+e_{1} \\\\\n&Y=\\mathbb{E}\\left[Y \\mid X_{1}, X_{2}\\right]+e_{2} .\n\\end{aligned}\n\\]\n我们将这两个错误清楚地写为 \\(e_{1}\\) 和 \\(e_{2}\\)，因为它们是不同的——改变条件信息会改变条件期望，因此也会改变回归误差。\n在我们对迭代期望的讨论中，我们已经看到，通过增加条件集，条件期望揭示了关于 \\(Y\\) 分布的更多细节。回归误差的含义是什么？\n事实证明，有一个简单的关系。我们可以将条件期望 \\(\\mathbb{E}[Y \\mid X]\\) 视为 \\(Y\\) 的“解释部分”。剩余的 \\(e=Y-\\mathbb{E}[Y \\mid X]\\) 是“无法解释的部分”。我们现在推导出的简单关系表明，当我们以更多变量为条件时，这个无法解释的部分的方差会减小。这种关系是单调的，因为增加信息量总是会降低无法解释部分的方差。\n定理 2.6 如果 \\(\\mathbb{E}\\left[Y^{2}\\right]<\\infty\\) 那么\n\\[\n\\operatorname{var}[Y] \\geq \\operatorname{var}\\left[Y-\\mathbb{E}\\left[Y \\mid X_{1}\\right]\\right] \\geq \\operatorname{var}\\left[Y-\\mathbb{E}\\left[Y \\mid X_{1}, X_{2}\\right]\\right] .\n\\]\n定理 \\(2.6\\) 表示，每当向条件信息中添加一个附加变量时，\\(Y\\) 与其条件期望之间的差异方差（弱）就会减小。\n定理 \\(2.6\\) 的证明在 2.33 节给出。"
  },
  {
    "objectID": "chpt02-ce-chn.html#最佳预测者",
    "href": "chpt02-ce-chn.html#最佳预测者",
    "title": "条件预期和预测",
    "section": "最佳预测者",
    "text": "最佳预测者\n假设给定一个随机向量 \\(X\\)，我们想要预测或预测 \\(Y\\)。我们可以将任何预测器写成 \\(X\\) 的函数 \\(g(X)\\)。 （事后）预测误差是实现的差异 \\(Y-g(X)\\)。预测误差大小的非随机度量是其平方的期望\n\\[\n\\mathbb{E}\\left[(Y-g(X))^{2}\\right] .\n\\]\n我们可以将最佳预测器定义为最小化 (2.10) 的函数 \\(g(X)\\)。什么函数是最好的预测器？事实证明，答案是 CEF \\(m(X)\\)。无论 \\((Y, X)\\) 的联合分布如何，这都成立。\n要看到这一点，请注意预测变量 \\(g(X)\\) 的均方误差是\n\\[\n\\begin{aligned}\n\\mathbb{E}\\left[(Y-g(X))^{2}\\right] &=\\mathbb{E}\\left[(e+m(X)-g(X))^{2}\\right] \\\\\n&=\\mathbb{E}\\left[e^{2}\\right]+2 \\mathbb{E}[e(m(X)-g(X))]+\\mathbb{E}\\left[(m(X)-g(X))^{2}\\right] \\\\\n&=\\mathbb{E}\\left[e^{2}\\right]+\\mathbb{E}\\left[(m(X)-g(X))^{2}\\right] \\\\\n& \\geq \\mathbb{E}\\left[e^{2}\\right] \\\\\n&=\\mathbb{E}\\left[(Y-m(X))^{2}\\right] .\n\\end{aligned}\n\\]\n第一个等式进行替换 \\(Y=m(X)+e\\)，第三个等式使用定理 2.4.4。通过设置 \\(g(X)=m(X)\\) 最小化第三个等式后的右手边，产生第四行的不等式。如定理 \\(2.5\\) 所示，在假设 \\(\\mathbb{E}\\left[Y^{2}\\right]<\\infty\\) 下最小值是有限的。\n我们在下面的结果中正式说明了这一点。\n定理 2.7 条件期望作为最佳预测器 如果 \\(\\mathbb{E}\\left[Y^{2}\\right]<\\infty\\)，那么对于任何预测器 \\(g(X)\\)，\n\\[\n\\mathbb{E}\\left[(Y-g(X))^{2}\\right] \\geq \\mathbb{E}\\left[(Y-m(X))^{2}\\right]\n\\]\n\\(m(X)=\\mathbb{E}[Y \\mid X]\\)\n在仅截取模型的上下文中考虑此结果可能会有所帮助\n\\[\n\\begin{aligned}\nY &=\\mu+e \\\\\n\\mathbb{E}[e] &=0 .\n\\end{aligned}\n\\]\n定理 \\(2.7\\) 表明，\\(Y\\)（在常量类中）的最佳预测器是无条件均值 \\(\\mu=\\mathbb{E}[Y]\\)，因为均值使均方预测误差最小化。"
  },
  {
    "objectID": "chpt02-ce-chn.html#条件方差",
    "href": "chpt02-ce-chn.html#条件方差",
    "title": "条件预期和预测",
    "section": "条件方差",
    "text": "条件方差\n虽然条件均值可以很好地衡量条件分布的位置，但它不能提供有关分布分布的信息。分散度的常用度量是条件方差。我们首先给出随机变量 \\(Y\\) 的条件方差的一般定义。\n定义2.1 如果\\(\\mathbb{E}\\left[Y^{2}\\right]<\\infty\\)，给定\\(X=x\\)，\\(Y\\)的条件方差为\n\\[\n\\sigma^{2}(x)=\\operatorname{var}[Y \\mid X=x]=\\mathbb{E}\\left[(Y-\\mathbb{E}[Y \\mid X=x])^{2} \\mid X=x\\right] .\n\\]\n被视为随机变量的条件方差是 \\(\\operatorname{var}[Y \\mid X]=\\sigma^{2}(X)\\)。\n条件方差不同于无条件方差 var \\([Y]\\)。不同之处在于条件方差是条件变量的函数。请注意，条件方差是条件二阶矩，以条件一阶矩为中心。\n给定这个定义，我们定义回归误差的条件方差。\n定义 2.2 如果 \\(\\mathbb{E}\\left[e^{2}\\right]<\\infty\\)，则给定 \\(X=x\\) 的回归误差 \\(e\\) 的条件方差为\n\\[\n\\sigma^{2}(x)=\\operatorname{var}[e \\mid X=x]=\\mathbb{E}\\left[e^{2} \\mid X=x\\right] .\n\\]\n\\(e\\) 被视为随机变量的条件方差是 \\(\\operatorname{var}[e \\mid X]=\\sigma^{2}(X)\\)。\n同样，条件方差 \\(\\sigma^{2}(x)\\) 与无条件方差 \\(\\sigma^{2}\\) 不同。条件方差是回归量的函数，无条件方差不是。通常，\\(\\sigma^{2}(x)\\) 是 \\(x\\) 的一个非平凡函数，并且可以采用任何形式，但必须限制它为非负数。考虑 \\(\\sigma^{2}(x)\\) 的一种方法是，它是给定 \\(X\\) 的 \\(e^{2}\\) 的条件均值。还要注意 \\(\\sigma^{2}(x)=\\operatorname{var}[Y \\mid X=x]\\) 所以它等效地是因变量的条件方差。\n\\(Y\\) 的方差与 \\(Y\\) 的度量单位不同。为了将方差转换为相同的度量单位，我们将条件标准差定义为其平方根 \\(\\sigma(x)=\\sqrt{\\sigma^{2}(x)}\\)。\n作为条件方差如何依赖于可观察值的示例，比较图 2.2 中显示的男性和女性的条件对数工资密度。密度之间的差异不仅是位置偏移，而且也是传播差异。具体来说，我们可以看到男性的对数工资密度比女性的分布更分散，而女性的工资密度则更峰值。事实上，男性工资的条件标准差是 \\(3.05\\)，女性工资的条件标准差是 \\(2.81\\)。因此，虽然男性的平均工资较高，但他们也更加分散。\n无条件方差通过以下恒等式与条件方差相关。\n定理 2.8 如果 \\(\\mathbb{E}\\left[Y^{2}\\right]<\\infty\\) 那么\n\\[\n\\operatorname{var}[Y]=\\mathbb{E}[\\operatorname{var}[Y \\mid X]]+\\operatorname{var}[\\mathbb{E}[Y \\mid X]] .\n\\]\n参见经济学家的概率和统计定理 \\(4.14\\)。定理 \\(2.8\\) 将无条件方差分解为有时称为“组内方差”和“组间方差”的变量。例如，如果 \\(X\\) 是教育水平，那么第一项是教育水平的条件期望的期望方差。第二项是控制教育后的方差。\n回归误差的条件均值为零，因此它的无条件误差方差等于期望的条件方差，或者等效地可以通过迭代期望定律找到。\n\\[\n\\sigma^{2}=\\mathbb{E}\\left[e^{2}\\right]=\\mathbb{E}\\left[\\mathbb{E}\\left[e^{2} \\mid X\\right]\\right]=\\mathbb{E}\\left[\\sigma^{2}(X)\\right] .\n\\]\n也就是说，无条件误差方差是平均条件方差。\n给定条件方差，我们可以定义重新调整的误差\n\\[\nu=\\frac{e}{\\sigma(X)} \\text {. }\n\\]\n我们计算出，因为 \\(\\sigma(X)\\) 是 \\(X\\) 的函数\n\\[\n\\mathbb{E}[u \\mid X]=\\mathbb{E}\\left[\\frac{e}{\\sigma(X)} \\mid X\\right]=\\frac{1}{\\sigma(X)} \\mathbb{E}[e \\mid X]=0\n\\]\n和\n\\[\n\\operatorname{var}[u \\mid X]=\\mathbb{E}\\left[u^{2} \\mid X\\right]=\\mathbb{E}\\left[\\frac{e^{2}}{\\sigma^{2}(X)} \\mid X\\right]=\\frac{1}{\\sigma^{2}(X)} \\mathbb{E}\\left[e^{2} \\mid X\\right]=\\frac{\\sigma^{2}(X)}{\\sigma^{2}(X)}=1 .\n\\]\n因此 \\(u\\) 的条件期望为零，条件方差为 1。\n注意 (2.11) 可以重写为\n\\[\ne=\\sigma(X) u .\n\\]\n并将其替换为 CEF 方程 (2.9) 中的 \\(e\\)，我们发现\n\\[\nY=m(X)+\\sigma(X) u .\n\\]\n这是 CEF 方程的另一种（均值方差）表示。\n许多计量经济学研究侧重于条件期望 \\(m(x)\\)，要么忽略条件方差 \\(\\sigma^{2}(x)\\)，将其视为常数 \\(\\sigma^{2}(x)=\\sigma^{2}\\)，要么将其视为令人讨厌的参数（不是主要关注的参数）。当条件分布的主要变化处于均值时，这是合适的，但在其他情况下可能是短视的。分散与许多经济主题相关，包括收入和财富分配、经济不平等和价格分散。条件色散（方差）可能是一个富有成果的研究课题。\n在一个经典笑话中模仿了狭隘地关注平均值的反常后果：\n一位经济学家一只脚站在一桶开水里，另一只脚站在一桶冰里。当被问及感觉如何时，他回答说：“平均而言，我感觉还不错。”\n显然，有问题的经济学家忽略了方差！"
  },
  {
    "objectID": "chpt02-ce-chn.html#同方差和异方差",
    "href": "chpt02-ce-chn.html#同方差和异方差",
    "title": "条件预期和预测",
    "section": "同方差和异方差",
    "text": "同方差和异方差\n当条件方差 \\(\\sigma^{2}(x)\\) 是一个常数并且独立于 \\(x\\) 时，会出现一个重要的特殊情况。这称为同方差。\n定义 2.3 如果 \\(\\sigma^{2}(x)=\\sigma^{2}\\) 不依赖于 \\(x\\)，则该误差是同方差的。\n在 \\(\\sigma^{2}(x)\\) 取决于 \\(x\\) 的一般情况下，我们说错误 \\(e\\) 是异方差的。\n定义 2.4 如果 \\(\\sigma^{2}(x)\\) 依赖于 \\(x\\)，则误差是异方差的。\n理解同方差和异方差的概念与条件方差有关，而不是无条件方差，这是有帮助的。根据定义，无条件方差 \\(\\sigma^{2}\\) 是一个常数，并且独立于回归量 \\(X\\)。因此，当我们谈论作为回归量函数的方差时，我们谈论的是条件方差 \\(\\sigma^{2}(x)\\)。\n一些较旧或介绍性的教科书将异方差描述为“\\(e\\) 的方差随观察而变化”的情况。这是一个糟糕且令人困惑的定义。理解异方差性意味着条件方差 \\(\\sigma^{2}(x)\\) 取决于可观察量更具建设性。\n较旧的教科书也倾向于将同方差描述为正确回归规范的组成部分，并将异方差描述为异常或偏差。这种描述影响了几代经济学家，但不幸的是它倒退了。正确的观点是异方差是通用的和“标准的”，而同方差是不寻常的和例外的。实证工作的默认设置应该是假设误差是异方差的，而不是相反的。\n与上述陈述明显矛盾的是，在对估计和推理方法的性质进行理论研究时，我们仍然会经常强加同方差假设。原因是在很多情况下，同方差极大地简化了理论计算，因此对教学非常有利。然而，应该永远记住，同方差性从未被强加，因为它被认为是经验模型的正确特征，而是因为它的简单性。"
  },
  {
    "objectID": "chpt02-ce-chn.html#异方差还是异方差",
    "href": "chpt02-ce-chn.html#异方差还是异方差",
    "title": "条件预期和预测",
    "section": "异方差还是异方差？",
    "text": "异方差还是异方差？\n同方差和异方差这两个词的拼写有些争议。早期的计量经济学教科书是分裂的，有些使用“c”作为异方差，有些使用“\\(\\mathrm{k}\\)”作为异方差。 McCulloch (1985) 指出这个词来源于希腊语词根。\n\\ 意思是“分散”。由于在 \\(\\sigma \\kappa \\varepsilon \\delta \\alpha v v v \\mu \\iota\\) 中希腊字母 \\(\\kappa\\) 的正确音译是“\\(\\mathrm{k}\\)”，这意味着这两个单词的正确英文拼写是“\\(\\mathrm{k}\\)”，如同方差和异方差。"
  },
  {
    "objectID": "chpt02-ce-chn.html#回归导数",
    "href": "chpt02-ce-chn.html#回归导数",
    "title": "条件预期和预测",
    "section": "回归导数",
    "text": "回归导数\n解释 CEF \\(m(x)=\\mathbb{E}[Y \\mid X=x]\\) 的一种方法是根据回归量 \\(X\\) 的边际变化如何暗示响应变量 \\(Y\\) 的条件期望的变化。通常考虑单个回归量的边际变化，比如 \\(X_{1}\\)，保持余数不变。当回归量 \\(X_{1}\\) 连续分布时，我们定义 \\(X_{1}\\) 变化的边际效应，保持变量 \\(X_{2}, \\ldots, X_{k}\\) 固定，作为 CEF 的偏导数\n\\[\n\\frac{\\partial}{\\partial x_{1}} m\\left(x_{1}, \\ldots, x_{k}\\right)\n\\]\n当 \\(X_{1}\\) 是离散的时，我们将边际效应定义为离散差。例如，如果 \\(X_{1}\\) 是二元的，那么 \\(X_{1}\\) 对 CEF 的边际效应是\n\\[\nm\\left(1, x_{2}, \\ldots, x_{k}\\right)-m\\left(0, x_{2}, \\ldots, x_{k}\\right)\n\\]\n我们可以用符号统一连续和离散的情况\n\\[\n\\nabla_{1} m(x)=\\left\\{\\begin{array}{cc}\n\\frac{\\partial}{\\partial x_{1}} m\\left(x_{1}, \\ldots, x_{k}\\right), & \\text { if } X_{1} \\text { is continuous } \\\\\nm\\left(1, x_{2}, \\ldots, x_{k}\\right)-m\\left(0, x_{2}, \\ldots, x_{k}\\right), & \\text { if } X_{1} \\text { is binary. }\n\\end{array}\\right.\n\\]\n将 \\(k\\) 效应收集到一个 \\(k \\times 1\\) 向量中，我们定义关于 \\(X\\) 的回归导数：\n\\[\n\\nabla m(x)=\\left[\\begin{array}{c}\n\\nabla_{1} m(x) \\\\\n\\nabla_{2} m(x) \\\\\n\\vdots \\\\\n\\nabla_{k} m(x)\n\\end{array}\\right]\n\\]\n当 \\(X\\) 的所有元素都是连续的时，我们有简化的 \\(\\nabla m(x)=\\frac{\\partial}{\\partial x} m(x)\\)，即偏导数的向量。\n关于回归导数的定义，有两点需要记住。首先，在保持其他变量不变的情况下计算每个变量的影响。这是经济学中常用的其他条件不变的概念。但是在回归导数的情况下，条件期望并不能真正保持其他所有条件不变。它只保持条件期望中包含的变量不变。这意味着回归导数取决于包含哪些回归量。例如，在工资对教育、经验、种族和性别的回归中，关于教育的回归导数显示了教育对预期工资的边际效应，保持不变的经验、种族和性别。但它并不保持个人不可观察的特征（如能力）和回归中未包含的变量（如教育质量）保持不变。\n其次，回归导数是 \\(Y\\) 的条件期望的变化，而不是 \\(Y\\) 对个人的实际价值的变化。很容易将回归导数视为 \\(Y\\) 实际值的变化，但这不是正确的解释。只有当误差 \\(e\\) 不受回归量 \\(X\\) 变化的影响时，回归导数 \\(\\nabla m(x)\\) 才是 \\(Y\\) 实际值的变化。我们回到第 2.30 节中对因果效应的讨论。"
  },
  {
    "objectID": "chpt02-ce-chn.html#线性-cef",
    "href": "chpt02-ce-chn.html#线性-cef",
    "title": "条件预期和预测",
    "section": "线性 CEF",
    "text": "线性 CEF\n一个重要的特殊情况是 CEF \\(m(x)=\\mathbb{E}[Y \\mid X=x]\\) 在 \\(x\\) 中是线性的。在这种情况下，我们可以将平均方程写为\n\\[\nm(x)=x_{1} \\beta_{1}+x_{2} \\beta_{2}+\\cdots+x_{k} \\beta_{k}+\\beta_{k+1} .\n\\]\n从符号上讲，把它写成向量 \\(x\\) 的简单函数很方便。一个简单的方法是通过将数字“1”列为元素来增加回归向量 \\(X\\)。我们将此称为“常数”，相应的系数称为“截距”。等效地，指定向量 \\(x\\) 的最后一个元素 \\({ }^{9}\\) 是 \\(x_{k}=1\\)。因此 (2.4) 被重新定义为 \\(k \\times 1\\) 向量\n\\[\nX=\\left(\\begin{array}{c}\nX_{1} \\\\\nX_{2} \\\\\n\\vdots \\\\\nX_{k-1} \\\\\n1\n\\end{array}\\right)\n\\]\n通过这个重新定义，CEF 是\n\\[\nm(x)=x_{1} \\beta_{1}+x_{2} \\beta_{2}+\\cdots+\\beta_{k}=x^{\\prime} \\beta\n\\]\n在哪里\n\\[\n\\beta=\\left(\\begin{array}{c}\n\\beta_{1} \\\\\n\\vdots \\\\\n\\beta_{k}\n\\end{array}\\right)\n\\]\n是一个 \\(k \\times 1\\) 系数向量。这是线性 CEF 模型。它也经常被称为线性回归模型，或 \\(Y\\) 对 \\(X\\) 的回归。\n在线性 CEF 模型中，回归导数就是系数向量。那是 \\(\\nabla m(x)=\\beta\\)。这是线性 CEF 模型的吸引人的特征之一。作为改变一个变量而保持其他变量不变的边际效应，这些系数具有简单而自然的解释。\n\\[\n\\begin{aligned}\n&\\text { Linear CEF Model } \\\\\n&\\begin{aligned}\nY &=X^{\\prime} \\beta+e \\\\\n\\mathbb{E}[e \\mid X] &=0\n\\end{aligned}\n\\end{aligned}\n\\]\n如果另外误差是同方差的，我们称之为同方差线性 CEF 模型。"
  },
  {
    "objectID": "chpt02-ce-chn.html#同方差线性-cef-模型",
    "href": "chpt02-ce-chn.html#同方差线性-cef-模型",
    "title": "条件预期和预测",
    "section": "同方差线性 CEF 模型",
    "text": "同方差线性 CEF 模型\n\\[\n\\begin{aligned}\nY &=X^{\\prime} \\beta+e \\\\\n\\mathbb{E}[e \\mid X] &=0 \\\\\n\\mathbb{E}\\left[e^{2} \\mid X\\right] &=\\sigma^{2}\n\\end{aligned}\n\\]\n\\({ }^{9}\\) 顺序无关紧要。它可以是任何元素。"
  },
  {
    "objectID": "chpt02-ce-chn.html#具有非线性效应的线性-cef",
    "href": "chpt02-ce-chn.html#具有非线性效应的线性-cef",
    "title": "条件预期和预测",
    "section": "具有非线性效应的线性 CEF",
    "text": "具有非线性效应的线性 CEF\n上一节的线性 CEF 模型没有看起来那么严格，因为我们可以将原始变量的非线性变换作为回归量。从这个意义上说，线性 CEF 框架是灵活的，可以捕捉许多非线性效应。\n例如，假设我们有两个标量变量 \\(X_{1}\\) 和 \\(X_{2}\\)。 CEF 可以采用二次形式\n\\[\nm\\left(x_{1}, x_{2}\\right)=x_{1} \\beta_{1}+x_{2} \\beta_{2}+x_{1}^{2} \\beta_{3}+x_{2}^{2} \\beta_{4}+x_{1} x_{2} \\beta_{5}+\\beta_{6} .\n\\]\n这个方程在回归变量 \\(\\left(x_{1}, x_{2}\\right)\\) 中是二次的，但在系数 \\(\\beta=\\left(\\beta_{1}, \\ldots, \\beta_{6}\\right)^{\\prime}\\) 中是线性的。我们仍然称 (2.14) 为线性 CEF，因为它是系数的线性函数。同时，它具有非线性效应，因为它在基础变量 \\(x_{1}\\) 和 \\(x_{2}\\) 中是非线性的。关键是要理解 (2.14) 在变量 \\(\\left(x_{1}, x_{2}\\right)\\) 中是二次的，但在系数 \\(\\beta\\) 中是线性的。\n为了简化表达式，我们定义了变换 \\(x_{3}=x_{1}^{2}, x_{4}=x_{2}^{2}, x_{5}=x_{1} x_{2}\\) 和 \\(x_{6}=1\\)，并将回归向量重新定义为 \\(x=\\left(x_{1}, \\ldots, x_{6}\\right)^{\\prime}\\)。通过这个重新定义，\\(m\\left(x_{1}, x_{2}\\right)=x^{\\prime} \\beta\\) 在 \\(\\beta\\) 中是线性的。对于大多数计量经济学目的（对 \\(\\beta\\) 的估计和推断），\\(\\beta\\) 中的线性是最重要的。\n一个例外是回归导数的分析。在诸如 (2.14) 等非线性方程中，回归导数应针对原始变量定义，而不是针对转换后的变量。因此\n\\[\n\\begin{aligned}\n&\\frac{\\partial}{\\partial x_{1}} m\\left(x_{1}, x_{2}\\right)=\\beta_{1}+2 x_{1} \\beta_{3}+x_{2} \\beta_{5} \\\\\n&\\frac{\\partial}{\\partial x_{2}} m\\left(x_{1}, x_{2}\\right)=\\beta_{2}+2 x_{2} \\beta_{4}+x_{1} \\beta_{5} .\n\\end{aligned}\n\\]\n我们看到，在模型（2.14）中，回归导数不是一个简单的系数，而是几个系数加上 \\(\\left(x_{1}, x_{2}\\right)\\) 的水平的函数。因此，很难单独解释这些系数。将它们解释为一个组更有用。\n我们通常将 \\(\\beta_{5}\\) 称为交互效应。请注意，它出现在两个回归导数方程中，并且在每个方程中都有一个对称的解释。如果 \\(\\beta_{5}>0\\) 那么关于 \\(x_{1}\\) 的回归导数在 \\(x_{2}\\) 的水平上增加（并且关于 \\(x_{2}\\) 的回归导数在 \\(x_{1}\\) 的水平上增加），而如果 \\(\\beta_{5}<0\\)反之亦然。"
  },
  {
    "objectID": "chpt02-ce-chn.html#带有虚拟变量的线性-cef",
    "href": "chpt02-ce-chn.html#带有虚拟变量的线性-cef",
    "title": "条件预期和预测",
    "section": "带有虚拟变量的线性 CEF",
    "text": "带有虚拟变量的线性 CEF\n当所有回归量都取一组有限的值时，结果证明 CEF 可以写成回归量的线性函数。\n这个最简单的例子是一个二进制变量，它只接受两个不同的值。例如，在传统数据集中，变量性别只取值男人和女人（或男性和女性）。二元变量在计量经济学应用中极为常见，也称为虚拟变量或指标变量。\n考虑单个二元回归器的简单情况。在这种情况下，条件期望只能取两个不同的值。例如，\n\\[\n\\mathbb{E}[Y \\mid \\text { gender }]=\\left\\{\\begin{array}{llc}\n\\mu_{0} & \\text { if } \\quad \\text { gender }=\\text { man } \\\\\n\\mu_{1} & \\text { if gender }=\\text { woman. }\n\\end{array}\\right.\n\\]\n为了便于数学处理，我们用值 \\(\\{0,1\\}\\) 记录虚拟变量。例如\n\\[\nX_{1}=\\left\\{\\begin{array}{llc}\n0 & \\text { if } & \\text { gender }=\\text { man } \\\\\n1 & \\text { if } & \\text { gender }=\\text { woman } .\n\\end{array}\\right.\n\\]\n给定这个符号，我们将条件期望写成虚拟变量 \\(X_{1}\\) 的线性函数。因此 \\(\\mathbb{E}\\left[Y \\mid X_{1}\\right]=\\beta_{1} X_{1}+\\beta_{2}\\) 其中 \\(\\beta_{1}=\\mu_{1}-\\mu_{0}\\) 和 \\(\\beta_{2}=\\mu_{0}\\)。在这个简单的回归方程中，截距 \\(\\beta_{2}\\) 等于 \\(X_{1}=0\\) 子群体（男性）的条件期望 \\(Y\\)，斜率 \\(\\beta_{1}\\) 等于两个子群体之间条件期望的差异。\n或者，我们可以将 \\(X_{1}\\) 定义为\n\\[\nX_{1}= \\begin{cases}1 & \\text { if } \\quad \\text { gender }=\\text { man } \\\\ 0 & \\text { if } \\quad \\text { gender }=\\text { woman } .\\end{cases}\n\\]\n在这种情况下，回归截距是对女性（而不是男性）的期望，并且回归斜率已转换符号。这两个回归是等价的，但系数的解释发生了变化。因此，理解变量的精确定义总是很重要的，并且说明性的标签很有帮助。例如，将 \\(X_{1}\\) 标记为“性别”无助于区分定义 (2.15) 和 (2.16)。相反，如果使用定义 (2.15)，最好将 \\(X_{1}\\) 标记为“女性”或“女性”，如果使用定义 (2.16)，则将其标记为“男性”或“男性”。\n现在假设我们有两个虚拟变量 \\(X_{1}\\) 和 \\(X_{2}\\)。例如，如果此人已婚，则为 \\(X_{2}=1\\)，否则为 \\(X_{2}=0\\)。给定 \\(X_{1}\\) 和 \\(X_{2}\\) 的条件期望最多取四个可能的值：\n\n在这种情况下，我们可以将条件均值写为 \\(X, X_{2}\\) 及其乘积 \\(X_{1} X_{2}\\) 的线性函数：\n\\[\n\\mathbb{E}\\left[Y \\mid X_{1}, X_{2}\\right]=\\beta_{1} X_{1}+\\beta_{2} X_{2}+\\beta_{3} X_{1} X_{2}+\\beta_{4}\n\\]\n其中 \\(\\beta_{1}=\\mu_{10}-\\mu_{00}, \\beta_{2}=\\mu_{01}-\\mu_{00}, \\beta_{3}=\\mu_{11}-\\mu_{10}-\\mu_{01}+\\mu_{00}\\) 和 \\(\\beta_{4}=\\mu_{00}\\)。\n我们可以将系数 \\(\\beta_{1}\\) 视为性别对未婚工薪者预期对数工资的影响，将系数 \\(\\beta_{2}\\) 视为婚姻对男性工薪者预期对数工资的影响，并将系数 \\(\\beta_{3}\\) 视为差异婚姻对女性和男性预期对数工资的影响之间的关系。或者，它也可以解释为性别对已婚和未婚工资收入者的预期对数工资的影响之间的差异。两种解释同样有效。我们经常将 \\(\\beta_{3}\\) 描述为衡量两个虚拟变量之间的交互作用，或交互效应，而将 \\(\\beta_{3}=0\\) 描述为交互效应为零的情况。\n在这个设置中，我们可以看到 CEF 在三个变量 \\(\\left(X_{1}, X_{2}, X_{1} X_{2}\\right)\\) 中是线性的。为了将模型放入 \\(2.15\\) 部分的框架中，我们将回归量 \\(X_{3}=X_{1} X_{2}\\) 和回归量向量定义为\n\\[\nX=\\left(\\begin{array}{c}\nX_{1} \\\\\nX_{2} \\\\\nX_{3} \\\\\n1\n\\end{array}\\right) .\n\\]\n因此，虽然我们从两个虚拟变量开始，但回归量（包括截距）的数量是四个。\n如果有三个虚拟变量 \\(X_{1}, X_{2}, X_{3}\\)，那么 \\(\\mathbb{E}\\left[Y \\mid X_{1}, X_{2}, X_{3}\\right]\\) 最多取 \\(2^{3}=8\\) 不同的值，并且可以写成线性函数\n\\[\n\\mathbb{E}\\left[Y \\mid X_{1}, X_{2}, X_{3}\\right]=\\beta_{1} X_{1}+\\beta_{2} X_{2}+\\beta_{3} X_{3}+\\beta_{4} X_{1} X_{2}+\\beta_{5} X_{1} X_{3}+\\beta_{6} X_{2} X_{3}+\\beta_{7 X 1} X_{2} X_{3}+\\beta_{8}\n\\]\n它有八个回归量，包括截距。一般来说，如果有 \\(p\\) 虚拟变量 \\(X_{1}, \\ldots, X_{p}\\) 那么 CEF \\(\\mathbb{E}\\left[Y \\mid X_{1}, X_{2}, \\ldots, X_{p}\\right]\\) 最多取 \\(2^{p}\\) 不同的值，并且可以写成 \\(2^{p}\\) 回归量的线性函数，包括 \\(X_{1}, X_{2}, \\ldots, X_{p}\\) 和所有交叉产品。包含所有 \\(2^{p}\\) 二元相互作用的线性回归模型称为饱和虚拟变量回归模型。它是条件期望的完整模型。相反，没有交互作用的模型等于\n\\[\n\\mathbb{E}\\left[Y \\mid X_{1}, X_{2}, \\ldots, X_{p}\\right]=\\beta_{1} X_{1}+\\beta_{2} X_{2}+\\cdots+\\beta_{p} X_{p}+\\beta_{p} .\n\\]\n这有 \\(p+1\\) 系数而不是 \\(2^{p}\\)。\n我们在本节开始时说，只要所有回归器只取有限数量的可能值，条件期望就是线性的。我们怎么能看到这个？取一个分类变量，例如种族。例如，我们之前将种族分为三类。我们可以记录分类变量，使用数字来表示每个类别，例如\n\\[\nX_{3}=\\left\\{\\begin{array}{lll}\n1 & \\text { if } & \\text { white } \\\\\n2 & \\text { if } & \\text { Black } \\\\\n3 & \\text { if } & \\text { other. }\n\\end{array}\\right.\n\\]\n这样做时，\\(X_{3}\\) 的值在量级方面没有意义，它们只是表示相关类别。\n当回归量是分类的时，给定 \\(X_{3}\\) 的条件期望 \\(Y\\) 对每种可能性取一个不同的值：\n\\[\n\\mathbb{E}\\left[Y \\mid X_{3}\\right]=\\left\\{\\begin{array}{lll}\n\\mu_{1} & \\text { if } & X_{3}=1 \\\\\n\\mu_{2} & \\text { if } & X_{3}=2 \\\\\n\\mu_{3} & \\text { if } & X_{3}=3 .\n\\end{array}\\right.\n\\]\n这不是 \\(X_{3}\\) 本身的线性函数，但可以通过为三个类别中的两个构建虚拟变量来使其成为线性函数。例如\n\\[\n\\begin{aligned}\n&X_{4}=\\left\\{\\begin{array}{llc}\n1 & \\text { if } & \\text { Black } \\\\\n0 & \\text { if } & \\text { not Black }\n\\end{array}\\right. \\\\\n&X_{5}=\\left\\{\\begin{array}{lll}\n1 & \\text { if } & \\text { other } \\\\\n0 & \\text { if } & \\text { not other. }\n\\end{array}\\right.\n\\end{aligned}\n\\]\n在这种情况下，分类变量 \\(X_{3}\\) 等价于一对虚拟变量 \\(\\left(X_{4}, X_{5}\\right)\\)。显式关系是\n\\[\nX_{3}=\\left\\{\\begin{array}{lll}\n1 & \\text { if } & X_{4}=0 \\text { and } X_{5}=0 \\\\\n2 & \\text { if } & X_{4}=1 \\text { and } X_{5}=0 \\\\\n3 & \\text { if } & X_{4}=0 \\text { and } X_{5}=1\n\\end{array}\\right.\n\\]\n鉴于这些转换，我们可以将 \\(Y\\) 的条件期望写成 \\(X_{4}\\) 和 \\(X_{5}\\) 的线性函数\n\\[\n\\mathbb{E}\\left[Y \\mid X_{3}\\right]=\\mathbb{E}\\left[Y \\mid X_{4}, X_{5}\\right]=\\beta_{1} X_{4}+\\beta_{2} X_{5}+\\beta_{3} .\n\\]\n我们可以将 CEF 写成 \\(\\mathbb{E}\\left[Y \\mid X_{3}\\right]\\) 或 \\(\\mathbb{E}\\left[Y \\mid X_{4}, X_{5}\\right]\\)（它们是等价的），但它只是 \\(X_{4}\\) 和 \\(X_{5}\\) 的线性函数。\n此设置类似于两个虚拟变量的情况，不同之处在于我们没有包括交互项 \\(X_{4} X_{5}\\)。这是因为事件 \\(\\left\\{X_{4}=1\\right.\\) 和 \\(\\left.X_{5}=1\\right\\}\\) 在构造上是空的，所以 \\(X_{4} X_{5}=0\\) 根据定义。"
  },
  {
    "objectID": "chpt02-ce-chn.html#最佳线性预测器",
    "href": "chpt02-ce-chn.html#最佳线性预测器",
    "title": "条件预期和预测",
    "section": "最佳线性预测器",
    "text": "最佳线性预测器\n虽然条件期望 \\(m(X)=\\mathbb{E}[Y \\mid X]\\) 是 \\(X\\) 的所有函数中 \\(Y\\) 的最佳预测器，但它的函数形式通常是未知的。特别是，线性 CEF 模型在经验上不太可能准确，除非 \\(X\\) 是离散且低维的，因此包含所有交互作用。因此，在大多数情况下，将线性规范 (2.13) 视为近似值更为现实。在本节中，我们通过简单的解释得出一个特定的近似值。\n定理 \\(2.7\\) 表明条件期望 \\(m(X)\\) 是最好的预测器，因为它在所有预测器中均方误差最低。通过扩展，我们可以通过所有线性预测变量中均方误差最小的线性函数来定义 CEF 的近似值。\n对于这个推导，我们需要以下正则性条件。\n假设 \\(2.1\\)\n\n\\(\\mathbb{E}\\left[Y^{2}\\right]<\\infty\\)\n\\(\\mathbb{E}\\|X\\|^{2}<\\infty\\)\n\\(\\boldsymbol{Q}_{X X}=\\mathbb{E}\\left[X X^{\\prime}\\right]\\) 是正定的。\n\n在假设 2.1.2 中，我们使用 \\(\\|x\\|=\\left(x^{\\prime} x\\right)^{1 / 2}\\) 来表示向量 \\(x\\) 的欧几里得长度。\n假设 \\(2.1\\) 的前两部分暗示变量 \\(Y\\) 和 \\(X\\) 具有有限的均值、方差和协方差。假设的第三部分更具技术性，其作用很快就会显现出来。相当于强加矩阵 \\(\\boldsymbol{Q}_{X X}=\\mathbb{E}\\left[X X^{\\prime}\\right]\\) 的列是线性独立的并且矩阵是可逆的。\n\\(Y\\) 的线性预测器是某个 \\(\\beta \\in \\mathbb{R}^{k}\\) 的函数 \\(X^{\\prime} \\beta\\)。均方预测误差为\n\\[\nS(\\beta)=\\mathbb{E}\\left[\\left(Y-X^{\\prime} \\beta\\right)^{2}\\right] .\n\\]\n给定 \\(X\\) 的 \\(Y\\) 的最佳线性预测器，写成 \\(\\mathscr{P}[Y \\mid X]\\)，是通过选择使 \\(S(\\beta)\\) 最小化的 \\(\\beta\\) 来找到的。\n定义 2.5 给定 \\(X\\) 的 \\(Y\\) 的最佳线性预测器是\n\\[\n\\mathscr{P}[Y \\mid X]=X^{\\prime} \\beta\n\\]\n其中 \\(\\beta\\) 最小化均方预测误差\n\\[\nS(\\beta)=\\mathbb{E}\\left[\\left(Y-X^{\\prime} \\beta\\right)^{2}\\right] .\n\\]\n最小化器\n\\[\n\\beta=\\underset{b \\in \\mathbb{R}^{k}}{\\operatorname{argmin}} S(b)\n\\]\n称为线性投影系数。我们现在计算其值的显式表达式。均方预测误差 (2.17) 可以写成 \\(\\beta\\) 的二次函数：\n\\[\nS(\\beta)=\\mathbb{E}\\left[Y^{2}\\right]-2 \\beta^{\\prime} \\mathbb{E}[X Y]+\\beta^{\\prime} \\mathbb{E}\\left[X X^{\\prime}\\right] \\beta .\n\\]\n\\(S(\\beta)\\) 的二次结构意味着我们可以显式求解最小化器。最小化的一阶条件（来自附录 A.20）是\n\\[\n0=\\frac{\\partial}{\\partial \\beta} S(\\beta)=-2 \\mathbb{E}[X Y]+2 \\mathbb{E}\\left[X X^{\\prime}\\right] \\beta .\n\\]\n将 \\((2.20)\\) 重写为\n\\[\n2 \\mathbb{E}[X Y]=2 \\mathbb{E}\\left[X X^{\\prime}\\right] \\beta\n\\]\n并除以 2 ，这个方程的形式为\n\\[\n\\boldsymbol{Q}_{X Y}=\\boldsymbol{Q}_{X X} \\beta\n\\]\n其中 \\(\\boldsymbol{Q}_{X Y}=\\mathbb{E}[X Y]\\) 是 \\(k \\times 1\\) 而 \\(\\boldsymbol{Q}_{X X}=\\mathbb{E}\\left[X X^{\\prime}\\right]\\) 是 \\(k \\times k\\)。通过反转矩阵 \\(\\boldsymbol{Q}_{X X}\\) 找到解决方案，并写成\n\\[\n\\beta=\\boldsymbol{Q}_{X X}^{-1} \\boldsymbol{Q}_{X Y}\n\\]\n或者\n\\[\n\\beta=\\left(\\mathbb{E}\\left[X X^{\\prime}\\right]\\right)^{-1} \\mathbb{E}[X Y] .\n\\]\n值得花时间理解表达式（2.22）中涉及的符号。 \\(\\boldsymbol{Q}_{X X}\\) 是 \\(k \\times k\\) 矩阵，\\(\\boldsymbol{Q}_{X Y}\\) 是 \\(k \\times 1\\) 列向量。因此，\\(\\frac{\\mathbb{E}[X Y]}{\\mathbb{E}\\left[X X^{\\prime}\\right]}\\) 或 \\(\\mathbb{E}[X Y]\\left(\\mathbb{E}\\left[X X^{\\prime}\\right]\\right)^{-1}\\) 等替代表达式是不连贯且不正确的。我们现在也可以看到假设 2.1.3 的作用。这等价于假设 \\(\\boldsymbol{Q}_{X X}\\) 有一个逆 \\(\\boldsymbol{Q}_{X X}^{-1}\\)，这是正规方程 (2.21) 的解唯一所必需的，并且等效地，对于 \\((2.22)\\) 是唯一定义的。在没有假设 \\(2.1 .3\\) 的情况下，方程 (2.21) 可能有多个解。\n我们现在有一个最佳线性预测器的显式表达式：\n\\[\n\\mathscr{P}[Y \\mid X]=X^{\\prime}\\left(\\mathbb{E}\\left[X X^{\\prime}\\right]\\right)^{-1} \\mathbb{E}[X Y] .\n\\]\n这个表达式也被称为 \\(Y\\) 在 \\(X\\) 上的线性投影。\n投影误差为\n\\[\ne=Y-X^{\\prime} \\beta .\n\\]\n当（且仅当）条件期望在 \\(X\\) 中为线性时，这等于回归方程的误差 (2.9)，否则它们是不同的。\n重写，我们将 \\(Y\\) 分解为线性预测器和误差\n\\[\nY=X^{\\prime} \\beta+e .\n\\]\n一般来说，我们称方程 (2.24) 或 \\(X^{\\prime} \\beta\\) 为给定 \\(X\\) 的 \\(Y\\) 的最佳线性预测器，或 \\(Y\\) 在 \\(X\\) 上的线性投影。等式 (2.24) 也经常被称为 \\(Y\\) 对 \\(X\\) 的回归，但由于经济学家在许多情况下使用术语“回归”，这有时会令人困惑。 （回想一下，我们在 \\(2.15\\) 节中说过，线性 CEF 模型也称为线性回归模型。）\n投影误差 \\(e\\) 的一个重要性质是\n\\[\n\\mathbb{E}[X e]=0 .\n\\]\n要看到这一点，使用定义 (2.23) 和 (2.22) 以及矩阵属性 \\(\\boldsymbol{A} \\boldsymbol{A}^{-1}=\\boldsymbol{I}\\) 和 \\(\\boldsymbol{I} \\boldsymbol{a}=\\boldsymbol{a}\\)，\n\\[\n\\begin{aligned}\n\\mathbb{E}[X e] &=\\mathbb{E}\\left[X\\left(Y-X^{\\prime} \\beta\\right)\\right] \\\\\n&=\\mathbb{E}[X Y]-\\mathbb{E}\\left[X X^{\\prime}\\right]\\left(\\mathbb{E}\\left[X X^{\\prime}\\right]\\right)^{-1} \\mathbb{E}[X Y] \\\\\n&=0\n\\end{aligned}\n\\]\n如声称的那样。\n方程 (2.25) 是一组 \\(k\\) 方程，每个回归量一个。换句话说，(2.25) 等价于\n\\[\n\\mathbb{E}\\left[X_{j} e\\right]=0\n\\]\n对于 \\(j=1, \\ldots, k\\)。与 (2.12) 中一样，回归向量 \\(X\\) 通常包含一个常数，例如\\(X_{k}=1\\)。在这种情况下，\\(j=k\\) 的 (2.27) 与\n\\[\n\\mathbb{E}[e]=0 .\n\\]\n因此，当回归向量包含一个常数时，投影误差的平均值为零。 （当 \\(X\\) 没有常数 (2.28) 时，不能保证。因为希望 \\(e\\) 具有零均值，这是在任何回归模型中始终包含常数的一个很好的理由。）\n观察到因为 \\(\\operatorname{cov}\\left(X_{j}, e\\right)=\\mathbb{E}\\left[X_{j} e\\right]-\\mathbb{E}\\left[X_{j}\\right] \\mathbb{E}[e]\\), then (2.27)-(2.28) 一起暗示变量 \\(X_{j}\\) 和 \\(e\\) 不相关也很有用。\n这样就完成了模型的推导。我们总结了一些最重要的属性。\n定理 2.9 线性投影模型的性质 在假设 2.1 下，\n\n矩 \\(\\mathbb{E}\\left[X X^{\\prime}\\right]\\) 和 \\(\\mathbb{E}[X Y]\\) 存在于有限元中。\n\n2.线性投影系数(2.18)存在，唯一，等于\n\\[\n\\beta=\\left(\\mathbb{E}\\left[X X^{\\prime}\\right]\\right)^{-1} \\mathbb{E}[X Y] .\n\\]\n\n给定 \\(X\\) 的 \\(Y\\) 的最佳线性预测器是\n\n\\[\n\\mathscr{P}(Y \\mid X)=X^{\\prime}\\left(\\mathbb{E}\\left[X X^{\\prime}\\right]\\right)^{-1} \\mathbb{E}[X Y] .\n\\]\n1.存在投影误差\\(e=Y-X^{\\prime} \\beta\\)。它满足 \\(\\mathbb{E}\\left[e^{2}\\right]<\\infty\\) 和 \\(\\mathbb{E}[X e]=0\\)。\n\n如果\\(X\\) 包含一个常数，那么\\(\\mathbb{E}[e]=0\\)。\n如果 \\(\\mathbb{E}|Y|^{r}<\\infty\\) 和 \\(\\mathbb{E}\\|X\\|^{r}<\\infty\\) 对应 \\(r \\geq 2\\)，那么 \\(\\mathbb{E}|e|^{r}<\\infty\\)。\n\n定理 \\(2.9\\) 的完整证明在第 2.33 节中给出。\n反思定理 2.9 的一般性是有用的。唯一的限制是假设 2.1。因此，对于任何具有有限方差的随机变量 \\((Y, X)\\)，我们可以定义一个具有定理 2.9 中列出的性质的线性方程 (2.24)。不需要更强的假设（例如线性 CEF 模型）。在这个意义上，线性模型（2.24）相当普遍地存在。但是，重要的是不要误解该声明的一般性。线性方程 (2.24) 被定义为最佳线性预测器。它不一定是条件均值，也不一定是结构或因果经济模型的参数。线性投影模型\n\\[\n\\begin{aligned}\nY &=X^{\\prime} \\beta+e \\\\\n\\mathbb{E}[X e] &=0 \\\\\n\\beta &=\\left(\\mathbb{E}\\left[X X^{\\prime}\\right]\\right)^{-1} \\mathbb{E}[X Y]\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "chpt02-ce-chn.html#可逆性和识别",
    "href": "chpt02-ce-chn.html#可逆性和识别",
    "title": "条件预期和预测",
    "section": "可逆性和识别",
    "text": "可逆性和识别\n只要 \\(k \\times k\\) 矩阵 \\(\\boldsymbol{Q}_{X X}=\\mathbb{E}\\left[X X^{\\prime}\\right]\\) 可逆，线性投影系数 \\(\\beta=\\left(\\mathbb{E}\\left[X X^{\\prime}\\right]\\right)^{-1} \\mathbb{E}[X Y]\\) 就存在并且是唯一的。矩阵 \\(\\boldsymbol{Q}_{X X}\\) 通常被称为设计矩阵，因为在实验设置中，研究人员能够通过操纵回归量 \\(X\\) 的分布来控制 \\(\\boldsymbol{Q}_{X X}\\)。\n观察任何非零 \\(\\alpha \\in \\mathbb{R}^{k}\\)，\n\\[\n\\alpha^{\\prime} \\boldsymbol{Q}_{X X} \\alpha=\\mathbb{E}\\left[\\alpha^{\\prime} X X^{\\prime} \\alpha\\right]=\\mathbb{E}\\left[\\left(\\alpha^{\\prime} X\\right)^{2}\\right] \\geq 0\n\\]\n所以 \\(\\boldsymbol{Q}_{X X}\\) 通过构造是半正定的，通常写为 \\(\\boldsymbol{Q}_{X X} \\geq 0\\)。它是正定的假设意味着这是一个严格的不等式\\(\\mathbb{E}\\left[\\left(\\alpha^{\\prime} X\\right)^{2}\\right]>0\\)。这通常写为 \\(\\boldsymbol{Q}_{X X}>0\\)。这个条件意味着不存在与 \\(\\alpha^{\\prime} X=0\\) 相同的非零向量 \\(\\alpha\\)。正定矩阵是可逆的。因此，当 \\(\\boldsymbol{Q}_{X X}>0\\) 时 \\(\\beta=\\left(\\mathbb{E}\\left[X X^{\\prime}\\right]\\right)^{-1} \\mathbb{E}[X Y]\\) 存在并且是唯一定义的。换句话说，如果我们可以排除 \\(X\\) 的线性函数退化的可能性，那么 \\(\\beta\\) 是唯一定义的。\n定理 \\(2.5\\) 表明线性投影系数 \\(\\beta\\) 在假设 2.1 下被识别（唯一确定）。关键是 \\(\\boldsymbol{Q}_{X X}\\) 的可逆性。否则，方程没有唯一解\n\\[\n\\boldsymbol{Q}_{X X} \\beta=\\boldsymbol{Q}_{X Y} .\n\\]\n当 \\(\\boldsymbol{Q}_{X X}\\) 不可逆时，(2.29) 有多种解。在这种情况下，系数 \\(\\beta\\) 没有被识别，因为它没有唯一值。"
  },
  {
    "objectID": "chpt02-ce-chn.html#最小化",
    "href": "chpt02-ce-chn.html#最小化",
    "title": "条件预期和预测",
    "section": "最小化",
    "text": "最小化\n均方预测误差 (2.19) 是具有形式向量参数的函数\n\\[\nf(x)=a-2 b^{\\prime} x+x^{\\prime} \\boldsymbol{C} x\n\\]\n其中 \\(\\boldsymbol{C}>0\\)。对于这种形式的任何函数，唯一的最小化器是\n\\[\nx=\\boldsymbol{C}^{-1} b .\n\\]\n为了看到这是唯一的最小化器，我们提供了两个证明。第一个使用矩阵微积分。来自附录 A.20\n\\[\n\\begin{gathered}\n\\frac{\\partial}{\\partial x}\\left(b^{\\prime} x\\right)=b \\\\\n\\frac{\\partial}{\\partial x}\\left(x^{\\prime} \\boldsymbol{C} x\\right)=2 \\boldsymbol{C} x \\\\\n\\frac{\\partial^{2}}{\\partial x \\partial x^{\\prime}}\\left(x^{\\prime} \\boldsymbol{C} x\\right)=2 \\boldsymbol{C} .\n\\end{gathered}\n\\]\n使用 (2.31) 和 (2.32)，我们发现\n\\[\n\\frac{\\partial}{\\partial x} f(x)=-2 b+2 \\boldsymbol{C} x .\n\\]\n最小化的一阶条件将此导数设置为零。因此解满足\\(-2 b+2 \\boldsymbol{C} x=0\\)。求解 \\(x\\) 我们找到 (2.30)。使用 (2.33) 我们还发现\n\\[\n\\frac{\\partial^{2}}{\\partial x \\partial x^{\\prime}} f(x)=2 \\boldsymbol{C}>0\n\\]\n这是最小化的二阶条件。这表明 (2.30) 是 \\(f(x)\\) 的唯一最小值。\n我们的第二个证明是代数的。将 \\(f(x)\\) 重写为\n\\[\nf(x)=\\left(a-b^{\\prime} \\boldsymbol{C}^{-1} b\\right)+\\left(x-\\boldsymbol{C}^{-1} b\\right)^{\\prime} \\boldsymbol{C}\\left(x-\\boldsymbol{C}^{-1} b\\right) .\n\\]\n第一项不依赖于 \\(x\\)，因此不影响最小化器。第二项是正定矩阵中的二次形式。这意味着对于任何非零 \\(\\alpha, \\alpha^{\\prime} \\boldsymbol{C} \\alpha>0\\)。因此对于 \\(x \\neq C^{-1} b\\)，第二项是严格正数，而对于 \\(x=C^{-1} b\\)，这个项等于 0。因此，如所声称的，它被最小化为 \\(x=C^{-1} b\\)。"
  },
  {
    "objectID": "chpt02-ce-chn.html#最佳线性预测器的插图",
    "href": "chpt02-ce-chn.html#最佳线性预测器的插图",
    "title": "条件预期和预测",
    "section": "最佳线性预测器的插图",
    "text": "最佳线性预测器的插图\n我们使用前面部分介绍的三个对数工资方程来说明最佳线性预测器（投影）。\n对于我们的第一个示例，我们考虑一个具有性别和种族两个虚拟变量的模型，类似于表 2.1。正如我们在 2.17 节中了解到的，该表中的条目可以等效地用线性 CEF 表示。为简单起见，让我们将 \\(\\log (\\) 工资 \\()\\) 的 CEF 视为 Black 和 female 的函数。\n\\[\n\\mathbb{E}[\\log (\\text { wage }) \\mid \\text { Black, female }]=-0.20 \\text { Black }-0.24 \\text { female }+0.10 \\text { Black } \\times \\text { female }+3.06 \\text {. }\n\\]\n这是一个 CEF，因为变量是二元的并且包括所有交互。\n现在考虑一个省略交互作用的更简单的模型。这是变量 Black 和 female 的线性投影\n\\[\n\\mathscr{P}[\\log (\\text { wage }) \\mid \\text { Black, female }]=-0.15 \\text { Black }-0.23 \\text { female }+3.06 .\n\\]\n有什么区别？完整的 CEF (2.34) 表明种族差距按性别区分：黑人男性（相对于非黑人男性）为 \\(20 %\\)，黑人女性（相对于非黑人女性）为 \\(10 %\\)。投影模型 (2.35) 简化了这一分析，计算了黑人工薪阶层的平均 \\(15 %\\) 工资差距，而忽略了性别的作用。请注意，尽管（2.35）中包含了性别这一事实。\n\n\n对教育的预测\n\n\n\n对经验的预测\n\n图 2.6：对数工资对教育和经验的预测\n对于我们的第二个示例，我们将对数工资的 CEF 视为白人男性受教育年限的函数，如图 \\(2.3\\) 所示，并在图 2.6(a) 中重复。叠加在图上的是两个投影。第一个（由虚线给出）是对数工资对教育年限的线性投影\n\\[\n\\mathscr{P}[\\log (\\text { wage }) \\mid \\text { education }]=0.11 \\text { education }+1.5 \\text {. }\n\\]\n这个简单的等式表明每受教育年的工资平均增加 \\(11 %\\)。对该图的检查表明，这种近似值对教育 \\(\\geq 9\\) 很有效，但对教育水平较低的个人预测不足。为了纠正这种不平衡，我们使用了一个线性样条方程，它允许高于和低于 9 年教育的不同回报率：\n\\[\n\\begin{aligned}\n&\\mathscr{P}[\\log (\\text { wage }) \\mid \\text { education, }(\\text { education }-9) \\times \\mathbb{1} \\text { education }>9\\}] \\\\\n&=0.02 \\text { education }+0.10 \\times(\\text { education }-9) \\times \\mathbb{1} \\text { education }>9\\}+2.3 .\n\\end{aligned}\n\\]\n这个方程用实线显示在图 2.6(a) 中，看起来拟合得更好。它表明每教育年低于 9 的平均工资增加 \\(2 %\\)，并且每教育年超过 9 的平均工资增加 \\(12 %\\)。它仍然是条件均值的近似值，但似乎相当合理。\n对于我们的第三个示例，我们将受过 12 年教育的白人男性的对数工资的 CEF 作为多年经验的函数，如图 \\(2.4\\) 所示，并在图 2.6(b) 中以实线重复。叠加在图上的是两个投影。第一个（由点划线给出）是经验的线性投影\n\\[\n\\mathscr{P}[\\log (\\text { wage }) \\mid \\text { experience }]=0.011 \\text { experience }+2.5\n\\]\n第二个（由虚线给出）是经验的线性投影及其平方\n\\[\n\\mathscr{P}[\\log (\\text { wage }) \\mid \\text { experience }]=0.046 \\text { experience }-0.0007 \\text { experience }^{2}+2.3 \\text {. }\n\\]\n从图 \\(2.6(\\mathrm{~b})\\) 的检查中可以清楚地看出，第一个线性投影是一个很差的近似值。它高估了年轻和年长工人的工资，低估了其余工人的工资，并错过了年长工薪阶层预期工资的强劲下滑。第二个投影更合适。我们可以称这个方程为二次投影，因为这个函数在经验上是二次的。"
  },
  {
    "objectID": "chpt02-ce-chn.html#线性预测误差方差",
    "href": "chpt02-ce-chn.html#线性预测误差方差",
    "title": "条件预期和预测",
    "section": "线性预测误差方差",
    "text": "线性预测误差方差\n与 CEF 模型一样，我们将误差方差定义为 \\(\\sigma^{2}=\\mathbb{E}\\left[e^{2}\\right]\\)。设置 \\(Q_{Y Y}=\\mathbb{E}\\left[Y^{2}\\right]\\) 和 \\(\\boldsymbol{Q}_{Y X}=\\) \\(\\mathbb{E}\\left[Y X^{\\prime}\\right]\\) 我们可以将 \\(\\sigma^{2}\\) 写为\n\\[\n\\begin{aligned}\n\\sigma^{2} &=\\mathbb{E}\\left[\\left(Y-X^{\\prime} \\beta\\right)^{2}\\right] \\\\\n&=\\mathbb{E}\\left[Y^{2}\\right]-2 \\mathbb{E}\\left[Y X^{\\prime}\\right] \\beta+\\beta^{\\prime} \\mathbb{E}\\left[X X^{\\prime}\\right] \\beta \\\\\n&=Q_{Y Y}-2 \\boldsymbol{Q}_{Y X} \\boldsymbol{Q}_{X X}^{-1} \\boldsymbol{Q}_{X Y}+\\boldsymbol{Q}_{Y X} \\boldsymbol{Q}_{X X}^{-1} \\boldsymbol{Q}_{X X} \\boldsymbol{Q}_{X X}^{-1} \\boldsymbol{Q}_{X Y} \\\\\n&=Q_{Y Y}-\\boldsymbol{Q}_{Y X} \\boldsymbol{Q}_{X X}^{-1} \\boldsymbol{Q}_{X Y} \\\\\n& \\stackrel{\\text { def }}{=} Q_{Y Y \\cdot X} .\n\\end{aligned}\n\\]\n这个公式的一个有用特征是它表明 \\(Q_{Y Y \\cdot X}=Q_{Y Y}-\\boldsymbol{Q}_{Y X} \\boldsymbol{Q}_{X X}^{-1} \\boldsymbol{Q}_{X Y}\\) 等于 \\(Y\\) 在 \\(X\\) 上的线性投影的误差方差。"
  },
  {
    "objectID": "chpt02-ce-chn.html#回归系数",
    "href": "chpt02-ce-chn.html#回归系数",
    "title": "条件预期和预测",
    "section": "回归系数",
    "text": "回归系数\n有时将常数与其他回归量分开并将线性投影方程写成格式是有用的\n\\[\nY=X^{\\prime} \\beta+\\alpha+e\n\\]\n其中 \\(\\alpha\\) 是截距，\\(X\\) 不包含常数。\n考虑这个方程的期望，我们发现\n\\[\n\\mathbb{E}[Y]=\\mathbb{E}\\left[X^{\\prime} \\beta\\right]+\\mathbb{E}[\\alpha]+\\mathbb{E}[e]\n\\]\n或 \\(\\mu_{Y}=\\mu_{X}^{\\prime} \\beta+\\alpha\\) 其中 \\(\\mu_{Y}=\\mathbb{E}[Y]\\) 和 \\(\\mu_{X}=\\mathbb{E}[X]\\)，因为 \\(\\mathbb{E}[e]=0\\) 来自 (2.28)。 （虽然 \\(X\\) 不包含常数，但方程（2.28）仍然适用。）重新排列，我们找到 \\(\\alpha=\\mu_{Y}-\\mu_{X}^{\\prime} \\beta\\)。从（2.37）中减去这个方程，我们发现\n\\[\nY-\\mu_{Y}=\\left(X-\\mu_{X}\\right)^{\\prime} \\beta+e,\n\\]\n中心变量 \\(Y-\\mu_{Y}\\) 和 \\(X-\\mu_{X}\\) 之间的线性方程。 （它们以均值为中心，均值为零的随机变量也是如此。）因为 \\(X-\\mu_{X}\\) 与 \\(e\\) 不相关，所以 (2.38) 也是一个线性投影。因此由线性投影模型的公式，\n\\[\n\\begin{aligned}\n\\beta &=\\left(\\mathbb{E}\\left[\\left(X-\\mu_{X}\\right)\\left(X-\\mu_{X}\\right)^{\\prime}\\right]\\right)^{-1} \\mathbb{E}\\left[\\left(X-\\mu_{X}\\right)\\left(Y-\\mu_{Y}\\right)\\right] \\\\\n&=\\operatorname{var}[X]^{-1} \\operatorname{cov}(X, Y)\n\\end{aligned}\n\\]\n仅是 \\(X\\) 和 \\(Y\\) 的协方差 \\({ }^{10}\\) 的函数。\n定理 2.10 在线性投影模型 \\(Y=X^{\\prime} \\beta+\\alpha+e\\) 中，\n\\[\n\\alpha=\\mu_{Y}-\\mu_{X}^{\\prime} \\beta\n\\]\n和\n\\[\n\\beta=\\operatorname{var}[X]^{-1} \\operatorname{cov}(X, Y) .\n\\]"
  },
  {
    "objectID": "chpt02-ce-chn.html#回归子向量",
    "href": "chpt02-ce-chn.html#回归子向量",
    "title": "条件预期和预测",
    "section": "回归子向量",
    "text": "回归子向量\n让回归量被划分为\n\\[\nX=\\left(\\begin{array}{l}\nX_{1} \\\\\nX_{2}\n\\end{array}\\right)\n\\]\n我们可以将 \\(Y\\) 在 \\(X\\) 上的投影写成\n\\[\n\\begin{aligned}\nY &=X^{\\prime} \\beta+e \\\\\n&=X_{1}^{\\prime} \\beta_{1}+X_{2}^{\\prime} \\beta_{2}+e \\\\\n\\mathbb{E}[X e] &=0 .\n\\end{aligned}\n\\]\n在本节中，我们推导出子向量 \\(\\beta_{1}\\) 和 \\(\\beta_{2}\\) 的公式。\n分区 \\(\\boldsymbol{Q}_{X X}\\) 与 \\(X\\) 一致\n\\[\n\\boldsymbol{Q}_{X X}=\\left[\\begin{array}{ll}\n\\boldsymbol{Q}_{11} & \\boldsymbol{Q}_{12} \\\\\n\\boldsymbol{Q}_{21} & \\boldsymbol{Q}_{22}\n\\end{array}\\right]=\\left[\\begin{array}{ll}\n\\mathbb{E}\\left[X_{1} X_{1}^{\\prime}\\right] & \\mathbb{E}\\left[X_{1} X_{2}^{\\prime}\\right] \\\\\n\\mathbb{E}\\left[X_{2} X_{1}^{\\prime}\\right] & \\mathbb{E}\\left[X_{2} X_{2}^{\\prime}\\right]\n\\end{array}\\right]\n\\]\n同样地\n\\[\n\\boldsymbol{Q}_{X Y}=\\left[\\begin{array}{l}\n\\boldsymbol{Q}_{1 Y} \\\\\n\\boldsymbol{Q}_{2 Y}\n\\end{array}\\right]=\\left[\\begin{array}{c}\n\\mathbb{E}\\left[X_{1} Y\\right] \\\\\n\\mathbb{E}\\left[X_{2} Y\\right]\n\\end{array}\\right] .\n\\]\n由分区矩阵求逆公式（A.3）\n\\[\n\\boldsymbol{Q}_{X X}^{-1}=\\left[\\begin{array}{ll}\n\\boldsymbol{Q}_{11} & \\boldsymbol{Q}_{12} \\\\\n\\boldsymbol{Q}_{21} & \\boldsymbol{Q}_{22}\n\\end{array}\\right]^{-1} \\stackrel{\\operatorname{def}}{=}\\left[\\begin{array}{ll}\n\\boldsymbol{Q}^{11} & \\boldsymbol{Q}^{12} \\\\\n\\boldsymbol{Q}^{21} & \\boldsymbol{Q}^{22}\n\\end{array}\\right]=\\left[\\begin{array}{cc}\n\\boldsymbol{Q}_{11 \\cdot 2}^{-1} & -\\boldsymbol{Q}_{11 \\cdot 2}^{-1} \\boldsymbol{Q}_{12} \\boldsymbol{Q}_{22}^{-1} \\\\\n-\\boldsymbol{Q}_{22 \\cdot 1}^{-1} \\boldsymbol{Q}_{21} \\boldsymbol{Q}_{11}^{-1} & \\boldsymbol{Q}_{22 \\cdot 1}^{-1}\n\\end{array}\\right]\n\\]\n\\({ }^{10}\\) 向量 \\(X\\) 和 \\(Z\\) 之间的协方差矩阵是 \\(\\operatorname{cov}(X, Z)=\\mathbb{E}\\left[(X-\\mathbb{E}[X])(Z-\\mathbb{E}[Z])^{\\prime}\\right]\\)。 \\(\\operatorname{vector} X\\) 的协方差矩阵是 \\(\\operatorname{var}[X]=\\operatorname{cov}(X, X)=\\mathbb{E}\\left[(X-\\mathbb{E}[X])(X-\\mathbb{E}[X])^{\\prime}\\right]\\)。其中 \\(\\boldsymbol{Q}_{11 \\cdot 2} \\stackrel{\\text { def }}{=} \\boldsymbol{Q}_{11}-\\boldsymbol{Q}_{12} \\boldsymbol{Q}_{22}^{-1} \\boldsymbol{Q}_{21}\\) 和 \\(\\boldsymbol{Q}_{22 \\cdot 1} \\stackrel{\\text { def }}{=} \\boldsymbol{Q}_{22}-\\boldsymbol{Q}_{21} \\boldsymbol{Q}_{11}^{-1} \\boldsymbol{Q}_{12}\\)。因此\n\\[\n\\begin{aligned}\n\\beta &=\\left(\\begin{array}{l}\n\\beta_{1} \\\\\n\\beta_{2}\n\\end{array}\\right) \\\\\n&=\\left[\\begin{array}{cc}\n\\boldsymbol{Q}_{11 \\cdot 2}^{-1} & -\\boldsymbol{Q}_{11 \\cdot 2}^{-1} \\boldsymbol{Q}_{12} \\boldsymbol{Q}_{22}^{-1} \\\\\n-\\boldsymbol{Q}_{22 \\cdot 1}^{-1} \\boldsymbol{Q}_{21} \\boldsymbol{Q}_{11}^{-1} & \\boldsymbol{Q}_{22 \\cdot 1}^{-1}\n\\end{array}\\right]\\left[\\begin{array}{l}\n\\boldsymbol{Q}_{1 Y} \\\\\n\\boldsymbol{Q}_{2 Y}\n\\end{array}\\right] \\\\\n&=\\left(\\begin{array}{c}\n\\boldsymbol{Q}_{11 \\cdot 2}^{-1}\\left(\\boldsymbol{Q}_{1 y}-\\boldsymbol{Q}_{12} \\boldsymbol{Q}_{22}^{-1} \\boldsymbol{Q}_{2 Y}\\right) \\\\\n\\boldsymbol{Q}_{22 \\cdot 1}^{-1}\\left(\\boldsymbol{Q}_{2 y}-\\boldsymbol{Q}_{21} \\boldsymbol{Q}_{11}^{-1} \\boldsymbol{Q}_{1 Y}\\right)\n\\end{array}\\right) \\\\\n&=\\left(\\begin{array}{c}\n\\boldsymbol{Q}_{11 \\cdot 2}^{-1} \\boldsymbol{Q}_{1 Y \\cdot 2} \\\\\n\\boldsymbol{Q}_{22 \\cdot 1}^{-1} \\boldsymbol{Q}_{2 Y \\cdot 1}\n\\end{array}\\right)\n\\end{aligned}\n\\]\n我们已经证明了 \\(\\beta_{1}=\\boldsymbol{Q}_{11 \\cdot 2}^{-1} \\boldsymbol{Q}_{1 Y \\cdot 2}\\) 和 \\(\\beta_{2}=\\boldsymbol{Q}_{22 \\cdot 1}^{-1} \\boldsymbol{Q}_{2 Y \\cdot 1}\\)。"
  },
  {
    "objectID": "chpt02-ce-chn.html#系数分解",
    "href": "chpt02-ce-chn.html#系数分解",
    "title": "条件预期和预测",
    "section": "系数分解",
    "text": "系数分解\n在上一节中，我们导出了系数子向量 \\(\\beta_{1}\\) 和 \\(\\beta_{2}\\) 的公式。我们现在使用这些公式根据迭代投影给出对系数的有用解释。\n对案例 \\(\\operatorname{dim}\\left(X_{1}\\right)=1\\) 取等式 (2.42)，使得 \\(\\beta_{1} \\in \\mathbb{R}\\)。\n\\[\nY=X_{1} \\beta_{1}+X_{2}^{\\prime} \\beta_{2}+e .\n\\]\n现在考虑 \\(X_{1}\\) 在 \\(X_{2}\\) 上的投影：\n\\[\n\\begin{aligned}\nX_{1} &=X_{2}^{\\prime} \\gamma_{2}+u_{1} \\\\\n\\mathbb{E}\\left[X_{2} u_{1}\\right] &=0 .\n\\end{aligned}\n\\]\n从 (2.22) 和 (2.36)，\\(\\gamma_{2}=\\boldsymbol{Q}_{22}^{-1} \\boldsymbol{Q}_{21}\\) 和 \\(\\mathbb{E}\\left[u_{1}^{2}\\right]=\\boldsymbol{Q}_{11 \\cdot 2}=\\boldsymbol{Q}_{11}-\\boldsymbol{Q}_{12} \\boldsymbol{Q}_{22}^{-1} \\boldsymbol{Q}_{21}\\)。我们也可以计算出\n\\[\n\\mathbb{E}\\left[u_{1} Y\\right]=\\mathbb{E}\\left[\\left(X_{1}-\\gamma_{2}^{\\prime} X_{2}\\right) Y\\right]=\\mathbb{E}\\left[X_{1} Y\\right]-\\gamma_{2}^{\\prime} \\mathbb{E}\\left[X_{2} Y\\right]=\\boldsymbol{Q}_{1 Y}-\\boldsymbol{Q}_{12} \\boldsymbol{Q}_{22}^{-1} \\boldsymbol{Q}_{2 Y}=\\boldsymbol{Q}_{1 Y \\cdot 2} .\n\\]\n我们发现\n\\[\n\\beta_{1}=\\boldsymbol{Q}_{11 \\cdot 2}^{-1} \\boldsymbol{Q}_{1 Y \\cdot 2}=\\frac{\\mathbb{E}\\left[u_{1} Y\\right]}{\\mathbb{E}\\left[u_{1}^{2}\\right]}\n\\]\n来自 \\(Y\\) 对 \\(u_{1}\\) 的简单回归的系数。\n这意味着在多元投影方程 (2.44) 中，系数 \\(\\beta_{1}\\) 等于 \\(Y\\) 在 \\(u_{1}\\) 上的回归的投影系数，\\(X_{1}\\) 在其他回归器 \\(NA\\)u_{1}$ 可以被认为是 \\(X_{1}\\) 的组成部分，其他回归量不能线性解释。因此，系数 \\(\\beta_{1}\\) 等于 \\(X_{1}\\) 在去除其他变量的影响后对 \\(Y\\) 的线性影响。\n变量 \\(X_{1}\\) 的选择没有什么特别之处。该推导对称地应用于线性投影中的所有系数。每个系数等于 \\(Y\\) 对来自该回归量在所有其他回归量上的投影的误差的简单回归。在线性控制所有其他回归变量之后，每个系数等于该变量对 \\(Y\\) 的线性影响。"
  },
  {
    "objectID": "chpt02-ce-chn.html#省略变量偏差",
    "href": "chpt02-ce-chn.html#省略变量偏差",
    "title": "条件预期和预测",
    "section": "省略变量偏差",
    "text": "省略变量偏差\n同样，让回归量按 (2.41) 进行划分。仅考虑 \\(Y\\) 在 \\(X_{1}\\) 上的投影。也许这样做是因为没有观察到变量 \\(X_{2}\\)。这是等式\n\\[\n\\begin{aligned}\nY &=X_{1}^{\\prime} \\gamma_{1}+u \\\\\n\\mathbb{E}\\left[X_{1} u\\right] &=0 .\n\\end{aligned}\n\\]\n请注意，我们将系数写成 \\(\\gamma_{1}\\) 而不是 \\(\\beta_{1}\\)，误差写成 \\(u\\) 而不是 \\(e\\)。这是因为 (2.45) 与 (2.42) 不同。 Goldberger (1991) 为 (2.42) 引入了长回归和 (2.45) 的短回归以强调区别。\n通常为 \\(\\beta_{1} \\neq \\gamma_{1}\\)，特殊情况除外。为了看到这一点，我们计算\n\\[\n\\begin{aligned}\n\\gamma_{1} &=\\left(\\mathbb{E}\\left[X_{1} X_{1}^{\\prime}\\right]\\right)^{-1} \\mathbb{E}\\left[X_{1} Y\\right] \\\\\n&=\\left(\\mathbb{E}\\left[X_{1} X_{1}^{\\prime}\\right]\\right)^{-1} \\mathbb{E}\\left[X_{1}\\left(X_{1}^{\\prime} \\beta_{1}+X_{2}^{\\prime} \\beta_{2}+e\\right)\\right] \\\\\n&=\\beta_{1}+\\left(\\mathbb{E}\\left[X_{1} X_{1}^{\\prime}\\right]\\right)^{-1} \\mathbb{E}\\left[X_{1} X_{2}^{\\prime}\\right] \\beta_{2} \\\\\n&=\\beta_{1}+\\Gamma_{12} \\beta_{2}\n\\end{aligned}\n\\]\n其中 \\(\\Gamma_{12}=\\boldsymbol{Q}_{11}^{-1} \\boldsymbol{Q}_{12}\\) 是 \\(X_{2}\\) 在 \\(X_{1}\\) 上的投影的系数矩阵，我们使用第 \\(2.22\\) 中的符号。\n观察 \\(\\gamma_{1}=\\beta_{1}+\\Gamma_{12} \\beta_{2} \\neq \\beta_{1}\\)，除非 \\(\\Gamma_{12}=0\\) 或 \\(\\beta_{2}=0\\)。因此，短期和长期回归具有不同的系数。它们仅在两种条件之一下是相同的。首先，如果 \\(X_{2}\\) 在 \\(X_{1}\\) 上的投影产生一组零系数（它们不相关），或者其次，如果 (2.42) 中 \\(X_{2}\\) 上的系数为零。 \\(\\gamma_{1}\\) 和 \\(\\beta_{1}\\) 之间的差异 \\(\\Gamma_{12} \\beta_{2}\\) 称为遗漏变量偏差。这是遗漏相关相关变量的结果。\n为避免遗漏变量偏差，标准建议是在估计模型中包含所有可能相关的变量。通过构造，一般模型将没有这种偏差。不幸的是，在许多情况下，完全遵循此建议是不可行的，因为没有观察到许多所需的变量。在这种情况下，应在实证调查过程中承认和讨论遗漏变量偏差的可能性。\n例如，假设 \\(Y\\) 是对数工资，\\(X_{1}\\) 是教育，\\(X_{2}\\) 是智力。假设教育和智力能力呈正相关（能力强的人获得更高水平的教育）似乎是合理的，这意味着 \\(\\Gamma_{12}>0\\)。假设以教育为条件，智力较高的人平均会获得较高的工资，这似乎也是合理的，因此 \\(\\beta_{2}>0\\)。这意味着 \\(\\Gamma_{12} \\beta_{2}>0\\) 和 \\(\\gamma_{1}=\\beta_{1}+\\Gamma_{12} \\beta_{2}>\\beta_{1}\\)。因此，似乎可以合理地预期，在忽略智力的工资对教育的回归中（因为后者没有被测量），教育的系数高于在包括智力的回归中。换句话说，在这种情况下，被遗漏的变量使回归系数向上偏移。例如，有可能 \\(\\beta_{1}=0\\) 使得教育对工资没有直接影响，但 \\(\\gamma_{1}=\\Gamma_{12} \\beta_{2}>0\\) 意味着仅教育的回归系数是正的，但它是教育和智力之间未建模相关性的结果。\n不幸的是，正如 Luca、Magnus 和 Peracchi (2018) 所发现的，上述对遗漏变量偏差的简单表征并没有立即延续到更复杂的设置。例如，假设我们比较三个嵌套投影\n\\[\n\\begin{aligned}\n&Y=X_{1}^{\\prime} \\gamma_{1}+u_{1} \\\\\n&Y=X_{1}^{\\prime} \\delta_{1}+X_{2}^{\\prime} \\delta_{2}+u_{2} \\\\\n&Y=X_{1}^{\\prime} \\beta_{1}+X_{2}^{\\prime} \\beta_{2}+X_{3}^{\\prime} \\beta_{3}+e .\n\\end{aligned}\n\\]\n我们可以称它们为短期、中期和长期回归。假设在长回归中感兴趣的参数是 \\(\\beta_{1}\\)。我们对在估计中回归时省略 \\(X_{3}\\) 以及在估计短回归时同时省略 \\(X_{2}\\) 和 \\(X_{3}\\) 的后果感兴趣。特别是我们对这个问题感兴趣：估计短期回归还是中期回归更好，因为两者都省略了 \\(X_{3}\\) ？直觉表明，中等回归应该“偏差较小”，但值得更详细地研究。通过与上述类似的计算，我们发现\n\\[\n\\begin{aligned}\n&\\gamma_{1}=\\beta_{1}+\\Gamma_{12} \\beta_{2}+\\Gamma_{13} \\beta_{3} \\\\\n&\\delta_{1}=\\beta_{1}+\\Gamma_{13 \\cdot 2} \\beta_{3}\n\\end{aligned}\n\\]\n其中 \\(\\Gamma_{13 \\cdot 2}=\\boldsymbol{Q}_{11 \\cdot 2}^{-1} \\boldsymbol{Q}_{13 \\cdot 2}\\) 使用 \\(2.22\\) 节中的符号。\n我们看到，短回归系数的偏差是 \\(\\Gamma_{12} \\beta_{2}+\\Gamma_{13} \\beta_{3}\\)，它同时依赖于 \\(\\beta_{2}\\) 和 \\(\\beta_{3}\\)，而中等回归系数的偏差是 \\(\\Gamma_{13 \\cdot 2} \\beta_{3}\\)，它只依赖于 \\(\\beta_{3}\\)。因此，中回归的偏差不那么复杂，直观上似乎比短回归的偏差更小。但是，不可能对两者进行严格的排名。 \\(\\gamma_{1}\\) 的偏差很可能比 \\(\\delta_{1}\\) 小。因此，作为一般规则，中回归的估计是否会比短回归的估计更小偏差是未知的。"
  },
  {
    "objectID": "chpt02-ce-chn.html#最佳线性逼近",
    "href": "chpt02-ce-chn.html#最佳线性逼近",
    "title": "条件预期和预测",
    "section": "最佳线性逼近",
    "text": "最佳线性逼近\n有其他方法可以构建条件期望 \\(m(X)\\) 的线性逼近 \\(X^{\\prime} \\beta\\)。在本节中，我们展示了一种替代方法可以产生与最佳线性预测器相同的答案。\n我们首先将 \\(X^{\\prime} \\beta\\) 到 \\(m(X)\\) 的均方逼近误差定义为 \\(X^{\\prime} \\beta\\) 和条件期望 \\(m(X)\\) 之间的期望平方差\n\\[\nd(\\beta)=\\mathbb{E}\\left[\\left(m(X)-X^{\\prime} \\beta\\right)^{2}\\right] .\n\\]\n函数 \\(d(\\beta)\\) 是对 \\(X^{\\prime} \\beta\\) 与 \\(m(X)\\) 偏差的度量。如果两个函数相同，则为 \\(d(\\beta)=0\\)，否则为 \\(d(\\beta)>0\\)。我们还可以将均方差 \\(d(\\beta)\\) 视为函数 \\(\\left(m(X)-X^{\\prime} \\beta\\right)^{2}\\) 的密度加权平均值，因为\n\\[\nd(\\beta)=\\int_{\\mathbb{R}^{k}}\\left(m(x)-x^{\\prime} \\beta\\right)^{2} f_{X}(x) d x\n\\]\n其中 \\(f_{X}(x)\\) 是 \\(X\\) 的边际密度。\n然后，我们可以将条件 \\(m(X)\\) 的最佳线性近似定义为通过选择 \\(\\beta\\) 以最小化 \\(d(\\beta)\\) 获得的函数 \\(X^{\\prime} \\beta\\)：\n\\[\n\\beta=\\underset{b \\in \\mathbb{R}^{k}}{\\operatorname{argmin}} d(b) .\n\\]\n与最佳线性预测器类似，我们通过预期平方误差来测量准确性。不同之处在于最佳线性预测器 (2.18) 选择 \\(\\beta\\) 以最小化预期平方预测误差，而最佳线性近似 (2.46) 选择 \\(\\beta\\) 以最小化预期平方逼近误差。\n尽管定义不同，但事实证明最佳线性预测器和最佳线性近似值是相同的。通过与（2.18）中相同的步骤加上条件期望的应用，我们可以发现\n\\[\n\\begin{aligned}\n\\beta &=\\left(\\mathbb{E}\\left[X X^{\\prime}\\right]\\right)^{-1} \\mathbb{E}[X m(X)] \\\\\n&=\\left(\\mathbb{E}\\left[X X^{\\prime}\\right]\\right)^{-1} \\mathbb{E}[X Y]\n\\end{aligned}\n\\]\n（见习题 2.19）。因此 (2.46) 等于 (2.18)。我们得出结论，定义（2.46）可以被视为线性投影系数的替代动机。"
  },
  {
    "objectID": "chpt02-ce-chn.html#回归均值",
    "href": "chpt02-ce-chn.html#回归均值",
    "title": "条件预期和预测",
    "section": "回归均值",
    "text": "回归均值\n回归一词起源于弗朗西斯·高尔顿（Francis Galton）（1886 年）的一篇有影响力的论文，他在该论文中研究了父母和孩子的身高（身高）的联合分布。实际上，他是在根据父母的身高估计孩子身高的条件期望。高尔顿发现这个条件期望与 \\(2 / 3\\) 的斜率近似线性。这意味着平均而言，孩子的身高比他或她父母的身高更平庸（平均）。高尔顿将这种现象称为均值回归，而标签回归至今仍用于描述大多数条件关系。\n高尔顿的基本见解之一是认识到如果 \\(Y\\) 和 \\(X\\) 的边际分布相同（例如，在稳定环境中孩子和父母的身高），那么线性投影中的回归斜率总是小于 1 .\n更准确地说，采用简单的线性投影\n\\[\nY=X \\beta+\\alpha+e\n\\]\n其中 \\(Y\\) 等于孩子的身高，\\(X\\) 等于父母的身高。假设 \\(Y\\) 和 \\(X\\) 具有相同的期望，因此 \\(\\mu_{Y}=\\mu_{X}=\\mu\\)。然后从 (2.39) \\(\\alpha=(1-\\beta) \\mu\\) 所以我们可以将线性投影 (2.49) 写为\n\\[\n\\mathscr{P}(Y \\mid X)=(1-\\beta) \\mu+X \\beta .\n\\]\n这表明孩子的预计身高是人口期望 \\(\\mu\\) 和父母身高 \\(X\\) 的加权平均值，权重为 \\(\\beta\\) 和 \\(1-\\beta\\)。当高度分布在各代之间是稳定的使得 \\(\\operatorname{var}[Y]=\\operatorname{var}[X]\\)，那么这个斜率就是 \\(Y\\) 和 \\(X\\) 的简单相关。使用 (2.40)\n\\[\n\\beta=\\frac{\\operatorname{cov}(X, Y)}{\\operatorname{var}[X]}=\\operatorname{corr}(X, Y) .\n\\]\n根据 Cauchy-Schwarz 不等式 (B.32)，\\(-1 \\leq \\operatorname{corr}(X, Y) \\leq 1\\)，\\(\\operatorname{corr}(X, Y)=1\\) 仅在退化情况 \\(Y=X\\) 中。因此，如果我们排除退化，\\(\\beta\\) 严格小于 1。\n这意味着平均而言，孩子的身高比父母的身高更平庸（接近人口平均水平）。\n一个常见的错误 - 称为回归谬误 - 是从 \\(\\beta<1\\) 推断总体正在收敛，这意味着其方差正在向零下降。这是一个谬误，因为我们在均值和方差不变的假设下推导出了蕴涵 \\(\\beta<1\\)。所以当然 \\(\\beta<1\\) 并不意味着方差 \\(Y\\) 小于 \\(X\\) 的方差。\n另一种看待这一点的方法是在方程（2.49）的上下文中检查收敛条件。由于 \\(X\\) 和 \\(e\\) 不相关，因此得出\n\\[\n\\operatorname{var}[Y]=\\beta^{2} \\operatorname{var}[X]+\\operatorname{var}[e] .\n\\]\n那么 \\(\\operatorname{var}[Y]<\\operatorname{var}[X]\\) 当且仅当\n\\[\n\\beta^{2}<1-\\frac{\\operatorname{var}[e]}{\\operatorname{var}[X]}\n\\]\n简单条件 \\(|\\beta|<1\\) 并未暗示这一点。\n回归谬误出现在相关的经验情况下。假设您按父母的身高将家庭分组，然后绘制随时间变化的每一代的平均身高。如果人口是稳定的，回归属性意味着地块线将收敛 - 孩子的身高将比他们的父母更平均。回归谬误是错误地得出人口正在收敛的结论。从这个例子中学到的一个信息是，这样的图会误导关于收敛的推断。回归谬误是微妙的。聪明的经济学家很容易屈服于它的诱惑。一个著名的例子是 Horace Secrist 于 1933 年出版的《商业中的平庸的胜利》。在这本书中，Secrist 仔细而详尽地记录了在 19201930 年以上的百货公司样本中，他根据 1920-1921 年将商店分组利润，并绘制了这些群体随后 10 年的平均利润，他发现了清晰而有说服力的证据表明趋同于“趋于平庸”。当然，没有发现——回归均值是稳定分布的必要特征。"
  },
  {
    "objectID": "chpt02-ce-chn.html#反向回归",
    "href": "chpt02-ce-chn.html#反向回归",
    "title": "条件预期和预测",
    "section": "反向回归",
    "text": "反向回归\n高尔顿注意到二元分布的另一个有趣特征。 \\(Y\\) 在 \\(X\\) 上的回归没有什么特别之处。我们还可以在 \\(Y\\) 上回归 \\(X\\)。 （在他的遗传例子中，这是给定孩子身高的父母身高的最佳线性预测因子。）这种回归采用以下形式\n\\[\nX=Y \\beta^{*}+\\alpha^{*}+e^{*} .\n\\]\n这有时被称为反向回归。在这个等式中，系数 \\(\\alpha^{*}, \\beta^{*}\\) 和误差 \\(e^{*}\\) 由线性投影定义。在一个稳定的人群中，我们发现\n\\[\n\\begin{gathered}\n\\beta^{*}=\\operatorname{corr}(X, Y)=\\beta \\\\\n\\alpha^{*}=(1-\\beta) \\mu=\\alpha\n\\end{gathered}\n\\]\n这与 \\(Y\\) 在 \\(X\\) 上的投影完全相同！截距和斜率在正向和反向投影中具有完全相同的值！ [这种平等并不是特别重要；这是假设 \\(X\\) 和 \\(Y\\) 具有相同方差的产物。]\n虽然这个代数发现非常简单，但它是违反直觉的。相反，对逆回归形式的一个常见但错误的猜测是取方程 (2.49)，除以 \\(\\beta\\) 并重写以找到方程\n\\[\nX=Y \\frac{1}{\\beta}-\\frac{\\alpha}{\\beta}-\\frac{1}{\\beta} e\n\\]\n表明 \\(X\\) 在 \\(Y\\) 上的投影应该有 \\(1 / \\beta\\) 而不是 \\(\\beta\\) 的斜率系数，并且截距是 \\(-\\alpha / \\beta\\) 而不是 \\(\\alpha\\)。什么地方出了错？方程（2.51）是完全有效的，因为它是对有效方程（2.49）的简单操作。问题是 (2.51) 既不是 CEF 也不是线性投影。反转投影（或 CEF）不会产生投影（或 CEF）。相反，(2.50) 是有效的投影，而不是 (2.51)。\n无论如何，Galton 的发现是，当变量标准化时，两个投影（\\(X\\) 上的 \\(Y\\) 和 \\(Y\\) 上的 \\(X\\)）的斜率等于相关性，并且两个方程都显示出均值回归。它不是因果关系，而是联合分布的自然特征。"
  },
  {
    "objectID": "chpt02-ce-chn.html#最佳线性投影的局限性",
    "href": "chpt02-ce-chn.html#最佳线性投影的局限性",
    "title": "条件预期和预测",
    "section": "最佳线性投影的局限性",
    "text": "最佳线性投影的局限性\n让我们比较一下线性投影和线性 CEF 模型。\n从定理 2.4.4 我们知道 CEF 错误具有属性 \\(\\mathbb{E}[X e]=0\\)。因此线性 CEF 是最好的线性投影。然而，反之则不成立，因为投影误差不一定满足 \\(\\mathbb{E}[e \\mid X]=0\\)。此外，线性投影可能不是 CEF 的近似值。\n为了在一个简单的例子中看到这些点，假设真正的过程是 \\(Y=X+X^{2}\\) 和 \\(X \\sim \\mathrm{N}(0,1)\\)。在这种情况下，真正的 CEF 是 \\(m(x)=x+x^{2}\\) 并且没有错误。现在考虑 \\(Y\\) 在 \\(X\\) 上的线性投影和一个常数，即模型 \\(Y=\\beta X+\\alpha+e\\)。由于 \\(X \\sim \\mathrm{N}(0,1)\\) 那么 \\(X\\) 和 \\(X^{2}\\) 是不相关的，并且线性投影采用 \\(\\mathscr{P}[Y \\mid X]=X+1\\) 的形式。这与真正的 CEF \\(m(X)=\\) \\(X+X^{2}\\) 完全不同。投影误差等于 \\(e=X^{2}-1\\)，它是 \\(X\\) 的确定性函数，但与 \\(X\\) 不相关。我们在这个例子中看到，投影误差不一定是 CEF 误差，线性投影可能不是 CEF 的近似值。\n\n图 2.7：条件期望和两个线性预测\n线性投影的另一个缺陷是当条件均值非线性时，它对回归量的边际分布很敏感。我们在图 \\(2.7\\) 中说明了 \\(Y\\) 和 \\(X\\) 的构造 \\({ }^{11}\\) 联合分布的问题。粗线是给定 \\(X\\) 的 \\(Y\\) 的非线性 CEF。数据分为两组 - 第 1 组和第 2 组 - 回归量 \\(X\\) 的边际分布不同，第 1 组的 \\(X\\) 平均值低于第 2 组。\\(Y\\) 的单独线性投影这两组的 \\(X\\) 在图中用细线显示。这两个预测是 CEF 的不同近似值。线性投影的一个缺陷是它会导致错误的结论，即 \\(X\\) 对 \\(Y\\) 的影响对于两组中的个人来说是不同的。这个结论是不正确的，因为实际上条件期望函数没有区别。明显的差异是非线性期望的线性近似与条件变量的不同边际分布相结合的副产品。\n\\({ }^{11}\\) 第 1 组中的 \\(X\\) 是 \\(\\mathrm{N}(2,1)\\)，第 2 组中的是 \\(\\mathrm{N}(4,1)\\)，\\(Y\\) 给定 \\(X\\) 的条件分布是 \\(\\mathrm{N}(m(X), 1)\\)，其中 \\(m(x)=2 x-x^{2} / 6\\)。这些函数绘制在 \\(0 \\leq x \\leq 6\\) 上。"
  },
  {
    "objectID": "chpt02-ce-chn.html#随机系数模型",
    "href": "chpt02-ce-chn.html#随机系数模型",
    "title": "条件预期和预测",
    "section": "随机系数模型",
    "text": "随机系数模型\n一个在符号上与线性 CEF 模型相似但在概念上不同的模型是线性随机系数模型。它采用 \\(Y=X^{\\prime} \\eta\\) 的形式，其中个体特定系数 \\(\\eta\\) 是随机的并且独立于 \\(X\\)。例如，如果 \\(X\\) 是受教育年限，\\(Y\\) 是对数工资，则 \\(\\eta\\) 是个人特定的受教育回报。如果一个人获得额外一年的教育，\\(\\eta\\) 是他们工资的实际变化。随机系数模型允许受教育的回报因人口而异。有些人的教育回报率可能很高（高 \\(\\eta\\) ），而另一些人的回报率低，可能是 0 ，甚至是负数。\n在线性 CEF 模型中，回归系数等于回归导数 - 由于回归变量 \\(\\beta=\\nabla m(X)\\) 的变化而导致的条件期望的变化。这不是对特定个体的影响，而是对总体平均数的影响。相反，在随机系数模型中，随机向量 \\(\\eta=\\nabla\\left(X^{\\prime} \\eta\\right)\\) 是真正的因果效应——响应变量 \\(Y\\) 本身由于回归量的变化而发生的变化。\n然而，有趣的是发现线性随机系数模型意味着线性 CEF。要看到这一点，让 \\(\\beta=\\mathbb{E}[\\eta]\\) 和 \\(\\Sigma=\\operatorname{var}[\\eta]\\) 表示 \\(\\eta\\) 的均值和协方差矩阵，然后将随机系数分解为 \\(\\eta=\\beta+u\\)，其中 \\(u\\) 独立于 \\(X\\) 分布，均值为零，协方差矩阵 \\(数学7\\)。然后我们可以写\n\\[\n\\mathbb{E}[Y \\mid X]=X^{\\prime} \\mathbb{E}[\\eta \\mid X]=X^{\\prime} \\mathbb{E}[\\eta]=X^{\\prime} \\beta\n\\]\n所以 CEF 在 \\(X\\) 中是线性的，系数 \\(\\beta\\) 等于随机系数 \\(\\eta\\) 的期望值。\n因此，我们可以将方程写为线性 CEF \\(Y=X^{\\prime} \\beta+e\\)，其中 \\(e=X^{\\prime} u\\) 和 \\(u=\\eta-\\beta\\)。错误在条件下为零：\\(\\mathbb{E}[e \\mid X]=0\\)。此外\n\\[\n\\operatorname{var}[e \\mid X]=X^{\\prime} \\operatorname{var}[\\eta] X=X^{\\prime} \\Sigma X\n\\]\n所以误差是条件异方差的，其方差是 \\(X\\) 的二次函数。\n定理 2.11 在线性随机系数模型 \\(Y=X^{\\prime} \\eta\\) 中，\\(\\eta\\) 独立于 \\(X, \\mathbb{E}\\|X\\|^{2}<\\infty\\) 和 \\(\\mathbb{E}\\|\\eta\\|^{2}<\\infty\\)，则\n\\[\n\\begin{aligned}\n\\mathbb{E}[Y \\mid X] &=X^{\\prime} \\beta \\\\\n\\operatorname{var}[Y \\mid X] &=X^{\\prime} \\Sigma X\n\\end{aligned}\n\\]\n其中 \\(\\beta=\\mathbb{E}[\\eta]\\) 和 \\(\\Sigma=\\operatorname{var}[\\eta]\\)"
  },
  {
    "objectID": "chpt02-ce-chn.html#因果效应",
    "href": "chpt02-ce-chn.html#因果效应",
    "title": "条件预期和预测",
    "section": "因果效应",
    "text": "因果效应\n到目前为止，我们已经避免了因果关系的概念，但计量经济分析的基本目标通常是衡量变量之间的因果关系。了解决策、行动和政策的原因和影响通常具有极大的兴趣。例如，我们可能对班级规模对考试成绩的影响、警察支出对犯罪率的影响、气候变化对经济活动的影响、受教育年限对工资的影响、制度结构对增长的影响、奖励对行为的有效性、健康结果的医疗程序，或任何各种可能的因果关系。在每种情况下，目标都是了解输入变化对结果的实际影响是什么。我们不仅对条件期望或线性投影感兴趣，我们还想知道实际的变化。\n两个固有的障碍是：（1）因果效应通常是特定于个人的； (2) 因果效应通常是无法观察到的。\n考虑学校教育对工资的影响。因果效应是如果我们可以在其他所有条件不变的情况下改变他们的教育水平，一个人将获得的实际工资差异。这是针对每个人的，因为他们在这两种不同情况下的就业结果是个人的。因果效应是未观察到的，因为我们能观察到的最多的是他们的实际教育水平和实际工资，但如果他们的教育不同，则不是反事实工资。\n具体假设有两个人，詹妮弗和乔治，他们都有可能成为高中毕业生或大学毕业生，并且根据他们的选择，他们都会得到不同的工资。例如，假设 Jennifer 大学毕业生每小时可赚取 \\(\\$ 10\\) an hour as a high-school graduate and \\(\\$ 20\\)，而 George 作为大学毕业生每小时可赚取 \\(\\$ 8\\) as a high-school graduate and \\(\\$ 12\\)。在这个例子中，对于乔治来说，学校教育的因果效应是每小时 \\(\\$ 10\\) a hour for Jennifer and \\(\\$ 4\\)。因果效应是特定于个体的，没有观察到因果效应。\nRubin (1974) 开发了潜在结果框架（也称为 Rubin 因果模型）来阐明这些问题。令 \\(Y\\) 为标量结果（例如，工资），\\(D\\) 为二元处理（例如，大学入学率）。作为二进制处理的规范不是必需的，但简化了符号。描述治疗对结果影响的灵活模型是\n\\[\nY=h(D, U)\n\\]\n其中 \\(U\\) 是 \\(\\ell \\times 1\\) 未观察到的随机因子，\\(h\\) 是函数关系。分别使用简化符号 \\(Y(0)=h(0, U)\\) 和 \\(Y(1)=h(1, U)\\) 来表示与非治疗和治疗相关的潜在结果也很常见。该符号隐含地保持 \\(U\\) 固定。潜在结果因每个人而异，因为它们取决于 \\(U\\)。例如，如果 \\(Y\\) 是个人的工资，则不可观察的 \\(U\\) 可能包括个人的能力、技能、职业道德、人际关系和偏好等特征，所有这些都可能影响他们的工资。在我们的示例中，这些因素通过标签“Jennifer”和“George”进行了总结。\n当我们改变 \\(D\\) 而保持 \\(U\\) 不变时，鲁宾将这种影响描述为因果关系。在我们的示例中，这意味着在保持其他属性不变的同时改变个人的教育。\n定义 2.6 在模型 (2.52) 中，\\(D\\) 对 \\(Y\\) 的因果效应是\n\\[\nC(U)=Y(1)-Y(0)=h(1, U)-h(0, U),\n\\]\n在保持 \\(U\\) 不变的情况下，由于治疗引起的 \\(Y\\) 的变化。\n理解 (2.53) 是一个定义，并不一定描述基本或实验意义上的因果关系，这可能会有所帮助。也许将（2.53）标记为结构效应（结构模型内的效应）会更合适。\n(2.53) 中定义的治疗 \\(C(U)\\) 的因果效应是异质和随机的，因为潜在结果 \\(Y(0)\\) 和 \\(Y(1)\\) 因个体而异。此外，对于给定的个人，我们不会同时观察 \\(Y(0)\\) 和 \\(Y(1)\\)，而只观察到已实现的价值\n\\[\nY=\\left\\{\\begin{array}{lll}\nY(0) & \\text { if } & D=0 \\\\\nY(1) & \\text { if } & D=1 .\n\\end{array}\\right.\n\\]\n表 2.3：示例分布\n|College Graduate|0|0|6|10|\\(\\$ 17.00\\)| |:—————|:|:|:|-:|———:| |Difference | | | | | \\(\\$ 8.25\\)|\n因此，没有观察到因果效应 \\(C(U)\\)。\nRubin 的目标是学习 \\(C(U)\\) 的分布特征，包括其预期值，他称之为平均因果效应。他定义如下。\n定义 2.7 在模型 (2.52) 中，\\(D\\) 对 \\(Y\\) 的平均因果效应为\n\\[\n\\mathrm{ACE}=\\mathbb{E}[C(U)]=\\int_{\\mathbb{R}^{\\ell}} C(u) f(u) d u\n\\]\n其中 \\(f(u)\\) 是 \\(U\\) 的密度。\nACE 是因果效应的总体平均值。扩展我们的 Jennifer&George 示例，假设一半的人口像 Jennifer，而另一半像 George。那么大学对工资的平均因果效应是 \\((10+4) / 2=\\$ 7\\) an hour.\n估计 ACE 的一个合理起点是比较治疗和未治疗个体的平均 \\(Y\\)。在我们的示例中，这是大学毕业生和高中毕业生的平均工资之间的差异。这与结果 \\(Y\\) 对治疗 \\(D\\) 的回归系数相同。这等于 ACE 吗？\n答案取决于处理 \\(D\\) 和未观察到的分量 \\(U\\) 之间的关系。如果 \\(D\\) 在实验中是随机分配的，那么 \\(D\\) 和 \\(U\\) 是独立的并且回归系数等于 ACE。但是，如果 \\(D\\) 和 \\(U\\) 是相关的，那么回归系数和 ACE 是不同的。为了看到这一点，观察治疗和未治疗人群的平均结果之间的差异是\n\\[\n\\mathbb{E}[Y \\mid D=1]-\\mathbb{E}[Y \\mid D=0]=\\int_{\\mathbb{R}^{\\ell}} h(1, u) f(u \\mid D=1) d u-\\int_{\\mathbb{R}^{\\ell}} h(1, u) f(u \\mid D=0) d u\n\\]\n其中 \\(f(u \\mid D)\\) 是给定 \\(D\\) 的 \\(U\\) 的条件密度。如果 \\(U\\) 独立于 \\(D\\) 则 \\(f(u \\mid D)=f(u)\\) 和上面的表达式等于 \\(\\int_{\\mathbb{R}^{\\ell}}(h(1, u)-h(0, u)) f(u) d u=\\) ACE。但是，如果 \\(U\\) 和 \\(D\\) 是相关的，则此等式将失败。\n为了说明，让我们回到詹妮弗和乔治的例子。假设所有高中生都参加了能力倾向测试。如果学生获得高 \\((\\mathrm{H})\\) 分数，他们以 \\(3 / 4\\) 的概率上大学，如果学生获得低 (L) 分，他们以 \\(1 / 4\\) 的概率上大学。进一步假设 Jennifer 以 3/4 的概率获得 \\(\\mathrm{H}\\) 的能力得分，而 George 以 \\(1 / 4\\) 的概率获得 \\(\\mathrm{H}\\) 的得分。在这种情况下，Jennifer 的 \\(62.5 %\\) 将上大学 \\({ }^{12}\\)，而 George 的 \\(37.5 %\\) 将上大学 \\({ }^{13}\\)。\n一位计量经济学家随机抽样 32 个人并收集有关教育程度和工资的数据，会发现表 2.3 中显示的工资分布。\n\\(12 \\mathbb{P}[\\) 大学 \\(\\mid\\) 詹妮弗 \\(]=\\mathbb{P}[\\) 大学 \\(\\mid H] \\mathbb{P}[H \\mid\\) 詹妮弗 \\(]+\\mathbb{P}[\\) 大学 \\(\\mid L] \\mathbb{P}[L \\mid\\) 詹妮弗 \\(]=(3 / 4)^{2}+(1 / 4)^{2} .\\)\n\\(13 \\mathbb{P}[\\) 大学 \\(\\mid\\) 乔治 \\(]=\\mathbb{P}[\\) 大学 \\(\\mid H] \\mathbb{P}[H \\mid\\) 乔治 \\(]+\\mathbb{P}[\\) 大学 \\(\\mid L] \\mathbb{P}[L \\mid\\) 乔治 \\(]=(3 / 4)(1 / 4)+(1 / 4)(3 / 4)\\).我们的计量经济学家发现高中毕业生的平均工资是 \\(\\$ 8.75\\) while the average wage among college graduates is \\(\\$ 17.00\\)。 \\(\\$ 8.25\\) is the econometrician’s regression coefficient for the effect of college on wages. But \\(\\$ 8.25\\) 的差异夸大了 \\(\\$ 7\\)NA\n要可视化表 \\(2.3\\)，请查看图 2.8。这四个点是表中的四个教育/工资对，点的大小根据工资分布进行了校准。这两条线是计量经济学家的回归线和平均因果效应。人口中的 Jennifer’s 对应于两条线以上的点，人口中的 George’s 对应于两条线以下的点。因为大多数 Jennifer 上过大学，而大多数 George 没有上过大学，所以回归线从平均因果效应向两个大点倾斜。\n\n图 2.8：平均因果效应与回归\n我们从这个分析中得到的第一个教训是，我们需要谨慎地将回归系数解释为因果效应。除非回归量（例如教育程度）可以解释为随机分配，否则不适合因果解释回归系数。\n我们的第二课是，如果我们以一组足够丰富的协变量为条件，就可以获得因果解释。我们现在探讨这个问题。\n假设除了结果 \\(Y\\) 和处理 \\(D\\) 之外，可观察量还包括一组协变量 \\(X\\)。我们扩展潜在结果模型 (2.52) 以包括 \\(X\\) ：\n\\[\nY=h(D, X, U) .\n\\]\n我们还扩展了因果效应的定义，以允许对 \\(X\\) 进行调节。\n定义 \\(2.8\\) 在模型 (2.54) 中，\\(D\\) 对 \\(Y\\) 的因果效应是\n\\[\nC(X, U)=h(1, X, U)-h(0, X, U),\n\\]\n由于保持 \\(X\\) 和 \\(U\\) 不变的治疗，\\(Y\\) 的变化。\n以 \\(X=x\\) 为条件的 \\(D\\) 对 \\(Y\\) 的条件平均因果效应是\n\\[\n\\operatorname{ACE}(x)=\\mathbb{E}[C(X, U) \\mid X=x]=\\int_{\\mathbb{R}^{\\ell}} C(x, u) f(u \\mid x) d u\n\\]\n其中 \\(f(u \\mid x)\\) 是给定 \\(X\\) 的 \\(U\\) 的条件密度。\n\\(D\\) 对 \\(Y\\) 的无条件平均因果效应为\n\\[\n\\mathrm{ACE}=\\mathbb{E}[C(X, U)]=\\int \\operatorname{ACE}(x) f(x) d x\n\\]\n其中 \\(f(x)\\) 是 \\(X\\) 的密度。\n条件平均因果效应 \\(\\operatorname{ACE}(x)\\) 是具有特征 \\(X=x\\) 的子群体的 ACE。给定对 \\((Y, D, X)\\) 的观察，我们想测量 \\(D\\) 对 \\(Y\\) 的因果影响，并且感兴趣是否可以通过 \\(Y\\) 对 \\((D, X)\\) 的回归来获得。我们想将 \\(D\\) 上的系数解释为因果效应。这合适吗？\n我们之前的分析表明，当 \\(U\\) 独立于回归变量时，可以获得因果解释。虽然这已经足够了，但它比必要的要强。相反，以下内容就足够了。\n定义 2.9 条件独立假设 (CIA)。以 \\(X\\) 为条件，随机变量 \\(D\\) 和 \\(U\\) 在统计上是独立的。\nCIA 暗示给定 \\((D, X)\\) 的 \\(U\\) 的条件密度仅取决于 \\(X\\)，因此 \\(f(u \\mid D, X)=\\) \\(f(u \\mid X)\\)。这意味着 \\(Y\\) 对 \\((D, X)\\) 的回归等于\n\\[\n\\begin{aligned}\nm(d, x) &=\\mathbb{E}[Y \\mid D=d, X=x] \\\\\n&=\\mathbb{E}[h(d, x, U) \\mid D=d, X=x] \\\\\n&=\\int h(d, x, u) f(u \\mid x) d u .\n\\end{aligned}\n\\]\n在 CIA 下，回归测量的治疗效果是\n\\[\n\\begin{aligned}\n\\nabla m(d, x) &=m(1, x)-m(0, x) \\\\\n&=\\int h(1, x, u) f(u \\mid x) d u-\\int h(0, x, u) f(u \\mid x) d u \\\\\n&=\\int C(x, u) f(u \\mid x) d u \\\\\n&=\\operatorname{ACE}(x) .\n\\end{aligned}\n\\]\n这是有条件的 ACE。因此，在 CIA 下，回归系数等于 ACE。\n我们推断 \\(Y\\) 对 \\((D, X)\\) 的回归揭示了当 CIA 成立时治疗的因果影响。这意味着，当我们可以证明回归量 \\(X\\) 足以控制与治疗相关的因素时，可以对回归分析进行因果解释。\n定理 2.12 在结构模型 (2.54) 中，条件独立假设意味着 \\(\\nabla m(d, x)=\\operatorname{ACE}(x)\\)，即关于治疗的回归导数等于条件 ACE。\n这是一个令人着迷的结果。它表明，只要在以适当的回归量为条件后，不可观察变量独立于处理变量，回归导数就等于条件因果效应。这意味着 CEF 具有因果经济意义，为 CEF 的估计提供了强有力的理由。\n了解中央情报局的关键作用很重要。如果 CIA 失败，则回归导数和 ACE 的等式 (2.55) 失败。 CIA 声明以 \\(X\\) 为条件，变量 \\(U\\) 和 \\(D\\) 是独立的。这意味着处理 \\(D\\) 不受未观察到的个体因素 \\(U\\) 的影响，并且实际上是随机的。这是一个强有力的假设。在工资/教育示例中，这意味着个人不会根据他们未观察到的特征来选择教育。\n然而，了解 CIA 弱于 \\(U\\) 与回归量 \\((D, X)\\) 的完全独立性也有帮助。只需要 \\(U\\) 和 \\(D\\) 在以 \\(X\\) 为条件后是独立的。如果 \\(X\\) 足够丰富，这可能不是限制性的。\n回到我们的例子，我们需要一个变量 \\(X\\)，它打破了 \\(D\\) 和 \\(U\\) 之间的依赖关系。在我们的示例中，此变量是能力倾向测试分数，因为上大学的决定是基于测试分数。因此，一旦我们以考试成绩为条件，教育程度和类型是独立的。\n要看到这一点，请观察如果学生的考试成绩是 \\(\\mathrm{H}\\)，那么对于 Jennifers 和 Georges，他们上大学 \\((D=1)\\) 的概率是 \\(3 / 4\\)。同样，如果他们的考试成绩是 \\(\\mathrm{L}\\)，那么他们上大学的概率对于这两种类型都是 \\(1 / 4\\)。这意味着大学入学率与类型无关，取决于能力倾向测试分数。\n有条件的 ACE 取决于考试成绩。在获得高分的学生中，\\(3 / 4\\) 是 Jennifers，\\(1 / 4\\) 是 Georges。因此，分数为 \\(\\mathrm{H}\\) 的学生的条件 ACE 是 \\((3 / 4) \\times 10+\\) \\((1 / 4) \\times 4=\\$ 8.50\\). Among students who receive a low test score, \\(1 / 4\\) are Jennifers and \\(3 / 4\\) are Georges. Thus the ACE for students with a score of \\(\\mathrm{L}\\) is \\((1 / 4) \\times 10 +(3 / 4) \\times 4=\\$ 5.50\\)。无条件 ACE 是平均值，\\(\\mathrm{ACE}=(8.50+5.50) / 2=\\$ 7\\), because \\(50 %\\) of students each receive scores of \\(\\mathrm{H}\\) and \\(\\mathrm{L}\\).\n定理 \\(2.12\\) 表明条件 ACE 由包含测试分数的回归揭示。要在工资分布中看到这一点，假设计量经济学家收集有关能力倾向测试分数以及教育和工资的数据。给定 32 个人的随机样本，我们期望在表 \\(2.4\\) 中找到工资分布。\n定义一个虚拟高分来表示获得高分的学生。工资对大学出勤率和考试成绩及其交互作用的回归是\n\\[\n\\mathbb{E}[\\text { wage } \\mid \\text { college, highscore }]=1.00 \\text { highscore }+5.50 \\text { college }+3.00 \\text { highscore } \\times \\text { college }+8.50 \\text {. }\n\\]\n大学的系数，\\(\\$ 5.50\\), is the regression derivative of college attendance for those with low test scores, and the sum of this coefficient with the interaction coefficient \\(\\$ 3.00\\) 等于 \\(\\$ 8.50\\) which is the Table 2.4: Example Distribution 2\n\n\n\n\n\n\n\n\n\n\n\n\n\\(\\$ 8\\)\n\\(\\$ 10\\)\n\\(\\$ 12\\)\n\\(\\$ 20\\)\nMean\n\n\n\n\nHigh-School Graduate + High Test Score\n1\n3\n0\n0\n\\(\\$ 9.50\\)\n\n\nCollege Graduate + High Test Score\n0\n0\n3\n9\n\\(\\$ 18.00\\)\n\n\nHigh-School Graduate + Low Test Score\n9\n3\n0\n0\n\\(\\$ 8.50\\)\n\n\nCollege Graduate + Low Test Score\n0\n0\n3\n1\n\\(\\$ 14.00\\)\n\n\n\n高分学生的大学出勤率的回归导数。 \\(\\$ 5.50\\) and \\(\\$ 8.50\\) 等于上面计算的条件因果效应。\n这表明，从回归 (2.56) 中，计量经济学家会发现大学对工资的影响是 \\(\\$ 8.50\\) for those with high test scores and \\(\\$ 5.50\\) 对于那些测试成绩低的人，平均影响为 \\(\\$ 7\\) (because \\(50 %\\) of students receive high and low test scores). This is the true average causal effect of college on wages. Thus the regression coefficient on college in (2.56) can be interpreted causally, while a regression omitting the aptitude test score does not reveal the causal effect of education.\n为了总结我们的发现，我们已经展示了一个简单的回归如何可能对因果效应进行错误的测量，但更仔细的回归可以揭示真正的因果效应。关键是要以一组适当丰富的协变量为条件，以使影响结果的其余未观察到的因素与治疗变量无关。"
  },
  {
    "objectID": "chpt02-ce-chn.html#条件期望的存在性和唯一性",
    "href": "chpt02-ce-chn.html#条件期望的存在性和唯一性",
    "title": "条件预期和预测",
    "section": "条件期望的存在性和唯一性*",
    "text": "条件期望的存在性和唯一性*\n在 \\(2.3\\) 和 \\(2.6\\) 部分中，我们定义了条件变量 \\(X\\) 是离散的并且变量 \\((Y, X)\\) 具有联合密度时的条件期望。我们已经探索了这些情况，因为这些情况是条件均值最容易描述和理解的情况。然而，条件均值的存在相当普遍，不涉及离散或连续随机变量的性质。\n为了证明这一说法的合理性，我们现在提出了概率论的一个深刻结果。它所说的是条件均值存在于所有联合分布 \\((Y, X)\\) 中，其中 \\(Y\\) 具有有限均值。\n定理 2.13 条件期望的存在 如果 \\(\\mathbb{E}|Y|<\\infty\\) 则存在一个函数 \\(m(x)\\) 使得对于定义了 \\(\\mathbb{P}[X \\in \\mathscr{X}]\\) 的所有集合 \\(\\mathscr{X}\\)，\n\\[\n\\mathbb{E}[\\mathbb{1}\\{X \\in \\mathscr{X}\\} Y]=\\mathbb{E}[\\mathbb{1}\\{X \\in \\mathscr{X}\\} m(X)]\n\\]\n函数 \\(m(X)\\) 几乎在任何地方都是唯一的，即如果 \\(h(x)\\) 满足 (2.57)，则存在一个集合 \\(S\\)，使得 \\(\\mathbb{P}[S]=1\\) 和 \\(m(x)=h(x)\\) 对应于 \\(x \\in S\\)。函数 \\(m(x)\\) 称为条件期望，写成 \\(m(x)=\\mathbb{E}[Y \\mid X=x]\\)\n例如，参见 Ash (1972)，定理 6.3.3。\n当 \\((Y, X)\\) 具有联合密度时，由 (2.57) 定义的条件期望 \\(m(x)\\) 专门用于 (2.6)。定义 (2.57) 的用处在于定理 \\(2.13\\) 表明条件期望 \\(m(X)\\) 对于所有有限均值分布都存在。这个定义允许 \\(Y\\) 是离散的或连续的，对于 \\(X\\) 是标量或向量值的，并且对于 \\(X\\) 的组件是离散的或连续分布的。\n您可能已经注意到定理 \\(2.13\\) 仅适用于定义了 \\(\\mathbb{P}[X \\in \\mathscr{X}]\\) 的集合 \\(\\mathscr{X}\\)。这是一个技术问题——可测量性——我们在这本教科书中很大程度上回避了这个问题。形式概率论仅适用于可测量的集合（为其定义了概率），因为事实证明并非所有集合都满足可测量性。这不是应用程序的实际问题，因此我们将这种区别推迟到正式的理论处理中。"
  },
  {
    "objectID": "chpt02-ce-chn.html#鉴别",
    "href": "chpt02-ce-chn.html#鉴别",
    "title": "条件预期和预测",
    "section": "鉴别*",
    "text": "鉴别*\n结构计量经济学建模中的一个关键和重要问题是识别，这意味着参数由观察到的变量的分布唯一确定。在无条件和有条件的期望的上下文中，它相对简单，但为了清楚起见，在这一点上引入和探索这个概念是值得的。\n令 \\(F\\) 表示观测数据的分布，例如 \\((Y, X)\\) 对的分布。令 \\(\\mathscr{F}\\) 是分布 \\(F\\) 的集合。让 \\(\\theta\\) 成为感兴趣的参数（例如，期望 \\(\\mathbb{E}[Y])\\).\n定义 2.10 如果对于所有 \\(F \\in \\mathscr{F}\\)，存在唯一确定的 \\(\\theta\\) 值，则在 \\(\\mathscr{F}\\) 上标识参数 \\(\\theta \\in \\mathbb{R}\\)。\n等效地，如果我们可以将 \\(\\theta\\) 写为集合 \\(\\mathscr{F}\\) 上的映射 \\(\\theta=g(F)\\)，则它被识别。对集合 \\(\\mathscr{F}\\) 的限制很重要。大多数参数仅在所有分布空间的严格子集上被识别。\n以期望 \\(\\mu=\\mathbb{E}[Y]\\) 为例。 \\(\\mathbb{E}|Y|<\\infty\\) 是唯一确定的，因此 \\(\\mu\\) 被标识为集合 \\(\\mathscr{F}=\\{F: \\mathbb{E}|Y|<\\infty\\}\\)。\n接下来，考虑条件期望。定理 \\(2.13\\) 证明 \\(\\mathbb{E}|Y|<\\infty\\) 是识别的充分条件。\n定理 2.14 条件期望的识别 如果 \\(\\mathbb{E}|Y|<\\infty\\)，则条件期望 \\(m(x)=\\mathbb{E}[Y \\mid X=x]\\) 几乎在任何地方都可以识别。\n只要我们排除退化情况，识别似乎是参数的一般属性。这对于观察数据的时刻是正确的，但对于更复杂的模型则不一定。作为一个恰当的例子，考虑审查的背景。令 \\(Y\\) 为分布为 \\(F\\) 的随机变量。我们没有观察 \\(Y\\)，而是观察由审查规则定义的 \\(Y^{*}\\)\n\\[\nY^{*}=\\left\\{\\begin{array}{cc}\nY & \\text { if } Y \\leq \\tau \\\\\n\\tau & \\text { if } Y>\\tau\n\\end{array}\\right.\n\\]\n也就是说，\\(Y^{*}\\) 的上限为 \\(\\tau\\)。一个常见的例子是收入调查，其中收入回答是“最高编码”，这意味着高于最高代码 \\(\\tau\\) 的收入被记录为最高代码。观察到的变量 \\(Y^{*}\\) 有分布\n\\[\nF^{*}(u)=\\left\\{\\begin{array}{cc}\nF(u) & \\text { for } u \\leq \\tau \\\\\n1 & \\text { for } u \\geq \\tau .\n\\end{array}\\right.\n\\]\n我们对分布 \\(F\\) 的特征感兴趣，而不是审查分布 \\(F^{*}\\)。例如，我们对预期工资 \\(\\mu=\\mathbb{E}[Y]\\) 感兴趣。困难在于我们不能从 \\(F^{*}\\) 计算 \\(\\mu\\)，除非在没有审查 \\(\\mathbb{P}[Y \\geq \\tau]=0\\) 的普通情况下。因此，期望 \\(\\mu\\) 不是从审查分布中普遍识别出来的。\n识别问题的典型解决方案是假设参数分布。例如，令 \\(\\mathscr{F}\\) 为正态分布 \\(Y \\sim \\mathrm{N}\\left(\\mu, \\sigma^{2}\\right)\\) 的集合。可以证明参数 \\(\\left(\\mu, \\sigma^{2}\\right)\\) 是为所有 \\(F \\in \\mathscr{F}\\) 识别的。也就是说，如果我们知道未经审查的分布是正态的，我们可以唯一地确定来自审查分布的参数。这通常称为参数识别，因为识别仅限于参数类分布。在现代计量经济学中，这通常被视为次优解决方案，因为只有通过使用任意且无法验证的参数假设才能实现识别。\n一个悲观的结论可能是，如果没有参数假设，就不可能从审查数据中识别出感兴趣的参数。有趣的是，这种悲观情绪是没有根据的。事实证明，我们可以识别 \\(\\alpha \\leq \\mathbb{P}[Y \\leq \\tau]\\) 的 \\(F\\) 的分位数 \\(q_{\\alpha}\\)。例如，如果分布的 \\(20 %\\) 被删失，我们可以识别 \\(\\alpha \\in(0,0.8)\\) 的所有分位数。这通常称为非参数识别，因为参数的识别不受参数类的限制。\n我们从这个小练习中学到的是，在删失数据的背景下，只能通过参数识别矩，而非删失分位数是非参数识别。部分信息是，对识别的研究可以帮助将注意力集中在可以从可用数据分布中学到的东西上。"
  },
  {
    "objectID": "chpt02-ce-chn.html#技术证明",
    "href": "chpt02-ce-chn.html#技术证明",
    "title": "条件预期和预测",
    "section": "技术证明*",
    "text": "技术证明*\n定理 2.1 的证明 为方便起见，假设变量具有联合密度 \\(f(y, x)\\)。由于 \\(\\mathbb{E}[Y \\mid X]\\) 只是随机向量 \\(X\\) 的函数，为了计算它的期望，我们对 \\(X\\) 的密度 \\(f_{X}(x)\\) 进行积分，即\n\\[\n\\mathbb{E}[\\mathbb{E}[Y \\mid X]]=\\int_{\\mathbb{R}^{k}} \\mathbb{E}[Y \\mid X] f_{X}(x) d x .\n\\]\n代入 (2.6) 并注意到 \\(f_{Y \\mid X}(y \\mid x) f_{X}(x)=f(y, x)\\)，我们发现上面的表达式等于\n\\[\n\\int_{\\mathbb{R}^{k}}\\left(\\int_{\\mathbb{R}} y f_{Y \\mid X}(y \\mid x) d y\\right) f_{X}(x) d x=\\int_{\\mathbb{R}^{k}} \\int_{\\mathbb{R}} y f(y, x) d y d x=\\mathbb{E}[Y]\n\\]\n\\(Y\\) 的无条件期望。\n定理 2.2 的证明 再次假设变量具有联合密度。观察这一点很有用\n\\[\nf\\left(y \\mid x_{1}, x_{2}\\right) f\\left(x_{2} \\mid x_{1}\\right)=\\frac{f\\left(y, x_{1}, x_{2}\\right)}{f\\left(x_{1}, x_{2}\\right)} \\frac{f\\left(x_{1}, x_{2}\\right)}{f\\left(x_{1}\\right)}=f\\left(y, x_{2} \\mid x_{1}\\right)\n\\]\n给定 \\(X_{1}\\) 的 \\(\\left(Y, X_{2}\\right)\\) 的密度。在这里，我们滥用了符号并使用单个符号 \\(f\\) 来表示各种无条件和条件密度，以减少符号混乱。\n注意\n\\[\n\\mathbb{E}\\left[Y \\mid X_{1}=x_{1}, X_{2}=x_{2}\\right]=\\int_{\\mathbb{R}} y f\\left(y \\mid x_{1}, x_{2}\\right) d y .\n\\]\n在给定 \\(X_{1}\\) 的情况下，对 \\(X_{2}\\) 的条件密度积分 (2.59)，并应用 (2.58) 我们发现\n\\[\n\\begin{aligned}\n\\mathbb{E}\\left[\\mathbb{E}\\left[Y \\mid X_{1}, X_{2}\\right] \\mid X_{1}=x_{1}\\right] &=\\int_{\\mathbb{R}^{k_{2}}} \\mathbb{E}\\left[Y \\mid X_{1}=x_{1}, X_{2}=x_{2}\\right] f\\left(x_{2} \\mid x_{1}\\right) d x_{2} \\\\\n&=\\int_{\\mathbb{R}^{k_{2}}}\\left(\\int_{\\mathbb{R}} y f\\left(y \\mid x_{1}, x_{2}\\right) d y\\right) f\\left(x_{2} \\mid x_{1}\\right) d x_{2} \\\\\n&=\\int_{\\mathbb{R}^{k_{2}}} \\int_{\\mathbb{R}} y f\\left(y \\mid x_{1}, x_{2}\\right) f\\left(x_{2} \\mid x_{1}\\right) d y d x_{2} \\\\\n&=\\int_{\\mathbb{R}^{k_{2}}} \\int_{\\mathbb{R}} y f\\left(y, x_{2} \\mid x_{1}\\right) d y d x_{2} \\\\\n&=\\mathbb{E}\\left[Y \\mid X_{1}=x_{1}\\right] .\n\\end{aligned}\n\\]\n这意味着如上所述的 \\(\\mathbb{E}\\left[\\mathbb{E}\\left[Y \\mid X_{1}, X_{2}\\right] \\mid X_{1}\\right]=\\mathbb{E}\\left[Y \\mid X_{1}\\right]\\)。\n定理 2.3 的证明\n\\[\n\\mathbb{E}[g(X) Y \\mid X=x]=\\int_{\\mathbb{R}} g(x) y f_{Y \\mid X}(y \\mid x) d y=g(x) \\int_{\\mathbb{R}} y f_{Y \\mid X}(y \\mid x) d y=g(x) \\mathbb{E}[Y \\mid X=x]\n\\]\n这意味着 \\(\\mathbb{E}[g(X) Y \\mid X]=g(X) \\mathbb{E}[Y \\mid X]\\) 是 (2.7)。方程 (2.8) 将迭代期望的简单定律 (定理 2.1) 应用于 (2.7)。\n定理 2.4 的证明将 Minkowski 不等式 (B.34) 应用于 \\(e=Y-m(X)\\)，\n\\[\n\\left(\\mathbb{E}|e|^{r}\\right)^{1 / r}=\\left(\\mathbb{E}|Y-m(X)|^{r}\\right)^{1 / r} \\leq\\left(\\mathbb{E}|Y|^{r}\\right)^{1 / r}+\\left(\\mathbb{E}|m(X)|^{r}\\right)^{1 / r}<\\infty,\n\\]\n其中右侧的两部分是有限的，因为假设 \\(\\mathbb{E}|Y|^{r}<\\infty\\) 和条件期望不等式 \\(\\mathbb{E}|m(X)|^{r}<\\) \\(\\infty\\) (B.29)。 \\(\\left(\\mathbb{E}|e|^{r}\\right)^{1 / r}<\\infty\\) 意味着 \\(\\mathbb{E}|e|^{r}<\\infty\\) 的事实。\n定理 2.6 的证明 \\(\\mathbb{E}\\left[Y^{2}\\right]<\\infty\\) 的假设意味着以下所有条件期望都存在。\n使用迭代期望定律（定理 2.2）\\(\\mathbb{E}\\left[Y \\mid X_{1}\\right]=\\mathbb{E}\\left(\\mathbb{E}\\left[Y \\mid X_{1}, X_{2}\\right] \\mid X_{1}\\right)\\) 和条件 Jensen 不等式（B.28），\n\\[\n\\left(\\mathbb{E}\\left[Y \\mid X_{1}\\right]\\right)^{2}=\\left(\\mathbb{E}\\left(\\mathbb{E}\\left[Y \\mid X_{1}, X_{2}\\right] \\mid X_{1}\\right)\\right)^{2} \\leq \\mathbb{E}\\left[\\left(\\mathbb{E}\\left[Y \\mid X_{1}, X_{2}\\right]\\right)^{2} \\mid X_{1}\\right] .\n\\]\n采取无条件的期望，这意味着\n\\[\n\\mathbb{E}\\left[\\left(\\mathbb{E}\\left[Y \\mid X_{1}\\right]\\right)^{2}\\right] \\leq \\mathbb{E}\\left[\\left(\\mathbb{E}\\left[Y \\mid X_{1}, X_{2}\\right]\\right)^{2}\\right] .\n\\]\n相似地，\n\\[\n(\\mathbb{E}[Y])^{2} \\leq \\mathbb{E}\\left[\\left(\\mathbb{E}\\left[Y \\mid X_{1}\\right]\\right)^{2}\\right] \\leq \\mathbb{E}\\left[\\left(\\mathbb{E}\\left[Y \\mid X_{1}, X_{2}\\right]\\right)^{2}\\right] .\n\\]\n变量 \\(Y, \\mathbb{E}\\left[Y \\mid X_{1}\\right]\\) 和 \\(\\mathbb{E}\\left[Y \\mid X_{1}, X_{2}\\right]\\) 都具有相同的期望 \\(\\mathbb{E}[Y]\\)，因此不等式 (2.60) 意味着方差是单调排列的：\n\\[\n0 \\leq \\operatorname{var}\\left(\\mathbb{E}\\left[Y \\mid X_{1}\\right]\\right) \\leq \\operatorname{var}\\left(\\mathbb{E}\\left[Y \\mid X_{1}, X_{2}\\right]\\right) .\n\\]\n定义 \\(e=Y-\\mathbb{E}[Y \\mid X]\\) 和 \\(u=\\mathbb{E}[Y \\mid X]-\\mu\\) 以便我们得到分解 \\(Y-\\mu=e+u\\)。注意 \\(\\mathbb{E}[e \\mid X]=0\\) 和 \\(u\\) 是 \\(X\\) 的函数。因此根据条件定理（定理 2.3），\\(\\mathbb{E}[e u]=0\\) 所以 \\(e\\) 和 \\(u\\) 是不相关的。它遵循\n\\[\n\\operatorname{var}[Y]=\\operatorname{var}[e]+\\operatorname{var}[u]=\\operatorname{var}[Y-\\mathbb{E}[Y \\mid X]]+\\operatorname{var}[\\mathbb{E}[Y \\mid X]]\n\\]\n应用于方差分解 (2.62) 的条件期望 (2.61) 的方差的单调性意味着差异方差的反向单调性，从而完成了证明。\n定理 2.9 的证明 对于第 1 部分，由期望不等式 (B.30)、(A.17) 和假设 2.1，\n\\[\n\\left\\|\\mathbb{E}\\left[X X^{\\prime}\\right]\\right\\| \\leq \\mathbb{E}\\left\\|X X^{\\prime}\\right\\|=\\mathbb{E}\\|X\\|^{2}<\\infty .\n\\]\n类似地，使用期望不等式 (B.30)、Cauchy-Schwarz 不等式 (B.32) 和假设 2.1，\n\\[\n\\|\\mathbb{E}[X Y]\\| \\leq \\mathbb{E}\\|X Y\\| \\leq\\left(\\mathbb{E}\\|X\\|^{2}\\right)^{1 / 2}\\left(\\mathbb{E}\\left[Y^{2}\\right]\\right)^{1 / 2}<\\infty .\n\\]\n因此，矩 \\(\\mathbb{E}[X Y]\\) 和 \\(\\mathbb{E}\\left[X X^{\\prime}\\right]\\) 是有限且明确定义的。\n对于第 2 部分，系数 \\(\\beta=\\left(\\mathbb{E}\\left[X X^{\\prime}\\right]\\right)^{-1} \\mathbb{E}[X Y]\\) 定义明确，因为在假设 2.1 下存在 \\(\\left(\\mathbb{E}\\left[X X^{\\prime}\\right]\\right)^{-1}\\)。\n第 3 部分来自定义 \\(2.5\\) 和部分 \\(2 .\\)\n对于第 4 部分，首先请注意\n\\[\n\\begin{aligned}\n\\mathbb{E}\\left[e^{2}\\right] &=\\mathbb{E}\\left[\\left(Y-X^{\\prime} \\beta\\right)^{2}\\right] \\\\\n&=\\mathbb{E}\\left[Y^{2}\\right]-2 \\mathbb{E}\\left[Y X^{\\prime}\\right] \\beta+\\beta^{\\prime} \\mathbb{E}\\left[X X^{\\prime}\\right] \\beta \\\\\n&=\\mathbb{E}\\left[Y^{2}\\right]-\\mathbb{E}\\left[Y X^{\\prime}\\right]\\left(\\mathbb{E}\\left[X X^{\\prime}\\right]\\right)^{-1} \\mathbb{E}[X Y] \\\\\n& \\leq \\mathbb{E}\\left[Y^{2}\\right]<\\infty .\n\\end{aligned}\n\\]\n第一个不等式成立，因为 \\(\\mathbb{E}\\left[Y X^{\\prime}\\right]\\left(\\mathbb{E}\\left[X X^{\\prime}\\right]\\right)^{-1} \\mathbb{E}[X Y]\\) 是二次形式，因此必然是非负的。其次，通过期望不等式（B.30）、Cauchy-Schwarz 不等式（B.32）和假设 2.1，\n\\[\n\\|\\mathbb{E}[X e]\\| \\leq \\mathbb{E}\\|X e\\|=\\left(\\mathbb{E}\\|X\\|^{2}\\right)^{1 / 2}\\left(\\mathbb{E}\\left[e^{2}\\right]\\right)^{1 / 2}<\\infty .\n\\]\n由此可见，期望 \\(\\mathbb{E}[X e]\\) 是有限的，并且通过计算 (2.26) 为零。\n对于第 6 部分，将 Minkowski 不等式 (B.34) 应用于 \\(e=Y-X^{\\prime} \\beta\\)，\n\\[\n\\begin{aligned}\n\\left(\\mathbb{E}|e|^{r}\\right)^{1 / r} &=\\left(\\mathbb{E}\\left|Y-X^{\\prime} \\beta\\right|^{r}\\right)^{1 / r} \\\\\n& \\leq\\left(\\mathbb{E}|Y|^{r}\\right)^{1 / r}+\\left(\\mathbb{E}\\left|X^{\\prime} \\beta\\right|^{r}\\right)^{1 / r} \\\\\n& \\leq\\left(\\mathbb{E}|Y|^{r}\\right)^{1 / r}+\\left(\\mathbb{E}\\|X\\|^{r}\\right)^{1 / r}\\|\\beta\\|<\\infty,\n\\end{aligned}\n\\]\n假设的最终不等式。"
  },
  {
    "objectID": "chpt02-ce-chn.html#练习",
    "href": "chpt02-ce-chn.html#练习",
    "title": "条件预期和预测",
    "section": "练习",
    "text": "练习\n练习 2.1 找到 \\(\\mathbb{E}\\left[\\mathbb{E}\\left[\\mathbb{E}\\left[Y \\mid X_{1}, X_{2}, X_{3}\\right] \\mid X_{1}, X_{2}\\right] \\mid X_{1}\\right]\\)\n练习 2.2 如果 \\(\\mathbb{E}[Y \\mid X]=a+b X\\)，求 \\(\\mathbb{E}[Y X]\\) 作为 \\(X\\) 矩的函数。\n练习 2.3 使用迭代期望定律证明定理 2.4.4。练习 2.4 假设随机变量 \\(Y\\) 和 \\(X\\) 只取值 0 和 1 ，并且具有以下联合概率分布\n\n\n\n\n\\(X=0\\)\n\\(X=1\\)\n\n\n\n\n\\(Y=0\\)\n\\(.1\\)\n\\(.2\\)\n\n\n\\(Y=1\\)\n\\(.4\\)\n\\(.3\\)\n\n\n\n为 \\(X=0\\) 和 \\(X=1\\) 找到 \\(\\mathbb{E}[Y \\mid X], \\mathbb{E}\\left[Y^{2} \\mid X\\right]\\) 和 \\(\\operatorname{var}[Y \\mid X]\\)\n练习 2.5 证明 \\(\\sigma^{2}(X)\\) 是给定 \\(X\\) 的 \\(e^{2}\\) 的最佳预测器：\n\n写下 \\(e^{2}\\) 的预测变量 \\(h(X)\\) 的均方误差。\n预测 \\(e^{2}\\) 是什么意思？\n证明 \\(\\sigma^{2}(X)\\) 最小化均方误差，因此是最好的预测器。\n\n练习 2.6 用 \\(Y=m(X)+e\\) 证明 \\(\\operatorname{var}[Y]=\\operatorname{var}[m(X)]+\\sigma^{2}\\)\n练习 2.7 证明条件方差可以写成 \\(\\sigma^{2}(X)=\\mathbb{E}\\left[Y^{2} \\mid X\\right]-(\\mathbb{E}[Y \\mid X])^{2}\\)。\n练习 2.8 假设 \\(Y\\) 是离散值的，只取非负整数的值，并且 \\(Y\\) 的条件分布给定 \\(X=x\\) 是泊松：\n\\[\n\\mathbb{P}[Y=j \\mid X=x]=\\frac{\\exp \\left(-x^{\\prime} \\beta\\right)\\left(x^{\\prime} \\beta\\right)^{j}}{j !}, \\quad j=0,1,2, \\ldots\n\\]\n计算 \\(\\mathbb{E}[Y \\mid X]\\) 和 \\(\\operatorname{var}[Y \\mid X]\\)。这是否证明了 \\(Y=X^{\\prime} \\beta+e\\) 形式的线性回归模型？\n\\[\n\\text { Hint: If } \\mathbb{P}[Y=j]=\\frac{\\exp (-\\lambda) \\lambda^{j}}{j !} \\text { then } \\mathbb{E}[Y]=\\lambda \\text { and } \\operatorname{var}[Y]=\\lambda \\text {. }\n\\]\n练习 2.9 假设你有两个回归量：\\(X_{1}\\) 是二元的（取值 0 和 1），\\(X_{2}\\) 是分类的，有 3 个类别 \\((A, B, C)\\)。将 \\(\\mathbb{E}\\left[Y \\mid X_{1}, X_{2}\\right]\\) 写为线性回归。\n练习 2.10 对或错。如果 \\(Y=X \\beta+e, X \\in \\mathbb{R}\\) 和 \\(\\mathbb{E}[e \\mid X]=0\\)，那么 \\(\\mathbb{E}\\left[X^{2} e\\right]=0\\)。\n练习 2.11 对或错。如果 \\(Y=X \\beta+e, X \\in \\mathbb{R}\\) 和 \\(\\mathbb{E}[X e]=0\\)，那么 \\(\\mathbb{E}\\left[X^{2} e\\right]=0\\)。\n练习 2.12 对或错。如果 \\(Y=X^{\\prime} \\beta+e\\) 和 \\(\\mathbb{E}[e \\mid X]=0\\)，则 \\(e\\) 独立于 \\(X\\)。\n练习 2.13 对或错。如果 \\(Y=X^{\\prime} \\beta+e\\) 和 \\(\\mathbb{E}[X e]=0\\)，那么 \\(\\mathbb{E}[e \\mid X]=0\\)。\n练习 2.14 对或错。如果 \\(Y=X^{\\prime} \\beta+e, \\mathbb{E}[e \\mid X]=0\\) 和 \\(\\mathbb{E}\\left[e^{2} \\mid X\\right]=\\sigma^{2}\\)，则 \\(e\\) 独立于 \\(X\\)。\n练习 2.15 考虑仅截距模型 \\(Y=\\alpha+e\\)，\\(\\alpha\\) 是最好的线性预测器。证明 \\(\\alpha=\\mathbb{E}[Y]\\)\n练习 2.16 让 \\(X\\) 和 \\(Y\\) 在 \\(0 \\leq x \\leq 1,0 \\leq y \\leq 1\\) 上有联合密度 \\(f(x, y)=\\frac{3}{2}\\left(x^{2}+y^{2}\\right)\\)。计算最佳线性预测器 \\(Y=\\alpha+\\beta X+e\\) 的系数。计算条件期望 \\(m(x)=\\) \\(\\mathbb{E}[Y \\mid X=x]\\)。最佳线性预测器和条件期望是否不同？练习 2.17 设 \\(X\\) 是一个随机变量，\\(\\mu=\\mathbb{E}[X]\\) 和 \\(\\sigma^{2}=\\operatorname{var}[X]\\)。定义\n\\[\ng\\left(x, \\mu, \\sigma^{2}\\right)=\\left(\\begin{array}{c}\nx-\\mu \\\\\n(x-\\mu)^{2}-\\sigma^{2}\n\\end{array}\\right) .\n\\]\n证明 \\(\\mathbb{E}[g(X, m, s)]=0\\) 当且仅当 \\(m=\\mu\\) 和 \\(s=\\sigma^{2}\\)。\n练习 2.18 假设 \\(X=\\left(1, X_{2}, X_{3}\\right)\\) 其中 \\(X_{3}=\\alpha_{1}+\\alpha_{2} X_{2}\\) 是 \\(X_{2}\\) 的线性函数。\n\n证明 \\(\\boldsymbol{Q}_{X X}=\\mathbb{E}\\left[X X^{\\prime}\\right]\\) 不可逆。\n使用 \\(X\\) 的线性变换来找到给定 \\(X\\) 的 \\(Y\\) 的最佳线性预测器的表达式。 （要明确，不要只使用广义逆公式。）\n\n习题 2.19 显示 (2.47)-(2.48)，即对于\n\\[\nd(\\beta)=\\mathbb{E}\\left[\\left(m(X)-X^{\\prime} \\beta\\right)^{2}\\right]\n\\]\n然后\n\\[\n\\beta=\\underset{b \\in \\mathbb{R}^{k}}{\\operatorname{argmin}} d(b)=\\left(\\mathbb{E}\\left[X X^{\\prime}\\right]\\right)^{-1} \\mathbb{E}[X m(X)]=\\left(\\mathbb{E}\\left[X X^{\\prime}\\right]\\right)^{-1} \\mathbb{E}[X Y] .\n\\]\n提示：要显示 \\(\\mathbb{E}[X m(X)]=\\mathbb{E}[X Y]\\)，请使用迭代期望定律。\n练习 2.20 当 \\((Y, X)\\) 具有联合密度 \\(f(y, x)\\) 时，验证 (2.57) 与 (2.6) 中定义的 \\(m(X)\\) 成立。\n练习 2.21 考虑短期和长期预测\n\\[\n\\begin{gathered}\nY=X \\gamma_{1}+e \\\\\nY=X \\beta_{1}+X^{2} \\beta_{2}+u\n\\end{gathered}\n\\]\n\n在什么条件下 \\(\\gamma_{1}=\\beta_{1}\\) ？\n取长投影为 \\(Y=X \\theta_{1}+X^{3} \\theta_{2}+v\\)。是否存在 \\(\\gamma_{1}=\\theta_{1}\\) 的条件？\n\n练习 2.22 取同方差模型\n\\[\n\\begin{aligned}\nY &=X_{1}^{\\prime} \\beta_{1}+X_{2}^{\\prime} \\beta_{2}+e \\\\\n\\mathbb{E}\\left[e \\mid X_{1}, X_{2}\\right] &=0 \\\\\n\\mathbb{E}\\left[e^{2} \\mid X_{1}, X_{2}\\right] &=\\sigma^{2} \\\\\n\\mathbb{E}\\left[X_{2} \\mid X_{1}\\right] &=\\Gamma X_{1} .\n\\end{aligned}\n\\]\n假设 \\(\\Gamma \\neq 0\\)。假设参数 \\(\\beta_{1}\\) 是感兴趣的。我们知道，排除 \\(X_{2}\\) 会在 \\(X_{2}\\) 上的投影系数中产生遗漏变量偏差。它还改变了方程误差。我们的问题是：对诱导方程误差的同方差性有什么影响？排除 \\(X_{2}\\) 是否会引起异方差？请明确点。"
  },
  {
    "objectID": "chpt03-algebra.html#introduction",
    "href": "chpt03-algebra.html#introduction",
    "title": "3  The Algebra of Least Squares",
    "section": "3.1 Introduction",
    "text": "3.1 Introduction\nIn this chapter we introduce the popular least squares estimator. Most of the discussion will be algebraic, with questions of distribution and inference deferred to later chapters."
  },
  {
    "objectID": "chpt03-algebra.html#samples",
    "href": "chpt03-algebra.html#samples",
    "title": "3  The Algebra of Least Squares",
    "section": "3.2 Samples",
    "text": "3.2 Samples\nIn Section \\(2.18\\) we derived and discussed the best linear predictor of \\(Y\\) given \\(X\\) for a pair of random variables \\((Y, X) \\in \\mathbb{R} \\times \\mathbb{R}^{k}\\) and called this the linear projection model. We are now interested in estimating the parameters of this model, in particular the projection coefficient\n\\[\n\\beta=\\left(\\mathbb{E}\\left[X X^{\\prime}\\right]\\right)^{-1} \\mathbb{E}[X Y] .\n\\]\nWe can estimate \\(\\beta\\) from samples which include joint measurements of \\((Y, X)\\). For example, supposing we are interested in estimating a wage equation, we would use a dataset with observations on wages (or weekly earnings), education, experience (or age), and demographic characteristics (gender, race, location). One possible dataset is the Current Population Survey (CPS), a survey of U.S. households which includes questions on employment, income, education, and demographic characteristics.\nNotationally we wish to distinguish observations (realizations) from the underlying random variables. The random variables are \\((Y, X)\\). The observations are \\(\\left(Y_{i}, X_{i}\\right)\\). From the vantage of the researcher the latter are numbers. From the vantage of statistical theory we view them as realizations of random variables. For individual observations we append a subscript \\(i\\) which runs from 1 to \\(n\\), thus the \\(i^{t h}\\) observation is \\(\\left(Y_{i}, X_{i}\\right)\\). The number \\(n\\) is the sample size. The dataset or sample is \\(\\left\\{\\left(Y_{i}, X_{i}\\right): i=1, \\ldots, n\\right\\}\\).\nFrom the viewpoint of empirical analysis a dataset is an array of numbers. It is typically organized as a table where each column is a variable and each row is an observation. For empirical analysis the dataset is fixed in the sense that they are numbers presented to the researcher. For statistical analysis we view the dataset as random, or more precisely as a realization of a random process.\nThe individual observations could be draws from a common (homogeneous) distribution or could be draws from heterogeneous distributions. The simplest approach is to assume homogeneity-that the observations are realizations from an identical underlying population \\(F\\).\nAssumption 3.1 The variables \\(\\left\\{\\left(Y_{1}, X_{1}\\right), \\ldots,\\left(Y_{i}, X_{i}\\right), \\ldots,\\left(Y_{n}, X_{n}\\right)\\right\\}\\) are identically distributed; they are draws from a common distribution \\(F\\). This assumption does not need to be viewed as literally true. Rather it is a useful modeling device so that parameters such as \\(\\beta\\) are well defined. This assumption should be interpreted as how we view an observation a priori, before we actually observe it. If I tell you that we have a sample with \\(n=59\\) observations set in no particular order, then it makes sense to view two observations, say 17 and 58 , as draws from the same distribution. We have no reason to expect anything special about either observation.\nIn econometric theory we refer to the underlying common distribution \\(F\\) as the population. Some authors prefer the label data-generating-process (DGP). You can think of it as a theoretical concept or an infinitely-large potential population. In contrast, we refer to the observations available to us \\(\\left\\{\\left(Y_{i}, X_{i}\\right)\\right.\\) : \\(i=1, \\ldots, n\\}\\) as the sample or dataset. In some contexts the dataset consists of all potential observations, for example administrative tax records may contain every single taxpayer in a political unit. Even in this case we can view the observations as if they are random draws from an underlying infinitely-large population as this will allow us to apply the tools of statistical theory.\nThe linear projection model applies to the random variables \\((Y, X)\\). This is the probability model described in Section 2.18. The model is\n\\[\nY=X^{\\prime} \\beta+e\n\\]\nwhere the linear projection coefficient \\(\\beta\\) is defined as\n\\[\n\\beta=\\underset{b \\in \\mathbb{R}^{k}}{\\operatorname{argmin}} S(b),\n\\]\nthe minimizer of the expected squared error\n\\[\nS(\\beta)=\\mathbb{E}\\left[\\left(Y-X^{\\prime} \\beta\\right)^{2}\\right] .\n\\]\nThe coefficient has the explicit solution (3.1)."
  },
  {
    "objectID": "chpt03-algebra.html#moment-estimators",
    "href": "chpt03-algebra.html#moment-estimators",
    "title": "3  The Algebra of Least Squares",
    "section": "3.3 Moment Estimators",
    "text": "3.3 Moment Estimators\nWe want to estimate the coefficient \\(\\beta\\) defined in (3.1) from the sample of observations. Notice that \\(\\beta\\) is written as a function of certain population expectations. In this context an appropriate estimator is the same function of the sample moments. Let’s explain this in detail.\nTo start, suppose that we are interested in the population mean \\(\\mu\\) of a random variable \\(Y\\) with distribution function \\(F\\)\n\\[\n\\mu=\\mathbb{E}[Y]=\\int_{-\\infty}^{\\infty} y d F(y) .\n\\]\nThe expectation \\(\\mu\\) is a function of the distribution \\(F\\). To estimate \\(\\mu\\) given \\(n\\) random variables \\(Y_{i}\\) from \\(F\\) a natural estimator is the sample mean\n\\[\n\\widehat{\\mu}=\\bar{Y}=\\frac{1}{n} \\sum_{i=1}^{n} Y_{i} .\n\\]\nNotice that we have written this using two pieces of notation. The notation \\(\\bar{Y}\\) with the bar on top is conventional for a sample mean. The notation \\(\\widehat{\\mu}\\) with the hat ” \\(\\wedge\\) ” is conventional in econometrics to denote an estimator of the parameter \\(\\mu\\). In this case \\(\\bar{Y}\\) is the estimator of \\(\\mu\\), so \\(\\widehat{\\mu}\\) and \\(\\bar{Y}\\) are the same. The sample mean \\(\\bar{Y}\\) can be viewed as the natural analog of the population mean (3.5) because \\(\\bar{Y}\\) equals the expectation (3.5) with respect to the empirical distribution - the discrete distribution which puts weight \\(1 / n\\) on each observation \\(Y_{i}\\). There are other justifications for \\(\\bar{Y}\\) as an estimator for \\(\\mu\\). We will defer these discussions for now. Suffice it to say that it is the conventional estimator. Now suppose that we are interested in a set of population expectations of possibly nonlinear functions of a random vector \\(Y\\), say \\(\\mu=\\mathbb{E}[h(Y)]\\). For example, we may be interested in the first two moments of \\(Y, \\mathbb{E}[Y]\\) and \\(\\mathbb{E}\\left[Y^{2}\\right]\\). In this case the natural estimator is the vector of sample means,\n\\[\n\\widehat{\\mu}=\\frac{1}{n} \\sum_{i=1}^{n} h\\left(Y_{i}\\right) .\n\\]\nWe call \\(\\widehat{\\mu}\\) the moment estimator for \\(\\mu\\). For example, if \\(h(y)=\\left(y, y^{2}\\right)^{\\prime}\\) then \\(\\widehat{\\mu}_{1}=n^{-1} \\sum_{i=1}^{n} Y_{i}\\) and \\(\\widehat{\\mu}_{2}=\\) \\(n^{-1} \\sum_{i=1}^{n} Y_{i}^{2}\\)\nNow suppose that we are interested in a nonlinear function of a set of moments. For example, consider the variance of \\(Y\\)\n\\[\n\\sigma^{2}=\\operatorname{var}[Y]=\\mathbb{E}\\left[Y^{2}\\right]-(\\mathbb{E}[Y])^{2} .\n\\]\nIn general, many parameters of interest can be written as a function of moments of \\(Y\\). Notationally, \\(\\beta=g(\\mu)\\) and \\(\\mu=\\mathbb{E}[h(Y)]\\). Here, \\(Y\\) are the random variables, \\(h(Y)\\) are functions (transformations) of the random variables, and \\(\\mu\\) is the expectation of these functions. \\(\\beta\\) is the parameter of interest, and is the (nonlinear) function \\(g(\\cdot)\\) of these expectations.\nIn this context a natural estimator of \\(\\beta\\) is obtained by replacing \\(\\mu\\) with \\(\\widehat{\\mu}\\). Thus \\(\\widehat{\\beta}=g(\\widehat{\\mu})\\). The estimator \\(\\widehat{\\beta}\\) is often called a plug-in estimator. We also call \\(\\widehat{\\beta}\\) a moment, or moment-based, estimator of \\(\\beta\\) since it is a natural extension of the moment estimator \\(\\widehat{\\mu}\\).\nTake the example of the variance \\(\\sigma^{2}=\\operatorname{var}[Y]\\). Its moment estimator is\n\\[\n\\widehat{\\sigma}^{2}=\\widehat{\\mu}_{2}-\\widehat{\\mu}_{1}^{2}=\\frac{1}{n} \\sum_{i=1}^{n} Y_{i}^{2}-\\left(\\frac{1}{n} \\sum_{i=1}^{n} Y_{i}\\right)^{2}\n\\]\nThis is not the only possible estimator for \\(\\sigma^{2}\\) (there is also the well-known bias-corrected estimator) but \\(\\widehat{\\sigma}^{2}\\) is a straightforward and simple choice."
  },
  {
    "objectID": "chpt03-algebra.html#least-squares-estimator",
    "href": "chpt03-algebra.html#least-squares-estimator",
    "title": "3  The Algebra of Least Squares",
    "section": "3.4 Least Squares Estimator",
    "text": "3.4 Least Squares Estimator\nThe linear projection coefficient \\(\\beta\\) is defined in (3.3) as the minimizer of the expected squared error \\(S(\\beta)\\) defined in (3.4). For given \\(\\beta\\), the expected squared error is the expectation of the squared error \\(\\left(Y-X^{\\prime} \\beta\\right)^{2}\\). The moment estimator of \\(S(\\beta)\\) is the sample average:\n\\[\n\\widehat{S}(\\beta)=\\frac{1}{n} \\sum_{i=1}^{n}\\left(Y_{i}-X_{i}^{\\prime} \\beta\\right)^{2}=\\frac{1}{n} \\operatorname{SSE}(\\beta)\n\\]\nwhere\n\\[\n\\operatorname{SSE}(\\beta)=\\sum_{i=1}^{n}\\left(Y_{i}-X_{i}^{\\prime} \\beta\\right)^{2}\n\\]\nis called the sum of squared errors function.\nSince \\(\\widehat{S}(\\beta)\\) is a sample average we can interpret it as an estimator of the expected squared error \\(S(\\beta)\\). Examining \\(\\widehat{S}(\\beta)\\) as a function of \\(\\beta\\) is informative about how \\(S(\\beta)\\) varies with \\(\\beta\\). Since the projection coefficient minimizes \\(S(\\beta)\\) an analog estimator minimizes (3.6).\nWe define the estimator \\(\\widehat{\\beta}\\) as the minimizer of \\(\\widehat{S}(\\beta)\\).\nDefinition \\(3.1\\) The least squares estimator is \\(\\widehat{\\beta}=\\underset{\\beta \\in \\mathbb{R}^{k}}{\\operatorname{argmin}} \\widehat{S}(\\beta)\\)\\ where \\(\\widehat{S}(\\beta)=\\frac{1}{n} \\sum_{i=1}^{n}\\left(Y_{i}-X_{i}^{\\prime} \\beta\\right)^{2}\\)\nAs \\(\\widehat{S}(\\beta)\\) is a scale multiple of \\(\\operatorname{SSE}(\\beta)\\) we may equivalently define \\(\\widehat{\\beta}\\) as the minimizer of \\(\\operatorname{SSE}(\\beta)\\). Hence \\(\\widehat{\\beta}\\) is commonly called the least squares (LS) estimator of \\(\\beta\\). The estimator is also commonly refered to as the ordinary least squares (OLS) estimator. For the origin of this label see the historical discussion on Adrien-Marie Legendre below. Here, as is common in econometrics, we put a hat ” \\(\\wedge\\) ” over the parameter \\(\\beta\\) to indicate that \\(\\widehat{\\beta}\\) is a sample estimator of \\(\\beta\\). This is a helpful convention. Just by seeing the symbol \\(\\widehat{\\beta}\\) we can immediately interpret it as an estimator (because of the hat) of the parameter \\(\\beta\\). Sometimes when we want to be explicit about the estimation method, we will write \\(\\widehat{\\beta}_{\\text {ols }}\\) to signify that it is the OLS estimator. It is also common to see the notation \\(\\widehat{\\beta}_{n}\\), where the subscript ” \\(n\\) ” indicates that the estimator depends on the sample size \\(n\\).\nIt is important to understand the distinction between population parameters such as \\(\\beta\\) and sample estimators such as \\(\\widehat{\\beta}\\). The population parameter \\(\\beta\\) is a non-random feature of the population while the sample estimator \\(\\widehat{\\beta}\\) is a random feature of a random sample. \\(\\beta\\) is fixed, while \\(\\widehat{\\beta}\\) varies across samples."
  },
  {
    "objectID": "chpt03-algebra.html#solving-for-least-squares-with-one-regressor",
    "href": "chpt03-algebra.html#solving-for-least-squares-with-one-regressor",
    "title": "3  The Algebra of Least Squares",
    "section": "3.5 Solving for Least Squares with One Regressor",
    "text": "3.5 Solving for Least Squares with One Regressor\nFor simplicity, we start by considering the case \\(k=1\\) so that there is a scalar regressor \\(X\\) and a scalar coefficient \\(\\beta\\). To illustrate, Figure 3.1(a) displays a scatter \\(\\operatorname{plot}^{1}\\) of 20 pairs \\(\\left(Y_{i}, X_{i}\\right)\\).\nThe sum of squared errors \\(\\operatorname{SSE}(\\beta)\\) is a function of \\(\\beta\\). Given \\(\\beta\\) we calculate the “error” \\(Y_{i}-X_{i} \\beta\\) by taking the vertical distance between \\(Y_{i}\\) and \\(X_{i} \\beta\\). This can be seen in Figure 3.1(a) by the vertical lines which connect the observations to the straight line. These vertical lines are the errors \\(Y_{i}-X_{i} \\beta\\). The sum of squared errors is the sum of the 20 squared lengths.\nThe sum of squared errors is the function\n\\[\n\\operatorname{SSE}(\\beta)=\\sum_{i=1}^{n}\\left(Y_{i}-X_{i} \\beta\\right)^{2}=\\left(\\sum_{i=1}^{n} Y_{i}^{2}\\right)-2 \\beta\\left(\\sum_{i=1}^{n} X_{i} Y_{i}\\right)+\\beta^{2}\\left(\\sum_{i=1}^{n} X_{i}^{2}\\right) .\n\\]\nThis is a quadratic function of \\(\\beta\\). The sum of squared error function is displayed in Figure \\(3.1\\) (b) over the range \\([2,4]\\). The coefficient \\(\\beta\\) ranges along the \\(x\\)-axis. The sum of squared errors \\(\\operatorname{SSE}(\\beta)\\) as a function of \\(\\beta\\) is displayed on the \\(y\\)-axis.\nThe OLS estimator \\(\\widehat{\\beta}\\) minimizes this function. From elementary algebra we know that the minimizer of the quadratic function \\(a-2 b x+c x^{2}\\) is \\(x=b / c\\). Thus the minimizer of \\(\\operatorname{SSE}(\\beta)\\) is\n\\[\n\\widehat{\\beta}=\\frac{\\sum_{i=1}^{n} X_{i} Y_{i}}{\\sum_{i=1}^{n} X_{i}^{2}}\n\\]\nFor example, the minimizer of the sum of squared error function displayed in Figure 3.1(b) is \\(\\widehat{\\beta}=3.07\\), and is marked on the \\(\\mathrm{x}\\)-axis.\nThe intercept-only model is the special case \\(X_{i}=1\\). In this case we find\n\\[\n\\widehat{\\beta}=\\frac{\\sum_{i=1}^{n} 1 Y_{i}}{\\sum_{i=1}^{n} 1^{2}}=\\frac{1}{n} \\sum_{i=1}^{n} Y_{i}=\\bar{Y},\n\\]\n\\({ }^{1}\\) The observations were generated by simulation as \\(X \\sim U[0,1]\\) and \\(Y \\sim \\mathrm{N}[3 X, 1]\\).\n\n\nDeviation from Fitted Line\n\n\n\nSum of Squared Error Function\n\nFigure 3.1: Regression With One Regressor\nthe sample mean of \\(Y_{i}\\). Here, as is common, we put a bar “-” over \\(Y\\) to indicate that the quantity is a sample mean. This shows that the OLS estimator in the intercept-only model is the sample mean.\nTechnically, the estimator \\(\\widehat{\\beta}\\) in (3.7) only exists if the denominator is non-zero. Since it is a sum of squares it is necessarily non-negative. Thus \\(\\widehat{\\beta}\\) exists if \\(\\sum_{i=1}^{n} X_{i}^{2}>0\\)."
  },
  {
    "objectID": "chpt03-algebra.html#solving-for-least-squares-with-multiple-regressors",
    "href": "chpt03-algebra.html#solving-for-least-squares-with-multiple-regressors",
    "title": "3  The Algebra of Least Squares",
    "section": "3.6 Solving for Least Squares with Multiple Regressors",
    "text": "3.6 Solving for Least Squares with Multiple Regressors\nWe now consider the case with \\(k>1\\) so that the coefficient \\(\\beta \\in \\mathbb{R}^{k}\\) is a vector.\nTo illustrate, Figure \\(3.2\\) displays a scatter plot of 100 triples \\(\\left(Y_{i}, X_{1 i}, X_{2 i}\\right)\\). The regression function \\(x^{\\prime} \\beta=x_{1} \\beta_{1}+x_{2} \\beta_{2}\\) is a 2-dimensional surface and is shown as the plane in Figure 3.2.\nThe sum of squared errors \\(\\operatorname{SSE}(\\beta)\\) is a function of the vector \\(\\beta\\). For any \\(\\beta\\) the error \\(Y_{i}-X_{i}^{\\prime} \\beta\\) is the vertical distance between \\(Y_{i}\\) and \\(X_{i}^{\\prime} \\beta\\). This can be seen in Figure \\(3.2\\) by the vertical lines which connect the observations to the plane. As in the single regressor case these vertical lines are the errors \\(e_{i}=Y_{i}-\\) \\(X_{i}^{\\prime} \\beta\\). The sum of squared errors is the sum of the 100 squared lengths.\nThe sum of squared errors can be written as\n\\[\n\\operatorname{SSE}(\\beta)=\\sum_{i=1}^{n} Y_{i}^{2}-2 \\beta^{\\prime} \\sum_{i=1}^{n} X_{i} Y_{i}+\\beta^{\\prime} \\sum_{i=1}^{n} X_{i} X_{i}^{\\prime} \\beta .\n\\]\nAs in the single regressor case this is a quadratic function in \\(\\beta\\). The difference is that in the multiple regressor case this is a vector-valued quadratic function. To visualize the sum of squared errors function Figure 3.3(a) displays \\(\\operatorname{SSE}(\\beta)\\). Another way to visualize a 3-dimensional surface is by a contour plot. A contour plot of the same \\(\\operatorname{SSE}(\\beta)\\) function is shown in Figure 3.3(b). The contour lines are points in the \\(\\left(\\beta_{1}, \\beta_{2}\\right)\\) space where \\(\\operatorname{SSE}(\\beta)\\) takes the same value. The contour lines are elliptical because \\(\\operatorname{SSE}(\\beta)\\) is quadratic.\n\nFigure 3.2: Regression with Two Variables\nThe least squares estimator \\(\\widehat{\\beta}\\) minimizes \\(\\operatorname{SSE}(\\beta)\\). A simple way to find the minimum is by solving the first-order conditions. The latter are\n\\[\n0=\\frac{\\partial}{\\partial \\beta} \\operatorname{SSE}(\\widehat{\\beta})=-2 \\sum_{i=1}^{n} X_{i} Y_{i}+2 \\sum_{i=1}^{n} X_{i} X_{i}^{\\prime} \\widehat{\\beta}\n\\]\nWe have written this using a single expression, but it is actually a system of \\(k\\) equations with \\(k\\) unknowns (the elements of \\(\\widehat{\\beta}\\) ).\nThe solution for \\(\\widehat{\\beta}\\) may be found by solving the system of \\(k\\) equations in (3.9). We can write this solution compactly using matrix algebra. Dividing (3.9) by 2 we obtain\n\\[\n\\sum_{i=1}^{n} X_{i} X_{i}^{\\prime} \\widehat{\\beta}=\\sum_{i=1}^{n} X_{i} Y_{i} .\n\\]\nThis is a system of equations of the form \\(\\boldsymbol{A} \\boldsymbol{b}=\\boldsymbol{c}\\) where \\(\\boldsymbol{A}\\) is \\(k \\times k\\) and \\(\\boldsymbol{b}\\) and \\(\\boldsymbol{c}\\) are \\(k \\times 1\\). The solution is \\(\\boldsymbol{b}=\\boldsymbol{A}^{-1} \\boldsymbol{c}\\), and can be obtained by pre-multiplying \\(\\boldsymbol{A} \\boldsymbol{b}=\\boldsymbol{c}\\) by \\(\\boldsymbol{A}^{-1}\\) and using the matrix inverse property \\(\\boldsymbol{A}^{-1} \\boldsymbol{A}=\\boldsymbol{I}_{k}\\). Applied to (3.10) we find an explicit formula for the least squares estimator\n\\[\n\\widehat{\\beta}=\\left(\\sum_{i=1}^{n} X_{i} X_{i}^{\\prime}\\right)^{-1}\\left(\\sum_{i=1}^{n} X_{i} Y_{i}\\right) .\n\\]\nThis is the natural estimator of the best linear projection coefficient \\(\\beta\\) defined in (3.3), and could also be called the linear projection estimator.\n\n\nSum of Squared Error Function\n\n\n\nSSE Contour\n\nFigure 3.3: SSE with Two Regressors\nRecall that we claimed that \\(\\widehat{\\beta}\\) in (3.11) is the minimizer of \\(\\operatorname{SSE}(\\beta)\\), and found it by solving the firstorder conditions. To be complete we should verify the second-order conditions. We calculate that\n\\[\n\\frac{\\partial^{2}}{\\partial \\beta \\partial \\beta^{\\prime}} \\operatorname{SSE}(\\beta)=2 \\sum_{i=1}^{n} X_{i} X_{i}^{\\prime}\n\\]\nwhich is a positive semi-definite matrix. If actually positive definite, then the second-order condition for minimization is satisfied, in which case \\(\\widehat{\\beta}\\) is the unique minimizer of \\(\\operatorname{SSE}(\\beta)\\).\nReturning to the example sum of squared errors function \\(\\operatorname{SSE}(\\beta)\\) displayed in Figure \\(3.3\\), the least squares estimator \\(\\widehat{\\beta}\\) is the the pair \\(\\left(\\widehat{\\beta}_{1}, \\widehat{\\beta}_{2}\\right)\\) which minimize this function; visually it is the low spot in the 3-dimensional graph, and is marked in Figure 3.3(b) as the center point of the contour plots.\nTake equation (3.11) and suppose that \\(k=1\\). In this case \\(X_{i}\\) is scalar so \\(X_{i} X_{i}^{\\prime}=X_{i}^{2}\\). Then (3.11) simplifies to the expression (3.7) previously derived. The expression (3.11) is a notationally simple generalization but requires a careful attention to vector and matrix manipulations.\nAlternatively, equation (3.1) writes the projection coefficient \\(\\beta\\) as an explicit function of the population moments \\(\\boldsymbol{Q}_{X Y}\\) and \\(\\boldsymbol{Q}_{X X}\\). Their moment estimators are the sample moments\n\\[\n\\begin{aligned}\n\\widehat{\\boldsymbol{Q}}_{X Y} &=\\frac{1}{n} \\sum_{i=1}^{n} X_{i} Y_{i} \\\\\n\\widehat{\\boldsymbol{Q}}_{X X} &=\\frac{1}{n} \\sum_{i=1}^{n} X_{i} X_{i}^{\\prime}\n\\end{aligned}\n\\]\nThe moment estimator of \\(\\beta\\) replaces the population moments in (3.1) with the sample moments:\n\\[\n\\begin{aligned}\n\\widehat{\\beta} &=\\widehat{\\boldsymbol{Q}}_{X X}^{-1} \\widehat{\\boldsymbol{Q}}_{X Y} \\\\\n&=\\left(\\frac{1}{n} \\sum_{i=1}^{n} X_{i} X_{i}^{\\prime}\\right)^{-1}\\left(\\frac{1}{n} \\sum_{i=1}^{n} X_{i} Y_{i}\\right) \\\\\n&=\\left(\\sum_{i=1}^{n} X_{i} X_{i}^{\\prime}\\right)^{-1}\\left(\\sum_{i=1}^{n} X_{i} Y_{i}\\right)\n\\end{aligned}\n\\]\nwhich is identical with (3.11).\nTechnically, the estimator \\(\\widehat{\\beta}\\) is unique and equals (3.11) only if the inverted matrix is actually invertible, which holds if (and only if) this matrix is positive definite. This excludes the case that \\(X_{i}\\) contains redundant regressors. This will be discussed further in Section 3.24.\nTheorem 3.1 If \\(\\sum_{i=1}^{n} X_{i} X_{i}^{\\prime}>0\\), the least squares estimator is unique and equals\n\\[\n\\widehat{\\beta}=\\left(\\sum_{i=1}^{n} X_{i} X_{i}^{\\prime}\\right)^{-1}\\left(\\sum_{i=1}^{n} X_{i} Y_{i}\\right) .\n\\]"
  },
  {
    "objectID": "chpt03-algebra.html#adrien-marie-legendre",
    "href": "chpt03-algebra.html#adrien-marie-legendre",
    "title": "3  The Algebra of Least Squares",
    "section": "3.7 Adrien-Marie Legendre",
    "text": "3.7 Adrien-Marie Legendre\nThe method of least squares was published in 1805 by the French mathematician Adrien-Marie Legendre (1752-1833). Legendre proposed least squares as a solution to the algebraic problem of solving a system of equations when the number of equations exceeded the number of unknowns. This was a vexing and common problem in astronomical measurement. As viewed by Legendre, (3.2) is a set of \\(n\\) equations with \\(k\\) unknowns. As the equations cannot be solved exactly, Legendre’s goal was to select \\(\\beta\\) to make the set of errors as small as possible. He proposed the sum of squared error criterion and derived the algebraic solution presented above. As he noted, the first-order conditions (3.9) is a system of \\(k\\) equations with \\(k\\) unknowns which can be solved by “ordinary” methods. Hence the method became known as Ordinary Least Squares and to this day we still use the abbreviation OLS to refer to Legendre’s estimation method."
  },
  {
    "objectID": "chpt03-algebra.html#illustration",
    "href": "chpt03-algebra.html#illustration",
    "title": "3  The Algebra of Least Squares",
    "section": "3.8 Illustration",
    "text": "3.8 Illustration\nWe illustrate the least squares estimator in practice with the data set used to calculate the estimates reported in Chapter 2. This is the March 2009 Current Population Survey, which has extensive information on the U.S. population. This data set is described in more detail in Section 3.22. For this illustration we use the sub-sample of married (spouse present) Black female wage earners with 12 years potential work experience. This sub-sample has 20 observations \\({ }^{2}\\).\nIn Table \\(3.1\\) we display the observations for reference. Each row is an individual observation which are the data for an individual person. The columns correspond to the variables (measurements) for the individuals. The second column is the reported wage (total annual earnings divided by hours worked). The third column is the natural logarithm of the wage. The fourth column is years of education. The fifth and six columns are further transformations, specifically the square of education and the product of education and \\(\\log\\) (wage). The bottom row are the sums of the elements in that column.\nTable 3.1: Observations From CPS Data Set\n\n\n\n\n\n\n\n\n\n\n\nObservation\nwage\n\\(\\log (\\) wage)\neducation\neducation \\(^{2}\\)\neducation \\(\\times \\log (\\) wage \\()\\)\n\n\n\n\n1\n\\(37.93\\)\n\\(3.64\\)\n18\n324\n\\(65.44\\)\n\n\n2\n\\(40.87\\)\n\\(3.71\\)\n18\n324\n\\(66.79\\)\n\n\n3\n\\(14.18\\)\n\\(2.65\\)\n13\n169\n\\(34.48\\)\n\n\n4\n\\(16.83\\)\n\\(2.82\\)\n16\n256\n\\(45.17\\)\n\n\n5\n\\(33.17\\)\n\\(3.50\\)\n16\n256\n\\(56.03\\)\n\n\n6\n\\(29.81\\)\n\\(3.39\\)\n18\n324\n\\(61.11\\)\n\n\n7\n\\(54.62\\)\n\\(4.00\\)\n16\n256\n\\(64.00\\)\n\n\n8\n\\(43.08\\)\n\\(3.76\\)\n18\n324\n\\(67.73\\)\n\n\n9\n\\(14.42\\)\n\\(2.67\\)\n12\n144\n\\(32.03\\)\n\n\n10\n\\(14.90\\)\n\\(2.70\\)\n16\n256\n\\(43.23\\)\n\n\n11\n\\(21.63\\)\n\\(3.07\\)\n18\n324\n\\(55.44\\)\n\n\n12\n\\(11.09\\)\n\\(2.41\\)\n16\n256\n\\(38.50\\)\n\n\n13\n\\(10.00\\)\n\\(2.30\\)\n13\n169\n\\(29.93\\)\n\n\n14\n\\(31.73\\)\n\\(3.46\\)\n14\n196\n\\(48.40\\)\n\n\n15\n\\(11.06\\)\n\\(2.40\\)\n12\n144\n\\(28.84\\)\n\n\n16\n\\(18.75\\)\n\\(2.93\\)\n16\n256\n\\(46.90\\)\n\n\n17\n\\(27.35\\)\n\\(3.31\\)\n14\n196\n\\(46.32\\)\n\n\n18\n\\(24.04\\)\n\\(3.18\\)\n16\n256\n\\(50.76\\)\n\n\n19\n\\(36.06\\)\n\\(3.59\\)\n18\n324\n\\(64.53\\)\n\n\n20\n\\(23.08\\)\n\\(3.14\\)\n16\n256\n\\(50.22\\)\n\n\nSum\n515\n\\(62.64\\)\n314\n5010\n\\(995.86\\)\n\n\n\nPutting the variables into the standard regression notation, let \\(Y_{i}\\) be \\(\\log (w a g e)\\) and \\(X_{i}\\) be years of education and an intercept. Then from the column sums in Table \\(3.1\\) we have\n\\[\n\\sum_{i=1}^{n} X_{i} Y_{i}=\\left(\\begin{array}{c}\n995.86 \\\\\n62.64\n\\end{array}\\right)\n\\]\nand\n\\[\n\\sum_{i=1}^{n} X_{i} X_{i}^{\\prime}=\\left(\\begin{array}{cc}\n5010 & 314 \\\\\n314 & 20\n\\end{array}\\right)\n\\]\nTaking the inverse we obtain\n\\[\n\\left(\\sum_{i=1}^{n} X_{i} X_{i}^{\\prime}\\right)^{-1}=\\left(\\begin{array}{cc}\n0.0125 & -0.196 \\\\\n-0.196 & 3.124\n\\end{array}\\right) .\n\\]\n\\({ }^{2}\\) This sample was selected specifically so that it has a small number of observations, facilitating exposition. Thus by matrix multiplication\n\\[\n\\widehat{\\beta}=\\left(\\begin{array}{cc}\n0.0125 & -0.196 \\\\\n-0.196 & 3.124\n\\end{array}\\right)\\left(\\begin{array}{c}\n995.86 \\\\\n62.64\n\\end{array}\\right)=\\left(\\begin{array}{c}\n0.155 \\\\\n0.698\n\\end{array}\\right) .\n\\]\nIn practice the regression estimates \\(\\widehat{\\beta}\\) are computed by computer software without the user taking the explicit steps listed above. However, it is useful to understand that the least squares estimator can be calculated by simple algebraic operations. If your data is in a spreadsheet similar to Table 3.1, then the listed transformations (logarithm, squares, cross-products, column sums) can be computed by spreadsheet operations. \\(\\widehat{\\beta}\\) could then be calculated by matrix inversion and multiplication. Once again, this is rarely done by applied economists because computer software is available to ease the process.\nWe often write the estimated equation using the format\n\\[\n\\widehat{\\log (\\text { wage })}=0.155 \\text { education }+0.698 \\text {. }\n\\]\nAn interpretation of the estimated equation is that each year of education is associated with a \\(16 %\\) increase in mean wages.\nAnother use of the estimated equation (3.12) is for prediction. Suppose one individual has 12 years of education and a second has 16. Using (3.12) we find that the first’s expected log wage is\n\\[\n\\widehat{\\log (\\text { wag } e)}=0.155 \\times 12+0.698=2.56\n\\]\nand for the second\n\\[\n\\widehat{\\log (\\text { wage })}=0.155 \\times 16+0.698=3.18 .\n\\]\nEquation (3.12) is called a bivariate regression as there are two variables. It is also called a simple regression as there is a single regressor. A multiple regression has two or more regressors and allows a more detailed investigation. Let’s take an example similar to (3.12) but include all levels of experience. This time we use the sub-sample of single (never married) Asian men which has 268 observations. Including as regressors years of potential work experience (experience) and its square (experience \\({ }^{2} / 100\\) ) (we divide by 100 to simplify reporting) we obtain the estimates\n\\[\n\\widehat{\\log (\\text { wage })}=0.143 \\text { education }+0.036 \\text { experience }-0.071 \\text { experience }^{2} / 100+0.575 \\text {. }\n\\]\nThese estimates suggest a \\(14 %\\) increase in mean wages per year of education holding experience constant."
  },
  {
    "objectID": "chpt03-algebra.html#least-squares-residuals",
    "href": "chpt03-algebra.html#least-squares-residuals",
    "title": "3  The Algebra of Least Squares",
    "section": "3.9 Least Squares Residuals",
    "text": "3.9 Least Squares Residuals\nAs a by-product of estimation we define the fitted value \\(\\widehat{Y}_{i}=X_{i}^{\\prime} \\widehat{\\beta}\\) and the residual\n\\[\n\\widehat{e}_{i}=Y_{i}-\\widehat{Y}_{i}=Y_{i}-X_{i}^{\\prime} \\widehat{\\beta}\n\\]\nSometimes \\(\\widehat{Y}_{i}\\) is called the predicted value but this is a misleading label. The fitted value \\(\\widehat{Y}_{i}\\) is a function of the entire sample including \\(Y_{i}\\), and thus cannot be interpreted as a valid prediction of \\(Y_{i}\\). It is thus more accurate to describe \\(\\widehat{Y}_{i}\\) as a fitted rather than a predicted value.\nNote that \\(Y_{i}=\\widehat{Y}_{i}+\\widehat{e}_{i}\\) and\n\\[\nY_{i}=X_{i}^{\\prime} \\widehat{\\beta}+\\widehat{e}_{i} .\n\\]\nWe make a distinction between the error \\(e_{i}\\) and the residual \\(\\widehat{e}_{i}\\). The error \\(e_{i}\\) is unobservable while the residual \\(\\widehat{e}_{i}\\) is an estimator. These two variables are frequently mislabeled which can cause confusion. Equation (3.9) implies that\n\\[\n\\sum_{i=1}^{n} X_{i} \\widehat{e}_{i}=0 .\n\\]\nTo see this by a direct calculation, using (3.14) and (3.11),\n\\[\n\\begin{aligned}\n\\sum_{i=1}^{n} X_{i} \\widehat{e}_{i} &=\\sum_{i=1}^{n} X_{i}\\left(Y_{i}-X_{i}^{\\prime} \\widehat{\\beta}\\right) \\\\\n&=\\sum_{i=1}^{n} X_{i} Y_{i}-\\sum_{i=1}^{n} X_{i} X_{i}^{\\prime} \\widehat{\\beta} \\\\\n&=\\sum_{i=1}^{n} X_{i} Y_{i}-\\sum_{i=1}^{n} X_{i} X_{i}^{\\prime}\\left(\\sum_{i=1}^{n} X_{i} X_{i}^{\\prime}\\right)^{-1}\\left(\\sum_{i=1}^{n} X_{i} Y_{i}\\right) \\\\\n&=\\sum_{i=1}^{n} X_{i} Y_{i}-\\sum_{i=1}^{n} X_{i} Y_{i}=0 .\n\\end{aligned}\n\\]\nWhen \\(X_{i}\\) contains a constant an implication of (3.16) is\n\\[\n\\frac{1}{n} \\sum_{i=1}^{n} \\widehat{e}_{i}=0 .\n\\]\nThus the residuals have a sample mean of zero and the sample correlation between the regressors and the residual is zero. These are algebraic results and hold true for all linear regression estimates."
  },
  {
    "objectID": "chpt03-algebra.html#demeaned-regressors",
    "href": "chpt03-algebra.html#demeaned-regressors",
    "title": "3  The Algebra of Least Squares",
    "section": "3.10 Demeaned Regressors",
    "text": "3.10 Demeaned Regressors\nSometimes it is useful to separate the constant from the other regressors and write the linear projection equation in the format\n\\[\nY_{i}=X_{i}^{\\prime} \\beta+\\alpha+e_{i}\n\\]\nwhere \\(\\alpha\\) is the intercept and \\(X_{i}\\) does not contain a constant. The least squares estimates and residuals can be written as \\(Y_{i}=X_{i}^{\\prime} \\widehat{\\beta}+\\widehat{\\alpha}+\\widehat{e}_{i}\\).\nIn this case (3.16) can be written as the equation system\n\\[\n\\begin{array}{r}\n\\sum_{i=1}^{n}\\left(Y_{i}-X_{i}^{\\prime} \\widehat{\\beta}-\\widehat{\\alpha}\\right)=0 \\\\\n\\sum_{i=1}^{n} X_{i}\\left(Y_{i}-X_{i}^{\\prime} \\widehat{\\beta}-\\widehat{\\alpha}\\right)=0 .\n\\end{array}\n\\]\nThe first equation implies\n\\[\n\\widehat{\\alpha}=\\bar{Y}-\\bar{X}^{\\prime} \\widehat{\\beta} .\n\\]\nSubtracting from the second we obtain\n\\[\n\\sum_{i=1}^{n} X_{i}\\left(\\left(Y_{i}-\\bar{Y}\\right)-\\left(X_{i}-\\bar{X}\\right)^{\\prime} \\widehat{\\beta}\\right)=0 .\n\\]\nSolving for \\(\\widehat{\\beta}\\) we find\n\\[\n\\begin{aligned}\n\\widehat{\\beta} &=\\left(\\sum_{i=1}^{n} X_{i}\\left(X_{i}-\\bar{X}\\right)^{\\prime}\\right)^{-1}\\left(\\sum_{i=1}^{n} X_{i}\\left(Y_{i}-\\bar{Y}\\right)\\right) \\\\\n&=\\left(\\sum_{i=1}^{n}\\left(X_{i}-\\bar{X}\\right)\\left(X_{i}-\\bar{X}\\right)^{\\prime}\\right)^{-1}\\left(\\sum_{i=1}^{n}\\left(X_{i}-\\bar{X}\\right)\\left(Y_{i}-\\bar{Y}\\right)\\right) .\n\\end{aligned}\n\\]\nThus the OLS estimator for the slope coefficients is OLS with demeaned data and no intercept.\nThe representation (3.18) is known as the demeaned formula for the least squares estimator."
  },
  {
    "objectID": "chpt03-algebra.html#model-in-matrix-notation",
    "href": "chpt03-algebra.html#model-in-matrix-notation",
    "title": "3  The Algebra of Least Squares",
    "section": "3.11 Model in Matrix Notation",
    "text": "3.11 Model in Matrix Notation\nFor many purposes, including computation, it is convenient to write the model and statistics in matrix notation. The \\(n\\) linear equations \\(Y_{i}=X_{i}^{\\prime} \\beta+e_{i}\\) make a system of \\(n\\) equations. We can stack these \\(n\\) equations together as\n\\[\n\\begin{aligned}\n&Y_{1}=X_{1}^{\\prime} \\beta+e_{1} \\\\\n&Y_{2}=X_{2}^{\\prime} \\beta+e_{2} \\\\\n&\\vdots \\\\\n&Y_{n}=X_{n}^{\\prime} \\beta+e_{n} .\n\\end{aligned}\n\\]\nDefine\n\\[\n\\boldsymbol{Y}=\\left(\\begin{array}{c}\nY_{1} \\\\\nY_{2} \\\\\n\\vdots \\\\\nY_{n}\n\\end{array}\\right), \\quad \\boldsymbol{X}=\\left(\\begin{array}{c}\nX_{1}^{\\prime} \\\\\nX_{2}^{\\prime} \\\\\n\\vdots \\\\\nX_{n}^{\\prime}\n\\end{array}\\right), \\quad \\boldsymbol{e}=\\left(\\begin{array}{c}\ne_{1} \\\\\ne_{2} \\\\\n\\vdots \\\\\ne_{n}\n\\end{array}\\right)\n\\]\nObserve that \\(\\boldsymbol{Y}\\) and \\(\\boldsymbol{e}\\) are \\(n \\times 1\\) vectors and \\(\\boldsymbol{X}\\) is an \\(n \\times k\\) matrix. The system of \\(n\\) equations can be compactly written in the single equation\n\\[\n\\boldsymbol{Y}=\\boldsymbol{X} \\beta+\\boldsymbol{e} .\n\\]\nSample sums can be written in matrix notation. For example\n\\[\n\\begin{aligned}\n&\\sum_{i=1}^{n} X_{i} X_{i}^{\\prime}=\\boldsymbol{X}^{\\prime} \\boldsymbol{X} \\\\\n&\\sum_{i=1}^{n} X_{i} Y_{i}=\\boldsymbol{X}^{\\prime} \\boldsymbol{Y} .\n\\end{aligned}\n\\]\nTherefore the least squares estimator can be written as\n\\[\n\\widehat{\\beta}=\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{Y}\\right) .\n\\]\nThe matrix version of (3.15) and estimated version of (3.19) is\n\\[\n\\boldsymbol{Y}=\\boldsymbol{X} \\widehat{\\beta}+\\widehat{\\boldsymbol{e}} .\n\\]\nEquivalently the residual vector is\n\\[\n\\widehat{\\boldsymbol{e}}=\\boldsymbol{Y}-\\boldsymbol{X} \\widehat{\\beta}\n\\]\nUsing the residual vector we can write (3.16) as\n\\[\n\\boldsymbol{X}^{\\prime} \\widehat{\\boldsymbol{e}}=0\n\\]\nIt can also be useful to write the sum of squared error criterion as\n\\[\n\\operatorname{SSE}(\\beta)=(\\boldsymbol{Y}-\\boldsymbol{X} \\beta)^{\\prime}(\\boldsymbol{Y}-\\boldsymbol{X} \\beta) .\n\\]\nUsing matrix notation we have simple expressions for most estimators. This is particularly convenient for computer programming as most languages allow matrix notation and manipulation. Theorem 3.2 Important Matrix Expressions\n\\[\n\\begin{aligned}\n\\widehat{\\beta} &=\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{Y}\\right) \\\\\n\\widehat{\\boldsymbol{e}} &=\\boldsymbol{Y}-\\boldsymbol{X} \\widehat{\\beta} \\\\\n\\boldsymbol{X}^{\\prime} \\widehat{\\boldsymbol{e}} &=0 .\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "chpt03-algebra.html#early-use-of-matrices",
    "href": "chpt03-algebra.html#early-use-of-matrices",
    "title": "3  The Algebra of Least Squares",
    "section": "3.12 Early Use of Matrices",
    "text": "3.12 Early Use of Matrices\nThe earliest known treatment of the use of matrix methods to solve simultaneous systems is found in Chapter 8 of the Chinese text The Nine Chapters on the Mathematical Art, written by several generations of scholars from the \\(10^{\\text {th }}\\) to \\(2^{\\text {nd }}\\) century BCE."
  },
  {
    "objectID": "chpt03-algebra.html#projection-matrix",
    "href": "chpt03-algebra.html#projection-matrix",
    "title": "3  The Algebra of Least Squares",
    "section": "3.13 Projection Matrix",
    "text": "3.13 Projection Matrix\nDefine the matrix\n\\[\n\\boldsymbol{P}=\\boldsymbol{X}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\boldsymbol{X}^{\\prime}\n\\]\nObserve that\n\\[\n\\boldsymbol{P} \\boldsymbol{X}=\\boldsymbol{X}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\boldsymbol{X}^{\\prime} \\boldsymbol{X}=\\boldsymbol{X} .\n\\]\nThis is a property of a projection matrix. More generally, for any matrix \\(\\boldsymbol{Z}\\) which can be written as \\(\\boldsymbol{Z}=\\boldsymbol{X} \\boldsymbol{\\Gamma}\\) for some matrix \\(\\Gamma\\) (we say that \\(\\boldsymbol{Z}\\) lies in the range space of \\(\\boldsymbol{X}\\) ), then\n\\[\n\\boldsymbol{P Z}=\\boldsymbol{P} \\boldsymbol{X} \\boldsymbol{\\Gamma}=\\boldsymbol{X}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\boldsymbol{X}^{\\prime} \\boldsymbol{X} \\boldsymbol{\\Gamma}=\\boldsymbol{X} \\boldsymbol{\\Gamma}=\\boldsymbol{Z} .\n\\]\nAs an important example, if we partition the matrix \\(\\boldsymbol{X}\\) into two matrices \\(\\boldsymbol{X}_{1}\\) and \\(\\boldsymbol{X}_{2}\\) so that \\(\\boldsymbol{X}=\\) \\(\\left[\\begin{array}{ll}\\boldsymbol{X}_{1} & \\boldsymbol{X}_{2}\\end{array}\\right]\\) then \\(\\boldsymbol{P} \\boldsymbol{X}_{1}=\\boldsymbol{X}_{1}\\). (See Exercise 3.7.)\nThe projection matrix \\(\\boldsymbol{P}\\) has the algebraic property that it is idempotent: \\(\\boldsymbol{P} \\boldsymbol{P}=\\boldsymbol{P}\\). See Theorem 3.3.2 below. For the general properties of projection matrices see Section A.11.\nThe matrix \\(\\boldsymbol{P}\\) creates the fitted values in a least squares regression:\n\\[\n\\boldsymbol{P} \\boldsymbol{Y}=\\boldsymbol{X}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\boldsymbol{X}^{\\prime} \\boldsymbol{Y}=\\boldsymbol{X} \\widehat{\\boldsymbol{\\beta}}=\\widehat{\\boldsymbol{Y}} \\text {. }\n\\]\nBecause of this property \\(\\boldsymbol{P}\\) is also known as the hat matrix.\nA special example of a projection matrix occurs when \\(X=\\mathbf{1}_{n}\\) is an \\(n\\)-vector of ones. Then\n\\[\n\\boldsymbol{P}=\\mathbf{1}_{n}\\left(\\mathbf{1}_{n}^{\\prime} \\mathbf{1}_{n}\\right)^{-1} \\mathbf{1}_{n}^{\\prime}=\\frac{1}{n} \\mathbf{1}_{n} \\mathbf{1}_{n}^{\\prime} .\n\\]\nNote that in this case\n\\[\n\\boldsymbol{P} \\boldsymbol{Y}=\\mathbf{1}_{n}\\left(\\mathbf{1}_{n}^{\\prime} \\mathbf{1}_{n}\\right)^{-1} \\mathbf{1}_{n}^{\\prime} \\boldsymbol{Y}=\\mathbf{1}_{n} \\bar{Y}\n\\]\ncreates an \\(n\\)-vector whose elements are the sample mean \\(\\bar{Y}\\).\nThe projection matrix \\(\\boldsymbol{P}\\) appears frequently in algebraic manipulations in least squares regression. The matrix has the following important properties. Theorem 3.3 The projection matrix \\(\\boldsymbol{P}=\\boldsymbol{X}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\boldsymbol{X}^{\\prime}\\) for any \\(n \\times k \\boldsymbol{X}\\) with \\(n \\geq\\) \\(k\\) has the following algebraic properties.\n\n\\(\\boldsymbol{P}\\) is symmetric \\(\\left(\\boldsymbol{P}^{\\prime}=\\boldsymbol{P}\\right)\\).\n\\(\\boldsymbol{P}\\) is idempotent \\((\\boldsymbol{P P}=\\boldsymbol{P})\\).\n\\(\\operatorname{tr} \\boldsymbol{P}=k\\).\nThe eigenvalues of \\(\\boldsymbol{P}\\) are 1 and 0 .\n\\(\\boldsymbol{P}\\) has \\(k\\) eigenvalues equalling 1 and \\(n-k\\) equalling 0 .\n\\(\\operatorname{rank}(\\boldsymbol{P})=k\\).\n\nWe close this section by proving the claims in Theorem 3.3. Part 1 holds because\n\\[\n\\begin{aligned}\n\\boldsymbol{P}^{\\prime} &=\\left(\\boldsymbol{X}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\boldsymbol{X}^{\\prime}\\right)^{\\prime} \\\\\n&=\\left(\\boldsymbol{X}^{\\prime}\\right)^{\\prime}\\left(\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1}\\right)^{\\prime}(\\boldsymbol{X})^{\\prime} \\\\\n&=\\boldsymbol{X}\\left(\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{\\prime}\\right)^{-1} \\boldsymbol{X}^{\\prime} \\\\\n&=\\boldsymbol{X}\\left((\\boldsymbol{X})^{\\prime}\\left(\\boldsymbol{X}^{\\prime}\\right)^{\\prime}\\right)^{-1} \\boldsymbol{X}^{\\prime}=\\boldsymbol{P} .\n\\end{aligned}\n\\]\nTo establish part 2, the fact that \\(\\boldsymbol{P X}=\\boldsymbol{X}\\) implies that\n\\[\n\\boldsymbol{P} \\boldsymbol{P}=\\boldsymbol{P} \\boldsymbol{X}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\boldsymbol{X}^{\\prime}=\\boldsymbol{X}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\boldsymbol{X}^{\\prime}=\\boldsymbol{P}\n\\]\nas claimed. For part 3 ,\n\\[\n\\operatorname{tr} \\boldsymbol{P}=\\operatorname{tr}\\left(\\boldsymbol{X}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\boldsymbol{X}^{\\prime}\\right)=\\operatorname{tr}\\left(\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)=\\operatorname{tr}\\left(\\boldsymbol{I}_{k}\\right)=k .\n\\]\nSee Appendix A.5 for definition and properties of the trace operator.\nAppendix A.11 shows that part 4 holds for any idempotent matrix. For part 5, since \\(\\operatorname{tr} \\boldsymbol{P}\\) equals the sum of the \\(n\\) eigenvalues and \\(\\operatorname{tr} \\boldsymbol{P}=k\\) by part 3, it follows that there are \\(k\\) eigenvalues equalling 1 and the remainder \\(n-k\\) equalling 0 .\nFor part 6, observe that \\(\\boldsymbol{P}\\) is positive semi-definite because its eigenvalues are all non-negative. By Theorem A.4.5 its rank equals the number of positive eigenvalues, which is \\(k\\) as claimed."
  },
  {
    "objectID": "chpt03-algebra.html#annihilator-matrix",
    "href": "chpt03-algebra.html#annihilator-matrix",
    "title": "3  The Algebra of Least Squares",
    "section": "3.14 Annihilator Matrix",
    "text": "3.14 Annihilator Matrix\nDefine\n\\[\n\\boldsymbol{M}=\\boldsymbol{I}_{n}-\\boldsymbol{P}=\\boldsymbol{I}_{n}-\\boldsymbol{X}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\boldsymbol{X}^{\\prime}\n\\]\nwhere \\(\\boldsymbol{I}_{n}\\) is the \\(n \\times n\\) identity matrix. Note that\n\\[\n\\boldsymbol{M} \\boldsymbol{X}=\\left(\\boldsymbol{I}_{n}-\\boldsymbol{P}\\right) \\boldsymbol{X}=\\boldsymbol{X}-\\boldsymbol{P} \\boldsymbol{X}=\\boldsymbol{X}-\\boldsymbol{X}=0 .\n\\]\nThus \\(\\boldsymbol{M}\\) and \\(\\boldsymbol{X}\\) are orthogonal. We call \\(\\boldsymbol{M}\\) the annihilator matrix due to the property that for any matrix \\(\\boldsymbol{Z}\\) in the range space of \\(\\boldsymbol{X}\\) then\n\\[\nM Z=Z-P Z=0 .\n\\]\nFor example, \\(\\boldsymbol{M} \\boldsymbol{X}_{1}=0\\) for any subcomponent \\(\\boldsymbol{X}_{1}\\) of \\(\\boldsymbol{X}\\), and \\(\\boldsymbol{M P}=0\\) (see Exercise 3.7).\nThe annihilator matrix \\(\\boldsymbol{M}\\) has similar properties with \\(\\boldsymbol{P}\\), including that \\(\\boldsymbol{M}\\) is symmetric \\(\\left(\\boldsymbol{M}^{\\prime}=\\boldsymbol{M}\\right)\\) and idempotent \\((\\boldsymbol{M} M=\\boldsymbol{M})\\). It is thus a projection matrix. Similarly to Theorem 3.3.3 we can calculate\n\\[\n\\operatorname{tr} M=n-k .\n\\]\n(See Exercise 3.9.) One implication is that the rank of \\(\\boldsymbol{M}\\) is \\(n-k\\).\nWhile \\(\\boldsymbol{P}\\) creates fitted values, \\(\\boldsymbol{M}\\) creates least squares residuals:\n\\[\nM Y=Y-P Y=Y-X \\widehat{\\beta}=\\widehat{\\boldsymbol{e}} .\n\\]\nAs discussed in the previous section, a special example of a projection matrix occurs when \\(\\boldsymbol{X}=\\mathbf{1}_{n}\\) is an \\(n\\)-vector of ones, so that \\(\\boldsymbol{P}=\\mathbf{1}_{n}\\left(\\mathbf{1}_{n}^{\\prime} \\mathbf{1}_{n}\\right)^{-1} \\mathbf{1}_{n}^{\\prime}\\). The associated annihilator matrix is\n\\[\n\\boldsymbol{M}=\\boldsymbol{I}_{n}-\\boldsymbol{P}=\\boldsymbol{I}_{n}-\\mathbf{1}_{n}\\left(\\mathbf{1}_{n}^{\\prime} \\mathbf{1}_{n}\\right)^{-1} \\mathbf{1}_{n}^{\\prime} .\n\\]\nWhile \\(\\boldsymbol{P}\\) creates a vector of sample means, \\(\\boldsymbol{M}\\) creates demeaned values:\n\\[\n\\boldsymbol{M Y}=\\boldsymbol{Y}-\\mathbf{1}_{n} \\bar{Y} .\n\\]\nFor simplicity we will often write the right-hand-side as \\(Y-\\bar{Y}\\). The \\(i^{t h}\\) element is \\(Y_{i}-\\bar{Y}\\), the demeaned value of \\(Y_{i}\\)\nWe can also use (3.23) to write an alternative expression for the residual vector. Substituting \\(\\boldsymbol{Y}=\\) \\(\\boldsymbol{X} \\beta+\\boldsymbol{e}\\) into \\(\\widehat{\\boldsymbol{e}}=\\boldsymbol{M} \\boldsymbol{Y}\\) and using \\(\\boldsymbol{M} \\boldsymbol{X}=\\mathbf{0}\\) we find\n\\[\n\\widehat{\\boldsymbol{e}}=\\boldsymbol{M} \\boldsymbol{Y}=\\boldsymbol{M}(\\boldsymbol{X} \\beta+\\boldsymbol{e})=\\boldsymbol{M} \\boldsymbol{e}\n\\]\nwhich is free of dependence on the regression coefficient \\(\\beta\\)."
  },
  {
    "objectID": "chpt03-algebra.html#estimation-of-error-variance",
    "href": "chpt03-algebra.html#estimation-of-error-variance",
    "title": "3  The Algebra of Least Squares",
    "section": "3.15 Estimation of Error Variance",
    "text": "3.15 Estimation of Error Variance\nThe error variance \\(\\sigma^{2}=\\mathbb{E}\\left[e^{2}\\right]\\) is a moment, so a natural estimator is a moment estimator. If \\(e_{i}\\) were observed we would estimate \\(\\sigma^{2}\\) by\n\\[\n\\widetilde{\\sigma}^{2}=\\frac{1}{n} \\sum_{i=1}^{n} e_{i}^{2} .\n\\]\nHowever, this is infeasible as \\(e_{i}\\) is not observed. In this case it is common to take a two-step approach to estimation. The residuals \\(\\widehat{e}_{i}\\) are calculated in the first step, and then we substitute \\(\\widehat{e}_{i}\\) for \\(e_{i}\\) in expression (3.25) to obtain the feasible estimator\n\\[\n\\widehat{\\sigma}^{2}=\\frac{1}{n} \\sum_{i=1}^{n} \\widehat{e}_{i}^{2} .\n\\]\nIn matrix notation, we can write (3.25) and (3.26) as \\(\\widetilde{\\sigma}^{2}=n^{-1} \\boldsymbol{e}^{\\prime} \\boldsymbol{e}\\) and\n\\[\n\\widehat{\\sigma}^{2}=n^{-1} \\widehat{\\boldsymbol{e}}^{\\prime} \\widehat{\\boldsymbol{e}} .\n\\]\nRecall the expressions \\(\\widehat{\\boldsymbol{e}}=\\boldsymbol{M} \\boldsymbol{Y}=\\boldsymbol{M} \\boldsymbol{e}\\) from (3.23) and (3.24). Applied to (3.27) we find\n\\[\n\\widehat{\\sigma}^{2}=n^{-1} \\widehat{\\boldsymbol{e}}^{\\prime} \\widehat{\\boldsymbol{e}}=n^{-1} \\boldsymbol{e}^{\\prime} \\boldsymbol{M M} \\boldsymbol{M}=n^{-1} \\boldsymbol{e}^{\\prime} \\boldsymbol{M} \\boldsymbol{e}\n\\]\nthe third equality because \\(M M=M\\).\nAn interesting implication is that\n\\[\n\\widetilde{\\sigma}^{2}-\\widehat{\\sigma}^{2}=n^{-1} \\boldsymbol{e}^{\\prime} \\boldsymbol{e}-n^{-1} \\boldsymbol{e}^{\\prime} \\boldsymbol{M} \\boldsymbol{e}=n^{-1} \\boldsymbol{e}^{\\prime} \\boldsymbol{P} \\boldsymbol{e} \\geq 0 .\n\\]\nThe final inequality holds because \\(\\boldsymbol{P}\\) is positive semi-definite and \\(\\boldsymbol{e}^{\\prime} \\boldsymbol{P} \\boldsymbol{e}\\) is a quadratic form. This shows that the feasible estimator \\(\\widehat{\\sigma}^{2}\\) is numerically smaller than the idealized estimator (3.25)."
  },
  {
    "objectID": "chpt03-algebra.html#analysis-of-variance",
    "href": "chpt03-algebra.html#analysis-of-variance",
    "title": "3  The Algebra of Least Squares",
    "section": "3.16 Analysis of Variance",
    "text": "3.16 Analysis of Variance\nAnother way of writing (3.23) is\n\\[\n\\boldsymbol{Y}=\\boldsymbol{P} \\boldsymbol{Y}+\\boldsymbol{M} \\boldsymbol{Y}=\\widehat{\\boldsymbol{Y}}+\\widehat{\\boldsymbol{e}} .\n\\]\nThis decomposition is orthogonal, that is\n\\[\n\\widehat{\\boldsymbol{Y}}^{\\prime} \\widehat{\\boldsymbol{e}}=(\\boldsymbol{P} \\boldsymbol{Y})^{\\prime}(\\boldsymbol{M} \\boldsymbol{Y})=\\boldsymbol{Y}^{\\prime} \\boldsymbol{P} \\boldsymbol{M} \\boldsymbol{Y}=0 .\n\\]\nIt follows that\n\\[\n\\boldsymbol{Y}^{\\prime} \\boldsymbol{Y}=\\widehat{\\boldsymbol{Y}}^{\\prime} \\widehat{\\boldsymbol{Y}}+2 \\widehat{\\boldsymbol{Y}}^{\\prime} \\widehat{\\boldsymbol{e}}+\\widehat{\\boldsymbol{e}}^{\\prime} \\widehat{\\boldsymbol{e}}=\\widehat{\\boldsymbol{Y}}^{\\prime} \\widehat{\\boldsymbol{Y}}+\\widehat{\\boldsymbol{e}}^{\\prime} \\widehat{\\boldsymbol{e}}\n\\]\nor\n\\[\n\\sum_{i=1}^{n} Y_{i}^{2}=\\sum_{i=1}^{n} \\widehat{Y}_{i}^{2}+\\sum_{i=1}^{n} \\widehat{e}_{i}^{2}\n\\]\nSubtracting \\(\\bar{Y}\\) from both sides of (3.29) we obtain\n\\[\n\\boldsymbol{Y}-\\mathbf{1}_{n} \\bar{Y}=\\widehat{\\boldsymbol{Y}}-\\mathbf{1}_{n} \\bar{Y}+\\widehat{\\boldsymbol{e}}\n\\]\nThis decomposition is also orthogonal when \\(X\\) contains a constant, as\n\\[\n\\left(\\widehat{\\boldsymbol{Y}}-\\mathbf{1}_{n} \\bar{Y}\\right)^{\\prime} \\widehat{\\boldsymbol{e}}=\\widehat{\\boldsymbol{Y}}^{\\prime} \\widehat{\\boldsymbol{e}}-\\bar{Y} \\mathbf{1}_{n}^{\\prime} \\widehat{\\boldsymbol{e}}=0\n\\]\nunder (3.17). It follows that\n\\[\n\\left(\\boldsymbol{Y}-\\mathbf{1}_{n} \\bar{Y}\\right)^{\\prime}\\left(\\boldsymbol{Y}-\\mathbf{1}_{n} \\bar{Y}\\right)=\\left(\\widehat{\\boldsymbol{Y}}-\\mathbf{1}_{n} \\bar{Y}\\right)^{\\prime}\\left(\\widehat{\\boldsymbol{Y}}-\\mathbf{1}_{n} \\bar{Y}\\right)+\\widehat{\\boldsymbol{e}}^{\\prime} \\widehat{\\boldsymbol{e}}\n\\]\nor\n\\[\n\\sum_{i=1}^{n}\\left(Y_{i}-\\bar{Y}\\right)^{2}=\\sum_{i=1}^{n}\\left(\\widehat{Y}_{i}-\\bar{Y}\\right)^{2}+\\sum_{i=1}^{n} \\widehat{e}_{i}^{2} .\n\\]\nThis is commonly called the analysis-of-variance formula for least squares regression.\nA commonly reported statistic is the coefficient of determination or R-squared:\n\\[\nR^{2}=\\frac{\\sum_{i=1}^{n}\\left(\\widehat{Y}_{i}-\\bar{Y}\\right)^{2}}{\\sum_{i=1}^{n}\\left(Y_{i}-\\bar{Y}\\right)^{2}}=1-\\frac{\\sum_{i=1}^{n} \\widehat{e}_{i}^{2}}{\\sum_{i=1}^{n}\\left(Y_{i}-\\bar{Y}\\right)^{2}} .\n\\]\nIt is often described as “the fraction of the sample variance of \\(Y\\) which is explained by the least squares fit”. \\(R^{2}\\) is a crude measure of regression fit. We have better measures of fit, but these require a statistical (not just algebraic) analysis and we will return to these issues later. One deficiency with \\(R^{2}\\) is that it increases when regressors are added to a regression (see Exercise 3.16) so the “fit” can be always increased by increasing the number of regressors.\nThe coefficient of determination was introduced by Wright (1921)."
  },
  {
    "objectID": "chpt03-algebra.html#projections",
    "href": "chpt03-algebra.html#projections",
    "title": "3  The Algebra of Least Squares",
    "section": "3.17 Projections",
    "text": "3.17 Projections\nOne way to visualize least squares fitting is as a projection operation.\nWrite the regressor matrix as \\(\\boldsymbol{X}=\\left[\\boldsymbol{X}_{1} \\boldsymbol{X}_{2} \\ldots \\boldsymbol{X}_{k}\\right]\\) where \\(\\boldsymbol{X}_{j}\\) is the \\(j^{t h}\\) column of \\(\\boldsymbol{X}\\). The range space \\(\\mathscr{R}(\\boldsymbol{X})\\) of \\(\\boldsymbol{X}\\) is the space consisting of all linear combinations of the columns \\(\\boldsymbol{X}_{1}, \\boldsymbol{X}_{2}, \\ldots, \\boldsymbol{X}_{k} . \\mathscr{R}(\\boldsymbol{X})\\) is a \\(k\\) dimensional surface contained in \\(\\mathbb{R}^{n}\\). If \\(k=2\\) then \\(\\mathscr{R}(\\boldsymbol{X})\\) is a plane. The operator \\(\\boldsymbol{P}=\\boldsymbol{X}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\boldsymbol{X}^{\\prime}\\) projects vectors onto \\(\\mathscr{R}(\\boldsymbol{X})\\). The fitted values \\(\\widehat{\\boldsymbol{Y}}=\\boldsymbol{P} \\boldsymbol{Y}\\) are the projection of \\(\\boldsymbol{Y}\\) onto \\(\\mathscr{R}(\\boldsymbol{X})\\).\nTo visualize examine Figure 3.4. This displays the case \\(n=3\\) and \\(k=2\\). Displayed are three vectors \\(\\boldsymbol{Y}, \\boldsymbol{X}_{1}\\), and \\(\\boldsymbol{X}_{2}\\), which are each elements of \\(\\mathbb{R}^{3}\\). The plane created by \\(\\boldsymbol{X}_{1}\\) and \\(\\boldsymbol{X}_{2}\\) is the range space \\(\\mathscr{R}(\\boldsymbol{X})\\). Regression fitted values are linear combinations of \\(\\boldsymbol{X}_{1}\\) and \\(\\boldsymbol{X}_{2}\\) and so lie on this plane. The fitted value \\(\\widehat{\\boldsymbol{Y}}\\) is the vector on this plane closest to \\(\\boldsymbol{Y}\\). The residual \\(\\widehat{\\boldsymbol{e}}=\\boldsymbol{Y}-\\widehat{\\boldsymbol{Y}}\\) is the difference between the two. The angle between the vectors \\(\\widehat{\\boldsymbol{Y}}\\) and \\(\\widehat{\\boldsymbol{e}}\\) is \\(90^{\\circ}\\), and therefore they are orthogonal as shown.\n\nFigure 3.4: Projection of \\(\\boldsymbol{Y}\\) onto \\(\\boldsymbol{X}_{1}\\) and \\(\\boldsymbol{X}_{2}\\)"
  },
  {
    "objectID": "chpt03-algebra.html#regression-components",
    "href": "chpt03-algebra.html#regression-components",
    "title": "3  The Algebra of Least Squares",
    "section": "3.18 Regression Components",
    "text": "3.18 Regression Components\nPartition \\(\\boldsymbol{X}=\\left[\\begin{array}{ll}\\boldsymbol{X}_{1} & \\boldsymbol{X}_{2}\\end{array}\\right]\\) and \\(\\beta=\\left(\\beta_{1}, \\beta_{2}\\right)\\). The regression model can be written as\n\\[\n\\boldsymbol{Y}=\\boldsymbol{X}_{1} \\beta_{1}+\\boldsymbol{X}_{2} \\beta_{2}+\\boldsymbol{e} .\n\\]\nThe OLS estimator of \\(\\beta=\\left(\\beta_{1}^{\\prime}, \\beta_{2}^{\\prime}\\right)^{\\prime}\\) is obtained by regression of \\(\\boldsymbol{Y}\\) on \\(\\boldsymbol{X}=\\left[\\boldsymbol{X}_{1} \\boldsymbol{X}_{2}\\right]\\) and can be written as\n\\[\n\\boldsymbol{Y}=\\boldsymbol{X} \\widehat{\\beta}+\\widehat{\\boldsymbol{e}}=\\boldsymbol{X}_{1} \\widehat{\\boldsymbol{\\beta}}_{1}+\\boldsymbol{X}_{2} \\widehat{\\boldsymbol{\\beta}}_{2}+\\widehat{\\boldsymbol{e}} .\n\\]\nWe are interested in algebraic expressions for \\(\\widehat{\\beta}_{1}\\) and \\(\\widehat{\\beta}_{2}\\).\nLet’s first focus on \\(\\widehat{\\beta}_{1}\\). The least squares estimator by definition is found by the joint minimization\n\\[\n\\left(\\widehat{\\beta}_{1}, \\widehat{\\beta}_{2}\\right)=\\underset{\\beta_{1}, \\beta_{2}}{\\operatorname{argmin}} \\operatorname{SSE}\\left(\\beta_{1}, \\beta_{2}\\right)\n\\]\nwhere\n\\[\n\\operatorname{SSE}\\left(\\beta_{1}, \\beta_{2}\\right)=\\left(\\boldsymbol{Y}-\\boldsymbol{X}_{1} \\beta_{1}-\\boldsymbol{X}_{2} \\beta_{2}\\right)^{\\prime}\\left(\\boldsymbol{Y}-\\boldsymbol{X}_{1} \\beta_{1}-\\boldsymbol{X}_{2} \\beta_{2}\\right) .\n\\]\nAn equivalent expression for \\(\\widehat{\\beta}_{1}\\) can be obtained by concentration (nested minimization). The solution (3.33) can be written as\n\\[\n\\widehat{\\beta}_{1}=\\underset{\\beta_{1}}{\\operatorname{argmin}}\\left(\\min _{\\beta_{2}} \\operatorname{SSE}\\left(\\beta_{1}, \\beta_{2}\\right)\\right) .\n\\]\nThe inner expression \\(\\min _{\\beta_{2}} \\operatorname{SSE}\\left(\\beta_{1}, \\beta_{2}\\right)\\) minimizes over \\(\\beta_{2}\\) while holding \\(\\beta_{1}\\) fixed. It is the lowest possible sum of squared errors given \\(\\beta_{1}\\). The outer minimization \\(\\operatorname{argmin}_{\\beta_{1}}\\) finds the coefficient \\(\\beta_{1}\\) which minimizes the “lowest possible sum of squared errors given \\(\\beta_{1}\\)”. This means that \\(\\widehat{\\beta}_{1}\\) as defined in (3.33) and (3.34) are algebraically identical.\nExamine the inner minimization problem in (3.34). This is simply the least squares regression of \\(\\boldsymbol{Y}-\\boldsymbol{X}_{1} \\beta_{1}\\) on \\(\\boldsymbol{X}_{2}\\). This has solution\n\\[\n\\underset{\\beta_{2}}{\\operatorname{argmin}} \\operatorname{SSE}\\left(\\beta_{1}, \\beta_{2}\\right)=\\left(\\boldsymbol{X}_{2}^{\\prime} \\boldsymbol{X}_{2}\\right)^{-1}\\left(\\boldsymbol{X}_{2}^{\\prime}\\left(\\boldsymbol{Y}-\\boldsymbol{X}_{1} \\beta_{1}\\right)\\right)\n\\]\nwith residuals\n\\[\n\\begin{aligned}\n\\boldsymbol{Y}-\\boldsymbol{X}_{1} \\beta_{1}-\\boldsymbol{X}_{2}\\left(\\boldsymbol{X}_{2}^{\\prime} \\boldsymbol{X}_{2}\\right)^{-1}\\left(\\boldsymbol{X}_{2}^{\\prime}\\left(\\boldsymbol{Y}-\\boldsymbol{X}_{1} \\beta_{1}\\right)\\right) &=\\left(\\boldsymbol{M}_{2} \\boldsymbol{Y}-\\boldsymbol{M}_{2} \\boldsymbol{X}_{1} \\beta_{1}\\right) \\\\\n&=\\boldsymbol{M}_{2}\\left(\\boldsymbol{Y}-\\boldsymbol{X}_{1} \\beta_{1}\\right)\n\\end{aligned}\n\\]\nwhere\n\\[\n\\boldsymbol{M}_{2}=\\boldsymbol{I}_{n}-\\boldsymbol{X}_{2}\\left(\\boldsymbol{X}_{2}^{\\prime} \\boldsymbol{X}_{2}\\right)^{-1} \\boldsymbol{X}_{2}^{\\prime}\n\\]\nis the annihilator matrix for \\(\\boldsymbol{X}_{2}\\). This means that the inner minimization problem (3.34) has minimized value\n\\[\n\\begin{aligned}\n\\min _{\\beta_{2}} \\operatorname{SSE}\\left(\\beta_{1}, \\beta_{2}\\right) &=\\left(\\boldsymbol{Y}-\\boldsymbol{X}_{1} \\beta_{1}\\right)^{\\prime} \\boldsymbol{M}_{2} \\boldsymbol{M}_{2}\\left(\\boldsymbol{Y}-\\boldsymbol{X}_{1} \\beta_{1}\\right) \\\\\n&=\\left(\\boldsymbol{Y}-\\boldsymbol{X}_{1} \\beta_{1}\\right)^{\\prime} \\boldsymbol{M}_{2}\\left(\\boldsymbol{Y}-\\boldsymbol{X}_{1} \\beta_{1}\\right)\n\\end{aligned}\n\\]\nwhere the second equality holds because \\(\\boldsymbol{M}_{2}\\) is idempotent. Substituting this into (3.34) we find\n\\[\n\\begin{aligned}\n\\widehat{\\beta}_{1} &=\\underset{\\beta_{1}}{\\operatorname{argmin}}\\left(\\boldsymbol{Y}-\\boldsymbol{X}_{1} \\beta_{1}\\right)^{\\prime} \\boldsymbol{M}_{2}\\left(\\boldsymbol{Y}-\\boldsymbol{X}_{1} \\beta_{1}\\right) \\\\\n&=\\left(\\boldsymbol{X}_{1}^{\\prime} \\boldsymbol{M}_{2} \\boldsymbol{X}_{1}\\right)^{-1}\\left(\\boldsymbol{X}_{1}^{\\prime} \\boldsymbol{M}_{2} \\boldsymbol{Y}\\right) .\n\\end{aligned}\n\\]\nBy a similar argument we find\n\\[\n\\widehat{\\beta}_{2}=\\left(\\boldsymbol{X}_{2}^{\\prime} \\boldsymbol{M}_{1} \\boldsymbol{X}_{2}\\right)^{-1}\\left(\\boldsymbol{X}_{2}^{\\prime} \\boldsymbol{M}_{1} \\boldsymbol{Y}\\right)\n\\]\nwhere\n\\[\n\\boldsymbol{M}_{1}=\\boldsymbol{I}_{n}-\\boldsymbol{X}_{1}\\left(\\boldsymbol{X}_{1}^{\\prime} \\boldsymbol{X}_{1}\\right)^{-1} \\boldsymbol{X}_{1}^{\\prime}\n\\]\nis the annihilator matrix for \\(\\boldsymbol{X}_{1}\\). Theorem 3.4 The least squares estimator \\(\\left(\\widehat{\\beta}_{1}, \\widehat{\\beta}_{2}\\right)\\) for (3.32) has the algebraic solution\n\\[\n\\begin{aligned}\n&\\widehat{\\beta}_{1}=\\left(\\boldsymbol{X}_{1}^{\\prime} \\boldsymbol{M}_{2} \\boldsymbol{X}_{1}\\right)^{-1}\\left(\\boldsymbol{X}_{1}^{\\prime} \\boldsymbol{M}_{2} \\boldsymbol{Y}\\right) \\\\\n&\\widehat{\\beta}_{2}=\\left(\\boldsymbol{X}_{2}^{\\prime} \\boldsymbol{M}_{1} \\boldsymbol{X}_{2}\\right)^{-1}\\left(\\boldsymbol{X}_{2}^{\\prime} \\boldsymbol{M}_{1} \\boldsymbol{Y}\\right)\n\\end{aligned}\n\\]\nwhere \\(\\boldsymbol{M}_{1}\\) and \\(\\boldsymbol{M}_{2}\\) are defined in (3.36) and (3.35), respectively."
  },
  {
    "objectID": "chpt03-algebra.html#regression-components-alternative-derivation",
    "href": "chpt03-algebra.html#regression-components-alternative-derivation",
    "title": "3  The Algebra of Least Squares",
    "section": "3.19 Regression Components (Alternative Derivation)*",
    "text": "3.19 Regression Components (Alternative Derivation)*\nAn alternative proof of Theorem \\(3.4\\) uses an algebraic argument based on the population calculations from Section 2.22. Since this is a classic derivation we present it here for completeness.\nPartition \\(\\widehat{\\boldsymbol{Q}}_{X X}\\) as\n\\[\n\\widehat{\\boldsymbol{Q}}_{X X}=\\left[\\begin{array}{ll}\n\\widehat{\\boldsymbol{Q}}_{11} & \\widehat{\\boldsymbol{Q}}_{12} \\\\\n\\widehat{\\boldsymbol{Q}}_{21} & \\widehat{\\boldsymbol{Q}}_{22}\n\\end{array}\\right]=\\left[\\begin{array}{ll}\n\\frac{1}{n} \\boldsymbol{X}_{1}^{\\prime} \\boldsymbol{X}_{1} & \\frac{1}{n} \\boldsymbol{X}_{1}^{\\prime} \\boldsymbol{X}_{2} \\\\\n\\frac{1}{n} \\boldsymbol{X}_{2}^{\\prime} \\boldsymbol{X}_{1} & \\frac{1}{n} \\boldsymbol{X}_{2}^{\\prime} \\boldsymbol{X}_{2}\n\\end{array}\\right]\n\\]\nand similarly \\(\\widehat{\\boldsymbol{Q}}_{X Y}\\) as\n\\[\n\\widehat{\\boldsymbol{Q}}_{X Y}=\\left[\\begin{array}{l}\n\\widehat{\\boldsymbol{Q}}_{1 Y} \\\\\n\\widehat{\\boldsymbol{Q}}_{2 Y}\n\\end{array}\\right]=\\left[\\begin{array}{c}\n\\frac{1}{n} \\boldsymbol{X}_{1}^{\\prime} \\boldsymbol{Y} \\\\\n\\frac{1}{n} \\boldsymbol{X}_{2}^{\\prime} \\boldsymbol{Y}\n\\end{array}\\right]\n\\]\nBy the partitioned matrix inversion formula (A.3)\n\\[\n\\widehat{\\boldsymbol{Q}}_{X X}^{-1}=\\left[\\begin{array}{ll}\n\\widehat{\\boldsymbol{Q}}_{11} & \\widehat{\\boldsymbol{Q}}_{12} \\\\\n\\widehat{\\boldsymbol{Q}}_{21} & \\widehat{\\boldsymbol{Q}}_{22}\n\\end{array}\\right]^{-1} \\stackrel{\\operatorname{def}}{=}\\left[\\begin{array}{cc}\n\\widehat{\\boldsymbol{Q}}^{11} & \\widehat{\\boldsymbol{Q}}^{12} \\\\\n\\widehat{\\boldsymbol{Q}}^{21} & \\widehat{\\boldsymbol{Q}}^{22}\n\\end{array}\\right]=\\left[\\begin{array}{cc}\n\\widehat{\\boldsymbol{Q}}_{11 \\cdot 2}^{-1} & -\\widehat{\\boldsymbol{Q}}_{11 \\cdot 2}^{-1} \\widehat{\\boldsymbol{Q}}_{12} \\widehat{\\boldsymbol{Q}}_{22}^{-1} \\\\\n-\\widehat{\\boldsymbol{Q}}_{22 \\cdot 1}^{-1} \\widehat{\\boldsymbol{Q}}_{21} \\widehat{\\boldsymbol{Q}}_{11}^{-1} & \\widehat{\\boldsymbol{Q}}_{22 \\cdot 1}^{-1}\n\\end{array}\\right]\n\\]\nwhere \\(\\widehat{\\boldsymbol{Q}}_{11 \\cdot 2}=\\widehat{\\boldsymbol{Q}}_{11}-\\widehat{\\boldsymbol{Q}}_{12} \\widehat{\\boldsymbol{Q}}_{22}^{-1} \\widehat{\\boldsymbol{Q}}_{21}\\) and \\(\\widehat{\\boldsymbol{Q}}_{22 \\cdot 1}=\\widehat{\\boldsymbol{Q}}_{22}-\\widehat{\\boldsymbol{Q}}_{21} \\widehat{\\boldsymbol{Q}}_{11}^{-1} \\widehat{\\boldsymbol{Q}}_{12}\\). Thus\n\\[\n\\begin{aligned}\n\\widehat{\\beta} &=\\left(\\begin{array}{c}\n\\widehat{\\beta}_{1} \\\\\n\\widehat{\\beta}_{2}\n\\end{array}\\right) \\\\\n&=\\left[\\begin{array}{cc}\n\\widehat{\\boldsymbol{Q}}_{11 \\cdot 2}^{-1} & -\\widehat{\\boldsymbol{Q}}_{11 \\cdot 2}^{-1} \\widehat{\\boldsymbol{Q}}_{12} \\widehat{\\boldsymbol{Q}}_{22}^{-1} \\\\\n-\\widehat{\\boldsymbol{Q}}_{22 \\cdot 1}^{-1} \\widehat{\\boldsymbol{Q}}_{21} \\widehat{\\boldsymbol{Q}}_{11}^{-1} & \\widehat{\\boldsymbol{Q}}_{22 \\cdot 1}^{-1}\n\\end{array}\\right]\\left[\\begin{array}{c}\n\\widehat{\\boldsymbol{Q}}_{1 Y} \\\\\n\\widehat{\\boldsymbol{Q}}_{2 Y}\n\\end{array}\\right] \\\\\n&=\\left(\\begin{array}{c}\n\\widehat{\\mathbf{Q}}_{11 \\cdot 2}^{-1} \\widehat{\\boldsymbol{Q}}_{1 Y \\cdot 2} \\\\\n\\widehat{\\mathbf{Q}}_{22 \\cdot 1}^{-1} \\widehat{\\mathbf{Q}}_{2 Y \\cdot 1}\n\\end{array}\\right)\n\\end{aligned}\n\\]\nNow\n\\[\n\\begin{aligned}\n\\widehat{\\boldsymbol{Q}}_{11 \\cdot 2} &=\\widehat{\\boldsymbol{Q}}_{11}-\\widehat{\\boldsymbol{Q}}_{12} \\widehat{\\boldsymbol{Q}}_{22}^{-1} \\widehat{\\boldsymbol{Q}}_{21} \\\\\n&=\\frac{1}{n} \\boldsymbol{X}_{1}^{\\prime} \\boldsymbol{X}_{1}-\\frac{1}{n} \\boldsymbol{X}_{1}^{\\prime} \\boldsymbol{X}_{2}\\left(\\frac{1}{n} \\boldsymbol{X}_{2}^{\\prime} \\boldsymbol{X}_{2}\\right)^{-1} \\frac{1}{n} \\boldsymbol{X}_{2}^{\\prime} \\boldsymbol{X}_{1} \\\\\n&=\\frac{1}{n} \\boldsymbol{X}_{1}^{\\prime} \\boldsymbol{M}_{2} \\boldsymbol{X}_{1}\n\\end{aligned}\n\\]\nand\n\\[\n\\begin{aligned}\n\\widehat{\\boldsymbol{Q}}_{1 y \\cdot 2} &=\\widehat{\\boldsymbol{Q}}_{1 Y}-\\widehat{\\boldsymbol{Q}}_{12} \\widehat{\\boldsymbol{Q}}_{22}^{-1} \\widehat{\\boldsymbol{Q}}_{2 Y} \\\\\n&=\\frac{1}{n} \\boldsymbol{X}_{1}^{\\prime} \\boldsymbol{Y}-\\frac{1}{n} \\boldsymbol{X}_{1}^{\\prime} \\boldsymbol{X}_{2}\\left(\\frac{1}{n} \\boldsymbol{X}_{2}^{\\prime} \\boldsymbol{X}_{2}\\right)^{-1} \\frac{1}{n} \\boldsymbol{X}_{2}^{\\prime} \\boldsymbol{Y} \\\\\n&=\\frac{1}{n} \\boldsymbol{X}_{1}^{\\prime} \\boldsymbol{M}_{2} \\boldsymbol{Y} .\n\\end{aligned}\n\\]\nEquation (3.38) follows.\nSimilarly to the calculation for \\(\\widehat{\\boldsymbol{Q}}_{11 \\cdot 2}\\) and \\(\\widehat{\\boldsymbol{Q}}_{1 Y \\cdot 2}\\) you can show that \\(\\widehat{\\boldsymbol{Q}}_{2 Y \\cdot 1}=\\frac{1}{n} \\boldsymbol{X}_{2}^{\\prime} \\boldsymbol{M}_{1} \\boldsymbol{Y}\\) and \\(\\widehat{\\boldsymbol{Q}}_{22 \\cdot 1}=\\) \\(\\frac{1}{n} \\boldsymbol{X}_{2}^{\\prime} \\boldsymbol{M}_{1} \\boldsymbol{X}_{2}\\). This establishes (3.37). Together, this is Theorem 3.4."
  },
  {
    "objectID": "chpt03-algebra.html#residual-regression",
    "href": "chpt03-algebra.html#residual-regression",
    "title": "3  The Algebra of Least Squares",
    "section": "3.20 Residual Regression",
    "text": "3.20 Residual Regression\nAs first recognized by Frisch and Waugh (1933) and extended by Lovell (1963), expressions (3.37) and (3.38) can be used to show that the least squares estimators \\(\\widehat{\\beta}_{1}\\) and \\(\\widehat{\\beta}_{2}\\) can be found by a two-step regression procedure.\nTake (3.38). Since \\(\\boldsymbol{M}_{1}\\) is idempotent, \\(\\boldsymbol{M}_{1}=\\boldsymbol{M}_{1} \\boldsymbol{M}_{1}\\) and thus\n\\[\n\\begin{aligned}\n\\widehat{\\beta}_{2} &=\\left(\\boldsymbol{X}_{2}^{\\prime} \\boldsymbol{M}_{1} \\boldsymbol{X}_{2}\\right)^{-1}\\left(\\boldsymbol{X}_{2}^{\\prime} \\boldsymbol{M}_{1} \\boldsymbol{Y}\\right) \\\\\n&=\\left(\\boldsymbol{X}_{2}^{\\prime} \\boldsymbol{M}_{1} \\boldsymbol{M}_{1} \\boldsymbol{X}_{2}\\right)^{-1}\\left(\\boldsymbol{X}_{2}^{\\prime} \\boldsymbol{M}_{1} \\boldsymbol{M}_{1} \\boldsymbol{Y}\\right) \\\\\n&=\\left(\\widetilde{\\boldsymbol{X}}_{2}^{\\prime} \\widetilde{\\boldsymbol{X}}_{2}\\right)^{-1}\\left(\\widetilde{\\boldsymbol{X}}_{2}^{\\prime} \\widetilde{\\boldsymbol{e}}_{1}\\right)\n\\end{aligned}\n\\]\nwhere \\(\\widetilde{\\boldsymbol{X}}_{2}=\\boldsymbol{M}_{1} \\boldsymbol{X}_{2}\\) and \\(\\widetilde{\\boldsymbol{e}}_{1}=\\boldsymbol{M}_{1} \\boldsymbol{Y}\\).\nThus the coefficient estimator \\(\\widehat{\\beta}_{2}\\) is algebraically equal to the least squares regression of \\(\\widetilde{\\boldsymbol{e}}_{1}\\) on \\(\\widetilde{\\boldsymbol{X}}_{2}\\). Notice that these two are \\(\\boldsymbol{Y}\\) and \\(\\boldsymbol{X}_{2}\\), respectively, premultiplied by \\(\\boldsymbol{M}_{1}\\). But we know that pre-multiplication by \\(\\boldsymbol{M}_{1}\\) creates least squares residuals. Therefore \\(\\widetilde{\\boldsymbol{e}}_{1}\\) is simply the least squares residual from a regression of \\(\\boldsymbol{Y}\\) on \\(\\boldsymbol{X}_{1}\\), and the columns of \\(\\widetilde{\\boldsymbol{X}}_{2}\\) are the least squares residuals from the regressions of the columns of \\(\\boldsymbol{X}_{2}\\) on \\(\\boldsymbol{X}_{1}\\).\nWe have proven the following theorem.\nTheorem 3.5 Frisch-Waugh-Lovell (FWL)\nIn the model (3.31), the OLS estimator of \\(\\beta_{2}\\) and the OLS residuals \\(\\widehat{\\boldsymbol{e}}\\) may be computed by either the OLS regression (3.32) or via the following algorithm:\n\nRegress \\(\\boldsymbol{Y}\\) on \\(\\boldsymbol{X}_{1}\\), obtain residuals \\(\\widetilde{\\boldsymbol{e}}_{1}\\);\nRegress \\(\\boldsymbol{X}_{2}\\) on \\(\\boldsymbol{X}_{1}\\), obtain residuals \\(\\widetilde{\\boldsymbol{X}}_{2}\\);\nRegress \\(\\widetilde{\\boldsymbol{e}}_{1}\\) on \\(\\widetilde{\\boldsymbol{X}}_{2}\\), obtain OLS estimates \\(\\widehat{\\beta}_{2}\\) and residuals \\(\\widehat{\\boldsymbol{e}}\\).\n\nIn some contexts (such as panel data models, to be introduced in Chapter 17), the FWL theorem can be used to greatly speed computation.\nThe FWL theorem is a direct analog of the coefficient representation obtained in Section 2.23. The result obtained in that section concerned the population projection coefficients; the result obtained here concern the least squares estimators. The key message is the same. In the least squares regression (3.32) the estimated coefficient \\(\\widehat{\\beta}_{2}\\) algebraically equals the regression of \\(\\boldsymbol{Y}\\) on the regressors \\(\\boldsymbol{X}_{2}\\) after the regressors \\(X_{1}\\) have been linearly projected out. Similarly, the coefficient estimate \\(\\widehat{\\beta}_{1}\\) algebraically equals the regression of \\(\\boldsymbol{Y}\\) on the regressors \\(\\boldsymbol{X}_{1}\\) after the regressors \\(\\boldsymbol{X}_{2}\\) have been linearly projected out. This result can be insightful when interpreting regression coefficients.\nA common application of the FWL theorem is the demeaning formula for regression obtained in (3.18). Partition \\(\\boldsymbol{X}=\\left[\\boldsymbol{X}_{1} \\boldsymbol{X}_{2}\\right]\\) where \\(\\boldsymbol{X}_{1}=\\mathbf{1}_{n}\\) is a vector of ones and \\(\\boldsymbol{X}_{2}\\) is a matrix of observed regressors. In this case \\(\\boldsymbol{M}_{1}=\\boldsymbol{I}_{n}-\\mathbf{1}_{n}\\left(\\mathbf{1}_{n}^{\\prime} \\mathbf{1}_{n}\\right)^{-1} \\mathbf{1}_{n}^{\\prime}\\). Observe that \\(\\widetilde{\\boldsymbol{X}}_{2}=\\boldsymbol{M}_{1} \\boldsymbol{X}_{2}=\\boldsymbol{X}_{2}-\\overline{\\boldsymbol{X}}_{2}\\) and \\(\\boldsymbol{M}_{1} \\boldsymbol{Y}=\\boldsymbol{Y}-\\overline{\\boldsymbol{Y}}\\) are the “demeaned” variables. The FWL theorem says that \\(\\widehat{\\beta}_{2}\\) is the OLS estimate from a regression of \\(Y_{i}-\\bar{Y}\\) on \\(X_{2 i}-\\bar{X}_{2}\\) :\n\\[\n\\widehat{\\beta}_{2}=\\left(\\sum_{i=1}^{n}\\left(X_{2 i}-\\bar{X}_{2}\\right)\\left(X_{2 i}-\\bar{X}_{2}\\right)^{\\prime}\\right)^{-1}\\left(\\sum_{i=1}^{n}\\left(X_{2 i}-\\bar{X}_{2}\\right)\\left(Y_{i}-\\bar{Y}\\right)\\right)\n\\]\nThis is (3.18).\nRagnar Frisch\\ Ragnar Frisch (1895-1973) was co-winner with Jan Tinbergen of the first No-\\ bel Memorial Prize in Economic Sciences in 1969 for their work in developing\\ and applying dynamic models for the analysis of economic problems. Frisch\\ made a number of foundational contributions to modern economics beyond the\\ Frisch-Waugh-Lovell Theorem, including formalizing consumer theory, produc-\\ tion theory, and business cycle theory."
  },
  {
    "objectID": "chpt03-algebra.html#leverage-values",
    "href": "chpt03-algebra.html#leverage-values",
    "title": "3  The Algebra of Least Squares",
    "section": "3.21 Leverage Values",
    "text": "3.21 Leverage Values\nThe leverage values for the regressor matrix \\(\\boldsymbol{X}\\) are the diagonal elements of the projection matrix \\(\\boldsymbol{P}=\\boldsymbol{X}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\boldsymbol{X}^{\\prime}\\). There are \\(n\\) leverage values, and are typically written as \\(h_{i i}\\) for \\(i=1, \\ldots, n\\). Since\n\\[\n\\boldsymbol{P}=\\left(\\begin{array}{c}\nX_{1}^{\\prime} \\\\\nX_{2}^{\\prime} \\\\\n\\vdots \\\\\nX_{n}^{\\prime}\n\\end{array}\\right)\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1}\\left(\\begin{array}{llll}\nX_{1} & X_{2} & \\cdots & X_{n}\n\\end{array}\\right)\n\\]\nthey are\n\\[\nh_{i i}=X_{i}^{\\prime}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} X_{i} .\n\\]\nThe leverage value \\(h_{i i}\\) is a normalized length of the observed regressor vector \\(X_{i}\\). They appear frequently in the algebraic and statistical analysis of least squares regression, including leave-one-out regression, influential observations, robust covariance matrix estimation, and cross-validation.\nA few properties of the leverage values are now listed.\nTheorem 3.6\n\n\\(0 \\leq h_{i i} \\leq 1\\).\n\\(h_{i i} \\geq 1 / n\\) if \\(X\\) includes an intercept.\n\\(\\sum_{i=1}^{n} h_{i i}=k\\).\n\nWe prove Theorem \\(3.6\\) below.\nThe leverage value \\(h_{i i}\\) measures how unusual the \\(i^{t h}\\) observation \\(X_{i}\\) is relative to the other observations in the sample. A large \\(h_{i i}\\) occurs when \\(X_{i}\\) is quite different from the other sample values. A measure of overall unusualness is the maximum leverage value\n\\[\n\\bar{h}=\\max _{1 \\leq i \\leq n} h_{i i} .\n\\]\nIt is common to say that a regression design is balanced when the leverage values are all roughly equal to one another. From Theorem 3.6.3 we deduce that complete balance occurs when \\(h_{i i}=\\bar{h}=k / n\\). An example of complete balance is when the regressors are all orthogonal dummy variables, each of which have equal occurrance of 0’s and 1’s.\nA regression design is unbalanced if some leverage values are highly unequal from the others. The most extreme case is \\(\\bar{h}=1\\). An example where this occurs is when there is a dummy regressor which takes the value 1 for only one observation in the sample.\nThe maximal leverage value (3.41) will change depending on the choice of regressors. For example, consider equation (3.13), the wage regression for single Asian men which has \\(n=268\\) observations. This regression has \\(\\bar{h}=0.33\\). If the squared experience regressor is omitted the leverage drops to \\(\\bar{h}=0.10\\). If a cubic in experience is added it increases to \\(\\bar{h}=0.76\\). And if a fourth and fifth power are added it increases to \\(\\bar{h}=0.99\\).\nSome inference procedures (such as robust covariance matrix estimation and cross-validation) are sensitive to high leverage values. We will return to these issues later.\nWe now prove Theorem 3.6. For part 1 let \\(s_{i}\\) be an \\(n \\times 1\\) unit vector with a 1 in the \\(i^{t h}\\) place and zeros elsewhere so that \\(h_{i i}=s_{i}^{\\prime} \\boldsymbol{P} s_{i}\\). Then applying the Quadratic Inequality (B.18) and Theorem 3.3.4,\n\\[\nh_{i i}=s_{i}^{\\prime} \\boldsymbol{P} s_{i} \\leq s_{i}^{\\prime} s_{i} \\lambda_{\\max }(\\boldsymbol{P})=1\n\\]\nas claimed.\nFor part 2 partition \\(X_{i}=\\left(1, Z_{i}^{\\prime}\\right)^{\\prime}\\). Without loss of generality we can replace \\(Z_{i}\\) with the demeaned values \\(Z_{i}^{*}=Z_{i}-\\bar{Z}\\). Then since \\(Z_{i}^{*}\\) and the intercept are orthgonal\n\\[\n\\begin{aligned}\nh_{i i} &=\\left(1, Z_{i}^{* \\prime}\\right)\\left[\\begin{array}{cc}\nn & 0 \\\\\n0 & Z^{* \\prime} Z^{*}\n\\end{array}\\right]^{-1}\\left(\\begin{array}{c}\n1 \\\\\nZ_{i}^{*}\n\\end{array}\\right) \\\\\n&=\\frac{1}{n}+Z_{i}^{* \\prime}\\left(Z^{* \\prime} Z^{*}\\right)^{-1} Z_{i}^{*} \\geq \\frac{1}{n} .\n\\end{aligned}\n\\]\nFor part 3, \\(\\sum_{i=1}^{n} h_{i i}=\\operatorname{tr} \\boldsymbol{P}=k\\) where the second equality is Theorem 3.3.3."
  },
  {
    "objectID": "chpt03-algebra.html#leave-one-out-regression",
    "href": "chpt03-algebra.html#leave-one-out-regression",
    "title": "3  The Algebra of Least Squares",
    "section": "3.22 Leave-One-Out Regression",
    "text": "3.22 Leave-One-Out Regression\nThere are a number of statistical procedures - residual analysis, jackknife variance estimation, crossvalidation, two-step estimation, hold-out sample evaluation - which make use of estimators constructed on sub-samples. Of particular importance is the case where we exclude a single observation and then repeat this for all observations. This is called leave-one-out (LOO) regression.\nSpecifically, the leave-one-out estimator of the regression coefficient \\(\\beta\\) is the least squares estimator constructed using the full sample excluding a single observation \\(i\\). This can be written as\n\\[\n\\begin{aligned}\n\\widehat{\\beta}_{(-i)} &=\\left(\\sum_{j \\neq i} X_{j} X_{j}^{\\prime}\\right)^{-1}\\left(\\sum_{j \\neq i} X_{j} Y_{j}\\right) \\\\\n&=\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}-X_{i} X_{i}^{\\prime}\\right)^{-1}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{Y}-X_{i} Y_{i}\\right) \\\\\n&=\\left(\\boldsymbol{X}_{(-i)}^{\\prime} \\boldsymbol{X}_{(-i)}\\right)^{-1} \\boldsymbol{X}_{(-i)}^{\\prime} \\boldsymbol{Y}_{(-i)} .\n\\end{aligned}\n\\]\nHere, \\(\\boldsymbol{X}_{(-i)}\\) and \\(\\boldsymbol{Y}_{(-i)}\\) are the data matrices omitting the \\(i^{t h}\\) row. The notation \\(\\widehat{\\beta}_{(-i)}\\) or \\(\\widehat{\\beta}_{-i}\\) is commonly used to denote an estimator with the \\(i^{t h}\\) observation omitted. There is a leave-one-out estimator for each observation, \\(i=1, \\ldots, n\\), so we have \\(n\\) such estimators.\nThe leave-one-out predicted value for \\(Y_{i}\\) is \\(\\widetilde{Y}_{i}=X_{i}^{\\prime} \\widehat{\\beta}_{(-i)}\\). This is the predicted value obtained by estimating \\(\\beta\\) on the sample without observation \\(i\\) and then using the covariate vector \\(X_{i}\\) to predict \\(Y_{i}\\). Notice that \\(\\widetilde{Y}_{i}\\) is an authentic prediction as \\(Y_{i}\\) is not used to construct \\(\\widetilde{Y}_{i}\\). This is in contrast to the fitted values \\(\\widehat{Y}_{i}\\) which are functions of \\(Y_{i}\\).\nThe leave-one-out residual, prediction error, or prediction residual is \\(\\widetilde{e}_{i}=Y_{i}-\\widetilde{Y}_{i}\\). The prediction errors may be used as estimators of the errors instead of the residuals. The prediction errors are better estimators than the residuals because the former are based on authentic predictions.\nThe leave-one-out formula (3.42) gives the unfortunate impression that the leave-one-out coefficients and errors are computationally cumbersome, requiring \\(n\\) separate regressions. In the context of linear regression this is fortunately not the case. There are simple linear expressions for \\(\\widehat{\\beta}_{(-i)}\\) and \\(\\widetilde{e}_{i}\\).\nTheorem 3.7 The leave-one-out estimator and prediction error equal\n\\[\n\\widehat{\\beta}_{(-i)}=\\widehat{\\beta}-\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} X_{i} \\widetilde{e}_{i}\n\\]\nand\n\\[\n\\widetilde{e}_{i}=\\left(1-h_{i i}\\right)^{-1} \\widehat{e}_{i}\n\\]\nwhere \\(h_{i i}\\) are the leverage values as defined in (3.40).\nWe prove Theorem \\(3.7\\) at the end of the section.\nEquation (3.43) shows that the leave-one-out coefficients can be calculated by a simple linear operation and do not need to be calculated using \\(n\\) separate regressions. Another interesting feature of equation (3.44) is that the prediction errors \\(\\widetilde{e}_{i}\\) are a simple scaling of the least squares residuals \\(\\widehat{e}_{i}\\) with the scaling dependent on the leverage values \\(h_{i i}\\). If \\(h_{i i}\\) is small then \\(\\widetilde{e}_{i} \\simeq \\widehat{e}_{i}\\). However if \\(h_{i i}\\) is large then \\(\\widetilde{e}_{i}\\) can be quite different from \\(\\widehat{e}_{i}\\). Thus the difference between the residuals and predicted values depends on the leverage values, that is, how unusual is \\(X_{i}\\). To write (3.44) in vector notation, define\n\\[\n\\begin{aligned}\n\\boldsymbol{M}^{*} &=\\left(\\boldsymbol{I}_{n}-\\operatorname{diag}\\left\\{h_{11}, . ., h_{n n}\\right\\}\\right)^{-1} \\\\\n&=\\operatorname{diag}\\left\\{\\left(1-h_{11}\\right)^{-1}, \\ldots,\\left(1-h_{n n}\\right)^{-1}\\right\\}\n\\end{aligned}\n\\]\nThen (3.44) is equivalent to\n\\[\n\\widetilde{\\boldsymbol{e}}=\\boldsymbol{M}^{*} \\widehat{\\boldsymbol{e}} .\n\\]\nOne use of the prediction errors is to estimate the out-of-sample mean squared error:\n\\[\n\\widetilde{\\sigma}^{2}=\\frac{1}{n} \\sum_{i=1}^{n} \\widetilde{e}_{i}^{2}=\\frac{1}{n} \\sum_{i=1}^{n}\\left(1-h_{i i}\\right)^{-2} \\widehat{e}_{i}^{2} .\n\\]\nThis is known as the sample mean squared prediction error. Its square root \\(\\widetilde{\\sigma}=\\sqrt{\\widetilde{\\sigma}^{2}}\\) is the prediction standard error.\nWe complete the section with a proof of Theorem 3.7. The leave-one-out estimator (3.42) can be written as\n\\[\n\\widehat{\\beta}_{(-i)}=\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}-X_{i} X_{i}^{\\prime}\\right)^{-1}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{Y}-X_{i} Y_{i}\\right) .\n\\]\nMultiply (3.47) by \\(\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}-X_{i} X_{i}^{\\prime}\\right)\\). We obtain\n\\[\n\\widehat{\\beta}_{(-i)}-\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} X_{i} X_{i}^{\\prime} \\widehat{\\beta}_{(-i)}=\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{Y}-X_{i} Y_{i}\\right)=\\widehat{\\beta}-\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} X_{i} Y_{i} .\n\\]\nRewriting\n\\[\n\\widehat{\\beta}_{(-i)}=\\widehat{\\beta}-\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} X_{i}\\left(Y_{i}-X_{i}^{\\prime} \\widehat{\\beta}_{(-i)}\\right)=\\widehat{\\beta}-\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} X_{i} \\widetilde{e}_{i}\n\\]\nwhich is (3.43). Premultiplying this expression by \\(X_{i}^{\\prime}\\) and using definition (3.40) we obtain\n\\[\nX_{i}^{\\prime} \\widehat{\\beta}_{(-i)}=X_{i}^{\\prime} \\widehat{\\beta}-X_{i}^{\\prime}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} X_{i} \\widetilde{e}_{i}=X_{i}^{\\prime} \\widehat{\\beta}-h_{i i} \\widetilde{e}_{i} .\n\\]\nUsing the definitions for \\(\\widehat{e}_{i}\\) and \\(\\widetilde{e}_{i}\\) we obtain \\(\\widetilde{e}_{i}=\\widehat{e}_{i}+h_{i i} \\widetilde{e}_{i}\\). Re-writing we obtain (3.44)."
  },
  {
    "objectID": "chpt03-algebra.html#influential-observations",
    "href": "chpt03-algebra.html#influential-observations",
    "title": "3  The Algebra of Least Squares",
    "section": "3.23 Influential Observations",
    "text": "3.23 Influential Observations\nAnother use of the leave-one-out estimator is to investigate the impact of influential observations, sometimes called outliers. We say that observation \\(i\\) is influential if its omission from the sample induces a substantial change in a parameter estimate of interest.\nFor illustration consider Figure \\(3.5\\) which shows a scatter plot of realizations \\(\\left(Y_{i}, X_{i}\\right)\\). The 25 observations shown with the open circles are generated by \\(X_{i} \\sim U[1,10]\\) and \\(Y_{i} \\sim \\mathrm{N}\\left(X_{i}, 4\\right)\\). The \\(26^{\\text {th }}\\) observation shown with the filled circle is \\(X_{26}=9, Y_{26}=0\\). (Imagine that \\(Y_{26}=0\\) was incorrectly recorded due to a mistaken key entry.) The figure shows both the least squares fitted line from the full sample and that obtained after deletion of the \\(26^{\\text {th }}\\) observation from the sample. In this example we can see how the \\(26^{\\text {th }}\\) observation (the “outlier”) greatly tilts the least squares fitted line towards the \\(26^{\\text {th }}\\) observation. In fact, the slope coefficient decreases from \\(0.97\\) (which is close to the true value of \\(1.00\\) ) to \\(0.56\\), which is substantially reduced. Neither \\(Y_{26}\\) nor \\(X_{26}\\) are unusual values relative to their marginal distributions so this outlier would not have been detected from examination of the marginal distributions of the data. The change in the slope coefficient of \\(-0.41\\) is meaningful and should raise concern to an applied economist.\nFrom (3.43) we know that\n\\[\n\\widehat{\\beta}-\\widehat{\\beta}_{(-i)}=\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} X_{i} \\widetilde{e}_{i}\n\\]\n\nFigure 3.5: Impact of an Influential Observation on the Least-Squares Estimator\nBy direct calculation of this quantity for each observation \\(i\\), we can directly discover if a specific observation \\(i\\) is influential for a coefficient estimate of interest.\nFor a general assessment, we can focus on the predicted values. The difference between the fullsample and leave-one-out predicted values is\n\\[\n\\widehat{Y}_{i}-\\widetilde{Y}_{i}=X_{i}^{\\prime} \\widehat{\\beta}-X_{i}^{\\prime} \\widehat{\\beta}_{(-i)}=X_{i}^{\\prime}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} X_{i} \\widetilde{e}_{i}=h_{i i} \\widetilde{e}_{i}\n\\]\nwhich is a simple function of the leverage values \\(h_{i i}\\) and prediction errors \\(\\widetilde{e}_{i}\\). Observation \\(i\\) is influential for the predicted value if \\(\\left|h_{i i} \\widetilde{e}_{i}\\right|\\) is large, which requires that both \\(h_{i i}\\) and \\(\\left|\\widetilde{e}_{i}\\right|\\) are large.\nOne way to think about this is that a large leverage value \\(h_{i i}\\) gives the potential for observation \\(i\\) to be influential. A large \\(h_{i i}\\) means that observation \\(i\\) is unusual in the sense that the regressor \\(X_{i}\\) is far from its sample mean. We call an observation with large \\(h_{i i}\\) a leverage point. A leverage point is not necessarily influential as the latter also requires that the prediction error \\(\\widetilde{e}_{i}\\) is large.\nTo determine if any individual observations are influential in this sense several diagnostics have been proposed (some names include DFITS, Cook’s Distance, and Welsch Distance). Unfortunately, from a statistical perspective it is difficult to recommend these diagnostics for applications as they are not based on statistical theory. Probably the most relevant measure is the change in the coefficient estimates given in (3.48). The ratio of these changes to the coefficient’s standard error is called its DFBETA, and is a postestimation diagnostic available in Stata. While there is no magic threshold, the concern is whether or not an individual observation meaningfully changes an estimated coefficient of interest. A simple diagnostic for influential observations is to calculate\n\\[\n\\text { Influence }=\\max _{1 \\leq i \\leq n}\\left|\\widehat{Y}_{i}-\\widetilde{Y}_{i}\\right|=\\max _{1 \\leq i \\leq n}\\left|h_{i i} \\widetilde{e}_{i}\\right| .\n\\]\nThis is the largest (absolute) change in the predicted value due to a single observation. If this diagnostic is large relative to the distribution of \\(Y\\) it may indicate that that observation is influential.\nIf an observation is determined to be influential what should be done? As a common cause of influential observations is data error, the influential observations should be examined for evidence that the observation was mis-recorded. Perhaps the observation falls outside of permitted ranges, or some observables are inconsistent (for example, a person is listed as having a job but receives earnings of \\(\\$ 0\\) ). If it is determined that an observation is incorrectly recorded, then the observation is typically deleted from the sample. This process is often called “cleaning the data”. The decisions made in this process involve a fair amount of individual judgment. [When this is done the proper practice is to retain the source data in its original form and create a program file which executes all cleaning operations (for example deletion of individual observations). The cleaned data file can be saved at this point, and then used for the subsequent statistical analysis. The point of retaining the source data and a specific program file which cleans the data is twofold: so that all decisions are documented, and so that modifications can be made in revisions and future research.] It is also possible that an observation is correctly measured, but unusual and influential. In this case it is unclear how to proceed. Some researchers will try to alter the specification to properly model the influential observation. Other researchers will delete the observation from the sample. The motivation for this choice is to prevent the results from being skewed or determined by individual observations. This latter practice is viewed skeptically by many researchers who believe it reduces the integrity of reported empirical results.\nFor an empirical illustration consider the log wage regression (3.13) for single Asian men. This regression, which has 268 observations, has Influence \\(=0.29\\). This means that the most influential observation, when deleted, changes the predicted (fitted) value of the dependent variable \\(\\log (\\) wage) by \\(0.29\\), or equivalently the average wage by \\(29 %\\). This is a meaningful change and suggests further investigation. We examine the influential observation, and find that its leverage \\(h_{i i}\\) is \\(0.33\\). It is a moderately large leverage value, meaning that the regressor \\(X_{i}\\) is somewhat unusual. Examining further, we find that this individual is 65 years old with 8 years education, so that his potential work experience is 51 years. This is the highest experience in the subsample - the next highest is 41 years. The large leverage is due to his unusual characteristics (very low education and very high experience) within this sample. Essentially, regression (3.13) is attempting to estimate the conditional mean at experience \\(=51\\) with only one observation. It is not surprising that this observation determines the fit and is thus influential. A reasonable conclusion is the regression function can only be estimated over a smaller range of experience. We restrict the sample to individuals with less than 45 years experience, re-estimate, and obtain the following estimates.\n\\[\n\\widehat{\\log (\\text { wage })}=0.144 \\text { education }+0.043 \\text { experience }-0.095 \\text { experience }^{2} / 100+0.531 \\text {. }\n\\]\nFor this regression, we calculate that Influence \\(=0.11\\), which is greatly reduced relative to the regression (3.13). Comparing (3.49) with (3.13), the slope coefficient for education is essentially unchanged, but the coefficients on experience and its square have slightly increased.\nBy eliminating the influential observation equation (3.49) can be viewed as a more robust estimate of the conditional mean for most levels of experience. Whether to report (3.13) or (3.49) in an application is largely a matter of judgment."
  },
  {
    "objectID": "chpt03-algebra.html#cps-data-set",
    "href": "chpt03-algebra.html#cps-data-set",
    "title": "3  The Algebra of Least Squares",
    "section": "3.24 CPS Data Set",
    "text": "3.24 CPS Data Set\nIn this section we describe the data set used in the empirical illustrations.\nThe Current Population Survey (CPS) is a monthly survey of about 57,000 U.S. households conducted by the Bureau of the Census of the Bureau of Labor Statistics. The CPS is the primary source of information on the labor force characteristics of the U.S. population. The survey covers employment, earnings, educational attainment, income, poverty, health insurance coverage, job experience, voting and registration, computer usage, veteran status, and other variables. Details can be found at . html.\nFrom the March 2009 survey we extracted the individuals with non-allocated variables who were fulltime employed (defined as those who had worked at least 36 hours per week for at least 48 weeks the past year), and excluded those in the military. This sample has 50,742 individuals. We extracted 14 variables from the CPS on these individuals and created the data set cps09mar. This data set, and all others used in this textbook, are available at http: . wisc . edu/ bhansen/econometrics/."
  },
  {
    "objectID": "chpt03-algebra.html#numerical-computation",
    "href": "chpt03-algebra.html#numerical-computation",
    "title": "3  The Algebra of Least Squares",
    "section": "3.25 Numerical Computation",
    "text": "3.25 Numerical Computation\nModern econometric estimation involves large samples and many covariates. Consequently, calculation of even simple statistics such as the least squares estimator requires a large number (millions) of arithmetic operations. In practice most economists don’t need to think much about this as it is done swiftly and effortlessly on personal computers. Nevertheless it is useful to understand the underlying calculation methods as choices can occasionally make substantive differences.\nWhile today nearly all statistical computations are made using statistical software running on electronic computers, this was not always the case. In the nineteenth and early twentieth centures “computer” was a job label for workers who made computations by hand. Computers were employed by astronomers and statistical laboratories. This fascinating job (and the fact that most computers employed in laboratories were women) has entered popular culture. For example the lives of several computers who worked for the early U.S. space program is described in the book and popular movie Hidden Figures, a fictional computer/astronaut is the protagonist of the novel The Calculating Stars, and the life of computer/astronomer Henrietta Swan Leavitt is dramatized in the play Silent Sky.\nUntil programmable electronic computers became available in the 1960s economics graduate students were routinely employed as computers. Sample sizes were considerably smaller than those seen today, but still the effort required to calculate by hand a regression with even \\(n=100\\) observations and \\(k=5\\) variables is considerable! If you are a current graduate student you should feel fortunate that the profession has moved on from the era of human computers! (Now research assistants do more elevated tasks such as writing Stata, R, and MATLAB code.)\nTo obtain the least squares estimate \\(\\widehat{\\beta}=\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{Y}\\right)\\) we need to either invert \\(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\) or solve a system of equations. To be specific, let \\(\\boldsymbol{A}=\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\) and \\(\\boldsymbol{c}=\\boldsymbol{X}^{\\prime} \\boldsymbol{Y}\\) so that the least squares estimate can be written as either the solution to\n\\[\n\\boldsymbol{A} \\widehat{\\beta}=\\boldsymbol{c}\n\\]\nor as\n\\[\n\\widehat{\\beta}=A^{-1} \\boldsymbol{c} .\n\\]\nThe equations (3.50) and (3.51) are algebraically identical but they suggest two distinct numerical approaches to obtain \\(\\widehat{\\beta}\\). (3.50) suggests solving a system of \\(k\\) equations. (3.51) suggests finding \\(A^{-1}\\) and then multiplying by \\(\\boldsymbol{c}\\). While the two expressions are algebraically identical the implied numerical approaches are different. In a nutshell, solving the system of equations (3.50) is numerically preferred to the matrix inversion problem (3.51). Directly solving (3.50) is faster and produces a solution with a higher degree of numerical accuracy. Thus (3.50) is generally recommended over (3.51). However, in most practical applications the choice will not make any practical difference. Contexts where the choice may make a difference is when the matrix \\(\\boldsymbol{A}\\) is ill-conditioned (to be discussed in Section 3.24) or of extremely high dimension.\nNumerical methods to solve the system of equations (3.50) and calculate \\(\\boldsymbol{A}^{-1}\\) are discussed in Sections A.18 and A.19, respectively.\nStatistical packages use a variety of matrix methods to solve (3.50). Stata uses the sweep algorithm which is a variant of the Gauss-Jordan algorithm discussed in Section A.18. (For the sweep algorithm see Goodnight (1979).) In R, solve (A, b) uses the QR decomposition. In MATLAB, A b uses the Cholesky decomposition when \\(A\\) is positive definite and the QR decomposition otherwise."
  },
  {
    "objectID": "chpt03-algebra.html#collinearity-errors",
    "href": "chpt03-algebra.html#collinearity-errors",
    "title": "3  The Algebra of Least Squares",
    "section": "3.26 Collinearity Errors",
    "text": "3.26 Collinearity Errors\nFor the least squares estimator to be uniquely defined the regressors cannot be linearly dependent. However, it is quite easy to attempt to calculate a regression with linearly dependent regressors. This can occur for many reasons, including the following.\n\nIncluding the same regressor twice.\nIncluding regressors which are a linear combination of one another, such as education, experience and age in the CPS data set example (recall, experience is defined as age-education-6).\nIncluding a dummy variable and its square.\nEstimating a regression on a sub-sample for which a dummy variable is either all zeros or all ones.\nIncluding a dummy variable interaction which yields all zeros.\nIncluding more regressors than observations.\n\nIn any of the above cases the regressors are linearly dependent so \\(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\) is singular and the least squares estimator is not unique. If you attempt to estimate the regression, you are likely to encounter an error message. (A possible exception is MATLAB using “A \\(\\backslash \\mathrm{b}\\)”, as discussed below.) The message may be that “system is exactly singular”, “system is computationally singular”, a variable is “omitted because of collinearity”, or a coefficient is listed as “NA”. In some cases (such as estimation in R using explicit matrix computation or MATLAB using the regress command) the program will stop execution. In other cases the program will continue to run. In Stata (and in the Im package in R), a regression will be reported but one or more variables will be omitted.\nIf any of these warnings or error messages appear, the correct response is to stop and examine the regression coding and data. Did you make an unintended mistake? Have you included a linearly dependent regressor? Are you estimating on a subsample for which the variables (in particular dummy variables) have no variation? If you can determine that one of these scenarios caused the error, the solution is immediately apparent. You need to respecify your model (either sample or regressors) so that the redundancy is eliminated. All empirical researchers encounter this error in the course of empirical work. You should not, however, simply accept output if the package has selected variables for omission. It is the researcher’s job to understand the underlying cause and enact a suitable remedy.\nThere is also a possibility that the statistical package will not detect and report the matrix singularity. If you compute in MATLAB using explicit matrix operations and use the recommended A \\(\\backslash \\mathrm{b}\\) command to compute the least squares estimator MATLAB may return a numerical solution without an error message even when the regressors are algebraically dependent. It is therefore recommended that you perform a numerical check for matrix singularity when using explicit matrix operations in MATLAB.\nHow can we numerically check if a matrix \\(\\boldsymbol{A}\\) is singular? A standard diagnostic is the reciprocal condition number\n\\[\nC=\\frac{\\lambda_{\\min }(\\boldsymbol{A})}{\\lambda_{\\max }(\\boldsymbol{A})} .\n\\]\nIf \\(C=0\\) then \\(\\boldsymbol{A}\\) is singular. If \\(C=1\\) then \\(\\boldsymbol{A}\\) is perfectly balanced. If \\(C\\) is extremely small we say that \\(\\boldsymbol{A}\\) is ill-conditioned. The reciprocal condition number can be calculated in MATLAB or R by the rcond command. Unfortunately, there is no accepted tolerance for how small \\(C\\) should be before regarding \\(\\boldsymbol{A}\\) as numerically singular, in part since rcond (A) can return a positive (but small) result even if \\(\\boldsymbol{A}\\) is algebraically singular. However, in double precision (which is typically used for computation) numerical accuracy is bounded by \\(2^{-52} \\simeq 2 \\mathrm{e}-16\\), suggesting the minimum bound \\(C \\geq 2 \\mathrm{e}-16\\).\nChecking for numerical singularity is complicated by the fact that low values of \\(C\\) can also be caused by unbalanced or highly correlated regressors.\nTo illustrate, consider a wage regression using the sample from (3.13) on powers of experience \\(X\\) from 1 through \\(k\\) (e.g. \\(X, X^{2}, X^{3}, \\ldots, X^{k}\\) ). We calculated the reciprocal condition number \\(C\\) for each \\(k\\), and found that \\(C\\) is decreasing as \\(k\\) increases, indicating increasing ill-conditioning. Indeed, for \\(k=\\) 5, we find \\(C=6 \\mathrm{e}-17\\), which is lower than double precision accuracy. This means that a regression on \\(\\left(X, X^{2}, X^{3}, X^{4}, X^{5}\\right)\\) is ill-conditioned. The regressor matrix, however, is not singular. The low value of \\(C\\) is not due to algebraic singularity but rather is due to a lack of balance and high collinearity.\nIll-conditioned regressors have the potential problem that the numerical results (the reported coefficient estimates) will be inaccurate. It may not be a concern in most applications as this only occurs in extreme cases. Nevertheless, we should try and avoid ill-conditioned regressions whenever possible.\nThere are strategies which can reduce or even eliminate ill-conditioning. Often it is sufficient to rescale the regressors. A simple rescaling which often works for non-negative regressors is to divide each by its sample mean, thus replace \\(X_{j i}\\) with \\(X_{j i} / \\bar{X}_{j}\\). In the above example with the powers of experience, this means replacing \\(X_{i}^{2}\\) with \\(X_{i}^{2} /\\left(n^{-1} \\sum_{i=1}^{n} X_{i}^{2}\\right)\\), etc. Doing so dramatically reduces the ill-conditioning. With this scaling, regressions for \\(k \\leq 11\\) satisfy \\(C \\geq 1 \\mathrm{e}-15\\). Another rescaling specific to a regression with powers is to first rescale the regressor to lie in \\([-1,1]\\) before taking powers. With this scaling, regressions for \\(k \\leq 16\\) satisfy \\(C \\geq 1 \\mathrm{e}-15\\). A simpler scaling option is to rescale the regressor to lie in \\([0,1]\\) before taking powers. With this scaling, regressions for \\(k \\leq 9\\) satisfy \\(C \\geq 1 \\mathrm{e}-15\\). This is often sufficient for applications.\nIll-conditioning can often be completely eliminated by orthogonalization of the regressors. This is achieved by sequentially regressing each variable (each column in \\(\\boldsymbol{X}\\) ) on the preceeding variables (each preceeding column), taking the residual, and then rescaling to have a unit variance. This will produce regressors which algebraically satisfy \\(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}=n \\boldsymbol{I}_{n}\\) and have a condition number of \\(C=1\\). If we apply this method to the above example, we obtain a condition number close to 1 for \\(k \\leq 20\\).\nWhat this shows is that when a regression has a small condition number it is important to examine the specification carefully. It is possible that the regressors are linearly dependent in which case one or more regressors will need to be omitted. It is also possible that the regressors are badly scaled in which case it may be useful to rescale some of the regressors. It is also possible that the variables are highly collinear in which case a possible solution is orthogonalization. These choices should be made by the researcher not by an automated software program."
  },
  {
    "objectID": "chpt03-algebra.html#programming",
    "href": "chpt03-algebra.html#programming",
    "title": "3  The Algebra of Least Squares",
    "section": "3.27 Programming",
    "text": "3.27 Programming\nMost packages allow both interactive programming (where you enter commands one-by-one) and batch programming (where you run a pre-written sequence of commands from a file). Interactive programming can be useful for exploratory analysis but eventually all work should be executed in batch mode. This is the best way to control and document your work.\nBatch programs are text files where each line executes a single command. For Stata, this file needs to have the filename extension “.do”, and for MATLAB “.m”. For R there is no specific naming requirements, though it is typical to use the extension “.r”. When writing batch files it is useful to include comments for documentation and readability. To execute a program file you type a command within the program.\nStata: do chapter3 executes the file chapter3. do.\nMATLAB: run chapter3 executes the file chapter3.m.\nR: source (‘chapter3.r’) executes the file chapter3.r.\nThere are similarities and differences between the commands used in these packages. For example:\n\nDifferent symbols are used to create comments. \\(*\\) in Stata, # in \\(\\mathrm{R}\\), and \\(%\\) in MATLAB.\nMATLAB uses the symbol ; to separate lines. Stata and R use a hard return.\nStata uses \\(\\ln ()\\) to compute natural logarithms. R and MATLAB use \\(\\log ()\\).\nThe symbol \\(=\\) is used to define a variable. \\(\\mathrm{R}\\) prefers \\(<-\\). Double equality \\(==\\) is used to test equality.\n\nWe now illustrate programming files for Stata, R, and MATLAB, which execute a portion of the empirical illustrations from Sections \\(3.7\\) and 3.21. For the R and MATLAB code we illustrate using explicit matrix operations. Alternatively, R and MATLAB have built-in functions which implement least squares regression without the need for explicit matrix operations. In \\(\\mathrm{R}\\) the standard function is \\(1 \\mathrm{~m}\\). In MATLAB the standard function is regress. The advantage of using explicit matrix operations as shown below is that you know exactly what computations are done and it is easier to go “out of the box” to execute new procedures. The advantage of using built-in functions is that coding is simplified and you are much less likely to make a coding error."
  },
  {
    "objectID": "chpt03-algebra.html#exercises",
    "href": "chpt03-algebra.html#exercises",
    "title": "3  The Algebra of Least Squares",
    "section": "3.28 Exercises",
    "text": "3.28 Exercises\nExercise 3.1 Let \\(Y\\) be a random variable with \\(\\mu=\\mathbb{E}[Y]\\) and \\(\\sigma^{2}=\\operatorname{var}[Y]\\). Define\n\\[\ng\\left(y, \\mu, \\sigma^{2}\\right)=\\left(\\begin{array}{c}\ny-\\mu \\\\\n(y-\\mu)^{2}-\\sigma^{2}\n\\end{array}\\right) .\n\\]\nLet \\(\\left(\\widehat{\\mu}, \\widehat{\\sigma}^{2}\\right)\\) be the values such that \\(\\bar{g}_{n}\\left(\\widehat{\\mu}, \\widehat{\\sigma}^{2}\\right)=0\\) where \\(\\bar{g}_{n}(m, s)=n^{-1} \\sum_{i=1}^{n} g\\left(y_{i}, m, s\\right)\\). Show that \\(\\widehat{\\mu}\\) and \\(\\widehat{\\sigma}^{2}\\) are the sample mean and variance.\nExercise 3.2 Consider the OLS regression of the \\(n \\times 1\\) vector \\(\\boldsymbol{Y}\\) on the \\(n \\times k\\) matrix \\(\\boldsymbol{X}\\). Consider an alternative set of regressors \\(\\boldsymbol{Z}=\\boldsymbol{X} \\boldsymbol{C}\\), where \\(\\boldsymbol{C}\\) is a \\(k \\times k\\) non-singular matrix. Thus, each column of \\(\\boldsymbol{Z}\\) is a mixture of some of the columns of \\(\\boldsymbol{X}\\). Compare the OLS estimates and residuals from the regression of \\(\\boldsymbol{Y}\\) on \\(\\boldsymbol{X}\\) to the OLS estimates from the regression of \\(\\boldsymbol{Y}\\) on \\(\\boldsymbol{Z}\\).\nExercise 3.3 Using matrix algebra, show \\(\\boldsymbol{X}^{\\prime} \\widehat{\\boldsymbol{e}}=0\\).\nExercise 3.4 Let \\(\\widehat{\\boldsymbol{e}}\\) be the OLS residual from a regression of \\(\\boldsymbol{Y}\\) on \\(\\boldsymbol{X}=\\left[\\boldsymbol{X}_{1} \\boldsymbol{X}_{2}\\right]\\). Find \\(\\boldsymbol{X}_{2}^{\\prime} \\widehat{\\boldsymbol{e}}\\).\nExercise 3.5 Let \\(\\widehat{\\boldsymbol{e}}\\) be the OLS residual from a regression of \\(\\boldsymbol{Y}\\) on \\(\\boldsymbol{X}\\). Find the OLS coefficient from a regression of \\(\\widehat{\\boldsymbol{e}}\\) on \\(\\boldsymbol{X}\\).\nExercise 3.6 Let \\(\\widehat{\\boldsymbol{Y}}=\\boldsymbol{X}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\boldsymbol{X}^{\\prime} \\boldsymbol{Y}\\). Find the OLS coefficient from a regression of \\(\\widehat{\\boldsymbol{Y}}\\) on \\(\\boldsymbol{X}\\).\nExercise 3.7 Show that if \\(\\boldsymbol{X}=\\left[\\boldsymbol{X}_{1} \\boldsymbol{X}_{2}\\right]\\) then \\(\\boldsymbol{P} \\boldsymbol{X}_{1}=\\boldsymbol{X}_{1}\\) and \\(\\boldsymbol{M} \\boldsymbol{X}_{1}=0 .\\)\nExercise 3.8 Show that \\(M\\) is idempotent: \\(M M=M\\).\nExercise 3.9 Show that \\(\\operatorname{tr} M=n-k\\).\nExercise 3.10 Show that if \\(\\boldsymbol{X}=\\left[\\boldsymbol{X}_{1} \\boldsymbol{X}_{2}\\right]\\) and \\(\\boldsymbol{X}_{1}^{\\prime} \\boldsymbol{X}_{2}=0\\) then \\(\\boldsymbol{P}=\\boldsymbol{P}_{1}+\\boldsymbol{P}_{2}\\).\nExercise 3.11 Show that when \\(X\\) contains a constant, \\(n^{-1} \\sum_{i=1}^{n} \\widehat{Y}_{i}=\\bar{Y}\\).\nExercise 3.12 A dummy variable takes on only the values 0 and 1 . It is used for categorical variables. Let \\(\\boldsymbol{D}_{1}\\) and \\(\\boldsymbol{D}_{2}\\) be vectors of 1’s and 0’s, with the \\(i^{\\text {th }}\\) element of \\(\\boldsymbol{D}_{1}\\) equaling 1 and that of \\(\\boldsymbol{D}_{2}\\) equaling 0 if the person is a man, and the reverse if the person is a woman. Suppose that there are \\(n_{1}\\) men and \\(n_{2}\\) women in the sample. Consider fitting the following three equations by OLS\n\\[\n\\begin{aligned}\n&\\boldsymbol{Y}=\\mu+\\boldsymbol{D}_{1} \\alpha_{1}+\\boldsymbol{D}_{2} \\alpha_{2}+\\boldsymbol{e} \\\\\n&\\boldsymbol{Y}=\\boldsymbol{D}_{1} \\alpha_{1}+\\boldsymbol{D}_{2} \\alpha_{2}+\\boldsymbol{e} \\\\\n&\\boldsymbol{Y}=\\mu+\\boldsymbol{D}_{1} \\phi+\\boldsymbol{e}\n\\end{aligned}\n\\]\nCan all three equations (3.52), (3.53), and (3.54) be estimated by OLS? Explain if not.\n\nCompare regressions (3.53) and (3.54). Is one more general than the other? Explain the relationship between the parameters in (3.53) and (3.54).\nCompute \\(\\mathbf{1}_{n}^{\\prime} \\boldsymbol{D}_{1}\\) and \\(\\mathbf{1}_{n}^{\\prime} \\boldsymbol{D}_{2}\\), where \\(\\mathbf{1}_{n}\\) is an \\(n \\times 1\\) vector of ones.\n\nExercise 3.13 Let \\(\\boldsymbol{D}_{1}\\) and \\(\\boldsymbol{D}_{2}\\) be defined as in the previous exercise.\n\nIn the OLS regression\n\n\\[\n\\boldsymbol{Y}=\\boldsymbol{D}_{1} \\widehat{\\gamma}_{1}+\\boldsymbol{D}_{2} \\widehat{\\gamma}_{2}+\\widehat{\\boldsymbol{u}}\n\\]\nshow that \\(\\widehat{\\gamma}_{1}\\) is the sample mean of the dependent variable among the men of the sample \\(\\left(\\bar{Y}_{1}\\right)\\), and that \\(\\widehat{\\gamma}_{2}\\) is the sample mean among the women \\(\\left(\\bar{Y}_{2}\\right)\\).\n\nLet \\(\\boldsymbol{X}(n \\times k)\\) be an additional matrix of regressors. Describe in words the transformations\n\n\\[\n\\begin{aligned}\n&\\boldsymbol{Y}^{*}=\\boldsymbol{Y}-\\boldsymbol{D}_{1} \\bar{Y}_{1}-\\boldsymbol{D}_{2} \\bar{Y}_{2} \\\\\n&\\boldsymbol{X}^{*}=\\boldsymbol{X}-\\boldsymbol{D}_{1} \\bar{X}_{1}^{\\prime}-\\boldsymbol{D}_{2} \\bar{X}_{2}^{\\prime}\n\\end{aligned}\n\\]\nwhere \\(\\bar{X}_{1}\\) and \\(\\bar{X}_{2}\\) are the \\(k \\times 1\\) means of the regressors for men and women, respectively. (c) Compare \\(\\widetilde{\\beta}\\) from the OLS regression\n\\[\n\\boldsymbol{Y}^{*}=\\boldsymbol{X}^{*} \\widetilde{\\boldsymbol{\\beta}}+\\widetilde{\\boldsymbol{e}}\n\\]\nwith \\(\\widehat{\\beta}\\) from the OLS regression\n\\[\n\\boldsymbol{Y}=\\boldsymbol{D}_{1} \\widehat{\\alpha}_{1}+\\boldsymbol{D}_{2} \\widehat{\\alpha}_{2}+\\boldsymbol{X} \\widehat{\\boldsymbol{\\beta}}+\\widehat{\\boldsymbol{e}} .\n\\]\nExercise 3.14 Let \\(\\widehat{\\beta}_{n}=\\left(\\boldsymbol{X}_{n}^{\\prime} \\boldsymbol{X}_{n}\\right)^{-1} \\boldsymbol{X}_{n}^{\\prime} \\boldsymbol{Y}_{n}\\) denote the OLS estimate when \\(\\boldsymbol{Y}_{n}\\) is \\(n \\times 1\\) and \\(\\boldsymbol{X}_{n}\\) is \\(n \\times k\\). A new observation \\(\\left(Y_{n+1}, X_{n+1}\\right)\\) becomes available. Prove that the OLS estimate computed using this additional observation is\n\\[\n\\widehat{\\beta}_{n+1}=\\widehat{\\beta}_{n}+\\frac{1}{1+X_{n+1}^{\\prime}\\left(\\boldsymbol{X}_{n}^{\\prime} \\boldsymbol{X}_{n}\\right)^{-1} X_{n+1}}\\left(\\boldsymbol{X}_{n}^{\\prime} \\boldsymbol{X}_{n}\\right)^{-1} X_{n+1}\\left(Y_{n+1}-X_{n+1}^{\\prime} \\widehat{\\beta}_{n}\\right)\n\\]\nExercise 3.15 Prove that \\(R^{2}\\) is the square of the sample correlation between \\(\\boldsymbol{Y}\\) and \\(\\widehat{\\boldsymbol{Y}}\\).\nExercise 3.16 Consider two least squares regressions\n\\[\n\\boldsymbol{Y}=\\boldsymbol{X}_{1} \\widetilde{\\beta}_{1}+\\widetilde{\\boldsymbol{e}}\n\\]\nand\n\\[\n\\boldsymbol{Y}=\\boldsymbol{X}_{1} \\widehat{\\beta}_{1}+\\boldsymbol{X}_{2} \\widehat{\\beta}_{2}+\\widehat{\\boldsymbol{e}} .\n\\]\nLet \\(R_{1}^{2}\\) and \\(R_{2}^{2}\\) be the \\(R\\)-squared from the two regressions. Show that \\(R_{2}^{2} \\geq R_{1}^{2}\\). Is there a case (explain) when there is equality \\(R_{2}^{2}=R_{1}^{2}\\) ?\nExercise 3.17 For \\(\\widetilde{\\sigma}^{2}\\) defined in (3.46), show that \\(\\widetilde{\\sigma}^{2} \\geq \\widehat{\\sigma}^{2}\\). Is equality possible?\nExercise 3.18 For which observations will \\(\\widehat{\\beta}_{(-i)}=\\widehat{\\beta}\\) ?\nExercise 3.19 For the intercept-only model \\(Y_{i}=\\beta+e_{i}\\), show that the leave-one-out prediction error is\n\\[\n\\widetilde{e}_{i}=\\left(\\frac{n}{n-1}\\right)\\left(Y_{i}-\\bar{Y}\\right) .\n\\]\nExercise 3.20 Define the leave-one-out estimator of \\(\\sigma^{2}\\),\n\\[\n\\widehat{\\sigma}_{(-i)}^{2}=\\frac{1}{n-1} \\sum_{j \\neq i}\\left(Y_{j}-X_{j}^{\\prime} \\widehat{\\beta}_{(-i)}\\right)^{2} .\n\\]\nThis is the estimator obtained from the sample with observation \\(i\\) omitted. Show that\n\\[\n\\widehat{\\sigma}_{(-i)}^{2}=\\frac{n}{n-1} \\widehat{\\sigma}^{2}-\\frac{\\widehat{e}_{i}^{2}}{(n-1)\\left(1-h_{i i}\\right)} .\n\\]\nExercise 3.21 Consider the least squares regression estimators\n\\[\nY_{i}=X_{1 i} \\widehat{\\beta}_{1}+X_{2 i} \\widehat{\\beta}_{2}+\\widehat{e}_{i}\n\\]\nand the “one regressor at a time” regression estimators\n\\[\nY_{i}=X_{1 i} \\widetilde{\\beta}_{1}+\\widetilde{e}_{1 i}, \\quad Y_{i}=X_{2 i} \\widetilde{\\beta}_{2}+\\widetilde{e}_{2 i}\n\\]\nUnder what condition does \\(\\widetilde{\\beta}_{1}=\\widehat{\\beta}_{1}\\) and \\(\\widetilde{\\beta}_{2}=\\widehat{\\beta}_{2}\\) ? Exercise 3.22 You estimate a least squares regression\n\\[\nY_{i}=X_{1 i}^{\\prime} \\widetilde{\\beta}_{1}+\\widetilde{u}_{i}\n\\]\nand then regress the residuals on another set of regressors\n\\[\n\\widetilde{u}_{i}=X_{2 i}^{\\prime} \\widetilde{\\beta}_{2}+\\widetilde{e}_{i}\n\\]\nDoes this second regression give you the same estimated coefficients as from estimation of a least squares regression on both set of regressors?\n\\[\nY_{i}=X_{1 i}^{\\prime} \\widehat{\\beta}_{1}+X_{2 i}^{\\prime} \\widehat{\\beta}_{2}+\\widehat{e}_{i}\n\\]\nIn other words, is it true that \\(\\widetilde{\\beta}_{2}=\\widehat{\\beta}_{2}\\) ? Explain your reasoning.\nExercise 3.23 The data matrix is \\((\\boldsymbol{Y}, \\boldsymbol{X})\\) with \\(\\boldsymbol{X}=\\left[\\boldsymbol{X}_{1}, \\boldsymbol{X}_{2}\\right]\\), and consider the transformed regressor matrix \\(\\boldsymbol{Z}=\\left[\\boldsymbol{X}_{1}, \\boldsymbol{X}_{2}-\\boldsymbol{X}_{1}\\right]\\). Suppose you do a least squares regression of \\(\\boldsymbol{Y}\\) on \\(\\boldsymbol{X}\\), and a least squares regression of \\(\\boldsymbol{Y}\\) on \\(\\boldsymbol{Z}\\). Let \\(\\widehat{\\sigma}^{2}\\) and \\(\\widetilde{\\sigma}^{2}\\) denote the residual variance estimates from the two regressions. Give a formula relating \\(\\widehat{\\sigma}^{2}\\) and \\(\\widetilde{\\sigma}^{2}\\) ? (Explain your reasoning.)\nExercise 3.24 Use the cps09mar data set described in Section \\(3.22\\) and available on the textbook website. Take the sub-sample used for equation (3.49) (see Section 3.25) for data construction)\n\nEstimate equation (3.49) and compute the equation \\(R^{2}\\) and sum of squared errors.\nRe-estimate the slope on education using the residual regression approach. Regress log(wage) on experience and its square, regress education on experience and its square, and the residuals on the residuals. Report the estimates from this final regression, along with the equation \\(R^{2}\\) and sum of squared errors. Does the slope coefficient equal the value in (3.49)? Explain.\nAre the \\(R^{2}\\) and sum-of-squared errors from parts (a) and (b) equal? Explain.\n\nExercise 3.25 Estimate equation (3.49) as in part (a) of the previous question. Let \\(\\widehat{e}_{i}\\) be the OLS residual, \\(\\widehat{Y}_{i}\\) the predicted value from the regression, \\(X_{1 i}\\) be education and \\(X_{2 i}\\) be experience. Numerically calculate the following:\\ (a) \\(\\sum_{i=1}^{n} \\widehat{e}_{i}\\)\\ (b) \\(\\sum_{i=1}^{n} X_{1 i} \\widehat{e}_{i}\\)\\ (c) \\(\\sum_{i=1}^{n} X_{2 i} \\widehat{e}_{i}\\)\\ (d) \\(\\sum_{i=1}^{n} X_{1 i}^{2} \\widehat{e}_{i}\\)\\ (e) \\(\\sum_{i=1}^{n} X_{2 i}^{2} \\widehat{e}_{i}\\)\\ (f) \\(\\sum_{i=1}^{n} \\widehat{Y}_{i} \\widehat{e}_{i}\\)\\ (g) \\(\\sum_{i=1}^{n} \\widehat{e}_{i}^{2}\\)\nAre these calculations consistent with the theoretical properties of OLS? Explain.\nExercise 3.26 Use the cps09mar data set. (a) Estimate a log wage regression for the subsample of white male Hispanics. In addition to education, experience, and its square, include a set of binary variables for regions and marital status. For regions, create dummy variables for Northeast, South, and West so that Midwest is the excluded group. For marital status, create variables for married, widowed or divorced, and separated, so that single (never married) is the excluded group.\n\nRepeat using a different econometric package. Compare your results. Do they agree?"
  },
  {
    "objectID": "chpt03-algebra-chn.html#介绍",
    "href": "chpt03-algebra-chn.html#介绍",
    "title": "最小二乘代数",
    "section": "介绍",
    "text": "介绍\n在本章中，我们将介绍流行的最小二乘估计器。大部分讨论都是代数的，分布和推理的问题推迟到后面的章节。"
  },
  {
    "objectID": "chpt03-algebra-chn.html#样品",
    "href": "chpt03-algebra-chn.html#样品",
    "title": "最小二乘代数",
    "section": "样品",
    "text": "样品\n在 \\(2.18\\) 部分，我们推导并讨论了给定 \\(X\\) 对随机变量 \\((Y, X) \\in \\mathbb{R} \\times \\mathbb{R}^{k}\\) 的 \\(Y\\) 的最佳线性预测器，并将其称为线性投影模型。我们现在对估计这个模型的参数感兴趣，特别是投影系数\n\\[\n\\beta=\\left(\\mathbb{E}\\left[X X^{\\prime}\\right]\\right)^{-1} \\mathbb{E}[X Y] .\n\\]\n我们可以从包含 \\((Y, X)\\) 的联合测量值的样本中估计 \\(\\beta\\)。例如，假设我们有兴趣估计工资方程，我们将使用一个数据集，其中包含工资（或每周收入）、教育、经验（或年龄）和人口特征（性别、种族、位置）的观察结果。一个可能的数据集是当前人口调查 (CPS)，这是一项对美国家庭的调查，其中包括有关就业、收入、教育和人口特征的问题。\n从符号上讲，我们希望将观察（实现）与潜在的随机变量区分开来。随机变量是 \\((Y, X)\\)。观察结果是 \\(\\left(Y_{i}, X_{i}\\right)\\)。从研究人员的角度来看，后者是数字。从统计理论的角度来看，我们将它们视为随机变量的实现。对于单个观察，我们附加一个从 1 到 \\(n\\) 的下标 \\(i\\)，因此 \\(i^{t h}\\) 观察是 \\(\\left(Y_{i}, X_{i}\\right)\\)。数字 \\(n\\) 是样本量。数据集或样本是 \\(\\left\\{\\left(Y_{i}, X_{i}\\right): i=1, \\ldots, n\\right\\}\\)。\n从经验分析的角度来看，数据集是一组数字。它通常组织为一个表，其中每一列是一个变量，每一行是一个观察值。对于经验分析，数据集是固定的，因为它们是呈现给研究人员的数字。对于统计分析，我们将数据集视为随机的，或者更准确地说是随机过程的实现。\n个别观察可以从一个共同的（同质的）分布中提取，也可以从异质的分布中提取。最简单的方法是假设同质性 - 即观察结果是来自相同基础总体 \\(F\\) 的实现。\n假设 3.1 变量 \\(\\left\\{\\left(Y_{1}, X_{1}\\right), \\ldots,\\left(Y_{i}, X_{i}\\right), \\ldots,\\left(Y_{n}, X_{n}\\right)\\right\\}\\) 同分布；它们来自一个共同的分布 \\(F\\)。这个假设不需要被视为字面上正确的。相反，它是一种有用的建模设备，因此可以很好地定义诸如 \\(\\beta\\) 之类的参数。这个假设应该被解释为我们在实际观察之前如何先验地看待观察。如果我告诉您我们有一个样本，其中 \\(n=59\\) 观察值没有按特定顺序设置，那么查看两个观察值（例如 17 和 58 ）是有意义的，因为它们来自相同的分布。我们没有理由期待这两种观察有什么特别之处。\n在计量经济学理论中，我们将潜在的共同分布 \\(F\\) 称为总体。一些作者更喜欢标签数据生成过程（DGP）。您可以将其视为一个理论概念或无限大的潜在人口。相比之下，我们将 \\(\\left\\{\\left(Y_{i}, X_{i}\\right)\\right.\\) : \\(i=1, \\ldots, n\\}\\) 可用的观察结果称为样本或数据集。在某些情况下，数据集包含所有潜在的观察结果，例如行政税务记录可能包含政治单位中的每个纳税人。即使在这种情况下，我们也可以将观察结果看作是从潜在的无限大群体中随机抽取的，因为这将使我们能够应用统计理论的工具。\n线性投影模型适用于随机变量 \\((Y, X)\\)。这是 2.18 节中描述的概率模型。模型是\n\\[\nY=X^{\\prime} \\beta+e\n\\]\n其中线性投影系数 \\(\\beta\\) 定义为\n\\[\n\\beta=\\underset{b \\in \\mathbb{R}^{k}}{\\operatorname{argmin}} S(b),\n\\]\n期望平方误差的最小值\n\\[\nS(\\beta)=\\mathbb{E}\\left[\\left(Y-X^{\\prime} \\beta\\right)^{2}\\right] .\n\\]\n该系数具有显式解 (3.1)。"
  },
  {
    "objectID": "chpt03-algebra-chn.html#矩估计器",
    "href": "chpt03-algebra-chn.html#矩估计器",
    "title": "最小二乘代数",
    "section": "矩估计器",
    "text": "矩估计器\n我们想从观测样本中估计 (3.1) 中定义的系数 \\(\\beta\\)。请注意，\\(\\beta\\) 是作为特定总体预期的函数编写的。在这种情况下，适当的估计器是样本矩的相同函数。让我们详细解释一下。\n首先，假设我们对具有分布函数 \\(F\\) 的随机变量 \\(Y\\) 的总体均值 \\(\\mu\\) 感兴趣\n\\[\n\\mu=\\mathbb{E}[Y]=\\int_{-\\infty}^{\\infty} y d F(y) .\n\\]\n期望 \\(\\mu\\) 是分布 \\(F\\) 的函数。从 \\(F\\) 给定 \\(n\\) 随机变量 \\(Y_{i}\\) 来估计 \\(\\mu\\)，自然估计量是样本均值\n\\[\n\\widehat{\\mu}=\\bar{Y}=\\frac{1}{n} \\sum_{i=1}^{n} Y_{i} .\n\\]\n请注意，我们使用两种符号来编写它。顶部带有条形的符号 \\(\\bar{Y}\\) 是样本均值的常规符号。带有帽子“\\(\\wedge\\)”的符号 \\(\\widehat{\\mu}\\) 在计量经济学中是传统的，用于表示参数 \\(\\mu\\) 的估计量。在这种情况下，\\(\\bar{Y}\\) 是 \\(\\mu\\) 的估计量，所以 \\(\\widehat{\\mu}\\) 和 \\(\\bar{Y}\\) 是相同的。样本均值 \\(\\bar{Y}\\) 可以被视为总体均值 (3.5) 的自然模拟，因为对于经验分布，\\(\\bar{Y}\\) 等于期望值 (3.5) - 离散分布将权重 \\(\\bar{Y}\\) 放在每个观察 \\(数学12\\)。将 \\(\\bar{Y}\\) 作为 \\(\\bar{Y}\\) 的估计量还有其他理由。我们将暂时推迟这些讨论。可以说它是传统的估计量就足够了。现在假设我们对随机向量 \\(\\bar{Y}\\)，比如说 \\(\\bar{Y}\\) 的可能非线性函数的一组总体期望感兴趣。例如，我们可能对 \\(\\bar{Y}\\) 和 \\(\\bar{Y}\\) 的前两个矩感兴趣。在这种情况下，自然估计量是样本均值的向量，\n\\[\n\\widehat{\\mu}=\\frac{1}{n} \\sum_{i=1}^{n} h\\left(Y_{i}\\right) .\n\\]\n我们称 \\(\\widehat{\\mu}\\) 为 \\(\\mu\\) 的矩估计量。例如，如果 \\(h(y)=\\left(y, y^{2}\\right)^{\\prime}\\) 那么 \\(\\widehat{\\mu}_{1}=n^{-1} \\sum_{i=1}^{n} Y_{i}\\) 和 \\(\\widehat{\\mu}_{2}=\\) \\(n^{-1} \\sum_{i=1}^{n} Y_{i}^{2}\\)\n现在假设我们对一组矩的非线性函数感兴趣。例如，考虑 \\(Y\\) 的方差\n\\[\n\\sigma^{2}=\\operatorname{var}[Y]=\\mathbb{E}\\left[Y^{2}\\right]-(\\mathbb{E}[Y])^{2} .\n\\]\n通常，许多感兴趣的参数可以写成\\(Y\\) 矩的函数。符号上，\\(\\beta=g(\\mu)\\) 和 \\(\\mu=\\mathbb{E}[h(Y)]\\)。这里，\\(Y\\) 是随机变量，\\(h(Y)\\) 是随机变量的函数（变换），\\(\\mu\\) 是这些函数的期望值。 \\(\\beta\\) 是感兴趣的参数，并且是这些期望的（非线性）函数 \\(g(\\cdot)\\)。\n在这种情况下，通过将 \\(\\mu\\) 替换为 \\(\\widehat{\\mu}\\) 来获得 \\(\\beta\\) 的自然估计量。因此 \\(\\widehat{\\beta}=g(\\widehat{\\mu})\\)。估计器 \\(\\widehat{\\beta}\\) 通常称为插件估计器。我们也将 \\(\\widehat{\\beta}\\) 称为 \\(\\beta\\) 的矩或基于矩的估计器，因为它是矩估计器 \\(\\widehat{\\mu}\\) 的自然扩展。\n以方差 \\(\\sigma^{2}=\\operatorname{var}[Y]\\) 为例。它的矩估计量是\n\\[\n\\widehat{\\sigma}^{2}=\\widehat{\\mu}_{2}-\\widehat{\\mu}_{1}^{2}=\\frac{1}{n} \\sum_{i=1}^{n} Y_{i}^{2}-\\left(\\frac{1}{n} \\sum_{i=1}^{n} Y_{i}\\right)^{2}\n\\]\n这不是 \\(\\sigma^{2}\\) 唯一可能的估计器（还有著名的偏差校正估计器），但 \\(\\widehat{\\sigma}^{2}\\) 是一个简单明了的选择。"
  },
  {
    "objectID": "chpt03-algebra-chn.html#最小二乘估计器",
    "href": "chpt03-algebra-chn.html#最小二乘估计器",
    "title": "最小二乘代数",
    "section": "最小二乘估计器",
    "text": "最小二乘估计器\n线性投影系数 \\(\\beta\\) 在 (3.3) 中定义为 (3.4) 中定义的期望平方误差 \\(S(\\beta)\\) 的最小值。对于给定的 \\(\\beta\\)，预期平方误差是平方误差 \\(\\left(Y-X^{\\prime} \\beta\\right)^{2}\\) 的期望值。 \\(S(\\beta)\\) 的矩估计量是样本平均值：\n\\[\n\\widehat{S}(\\beta)=\\frac{1}{n} \\sum_{i=1}^{n}\\left(Y_{i}-X_{i}^{\\prime} \\beta\\right)^{2}=\\frac{1}{n} \\operatorname{SSE}(\\beta)\n\\]\n在哪里\n\\[\n\\operatorname{SSE}(\\beta)=\\sum_{i=1}^{n}\\left(Y_{i}-X_{i}^{\\prime} \\beta\\right)^{2}\n\\]\n称为误差平方和函数。\n由于 \\(\\widehat{S}(\\beta)\\) 是样本平均值，我们可以将其解释为预期平方误差 \\(S(\\beta)\\) 的估计量。将 \\(\\widehat{S}(\\beta)\\) 作为 \\(\\beta\\) 的函数来检查 \\(S(\\beta)\\) 如何随 \\(\\beta\\) 变化。由于投影系数使 \\(S(\\beta)\\) 最小化，因此模拟估计器最小化 (3.6)。\n我们将估计器 \\(\\widehat{\\beta}\\) 定义为 \\(\\widehat{S}(\\beta)\\) 的最小化器。\n定义 \\(3.1\\) 最小二乘估计量是 \\(\\widehat{\\beta}=\\underset{\\beta \\in \\mathbb{R}^{k}}{\\operatorname{argmin}} \\widehat{S}(\\beta)\\)\\ \\(\\widehat{S}(\\beta)=\\frac{1}{n} \\sum_{i=1}^{n}\\left(Y_{i}-X_{i}^{\\prime} \\beta\\right)^{2}\\)\n由于 \\(\\widehat{S}(\\beta)\\) 是 \\(\\operatorname{SSE}(\\beta)\\) 的比例倍数，我们可以等效地将 \\(\\widehat{\\beta}\\) 定义为 \\(\\operatorname{SSE}(\\beta)\\) 的最小化器。因此 \\(\\widehat{\\beta}\\) 通常被称为 \\(\\beta\\) 的最小二乘 (LS) 估计量。估计器通常也称为普通最小二乘 (OLS) 估计器。有关此标签的起源，请参见下面关于 Adrien-Marie Legendre 的历史讨论。在这里，正如计量经济学中常见的那样，我们在参数 \\(\\beta\\) 上加上一个帽子“\\(\\wedge\\)”，以表明 \\(\\widehat{\\beta}\\) 是 \\(\\widehat{S}(\\beta)\\) 的样本估计量。这是一个有用的约定。只需看到符号 \\(\\widehat{S}(\\beta)\\)，我们就可以立即将其解释为参数 \\(\\widehat{S}(\\beta)\\) 的估计量（因为帽子）。有时当我们想明确估计方法时，我们会写 \\(\\widehat{S}(\\beta)\\) 来表示它是 OLS 估计量。符号 \\(\\widehat{S}(\\beta)\\) 也很常见，其中下标“\\(\\widehat{S}(\\beta)\\)”表示估计量取决于样本大小 \\(\\widehat{S}(\\beta)\\)。\n了解总体参数（例如 \\(\\beta\\)）和样本估计量（例如 \\(\\widehat{\\beta}\\)）之间的区别很重要。总体参数 \\(\\beta\\) 是总体的非随机特征，而样本估计量 \\(\\widehat{\\beta}\\) 是随机样本的随机特征。 \\(\\beta\\) 是固定的，而 \\(\\widehat{\\beta}\\) 因样本而异。"
  },
  {
    "objectID": "chpt03-algebra-chn.html#用一个回归器求解最小二乘",
    "href": "chpt03-algebra-chn.html#用一个回归器求解最小二乘",
    "title": "最小二乘代数",
    "section": "用一个回归器求解最小二乘",
    "text": "用一个回归器求解最小二乘\n为简单起见，我们首先考虑 \\(k=1\\) 的情况，因此有一个标量回归量 \\(X\\) 和一个标量系数 \\(\\beta\\)。为了说明，图 3.1(a) 显示了 20 对 \\(\\left(Y_{i}, X_{i}\\right)\\) 的散布 \\(\\operatorname{plot}^{1}\\)。\n误差平方和 \\(\\operatorname{SSE}(\\beta)\\) 是 \\(\\beta\\) 的函数。给定 \\(\\beta\\)，我们通过获取 \\(Y_{i}\\) 和 \\(X_{i} \\beta\\) 之间的垂直距离来计算“误差”\\(Y_{i}-X_{i} \\beta\\)。这可以在图 3.1(a) 中通过将观测值与直线连接起来的垂直线看出。这些垂直线是错误 \\(Y_{i}-X_{i} \\beta\\)。误差平方和是 20 个平方长度之和。\n误差平方和是函数\n\\[\n\\operatorname{SSE}(\\beta)=\\sum_{i=1}^{n}\\left(Y_{i}-X_{i} \\beta\\right)^{2}=\\left(\\sum_{i=1}^{n} Y_{i}^{2}\\right)-2 \\beta\\left(\\sum_{i=1}^{n} X_{i} Y_{i}\\right)+\\beta^{2}\\left(\\sum_{i=1}^{n} X_{i}^{2}\\right) .\n\\]\n这是 \\(\\beta\\) 的二次函数。图 \\(3.1\\) (b) 在 \\([2,4]\\) 范围内显示平方误差函数的总和。系数 \\(\\beta\\) 的范围沿 \\(x\\) 轴。作为 \\(\\beta\\) 函数的误差平方和 \\(\\operatorname{SSE}(\\beta)\\) 显示在 \\(y\\) 轴上。\nOLS 估计器 \\(\\widehat{\\beta}\\) 最小化了这个函数。从初等代数我们知道二次函数 \\(a-2 b x+c x^{2}\\) 的最小值是 \\(x=b / c\\)。因此 \\(\\operatorname{SSE}(\\beta)\\) 的最小化器是\n\\[\n\\widehat{\\beta}=\\frac{\\sum_{i=1}^{n} X_{i} Y_{i}}{\\sum_{i=1}^{n} X_{i}^{2}}\n\\]\n例如，图 3.1(b) 中显示的误差平方和函数的最小值是 \\(\\widehat{\\beta}=3.07\\)，并标记在 \\(\\mathrm{x}\\) 轴上。\n只截取模型是特例 \\(X_{i}=1\\)。在这种情况下，我们发现\n\\[\n\\widehat{\\beta}=\\frac{\\sum_{i=1}^{n} 1 Y_{i}}{\\sum_{i=1}^{n} 1^{2}}=\\frac{1}{n} \\sum_{i=1}^{n} Y_{i}=\\bar{Y},\n\\]\n\\({ }^{1}\\) 观察结果是通过模拟生成的 \\(X \\sim U[0,1]\\) 和 \\(Y \\sim \\mathrm{N}[3 X, 1]\\)。\n\n\n与拟合线的偏差\n\n\n\n平方误差函数之和\n\n图 3.1：使用一个回归器的回归\n\\(Y_{i}\\) 的样本均值。在这里，通常情况下，我们在 \\(Y\\) 上放置一个“-”条，表示该数量是样本均值。这表明仅截距模型中的 OLS 估计量是样本均值。\n从技术上讲，(3.7) 中的估计量 \\(\\widehat{\\beta}\\) 只有在分母非零时才存在。因为它是平方和，所以它必然是非负的。因此，如果 \\(\\sum_{i=1}^{n} X_{i}^{2}>0\\)，则 \\(\\widehat{\\beta}\\) 存在。"
  },
  {
    "objectID": "chpt03-algebra-chn.html#使用多个回归器求解最小二乘",
    "href": "chpt03-algebra-chn.html#使用多个回归器求解最小二乘",
    "title": "最小二乘代数",
    "section": "使用多个回归器求解最小二乘",
    "text": "使用多个回归器求解最小二乘\n我们现在考虑 \\(k>1\\) 的情况，因此系数 \\(\\beta \\in \\mathbb{R}^{k}\\) 是一个向量。\n为了说明，图 \\(3.2\\) 显示了 100 个三元组 \\(\\left(Y_{i}, X_{1 i}, X_{2 i}\\right)\\) 的散点图。回归函数 \\(x^{\\prime} \\beta=x_{1} \\beta_{1}+x_{2} \\beta_{2}\\) 是一个二维曲面，如图 3.2 中的平面所示。\n误差平方和 \\(\\operatorname{SSE}(\\beta)\\) 是向量 \\(\\beta\\) 的函数。对于任何 \\(\\beta\\)，误差 \\(Y_{i}-X_{i}^{\\prime} \\beta\\) 是 \\(Y_{i}\\) 和 \\(X_{i}^{\\prime} \\beta\\) 之间的垂直距离。这可以在图 \\(3.2\\) 中通过将观察结果连接到平面的垂直线看出。在单一回归量的情况下，这些垂直线是错误 \\(e_{i}=Y_{i}-\\) \\(X_{i}^{\\prime} \\beta\\)。误差平方和是 100 个平方长度之和。\n误差平方和可以写为\n\\[\n\\operatorname{SSE}(\\beta)=\\sum_{i=1}^{n} Y_{i}^{2}-2 \\beta^{\\prime} \\sum_{i=1}^{n} X_{i} Y_{i}+\\beta^{\\prime} \\sum_{i=1}^{n} X_{i} X_{i}^{\\prime} \\beta .\n\\]\n与单一回归量情况一样，这是 \\(\\beta\\) 中的二次函数。不同之处在于，在多重回归的情况下，这是一个向量值二次函数。为了使误差平方和函数可视化，图 3.3(a) 显示了 \\(\\operatorname{SSE}(\\beta)\\)。另一种可视化 3 维表面的方法是使用等高线图。图 3.3(b) 显示了相同 \\(\\operatorname{SSE}(\\beta)\\) 函数的等高线图。等高线是 \\(\\left(\\beta_{1}, \\beta_{2}\\right)\\) 空间中的点，其中 \\(\\operatorname{SSE}(\\beta)\\) 取相同的值。等高线是椭圆的，因为 \\(\\operatorname{SSE}(\\beta)\\) 是二次的。\n\n图 3.2：具有两个变量的回归\n最小二乘估计器 \\(\\widehat{\\beta}\\) 最小化 \\(\\operatorname{SSE}(\\beta)\\)。找到最小值的一种简单方法是求解一阶条件。后者是\n\\[\n0=\\frac{\\partial}{\\partial \\beta} \\operatorname{SSE}(\\widehat{\\beta})=-2 \\sum_{i=1}^{n} X_{i} Y_{i}+2 \\sum_{i=1}^{n} X_{i} X_{i}^{\\prime} \\widehat{\\beta}\n\\]\n我们使用单个表达式编写了这个，但它实际上是一个带有 \\(k\\) 未知数（\\(\\widehat{\\beta}\\) 的元素）的 \\(k\\) 方程系统。\n\\(\\widehat{\\beta}\\) 的解可以通过求解 (3.9) 中的 \\(k\\) 方程组来找到。我们可以使用矩阵代数紧凑地编写此解决方案。将 (3.9) 除以 2 我们得到\n\\[\n\\sum_{i=1}^{n} X_{i} X_{i}^{\\prime} \\widehat{\\beta}=\\sum_{i=1}^{n} X_{i} Y_{i} .\n\\]\n这是一个形式为 \\(\\boldsymbol{A} \\boldsymbol{b}=\\boldsymbol{c}\\) 的方程组，其中 \\(\\boldsymbol{A}\\) 是 \\(k \\times k\\)，\\(\\boldsymbol{b}\\) 和 \\(\\boldsymbol{c}\\) 是 \\(k \\times 1\\)。解是 \\(\\boldsymbol{b}=\\boldsymbol{A}^{-1} \\boldsymbol{c}\\)，可以通过将 \\(\\boldsymbol{A} \\boldsymbol{b}=\\boldsymbol{c}\\) 与 \\(\\boldsymbol{A}^{-1}\\) 预乘并使用矩阵逆属性 \\(\\boldsymbol{A} \\boldsymbol{b}=\\boldsymbol{c}\\) 来获得。应用于（3.10），我们找到了最小二乘估计量的明确公式\n\\[\n\\widehat{\\beta}=\\left(\\sum_{i=1}^{n} X_{i} X_{i}^{\\prime}\\right)^{-1}\\left(\\sum_{i=1}^{n} X_{i} Y_{i}\\right) .\n\\]\n这是（3.3）中定义的最佳线性投影系数\\(\\beta\\)的自然估计量，也可以称为线性投影估计量。\n\n\n平方误差函数之和\n\n\n\n上证所轮廓\n\n图 3.3：具有两个回归量的 SSE\n回想一下，我们声称 (3.11) 中的 \\(\\widehat{\\beta}\\) 是 \\(\\operatorname{SSE}(\\beta)\\) 的最小值，并通过求解一阶条件找到它。为了完整，我们应该验证二阶条件。我们计算得出\n\\[\n\\frac{\\partial^{2}}{\\partial \\beta \\partial \\beta^{\\prime}} \\operatorname{SSE}(\\beta)=2 \\sum_{i=1}^{n} X_{i} X_{i}^{\\prime}\n\\]\n是一个半正定矩阵。如果实际上是正定的，则满足最小化的二阶条件，在这种情况下 \\(\\widehat{\\beta}\\) 是 \\(\\operatorname{SSE}(\\beta)\\) 的唯一最小化器。\n回到图 \\(3.3\\) 中显示的误差平方和函数 \\(\\operatorname{SSE}(\\beta)\\) 的示例，最小二乘估计器 \\(\\widehat{\\beta}\\) 是使该函数最小化的对 \\(\\left(\\widehat{\\beta}_{1}, \\widehat{\\beta}_{2}\\right)\\)；从视觉上看，它是 3 维图中的低点，在图 3.3(b) 中标记为等高线图的中心点。\n取方程 (3.11) 并假设 \\(k=1\\)。在这种情况下，\\(X_{i}\\) 是标量，所以 \\(X_{i} X_{i}^{\\prime}=X_{i}^{2}\\)。然后 (3.11) 简化为先前导出的表达式 (3.7)。表达式 (3.11) 是一个符号简单的概括，但需要仔细注意向量和矩阵操作。\n或者，等式 (3.1) 将投影系数 \\(\\beta\\) 写为总体矩 \\(\\boldsymbol{Q}_{X Y}\\) 和 \\(\\boldsymbol{Q}_{X X}\\) 的显式函数。他们的矩估计器是样本矩\n\\[\n\\begin{aligned}\n\\widehat{\\boldsymbol{Q}}_{X Y} &=\\frac{1}{n} \\sum_{i=1}^{n} X_{i} Y_{i} \\\\\n\\widehat{\\boldsymbol{Q}}_{X X} &=\\frac{1}{n} \\sum_{i=1}^{n} X_{i} X_{i}^{\\prime}\n\\end{aligned}\n\\]\n\\(\\beta\\) 的矩估计器将 (3.1) 中的总体矩替换为样本矩：\n\\[\n\\begin{aligned}\n\\widehat{\\beta} &=\\widehat{\\boldsymbol{Q}}_{X X}^{-1} \\widehat{\\boldsymbol{Q}}_{X Y} \\\\\n&=\\left(\\frac{1}{n} \\sum_{i=1}^{n} X_{i} X_{i}^{\\prime}\\right)^{-1}\\left(\\frac{1}{n} \\sum_{i=1}^{n} X_{i} Y_{i}\\right) \\\\\n&=\\left(\\sum_{i=1}^{n} X_{i} X_{i}^{\\prime}\\right)^{-1}\\left(\\sum_{i=1}^{n} X_{i} Y_{i}\\right)\n\\end{aligned}\n\\]\n与 (3.11) 相同。\n从技术上讲，估计量 \\(\\widehat{\\beta}\\) 是唯一的，并且仅当倒置矩阵实际上是可逆的时才等于 (3.11)，当（且仅当）该矩阵是正定时，它才成立。这不包括 \\(X_{i}\\) 包含冗余回归量的情况。这将在第 3.24 节中进一步讨论。\n定理 3.1 如果 \\(\\sum_{i=1}^{n} X_{i} X_{i}^{\\prime}>0\\)，最小二乘估计量是唯一的并且等于\n\\[\n\\widehat{\\beta}=\\left(\\sum_{i=1}^{n} X_{i} X_{i}^{\\prime}\\right)^{-1}\\left(\\sum_{i=1}^{n} X_{i} Y_{i}\\right) .\n\\]"
  },
  {
    "objectID": "chpt03-algebra-chn.html#阿德里安玛丽勒让德",
    "href": "chpt03-algebra-chn.html#阿德里安玛丽勒让德",
    "title": "最小二乘代数",
    "section": "阿德里安·玛丽·勒让德",
    "text": "阿德里安·玛丽·勒让德\n最小二乘法由法国数学家 Adrien-Marie Legendre (1752-1833) 于 1805 年发表。当方程的数量超过未知数的数量时，勒让德提出最小二乘法作为求解方程组的代数问题的解决方案。这是天文测量中一个令人头疼的普遍问题。在勒让德看来，(3.2) 是一组具有 \\(k\\) 未知数的 \\(n\\) 方程。由于方程不能精确求解，Legendre 的目标是选择 \\(\\beta\\) 以使误差集尽可能小。他提出了误差平方和的准则，并推导出了上面提出的代数解。正如他所指出的，一阶条件 (3.9) 是具有 \\(k\\) 未知数的 \\(k\\) 方程组，可以通过“普通”方法求解。因此该方法被称为普通最小二乘法，直到今天我们仍然使用缩写 OLS 来指代勒让德的估计方法。"
  },
  {
    "objectID": "chpt03-algebra-chn.html#插图",
    "href": "chpt03-algebra-chn.html#插图",
    "title": "最小二乘代数",
    "section": "插图",
    "text": "插图\n我们使用用于计算第 2 章中报告的估计值的数据集来说明实践中的最小二乘估计量。这是 2009 年 3 月的当前人口调查，其中包含有关美国人口的大量信息。该数据集在第 3.22 节中有更详细的描述。在本例中，我们使用已婚（配偶在场）具有 12 年潜在工作经验的黑人女性工薪族的子样本。这个子样本有 20 个观测值 \\({ }^{2}\\)。\n在表 \\(3.1\\) 中，我们显示观察结果以供参考。每一行都是一个单独的观察，它是一个人的数据。这些列对应于个体的变量（测量值）。第二列是报告的工资（年总收入除以工作时间）。第三列是工资的自然对数。第四栏是受教育年限。第五和第六列是进一步的变换，具体是教育的平方和教育与\\(\\log\\)（工资）的乘积。底行是该列中元素的总和。\n表 3.1：来自 CPS 数据集的观察结果\n\n\n\n\n\n\n\n\n\n\n\nObservation\nwage\n\\(\\log (\\) wage)\neducation\neducation \\(^{2}\\)\neducation \\(\\times \\log (\\) wage \\()\\)\n\n\n\n\n1\n\\(37.93\\)\n\\(3.64\\)\n18\n324\n\\(65.44\\)\n\n\n2\n\\(40.87\\)\n\\(3.71\\)\n18\n324\n\\(66.79\\)\n\n\n3\n\\(14.18\\)\n\\(2.65\\)\n13\n169\n\\(34.48\\)\n\n\n4\n\\(16.83\\)\n\\(2.82\\)\n16\n256\n\\(45.17\\)\n\n\n5\n\\(33.17\\)\n\\(3.50\\)\n16\n256\n\\(56.03\\)\n\n\n6\n\\(29.81\\)\n\\(3.39\\)\n18\n324\n\\(61.11\\)\n\n\n7\n\\(54.62\\)\n\\(4.00\\)\n16\n256\n\\(64.00\\)\n\n\n8\n\\(43.08\\)\n\\(3.76\\)\n18\n324\n\\(67.73\\)\n\n\n9\n\\(14.42\\)\n\\(2.67\\)\n12\n144\n\\(32.03\\)\n\n\n10\n\\(14.90\\)\n\\(2.70\\)\n16\n256\n\\(43.23\\)\n\n\n11\n\\(21.63\\)\n\\(3.07\\)\n18\n324\n\\(55.44\\)\n\n\n12\n\\(11.09\\)\n\\(2.41\\)\n16\n256\n\\(38.50\\)\n\n\n13\n\\(10.00\\)\n\\(2.30\\)\n13\n169\n\\(29.93\\)\n\n\n14\n\\(31.73\\)\n\\(3.46\\)\n14\n196\n\\(48.40\\)\n\n\n15\n\\(11.06\\)\n\\(2.40\\)\n12\n144\n\\(28.84\\)\n\n\n16\n\\(18.75\\)\n\\(2.93\\)\n16\n256\n\\(46.90\\)\n\n\n17\n\\(27.35\\)\n\\(3.31\\)\n14\n196\n\\(46.32\\)\n\n\n18\n\\(24.04\\)\n\\(3.18\\)\n16\n256\n\\(50.76\\)\n\n\n19\n\\(36.06\\)\n\\(3.59\\)\n18\n324\n\\(64.53\\)\n\n\n20\n\\(23.08\\)\n\\(3.14\\)\n16\n256\n\\(50.22\\)\n\n\nSum\n515\n\\(62.64\\)\n314\n5010\n\\(995.86\\)\n\n\n\n将变量放入标准回归符号中，令 \\(Y_{i}\\) 为 \\(\\log (w a g e)\\)，\\(X_{i}\\) 为教育年限和截距。然后从表 \\(3.1\\) 中的列总和我们有\n\\[\n\\sum_{i=1}^{n} X_{i} Y_{i}=\\left(\\begin{array}{c}\n995.86 \\\\\n62.64\n\\end{array}\\right)\n\\]\n和\n\\[\n\\sum_{i=1}^{n} X_{i} X_{i}^{\\prime}=\\left(\\begin{array}{cc}\n5010 & 314 \\\\\n314 & 20\n\\end{array}\\right)\n\\]\n取逆我们得到\n\\[\n\\left(\\sum_{i=1}^{n} X_{i} X_{i}^{\\prime}\\right)^{-1}=\\left(\\begin{array}{cc}\n0.0125 & -0.196 \\\\\n-0.196 & 3.124\n\\end{array}\\right) .\n\\]\n\\({ }^{2}\\) 这个样本是专门选择的，所以它有少量的观察，便于说明。因此通过矩阵乘法\n\\[\n\\widehat{\\beta}=\\left(\\begin{array}{cc}\n0.0125 & -0.196 \\\\\n-0.196 & 3.124\n\\end{array}\\right)\\left(\\begin{array}{c}\n995.86 \\\\\n62.64\n\\end{array}\\right)=\\left(\\begin{array}{c}\n0.155 \\\\\n0.698\n\\end{array}\\right) .\n\\]\n在实践中，回归估计 \\(\\widehat{\\beta}\\) 是由计算机软件计算的，用户无需执行上面列出的明确步骤。然而，了解最小二乘估计量可以通过简单的代数运算来计算是有用的。如果您的数据位于类似于表 3.1 的电子表格中，则可以通过电子表格操作计算列出的转换（对数、平方、叉积、列和）。然后可以通过矩阵求逆和乘法计算 \\(\\widehat{\\beta}\\)。再一次，应用经济学家很少这样做，因为可以使用计算机软件来简化这一过程。\n我们经常使用以下格式编写估计方程\n\\[\n\\widehat{\\log (\\text { wage })}=0.155 \\text { education }+0.698 \\text {. }\n\\]\n对估计方程的解释是，每一年的教育都与平均工资增加 \\(16 %\\) 相关。\n估计方程（3.12）的另一个用途是用于预测。假设一个人有 12 年的教育，第二个有 16 年。使用 (3.12) 我们发现第一个人的期望对数工资是\n\\[\n\\widehat{\\log (\\text { wag } e)}=0.155 \\times 12+0.698=2.56\n\\]\n第二个\n\\[\n\\widehat{\\log (\\text { wage })}=0.155 \\times 16+0.698=3.18 .\n\\]\n方程（3.12）被称为二元回归，因为有两个变量。它也称为简单回归，因为只有一个回归量。多元回归有两个或更多回归量，并允许进行更详细的调查。让我们举一个类似于 (3.12) 的例子，但包括所有级别的经验。这次我们使用有 268 个观察值的单身（未婚）亚洲男性的子样本。包括作为回归器的潜在工作经验（经验）及其平方（经验 \\({ }^{2} / 100\\) ）（我们除以 100 以简化报告），我们获得了估计值\n\\[\n\\widehat{\\log (\\text { wage })}=0.143 \\text { education }+0.036 \\text { experience }-0.071 \\text { experience }^{2} / 100+0.575 \\text {. }\n\\]\n这些估计表明，在保持经验不变的情况下，每年的平均工资增加了 \\(14 %\\)。"
  },
  {
    "objectID": "chpt03-algebra-chn.html#最小二乘残差",
    "href": "chpt03-algebra-chn.html#最小二乘残差",
    "title": "最小二乘代数",
    "section": "最小二乘残差",
    "text": "最小二乘残差\n作为估计的副产品，我们定义了拟合值 \\(\\widehat{Y}_{i}=X_{i}^{\\prime} \\widehat{\\beta}\\) 和残差\n\\[\n\\widehat{e}_{i}=Y_{i}-\\widehat{Y}_{i}=Y_{i}-X_{i}^{\\prime} \\widehat{\\beta}\n\\]\n有时 \\(\\widehat{Y}_{i}\\) 被称为预测值，但这是一个误导性标签。拟合值 \\(\\widehat{Y}_{i}\\) 是包括 \\(Y_{i}\\) 在内的整个样本的函数，因此不能解释为 \\(Y_{i}\\) 的有效预测。因此，将 \\(\\widehat{Y}_{i}\\) 描述为拟合值而不是预测值更准确。\n注意 \\(Y_{i}=\\widehat{Y}_{i}+\\widehat{e}_{i}\\) 和\n\\[\nY_{i}=X_{i}^{\\prime} \\widehat{\\beta}+\\widehat{e}_{i} .\n\\]\n我们区分了误差 \\(e_{i}\\) 和残差 \\(\\widehat{e}_{i}\\)。误差 \\(e_{i}\\) 是不可观察的，而残差 \\(\\widehat{e}_{i}\\) 是一个估计量。这两个变量经常被错误标记，这可能会导致混淆。等式 (3.9) 意味着\n\\[\n\\sum_{i=1}^{n} X_{i} \\widehat{e}_{i}=0 .\n\\]\n要通过直接计算看到这一点，使用 (3.14) 和 (3.11)，\n\\[\n\\begin{aligned}\n\\sum_{i=1}^{n} X_{i} \\widehat{e}_{i} &=\\sum_{i=1}^{n} X_{i}\\left(Y_{i}-X_{i}^{\\prime} \\widehat{\\beta}\\right) \\\\\n&=\\sum_{i=1}^{n} X_{i} Y_{i}-\\sum_{i=1}^{n} X_{i} X_{i}^{\\prime} \\widehat{\\beta} \\\\\n&=\\sum_{i=1}^{n} X_{i} Y_{i}-\\sum_{i=1}^{n} X_{i} X_{i}^{\\prime}\\left(\\sum_{i=1}^{n} X_{i} X_{i}^{\\prime}\\right)^{-1}\\left(\\sum_{i=1}^{n} X_{i} Y_{i}\\right) \\\\\n&=\\sum_{i=1}^{n} X_{i} Y_{i}-\\sum_{i=1}^{n} X_{i} Y_{i}=0 .\n\\end{aligned}\n\\]\n当 \\(X_{i}\\) 包含一个常数时， (3.16) 的含义是\n\\[\n\\frac{1}{n} \\sum_{i=1}^{n} \\widehat{e}_{i}=0 .\n\\]\n因此，残差的样本均值为零，回归量和残差之间的样本相关性为零。这些是代数结果，适用于所有线性回归估计。"
  },
  {
    "objectID": "chpt03-algebra-chn.html#贬低回归者",
    "href": "chpt03-algebra-chn.html#贬低回归者",
    "title": "最小二乘代数",
    "section": "贬低回归者",
    "text": "贬低回归者\n有时将常数与其他回归量分开并将线性投影方程写成格式是有用的\n\\[\nY_{i}=X_{i}^{\\prime} \\beta+\\alpha+e_{i}\n\\]\n其中 \\(\\alpha\\) 是截距，\\(X_{i}\\) 不包含常数。最小二乘估计和残差可以写成 \\(Y_{i}=X_{i}^{\\prime} \\widehat{\\beta}+\\widehat{\\alpha}+\\widehat{e}_{i}\\)。\n在这种情况下（3.16）可以写成方程组\n\\[\n\\begin{array}{r}\n\\sum_{i=1}^{n}\\left(Y_{i}-X_{i}^{\\prime} \\widehat{\\beta}-\\widehat{\\alpha}\\right)=0 \\\\\n\\sum_{i=1}^{n} X_{i}\\left(Y_{i}-X_{i}^{\\prime} \\widehat{\\beta}-\\widehat{\\alpha}\\right)=0 .\n\\end{array}\n\\]\n第一个方程意味着\n\\[\n\\widehat{\\alpha}=\\bar{Y}-\\bar{X}^{\\prime} \\widehat{\\beta} .\n\\]\n从我们得到的秒中减去\n\\[\n\\sum_{i=1}^{n} X_{i}\\left(\\left(Y_{i}-\\bar{Y}\\right)-\\left(X_{i}-\\bar{X}\\right)^{\\prime} \\widehat{\\beta}\\right)=0 .\n\\]\n求解 \\(\\widehat{\\beta}\\) 我们发现\n\\[\n\\begin{aligned}\n\\widehat{\\beta} &=\\left(\\sum_{i=1}^{n} X_{i}\\left(X_{i}-\\bar{X}\\right)^{\\prime}\\right)^{-1}\\left(\\sum_{i=1}^{n} X_{i}\\left(Y_{i}-\\bar{Y}\\right)\\right) \\\\\n&=\\left(\\sum_{i=1}^{n}\\left(X_{i}-\\bar{X}\\right)\\left(X_{i}-\\bar{X}\\right)^{\\prime}\\right)^{-1}\\left(\\sum_{i=1}^{n}\\left(X_{i}-\\bar{X}\\right)\\left(Y_{i}-\\bar{Y}\\right)\\right) .\n\\end{aligned}\n\\]\n因此，斜率系数的 OLS 估计量是具有退化数据且没有截距的 OLS。\n表示 (3.18) 被称为最小二乘估计量的退化公式。"
  },
  {
    "objectID": "chpt03-algebra-chn.html#矩阵表示法模型",
    "href": "chpt03-algebra-chn.html#矩阵表示法模型",
    "title": "最小二乘代数",
    "section": "矩阵表示法模型",
    "text": "矩阵表示法模型\n对于许多目的，包括计算，用矩阵表示法编写模型和统计数据很方便。 \\(n\\) 线性方程 \\(Y_{i}=X_{i}^{\\prime} \\beta+e_{i}\\) 构成了一个 \\(n\\) 方程组。我们可以将这些 \\(n\\) 方程堆叠在一起作为\n\\[\n\\begin{aligned}\n&Y_{1}=X_{1}^{\\prime} \\beta+e_{1} \\\\\n&Y_{2}=X_{2}^{\\prime} \\beta+e_{2} \\\\\n&\\vdots \\\\\n&Y_{n}=X_{n}^{\\prime} \\beta+e_{n} .\n\\end{aligned}\n\\]\n定义\n\\[\n\\boldsymbol{Y}=\\left(\\begin{array}{c}\nY_{1} \\\\\nY_{2} \\\\\n\\vdots \\\\\nY_{n}\n\\end{array}\\right), \\quad \\boldsymbol{X}=\\left(\\begin{array}{c}\nX_{1}^{\\prime} \\\\\nX_{2}^{\\prime} \\\\\n\\vdots \\\\\nX_{n}^{\\prime}\n\\end{array}\\right), \\quad \\boldsymbol{e}=\\left(\\begin{array}{c}\ne_{1} \\\\\ne_{2} \\\\\n\\vdots \\\\\ne_{n}\n\\end{array}\\right)\n\\]\n观察 \\(\\boldsymbol{Y}\\) 和 \\(\\boldsymbol{e}\\) 是 \\(n \\times 1\\) 向量，\\(\\boldsymbol{X}\\) 是 \\(n \\times k\\) 矩阵。 \\(n\\) 方程组可以紧凑地写成单个方程\n\\[\n\\boldsymbol{Y}=\\boldsymbol{X} \\beta+\\boldsymbol{e} .\n\\]\n样本总和可以用矩阵表示法编写。例如\n\\[\n\\begin{aligned}\n&\\sum_{i=1}^{n} X_{i} X_{i}^{\\prime}=\\boldsymbol{X}^{\\prime} \\boldsymbol{X} \\\\\n&\\sum_{i=1}^{n} X_{i} Y_{i}=\\boldsymbol{X}^{\\prime} \\boldsymbol{Y} .\n\\end{aligned}\n\\]\n因此最小二乘估计量可以写成\n\\[\n\\widehat{\\beta}=\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{Y}\\right) .\n\\]\n(3.15) 的矩阵版本和 (3.19) 的估计版本是\n\\[\n\\boldsymbol{Y}=\\boldsymbol{X} \\widehat{\\beta}+\\widehat{\\boldsymbol{e}} .\n\\]\n等价的残差向量是\n\\[\n\\widehat{\\boldsymbol{e}}=\\boldsymbol{Y}-\\boldsymbol{X} \\widehat{\\beta}\n\\]\n使用残差向量，我们可以将 (3.16) 写为\n\\[\n\\boldsymbol{X}^{\\prime} \\widehat{\\boldsymbol{e}}=0\n\\]\n将误差平方和标准写为\n\\[\n\\operatorname{SSE}(\\beta)=(\\boldsymbol{Y}-\\boldsymbol{X} \\beta)^{\\prime}(\\boldsymbol{Y}-\\boldsymbol{X} \\beta) .\n\\]\n使用矩阵表示法，我们对大多数估计器都有简单的表达式。这对于计算机编程特别方便，因为大多数语言都允许矩阵表示法和操作。定理 3.2 重要的矩阵表达式\n\\[\n\\begin{aligned}\n\\widehat{\\beta} &=\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{Y}\\right) \\\\\n\\widehat{\\boldsymbol{e}} &=\\boldsymbol{Y}-\\boldsymbol{X} \\widehat{\\beta} \\\\\n\\boldsymbol{X}^{\\prime} \\widehat{\\boldsymbol{e}} &=0 .\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "chpt03-algebra-chn.html#早期使用矩阵",
    "href": "chpt03-algebra-chn.html#早期使用矩阵",
    "title": "最小二乘代数",
    "section": "早期使用矩阵",
    "text": "早期使用矩阵\n已知最早使用矩阵方法求解联立系统的处理方法见于公元前 \\(10^{\\text {th }}\\) 至 \\(2^{\\text {nd }}\\) 世纪几代学者所著的《数学艺术九章》的第 8 章。"
  },
  {
    "objectID": "chpt03-algebra-chn.html#投影矩阵",
    "href": "chpt03-algebra-chn.html#投影矩阵",
    "title": "最小二乘代数",
    "section": "投影矩阵",
    "text": "投影矩阵\n定义矩阵\n\\[\n\\boldsymbol{P}=\\boldsymbol{X}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\boldsymbol{X}^{\\prime}\n\\]\n请注意\n\\[\n\\boldsymbol{P} \\boldsymbol{X}=\\boldsymbol{X}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\boldsymbol{X}^{\\prime} \\boldsymbol{X}=\\boldsymbol{X} .\n\\]\n这是投影矩阵的属性。更一般地，对于任何矩阵 \\(\\boldsymbol{Z}\\) 可以写成 \\(\\boldsymbol{Z}=\\boldsymbol{X} \\boldsymbol{\\Gamma}\\) 对于某个矩阵 \\(\\Gamma\\) （我们说 \\(\\boldsymbol{Z}\\) 位于 \\(\\boldsymbol{X}\\) 的范围空间中），然后\n\\[\n\\boldsymbol{P Z}=\\boldsymbol{P} \\boldsymbol{X} \\boldsymbol{\\Gamma}=\\boldsymbol{X}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\boldsymbol{X}^{\\prime} \\boldsymbol{X} \\boldsymbol{\\Gamma}=\\boldsymbol{X} \\boldsymbol{\\Gamma}=\\boldsymbol{Z} .\n\\]\n举一个重要的例子，如果我们将矩阵 \\(\\boldsymbol{X}\\) 划分为两个矩阵 \\(\\boldsymbol{X}_{1}\\) 和 \\(\\boldsymbol{X}_{2}\\)，那么 \\(\\boldsymbol{X}=\\) 和 \\(\\left[\\begin{array}{ll}\\boldsymbol{X}_{1} & \\boldsymbol{X}_{2}\\end{array}\\right]\\) 然后是 \\(\\boldsymbol{P} \\boldsymbol{X}_{1}=\\boldsymbol{X}_{1}\\)。 （见练习 3.7。）\n投影矩阵 \\(\\boldsymbol{P}\\) 具有幂等的代数性质：\\(\\boldsymbol{P} \\boldsymbol{P}=\\boldsymbol{P}\\)。见下文定理 3.3.2。有关投影矩阵的一般属性，请参见第 A.11 节。\n矩阵 \\(\\boldsymbol{P}\\) 在最小二乘回归中创建拟合值：\n\\[\n\\boldsymbol{P} \\boldsymbol{Y}=\\boldsymbol{X}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\boldsymbol{X}^{\\prime} \\boldsymbol{Y}=\\boldsymbol{X} \\widehat{\\boldsymbol{\\beta}}=\\widehat{\\boldsymbol{Y}} \\text {. }\n\\]\n由于这个属性，\\(\\boldsymbol{P}\\) 也被称为帽子矩阵。\n当 \\(X=\\mathbf{1}_{n}\\) 是一个由 1 组成的 \\(n\\) 向量时，会出现一个投影矩阵的特殊示例。然后\n\\[\n\\boldsymbol{P}=\\mathbf{1}_{n}\\left(\\mathbf{1}_{n}^{\\prime} \\mathbf{1}_{n}\\right)^{-1} \\mathbf{1}_{n}^{\\prime}=\\frac{1}{n} \\mathbf{1}_{n} \\mathbf{1}_{n}^{\\prime} .\n\\]\n请注意，在这种情况下\n\\[\n\\boldsymbol{P} \\boldsymbol{Y}=\\mathbf{1}_{n}\\left(\\mathbf{1}_{n}^{\\prime} \\mathbf{1}_{n}\\right)^{-1} \\mathbf{1}_{n}^{\\prime} \\boldsymbol{Y}=\\mathbf{1}_{n} \\bar{Y}\n\\]\n创建一个 \\(n\\)-vector，其元素是样本均值 \\(\\bar{Y}\\)。\n投影矩阵 \\(\\boldsymbol{P}\\) 经常出现在最小二乘回归的代数运算中。该矩阵具有以下重要性质。定理 3.3 任何 \\(n \\times k \\boldsymbol{X}\\) 与 \\(n \\geq\\) \\(k\\) 的投影矩阵 \\(\\boldsymbol{P}=\\boldsymbol{X}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\boldsymbol{X}^{\\prime}\\) 具有以下代数性质。\n\n\\(\\boldsymbol{P}\\) 是对称的 \\(\\left(\\boldsymbol{P}^{\\prime}=\\boldsymbol{P}\\right)\\)。\n\\(\\boldsymbol{P}\\) 是幂等的 \\((\\boldsymbol{P P}=\\boldsymbol{P})\\)。\n\\(\\operatorname{tr} \\boldsymbol{P}=k\\)。\n\\(\\boldsymbol{P}\\) 的特征值为 1 和 0 。\n\\(\\boldsymbol{P}\\) 的 \\(k\\) 特征值等于 1 和 \\(n-k\\) 等于 0 。\n\\(\\operatorname{rank}(\\boldsymbol{P})=k\\)。\n\n我们通过证明定理 3.3 中的主张来结束本节。第 1 部分成立，因为\n\\[\n\\begin{aligned}\n\\boldsymbol{P}^{\\prime} &=\\left(\\boldsymbol{X}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\boldsymbol{X}^{\\prime}\\right)^{\\prime} \\\\\n&=\\left(\\boldsymbol{X}^{\\prime}\\right)^{\\prime}\\left(\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1}\\right)^{\\prime}(\\boldsymbol{X})^{\\prime} \\\\\n&=\\boldsymbol{X}\\left(\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{\\prime}\\right)^{-1} \\boldsymbol{X}^{\\prime} \\\\\n&=\\boldsymbol{X}\\left((\\boldsymbol{X})^{\\prime}\\left(\\boldsymbol{X}^{\\prime}\\right)^{\\prime}\\right)^{-1} \\boldsymbol{X}^{\\prime}=\\boldsymbol{P} .\n\\end{aligned}\n\\]\n为了建立第 2 部分，\\(\\boldsymbol{P X}=\\boldsymbol{X}\\) 的事实意味着\n\\[\n\\boldsymbol{P} \\boldsymbol{P}=\\boldsymbol{P} \\boldsymbol{X}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\boldsymbol{X}^{\\prime}=\\boldsymbol{X}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\boldsymbol{X}^{\\prime}=\\boldsymbol{P}\n\\]\n如声称的那样。对于第 3 部分，\n\\[\n\\operatorname{tr} \\boldsymbol{P}=\\operatorname{tr}\\left(\\boldsymbol{X}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\boldsymbol{X}^{\\prime}\\right)=\\operatorname{tr}\\left(\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)=\\operatorname{tr}\\left(\\boldsymbol{I}_{k}\\right)=k .\n\\]\n跟踪算子的定义和属性见附录 A.5。\n附录 A.11 表明第 4 部分适用于任何幂等矩阵。对于第 5 部分，由于 \\(\\operatorname{tr} \\boldsymbol{P}\\) 等于第 3 部分的 \\(n\\) 特征值和 \\(\\operatorname{tr} \\boldsymbol{P}=k\\) 之和，因此有 \\(k\\) 特征值等于 1，其余 \\(n-k\\) 等于 0。\n对于第 6 部分，观察 \\(\\boldsymbol{P}\\) 是半正定的，因为它的特征值都是非负的。根据定理 A.4.5，它的秩等于正特征值的数量，即声称的 \\(k\\)。"
  },
  {
    "objectID": "chpt03-algebra-chn.html#歼灭者矩阵",
    "href": "chpt03-algebra-chn.html#歼灭者矩阵",
    "title": "最小二乘代数",
    "section": "歼灭者矩阵",
    "text": "歼灭者矩阵\n定义\n\\[\n\\boldsymbol{M}=\\boldsymbol{I}_{n}-\\boldsymbol{P}=\\boldsymbol{I}_{n}-\\boldsymbol{X}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\boldsymbol{X}^{\\prime}\n\\]\n其中 \\(\\boldsymbol{I}_{n}\\) 是 \\(n \\times n\\) 单位矩阵。注意\n\\[\n\\boldsymbol{M} \\boldsymbol{X}=\\left(\\boldsymbol{I}_{n}-\\boldsymbol{P}\\right) \\boldsymbol{X}=\\boldsymbol{X}-\\boldsymbol{P} \\boldsymbol{X}=\\boldsymbol{X}-\\boldsymbol{X}=0 .\n\\]\n因此 \\(\\boldsymbol{M}\\) 和 \\(\\boldsymbol{X}\\) 是正交的。我们称 \\(\\boldsymbol{M}\\) 为歼灭矩阵，因为对于 \\(\\boldsymbol{X}\\) 的范围空间中的任何矩阵 \\(\\boldsymbol{Z}\\)，那么\n\\[\nM Z=Z-P Z=0 .\n\\]\n例如，\\(\\boldsymbol{M} \\boldsymbol{X}_{1}=0\\) 表示 \\(\\boldsymbol{X}\\) 和 \\(\\boldsymbol{M P}=0\\) 的任何子组件 \\(\\boldsymbol{X}_{1}\\)（参见练习 3.7）。\n湮没矩阵 \\(\\boldsymbol{M}\\) 与 \\(\\boldsymbol{P}\\) 具有相似的性质，包括 \\(\\boldsymbol{M}\\) 是对称的 \\(\\left(\\boldsymbol{M}^{\\prime}=\\boldsymbol{M}\\right)\\) 和幂等的 \\((\\boldsymbol{M} M=\\boldsymbol{M})\\)。因此它是一个投影矩阵。与定理 3.3.3 类似，我们可以计算\n\\[\n\\operatorname{tr} M=n-k .\n\\]\n（见习题 3.9。）一个暗示是 \\(\\boldsymbol{M}\\) 的秩是 \\(n-k\\)。\n\\(\\boldsymbol{P}\\) 创建拟合值，\\(\\boldsymbol{M}\\) 创建最小二乘残差：\n\\[\nM Y=Y-P Y=Y-X \\widehat{\\beta}=\\widehat{\\boldsymbol{e}} .\n\\]\n如上一节所述，投影矩阵的一个特殊示例出现在 \\(\\boldsymbol{X}=\\mathbf{1}_{n}\\) 是一个由 1 组成的 \\(n\\)-vector 时，因此 \\(\\boldsymbol{P}=\\mathbf{1}_{n}\\left(\\mathbf{1}_{n}^{\\prime} \\mathbf{1}_{n}\\right)^{-1} \\mathbf{1}_{n}^{\\prime}\\).相关的湮没矩阵是\n\\[\n\\boldsymbol{M}=\\boldsymbol{I}_{n}-\\boldsymbol{P}=\\boldsymbol{I}_{n}-\\mathbf{1}_{n}\\left(\\mathbf{1}_{n}^{\\prime} \\mathbf{1}_{n}\\right)^{-1} \\mathbf{1}_{n}^{\\prime} .\n\\]\n\\(\\boldsymbol{P}\\) 创建样本均值向量，\\(\\boldsymbol{M}\\) 创建贬值值：\n\\[\n\\boldsymbol{M Y}=\\boldsymbol{Y}-\\mathbf{1}_{n} \\bar{Y} .\n\\]\n为简单起见，我们通常将右侧写为 \\(Y-\\bar{Y}\\)。 \\(i^{t h}\\) 元素是 \\(Y_{i}-\\bar{Y}\\)，\\(Y_{i}\\) 的贬值\n我们还可以使用 (3.23) 为残差向量写一个替代表达式。将 \\(\\boldsymbol{Y}=\\) \\(\\boldsymbol{X} \\beta+\\boldsymbol{e}\\) 代入 \\(\\widehat{\\boldsymbol{e}}=\\boldsymbol{M} \\boldsymbol{Y}\\) 并使用 \\(\\boldsymbol{M} \\boldsymbol{X}=\\mathbf{0}\\) 我们发现\n\\[\n\\widehat{\\boldsymbol{e}}=\\boldsymbol{M} \\boldsymbol{Y}=\\boldsymbol{M}(\\boldsymbol{X} \\beta+\\boldsymbol{e})=\\boldsymbol{M} \\boldsymbol{e}\n\\]\n它不依赖于回归系数 \\(\\beta\\)。"
  },
  {
    "objectID": "chpt03-algebra-chn.html#误差方差的估计",
    "href": "chpt03-algebra-chn.html#误差方差的估计",
    "title": "最小二乘代数",
    "section": "误差方差的估计",
    "text": "误差方差的估计\n误差方差 \\(\\sigma^{2}=\\mathbb{E}\\left[e^{2}\\right]\\) 是矩，因此自然估计量是矩估计量。如果观察到 \\(e_{i}\\)，我们将估计 \\(\\sigma^{2}\\)\n\\[\n\\widetilde{\\sigma}^{2}=\\frac{1}{n} \\sum_{i=1}^{n} e_{i}^{2} .\n\\]\n但是，这是不可行的，因为没有观察到 \\(e_{i}\\)。在这种情况下，通常采用两步法进行估计。第一步计算残差\\(\\widehat{e}_{i}\\)，然后我们将表达式（3.25）中的\\(\\widehat{e}_{i}\\)代入\\(e_{i}\\)，得到可行估计量\n\\[\n\\widehat{\\sigma}^{2}=\\frac{1}{n} \\sum_{i=1}^{n} \\widehat{e}_{i}^{2} .\n\\]\n在矩阵表示法中，我们可以将 (3.25) 和 (3.26) 写为 \\(\\widetilde{\\sigma}^{2}=n^{-1} \\boldsymbol{e}^{\\prime} \\boldsymbol{e}\\) 和\n\\[\n\\widehat{\\sigma}^{2}=n^{-1} \\widehat{\\boldsymbol{e}}^{\\prime} \\widehat{\\boldsymbol{e}} .\n\\]\n回忆 (3.23) 和 (3.24) 中的表达式 \\(\\widehat{\\boldsymbol{e}}=\\boldsymbol{M} \\boldsymbol{Y}=\\boldsymbol{M} \\boldsymbol{e}\\)。应用于 (3.27) 我们发现\n\\[\n\\widehat{\\sigma}^{2}=n^{-1} \\widehat{\\boldsymbol{e}}^{\\prime} \\widehat{\\boldsymbol{e}}=n^{-1} \\boldsymbol{e}^{\\prime} \\boldsymbol{M M} \\boldsymbol{M}=n^{-1} \\boldsymbol{e}^{\\prime} \\boldsymbol{M} \\boldsymbol{e}\n\\]\n第三个相等，因为 \\(M M=M\\)。\n一个有趣的暗示是\n\\[\n\\widetilde{\\sigma}^{2}-\\widehat{\\sigma}^{2}=n^{-1} \\boldsymbol{e}^{\\prime} \\boldsymbol{e}-n^{-1} \\boldsymbol{e}^{\\prime} \\boldsymbol{M} \\boldsymbol{e}=n^{-1} \\boldsymbol{e}^{\\prime} \\boldsymbol{P} \\boldsymbol{e} \\geq 0 .\n\\]\n最后的不等式成立，因为 \\(\\boldsymbol{P}\\) 是半正定的，而 \\(\\boldsymbol{e}^{\\prime} \\boldsymbol{P} \\boldsymbol{e}\\) 是二次形式。这表明可行估计量 \\(\\widehat{\\sigma}^{2}\\) 在数值上小于理想化估计量 (3.25)。"
  },
  {
    "objectID": "chpt03-algebra-chn.html#方差分析",
    "href": "chpt03-algebra-chn.html#方差分析",
    "title": "最小二乘代数",
    "section": "方差分析",
    "text": "方差分析\n另一种写法（3.23）是\n\\[\n\\boldsymbol{Y}=\\boldsymbol{P} \\boldsymbol{Y}+\\boldsymbol{M} \\boldsymbol{Y}=\\widehat{\\boldsymbol{Y}}+\\widehat{\\boldsymbol{e}} .\n\\]\n这种分解是正交的，即\n\\[\n\\widehat{\\boldsymbol{Y}}^{\\prime} \\widehat{\\boldsymbol{e}}=(\\boldsymbol{P} \\boldsymbol{Y})^{\\prime}(\\boldsymbol{M} \\boldsymbol{Y})=\\boldsymbol{Y}^{\\prime} \\boldsymbol{P} \\boldsymbol{M} \\boldsymbol{Y}=0 .\n\\]\n它遵循\n\\[\n\\boldsymbol{Y}^{\\prime} \\boldsymbol{Y}=\\widehat{\\boldsymbol{Y}}^{\\prime} \\widehat{\\boldsymbol{Y}}+2 \\widehat{\\boldsymbol{Y}}^{\\prime} \\widehat{\\boldsymbol{e}}+\\widehat{\\boldsymbol{e}}^{\\prime} \\widehat{\\boldsymbol{e}}=\\widehat{\\boldsymbol{Y}}^{\\prime} \\widehat{\\boldsymbol{Y}}+\\widehat{\\boldsymbol{e}}^{\\prime} \\widehat{\\boldsymbol{e}}\n\\]\n或者\n\\[\n\\sum_{i=1}^{n} Y_{i}^{2}=\\sum_{i=1}^{n} \\widehat{Y}_{i}^{2}+\\sum_{i=1}^{n} \\widehat{e}_{i}^{2}\n\\]\n从 (3.29) 的两边减去 \\(\\bar{Y}\\)，我们得到\n\\[\n\\boldsymbol{Y}-\\mathbf{1}_{n} \\bar{Y}=\\widehat{\\boldsymbol{Y}}-\\mathbf{1}_{n} \\bar{Y}+\\widehat{\\boldsymbol{e}}\n\\]\n当 \\(X\\) 包含一个常数时，这种分解也是正交的，如\n\\[\n\\left(\\widehat{\\boldsymbol{Y}}-\\mathbf{1}_{n} \\bar{Y}\\right)^{\\prime} \\widehat{\\boldsymbol{e}}=\\widehat{\\boldsymbol{Y}}^{\\prime} \\widehat{\\boldsymbol{e}}-\\bar{Y} \\mathbf{1}_{n}^{\\prime} \\widehat{\\boldsymbol{e}}=0\n\\]\n根据（3.17）。它遵循\n\\[\n\\left(\\boldsymbol{Y}-\\mathbf{1}_{n} \\bar{Y}\\right)^{\\prime}\\left(\\boldsymbol{Y}-\\mathbf{1}_{n} \\bar{Y}\\right)=\\left(\\widehat{\\boldsymbol{Y}}-\\mathbf{1}_{n} \\bar{Y}\\right)^{\\prime}\\left(\\widehat{\\boldsymbol{Y}}-\\mathbf{1}_{n} \\bar{Y}\\right)+\\widehat{\\boldsymbol{e}}^{\\prime} \\widehat{\\boldsymbol{e}}\n\\]\n或者\n\\[\n\\sum_{i=1}^{n}\\left(Y_{i}-\\bar{Y}\\right)^{2}=\\sum_{i=1}^{n}\\left(\\widehat{Y}_{i}-\\bar{Y}\\right)^{2}+\\sum_{i=1}^{n} \\widehat{e}_{i}^{2} .\n\\]\n这通常称为最小二乘回归的方差分析公式。\n一个常见的统计数据是决定系数或 R 平方：\n\\[\nR^{2}=\\frac{\\sum_{i=1}^{n}\\left(\\widehat{Y}_{i}-\\bar{Y}\\right)^{2}}{\\sum_{i=1}^{n}\\left(Y_{i}-\\bar{Y}\\right)^{2}}=1-\\frac{\\sum_{i=1}^{n} \\widehat{e}_{i}^{2}}{\\sum_{i=1}^{n}\\left(Y_{i}-\\bar{Y}\\right)^{2}} .\n\\]\n它通常被描述为“由最小二乘拟合解释的 \\(Y\\) 的样本方差的分数”。 \\(R^{2}\\) 是回归拟合的粗略度量。我们有更好的拟合度量，但这些需要统计（不仅仅是代数）分析，我们稍后会回到这些问题。 \\(R^{2}\\) 的一个缺陷是当回归变量添加到回归时它会增加（参见练习 3.16），因此“拟合”总是可以通过增加回归变量的数量来增加。\nWright (1921) 引入了决定系数。"
  },
  {
    "objectID": "chpt03-algebra-chn.html#预测",
    "href": "chpt03-algebra-chn.html#预测",
    "title": "最小二乘代数",
    "section": "预测",
    "text": "预测\n可视化最小二乘拟合的一种方法是作为投影操作。\n将回归矩阵写为 \\(\\boldsymbol{X}=\\left[\\boldsymbol{X}_{1} \\boldsymbol{X}_{2} \\ldots \\boldsymbol{X}_{k}\\right]\\)，其中 \\(\\boldsymbol{X}_{j}\\) 是 \\(\\boldsymbol{X}\\) 的 \\(j^{t h}\\) 列。 \\(\\boldsymbol{X}\\) 的范围空间 \\(\\mathscr{R}(\\boldsymbol{X})\\) 是由列的所有线性组合组成的空间 \\(\\boldsymbol{X}_{1}, \\boldsymbol{X}_{2}, \\ldots, \\boldsymbol{X}_{k} . \\mathscr{R}(\\boldsymbol{X})\\) 是 \\(\\mathbb{R}^{n}\\) 中包含的 \\(k\\) 维曲面。如果 \\(\\boldsymbol{X}=\\left[\\boldsymbol{X}_{1} \\boldsymbol{X}_{2} \\ldots \\boldsymbol{X}_{k}\\right]\\) 则 \\(\\boldsymbol{X}=\\left[\\boldsymbol{X}_{1} \\boldsymbol{X}_{2} \\ldots \\boldsymbol{X}_{k}\\right]\\) 是平面。运算符 \\(\\boldsymbol{X}=\\left[\\boldsymbol{X}_{1} \\boldsymbol{X}_{2} \\ldots \\boldsymbol{X}_{k}\\right]\\) 将向量投影到 \\(\\boldsymbol{X}=\\left[\\boldsymbol{X}_{1} \\boldsymbol{X}_{2} \\ldots \\boldsymbol{X}_{k}\\right]\\) 上。拟合值 \\(\\boldsymbol{X}=\\left[\\boldsymbol{X}_{1} \\boldsymbol{X}_{2} \\ldots \\boldsymbol{X}_{k}\\right]\\) 是 \\(\\boldsymbol{X}=\\left[\\boldsymbol{X}_{1} \\boldsymbol{X}_{2} \\ldots \\boldsymbol{X}_{k}\\right]\\) 到 \\(\\boldsymbol{X}=\\left[\\boldsymbol{X}_{1} \\boldsymbol{X}_{2} \\ldots \\boldsymbol{X}_{k}\\right]\\) 的投影。\n可视化检查图 3.4。这将显示 \\(n=3\\) 和 \\(k=2\\) 的情况。显示的是三个向量 \\(\\boldsymbol{Y}, \\boldsymbol{X}_{1}\\) 和 \\(\\boldsymbol{X}_{2}\\)，它们是 \\(\\mathbb{R}^{3}\\) 的每个元素。 \\(\\boldsymbol{X}_{1}\\) 和 \\(\\boldsymbol{X}_{2}\\) 创建的平面是范围空间 \\(\\mathscr{R}(\\boldsymbol{X})\\)。回归拟合值是 \\(\\boldsymbol{X}_{1}\\) 和 \\(n=3\\) 的线性组合，因此位于该平面上。拟合值 \\(n=3\\) 是该平面上最接近 \\(n=3\\) 的向量。残差 \\(n=3\\) 是两者之差。向量 \\(n=3\\) 和 \\(n=3\\) 之间的角度是 \\(n=3\\)，因此它们是正交的，如图所示。\n\n图 3.4：\\(\\boldsymbol{Y}\\) 到 \\(\\boldsymbol{X}_{1}\\) 和 \\(\\boldsymbol{X}_{2}\\) 的投影"
  },
  {
    "objectID": "chpt03-algebra-chn.html#回归组件",
    "href": "chpt03-algebra-chn.html#回归组件",
    "title": "最小二乘代数",
    "section": "回归组件",
    "text": "回归组件\n分区 \\(\\boldsymbol{X}=\\left[\\begin{array}{ll}\\boldsymbol{X}_{1} & \\boldsymbol{X}_{2}\\end{array}\\right]\\) 和 \\(\\beta=\\left(\\beta_{1}, \\beta_{2}\\right)\\)。回归模型可以写成\n\\[\n\\boldsymbol{Y}=\\boldsymbol{X}_{1} \\beta_{1}+\\boldsymbol{X}_{2} \\beta_{2}+\\boldsymbol{e} .\n\\]\n\\(\\beta=\\left(\\beta_{1}^{\\prime}, \\beta_{2}^{\\prime}\\right)^{\\prime}\\) 的 OLS 估计量是通过 \\(\\boldsymbol{Y}\\) 对 \\(\\boldsymbol{X}=\\left[\\boldsymbol{X}_{1} \\boldsymbol{X}_{2}\\right]\\) 的回归得到的，可以写为\n\\[\n\\boldsymbol{Y}=\\boldsymbol{X} \\widehat{\\beta}+\\widehat{\\boldsymbol{e}}=\\boldsymbol{X}_{1} \\widehat{\\boldsymbol{\\beta}}_{1}+\\boldsymbol{X}_{2} \\widehat{\\boldsymbol{\\beta}}_{2}+\\widehat{\\boldsymbol{e}} .\n\\]\n我们对 \\(\\widehat{\\beta}_{1}\\) 和 \\(\\widehat{\\beta}_{2}\\) 的代数表达式感兴趣。\n让我们首先关注 \\(\\widehat{\\beta}_{1}\\)。根据定义，最小二乘估计量是通过联合最小化找到的\n\\[\n\\left(\\widehat{\\beta}_{1}, \\widehat{\\beta}_{2}\\right)=\\underset{\\beta_{1}, \\beta_{2}}{\\operatorname{argmin}} \\operatorname{SSE}\\left(\\beta_{1}, \\beta_{2}\\right)\n\\]\n在哪里\n\\[\n\\operatorname{SSE}\\left(\\beta_{1}, \\beta_{2}\\right)=\\left(\\boldsymbol{Y}-\\boldsymbol{X}_{1} \\beta_{1}-\\boldsymbol{X}_{2} \\beta_{2}\\right)^{\\prime}\\left(\\boldsymbol{Y}-\\boldsymbol{X}_{1} \\beta_{1}-\\boldsymbol{X}_{2} \\beta_{2}\\right) .\n\\]\n\\(\\widehat{\\beta}_{1}\\) 的等价表达式可以通过集中（嵌套最小化）获得。解 (3.33) 可以写成\n\\[\n\\widehat{\\beta}_{1}=\\underset{\\beta_{1}}{\\operatorname{argmin}}\\left(\\min _{\\beta_{2}} \\operatorname{SSE}\\left(\\beta_{1}, \\beta_{2}\\right)\\right) .\n\\]\n内部表达式 \\(\\min _{\\beta_{2}} \\operatorname{SSE}\\left(\\beta_{1}, \\beta_{2}\\right)\\) 在 \\(\\beta_{1}\\) 固定的同时最小化了 \\(\\beta_{2}\\)。它是给定 \\(\\beta_{1}\\) 的最小可能误差平方和。外部最小化 \\(\\operatorname{argmin}_{\\beta_{1}}\\) 找到系数 \\(\\beta_{1}\\)，它使“给定 \\(\\beta_{1}\\) 的最小可能平方误差总和”最小化。这意味着 (3.33) 和 (3.34) 中定义的 \\(\\widehat{\\beta}_{1}\\) 在代数上是相同的。\n检查 (3.34) 中的内部最小化问题。这只是 \\(\\boldsymbol{Y}-\\boldsymbol{X}_{1} \\beta_{1}\\) 对 \\(\\boldsymbol{X}_{2}\\) 的最小二乘回归。这有解决方案\n\\[\n\\underset{\\beta_{2}}{\\operatorname{argmin}} \\operatorname{SSE}\\left(\\beta_{1}, \\beta_{2}\\right)=\\left(\\boldsymbol{X}_{2}^{\\prime} \\boldsymbol{X}_{2}\\right)^{-1}\\left(\\boldsymbol{X}_{2}^{\\prime}\\left(\\boldsymbol{Y}-\\boldsymbol{X}_{1} \\beta_{1}\\right)\\right)\n\\]\n有残差\n\\[\n\\begin{aligned}\n\\boldsymbol{Y}-\\boldsymbol{X}_{1} \\beta_{1}-\\boldsymbol{X}_{2}\\left(\\boldsymbol{X}_{2}^{\\prime} \\boldsymbol{X}_{2}\\right)^{-1}\\left(\\boldsymbol{X}_{2}^{\\prime}\\left(\\boldsymbol{Y}-\\boldsymbol{X}_{1} \\beta_{1}\\right)\\right) &=\\left(\\boldsymbol{M}_{2} \\boldsymbol{Y}-\\boldsymbol{M}_{2} \\boldsymbol{X}_{1} \\beta_{1}\\right) \\\\\n&=\\boldsymbol{M}_{2}\\left(\\boldsymbol{Y}-\\boldsymbol{X}_{1} \\beta_{1}\\right)\n\\end{aligned}\n\\]\n在哪里\n\\[\n\\boldsymbol{M}_{2}=\\boldsymbol{I}_{n}-\\boldsymbol{X}_{2}\\left(\\boldsymbol{X}_{2}^{\\prime} \\boldsymbol{X}_{2}\\right)^{-1} \\boldsymbol{X}_{2}^{\\prime}\n\\]\n是 \\(\\boldsymbol{X}_{2}\\) 的湮没矩阵。这意味着内部最小化问题（3.34）具有最小化值\n\\[\n\\begin{aligned}\n\\min _{\\beta_{2}} \\operatorname{SSE}\\left(\\beta_{1}, \\beta_{2}\\right) &=\\left(\\boldsymbol{Y}-\\boldsymbol{X}_{1} \\beta_{1}\\right)^{\\prime} \\boldsymbol{M}_{2} \\boldsymbol{M}_{2}\\left(\\boldsymbol{Y}-\\boldsymbol{X}_{1} \\beta_{1}\\right) \\\\\n&=\\left(\\boldsymbol{Y}-\\boldsymbol{X}_{1} \\beta_{1}\\right)^{\\prime} \\boldsymbol{M}_{2}\\left(\\boldsymbol{Y}-\\boldsymbol{X}_{1} \\beta_{1}\\right)\n\\end{aligned}\n\\]\n其中第二个等式成立，因为 \\(\\boldsymbol{M}_{2}\\) 是幂等的。将其代入 (3.34) 我们发现\n\\[\n\\begin{aligned}\n\\widehat{\\beta}_{1} &=\\underset{\\beta_{1}}{\\operatorname{argmin}}\\left(\\boldsymbol{Y}-\\boldsymbol{X}_{1} \\beta_{1}\\right)^{\\prime} \\boldsymbol{M}_{2}\\left(\\boldsymbol{Y}-\\boldsymbol{X}_{1} \\beta_{1}\\right) \\\\\n&=\\left(\\boldsymbol{X}_{1}^{\\prime} \\boldsymbol{M}_{2} \\boldsymbol{X}_{1}\\right)^{-1}\\left(\\boldsymbol{X}_{1}^{\\prime} \\boldsymbol{M}_{2} \\boldsymbol{Y}\\right) .\n\\end{aligned}\n\\]\n通过类似的论证，我们发现\n\\[\n\\widehat{\\beta}_{2}=\\left(\\boldsymbol{X}_{2}^{\\prime} \\boldsymbol{M}_{1} \\boldsymbol{X}_{2}\\right)^{-1}\\left(\\boldsymbol{X}_{2}^{\\prime} \\boldsymbol{M}_{1} \\boldsymbol{Y}\\right)\n\\]\n在哪里\n\\[\n\\boldsymbol{M}_{1}=\\boldsymbol{I}_{n}-\\boldsymbol{X}_{1}\\left(\\boldsymbol{X}_{1}^{\\prime} \\boldsymbol{X}_{1}\\right)^{-1} \\boldsymbol{X}_{1}^{\\prime}\n\\]\n是 \\(\\boldsymbol{X}_{1}\\) 的湮没矩阵。定理 3.4 (3.32) 的最小二乘估计量 \\(\\left(\\widehat{\\beta}_{1}, \\widehat{\\beta}_{2}\\right)\\) 有代数解\n\\[\n\\begin{aligned}\n&\\widehat{\\beta}_{1}=\\left(\\boldsymbol{X}_{1}^{\\prime} \\boldsymbol{M}_{2} \\boldsymbol{X}_{1}\\right)^{-1}\\left(\\boldsymbol{X}_{1}^{\\prime} \\boldsymbol{M}_{2} \\boldsymbol{Y}\\right) \\\\\n&\\widehat{\\beta}_{2}=\\left(\\boldsymbol{X}_{2}^{\\prime} \\boldsymbol{M}_{1} \\boldsymbol{X}_{2}\\right)^{-1}\\left(\\boldsymbol{X}_{2}^{\\prime} \\boldsymbol{M}_{1} \\boldsymbol{Y}\\right)\n\\end{aligned}\n\\]\n其中 \\(\\boldsymbol{M}_{1}\\) 和 \\(\\boldsymbol{M}_{2}\\) 分别在 (3.36) 和 (3.35) 中定义。"
  },
  {
    "objectID": "chpt03-algebra-chn.html#回归组件替代推导",
    "href": "chpt03-algebra-chn.html#回归组件替代推导",
    "title": "最小二乘代数",
    "section": "回归组件（替代推导）*",
    "text": "回归组件（替代推导）*\n定理 \\(3.4\\) 的另一种证明使用基于 2.22 节的总体计算的代数参数。由于这是一个经典的推导，为了完整起见，我们在这里展示它。\n分区 \\(\\widehat{\\boldsymbol{Q}}_{X X}\\) 为\n\\[\n\\widehat{\\boldsymbol{Q}}_{X X}=\\left[\\begin{array}{ll}\n\\widehat{\\boldsymbol{Q}}_{11} & \\widehat{\\boldsymbol{Q}}_{12} \\\\\n\\widehat{\\boldsymbol{Q}}_{21} & \\widehat{\\boldsymbol{Q}}_{22}\n\\end{array}\\right]=\\left[\\begin{array}{ll}\n\\frac{1}{n} \\boldsymbol{X}_{1}^{\\prime} \\boldsymbol{X}_{1} & \\frac{1}{n} \\boldsymbol{X}_{1}^{\\prime} \\boldsymbol{X}_{2} \\\\\n\\frac{1}{n} \\boldsymbol{X}_{2}^{\\prime} \\boldsymbol{X}_{1} & \\frac{1}{n} \\boldsymbol{X}_{2}^{\\prime} \\boldsymbol{X}_{2}\n\\end{array}\\right]\n\\]\n同样 \\(\\widehat{\\boldsymbol{Q}}_{X Y}\\) 为\n\\[\n\\widehat{\\boldsymbol{Q}}_{X Y}=\\left[\\begin{array}{l}\n\\widehat{\\boldsymbol{Q}}_{1 Y} \\\\\n\\widehat{\\boldsymbol{Q}}_{2 Y}\n\\end{array}\\right]=\\left[\\begin{array}{c}\n\\frac{1}{n} \\boldsymbol{X}_{1}^{\\prime} \\boldsymbol{Y} \\\\\n\\frac{1}{n} \\boldsymbol{X}_{2}^{\\prime} \\boldsymbol{Y}\n\\end{array}\\right]\n\\]\n由分区矩阵求逆公式（A.3）\n\\[\n\\widehat{\\boldsymbol{Q}}_{X X}^{-1}=\\left[\\begin{array}{ll}\n\\widehat{\\boldsymbol{Q}}_{11} & \\widehat{\\boldsymbol{Q}}_{12} \\\\\n\\widehat{\\boldsymbol{Q}}_{21} & \\widehat{\\boldsymbol{Q}}_{22}\n\\end{array}\\right]^{-1} \\stackrel{\\operatorname{def}}{=}\\left[\\begin{array}{cc}\n\\widehat{\\boldsymbol{Q}}^{11} & \\widehat{\\boldsymbol{Q}}^{12} \\\\\n\\widehat{\\boldsymbol{Q}}^{21} & \\widehat{\\boldsymbol{Q}}^{22}\n\\end{array}\\right]=\\left[\\begin{array}{cc}\n\\widehat{\\boldsymbol{Q}}_{11 \\cdot 2}^{-1} & -\\widehat{\\boldsymbol{Q}}_{11 \\cdot 2}^{-1} \\widehat{\\boldsymbol{Q}}_{12} \\widehat{\\boldsymbol{Q}}_{22}^{-1} \\\\\n-\\widehat{\\boldsymbol{Q}}_{22 \\cdot 1}^{-1} \\widehat{\\boldsymbol{Q}}_{21} \\widehat{\\boldsymbol{Q}}_{11}^{-1} & \\widehat{\\boldsymbol{Q}}_{22 \\cdot 1}^{-1}\n\\end{array}\\right]\n\\]\n其中 \\(\\widehat{\\boldsymbol{Q}}_{11 \\cdot 2}=\\widehat{\\boldsymbol{Q}}_{11}-\\widehat{\\boldsymbol{Q}}_{12} \\widehat{\\boldsymbol{Q}}_{22}^{-1} \\widehat{\\boldsymbol{Q}}_{21}\\) 和 \\(\\widehat{\\boldsymbol{Q}}_{22 \\cdot 1}=\\widehat{\\boldsymbol{Q}}_{22}-\\widehat{\\boldsymbol{Q}}_{21} \\widehat{\\boldsymbol{Q}}_{11}^{-1} \\widehat{\\boldsymbol{Q}}_{12}\\)。因此\n\\[\n\\begin{aligned}\n\\widehat{\\beta} &=\\left(\\begin{array}{c}\n\\widehat{\\beta}_{1} \\\\\n\\widehat{\\beta}_{2}\n\\end{array}\\right) \\\\\n&=\\left[\\begin{array}{cc}\n\\widehat{\\boldsymbol{Q}}_{11 \\cdot 2}^{-1} & -\\widehat{\\boldsymbol{Q}}_{11 \\cdot 2}^{-1} \\widehat{\\boldsymbol{Q}}_{12} \\widehat{\\boldsymbol{Q}}_{22}^{-1} \\\\\n-\\widehat{\\boldsymbol{Q}}_{22 \\cdot 1}^{-1} \\widehat{\\boldsymbol{Q}}_{21} \\widehat{\\boldsymbol{Q}}_{11}^{-1} & \\widehat{\\boldsymbol{Q}}_{22 \\cdot 1}^{-1}\n\\end{array}\\right]\\left[\\begin{array}{c}\n\\widehat{\\boldsymbol{Q}}_{1 Y} \\\\\n\\widehat{\\boldsymbol{Q}}_{2 Y}\n\\end{array}\\right] \\\\\n&=\\left(\\begin{array}{c}\n\\widehat{\\mathbf{Q}}_{11 \\cdot 2}^{-1} \\widehat{\\boldsymbol{Q}}_{1 Y \\cdot 2} \\\\\n\\widehat{\\mathbf{Q}}_{22 \\cdot 1}^{-1} \\widehat{\\mathbf{Q}}_{2 Y \\cdot 1}\n\\end{array}\\right)\n\\end{aligned}\n\\]\n现在\n\\[\n\\begin{aligned}\n\\widehat{\\boldsymbol{Q}}_{11 \\cdot 2} &=\\widehat{\\boldsymbol{Q}}_{11}-\\widehat{\\boldsymbol{Q}}_{12} \\widehat{\\boldsymbol{Q}}_{22}^{-1} \\widehat{\\boldsymbol{Q}}_{21} \\\\\n&=\\frac{1}{n} \\boldsymbol{X}_{1}^{\\prime} \\boldsymbol{X}_{1}-\\frac{1}{n} \\boldsymbol{X}_{1}^{\\prime} \\boldsymbol{X}_{2}\\left(\\frac{1}{n} \\boldsymbol{X}_{2}^{\\prime} \\boldsymbol{X}_{2}\\right)^{-1} \\frac{1}{n} \\boldsymbol{X}_{2}^{\\prime} \\boldsymbol{X}_{1} \\\\\n&=\\frac{1}{n} \\boldsymbol{X}_{1}^{\\prime} \\boldsymbol{M}_{2} \\boldsymbol{X}_{1}\n\\end{aligned}\n\\]\n和\n\\[\n\\begin{aligned}\n\\widehat{\\boldsymbol{Q}}_{1 y \\cdot 2} &=\\widehat{\\boldsymbol{Q}}_{1 Y}-\\widehat{\\boldsymbol{Q}}_{12} \\widehat{\\boldsymbol{Q}}_{22}^{-1} \\widehat{\\boldsymbol{Q}}_{2 Y} \\\\\n&=\\frac{1}{n} \\boldsymbol{X}_{1}^{\\prime} \\boldsymbol{Y}-\\frac{1}{n} \\boldsymbol{X}_{1}^{\\prime} \\boldsymbol{X}_{2}\\left(\\frac{1}{n} \\boldsymbol{X}_{2}^{\\prime} \\boldsymbol{X}_{2}\\right)^{-1} \\frac{1}{n} \\boldsymbol{X}_{2}^{\\prime} \\boldsymbol{Y} \\\\\n&=\\frac{1}{n} \\boldsymbol{X}_{1}^{\\prime} \\boldsymbol{M}_{2} \\boldsymbol{Y} .\n\\end{aligned}\n\\]\n方程（3.38）如下。\n与 \\(\\widehat{\\boldsymbol{Q}}_{11 \\cdot 2}\\) 和 \\(\\widehat{\\boldsymbol{Q}}_{1 Y \\cdot 2}\\) 的计算类似，您可以证明 \\(\\widehat{\\boldsymbol{Q}}_{2 Y \\cdot 1}=\\frac{1}{n} \\boldsymbol{X}_{2}^{\\prime} \\boldsymbol{M}_{1} \\boldsymbol{Y}\\) 和 \\(\\widehat{\\boldsymbol{Q}}_{22 \\cdot 1}=\\) \\(\\frac{1}{n} \\boldsymbol{X}_{2}^{\\prime} \\boldsymbol{M}_{1} \\boldsymbol{X}_{2}\\)。这建立了（3.37）。这就是定理 3.4。"
  },
  {
    "objectID": "chpt03-algebra-chn.html#残差回归",
    "href": "chpt03-algebra-chn.html#残差回归",
    "title": "最小二乘代数",
    "section": "残差回归",
    "text": "残差回归\n正如 Frisch 和 Waugh (1933) 首次认识到并由 Lovell (1963) 扩展的那样，表达式 (3.37) 和 (3.38) 可用于表明最小二乘估计量 \\(\\widehat{\\beta}_{1}\\) 和 \\(\\widehat{\\beta}_{2}\\) 可以通过两个-逐步回归过程。\n取 (3.38)。由于 \\(\\boldsymbol{M}_{1}\\) 是幂等的，\\(\\boldsymbol{M}_{1}=\\boldsymbol{M}_{1} \\boldsymbol{M}_{1}\\) 因而\n\\[\n\\begin{aligned}\n\\widehat{\\beta}_{2} &=\\left(\\boldsymbol{X}_{2}^{\\prime} \\boldsymbol{M}_{1} \\boldsymbol{X}_{2}\\right)^{-1}\\left(\\boldsymbol{X}_{2}^{\\prime} \\boldsymbol{M}_{1} \\boldsymbol{Y}\\right) \\\\\n&=\\left(\\boldsymbol{X}_{2}^{\\prime} \\boldsymbol{M}_{1} \\boldsymbol{M}_{1} \\boldsymbol{X}_{2}\\right)^{-1}\\left(\\boldsymbol{X}_{2}^{\\prime} \\boldsymbol{M}_{1} \\boldsymbol{M}_{1} \\boldsymbol{Y}\\right) \\\\\n&=\\left(\\widetilde{\\boldsymbol{X}}_{2}^{\\prime} \\widetilde{\\boldsymbol{X}}_{2}\\right)^{-1}\\left(\\widetilde{\\boldsymbol{X}}_{2}^{\\prime} \\widetilde{\\boldsymbol{e}}_{1}\\right)\n\\end{aligned}\n\\]\n其中 \\(\\widetilde{\\boldsymbol{X}}_{2}=\\boldsymbol{M}_{1} \\boldsymbol{X}_{2}\\) 和 \\(\\widetilde{\\boldsymbol{e}}_{1}=\\boldsymbol{M}_{1} \\boldsymbol{Y}\\)。\n因此，系数估计量 \\(\\widehat{\\beta}_{2}\\) 在代数上等于 \\(\\widetilde{\\boldsymbol{e}}_{1}\\) 对 \\(\\widetilde{\\boldsymbol{X}}_{2}\\) 的最小二乘回归。请注意，这两个分别是 \\(\\boldsymbol{Y}\\) 和 \\(\\boldsymbol{X}_{2}\\)，预乘以 \\(\\boldsymbol{M}_{1}\\)。但是我们知道 \\(\\boldsymbol{M}_{1}\\) 的预乘会产生最小二乘残差。因此 \\(\\widetilde{\\boldsymbol{e}}_{1}\\) 只是 \\(\\boldsymbol{Y}\\) 对 \\(\\widehat{\\beta}_{2}\\) 的回归的最小二乘残差，\\(\\widehat{\\beta}_{2}\\) 的列是 \\(\\widehat{\\beta}_{2}\\) 的列对 \\(\\widehat{\\beta}_{2}\\) 的回归的最小二乘残差。\n我们已经证明了以下定理。\n定理 3.5 Frisch-Waugh-Lovell (FWL)\n在模型 (3.31) 中，\\(\\beta_{2}\\) 的 OLS 估计量和 OLS 残差 \\(\\widehat{\\boldsymbol{e}}\\) 可以通过 OLS 回归 (3.32) 或通过以下算法计算：\n1、对\\(\\boldsymbol{X}_{1}\\)回归\\(\\boldsymbol{Y}\\)，得到残差\\(\\widetilde{\\boldsymbol{e}}_{1}\\)；\n2.对\\(\\boldsymbol{X}_{1}\\)回归\\(\\boldsymbol{X}_{2}\\)，得到残差\\(\\widetilde{\\boldsymbol{X}}_{2}\\)；\n\n对\\(\\widetilde{\\boldsymbol{X}}_{2}\\) 回归\\(\\widetilde{\\boldsymbol{e}}_{1}\\)，得到OLS 估计\\(\\widehat{\\beta}_{2}\\) 和残差\\(\\widehat{\\boldsymbol{e}}\\)。\n\n在某些情况下（例如面板数据模型，将在第 17 章中介绍），FWL 定理可用于大大加快计算速度。\nFWL 定理是 2.23 节中获得的系数表示的直接模拟。该部分获得的结果涉及人口预测系数；此处获得的结果涉及最小二乘估计量。关键信息是相同的。在最小二乘回归 (3.32) 中，估计系数 \\(\\widehat{\\beta}_{2}\\) 在回归量 \\(X_{1}\\) 被线性投影后，在代数上等于 \\(\\boldsymbol{Y}\\) 对回归量 \\(\\boldsymbol{X}_{2}\\) 的回归。类似地，在回归量 \\(\\boldsymbol{X}_{2}\\) 被线性投影后，系数估计 \\(\\widehat{\\beta}_{1}\\) 在代数上等于 \\(\\boldsymbol{Y}\\) 对回归量 \\(\\boldsymbol{X}_{1}\\) 的回归。在解释回归系数时，这个结果可能很有见地。\nFWL 定理的一个常见应用是在 (3.18) 中获得的回归贬义公式。分区 \\(\\boldsymbol{X}=\\left[\\boldsymbol{X}_{1} \\boldsymbol{X}_{2}\\right]\\) 其中 \\(\\boldsymbol{X}_{1}=\\mathbf{1}_{n}\\) 是一个向量，\\(\\boldsymbol{X}_{2}\\) 是观察到的回归量矩阵。在这种情况下 \\(\\boldsymbol{M}_{1}=\\boldsymbol{I}_{n}-\\mathbf{1}_{n}\\left(\\mathbf{1}_{n}^{\\prime} \\mathbf{1}_{n}\\right)^{-1} \\mathbf{1}_{n}^{\\prime}\\)。注意 \\(\\widetilde{\\boldsymbol{X}}_{2}=\\boldsymbol{M}_{1} \\boldsymbol{X}_{2}=\\boldsymbol{X}_{2}-\\overline{\\boldsymbol{X}}_{2}\\) 和 \\(\\boldsymbol{M}_{1} \\boldsymbol{Y}=\\boldsymbol{Y}-\\overline{\\boldsymbol{Y}}\\) 是“贬低”的变量。 FWL 定理说 \\(\\widehat{\\beta}_{2}\\) 是从 \\(Y_{i}-\\bar{Y}\\) 对 \\(X_{2 i}-\\bar{X}_{2}\\) 的回归得到的 OLS 估计：\n\\[\n\\widehat{\\beta}_{2}=\\left(\\sum_{i=1}^{n}\\left(X_{2 i}-\\bar{X}_{2}\\right)\\left(X_{2 i}-\\bar{X}_{2}\\right)^{\\prime}\\right)^{-1}\\left(\\sum_{i=1}^{n}\\left(X_{2 i}-\\bar{X}_{2}\\right)\\left(Y_{i}-\\bar{Y}\\right)\\right)\n\\]\n这是（3.18）。\n拉格纳新鲜\\ Ragnar Frisch (1895-1973) 与第一届 No-\\ 的 Jan Tinbergen 共同获胜 1969 年贝尔经济科学纪念奖，以表彰他们在发展中国家的工作 并应用动态模型分析经济问题。弗里施\\ 为现代经济学做出了许多基础性贡献。 Frisch-Waugh-Lovell 定理，包括形式化消费者理论，产品-\\ 理论和商业周期理论。"
  },
  {
    "objectID": "chpt03-algebra-chn.html#杠杆价值",
    "href": "chpt03-algebra-chn.html#杠杆价值",
    "title": "最小二乘代数",
    "section": "杠杆价值",
    "text": "杠杆价值\n回归矩阵 \\(\\boldsymbol{X}\\) 的杠杆值是投影矩阵 \\(\\boldsymbol{P}=\\boldsymbol{X}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\boldsymbol{X}^{\\prime}\\) 的对角线元素。有 \\(n\\) 杠杆值，通常写为 \\(h_{i i}\\) 对应 \\(i=1, \\ldots, n\\)。自从\n\\[\n\\boldsymbol{P}=\\left(\\begin{array}{c}\nX_{1}^{\\prime} \\\\\nX_{2}^{\\prime} \\\\\n\\vdots \\\\\nX_{n}^{\\prime}\n\\end{array}\\right)\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1}\\left(\\begin{array}{llll}\nX_{1} & X_{2} & \\cdots & X_{n}\n\\end{array}\\right)\n\\]\n他们是\n\\[\nh_{i i}=X_{i}^{\\prime}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} X_{i} .\n\\]\n杠杆值 \\(h_{i i}\\) 是观察到的回归向量 \\(X_{i}\\) 的标准化长度。它们经常出现在最小二乘回归的代数和统计分析中，包括留一法回归、有影响的观察、稳健的协方差矩阵估计和交叉验证。\n现在列出了杠杆值的一些属性。\n定理 3.6\n\n\\(0 \\leq h_{i i} \\leq 1\\)。\n\\(h_{i i} \\geq 1 / n\\) 如果 \\(X\\) 包含截距。\n\\(\\sum_{i=1}^{n} h_{i i}=k\\)。\n\n我们在下面证明定理 \\(3.6\\)。\n杠杆值 \\(h_{i i}\\) 衡量 \\(i^{t h}\\) 观察 \\(X_{i}\\) 相对于样本中其他观察的异常程度。当 \\(X_{i}\\) 与其他样本值完全不同时，会出现较大的 \\(h_{i i}\\)。衡量整体异常性的是最大杠杆值\n\\[\n\\bar{h}=\\max _{1 \\leq i \\leq n} h_{i i} .\n\\]\n通常说，当杠杆值都大致相等时，回归设计是平衡的。从定理 3.6.3 我们推导出当 \\(h_{i i}=\\bar{h}=k / n\\) 时出现完全平衡。完全平衡的一个例子是，当回归变量都是正交虚拟变量时，每个变量都有相同的 0 和 1 出现。\n如果某些杠杆值与其他杠杆值高度不相等，则回归设计是不平衡的。最极端的情况是 \\(\\bar{h}=1\\)。发生这种情况的一个示例是，当有一个虚拟回归器仅对样本中的一个观察值取值为 1 时。\n最大杠杆值 (3.41) 将根据回归变量的选择而变化。例如，考虑方程 (3.13)，对具有 \\(n=268\\) 观察值的单身亚洲男性的工资回归。这个回归有 \\(\\bar{h}=0.33\\)。如果省略平方经验回归量，则杠杆率降至 \\(\\bar{h}=0.10\\)。如果添加一个立方经验，它会增加到 \\(\\bar{h}=0.76\\)。如果四次方和五次方相加，则增加到 \\(\\bar{h}=0.99\\)。\n一些推理过程（例如稳健的协方差矩阵估计和交叉验证）对高杠杆值很敏感。我们稍后会回到这些问题。\n我们现在证明定理 3.6。对于第 1 部分，令 \\(s_{i}\\) 为 \\(n \\times 1\\) 单位向量，其中 \\(i^{t h}\\) 位置为 1，其他位置为零，因此 \\(h_{i i}=s_{i}^{\\prime} \\boldsymbol{P} s_{i}\\)。然后应用二次不等式 (B.18) 和定理 3.3.4，\n\\[\nh_{i i}=s_{i}^{\\prime} \\boldsymbol{P} s_{i} \\leq s_{i}^{\\prime} s_{i} \\lambda_{\\max }(\\boldsymbol{P})=1\n\\]\n如声称的那样。\n对于第 2 部分分区 \\(X_{i}=\\left(1, Z_{i}^{\\prime}\\right)^{\\prime}\\)。不失一般性，我们可以用贬值的值 \\(Z_{i}^{*}=Z_{i}-\\bar{Z}\\) 替换 \\(Z_{i}\\)。然后因为 \\(Z_{i}^{*}\\) 和截距是正交的\n\\[\n\\begin{aligned}\nh_{i i} &=\\left(1, Z_{i}^{* \\prime}\\right)\\left[\\begin{array}{cc}\nn & 0 \\\\\n0 & Z^{* \\prime} Z^{*}\n\\end{array}\\right]^{-1}\\left(\\begin{array}{c}\n1 \\\\\nZ_{i}^{*}\n\\end{array}\\right) \\\\\n&=\\frac{1}{n}+Z_{i}^{* \\prime}\\left(Z^{* \\prime} Z^{*}\\right)^{-1} Z_{i}^{*} \\geq \\frac{1}{n} .\n\\end{aligned}\n\\]\n对于第 3 部分，\\(\\sum_{i=1}^{n} h_{i i}=\\operatorname{tr} \\boldsymbol{P}=k\\)，其中第二个等式是定理 3.3.3。"
  },
  {
    "objectID": "chpt03-algebra-chn.html#留一法回归",
    "href": "chpt03-algebra-chn.html#留一法回归",
    "title": "最小二乘代数",
    "section": "留一法回归",
    "text": "留一法回归\n有许多统计程序——残差分析、折刀方差估计、交叉验证、两步估计、保留样本评估——它们利用在子样本上构建的估计器。特别重要的是我们排除单个观察然后对所有观察重复此操作的情况。这称为留一法 (LOO) 回归。\n具体来说，回归系数 \\(\\beta\\) 的留一估计量是使用不包括单个观测值 \\(i\\) 的完整样本构建的最小二乘估计量。这可以写成\n\\[\n\\begin{aligned}\n\\widehat{\\beta}_{(-i)} &=\\left(\\sum_{j \\neq i} X_{j} X_{j}^{\\prime}\\right)^{-1}\\left(\\sum_{j \\neq i} X_{j} Y_{j}\\right) \\\\\n&=\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}-X_{i} X_{i}^{\\prime}\\right)^{-1}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{Y}-X_{i} Y_{i}\\right) \\\\\n&=\\left(\\boldsymbol{X}_{(-i)}^{\\prime} \\boldsymbol{X}_{(-i)}\\right)^{-1} \\boldsymbol{X}_{(-i)}^{\\prime} \\boldsymbol{Y}_{(-i)} .\n\\end{aligned}\n\\]\n这里，\\(\\boldsymbol{X}_{(-i)}\\) 和 \\(\\boldsymbol{Y}_{(-i)}\\) 是省略了 \\(i^{t h}\\) 行的数据矩阵。符号 \\(\\widehat{\\beta}_{(-i)}\\) 或 \\(\\widehat{\\beta}_{-i}\\) 通常用于表示省略了 \\(i^{t h}\\) 观察的估计量。每个观察值都有一个留一估计量，\\(i=1, \\ldots, n\\)，所以我们有 \\(n\\) 这样的估计量。\n\\(Y_{i}\\) 的留一预测值为 \\(\\widetilde{Y}_{i}=X_{i}^{\\prime} \\widehat{\\beta}_{(-i)}\\)。这是通过在没有观察 \\(i\\) 的情况下估计样本上的 \\(\\beta\\)，然后使用协变量向量 \\(X_{i}\\) 预测 \\(Y_{i}\\) 获得的预测值。请注意，\\(\\widetilde{Y}_{i}\\) 是真实的预测，因为 \\(Y_{i}\\) 不用于构造 \\(\\widetilde{Y}_{i}\\)。这与作为 \\(Y_{i}\\) 的函数的拟合值 \\(Y_{i}\\) 形成对比。\n留一残差、预测误差或预测残差是 \\(\\widetilde{e}_{i}=Y_{i}-\\widetilde{Y}_{i}\\)。预测误差可以用作误差的估计量而不是残差。预测误差是比残差更好的估计量，因为前者是基于真实的预测。\n留一法公式 (3.42) 给人的印象是留一法系数和误差在计算上很麻烦，需要 \\(n\\) 单独的回归。幸运的是，在线性回归的背景下，情况并非如此。 \\(\\widehat{\\beta}_{(-i)}\\) 和 \\(\\widetilde{e}_{i}\\) 有简单的线性表达式。\n定理 3.7 留一估计量和预测误差相等\n\\[\n\\widehat{\\beta}_{(-i)}=\\widehat{\\beta}-\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} X_{i} \\widetilde{e}_{i}\n\\]\n和\n\\[\n\\widetilde{e}_{i}=\\left(1-h_{i i}\\right)^{-1} \\widehat{e}_{i}\n\\]\n其中 \\(h_{i i}\\) 是 (3.40) 中定义的杠杆值。\n我们在本节末尾证明了定理 \\(3.7\\)。\n等式 (3.43) 表明，留一法系数可以通过简单的线性运算来计算，不需要使用 \\(n\\) 单独的回归来计算。方程 (3.44) 的另一个有趣特征是预测误差 \\(\\widetilde{e}_{i}\\) 是最小二乘残差 \\(\\widehat{e}_{i}\\) 的简单缩放，缩放取决于杠杆值 \\(h_{i i}\\)。如果 \\(h_{i i}\\) 很小，那么 \\(\\widetilde{e}_{i} \\simeq \\widehat{e}_{i}\\)。但是，如果 \\(h_{i i}\\) 很大，那么 \\(\\widetilde{e}_{i}\\) 可能与 \\(\\widehat{e}_{i}\\) 完全不同。因此，残差和预测值之间的差异取决于杠杆值，即 \\(n\\) 的异常程度。要将 (3.44) 写成矢量符号，定义\n\\[\n\\begin{aligned}\n\\boldsymbol{M}^{*} &=\\left(\\boldsymbol{I}_{n}-\\operatorname{diag}\\left\\{h_{11}, . ., h_{n n}\\right\\}\\right)^{-1} \\\\\n&=\\operatorname{diag}\\left\\{\\left(1-h_{11}\\right)^{-1}, \\ldots,\\left(1-h_{n n}\\right)^{-1}\\right\\}\n\\end{aligned}\n\\]\n那么 (3.44) 等价于\n\\[\n\\widetilde{\\boldsymbol{e}}=\\boldsymbol{M}^{*} \\widehat{\\boldsymbol{e}} .\n\\]\n预测误差的一种用途是估计样本外均方误差：\n\\[\n\\widetilde{\\sigma}^{2}=\\frac{1}{n} \\sum_{i=1}^{n} \\widetilde{e}_{i}^{2}=\\frac{1}{n} \\sum_{i=1}^{n}\\left(1-h_{i i}\\right)^{-2} \\widehat{e}_{i}^{2} .\n\\]\n这称为样本均方预测误差。它的平方根 \\(\\widetilde{\\sigma}=\\sqrt{\\widetilde{\\sigma}^{2}}\\) 是预测标准误差。\n我们用定理 3.7 的证明来完成本节。留一估计量 (3.42) 可以写为\n\\[\n\\widehat{\\beta}_{(-i)}=\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}-X_{i} X_{i}^{\\prime}\\right)^{-1}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{Y}-X_{i} Y_{i}\\right) .\n\\]\n将 (3.47) 乘以 \\(\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}-X_{i} X_{i}^{\\prime}\\right)\\)。我们获得\n\\[\n\\widehat{\\beta}_{(-i)}-\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} X_{i} X_{i}^{\\prime} \\widehat{\\beta}_{(-i)}=\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{Y}-X_{i} Y_{i}\\right)=\\widehat{\\beta}-\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} X_{i} Y_{i} .\n\\]\n重写\n\\[\n\\widehat{\\beta}_{(-i)}=\\widehat{\\beta}-\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} X_{i}\\left(Y_{i}-X_{i}^{\\prime} \\widehat{\\beta}_{(-i)}\\right)=\\widehat{\\beta}-\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} X_{i} \\widetilde{e}_{i}\n\\]\n即 (3.43)。将此表达式预乘以 \\(X_{i}^{\\prime}\\) 并使用定义 (3.40) 我们得到\n\\[\nX_{i}^{\\prime} \\widehat{\\beta}_{(-i)}=X_{i}^{\\prime} \\widehat{\\beta}-X_{i}^{\\prime}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} X_{i} \\widetilde{e}_{i}=X_{i}^{\\prime} \\widehat{\\beta}-h_{i i} \\widetilde{e}_{i} .\n\\]\n使用 \\(\\widehat{e}_{i}\\) 和 \\(\\widetilde{e}_{i}\\) 的定义，我们得到 \\(\\widetilde{e}_{i}=\\widehat{e}_{i}+h_{i i} \\widetilde{e}_{i}\\)。重写我们得到（3.44）。"
  },
  {
    "objectID": "chpt03-algebra-chn.html#有影响的观察",
    "href": "chpt03-algebra-chn.html#有影响的观察",
    "title": "最小二乘代数",
    "section": "有影响的观察",
    "text": "有影响的观察\n留一法估计量的另一个用途是调查有影响的观察的影响，有时称为异常值。如果从样本中遗漏它会导致感兴趣的参数估计发生重大变化，我们说观察 \\(i\\) 是有影响的。\n为了说明，请考虑图 \\(3.5\\)，它显示了实现 \\(\\left(Y_{i}, X_{i}\\right)\\) 的散点图。用空心圆圈显示的 25 个观测值是由 \\(X_{i} \\sim U[1,10]\\) 和 \\(Y_{i} \\sim \\mathrm{N}\\left(X_{i}, 4\\right)\\) 生成的。用实心圆圈显示的 \\(26^{\\text {th }}\\) 观察结果是 \\(X_{26}=9, Y_{26}=0\\)。 （假设 \\(Y_{26}=0\\) 由于输入错误而被错误记录。）该图显示了来自完整样本的最小二乘拟合线和从样本中删除 \\(26^{\\text {th }}\\) 观测值后获得的拟合线。在这个例子中，我们可以看到 \\(26^{\\text {th }}\\) 观察值（“异常值”）如何使最小二乘拟合线向 \\(3.5\\) 观察值大幅倾斜。事实上，斜率系数从 \\(3.5\\)（接近 \\(3.5\\) 的真实值）减小到 \\(3.5\\)，大幅降低。 \\(3.5\\) 和 \\(3.5\\) 都不是相对于它们的边际分布的异常值，因此通过检查数据的边际分布不会检测到这个异常值。 \\(3.5\\) 的斜率系数的变化是有意义的，应该引起应用经济学家的关注。\n从 (3.43) 我们知道\n\\[\n\\widehat{\\beta}-\\widehat{\\beta}_{(-i)}=\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} X_{i} \\widetilde{e}_{i}\n\\]\n\n图 3.5：有影响的观察对最小二乘估计量的影响\n通过直接计算每个观测值 \\(i\\) 的数量，我们可以直接发现特定观测值 \\(i\\) 是否对感兴趣的系数估计有影响。\n对于一般性评估，我们可以关注预测值。全样本和留一法预测值之间的差异是\n\\[\n\\widehat{Y}_{i}-\\widetilde{Y}_{i}=X_{i}^{\\prime} \\widehat{\\beta}-X_{i}^{\\prime} \\widehat{\\beta}_{(-i)}=X_{i}^{\\prime}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} X_{i} \\widetilde{e}_{i}=h_{i i} \\widetilde{e}_{i}\n\\]\n这是杠杆值 \\(h_{i i}\\) 和预测误差 \\(\\widetilde{e}_{i}\\) 的简单函数。如果 \\(\\left|h_{i i} \\widetilde{e}_{i}\\right|\\) 很大，则观察 \\(i\\) 对预测值有影响，这要求 \\(h_{i i}\\) 和 \\(\\left|\\widetilde{e}_{i}\\right|\\) 都很大。\n考虑这一点的一种方法是，大的杠杆值 \\(h_{i i}\\) 使观察 \\(i\\) 有可能产生影响。较大的 \\(h_{i i}\\) 意味着观察 \\(i\\) 是不寻常的，因为回归量 \\(X_{i}\\) 远离其样本均值。我们将具有大 \\(h_{i i}\\) 的观察称为杠杆点。杠杆点不一定有影响，因为后者还要求预测误差 \\(\\widetilde{e}_{i}\\) 很大。\n为了确定任何个体观察在这个意义上是否有影响，已经提出了几种诊断方法（一些名称包括 DFITS、Cook 距离和 Welsch 距离）。不幸的是，从统计角度来看，很难将这些诊断推荐用于应用程序，因为它们不是基于统计理论。可能最相关的衡量标准是（3.48）中给出的系数估计值的变化。这些变化与系数标准误差的比率称为其 DFBETA，是 Stata 中可用的后估计诊断。虽然没有神奇的阈值，但问题在于单个观察是否有意义地改变了估计的兴趣系数。有影响的观察的一个简单诊断是计算\n\\[\n\\text { Influence }=\\max _{1 \\leq i \\leq n}\\left|\\widehat{Y}_{i}-\\widetilde{Y}_{i}\\right|=\\max _{1 \\leq i \\leq n}\\left|h_{i i} \\widetilde{e}_{i}\\right| .\n\\]\n这是由于单次观察导致的预测值的最大（绝对）变化。如果此诊断相对于 \\(Y\\) 的分布较大，则可能表明该观察是有影响的。\n如果某个观察结果被确定为有影响力，应该怎么做？由于有影响的观察的一个常见原因是数据错误，因此应检查有影响的观察，以寻找错误记录观察的证据。可能观察值超出了允许的范围，或者某些观察值不一致（例如，一个人被列为有工作但收入为 \\(\\$ 0\\) ）。如果确定不正确地记录了观察，则通常从样本中删除观察。这个过程通常被称为“清理数据”。在此过程中做出的决定涉及相当多的个人判断。 [完成此操作后，正确的做法是以原始形式保留源数据并创建执行所有清理操作（例如删除单个观察值）的程序文件。清洗后的数据文件此时可以保存起来，用于后续的统计分析。保留源数据和清理数据的特定程序文件的意义是双重的：以便记录所有决策，以便在修订和未来研究中进行修改。] 也有可能正确测量观察结果，但不寻常且有影响力。在这种情况下，尚不清楚如何进行。一些研究人员将尝试更改规范以正确模拟有影响的观察。其他研究人员将从样本中删除观察结果。这种选择的动机是防止结果被个人观察歪曲或确定。许多研究人员对后一种做法持怀疑态度，他们认为这会降低报告的实证结果的完整性。\n作为实证说明，请考虑单身亚洲男性的对数工资回归 (3.13)。这个回归有 268 个观测值，影响 \\(=0.29\\)。这意味着当删除最有影响的观察时，因变量 \\(\\log (\\) 工资的预测（拟合）值改变了 \\(0.29\\)，或者等效地改变了平均工资 \\(29 %\\)。这是一个有意义的变化，建议进一步调查。我们检查有影响的观察结果，发现它的杠杆 \\(h_{i i}\\) 是 \\(0.33\\)。这是一个中等大的杠杆值，这意味着回归量 \\(X_{i}\\) 有点不寻常。进一步考察，我们发现这个人今年 65 岁，受过 8 年教育，因此他的潜在工作经验是 51 年。这是子样本中最高的经验 - 次高的是 41 年。巨大的影响力是由于他在这个样本中的不同寻常的特征（非常低的教育和非常高的经验）。本质上，回归 (3.13) 试图仅通过一次观察来估计经验 \\(=51\\) 的条件均值。毫不奇怪，这种观察决定了拟合并因此具有影响力。一个合理的结论是回归函数只能在较小的经验范围内进行估计。我们将样本限制为经验不足 45 年的个人，重新估计，并获得以下估计。\n\\[\n\\widehat{\\log (\\text { wage })}=0.144 \\text { education }+0.043 \\text { experience }-0.095 \\text { experience }^{2} / 100+0.531 \\text {. }\n\\]\n对于这个回归，我们计算了影响 \\(=0.11\\)，它相对于回归 (3.13) 大大降低了。 （3.49）与（3.13）比较，教育的斜率系数基本不变，但经验系数及其平方略有增加。\n通过消除有影响的观察方程 (3.49) 可以被视为对大多数经验水平的条件均值的更稳健估计。在申请中是否报告 (3.13) 或 (3.49) 在很大程度上取决于判断。"
  },
  {
    "objectID": "chpt03-algebra-chn.html#cps-数据集",
    "href": "chpt03-algebra-chn.html#cps-数据集",
    "title": "最小二乘代数",
    "section": "CPS 数据集",
    "text": "CPS 数据集\n在本节中，我们将描述经验插图中使用的数据集。\n当前人口调查 (CPS) 是由美国劳工统计局人口普查局对约 57,000 个美国家庭进行的月度调查。 CPS 是有关美国人口劳动力特征的主要信息来源。该调查涵盖就业、收入、教育程度、收入、贫困、健康保险范围、工作经验、投票和登记、计算机使用、退伍军人身份和其他变量。详情可在 找到。 html。\n从 2009 年 3 月的调查中，我们提取了具有未分配变量的全职工作人员（定义为过去一年每周工作至少 36 小时、至少工作 48 周的人员），并排除了那些在军队中的人员。该样本有 50,742 个人。我们从这些个体的 CPS 中提取了 14 个变量，并创建了数据集 cps09mar。该数据集以及本教科书中使用的所有其他数据集可在 http: 获得。威斯克edu/bhansen/econometrics/。"
  },
  {
    "objectID": "chpt03-algebra-chn.html#数值计算",
    "href": "chpt03-algebra-chn.html#数值计算",
    "title": "最小二乘代数",
    "section": "数值计算",
    "text": "数值计算\n现代计量经济学估计涉及大样本和许多协变量。因此，即使是简单的统计数据（例如最小二乘估计量）的计算也需要大量（数百万）的算术运算。在实践中，大多数经济学家不需要过多考虑这一点，因为它可以在个人电脑上快速轻松地完成。尽管如此，了解基本的计算方法还是很有用的，因为选择有时会产生实质性的差异。\n虽然今天几乎所有的统计计算都是使用在电子计算机上运行的统计软件进行的，但情况并非总是如此。在 19 世纪和 20 世纪初，“计算机”是手工计算工人的工作标签。天文学家和统计实验室使用计算机。这项令人着迷的工作（以及实验室中使用的大多数计算机都是女性这一事实）已经进入了流行文化。例如，为美国早期太空计划工作的几台计算机的生活在这本书和流行电影 Hidden Figures 中有所描述，虚构的计算机/宇航员是小说《计算之星》的主角，以及计算机/天文学家 Henrietta Swan 的生活莱维特在《寂静的天空》中被戏剧化。\n在 1960 年代可编程电子计算机出现之前，经济学研究生通常被聘为计算机。样本量比今天看到的要小得多，但是手动计算带有 \\(n=100\\) 观察值和 \\(k=5\\) 变量的回归所需的工作量仍然很大！如果你现在是一名研究生，你应该为这个行业已经从人机时代发展而感到幸运！ （现在研究助理可以完成更多更高级的任务，例如编写 Stata、R 和 MATLAB 代码。）\n要获得最小二乘估计 \\(\\widehat{\\beta}=\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{Y}\\right)\\)，我们需要反转 \\(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\) 或求解方程组。具体来说，让 \\(\\boldsymbol{A}=\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\) 和 \\(\\boldsymbol{c}=\\boldsymbol{X}^{\\prime} \\boldsymbol{Y}\\) 使得最小二乘估计可以写成\n\\[\n\\boldsymbol{A} \\widehat{\\beta}=\\boldsymbol{c}\n\\]\n或作为\n\\[\n\\widehat{\\beta}=A^{-1} \\boldsymbol{c} .\n\\]\n方程 (3.50) 和 (3.51) 在代数上是相同的，但它们提出了两种不同的数值方法来获得 \\(\\widehat{\\beta}\\)。 (3.50) 建议求解 \\(k\\) 方程组。 (3.51) 建议找到 \\(A^{-1}\\)，然后乘以 \\(\\boldsymbol{c}\\)。虽然这两个表达式在代数上是相同的，但隐含的数值方法是不同的。简而言之，求解方程组 (3.50) 在数值上优于矩阵求逆问题 (3.51)。直接求解 (3.50) 速度更快，并产生具有更高数值精度的解。因此，通常推荐（3.50）超过（3.51）。然而，在大多数实际应用中，选择不会产生任何实际差异。当矩阵 \\(\\boldsymbol{A}\\) 是病态的（将在第 3.24 节中讨论）或具有极高维数时，选择可能会产生影响的上下文。\n求解方程组 (3.50) 和计算 \\(\\boldsymbol{A}^{-1}\\) 的数值方法分别在第 A.18 节和 A.19 节中讨论。\n统计包使用多种矩阵方法来求解（3.50）。 Stata 使用扫描算法，它是 A.18 节中讨论的 Gauss-Jordan 算法的变体。 （有关扫描算法，请参阅 Goodnight (1979)。）在 R 中，求解 (A, b) 使用 QR 分解。在 MATLAB 中，当 \\(A\\) 为正定时，A b 使用 Cholesky 分解，否则使用 QR 分解。"
  },
  {
    "objectID": "chpt03-algebra-chn.html#共线性误差",
    "href": "chpt03-algebra-chn.html#共线性误差",
    "title": "最小二乘代数",
    "section": "共线性误差",
    "text": "共线性误差\n对于要唯一定义的最小二乘估计量，回归量不能线性相关。然而，尝试用线性相关的回归器计算回归是很容易的。发生这种情况的原因有很多，包括以下原因。\n\n包括两次相同的回归量。\n\n2.在CPS数据集示例中包括相互线性组合的回归量，例如教育、经验和年龄（回忆一下，经验被定义为年龄-教育-6）。\n\n包括一个虚拟变量及其平方。\n估计子样本的回归，其中虚拟变量是全零或全一。\n包括一个产生全零的虚拟变量交互作用。\n包含比观察更多的回归变量。\n\n在上述任何一种情况下，回归量都是线性相关的，因此 \\(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\) 是奇异的，并且最小二乘估计量不是唯一的。如果您尝试估计回归，您可能会遇到错误消息。 （一个可能的例外是 MATLAB 使用“A \\(\\backslash \\mathrm{b}\\)”，如下所述。）消息可能是“系统完全奇异”、“系统在计算上是奇异的”、变量“由于共线性而被省略”或系数被列为“NA”。在某些情况下（例如在 R 中使用显式矩阵计算或在 MATLAB 中使用 regress 命令进行估计）程序将停止执行。在其他情况下，程序将继续运行。在 Stata（以及 R 中的 Im 包）中，将报告回归，但将省略一个或多个变量。\n如果出现任何这些警告或错误消息，正确的响应是停止并检查回归编码和数据。你犯了一个不经意的错误吗？你是否包含了一个线性相关的回归器？您是否在估计变量（特别是虚拟变量）没有变化的子样本？如果您可以确定其中一种情况导致了错误，那么解决方案就会立即显现出来。您需要重新指定模型（样本或回归量）以消除冗余。所有的实证研究人员在实证工作的过程中都会遇到这个错误。但是，如果包选择了要省略的变量，则不应简单地接受输出。研究人员的工作是了解根本原因并制定合适的补救措施。\n统计包也有可能不会检测和报告矩阵奇异性。如果您在 MATLAB 中使用显式矩阵运算进行计算并使用推荐的 A \\(\\backslash \\mathrm{b}\\) 命令来计算最小二乘估计量，则即使回归量在代数上相关，MATLAB 也可能会返回没有错误消息的数值解。因此，建议您在 MATLAB 中使用显式矩阵运算时对矩阵奇异性进行数值检查。\n我们如何在数字上检查矩阵 \\(\\boldsymbol{A}\\) 是否是奇异的？标准诊断是倒数条件数\n\\[\nC=\\frac{\\lambda_{\\min }(\\boldsymbol{A})}{\\lambda_{\\max }(\\boldsymbol{A})} .\n\\]\n如果 \\(C=0\\) 则 \\(\\boldsymbol{A}\\) 是单数。如果 \\(C=1\\) 那么 \\(\\boldsymbol{A}\\) 是完全平衡的。如果 \\(C\\) 非常小，我们说 \\(\\boldsymbol{A}\\) 是病态的。倒数条件数可以在 MATLAB 或 R 中通过 rcond 命令计算。不幸的是，在将 \\(\\boldsymbol{A}\\) 视为数值奇异之前，对于 \\(C\\) 应该有多小没有可接受的容忍度，部分原因是即使 \\(\\boldsymbol{A}\\) 是代数奇异的，rcond (A) 也可以返回正（但很小）结果。但是，在双精度（通常用于计算）中，数值精度受 \\(C=0\\) 的限制，这表明了最小范围 \\(C=0\\)。\n由于 \\(C\\) 的低值也可能是由不平衡或高度相关的回归量引起的，因此检查数值奇异性很复杂。\n为了说明，考虑使用 (3.13) 中的样本对从 1 到 \\(k\\) 的经验幂 \\(X\\) 进行工资回归（例如 \\(X, X^{2}, X^{3}, \\ldots, X^{k}\\) ）。我们计算了每个 \\(k\\) 的倒数条件数 \\(C\\)，发现 \\(C\\) 随着 \\(k\\) 的增加而减少，表明病态条件增加。实际上，对于 \\(k=\\) 5，我们发现 \\(C=6 \\mathrm{e}-17\\)，它低于双精度精度。这意味着 \\(X\\) 的回归是病态的。然而，回归矩阵不是奇异的。 \\(X\\) 的低值不是由于代数奇异性，而是由于缺乏平衡和高共线性。\n病态回归变量存在数值结果（报告的系数估计）不准确的潜在问题。在大多数应用程序中这可能不是问题，因为这只发生在极端情况下。然而，我们应该尽可能避免病态回归。\n有一些策略可以减少甚至消除不良条件。通常，重新调整回归量就足够了。一个通常适用于非负回归器的简单重新缩放是将每个变量除以其样本均值，从而将 \\(X_{j i}\\) 替换为 \\(X_{j i} / \\bar{X}_{j}\\)。在上面带有经验的例子中，这意味着将 \\(X_{i}^{2}\\) 替换为 \\(X_{i}^{2} /\\left(n^{-1} \\sum_{i=1}^{n} X_{i}^{2}\\right)\\) 等。这样做可以显着减少病态条件。通过这种缩放，\\(k \\leq 11\\) 的回归满足 \\(C \\geq 1 \\mathrm{e}-15\\)。另一个特定于幂回归的重新缩放是在获取幂之前首先重新缩放回归器以位于 \\([-1,1]\\) 中。通过这种缩放，\\(k \\leq 16\\) 的回归满足 \\(C \\geq 1 \\mathrm{e}-15\\)。一个更简单的缩放选项是在获取权力之前重新缩放回归量以位于 \\(X_{j i}\\) 中。通过这种缩放，\\(X_{j i}\\) 的回归满足 \\(X_{j i}\\)。这对于应用程序来说通常是足够的。\n病态条件通常可以通过回归量的正交化完全消除。这是通过在前面的变量（每个前面的列）上顺序回归每个变量（\\(\\boldsymbol{X}\\) 中的每一列），获取残差，然后重新缩放以获得单位方差来实现的。这将产生在代数上满足 \\(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}=n \\boldsymbol{I}_{n}\\) 并且条件数为 \\(C=1\\) 的回归量。如果我们将这个方法应用到上面的例子中，我们会得到一个接近 1 的 \\(k \\leq 20\\) 的条件数。\n这表明，当回归的条件数较小时，仔细检查规范很重要。回归量可能是线性相关的，在这种情况下，需要省略一个或多个回归量。回归量也有可能被严重缩放，在这种情况下，重新缩放一些回归量可能很有用。变量也可能是高度共线的，在这种情况下，可能的解决方案是正交化。这些选择应该由研究人员做出，而不是由自动化软件程序做出。"
  },
  {
    "objectID": "chpt03-algebra-chn.html#编程",
    "href": "chpt03-algebra-chn.html#编程",
    "title": "最小二乘代数",
    "section": "编程",
    "text": "编程\n大多数程序包都允许交互式编程（您一个接一个地输入命令）和批处理编程（您从文件中运行预先编写的命令序列）。交互式编程对于探索性分析很有用，但最终所有工作都应该以批处理模式执行。这是控制和记录您的工作的最佳方式。\n批处理程序是文本文件，其中每一行执行一个命令。对于 Stata，此文件需要具有文件扩展名“.do”，对于 MATLAB，该文件需要具有“.m”。对于 R，没有特定的命名要求，尽管通常使用扩展名“.r”。在编写批处理文件时，包含文档和可读性的注释很有用。要执行程序文件，您在程序中键入命令。\nStata：do chapter3 执行文件 chapter3。做。\nMATLAB：run chapter3 执行文件 chapter3.m。\nR: source (‘chapter3.r’) 执行文件chapter3.r。\n这些包中使用的命令有相同点和不同点。例如：\n\n不同的符号用于创建注释。 \\(*\\) 在 Stata 中，# 在 \\(\\mathrm{R}\\) 中，\\(%\\) 在 MATLAB 中。\nMATLAB 使用符号 ;分隔线。 Stata 和 R 使用硬回报。\nStata 使用 \\(\\ln ()\\) 计算自然对数。 R 和 MATLAB 使用 \\(\\log ()\\)。\n符号\\(=\\) 用于定义变量。 \\(\\mathrm{R}\\) 更喜欢 \\(<-\\)。双重相等 \\(==\\) 用于测试相等性。\n\n我们现在说明 Stata、R 和 MATLAB 的编程文件，它们执行 \\(3.7\\) 和 3.21 节中的部分经验说明。对于 R 和 MATLAB 代码，我们使用显式矩阵运算进行说明。或者，R 和 MATLAB 具有内置函数，无需显式矩阵运算即可实现最小二乘回归。在 \\(\\mathrm{R}\\) 中，标准函数是 \\(1 \\mathrm{~m}\\)。在 MATLAB 中，标准函数是回归。使用如下所示的显式矩阵运算的优点是您确切地知道完成了哪些计算，并且更容易“开箱即用”来执行新过程。使用内置函数的好处是简化了编码，并且您不太可能出现编码错误。"
  },
  {
    "objectID": "chpt03-algebra-chn.html#练习",
    "href": "chpt03-algebra-chn.html#练习",
    "title": "最小二乘代数",
    "section": "练习",
    "text": "练习\n练习 3.1 设 \\(Y\\) 是一个随机变量，\\(\\mu=\\mathbb{E}[Y]\\) 和 \\(\\sigma^{2}=\\operatorname{var}[Y]\\)。定义\n\\[\ng\\left(y, \\mu, \\sigma^{2}\\right)=\\left(\\begin{array}{c}\ny-\\mu \\\\\n(y-\\mu)^{2}-\\sigma^{2}\n\\end{array}\\right) .\n\\]\n令 \\(\\left(\\widehat{\\mu}, \\widehat{\\sigma}^{2}\\right)\\) 为 \\(\\bar{g}_{n}\\left(\\widehat{\\mu}, \\widehat{\\sigma}^{2}\\right)=0\\) where \\(\\bar{g}_{n}(m, s)=n^{-1} \\sum_{i=1}^{n} g\\left(y_{i}, m, s\\right)\\) 的值。证明 \\(\\widehat{\\mu}\\) 和 \\(\\widehat{\\sigma}^{2}\\) 是样本均值和方差。\n练习 3.2 考虑 \\(n \\times 1\\) 向量 \\(\\boldsymbol{Y}\\) 在 \\(n \\times k\\) 矩阵 \\(\\boldsymbol{X}\\) 上的 OLS 回归。考虑一组替代回归量 \\(\\boldsymbol{Z}=\\boldsymbol{X} \\boldsymbol{C}\\)，其中 \\(\\boldsymbol{C}\\) 是 \\(k \\times k\\) 非奇异矩阵。因此，\\(\\boldsymbol{Z}\\) 的每一列都是 \\(\\boldsymbol{X}\\) 的一些列的混合。比较 \\(n \\times 1\\) 对 \\(n \\times 1\\) 的回归的 OLS 估计和残差与 \\(n \\times 1\\) 对 \\(n \\times 1\\) 的回归的 OLS 估计。\n练习 3.3 使用矩阵代数，显示 \\(\\boldsymbol{X}^{\\prime} \\widehat{\\boldsymbol{e}}=0\\)。\n练习 3.4 令 \\(\\widehat{\\boldsymbol{e}}\\) 是 \\(\\boldsymbol{Y}\\) 对 \\(\\boldsymbol{X}=\\left[\\boldsymbol{X}_{1} \\boldsymbol{X}_{2}\\right]\\) 的回归的 OLS 残差。找到 \\(\\boldsymbol{X}_{2}^{\\prime} \\widehat{\\boldsymbol{e}}\\)。\n练习 3.5 令 \\(\\widehat{\\boldsymbol{e}}\\) 是 \\(\\boldsymbol{Y}\\) 对 \\(\\boldsymbol{X}\\) 的回归的 OLS 残差。从 \\(\\widehat{\\boldsymbol{e}}\\) 对 \\(\\boldsymbol{X}\\) 的回归中找到 OLS 系数。\n练习 3.6 设 \\(\\widehat{\\boldsymbol{Y}}=\\boldsymbol{X}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\boldsymbol{X}^{\\prime} \\boldsymbol{Y}\\)。从 \\(\\widehat{\\boldsymbol{Y}}\\) 对 \\(\\boldsymbol{X}\\) 的回归中找到 OLS 系数。\n练习 3.7 证明如果 \\(\\boldsymbol{X}=\\left[\\boldsymbol{X}_{1} \\boldsymbol{X}_{2}\\right]\\) 那么 \\(\\boldsymbol{P} \\boldsymbol{X}_{1}=\\boldsymbol{X}_{1}\\) 和 \\(\\boldsymbol{M} \\boldsymbol{X}_{1}=0 .\\)\n练习 3.8 证明 \\(M\\) 是幂等的：\\(M M=M\\)。\n练习 3.9 证明 \\(\\operatorname{tr} M=n-k\\)。\n练习 3.10 证明如果 \\(\\boldsymbol{X}=\\left[\\boldsymbol{X}_{1} \\boldsymbol{X}_{2}\\right]\\) 和 \\(\\boldsymbol{X}_{1}^{\\prime} \\boldsymbol{X}_{2}=0\\) 那么 \\(\\boldsymbol{P}=\\boldsymbol{P}_{1}+\\boldsymbol{P}_{2}\\)。\n练习 3.11 证明当 \\(X\\) 包含一个常数 \\(n^{-1} \\sum_{i=1}^{n} \\widehat{Y}_{i}=\\bar{Y}\\)。\n练习 3.12 一个虚拟变量只取值 0 和 1 。它用于分类变量。令 \\(\\boldsymbol{D}_{1}\\) 和 \\(\\boldsymbol{D}_{2}\\) 为 1 和 0 的向量，如果此人是男性，\\(\\boldsymbol{D}_{1}\\) 的 \\(i^{\\text {th }}\\) 元素等于 1，\\(\\boldsymbol{D}_{2}\\) 的元素等于 0，如果此人是男性，则相反女士。假设样本中有 \\(n_{1}\\) 男性和 \\(n_{2}\\) 女性。考虑通过 OLS 拟合以下三个方程\n\\[\n\\begin{aligned}\n&\\boldsymbol{Y}=\\mu+\\boldsymbol{D}_{1} \\alpha_{1}+\\boldsymbol{D}_{2} \\alpha_{2}+\\boldsymbol{e} \\\\\n&\\boldsymbol{Y}=\\boldsymbol{D}_{1} \\alpha_{1}+\\boldsymbol{D}_{2} \\alpha_{2}+\\boldsymbol{e} \\\\\n&\\boldsymbol{Y}=\\mu+\\boldsymbol{D}_{1} \\phi+\\boldsymbol{e}\n\\end{aligned}\n\\]\nOLS 可以估计所有三个方程 (3.52)、(3.53) 和 (3.54) 吗？如果不是，请解释。\n\n比较回归 (3.53) 和 (3.54)。一个比另一个更通用吗？解释（3.53）和（3.54）中参数之间的关系。\n计算 \\(\\mathbf{1}_{n}^{\\prime} \\boldsymbol{D}_{1}\\) 和 \\(\\mathbf{1}_{n}^{\\prime} \\boldsymbol{D}_{2}\\)，其中 \\(\\mathbf{1}_{n}\\) 是向量的 \\(n \\times 1\\)。\n\n练习 3.13 让 \\(\\boldsymbol{D}_{1}\\) 和 \\(\\boldsymbol{D}_{2}\\) 被定义为在前面的练习中。\n\n在 OLS 回归中\n\n\\[\n\\boldsymbol{Y}=\\boldsymbol{D}_{1} \\widehat{\\gamma}_{1}+\\boldsymbol{D}_{2} \\widehat{\\gamma}_{2}+\\widehat{\\boldsymbol{u}}\n\\]\n表明 \\(\\widehat{\\gamma}_{1}\\) 是样本 \\(\\left(\\bar{Y}_{1}\\right)\\) 的男性中因变量的样本均值，\\(\\widehat{\\gamma}_{2}\\) 是女性 \\(\\left(\\bar{Y}_{2}\\right)\\) 中的样本均值。\n\n令 \\(\\boldsymbol{X}(n \\times k)\\) 是一个附加的回归矩阵。用文字描述转变\n\n\\[\n\\begin{aligned}\n&\\boldsymbol{Y}^{*}=\\boldsymbol{Y}-\\boldsymbol{D}_{1} \\bar{Y}_{1}-\\boldsymbol{D}_{2} \\bar{Y}_{2} \\\\\n&\\boldsymbol{X}^{*}=\\boldsymbol{X}-\\boldsymbol{D}_{1} \\bar{X}_{1}^{\\prime}-\\boldsymbol{D}_{2} \\bar{X}_{2}^{\\prime}\n\\end{aligned}\n\\]\n其中 \\(\\bar{X}_{1}\\) 和 \\(\\bar{X}_{2}\\) 分别是男性和女性回归量的 \\(k \\times 1\\) 均值。 (c) 比较 OLS 回归中的 \\(\\widetilde{\\beta}\\)\n\\[\n\\boldsymbol{Y}^{*}=\\boldsymbol{X}^{*} \\widetilde{\\boldsymbol{\\beta}}+\\widetilde{\\boldsymbol{e}}\n\\]\n来自 OLS 回归的 \\(\\widehat{\\beta}\\)\n\\[\n\\boldsymbol{Y}=\\boldsymbol{D}_{1} \\widehat{\\alpha}_{1}+\\boldsymbol{D}_{2} \\widehat{\\alpha}_{2}+\\boldsymbol{X} \\widehat{\\boldsymbol{\\beta}}+\\widehat{\\boldsymbol{e}} .\n\\]\n练习 3.14 当 \\(\\boldsymbol{Y}_{n}\\) 是 \\(n \\times 1\\) 并且 \\(\\boldsymbol{X}_{n}\\) 是 \\(n \\times k\\) 时，让 \\(\\widehat{\\beta}_{n}=\\left(\\boldsymbol{X}_{n}^{\\prime} \\boldsymbol{X}_{n}\\right)^{-1} \\boldsymbol{X}_{n}^{\\prime} \\boldsymbol{Y}_{n}\\) 表示 OLS 估计。一个新的观察 \\(\\left(Y_{n+1}, X_{n+1}\\right)\\) 变得可用。证明使用这个额外观察计算的 OLS 估计是\n\\[\n\\widehat{\\beta}_{n+1}=\\widehat{\\beta}_{n}+\\frac{1}{1+X_{n+1}^{\\prime}\\left(\\boldsymbol{X}_{n}^{\\prime} \\boldsymbol{X}_{n}\\right)^{-1} X_{n+1}}\\left(\\boldsymbol{X}_{n}^{\\prime} \\boldsymbol{X}_{n}\\right)^{-1} X_{n+1}\\left(Y_{n+1}-X_{n+1}^{\\prime} \\widehat{\\beta}_{n}\\right)\n\\]\n练习 3.15 证明 \\(R^{2}\\) 是 \\(\\boldsymbol{Y}\\) 和 \\(\\widehat{\\boldsymbol{Y}}\\) 之间样本相关性的平方。\n练习 3.16 考虑两个最小二乘回归\n\\[\n\\boldsymbol{Y}=\\boldsymbol{X}_{1} \\widetilde{\\beta}_{1}+\\widetilde{\\boldsymbol{e}}\n\\]\n和\n\\[\n\\boldsymbol{Y}=\\boldsymbol{X}_{1} \\widehat{\\beta}_{1}+\\boldsymbol{X}_{2} \\widehat{\\beta}_{2}+\\widehat{\\boldsymbol{e}} .\n\\]\n让 \\(R_{1}^{2}\\) 和 \\(R_{2}^{2}\\) 是来自两个回归的 \\(R\\)-squared。显示 \\(R_{2}^{2} \\geq R_{1}^{2}\\)。是否存在相等 \\(R_{2}^{2}=R_{1}^{2}\\) 的情况（解释）？\n练习 3.17 对于 (3.46) 中定义的 \\(\\widetilde{\\sigma}^{2}\\)，证明 \\(\\widetilde{\\sigma}^{2} \\geq \\widehat{\\sigma}^{2}\\)。平等可能吗？\n习题 3.18 \\(\\widehat{\\beta}_{(-i)}=\\widehat{\\beta}\\) 对哪些观察结果？\n练习 3.19 对于仅截距模型 \\(Y_{i}=\\beta+e_{i}\\)，证明留一法预测误差为\n\\[\n\\widetilde{e}_{i}=\\left(\\frac{n}{n-1}\\right)\\left(Y_{i}-\\bar{Y}\\right) .\n\\]\n练习 3.20 定义 \\(\\sigma^{2}\\) 的留一估计量，\n\\[\n\\widehat{\\sigma}_{(-i)}^{2}=\\frac{1}{n-1} \\sum_{j \\neq i}\\left(Y_{j}-X_{j}^{\\prime} \\widehat{\\beta}_{(-i)}\\right)^{2} .\n\\]\n这是从样本中获得的估计量，省略了观察 \\(i\\)。显示\n\\[\n\\widehat{\\sigma}_{(-i)}^{2}=\\frac{n}{n-1} \\widehat{\\sigma}^{2}-\\frac{\\widehat{e}_{i}^{2}}{(n-1)\\left(1-h_{i i}\\right)} .\n\\]\n练习 3.21 考虑最小二乘回归估计量\n\\[\nY_{i}=X_{1 i} \\widehat{\\beta}_{1}+X_{2 i} \\widehat{\\beta}_{2}+\\widehat{e}_{i}\n\\]\n和“一次一个回归器”回归估计器\n\\[\nY_{i}=X_{1 i} \\widetilde{\\beta}_{1}+\\widetilde{e}_{1 i}, \\quad Y_{i}=X_{2 i} \\widetilde{\\beta}_{2}+\\widetilde{e}_{2 i}\n\\]\n在什么条件下 \\(\\widetilde{\\beta}_{1}=\\widehat{\\beta}_{1}\\) 和 \\(\\widetilde{\\beta}_{2}=\\widehat{\\beta}_{2}\\) ？练习 3.22 你估计一个最小二乘回归\n\\[\nY_{i}=X_{1 i}^{\\prime} \\widetilde{\\beta}_{1}+\\widetilde{u}_{i}\n\\]\n然后在另一组回归器上回归残差\n\\[\n\\widetilde{u}_{i}=X_{2 i}^{\\prime} \\widetilde{\\beta}_{2}+\\widetilde{e}_{i}\n\\]\n这第二个回归是否为您提供与两组回归量的最小二乘回归估计相同的估计系数？\n\\[\nY_{i}=X_{1 i}^{\\prime} \\widehat{\\beta}_{1}+X_{2 i}^{\\prime} \\widehat{\\beta}_{2}+\\widehat{e}_{i}\n\\]\n换句话说，\\(\\widetilde{\\beta}_{2}=\\widehat{\\beta}_{2}\\) 是真的吗？解释你的推理。\n练习 3.23 数据矩阵是 \\((\\boldsymbol{Y}, \\boldsymbol{X})\\) 和 \\(\\boldsymbol{X}=\\left[\\boldsymbol{X}_{1}, \\boldsymbol{X}_{2}\\right]\\)，考虑转换后的回归矩阵 \\(\\boldsymbol{Z}=\\left[\\boldsymbol{X}_{1}, \\boldsymbol{X}_{2}-\\boldsymbol{X}_{1}\\right]\\)。假设您对 \\(\\boldsymbol{X}\\) 进行 \\(\\boldsymbol{Y}\\) 的最小二乘回归，并在 \\(\\boldsymbol{Z}\\) 上进行 \\(\\boldsymbol{Y}\\) 的最小二乘回归。让 \\(\\widehat{\\sigma}^{2}\\) 和 \\(\\widetilde{\\sigma}^{2}\\) 表示来自两个回归的残差方差估计。给出一个有关 \\((\\boldsymbol{Y}, \\boldsymbol{X})\\) 和 \\((\\boldsymbol{Y}, \\boldsymbol{X})\\) 的公式？ （解释你的理由。）\n练习 3.24 使用 \\(3.22\\) 节中描述的 cps09mar 数据集，该数据集可在教科书网站上找到。取用于方程（3.49）的子样本（见第 3.25 节）进行数据构建）\n\n估计方程 (3.49) 并计算方程 \\(R^{2}\\) 和误差平方和。\n使用残差回归法重新估计教育的斜率。经验及其平方的回归日志（工资），经验及其平方的回归教育，以及残差的残差。报告最终回归的估计值，以及方程 \\(R^{2}\\) 和误差平方和。斜率系数是否等于（3.49）中的值？解释。\n\n和 (b) 部分的 \\(R^{2}\\) 和平方和误差是否相等？解释。\n\n\n练习 3.25 估计方程（3.49），如上题的（a）部分。令 \\(\\widehat{e}_{i}\\) 为 OLS 残差，\\(\\widehat{Y}_{i}\\) 为回归的预测值，\\(X_{1 i}\\) 为教育，\\(X_{2 i}\\) 为经验。数值计算如下：\\ (a) \\(\\sum_{i=1}^{n} \\widehat{e}_{i}\\)\\ (b) \\(\\sum_{i=1}^{n} X_{1 i} \\widehat{e}_{i}\\)\\ (c) \\(\\sum_{i=1}^{n} X_{2 i} \\widehat{e}_{i}\\)\\ (d) \\(\\sum_{i=1}^{n} X_{1 i}^{2} \\widehat{e}_{i}\\)\\ (e) \\(\\sum_{i=1}^{n} X_{2 i}^{2} \\widehat{e}_{i}\\)\\ (f) \\(\\sum_{i=1}^{n} \\widehat{Y}_{i} \\widehat{e}_{i}\\)\\ (g) \\(\\sum_{i=1}^{n} \\widehat{e}_{i}^{2}\\)\n这些计算是否与 OLS 的理论性质一致？解释。\n练习 3.26 使用 cps09mar 数据集。 (a) 估计西班牙裔白人男性子样本的对数工资回归。除了教育、经验及其平方之外，还包括一组用于地区和婚姻状况的二元变量。对于区域，为 Northeast、South 和 West 创建虚拟变量，以便将 Midwest 排除在外。对于婚姻状况，为已婚、丧偶或离婚和分居创建变量，因此单身（从未结婚）是被排除的组。\n\n重复使用不同的计量经济学包。比较你的结果。他们同意吗？"
  },
  {
    "objectID": "chpt04-lsr.html#introduction",
    "href": "chpt04-lsr.html#introduction",
    "title": "4  Least Squares Regression",
    "section": "4.1 Introduction",
    "text": "4.1 Introduction\nIn this chapter we investigate some finite-sample properties of the least squares estimator in the linear regression model. In particular we calculate its finite-sample expectation and covariance matrix and propose standard errors for the coefficient estimators."
  },
  {
    "objectID": "chpt04-lsr.html#random-sampling",
    "href": "chpt04-lsr.html#random-sampling",
    "title": "4  Least Squares Regression",
    "section": "4.2 Random Sampling",
    "text": "4.2 Random Sampling\nAssumption \\(3.1\\) specified that the observations have identical distributions. To derive the finitesample properties of the estimators we will need to additionally specify the dependence structure across the observations.\nThe simplest context is when the observations are mutually independent in which case we say that they are independent and identically distributed or i.i.d. It is also common to describe i.i.d. observations as a random sample. Traditionally, random sampling has been the default assumption in crosssection (e.g. survey) contexts. It is quite convenient as i.i.d. sampling leads to straightforward expressions for estimation variance. The assumption seems appropriate (meaning that it should be approximately valid) when samples are small and relatively dispersed. That is, if you randomly sample 1000 people from a large country such as the United States it seems reasonable to model their responses as mutually independent.\nAssumption 4.1 The random variables \\(\\left\\{\\left(Y_{1}, X_{1}\\right), \\ldots,\\left(Y_{i}, X_{i}\\right), \\ldots,\\left(Y_{n}, X_{n}\\right)\\right\\}\\) are independent and identically distributed.\nFor most of this chapter we will use Assumption \\(4.1\\) to derive properties of the OLS estimator.\nAssumption \\(4.1\\) means that if you take any two individuals \\(i \\neq j\\) in a sample, the values \\(\\left(Y_{i}, X_{i}\\right)\\) are independent of the values \\(\\left(Y_{j}, X_{j}\\right)\\) yet have the same distribution. Independence means that the decisions and choices of individual \\(i\\) do not affect the decisions of individual \\(j\\) and conversely.\nThis assumption may be violated if individuals in the sample are connected in some way, for example if they are neighbors, members of the same village, classmates at a school, or even firms within a specific industry. In this case it seems plausible that decisions may be inter-connected and thus mutually dependent rather than independent. Allowing for such interactions complicates inference and requires specialized treatment. A currently popular approach which allows for mutual dependence is known as clustered dependence which assumes that that observations are grouped into “clusters” (for example, schools). We will discuss clustering in more detail in Section 4.21."
  },
  {
    "objectID": "chpt04-lsr.html#sample-mean",
    "href": "chpt04-lsr.html#sample-mean",
    "title": "4  Least Squares Regression",
    "section": "4.3 Sample Mean",
    "text": "4.3 Sample Mean\nWe start with the simplest setting of the intercept-only model\n\\[\n\\begin{aligned}\nY &=\\mu+e \\\\\n\\mathbb{E}[e] &=0 .\n\\end{aligned}\n\\]\nwhich is equivalent to the regression model with \\(k=1\\) and \\(X=1\\). In the intercept model \\(\\mu=\\mathbb{E}[Y]\\) is the expectation of \\(Y\\). (See Exercise 2.15.) The least squares estimator \\(\\widehat{\\mu}=\\bar{Y}\\) equals the sample mean as shown in equation (3.8).\nWe now calculate the expectation and variance of the estimator \\(\\bar{Y}\\). Since the sample mean is a linear function of the observations its expectation is simple to calculate\n\\[\n\\mathbb{E}[\\bar{Y}]=\\mathbb{E}\\left[\\frac{1}{n} \\sum_{i=1}^{n} Y_{i}\\right]=\\frac{1}{n} \\sum_{i=1}^{n} \\mathbb{E}\\left[Y_{i}\\right]=\\mu .\n\\]\nThis shows that the expected value of the least squares estimator (the sample mean) equals the projection coefficient (the population expectation). An estimator with the property that its expectation equals the parameter it is estimating is called unbiased.\nDefinition \\(4.1\\) An estimator \\(\\widehat{\\theta}\\) for \\(\\theta\\) is unbiased if \\(\\mathbb{E}[\\widehat{\\theta}]=\\theta\\)\nWe next calculate the variance of the estimator \\(\\bar{Y}\\) under Assumption 4.1. Making the substitution \\(Y_{i}=\\mu+e_{i}\\) we find\n\\[\n\\bar{Y}-\\mu=\\frac{1}{n} \\sum_{i=1}^{n} e_{i} .\n\\]\nThen\n\\[\n\\begin{aligned}\n\\operatorname{var}[\\bar{Y}] &=\\mathbb{E}\\left[(\\bar{Y}-\\mu)^{2}\\right] \\\\\n&=\\mathbb{E}\\left[\\left(\\frac{1}{n} \\sum_{i=1}^{n} e_{i}\\right)\\left(\\frac{1}{n} \\sum_{j=1}^{n} e_{j}\\right)\\right] \\\\\n&=\\frac{1}{n^{2}} \\sum_{i=1}^{n} \\sum_{j=1}^{n} \\mathbb{E}\\left[e_{i} e_{j}\\right] \\\\\n&=\\frac{1}{n^{2}} \\sum_{i=1}^{n} \\sigma^{2} \\\\\n&=\\frac{1}{n} \\sigma^{2} .\n\\end{aligned}\n\\]\nThe second-to-last inequality is because \\(\\mathbb{E}\\left[e_{i} e_{j}\\right]=\\sigma^{2}\\) for \\(i=j\\) yet \\(\\mathbb{E}\\left[e_{i} e_{j}\\right]=0\\) for \\(i \\neq j\\) due to independence.\nWe have shown that \\(\\operatorname{var}[\\bar{Y}]=\\frac{1}{n} \\sigma^{2}\\). This is the familiar formula for the variance of the sample mean."
  },
  {
    "objectID": "chpt04-lsr.html#linear-regression-model",
    "href": "chpt04-lsr.html#linear-regression-model",
    "title": "4  Least Squares Regression",
    "section": "4.4 Linear Regression Model",
    "text": "4.4 Linear Regression Model\nWe now consider the linear regression model. Throughout this chapter we maintain the following.\nAssumption 4.2 Linear Regression Model The variables \\((Y, X)\\) satisfy the linear regression equation\n\\[\n\\begin{aligned}\nY &=X^{\\prime} \\beta+e \\\\\n\\mathbb{E}[e \\mid X] &=0 .\n\\end{aligned}\n\\]\nThe variables have finite second moments\n\\[\n\\begin{aligned}\n&\\mathbb{E}\\left[Y^{2}\\right]<\\infty \\\\\n&\\mathbb{E}\\|X\\|^{2}<\\infty\n\\end{aligned}\n\\]\nand an invertible design matrix\n\\[\n\\boldsymbol{Q}_{X X}=\\mathbb{E}\\left[X X^{\\prime}\\right]>0 .\n\\]\nWe will consider both the general case of heteroskedastic regression where the conditional variance \\(\\mathbb{E}\\left[e^{2} \\mid X\\right]=\\sigma^{2}(X)\\) is unrestricted, and the specialized case of homoskedastic regression where the conditional variance is constant. In the latter case we add the following assumption.\nAssumption 4.3 Homoskedastic Linear Regression Model In addition to Assumption \\(4.2\\)\n\\[\n\\mathbb{E}\\left[e^{2} \\mid X\\right]=\\sigma^{2}(X)=\\sigma^{2}\n\\]\nis independent of \\(X\\)."
  },
  {
    "objectID": "chpt04-lsr.html#expectation-of-least-squares-estimator",
    "href": "chpt04-lsr.html#expectation-of-least-squares-estimator",
    "title": "4  Least Squares Regression",
    "section": "4.5 Expectation of Least Squares Estimator",
    "text": "4.5 Expectation of Least Squares Estimator\nIn this section we show that the OLS estimator is unbiased in the linear regression model. This calculation can be done using either summation notation or matrix notation. We will use both.\nFirst take summation notation. Observe that under (4.1)-(4.2)\n\\[\n\\mathbb{E}\\left[Y_{i} \\mid X_{1}, \\ldots, X_{n}\\right]=\\mathbb{E}\\left[Y_{i} \\mid X_{i}\\right]=X_{i}^{\\prime} \\beta .\n\\]\nThe first equality states that the conditional expectation of \\(Y_{i}\\) given \\(\\left\\{X_{1}, \\ldots, X_{n}\\right\\}\\) only depends on \\(X_{i}\\) because the observations are independent across \\(i\\). The second equality is the assumption of a linear conditional expectation. Using definition (3.11), the conditioning theorem (Theorem 2.3), the linearity of expectations, (4.4), and properties of the matrix inverse,\n\\[\n\\begin{aligned}\n\\mathbb{E}\\left[\\widehat{\\beta} \\mid X_{1}, \\ldots, X_{n}\\right] &=\\mathbb{E}\\left[\\left(\\sum_{i=1}^{n} X_{i} X_{i}^{\\prime}\\right)^{-1}\\left(\\sum_{i=1}^{n} X_{i} Y_{i}\\right) \\mid X_{1}, \\ldots, X_{n}\\right] \\\\\n&=\\left(\\sum_{i=1}^{n} X_{i} X_{i}^{\\prime}\\right)^{-1} \\mathbb{E}\\left[\\left(\\sum_{i=1}^{n} X_{i} Y_{i}\\right) \\mid X_{1}, \\ldots, X_{n}\\right] \\\\\n&=\\left(\\sum_{i=1}^{n} X_{i} X_{i}^{\\prime}\\right)^{-1} \\sum_{i=1}^{n} \\mathbb{E}\\left[X_{i} Y_{i} \\mid X_{1}, \\ldots, X_{n}\\right] \\\\\n&=\\left(\\sum_{i=1}^{n} X_{i} X_{i}^{\\prime}\\right)^{-1} \\sum_{i=1}^{n} X_{i} \\mathbb{E}\\left[Y_{i} \\mid X_{i}\\right] \\\\\n&=\\left(\\sum_{i=1}^{n} X_{i} X_{i}^{\\prime}\\right)^{-1} \\sum_{i=1}^{n} X_{i} X_{i}^{\\prime} \\beta \\\\\n&=\\beta .\n\\end{aligned}\n\\]\nNow let’s show the same result using matrix notation. (4.4) implies\n\\[\n\\mathbb{E}[\\boldsymbol{Y} \\mid \\boldsymbol{X}]=\\left(\\begin{array}{c}\n\\vdots \\\\\n\\mathbb{E}\\left[Y_{i} \\mid \\boldsymbol{X}\\right] \\\\\n\\vdots\n\\end{array}\\right)=\\left(\\begin{array}{c}\n\\vdots \\\\\nX_{i}^{\\prime} \\beta \\\\\n\\vdots\n\\end{array}\\right)=\\boldsymbol{X} \\beta .\n\\]\nSimilarly\n\\[\n\\mathbb{E}[\\boldsymbol{e} \\mid \\boldsymbol{X}]=\\left(\\begin{array}{c}\n\\vdots \\\\\n\\mathbb{E}\\left[e_{i} \\mid \\boldsymbol{X}\\right] \\\\\n\\vdots\n\\end{array}\\right)=\\left(\\begin{array}{c}\n\\vdots \\\\\n\\mathbb{E}\\left[e_{i} \\mid X_{i}\\right] \\\\\n\\vdots\n\\end{array}\\right)=0 .\n\\]\nUsing \\(\\widehat{\\beta}=\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{Y}\\right)\\), the conditioning theorem, the linearity of expectations, (4.5), and the properties of the matrix inverse,\n\\[\n\\begin{aligned}\n\\mathbb{E}[\\widehat{\\beta} \\mid \\boldsymbol{X}] &=\\mathbb{E}\\left[\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\boldsymbol{X}^{\\prime} \\boldsymbol{Y} \\mid \\boldsymbol{X}\\right] \\\\\n&=\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\boldsymbol{X}^{\\prime} \\mathbb{E}[\\boldsymbol{Y} \\mid \\boldsymbol{X}] \\\\\n&=\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\boldsymbol{X}^{\\prime} \\boldsymbol{X} \\beta \\\\\n&=\\beta .\n\\end{aligned}\n\\]\nAt the risk of belaboring the derivation, another way to calculate the same result is as follows. Insert \\(\\boldsymbol{Y}=\\boldsymbol{X} \\beta+\\boldsymbol{e}\\) into the formula for \\(\\widehat{\\beta}\\) to obtain\n\\[\n\\begin{aligned}\n\\widehat{\\beta} &=\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1}\\left(\\boldsymbol{X}^{\\prime}(\\boldsymbol{X} \\beta+\\boldsymbol{e})\\right) \\\\\n&=\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\boldsymbol{X}^{\\prime} \\boldsymbol{X} \\beta+\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{e}\\right) \\\\\n&=\\beta+\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\boldsymbol{X}^{\\prime} \\boldsymbol{e} .\n\\end{aligned}\n\\]\nThis is a useful linear decomposition of the estimator \\(\\widehat{\\beta}\\) into the true parameter \\(\\beta\\) and the stochastic component \\(\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\boldsymbol{X}^{\\prime} \\boldsymbol{e}\\). Once again, we can calculate that\n\\[\n\\begin{aligned}\n\\mathbb{E}[\\widehat{\\beta}-\\beta \\mid \\boldsymbol{X}] &=\\mathbb{E}\\left[\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\boldsymbol{X}^{\\prime} \\boldsymbol{e} \\mid \\boldsymbol{X}\\right] \\\\\n&=\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\boldsymbol{X}^{\\prime} \\mathbb{E}[\\boldsymbol{e} \\mid \\boldsymbol{X}]=0 .\n\\end{aligned}\n\\]\nRegardless of the method we have shown that \\(\\mathbb{E}[\\widehat{\\beta} \\mid \\boldsymbol{X}]=\\beta\\). We have shown the following theorem.\nTheorem 4.1 Expectation of Least Squares Estimator In the linear regression model (Assumption 4.2) with i.i.d. sampling (Assumption 4.1)\n\\[\n\\mathbb{E}[\\widehat{\\beta} \\mid \\boldsymbol{X}]=\\beta .\n\\]\nEquation (4.7) says that the estimator \\(\\widehat{\\beta}\\) is unbiased for \\(\\beta\\), conditional on \\(\\boldsymbol{X}\\). This means that the conditional distribution of \\(\\widehat{\\beta}\\) is centered at \\(\\beta\\). By “conditional on \\(X\\)” this means that the distribution is unbiased for any realization of the regressor matrix \\(\\boldsymbol{X}\\). In conditional models we simply refer to this as saying \\(\" \\widehat{\\beta}\\) is unbiased for \\(\\beta\\) “.\nIt is worth mentioning that Theorem 4.1, and all finite sample results in this chapter, make the implicit assumption that \\(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\) is full rank with probability one."
  },
  {
    "objectID": "chpt04-lsr.html#variance-of-least-squares-estimator",
    "href": "chpt04-lsr.html#variance-of-least-squares-estimator",
    "title": "4  Least Squares Regression",
    "section": "4.6 Variance of Least Squares Estimator",
    "text": "4.6 Variance of Least Squares Estimator\nIn this section we calculate the conditional variance of the OLS estimator.\nFor any \\(r \\times 1\\) random vector \\(Z\\) define the \\(r \\times r\\) covariance matrix\n\\[\n\\operatorname{var}[Z]=\\mathbb{E}\\left[(Z-\\mathbb{E}[Z])(Z-\\mathbb{E}[Z])^{\\prime}\\right]=\\mathbb{E}\\left[Z Z^{\\prime}\\right]-(\\mathbb{E}[Z])(\\mathbb{E}[Z])^{\\prime}\n\\]\nand for any pair \\((Z, X)\\) define the conditional covariance matrix\n\\[\n\\operatorname{var}[Z \\mid X]=\\mathbb{E}\\left[(Z-\\mathbb{E}[Z \\mid X])(Z-\\mathbb{E}[Z \\mid X])^{\\prime} \\mid X\\right] .\n\\]\nWe define \\(\\boldsymbol{V}_{\\widehat{\\beta}} \\stackrel{\\text { def }}{=} \\operatorname{var}[\\widehat{\\beta} \\mid \\boldsymbol{X}]\\) as the conditional covariance matrix of the regression coefficient estimators. We now derive its form.\nThe conditional covariance matrix of the \\(n \\times 1\\) regression error \\(\\boldsymbol{e}\\) is the \\(n \\times n\\) matrix\n\\[\n\\operatorname{var}[\\boldsymbol{e} \\mid \\boldsymbol{X}]=\\mathbb{E}\\left[\\boldsymbol{e} \\boldsymbol{e}^{\\prime} \\mid \\boldsymbol{X}\\right] \\stackrel{\\text { def }}{=} \\boldsymbol{D} .\n\\]\nThe \\(i^{t h}\\) diagonal element of \\(\\boldsymbol{D}\\) is\n\\[\n\\mathbb{E}\\left[e_{i}^{2} \\mid \\boldsymbol{X}\\right]=\\mathbb{E}\\left[e_{i}^{2} \\mid X_{i}\\right]=\\sigma_{i}^{2}\n\\]\nwhile the \\(i j^{t h}\\) off-diagonal element of \\(\\boldsymbol{D}\\) is\n\\[\n\\mathbb{E}\\left[e_{i} e_{j} \\mid \\boldsymbol{X}\\right]=\\mathbb{E}\\left(e_{i} \\mid X_{i}\\right) \\mathbb{E}\\left[e_{j} \\mid X_{j}\\right]=0\n\\]\nwhere the first equality uses independence of the observations (Assumption 4.1) and the second is (4.2). Thus \\(\\boldsymbol{D}\\) is a diagonal matrix with \\(i^{t h}\\) diagonal element \\(\\sigma_{i}^{2}\\) :\n\\[\n\\boldsymbol{D}=\\operatorname{diag}\\left(\\sigma_{1}^{2}, \\ldots, \\sigma_{n}^{2}\\right)=\\left(\\begin{array}{cccc}\n\\sigma_{1}^{2} & 0 & \\cdots & 0 \\\\\n0 & \\sigma_{2}^{2} & \\cdots & 0 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & \\cdots & \\sigma_{n}^{2}\n\\end{array}\\right)\n\\]\nIn the special case of the linear homoskedastic regression model (4.3), then \\(\\mathbb{E}\\left[e_{i}^{2} \\mid X_{i}\\right]=\\sigma_{i}^{2}=\\sigma^{2}\\) and we have the simplification \\(\\boldsymbol{D}=\\boldsymbol{I}_{n} \\sigma^{2}\\). In general, however, \\(\\boldsymbol{D}\\) need not necessarily take this simplified form.\nFor any \\(n \\times r\\) matrix \\(\\boldsymbol{A}=\\boldsymbol{A}(\\boldsymbol{X})\\),\n\\[\n\\operatorname{var}\\left[\\boldsymbol{A}^{\\prime} \\boldsymbol{Y} \\mid \\boldsymbol{X}\\right]=\\operatorname{var}\\left[\\boldsymbol{A}^{\\prime} \\boldsymbol{e} \\mid \\boldsymbol{X}\\right]=\\boldsymbol{A}^{\\prime} \\boldsymbol{D} \\boldsymbol{A} .\n\\]\nIn particular, we can write \\(\\widehat{\\beta}=\\boldsymbol{A}^{\\prime} \\boldsymbol{Y}\\) where \\(\\boldsymbol{A}=\\boldsymbol{X}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1}\\) and thus\n\\[\n\\boldsymbol{V}_{\\widehat{\\beta}}=\\operatorname{var}[\\widehat{\\beta} \\mid \\boldsymbol{X}]=\\boldsymbol{A}^{\\prime} \\boldsymbol{D} \\boldsymbol{A}=\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\boldsymbol{X}^{\\prime} \\boldsymbol{D} \\boldsymbol{X}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} .\n\\]\nIt is useful to note that\n\\[\n\\boldsymbol{X}^{\\prime} \\boldsymbol{D} \\boldsymbol{X}=\\sum_{i=1}^{n} X_{i} X_{i}^{\\prime} \\sigma_{i}^{2},\n\\]\na weighted version of \\(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\).\nIn the special case of the linear homoskedastic regression model, \\(\\boldsymbol{D}=\\boldsymbol{I}_{n} \\sigma^{2}\\), so \\(\\boldsymbol{X}^{\\prime} \\boldsymbol{D} \\boldsymbol{X}=\\boldsymbol{X}^{\\prime} \\boldsymbol{X} \\sigma^{2}\\), and the covariance matrix simplifies to \\(\\boldsymbol{V}_{\\widehat{\\beta}}=\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\sigma^{2}\\).\nTheorem 4.2 Variance of Least Squares Estimator In the linear regression model (Assumption 4.2) with i.i.d. sampling (Assumption 4.1)\n\\[\n\\boldsymbol{V}_{\\widehat{\\beta}}=\\operatorname{var}[\\widehat{\\beta} \\mid \\boldsymbol{X}]=\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{D} \\boldsymbol{X}\\right)\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1}\n\\]\nwhere \\(\\boldsymbol{D}\\) is defined in (4.8). If in addition the error is homoskedastic (Assumption 4.3) then (4.10) simplifies to \\(\\boldsymbol{V}_{\\widehat{\\beta}}=\\sigma^{2}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1}\\)."
  },
  {
    "objectID": "chpt04-lsr.html#unconditional-moments",
    "href": "chpt04-lsr.html#unconditional-moments",
    "title": "4  Least Squares Regression",
    "section": "4.7 Unconditional Moments",
    "text": "4.7 Unconditional Moments\nThe previous sections derived the form of the conditional expectation and variance of the least squares estimator where we conditioned on the regressor matrix \\(\\boldsymbol{X}\\). What about the unconditional expectation and variance?\nIndeed, it is not obvious if \\(\\widehat{\\beta}\\) has a finite expectation or variance. Take the case of a single dummy variable regressor \\(D_{i}\\) with no intercept. Assume \\(\\mathbb{P}\\left[D_{i}=1\\right]=p<1\\). Then\n\\[\n\\widehat{\\beta}=\\frac{\\sum_{i=1}^{n} D_{i} Y_{i}}{\\sum_{i=1}^{n} D_{i}}\n\\]\nis well defined if \\(\\sum_{i=1}^{n} D_{i}>0\\). However, \\(\\mathbb{P}\\left[\\sum_{i=1}^{n} D_{i}=0\\right]=(1-p)^{n}>0\\). This means that with positive (but small) probability \\(\\widehat{\\beta}\\) does not exist. Consequently \\(\\widehat{\\beta}\\) has no finite moments! We ignore this complication in practice but it does pose a conundrum for theory. This existence problem arises whenever there are discrete regressors.\nThis dilemma is avoided when the regressors have continuous distributions. A clean statement was obtained by Kinal (1980) under the assumption of normal regressors and errors. Theorem 4.3 Kinal (1980)\nIn the linear regression model with i.i.d. sampling, if in addition \\((X, e)\\) have a joint normal distribution, then for any \\(r, \\mathbb{E}\\|\\widehat{\\beta}\\|^{r}<\\infty\\) if and only if \\(r<n-k+1\\).\nThis shows that when the errors and regressors are normally distributed that the least squares estimator possesses all moments up to \\(n-k\\) which includes all moments of practical interest. The normality assumption is not critical for this result. What is key is the assumption that the regressors are continuously distributed.\nThe law of iterated expectations (Theorem 2.1) combined with Theorems \\(4.1\\) and \\(4.3\\) allow us to deduce that the least squares estimator is unconditionally unbiased. Under the normality assumption Theorem \\(4.3\\) allows us to apply the law of iterated expectations, and thus using Theorems \\(4.1\\) we deduce that if \\(n>k\\)\n\\[\n\\mathbb{E}[\\widehat{\\beta}]=\\mathbb{E}[\\mathbb{E}[\\widehat{\\beta} \\mid \\boldsymbol{X}]]=\\beta .\n\\]\nHence \\(\\widehat{\\beta}\\) is unconditionally unbiased as asserted.\nFurthermore, if \\(n-k>1\\) then \\(\\mathbb{E}\\|\\widehat{\\beta}\\|^{2}<\\infty\\) and \\(\\widehat{\\beta}\\) has a finite unconditional variance. Using Theorem \\(2.8\\) we can calculate explicitly that\n\\[\n\\operatorname{var}[\\widehat{\\beta}]=\\mathbb{E}[\\operatorname{var}[\\widehat{\\beta} \\mid \\boldsymbol{X}]]+\\operatorname{var}[\\mathbb{E}[\\widehat{\\beta} \\mid \\boldsymbol{X}]]=\\mathbb{E}\\left[\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{D} \\boldsymbol{X}\\right)\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1}\\right]\n\\]\nthe second equality because \\(\\mathbb{E}[\\widehat{\\beta} \\mid \\boldsymbol{X}]=\\beta\\) has zero variance. In the homoskedastic case this simplifies to\n\\[\n\\operatorname{var}[\\widehat{\\beta}]=\\sigma^{2} \\mathbb{E}\\left[\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1}\\right] .\n\\]\nIn both cases the expectation cannot pass through the matrix inverse because this is a nonlinear function. Thus there is not a simple expression for the unconditional variance, other than stating that is it the expectation of the conditional variance."
  },
  {
    "objectID": "chpt04-lsr.html#gauss-markov-theorem",
    "href": "chpt04-lsr.html#gauss-markov-theorem",
    "title": "4  Least Squares Regression",
    "section": "4.8 Gauss-Markov Theorem",
    "text": "4.8 Gauss-Markov Theorem\nThe Gauss-Markov Theorem is one of the most celebrated results in econometric theory. It provides a classical justification for the least squares estimator, showing that it is lowest variance among unbiased estimators.\nWrite the homoskedastic linear regression model in vector format as\n\\[\n\\begin{aligned}\n\\boldsymbol{Y} &=\\boldsymbol{X} \\beta+\\boldsymbol{e} \\\\\n\\mathbb{E}[\\boldsymbol{e} \\mid \\boldsymbol{X}] &=0 \\\\\n\\operatorname{var}[\\boldsymbol{e} \\mid \\boldsymbol{X}] &=\\boldsymbol{I}_{n} \\sigma^{2} .\n\\end{aligned}\n\\]\nIn this model we know that the least squares estimator is unbiased for \\(\\beta\\) and has covariance matrix \\(\\sigma^{2}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1}\\). The question raised in this section is if there exists an alternative unbiased estimator \\(\\widetilde{\\beta}\\) which has a smaller covariance matrix.\nThe following version of the theorem is due to B. E. Hansen (2021).\nTheorem 4.4 Gauss-Markov Take the homoskedastic linear regression model (4.11)-(4.13). If \\(\\widetilde{\\beta}\\) is an unbiased estimator of \\(\\beta\\) then\n\\[\n\\operatorname{var}[\\widetilde{\\beta} \\mid \\boldsymbol{X}] \\geq \\sigma^{2}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1}\n\\]\nTheorem \\(4.4\\) provides a lower bound on the covariance matrix of unbiased estimators under the assumption of homoskedasticity. It says that no unbiased estimator can have a variance matrix smaller (in the positive definite sense) than \\(\\sigma^{2}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1}\\). Since the variance of the OLS estimator is exactly equal to this bound this means that no unbiased estimator has a lower variance than OLS. Consequently we describe OLS as efficient in the class of unbiased estimators.\nThis earliest version of Theorem \\(4.4\\) was articulated by Carl Friedrich Gauss in 1823. Andreı̆ Andreevich Markov provided a textbook treatment of the theorem in 1912, and clarified the central role of unbiasedness, which Gauss had only assumed implicitly.\nTheir versions of the Theorem restricted attention to linear estimators of \\(\\beta\\), which are estimators that can be written as \\(\\widetilde{\\beta}=\\boldsymbol{A}^{\\prime} \\boldsymbol{Y}\\), where \\(\\boldsymbol{A}=\\boldsymbol{A}(\\boldsymbol{X})\\) is an \\(m \\times n\\) function of the regressors \\(\\boldsymbol{X}\\). Linearity in this context means “linear in \\(\\boldsymbol{Y}\\)”. This restriction simplifies variance calculations, but greatly limits the class of estimators.This classical version of the Theorem gave rise to the description of OLS as the best linear unbiased estimator (BLUE). However, Theorem \\(4.4\\) as stated above shows that OLS is the best unbiased estimator (BUE).\nThe derivation of the Gauss-Markov Theorem under the restriction to linear estimators is straightforward, so we now provide this demonstration. For \\(\\widetilde{\\beta}=\\boldsymbol{A}^{\\prime} \\boldsymbol{Y}\\) we have\n\\[\n\\mathbb{E}[\\widetilde{\\beta} \\mid \\boldsymbol{X}]=\\boldsymbol{A}^{\\prime} \\mathbb{E}[\\boldsymbol{Y} \\mid \\boldsymbol{X}]=\\boldsymbol{A}^{\\prime} \\boldsymbol{X} \\beta,\n\\]\nthe second equality because \\(\\mathbb{E}[\\boldsymbol{Y} \\mid \\boldsymbol{X}]=\\boldsymbol{X} \\beta\\). Then \\(\\widetilde{\\beta}\\) is unbiased for all \\(\\beta\\) if (and only if) \\(\\boldsymbol{A}^{\\prime} \\boldsymbol{X}=\\boldsymbol{I}_{k}\\). Furthermore, we saw in (4.9) that\n\\[\n\\operatorname{var}[\\widetilde{\\beta} \\mid \\boldsymbol{X}]=\\operatorname{var}\\left[\\boldsymbol{A}^{\\prime} \\boldsymbol{Y} \\mid \\boldsymbol{X}\\right]=\\boldsymbol{A}^{\\prime} \\boldsymbol{D} \\boldsymbol{A}=\\boldsymbol{A}^{\\prime} \\boldsymbol{A} \\boldsymbol{\\sigma}^{2}\n\\]\nthe last equality using the homoskedasticity assumption (4.13). To establish the Theorem we need to show that for any such matrix \\(\\boldsymbol{A}\\),\n\\[\n\\boldsymbol{A}^{\\prime} \\boldsymbol{A} \\geq\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\text {. }\n\\]\nSet \\(\\boldsymbol{C}=\\boldsymbol{A}-\\boldsymbol{X}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1}\\). Note that \\(\\boldsymbol{X}^{\\prime} \\boldsymbol{C}=0\\). We calculate that\n\\[\n\\begin{aligned}\n\\boldsymbol{A}^{\\prime} \\boldsymbol{A}-\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} &=\\left(\\boldsymbol{C}+\\boldsymbol{X}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1}\\right)^{\\prime}\\left(\\boldsymbol{C}+\\boldsymbol{X}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1}\\right)-\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\\\\n&=\\boldsymbol{C}^{\\prime} \\boldsymbol{C}+\\boldsymbol{C}^{\\prime} \\boldsymbol{X}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1}+\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\boldsymbol{X}^{\\prime} \\boldsymbol{C} \\\\\n&+\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1}-\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\\\\n&=\\boldsymbol{C}^{\\prime} \\boldsymbol{C} \\geq 0\n\\end{aligned}\n\\]\nThe final inequality states that the matrix \\(\\boldsymbol{C}^{\\prime} \\boldsymbol{C}\\) is positive semi-definite which is a property of quadratic forms (see Appendix A.10). We have shown (4.14) as required.\nThe above derivation imposed the restriction that the estimator \\(\\widetilde{\\beta}\\) is linear in \\(\\boldsymbol{Y}\\). The proof of Theorem \\(4.4\\) in the general case is considerably more advanced. Here, we provide a simplified sketch of the argument for interested readers, with a complete proof in Section 4.24. For simplicity, treat the regressors \\(X\\) as fixed, and suppose that \\(Y\\) has a density \\(f(y)\\) with bounded support \\(\\mathscr{Y}\\). Without loss of generality assume that the true coefficient equals \\(\\beta_{0}=0\\).\nSince \\(Y\\) has bounded support \\(\\mathscr{Y}\\) there is a set \\(B \\subset \\mathbb{R}^{m}\\) such that \\(\\left|y X^{\\prime} \\beta / \\sigma^{2}\\right|<1\\) for all \\(\\beta \\in B\\) and \\(y \\in \\mathscr{Y}\\). For such values of \\(\\beta\\), define the auxiliary density function\n\\[\nf_{\\beta}(y)=f(y)\\left(1+y X^{\\prime} \\beta / \\sigma^{2}\\right) .\n\\]\nUnder the assumptions, \\(0 \\leq f_{\\beta}(y) \\leq 2 f(y), f_{\\beta}(y)\\) has support \\(\\mathscr{Y}\\), and \\(\\int_{\\mathscr{Y}} f_{\\beta}(y) d y=1\\). To see the later, observe that \\(\\int_{\\mathscr{Y}} y f(y) d y=X^{\\prime} \\beta_{0}=0\\) under the normalization \\(\\beta_{0}=0\\), and thus\n\\[\n\\int_{\\mathscr{Y}} f_{\\beta}(y) d y=\\int_{\\mathscr{Y}} f(y) d y+\\int_{\\mathscr{Y}} f(y) y d y X^{\\prime} \\beta / \\sigma^{2}=1\n\\]\nbecause \\(\\int_{\\mathscr{Y}} f(y) d y=1\\). Thus \\(f_{\\beta}\\) is a parametric family of density functions. Evaluated at \\(\\beta_{0}\\) we see that \\(f_{0}=f\\), which means that \\(f_{\\beta}\\) is a correctly-specified parametric family with true parameter value \\(\\beta_{0}=0\\).\nTo illustrate, take the case \\(X=1\\). Figure 4.1 displays an example density \\(f(y)=(3 / 4)\\left(1-y^{2}\\right)\\) on \\([-1,1]\\) with auxiliary density \\(f_{\\beta}(y)=f(y)(1+y)\\). We can see how the auxiliary density is a tilted version of the original density \\(f(y)\\).\n\nFigure 4.1: Original and Auxiliary Density\nLet \\(\\mathbb{E}_{\\beta}\\) denote expectation with respect to the auxiliary distribution. Since \\(\\int_{\\mathscr{Y}} y f(y) d y=0\\) and \\(\\int_{\\mathscr{Y}} y^{2} f(y) d y=\\) \\(\\sigma^{2}\\), we find\n\\[\n\\mathbb{E}_{\\beta}[Y]=\\int_{\\mathscr{Y}} y f_{\\beta}(y) d y=\\int_{\\mathscr{Y}} y f(y) d y+\\int_{\\mathscr{Y}} y^{2} f(y) d y X^{\\prime} \\beta / \\sigma^{2}=X^{\\prime} \\beta .\n\\]\nThis shows that \\(f_{\\beta}\\) is a regression model with regression coefficient \\(\\beta\\).\nIn Figure 4.1, the means of the two densities are indicated by the arrows to the \\(\\mathrm{x}\\)-axis. In this example we can see how the auxiliary density has a larger expected value, because the density has been tilted to the right.\nThe parametric family \\(f_{\\beta}\\) over \\(\\beta \\in B\\) has the following properties: its expectation is \\(X^{\\prime} \\beta\\), its variance is finite, the true value \\(\\beta_{0}\\) lies in the interior of \\(B\\), and the support of the distribution does not depend on \\(\\beta\\).\nThe likelihood score of the auxiliary density function for an observation, using the fact that \\(Y_{i}=e_{i}\\), is\n\\[\nS_{i}=\\left.\\frac{\\partial}{\\partial \\beta}\\left(\\log f_{\\beta}\\left(Y_{i}\\right)\\right)\\right|_{\\beta=0}=\\left.\\frac{\\partial}{\\partial \\beta}\\left(\\log f\\left(e_{i}\\right)+\\log \\left(1+e_{i} X_{i}^{\\prime} \\beta / \\sigma^{2}\\right)\\right)\\right|_{\\beta=0}=X_{i} e_{i} / \\sigma^{2} .\n\\]\nTherefore the information matrix is\n\\[\n\\mathscr{I}=\\sum_{i=1}^{n} \\mathbb{E}\\left[S_{i} S_{i}^{\\prime}\\right]=\\sum_{i=1}^{n} X_{i} X_{i}^{\\prime} \\mathbb{E}\\left[e_{i}^{2}\\right] / \\sigma^{4}=\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right) / \\sigma^{2} .\n\\]\nBy assumption, \\(\\widetilde{\\beta}\\) is unbiased. The Cramér-Rao lower bound states that\n\\[\n\\operatorname{var}[\\widetilde{\\beta}] \\geq \\mathscr{I}^{-1}=\\sigma^{2}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} .\n\\]\nThis is the variance lower bound, completing the proof of Theorem 4.4.\nThe above argument is rather tricky. At its core is the observation that the model \\(f_{\\beta}\\) is a submodel of the set of all linear regression models. The Cramér-Rao bound over any regular parametric submodel is a lower bound on the variance of any unbiased estimator. This means that the Cramér-Rao bound over \\(f_{\\beta}\\) is a lower bound for unbiased estimation of the regression coefficient. The model \\(f_{\\beta}\\) was selected judiciously so that its Cramér-Rao bound equals the variance of the least squares estimator, and this is sufficient to establish the bound."
  },
  {
    "objectID": "chpt04-lsr.html#generalized-least-squares",
    "href": "chpt04-lsr.html#generalized-least-squares",
    "title": "4  Least Squares Regression",
    "section": "4.9 Generalized Least Squares",
    "text": "4.9 Generalized Least Squares\nTake the linear regression model in matrix format\n\\[\n\\boldsymbol{Y}=\\boldsymbol{X} \\beta+\\boldsymbol{e} .\n\\]\nConsider a generalized situation where the observation errors are possibly correlated and/or heteroskedastic. Specifically, suppose that\n\\[\n\\begin{gathered}\n\\mathbb{E}[\\boldsymbol{e} \\mid \\boldsymbol{X}]=0 \\\\\n\\operatorname{var}[\\boldsymbol{e} \\mid \\boldsymbol{X}]=\\Sigma \\sigma^{2}\n\\end{gathered}\n\\]\nfor some \\(n \\times n\\) matrix \\(\\Sigma>0\\), possibly a function of \\(\\boldsymbol{X}\\), and some scalar \\(\\sigma^{2}\\). This includes the independent sampling framework where \\(\\Sigma\\) is diagonal but allows for non-diagonal covariance matrices as well. As a scaled covariance matrix, \\(\\Sigma\\) is necessarily symmetric and positive semi-definite.\nUnder these assumptions, by arguments similar to the previous sections we can calculate the expectation and variance of the OLS estimator:\n\\[\n\\begin{gathered}\n\\mathbb{E}[\\widehat{\\beta} \\mid \\boldsymbol{X}]=\\beta \\\\\n\\operatorname{var}[\\widehat{\\beta} \\mid \\boldsymbol{X}]=\\sigma^{2}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1}\\left(\\boldsymbol{X}^{\\prime} \\Sigma \\boldsymbol{X}\\right)\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1}\n\\end{gathered}\n\\]\n(see Exercise 4.5).\nAitken (1935) established a generalization of the Gauss-Markov Theorem. The following statement is due to B. E. Hansen (2021). Theorem 4.5 Take the linear regression model (4.17)-(4.19). If \\(\\widetilde{\\beta}\\) is an unbiased estimator of \\(\\beta\\) then\n\\[\n\\operatorname{var}[\\widetilde{\\beta} \\mid \\boldsymbol{X}] \\geq \\sigma^{2}\\left(\\boldsymbol{X}^{\\prime} \\Sigma^{-1} \\boldsymbol{X}\\right)^{-1}\n\\]\nWe defer the proof to Section 4.24. See also Exercise 4.6.\nTheorem \\(4.5\\) provides a lower bound on the covariance matrix of unbiased estimators. Theorem \\(4.4\\) was the special case \\(\\Sigma=\\boldsymbol{I}_{n}\\).\nWhen \\(\\Sigma\\) is known, Aitken (1935) constructed an estimator which achieves the lower bound in Theorem 4.5. Take the linear model (4.17) and pre-multiply by \\(\\Sigma^{-1 / 2}\\). This produces the equation \\(\\tilde{\\boldsymbol{Y}}=\\widetilde{\\boldsymbol{X}} \\beta+\\widetilde{\\boldsymbol{e}}\\) where \\(\\tilde{\\boldsymbol{Y}}=\\Sigma^{-1 / 2} \\boldsymbol{Y}, \\widetilde{\\boldsymbol{X}}=\\Sigma^{-1 / 2} \\boldsymbol{X}\\), and \\(\\widetilde{\\boldsymbol{e}}=\\Sigma^{-1 / 2} \\boldsymbol{e}\\). Consider OLS estimation of \\(\\beta\\) in this equation.\n\\[\n\\begin{aligned}\n\\widetilde{\\beta}_{\\text {gls }} &=\\left(\\widetilde{\\boldsymbol{X}}^{\\prime} \\widetilde{\\boldsymbol{X}}\\right)^{-1} \\widetilde{\\boldsymbol{X}}^{\\prime} \\widetilde{\\boldsymbol{Y}} \\\\\n&=\\left(\\left(\\Sigma^{-1 / 2} \\boldsymbol{X}\\right)^{\\prime}\\left(\\Sigma^{-1 / 2} \\boldsymbol{X}\\right)\\right)^{-1}\\left(\\Sigma^{-1 / 2} \\boldsymbol{X}\\right)^{\\prime}\\left(\\Sigma^{-1 / 2} \\boldsymbol{Y}\\right) \\\\\n&=\\left(\\boldsymbol{X}^{\\prime} \\Sigma^{-1} \\boldsymbol{X}\\right)^{-1} \\boldsymbol{X}^{\\prime} \\Sigma^{-1} \\boldsymbol{Y} .\n\\end{aligned}\n\\]\nThis is called the Generalized Least Squares (GLS) estimator of \\(\\beta\\).\nYou can calculate that\n\\[\n\\begin{gathered}\n\\mathbb{E}\\left[\\widetilde{\\beta}_{\\text {gls }} \\mid \\boldsymbol{X}\\right]=\\beta \\\\\n\\operatorname{var}\\left[\\widetilde{\\beta}_{\\text {gls }} \\mid \\boldsymbol{X}\\right]=\\sigma^{2}\\left(\\boldsymbol{X}^{\\prime} \\Sigma^{-1} \\boldsymbol{X}\\right)^{-1} .\n\\end{gathered}\n\\]\nThis shows that the GLS estimator is unbiased and has a covariance matrix which equals the lower bound from Theorem 4.5. This shows that the lower bound is sharp. GLS is thus efficient in the class of unbiased estimators.\nIn the linear regression model with independent observations and known conditional variances, so that \\(\\Sigma=\\boldsymbol{D}=\\operatorname{diag}\\left(\\sigma_{1}^{2}, \\ldots, \\sigma_{n}^{2}\\right)\\), the GLS estimator takes the form\n\\[\n\\begin{aligned}\n\\widetilde{\\beta}_{\\mathrm{gls}} &=\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{D}^{-1} \\boldsymbol{X}\\right)^{-1} \\boldsymbol{X}^{\\prime} \\boldsymbol{D}^{-1} \\boldsymbol{Y} \\\\\n&=\\left(\\sum_{i=1}^{n} \\sigma_{i}^{-2} X_{i} X_{i}^{\\prime}\\right)^{-1}\\left(\\sum_{i=1}^{n} \\sigma_{i}^{-2} X_{i} Y_{i}\\right) .\n\\end{aligned}\n\\]\nThe assumption \\(\\Sigma>0\\) in this case reduces to \\(\\sigma_{i}^{2}>0\\) for \\(i=1, \\ldots n\\).\nIn most settings the matrix \\(\\Sigma\\) is unknown so the GLS estimator is not feasible. However, the form of the GLS estimator motivates feasible versions, effectively by replacing \\(\\Sigma\\) with a suitable estimator."
  },
  {
    "objectID": "chpt04-lsr.html#residuals",
    "href": "chpt04-lsr.html#residuals",
    "title": "4  Least Squares Regression",
    "section": "4.10 Residuals",
    "text": "4.10 Residuals\nWhat are some properties of the residuals \\(\\widehat{e}_{i}=Y_{i}-X_{i}^{\\prime} \\widehat{\\beta}\\) and prediction errors \\(\\widetilde{e}_{i}=Y_{i}-X_{i}^{\\prime} \\widehat{\\beta}_{(-i)}\\) in the context of the linear regression model?\nRecall from (3.24) that we can write the residuals in vector notation as \\(\\widehat{\\boldsymbol{e}}=\\boldsymbol{M} \\boldsymbol{e}\\) where \\(\\boldsymbol{M}=\\boldsymbol{I}_{n}-\\) \\(\\boldsymbol{X}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\boldsymbol{X}^{\\prime}\\) is the orthogonal projection matrix. Using the properties of conditional expectation\n\\[\n\\mathbb{E}[\\widehat{\\boldsymbol{e}} \\mid \\boldsymbol{X}]=\\mathbb{E}[\\boldsymbol{M e} \\mid \\boldsymbol{X}]=\\boldsymbol{M} \\mathbb{E}[\\boldsymbol{e} \\mid \\boldsymbol{X}]=0\n\\]\nand\n\\[\n\\operatorname{var}[\\widehat{\\boldsymbol{e}} \\mid \\boldsymbol{X}]=\\operatorname{var}[\\boldsymbol{M} \\boldsymbol{e} \\mid \\boldsymbol{X}]=\\boldsymbol{M} \\operatorname{var}[\\boldsymbol{e} \\mid \\boldsymbol{X}] \\boldsymbol{M}=\\boldsymbol{M D} \\boldsymbol{M}\n\\]\nwhere \\(\\boldsymbol{D}\\) is defined in (4.8).\nWe can simplify this expression under the assumption of conditional homoskedasticity\n\\[\n\\mathbb{E}\\left[e^{2} \\mid X\\right]=\\sigma^{2} .\n\\]\nIn this case (4.25) simplifies to\n\\[\n\\operatorname{var}[\\widehat{\\boldsymbol{e}} \\mid \\boldsymbol{X}]=\\boldsymbol{M} \\sigma^{2} .\n\\]\nIn particular, for a single observation \\(i\\) we can find the variance of \\(\\widehat{e}_{i}\\) by taking the \\(i^{t h}\\) diagonal element of (4.26). Since the \\(i^{t h}\\) diagonal element of \\(M\\) is \\(1-h_{i i}\\) as defined in (3.40) we obtain\n\\[\n\\operatorname{var}\\left[\\widehat{e}_{i} \\mid \\boldsymbol{X}\\right]=\\mathbb{E}\\left[\\widehat{e}_{i}^{2} \\mid \\boldsymbol{X}\\right]=\\left(1-h_{i i}\\right) \\sigma^{2} .\n\\]\nAs this variance is a function of \\(h_{i i}\\) and hence \\(X_{i}\\) the residuals \\(\\widehat{e}_{i}\\) are heteroskedastic even if the errors \\(e_{i}\\) are homoskedastic. Notice as well that (4.27) implies \\(\\widehat{e}_{i}^{2}\\) is a biased estimator of \\(\\sigma^{2}\\).\nSimilarly, recall from (3.45) that the prediction errors \\(\\widetilde{e}_{i}=\\left(1-h_{i i}\\right)^{-1} \\widehat{e}_{i}\\) can be written in vector notation as \\(\\widetilde{\\boldsymbol{e}}=\\boldsymbol{M}^{*} \\widehat{\\boldsymbol{e}}\\) where \\(\\boldsymbol{M}^{*}\\) is a diagonal matrix with \\(i^{t h}\\) diagonal element \\(\\left(1-h_{i i}\\right)^{-1}\\). Thus \\(\\widetilde{\\boldsymbol{e}}=\\boldsymbol{M}^{*} \\boldsymbol{M} \\boldsymbol{e}\\). We can calculate that\n\\[\n\\mathbb{E}[\\tilde{\\boldsymbol{e}} \\mid \\boldsymbol{X}]=\\boldsymbol{M}^{*} \\boldsymbol{M} \\mathbb{E}[\\boldsymbol{e} \\mid \\boldsymbol{X}]=0\n\\]\nand\n\\[\n\\operatorname{var}[\\widetilde{\\boldsymbol{e}} \\mid \\boldsymbol{X}]=\\boldsymbol{M}^{*} \\boldsymbol{M} \\operatorname{var}[\\boldsymbol{e} \\mid \\boldsymbol{X}] \\boldsymbol{M} \\boldsymbol{M}^{*}=\\boldsymbol{M}^{*} \\boldsymbol{M D} \\boldsymbol{M} \\boldsymbol{M}^{*}\n\\]\nwhich simplifies under homoskedasticity to\n\\[\n\\operatorname{var}[\\widetilde{\\boldsymbol{e}} \\mid \\boldsymbol{X}]=\\boldsymbol{M}^{*} \\boldsymbol{M} \\boldsymbol{M} \\boldsymbol{M}^{*} \\sigma^{2}=\\boldsymbol{M}^{*} \\boldsymbol{M} \\boldsymbol{M}^{*} \\sigma^{2} .\n\\]\nThe variance of the \\(i^{t h}\\) prediction error is then\n\\[\n\\begin{aligned}\n\\operatorname{var}\\left[\\widetilde{e}_{i} \\mid \\boldsymbol{X}\\right] &=\\mathbb{E}\\left[\\widetilde{e}_{i}^{2} \\mid \\boldsymbol{X}\\right] \\\\\n&=\\left(1-h_{i i}\\right)^{-1}\\left(1-h_{i i}\\right)\\left(1-h_{i i}\\right)^{-1} \\sigma^{2} \\\\\n&=\\left(1-h_{i i}\\right)^{-1} \\sigma^{2} .\n\\end{aligned}\n\\]\nA residual with constant conditional variance can be obtained by rescaling. The standardized residuals are\n\\[\n\\bar{e}_{i}=\\left(1-h_{i i}\\right)^{-1 / 2} \\widehat{e}_{i},\n\\]\nand in vector notation\n\\[\n\\overline{\\boldsymbol{e}}=\\left(\\bar{e}_{1}, \\ldots, \\bar{e}_{n}\\right)^{\\prime}=\\boldsymbol{M}^{* 1 / 2} \\boldsymbol{M e} .\n\\]\nFrom the above calculations, under homoskedasticity,\n\\[\n\\operatorname{var}[\\overline{\\boldsymbol{e}} \\mid \\boldsymbol{X}]=\\boldsymbol{M}^{* 1 / 2} \\boldsymbol{M} \\boldsymbol{M}^{* 1 / 2} \\sigma^{2}\n\\]\nand\n\\[\n\\operatorname{var}\\left[\\bar{e}_{i} \\mid \\boldsymbol{X}\\right]=\\mathbb{E}\\left[\\bar{e}_{i}^{2} \\mid \\boldsymbol{X}\\right]=\\sigma^{2}\n\\]\nand thus these standardized residuals have the same bias and variance as the original errors when the latter are homoskedastic."
  },
  {
    "objectID": "chpt04-lsr.html#estimation-of-error-variance",
    "href": "chpt04-lsr.html#estimation-of-error-variance",
    "title": "4  Least Squares Regression",
    "section": "4.11 Estimation of Error Variance",
    "text": "4.11 Estimation of Error Variance\nThe error variance \\(\\sigma^{2}=\\mathbb{E}\\left[e^{2}\\right]\\) can be a parameter of interest even in a heteroskedastic regression or a projection model. \\(\\sigma^{2}\\) measures the variation in the “unexplained” part of the regression. Its method of moments estimator (MME) is the sample average of the squared residuals:\n\\[\n\\widehat{\\sigma}^{2}=\\frac{1}{n} \\sum_{i=1}^{n} \\widehat{e}_{i}^{2} .\n\\]\nIn the linear regression model we can calculate the expectation of \\(\\widehat{\\sigma}^{2}\\). From (3.28) and the properties of the trace operator observe that\n\\[\n\\widehat{\\sigma}^{2}=\\frac{1}{n} \\boldsymbol{e}^{\\prime} \\boldsymbol{M} \\boldsymbol{e}=\\frac{1}{n} \\operatorname{tr}\\left(\\boldsymbol{e}^{\\prime} \\boldsymbol{M} \\boldsymbol{e}\\right)=\\frac{1}{n} \\operatorname{tr}\\left(\\boldsymbol{M e}^{\\prime}\\right) .\n\\]\nThen\n\\[\n\\begin{aligned}\n\\mathbb{E}\\left[\\widehat{\\sigma}^{2} \\mid \\boldsymbol{X}\\right] &=\\frac{1}{n} \\operatorname{tr}\\left(\\mathbb{E}\\left[\\boldsymbol{M e e}^{\\prime} \\mid \\boldsymbol{X}\\right]\\right) \\\\\n&=\\frac{1}{n} \\operatorname{tr}\\left(\\boldsymbol{M}\\left[\\boldsymbol{e} \\boldsymbol{e}^{\\prime} \\mid \\boldsymbol{X}\\right]\\right) \\\\\n&=\\frac{1}{n} \\operatorname{tr}(\\boldsymbol{M D}) \\\\\n&=\\frac{1}{n} \\sum_{i=1}^{n}\\left(1-h_{i i}\\right) \\sigma_{i}^{2}\n\\end{aligned}\n\\]\nThe final equality holds because the trace is the sum of the diagonal elements of \\(\\boldsymbol{M D}\\), and because \\(\\boldsymbol{D}\\) is diagonal the diagonal elements of \\(M D\\) are the product of the diagonal elements of \\(M\\) and \\(\\boldsymbol{D}\\) which are \\(1-h_{i i}\\) and \\(\\sigma_{i}^{2}\\), respectively.\nAdding the assumption of conditional homoskedasticity \\(\\mathbb{E}\\left[e^{2} \\mid X\\right]=\\sigma^{2}\\) so that \\(\\boldsymbol{D}=\\boldsymbol{I}_{n} \\sigma^{2}\\), then (4.30) simplifies to\n\\[\n\\mathbb{E}\\left[\\widehat{\\sigma}^{2} \\mid \\boldsymbol{X}\\right]=\\frac{1}{n} \\operatorname{tr}\\left(\\boldsymbol{M} \\sigma^{2}\\right)=\\sigma^{2}\\left(\\frac{n-k}{n}\\right)\n\\]\nthe final equality by (3.22). This calculation shows that \\(\\widehat{\\sigma}^{2}\\) is biased towards zero. The order of the bias depends on \\(k / n\\), the ratio of the number of estimated coefficients to the sample size.\nAnother way to see this is to use (4.27). Note that\n\\[\n\\mathbb{E}\\left[\\widehat{\\sigma}^{2} \\mid \\boldsymbol{X}\\right]=\\frac{1}{n} \\sum_{i=1}^{n} \\mathbb{E}\\left[\\widehat{e}_{i}^{2} \\mid \\boldsymbol{X}\\right]=\\frac{1}{n} \\sum_{i=1}^{n}\\left(1-h_{i i}\\right) \\sigma^{2}=\\left(\\frac{n-k}{n}\\right) \\sigma^{2}\n\\]\nthe last equality using Theorem 3.6.\nSince the bias takes a scale form a classic method to obtain an unbiased estimator is by rescaling. Define\n\\[\ns^{2}=\\frac{1}{n-k} \\sum_{i=1}^{n} \\widehat{e}_{i}^{2} .\n\\]\nBy the above calculation \\(\\mathbb{E}\\left[s^{2} \\mid \\boldsymbol{X}\\right]=\\sigma^{2}\\) and \\(\\mathbb{E}\\left[s^{2}\\right]=\\sigma^{2}\\). Hence the estimator \\(s^{2}\\) is unbiased for \\(\\sigma^{2}\\). Consequently, \\(s^{2}\\) is known as the bias-corrected estimator for \\(\\sigma^{2}\\) and in empirical practice \\(s^{2}\\) is the most widely used estimator for \\(\\sigma^{2}\\). Interestingly, this is not the only method to construct an unbiased estimator for \\(\\sigma^{2}\\). An estimator constructed with the standardized residuals \\(\\bar{e}_{i}\\) from (4.28) is\n\\[\n\\bar{\\sigma}^{2}=\\frac{1}{n} \\sum_{i=1}^{n} \\bar{e}_{i}^{2}=\\frac{1}{n} \\sum_{i=1}^{n}\\left(1-h_{i i}\\right)^{-1} \\widehat{e}_{i}^{2} .\n\\]\nYou can show (see Exercise 4.9) that\n\\[\n\\mathbb{E}\\left[\\bar{\\sigma}^{2} \\mid \\boldsymbol{X}\\right]=\\sigma^{2}\n\\]\nand thus \\(\\bar{\\sigma}^{2}\\) is unbiased for \\(\\sigma^{2}\\) (in the homoskedastic linear regression model).\nWhen \\(k / n\\) is small the estimators \\(\\widehat{\\sigma}^{2}, s^{2}\\) and \\(\\bar{\\sigma}^{2}\\) are likely to be similar to one another. However, if \\(k / n\\) is large then \\(s^{2}\\) and \\(\\bar{\\sigma}^{2}\\) are generally preferred to \\(\\widehat{\\sigma}^{2}\\). Consequently it is best to use one of the biascorrected variance estimators in applications."
  },
  {
    "objectID": "chpt04-lsr.html#mean-square-forecast-error",
    "href": "chpt04-lsr.html#mean-square-forecast-error",
    "title": "4  Least Squares Regression",
    "section": "4.12 Mean-Square Forecast Error",
    "text": "4.12 Mean-Square Forecast Error\nOne use of an estimated regression is to predict out-of-sample. Consider an out-of-sample realization \\(\\left(Y_{n+1}, X_{n+1}\\right)\\) where \\(X_{n+1}\\) is observed but not \\(Y_{n+1}\\). Given the coefficient estimator \\(\\widehat{\\beta}\\) the standard point estimator of \\(\\mathbb{E}\\left[Y_{n+1} \\mid X_{n+1}\\right]=X_{n+1}^{\\prime} \\beta\\) is \\(\\widetilde{Y}_{n+1}=X_{n+1}^{\\prime} \\widehat{\\beta}\\). The forecast error is the difference between the actual value \\(Y_{n+1}\\) and the point forecast \\(\\widetilde{Y}_{n+1}\\). This is the forecast error \\(\\widetilde{e}_{n+1}=Y_{n+1}-\\widetilde{Y}_{n+1}\\). The meansquared forecast error (MSFE) is its expected squared value \\(\\operatorname{MSFE}_{n}=\\mathbb{E}\\left[\\widetilde{e}_{n+1}^{2}\\right]\\). In the linear regression model \\(\\widetilde{e}_{n+1}=e_{n+1}-X_{n+1}^{\\prime}(\\widehat{\\beta}-\\beta)\\) so\n\\[\n\\operatorname{MSFE}_{n}=\\mathbb{E}\\left[e_{n+1}^{2}\\right]-2 \\mathbb{E}\\left[e_{n+1} X_{n+1}^{\\prime}(\\widehat{\\beta}-\\beta)\\right]+\\mathbb{E}\\left[X_{n+1}^{\\prime}(\\widehat{\\beta}-\\beta)(\\widehat{\\beta}-\\beta)^{\\prime} X_{n+1}\\right] .\n\\]\nThe first term in (4.33) is \\(\\sigma^{2}\\). The second term in (4.33) is zero because \\(e_{n+1} X_{n+1}^{\\prime}\\) is independent of \\(\\widehat{\\beta}-\\beta\\) and both are mean zero. Using the properties of the trace operator the third term in (4.33) is\n\\[\n\\begin{aligned}\n&\\operatorname{tr}\\left(\\mathbb{E}\\left[X_{n+1} X_{n+1}^{\\prime}\\right] \\mathbb{E}\\left[(\\widehat{\\beta}-\\beta)(\\widehat{\\beta}-\\beta)^{\\prime}\\right]\\right) \\\\\n&=\\operatorname{tr}\\left(\\mathbb{E}\\left[X_{n+1} X_{n+1}^{\\prime}\\right] \\mathbb{E}\\left[\\mathbb{E}\\left[(\\widehat{\\beta}-\\beta)(\\widehat{\\beta}-\\beta)^{\\prime} \\mid \\boldsymbol{X}\\right]\\right]\\right) \\\\\n&=\\operatorname{tr}\\left(\\mathbb{E}\\left[X_{n+1} X_{n+1}^{\\prime}\\right] \\mathbb{E}\\left[\\boldsymbol{V}_{\\widehat{\\beta}}\\right]\\right) \\\\\n&=\\mathbb{E}\\left[\\operatorname{tr}\\left(\\left(X_{n+1} X_{n+1}^{\\prime}\\right) \\boldsymbol{V}_{\\widehat{\\beta}}\\right)\\right] \\\\\n&=\\mathbb{E}\\left[X_{n+1}^{\\prime} \\boldsymbol{V}_{\\widehat{\\beta}} X_{n+1}\\right]\n\\end{aligned}\n\\]\nwhere we use the fact that \\(X_{n+1}\\) is independent of \\(\\widehat{\\beta}\\), the definition \\(\\boldsymbol{V}_{\\widehat{\\beta}}=\\mathbb{E}\\left[(\\widehat{\\beta}-\\beta)(\\widehat{\\beta}-\\beta)^{\\prime} \\mid \\boldsymbol{X}\\right]\\), and the fact that \\(X_{n+1}\\) is independent of \\(\\boldsymbol{V}_{\\widehat{\\beta}}\\). Thus\n\\[\n\\operatorname{MSFE}_{n}=\\sigma^{2}+\\mathbb{E}\\left[X_{n+1}^{\\prime} \\boldsymbol{V}_{\\widehat{\\beta}} X_{n+1}\\right] .\n\\]\nUnder conditional homoskedasticity this simplifies to\n\\[\n\\operatorname{MSFE}_{n}=\\sigma^{2}\\left(1+\\mathbb{E}\\left[X_{n+1}^{\\prime}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} X_{n+1}\\right]\\right) .\n\\]\nA simple estimator for the MSFE is obtained by averaging the squared prediction errors (3.46)\n\\[\n\\widetilde{\\sigma}^{2}=\\frac{1}{n} \\sum_{i=1}^{n} \\widetilde{e}_{i}^{2}\n\\]\nwhere \\(\\widetilde{e}_{i}=Y_{i}-X_{i}^{\\prime} \\widehat{\\beta}_{(-i)}=\\widehat{e}_{i}\\left(1-h_{i i}\\right)^{-1}\\). Indeed, we can calculate that\n\\[\n\\begin{aligned}\n\\mathbb{E}\\left[\\widetilde{\\sigma}^{2}\\right] &=\\mathbb{E}\\left[\\widetilde{e}_{i}^{2}\\right] \\\\\n&=\\mathbb{E}\\left[\\left(e_{i}-X_{i}^{\\prime}\\left(\\widehat{\\beta}_{(-i)}-\\beta\\right)\\right)^{2}\\right] \\\\\n&=\\sigma^{2}+\\mathbb{E}\\left[X_{i}^{\\prime}\\left(\\widehat{\\beta}_{(-i)}-\\beta\\right)\\left(\\widehat{\\beta}_{(-i)}-\\beta\\right)^{\\prime} X_{i}\\right] .\n\\end{aligned}\n\\]\nBy a similar calculation as in (4.34) we find\n\\[\n\\mathbb{E}\\left[\\widetilde{\\sigma}^{2}\\right]=\\sigma^{2}+\\mathbb{E}\\left[X_{i}^{\\prime} \\boldsymbol{V}_{\\widehat{\\beta}_{(-i)}} X_{i}\\right]=\\operatorname{MSFE}_{n-1} .\n\\]\nThis is the MSFE based on a sample of size \\(n-1\\) rather than size \\(n\\). The difference arises because the in-sample prediction errors \\(\\widetilde{e}_{i}\\) for \\(i \\leq n\\) are calculated using an effective sample size of \\(n-1\\), while the out-of sample prediction error \\(\\widetilde{e}_{n+1}\\) is calculated from a sample with the full \\(n\\) observations. Unless \\(n\\) is very small we should expect \\(\\operatorname{MSFE}_{n-1}\\) (the MSFE based on \\(n-1\\) observations) to be close to \\(\\mathrm{MSFE}_{n}\\) (the MSFE based on \\(n\\) observations). Thus \\(\\widetilde{\\sigma}^{2}\\) is a reasonable estimator for MSFE \\(n\\).\nTheorem 4.6 MSFE In the linear regression model (Assumption 4.2) and i.i.d. sampling (Assumption 4.1)\n\\[\n\\operatorname{MSFE}_{n}=\\mathbb{E}\\left[\\widetilde{e}_{n+1}^{2}\\right]=\\sigma^{2}+\\mathbb{E}\\left[X_{n+1}^{\\prime} \\boldsymbol{V}_{\\widehat{\\beta}} X_{n+1}\\right]\n\\]\nwhere \\(\\boldsymbol{V}_{\\widehat{\\beta}}=\\operatorname{var}[\\widehat{\\beta} \\mid \\boldsymbol{X}]\\). Furthermore, \\(\\widetilde{\\sigma}^{2}\\) defined in (3.46) is an unbiased estimator of \\(\\operatorname{MSFE}_{n-1}\\), because \\(\\mathbb{E}\\left[\\widetilde{\\sigma}^{2}\\right]=\\operatorname{MSFE}_{n-1}\\)."
  },
  {
    "objectID": "chpt04-lsr.html#covariance-matrix-estimation-under-homoskedasticity",
    "href": "chpt04-lsr.html#covariance-matrix-estimation-under-homoskedasticity",
    "title": "4  Least Squares Regression",
    "section": "4.13 Covariance Matrix Estimation Under Homoskedasticity",
    "text": "4.13 Covariance Matrix Estimation Under Homoskedasticity\nFor inference we need an estimator of the covariance matrix \\(\\boldsymbol{V}_{\\widehat{\\beta}}\\) of the least squares estimator. In this section we consider the homoskedastic regression model (Assumption 4.3).\nUnder homoskedasticity the covariance matrix takes the simple form\n\\[\n\\boldsymbol{V}_{\\widehat{\\beta}}^{0}=\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\sigma^{2}\n\\]\nwhich is known up to the scale \\(\\sigma^{2}\\). In Section \\(4.11\\) we discussed three estimators of \\(\\sigma^{2}\\). The most commonly used choice is \\(s^{2}\\) leading to the classic covariance matrix estimator\n\\[\n\\widehat{\\boldsymbol{V}}_{\\widehat{\\beta}}^{0}=\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} s^{2} .\n\\]\nSince \\(s^{2}\\) is conditionally unbiased for \\(\\sigma^{2}\\) it is simple to calculate that \\(\\widehat{\\boldsymbol{V}}_{\\widehat{\\beta}}^{0}\\) is conditionally unbiased for \\(\\boldsymbol{V}_{\\widehat{\\beta}}\\) under the assumption of homoskedasticity:\n\\[\n\\mathbb{E}\\left[\\widehat{\\boldsymbol{V}}_{\\widehat{\\beta}}^{0} \\mid \\boldsymbol{X}\\right]=\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\mathbb{E}\\left[s^{2} \\mid \\boldsymbol{X}\\right]=\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\sigma^{2}=\\boldsymbol{V}_{\\widehat{\\beta}} .\n\\]\nThis was the dominant covariance matrix estimator in applied econometrics for many years and is still the default method in most regression packages. For example, Stata uses the covariance matrix estimator (4.35) by default in linear regression unless an alternative is specified. If the estimator (4.35) is used but the regression error is heteroskedastic it is possible for \\(\\widehat{\\boldsymbol{V}}_{\\widehat{\\beta}}^{0}\\) to be quite biased for the correct covariance matrix \\(\\boldsymbol{V}_{\\widehat{\\beta}}=\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{D} \\boldsymbol{X}\\right)\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1}\\). For example, suppose \\(k=1\\) and \\(\\sigma_{i}^{2}=X_{i}^{2}\\) with \\(\\mathbb{E}[X]=0\\). The ratio of the true variance of the least squares estimator to the expectation of the variance estimator is\n\\[\n\\frac{\\boldsymbol{V}_{\\widehat{\\beta}}}{\\mathbb{E}\\left[\\widehat{\\boldsymbol{V}}_{\\widehat{\\beta}}^{0} \\mid \\boldsymbol{X}\\right]}=\\frac{\\sum_{i=1}^{n} X_{i}^{4}}{\\sigma^{2} \\sum_{i=1}^{n} X_{i}^{2}} \\simeq \\frac{\\mathbb{E}\\left[X^{4}\\right]}{\\left(\\mathbb{E}\\left[X^{2}\\right]\\right)^{2}} \\stackrel{\\text { def }}{=} \\kappa\n\\]\n(Notice that we use the fact that \\(\\sigma_{i}^{2}=X_{i}^{2}\\) implies \\(\\sigma^{2}=\\mathbb{E}\\left[\\sigma_{i}^{2}\\right]=\\mathbb{E}\\left[X^{2}\\right]\\).) The constant \\(\\kappa\\) is the standardized fourth moment (or kurtosis) of the regressor \\(X\\) and can be any number greater than one. For example, if \\(X \\sim \\mathrm{N}\\left(0, \\sigma^{2}\\right)\\) then \\(\\kappa=3\\), so the true variance \\(\\boldsymbol{V}_{\\widehat{\\beta}}\\) is three times larger than the expected homoskedastic estimator \\(\\widehat{\\boldsymbol{V}}_{\\widehat{\\beta}}^{0}\\). But \\(\\kappa\\) can be much larger. Take, for example, the variable wage in the CPS data set. It satisfies \\(\\kappa=30\\) so that if the conditional variance equals \\(\\sigma_{i}^{2}=X_{i}^{2}\\) then the true variance \\(\\boldsymbol{V}_{\\widehat{\\beta}}\\) is 30 times larger than the expected homoskedastic estimator \\(\\widehat{\\boldsymbol{V}}_{\\widehat{\\beta}}^{0}\\). While this is an extreme case the point is that the classic covariance matrix estimator (4.35) may be quite biased when the homoskedasticity assumption fails."
  },
  {
    "objectID": "chpt04-lsr.html#covariance-matrix-estimation-under-heteroskedasticity",
    "href": "chpt04-lsr.html#covariance-matrix-estimation-under-heteroskedasticity",
    "title": "4  Least Squares Regression",
    "section": "4.14 Covariance Matrix Estimation Under Heteroskedasticity",
    "text": "4.14 Covariance Matrix Estimation Under Heteroskedasticity\nIn the previous section we showed that that the classic covariance matrix estimator can be highly biased if homoskedasticity fails. In this section we show how to construct covariance matrix estimators which do not require homoskedasticity.\nRecall that the general form for the covariance matrix is\n\\[\n\\boldsymbol{V}_{\\widehat{\\beta}}=\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{D} \\boldsymbol{X}\\right)\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} .\n\\]\nwith \\(\\boldsymbol{D}\\) defined in (4.8). This depends on the unknown matrix \\(\\boldsymbol{D}\\) which we can write as\n\\[\n\\boldsymbol{D}=\\operatorname{diag}\\left(\\sigma_{1}^{2}, \\ldots, \\sigma_{n}^{2}\\right)=\\mathbb{E}\\left[\\boldsymbol{e} \\boldsymbol{e}^{\\prime} \\mid \\boldsymbol{X}\\right]=\\mathbb{E}[\\widetilde{\\boldsymbol{D}} \\mid \\boldsymbol{X}]\n\\]\nwhere \\(\\widetilde{\\boldsymbol{D}}=\\operatorname{diag}\\left(e_{1}^{2}, \\ldots, e_{n}^{2}\\right)\\). Thus \\(\\widetilde{\\boldsymbol{D}}\\) is a conditionally unbiased estimator for \\(\\boldsymbol{D}\\). If the squared errors \\(e_{i}^{2}\\) were observable, we could construct an unbiased estimator for \\(\\boldsymbol{V}_{\\widehat{\\beta}}\\) as\n\\[\n\\begin{aligned}\n\\widehat{\\boldsymbol{V}}_{\\widehat{\\beta}}^{\\text {ideal }} &=\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1}\\left(\\boldsymbol{X}^{\\prime} \\widetilde{\\boldsymbol{D}} \\boldsymbol{X}\\right)\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\\\\n&=\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1}\\left(\\sum_{i=1}^{n} X_{i} X_{i}^{\\prime} e_{i}^{2}\\right)\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1}\n\\end{aligned}\n\\]\nIndeed,\n\\[\n\\begin{aligned}\n\\mathbb{E}\\left[\\widehat{\\boldsymbol{V}}_{\\widehat{\\beta}}^{\\text {ideal }} \\mid \\boldsymbol{X}\\right] &=\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1}\\left(\\sum_{i=1}^{n} X_{i} X_{i}^{\\prime} \\mathbb{E}\\left[e_{i}^{2} \\mid \\boldsymbol{X}\\right]\\right)\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\\\\n&=\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1}\\left(\\sum_{i=1}^{n} X_{i} X_{i}^{\\prime} \\sigma_{i}^{2}\\right)\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\\\\n&=\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{D} \\boldsymbol{X}\\right)\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1}=\\boldsymbol{V}_{\\widehat{\\beta}}\n\\end{aligned}\n\\]\nverifying that \\(\\widehat{\\boldsymbol{V}}_{\\widehat{\\beta}}^{\\text {ideal }}\\) is unbiased for \\(\\boldsymbol{V}_{\\widehat{\\beta}}\\). Since the errors \\(e_{i}^{2}\\) are unobserved \\(\\widehat{\\boldsymbol{V}}_{\\widehat{\\beta}}^{\\text {ideal }}\\) is not a feasible estimator. However, we can replace \\(e_{i}^{2}\\) with the squared residuals \\(\\widehat{e}_{i}^{2}\\). Making this substitution we obtain the estimator\n\\[\n\\widehat{\\boldsymbol{V}}_{\\widehat{\\beta}}^{\\mathrm{HC} 0}=\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1}\\left(\\sum_{i=1}^{n} X_{i} X_{i}^{\\prime} \\widehat{e}_{i}^{2}\\right)\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} .\n\\]\nThe label “HC” refers to “heteroskedasticity-consistent”. The label “HC0” refers to this being the baseline heteroskedasticity-consistent covariance matrix estimator.\nWe know, however, that \\(\\widehat{e}_{i}^{2}\\) is biased towards zero (recall equation (4.27)). To estimate the variance \\(\\sigma^{2}\\) the unbiased estimator \\(s^{2}\\) scales the moment estimator \\(\\widehat{\\sigma}^{2}\\) by \\(n /(n-k)\\). Making the same adjustment we obtain the estimator\n\\[\n\\widehat{\\boldsymbol{V}}_{\\widehat{\\beta}}^{\\mathrm{HC1}}=\\left(\\frac{n}{n-k}\\right)\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1}\\left(\\sum_{i=1}^{n} X_{i} X_{i}^{\\prime} \\widehat{e}_{i}^{2}\\right)\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} .\n\\]\nWhile the scaling by \\(n /(n-k)\\) is \\(a d h o c, \\mathrm{HCl}\\) is often recommended over the unscaled HC0 estimator.\nAlternatively, we could use the standardized residuals \\(\\bar{e}_{i}\\) or the prediction errors \\(\\widetilde{e}_{i}\\), yielding the “HC2” and “HC3” estimators\n\\[\n\\begin{aligned}\n\\widehat{\\boldsymbol{V}}_{\\widehat{\\beta}}^{\\mathrm{HC} 2} &=\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1}\\left(\\sum_{i=1}^{n} X_{i} X_{i}^{\\prime} \\bar{e}_{i}^{2}\\right)\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\\\\n&=\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1}\\left(\\sum_{i=1}^{n}\\left(1-h_{i i}\\right)^{-1} X_{i} X_{i}^{\\prime} \\widehat{e}_{i}^{2}\\right)\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1}\n\\end{aligned}\n\\]\nand\n\\[\n\\begin{aligned}\n\\widehat{\\boldsymbol{V}}_{\\widehat{\\beta}}^{\\mathrm{HC} 3} &=\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1}\\left(\\sum_{i=1}^{n} X_{i} X_{i}^{\\prime} \\tilde{e}_{i}^{2}\\right)\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\\\\n&=\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1}\\left(\\sum_{i=1}^{n}\\left(1-h_{i i}\\right)^{-2} X_{i} X_{i}^{\\prime} \\widehat{e}_{i}^{2}\\right)\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} .\n\\end{aligned}\n\\]\nThe four estimators \\(\\mathrm{HC}\\), \\(\\mathrm{HC1}\\), HC2, and HC3 are collectively called robust, heteroskedasticityconsistent, or heteroskedasticity-robust covariance matrix estimators. The HC0 estimator was first developed by Eicker (1963) and introduced to econometrics by White (1980) and is sometimes called the Eicker-White or White covariance matrix estimator. The degree-of-freedom adjustment in \\(\\mathrm{HCl}\\) was recommended by Hinkley (1977) and is the default robust covariance matrix estimator implemented in Stata. It is implement by the “, \\(r\\)” option. In current applied econometric practice this is the most popular covariance matrix estimator. The HC2 estimator was introduced by Horn, Horn and Duncan (1975) and is implemented using the vce (hc2) option in Stata. The HC3 estimator was derived by MacKinnon and White (1985) from the jackknife principle (see Section 10.3), and by Andrews (1991a) based on the principle of leave-one-out cross-validation, and is implemented using the vce(hc3) option in Stata.\nSince \\(\\left(1-h_{i i}\\right)^{-2}>\\left(1-h_{i i}\\right)^{-1}>1\\) it is straightforward to show that\n\\[\n\\widehat{\\boldsymbol{V}}_{\\widehat{\\beta}}^{\\mathrm{HC} 0}<\\widehat{\\boldsymbol{V}}_{\\widehat{\\beta}}^{\\mathrm{HC} 2}<\\widehat{\\boldsymbol{V}}_{\\widehat{\\beta}}^{\\mathrm{HC} 3} .\n\\]\n(See Exercise 4.10.) The inequality \\(\\boldsymbol{A}<\\boldsymbol{B}\\) when applied to matrices means that the matrix \\(\\boldsymbol{B}-\\boldsymbol{A}\\) is positive definite. In general, the bias of the covariance matrix estimators is complicated but simplify under the assumption of homoskedasticity (4.3). For example, using (4.27),\n\\[\n\\begin{aligned}\n\\mathbb{E}\\left[\\widehat{\\boldsymbol{V}}_{\\widehat{\\beta}}^{\\mathrm{HC} 0} \\mid \\boldsymbol{X}\\right] &=\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1}\\left(\\sum_{i=1}^{n} X_{i} X_{i}^{\\prime} \\mathbb{E}\\left[\\widehat{e}_{i}^{2} \\mid \\boldsymbol{X}\\right]\\right)\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\\\\n&=\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1}\\left(\\sum_{i=1}^{n} X_{i} X_{i}^{\\prime}\\left(1-h_{i i}\\right) \\sigma^{2}\\right)\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\\\\n&=\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\sigma^{2}-\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1}\\left(\\sum_{i=1}^{n} X_{i} X_{i}^{\\prime} h_{i i}\\right)\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\sigma^{2} \\\\\n&<\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\sigma^{2}=\\boldsymbol{V}_{\\widehat{\\beta}}\n\\end{aligned}\n\\]\nThis calculation shows that \\(\\widehat{\\boldsymbol{V}}_{\\widehat{\\beta}}^{\\mathrm{HC} 0}\\) is biased towards zero.\nBy a similar calculation (again under homoskedasticity) we can calculate that the HC2 estimator is unbiased\n\\[\n\\mathbb{E}\\left[\\widehat{\\boldsymbol{V}}_{\\widehat{\\beta}}^{\\mathrm{HC} 2} \\mid \\boldsymbol{X}\\right]=\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\sigma^{2} .\n\\]\n(See Exercise 4.11.)\nIt might seem rather odd to compare the bias of heteroskedasticity-robust estimators under the assumption of homoskedasticity but it does give us a baseline for comparison.\nAnother interesting calculation shows that in general (that is, without assuming homoskedasticity) the HC3 estimator is biased away from zero. Indeed, using the definition of the prediction errors (3.44)\n\\[\n\\widetilde{e}_{i}=Y_{i}-X_{i}^{\\prime} \\widehat{\\beta}_{(-i)}=e_{i}-X_{i}^{\\prime}\\left(\\widehat{\\beta}_{(-i)}-\\beta\\right)\n\\]\nso\n\\[\n\\widetilde{e}_{i}^{2}=e_{i}^{2}-2 X_{i}^{\\prime}\\left(\\widehat{\\beta}_{(-i)}-\\beta\\right) e_{i}+\\left(X_{i}^{\\prime}\\left(\\widehat{\\beta}_{(-i)}-\\beta\\right)\\right)^{2} .\n\\]\nNote that \\(e_{i}\\) and \\(\\widehat{\\beta}_{(-i)}\\) are functions of non-overlapping observations and are thus independent. Hence \\(\\mathbb{E}\\left[\\left(\\widehat{\\beta}_{(-i)}-\\beta\\right) e_{i} \\mid \\boldsymbol{X}\\right]=0\\) and\n\\[\n\\begin{aligned}\n\\mathbb{E}\\left[\\widetilde{e}_{i}^{2} \\mid \\boldsymbol{X}\\right] &=\\mathbb{E}\\left[e_{i}^{2} \\mid \\boldsymbol{X}\\right]-2 X_{i}^{\\prime} \\mathbb{E}\\left[\\left(\\widehat{\\beta}_{(-i)}-\\beta\\right) e_{i} \\mid \\boldsymbol{X}\\right]+\\mathbb{E}\\left[\\left(X_{i}^{\\prime}\\left(\\widehat{\\beta}_{(-i)}-\\beta\\right)\\right)^{2} \\mid \\boldsymbol{X}\\right] \\\\\n&=\\sigma_{i}^{2}+\\mathbb{E}\\left[\\left(X_{i}^{\\prime}\\left(\\widehat{\\beta}_{(-i)}-\\beta\\right)\\right)^{2} \\mid \\boldsymbol{X}\\right] \\\\\n& \\geq \\sigma_{i}^{2} .\n\\end{aligned}\n\\]\nIt follows that\n\\[\n\\begin{aligned}\n\\mathbb{E}\\left[\\widehat{\\boldsymbol{V}}_{\\widehat{\\beta}}^{\\mathrm{HC} 3} \\mid \\boldsymbol{X}\\right] &=\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1}\\left(\\sum_{i=1}^{n} X_{i} X_{i}^{\\prime} \\mathbb{E}\\left[\\tilde{e}_{i}^{2} \\mid \\boldsymbol{X}\\right]\\right)\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\\\\n& \\geq\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1}\\left(\\sum_{i=1}^{n} X_{i} X_{i}^{\\prime} \\sigma_{i}^{2}\\right)\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1}=\\boldsymbol{V}_{\\widehat{\\beta}}\n\\end{aligned}\n\\]\nThis means that the HC3 estimator is conservative in the sense that it is weakly larger (in expectation) than the correct variance for any realization of \\(\\boldsymbol{X}\\).\nWe have introduced five covariance matrix estimators, including the homoskedastic estimator \\(\\widehat{\\boldsymbol{V}}_{\\widehat{\\beta}}^{0}\\) and the four HC estimators. Which should you use? The classic estimator \\(\\widehat{\\boldsymbol{V}}_{\\widehat{\\beta}}^{0}\\) is typically a poor choice as it is only valid under the unlikely homoskedasticity restriction. For this reason it is not typically used in contemporary econometric research. Unfortunately, standard regression packages set their default choice as \\(\\widehat{\\boldsymbol{V}}_{\\widehat{\\beta}}^{0}\\) so users must intentionally select a robust covariance matrix estimator.\nOf the four robust estimators \\(\\mathrm{HCl}\\) is the most commonly used as it is the default robust covariance matrix option in Stata. However, HC2 and HC3 are preferred. HC2 is unbiased (under homoskedasticity) and HC3 is conservative for any \\(\\boldsymbol{X}\\). In most applications \\(\\mathrm{HC} 1, \\mathrm{HC} 2\\), and \\(\\mathrm{HC} 3\\) will be similar so this choice will not matter. The context where the estimators can differ substantially is when the sample has a large leverage value \\(h_{i i}\\) for at least one observation. You can see this by comparing the formulas (4.37), (4.38), and (4.39) and noting that the only difference is the scaling by the leverage values \\(h_{i i}\\). If there is an observation with \\(h_{i i}\\) close to one, then \\(\\left(1-h_{i i}\\right)^{-1}\\) and \\(\\left(1-h_{i i}\\right)^{-2}\\) will be large, giving this observation much greater weight in the covariance matrix formula."
  },
  {
    "objectID": "chpt04-lsr.html#standard-errors",
    "href": "chpt04-lsr.html#standard-errors",
    "title": "4  Least Squares Regression",
    "section": "4.15 Standard Errors",
    "text": "4.15 Standard Errors\nA variance estimator such as \\(\\widehat{\\boldsymbol{V}}_{\\widehat{\\beta}}\\) is an estimator of the variance of the distribution of \\(\\widehat{\\beta}\\). A more easily interpretable measure of spread is its square root - the standard deviation. This is so important when discussing the distribution of parameter estimators we have a special name for estimates of their standard deviation.\nDefinition \\(4.2\\) A standard error \\(s(\\widehat{\\beta})\\) for a real-valued estimator \\(\\widehat{\\beta}\\) is an estimator of the standard deviation of the distribution of \\(\\widehat{\\beta}\\).\nWhen \\(\\beta\\) is a vector with estimator \\(\\widehat{\\beta}\\) and covariance matrix estimator \\(\\widehat{\\boldsymbol{V}}_{\\widehat{\\beta}}\\), standard errors for individual elements are the square roots of the diagonal elements of \\(\\widehat{\\boldsymbol{V}}_{\\widehat{\\beta}}\\). That is,\n\\[\ns\\left(\\widehat{\\beta}_{j}\\right)=\\sqrt{\\widehat{\\boldsymbol{V}}_{\\widehat{\\beta}_{j}}}=\\sqrt{\\left[\\widehat{\\boldsymbol{V}}_{\\widehat{\\beta}}\\right]_{j j}}\n\\]\nWhen the classical covariance matrix estimator (4.35) is used the standard error takes the simple form\n\\[\ns\\left(\\widehat{\\beta}_{j}\\right)=s \\sqrt{\\left[\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1}\\right]_{j j}} .\n\\]\nAs we discussed in the previous section there are multiple possible covariance matrix estimators so standard errors are not unique. It is therefore important to understand what formula and method is used by an author when studying their work. It is also important to understand that a particular standard error may be relevant under one set of model assumptions but not under another set of assumptions.\nTo illustrate, we return to the log wage regression (3.12) of Section 3.7. We calculate that \\(s^{2}=0.160\\). Therefore the homoskedastic covariance matrix estimate is\n\\[\n\\widehat{\\boldsymbol{V}}_{\\widehat{\\beta}}^{0}=\\left(\\begin{array}{cc}\n5010 & 314 \\\\\n314 & 20\n\\end{array}\\right)^{-1} 0.160=\\left(\\begin{array}{cc}\n0.002 & -0.031 \\\\\n-0.031 & 0.499\n\\end{array}\\right)\n\\]\nWe also calculate that\n\\[\n\\sum_{i=1}^{n}\\left(1-h_{i i}\\right)^{-1} X_{i} X_{i}^{\\prime} \\widehat{e}_{i}^{2}=\\left(\\begin{array}{cc}\n763.26 & 48.513 \\\\\n48.513 & 3.1078\n\\end{array}\\right) .\n\\]\nTherefore the HC2 covariance matrix estimate is\n\\[\n\\begin{aligned}\n\\widehat{\\boldsymbol{V}}_{\\widehat{\\beta}}^{\\mathrm{HC} 2} &=\\left(\\begin{array}{cc}\n5010 & 314 \\\\\n314 & 20\n\\end{array}\\right)^{-1}\\left(\\begin{array}{cc}\n763.26 & 48.513 \\\\\n48.513 & 3.1078\n\\end{array}\\right)\\left(\\begin{array}{cc}\n5010 & 314 \\\\\n314 & 20\n\\end{array}\\right)^{-1} \\\\\n&=\\left(\\begin{array}{cc}\n0.001 & -0.015 \\\\\n-0.015 & 0.243\n\\end{array}\\right) .\n\\end{aligned}\n\\]\nThe standard errors are the square roots of the diagonal elements of these matrices. A conventional format to write the estimated equation with standard errors is\n\nAlternatively, standard errors could be calculated using the other formulae. We report the different standard errors in the following table.\nTable 4.1: Standard Errors\n\n\n\n\nEducation\nIntercept\n\n\n\n\nHomoskedastic (4.35)\n\\(0.045\\)\n\\(0.707\\)\n\n\nHC0 (4.36)\n\\(0.029\\)\n\\(0.461\\)\n\n\nHC1 \\((4.37)\\)\n\\(0.030\\)\n\\(0.486\\)\n\n\nHC2 \\((4.38)\\)\n\\(0.031\\)\n\\(0.493\\)\n\n\nHC3 \\((4.39)\\)\n\\(0.033\\)\n\\(0.527\\)\n\n\n\nThe homoskedastic standard errors are noticeably different (larger in this case) than the others. The robust standard errors are reasonably close to one another though the HC3 standard errors are larger than the others."
  },
  {
    "objectID": "chpt04-lsr.html#estimation-with-sparse-dummy-variables",
    "href": "chpt04-lsr.html#estimation-with-sparse-dummy-variables",
    "title": "4  Least Squares Regression",
    "section": "4.16 Estimation with Sparse Dummy Variables",
    "text": "4.16 Estimation with Sparse Dummy Variables\nThe heteroskedasticity-robust covariance matrix estimators can be quite imprecise in some contexts. One is in the presence of sparse dummy variables - when a dummy variable only takes the value 1 or 0 for very few observations. In these contexts one component of the covariance matrix is estimated on just those few observations and will be imprecise. This is effectively hidden from the user. To see the problem, let \\(D\\) be a dummy variable (takes on the values 1 and 0 ) and consider the dummy variable regression\n\\[\nY=\\beta_{1} D+\\beta_{2}+e .\n\\]\nThe number of observations for which \\(D_{i}=1\\) is \\(n_{1}=\\sum_{i=1}^{n} D_{i}\\). The number of observations for which \\(D_{i}=0\\) is \\(n_{2}=n-n_{1}\\). We say the design is sparse if \\(n_{1}\\) or \\(n_{2}\\) is small.\nTo simplify our analysis, we take the extreme case \\(n_{1}=1\\). The ideas extend to the case of \\(n_{1}>1\\) but small, though with less dramatic effects.\nIn the regression model (4.45) we can calculate that the true covariance matrix of the least squares estimator for the coefficients under the simplifying assumption of conditional homoskedasticity is\n\\[\n\\boldsymbol{V}_{\\widehat{\\beta}}=\\sigma^{2}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1}=\\sigma^{2}\\left(\\begin{array}{ll}\n1 & 1 \\\\\n1 & n\n\\end{array}\\right)^{-1}=\\frac{\\sigma^{2}}{n-1}\\left(\\begin{array}{cc}\nn & -1 \\\\\n-1 & 1\n\\end{array}\\right)\n\\]\nIn particular, the variance of the estimator for the coefficient on the dummy variable is\n\\[\nV_{\\widehat{\\beta}_{1}}=\\sigma^{2} \\frac{n}{n-1} .\n\\]\nEssentially, the coefficient \\(\\beta_{1}\\) is estimated from a single observation so its variance is roughly unaffected by sample size. An important message is that certain coefficient estimators in the presence of sparse dummy variables will be imprecise, regardless of the sample size. A large sample alone is not sufficient to ensure precise estimation.\nNow let’s examine the standard HC1 covariance matrix estimator (4.37). The regression has perfect fit for the observation for which \\(D_{i}=1\\) so the corresponding residual is \\(\\widehat{e}_{i}=0\\). It follows that \\(D_{i} \\widehat{e}_{i}=0\\) for all \\(i\\) (either \\(D_{i}=0\\) or \\(\\widehat{e}_{i}=0\\) ). Hence\n\\[\n\\sum_{i=1}^{n} X_{i} X_{i}^{\\prime} \\hat{e}_{i}^{2}=\\left(\\begin{array}{cc}\n0 & 0 \\\\\n0 & \\sum_{i=1}^{n} \\widehat{e}_{i}^{2}\n\\end{array}\\right)=\\left(\\begin{array}{cc}\n0 & 0 \\\\\n0 & (n-2) s^{2}\n\\end{array}\\right)\n\\]\nwhere \\(s^{2}=(n-2)^{-1} \\sum_{i=1}^{n} \\widehat{e}_{i}^{2}\\) is the bias-corrected estimator of \\(\\sigma^{2}\\). Together we find that\n\\[\n\\begin{aligned}\n\\widehat{\\boldsymbol{V}}_{\\widehat{\\beta}}^{\\mathrm{HC1}} &=\\left(\\frac{n}{n-2}\\right) \\frac{1}{(n-1)^{2}}\\left(\\begin{array}{cc}\nn & -1 \\\\\n-1 & 1\n\\end{array}\\right)\\left(\\begin{array}{cc}\n0 & 0 \\\\\n0 & (n-2) s^{2}\n\\end{array}\\right)\\left(\\begin{array}{cc}\nn & -1 \\\\\n-1 & 1\n\\end{array}\\right) \\\\\n&=s^{2} \\frac{n}{(n-1)^{2}}\\left(\\begin{array}{cc}\n1 & -1 \\\\\n-1 & 1\n\\end{array}\\right) .\n\\end{aligned}\n\\]\nIn particular, the estimator for \\(V_{\\widehat{\\beta}_{1}}\\) is\n\\[\n\\widehat{V}_{\\widehat{\\beta}_{1}}^{\\mathrm{HC} 1}=s^{2} \\frac{n}{(n-1)^{2}}\n\\]\nIt has expectation\n\\[\n\\mathbb{E}\\left[\\widehat{V}_{\\widehat{\\beta}_{1}}^{\\mathrm{HC1}}\\right]=\\sigma^{2} \\frac{n}{(n-1)^{2}}=\\frac{V_{\\widehat{\\beta}_{1}}}{n-1}<<V_{\\widehat{\\beta}_{1}} .\n\\]\nThe variance estimator \\(\\widehat{V}_{\\widehat{\\beta}_{1}}^{\\mathrm{HCl}}\\) is extremely biased for \\(V_{\\widehat{\\beta}_{1}}\\). It is too small by a multiple of \\(n\\) ! The reported variance - and standard error - is misleadingly small. The variance estimate erroneously mis-states the precision of \\(\\widehat{\\beta}_{1}\\).\nThe fact that \\(\\widehat{V}_{\\widehat{\\beta}_{1}}^{\\mathrm{HCl}}\\) is biased is unlikely to be noticed by an applied researcher. Nothing in the reported output will alert a researcher to the problem. Another way to see the issue is to consider the estimator \\(\\widehat{\\theta}=\\widehat{\\beta}_{1}+\\widehat{\\beta}_{2}\\) for the sum of the coefficients \\(\\theta=\\beta_{1}+\\beta_{2}\\). This estimator has true variance \\(\\sigma^{2}\\). The variance estimator, however is \\(\\widehat{\\boldsymbol{V}}_{\\widehat{\\theta}}^{\\mathrm{HC1}}=0\\) ! (It equals the sum of the four elements in \\(\\widehat{\\boldsymbol{V}}_{\\widehat{\\beta}}^{\\mathrm{HC1}}\\) ). Clearly, the estimator ” 0 ” is biased for the true value \\(\\sigma^{2}\\).\nAnother insight is to examine the leverage values. The (single) observation with \\(D_{i}=1\\) has\n\\[\nh_{i i}=\\frac{1}{n-1}\\left(\\begin{array}{ll}\n1 & 1\n\\end{array}\\right)\\left(\\begin{array}{cc}\nn & -1 \\\\\n-1 & 1\n\\end{array}\\right)\\left(\\begin{array}{l}\n1 \\\\\n1\n\\end{array}\\right)=1 .\n\\]\nThis is an extreme leverage value.\nA possible solution is to replace the biased covariance matrix estimator \\(\\widehat{V}_{\\widehat{\\beta}_{1}}^{\\mathrm{HC1}}\\) with the unbiased estimator \\(\\widehat{V}_{\\widehat{\\beta}_{1}}^{\\mathrm{HC} 2}\\) (unbiased under homoskedasticity) or the conservative estimator \\(\\widehat{V}_{\\widehat{\\beta}_{1}}^{\\mathrm{HC}} .\\) Neither approach can be done in the extreme sparse case \\(n_{1}=1\\) (for \\(\\widehat{V}_{\\widehat{\\beta}_{1}}^{\\mathrm{HC} 2}\\) and \\(\\widehat{V}_{\\widehat{\\beta}_{1}}^{\\mathrm{HC}}\\) cannot be calculated if \\(h_{i i}=1\\) for any observation) but applies otherwise. When \\(h_{i i}=1\\) for an observation then \\(\\widehat{V}_{\\widehat{\\beta}_{1}}^{\\mathrm{HC} 2}\\) and \\(\\widehat{V}_{\\widehat{\\beta}_{1}}^{\\mathrm{HC} 3}\\) cannot be calculated. In this case unbiased covariance matrix estimation appears to be impossible.\nIt is unclear if there is a best practice to avoid this situation. Once possibility is to calculate the maximum leverage value. If it is very large calculate the standard errors using several methods to see if variation occurs."
  },
  {
    "objectID": "chpt04-lsr.html#computation",
    "href": "chpt04-lsr.html#computation",
    "title": "4  Least Squares Regression",
    "section": "4.17 Computation",
    "text": "4.17 Computation\nWe illustrate methods to compute standard errors for equation (3.13) extending the code of Section \\(3.25 .\\)\nStata do File (continued)\n\nHomoskedastic formula (4.35):\n\nreg wage education experience exp2 if \\((\\mathrm{mnwf}==1)\\)\n\n\\(\\quad\\) HC1 formula (4.37):\n\nreg wage education experience exp2 if \\((\\operatorname{mnwf}==1), \\mathrm{r}\\)\n\n\\(\\mathrm{HC} 2\\) formula (4.38):\n\nreg wage education experience \\(\\exp 2\\) if \\((\\mathrm{mnwf}==1)\\), vce \\((\\mathrm{hc} 2)\\)\n\n\\(\\quad\\) HC3 formula (4.39):\n\nreg wage education experience exp2 if (mnwf \\(==1)\\), vce \\((\\mathrm{hc} 3)\\)"
  },
  {
    "objectID": "chpt04-lsr.html#measures-of-fit",
    "href": "chpt04-lsr.html#measures-of-fit",
    "title": "4  Least Squares Regression",
    "section": "4.18 Measures of Fit",
    "text": "4.18 Measures of Fit\nAs we described in the previous chapter a commonly reported measure of regression fit is the regression \\(R^{2}\\) defined as\n\\[\nR^{2}=1-\\frac{\\sum_{i=1}^{n} \\widehat{e}_{i}^{2}}{\\sum_{i=1}^{n}\\left(Y_{i}-\\bar{Y}\\right)^{2}}=1-\\frac{\\widehat{\\sigma}^{2}}{\\widehat{\\sigma}_{Y}^{2}} .\n\\]\nwhere \\(\\widehat{\\sigma}_{Y}^{2}=n^{-1} \\sum_{i=1}^{n}\\left(Y_{i}-\\bar{Y}\\right)^{2} \\cdot R^{2}\\) is an estimator of the population parameter\n\\[\n\\rho^{2}=\\frac{\\operatorname{var}\\left[X^{\\prime} \\beta\\right]}{\\operatorname{var}[Y]}=1-\\frac{\\sigma^{2}}{\\sigma_{Y}^{2}} .\n\\]\nHowever, \\(\\widehat{\\sigma}^{2}\\) and \\(\\widehat{\\sigma}_{Y}^{2}\\) are biased. Theil (1961) proposed replacing these by the unbiased versions \\(s^{2}\\) and \\(\\widetilde{\\sigma}_{Y}^{2}=(n-1)^{-1} \\sum_{i=1}^{n}\\left(Y_{i}-\\bar{Y}\\right)^{2}\\) yielding what is known as R-bar-squared or adjusted R-squared:\n\\[\n\\bar{R}^{2}=1-\\frac{s^{2}}{\\widetilde{\\sigma}_{Y}^{2}}=1-\\frac{(n-1)^{-1} \\sum_{i=1}^{n} \\widehat{e}_{i}^{2}}{(n-k)^{-1} \\sum_{i=1}^{n}\\left(Y_{i}-\\bar{Y}\\right)^{2}} .\n\\]\nWhile \\(\\bar{R}^{2}\\) is an improvement on \\(R^{2}\\) a much better improvement is\n\\[\n\\widetilde{R}^{2}=1-\\frac{\\sum_{i=1}^{n} \\widetilde{e}_{i}^{2}}{\\sum_{i=1}^{n}\\left(Y_{i}-\\bar{Y}\\right)^{2}}=1-\\frac{\\widetilde{\\sigma}^{2}}{\\widehat{\\sigma}_{Y}^{2}}\n\\]\nwhere \\(\\widetilde{e}_{i}\\) are the prediction errors (3.44) and \\(\\widetilde{\\sigma}^{2}\\) is the MSPE from (3.46). As described in Section (4.12) \\(\\widetilde{\\sigma}^{2}\\) is a good estimator of the out-of-sample mean-squared forecast error so \\(\\widetilde{R}^{2}\\) is a good estimator of the percentage of the forecast variance which is explained by the regression forecast. In this sense \\(\\widetilde{R}^{2}\\) is a good measure of fit.\nOne problem with \\(R^{2}\\) which is partially corrected by \\(\\bar{R}^{2}\\) and fully corrected by \\(\\widetilde{R}^{2}\\) is that \\(R^{2}\\) necessarily increases when regressors are added to a regression model. This occurs because \\(R^{2}\\) is a negative function of the sum of squared residuals which cannot increase when a regressor is added. In contrast, \\(\\bar{R}^{2}\\) and \\(\\widetilde{R}^{2}\\) are non-monotonic in the number of regressors. \\(\\widetilde{R}^{2}\\) can even be negative, which occurs when an estimated model predicts worse than a constant-only model.\nIn the statistical literature the MSPE \\(\\widetilde{\\sigma}^{2}\\) is known as the leave-one-out cross validation criterion and is popular for model comparison and selection, especially in high-dimensional and nonparametric contexts. It is equivalent to use \\(\\widetilde{R}^{2}\\) or \\(\\widetilde{\\sigma}^{2}\\) to compare and select models. Models with high \\(\\widetilde{R}^{2}\\) (or low \\(\\widetilde{\\sigma}^{2}\\) ) are better models in terms of expected out of sample squared error. In contrast, \\(R^{2}\\) cannot be used for model selection as it necessarily increases when regressors are added to a regression model. \\(\\bar{R}^{2}\\) is also an inappropriate choice for model selection (it tends to select models with too many parameters) though a justification of this assertion requires a study of the theory of model selection. Unfortunately, \\(\\bar{R}^{2}\\) is routinely used by some economists, possibly as a hold-over from previous generations.\nIn summary, it is recommended to omit \\(R^{2}\\) and \\(\\bar{R}^{2}\\). If a measure of fit is desired, report \\(\\widetilde{R}^{2}\\) or \\(\\widetilde{\\sigma}^{2}\\)."
  },
  {
    "objectID": "chpt04-lsr.html#empirical-example",
    "href": "chpt04-lsr.html#empirical-example",
    "title": "4  Least Squares Regression",
    "section": "4.19 Empirical Example",
    "text": "4.19 Empirical Example\nWe again return to our wage equation but use a much larger sample of all individuals with at least 12 years of education. For regressors we include years of education, potential work experience, experience squared, and dummy variable indicators for the following: female, female union member, male union member, married female \\({ }^{1}\\), married male, formerly married female \\({ }^{2}\\), formerly married male, Hispanic, Black, American Indian, Asian, and mixed race \\({ }^{3}\\). The available sample is 46,943 so the parameter estimates are quite precise and reported in Table 4.2. For standard errors we use the unbiased HC2 formula.\nTable \\(4.2\\) displays the parameter estimates in a standard tabular format. Parameter estimates and standard errors are reported for all coefficients. In addition to the coefficient estimates the table also reports the estimated error standard deviation and the sample size. These are useful summary measures of fit which aid readers.\nTable 4.2: OLS Estimates of Linear Equation for \\(\\log (\\) wage \\()\\)\n\n\n\n\n\\(\\widehat{\\beta}\\)\n\\(s(\\widehat{\\beta})\\)\n\n\n\n\nEducation\n\\(0.117\\)\n\\(0.001\\)\n\n\nExperience\n\\(0.033\\)\n\\(0.001\\)\n\n\nExperience \\(^{2} / 100\\)\n\\(-0.056\\)\n\\(0.002\\)\n\n\nFemale\n\\(-0.098\\)\n\\(0.011\\)\n\n\nFemale Union Member\n\\(0.023\\)\n\\(0.020\\)\n\n\nMale Union Member\n\\(0.095\\)\n\\(0.020\\)\n\n\nMarried Female\n\\(0.016\\)\n\\(0.010\\)\n\n\nMarried Male\n\\(0.211\\)\n\\(0.010\\)\n\n\nFormerly Married Female\n\\(-0.006\\)\n\\(0.012\\)\n\n\nFormerly Married Male\n\\(0.083\\)\n\\(0.015\\)\n\n\nHispanic\n\\(-0.108\\)\n\\(0.008\\)\n\n\nBlack\n\\(-0.096\\)\n\\(0.008\\)\n\n\nAmerican Indian\n\\(-0.137\\)\n\\(0.027\\)\n\n\nAsian\n\\(-0.038\\)\n\\(0.013\\)\n\n\nMixed Race\n\\(-0.041\\)\n\\(0.021\\)\n\n\nIntercept\n\\(0.909\\)\n\\(0.021\\)\n\n\n\\(\\widehat{\\sigma}\\)\n\\(0.565\\)\n\n\n\nSample Size\n46,943\n\n\n\n\nStandard errors are heteroskedasticity-consistent (Horn-Horn-Duncan formula).\nAs a general rule it is advisable to always report standard errors along with parameter estimates. This allows readers to assess the precision of the parameter estimates, and as we will discuss in later chapters, form confidence intervals and t-tests for individual coefficients if desired.\nThe results in Table \\(4.2\\) confirm our earlier findings that the return to a year of education is approximately \\(12 %\\), the return to experience is concave, single women earn approximately \\(10 %\\) less then single men, and Blacks earn about \\(10 %\\) less than whites. In addition, we see that Hispanics earn about \\(11 %\\) less than whites, American Indians \\(14 %\\) less, and Asians and Mixed races about \\(4 %\\) less. We also see there\n\\({ }^{1}\\) Defining “married” as marital code 1,2 , or \\(3 .\\)\n\\({ }^{2}\\) Defining “formerly married” as marital code 4,5 , or 6 .\n\\({ }^{3}\\) Race code 6 or higher. are wage premiums for men who are members of a labor union (about \\(10 %\\) ), married (about 21%) or formerly married (about \\(8 %\\) ), but no similar premiums are apparent for women."
  },
  {
    "objectID": "chpt04-lsr.html#multicollinearity",
    "href": "chpt04-lsr.html#multicollinearity",
    "title": "4  Least Squares Regression",
    "section": "4.20 Multicollinearity",
    "text": "4.20 Multicollinearity\nAs discussed in Section 3.24, if \\(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\) is singular then \\(\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1}\\) and \\(\\widehat{\\beta}\\) are not defined. This situation is called strict multicollinearity as the columns of \\(\\boldsymbol{X}\\) are linearly dependent, i.e., there is some \\(\\alpha \\neq 0\\) such that \\(\\boldsymbol{X} \\alpha=0\\). Most commonly this arises when sets of regressors are included which are identically related. In Section \\(3.24\\) we discussed possible causes of strict multicollinearity and discussed the related problem of ill-conditioning which can cause numerical inaccuracies in severe cases.\nA related common situation is near multicollinearity which is often called “multicollinearity” for brevity. This is the situation when the regressors are highly correlated. An implication of near multicollinearity is that individual coefficient estimates will be imprecise. This is not necessarily a problem for econometric analysis if the reported standard errors are accurate. However, robust standard errors can be sensitive to large leverage values which can occur under near multicollinearity. This leads to the undesirable situation where the coefficient estimates are imprecise yet the standard errors are misleadingly small.\nWe can see the impact of near multicollinearity on precision in a simple homoskedastic linear regression model with two regressors\n\\[\nY=X_{1} \\beta_{1}+X_{2} \\beta_{2}+e\n\\]\nand\n\\[\n\\frac{1}{n} \\boldsymbol{X}^{\\prime} \\boldsymbol{X}=\\left(\\begin{array}{ll}\n1 & \\rho \\\\\n\\rho & 1\n\\end{array}\\right) .\n\\]\nIn this case\n\\[\n\\operatorname{var}[\\widehat{\\beta} \\mid \\boldsymbol{X}]=\\frac{\\sigma^{2}}{n}\\left(\\begin{array}{ll}\n1 & \\rho \\\\\n\\rho & 1\n\\end{array}\\right)^{-1}=\\frac{\\sigma^{2}}{n\\left(1-\\rho^{2}\\right)}\\left(\\begin{array}{cc}\n1 & -\\rho \\\\\n-\\rho & 1\n\\end{array}\\right) .\n\\]\nThe correlation \\(\\rho\\) indexes collinearity since as \\(\\rho\\) approaches 1 the matrix becomes singular. We can see the effect of collinearity on precision by observing that the variance of a coefficient estimate \\(\\sigma^{2}\\left[n\\left(1-\\rho^{2}\\right)\\right]^{-1}\\) approaches infinity as \\(\\rho\\) approaches 1 . Thus the more “collinear” are the regressors the worse the precision of the individual coefficient estimates.\nWhat is happening is that when the regressors are highly dependent it is statistically difficult to disentangle the impact of \\(\\beta_{1}\\) from that of \\(\\beta_{2}\\). As a consequence the precision of individual estimates are reduced.\nMany early-generation textbooks overemphasized multicollinearity. An amusing parody of these texts is Micronumerosity, Chapter \\(23.3\\) of Goldberger’s A Course in Econometrics (1991). Among the witty remarks of his chapter are the following.\nThe extreme case, ‘exact micronumerosity’, arises when \\(n=0\\), in which case the sample estimate of \\(\\mu\\) is not unique. (Technically, there is a violation of the rank condition \\(n>0\\) : the matrix 0 is singular.)\nTests for the presence of micronumerosity require the judicious use of various fingers. Some researchers prefer a single finger, others use their toes, still others let their thumbs rule.\nA generally reliable guide may be obtained by counting the number of observations. Most of the time in econometric analysis, when \\(n\\) is close to zero, it is also far from infinity.\nArthur S. Goldberger, A Course in Econometrics (1991), pp. \\(249 .\\) To understand Goldberger’s basic point you should notice that the estimation variance \\(\\sigma^{2}\\left[n\\left(1-\\rho^{2}\\right)\\right]^{-1}\\) depends equally and symmetrically on the correlation \\(\\rho\\) and the sample size \\(n\\). He was pointing out that the only statistical implication of multicollinearity in the homoskedastic model is a lack of precision. Small sample sizes have the exact same implication.\n\n\n\n\n\n\nArthur S. Goldberger\n\n\n\n\nArt Goldberger (1930-2009) was one of the most distinguished members of the\n\n\nDepartment of Economics at the University of Wisconsin. His Ph.D. thesis devel-\n\n\noped a pioneering macroeconometric forecasting model (the Klein-Goldberger\n\n\nmodel). Most of his remaining career focused on microeconometric issues. He\n\n\nwas the leading pioneer of what has been called the Wisconsin Tradition of em-\n\n\npirical work - a combination of formal econometric theory with a careful critical\n\n\nanalysis of empirical work. Goldberger wrote a series of highly regarded and in-\n\n\nfluential graduate econometric textbooks, including Econometric Theory (1964),\n\n\nTopics in Regression Analysis (1968), and A Course in Econometrics (1991)."
  },
  {
    "objectID": "chpt04-lsr.html#clustered-sampling",
    "href": "chpt04-lsr.html#clustered-sampling",
    "title": "4  Least Squares Regression",
    "section": "4.21 Clustered Sampling",
    "text": "4.21 Clustered Sampling\nIn Section \\(4.2\\) we briefly mentioned clustered sampling as an alternative to the assumption of random sampling. We now introduce the framework in more detail and extend the primary results of this chapter to encompass clustered dependence.\nIt might be easiest to understand the idea of clusters by considering a concrete example. Duflo, Dupas, and Kremer (2011) investigate the impact of tracking (assigning students based on initial test score) on educational attainment in a randomized experiment. An extract of their data set is available on the textbook webpage in the file DDK2011.\nIn 2005, 140 primary schools in Kenya received funding to hire an extra first grade teacher to reduce class sizes. In half of the schools (selected randomly) students were assigned to classrooms based on an initial test score (“tracking”); in the remaining schools the students were randomly assigned to classrooms. For their analysis the authors restricted attention to the 121 schools which initially had a single first-grade class.\nThe key regression \\({ }^{4}\\) in the paper is\n\\[\n\\text { TestScore }_{i g}=-0.071+0.138 \\text { Tracking }_{g}+e_{i g}\n\\]\nwhere TestScore \\({ }_{i g}\\) is the standardized test score (normalized to have mean 0 and variance 1) of student \\(i\\) in school \\(g\\), and Tracking \\(g\\) is a dummy equal to 1 if school \\(g\\) was tracking. The OLS estimates indicate that schools which tracked the students had an overall increase in test scores by about \\(0.14\\) standard deviations, which is meaningful. More general versions of this regression are estimated, many of which take the form\n\\[\n\\text { TestScore }_{i g}=\\alpha+\\gamma \\text { Tracking }_{g}+X_{i g}^{\\prime} \\beta+e_{i g}\n\\]\nwhere \\(X_{i g}\\) is a set of controls specific to the student (including age, gender, and initial test score).\n\\({ }^{4}\\) Table 2, column (1). Duflo, Dupas and Kremer (2011) report a coefficient estimate of \\(0.139\\), perhaps due to a slightly different calculation to standardize the test score. A difficulty with applying the classical regression framework is that student achievement is likely correlated within a given school. Student achievement may be affected by local demographics, individual teachers, and classmates, all of which imply dependence. These concerns, however, do not suggest that achievement will be correlated across schools, so it seems reasonable to model achievement across schools as mutually independent. We call such dependence clustered.\nIn clustering contexts it is convenient to double index the observations as \\(\\left(Y_{i g}, X_{i g}\\right)\\) where \\(g=1, \\ldots, G\\) indexes the cluster and \\(i=1, \\ldots, n_{g}\\) indexes the individual within the \\(g^{t h}\\) cluster. The number of observations per cluster \\(n_{g}\\) may vary across clusters. The number of clusters is \\(G\\). The total number of observations is \\(n=\\sum_{g=1}^{G} n_{g}\\). In the Kenyan schooling example the number of clusters (schools) in the estimation sample is \\(G=121\\), the number of students per school varies from 19 to 62 , and the total number of observations is \\(n=5795\\).\nWhile it is typical to write the observations using the double index notation \\(\\left(Y_{i g}, X_{i g}\\right)\\) it is also useful to use cluster-level notation. Let \\(\\boldsymbol{Y}_{g}=\\left(Y_{1 g}, \\ldots, Y_{n_{g} g}\\right)^{\\prime}\\) and \\(\\boldsymbol{X}_{g}=\\left(X_{1 g}, \\ldots, X_{n_{g} g}\\right)^{\\prime}\\) denote the \\(n_{g} \\times 1\\) vector of dependent variables and \\(n_{g} \\times k\\) matrix of regressors for the \\(g^{t h}\\) cluster. A linear regression model can be written by individual as\n\\[\nY_{i g}=X_{i g}^{\\prime} \\beta+e_{i g}\n\\]\nand using cluster notation as\n\\[\n\\boldsymbol{Y}_{g}=\\boldsymbol{X}_{g} \\beta+\\boldsymbol{e}_{g}\n\\]\nwhere \\(\\boldsymbol{e}_{g}=\\left(e_{1 g}, \\ldots, e_{n_{g} g}\\right)^{\\prime}\\) is a \\(n_{g} \\times 1\\) error vector. We can also stack the observations into full sample matrices and write the model as\n\\[\n\\boldsymbol{Y}=\\boldsymbol{X} \\beta+\\boldsymbol{e} .\n\\]\nUsing this notation we can write the sums over the observations using the double sum \\(\\sum_{g=1}^{G} \\sum_{i=1}^{n_{g}}\\). This is the sum across clusters of the sum across observations within each cluster. The OLS estimator can be written as\n\\[\n\\begin{aligned}\n\\widehat{\\beta} &=\\left(\\sum_{g=1}^{G} \\sum_{i=1}^{n_{g}} X_{i g} X_{i g}^{\\prime}\\right)^{-1}\\left(\\sum_{g=1}^{G} \\sum_{i=1}^{n_{g}} X_{i g} Y_{i g}\\right) \\\\\n&=\\left(\\sum_{g=1}^{G} \\boldsymbol{X}_{g}^{\\prime} \\boldsymbol{X}_{g}\\right)^{-1}\\left(\\sum_{g=1}^{G} \\boldsymbol{X}_{g}^{\\prime} \\boldsymbol{Y}_{g}\\right) \\\\\n&=\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{Y}\\right)\n\\end{aligned}\n\\]\nThe residuals are \\(\\widehat{e}_{i g}=Y_{i g}-X_{i g}^{\\prime} \\widehat{\\beta}\\) in individual level notation and \\(\\widehat{\\boldsymbol{e}}_{g}=\\boldsymbol{Y}_{g}-\\boldsymbol{X}_{g} \\widehat{\\beta}\\) in cluster level notation.\nThe standard clustering assumption is that the clusters are known to the researcher and that the observations are independent across clusters.\nAssumption 4.4 The clusters \\(\\left(\\boldsymbol{Y}_{g}, \\boldsymbol{X}_{g}\\right)\\) are mutually independent across clusters \\(g\\).\nIn our example clusters are schools. In other common applications cluster dependence has been assumed within individual classrooms, families, villages, regions, and within larger units such as industries and states. This choice is up to the researcher though the justification will depend on the context, the nature of the data, and will reflect information and assumptions on the dependence structure across observations. The model is a linear regression under the assumption\n\\[\n\\mathbb{E}\\left[\\boldsymbol{e}_{g} \\mid \\boldsymbol{X}_{g}\\right]=0 .\n\\]\nThis is the same as assuming that the individual errors are conditionally mean zero\n\\[\n\\mathbb{E}\\left[e_{i g} \\mid \\boldsymbol{X}_{g}\\right]=0\n\\]\nor that the conditional expectation of \\(\\boldsymbol{Y}_{g}\\) given \\(\\boldsymbol{X}_{g}\\) is linear. As in the independent case equation (4.50) means that the linear regression model is correctly specified. In the clustered regression model this requires that all interaction effects within clusters have been accounted for in the specification of the individual regressors \\(X_{i g}\\).\nIn the regression (4.46) the conditional expectation is necessarily linear and satisfies (4.50) since the\n\\ controls, (4.50) requires that the achievement of any student is unaffected by the individual controls (e.g. age, gender, and initial test score) of other students within the same school.\nGiven (4.50) we can calculate the expectation of the OLS estimator. Substituting (4.48) into (4.49) we find\n\\[\n\\widehat{\\beta}-\\beta=\\left(\\sum_{g=1}^{G} \\boldsymbol{X}_{g}^{\\prime} \\boldsymbol{X}_{g}\\right)^{-1}\\left(\\sum_{g=1}^{G} \\boldsymbol{X}_{g}^{\\prime} \\boldsymbol{e}_{g}\\right)\n\\]\nThe mean of \\(\\widehat{\\beta}-\\beta\\) conditioning on all the regressors is\n\\[\n\\begin{aligned}\n\\mathbb{E}[\\widehat{\\beta}-\\beta \\mid \\boldsymbol{X}] &=\\left(\\sum_{g=1}^{G} \\boldsymbol{X}_{g}^{\\prime} \\boldsymbol{X}_{g}\\right)^{-1}\\left(\\sum_{g=1}^{G} \\boldsymbol{X}_{g}^{\\prime} \\mathbb{E}\\left[\\boldsymbol{e}_{g} \\mid \\boldsymbol{X}\\right]\\right) \\\\\n&=\\left(\\sum_{g=1}^{G} \\boldsymbol{X}_{g}^{\\prime} \\boldsymbol{X}_{g}\\right)^{-1}\\left(\\sum_{g=1}^{G} \\boldsymbol{X}_{g}^{\\prime} \\mathbb{E}\\left[\\boldsymbol{e}_{g} \\mid \\boldsymbol{X}_{g}\\right]\\right) \\\\\n&=0 .\n\\end{aligned}\n\\]\nThe first equality holds by linearity, the second by Assumption 4.4, and the third by (4.50).\nThis shows that OLS is unbiased under clustering if the conditional expectation is linear.\nTheorem 4.7 In the clustered linear regression model (Assumption \\(4.4\\) and (4.50)) \\(\\mathbb{E}[\\widehat{\\beta} \\mid \\boldsymbol{X}]=\\beta\\).\nNow consider the covariance matrix of \\(\\widehat{\\beta}\\). Let \\(\\Sigma_{g}=\\mathbb{E}\\left[\\boldsymbol{e}_{g} \\boldsymbol{e}_{g}^{\\prime} \\mid \\boldsymbol{X}_{g}\\right]\\) denote the \\(n_{g} \\times n_{g}\\) conditional covariance matrix of the errors within the \\(g^{t h}\\) cluster. Since the observations are independent across clusters,\n\\[\n\\begin{aligned}\n\\operatorname{var}\\left[\\left(\\sum_{g=1}^{G} \\boldsymbol{X}_{g}^{\\prime} \\boldsymbol{e}_{g}\\right) \\mid \\boldsymbol{X}\\right] &=\\sum_{g=1}^{G} \\operatorname{var}\\left[\\boldsymbol{X}_{g}^{\\prime} \\boldsymbol{e}_{g} \\mid \\boldsymbol{X}_{g}\\right] \\\\\n&=\\sum_{g=1}^{G} \\boldsymbol{X}_{g}^{\\prime} \\mathbb{E}\\left[\\boldsymbol{e}_{g} \\boldsymbol{e}_{g}^{\\prime} \\mid \\boldsymbol{X}_{g}\\right] \\boldsymbol{X}_{g} \\\\\n&=\\sum_{g=1}^{G} \\boldsymbol{X}_{g}^{\\prime} \\Sigma_{g} \\boldsymbol{X}_{g} \\\\\n& \\stackrel{\\text { def }}{=} \\Omega_{n}\n\\end{aligned}\n\\]\nIt follows that\n\\[\n\\boldsymbol{V}_{\\widehat{\\beta}}=\\operatorname{var}[\\widehat{\\beta} \\mid \\boldsymbol{X}]=\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\Omega_{n}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} .\n\\]\nThis differs from the formula in the independent case due to the correlation between observations within clusters. The magnitude of the difference depends on the degree of correlation between observations within clusters and the number of observations within clusters. To see this, suppose that all clusters have the same number of observations \\(n_{g}=N, \\mathbb{E}\\left[e_{i g}^{2} \\mid \\boldsymbol{X}_{g}\\right]=\\sigma^{2}, \\mathbb{E}\\left[e_{i g} e_{\\ell g} \\mid \\boldsymbol{X}_{g}\\right]=\\sigma^{2} \\rho\\) for \\(i \\neq \\ell\\), and the regressors \\(X_{i g}\\) do not vary within a cluster. In this case the exact variance of the OLS estimator equals \\({ }^{5}\\) (after some calculations)\n\\[\n\\boldsymbol{V}_{\\widehat{\\beta}}=\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\sigma^{2}(1+\\rho(N-1)) .\n\\]\nIf \\(\\rho>0\\) the exact variance is appropriately a multiple \\(\\rho N\\) of the conventional formula. In the Kenyan school example the average cluster size is 48 . If \\(\\rho=0.25\\) this means the exact variance exceeds the conventional formula by a factor of about twelve. In this case the correct standard errors (the square root of the variance) are a multiple of about three times the conventional formula. This is a substantial difference and should not be neglected.\nArellano (1987) proposed a cluster-robust covariance matrix estimator which is an extension of the White estimator. Recall that the insight of the White covariance estimator is that the squared error \\(e_{i}^{2}\\) is unbiased for \\(\\mathbb{E}\\left[e_{i}^{2} \\mid X_{i}\\right]=\\sigma_{i}^{2}\\). Similarly, with cluster dependence the matrix \\(\\boldsymbol{e}_{g} \\boldsymbol{e}_{g}^{\\prime}\\) is unbiased for \\(\\mathbb{E}\\left[\\boldsymbol{e}_{g} \\boldsymbol{e}_{g}^{\\prime} \\mid \\boldsymbol{X}_{g}\\right]=\\Sigma_{g}\\). This means that an unbiased estimator for (4.51) is \\(\\widetilde{\\Omega}_{n}=\\sum_{g=1}^{G} \\boldsymbol{X}_{g}^{\\prime} \\boldsymbol{e}_{g} \\boldsymbol{e}_{g}^{\\prime} \\boldsymbol{X}_{g}\\). This is not feasible, but we can replace the unknown errors by the OLS residuals to obtain Arellano’s estimator:\n\\[\n\\begin{aligned}\n\\widehat{\\Omega}_{n} &=\\sum_{g=1}^{G} \\boldsymbol{X}_{g}^{\\prime} \\widehat{\\boldsymbol{e}}_{g} \\widehat{\\boldsymbol{e}}_{g}^{\\prime} \\boldsymbol{X}_{g} \\\\\n&=\\sum_{g=1}^{G} \\sum_{i=1}^{n_{g}} \\sum_{\\ell=1}^{n_{g}} X_{i g} X_{\\ell g}^{\\prime} \\widehat{e}_{i g} \\widehat{e}_{\\ell g} \\\\\n&=\\sum_{g=1}^{G}\\left(\\sum_{i=1}^{n_{g}} X_{i g} \\widehat{e}_{i g}\\right)\\left(\\sum_{\\ell=1}^{n_{g}} X_{\\ell g} \\widehat{e}_{\\ell g}\\right)^{\\prime} .\n\\end{aligned}\n\\]\nThe three expressions in (4.54) give three equivalent formulae which could be used to calculate \\(\\widehat{\\Omega}_{n}\\). The final expression writes \\(\\widehat{\\Omega}_{n}\\) in terms of the cluster sums \\(\\sum_{\\ell=1}^{n_{g}} X_{\\ell g} \\widehat{e}_{\\ell g}\\) which is the basis for our example \\(\\mathrm{R}\\) and MATLAB codes shown below.\nGiven the expressions (4.51)-(4.52) a natural cluster covariance matrix estimator takes the form\n\\[\n\\widehat{\\boldsymbol{V}}_{\\widehat{\\beta}}=a_{n}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\widehat{\\Omega}_{n}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1}\n\\]\nwhere \\(a_{n}\\) is a possible finite-sample adjustment. The Stata cluster command uses\n\\[\na_{n}=\\left(\\frac{n-1}{n-k}\\right)\\left(\\frac{G}{G-1}\\right) .\n\\]\nThe factor \\(G /(G-1)\\) was derived by Chris Hansen (2007) in the context of equal-sized clusters to improve performance when the number of clusters \\(G\\) is small. The factor \\((n-1) /(n-k)\\) is an \\(a d\\) hoc generalization which nests the adjustment used in (4.37) since \\(G=n\\) implies the simplification \\(a_{n}=n /(n-k)\\).\nAlternative cluster-robust covariance matrix estimators can be constructed using cluster-level prediction errors such as \\(\\widetilde{\\boldsymbol{e}}_{g}=\\boldsymbol{Y}_{g}-\\boldsymbol{X}_{g} \\widehat{\\beta}_{(-g)}\\) where \\(\\widehat{\\beta}_{(-g)}\\) is the least squares estimator omitting cluster \\(g\\). As in Section 3.20, we can show that\n\\[\n\\widetilde{\\boldsymbol{e}}_{g}=\\left(\\boldsymbol{I}_{n_{g}}-\\boldsymbol{X}_{g}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\boldsymbol{X}_{g}^{\\prime}\\right)^{-1} \\widehat{\\boldsymbol{e}}_{g}\n\\]\n\\({ }^{5}\\) This formula is due to Moulton (1990). and\n\\[\n\\widehat{\\beta}_{(-g)}=\\widehat{\\beta}-\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\boldsymbol{X}_{g}^{\\prime} \\widetilde{\\boldsymbol{e}}_{g} .\n\\]\nWe then have the robust covariance matrix estimator\n\\[\n\\widehat{\\boldsymbol{V}}_{\\widehat{\\beta}}^{\\mathrm{CR} 3}=\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1}\\left(\\sum_{g=1}^{G} \\boldsymbol{X}_{g}^{\\prime} \\widetilde{\\boldsymbol{e}}_{g} \\widetilde{\\boldsymbol{e}}_{g}^{\\prime} \\boldsymbol{X}_{g}\\right)\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} .\n\\]\nThe label “CR” refers to “cluster-robust” and “CR3” refers to the analogous formula for the HC3 estimator.\nSimilarly to the heteroskedastic-robust case you can show that CR3 is a conservative estimator for \\(\\boldsymbol{V}_{\\widehat{\\beta}}\\) in the sense that the conditional expectation of \\(\\widehat{\\boldsymbol{V}}_{\\widehat{\\beta}}^{\\mathrm{CR} 3}\\) exceeds \\(\\boldsymbol{V}_{\\widehat{\\beta}}\\). This covariance matrix estimator is more cumbersome to implement, however, as the cluster-level prediction errors (4.57) cannot be calculated in a simple linear operation and requires a loop across clusters to calculate.\nTo illustrate in the context of the Kenyan schooling example we present the regression of student test scores on the school-level tracking dummy with two standard errors displayed. The first (in parenthesis) is the conventional robust standard error. The second [in square brackets] is the clustered standard error (4.55)-(4.56) where clustering is at the level of the school.\n\nWe can see that the cluster-robust standard errors are roughly three times the conventional robust standard errors. Consequently, confidence intervals for the coefficients are greatly affected by the choice.\nFor illustration, we list here the commands needed to produce the regression results with clustered standard errors in Stata, R, and MATLAB.\n\nYou can see that clustered standard errors are simple to calculate in Stata.\n\nProgramming clustered standard errors in \\(\\mathrm{R}\\) is also relatively easy due to the convenient rowsum command which sums variables within clusters.\n\nHere we see that programming clustered standard errors in MATLAB is less convenient than the other packages but still can be executed with just a few lines of code. This example uses the accumarray command which is similar to the rowsum command in \\(\\mathrm{R}\\) but only can be applied to vectors (hence the loop across the regressors) and works best if the clusterid variable are indices (which is why the original schoolid variable is transformed into indices in schoolidx. Application of these commands requires care and attention."
  },
  {
    "objectID": "chpt04-lsr.html#inference-with-clustered-samples",
    "href": "chpt04-lsr.html#inference-with-clustered-samples",
    "title": "4  Least Squares Regression",
    "section": "4.22 Inference with Clustered Samples",
    "text": "4.22 Inference with Clustered Samples\nIn this section we give some cautionary remarks and general advice about cluster-robust inference in econometric practice. There has been remarkably little theoretical research about the properties of cluster-robust methods - until quite recently - so these remarks may become dated rather quickly.\nIn many respects cluster-robust inference should be viewed similarly to heteroskedaticity-robust inference where a “cluster” in the cluster-robust case is interpreted similarly to an “observation” in the heteroskedasticity-robust case. In particular, the effective sample size should be viewed as the number of clusters, not the “sample size” \\(n\\). This is because the cluster-robust covariance matrix estimator effectively treats each cluster as a single observation and estimates the covariance matrix based on the variation across cluster means. Hence if there are only \\(G=50\\) clusters inference should be viewed as (at best) similar to heteroskedasticity-robust inference with \\(n=50\\) observations. This is a bit unsettling when the number of regressors is large (say \\(k=20\\) ) for then the covariance matrix will be estimated imprecisely.\nFurthermore, most cluster-robust theory (for example, the work of Chris Hansen (2007)) assumes that the clusters are homogeneous including the assumption that the cluster sizes are all identical. This turns out to be a very important simplication. When this is violated - when, for example, cluster sizes are highly heterogeneous - the regression should be viewed as roughly equivalent to the heteroskedastic case with an extremely high degree of heteroskedasticity. Cluster sums have variances which are proportional to the cluster sizes so if the latter is heterogeneous so will be the variances of the cluster sums. This also has a large effect on finite sample inference. When clusters are heterogeneous then cluster-robust inference is similar to heteroskedasticity-robust inference with highly heteroskedastic observations.\nPut together, if the number of clusters \\(G\\) is small and the number of observations per cluster is highly varied then we should interpret inferential statements with a great degree of caution. Unfortunately, small \\(G\\) with heterogeneous cluster sizes is commonplace. Many empirical studies on U.S. data cluster at the “state” level meaning that there are 50 or 51 clusters (the District of Columbia is typically treated as a state). The number of observations vary considerably across states since the populations are highly unequal. Thus when you read empirical papers with individual-level data but clustered at the “state” level you should be cautious and recognize that this is equivalent to inference with a small number of extremely heterogeneous observations.\nA further complication occurs when we are interested in treatment as in the tracking example given in the previous section. In many cases (including Duflo, Dupas, and Kremer (2011)) the interest is in the effect of a treatment applied at the cluster level (e.g., schools). In many cases (not, however, Duflo, Dupas, and Kremer (2011)), the number of treated clusters is small relative to the total number of clusters; in an extreme case there is just a single treated cluster. Based on the reasoning given above these applications should be interpreted as equivalent to heteroskedasticity-robust inference with a sparse dummy variable as discussed in Section 4.16. As discussed there, standard error estimates can be erroneously small. In the extreme of a single treated cluster (in the example, if only a single school was tracked) then the estimated coefficient on tracking will be very imprecisely estimated yet will have a misleadingly small cluster standard error. In general, reported standard errors will greatly understate the imprecision of parameter estimates."
  },
  {
    "objectID": "chpt04-lsr.html#at-what-level-to-cluster",
    "href": "chpt04-lsr.html#at-what-level-to-cluster",
    "title": "4  Least Squares Regression",
    "section": "4.23 At What Level to Cluster?",
    "text": "4.23 At What Level to Cluster?\nA practical question which arises in the context of cluster-robust inference is “At what level should we cluster?” In some examples you could cluster at a very fine level, such as families or classrooms, or at higher levels of aggregation, such as neighborhoods, schools, towns, counties, or states. What is the correct level at which to cluster? Rules of thumb have been advocated by practitioners but at present there is little formal analysis to provide useful guidance. What do we know?\nFirst, suppose cluster dependence is ignored or imposed at too fine a level (e.g. clustering by households instead of villages). Then variance estimators will be biased as they will omit covariance terms. As correlation is typically positive, this suggests that standard errors will be too small giving rise to spurious indications of significance and precision.\nSecond, suppose cluster dependence is imposed at too aggregate a measure (e.g. clustering by states rather than villages). This does not cause bias. But the variance estimators will contain many extra components so the precision of the covariance matrix estimator will be poor. This means that reported standard errors will be imprecise - more random - than if clustering had been less aggregate.\nThese considerations show that there is a trade-off between bias and variance in the estimation of the covariance matrix by cluster-robust methods. It is not at all clear-based on current theory - what to do. I state this emphatically. We really do not know what is the “correct” level at which to do cluster-robust inference. This is a very interesting question and should certainly be explored by econometric research. One challenge is that in empirical practice many people have observed: “Clustering is important. Standard errors change a lot whether or not we cluster. Therefore we should only report clustered standard errors.” The flaw in this reasoning is that we do not know why in a specific empirical example the standard errors change under clustering. One possibility is that clustering reduces bias and thus is more accurate. The other possibility is that clustering adds sampling noise and is thus less accurate. In reality it is likely that both factors are present.\nIn any event a researcher should be aware of the number of clusters used in the reported calculations and should treat the number of clusters as the effective sample size for assessing inference. If the number of clusters is, say, \\(G=20\\), this should be treated as a very small sample.\nTo illustrate the thought experiment consider the empirical example of Duflo, Dupas, and Kremer (2011). They reported standard errors clustered at the school level and the application uses 111 schools. Thus \\(G=111\\) is the effective sample size. The number of observations (students) ranges from 19 to 62 , which is reasonably homogeneous. This seems like a well balanced application of clustered variance estimation. However, one could imagine clustering at a different level of aggregation. We might consider clustering at a less aggregate level such as the classroom level, but this cannot be done in this particular application as there was only one classroom per school. Clustering at a more aggregate level could be done in this application at the level of the “zone”. However, there are only 9 zones. Thus if we cluster by zone, \\(G=9\\) is the effective sample size which would lead to imprecise standard errors. In this particular example clustering at the school level (as done by the authors) is indeed the prudent choice."
  },
  {
    "objectID": "chpt04-lsr.html#technical-proofs",
    "href": "chpt04-lsr.html#technical-proofs",
    "title": "4  Least Squares Regression",
    "section": "4.24 Technical Proofs*",
    "text": "4.24 Technical Proofs*\nProof of Theorems \\(\\mathbf{4 . 4}\\) and \\(\\mathbf{4 . 5}\\) Theorem \\(4.4\\) is a special case so we focus on Theorem 4.5. This argument is taken from B. E. Hansen (2021).\nOur approach is to calculate the Cramér-Rao bound for a carefully crafted parametric model. This is based on an insight of Newey (1990, Appendix B) for the simpler context of a population expectation.\nWithout loss of generality, assume that the true coefficient equals \\(\\beta_{0}=0\\) and that \\(\\sigma^{2}=1\\). These are merely normalizations which simplify the notation. Also assume that \\(\\boldsymbol{Y}\\) has a joint density \\(f(\\boldsymbol{y})\\). This assumption can be avoided through use of the Radon-Nikodym derivative.\nDefine the truncation function \\(\\mathbb{R}^{n} \\rightarrow \\mathbb{R}^{n}\\)\n\\[\n\\psi_{c}(\\boldsymbol{y})=\\boldsymbol{y} \\mathbb{1}\\left\\{\\left|\\boldsymbol{X}^{\\prime} \\Sigma^{-1} \\boldsymbol{y}\\right| \\leq c\\right\\}-\\mathbb{E}\\left[\\boldsymbol{Y} \\mathbb{1}\\left\\{\\left|\\boldsymbol{X}^{\\prime} \\Sigma^{-1} \\boldsymbol{Y}\\right| \\leq c\\right\\}\\right] .\n\\]\nNotice that it satisfies \\(\\left|\\psi_{c}(\\boldsymbol{y})\\right| \\leq 2 c, \\mathbb{E}\\left[\\psi_{c}(\\boldsymbol{Y})\\right]=0\\), and\n\\[\n\\mathbb{E}\\left[\\boldsymbol{Y} \\psi_{c}(\\boldsymbol{Y})^{\\prime}\\right]=\\mathbb{E}\\left[\\boldsymbol{Y} \\boldsymbol{Y}^{\\prime} \\mathbb{1}\\left\\{\\left|\\boldsymbol{X}^{\\prime} \\Sigma^{-1} \\boldsymbol{Y}\\right| \\leq c\\right\\}\\right] \\stackrel{\\text { def }}{=} \\Sigma_{c} .\n\\]\nAs \\(c \\rightarrow \\infty, \\Sigma_{c} \\rightarrow \\mathbb{E}\\left[\\boldsymbol{Y} \\boldsymbol{Y}^{\\prime}\\right]=\\Sigma\\). Pick \\(c\\) sufficiently large so that \\(\\Sigma_{c}>0\\), which is feasible because \\(\\Sigma>0\\).\nDefine the auxiliary joint density function\n\\[\nf_{\\beta}(\\boldsymbol{y})=f(\\boldsymbol{y})\\left(1+\\psi_{c}(\\boldsymbol{y})^{\\prime} \\Sigma_{c}^{-1} \\boldsymbol{X} \\beta\\right)\n\\]\nfor parameters \\(\\beta\\) in the set\n\\[\nB=\\left\\{\\beta \\in \\mathbb{R}^{m}:\\|\\beta\\| \\leq \\frac{1}{2 c}\\right\\} .\n\\]\nThe bounds imply that for \\(\\beta \\in B\\) and all \\(y\\)\n\\[\n\\left|\\psi_{c}(\\boldsymbol{y})^{\\prime} \\Sigma_{c}^{-1} \\boldsymbol{X} \\beta\\right|<1 .\n\\]\nThis implies that \\(f_{\\beta}\\) has the same support as \\(f\\) and satisfies the bounds\n\\[\n0<f_{\\beta}(y)<2 f(y) .\n\\]\nWe calculate that\n\\[\n\\begin{aligned}\n\\int f_{\\beta}(\\boldsymbol{y}) d \\boldsymbol{y} &=\\int f(\\boldsymbol{y}) d \\boldsymbol{y}+\\int \\psi_{c}(\\boldsymbol{y})^{\\prime} \\Sigma_{c}^{-1} \\boldsymbol{X} \\beta f_{\\beta}(\\boldsymbol{y}) d \\boldsymbol{y} \\\\\n&=1+\\mathbb{E}\\left[\\psi_{c}(\\boldsymbol{Y})\\right]^{\\prime} \\Sigma_{c}^{-1} \\boldsymbol{X} \\beta \\\\\n&=1\n\\end{aligned}\n\\]\nthe last equality because \\(\\mathbb{E}\\left[\\psi_{c}(\\boldsymbol{Y})\\right]=0\\). Together, these facts imply that \\(f_{\\beta}\\) is a valid density function, and over \\(\\beta \\in B\\) is a parametric family for \\(\\boldsymbol{Y}\\). Evaluated at \\(\\beta_{0}=0\\), which is in the interior of \\(B\\), we see \\(f_{0}=f\\). This means that \\(f_{\\beta}\\) is a correctly-specified parametric family with the true parameter value \\(\\beta_{0}\\).\nLet \\(\\mathbb{E}_{\\beta}\\) denote expectation under the density \\(f_{\\beta}\\). The expectation of \\(\\boldsymbol{Y}\\) in this model is\n\\[\n\\begin{aligned}\n\\mathbb{E}_{\\beta}[\\boldsymbol{Y}] &=\\int \\boldsymbol{y} f_{\\beta}(\\boldsymbol{y}) d \\boldsymbol{y} \\\\\n&=\\int \\boldsymbol{y} f(\\boldsymbol{y}) d \\boldsymbol{y}+\\int \\boldsymbol{y} \\psi_{c}(\\boldsymbol{y})^{\\prime} \\Sigma_{c}^{-1} \\boldsymbol{X} \\beta f_{\\beta}(\\boldsymbol{y}) d \\boldsymbol{y} \\\\\n&=\\mathbb{E}[\\boldsymbol{Y}]+\\mathbb{E}\\left[\\boldsymbol{Y} \\psi_{c}(\\boldsymbol{Y})^{\\prime}\\right] \\Sigma_{c}^{-1} \\boldsymbol{X} \\beta \\\\\n&=\\boldsymbol{X} \\beta\n\\end{aligned}\n\\]\nbecause \\(\\mathbb{E}[\\boldsymbol{Y}]=0\\) and \\(\\mathbb{E}\\left[\\boldsymbol{Y}_{c}(\\boldsymbol{Y})^{\\prime}\\right]=\\Sigma_{c}\\). Thus, the model \\(f_{\\beta}\\) is a linear regression with regression coefficient \\(\\beta\\).\nThe bound (4.61) implies\n\\[\n\\mathbb{E}_{\\beta}\\left[\\|\\boldsymbol{Y}\\|^{2}\\right]=\\int\\|\\boldsymbol{y}\\|^{2} f_{\\beta}(\\boldsymbol{y}) d \\boldsymbol{y} \\leq 2 \\int\\|\\boldsymbol{y}\\|^{2} f(\\boldsymbol{y}) d \\boldsymbol{y}=2 \\mathbb{E}\\left[\\|\\boldsymbol{Y}\\|^{2}\\right]=2 \\operatorname{tr}(\\Sigma)<\\infty .\n\\]\nThis means that \\(f_{\\beta}\\) has a finite variance for all \\(\\beta \\in B\\).\nThe likelihood score for \\(f_{\\beta}\\) is\n\\[\n\\begin{aligned}\nS &=\\left.\\frac{\\partial}{\\partial \\beta} \\log f_{\\beta}(\\boldsymbol{Y})\\right|_{\\beta=0} \\\\\n&=\\left.\\frac{\\partial}{\\partial \\beta} \\log \\left(1+\\psi_{c}(\\boldsymbol{Y})^{\\prime} \\Sigma_{c}^{-1} \\boldsymbol{X} \\beta\\right)\\right|_{\\beta=0} \\\\\n&=\\boldsymbol{X}^{\\prime} \\Sigma_{c}^{-1} \\psi_{c}(\\boldsymbol{Y}) .\n\\end{aligned}\n\\]\nThe information matrix is\n\\[\n\\begin{aligned}\n\\mathscr{I}_{c} &=\\mathbb{E}\\left[S S^{\\prime}\\right] \\\\\n&=\\boldsymbol{X}^{\\prime} \\Sigma_{c}^{-1} \\mathbb{E}\\left[\\psi_{c}(\\boldsymbol{Y}) \\psi_{c}(\\boldsymbol{Y})^{\\prime}\\right] \\Sigma_{c}^{-1} \\boldsymbol{X} \\\\\n& \\leq \\boldsymbol{X}^{\\prime} \\Sigma_{c}^{-1} \\boldsymbol{X},\n\\end{aligned}\n\\]\nwhere the inequality is\n\\[\n\\mathbb{E}\\left[\\psi_{c}(\\boldsymbol{Y}) \\psi_{c}(\\boldsymbol{Y})^{\\prime}\\right]=\\Sigma_{c}-\\mathbb{E}\\left[\\boldsymbol{Y} \\mathbb{1}\\left\\{\\left|\\boldsymbol{X}^{\\prime} \\Sigma^{-1} \\boldsymbol{Y}\\right| \\leq c\\right\\}\\right] \\mathbb{E}\\left[\\boldsymbol{Y} \\mathbb{1}\\left\\{\\left|\\boldsymbol{X}^{\\prime} \\Sigma^{-1} \\boldsymbol{Y}\\right| \\leq c\\right\\}\\right]^{\\prime} \\leq \\Sigma_{c} .\n\\]\nBy assumption, the estimator \\(\\widetilde{\\beta}\\) is unbiased for \\(\\beta\\). The model \\(f_{\\beta}\\) is regular (it is correctly specified as it contains the true density \\(f\\), the support of \\(Y\\) does not depend on \\(\\beta\\), and the true value \\(\\beta_{0}=0\\) lies in the interior of \\(B\\) ). Thus by the Cramér-Rao Theorem (Theorem \\(10.6\\) of Probability and Statistics for Economists)\n\\[\n\\operatorname{var}[\\widetilde{\\beta}] \\geq \\mathscr{I}_{c}^{-1} \\geq\\left(\\boldsymbol{X}^{\\prime} \\Sigma_{c}^{-1} \\boldsymbol{X}\\right)^{-1}\n\\]\nwhere the second inequality is (4.62). Since this holds for all \\(c\\), and \\(\\Sigma_{c} \\rightarrow \\Sigma\\) as \\(c \\rightarrow \\infty\\),\n\\[\n\\operatorname{var}[\\widetilde{\\beta}] \\geq \\limsup _{c \\rightarrow \\infty}\\left(\\boldsymbol{X}^{\\prime} \\Sigma_{c}^{-1} \\boldsymbol{X}\\right)^{-1}=\\left(\\boldsymbol{X}^{\\prime} \\Sigma^{-1} \\boldsymbol{X}\\right)^{-1} .\n\\]\nThis is the variance lower bound."
  },
  {
    "objectID": "chpt04-lsr.html#exercises",
    "href": "chpt04-lsr.html#exercises",
    "title": "4  Least Squares Regression",
    "section": "4.25 Exercises",
    "text": "4.25 Exercises\nExercise 4.1 For some integer \\(k\\), set \\(\\mu_{k}=\\mathbb{E}\\left[Y^{k}\\right]\\).\n\nConstruct an estimator \\(\\widehat{\\mu}_{k}\\) for \\(\\mu_{k}\\).\nShow that \\(\\widehat{\\mu}_{k}\\) is unbiased for \\(\\mu_{k}\\).\nCalculate the variance of \\(\\widehat{\\mu}_{k}\\), say \\(\\operatorname{var}\\left[\\widehat{\\mu}_{k}\\right]\\). What assumption is needed for \\(\\operatorname{var}\\left[\\widehat{\\mu}_{k}\\right]\\) to be finite?\nPropose an estimator of \\(\\operatorname{var}\\left[\\widehat{\\mu}_{k}\\right]\\)\n\nExercise 4.2 Calculate \\(\\mathbb{E}\\left[(\\bar{Y}-\\mu)^{3}\\right]\\), the skewness of \\(\\bar{Y}\\). Under what condition is it zero?\nExercise 4.3 Explain the difference between \\(\\bar{Y}\\) and \\(\\mu\\). Explain the difference between \\(n^{-1} \\sum_{i=1}^{n} X_{i} X_{i}^{\\prime}\\) and \\(\\mathbb{E}\\left[X_{i} X_{i}^{\\prime}\\right]\\)\nExercise 4.4 True or False. If \\(Y=X^{\\prime} \\beta+e, X \\in \\mathbb{R}, \\mathbb{E}[e \\mid X]=0\\), and \\(\\widehat{e}_{i}\\) is the OLS residual from the regression of \\(Y_{i}\\) on \\(X_{i}\\), then \\(\\sum_{i=1}^{n} X_{i}^{2} \\widehat{e}_{i}=0\\).\nExercise 4.5 Prove (4.20) and (4.21).\nExercise 4.6 Prove Theorem \\(4.5\\) under the restriction to linear estimators.\nExercise 4.7 Let \\(\\widetilde{\\beta}\\) be the GLS estimator (4.22) under the assumptions (4.18) and (4.19). Assume that \\(\\Sigma\\) is known and \\(\\sigma^{2}\\) is fdunknown. Define the residual vector \\(\\widetilde{\\boldsymbol{e}}=\\boldsymbol{Y}-\\boldsymbol{X} \\widetilde{\\beta}\\), and an estimator for \\(\\sigma^{2}\\)\n\\[\n\\widetilde{\\sigma}^{2}=\\frac{1}{n-k} \\widetilde{\\boldsymbol{e}}^{\\prime} \\Sigma^{-1} \\widetilde{\\boldsymbol{e}}\n\\]\n\nShow (4.23).\nShow (4.24).\nProve that \\(\\widetilde{\\boldsymbol{e}}=\\boldsymbol{M}_{1} \\boldsymbol{e}\\), where \\(\\boldsymbol{M}_{1}=\\boldsymbol{I}-\\boldsymbol{X}\\left(\\boldsymbol{X}^{\\prime} \\Sigma^{-1} \\boldsymbol{X}\\right)^{-1} \\boldsymbol{X}^{\\prime} \\Sigma^{-1}\\).\nProve that \\(\\boldsymbol{M}_{1}^{\\prime} \\Sigma^{-1} \\boldsymbol{M}_{1}=\\Sigma^{-1}-\\Sigma^{-1} \\boldsymbol{X}\\left(\\boldsymbol{X}^{\\prime} \\Sigma^{-1} \\boldsymbol{X}\\right)^{-1} \\boldsymbol{X}^{\\prime} \\Sigma^{-1}\\). (e) Find \\(\\mathbb{E}\\left[\\widetilde{\\sigma}^{2} \\mid \\boldsymbol{X}\\right]\\).\nIs \\(\\widetilde{\\sigma}^{2}\\) a reasonable estimator for \\(\\sigma^{2}\\) ?\n\nExercise 4.8 Let \\(\\left(Y_{i}, X_{i}\\right)\\) be a random sample with \\(\\mathbb{E}[Y \\mid X]=X^{\\prime} \\beta\\). Consider the Weighted Least Squares (WLS) estimator \\(\\widetilde{\\beta}_{\\text {wls }}=\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{W} \\boldsymbol{X}\\right)^{-1}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{W} \\boldsymbol{Y}\\right)\\) where \\(\\boldsymbol{W}=\\operatorname{diag}\\left(w_{1}, \\ldots, w_{n}\\right)\\) and \\(w_{i}=X_{j i}^{-2}\\), where \\(X_{j i}\\) is one of the \\(X_{i}\\).\n\nIn which contexts would \\(\\widetilde{\\beta}_{\\mathrm{wls}}\\) be a good estimator?\nUsing your intuition, in which situations do you expect \\(\\widetilde{\\beta}_{\\text {wls }}\\) to perform better than OLS?\n\nExercise 4.9 Show (4.32) in the homoskedastic regression model.\nExercise 4.10 Prove (4.40).\nExercise 4.11 Show (4.41) in the homoskedastic regression model.\nExercise 4.12 Let \\(\\mu=\\mathbb{E}[Y], \\sigma^{2}=\\mathbb{E}\\left[(Y-\\mu)^{2}\\right]\\) and \\(\\mu_{3}=\\mathbb{E}\\left[(Y-\\mu)^{3}\\right]\\) and consider the sample mean \\(\\bar{Y}=\\) \\(\\frac{1}{n} \\sum_{i=1}^{n} Y_{i}\\). Find \\(\\mathbb{E}\\left[(\\bar{Y}-\\mu)^{3}\\right]\\) as a function of \\(\\mu, \\sigma^{2}, \\mu_{3}\\) and \\(n\\).\nExercise 4.13 Take the simple regression model \\(Y=X \\beta+e, X \\in \\mathbb{R}, \\mathbb{E}[e \\mid X]=0\\). Define \\(\\sigma_{i}^{2}=\\mathbb{E}\\left[e_{i}^{2} \\mid X_{i}\\right]\\) and \\(\\mu_{3 i}=\\mathbb{E}\\left[e_{i}^{3} \\mid X_{i}\\right]\\) and consider the OLS coefficient \\(\\widehat{\\beta}\\). Find \\(\\mathbb{E}\\left[(\\widehat{\\beta}-\\beta)^{3} \\mid \\boldsymbol{X}\\right]\\).\nExercise 4.14 Take a regression model \\(Y=X \\beta+e\\) with \\(\\mathbb{E}[e \\mid X]=0\\) and i.i.d. observations \\(\\left(Y_{i}, X_{i}\\right)\\) and scalar \\(X\\). The parameter of interest is \\(\\theta=\\beta^{2}\\). Consider the OLS estimators \\(\\widehat{\\beta}\\) and \\(\\widehat{\\theta}=\\widehat{\\beta}^{2}\\).\n\nFind \\(\\mathbb{E}[\\widehat{\\theta} \\mid \\boldsymbol{X}]\\) using our knowledge of \\(\\mathbb{E}[\\widehat{\\beta} \\mid \\boldsymbol{X}]\\) and \\(V_{\\widehat{\\beta}}=\\operatorname{var}[\\widehat{\\beta} \\mid \\boldsymbol{X}]\\). Is \\(\\widehat{\\theta}\\) biased for \\(\\theta\\) ?\nSuggest an (approximate) biased-corrected estimator \\(\\widehat{\\theta}^{*}\\) using an estimator \\(\\widehat{V}_{\\widehat{\\beta}}\\) for \\(V_{\\widehat{\\beta}}\\).\nFor \\(\\widehat{\\theta}^{*}\\) to be potentially unbiased, which estimator of \\(V_{\\widehat{\\beta}}\\) is most appropriate?\n\nUnder which conditions is \\(\\widehat{\\theta}^{*}\\) unbiased?\nExercise 4.15 Consider an i.i.d. sample \\(\\left\\{Y_{i}, X_{i}\\right\\} i=1, \\ldots, n\\) where \\(X\\) is \\(k \\times 1\\). Assume the linear conditional expectation model \\(Y=X^{\\prime} \\beta+e\\) with \\(\\mathbb{E}[e \\mid X]=0\\). Assume that \\(n^{-1} \\boldsymbol{X}^{\\prime} \\boldsymbol{X}=\\boldsymbol{I}_{k}\\) (orthonormal regressors). Consider the OLS estimator \\(\\widehat{\\beta}\\).\n\nFind \\(\\boldsymbol{V}_{\\widehat{\\beta}}=\\operatorname{var}[\\widehat{\\beta}]\\)\nIn general, are \\(\\widehat{\\beta}_{j}\\) and \\(\\widehat{\\beta}_{\\ell}\\) for \\(j \\neq \\ell\\) correlated or uncorrelated?\nFind a sufficient condition so that \\(\\widehat{\\beta}_{j}\\) and \\(\\widehat{\\beta}_{\\ell}\\) for \\(j \\neq \\ell\\) are uncorrelated.\n\nExercise 4.16 Take the linear homoskedastic CEF\n\\[\n\\begin{aligned}\nY^{*} &=X^{\\prime} \\beta+e \\\\\n\\mathbb{E}[e \\mid X] &=0 \\\\\n\\mathbb{E}\\left[e^{2} \\mid X\\right] &=\\sigma^{2}\n\\end{aligned}\n\\]\nand suppose that \\(Y^{*}\\) is measured with error. Instead of \\(Y^{*}\\), we observe \\(Y=Y^{*}+u\\) where \\(u\\) is measurement error. Suppose that \\(e\\) and \\(u\\) are independent and\n\\[\n\\begin{aligned}\n\\mathbb{E}[u \\mid X] &=0 \\\\\n\\mathbb{E}\\left[u^{2} \\mid X\\right] &=\\sigma_{u}^{2}(X)\n\\end{aligned}\n\\]\n\nDerive an equation for \\(Y\\) as a function of \\(X\\). Be explicit to write the error term as a function of the structural errors \\(e\\) and \\(u\\). What is the effect of this measurement error on the model (4.63)?\nDescribe the effect of this measurement error on OLS estimation of \\(\\beta\\) in the feasible regression of the observed \\(Y\\) on \\(X\\).\nDescribe the effect (if any) of this measurement error on standard error calculation for \\(\\widehat{\\beta}\\).\n\nExercise 4.17 Suppose that for the random variables \\((Y, X)\\) with \\(X>0\\) an economic model implies\n\\[\n\\mathbb{E}[Y \\mid X]=(\\gamma+\\theta X)^{1 / 2} .\n\\]\nA friend suggests that you estimate \\(\\gamma\\) and \\(\\theta\\) by the linear regression of \\(Y^{2}\\) on \\(X\\), that is, to estimate the equation\n\\[\nY^{2}=\\alpha+\\beta X+e .\n\\]\n\nInvestigate your friend’s suggestion. Define \\(u=Y-(\\gamma+\\theta X)^{1 / 2}\\). Show that \\(\\mathbb{E}[u \\mid X]=0\\) is implied by (4.64).\nUse \\(Y=(\\gamma+\\theta X)^{1 / 2}+u\\) to calculate \\(\\mathbb{E}\\left[Y^{2} \\mid X\\right]\\). What does this tell you about the implied equation (4.65)?\nCan you recover either \\(\\gamma\\) and/or \\(\\theta\\) from estimation of (4.65)? Are additional assumptions required?\nIs this a reasonable suggestion?\n\nExercise 4.18 Take the model\n\\[\n\\begin{aligned}\nY &=X_{1}^{\\prime} \\beta_{1}+X_{2}^{\\prime} \\beta_{2}+e \\\\\n\\mathbb{E}[e \\mid X] &=0 \\\\\n\\mathbb{E}\\left[e^{2} \\mid X\\right] &=\\sigma^{2}\n\\end{aligned}\n\\]\nwhere \\(X=\\left(X_{1}, X_{2}\\right)\\), with \\(X_{1} k_{1} \\times 1\\) and \\(X_{2} k_{2} \\times 1\\). Consider the short regression \\(Y_{i}=X_{1 i}^{\\prime} \\widehat{\\beta}_{1}+\\widehat{e}_{i}\\) and define the error variance estimator \\(s^{2}=\\left(n-k_{1}\\right)^{-1} \\sum_{i=1}^{n} \\widehat{e}_{i}^{2}\\). Find \\(\\mathbb{E}\\left[s^{2} \\mid \\boldsymbol{X}\\right]\\).\nExercise 4.19 Let \\(\\boldsymbol{Y}\\) be \\(n \\times 1, \\boldsymbol{X}\\) be \\(n \\times k\\), and \\(\\boldsymbol{X}^{*}=\\boldsymbol{X} \\boldsymbol{C}\\) where \\(\\boldsymbol{C}\\) is \\(k \\times k\\) and full-rank. Let \\(\\widehat{\\beta}\\) be the least squares estimator from the regression of \\(Y\\) on \\(X\\), and let \\(\\widehat{V}\\) be the estimate of its asymptotic covariance matrix. Let \\(\\widehat{\\beta}^{*}\\) and \\(\\widehat{\\boldsymbol{V}}^{*}\\) be those from the regression of \\(\\boldsymbol{Y}\\) on \\(\\boldsymbol{X}^{*}\\). Derive an expression for \\(\\widehat{\\boldsymbol{V}}^{*}\\) as a function of \\(\\widehat{V}\\).\nExercise 4.20 Take the model in vector notation\n\\[\n\\begin{aligned}\n\\boldsymbol{Y} &=\\boldsymbol{X} \\beta+\\boldsymbol{e} \\\\\n\\mathbb{E}[\\boldsymbol{e} \\mid \\boldsymbol{X}] &=0 \\\\\n\\mathbb{E}\\left[\\boldsymbol{e} \\boldsymbol{e}^{\\prime} \\mid \\boldsymbol{X}\\right] &=\\Sigma .\n\\end{aligned}\n\\]\nAssume for simplicity that \\(\\Sigma\\) is known. Consider the OLS and GLS estimators \\(\\widehat{\\beta}=\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{Y}\\right)\\) and \\(\\widetilde{\\beta}=\\left(\\boldsymbol{X}^{\\prime} \\Sigma^{-1} \\boldsymbol{X}\\right)^{-1}\\left(\\boldsymbol{X}^{\\prime} \\Sigma^{-1} \\boldsymbol{Y}\\right)\\). Compute the (conditional) covariance between \\(\\widehat{\\beta}\\) and \\(\\widetilde{\\beta}\\) :\n\\[\n\\mathbb{E}\\left[(\\widehat{\\beta}-\\beta)(\\widetilde{\\beta}-\\beta)^{\\prime} \\mid \\boldsymbol{X}\\right]\n\\]\nFind the (conditional) covariance matrix for \\(\\widehat{\\beta}-\\widetilde{\\beta}\\) :\n\\[\n\\mathbb{E}\\left[(\\widehat{\\beta}-\\widetilde{\\beta})(\\widehat{\\beta}-\\beta)^{\\prime} \\mid \\boldsymbol{X}\\right] .\n\\]\nExercise 4.21 The model is\n\\[\n\\begin{aligned}\nY_{i} &=X_{i}^{\\prime} \\beta+e_{i} \\\\\n\\mathbb{E}\\left[e_{i} \\mid X_{i}\\right] &=0 \\\\\n\\mathbb{E}\\left[e_{i}^{2} \\mid X_{i}\\right] &=\\sigma_{i}^{2} \\\\\n\\Sigma &=\\operatorname{diag}\\left\\{\\sigma_{1}^{2}, \\ldots, \\sigma_{n}^{2}\\right\\} .\n\\end{aligned}\n\\]\nThe parameter \\(\\beta\\) is estimated by OLS \\(\\widehat{\\beta}=\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\boldsymbol{X}^{\\prime} \\boldsymbol{Y}\\) and GLS \\(\\widetilde{\\beta}=\\left(\\boldsymbol{X}^{\\prime} \\Sigma^{-1} \\boldsymbol{X}\\right)^{-1} \\boldsymbol{X}^{\\prime} \\Sigma^{-1} \\boldsymbol{Y}\\). Let \\(\\widehat{\\boldsymbol{e}}=\\boldsymbol{Y}-\\boldsymbol{X} \\widehat{\\beta}\\) and \\(\\widetilde{\\boldsymbol{e}}=\\boldsymbol{Y}-\\boldsymbol{X} \\widetilde{\\beta}\\) denote the residuals. Let \\(\\widehat{R}^{2}=1-\\widehat{\\boldsymbol{e}}^{\\prime} \\widehat{\\boldsymbol{e}} /\\left(\\boldsymbol{Y}^{* \\prime} \\boldsymbol{Y}^{*}\\right)\\) and \\(\\widetilde{R}^{2}=1-\\widetilde{\\boldsymbol{e}}^{\\prime} \\widetilde{\\boldsymbol{e}} /\\left(\\boldsymbol{Y}^{* \\prime} \\boldsymbol{Y}^{*}\\right)\\) denote the equation \\(R^{2}\\) where \\(\\boldsymbol{Y}^{*}=\\boldsymbol{Y}-\\bar{Y}\\). If the error \\(e_{i}\\) is truly heteroskedastic will \\(\\widehat{R}^{2}\\) or \\(\\widetilde{R}^{2}\\) be smaller?\nExercise 4.22 An economist friend tells you that the assumption that the observations \\(\\left(Y_{i}, X_{i}\\right)\\) are i.i.d. implies that the regression \\(Y=X^{\\prime} \\beta+e\\) is homoskedastic. Do you agree with your friend? How would you explain your position?\nExercise 4.23 Take the linear regression model with \\(\\mathbb{E}[\\boldsymbol{Y} \\mid \\boldsymbol{X}]=\\boldsymbol{X} \\beta\\). Define the ridge regression estimator\n\\[\n\\widehat{\\beta}=\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}+\\boldsymbol{I}_{k} \\lambda\\right)^{-1} \\boldsymbol{X}^{\\prime} \\boldsymbol{Y}\n\\]\nwhere \\(\\lambda>0\\) is a fixed constant. Find \\(E[\\widehat{\\beta} \\mid \\boldsymbol{X}]\\). Is \\(\\widehat{\\beta}\\) biased for \\(\\beta\\) ?\nExercise 4.24 Continue the empirical analysis in Exercise 3.24.\n\nCalculate standard errors using the homoskedasticity formula and using the four covariance matrices from Section \\(4.14 .\\)\nRepeat in a second programming language. Are they identical?\n\nExercise 4.25 Continue the empirical analysis in Exercise 3.26. Calculate standard errors using the HC3 method. Repeat in your second programming language. Are they identical?\nExercise 4.26 Extend the empirical analysis reported in Section \\(4.21\\) using the DDK2011 dataset on the textbook website.. Do a regression of standardized test score (totalscore normalized to have zero mean and variance 1) on tracking, age, gender, being assigned to the contract teacher, and student’s percentile in the initial distribution. (The sample size will be smaller as some observations have missing variables.) Calculate standard errors using both the conventional robust formula, and clustering based on the school.\n\nCompare the two sets of standard errors. Which standard error changes the most by clustering? Which changes the least?\nHow does the coefficient on tracking change by inclusion of the individual controls (in comparison to the results from (4.60))?"
  },
  {
    "objectID": "chpt05-normal-reg.html#introduction",
    "href": "chpt05-normal-reg.html#introduction",
    "title": "5  Normal Regression",
    "section": "5.1 Introduction",
    "text": "5.1 Introduction\nThis chapter introduces the normal regression model, which is a special case of the linear regression model. It is important as normality allows precise distributional characterizations and sharp inferences. It also provides a baseline for comparison with alternative inference methods, such as asymptotic approximations and the bootstrap.\nThe normal regression model is a fully parametric setting where maximum likelihood estimation is appropriate. Therefore in this chapter we introduce likelihood methods. The method of maximum likelihood is a powerful statistical method for parametric models (such as the normal regression model) and is widely used in econometric practice.\nWe start the chapter with a review of the definition and properties of the normal distribution. For detail and mathematical proofs see Chapter 5 of Probability and Statistics for Economists."
  },
  {
    "objectID": "chpt05-normal-reg.html#the-normal-distribution",
    "href": "chpt05-normal-reg.html#the-normal-distribution",
    "title": "5  Normal Regression",
    "section": "5.2 The Normal Distribution",
    "text": "5.2 The Normal Distribution\nWe say that a random variable \\(Z\\) has the standard normal distribution, or Gaussian, written \\(Z \\sim\\) \\(\\mathrm{N}(0,1)\\), if it has the density\n\\[\n\\phi(x)=\\frac{1}{\\sqrt{2 \\pi}} \\exp \\left(-\\frac{x^{2}}{2}\\right), \\quad-\\infty<x<\\infty .\n\\]\nThe standard normal density is typically written with the symbol \\(\\phi(x)\\) and the corresponding distribution function by \\(\\Phi(x)\\). Plots of the standard normal density function \\(\\phi(x)\\) and distribution function \\(\\Phi(x)\\) are displayed in Figure 5.1.\n\n\nNormal Density\n\n\n\nNormal Distribution\n\nFigure 5.1: Standard Normal Density and Distribution\nTheorem 5.1 If \\(Z \\sim \\mathrm{N}(0,1)\\) then\n\nAll integer moments of \\(Z\\) are finite.\nAll odd moments of \\(Z\\) equal 0 .\nFor any positive integer \\(m\\)\n\n\\[\n\\mathbb{E}\\left[Z^{2 m}\\right]=(2 m-1) ! !=(2 m-1) \\times(2 m-3) \\times \\cdots \\times 1 .\n\\]\n 1. For any \\(r>0\\)\n\\[\n\\mathbb{E}|Z|^{r}=\\frac{2^{r / 2}}{\\sqrt{\\pi}} \\Gamma\\left(\\frac{r+1}{2}\\right)\n\\]\nwhere \\(\\Gamma(t)=\\int_{0}^{\\infty} u^{t-1} e^{-u} d u\\) is the gamma function.\nIf \\(Z \\sim \\mathrm{N}(0,1)\\) and \\(X=\\mu+\\sigma Z\\) for \\(\\mu \\in \\mathbb{R}\\) and \\(\\sigma \\geq 0\\) then \\(X\\) has the univariate normal distribution, written \\(X \\sim \\mathrm{N}\\left(\\mu, \\sigma^{2}\\right)\\). By change-of-variables \\(X\\) has the density\n\\[\nf(x)=\\frac{1}{\\sqrt{2 \\pi \\sigma^{2}}} \\exp \\left(-\\frac{(x-\\mu)^{2}}{2 \\sigma^{2}}\\right), \\quad-\\infty<x<\\infty .\n\\]\nThe expectation and variance of \\(X\\) are \\(\\mu\\) and \\(\\sigma^{2}\\), respectively.\nThe normal distribution and its relatives (the chi-square, student t, F, non-central chi-square, and F) are frequently used for inference to calculate critical values and \\(\\mathrm{p}\\)-values. This involves evaluating the normal cdf \\(\\Phi(x)\\) and its inverse. Since the cdf \\(\\Phi(x)\\) is not available in closed form, statistical textbooks have traditionally provided tables for this purpose. Such tables are not used currently as these calculations are embedded in modern statistical software. For convenience, we list the appropriate commands in MATLAB, R, and Stata to compute the cumulative distribution function of commonly used statistical distributions.\n\nHere we list the appropriate commands to compute the inverse probabilities (quantiles) of the same distributions.\n\n\n\n\n\n\n\n\n\nTo calculate \\(x\\) which solves \\(p=\\mathbb{P}[Z \\leq x]\\) for given \\(p\\)\n\n\n\n\n\n\n\n\\(\\mathrm{N}(0,1)\\)\nMATLAB\n\\(\\mathrm{R}\\)\nStata\n\n\n\n\\(\\operatorname{norminv}(\\mathrm{p})\\)\n\\(\\mathrm{qnorm}(\\mathrm{p})\\)\ninvnormal \\((\\mathrm{p})\\)\n\n\n\\(t_{r}\\)\n\\(\\operatorname{tinv}(\\mathrm{p}, \\mathrm{r})\\)\n\\(\\mathrm{qchisq}(\\mathrm{p}, \\mathrm{r})\\)\ninvchi2 \\((\\mathrm{r}, \\mathrm{p})\\)\n\n\n\\(F_{r, k}\\)\n\\(\\mathrm{finv}(\\mathrm{p}, \\mathrm{r}, \\mathrm{k})\\)\n\\(\\mathrm{qf}(\\mathrm{p}, \\mathrm{r}, \\mathrm{k})\\)\ninvttail \\((\\mathrm{r}, 1-\\mathrm{p})\\)\n\n\n\\(\\chi_{r}^{2}(d)\\)\n\\(\\mathrm{ncx2inv}(\\mathrm{p}, \\mathrm{r}, \\mathrm{d})\\)\n\\(\\mathrm{qchisq}(\\mathrm{p}, \\mathrm{r}, \\mathrm{d})\\)\ninvnchi2 \\((\\mathrm{r}, \\mathrm{d}, \\mathrm{p})\\)\n\n\n\\(F_{r, k}(d)\\)\n\\(\\mathrm{ncfinv}(\\mathrm{p}, \\mathrm{r}, \\mathrm{k}, \\mathrm{d})\\)\n\\(\\mathrm{qf}(\\mathrm{p}, \\mathrm{r}, \\mathrm{k}, \\mathrm{d})\\)\ninvnFtail \\((\\mathrm{r}, \\mathrm{k}, \\mathrm{d}, 1-\\mathrm{p})\\)"
  },
  {
    "objectID": "chpt05-normal-reg.html#multivariate-normal-distribution",
    "href": "chpt05-normal-reg.html#multivariate-normal-distribution",
    "title": "5  Normal Regression",
    "section": "5.3 Multivariate Normal Distribution",
    "text": "5.3 Multivariate Normal Distribution\nWe say that the \\(k\\)-vector \\(Z\\) has a multivariate standard normal distribution, written \\(Z \\sim \\mathrm{N}\\left(0, \\boldsymbol{I}_{k}\\right)\\), if it has the joint density\n\\[\nf(x)=\\frac{1}{(2 \\pi)^{k / 2}} \\exp \\left(-\\frac{x^{\\prime} x}{2}\\right), \\quad x \\in \\mathbb{R}^{k}\n\\]\nThe mean and covariance matrix of \\(Z\\) are 0 and \\(\\boldsymbol{I}_{k}\\), respectively. The multivariate joint density factors into the product of univariate normal densities, so the elements of \\(Z\\) are mutually independent standard normals. If \\(Z \\sim \\mathrm{N}\\left(0, \\boldsymbol{I}_{k}\\right)\\) and \\(X=\\mu+\\boldsymbol{B} Z\\) then the \\(k\\)-vector \\(X\\) has a multivariate normal distribution, written \\(X \\sim \\mathrm{N}(\\mu, \\Sigma)\\) where \\(\\Sigma=\\boldsymbol{B} \\boldsymbol{B}^{\\prime} \\geq 0\\). If \\(\\Sigma>0\\) then by change-of-variables \\(X\\) has the joint density function\n\\[\nf(x)=\\frac{1}{(2 \\pi)^{k / 2} \\operatorname{det}(\\Sigma)^{1 / 2}} \\exp \\left(-\\frac{(x-\\mu)^{\\prime} \\Sigma^{-1}(x-\\mu)}{2}\\right), \\quad x \\in \\mathbb{R}^{k} .\n\\]\nThe expectation and covariance matrix of \\(X\\) are \\(\\mu\\) and \\(\\Sigma\\), respectively. By setting \\(k=1\\) you can check that the multivariate normal simplifies to the univariate normal.\nAn important property of normal random vectors is that affine functions are multivariate normal.\nTheorem 5.2 If \\(X \\sim \\mathrm{N}(\\mu, \\Sigma)\\) and \\(Y=\\boldsymbol{a}+\\boldsymbol{B} X\\), then \\(Y \\sim \\mathrm{N}\\left(\\boldsymbol{a}+\\boldsymbol{B} \\mu, \\boldsymbol{B} \\Sigma \\boldsymbol{B}^{\\prime}\\right)\\).\nOne simple implication of Theorem \\(5.2\\) is that if \\(X\\) is multivariate normal then each component of \\(X\\) is univariate normal.\nAnother useful property of the multivariate normal distribution is that uncorrelatedness is the same as independence. That is, if a vector is multivariate normal, subsets of variables are independent if and only if they are uncorrelated.\nTheorem 5.3 Properties of the Multivariate Normal Distribution\n\nThe expectation and covariance matrix of \\(X \\sim \\mathrm{N}(\\mu, \\Sigma)\\) are \\(\\mathbb{E}[X]=\\mu\\) and \\(\\operatorname{var}[X]=\\Sigma\\).\nIf \\((X, Y)\\) are multivariate normal, \\(X\\) and \\(Y\\) are uncorrelated if and only if they are independent.\nIf \\(X \\sim \\mathrm{N}(\\mu, \\Sigma)\\) and \\(Y=\\boldsymbol{a}+\\boldsymbol{B} X\\), then \\(Y \\sim \\mathrm{N}\\left(\\boldsymbol{a}+\\boldsymbol{B} \\mu, \\boldsymbol{B} \\Sigma \\boldsymbol{B}^{\\prime}\\right)\\).\nIf \\(X \\sim \\mathrm{N}\\left(0, \\boldsymbol{I}_{k}\\right)\\) then \\(X^{\\prime} X \\sim \\chi_{k}^{2}\\), chi-square with \\(k\\) degrees of freedom.\nIf \\(X \\sim \\mathrm{N}(0, \\Sigma)\\) with \\(\\Sigma>0\\) then \\(X^{\\prime} \\Sigma^{-1} X \\sim \\chi_{k}^{2}\\) where \\(k=\\operatorname{dim}(X)\\).\nIf \\(X \\sim \\mathrm{N}(\\mu, \\Sigma)\\) with \\(\\Sigma>0, r \\times r\\), then \\(X^{\\prime} \\Sigma^{-1} X \\sim \\chi_{r}^{2}(\\lambda)\\) where \\(\\lambda=\\mu^{\\prime} \\Sigma^{-1} \\mu\\).\nIf \\(Z \\sim \\mathrm{N}(0,1)\\) and \\(Q \\sim \\chi_{k}^{2}\\) are independent then \\(Z / \\sqrt{Q / k} \\sim t_{k}\\), student t with \\(k\\) degrees of freedom.\nIf \\((Y, X)\\) are multivariate normal\n\n\\[\n\\left(\\begin{array}{l}\nY \\\\\nX\n\\end{array}\\right) \\sim \\mathrm{N}\\left(\\left(\\begin{array}{l}\n\\mu_{Y} \\\\\n\\mu_{X}\n\\end{array}\\right),\\left(\\begin{array}{cc}\n\\Sigma_{Y Y} & \\Sigma_{Y X} \\\\\n\\Sigma_{X Y} & \\Sigma_{X X}\n\\end{array}\\right)\\right)\n\\]\nwith \\(\\Sigma_{Y Y}>0\\) and \\(\\Sigma_{X X}>0\\), then the conditional distributions are\n\\[\n\\begin{aligned}\n&Y \\mid X \\sim \\mathrm{N}\\left(\\mu_{Y}+\\Sigma_{Y X} \\Sigma_{X X}^{-1}\\left(X-\\mu_{X}\\right), \\Sigma_{Y Y}-\\Sigma_{Y X} \\Sigma_{X X}^{-1} \\Sigma_{X Y}\\right) \\\\\n&X \\mid Y \\sim \\mathrm{N}\\left(\\mu_{X}+\\Sigma_{X Y} \\Sigma_{Y Y}^{-1}\\left(Y-\\mu_{Y}\\right), \\Sigma_{X X}-\\Sigma_{X Y} \\Sigma_{Y Y}^{-1} \\Sigma_{Y X}\\right) .\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "chpt05-normal-reg.html#joint-normality-and-linear-regression",
    "href": "chpt05-normal-reg.html#joint-normality-and-linear-regression",
    "title": "5  Normal Regression",
    "section": "5.4 Joint Normality and Linear Regression",
    "text": "5.4 Joint Normality and Linear Regression\ngiven \\(X\\)Suppose the variables \\((Y, X)\\) are jointly normally distributed. Consider the best linear predictor of \\(Y\\)\n\\[\nY=X^{\\prime} \\beta+\\alpha+e .\n\\]\nBy the properties of the best linear predictor, \\(\\mathbb{E}[X e]=0\\) and \\(\\mathbb{E}[e]=0\\), so \\(X\\) and \\(e\\) are uncorrelated. Since \\((e, X)\\) is an affine transformation of the normal vector \\((Y, X)\\) it follows that \\((e, X)\\) is jointly normal (Theorem 5.2). Since \\((e, X)\\) is jointly normal and uncorrelated they are independent (Theorem 5.3). Independence implies that\n\\[\n\\mathbb{E}[e \\mid X]=\\mathbb{E}[e]=0\n\\]\nand\n\\[\n\\mathbb{E}\\left[e^{2} \\mid X\\right]=\\mathbb{E}\\left[e^{2}\\right]=\\sigma^{2}\n\\]\nwhich are properties of a homoskedastic linear CEF.\nWe have shown that when \\((Y, X)\\) are jointly normally distributed they satisfy a normal linear CEF\n\\[\nY=X^{\\prime} \\beta+\\alpha+e\n\\]\nwhere\n\\[\ne \\sim \\mathrm{N}\\left(0, \\sigma^{2}\\right)\n\\]\nis independent of \\(X\\). This result can also be deduced from Theorem 5.3.7.\nThis is a classical motivation for the linear regression model."
  },
  {
    "objectID": "chpt05-normal-reg.html#normal-regression-model",
    "href": "chpt05-normal-reg.html#normal-regression-model",
    "title": "5  Normal Regression",
    "section": "5.5 Normal Regression Model",
    "text": "5.5 Normal Regression Model\nThe normal regression model is the linear regression model with an independent normal error\n\\[\n\\begin{gathered}\nY=X^{\\prime} \\beta+e \\\\\ne \\sim \\mathrm{N}\\left(0, \\sigma^{2}\\right) .\n\\end{gathered}\n\\]\nAs we learned in Section 5.4, the normal regression model holds when \\((Y, X)\\) are jointly normally distributed. Normal regression, however, does not require joint normality. All that is required is that the conditional distribution of \\(Y\\) given \\(X\\) is normal (the marginal distribution of \\(X\\) is unrestricted). In this sense the normal regression model is broader than joint normality. Notice that for notational convenience we have written (5.1) so that \\(X\\) contains the intercept.\nNormal regression is a parametric model where likelihood methods can be used for estimation, testing, and distribution theory. The likelihood is the name for the joint probability density of the data, evaluated at the observed sample, and viewed as a function of the parameters. The maximum likelihood estimator is the value which maximizes this likelihood function. Let us now derive the likelihood of the normal regression model.\nFirst, observe that model (5.1) is equivalent to the statement that the conditional density of \\(Y\\) given \\(X\\) takes the form\n\\[\nf(y \\mid x)=\\frac{1}{\\left(2 \\pi \\sigma^{2}\\right)^{1 / 2}} \\exp \\left(-\\frac{1}{2 \\sigma^{2}}\\left(y-x^{\\prime} \\beta\\right)^{2}\\right)\n\\]\nUnder the assumption that the observations are mutually independent this implies that the conditional density of \\(\\left(Y_{1}, \\ldots, Y_{n}\\right)\\) given \\(\\left(X_{1}, \\ldots, X_{n}\\right)\\) is\n\\[\n\\begin{aligned}\nf\\left(y_{1}, \\ldots, y_{n} \\mid x_{1}, \\ldots, x_{n}\\right) &=\\prod_{i=1}^{n} f\\left(y_{i} \\mid x_{i}\\right) \\\\\n&=\\prod_{i=1}^{n} \\frac{1}{\\left(2 \\pi \\sigma^{2}\\right)^{1 / 2}} \\exp \\left(-\\frac{1}{2 \\sigma^{2}}\\left(y_{i}-x_{i}^{\\prime} \\beta\\right)^{2}\\right) \\\\\n&=\\frac{1}{\\left(2 \\pi \\sigma^{2}\\right)^{n / 2}} \\exp \\left(-\\frac{1}{2 \\sigma^{2}} \\sum_{i=1}^{n}\\left(y_{i}-x_{i}^{\\prime} \\beta\\right)^{2}\\right) \\\\\n& \\stackrel{\\operatorname{def}}{=} L_{n}\\left(\\beta, \\sigma^{2}\\right) .\n\\end{aligned}\n\\]\nThis is called the likelihood function when evaluated at the sample data.\nFor convenience it is typical to work with the natural logarithm\n\\[\n\\log L_{n}\\left(\\beta, \\sigma^{2}\\right)=-\\frac{n}{2} \\log \\left(2 \\pi \\sigma^{2}\\right)-\\frac{1}{2 \\sigma^{2}} \\sum_{i=1}^{n}\\left(Y_{i}-X_{i}^{\\prime} \\beta\\right)^{2} \\stackrel{\\text { def }}{=} \\ell_{n}\\left(\\beta, \\sigma^{2}\\right)\n\\]\nwhich is called the log-likelihood function.\nThe maximum likelihood estimator (MLE) \\(\\left(\\widehat{\\beta}_{\\mathrm{mle}}, \\widehat{\\sigma}_{\\mathrm{mle}}^{2}\\right)\\) is the value which maximizes the log-likelihood. We can write the maximization problem as\n\\[\n\\left(\\widehat{\\beta}_{\\mathrm{mle}}, \\widehat{\\sigma}_{\\mathrm{mle}}^{2}\\right)=\\underset{\\beta \\in \\mathbb{R}^{k}, \\sigma^{2}>0}{\\operatorname{argmax}} \\ell_{n}\\left(\\beta, \\sigma^{2}\\right) .\n\\]\nIn most applications of maximum likelihood the MLE must be found by numerical methods. However in the case of the normal regression model we can find an explicit expression for \\(\\widehat{\\beta}_{\\text {mle }}\\) and \\(\\widehat{\\sigma}_{\\text {mle }}^{2}\\).\nThe maximizers \\(\\left(\\widehat{\\beta}_{\\text {mle }}, \\widehat{\\sigma}_{\\text {mle }}^{2}\\right)\\) of (5.3) jointly solve the first-order conditions (FOC)\n\\[\n\\begin{aligned}\n&0=\\left.\\frac{\\partial}{\\partial \\beta} \\ell_{n}\\left(\\beta, \\sigma^{2}\\right)\\right|_{\\beta=\\widehat{\\beta}_{\\mathrm{mle}}, \\sigma^{2}=\\widehat{\\sigma}_{\\mathrm{mle}}^{2}}=\\frac{1}{\\widehat{\\sigma}_{\\mathrm{mle}}^{2}} \\sum_{i=1}^{n} X_{i}\\left(Y_{i}-X_{i}^{\\prime} \\widehat{\\beta}_{\\mathrm{mle}}\\right) \\\\\n&0=\\left.\\frac{\\partial}{\\partial \\sigma^{2}} \\ell_{n}\\left(\\beta, \\sigma^{2}\\right)\\right|_{\\beta=\\widehat{\\beta}_{\\mathrm{mle}}, \\sigma^{2}=\\widehat{\\sigma}_{\\mathrm{mle}}^{2}}=-\\frac{n}{2 \\widehat{\\sigma}_{\\mathrm{mle}}^{2}}+\\frac{1}{2 \\widehat{\\sigma}_{\\mathrm{mle}}^{4}} \\sum_{i=1}^{n}\\left(Y_{i}-X_{i}^{\\prime} \\widehat{\\beta}_{\\mathrm{mle}}\\right)^{2} .\n\\end{aligned}\n\\]\nThe first FOC (5.4) is proportional to the first-order conditions for the least squares minimization problem of Section 3.6. It follows that the MLE satisfies\n\\[\n\\widehat{\\beta}_{\\mathrm{mle}}=\\left(\\sum_{i=1}^{n} X_{i} X_{i}^{\\prime}\\right)^{-1}\\left(\\sum_{i=1}^{n} X_{i} Y_{i}\\right)=\\widehat{\\beta}_{\\mathrm{ols}} .\n\\]\nThat is, the MLE for \\(\\beta\\) is algebraically identical to the OLS estimator.\nSolving the second FOC (5.5) for \\(\\widehat{\\sigma}_{\\mathrm{mle}}^{2}\\) we find\n\\[\n\\widehat{\\sigma}_{\\mathrm{mle}}^{2}=\\frac{1}{n} \\sum_{i=1}^{n}\\left(Y_{i}-X_{i}^{\\prime} \\widehat{\\beta}_{\\mathrm{mle}}\\right)^{2}=\\frac{1}{n} \\sum_{i=1}^{n}\\left(Y_{i}-X_{i}^{\\prime} \\widehat{\\beta}_{\\mathrm{ols}}\\right)^{2}=\\frac{1}{n} \\sum_{i=1}^{n} \\widehat{e}_{i}^{2}=\\widehat{\\sigma}_{\\mathrm{ols}}^{2}\n\\]\nThus the MLE for \\(\\sigma^{2}\\) is identical to the OLS/moment estimator from (3.26).\nSince the OLS estimator and MLE under normality are equivalent, \\(\\widehat{\\beta}\\) is described by some authors as the maximum likelihood estimator, and by other authors as the least squares estimator. It is important to remember, however, that \\(\\widehat{\\beta}\\) is only the MLE when the error \\(e\\) has a known normal distribution and not otherwise.\nPlugging the estimators into (5.2) we obtain the maximized log-likelihood\n\\[\n\\ell_{n}\\left(\\widehat{\\beta}_{\\text {mle }}, \\widehat{\\sigma}_{\\text {mle }}^{2}\\right)=-\\frac{n}{2} \\log \\left(2 \\pi \\widehat{\\sigma}_{\\text {mle }}^{2}\\right)-\\frac{n}{2} .\n\\]\nThe log-likelihood is typically reported as a measure of fit.\nIt may seem surprising that the MLE \\(\\widehat{\\beta}_{\\mathrm{mle}}\\) is algebraically equal to the OLS estimator despite emerging from quite different motivations. It is not completely accidental. The least squares estimator minimizes a particular sample loss function - the sum of squared error criterion - and most loss functions are equivalent to the likelihood of a specific parametric distribution, in this case the normal regression model. In this sense it is not surprising that the least squares estimator can be motivated as either the minimizer of a sample loss function or as the maximizer of a likelihood function."
  },
  {
    "objectID": "chpt05-normal-reg.html#distribution-of-ols-coefficient-vector",
    "href": "chpt05-normal-reg.html#distribution-of-ols-coefficient-vector",
    "title": "5  Normal Regression",
    "section": "5.6 Distribution of OLS Coefficient Vector",
    "text": "5.6 Distribution of OLS Coefficient Vector\nIn the normal linear regression model we can derive exact sampling distributions for the OLS/MLE estimator, residuals, and variance estimator. In this section we derive the distribution of the OLS coefficient estimator.\nThe normality assumption \\(e \\mid X \\sim \\mathrm{N}\\left(0, \\sigma^{2}\\right)\\) combined with independence of the observations has the multivariate implication\n\\[\n\\boldsymbol{e} \\mid \\boldsymbol{X} \\sim \\mathrm{N}\\left(0, \\boldsymbol{I}_{n} \\sigma^{2}\\right) .\n\\]\nThat is, the error vector \\(\\boldsymbol{e}\\) is independent of \\(\\boldsymbol{X}\\) and is normally distributed.\nRecall that the OLS estimator satisfies\n\\[\n\\widehat{\\beta}-\\beta=\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\boldsymbol{X}^{\\prime} \\boldsymbol{e}\n\\]\nwhich is a linear function of \\(\\boldsymbol{e}\\). Since linear functions of normals are also normal (Theorem 5.2) this implies that conditional on \\(\\boldsymbol{X}\\),\n\\[\n\\begin{aligned}\n\\widehat{\\beta}-\\beta \\mid \\boldsymbol{X} \\sim\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\boldsymbol{X}^{\\prime} \\mathrm{N}\\left(0, \\boldsymbol{I}_{n} \\sigma^{2}\\right) \\\\\n& \\sim \\mathrm{N}\\left(0, \\sigma^{2}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1}\\right) \\\\\n=\\mathrm{N}\\left(0, \\sigma^{2}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1}\\right) .\n\\end{aligned}\n\\]\nThis shows that under the assumption of normal errors the OLS estimator has an exact normal distribution.\nTheorem 5.4 In the normal regression model,\n\\[\n\\widehat{\\beta} \\mid \\boldsymbol{X} \\sim \\mathrm{N}\\left(\\beta, \\sigma^{2}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1}\\right) .\n\\]\nTheorems \\(5.2\\) and \\(5.4\\) imply that any affine function of the OLS estimator is also normally distributed including individual components. Letting \\(\\beta_{j}\\) and \\(\\widehat{\\beta}_{j}\\) denote the \\(j^{t h}\\) elements of \\(\\beta\\) and \\(\\widehat{\\beta}\\), we have\n\\[\n\\widehat{\\beta}_{j} \\mid \\boldsymbol{X} \\sim \\mathrm{N}\\left(\\beta_{j}, \\sigma^{2}\\left[\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1}\\right]_{j j}\\right)\n\\]\nTheorem \\(5.4\\) is a statement about the conditional distribution. What about the unconditional distribution? In Section \\(4.7\\) we presented Kinal’s theorem about the existence of moments for the joint normal regression model. We re-state the result here.\nTheorem 5.5 Kinal (1980) If \\((Y, X)\\) are jointly normal, then for any \\(r, \\mathbb{E}\\|\\widehat{\\beta}\\|^{r}<\\) \\(\\infty\\) if and only if \\(r<n-k+1\\)."
  },
  {
    "objectID": "chpt05-normal-reg.html#distribution-of-ols-residual-vector",
    "href": "chpt05-normal-reg.html#distribution-of-ols-residual-vector",
    "title": "5  Normal Regression",
    "section": "5.7 Distribution of OLS Residual Vector",
    "text": "5.7 Distribution of OLS Residual Vector\nConsider the OLS residual vector. Recall from (3.24) that \\(\\widehat{\\boldsymbol{e}}=\\boldsymbol{M} \\boldsymbol{e}\\) where \\(\\boldsymbol{M}=\\boldsymbol{I}_{n}-\\boldsymbol{X}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\boldsymbol{X}^{\\prime}\\). This shows that \\(\\widehat{\\boldsymbol{e}}\\) is linear in \\(\\boldsymbol{e}\\). So conditional on \\(\\boldsymbol{X}\\)\n\\[\n\\widehat{\\boldsymbol{e}}=\\boldsymbol{M} \\boldsymbol{e} \\mid \\boldsymbol{X} \\sim \\mathrm{N}\\left(0, \\sigma^{2} \\boldsymbol{M} \\boldsymbol{M}\\right)=\\mathrm{N}\\left(0, \\sigma^{2} \\boldsymbol{M}\\right)\n\\]\nthe final equality because \\(M\\) is idempotent (see Section 3.12). This shows that the residual vector has an exact normal distribution.\nFurthermore, it is useful to find the joint distribution of \\(\\widehat{\\beta}\\) and \\(\\widehat{\\boldsymbol{e}}\\). This is easiest done by writing the two as a stacked linear function of the error \\(\\boldsymbol{e}\\). Indeed,\n\\[\n\\left(\\begin{array}{c}\n\\widehat{\\beta}-\\beta \\\\\n\\widehat{\\boldsymbol{e}}\n\\end{array}\\right)=\\left(\\begin{array}{c}\n\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\boldsymbol{X}^{\\prime} \\boldsymbol{e} \\\\\n\\boldsymbol{M} \\boldsymbol{e}\n\\end{array}\\right)=\\left(\\begin{array}{c}\n\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\boldsymbol{X}^{\\prime} \\\\\n\\boldsymbol{M}\n\\end{array}\\right) \\boldsymbol{e}\n\\]\nwhich is a linear function of \\(\\boldsymbol{e}\\). The vector has a joint normal distribution with covariance matrix\n\\[\n\\left(\\begin{array}{cc}\n\\sigma^{2}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} & 0 \\\\\n0 & \\sigma^{2} \\boldsymbol{M}\n\\end{array}\\right)\n\\]\nThe off-diagonal block is zero because \\(\\boldsymbol{X}^{\\prime} \\boldsymbol{M}=0\\) from (3.21). Since this is zero it follows that \\(\\widehat{\\beta}\\) and \\(\\widehat{\\boldsymbol{e}}\\) are statistically independent (Theorem 5.3.2). Theorem 5.6 In the normal regression model, \\(\\widehat{\\boldsymbol{e}} \\mid \\boldsymbol{X} \\sim \\mathrm{N}\\left(0, \\sigma^{2} \\boldsymbol{M}\\right)\\) and is independent of \\(\\widehat{\\beta}\\).\nThe fact that \\(\\widehat{\\beta}\\) and \\(\\widehat{\\boldsymbol{e}}\\) are independent implies that \\(\\widehat{\\beta}\\) is independent of any function of the residual vector including individual residuals \\(\\widehat{e}_{i}\\) and the variance estimators \\(s^{2}\\) and \\(\\widehat{\\sigma}^{2}\\)."
  },
  {
    "objectID": "chpt05-normal-reg.html#distribution-of-variance-estimator",
    "href": "chpt05-normal-reg.html#distribution-of-variance-estimator",
    "title": "5  Normal Regression",
    "section": "5.8 Distribution of Variance Estimator",
    "text": "5.8 Distribution of Variance Estimator\nNext, consider the variance estimator \\(s^{2}\\) from (4.31). Using (3.28) it satisfies \\((n-k) s^{2}=\\widehat{\\boldsymbol{e}}^{\\prime} \\widehat{\\boldsymbol{e}}=\\boldsymbol{e}^{\\prime} \\boldsymbol{M} \\boldsymbol{e}\\). The spectral decomposition of \\(\\boldsymbol{M}\\) (equation (A.4)) is \\(\\boldsymbol{M}=\\boldsymbol{H} \\Lambda \\boldsymbol{H}^{\\prime}\\) where \\(\\boldsymbol{H}^{\\prime} \\boldsymbol{H}=\\boldsymbol{I}_{n}\\) and \\(\\Lambda\\) is diagonal with the eigenvalues of \\(\\boldsymbol{M}\\) on the diagonal. Since \\(\\boldsymbol{M}\\) is idempotent with rank \\(n-k\\) (see Section 3.12) it has \\(n-k\\) eigenvalues equalling 1 and \\(k\\) eigenvalues equalling 0 , so\n\\[\n\\Lambda=\\left[\\begin{array}{cc}\n\\boldsymbol{I}_{n-k} & \\mathbf{0} \\\\\n\\mathbf{0} & \\mathbf{0}_{k}\n\\end{array}\\right] .\n\\]\nLet \\(\\boldsymbol{u}=\\boldsymbol{H}^{\\prime} \\boldsymbol{e} \\sim \\mathrm{N}\\left(\\mathbf{0}, \\boldsymbol{I}_{n} \\sigma^{2}\\right)\\) (see Exercise 5.2) and partition \\(\\boldsymbol{u}=\\left(\\boldsymbol{u}_{1}^{\\prime}, \\boldsymbol{u}_{2}^{\\prime}\\right)^{\\prime}\\) where \\(\\boldsymbol{u}_{1} \\sim \\mathrm{N}\\left(0, \\boldsymbol{I}_{n-k} \\sigma^{2}\\right)\\). Then\n\\[\n\\begin{aligned}\n(n-k) s^{2} &=\\boldsymbol{e}^{\\prime} \\boldsymbol{M} \\boldsymbol{e} \\\\\n&=\\boldsymbol{e}^{\\prime} \\boldsymbol{H}\\left[\\begin{array}{cc}\n\\boldsymbol{I}_{n-k} & \\mathbf{0} \\\\\n\\mathbf{0} & \\mathbf{0}\n\\end{array}\\right] \\boldsymbol{H}^{\\prime} \\boldsymbol{e} \\\\\n&=\\boldsymbol{u}^{\\prime}\\left[\\begin{array}{cc}\n\\boldsymbol{I}_{n-k} & \\mathbf{0} \\\\\n\\mathbf{0} & \\mathbf{0}\n\\end{array}\\right] \\boldsymbol{u} \\\\\n&=\\boldsymbol{u}_{1}^{\\prime} \\boldsymbol{u}_{1} \\\\\n& \\sim \\sigma^{2} \\chi_{n-k}^{2} .\n\\end{aligned}\n\\]\nWe see that in the normal regression model the exact distribution of \\(s^{2}\\) is a scaled chi-square.\nSince \\(\\widehat{\\boldsymbol{e}}\\) is independent of \\(\\widehat{\\beta}\\) it follows that \\(s^{2}\\) is independent of \\(\\widehat{\\beta}\\) as well.\nTheorem 5.7 In the normal regression model,\n\\[\n\\frac{(n-k) s^{2}}{\\sigma^{2}} \\sim \\chi_{n-k}^{2}\n\\]\nand is independent of \\(\\widehat{\\beta}\\)."
  },
  {
    "objectID": "chpt05-normal-reg.html#t-statistic",
    "href": "chpt05-normal-reg.html#t-statistic",
    "title": "5  Normal Regression",
    "section": "5.9 t-statistic",
    "text": "5.9 t-statistic\nAn alternative way of writing (5.7) is\n\\[\n\\frac{\\widehat{\\beta}_{j}-\\beta_{j}}{\\sqrt{\\sigma^{2}\\left[\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1}\\right]_{j j}}} \\sim \\mathrm{N}(0,1) .\n\\]\nThis is sometimes called a standardized statistic as the distribution is the standard normal.\nNow take the standardized statistic and replace the unknown variance \\(\\sigma^{2}\\) with its estimator \\(s^{2}\\). We call this a t-ratio or t-statistic\n\\[\nT=\\frac{\\widehat{\\beta}_{j}-\\beta_{j}}{\\sqrt{s^{2}\\left[\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1}\\right]_{j j}}}=\\frac{\\widehat{\\beta}_{j}-\\beta_{j}}{s\\left(\\widehat{\\beta}_{j}\\right)}\n\\]\nwhere \\(s\\left(\\widehat{\\beta}_{j}\\right)\\) is the classical (homoskedastic) standard error for \\(\\widehat{\\beta}_{j}\\) from (4.42). We will sometimes write the t-statistic as \\(T\\left(\\beta_{j}\\right)\\) to explicitly indicate its dependence on the parameter value \\(\\beta_{j}\\), and sometimes will simplify notation and write the \\(\\mathrm{t}\\)-statistic as \\(T\\) when the dependence is clear from the context.\nWith algebraic re-scaling we can write the t-statistic as the ratio of the standardized statistic and the square root of the scaled variance estimator. Since the distributions of these two components are normal and chi-square, respectively, and independent, we deduce that the t-statistic has the distribution\n\\[\n\\begin{aligned}\nT &=\\frac{\\widehat{\\beta}_{j}-\\beta_{j}}{\\sqrt{\\sigma^{2}\\left[\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1}\\right]_{j j}}} / \\sqrt{\\frac{(n-k) s^{2}}{\\sigma^{2}} /(n-k)} \\\\\n& \\sim \\frac{\\mathrm{N}(0,1)}{\\sqrt{\\chi_{n-k}^{2} /(n-k)}} \\\\\n& \\sim t_{n-k}\n\\end{aligned}\n\\]\na student \\(t\\) distribution with \\(n-k\\) degrees of freedom.\nThis derivation shows that the t-ratio has a sampling distribution which depends only on the quantity \\(n-k\\). The distribution does not depend on any other features of the data. In this context, we say that the distribution of the t-ratio is pivotal, meaning that it does not depend on unknowns.\nThe trick behind this result is scaling the centered coefficient by its standard error, and recognizing that each depends on the unknown \\(\\sigma\\) only through scale. Thus the ratio of the two does not depend on \\(\\sigma\\). This trick (scaling to eliminate dependence on unknowns) is known as studentization.\nTheorem 5.8 In the normal regression model, \\(T \\sim t_{n-k}\\).\nAn important caveat about Theorem \\(5.8\\) is that it only applies to the t-statistic constructed with the homoskedastic (old-fashioned) standard error. It does not apply to a t-statistic constructed with any of the robust standard errors. In fact, the robust t-statistics can have finite sample distributions which deviate considerably from \\(t_{n-k}\\) even when the regression errors are independent \\(\\mathrm{N}\\left(0, \\sigma^{2}\\right)\\). Thus the distributional result in Theorem \\(5.8\\) and the use of the t distribution in finite samples is only exact when applied to classical t-statistics under the normality assumption."
  },
  {
    "objectID": "chpt05-normal-reg.html#confidence-intervals-for-regression-coefficients",
    "href": "chpt05-normal-reg.html#confidence-intervals-for-regression-coefficients",
    "title": "5  Normal Regression",
    "section": "5.10 Confidence Intervals for Regression Coefficients",
    "text": "5.10 Confidence Intervals for Regression Coefficients\nThe OLS estimator \\(\\widehat{\\beta}\\) is a point estimator for a coefficient \\(\\beta\\). A broader concept is a set or interval estimator which takes the form \\(\\widehat{C}=[\\widehat{L}, \\widehat{U}]\\). The goal of an interval estimator \\(\\widehat{C}\\) is to contain the true value, e.g. \\(\\beta \\in \\widehat{C}\\), with high probability.\nThe interval estimator \\(\\widehat{C}\\) is a function of the data and hence is random. An interval estimator \\(\\widehat{C}\\) is called a \\(1-\\alpha\\) confidence interval when \\(\\mathbb{P}[\\beta \\in \\widehat{C}]=1-\\alpha\\) for a selected value of \\(\\alpha\\). The value \\(1-\\alpha\\) is called the coverage probability. Typical choices for the coverage probability \\(1-\\alpha\\) are \\(0.95\\) or \\(0.90\\).\nThe probability calculation \\(\\mathbb{P}[\\beta \\in \\widehat{C}]\\) is easily mis-interpreted as treating \\(\\beta\\) as random and \\(\\widehat{C}\\) as fixed. (The probability that \\(\\beta\\) is in \\(\\widehat{C}\\).) This is not the appropriate interpretation. Instead, the correct interpretation is that the probability \\(\\mathbb{P}[\\beta \\in \\widehat{C}]\\) treats the point \\(\\beta\\) as fixed and the set \\(\\widehat{C}\\) as random. It is the probability that the random set \\(\\widehat{C}\\) covers (or contains) the fixed true coefficient \\(\\beta\\).\nThere is not a unique method to construct confidence intervals. For example, one simple (yet silly) interval is\n\\[\n\\widehat{C}=\\left\\{\\begin{array}{cc}\n\\mathbb{R} & \\text { with probability } 1-\\alpha \\\\\n\\{\\widehat{\\beta}\\} & \\text { with probability } \\alpha .\n\\end{array}\\right.\n\\]\nIf \\(\\widehat{\\beta}\\) has a continuous distribution, then by construction \\(\\mathbb{P}[\\beta \\in \\widehat{C}]=1-\\alpha\\), so this confidence interval has perfect coverage. However, \\(\\widehat{C}\\) is uninformative about \\(\\widehat{\\beta}\\) and is therefore not useful.\nInstead, a good choice for a confidence interval for the regression coefficient \\(\\beta\\) is obtained by adding and subtracting from the estimator \\(\\widehat{\\beta}\\) a fixed multiple of its standard error:\n\\[\n\\widehat{C}=[\\widehat{\\beta}-c \\times s(\\widehat{\\beta}), \\quad \\widehat{\\beta}+c \\times s(\\widehat{\\beta})]\n\\]\nwhere \\(c>0\\) is a pre-specified constant. This confidence interval is symmetric about the point estimator \\(\\widehat{\\beta}\\) and its length is proportional to the standard error \\(s(\\widehat{\\beta})\\).\nEquivalently, \\(\\widehat{C}\\) is the set of parameter values for \\(\\beta\\) such that the t-statistic \\(T(\\beta)\\) is smaller (in absolute value) than \\(c\\), that is\n\\[\n\\widehat{C}=\\{\\beta:|T(\\beta)| \\leq c\\}=\\left\\{\\beta:-c \\leq \\frac{\\widehat{\\beta}-\\beta}{s(\\widehat{\\beta})} \\leq c\\right\\} .\n\\]\nThe coverage probability of this confidence interval is\n\\[\n\\begin{aligned}\n\\mathbb{P}[\\beta \\in \\widehat{C}] &=\\mathbb{P}[|T(\\beta)| \\leq c] \\\\\n&=\\mathbb{P}[-c \\leq T(\\beta) \\leq c] .\n\\end{aligned}\n\\]\nSince the t-statistic \\(T(\\beta)\\) has the \\(t_{n-k}\\) distribution, (5.9) equals \\(F(c)-F(-c)\\), where \\(F(u)\\) is the student \\(t\\) distribution function with \\(n-k\\) degrees of freedom. Since \\(F(-c)=1-F(c)\\) (see Exercise 5.8), we can write (5.9) as\n\\[\n\\mathbb{P}[\\beta \\in \\widehat{C}]=2 F(c)-1 .\n\\]\nThis is the coverage probability of the interval \\(\\widehat{C}\\), and only depends on the constant \\(c\\).\nAs we mentioned before, a confidence interval has the coverage probability \\(1-\\alpha\\). This requires selecting the constant \\(c\\) so that \\(F(c)=1-\\alpha / 2\\). This holds if \\(c\\) equals the \\(1-\\alpha / 2\\) quantile of the \\(t_{n-k}\\) distribution. As there is no closed form expression for these quantiles we compute their values numerically. For example, by tinv (1-alpha/2,n-k) in MATLAB. With this choice the confidence interval (5.8) has exact coverage probability \\(1-\\alpha\\). By default, Stata reports \\(95 %\\) confidence intervals \\(\\widehat{C}\\) for each estimated regression coefficient using the same formula.\nTheorem 5.9 In the normal regression model, (5.8) with \\(c=F^{-1}(1-\\alpha / 2)\\) has coverage probability \\(\\mathbb{P}[\\beta \\in \\widehat{C}]=1-\\alpha\\). When the degree of freedom is large the distinction between the student \\(t\\) and the normal distribution is negligible. In particular, for \\(n-k \\geq 61\\) we have \\(c \\leq 2.00\\) for a \\(95 %\\) interval. Using this value we obtain the most commonly used confidence interval in applied econometric practice:\n\\[\n\\widehat{C}=[\\widehat{\\beta}-2 s(\\widehat{\\beta}), \\quad \\widehat{\\beta}+2 s(\\widehat{\\beta})] .\n\\]\nThis is a useful rule-of-thumb. This \\(95 %\\) confidence interval \\(\\widehat{C}\\) is simple to compute and can be easily calculated from coefficient estimates and standard errors.\nTheorem 5.10 In the normal regression model, if \\(n-k \\geq 61\\) then (5.10) has coverage probability \\(\\mathbb{P}[\\beta \\in \\widehat{C}] \\geq 0.95\\).\nConfidence intervals are a simple yet effective tool to assess estimation uncertainty. When reading a set of empirical results look at the estimated coefficient estimates and the standard errors. For a parameter of interest compute the confidence interval \\(\\widehat{C}\\) and consider the meaning of the spread of the suggested values. If the range of values in the confidence interval are too wide to learn about \\(\\beta\\) then do not jump to a conclusion about \\(\\beta\\) based on the point estimate alone."
  },
  {
    "objectID": "chpt05-normal-reg.html#confidence-intervals-for-error-variance",
    "href": "chpt05-normal-reg.html#confidence-intervals-for-error-variance",
    "title": "5  Normal Regression",
    "section": "5.11 Confidence Intervals for Error Variance",
    "text": "5.11 Confidence Intervals for Error Variance\nWe can also construct a confidence interval for the regression error variance \\(\\sigma^{2}\\) using the sampling distribution of \\(s^{2}\\) from Theorem 5.7. This states that in the normal regression model\n\\[\n\\frac{(n-k) s^{2}}{\\sigma^{2}} \\sim \\chi_{n-k}^{2} .\n\\]\nLet \\(F(u)\\) denote the \\(\\chi_{n-k}^{2}\\) distribution function and for some \\(\\alpha\\) set \\(c_{1}=F^{-1}(\\alpha / 2)\\) and \\(c_{2}=F^{-1}(1-\\alpha / 2)\\) (the \\(\\alpha / 2\\) and \\(1-\\alpha / 2\\) quantiles of the \\(\\chi_{n-k}^{2}\\) distribution). Equation (5.11) implies that\n\\[\n\\mathbb{P}\\left[c_{1} \\leq \\frac{(n-k) s^{2}}{\\sigma^{2}} \\leq c_{2}\\right]=F\\left(c_{2}\\right)-F\\left(c_{1}\\right)=1-\\alpha .\n\\]\nRewriting the inequalities we find\n\\[\n\\mathbb{P}\\left[\\frac{(n-k) s^{2}}{c_{2}} \\leq \\sigma^{2} \\leq \\frac{(n-k) s^{2}}{c_{1}}\\right]=1-\\alpha .\n\\]\nThis shows that an exact \\(1-\\alpha\\) confidence interval for \\(\\sigma^{2}\\) is\n\\[\n\\widehat{C}=\\left[\\frac{(n-k) s^{2}}{c_{2}}, \\quad \\frac{(n-k) s^{2}}{c_{1}}\\right] .\n\\]\nTheorem 5.11 In the normal regression model (5.12) has coverage probability \\(\\mathbb{P}\\left[\\sigma^{2} \\in \\widehat{C}\\right]=1-\\alpha\\).\nThe confidence interval (5.12) for \\(\\sigma^{2}\\) is asymmetric about the point estimate \\(s^{2}\\) due to the latter’s asymmetric sampling distribution."
  },
  {
    "objectID": "chpt05-normal-reg.html#t-test",
    "href": "chpt05-normal-reg.html#t-test",
    "title": "5  Normal Regression",
    "section": "5.12 t Test",
    "text": "5.12 t Test\nA typical goal in an econometric exercise is to assess whether or not a coefficient \\(\\beta\\) equals a specific value \\(\\beta_{0}\\). Often the specific value to be tested is \\(\\beta_{0}=0\\) but this is not essential. This is called hypothesis testing, a subject which will be explored in detail in Chapter 9. In this section and the following we give a short introduction specific to the normal regression model.\nFor simplicity write the coefficient to be tested as \\(\\beta\\). The null hypothesis is\n\\[\n\\mathbb{M}_{0}: \\beta=\\beta_{0} .\n\\]\nThis states that the hypothesis is that the true value of \\(\\beta\\) equals the hypothesized value \\(\\beta_{0}\\).\nThe alternative hypothesis is the complement of \\(\\mathbb{M}_{0}\\), and is written as\n\\[\n\\mathbb{H}_{1}: \\beta \\neq \\beta_{0} .\n\\]\nThis states that the true value of \\(\\beta\\) does not equal the hypothesized value.\nWe are interested in testing \\(\\mathbb{M}_{0}\\) against \\(\\mathbb{M}_{1}\\). The method is to design a statistic which is informative about \\(\\mathbb{M}_{1}\\). If the observed value of the statistic is consistent with random variation under the assumption that \\(\\mathbb{M}_{0}\\) is true, then we deduce that there is no evidence against \\(\\mathbb{H}_{0}\\) and consequently do not reject \\(\\mathbb{H}_{0}\\). However, if the statistic takes a value which is unlikely to occur under the assumption that \\(\\mathbb{M}_{0}\\) is true, then we deduce that there is evidence against \\(\\mathbb{M}_{0}\\) and consequently we reject \\(\\mathbb{M}_{0}\\) in favor of \\(\\mathbb{M}_{1}\\). The main steps are to design a test statistic and to characterize its sampling distribution.\nThe standard statistic to test \\(\\mathbb{M}_{0}\\) against \\(\\mathbb{M}_{1}\\) is the absolute value of the t-statistic\n\\[\n|T|=\\left|\\frac{\\widehat{\\beta}-\\beta_{0}}{s(\\widehat{\\beta})}\\right| .\n\\]\nIf \\(\\mathbb{M}_{0}\\) is true then we expect \\(|T|\\) to be small, but if \\(\\mathbb{M}_{1}\\) is true then we would expect \\(|T|\\) to be large. Hence the standard rule is to reject \\(\\mathbb{M}_{0}\\) in favor of \\(\\mathbb{M}_{1}\\) for large values of the t-statistic \\(|T|\\) and otherwise fail to reject \\(\\mathbb{H}_{0}\\). Thus the hypothesis test takes the form\n\\[\n\\text { Reject } \\mathbb{M}_{0} \\text { if }|T|>c \\text {. }\n\\]\nThe constant \\(c\\) which appears in the statement of the test is called the critical value. Its value is selected to control the probability of false rejections. When the null hypothesis is true \\(T\\) has an exact \\(t_{n-k}\\) distribution in the normal regression model. Thus for a given value of \\(c\\) the probability of false rejection is\n\\[\n\\begin{aligned}\n\\mathbb{P}\\left[\\text { Reject } \\mathbb{H}_{0} \\mid \\mathbb{B}_{0}\\right] &=\\mathbb{P}\\left[|T|>c \\mid \\mathbb{M}_{0}\\right] \\\\\n&=\\mathbb{P}\\left[T>c \\mid \\mathbb{H}_{0}\\right]+\\mathbb{P}\\left[T<-c \\mid \\mathbb{M}_{0}\\right] \\\\\n&=1-F(c)+F(-c) \\\\\n&=2(1-F(c))\n\\end{aligned}\n\\]\nwhere \\(F(u)\\) is the \\(t_{n-k}\\) distribution function. This is the probability of false rejection and is decreasing in the critical value \\(c\\). We select the value \\(c\\) so that this probability equals a pre-selected value called the significance level which is typically written as \\(\\alpha\\). It is conventional to set \\(\\alpha=0.05\\), though this is not a hard rule. We then select \\(c\\) so that \\(F(c)=1-\\alpha / 2\\), which means that \\(c\\) is the \\(1-\\alpha / 2\\) quantile (inverse CDF) of the \\(t_{n-k}\\) distribution, the same as used for confidence intervals. With this choice the decision rule “Reject \\(\\mathbb{M}_{0}\\) if \\(|T|>c\\)” has a significance level (false rejection probability) of \\(\\alpha\\). Theorem 5.12 In the normal regression model if the null hypothesis (5.13) is true, then for \\(|T|\\) defined in (5.14) \\(T \\sim t_{n-k}\\). If \\(c\\) is set so that \\(\\mathbb{P}\\left[\\left|t_{n-k}\\right| \\geq c\\right]=\\) \\(\\alpha\\),then the test “Reject \\(\\mathbb{M}_{0}\\) in favor of \\(\\mathbb{M}_{1}\\) if \\(|T|>c\\)” has significance level \\(\\alpha\\).\nTo report the result of a hypothesis test we need to pre-determine the significance level \\(\\alpha\\) in order to calculate the critical value \\(c\\). This can be inconvenient and arbitrary. A simplification is to report what is known as the p-value of the test. In general, when a test takes the form “Reject \\(\\mathbb{B}_{0}\\) if \\(S>c\\)” and \\(S\\) has null distribution \\(G(u)\\) then the p-value of the test is \\(p=1-G(S)\\). A test with significance level \\(\\alpha\\) can be restated as “Reject \\(\\mathbb{M}_{0}\\) if \\(p<\\alpha\\)”. It is sufficient to report the p-value \\(p\\) and we can interpret the value of \\(p\\) as indexing the test’s strength of rejection of the null hypothesis. Thus a \\(\\mathrm{p}\\)-value of \\(0.07\\) might be interpreted as “nearly significant”, \\(0.05\\) as “borderline significant”, and \\(0.001\\) as “highly significant”. In the context of the normal regression model the p-value of a t-statistic \\(|T|\\) is \\(p=2\\left(1-F_{n-k}(|T|)\\right)\\) where \\(F_{n-k}\\) is the \\(t_{n-k}\\) CDF. For example, in MATLAB the calculation is \\(2 *(1-\\mathrm{t} c d f(\\mathrm{abs}(\\mathrm{t}), \\mathrm{n}-\\mathrm{k}))\\). In Stata, the default is that for any estimated regression, t-statistics for each estimated coefficient are reported along with their p-values calculated using this same formula. These t-statistics test the hypotheses that each coefficient is zero.\nA p-value reports the strength of evidence against \\(\\mathbb{M}_{0}\\) but is not itself a probability. A common misunderstanding is that the p-value is the “probability that the null hypothesis is true”. This is an incorrect interpretation. It is a statistic, is random, and is a measure of the evidence against \\(\\mathbb{M}_{0}\\). Nothing more."
  },
  {
    "objectID": "chpt05-normal-reg.html#likelihood-ratio-test",
    "href": "chpt05-normal-reg.html#likelihood-ratio-test",
    "title": "5  Normal Regression",
    "section": "5.13 Likelihood Ratio Test",
    "text": "5.13 Likelihood Ratio Test\nIn the previous section we described the t-test as the standard method to test a hypothesis on a single coefficient in a regression. In many contexts, however, we want to simultaneously assess a set of coefficients. In the normal regression model, this can be done by an \\(F\\) test which can be derived from the likelihood ratio test.\nPartition the regressors as \\(X=\\left(X_{1}^{\\prime}, X_{2}^{\\prime}\\right)\\) and similarly partition the coefficient vector as \\(\\beta=\\left(\\beta_{1}^{\\prime}, \\beta_{2}^{\\prime}\\right)^{\\prime}\\). The regression model can be written as\n\\[\nY=X_{1}^{\\prime} \\beta_{1}+X_{2}^{\\prime} \\beta_{2}+e .\n\\]\nLet \\(k=\\operatorname{dim}(X), k_{1}=\\operatorname{dim}\\left(X_{1}\\right)\\), and \\(q=\\operatorname{dim}\\left(X_{2}\\right)\\), so that \\(k=k_{1}+q\\). Partition the variables so that the hypothesis is that the second set of coefficients are zero, or\n\\[\n\\mathbb{H}_{0}: \\beta_{2}=0 .\n\\]\nIf \\(\\mathbb{M}_{0}\\) is true then the regressors \\(X_{2}\\) can be omitted from the regression. In this case we can write (5.15) as\n\\[\nY=X_{1}^{\\prime} \\beta_{1}+e .\n\\]\nWe call (5.17) the null model. The alternative hypothesis is that at least one element of \\(\\beta_{2}\\) is non-zero and is written as \\(\\mathbb{H}_{1}: \\beta_{2} \\neq 0\\).\nWhen models are estimated by maximum likelihood a well-accepted testing procedure is to reject \\(\\mathbb{M}_{0}\\) in favor of \\(\\mathbb{M}_{1}\\) for large values of the Likelihood Ratio - the ratio of the maximized likelihood function under \\(\\mathbb{H}_{1}\\) and \\(\\mathbb{H}_{0}\\), respectively. We now construct this statistic in the normal regression model. Recall from (5.6) that the maximized log-likelihood equals\n\\[\n\\ell_{n}\\left(\\widehat{\\beta}, \\widehat{\\sigma}^{2}\\right)=-\\frac{n}{2} \\log \\left(2 \\pi \\widehat{\\sigma}^{2}\\right)-\\frac{n}{2} .\n\\]\nWe similarly calculate the maximized log-likelihood for the constrained model (5.17). By the same steps for derivation of the unconstrained MLE we find that the MLE for (5.17) is OLS of \\(Y\\) on \\(X_{1}\\). We can write this estimator as\n\\[\n\\widetilde{\\beta}_{1}=\\left(\\boldsymbol{X}_{1}^{\\prime} \\boldsymbol{X}_{1}\\right)^{-1} \\boldsymbol{X}_{1}^{\\prime} \\boldsymbol{Y}\n\\]\nwith residual \\(\\widetilde{e}_{i}=Y_{i}-X_{1 i}^{\\prime} \\widetilde{\\beta}_{1}\\) and error variance estimate \\(\\widetilde{\\sigma}^{2}=\\frac{1}{n} \\sum_{i=1}^{n} \\widetilde{e}_{i}^{2}\\). We use tildes ” \\(\\sim\\) ” rather than hats ” \\(\\wedge\\) ” above the constrained estimates to distinguish them from the unconstrained estimates. You can calculate similar to (5.6) that the maximized constrained log-likelihood is\n\\[\n\\ell_{n}\\left(\\widetilde{\\beta}_{1}, \\widetilde{\\sigma}^{2}\\right)=-\\frac{n}{2} \\log \\left(2 \\pi \\widetilde{\\sigma}^{2}\\right)-\\frac{n}{2} .\n\\]\nA classic testing procedure is to reject \\(\\mathbb{H}_{0}\\) for large values of the ratio of the maximized likelihoods. Equivalently the test rejects \\(\\mathbb{H}_{0}\\) for large values of twice the difference in the log-likelihood functions. (Multiplying the likelihood difference by two turns out to be a useful scaling.) This equals\n\\[\n\\begin{aligned}\n\\mathrm{LR} &=2\\left(\\ell_{n}\\left(\\widehat{\\beta}, \\widehat{\\sigma}^{2}\\right)-\\ell_{n}\\left(\\widetilde{\\beta}_{1}, \\widetilde{\\sigma}^{2}\\right)\\right) \\\\\n&=2\\left(\\left(-\\frac{n}{2} \\log \\left(2 \\pi \\widehat{\\sigma}^{2}\\right)-\\frac{n}{2}\\right)-\\left(-\\frac{n}{2} \\log \\left(2 \\pi \\widetilde{\\sigma}^{2}\\right)-\\frac{n}{2}\\right)\\right) \\\\\n&=n \\log \\left(\\frac{\\widetilde{\\sigma}^{2}}{\\widehat{\\sigma}^{2}}\\right) .\n\\end{aligned}\n\\]\nThe likelihood ratio test rejects \\(\\mathbb{H}_{0}\\) for large values of LR, or equivalently (see Exercise \\(5.10\\) ) for large values of\n\\[\n\\mathrm{F}=\\frac{\\left(\\widetilde{\\sigma}^{2}-\\widehat{\\sigma}^{2}\\right) / q}{\\widehat{\\sigma}^{2} /(n-k)} .\n\\]\nThis is known as the \\(F\\) statistic for the test of hypothesis \\(\\mathbb{M}_{0}\\) against \\(\\mathbb{M}_{1}\\).\nTo develop an appropriate critical value we need the null distribution of \\(F\\). Recall from (3.28) that \\(n \\widehat{\\sigma}^{2}=\\boldsymbol{e}^{\\prime} \\boldsymbol{M} \\boldsymbol{e}\\) where \\(\\boldsymbol{M}=\\boldsymbol{I}_{n}-\\boldsymbol{P}\\) with \\(\\boldsymbol{P}=\\boldsymbol{X}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\boldsymbol{X}^{\\prime}\\). Similarly, under \\(\\mathbb{H}_{0}, n \\widetilde{\\sigma}^{2}=\\boldsymbol{e}^{\\prime} \\boldsymbol{M}_{1} \\boldsymbol{e}\\) where \\(\\boldsymbol{M}=\\) \\(\\boldsymbol{I}_{n}-\\boldsymbol{P}_{1}\\) with \\(\\boldsymbol{P}_{1}=\\boldsymbol{X}_{1}\\left(\\boldsymbol{X}_{1}^{\\prime} \\boldsymbol{X}_{1}\\right)^{-1} \\boldsymbol{X}_{1}^{\\prime}\\). You can calculate that \\(\\boldsymbol{M}_{1}-\\boldsymbol{M}=\\boldsymbol{P}-\\boldsymbol{P}_{1}\\) is idempotent with rank \\(q\\). Furthermore, \\(\\left(\\boldsymbol{M}_{1}-\\boldsymbol{M}\\right) \\boldsymbol{M}=0\\). It follows that \\(\\boldsymbol{e}^{\\prime}\\left(\\boldsymbol{M}_{1}-\\boldsymbol{M}\\right) \\boldsymbol{e} \\sim \\chi_{q}^{2}\\) and is independent of \\(\\boldsymbol{e}^{\\prime} \\boldsymbol{M} \\boldsymbol{e}\\). Hence\n\\[\n\\mathrm{F}=\\frac{\\boldsymbol{e}^{\\prime}\\left(\\boldsymbol{M}_{1}-\\boldsymbol{M}\\right) \\boldsymbol{e} / q}{\\boldsymbol{e}^{\\prime} \\boldsymbol{M} \\boldsymbol{e} /(n-k)} \\sim \\frac{\\chi_{q}^{2} / q}{\\chi_{n-k}^{2} /(n-k)} \\sim F_{q, n-k}\n\\]\nan exact \\(F\\) distribution with degrees of freedom \\(q\\) and \\(n-k\\), respectively. Thus under \\(\\mathbb{H}_{0}\\), the \\(F\\) statistic has an exact \\(F\\) distribution.\nThe critical values are selected from the upper tail of the \\(F\\) distribution. For a given significance level \\(\\alpha\\) (typically \\(\\alpha=0.05\\) ) we select the critical value \\(c\\) so that \\(\\mathbb{P}\\left[F_{q, n-k} \\geq c\\right]=\\alpha\\). For example, in MATLAB the expression is \\(f \\operatorname{inv}(1-\\alpha, \\mathrm{q}, \\mathrm{n}-\\mathrm{k})\\). The test rejects \\(\\mathbb{M}_{0}\\) in favor of \\(\\mathbb{B}_{1}\\) if \\(F>c\\) and does not reject \\(\\mathbb{H}_{0}\\) otherwise. The p-value of the test is \\(p=1-G_{q, n-k}(F)\\) where \\(G_{q, n-k}(u)\\) is the \\(F_{q, n-k}\\) distribution function. In MATLAB, the p-value is computed as \\(1-\\mathrm{f} c d f(\\mathrm{f}, \\mathrm{q}, \\mathrm{n}-\\mathrm{k})\\). It is equivalent to reject \\(\\mathbb{H}_{0}\\) if \\(F>c\\) or \\(p<\\alpha\\).\nIn Stata, the command to test multiple coefficients takes the form ‘test X1 X2’ where X1 and X2 are the names of the variables whose coefficients are tested. Stata then reports the F statistic for the hypothesis that the coefficients are jointly zero along with the p-value calculated using the \\(F\\) distribution.\nTheorem 5.13 In the normal regression model, if the null hypothesis (5.16) is true, then for \\(F\\) defined in (5.19), \\(F \\sim F_{q, n-k}\\). If \\(c\\) is set so that \\(\\mathbb{P}\\left[F_{q, n-k} \\geq c\\right]=\\alpha\\) then the test “Reject \\(\\mathbb{M}_{0}\\) in favor of \\(\\mathbb{M}_{1}\\) if \\(F>c\\)” has significance level \\(\\alpha\\). Theorem \\(5.13\\) justifies the \\(F\\) test in the normal regression model with critical values from the \\(F\\) distribution."
  },
  {
    "objectID": "chpt05-normal-reg.html#information-bound-for-normal-regression",
    "href": "chpt05-normal-reg.html#information-bound-for-normal-regression",
    "title": "5  Normal Regression",
    "section": "5.14 Information Bound for Normal Regression",
    "text": "5.14 Information Bound for Normal Regression\nThis section requires a familiarity with the theory of the Cramér-Rao Lower Bound. See Chapter 10 of Probability and Statistics for Economists.\nThe likelihood scores for the normal regression model are\n\\[\n\\frac{\\partial}{\\partial \\beta} \\ell_{n}\\left(\\beta, \\sigma^{2}\\right)=\\frac{1}{\\sigma^{2}} \\sum_{i=1}^{n} X_{i}\\left(Y_{i}-X_{i}^{\\prime} \\beta\\right)=\\frac{1}{\\sigma^{2}} \\sum_{i=1}^{n} X_{i} e_{i}\n\\]\nand\n\\[\n\\frac{\\partial}{\\partial \\sigma^{2}} \\ell_{n}\\left(\\beta, \\sigma^{2}\\right)=-\\frac{n}{2 \\sigma^{2}}+\\frac{1}{2 \\sigma^{4}} \\sum_{i=1}^{n}\\left(Y_{i}-X_{i}^{\\prime} \\beta\\right)^{2}=\\frac{1}{2 \\sigma^{4}} \\sum_{i=1}^{n}\\left(e_{i}^{2}-\\sigma^{2}\\right)\n\\]\nIt follows that the information matrix is\n\\[\n\\mathscr{I}=\\operatorname{var}\\left[\\begin{array}{c}\n\\frac{\\partial}{\\partial \\beta} \\ell\\left(\\beta, \\sigma^{2}\\right) \\\\\n\\frac{\\partial}{\\partial \\sigma^{2}} \\ell\\left(\\beta, \\sigma^{2}\\right)\n\\end{array} \\mid \\boldsymbol{X}\\right]=\\left(\\begin{array}{cc}\n\\frac{1}{\\sigma^{2}} \\boldsymbol{X}^{\\prime} \\boldsymbol{X} & 0 \\\\\n0 & \\frac{2 \\sigma^{4}}{n}\n\\end{array}\\right)\n\\]\n(see Exercise 5.11). The Cramér-Rao Lower Bound is\n\\[\n\\mathscr{I}^{-1}=\\left(\\begin{array}{cc}\n\\sigma^{2}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} & 0 \\\\\n0 & \\frac{2 \\sigma^{4}}{n}\n\\end{array}\\right)\n\\]\nThis shows that the lower bound for estimation of \\(\\beta\\) is \\(\\sigma^{2}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1}\\) and the lower bound for \\(\\sigma^{2}\\) is \\(2 \\sigma^{4} / n\\).\nThe unbiased variance estimator \\(s^{2}\\) of \\(\\sigma^{2}\\) has variance \\(2 \\sigma^{4} /(n-k)\\) (see Exercise 5.12) which is larger than the Cramér-Rao lower bound \\(2 \\sigma^{4} / n\\). Thus in contrast to the coefficient estimator, the variance estimator is not Cramér-Rao efficient."
  },
  {
    "objectID": "chpt05-normal-reg.html#exercises",
    "href": "chpt05-normal-reg.html#exercises",
    "title": "5  Normal Regression",
    "section": "5.15 Exercises",
    "text": "5.15 Exercises\nExercise 5.1 Show that if \\(Q \\sim \\chi_{r}^{2}\\), then \\(\\mathbb{E}[Q]=r\\) and \\(\\operatorname{var}[Q]=2 r\\).\nHint: Use the representation \\(Q=\\sum_{i=1}^{n} Z_{i}^{2}\\) with \\(Z_{i}\\) independent \\(\\mathrm{N}(0,1)\\).\nExercise 5.2 Show that if \\(\\boldsymbol{e} \\sim \\mathrm{N}\\left(0, \\boldsymbol{I}_{n} \\sigma^{2}\\right)\\) and \\(\\boldsymbol{H}^{\\prime} \\boldsymbol{H}=\\boldsymbol{I}_{n}\\) then \\(\\boldsymbol{u}=\\boldsymbol{H}^{\\prime} \\boldsymbol{e} \\sim \\mathrm{N}\\left(0, \\boldsymbol{I}_{n} \\sigma^{2}\\right)\\).\nExercise 5.3 Show that if \\(\\boldsymbol{e} \\sim \\mathrm{N}(0, \\Sigma)\\) and \\(\\Sigma=\\boldsymbol{A} \\boldsymbol{A}^{\\prime}\\) then \\(\\boldsymbol{u}=\\boldsymbol{A}^{-1} \\boldsymbol{e} \\sim \\mathrm{N}\\left(0, \\boldsymbol{I}_{n}\\right)\\).\nExercise 5.4 Show that \\(\\operatorname{argmax}_{\\theta \\in \\Theta} \\ell_{n}(\\theta)=\\operatorname{argmax}_{\\theta \\in \\Theta} L_{n}(\\theta)\\).\nExercise 5.5 For the regression in-sample predicted values \\(\\widehat{Y}_{i}\\) show that \\(\\widehat{Y}_{i} \\mid \\boldsymbol{X} \\sim \\mathrm{N}\\left(X_{i}^{\\prime} \\beta, \\sigma^{2} h_{i i}\\right)\\) where \\(h_{i i}\\) are the leverage values (3.40).\nExercise 5.6 In the normal regression model show that the leave-one out prediction errors \\(\\widetilde{e}_{i}\\) and the standardized residuals \\(\\bar{e}_{i}\\) are independent of \\(\\widehat{\\beta}\\), conditional on \\(\\boldsymbol{X}\\).\nHint: Use (3.45) and (4.29). Exercise 5.7 In the normal regression model show that the robust covariance matrices \\(\\widehat{\\boldsymbol{V}}_{\\widehat{\\beta}}^{\\mathrm{HC} 0}, \\widehat{\\boldsymbol{V}}_{\\widehat{\\beta}}^{\\mathrm{HCl}}\\), \\(\\widehat{\\boldsymbol{V}}_{\\widehat{\\beta}}^{\\mathrm{HC} 2}\\), and \\(\\widehat{\\boldsymbol{V}}_{\\widehat{\\beta}}^{\\mathrm{HC}}\\) are independent of the OLS estimator \\(\\widehat{\\beta}\\), conditional on \\(\\boldsymbol{X}\\).\nExercise 5.8 Let \\(F(u)\\) be the distribution function of a random variable \\(X\\) whose density is symmetric about zero. (This includes the standard normal and the student \\(t\\).) Show that \\(F(-u)=1-F(u)\\).\nExercise 5.9 Let \\(\\widehat{C}_{\\beta}=[L, U]\\) be a \\(1-\\alpha\\) confidence interval for \\(\\beta\\), and consider the transformation \\(\\theta=g(\\beta)\\) where \\(g(\\cdot)\\) is monotonically increasing. Consider the confidence interval \\(\\widehat{C}_{\\theta}=[g(L), g(U)]\\) for \\(\\theta\\). Show that \\(\\mathbb{P}\\left[\\theta \\in \\widehat{C}_{\\theta}\\right]=\\mathbb{P}\\left[\\beta \\in \\widehat{C}_{\\beta}\\right]\\). Use this result to develop a confidence interval for \\(\\sigma\\).\nExercise 5.10 Show that the test “Reject \\(\\mathbb{M}_{0}\\) if \\(L R \\geq c_{1}\\)” for LR defined in (5.18), and the test “Reject \\(\\mathbb{H}_{0}\\) if \\(\\mathrm{F} \\geq c_{2}\\)” for \\(\\mathrm{F}\\) defined in (5.19), yield the same decisions if \\(c_{2}=\\left(\\exp \\left(c_{1} / n\\right)-1\\right)(n-k) / q\\). Does this mean that the two tests are equivalent?\nExercise 5.11 Show (5.20).\nExercise 5.12 In the normal regression model let \\(s^{2}\\) be the unbiased estimator of the error variance \\(\\sigma^{2}\\) from (4.31).\n\nShow that \\(\\operatorname{var}\\left[s^{2}\\right]=2 \\sigma^{4} /(n-k)\\).\nShow that \\(\\operatorname{var}\\left[s^{2}\\right]\\) is strictly larger than the Cramér-Rao Lower Bound for \\(\\sigma^{2}\\)."
  },
  {
    "objectID": "chpt06-review.html#introduction",
    "href": "chpt06-review.html#introduction",
    "title": "6  A Review of Large Sample Asymptotics",
    "section": "6.1 Introduction",
    "text": "6.1 Introduction\nThe most widely-used tool in sampling theory is large sample asymptotics. By “asymptotics” we mean approximating a finite-sample sampling distribution by taking its limit as the sample size diverges to infinity. In this chapter we provide a brief review of the main results of large sample asymptotics. It is meant as a reference, not as a teaching guide. Asymptotic theory is covered in detail in Chapters 7-9 of Probability and Statistics for Economists. If you have not previous studied asymptotic theory in detail you should study these chapters before proceeding."
  },
  {
    "objectID": "chpt06-review.html#modes-of-convergence",
    "href": "chpt06-review.html#modes-of-convergence",
    "title": "6  A Review of Large Sample Asymptotics",
    "section": "6.2 Modes of Convergence",
    "text": "6.2 Modes of Convergence\nDefinition 6.1 A sequence of random vectors \\(Z_{n} \\in \\mathbb{R}^{k}\\) converges in probability to \\(Z\\) as \\(n \\rightarrow \\infty\\), denoted \\(Z_{n} \\underset{p}{\\rightarrow} Z\\) or alternatively \\(\\operatorname{plim}_{n \\rightarrow \\infty} Z_{n}=Z\\), if for all \\(\\delta>0\\)\n\\[\n\\lim _{n \\rightarrow \\infty} \\mathbb{P}\\left[\\left\\|Z_{n}-Z\\right\\| \\leq \\delta\\right]=1 .\n\\]\nWe call \\(Z\\) the probability limit (or plim) of \\(Z_{n}\\).\nThe above definition treats random variables and random vectors simultaneously using the vector norm. It is useful to know that for a random vector, (6.1) holds if and only if each element in the vector converges in probability to its limit.\nDefinition 6.2 Let \\(Z_{n}\\) be a sequence of random vectors with distributions \\(F_{n}(u)=\\mathbb{P}\\left[Z_{n} \\leq u\\right]\\). We say that \\(Z_{n}\\) converges in distribution to \\(Z\\) as \\(n \\rightarrow \\infty\\), denoted \\(Z_{n} \\underset{d}{\\rightarrow} Z\\), if for all \\(u\\) at which \\(F(u)=\\mathbb{P}[Z \\leq u]\\) is continuous, \\(F_{n}(u) \\rightarrow\\) \\(F(u)\\) as \\(n \\rightarrow \\infty\\). We refer to \\(Z\\) and its distribution \\(F(u)\\) as the asymptotic distribution, large sample distribution, or limit distribution of \\(Z_{n}\\)."
  },
  {
    "objectID": "chpt06-review.html#weak-law-of-large-numbers",
    "href": "chpt06-review.html#weak-law-of-large-numbers",
    "title": "6  A Review of Large Sample Asymptotics",
    "section": "6.3 Weak Law of Large Numbers",
    "text": "6.3 Weak Law of Large Numbers\nTheorem 6.1 Weak Law of Large Numbers (WLLN)\nIf \\(Y_{i} \\in \\mathbb{R}^{k}\\) are i.i.d. and \\(\\mathbb{E}\\|Y\\|<\\infty\\), then as \\(n \\rightarrow \\infty\\),\n\\[\n\\bar{Y}=\\frac{1}{n} \\sum_{i=1}^{n} Y_{i} \\underset{p}{\\longrightarrow}[Y] .\n\\]\nThe WLLN shows that the sample mean \\(\\bar{Y}\\) converges in probability to the true population expectation \\(\\mu\\). The result applies to any transformation of a random vector with a finite mean.\nTheorem 6.2 If \\(Y_{i} \\in \\mathbb{R}^{k}\\) are i.i.d., \\(h(y): \\mathbb{R}^{k} \\rightarrow \\mathbb{R}^{q}\\), and \\(\\mathbb{E}\\|h(Y)\\|<\\infty\\), then \\(\\widehat{\\mu}=\\) \\(\\frac{1}{n} \\sum_{i=1}^{n} h\\left(Y_{i}\\right) \\underset{p}{\\rightarrow} \\mu=\\mathbb{E}[h(Y)]\\) as \\(n \\rightarrow \\infty\\).\nAn estimator which converges in probability to the population value is called consistent.\nDefinition 6.3 An estimator \\(\\widehat{\\theta}\\) of \\(\\theta\\) is consistent if \\(\\widehat{\\theta} \\underset{p}{\\longrightarrow} \\theta\\) as \\(n \\rightarrow \\infty\\)."
  },
  {
    "objectID": "chpt06-review.html#central-limit-theorem",
    "href": "chpt06-review.html#central-limit-theorem",
    "title": "6  A Review of Large Sample Asymptotics",
    "section": "6.4 Central Limit Theorem",
    "text": "6.4 Central Limit Theorem\nTheorem 6.3 Multivariate Lindeberg-Lévy Central Limit Theorem (CLT). If \\(Y_{i} \\in \\mathbb{R}^{k}\\) are i.i.d. and \\(\\mathbb{E}\\|Y\\|^{2}<\\infty\\), then as \\(n \\rightarrow \\infty\\)\n\\[\n\\sqrt{n}(\\bar{Y}-\\mu) \\underset{d}{\\longrightarrow} \\mathrm{N}(0, \\boldsymbol{V})\n\\]\nwhere \\(\\mu=\\mathbb{E}[Y]\\) and \\(\\boldsymbol{V}=\\mathbb{E}\\left[(Y-\\mu)(Y-\\mu)^{\\prime}\\right]\\).\nThe central limit theorem shows that the distribution of the sample mean is approximately normal in large samples. For some applications it may be useful to notice that Theorem \\(6.3\\) does not impose any restrictions on \\(\\boldsymbol{V}\\) other than that the elements are finite. Therefore this result allows for the possibility of singular \\(V\\).\nThe following two generalizations allow for heterogeneous random variables. Theorem 6.4 Multivariate Lindeberg CLT. Suppose that for all \\(n, Y_{n i} \\in \\mathbb{R}^{k}, i=\\) \\(1, \\ldots, r_{n}\\), are independent but not necessarily identically distributed with expectations \\(\\mathbb{E}\\left[Y_{n i}\\right]=0\\) and variance matrices \\(\\boldsymbol{V}_{n i}=\\mathbb{E}\\left[Y_{n i} Y_{n i}^{\\prime}\\right]\\). Set \\(\\overline{\\boldsymbol{V}}_{n}=\\sum_{i=1}^{n} \\boldsymbol{V}_{n i}\\). Suppose \\(v_{n}^{2}=\\lambda_{\\min }\\left(\\overline{\\boldsymbol{V}}_{n}\\right)>0\\) and for all \\(\\epsilon>0\\)\n\\[\n\\lim _{n \\rightarrow \\infty} \\frac{1}{v_{n}^{2}} \\sum_{i=1}^{r_{n}} \\mathbb{E}\\left[\\left\\|Y_{n i}\\right\\|^{2} \\mathbb{1}\\left\\{\\left\\|Y_{n i}\\right\\|^{2} \\geq \\epsilon v_{n}^{2}\\right\\}\\right]=0 .\n\\]\nThen as \\(n \\rightarrow \\infty\\)\n\\[\n\\overline{\\boldsymbol{V}}_{n}^{-1 / 2} \\sum_{i=1}^{r_{n}} Y_{n i} \\underset{d}{\\longrightarrow} \\mathrm{N}\\left(0, \\boldsymbol{I}_{k}\\right) .\n\\]\nTheorem 6.5 Suppose \\(Y_{n i} \\in \\mathbb{R}^{k}\\) are independent but not necessarily identically distributed with expectations \\(\\mathbb{E}\\left[Y_{n i}\\right]=0\\) and variance matrices \\(\\boldsymbol{V}_{n i}=\\) \\(\\mathbb{E}\\left[Y_{n i} Y_{n i}^{\\prime}\\right]\\). Suppose\n\\[\n\\frac{1}{n} \\sum_{i=1}^{n} \\boldsymbol{V}_{n i} \\rightarrow \\boldsymbol{V}>0\n\\]\nand for some \\(\\delta>0\\)\n\\[\n\\sup _{n, i} \\mathbb{E}\\left\\|Y_{n i}\\right\\|^{2+\\delta}<\\infty .\n\\]\nThen as \\(n \\rightarrow \\infty\\)\n\\[\n\\sqrt{n} \\bar{Y} \\underset{d}{\\longrightarrow} \\mathrm{N}(0, \\boldsymbol{V})\n\\]"
  },
  {
    "objectID": "chpt06-review.html#continuous-mapping-theorem-and-delta-method",
    "href": "chpt06-review.html#continuous-mapping-theorem-and-delta-method",
    "title": "6  A Review of Large Sample Asymptotics",
    "section": "6.5 Continuous Mapping Theorem and Delta Method",
    "text": "6.5 Continuous Mapping Theorem and Delta Method\nContinuous functions are limit-preserving. There are two forms of the continuous mapping theorem, for convergence in probability and convergence in distribution.\nTheorem 6.6 Continuous Mapping Theorem (CMT). Let \\(Z_{n} \\in \\mathbb{R}^{k}\\) and \\(g(u):\\) \\(\\mathbb{R}^{k} \\rightarrow \\mathbb{R}^{q}\\). If \\(Z_{n} \\underset{p}{\\longrightarrow}\\) as \\(n \\rightarrow \\infty\\) and \\(g(u)\\) is continuous at \\(c\\) then \\(g\\left(Z_{n}\\right) \\underset{p}{\\longrightarrow} g(c)\\) as \\(n \\rightarrow \\infty\\)\nTheorem 6.7 Continuous Mapping Theorem. If \\(Z_{n} \\underset{d}{\\longrightarrow} Z\\) as \\(n \\rightarrow \\infty\\) and \\(g:\\) \\(\\mathbb{R}^{m} \\rightarrow \\mathbb{R}^{k}\\) has the set of discontinuity points \\(D_{g}\\) such that \\(\\mathbb{P}\\left[Z \\in D_{g}\\right]=0\\), then \\(g\\left(Z_{n}\\right) \\underset{d}{\\longrightarrow} g(Z)\\) as \\(n \\rightarrow \\infty\\) Differentiable functions of asymptotically normal random estimators are asymptotically normal.\nTheorem 6.8 Delta Method. Let \\(\\mu \\in \\mathbb{R}^{k}\\) and \\(g(u): \\mathbb{R}^{k} \\rightarrow \\mathbb{R}^{q}\\). If \\(\\sqrt{n}(\\widehat{\\mu}-\\mu) \\underset{d}{\\rightarrow}\\), where \\(g(u)\\) is continuously differentiable in a neighborhood of \\(\\mu\\), then as \\(n \\rightarrow\\) \\(\\infty\\)\n\\[\n\\sqrt{n}(g(\\widehat{\\mu})-g(\\mu)) \\underset{d}{\\longrightarrow} \\boldsymbol{G}^{\\prime} \\xi\n\\]\nwhere \\(\\boldsymbol{G}(u)=\\frac{\\partial}{\\partial u} g(u)^{\\prime}\\) and \\(\\boldsymbol{G}=\\boldsymbol{G}(\\mu)\\). In particular, if \\(\\xi \\sim \\mathrm{N}(0, \\boldsymbol{V})\\) then as \\(n \\rightarrow \\infty\\)\n\\[\n\\sqrt{n}(g(\\widehat{\\mu})-g(\\mu)) \\underset{d}{\\longrightarrow} \\mathrm{N}\\left(0, \\boldsymbol{G}^{\\prime} \\boldsymbol{V} \\boldsymbol{G}\\right) .\n\\]"
  },
  {
    "objectID": "chpt06-review.html#smooth-function-model",
    "href": "chpt06-review.html#smooth-function-model",
    "title": "6  A Review of Large Sample Asymptotics",
    "section": "6.6 Smooth Function Model",
    "text": "6.6 Smooth Function Model\nThe smooth function model is \\(\\theta=g(\\mu)\\) where \\(\\mu=\\mathbb{E}[h(Y)]\\) and \\(g(\\mu)\\) is smooth in a suitable sense.\nThe parameter \\(\\theta=g(\\mu)\\) is not a population moment so it does not have a direct moment estimator. Instead, it is common to use a plug-in estimator formed by replacing the unknown \\(\\mu\\) with its point estimator \\(\\widehat{\\mu}\\) and then “plugging” this into the expression for \\(\\theta\\). The first step is the sample mean \\(\\widehat{\\mu}=n^{-1} \\sum_{i=1}^{n} h\\left(Y_{i}\\right)\\). The second step is the transformation \\(\\widehat{\\theta}=g(\\widehat{\\mu})\\). The hat ” \\(\\wedge\\) ” indicates that \\(\\widehat{\\theta}\\) is a sample estimator of \\(\\theta\\). The smooth function model includes a broad class of estimators including sample variances and the least squares estimator.\nTheorem 6.9 If \\(Y_{i} \\in \\mathbb{R}^{m}\\) are i.i.d., \\(h(u): \\mathbb{R}^{m} \\rightarrow \\mathbb{R}^{k}, \\mathbb{E}\\|h(Y)\\|<\\infty\\), and \\(g(u):\\) \\(\\mathbb{R}^{k} \\rightarrow \\mathbb{R}^{q}\\) is continuous at \\(\\mu\\), then \\(\\widehat{\\theta} \\underset{p}{\\longrightarrow} \\theta\\) as \\(n \\rightarrow \\infty\\).\nTheorem 6.10 If \\(Y_{i} \\in \\mathbb{R}^{m}\\) are i.i.d., \\(h(u): \\mathbb{R}^{m} \\rightarrow \\mathbb{R}^{k}, \\mathbb{E}\\|h(Y)\\|^{2}<\\infty, g(u): \\mathbb{R}^{k} \\rightarrow\\) \\(\\mathbb{R}^{q}\\), and \\(\\boldsymbol{G}(u)=\\frac{\\partial}{\\partial u} g(u)^{\\prime}\\) is continuous in a neighborhood of \\(\\mu\\), then as \\(n \\rightarrow \\infty\\)\n\\[\n\\sqrt{n}(\\widehat{\\theta}-\\theta) \\underset{d}{\\longrightarrow} \\mathrm{N}\\left(0, \\boldsymbol{V}_{\\theta}\\right)\n\\]\nwhere \\(\\boldsymbol{V}_{\\theta}=\\boldsymbol{G}^{\\prime} \\boldsymbol{V} \\boldsymbol{G}, \\boldsymbol{V}=\\mathbb{E}\\left[(h(Y)-\\mu)(h(Y)-\\mu)^{\\prime}\\right]\\), and \\(\\boldsymbol{G}=\\boldsymbol{G}(\\mu)\\).\nTheorem \\(6.9\\) establishes the consistency of \\(\\widehat{\\theta}\\) for \\(\\theta\\) and Theorem \\(6.10\\) establishes its asymptotic normality. It is instructive to compare the conditions. Consistency requires that \\(h(Y)\\) has a finite expectation; asymptotic normality requires that \\(h(Y)\\) has a finite variance. Consistency requires that \\(g(u)\\) be continuous; asymptotic normality requires that \\(g(u)\\) is continuously differentiable."
  },
  {
    "objectID": "chpt06-review.html#stochastic-order-symbols",
    "href": "chpt06-review.html#stochastic-order-symbols",
    "title": "6  A Review of Large Sample Asymptotics",
    "section": "6.7 Stochastic Order Symbols",
    "text": "6.7 Stochastic Order Symbols\nIt is convenient to have simple symbols for random variables and vectors which converge in probability to zero or are stochastically bounded. In this section we introduce some of the most common notation.\nLet \\(Z_{n}\\) and \\(a_{n}, n=1,2, \\ldots\\) be sequences of random variables and constants. The notation\n\\[\nZ_{n}=o_{p}(1)\n\\]\n(“small oh-P-one”) means that \\(Z_{n} \\underset{p}{\\longrightarrow} 0\\) as \\(n \\rightarrow \\infty\\). We also write\n\\[\nZ_{n}=o_{p}\\left(a_{n}\\right)\n\\]\nif \\(a_{n}^{-1} Z_{n}=o_{p}(1)\\)\nSimilarly, the notation \\(Z_{n}=O_{p}\\) (1) (“big oh-P-one”) means that \\(Z_{n}\\) is bounded in probability. Precisely, for any \\(\\epsilon>0\\) there is a constant \\(M_{\\epsilon}<\\infty\\) such that\n\\[\n\\limsup _{n \\rightarrow \\infty} \\mathbb{P}\\left[\\left|Z_{n}\\right|>M_{\\epsilon}\\right] \\leq \\epsilon .\n\\]\nFurthermore, we write\n\\[\nZ_{n}=O_{p}\\left(a_{n}\\right)\n\\]\nif \\(a_{n}^{-1} Z_{n}=O_{p}(1)\\).\n\\(O_{p}(1)\\) is weaker than \\(o_{p}(1)\\) in the sense that \\(Z_{n}=o_{p}(1)\\) implies \\(Z_{n}=O_{p}(1)\\) but not the reverse. However, if \\(Z_{n}=O_{p}\\left(a_{n}\\right)\\) then \\(Z_{n}=o_{p}\\left(b_{n}\\right)\\) for any \\(b_{n}\\) such that \\(a_{n} / b_{n} \\rightarrow 0\\).\nA random sequence with a bounded moment is stochastically bounded.\nTheorem 6.11 If \\(Z_{n}\\) is a random vector which satisfies \\(\\mathbb{E}\\left\\|Z_{n}\\right\\|^{\\delta}=O\\left(a_{n}\\right)\\) for some sequence \\(a_{n}\\) and \\(\\delta>0\\), then \\(Z_{n}=O_{p}\\left(a_{n}^{1 / \\delta}\\right)\\). Similarly, \\(\\mathbb{E}\\left\\|Z_{n}\\right\\|^{\\delta}=o\\left(a_{n}\\right)\\) implies \\(Z_{n}=o_{p}\\left(a_{n}^{1 / \\delta}\\right)\\).\nThere are many simple rules for manipulating \\(o_{p}(1)\\) and \\(O_{p}(1)\\) sequences which can be deduced from the continuous mapping theorem. For example,\n\\[\n\\begin{aligned}\no_{p}(1)+o_{p}(1) &=o_{p}(1) \\\\\no_{p}(1)+O_{p}(1) &=O_{p}(1) \\\\\nO_{p}(1)+O_{p}(1) &=O_{p}(1) \\\\\no_{p}(1) o_{p}(1) &=o_{p}(1) \\\\\no_{p}(1) O_{p}(1) &=o_{p}(1) \\\\\nO_{p}(1) O_{p}(1) &=O_{p}(1) .\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "chpt06-review.html#convergence-of-moments",
    "href": "chpt06-review.html#convergence-of-moments",
    "title": "6  A Review of Large Sample Asymptotics",
    "section": "6.8 Convergence of Moments",
    "text": "6.8 Convergence of Moments\nWe give a sufficient condition for the existence of the mean of the asymptotic distribution, define uniform integrability, provide a primitive condition for uniform integrability, and show that uniform integrability is the key condition under which \\(\\mathbb{E}\\left[Z_{n}\\right]\\) converges to \\(\\mathbb{E}[Z]\\). Theorem 6.12 If \\(Z_{n} \\underset{d}{\\longrightarrow} Z\\) and \\(\\mathbb{E}\\left\\|Z_{n}\\right\\| \\leq C\\) then \\(\\mathbb{E}\\|Z\\| \\leq C\\).\nDefinition 6.4 The random vector \\(Z_{n}\\) is uniformly integrable as \\(n \\rightarrow \\infty\\) if\n\\[\n\\lim _{M \\rightarrow \\infty} \\limsup _{n \\rightarrow \\infty} \\mathbb{E}\\left[\\left\\|Z_{n}\\right\\| \\mathbb{1}\\left\\{\\left\\|Z_{n}\\right\\|>M\\right\\}\\right]=0\n\\]\nTheorem 6.13 If for some \\(\\delta>0\\), \\(\\mathbb{E}\\left\\|Z_{n}\\right\\|^{1+\\delta} \\leq C<\\infty\\), then \\(Z_{n}\\) is uniformly integrable.\nTheorem 6.14 If \\(Z_{n} \\underset{d}{\\longrightarrow} Z\\) and \\(Z_{n}\\) is uniformly integrable then \\(\\mathbb{E}\\left[Z_{n}\\right] \\longrightarrow \\mathbb{E}[Z]\\).\nThe following is a uniform stochastic bound.\nTheorem 6.15 If \\(\\left|Y_{i}\\right|^{r}\\) is uniformly integrable, then as \\(n \\rightarrow \\infty\\)\n\\[\nn^{-1 / r} \\max _{1 \\leq i \\leq n}\\left|Y_{i}\\right| \\underset{p}{\\longrightarrow} 0\n\\]\nEquation (6.6) implies that if \\(Y\\) has \\(r\\) finite moments then the largest observation will diverge at a rate slower than \\(n^{1 / r}\\). The higher the moments, the slower the rate of divergence."
  },
  {
    "objectID": "chpt07-asymptotic-ls.html#introduction",
    "href": "chpt07-asymptotic-ls.html#introduction",
    "title": "7  Asymptotic Theory for Least Squares",
    "section": "7.1 Introduction",
    "text": "7.1 Introduction\nIt turns out that the asymptotic theory of least squares estimation applies equally to the projection model and the linear CEF model. Therefore the results in this chapter will be stated for the broader projection model described in Section 2.18. Recall that the model is \\(Y=X^{\\prime} \\beta+e\\) with the linear projection coefficient \\(\\beta=\\left(\\mathbb{E}\\left[X X^{\\prime}\\right]\\right)^{-1} \\mathbb{E}[X Y]\\).\nMaintained assumptions in this chapter will be random sampling (Assumption 1.2) and finite second moments (Assumption 2.1). We restate these here for clarity.\nAssumption 7.1\n\nThe variables \\(\\left(Y_{i}, X_{i}\\right), i=1, \\ldots, n\\), are i.i.d.\n\\(\\mathbb{E}\\left[Y^{2}\\right]<\\infty\\).\n\\(\\mathbb{E}\\|X\\|^{2}<\\infty\\).\n\\(\\boldsymbol{Q}_{X X}=\\mathbb{E}\\left[X X^{\\prime}\\right]\\) is positive definite.\n\nThe distributional results will require a strengthening of these assumptions to finite fourth moments. We discuss the specific conditions in Section 7.3."
  },
  {
    "objectID": "chpt07-asymptotic-ls.html#consistency-of-least-squares-estimator",
    "href": "chpt07-asymptotic-ls.html#consistency-of-least-squares-estimator",
    "title": "7  Asymptotic Theory for Least Squares",
    "section": "7.2 Consistency of Least Squares Estimator",
    "text": "7.2 Consistency of Least Squares Estimator\nIn this section we use the weak law of large numbers (WLLN, Theorem 6.1 and Theorem 6.2) and continuous mapping theorem (CMT, Theorem 6.6) to show that the least squares estimator \\(\\widehat{\\beta}\\) is consistent for the projection coefficient \\(\\beta\\).\nThis derivation is based on three key components. First, the OLS estimator can be written as a continuous function of a set of sample moments. Second, the WLLN shows that sample moments converge in probability to population moments. And third, the CMT states that continuous functions preserve convergence in probability. We now explain each step in brief and then in greater detail. First, observe that the OLS estimator\n\\[\n\\widehat{\\beta}=\\left(\\frac{1}{n} \\sum_{i=1}^{n} X_{i} X_{i}^{\\prime}\\right)^{-1}\\left(\\frac{1}{n} \\sum_{i=1}^{n} X_{i} Y_{i}\\right)=\\widehat{\\boldsymbol{Q}}_{X X}^{-1} \\widehat{\\boldsymbol{Q}}_{X Y}\n\\]\nis a function of the sample moments \\(\\widehat{\\boldsymbol{Q}}_{X X}=\\frac{1}{n} \\sum_{i=1}^{n} X_{i} X_{i}^{\\prime}\\) and \\(\\widehat{\\boldsymbol{Q}}_{X Y}=\\frac{1}{n} \\sum_{i=1}^{n} X_{i} Y_{i}\\).\nSecond, by an application of the WLLN these sample moments converge in probability to their population expectations. Specifically, the fact that \\(\\left(Y_{i}, X_{i}\\right)\\) are mutually i.i.d. implies that any function of \\(\\left(Y_{i}, X_{i}\\right)\\) is i.i.d., including \\(X_{i} X_{i}^{\\prime}\\) and \\(X_{i} Y_{i}\\). These variables also have finite expectations under Assumption 7.1. Under these conditions, the WLLN implies that as \\(n \\rightarrow \\infty\\),\n\\[\n\\widehat{\\boldsymbol{Q}}_{X X}=\\frac{1}{n} \\sum_{i=1}^{n} X_{i} X_{i}^{\\prime} \\underset{p}{\\longrightarrow}\\left[X X^{\\prime}\\right]=\\boldsymbol{Q}_{X X}\n\\]\nand\n\\[\n\\widehat{\\boldsymbol{Q}}_{X Y}=\\frac{1}{n} \\sum_{i=1}^{n} X_{i} Y_{i} \\underset{p}{\\longrightarrow}[X Y]=\\boldsymbol{Q}_{X Y}\n\\]\nThird, the CMT allows us to combine these equations to show that \\(\\widehat{\\beta}\\) converges in probability to \\(\\beta\\). Specifically, as \\(n \\rightarrow \\infty\\),\n\\[\n\\widehat{\\beta}=\\widehat{\\boldsymbol{Q}}_{X X}^{-1} \\widehat{\\boldsymbol{Q}}_{X Y} \\underset{p}{\\longrightarrow} \\boldsymbol{Q}_{X X}^{-1} \\boldsymbol{Q}_{X Y}=\\beta .\n\\]\nWe have shown that \\(\\widehat{\\beta} \\underset{p}{\\rightarrow} \\beta\\) as \\(n \\rightarrow \\infty\\). In words, the OLS estimator converges in probability to the projection coefficient vector \\(\\beta\\) as the sample size \\(n\\) gets large.\nTo fully understand the application of the CMT we walk through it in detail. We can write\n\\[\n\\widehat{\\beta}=g\\left(\\widehat{\\boldsymbol{Q}}_{X X}, \\widehat{\\boldsymbol{Q}}_{X Y}\\right)\n\\]\nwhere \\(g(\\boldsymbol{A}, \\boldsymbol{b})=\\boldsymbol{A}^{-1} \\boldsymbol{b}\\) is a function of \\(\\boldsymbol{A}\\) and \\(\\boldsymbol{b}\\). The function \\(\\boldsymbol{g}(\\boldsymbol{A}, \\boldsymbol{b})\\) is a continuous function of \\(\\boldsymbol{A}\\) and \\(\\boldsymbol{b}\\) at all values of the arguments such that \\(A^{-1}\\) exists. Assumption \\(7.1\\) specifies that \\(\\boldsymbol{Q}_{X X}\\) is positive definite, which means that \\(\\boldsymbol{Q}_{X X}^{-1}\\) exists. Thus \\(\\boldsymbol{g}(\\boldsymbol{A}, \\boldsymbol{b})\\) is continuous at \\(\\boldsymbol{A}=\\boldsymbol{Q}_{X X}\\). This justifies the application of the CMT in (7.2).\nFor a slightly different demonstration of (7.2) recall that (4.6) implies that\n\\[\n\\widehat{\\beta}-\\beta=\\widehat{\\boldsymbol{Q}}_{X X}^{-1} \\widehat{\\boldsymbol{Q}}_{X e}\n\\]\nwhere\n\\[\n\\widehat{\\boldsymbol{Q}}_{X e}=\\frac{1}{n} \\sum_{i=1}^{n} X_{i} e_{i} .\n\\]\nThe WLLN and (2.25) imply\n\\[\n\\widehat{\\boldsymbol{Q}}_{X e} \\underset{p}{\\longrightarrow} \\mathbb{E}[X e]=0 .\n\\]\nTherefore\n\\[\n\\widehat{\\beta}-\\beta=\\widehat{\\boldsymbol{Q}}_{X X}^{-1} \\widehat{\\boldsymbol{Q}}_{X e} \\underset{p}{\\longrightarrow} \\boldsymbol{Q}_{X X}^{-1} 0=0\n\\]\nwhich is the same as \\(\\widehat{\\beta} \\underset{p}{\\vec{p}}\\). Theorem 7.1 Consistency of Least Squares. Under Assumption 7.1, \\(\\widehat{\\boldsymbol{Q}}_{X X} \\vec{p}\\) \\(\\boldsymbol{Q}_{X X}, \\widehat{\\boldsymbol{Q}}_{X Y} \\underset{p}{\\boldsymbol{Q}_{X Y}}, \\widehat{\\boldsymbol{Q}}_{X X}^{-1} \\vec{p} \\boldsymbol{Q}_{X X}^{-1}, \\widehat{\\boldsymbol{Q}}_{X e} \\underset{p}{\\rightarrow} 0\\), and \\(\\widehat{\\beta} \\underset{p}{\\overrightarrow{3}} \\beta\\) as \\(n \\rightarrow \\infty\\)\nTheorem \\(7.1\\) states that the OLS estimator \\(\\widehat{\\beta}\\) converges in probability to \\(\\beta\\) as \\(n\\) increases and thus \\(\\widehat{\\beta}\\) is consistent for \\(\\beta\\). In the stochastic order notation, Theorem \\(7.1\\) can be equivalently written as\n\\[\n\\widehat{\\beta}=\\beta+o_{p}(1) .\n\\]\nTo illustrate the effect of sample size on the least squares estimator consider the least squares regression\n\\[\n\\log (\\text { wage })=\\beta_{1} \\text { education }+\\beta_{2} \\text { experience }+\\beta_{3} \\text { experience }^{2}+\\beta_{4}+e .\n\\]\nWe use the sample of 24,344 white men from the March 2009 CPS. We randomly sorted the observations and sequentially estimated the model by least squares starting with the first 5 observations and continuing until the full sample is used. The sequence of estimates are displayed in Figure 7.1. You can see how the least squares estimate changes with the sample size. As the number of observations increases it settles down to the full-sample estimate \\(\\widehat{\\beta}_{1}=0.114\\).\n\nFigure 7.1: The Least-Squares Estimator as a Function of Sample Size"
  },
  {
    "objectID": "chpt07-asymptotic-ls.html#asymptotic-normality",
    "href": "chpt07-asymptotic-ls.html#asymptotic-normality",
    "title": "7  Asymptotic Theory for Least Squares",
    "section": "7.3 Asymptotic Normality",
    "text": "7.3 Asymptotic Normality\nWe started this chapter discussing the need for an approximation to the distribution of the OLS estimator \\(\\widehat{\\beta}\\). In Section \\(7.2\\) we showed that \\(\\widehat{\\beta}\\) converges in probability to \\(\\beta\\). Consistency is a good first step, but in itself does not describe the distribution of the estimator. In this section we derive an approximation typically called the asymptotic distribution.\nThe derivation starts by writing the estimator as a function of sample moments. One of the moments must be written as a sum of zero-mean random vectors and normalized so that the central limit theorem can be applied. The steps are as follows.\nTake equation (7.3) and multiply it by \\(\\sqrt{n}\\). This yields the expression\n\\[\n\\sqrt{n}(\\widehat{\\beta}-\\beta)=\\left(\\frac{1}{n} \\sum_{i=1}^{n} X_{i} X_{i}^{\\prime}\\right)^{-1}\\left(\\frac{1}{\\sqrt{n}} \\sum_{i=1}^{n} X_{i} e_{i}\\right)\n\\]\nThis shows that the normalized and centered estimator \\(\\sqrt{n}(\\widehat{\\beta}-\\beta)\\) is a function of the sample average \\(n^{-1} \\sum_{i=1}^{n} X_{i} X_{i}^{\\prime}\\) and the normalized sample average \\(n^{-1 / 2} \\sum_{i=1}^{n} X_{i} e_{i}\\).\nThe random pairs \\(\\left(Y_{i}, X_{i}\\right)\\) are i.i.d., meaning that they are independent across \\(i\\) and identically distributed. Any function of \\(\\left(Y_{i}, X_{i}\\right)\\) is also i.i.d. This includes \\(e_{i}=Y_{i}-X_{i}^{\\prime} \\beta\\) and the product \\(X_{i} e_{i}\\). The latter is mean-zero \\((\\mathbb{E}[X e]=0)\\) and has \\(k \\times k\\) covariance matrix\n\\[\n\\Omega=\\mathbb{E}\\left[(X e)(X e)^{\\prime}\\right]=\\mathbb{E}\\left[X X^{\\prime} e^{2}\\right] .\n\\]\nWe show below that \\(\\Omega\\) has finite elements under a strengthening of Assumption 7.1. Since \\(X_{i} e_{i}\\) is i.i.d., mean zero, and finite variance, the central limit theorem (Theorem 6.3) implies\n\\[\n\\frac{1}{\\sqrt{n}} \\sum_{i=1}^{n} X_{i} e_{i} \\underset{d}{\\longrightarrow} \\mathrm{N}(0, \\Omega)\n\\]\nWe state the required conditions here.\nAssumption 7.2\n\nThe variables \\(\\left(Y_{i}, X_{\\boldsymbol{i}}\\right), i=1, \\ldots, n\\), are i.i.d..\n\\(\\mathbb{E}\\left[Y^{4}\\right]<\\infty\\).\n\\(\\mathbb{E}\\|X\\|^{4}<\\infty\\).\n\\(\\boldsymbol{Q}_{X X}=\\mathbb{E}\\left[X X^{\\prime}\\right]\\) is positive definite.\n\nAssumption \\(7.2\\) implies that \\(\\Omega<\\infty\\). To see this, take its \\(j \\ell^{t h}\\) element, \\(\\mathbb{E}\\left[X_{j} X_{\\ell} e^{2}\\right]\\). Theorem 2.9.6 shows that \\(\\mathbb{E}\\left[e^{4}\\right]<\\infty\\). By the expectation inequality (B.30), the \\(j \\ell^{t h}\\) element of \\(\\Omega\\) is bounded by\n\\[\n\\left|\\mathbb{E}\\left[X_{j} X_{\\ell} e^{2}\\right]\\right| \\leq \\mathbb{E}\\left|X_{j} X_{\\ell} e^{2}\\right|=\\mathbb{E}\\left[\\left|X_{j}\\right|\\left|X_{\\ell}\\right| e^{2}\\right] .\n\\]\nBy two applications of the Cauchy-Schwarz inequality (B.32), this is smaller than\n\\[\n\\left(\\mathbb{E}\\left[X_{j}^{2} X_{\\ell}^{2}\\right]\\right)^{1 / 2}\\left(\\mathbb{E}\\left[e^{4}\\right]\\right)^{1 / 2} \\leq\\left(\\mathbb{E}\\left[X_{j}^{4}\\right]\\right)^{1 / 4}\\left(\\mathbb{E}\\left[X_{\\ell}^{4}\\right]\\right)^{1 / 4}\\left(\\mathbb{E}\\left[e^{4}\\right]\\right)^{1 / 2}<\\infty\n\\]\nwhere the finiteness holds under Assumption 7.2.2 and 7.2.3. Thus \\(\\Omega<\\infty\\).\nAn alternative way to show that the elements of \\(\\Omega\\) are finite is by using a matrix norm \\(\\|\\cdot\\|\\) (See Appendix A.23). Then by the expectation inequality, the Cauchy-Schwarz inequality, Assumption 7.2.3, and \\(\\mathbb{E}\\left[e^{4}\\right]<\\infty\\),\n\\[\n\\|\\Omega\\| \\leq \\mathbb{E}\\left\\|X X^{\\prime} e^{2}\\right\\|=\\mathbb{E}\\left[\\|X\\|^{2} e^{2}\\right] \\leq\\left(\\mathbb{E}\\|X\\|^{4}\\right)^{1 / 2}\\left(\\mathbb{E}\\left[e^{4}\\right]\\right)^{1 / 2}<\\infty .\n\\]\nThis is a more compact argument (often described as more elegant) but such manipulations should not be done without understanding the notation and the applicability of each step of the argument.\nRegardless, the finiteness of the covariance matrix means that we can apply the multivariate CLT (Theorem 6.3).\nTheorem 7.2 Assumption \\(7.2\\) implies that\n\\[\n\\Omega<\\infty\n\\]\nand\n\\[\n\\frac{1}{\\sqrt{n}} \\sum_{i=1}^{n} X_{i} e_{i} \\underset{d}{\\longrightarrow} \\mathrm{N}(0, \\Omega)\n\\]\nas \\(n \\rightarrow \\infty\\)\nPutting together (7.1), (7.5), and (7.7),\n\\[\n\\sqrt{n}(\\widehat{\\beta}-\\beta) \\underset{d}{\\longrightarrow} \\boldsymbol{Q}_{X X}^{-1} \\mathrm{~N}(0, \\Omega)=\\mathrm{N}\\left(0, \\boldsymbol{Q}_{X X}^{-1} \\Omega \\boldsymbol{Q}_{X X}^{-1}\\right)\n\\]\nas \\(n \\rightarrow \\infty\\). The final equality follows from the property that linear combinations of normal vectors are also normal (Theorem 5.2).\nWe have derived the asymptotic normal approximation to the distribution of the least squares estimator.\nTheorem 7.3 Asymptotic Normality of Least Squares Estimator Under Assumption 7.2, as \\(n \\rightarrow \\infty\\)\n\\[\n\\sqrt{n}(\\widehat{\\beta}-\\beta) \\underset{d}{\\longrightarrow} \\mathrm{N}\\left(0, \\boldsymbol{V}_{\\beta}\\right)\n\\]\nwhere \\(\\boldsymbol{Q}_{X X}=\\mathbb{E}\\left[X X^{\\prime}\\right], \\Omega=\\mathbb{E}\\left[X X^{\\prime} e^{2}\\right]\\), and\n\\[\n\\boldsymbol{V}_{\\beta}=\\boldsymbol{Q}_{X X}^{-1} \\Omega \\boldsymbol{Q}_{X X}^{-1} .\n\\]\nIn the stochastic order notation, Theorem \\(7.3\\) implies that \\(\\widehat{\\beta}=\\beta+O_{p}\\left(n^{-1 / 2}\\right)\\) which is stronger than (7.4).\nThe matrix \\(\\boldsymbol{V}_{\\beta}=\\boldsymbol{Q}_{X X}^{-1} \\Omega \\boldsymbol{Q}_{X X}^{-1}\\) is the variance of the asymptotic distribution of \\(\\sqrt{n}(\\widehat{\\beta}-\\beta)\\). Consequently, \\(\\boldsymbol{V}_{\\beta}\\) is often referred to as the asymptotic covariance matrix of \\(\\widehat{\\beta}\\). The expression \\(\\boldsymbol{V}_{\\beta}=\\boldsymbol{Q}_{X X}^{-1} \\Omega \\boldsymbol{Q}_{X X}^{-1}\\) is called a sandwich form as the matrix \\(\\Omega\\) is sandwiched between two copies of \\(\\boldsymbol{Q}_{X X}^{-1}\\). It is useful to compare the variance of the asymptotic distribution given in (7.8) and the finite-sample conditional variance in the CEF model as given in (4.10):\n\\[\n\\boldsymbol{V}_{\\widehat{\\beta}}=\\operatorname{var}[\\widehat{\\beta} \\mid \\boldsymbol{X}]=\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{D} \\boldsymbol{X}\\right)\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} .\n\\]\nNotice that \\(\\boldsymbol{V}_{\\widehat{\\beta}}\\) is the exact conditional variance of \\(\\widehat{\\beta}\\) and \\(\\boldsymbol{V}_{\\beta}\\) is the asymptotic variance of \\(\\sqrt{n}(\\widehat{\\beta}-\\beta)\\). Thus \\(\\boldsymbol{V}_{\\beta}\\) should be (roughly) \\(n\\) times as large as \\(\\boldsymbol{V}_{\\widehat{\\beta}}\\), or \\(\\boldsymbol{V}_{\\beta} \\approx n \\boldsymbol{V}_{\\widehat{\\beta}}\\). Indeed, multiplying (7.9) by \\(n\\) and distributing we find\n\\[\nn \\boldsymbol{V}_{\\widehat{\\beta}}=\\left(\\frac{1}{n} \\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1}\\left(\\frac{1}{n} \\boldsymbol{X}^{\\prime} \\boldsymbol{D} \\boldsymbol{X}\\right)\\left(\\frac{1}{n} \\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1}\n\\]\nwhich looks like an estimator of \\(\\boldsymbol{V}_{\\beta}\\). Indeed, as \\(n \\rightarrow \\infty, n \\boldsymbol{V}_{\\widehat{\\beta}} \\underset{p}{\\rightarrow} \\boldsymbol{V}_{\\beta}\\). The expression \\(\\boldsymbol{V}_{\\widehat{\\beta}}\\) is useful for practical inference (such as computation of standard errors and tests) as it is the variance of the estimator \\(\\widehat{\\beta}\\), while \\(V_{\\beta}\\) is useful for asymptotic theory as it is well defined in the limit as \\(n\\) goes to infinity. We will make use of both symbols and it will be advisable to adhere to this convention.\nThere is a special case where \\(\\Omega\\) and \\(\\boldsymbol{V}_{\\beta}\\) simplify. Suppose that\n\\[\n\\operatorname{cov}\\left(X X^{\\prime}, e^{2}\\right)=0 .\n\\]\nCondition (7.10) holds in the homoskedastic linear regression model but is somewhat broader. Under (7.10) the asymptotic variance formulae simplify as\n\\[\n\\begin{aligned}\n\\Omega &=\\mathbb{E}\\left[X X^{\\prime}\\right] \\mathbb{E}\\left[e^{2}\\right]=\\boldsymbol{Q}_{X X} \\sigma^{2} \\\\\n\\boldsymbol{V}_{\\beta} &=\\boldsymbol{Q}_{X X}^{-1} \\Omega \\boldsymbol{Q}_{X X}^{-1}=\\boldsymbol{Q}_{X X}^{-1} \\sigma^{2} \\equiv \\boldsymbol{V}_{\\beta}^{0} .\n\\end{aligned}\n\\]\nIn (7.11) we define \\(\\boldsymbol{V}_{\\beta}^{0}=\\boldsymbol{Q}_{X X}^{-1} \\sigma^{2}\\) whether (7.10) is true or false. When (7.10) is true then \\(\\boldsymbol{V}_{\\beta}=\\boldsymbol{V}_{\\beta}^{0}\\), otherwise \\(\\boldsymbol{V}_{\\beta} \\neq \\boldsymbol{V}_{\\beta}^{0}\\). We call \\(\\boldsymbol{V}_{\\beta}^{0}\\) the homoskedastic asymptotic covariance matrix.\nTheorem \\(7.3\\) states that the sampling distribution of the least squares estimator, after rescaling, is approximately normal when the sample size \\(n\\) is sufficiently large. This holds true for all joint distributions of \\((Y, X)\\) which satisfy the conditions of Assumption 7.2. Consequently, asymptotic normality is routinely used to approximate the finite sample distribution of \\(\\sqrt{n}(\\widehat{\\beta}-\\beta)\\).\nA difficulty is that for any fixed \\(n\\) the sampling distribution of \\(\\widehat{\\beta}\\) can be arbitrarily far from the normal distribution. The normal approximation improves as \\(n\\) increases, but how large should \\(n\\) be in order for the approximation to be useful? Unfortunately, there is no simple answer to this reasonable question. The trouble is that no matter how large is the sample size, the normal approximation is arbitrarily poor for some data distribution satisfying the assumptions. We illustrate this problem using a simulation. Let \\(Y=\\beta_{1} X+\\beta_{2}+e\\) where \\(X\\) is \\(\\mathrm{N}(0,1)\\) and \\(e\\) is independent of \\(X\\) with the Double Pareto density \\(f(e)=\\frac{\\alpha}{2}|e|^{-\\alpha-1},|e| \\geq 1\\). If \\(\\alpha>2\\) the error \\(e\\) has zero mean and variance \\(\\alpha /(\\alpha-2)\\). As \\(\\alpha\\) approaches 2 , however, its variance diverges to infinity. In this context the normalized least squares slope estimator \\(\\sqrt{n \\frac{\\alpha-2}{\\alpha}}\\left(\\widehat{\\beta}_{1}-\\beta_{1}\\right)\\) has the \\(\\mathrm{N}(0,1)\\) asymptotic distribution for any \\(\\alpha>2\\). In Figure \\(7.2(\\) a) we display the finite sample densities of the normalized estimator \\(\\sqrt{n \\frac{\\alpha-2}{\\alpha}}\\left(\\widehat{\\beta}_{1}-\\beta_{1}\\right)\\), setting \\(n=100\\) and varying the parameter \\(\\alpha\\). For \\(\\alpha=3.0\\) the density is very close to the \\(\\mathrm{N}(0,1)\\) density. As \\(\\alpha\\) diminishes the density changes significantly, concentrating most of the probability mass around zero.\nAnother example is shown in Figure 7.2(b). Here the model is \\(Y=\\beta+e\\) where\n\\[\ne=\\frac{u^{r}-\\mathbb{E}\\left[u^{r}\\right]}{\\left(\\mathbb{E}\\left[u^{2 r}\\right]-\\left(\\mathbb{E}\\left[u^{r}\\right]\\right)^{2}\\right)^{1 / 2}}\n\\]\nand \\(u \\sim \\mathrm{N}(0,1)\\). We show the sampling distribution of \\(\\sqrt{n}(\\widehat{\\beta}-\\beta)\\) for \\(n=100\\), varying \\(r=1,4,6\\) and 8 . As \\(r\\) increases, the sampling distribution becomes highly skewed and non-normal. The lesson from Figure \\(7.2\\) is that the \\(\\mathrm{N}(0,1)\\) asymptotic approximation is never guaranteed to be accurate.\n\n\nDouble Pareto Error\n\n\n\nError Process (7.12)\n\nFigure 7.2: Density of Normalized OLS Estimator"
  },
  {
    "objectID": "chpt07-asymptotic-ls.html#joint-distribution",
    "href": "chpt07-asymptotic-ls.html#joint-distribution",
    "title": "7  Asymptotic Theory for Least Squares",
    "section": "7.4 Joint Distribution",
    "text": "7.4 Joint Distribution\nTheorem \\(7.3\\) gives the joint asymptotic distribution of the coefficient estimators. We can use the result to study the covariance between the coefficient estimators. For simplicity, take the case of two regressors, no intercept, and homoskedastic error. Assume the regressors are mean zero, variance one, with correlation \\(\\rho\\). Then using the formula for inversion of a \\(2 \\times 2\\) matrix,\n\\[\n\\boldsymbol{V}_{\\beta}^{0}=\\sigma^{2} \\boldsymbol{Q}_{X X}^{-1}=\\frac{\\sigma^{2}}{1-\\rho^{2}}\\left[\\begin{array}{cc}\n1 & -\\rho \\\\\n-\\rho & 1\n\\end{array}\\right] .\n\\]\nThus if \\(X_{1}\\) and \\(X_{2}\\) are positively correlated \\((\\rho>0)\\) then \\(\\widehat{\\beta}_{1}\\) and \\(\\widehat{\\beta}_{2}\\) are negatively correlated (and viceversa).\nFor illustration, Figure 7.3(a) displays the probability contours of the joint asymptotic distribution of \\(\\widehat{\\beta}_{1}-\\beta_{1}\\) and \\(\\widehat{\\beta}_{2}-\\beta_{2}\\) when \\(\\beta_{1}=\\beta_{2}=0\\) and \\(\\rho=0.5\\). The coefficient estimators are negatively correlated because the regressors are positively correlated. This means that if \\(\\widehat{\\beta}_{1}\\) is unusually negative, it is likely that \\(\\widehat{\\beta}_{2}\\) is unusually positive, or conversely. It is also unlikely that we will observe both \\(\\widehat{\\beta}_{1}\\) and \\(\\widehat{\\beta}_{2}\\) unusually large and of the same sign.\nThis finding that the correlation of the regressors is of opposite sign of the correlation of the coefficient estimates is sensitive to the assumption of homoskedasticity. If the errors are heteroskedastic then this relationship is not guaranteed.\nThis can be seen through a simple constructed example. Suppose that \\(X_{1}\\) and \\(X_{2}\\) only take the values \\(\\{-1,+1\\}\\), symmetrically, with \\(\\mathbb{P}\\left[X_{1}=X_{2}=1\\right]=\\mathbb{P}\\left[X_{1}=X_{2}=-1\\right]=3 / 8\\), and \\(\\mathbb{P}\\left[X_{1}=1, X_{2}=-1\\right]=\\) \\(\\mathbb{P}\\left[X_{1}=-1, X_{2}=1\\right]=1 / 8\\). You can check that the regressors are mean zero, unit variance and correlation \\(0.5\\), which is identical with the setting displayed in Figure 7.3(a).\nNow suppose that the error is heteroskedastic. Specifically, suppose that \\(\\mathbb{E}\\left[e^{2} \\mid X_{1}=X_{2}\\right]=5 / 4\\) and \\(\\mathbb{E}\\left[e^{2} \\mid X_{1} \\neq X_{2}\\right]=1 / 4\\). You can check that \\(\\mathbb{E}\\left[e^{2}\\right]=1\\), \\(\\mathbb{E}\\left[X_{1}^{2} e^{2}\\right]=\\mathbb{E}\\left[X_{2}^{2} e^{2}\\right]=1\\) and \\(\\mathbb{E}\\left[X_{1} X_{2} e_{i}^{2}\\right]=7 / 8\\). There-\n\n\nHomoskedastic Case\n\n\n\nHeteroskedastic Case\n\nFigure 7.3: Contours of Joint Distribution of \\(\\widehat{\\beta}_{1}\\) and \\(\\widehat{\\beta}_{2}\\)\nfore\n\\[\n\\begin{aligned}\n\\boldsymbol{V}_{\\beta} &=\\boldsymbol{Q}_{X X}^{-1} \\Omega \\boldsymbol{Q}_{X X}^{-1} \\\\\n&=\\frac{9}{16}\\left[\\begin{array}{cc}\n1 & -\\frac{1}{2} \\\\\n-\\frac{1}{2} & 1\n\\end{array}\\right]\\left[\\begin{array}{cc}\n1 & \\frac{7}{8} \\\\\n\\frac{7}{8} & 1\n\\end{array}\\right]\\left[\\begin{array}{cc}\n1 & -\\frac{1}{2} \\\\\n-\\frac{1}{2} & 1\n\\end{array}\\right] \\\\\n&=\\frac{4}{3}\\left[\\begin{array}{cc}\n1 & \\frac{1}{4} \\\\\n\\frac{1}{4} & 1\n\\end{array}\\right]\n\\end{aligned}\n\\]\nThus the coefficient estimators \\(\\widehat{\\beta}_{1}\\) and \\(\\widehat{\\beta}_{2}\\) are positively correlated (their correlation is \\(1 / 4\\).) The joint probability contours of their asymptotic distribution is displayed in Figure 7.3(b). We can see how the two estimators are positively associated.\nWhat we found through this example is that in the presence of heteroskedasticity there is no simple relationship between the correlation of the regressors and the correlation of the parameter estimators.\nWe can extend the above analysis to study the covariance between coefficient sub-vectors. For example, partitioning \\(X^{\\prime}=\\left(X_{1}^{\\prime}, X_{2}^{\\prime}\\right)\\) and \\(\\beta^{\\prime}=\\left(\\beta_{1}^{\\prime}, \\beta_{2}^{\\prime}\\right)\\), we can write the general model as\n\\[\nY=X_{1}^{\\prime} \\beta_{1}+X_{2}^{\\prime} \\beta_{2}+e\n\\]\nand the coefficient estimates as \\(\\widehat{\\beta}^{\\prime}=\\left(\\widehat{\\beta}_{1}^{\\prime}, \\widehat{\\beta}_{2}^{\\prime}\\right)\\). Make the partitions\n\\[\n\\boldsymbol{Q}_{X X}=\\left[\\begin{array}{ll}\n\\boldsymbol{Q}_{11} & \\boldsymbol{Q}_{12} \\\\\n\\boldsymbol{Q}_{21} & \\boldsymbol{Q}_{22}\n\\end{array}\\right], \\quad \\Omega=\\left[\\begin{array}{ll}\n\\Omega_{11} & \\Omega_{12} \\\\\n\\Omega_{21} & \\Omega_{22}\n\\end{array}\\right] .\n\\]\nFrom (2.43)\n\\[\n\\boldsymbol{Q}_{X X}^{-1}=\\left[\\begin{array}{cc}\n\\boldsymbol{Q}_{11 \\cdot 2}^{-1} & -\\boldsymbol{Q}_{11 \\cdot 2}^{-1} \\boldsymbol{Q}_{12} \\boldsymbol{Q}_{22}^{-1} \\\\\n-\\boldsymbol{Q}_{22 \\cdot 1}^{-1} \\boldsymbol{Q}_{21} \\boldsymbol{Q}_{11}^{-1} & \\boldsymbol{Q}_{22 \\cdot 1}^{-1}\n\\end{array}\\right]\n\\]\nwhere \\(\\boldsymbol{Q}_{11 \\cdot 2}=\\boldsymbol{Q}_{11}-\\boldsymbol{Q}_{12} \\boldsymbol{Q}_{22}^{-1} \\boldsymbol{Q}_{21}\\) and \\(\\boldsymbol{Q}_{22 \\cdot 1}=\\boldsymbol{Q}_{22}-\\boldsymbol{Q}_{21} \\boldsymbol{Q}_{11}^{-1} \\boldsymbol{Q}_{12}\\). Thus when the error is homoskedastic\n\\[\n\\operatorname{cov}\\left(\\widehat{\\beta}_{1}, \\widehat{\\beta}_{2}\\right)=-\\sigma^{2} \\boldsymbol{Q}_{11 \\cdot 2}^{-1} \\boldsymbol{Q}_{12} \\boldsymbol{Q}_{22}^{-1}\n\\]\nwhich is a matrix generalization of the two-regressor case.\nIn general you can show that (Exercise 7.5)\n\\[\n\\boldsymbol{V}_{\\boldsymbol{\\beta}}=\\left[\\begin{array}{ll}\n\\boldsymbol{V}_{11} & \\boldsymbol{V}_{12} \\\\\n\\boldsymbol{V}_{21} & \\boldsymbol{V}_{22}\n\\end{array}\\right]\n\\]\nwhere\n\\[\n\\begin{aligned}\n\\boldsymbol{V}_{11} &=\\boldsymbol{Q}_{11 \\cdot 2}^{-1}\\left(\\Omega_{11}-\\boldsymbol{Q}_{12} \\boldsymbol{Q}_{22}^{-1} \\Omega_{21}-\\Omega_{12} \\boldsymbol{Q}_{22}^{-1} \\boldsymbol{Q}_{21}+\\boldsymbol{Q}_{12} \\boldsymbol{Q}_{22}^{-1} \\Omega_{22} \\boldsymbol{Q}_{22}^{-1} \\boldsymbol{Q}_{21}\\right) \\boldsymbol{Q}_{11 \\cdot 2}^{-1} \\\\\n\\boldsymbol{V}_{21} &=\\boldsymbol{Q}_{22 \\cdot 1}^{-1}\\left(\\Omega_{21}-\\boldsymbol{Q}_{21} \\boldsymbol{Q}_{11}^{-1} \\Omega_{11}-\\Omega_{22} \\boldsymbol{Q}_{22}^{-1} \\boldsymbol{Q}_{21}+\\boldsymbol{Q}_{21} \\boldsymbol{Q}_{11}^{-1} \\Omega_{12} \\boldsymbol{Q}_{22}^{-1} \\boldsymbol{Q}_{21}\\right) \\boldsymbol{Q}_{11 \\cdot 2}^{-1} \\\\\n\\boldsymbol{V}_{22} &=\\boldsymbol{Q}_{22 \\cdot 1}^{-1}\\left(\\Omega_{22}-\\boldsymbol{Q}_{21} \\boldsymbol{Q}_{11}^{-1} \\Omega_{12}-\\Omega_{21} \\boldsymbol{Q}_{11}^{-1} \\boldsymbol{Q}_{12}+\\boldsymbol{Q}_{21} \\boldsymbol{Q}_{11}^{-1} \\Omega_{11} \\boldsymbol{Q}_{11}^{-1} \\boldsymbol{Q}_{12}\\right) \\boldsymbol{Q}_{22 \\cdot 1}^{-1}\n\\end{aligned}\n\\]\nUnfortunately, these expressions are not easily interpretable."
  },
  {
    "objectID": "chpt07-asymptotic-ls.html#consistency-of-error-variance-estimators",
    "href": "chpt07-asymptotic-ls.html#consistency-of-error-variance-estimators",
    "title": "7  Asymptotic Theory for Least Squares",
    "section": "7.5 Consistency of Error Variance Estimators",
    "text": "7.5 Consistency of Error Variance Estimators\nUsing the methods of Section \\(7.2\\) we can show that the estimators \\(\\widehat{\\sigma}^{2}=n^{-1} \\sum_{i=1}^{n} \\widehat{e}_{i}^{2}\\) and \\(s^{2}=(n-k)^{-1} \\sum_{i=1}^{n} \\widehat{e}_{i}^{2}\\) are consistent for \\(\\sigma^{2}\\).\nThe trick is to write the residual \\(\\widehat{e}_{i}\\) as equal to the error \\(e_{i}\\) plus a deviation\n\\[\n\\widehat{e}_{i}=Y_{i}-X_{i}^{\\prime} \\widehat{\\beta}=e_{i}-X_{i}^{\\prime}(\\widehat{\\beta}-\\beta) .\n\\]\nThus the squared residual equals the squared error plus a deviation\n\\[\n\\widehat{e}_{i}^{2}=e_{i}^{2}-2 e_{i} X_{i}^{\\prime}(\\widehat{\\beta}-\\beta)+(\\widehat{\\beta}-\\beta)^{\\prime} X_{i} X_{i}^{\\prime}(\\widehat{\\beta}-\\beta) .\n\\]\nSo when we take the average of the squared residuals we obtain the average of the squared errors, plus two terms which are (hopefully) asymptotically negligible. This average is:\n\\[\n\\widehat{\\sigma}^{2}=\\frac{1}{n} \\sum_{i=1}^{n} e_{i}^{2}-2\\left(\\frac{1}{n} \\sum_{i=1}^{n} e_{i} X_{i}^{\\prime}\\right)(\\widehat{\\beta}-\\beta)+(\\widehat{\\beta}-\\beta)^{\\prime}\\left(\\frac{1}{n} \\sum_{i=1}^{n} X_{i} X_{i}^{\\prime}\\right)(\\widehat{\\beta}-\\beta) .\n\\]\nThe WLLN implies that\n\\[\n\\begin{aligned}\n&\\frac{1}{n} \\sum_{i=1}^{n} e_{i}^{2} \\underset{p}{\\longrightarrow} \\sigma^{2} \\\\\n&\\frac{1}{n} \\sum_{i=1}^{n} e_{i} X_{i}^{\\prime} \\underset{p}{\\longrightarrow} \\mathbb{E}\\left[e X^{\\prime}\\right]=0 \\\\\n&\\frac{1}{n} \\sum_{i=1}^{n} X_{i} X_{i}^{\\prime} \\underset{p}{\\longrightarrow}\\left[X X^{\\prime}\\right]=\\boldsymbol{Q}_{X X}\n\\end{aligned}\n\\]\nTheorem \\(7.1\\) shows that \\(\\widehat{\\beta} \\underset{p}{\\rightarrow} \\beta\\). Hence (7.18) converges in probability to \\(\\sigma^{2}\\) as desired.\nFinally, since \\(n /(n-k) \\rightarrow 1\\) as \\(n \\rightarrow \\infty\\) it follows that \\(s^{2}=\\left(\\frac{n}{n-k}\\right) \\widehat{\\sigma}^{2} \\underset{p}{\\rightarrow} \\sigma^{2}\\). Thus both estimators are consistent. Theorem 7.4 Under Assumption 7.1, \\(\\widehat{\\sigma}^{2} \\underset{p}{\\longrightarrow} \\sigma^{2}\\) and \\(s^{2} \\underset{p}{\\rightarrow} \\sigma^{2}\\) as \\(n \\rightarrow \\infty\\)."
  },
  {
    "objectID": "chpt07-asymptotic-ls.html#homoskedastic-covariance-matrix-estimation",
    "href": "chpt07-asymptotic-ls.html#homoskedastic-covariance-matrix-estimation",
    "title": "7  Asymptotic Theory for Least Squares",
    "section": "7.6 Homoskedastic Covariance Matrix Estimation",
    "text": "7.6 Homoskedastic Covariance Matrix Estimation\nTheorem \\(7.3\\) shows that \\(\\sqrt{n}(\\widehat{\\beta}-\\beta)\\) is asymptotically normal with asymptotic covariance matrix \\(\\boldsymbol{V}_{\\beta}\\). For asymptotic inference (confidence intervals and tests) we need a consistent estimator of \\(\\boldsymbol{V}_{\\beta}\\). Under homoskedasticity \\(\\boldsymbol{V}_{\\beta}\\) simplifies to \\(\\boldsymbol{V}_{\\beta}^{0}=\\boldsymbol{Q}_{X X}^{-1} \\sigma^{2}\\) and in this section we consider the simplified problem of estimating \\(V_{\\beta}^{0}\\).\nThe standard moment estimator of \\(\\boldsymbol{Q}_{X X}\\) is \\(\\widehat{\\boldsymbol{Q}}_{X X}\\) defined in (7.1) and thus an estimator for \\(\\boldsymbol{Q}_{X X}^{-1}\\) is \\(\\widehat{\\boldsymbol{Q}}_{X X}^{-1}\\). The standard estimator of \\(\\sigma^{2}\\) is the unbiased estimator \\(s^{2}\\) defined in (4.31). Thus a natural plug-in estimator for \\(\\boldsymbol{V}_{\\beta}^{0}=\\boldsymbol{Q}_{X X}^{-1} \\sigma^{2}\\) is \\(\\widehat{\\boldsymbol{V}}_{\\beta}^{0}=\\widehat{\\boldsymbol{Q}}_{X X}^{-1} s^{2}\\).\nConsistency of \\(\\widehat{\\boldsymbol{V}}_{\\beta}^{0}\\) for \\(\\boldsymbol{V}_{\\beta}^{0}\\) follows from consistency of the moment estimators \\(\\widehat{\\boldsymbol{Q}}_{X X}\\) and \\(s^{2}\\) and an application of the continuous mapping theorem. Specifically, Theorem \\(7.1\\) established \\(\\widehat{\\boldsymbol{Q}}_{X X} \\underset{p}{\\rightarrow} \\boldsymbol{Q}_{X X}\\), and Theorem \\(7.4\\) established \\(s^{2} \\underset{p}{\\rightarrow} \\sigma^{2}\\). The function \\(\\boldsymbol{V}_{\\beta}^{0}=\\boldsymbol{Q}_{X X}^{-1} \\sigma^{2}\\) is a continuous function of \\(\\boldsymbol{Q}_{X X}\\) and \\(\\sigma^{2}\\) so long as \\(\\boldsymbol{Q}_{X X}>0\\), which holds true under Assumption 7.1.4. It follows by the CMT that\n\\[\n\\widehat{\\boldsymbol{V}}_{\\beta}^{0}=\\widehat{\\boldsymbol{Q}}_{X X}^{-1} s^{2} \\underset{p}{\\longrightarrow} \\boldsymbol{Q}_{X X}^{-1} \\sigma^{2}=\\boldsymbol{V}_{\\beta}^{0}\n\\]\nso that \\(\\widehat{\\boldsymbol{V}}_{\\beta}^{0}\\) is consistent for \\(\\boldsymbol{V}_{\\beta}^{0}\\).\nTheorem 7.5 Under Assumption 7.1, \\(\\widehat{\\boldsymbol{V}}_{\\beta}^{0} \\underset{p}{\\rightarrow} \\boldsymbol{V}_{\\beta}^{0}\\) as \\(n \\rightarrow \\infty\\)\nIt is instructive to notice that Theorem \\(7.5\\) does not require the assumption of homoskedasticity. That is, \\(\\widehat{V}_{\\beta}^{0}\\) is consistent for \\(\\boldsymbol{V}_{\\beta}^{0}\\) regardless if the regression is homoskedastic or heteroskedastic. However, \\(\\boldsymbol{V}_{\\beta}^{0}=\\boldsymbol{V}_{\\beta}=\\operatorname{avar}[\\widehat{\\beta}]\\) only under homoskedasticity. Thus, in the general case \\(\\widehat{\\boldsymbol{V}}_{\\beta}^{0}\\) is consistent for a welldefined but non-useful object."
  },
  {
    "objectID": "chpt07-asymptotic-ls.html#heteroskedastic-covariance-matrix-estimation",
    "href": "chpt07-asymptotic-ls.html#heteroskedastic-covariance-matrix-estimation",
    "title": "7  Asymptotic Theory for Least Squares",
    "section": "7.7 Heteroskedastic Covariance Matrix Estimation",
    "text": "7.7 Heteroskedastic Covariance Matrix Estimation\nTheorems \\(7.3\\) established that the asymptotic covariance matrix of \\(\\sqrt{n}(\\widehat{\\beta}-\\beta)\\) is \\(\\boldsymbol{V}_{\\beta}=\\boldsymbol{Q}_{X X}^{-1} \\Omega \\boldsymbol{Q}_{X X}^{-1}\\). We now consider estimation of this covariance matrix without imposing homoskedasticity. The standard approach is to use a plug-in estimator which replaces the unknowns with sample moments.\nAs described in the previous section a natural estimator for \\(\\boldsymbol{Q}_{X X}^{-1}\\) is \\(\\widehat{\\boldsymbol{Q}}_{X X}^{-1}\\) where \\(\\widehat{\\boldsymbol{Q}}_{X X}\\) defined in (7.1). The moment estimator for \\(\\Omega\\) is\n\\[\n\\widehat{\\Omega}=\\frac{1}{n} \\sum_{i=1}^{n} X_{i} X_{i}^{\\prime} \\widehat{e}_{i}^{2},\n\\]\nleading to the plug-in covariance matrix estimator\n\\[\n\\widehat{\\boldsymbol{V}}_{\\beta}^{\\mathrm{HC} 0}=\\widehat{\\boldsymbol{Q}}_{X X}^{-1} \\widehat{\\Omega} \\widehat{\\boldsymbol{Q}}_{X X}^{-1} .\n\\]\nYou can check that \\(\\widehat{\\boldsymbol{V}}_{\\beta}^{\\mathrm{HC} 0}=n \\widehat{\\boldsymbol{V}}_{\\widehat{\\beta}}^{\\mathrm{HC} 0}\\) where \\(\\widehat{\\boldsymbol{V}}_{\\widehat{\\beta}}^{\\mathrm{HC} 0}\\) is the HC0 covariance matrix estimator from (4.36).\nAs shown in Theorem 7.1, \\(\\widehat{\\boldsymbol{Q}}_{X X}^{-1} \\underset{p}{\\rightarrow} \\boldsymbol{Q}_{X X}^{-1}\\), so we just need to verify the consistency of \\(\\widehat{\\Omega}\\). The key is to replace the squared residual \\(\\widehat{e}_{i}^{2}\\) with the squared error \\(e_{i}^{2}\\), and then show that the difference is asymptotically negligible.\nSpecifically, observe that\n\\[\n\\begin{aligned}\n\\widehat{\\Omega} &=\\frac{1}{n} \\sum_{i=1}^{n} X_{i} X_{i}^{\\prime} \\widehat{e}_{i}^{2} \\\\\n&=\\frac{1}{n} \\sum_{i=1}^{n} X_{i} X_{i}^{\\prime} e_{i}^{2}+\\frac{1}{n} \\sum_{i=1}^{n} X_{i} X_{i}^{\\prime}\\left(\\widehat{e}_{i}^{2}-e_{i}^{2}\\right) .\n\\end{aligned}\n\\]\nThe first term is an average of the i.i.d. random variables \\(X_{i} X_{i}^{\\prime} e_{i}^{2}\\), and therefore by the WLLN converges in probability to its expectation, namely,\n\\[\n\\frac{1}{n} \\sum_{i=1}^{n} X_{i} X_{i}^{\\prime} e_{i}^{2} \\underset{p}{\\longrightarrow}\\left[X X^{\\prime} e^{2}\\right]=\\Omega .\n\\]\nTechnically, this requires that \\(\\Omega\\) has finite elements, which was shown in (7.6).\nTo establish that \\(\\widehat{\\Omega}\\) is consistent for \\(\\Omega\\) it remains to show that\n\\[\n\\frac{1}{n} \\sum_{i=1}^{n} X_{i} X_{i}^{\\prime}\\left(\\widehat{e}_{i}^{2}-e_{i}^{2}\\right) \\underset{p}{\\longrightarrow} 0\n\\]\nThere are multiple ways to do this. A reasonably straightforward yet slightly tedious derivation is to start by applying the triangle inequality (B.16) using a matrix norm:\n\\[\n\\begin{aligned}\n\\left\\|\\frac{1}{n} \\sum_{i=1}^{n} X_{i} X_{i}^{\\prime}\\left(\\widehat{e}_{i}^{2}-e_{i}^{2}\\right)\\right\\| & \\leq \\frac{1}{n} \\sum_{i=1}^{n}\\left\\|X_{i} X_{i}^{\\prime}\\left(\\widehat{e}_{i}^{2}-e_{i}^{2}\\right)\\right\\| \\\\\n&=\\frac{1}{n} \\sum_{i=1}^{n}\\left\\|X_{i}\\right\\|^{2}\\left|\\widehat{e}_{i}^{2}-e_{i}^{2}\\right| .\n\\end{aligned}\n\\]\nThen recalling the expression for the squared residual (7.17), apply the triangle inequality (B.1) and then the Schwarz inequality (B.12) twice\n\\[\n\\begin{aligned}\n\\left|\\widehat{e}_{i}^{2}-e_{i}^{2}\\right| & \\leq 2\\left|e_{i} X_{i}^{\\prime}(\\widehat{\\beta}-\\beta)\\right|+(\\widehat{\\beta}-\\beta)^{\\prime} X_{i} X_{i}^{\\prime}(\\widehat{\\beta}-\\beta) \\\\\n&=2\\left|e_{i}\\right|\\left|X_{i}^{\\prime}(\\widehat{\\beta}-\\beta)\\right|+\\left|(\\widehat{\\beta}-\\beta)^{\\prime} X_{i}\\right|^{2} \\\\\n& \\leq 2\\left|e_{i}\\right|\\left\\|X_{i}\\right\\|\\|\\widehat{\\beta}-\\beta\\|+\\left\\|X_{i}\\right\\|^{2}\\|\\widehat{\\beta}-\\beta\\|^{2}\n\\end{aligned}\n\\]\nCombining (7.21) and (7.22), we find\n\\[\n\\begin{aligned}\n\\left\\|\\frac{1}{n} \\sum_{i=1}^{n} X_{i} X_{i}^{\\prime}\\left(\\widehat{e}_{i}^{2}-e_{i}^{2}\\right)\\right\\| & \\leq 2\\left(\\frac{1}{n} \\sum_{i=1}^{n}\\left\\|X_{i}\\right\\|^{3}\\left|e_{i}\\right|\\right)\\|\\widehat{\\beta}-\\beta\\|+\\left(\\frac{1}{n} \\sum_{i=1}^{n}\\left\\|X_{i}\\right\\|^{4}\\right)\\|\\widehat{\\beta}-\\beta\\|^{2} \\\\\n&=o_{p}(1) .\n\\end{aligned}\n\\]\nThe expression is \\(o_{p}(1)\\) because \\(\\|\\widehat{\\beta}-\\beta\\| \\underset{p}{\\longrightarrow} 0\\) and both averages in parenthesis are averages of random variables with finite expectation under Assumption \\(7.2\\) (and are thus \\(O_{p}(1)\\) ). Indeed, by Hölder’s inequality (B.31)\n\\[\n\\mathbb{E}\\left[\\|X\\|^{3}|e|\\right] \\leq\\left(\\mathbb{E}\\left[\\left(\\|X\\|^{3}\\right)^{4 / 3}\\right]\\right)^{3 / 4}\\left(\\mathbb{E}\\left[e^{4}\\right]\\right)^{1 / 4}=\\left(\\mathbb{E}\\|X\\|^{4}\\right)^{3 / 4}\\left(\\mathbb{E}\\left[e^{4}\\right]\\right)^{1 / 4}<\\infty .\n\\]\nWe have established (7.20) as desired. Theorem 7.6 Under Assumption 7.2, as \\(n \\rightarrow \\infty, \\widehat{\\Omega} \\underset{p}{\\longrightarrow} \\Omega\\) and \\(\\widehat{\\boldsymbol{V}}_{\\beta}^{\\mathrm{HC} 0} \\underset{p}{\\longrightarrow} \\boldsymbol{V}_{\\beta}\\)\nFor an alternative proof of this result, see Section 7.20."
  },
  {
    "objectID": "chpt07-asymptotic-ls.html#summary-of-covariance-matrix-notation",
    "href": "chpt07-asymptotic-ls.html#summary-of-covariance-matrix-notation",
    "title": "7  Asymptotic Theory for Least Squares",
    "section": "7.8 Summary of Covariance Matrix Notation",
    "text": "7.8 Summary of Covariance Matrix Notation\nThe notation we have introduced may be somewhat confusing so it is helpful to write it down in one place.\nThe exact variance of \\(\\widehat{\\beta}\\) (under the assumptions of the linear regression model) and the asymptotic variance of \\(\\sqrt{n}(\\widehat{\\beta}-\\beta)\\) (under the more general assumptions of the linear projection model) are\n\\[\n\\begin{aligned}\n&\\boldsymbol{V}_{\\widehat{\\beta}}=\\operatorname{var}[\\widehat{\\beta} \\mid \\boldsymbol{X}]=\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{D} \\boldsymbol{X}\\right)\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\\\\n&\\boldsymbol{V}_{\\beta}=\\operatorname{avar}[\\sqrt{n}(\\widehat{\\beta}-\\beta)]=\\boldsymbol{Q}_{X X}^{-1} \\Omega \\boldsymbol{Q}_{X X}^{-1}\n\\end{aligned}\n\\]\nThe HC0 estimators of these two covariance matrices are\n\\[\n\\begin{aligned}\n\\widehat{\\boldsymbol{V}}_{\\widehat{\\beta}}^{\\mathrm{HC} 0} &=\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1}\\left(\\sum_{i=1}^{n} X_{i} X_{i}^{\\prime} \\widehat{e}_{i}^{2}\\right)\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\\\\n\\widehat{\\boldsymbol{V}}_{\\beta}^{\\mathrm{HC} 0} &=\\widehat{\\boldsymbol{Q}}_{X X}^{-1} \\widehat{\\Omega} \\widehat{\\boldsymbol{Q}}_{X X}^{-1}\n\\end{aligned}\n\\]\nand satisfy the simple relationship \\(\\widehat{\\boldsymbol{V}}_{\\beta}^{\\mathrm{HC} 0}=n \\widehat{\\boldsymbol{V}}_{\\widehat{\\beta}}^{\\mathrm{HC}}\\).\nSimilarly, under the assumption of homoskedasticity the exact and asymptotic variances simplify to\n\\[\n\\begin{aligned}\n\\boldsymbol{V}_{\\widehat{\\beta}}^{0} &=\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\sigma^{2} \\\\\n\\boldsymbol{V}_{\\beta}^{0} &=\\boldsymbol{Q}_{X X}^{-1} \\sigma^{2} .\n\\end{aligned}\n\\]\nTheir standard estimators are\n\\[\n\\begin{aligned}\n\\widehat{\\boldsymbol{V}}_{\\widehat{\\beta}}^{0} &=\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} s^{2} \\\\\n\\widehat{\\boldsymbol{V}}_{\\beta}^{0} &=\\widehat{\\boldsymbol{Q}}_{X X}^{-1} s^{2}\n\\end{aligned}\n\\]\nwhich also satisfy the relationship \\(\\widehat{\\boldsymbol{V}}_{\\beta}^{0}=n \\widehat{\\boldsymbol{V}}_{\\widehat{\\beta}}^{0}\\).\nThe exact formula and estimators are useful when constructing test statistics and standard errors. However, for theoretical purposes the asymptotic formula (variances and their estimates) are more useful as these retain non-generate limits as the sample sizes diverge. That is why both sets of notation are useful."
  },
  {
    "objectID": "chpt07-asymptotic-ls.html#alternative-covariance-matrix-estimators",
    "href": "chpt07-asymptotic-ls.html#alternative-covariance-matrix-estimators",
    "title": "7  Asymptotic Theory for Least Squares",
    "section": "7.9 Alternative Covariance Matrix Estimators*",
    "text": "7.9 Alternative Covariance Matrix Estimators*\nIn Section \\(7.7\\) we introduced \\(\\widehat{\\boldsymbol{V}}_{\\beta}^{\\mathrm{HC} 0}\\) as an estimator of \\(\\boldsymbol{V}_{\\beta} \\cdot \\widehat{\\boldsymbol{V}}_{\\beta}^{\\mathrm{HC} 0}\\) is a scaled version of \\(\\widehat{\\boldsymbol{V}}_{\\widehat{\\beta}}^{\\mathrm{HC} 0}\\) from Section 4.14, where we also introduced the alternative HC1, HC2, and HC3 heteroskedasticity-robust covariance matrix estimators. We now discuss the consistency properties of these estimators.\nTo do so we introduce their scaled versions, e.g. \\(\\widehat{\\boldsymbol{V}}_{\\beta}^{\\mathrm{HC} 1}=n \\widehat{\\boldsymbol{V}}_{\\widehat{\\beta}}^{\\mathrm{HC} 1}, \\widehat{\\boldsymbol{V}}_{\\beta}^{\\mathrm{HC} 2}=n \\widehat{\\boldsymbol{V}}_{\\widehat{\\beta}}^{\\mathrm{HC} 2}\\), and \\(\\widehat{\\boldsymbol{V}}_{\\beta}^{\\mathrm{HC} 3}=n \\widehat{\\boldsymbol{V}}_{\\widehat{\\beta}}^{\\mathrm{HC} 3}\\). These are (alternative) estimators of the asymptotic covariance matrix \\(\\boldsymbol{V}_{\\beta}\\). First, consider \\(\\widehat{\\boldsymbol{V}}_{\\beta}^{\\mathrm{HC} 1}\\). Notice that \\(\\widehat{\\boldsymbol{V}}_{\\beta}^{\\mathrm{HC} 1}=n \\widehat{\\boldsymbol{V}}_{\\widehat{\\beta}}^{\\mathrm{HC1}}=\\frac{n}{n-k} \\widehat{\\boldsymbol{V}}_{\\beta}^{\\mathrm{HC} 0}\\) where \\(\\widehat{\\boldsymbol{V}}_{\\beta}^{\\mathrm{HC}}\\) was defined in (7.19) and shown consistent for \\(\\boldsymbol{V}_{\\beta}\\) in Theorem 7.6. If \\(k\\) is fixed as \\(n \\rightarrow \\infty\\), then \\(\\frac{n}{n-k} \\rightarrow 1\\) and thus\n\\[\n\\widehat{\\boldsymbol{V}}_{\\beta}^{\\mathrm{HC} 1}=(1+o(1)) \\widehat{\\boldsymbol{V}}_{\\beta}^{\\mathrm{HC} 0} \\underset{p}{\\longrightarrow} \\boldsymbol{V}_{\\beta} .\n\\]\nThus \\(\\widehat{\\boldsymbol{V}}_{\\beta}^{\\mathrm{HC} 1}\\) is consistent for \\(\\boldsymbol{V}_{\\beta}\\).\nThe alternative estimators \\(\\widehat{\\boldsymbol{V}}_{\\beta}^{\\mathrm{HC} 2}\\) and \\(\\widehat{\\boldsymbol{V}}_{\\beta}^{\\mathrm{HC} 3}\\) take the form (7.19) but with \\(\\widehat{\\Omega}\\) replaced by\n\\[\n\\widetilde{\\Omega}=\\frac{1}{n} \\sum_{i=1}^{n}\\left(1-h_{i i}\\right)^{-2} X_{i} X_{i}^{\\prime} \\widehat{e}_{i}^{2}\n\\]\nand\n\\[\n\\bar{\\Omega}=\\frac{1}{n} \\sum_{i=1}^{n}\\left(1-h_{i i}\\right)^{-1} X_{i} X_{i}^{\\prime} \\hat{e}_{i}^{2},\n\\]\nrespectively. To show that these estimators also consistent for \\(\\boldsymbol{V}_{\\beta}\\) given \\(\\widehat{\\Omega} \\underset{p}{\\vec{a}} \\Omega\\) it is sufficient to show that the differences \\(\\widetilde{\\Omega}-\\widehat{\\Omega}\\) and \\(\\bar{\\Omega}-\\widehat{\\Omega}\\) converge in probability to zero as \\(n \\rightarrow \\infty\\).\nThe trick is the fact that the leverage values are asymptotically negligible:\n\\[\nh_{n}^{*}=\\max _{1 \\leq i \\leq n} h_{i i}=o_{p}(1) .\n\\]\n(See Theorem \\(7.17\\) in Section 7.21.) Then using the triangle inequality (B.16)\n\\[\n\\begin{aligned}\n\\|\\bar{\\Omega}-\\widehat{\\Omega}\\| & \\leq \\frac{1}{n} \\sum_{i=1}^{n}\\left\\|X_{i} X_{i}^{\\prime}\\right\\| \\widehat{e}_{i}^{2}\\left|\\left(1-h_{i i}\\right)^{-1}-1\\right| \\\\\n& \\leq\\left(\\frac{1}{n} \\sum_{i=1}^{n}\\left\\|X_{i}\\right\\|^{2} \\widehat{e}_{i}^{2}\\right)\\left|\\left(1-h_{n}^{*}\\right)^{-1}-1\\right| .\n\\end{aligned}\n\\]\nThe sum in parenthesis can be shown to be \\(O_{p}(1)\\) under Assumption \\(7.2\\) by the same argument as in in the proof of Theorem 7.6. (In fact, it can be shown to converge in probability to \\(\\mathbb{E}\\left[\\|X\\|^{2} e^{2}\\right]\\).) The term in absolute values is \\(o_{p}(1)\\) by (7.24). Thus the product is \\(o_{p}(1)\\) which means that \\(\\bar{\\Omega}=\\widehat{\\Omega}+o_{p}(1) \\underset{p}{\\longrightarrow}\\).\nSimilarly,\n\\[\n\\begin{aligned}\n\\|\\widetilde{\\Omega}-\\widehat{\\Omega}\\| & \\leq \\frac{1}{n} \\sum_{i=1}^{n}\\left\\|X_{i} X_{i}^{\\prime}\\right\\| \\widehat{e}_{i}^{2}\\left|\\left(1-h_{i i}\\right)^{-2}-1\\right| \\\\\n& \\leq\\left(\\frac{1}{n} \\sum_{i=1}^{n}\\left\\|X_{i}\\right\\|^{2} \\widehat{e}_{i}^{2}\\right)\\left|\\left(1-h_{n}^{*}\\right)^{-2}-1\\right| \\\\\n&=o_{p}(1) .\n\\end{aligned}\n\\]\nTheorem 7.7 Under Assumption 7.2, as \\(n \\rightarrow \\infty, \\widetilde{\\Omega} \\underset{p}{\\longrightarrow} \\Omega, \\bar{\\Omega} \\underset{p}{\\longrightarrow} \\Omega, \\widehat{V}_{\\beta}^{\\mathrm{HC1}} \\underset{p}{\\longrightarrow}\\) \\(\\boldsymbol{V}_{\\beta}, \\widehat{\\boldsymbol{V}}_{\\beta}^{\\mathrm{HC} 2} \\underset{p}{\\longrightarrow} \\boldsymbol{V}_{\\beta}\\), and \\(\\widehat{\\boldsymbol{V}}_{\\beta}^{\\mathrm{HC} 3} \\underset{p}{\\longrightarrow} \\boldsymbol{V}_{\\beta}\\)\nTheorem \\(7.7\\) shows that the alternative covariance matrix estimators are also consistent for the asymptotic covariance matrix.\nTo simplify notation, for the remainder of the chapter we will use the notation \\(\\widehat{\\boldsymbol{V}}_{\\beta}\\) and \\(\\widehat{\\boldsymbol{V}}_{\\widehat{\\beta}}\\) to refer to any of the heteroskedasticity-consistent covariance matrix estimators \\(\\mathrm{HC}\\), \\(\\mathrm{HC} 1\\), HC2, and \\(\\mathrm{HC3}\\), as they all have the same asymptotic limits."
  },
  {
    "objectID": "chpt07-asymptotic-ls.html#functions-of-parameters",
    "href": "chpt07-asymptotic-ls.html#functions-of-parameters",
    "title": "7  Asymptotic Theory for Least Squares",
    "section": "7.10 Functions of Parameters",
    "text": "7.10 Functions of Parameters\nIn most serious applications a researcher is actually interested in a specific transformation of the coefficient vector \\(\\beta=\\left(\\beta_{1}, \\ldots, \\beta_{k}\\right)\\). For example, the researcher may be interested in a single coefficient \\(\\beta_{j}\\) or a ratio \\(\\beta_{j} / \\beta_{l}\\). More generally, interest may focus on a quantity such as consumer surplus which could be a complicated function of the coefficients. In any of these cases we can write the parameter of interest \\(\\theta\\) as a function of the coefficients, e.g. \\(\\theta=r(\\beta)\\) for some function \\(r: \\mathbb{R}^{k} \\rightarrow \\mathbb{R}^{q}\\). The estimate of \\(\\theta\\) is\n\\[\n\\widehat{\\theta}=r(\\widehat{\\beta}) .\n\\]\nBy the continuous mapping theorem (Theorem 6.6) and the fact \\(\\widehat{\\beta} \\underset{p}{\\longrightarrow} \\beta\\) we can deduce that \\(\\widehat{\\theta}\\) is consistent for \\(\\theta\\) if the function \\(r(\\cdot)\\) is continuous.\nTheorem 7.8 Under Assumption 7.1, if \\(r(\\beta)\\) is continuous at the true value of \\(\\beta\\) then as \\(n \\rightarrow \\infty, \\widehat{\\theta} \\underset{p}{\\longrightarrow} \\theta\\)\nFurthermore, if the transformation is sufficiently smooth, by the Delta Method (Theorem 6.8) we can show that \\(\\widehat{\\theta}\\) is asymptotically normal.\nAssumption 7.3 \\(r(\\beta): \\mathbb{R}^{k} \\rightarrow \\mathbb{R}^{q}\\) is continuously differentiable at the true value of \\(\\beta\\) and \\(\\boldsymbol{R}=\\frac{\\partial}{\\partial \\beta} r(\\beta)^{\\prime}\\) has rank \\(q\\).\nTheorem 7.9 Asymptotic Distribution of Functions of Parameters Under Assumptions \\(7.2\\) and 7.3, as \\(n \\rightarrow \\infty\\),\n\\[\n\\sqrt{n}(\\widehat{\\theta}-\\theta) \\underset{d}{\\longrightarrow} \\mathrm{N}\\left(0, \\boldsymbol{V}_{\\theta}\\right)\n\\]\nwhere \\(\\boldsymbol{V}_{\\theta}=\\boldsymbol{R}^{\\prime} \\boldsymbol{V}_{\\beta} \\boldsymbol{R}\\).\nIn many cases the function \\(r(\\beta)\\) is linear:\n\\[\nr(\\beta)=\\boldsymbol{R}^{\\prime} \\beta\n\\]\nfor some \\(k \\times q\\) matrix \\(\\boldsymbol{R}\\). In particular if \\(\\boldsymbol{R}\\) is a “selector matrix”\n\\[\n\\boldsymbol{R}=\\left(\\begin{array}{l}\n\\boldsymbol{I} \\\\\n0\n\\end{array}\\right)\n\\]\nthen we can partition \\(\\beta=\\left(\\beta_{1}^{\\prime}, \\beta_{2}^{\\prime}\\right)^{\\prime}\\) so that \\(\\boldsymbol{R}^{\\prime} \\beta=\\beta_{1}\\). Then\n\\[\n\\boldsymbol{V}_{\\boldsymbol{\\theta}}=\\left(\\begin{array}{ll}\n\\boldsymbol{I} & 0\n\\end{array}\\right) \\boldsymbol{V}_{\\beta}\\left(\\begin{array}{l}\n\\boldsymbol{I} \\\\\n0\n\\end{array}\\right)=\\boldsymbol{V}_{11},\n\\]\nthe upper-left sub-matrix of \\(V_{11}\\) given in (7.14). In this case (7.25) states that\n\\[\n\\sqrt{n}\\left(\\widehat{\\beta}_{1}-\\beta_{1}\\right) \\underset{d}{\\longrightarrow} \\mathrm{N}\\left(0, V_{11}\\right) .\n\\]\nThat is, subsets of \\(\\widehat{\\beta}\\) are approximately normal with variances given by the conformable subcomponents of \\(V\\).\nTo illustrate the case of a nonlinear transformation take the example \\(\\theta=\\beta_{j} / \\beta_{l}\\) for \\(j \\neq l\\). Then\n\\[\n\\boldsymbol{R}=\\frac{\\partial}{\\partial \\beta} r(\\beta)=\\left(\\begin{array}{c}\n\\frac{\\partial}{\\partial \\beta_{1}}\\left(\\beta_{j} / \\beta_{l}\\right) \\\\\n\\vdots \\\\\n\\frac{\\partial}{\\partial \\beta_{j}}\\left(\\beta_{j} / \\beta_{l}\\right) \\\\\n\\vdots \\\\\n\\frac{\\partial}{\\partial \\beta_{\\ell}}\\left(\\beta_{j} / \\beta_{l}\\right) \\\\\n\\vdots \\\\\n\\frac{\\partial}{\\partial \\beta_{k}}\\left(\\beta_{j} / \\beta_{l}\\right)\n\\end{array}\\right)=\\left(\\begin{array}{c}\n0 \\\\\n\\vdots \\\\\n1 / \\beta_{l} \\\\\n\\vdots \\\\\n-\\beta_{j} / \\beta_{l}^{2} \\\\\n\\vdots \\\\\n0\n\\end{array}\\right)\n\\]\nso\n\\[\n\\boldsymbol{V}_{\\theta}=\\boldsymbol{V}_{j j} / \\beta_{l}^{2}+\\boldsymbol{V}_{l l} \\beta_{j}^{2} / \\beta_{l}^{4}-2 \\boldsymbol{V}_{j l} \\beta_{j} / \\beta_{l}^{3}\n\\]\nwhere \\(\\boldsymbol{V}_{a b}\\) denotes the \\(a b^{t h}\\) element of \\(\\boldsymbol{V}_{\\beta}\\).\nFor inference we need an estimator of the asymptotic covariance matrix \\(\\boldsymbol{V}_{\\theta}=\\boldsymbol{R}^{\\prime} \\boldsymbol{V}_{\\beta} \\boldsymbol{R}\\). For this it is typical to use the plug-in estimator\n\\[\n\\widehat{\\boldsymbol{R}}=\\frac{\\partial}{\\partial \\beta} r(\\widehat{\\beta})^{\\prime} .\n\\]\nThe derivative in (7.27) may be calculated analytically or numerically. By analytically, we mean working out the formula for the derivative and replacing the unknowns by point estimates. For example, if \\(\\theta=\\) \\(\\beta_{j} / \\beta_{l}\\) then \\(\\frac{\\partial}{\\partial \\beta} r(\\beta)\\) is (7.26). However in some cases the function \\(r(\\beta)\\) may be extremely complicated and a formula for the analytic derivative may not be easily available. In this case numerical differentiation may be preferable. Let \\(\\delta_{l}=(0 \\cdots 1 \\cdots 0)^{\\prime}\\) be the unit vector with the ” 1 ” in the \\(l^{\\text {th }}\\) place. The \\(j l^{t h}\\) element of a numerical derivative \\(\\widehat{\\boldsymbol{R}}\\) is\nfor some small \\(\\epsilon\\).\n\\[\n\\widehat{\\boldsymbol{R}}_{j l}=\\frac{r_{j}\\left(\\widehat{\\beta}+\\delta_{l} \\epsilon\\right)-r_{j}(\\widehat{\\beta})}{\\epsilon}\n\\]\nThe estimator of \\(\\boldsymbol{V}_{\\theta}\\) is\n\\[\n\\widehat{\\boldsymbol{V}}_{\\theta}=\\widehat{\\boldsymbol{R}}^{\\prime} \\widehat{\\boldsymbol{V}}_{\\beta} \\widehat{\\boldsymbol{R}} \\text {. }\n\\]\nAlternatively, the homoskedastic covariance matrix estimator could be used leading to a homoskedastic covariance matrix estimator for \\(\\theta\\).\n\\[\n\\widehat{\\boldsymbol{V}}_{\\theta}^{0}=\\widehat{\\boldsymbol{R}}^{\\prime} \\widehat{\\boldsymbol{V}}_{\\beta}^{0} \\widehat{\\boldsymbol{R}}=\\widehat{\\boldsymbol{R}}^{\\prime} \\widehat{\\boldsymbol{Q}}_{X X}^{-1} \\widehat{\\boldsymbol{R}} s^{2} .\n\\]\nGiven (7.27), (7.28) and (7.29) are simple to calculate using matrix operations.\nAs the primary justification for \\(\\widehat{\\boldsymbol{V}}_{\\theta}\\) is the asymptotic approximation (7.25), \\(\\widehat{\\boldsymbol{V}}_{\\theta}\\) is often called an asymptotic covariance matrix estimator.\nThe estimator \\(\\widehat{\\boldsymbol{V}}_{\\theta}\\) is consistent for \\(\\boldsymbol{V}_{\\theta}\\) under the conditions of Theorem \\(7.9\\) because \\(\\widehat{\\boldsymbol{V}}_{\\beta} \\vec{p}_{\\boldsymbol{V}}\\) by Theorem \\(7.6\\) and\n\\[\n\\widehat{\\boldsymbol{R}}=\\frac{\\partial}{\\partial \\beta} r(\\widehat{\\beta})^{\\prime} \\underset{p}{\\Rightarrow} \\frac{\\partial}{\\partial \\beta} r(\\beta)^{\\prime}=\\boldsymbol{R}\n\\]\nbecause \\(\\widehat{\\beta} \\underset{p}{\\longrightarrow} \\beta\\) and the function \\(\\frac{\\partial}{\\partial \\beta} r(\\beta)^{\\prime}\\) is continuous in \\(\\beta\\). Theorem 7.10 Under Assumptions \\(7.2\\) and 7.3, as \\(n \\rightarrow \\infty, \\widehat{\\boldsymbol{V}}_{\\theta} \\underset{p}{\\longrightarrow} \\boldsymbol{V}_{\\theta}\\)\nTheorem 7.10 shows that \\(\\widehat{\\boldsymbol{V}}_{\\theta}\\) is consistent for \\(\\boldsymbol{V}_{\\theta}\\) and thus may be used for asymptotic inference. In practice we may set\n\\[\n\\widehat{\\boldsymbol{V}}_{\\widehat{\\theta}}=\\widehat{\\boldsymbol{R}}^{\\prime} \\widehat{\\boldsymbol{V}}_{\\widehat{\\beta}} \\widehat{\\boldsymbol{R}}=n^{-1} \\widehat{\\boldsymbol{R}}^{\\prime} \\widehat{\\boldsymbol{V}}_{\\beta} \\widehat{\\boldsymbol{R}}\n\\]\nas an estimator of the variance of \\(\\widehat{\\theta}\\)."
  },
  {
    "objectID": "chpt07-asymptotic-ls.html#asymptotic-standard-errors",
    "href": "chpt07-asymptotic-ls.html#asymptotic-standard-errors",
    "title": "7  Asymptotic Theory for Least Squares",
    "section": "7.11 Asymptotic Standard Errors",
    "text": "7.11 Asymptotic Standard Errors\nAs described in Section 4.15, a standard error is an estimator of the standard deviation of the distribution of an estimator. Thus if \\(\\widehat{\\boldsymbol{V}}_{\\widehat{\\beta}}\\) is an estimator of the covariance matrix of \\(\\widehat{\\beta}\\) then standard errors are the square roots of the diagonal elements of this matrix. These take the form\n\\[\ns\\left(\\widehat{\\beta}_{j}\\right)=\\sqrt{\\widehat{\\boldsymbol{V}}_{\\widehat{\\beta}_{j}}}=\\sqrt{\\left[\\widehat{\\boldsymbol{V}}_{\\widehat{\\beta}}\\right]_{j j}} .\n\\]\nStandard errors for \\(\\hat{\\theta}\\) are constructed similarly. Supposing that \\(\\theta=h(\\beta)\\) is real-valued then the standard error for \\(\\widehat{\\theta}\\) is the square root of \\((7.30)\\)\n\\[\ns(\\widehat{\\theta})=\\sqrt{\\widehat{\\boldsymbol{R}}^{\\prime} \\widehat{\\boldsymbol{V}}_{\\widehat{\\beta}} \\widehat{\\boldsymbol{R}}}=\\sqrt{n^{-1} \\widehat{\\boldsymbol{R}}^{\\prime} \\widehat{\\boldsymbol{V}}_{\\beta} \\widehat{\\boldsymbol{R}}}\n\\]\nWhen the justification is based on asymptotic theory we call \\(s\\left(\\widehat{\\beta}_{j}\\right)\\) or \\(s(\\widehat{\\theta})\\) an asymptotic standard error for \\(\\widehat{\\beta}_{j}\\) or \\(\\widehat{\\theta}\\). When reporting your results it is good practice to report standard errors for each reported estimate and this includes functions and transformations of your parameter estimates. This helps users of the work (including yourself) assess the estimation precision.\nWe illustrate using the log wage regression\n\\[\n\\log (\\text { wage })=\\beta_{1} \\text { education }+\\beta_{2} \\text { experience }+\\beta_{3} \\text { experience }^{2} / 100+\\beta_{4}+e .\n\\]\nConsider the following three parameters of interest.\n\nPercentage return to education:\n\n\\[\n\\theta_{1}=100 \\beta_{1}\n\\]\n(100 times the partial derivative of the conditional expectation of \\(\\log (\\) wage) with respect to education.)\n 1. Percentage return to experience for individuals with 10 years of experience:\n\\[\n\\theta_{2}=100 \\beta_{2}+20 \\beta_{3}\n\\]\n(100 times the partial derivative of the conditional expectation of log wages with respect to experience, evaluated at experience \\(=10\\).) 3. Experience level which maximizes expected log wages:\n\\[\n\\theta_{3}=-50 \\beta_{2} / \\beta_{3}\n\\]\n(The level of experience at which the partial derivative of the conditional expectation of log(wage) with respect to experience equals 0 .)\nThe \\(4 \\times 1\\) vector \\(\\boldsymbol{R}\\) for these three parameters is\n\\[\n\\boldsymbol{R}=\\left(\\begin{array}{c}\n100 \\\\\n0 \\\\\n0 \\\\\n0\n\\end{array}\\right), \\quad\\left(\\begin{array}{c}\n0 \\\\\n100 \\\\\n20 \\\\\n0\n\\end{array}\\right), \\quad\\left(\\begin{array}{c}\n0 \\\\\n-50 / \\beta_{3} \\\\\n50 \\beta_{2} / \\beta_{3}^{2} \\\\\n0\n\\end{array}\\right),\n\\]\nrespectively.\nWe use the subsample of married Black women (all experience levels) which has 982 observations. The point estimates and standard errors are\n\nThe standard errors are the square roots of the HC2 covariance matrix estimate\n\\[\n\\overline{\\boldsymbol{V}}_{\\widehat{\\beta}}=\\left(\\begin{array}{cccc}\n0.632 & 0.131 & -0.143 & -11.1 \\\\\n0.131 & 0.390 & -0.731 & -6.25 \\\\\n-0.143 & -0.731 & 1.48 & 9.43 \\\\\n-11.1 & -6.25 & 9.43 & 246\n\\end{array}\\right) \\times 10^{-4} .\n\\]\nWe calculate that\n\\[\n\\begin{aligned}\n& \\widehat{\\theta}_{1}=100 \\widehat{\\beta}_{1}=100 \\times 0.118=11.8 \\\\\n& s\\left(\\widehat{\\theta}_{1}\\right)=\\sqrt{100^{2} \\times 0.632 \\times 10^{-4}}=0.8 \\\\\n& \\widehat{\\theta}_{2}=100 \\widehat{\\beta}_{2}+20 \\widehat{\\beta}_{3}=100 \\times 0.016-20 \\times 0.022=1.16 \\\\\n& s\\left(\\widehat{\\theta}_{2}\\right)=\\sqrt{\\left(\\begin{array}{ll}100 & 20\\end{array}\\right)\\left(\\begin{array}{cc}0.390 & -0.731 \\\\-0.731 & 1.48\\end{array}\\right)\\left(\\begin{array}{c}100 \\\\20\\end{array}\\right) \\times 10^{-4}}=0.55 \\\\\n& \\widehat{\\theta}_{3}=-50 \\widehat{\\beta}_{2} / \\widehat{\\beta}_{3}=50 \\times 0.016 / 0.022=35.2\n\\end{aligned}\n\\]\nThe calculations show that the estimate of the percentage return to education is \\(12 %\\) per year with a standard error of 0.8. The estimate of the percentage return to experience for those with 10 years of experience is \\(1.2 %\\) per year with a standard error of \\(0.6\\). The estimate of the experience level which maximizes expected log wages is 35 years with a standard error of 7 .\nIn Stata the nlcom command can be used after estimation to perform the same calculations. To illustrate, after estimation of (7.31) use the commands given below. In each case, Stata reports the coefficient estimate, asymptotic standard error, and \\(95 %\\) confidence interval.\n\nStata Commands\\ nlcom 100_b[education]\\ nlcom 100_b[experience]+20_b[exp2]\\ nlcom -50_b[experience \\(] / 0_{-} \\mathrm{b}[\\exp 2]\\)"
  },
  {
    "objectID": "chpt07-asymptotic-ls.html#t-statistic",
    "href": "chpt07-asymptotic-ls.html#t-statistic",
    "title": "7  Asymptotic Theory for Least Squares",
    "section": "7.12 t-statistic",
    "text": "7.12 t-statistic\nLet \\(\\theta=r(\\beta): \\mathbb{R}^{k} \\rightarrow \\mathbb{R}\\) be a parameter of interest, \\(\\widehat{\\theta}\\) its estimator, and \\(s(\\widehat{\\theta})\\) its asymptotic standard error. Consider the statistic\n\\[\nT(\\theta)=\\frac{\\widehat{\\theta}-\\theta}{s(\\widehat{\\theta})} .\n\\]\nDifferent writers call (7.33) a t-statistic, a t-ratio, a z-statistic, or a studentized statistic, sometimes using the different labels to distinguish between finite-sample and asymptotic inference. As the statistics themselves are always (7.33) we won’t make this distinction, and will simply refer to \\(T(\\theta)\\) as a t-statistic or a t-ratio. We also often suppress the parameter dependence, writing it as \\(T\\). The t-statistic is a function of the estimator, its standard error, and the parameter.\nBy Theorems \\(7.9\\) and \\(7.10, \\sqrt{n}(\\widehat{\\theta}-\\theta) \\underset{d}{\\longrightarrow} \\mathrm{N}\\left(0, V_{\\theta}\\right)\\) and \\(\\widehat{V}_{\\theta} \\underset{p}{\\longrightarrow} V_{\\theta}\\). Thus\n\\[\n\\begin{aligned}\nT(\\theta) &=\\frac{\\widehat{\\theta}-\\theta}{s(\\widehat{\\theta})} \\\\\n&=\\frac{\\sqrt{n}(\\widehat{\\theta}-\\theta)}{\\sqrt{\\widehat{V}_{\\theta}}} \\\\\n& \\rightarrow \\frac{\\mathrm{N}\\left(0, V_{\\theta}\\right)}{\\sqrt{V_{\\theta}}} \\\\\n&=Z \\sim \\mathrm{N}(0,1) .\n\\end{aligned}\n\\]\nThe last equality is the property that affine functions of normal variables are normal (Theorem 5.2).\nThis calculation requires that \\(V_{\\theta}>0\\), otherwise the continuous mapping theorem cannot be employed. In practice this is an innocuous requirement as it only excludes degenerate sampling distributions. Formally we add the following assumption.\nAssumption 7.4 \\(V_{\\theta}=R^{\\prime} V_{\\beta} R>0\\).\nAssumption \\(7.4\\) states that \\(\\boldsymbol{V}_{\\theta}\\) is positive definite. Since \\(\\boldsymbol{R}\\) is full rank under Assumption \\(7.3\\) a sufficient condition is that \\(\\boldsymbol{V}_{\\beta}>0\\). Since \\(\\boldsymbol{Q}_{X X}>0\\) a sufficient condition is \\(\\Omega>0\\). Thus Assumption \\(7.4\\) could be replaced by the assumption \\(\\Omega>0\\). Assumption \\(7.4\\) is weaker so this is what we use.\nThus the asymptotic distribution of the t-ratio \\(T(\\theta)\\) is standard normal. Since this distribution does not depend on the parameters we say that \\(T(\\theta)\\) is asymptotically pivotal. In finite samples \\(T(\\theta)\\) is not necessarily pivotal but the property means that the dependence on unknowns diminishes as \\(n\\) increases. It is also useful to consider the distribution of the absolute t-ratio \\(|T(\\theta)|\\). Since \\(T(\\theta) \\underset{d}{\\longrightarrow} Z\\) the continuous mapping theorem yields \\(|T(\\theta)| \\underset{d}{\\longrightarrow}|Z|\\). Letting \\(\\Phi(u)=\\mathbb{P}[Z \\leq u]\\) denote the standard normal distribution function we calculate that the distribution of \\(|Z|\\) is\n\\[\n\\begin{aligned}\n\\mathbb{P}[|Z| \\leq u] &=\\mathbb{P}[-u \\leq Z \\leq u] \\\\\n&=\\mathbb{P}[Z \\leq u]-\\mathbb{P}[Z<-u] \\\\\n&=\\Phi(u)-\\Phi(-u) \\\\\n&=2 \\Phi(u)-1 .\n\\end{aligned}\n\\]\nTheorem 7.11 Under Assumptions 7.2, 7.3, and 7.4, \\(T(\\theta) \\underset{d}{\\longrightarrow} Z \\sim \\mathrm{N}(0,1)\\) and \\(|T(\\theta)| \\underset{d}{\\longrightarrow}|Z|\\)\nThe asymptotic normality of Theorem \\(7.11\\) is used to justify confidence intervals and tests for the parameters."
  },
  {
    "objectID": "chpt07-asymptotic-ls.html#confidence-intervals",
    "href": "chpt07-asymptotic-ls.html#confidence-intervals",
    "title": "7  Asymptotic Theory for Least Squares",
    "section": "7.13 Confidence Intervals",
    "text": "7.13 Confidence Intervals\nThe estimator \\(\\hat{\\theta}\\) is a point estimator for \\(\\theta\\), meaning that \\(\\widehat{\\theta}\\) is a single value in \\(\\mathbb{R}^{q}\\). A broader concept is a set estimator \\(\\widehat{C}\\) which is a collection of values in \\(\\mathbb{R}^{q}\\). When the parameter \\(\\theta\\) is real-valued then it is common to focus on sets of the form \\(\\widehat{C}=[\\widehat{L}, \\widehat{U}]\\) which is called an interval estimator for \\(\\theta\\).\nAn interval estimator \\(\\widehat{C}\\) is a function of the data and hence is random. The coverage probability of the interval \\(\\widehat{C}=[\\widehat{L}, \\widehat{U}]\\) is \\(\\mathbb{P}[\\theta \\in \\widehat{C}]\\). The randomness comes from \\(\\widehat{C}\\) as the parameter \\(\\theta\\) is treated as fixed. In Section \\(5.10\\) we introduced confidence intervals for the normal regression model which used the finite sample distribution of the t-statistic. When we are outside the normal regression model we cannot rely on the exact normal distribution theory but instead use asymptotic approximations. A benefit is that we can construct confidence intervals for general parameters of interest \\(\\theta\\) not just regression coefficients.\nAn interval estimator \\(\\widehat{C}\\) is called a confidence interval when the goal is to set the coverage probability to equal a pre-specified target such as \\(90 %\\) or \\(95 %\\). \\(\\widehat{C}\\) is called a \\(1-\\alpha\\) confidence interval if \\(\\inf _{\\theta} \\mathbb{P}_{\\theta}[\\theta \\in \\widehat{C}]=1-\\alpha\\).\nWhen \\(\\widehat{\\theta}\\) is asymptotically normal with standard error \\(s(\\widehat{\\theta})\\) the conventional confidence interval for \\(\\theta\\) takes the form\n\\[\n\\widehat{C}=[\\widehat{\\theta}-c \\times s(\\widehat{\\theta}), \\quad \\widehat{\\theta}+c \\times s(\\widehat{\\theta})]\n\\]\nwhere \\(c\\) equals the \\(1-\\alpha\\) quantile of the distribution of \\(|Z|\\). Using (7.34) we calculate that \\(c\\) is equivalently the \\(1-\\alpha / 2\\) quantile of the standard normal distribution. Thus, \\(c\\) solves\n\\[\n2 \\Phi(c)-1=1-\\alpha .\n\\]\nThis can be computed by, for example, norminv \\((1-\\alpha / 2)\\) in MATLAB. The confidence interval (7.35) is symmetric about the point estimator \\(\\widehat{\\theta}\\) and its length is proportional to the standard error \\(s(\\widehat{\\theta})\\).\nEquivalently, (7.35) is the set of parameter values for \\(\\theta\\) such that the t-statistic \\(T(\\theta)\\) is smaller (in absolute value) than \\(c\\), that is\n\\[\n\\widehat{C}=\\{\\theta:|T(\\theta)| \\leq c\\}=\\left\\{\\theta:-c \\leq \\frac{\\widehat{\\theta}-\\theta}{s(\\widehat{\\theta})} \\leq c\\right\\} .\n\\]\nThe coverage probability of this confidence interval is\n\\[\n\\mathbb{P}[\\theta \\in \\widehat{C}]=\\mathbb{P}[|T(\\theta)| \\leq c] \\rightarrow \\mathbb{P}[|Z| \\leq c]=1-\\alpha\n\\]\nwhere the limit is taken as \\(n \\rightarrow \\infty\\), and holds because \\(T(\\theta)\\) is asymptotically \\(|Z|\\) by Theorem \\(7.11\\). We call the limit the asymptotic coverage probability and call \\(\\widehat{C}\\) an asymptotic \\(1-\\alpha %\\) confidence interval for \\(\\theta\\). Since the t-ratio is asymptotically pivotal the asymptotic coverage probability is independent of the parameter \\(\\theta\\).\nIt is useful to contrast the confidence interval (7.35) with (5.8) for the normal regression model. They are similar but there are differences. The normal regression interval (5.8) only applies to regression coefficients \\(\\beta\\) not to functions \\(\\theta\\) of the coefficients. The normal interval (5.8) also is constructed with the homoskedastic standard error, while (7.35) can be constructed with a heteroskedastic-robust standard error. Furthermore, the constants \\(c\\) in (5.8) are calculated using the student \\(t\\) distribution, while \\(c\\) in (7.35) are calculated using the normal distribution. The difference between the student \\(t\\) and normal values are typically small in practice (since sample sizes are large in typical economic applications). However, since the student \\(t\\) values are larger it results in slightly larger confidence intervals which is reasonable. (A practical rule of thumb is that if the sample sizes are sufficiently small that it makes a difference then neither (5.8) nor (7.35) should be trusted.) Despite these differences the coincidence of the intervals means that inference on regression coefficients is generally robust to using either the exact normal sampling assumption or the asymptotic large sample approximation, at least in large samples.\nStata by default reports \\(95 %\\) confidence intervals for each coefficient where the critical values \\(c\\) are calculated using the \\(t_{n-k}\\) distribution. This is done for all standard error methods even though it is only exact for homoskedastic standard errors and under normality.\nThe standard coverage probability for confidence intervals is \\(95 %\\), leading to the choice \\(c=1.96\\) for the constant in (7.35). Rounding \\(1.96\\) to 2 , we obtain the most commonly used confidence interval in applied econometric practice\n\\[\n\\widehat{C}=[\\widehat{\\theta}-2 s(\\widehat{\\theta}), \\quad \\widehat{\\theta}+2 s(\\widehat{\\theta})] .\n\\]\nThis is a useful rule-of thumb. This asymptotic \\(95 %\\) confidence interval \\(\\widehat{C}\\) is simple to compute and can be roughly calculated from tables of coefficient estimates and standard errors. (Technically, it is an asymptotic \\(95.4 %\\) interval due to the substitution of \\(2.0\\) for \\(1.96\\) but this distinction is overly precise.)\nTheorem 7.12 Under Assumptions 7.2, 7.3 and 7.4, for \\(\\widehat{C}\\) defined in (7.35) with \\(c=\\Phi^{-1}(1-\\alpha / 2), \\mathbb{P}[\\theta \\in \\widehat{C}] \\rightarrow 1-\\alpha\\). For \\(c=1.96, \\mathbb{P}[\\theta \\in \\widehat{C}] \\rightarrow 0.95\\).\nConfidence intervals are a simple yet effective tool to assess estimation uncertainty. When reading a set of empirical results look at the estimated coefficient estimates and the standard errors. For a parameter of interest compute the confidence interval \\(\\widehat{C}\\) and consider the meaning of the spread of the suggested values. If the range of values in the confidence interval are too wide to learn about \\(\\theta\\) then do not jump to a conclusion about \\(\\theta\\) based on the point estimate alone.\nFor illustration, consider the three examples presented in Section \\(7.11\\) based on the log wage regression for married Black women.\nPercentage return to education. A 95% asymptotic confidence interval is \\(11.8 \\pm 1.96 \\times 0.8=[10.2\\), 13.3]. This is reasonably tight.\nPercentage return to experience (per year) for individuals with 10 years experience. A \\(90 %\\) asymptotic confidence interval is \\(1.1 \\pm 1.645 \\times 0.4=[0.5,1.8]\\). The interval is positive but broad. This indicates that the return to experience is positive, but of uncertain magnitude. Experience level which maximizes expected log wages. An \\(80 %\\) asymptotic confidence interval is \\(35 \\pm 1.28 \\times 7=[26,44]\\). This is rather imprecise, indicating that the estimates are not very informative regarding this parameter."
  },
  {
    "objectID": "chpt07-asymptotic-ls.html#regression-intervals",
    "href": "chpt07-asymptotic-ls.html#regression-intervals",
    "title": "7  Asymptotic Theory for Least Squares",
    "section": "7.14 Regression Intervals",
    "text": "7.14 Regression Intervals\nIn the linear regression model the conditional expectation of \\(Y\\) given \\(X=x\\) is\n\\[\nm(x)=\\mathbb{E}[Y \\mid X=x]=x^{\\prime} \\beta .\n\\]\nIn some cases we want to estimate \\(m(x)\\) at a particular point \\(x\\). Notice that this is a linear function of \\(\\beta\\). Letting \\(r(\\beta)=x^{\\prime} \\beta\\) and \\(\\theta=r(\\beta)\\) we see that \\(\\hat{m}(x)=\\widehat{\\theta}=x^{\\prime} \\widehat{\\beta}\\) and \\(\\boldsymbol{R}=x\\) so \\(s(\\widehat{\\theta})=\\sqrt{x^{\\prime} \\widehat{\\boldsymbol{V}}_{\\widehat{\\beta}} x}\\). Thus an asymptotic \\(95 %\\) confidence interval for \\(m(x)\\) is\n\\[\n\\left[x^{\\prime} \\widehat{\\beta} \\pm 1.96 \\sqrt{x^{\\prime} \\widehat{\\boldsymbol{V}}_{\\widehat{\\beta}} x}\\right] .\n\\]\nIt is interesting to observe that if this is viewed as a function of \\(x\\) the width of the confidence interval is dependent on \\(x\\).\nTo illustrate we return to the log wage regression (3.12) of Section 3.7. The estimated regression equation is\n\\[\n\\widehat{\\log (\\text { wage })}=x^{\\prime} \\widehat{\\beta}=0.155 x+0.698\n\\]\nwhere \\(x=e d u c a t i o n\\). The covariance matrix estimate from (4.43) is\n\\[\n\\widehat{\\boldsymbol{V}}_{\\widehat{\\beta}}=\\left(\\begin{array}{cc}\n0.001 & -0.015 \\\\\n-0.015 & 0.243\n\\end{array}\\right) .\n\\]\nThus the \\(95 %\\) confidence interval for the regression is\n\\[\n0.155 x+0.698 \\pm 1.96 \\sqrt{0.001 x^{2}-0.030 x+0.243} .\n\\]\nThe estimated regression and 95% intervals are shown in Figure 7.4(a). Notice that the confidence bands take a hyperbolic shape. This means that the regression line is less precisely estimated for large and small values of education.\nPlots of the estimated regression line and confidence intervals are especially useful when the regression includes nonlinear terms. To illustrate consider the log wage regression (7.31) which includes experience and its square and covariance matrix estimate (7.32). We are interested in plotting the regression estimate and regression intervals as a function of experience. Since the regression also includes education, to plot the estimates in a simple graph we fix education at a specific value. We select education=12. This only affects the level of the estimated regression since education enters without an interaction. Define the points of evaluation\n\\[\nz(x)=\\left(\\begin{array}{c}\n12 \\\\\nx \\\\\nx^{2} / 100 \\\\\n1\n\\end{array}\\right)\n\\]\nwhere \\(x=\\) experience.\n\n\nWage on Education\n\n\n\nWage on Experience\n\nFigure 7.4: Regression Intervals\nThe \\(95 %\\) regression interval for education \\(=12\\) as a function of \\(x=\\) experience is\n\\[\n\\begin{aligned}\n& 0.118 \\times 12+0.016 x-0.022 x^{2} / 100+0.947 \\\\\n& \\pm 1.96 \\sqrt{z(x)^{\\prime}\\left(\\begin{array}{cccc}0.632 & 0.131 & -0.143 & -11.1 \\\\0.131 & 0.390 & -0.731 & -6.25 \\\\-0.143 & -0.731 & 1.48 & 9.43 \\\\-11.1 & -6.25 & 9.43 & 246\\end{array}\\right) z(x) \\times 10^{-4}} \\\\\n& =0.016 x-.00022 x^{2}+2.36 \\\\\n& \\pm 0.0196 \\sqrt{70.608-9.356 x+0.54428 x^{2}-0.01462 x^{3}+0.000148 x^{4}} \\text {. }\n\\end{aligned}\n\\]\nThe estimated regression and 95% intervals are shown in Figure 7.4(b). The regression interval widens greatly for small and large values of experience indicating considerable uncertainty about the effect of experience on mean wages for this population. The confidence bands take a more complicated shape than in Figure 7.4(a) due to the nonlinear specification."
  },
  {
    "objectID": "chpt07-asymptotic-ls.html#forecast-intervals",
    "href": "chpt07-asymptotic-ls.html#forecast-intervals",
    "title": "7  Asymptotic Theory for Least Squares",
    "section": "7.15 Forecast Intervals",
    "text": "7.15 Forecast Intervals\nSuppose we are given a value of the regressor vector \\(X_{n+1}\\) for an individual outside the sample and we want to forecast (guess) \\(Y_{n+1}\\) for this individual. This is equivalent to forecasting \\(Y_{n+1}\\) given \\(X_{n+1}=x\\) which will generally be a function of \\(x\\). A reasonable forecasting rule is the conditional expectation \\(m(x)\\) as it is the mean-square minimizing forecast. A point forecast is the estimated conditional expectation \\(\\widehat{m}(x)=x^{\\prime} \\widehat{\\beta}\\). We would also like a measure of uncertainty for the forecast.\nThe forecast error is \\(\\widehat{e}_{n+1}=Y_{n+1}-\\widehat{m}(x)=e_{n+1}-x^{\\prime}(\\widehat{\\beta}-\\beta)\\). As the out-of-sample error \\(e_{n+1}\\) is inde- pendent of the in-sample estimator \\(\\widehat{\\beta}\\) this has conditional variance\n\\[\n\\begin{aligned}\n\\mathbb{E}\\left[\\widehat{e}_{n+1}^{2} \\mid X_{n+1}=x\\right] &=\\mathbb{E}\\left[e_{n+1}^{2}-2 x^{\\prime}(\\widehat{\\beta}-\\beta) e_{n+1}+x^{\\prime}(\\widehat{\\beta}-\\beta)(\\widehat{\\beta}-\\beta)^{\\prime} x \\mid X_{n+1}=x\\right] \\\\\n&=\\mathbb{E}\\left[e_{n+1}^{2} \\mid X_{n+1}=x\\right]+x^{\\prime} \\mathbb{E}\\left[(\\widehat{\\beta}-\\beta)(\\widehat{\\beta}-\\beta)^{\\prime}\\right] x \\\\\n&=\\sigma^{2}(x)+x^{\\prime} \\boldsymbol{V}_{\\widehat{\\beta}} x .\n\\end{aligned}\n\\]\nUnder homoskedasticity, \\(\\mathbb{E}\\left[e_{n+1}^{2} \\mid X_{n+1}\\right]=\\sigma^{2}\\). In this case a simple estimator of (7.36) is \\(\\widehat{\\sigma}^{2}+x^{\\prime} \\boldsymbol{V}_{\\widehat{\\beta}} x\\) so a standard error for the forecast is \\(\\widehat{s}(x)=\\sqrt{\\widehat{\\sigma}^{2}+x^{\\prime} \\boldsymbol{V}_{\\widehat{\\beta}} x}\\). Notice that this is different from the standard error for the conditional expectation.\nThe conventional 95% forecast interval for \\(Y_{n+1}\\) uses a normal approximation and equals \\(\\left[x^{\\prime} \\widehat{\\beta} \\pm 2 \\widehat{s}(x)\\right]\\). It is difficult, however, to fully justify this choice. It would be correct if we have a normal approximation to the ratio\n\\[\n\\frac{e_{n+1}-x^{\\prime}(\\widehat{\\beta}-\\beta)}{\\widehat{s}(x)} .\n\\]\nThe difficulty is that the equation error \\(e_{n+1}\\) is generally non-normal and asymptotic theory cannot be applied to a single observation. The only special exception is the case where \\(e_{n+1}\\) has the exact distribution \\(\\mathrm{N}\\left(0, \\sigma^{2}\\right)\\) which is generally invalid.\nAn accurate forecast interval would use the conditional distribution of \\(e_{n+1}\\) given \\(X_{n+1}=x\\), which is more challenging to estimate. Due to this difficulty many applied forecasters use the simple approximate interval \\(\\left[x^{\\prime} \\widehat{\\beta} \\pm 2 \\widehat{s}(x)\\right]\\) despite the lack of a convincing justification."
  },
  {
    "objectID": "chpt07-asymptotic-ls.html#wald-statistic",
    "href": "chpt07-asymptotic-ls.html#wald-statistic",
    "title": "7  Asymptotic Theory for Least Squares",
    "section": "7.16 Wald Statistic",
    "text": "7.16 Wald Statistic\nLet \\(\\theta=r(\\beta): \\mathbb{R}^{k} \\rightarrow \\mathbb{R}^{q}\\) be any parameter vector of interest, \\(\\widehat{\\theta}\\) its estimator, and \\(\\widehat{\\boldsymbol{V}}_{\\widehat{\\theta}}\\) its covariance matrix estimator. Consider the quadratic form\n\\[\nW(\\theta)=(\\widehat{\\theta}-\\theta)^{\\prime} \\widehat{\\mathbf{V}}_{\\widehat{\\theta}}^{-1}(\\widehat{\\theta}-\\theta)=n(\\widehat{\\theta}-\\theta)^{\\prime} \\widehat{\\boldsymbol{V}}_{\\theta}^{-1}(\\widehat{\\theta}-\\theta) .\n\\]\nwhere \\(\\widehat{\\boldsymbol{V}}_{\\theta}=n \\widehat{\\boldsymbol{V}}_{\\widehat{\\theta}}\\). When \\(q=1\\), then \\(W(\\theta)=T(\\theta)^{2}\\) is the square of the t-ratio. When \\(q>1, W(\\theta)\\) is typically called a Wald statistic as it was proposed by Wald (1943). We are interested in its sampling distribution.\nThe asymptotic distribution of \\(W(\\theta)\\) is simple to derive given Theorem \\(7.9\\) and Theorem 7.10. They show that \\(\\sqrt{n}(\\widehat{\\theta}-\\theta) \\underset{d}{\\longrightarrow} Z \\sim \\mathrm{N}\\left(0, \\boldsymbol{V}_{\\theta}\\right)\\) and \\(\\widehat{\\boldsymbol{V}}_{\\theta} \\underset{p}{\\longrightarrow} \\boldsymbol{V}_{\\theta}\\). It follows that\n\\[\nW(\\theta)=\\sqrt{n}(\\widehat{\\theta}-\\theta)^{\\prime} \\widehat{\\boldsymbol{V}}_{\\theta}^{-1} \\sqrt{n}(\\widehat{\\theta}-\\theta) \\underset{d}{\\longrightarrow} Z^{\\prime} \\boldsymbol{V}_{\\theta}^{-1} Z\n\\]\na quadratic in the normal random vector \\(Z\\). As shown in Theorem \\(5.3 .5\\) the distribution of this quadratic form is \\(\\chi_{q}^{2}\\), a chi-square random variable with \\(q\\) degrees of freedom.\nTheorem 7.13 Under Assumptions 7.2, \\(7.3\\) and 7.4, as \\(n \\rightarrow \\infty, W(\\theta) \\underset{d}{\\longrightarrow} \\chi_{q}^{2}\\).\nTheorem \\(7.13\\) is used to justify multivariate confidence regions and multivariate hypothesis tests."
  },
  {
    "objectID": "chpt07-asymptotic-ls.html#homoskedastic-wald-statistic",
    "href": "chpt07-asymptotic-ls.html#homoskedastic-wald-statistic",
    "title": "7  Asymptotic Theory for Least Squares",
    "section": "7.17 Homoskedastic Wald Statistic",
    "text": "7.17 Homoskedastic Wald Statistic\nUnder the conditional homoskedasticity assumption \\(\\mathbb{E}\\left[e^{2} \\mid X\\right]=\\sigma^{2}\\) we can construct the Wald statistic using the homoskedastic covariance matrix estimator \\(\\widehat{\\boldsymbol{V}}_{\\theta}^{0}\\) defined in (7.29). This yields a homoskedastic Wald statistic\n\\[\nW^{0}(\\theta)=(\\widehat{\\theta}-\\theta)^{\\prime}\\left(\\widehat{\\boldsymbol{V}}_{\\widehat{\\theta}}^{0}\\right)^{-1}(\\widehat{\\theta}-\\theta)=n(\\widehat{\\theta}-\\theta)^{\\prime}\\left(\\widehat{\\boldsymbol{V}}_{\\theta}^{0}\\right)^{-1}(\\widehat{\\theta}-\\theta) .\n\\]\nUnder the assumption of conditional homoskedasticity it has the same asymptotic distribution as \\(W(\\theta)\\)\nTheorem 7.14 Under Assumptions 7.2, 7.3, and \\(\\mathbb{E}\\left[e^{2} \\mid X\\right]=\\sigma^{2}>0\\), as \\(n \\rightarrow \\infty\\), \\(W^{0}(\\theta) \\underset{d}{\\longrightarrow} \\chi_{q}^{2}\\)"
  },
  {
    "objectID": "chpt07-asymptotic-ls.html#confidence-regions",
    "href": "chpt07-asymptotic-ls.html#confidence-regions",
    "title": "7  Asymptotic Theory for Least Squares",
    "section": "7.18 Confidence Regions",
    "text": "7.18 Confidence Regions\nA confidence region \\(\\widehat{C}\\) is a set estimator for \\(\\theta \\in \\mathbb{R}^{q}\\) when \\(q>1\\). A confidence region \\(\\widehat{C}\\) is a set in \\(\\mathbb{R}^{q}\\) intended to cover the true parameter value with a pre-selected probability \\(1-\\alpha\\). Thus an ideal confidence region has the coverage probability \\(\\mathbb{P}[\\theta \\in \\widehat{C}]=1-\\alpha\\). In practice it is typically not possible to construct a region with exact coverage but we can calculate its asymptotic coverage.\nWhen the parameter estimator satisfies the conditions of Theorem \\(7.13\\) a good choice for a confidence region is the ellipse\n\\[\n\\widehat{C}=\\left\\{\\theta: W(\\theta) \\leq c_{1-\\alpha}\\right\\}\n\\]\nwith \\(c_{1-\\alpha}\\) the \\(1-\\alpha\\) quantile of the \\(\\chi_{q}^{2}\\) distribution. (Thus \\(F_{q}\\left(c_{1-\\alpha}\\right)=1-\\alpha\\).) It can be computed by, for example, chi2inv \\((1-\\alpha, q)\\) in MATLAB.\nTheorem \\(7.13\\) implies\n\\[\n\\mathbb{P}[\\theta \\in \\widehat{C}] \\rightarrow \\mathbb{P}\\left[\\chi_{q}^{2} \\leq c_{1-\\alpha}\\right]=1-\\alpha\n\\]\nwhich shows that \\(\\widehat{C}\\) has asymptotic coverage \\(1-\\alpha\\).\nTo illustrate the construction of a confidence region, consider the estimated regression (7.31) of\n\nSuppose that the two parameters of interest are the percentage return to education \\(\\theta_{1}=100 \\beta_{1}\\) and the percentage return to experience for individuals with 10 years experience \\(\\theta_{2}=100 \\beta_{2}+20 \\beta_{3}\\). These two parameters are a linear transformation of the regression parameters with point estimates\n\\[\n\\widehat{\\theta}=\\left(\\begin{array}{cccc}\n100 & 0 & 0 & 0 \\\\\n0 & 100 & 20 & 0\n\\end{array}\\right) \\widehat{\\beta}=\\left(\\begin{array}{c}\n11.8 \\\\\n1.2\n\\end{array}\\right),\n\\]\nand have the covariance matrix estimate\n\\[\n\\widehat{\\boldsymbol{V}}_{\\widehat{\\theta}}=\\left(\\begin{array}{cccc}\n0 & 100 & 0 & 0 \\\\\n0 & 0 & 100 & 20\n\\end{array}\\right) \\widehat{\\boldsymbol{V}}_{\\widehat{\\beta}}\\left(\\begin{array}{cc}\n0 & 0 \\\\\n100 & 0 \\\\\n0 & 100 \\\\\n0 & 20\n\\end{array}\\right)=\\left(\\begin{array}{cc}\n0.632 & 0.103 \\\\\n0.103 & 0.157\n\\end{array}\\right)\n\\]\nwith inverse\n\\[\n\\widehat{\\boldsymbol{V}}_{\\widehat{\\theta}}^{-1}=\\left(\\begin{array}{cc}\n1.77 & -1.16 \\\\\n-1.16 & 7.13\n\\end{array}\\right) .\n\\]\nThus the Wald statistic is\n\\[\n\\begin{aligned}\nW(\\theta) &=(\\widehat{\\theta}-\\theta)^{\\prime} \\widehat{\\boldsymbol{V}}_{\\widehat{\\theta}}^{-1}(\\widehat{\\theta}-\\theta) \\\\\n&=\\left(\\begin{array}{c}\n11.8-\\theta_{1} \\\\\n1.2-\\theta_{2}\n\\end{array}\\right)^{\\prime}\\left(\\begin{array}{cc}\n1.77 & -1.16 \\\\\n-1.16 & 7.13\n\\end{array}\\right)\\left(\\begin{array}{c}\n11.8-\\theta_{1} \\\\\n1.2-\\theta_{2}\n\\end{array}\\right) \\\\\n&=1.77\\left(11.8-\\theta_{1}\\right)^{2}-2.32\\left(11.8-\\theta_{1}\\right)\\left(1.2-\\theta_{2}\\right)+7.13\\left(1.2-\\theta_{2}\\right)^{2} .\n\\end{aligned}\n\\]\nThe \\(90 %\\) quantile of the \\(\\chi_{2}^{2}\\) distribution is \\(4.605\\) (we use the \\(\\chi_{2}^{2}\\) distribution as the dimension of \\(\\theta\\) is two) so an asymptotic \\(90 %\\) confidence region for the two parameters is the interior of the ellipse \\(W(\\theta)=\\) \\(4.605\\) which is displayed in Figure 7.5. Since the estimated correlation of the two coefficient estimates is modest (about \\(0.3\\) ) the region is modestly elliptical.\n\nFigure 7.5: Confidence Region for Return to Experience and Return to Education"
  },
  {
    "objectID": "chpt07-asymptotic-ls.html#edgeworth-expansion",
    "href": "chpt07-asymptotic-ls.html#edgeworth-expansion",
    "title": "7  Asymptotic Theory for Least Squares",
    "section": "7.19 Edgeworth Expansion*",
    "text": "7.19 Edgeworth Expansion*\nTheorem \\(7.11\\) showed that the t-ratio \\(T(\\theta)\\) is asymptotically normal. In practice this means that we use the normal distribution to approximate the finite sample distribution of \\(T\\). How good is this approximation? Some insight into the accuracy of the normal approximation can be obtained by an Edgeworth expansion which is a higher-order approximation to the distribution of \\(T\\). The following result is an application of Theorem \\(9.11\\) of Probability and Statistics for Economists.\nTheorem 7.15 Under Assumptions 7.2, 7.3, \\(\\Omega>0, \\mathbb{E}\\|e\\|^{16}<\\infty, \\mathbb{E}\\|X\\|^{16}<\\) \\(\\infty, g(\\beta)\\) has five continuous derivatives in a neighborhood of \\(\\beta\\), and \\(\\mathbb{E}\\left[\\exp \\left(t\\left(\\|e\\|^{4}+\\|X\\|^{4}\\right)\\right)\\right] \\leq B<1\\), as \\(n \\rightarrow \\infty\\)\n\\[\n\\mathbb{P}[T(\\theta) \\leq x]=\\Phi(x)+n^{-1 / 2} p_{1}(x) \\phi(x)+n^{-1} p_{2}(x) \\phi(x)+o\\left(n^{-1}\\right)\n\\]\nuniformly in \\(x\\), where \\(p_{1}(x)\\) is an even polynomial of order 2 and \\(p_{2}(x)\\) is an odd polynomial of degree 5 with coefficients depending on the moments of \\(e\\) and \\(X\\) up to order \\(16 .\\)\nTheorem \\(7.15\\) shows that the finite sample distribution of the t-ratio can be approximated up to \\(o\\left(n^{-1}\\right)\\) by the sum of three terms, the first being the standard normal distribution, the second a \\(O\\left(n^{-1 / 2}\\right)\\) adjustment, and the third a \\(O\\left(n^{-1}\\right)\\) adjustment.\nConsider a one-sided confidence interval \\(\\widehat{C}=\\left[\\widehat{\\theta}-z_{1-\\alpha} s(\\widehat{\\theta}), \\infty\\right)\\) where \\(z_{1-\\alpha}\\) is the \\(1-\\alpha^{t h}\\) quantile of \\(Z \\sim \\mathrm{N}(0,1)\\), thus \\(\\Phi\\left(z_{1-\\alpha}\\right)-1-\\alpha\\). Then\n\\[\n\\begin{aligned}\n\\mathbb{P}[\\theta \\in \\widehat{C}] &=\\mathbb{P}\\left[T(\\theta) \\leq z_{1-\\alpha}\\right] \\\\\n&=\\Phi\\left(z_{1-\\alpha}\\right)+n^{-1 / 2} p_{1}\\left(z_{1-\\alpha}\\right) \\phi\\left(z_{1-\\alpha}\\right)+O\\left(n^{-1}\\right) \\\\\n&=1-\\alpha+O\\left(n^{-1 / 2}\\right) .\n\\end{aligned}\n\\]\nThis means that the actual coverage is within \\(O\\left(n^{-1 / 2}\\right)\\) of the desired \\(1-\\alpha\\) level.\nNow consider a two-sided interval \\(\\widehat{C}=\\left[\\widehat{\\theta}-z_{1-\\alpha / 2} s(\\widehat{\\theta}), \\widehat{\\theta}+z_{1-\\alpha / 2} s(\\widehat{\\theta})\\right]\\). It has coverage\n\\[\n\\begin{aligned}\n\\mathbb{P}[\\theta \\in \\widehat{C}] &=\\mathbb{P}\\left[|T(\\theta)| \\leq z_{1-\\alpha / 2}\\right] \\\\\n&=2 \\Phi\\left(z_{1-\\alpha / 2}\\right)-1+n^{-1} 2 p_{2}\\left(z_{1-\\alpha / 2}\\right) \\phi\\left(z_{1-\\alpha / 2}\\right)+o\\left(n^{-1}\\right) \\\\\n&=1-\\alpha+O\\left(n^{-1}\\right) .\n\\end{aligned}\n\\]\nThis means that the actual coverage is within \\(O\\left(n^{-1}\\right)\\) of the desired \\(1-\\alpha\\) level. The accuracy is better than the one-sided interval because the \\(O\\left(n^{-1 / 2}\\right)\\) term in the Edgeworth expansion has offsetting effects in the two tails of the distribution."
  },
  {
    "objectID": "chpt07-asymptotic-ls.html#uniformly-consistent-residuals",
    "href": "chpt07-asymptotic-ls.html#uniformly-consistent-residuals",
    "title": "7  Asymptotic Theory for Least Squares",
    "section": "7.20 Uniformly Consistent Residuals*",
    "text": "7.20 Uniformly Consistent Residuals*\nIt seems natural to view the residuals \\(\\widehat{e}_{i}\\) as estimators of the unknown errors \\(e_{i}\\). Are they consistent? In this section we develop a convergence result.\nWe can write the residual as\n\\[\n\\widehat{e}_{i}=Y_{i}-X_{i}^{\\prime} \\widehat{\\beta}=e_{i}-X_{i}^{\\prime}(\\widehat{\\beta}-\\beta) .\n\\]\nSince \\(\\widehat{\\beta}-\\beta \\underset{p}{\\longrightarrow} 0\\) it seems reasonable to guess that \\(\\widehat{e}_{i}\\) will be close to \\(e_{i}\\) if \\(n\\) is large.\nWe can bound the difference in (7.39) using the Schwarz inequality (B.12) to find\n\\[\n\\left|\\widehat{e}_{i}-e_{i}\\right|=\\left|X_{i}^{\\prime}(\\widehat{\\beta}-\\beta)\\right| \\leq\\left\\|X_{i}\\right\\|\\|\\widehat{\\beta}-\\beta\\| .\n\\]\nTo bound (7.40) we can use \\(\\|\\widehat{\\beta}-\\beta\\|=O_{p}\\left(n^{-1 / 2}\\right)\\) from Theorem 7.3. We also need to bound the random variable \\(\\left\\|X_{i}\\right\\|\\). If the regressor is bounded, that is, \\(\\left\\|X_{i}\\right\\| \\leq B<\\infty\\), then \\(\\left|\\widehat{e}_{i}-e_{i}\\right| \\leq B\\|\\widehat{\\beta}-\\beta\\|=O_{p}\\left(n^{-1 / 2}\\right)\\). However if the regressor does not have bounded support then we have to be more careful.\nThe key is Theorem \\(6.15\\) which shows that \\(\\mathbb{E}\\|X\\|^{r}<\\infty\\) implies \\(X_{i}=o_{p}\\left(n^{1 / r}\\right)\\) uniformly in \\(i\\), or\n\\[\nn^{-1 / r} \\max _{1 \\leq i \\leq n}\\left\\|X_{i}\\right\\| \\underset{p}{\\longrightarrow} 0 .\n\\]\nApplied to (7.40) we obtain\n\\[\n\\max _{1 \\leq i \\leq n}\\left|\\widehat{e}_{i}-e_{i}\\right| \\leq \\max _{1 \\leq i \\leq n}\\left\\|X_{i}\\right\\|\\|\\widehat{\\beta}-\\beta\\|=o_{p}\\left(n^{-1 / 2+1 / r}\\right) .\n\\]\nWe have shown the following.\nTheorem 7.16 Under Assumption \\(7.2\\) and \\(\\mathbb{E}\\|X\\|^{r}<\\infty\\), then\n\\[\n\\max _{1 \\leq i \\leq n}\\left|\\widehat{e}_{i}-e_{i}\\right|=o_{p}\\left(n^{-1 / 2+1 / r}\\right) .\n\\]\nThe rate of convergence in (7.41) depends on \\(r\\). Assumption \\(7.2\\) requires \\(r \\geq 4\\) so the rate of convergence is at least \\(o_{p}\\left(n^{-1 / 4}\\right)\\). As \\(r\\) increases the rate improves.\nWe mentioned in Section \\(7.7\\) that there are multiple ways to prove the consistency of the covariance matrix estimator \\(\\widehat{\\Omega}\\). We now show that Theorem \\(7.16\\) provides one simple method to establish (7.23) and thus Theorem 7.6. Let \\(q_{n}=\\max _{1 \\leq i \\leq n}\\left|\\widehat{e}_{i}-e_{i}\\right|=o_{p}\\left(n^{-1 / 4}\\right)\\). Since \\(\\widehat{e}_{i}^{2}-e_{i}^{2}=2 e_{i}\\left(\\widehat{e}_{i}-e_{i}\\right)+\\left(\\widehat{e}_{i}-e_{i}\\right)^{2}\\), then\n\\[\n\\begin{aligned}\n\\left\\|\\frac{1}{n} \\sum_{i=1}^{n} X_{i} X_{i}^{\\prime}\\left(\\widehat{e}_{i}^{2}-e_{i}^{2}\\right)\\right\\| & \\leq \\frac{1}{n} \\sum_{i=1}^{n}\\left\\|X_{i} X_{i}^{\\prime}\\right\\|\\left|\\widehat{e}_{i}^{2}-e_{i}^{2}\\right| \\\\\n& \\leq \\frac{2}{n} \\sum_{i=1}^{n}\\left\\|X_{i}\\right\\|^{2}\\left|e _ { i } \\left\\|\\widehat{e}_{i}-e_{i}\\left|+\\frac{1}{n} \\sum_{i=1}^{n}\\left\\|X_{i}\\right\\|^{2}\\right| \\widehat{e}_{i}-\\left.e_{i}\\right|^{2}\\right.\\right.\\\\\n& \\leq \\frac{2}{n} \\sum_{i=1}^{n}\\left\\|X_{i}\\right\\|^{2}\\left|e_{i}\\right| q_{n}+\\frac{1}{n} \\sum_{i=1}^{n}\\left\\|X_{i}\\right\\|^{2} q_{n}^{2} \\\\\n& \\leq o_{p}\\left(n^{-1 / 4}\\right) .\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "chpt07-asymptotic-ls.html#asymptotic-leverage",
    "href": "chpt07-asymptotic-ls.html#asymptotic-leverage",
    "title": "7  Asymptotic Theory for Least Squares",
    "section": "7.21 Asymptotic Leverage*",
    "text": "7.21 Asymptotic Leverage*\nRecall the definition of leverage from (3.40) \\(h_{i i}=X_{i}^{\\prime}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} X_{i}\\). These are the diagonal elements of the projection matrix \\(\\boldsymbol{P}\\) and appear in the formula for leave-one-out prediction errors and \\(\\mathrm{HC} 2\\) and HC3 covariance matrix estimators. We can show that under i.i.d. sampling the leverage values are uniformly asymptotically small.\nLet \\(\\lambda_{\\min }(\\boldsymbol{A})\\) and \\(\\lambda_{\\max }(\\boldsymbol{A})\\) denote the smallest and largest eigenvalues of a symmetric square matrix \\(\\boldsymbol{A}\\) and note that \\(\\lambda_{\\max }\\left(\\boldsymbol{A}^{-1}\\right)=\\left(\\lambda_{\\min }(\\boldsymbol{A})\\right)^{-1}\\). Since \\(\\frac{1}{n} \\boldsymbol{X}^{\\prime} \\boldsymbol{X} \\underset{p}{\\rightarrow} \\boldsymbol{Q}_{X X}>0\\), by the CMT \\(\\lambda_{\\min }\\left(\\frac{1}{n} \\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right) \\underset{p}{\\rightarrow} \\lambda_{\\min }\\left(\\boldsymbol{Q}_{X X}\\right)>0\\). (The latter is positive since \\(\\boldsymbol{Q}_{X X}\\) is positive definite and thus all its eigenvalues are positive.) Then by the Quadratic Inequality (B.18)\n\\[\n\\begin{aligned}\nh_{i i} &=X_{i}^{\\prime}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} X_{i} \\\\\n& \\leq \\lambda_{\\max }\\left(\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1}\\right)\\left(X_{i}^{\\prime} X_{i}\\right) \\\\\n&=\\left(\\lambda_{\\min }\\left(\\frac{1}{n} \\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)\\right)^{-1} \\frac{1}{n}\\left\\|X_{i}\\right\\|^{2} \\\\\n& \\leq\\left(\\lambda_{\\min }\\left(\\boldsymbol{Q}_{X X}\\right)+o_{p}(1)\\right)^{-1} \\frac{1}{n} \\max _{1 \\leq i \\leq n}\\left\\|X_{i}\\right\\|^{2} .\n\\end{aligned}\n\\]\nTheorem \\(6.15\\) shows that \\(\\mathbb{E}\\|X\\|^{r}<\\infty\\) implies \\(\\max _{1 \\leq i \\leq n}\\left\\|X_{i}\\right\\|^{2}=\\left(\\max _{1 \\leq i \\leq n}\\left\\|X_{i}\\right\\|\\right)^{2}=o_{p}\\left(n^{2 / r}\\right)\\) and thus (7.42) is \\(o_{p}\\left(n^{2 / r-1}\\right)\\)\nTheorem 7.17 If \\(X_{i}\\) is i.i.d., \\(\\boldsymbol{Q}_{X X}>0\\), and \\(\\mathbb{E}\\|X\\|^{r}<\\infty\\) for some \\(r \\geq 2\\), then \\(\\max _{1 \\leq i \\leq n} h_{i i}=o_{p}\\left(n^{2 / r-1}\\right)\\).\nFor any \\(r \\geq 2\\) then \\(h_{i i}=o_{p}\\) (1) (uniformly in \\(i \\leq n\\) ). Larger \\(r\\) implies a faster rate of convergence. For example \\(r=4\\) implies \\(h_{i i}=o_{p}\\left(n^{-1 / 2}\\right)\\).\nTheorem (7.17) implies that under random sampling with finite variances and large samples no individual observation should have a large leverage value. Consequently, individual observations should not be influential unless one of these conditions is violated."
  },
  {
    "objectID": "chpt07-asymptotic-ls.html#exercises",
    "href": "chpt07-asymptotic-ls.html#exercises",
    "title": "7  Asymptotic Theory for Least Squares",
    "section": "7.22 Exercises",
    "text": "7.22 Exercises\nExercise 7.1 Take the model \\(Y=X_{1}^{\\prime} \\beta_{1}+X_{2}^{\\prime} \\beta_{2}+e\\) with \\(\\mathbb{E}[X e]=0\\). Suppose that \\(\\beta_{1}\\) is estimated by regressing \\(Y\\) on \\(X_{1}\\) only. Find the probability limit of this estimator. In general, is it consistent for \\(\\beta_{1}\\) ? If not, under what conditions is this estimator consistent for \\(\\beta_{1}\\) ?\nExercise 7.2 Take the model \\(Y=X^{\\prime} \\beta+e\\) with \\(\\mathbb{E}[X e]=0\\). Define the ridge regression estimator\n\\[\n\\widehat{\\beta}=\\left(\\sum_{i=1}^{n} X_{i} X_{i}^{\\prime}+\\lambda \\boldsymbol{I}_{k}\\right)^{-1}\\left(\\sum_{i=1}^{n} X_{i} Y_{i}\\right)\n\\]\nhere \\(\\lambda>0\\) is a fixed constant. Find the probability limit of \\(\\widehat{\\beta}\\) as \\(n \\rightarrow \\infty\\). Is \\(\\widehat{\\beta}\\) consistent for \\(\\beta\\) ?\nExercise 7.3 For the ridge regression estimator (7.43), set \\(\\lambda=c n\\) where \\(c>0\\) is fixed as \\(n \\rightarrow \\infty\\). Find the probability limit of \\(\\widehat{\\beta}\\) as \\(n \\rightarrow \\infty\\). Exercise 7.4 Verify some of the calculations reported in Section 7.4. Specifically, suppose that \\(X_{1}\\) and \\(X_{2}\\) only take the values \\(\\{-1,+1\\}\\), symmetrically, with\n\\[\n\\begin{aligned}\n\\mathbb{P}\\left[X_{1}=X_{2}=1\\right] &=\\mathbb{P}\\left[X_{1}=X_{2}=-1\\right]=3 / 8 \\\\\n\\mathbb{P}\\left[X_{1}=1, X_{2}=-1\\right] &=\\mathbb{P}\\left[X_{1}=-1, X_{2 i}=1\\right]=1 / 8 \\\\\n\\mathbb{E}\\left[e_{i}^{2} \\mid X_{1}=X_{2}\\right] &=\\frac{5}{4} \\\\\n\\mathbb{E}\\left[e_{i}^{2} \\mid X_{1} \\neq X_{2}\\right] &=\\frac{1}{4} .\n\\end{aligned}\n\\]\nVerify the following:\\ (a) \\(\\mathbb{E}\\left[X_{1}\\right]=0\\)\\ (b) \\(\\mathbb{E}\\left[X_{1}^{2}\\right]=1\\)\\ (c) \\(\\mathbb{E}\\left[X_{1} X_{2}\\right]=\\frac{1}{2}\\)\\ (d) \\(\\mathbb{E}\\left[e^{2}\\right]=1\\)\\ (e) \\(\\mathbb{E}\\left[X_{1}^{2} e^{2}\\right]=1\\)\\ (f) \\(\\mathbb{E}\\left[X_{1} X_{2} e^{2}\\right]=\\frac{7}{8}\\).\nExercise 7.5 Show (7.13)-(7.16).\nExercise \\(7.6\\) The model is\n\\[\n\\begin{aligned}\nY &=X^{\\prime} \\beta+e \\\\\n\\mathbb{E}[X e] &=0 \\\\\n\\Omega &=\\mathbb{E}\\left[X X^{\\prime} e^{2}\\right] .\n\\end{aligned}\n\\]\nFind the method of moments estimators \\((\\widehat{\\beta}, \\widehat{\\Omega})\\) for \\((\\beta, \\Omega)\\).\nExercise 7.7 Of the variables \\(\\left(Y^{*}, Y, X\\right)\\) only the pair \\((Y, X)\\) are observed. In this case we say that \\(Y^{*}\\) is a latent variable. Suppose\n\\[\n\\begin{aligned}\nY^{*} &=X^{\\prime} \\beta+e \\\\\n\\mathbb{E}[X e] &=0 \\\\\nY &=Y^{*}+u\n\\end{aligned}\n\\]\nwhere \\(u\\) is a measurement error satisfying\n\\[\n\\begin{aligned}\n\\mathbb{E}[X u] &=0 \\\\\n\\mathbb{E}\\left[Y^{*} u\\right] &=0 .\n\\end{aligned}\n\\]\nLet \\(\\widehat{\\beta}\\) denote the OLS coefficient from the regression of \\(Y\\) on \\(X\\).\n\nIs \\(\\beta\\) the coefficient from the linear projection of \\(Y\\) on \\(X\\) ? (b) Is \\(\\widehat{\\beta}\\) consistent for \\(\\beta\\) as \\(n \\rightarrow \\infty\\) ?\nFind the asymptotic distribution of \\(\\sqrt{n}(\\widehat{\\beta}-\\beta)\\) as \\(n \\rightarrow \\infty\\).\n\nExercise 7.8 Find the asymptotic distribution of \\(\\sqrt{n}\\left(\\widehat{\\sigma}^{2}-\\sigma^{2}\\right)\\) as \\(n \\rightarrow \\infty\\).\nExercise 7.9 The model is \\(Y=X \\beta+e\\) with \\(\\mathbb{E}[e \\mid X]=0\\) and \\(X \\in \\mathbb{R}\\). Consider the two estimators\n\\[\n\\begin{aligned}\n&\\widehat{\\beta}=\\frac{\\sum_{i=1}^{n} X_{i} Y_{i}}{\\sum_{i=1}^{n} X_{i}^{2}} \\\\\n&\\widetilde{\\beta}=\\frac{1}{n} \\sum_{i=1}^{n} \\frac{Y_{i}}{X_{i}} .\n\\end{aligned}\n\\]\n\nUnder the stated assumptions are both estimators consistent for \\(\\beta\\) ?\nAre there conditions under which either estimator is efficient?\n\nExercise 7.10 In the homoskedastic regression model \\(Y=X^{\\prime} \\beta+e\\) with \\(\\mathbb{E}[e \\mid x]=0\\) and \\(\\mathbb{E}\\left[e^{2} \\mid X\\right]=\\sigma^{2}\\) suppose \\(\\widehat{\\beta}\\) is the OLS estimator of \\(\\beta\\) with covariance matrix estimator \\(\\widehat{\\boldsymbol{V}}_{\\widehat{\\beta}}\\) based on a sample of size \\(n\\). Let \\(\\widehat{\\sigma}^{2}\\) be the estimator of \\(\\sigma^{2}\\). You wish to forecast an out-of-sample value of \\(Y_{n+1}\\) given that \\(X_{n+1}=x\\). Thus the available information is the sample, the estimates \\(\\left(\\widehat{\\beta}, \\widehat{\\boldsymbol{V}}_{\\widehat{\\beta}}, \\widehat{\\sigma}^{2}\\right)\\), the residuals \\(\\widehat{e}_{i}\\), and the out-of-sample value of the regressors \\(X_{n+1}\\).\n\nFind a point forecast of \\(Y_{n+1}\\).\nFind an estimator of the variance of this forecast.\n\nExercise 7.11 Take a regression model with i.i.d. observations \\(\\left(Y_{i}, X_{i}\\right)\\) with \\(X \\in \\mathbb{R}\\)\n\\[\n\\begin{aligned}\nY &=X \\beta+e \\\\\n\\mathbb{E}[e \\mid X] &=0 \\\\\n\\Omega &=\\mathbb{E}\\left[X^{2} e^{2}\\right] .\n\\end{aligned}\n\\]\nLet \\(\\widehat{\\beta}\\) be the OLS estimator of \\(\\beta\\) with residuals \\(\\widehat{e}_{i}=Y_{i}-X_{i} \\widehat{\\beta}\\). Consider the estimators of \\(\\Omega\\)\n\\[\n\\begin{aligned}\n&\\widetilde{\\Omega}=\\frac{1}{n} \\sum_{i=1}^{n} X_{i}^{2} e_{i}^{2} \\\\\n&\\widehat{\\Omega}=\\frac{1}{n} \\sum_{i=1}^{n} X_{i}^{2} \\widehat{e}_{i}^{2} .\n\\end{aligned}\n\\]\n\nFind the asymptotic distribution of \\(\\sqrt{n}(\\widetilde{\\Omega}-\\Omega)\\) as \\(n \\rightarrow \\infty\\).\nFind the asymptotic distribution of \\(\\sqrt{n}(\\widehat{\\Omega}-\\Omega)\\) as \\(n \\rightarrow \\infty\\).\nHow do you use the regression assumption \\(\\mathbb{E}\\left[e_{i} \\mid X_{i}\\right]=0\\) in your answer to (b)?\n\nExercise 7.12 Consider the model\n\\[\n\\begin{aligned}\nY &=\\alpha+\\beta X+e \\\\\n\\mathbb{E}[e] &=0 \\\\\n\\mathbb{E}[X e] &=0\n\\end{aligned}\n\\]\nwith both \\(Y\\) and \\(X\\) scalar. Assuming \\(\\alpha>0\\) and \\(\\beta<0\\) suppose the parameter of interest is the area under the regression curve (e.g. consumer surplus), which is \\(A=-\\alpha^{2} / 2 \\beta\\).\nLet \\(\\widehat{\\theta}=(\\widehat{\\alpha}, \\widehat{\\beta})^{\\prime}\\) be the least squares estimators of \\(\\theta=(\\alpha, \\beta)^{\\prime}\\) so that \\(\\sqrt{n}(\\widehat{\\theta}-\\theta) \\rightarrow{ }_{d} N\\left(0, \\boldsymbol{V}_{\\theta}\\right)\\) and let \\(\\widehat{\\boldsymbol{V}}_{\\theta}\\) be a standard estimator for \\(\\boldsymbol{V}_{\\theta}\\).\n\nGiven the above, describe an estimator of \\(A\\).\nConstruct an asymptotic \\(1-\\eta\\) confidence interval for \\(A\\).\n\nExercise 7.13 Consider an i.i.d. sample \\(\\left\\{Y_{i}, X_{i}\\right\\} i=1, \\ldots, n\\) where \\(Y\\) and \\(X\\) are scalar. Consider the reverse projection model \\(X=Y \\gamma+u\\) with \\(\\mathbb{E}[Y u]=0\\) and define the parameter of interest as \\(\\theta=1 / \\gamma\\).\n\nPropose an estimator \\(\\widehat{\\gamma}\\) of \\(\\gamma\\).\nPropose an estimator \\(\\widehat{\\theta}\\) of \\(\\theta\\).\nFind the asymptotic distribution of \\(\\widehat{\\theta}\\).\nFind an asymptotic standard error for \\(\\widehat{\\theta}\\).\n\nExercise 7.14 Take the model\n\\[\n\\begin{aligned}\nY &=X_{1} \\beta_{1}+X_{2} \\beta_{2}+e \\\\\n\\mathbb{E}[X e] &=0\n\\end{aligned}\n\\]\nwith both \\(\\beta_{1} \\in \\mathbb{R}\\) and \\(\\beta_{2} \\in \\mathbb{R}\\), and define the parameter \\(\\theta=\\beta_{1} \\beta_{2}\\).\n\nWhat is the appropriate estimator \\(\\widehat{\\theta}\\) for \\(\\theta\\) ?\nFind the asymptotic distribution of \\(\\widehat{\\theta}\\) under standard regularity conditions.\nShow how to calculate an asymptotic \\(95 %\\) confidence interval for \\(\\theta\\).\n\nExercise 7.15 Take the linear model \\(Y=X \\beta+e\\) with \\(\\mathbb{E}[e \\mid X]=0\\) and \\(X \\in \\mathbb{R}\\). Consider the estimator\n\\[\n\\widehat{\\beta}=\\frac{\\sum_{i=1}^{n} X_{i}^{3} Y_{i}}{\\sum_{i=1}^{n} X_{i}^{4}}\n\\]\nFind the asymptotic distribution of \\(\\sqrt{n}(\\widehat{\\beta}-\\beta)\\) as \\(n \\rightarrow \\infty\\).\nExercise 7.16 From an i.i.d. sample \\(\\left(Y_{i}, X_{i}\\right)\\) of size \\(n\\) you randomly take half the observations. You estimate a least squares regression of \\(Y\\) on \\(X\\) using only this sub-sample. Is the estimated slope coefficient \\(\\widehat{\\beta}\\) consistent for the population projection coefficient? Explain your reasoning.\nExercise 7.17 An economist reports a set of parameter estimates, including the coefficient estimates \\(\\widehat{\\beta}_{1}=1.0, \\widehat{\\beta}_{2}=0.8\\), and standard errors \\(s\\left(\\widehat{\\beta}_{1}\\right)=0.07\\) and \\(s\\left(\\widehat{\\beta}_{2}\\right)=0.07\\). The author writes “The estimates show that \\(\\beta_{1}\\) is larger than \\(\\beta_{2} . \"\\)\n\nWrite down the formula for an asymptotic 95% confidence interval for \\(\\theta=\\beta_{1}-\\beta_{2}\\), expressed as a function of \\(\\widehat{\\beta}_{1}, \\widehat{\\beta}_{2}, s\\left(\\widehat{\\beta}_{1}\\right), s\\left(\\widehat{\\beta}_{2}\\right)\\) and \\(\\widehat{\\rho}\\), where \\(\\widehat{\\rho}\\) is the estimated correlation between \\(\\widehat{\\beta}_{1}\\) and \\(\\widehat{\\beta}_{2}\\).\nCan \\(\\widehat{\\rho}\\) be calculated from the reported information? (c) Is the author correct? Does the reported information support the author’s claim?\n\nExercise 7.18 Suppose an economic model suggests\n\\[\nm(x)=\\mathbb{E}[Y \\mid X=x]=\\beta_{0}+\\beta_{1} x+\\beta_{2} x^{2}\n\\]\nwhere \\(X \\in \\mathbb{R}\\). You have a random sample \\(\\left(Y_{i}, X_{i}\\right), i=1, \\ldots, n\\).\n\nDescribe how to estimate \\(m(x)\\) at a given value \\(x\\).\nDescribe (be specific) an appropriate confidence interval for \\(m(x)\\).\n\nExercise 7.19 Take the model \\(Y=X^{\\prime} \\beta+e\\) with \\(\\mathbb{E}[X e]=0\\) and suppose you have observations \\(i=1, \\ldots, 2 n\\). (The number of observations is \\(2 n\\).) You randomly split the sample in half, (each has \\(n\\) observations), calculate \\(\\widehat{\\beta}_{1}\\) by least squares on the first sample, and \\(\\widehat{\\beta}_{2}\\) by least squares on the second sample. What is the asymptotic distribution of \\(\\sqrt{n}\\left(\\widehat{\\beta}_{1}-\\widehat{\\beta}_{2}\\right)\\) ?\nExercise 7.20 The variables \\(\\left\\{Y_{i}, X_{i}, W_{i}\\right\\}\\) are a random sample. The parameter \\(\\beta\\) is estimated by minimizing the criterion function\n\\[\nS(\\beta)=\\sum_{i=1}^{n} W_{i}\\left(Y_{i}-X_{i}^{\\prime} \\beta\\right)^{2}\n\\]\nThat is \\(\\widehat{\\beta}=\\operatorname{argmin}_{\\beta} S(\\beta)\\).\n\nFind an explicit expression for \\(\\widehat{\\beta}\\).\nWhat population parameter \\(\\beta\\) is \\(\\widehat{\\beta}\\) estimating? Be explicit about any assumptions you need to impose. Do not make more assumptions than necessary.\nFind the probability limit for \\(\\widehat{\\beta}\\) as \\(n \\rightarrow \\infty\\).\nFind the asymptotic distribution of \\(\\sqrt{n}(\\widehat{\\beta}-\\beta)\\) as \\(n \\rightarrow \\infty\\).\n\nExercise 7.21 Take the model\n\\[\n\\begin{aligned}\nY &=X^{\\prime} \\beta+e \\\\\n\\mathbb{E}[e \\mid X] &=0 \\\\\n\\mathbb{E}\\left[e^{2} \\mid X\\right] &=Z^{\\prime} \\gamma\n\\end{aligned}\n\\]\nwhere \\(Z\\) is a (vector) function of \\(X\\). The sample is \\(i=1, \\ldots, n\\) with i.i.d. observations. Assume that \\(Z^{\\prime} \\gamma>0\\) for all \\(Z\\). Suppose you want to forecast \\(Y_{n+1}\\) given \\(X_{n+1}=x\\) and \\(Z_{n+1}=z\\) for an out-of-sample observation \\(n+1\\). Describe how you would construct a point forecast and a forecast interval for \\(Y_{n+1}\\).\nExercise 7.22 Take the model\n\\[\n\\begin{aligned}\nY &=X^{\\prime} \\beta+e \\\\\n\\mathbb{E}[e \\mid X] &=0 \\\\\nZ &=X^{\\prime} \\beta \\gamma+u \\\\\n\\mathbb{E}[u \\mid X] &=0\n\\end{aligned}\n\\]\nwhere \\(X\\) is a \\(k\\) vector and \\(Z\\) is scalar. Your goal is to estimate the scalar parameter \\(\\gamma\\). You use a two-step estimator: - Estimate \\(\\widehat{\\beta}\\) by least squares of \\(Y\\) on \\(X\\).\n\nEstimate \\(\\widehat{\\gamma}\\) by least squares of \\(Z\\) on \\(X^{\\prime} \\widehat{\\beta}\\).\n\n\nShow that \\(\\widehat{\\gamma}\\) is consistent for \\(\\gamma\\).\nFind the asymptotic distribution of \\(\\widehat{\\gamma}\\) when \\(\\gamma=0\\)\n\nExercise 7.23 The model is \\(Y=X+e\\) with \\(\\mathbb{E}[e \\mid X]=0\\) and \\(X \\in \\mathbb{R}\\). Consider the estimator\n\\[\n\\widetilde{\\beta}=\\frac{1}{n} \\sum_{i=1}^{n} \\frac{Y_{i}}{X_{i}} .\n\\]\nFind conditions under which \\(\\widetilde{\\beta}\\) is consistent for \\(\\beta\\) as \\(n \\rightarrow \\infty\\).\nExercise 7.24 The parameter \\(\\beta\\) is defined in the model \\(Y=X^{*} \\beta+e\\) where \\(e\\) is independent of \\(X^{*} \\geq 0\\), \\(\\mathbb{E}[e]=0, \\mathbb{E}\\left[e^{2}\\right]=\\sigma^{2}\\). The observables are \\((Y, X)\\) where \\(X=X^{*} v\\) and \\(v>0\\) is random scale measurement error, independent of \\(X^{*}\\) and \\(e\\). Consider the least squares estimator \\(\\widehat{\\beta}\\) for \\(\\beta\\).\n\nFind the plim of \\(\\widehat{\\beta}\\) expressed in terms of \\(\\beta\\) and moments of \\((X, v, e)\\).\nCan you find a non-trivial condition under which \\(\\widehat{\\beta}\\) is consistent for \\(\\beta\\) ? (By non-trivial we mean something other than \\(v=1\\).)\n\nExercise 7.25 Take the projection model \\(Y=X^{\\prime} \\beta+e\\) with \\(\\mathbb{E}[X e]=0\\). For a positive function \\(w(x)\\) let \\(W_{i}=w\\left(X_{i}\\right)\\). Consider the estimator\n\\[\n\\widetilde{\\beta}=\\left(\\sum_{i=1}^{n} W_{i} X_{i} X_{i}^{\\prime}\\right)^{-1}\\left(\\sum_{i=1}^{n} W_{i} X_{i} Y_{i}\\right) .\n\\]\nFind the probability limit (as \\(n \\rightarrow \\infty\\) ) of \\(\\widetilde{\\beta}\\). Do you need to add an assumption? Is \\(\\widetilde{\\beta}\\) consistent for \\(\\widetilde{\\beta}\\) ? If not, under what assumption is \\(\\widetilde{\\beta}\\) consistent for \\(\\beta\\) ?\nExercise 7.26 Take the regression model\n\\[\n\\begin{aligned}\nY &=X^{\\prime} \\beta+e \\\\\n\\mathbb{E}[e \\mid X] &=0 \\\\\n\\mathbb{E}\\left[e^{2} \\mid X=x\\right] &=\\sigma^{2}(x)\n\\end{aligned}\n\\]\nwith \\(X \\in \\mathbb{R}^{k}\\). Assume that \\(\\mathbb{P}[e=0]=0\\). Consider the infeasible estimator\n\\[\n\\widetilde{\\beta}=\\left(\\sum_{i=1}^{n} e_{i}^{-2} X_{i} X_{i}^{\\prime}\\right)^{-1}\\left(\\sum_{i=1}^{n} e_{i}^{-2} X_{i} Y_{i}\\right) .\n\\]\nThis is a WLS estimator using the weights \\(e_{i}^{-2}\\).\n\nFind the asymptotic distribution of \\(\\widetilde{\\beta}\\).\nContrast your result with the asymptotic distribution of infeasible GLS. Exercise 7.27 The model is \\(Y=X^{\\prime} \\beta+e\\) with \\(\\mathbb{E}[e \\mid X]=0\\). An econometrician is worried about the impact of some unusually large values of the regressors. The model is thus estimated on the subsample for which \\(\\left|X_{i}\\right| \\leq c\\) for some fixed \\(c\\). Let \\(\\widetilde{\\beta}\\) denote the OLS estimator on this subsample. It equals\n\n\\[\n\\widetilde{\\beta}=\\left(\\sum_{i=1}^{n} X_{i} X_{i}^{\\prime} \\mathbb{1}\\left\\{\\left|X_{i}\\right| \\leq c\\right\\}\\right)^{-1}\\left(\\sum_{i=1}^{n} X_{i} Y_{i} \\mathbb{1}\\left\\{\\left|X_{i}\\right| \\leq c\\right\\}\\right) .\n\\]\n\nShow that \\(\\widetilde{\\beta} \\underset{p}{\\longrightarrow} \\beta\\).\nFind the asymptotic distribution of \\(\\sqrt{n}(\\widetilde{\\beta}-\\beta)\\).\n\nExercise 7.28 As in Exercise 3.26, use the cps09mar dataset and the subsample of white male Hispanics. Estimate the regression\n\\[\n\\widehat{\\log (\\text { wage })}=\\beta_{1} \\text { education }+\\beta_{2} \\text { experience }+\\beta_{3} \\text { experience }^{2} / 100+\\beta_{4} .\n\\]\n\nReport the coefficient estimates and robust standard errors.\nLet \\(\\theta\\) be the ratio of the return to one year of education to the return to one year of experience for experience \\(=10\\). Write \\(\\theta\\) as a function of the regression coefficients and variables. Compute \\(\\widehat{\\theta}\\) from the estimated model.\nWrite out the formula for the asymptotic standard error for \\(\\hat{\\theta}\\) as a function of the covariance matrix for \\(\\widehat{\\beta}\\). Compute \\(s(\\widehat{\\theta})\\) from the estimated model.\nConstruct a \\(90 %\\) asymptotic confidence interval for \\(\\theta\\) from the estimated model.\nCompute the regression function at education \\(=12\\) and experience \\(=20\\). Compute a 95% confidence interval for the regression function at this point.\nConsider an out-of-sample individual with 16 years of education and 5 years experience. Construct an \\(80 %\\) forecast interval for their log wage and wage. [To obtain the forecast interval for the wage, apply the exponential function to both endpoints.]"
  },
  {
    "objectID": "chpt08-restricted-est.html#introduction",
    "href": "chpt08-restricted-est.html#introduction",
    "title": "8  Restricted Estimation",
    "section": "8.1 Introduction",
    "text": "8.1 Introduction\nIn the linear projection model\n\\[\n\\begin{aligned}\nY &=X^{\\prime} \\beta+e \\\\\n\\mathbb{E}[X e] &=0\n\\end{aligned}\n\\]\na common task is to impose a constraint on the coefficient vector \\(\\beta\\). For example, partitioning \\(X^{\\prime}=\\) \\(\\left(X_{1}^{\\prime}, X_{2}^{\\prime}\\right)\\) and \\(\\beta^{\\prime}=\\left(\\beta_{1}^{\\prime}, \\beta_{2}^{\\prime}\\right)\\) a typical constraint is an exclusion restriction of the form \\(\\beta_{2}=0\\). In this case the constrained model is\n\\[\n\\begin{aligned}\nY &=X_{1}^{\\prime} \\beta_{1}+e \\\\\n\\mathbb{E}[X e] &=0 .\n\\end{aligned}\n\\]\nAt first glance this appears the same as the linear projection model but there is one important difference: the error \\(e\\) is uncorrelated with the entire regressor vector \\(X^{\\prime}=\\left(X_{1}^{\\prime}, X_{2}^{\\prime}\\right)\\) not just the included regressor \\(X_{1}\\).\nIn general, a set of \\(q\\) linear constraints on \\(\\beta\\) takes the form\n\\[\n\\boldsymbol{R}^{\\prime} \\beta=\\boldsymbol{c}\n\\]\nwhere \\(\\boldsymbol{R}\\) is \\(k \\times q, \\operatorname{rank}(\\boldsymbol{R})=q<k\\), and \\(\\boldsymbol{c}\\) is \\(q \\times 1\\). The assumption that \\(\\boldsymbol{R}\\) is full rank means that the constraints are linearly independent (there are no redundant or contradictory constraints). We define the restricted parameter space \\(B\\) as the set of values of \\(\\beta\\) which satisfy (8.1), that is\n\\[\nB=\\left\\{\\beta: \\boldsymbol{R}^{\\prime} \\beta=\\boldsymbol{c}\\right\\} .\n\\]\nSometimes we will call (8.1) a constraint and sometimes a restriction. They are the same thing. Similarly sometimes we will call estimators which satisfy (8.1) constrained estimators and sometimes restricted estimators. They mean the same thing.\nThe constraint \\(\\beta_{2}=0\\) discussed above is a special case of the constraint (8.1) with\n\\[\n\\boldsymbol{R}=\\left(\\begin{array}{c}\n0 \\\\\n\\boldsymbol{I}_{k_{2}}\n\\end{array}\\right)\n\\]\na selector matrix, and \\(\\boldsymbol{c}=0 .\\) Another common restriction is that a set of coefficients sum to a known constant, i.e. \\(\\beta_{1}+\\beta_{2}=1\\). For example, this constraint arises in a constant-return-to-scale production function. Other common restrictions include the equality of coefficients \\(\\beta_{1}=\\beta_{2}\\), and equal and offsetting coefficients \\(\\beta_{1}=-\\beta_{2}\\).\nA typical reason to impose a constraint is that we believe (or have information) that the constraint is true. By imposing the constraint we hope to improve estimation efficiency. The goal is to obtain consistent estimates with reduced variance relative to the unconstrained estimator.\nThe questions then arise: How should we estimate the coefficient vector \\(\\beta\\) imposing the linear restriction (8.1)? If we impose such constraints what is the sampling distribution of the resulting estimator? How should we calculate standard errors? These are the questions explored in this chapter."
  },
  {
    "objectID": "chpt08-restricted-est.html#constrained-least-squares",
    "href": "chpt08-restricted-est.html#constrained-least-squares",
    "title": "8  Restricted Estimation",
    "section": "8.2 Constrained Least Squares",
    "text": "8.2 Constrained Least Squares\nAn intuitively appealing method to estimate a constrained linear projection is to minimize the least squares criterion subject to the constraint \\(\\boldsymbol{R}^{\\prime} \\beta=\\boldsymbol{c}\\).\nThe constrained least squares estimator is\n\\[\n\\widetilde{\\beta}_{\\mathrm{cls}}=\\underset{\\boldsymbol{R}^{\\prime} \\beta=\\boldsymbol{c}}{\\operatorname{argmin}} \\operatorname{SSE}(\\beta)\n\\]\nwhere\n\\[\n\\operatorname{SSE}(\\beta)=\\sum_{i=1}^{n}\\left(Y_{i}-X_{i}^{\\prime} \\beta\\right)^{2}=\\boldsymbol{Y}^{\\prime} \\boldsymbol{Y}-2 \\boldsymbol{Y}^{\\prime} \\boldsymbol{X} \\beta+\\beta^{\\prime} \\boldsymbol{X}^{\\prime} \\boldsymbol{X} \\beta\n\\]\nThe estimator \\(\\widetilde{\\beta}_{\\text {cls }}\\) minimizes the sum of squared errors over all \\(\\beta \\in B\\), or equivalently such that the restriction (8.1) holds. We call \\(\\widetilde{\\beta}_{\\text {cls }}\\) the constrained least squares (CLS) estimator. We use the convention of using a tilde ” \\(\\sim\\) ” rather than a hat ” \\(\\wedge\\) ” to indicate that \\(\\widetilde{\\beta}_{\\text {cls }}\\) is a restricted estimator in contrast to the unrestricted least squares estimator \\(\\widehat{\\beta}\\) and write it as \\(\\widetilde{\\beta}_{\\text {cls }}\\) to be clear that the estimation method is CLS.\nOne method to find the solution to (8.3) is the technique of Lagrange multipliers. The problem (8.3) is equivalent to finding the critical points of the Lagrangian\n\\[\n\\mathscr{L}(\\beta, \\lambda)=\\frac{1}{2} \\operatorname{SSE}(\\beta)+\\lambda^{\\prime}\\left(\\boldsymbol{R}^{\\prime} \\beta-\\boldsymbol{c}\\right)\n\\]\nover \\((\\beta, \\lambda)\\) where \\(\\lambda\\) is an \\(s \\times 1\\) vector of Lagrange multipliers. The solution is a saddlepoint. The Lagrangian is minimized over \\(\\beta\\) while maximized over \\(\\lambda\\). The first-order conditions for the solution of (8.5) are\n\\[\n\\frac{\\partial}{\\partial \\beta} \\mathscr{L}\\left(\\widetilde{\\beta}_{\\mathrm{cls}}, \\widetilde{\\lambda}_{\\mathrm{cls}}\\right)=-\\boldsymbol{X}^{\\prime} \\boldsymbol{Y}+\\boldsymbol{X}^{\\prime} \\boldsymbol{X} \\widetilde{\\beta}_{\\mathrm{cls}}+\\boldsymbol{R} \\widetilde{\\lambda}_{\\mathrm{cls}}=0\n\\]\nand\n\\[\n\\frac{\\partial}{\\partial \\lambda} \\mathscr{L}\\left(\\widetilde{\\beta}_{\\mathrm{cls}}, \\widetilde{\\lambda}_{\\mathrm{cls}}\\right)=\\boldsymbol{R}^{\\prime} \\widetilde{\\beta}-\\boldsymbol{c}=0\n\\]\nPremultiplying (8.6) by \\(\\boldsymbol{R}^{\\prime}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1}\\) we obtain\n\\[\n-\\boldsymbol{R}^{\\prime} \\widehat{\\beta}+\\boldsymbol{R}^{\\prime} \\widetilde{\\beta}_{\\mathrm{cls}}+\\boldsymbol{R}^{\\prime}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\boldsymbol{R} \\tilde{\\lambda}_{\\text {cls }}=0\n\\]\nwhere \\(\\widehat{\\beta}=\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\boldsymbol{X}^{\\prime} \\boldsymbol{Y}\\) is the unrestricted least squares estimator. Imposing \\(\\boldsymbol{R}^{\\prime} \\widetilde{\\beta}_{\\text {cls }}-\\boldsymbol{c}=0\\) from (8.7) and solving for \\(\\widetilde{\\lambda}_{\\text {cls we find }}\\)\n\\[\n\\tilde{\\lambda}_{\\text {cls }}=\\left[\\boldsymbol{R}^{\\prime}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\boldsymbol{R}\\right]^{-1}\\left(\\boldsymbol{R}^{\\prime} \\widehat{\\beta}-\\boldsymbol{c}\\right) .\n\\]\nNotice that \\(\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1}>0\\) and \\(\\boldsymbol{R}\\) full rank imply that \\(\\boldsymbol{R}^{\\prime}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\boldsymbol{R}>0\\) and is hence invertible. (See Section A.10.) Substituting this expression into (8.6) and solving for \\(\\widetilde{\\beta}_{\\text {cls }}\\) we find the solution to the constrained minimization problem (8.3)\n\\[\n\\widetilde{\\beta}_{\\text {cls }}=\\widehat{\\beta}_{\\text {ols }}-\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\boldsymbol{R}\\left[\\boldsymbol{R}^{\\prime}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\boldsymbol{R}\\right]^{-1}\\left(\\boldsymbol{R}^{\\prime} \\widehat{\\beta}_{\\text {ols }}-\\boldsymbol{c}\\right) .\n\\]\n(See Exercise \\(8.5\\) to verify that (8.8) satisfies (8.1).)\nThis is a general formula for the CLS estimator. It also can be written as\n\\[\n\\widetilde{\\beta}_{\\mathrm{cls}}=\\widehat{\\beta}_{\\mathrm{ols}}-\\widehat{\\boldsymbol{Q}}_{X X}^{-1} \\boldsymbol{R}\\left(\\boldsymbol{R}^{\\prime} \\widehat{\\boldsymbol{Q}}_{X X}^{-1} \\boldsymbol{R}\\right)^{-1}\\left(\\boldsymbol{R}^{\\prime} \\widehat{\\beta}_{\\mathrm{ols}}-\\boldsymbol{c}\\right) .\n\\]\nThe CLS residuals are \\(\\widetilde{e}_{i}=Y_{i}-X_{i}^{\\prime} \\widetilde{\\beta}_{\\text {cls }}\\) and are written in vector notation as \\(\\widetilde{\\boldsymbol{e}}\\).\nTo illustrate we generated a random sample of 100 observations for the variables \\(\\left(Y, X_{1}, X_{2}\\right)\\) and calculated the sum of squared errors function for the regression of \\(Y\\) on \\(X_{1}\\) and \\(X_{2}\\). Figure \\(8.1\\) displays contour plots of the sum of squared errors function. The center of the contour plots is the least squares minimizer \\(\\widehat{\\beta}_{\\text {ols }}=(0.33,0.26)^{\\prime}\\). Suppose it is desired to estimate the coefficients subject to the constraint \\(\\beta_{1}+\\beta_{2}=1\\). This constraint is displayed in the figure by the straight line. The constrained least squares estimator is the point on this straight line which yields the smallest sum of squared errors. This is the point which intersects with the lowest contour plot. The solution is the point where a contour plot is tangent to the constraint line and is marked as \\(\\widetilde{\\beta}_{\\mathrm{cls}}=(0.52,0.48)^{\\prime}\\).\n\nFigure 8.1: Constrained Least Squares Criterion\nIn Stata constrained least squares is implemented using the cnsreg command."
  },
  {
    "objectID": "chpt08-restricted-est.html#exclusion-restriction",
    "href": "chpt08-restricted-est.html#exclusion-restriction",
    "title": "8  Restricted Estimation",
    "section": "8.3 Exclusion Restriction",
    "text": "8.3 Exclusion Restriction\nWhile (8.8) is a general formula for CLS, in most cases the estimator can be found by applying least squares to a reparameterized equation. To illustrate let us return to the first example presented at the beginning of the chapter - a simple exclusion restriction. Recall that the unconstrained model is\n\\[\nY=X_{1}^{\\prime} \\beta_{1}+X_{2}^{\\prime} \\beta_{2}+e\n\\]\nthe exclusion restriction is \\(\\beta_{2}=0\\), and the constrained equation is\n\\[\nY=X_{1}^{\\prime} \\beta_{1}+e .\n\\]\nIn this setting the CLS estimator is OLS of \\(Y\\) on \\(X_{1}\\). (See Exercise 8.1.) We can write this as\n\\[\n\\widetilde{\\beta}_{1}=\\left(\\sum_{i=1}^{n} X_{1 i} X_{1 i}^{\\prime}\\right)^{-1}\\left(\\sum_{i=1}^{n} X_{1 i} Y_{i}\\right) .\n\\]\nThe CLS estimator of the entire vector \\(\\beta^{\\prime}=\\left(\\beta_{1}^{\\prime}, \\beta_{2}^{\\prime}\\right)\\) is\n\\[\n\\widetilde{\\beta}=\\left(\\begin{array}{c}\n\\widetilde{\\beta}_{1} \\\\\n0\n\\end{array}\\right) .\n\\]\nIt is not immediately obvious but (8.8) and (8.13) are algebraically identical. To see this the first component of (8.8) with (8.2) is\n\\[\n\\widetilde{\\beta}_{1}=\\left(\\begin{array}{ll}\n\\boldsymbol{I}_{k_{2}} & 0\n\\end{array}\\right)\\left[\\widehat{\\beta}-\\widehat{\\boldsymbol{Q}}_{X X}^{-1}\\left(\\begin{array}{c}\n0 \\\\\n\\boldsymbol{I}_{k_{2}}\n\\end{array}\\right)\\left[\\left(\\begin{array}{ll}\n0 & \\boldsymbol{I}_{k_{2}}\n\\end{array}\\right) \\widehat{\\boldsymbol{Q}}_{X X}^{-1}\\left(\\begin{array}{c}\n0 \\\\\n\\boldsymbol{I}_{k_{2}}\n\\end{array}\\right)\\right]^{-1}\\left(\\begin{array}{cc}\n0 & \\boldsymbol{I}_{k_{2}}\n\\end{array}\\right) \\widehat{\\beta}\\right] .\n\\]\nUsing (3.39) this equals\n\\[\n\\begin{aligned}\n& \\widetilde{\\beta}_{1}=\\widehat{\\beta}_{1}-\\widehat{\\boldsymbol{Q}}^{12}\\left(\\widehat{\\boldsymbol{Q}}^{22}\\right)^{-1} \\widehat{\\beta}_{2} \\\\\n& =\\widehat{\\beta}_{1}+\\widehat{\\boldsymbol{Q}}_{11 \\cdot 2}^{-1} \\widehat{\\boldsymbol{Q}}_{12} \\widehat{\\boldsymbol{Q}}_{22}^{-1} \\widehat{\\boldsymbol{Q}}_{22 \\cdot 1} \\widehat{\\beta}_{2} \\\\\n& =\\widehat{\\boldsymbol{Q}}_{11 \\cdot 2}^{-1}\\left(\\widehat{\\boldsymbol{Q}}_{1 Y}-\\widehat{\\boldsymbol{Q}}_{12} \\widehat{\\boldsymbol{Q}}_{22}^{-1} \\widehat{\\boldsymbol{Q}}_{2 Y}\\right) \\\\\n& +\\widehat{\\boldsymbol{Q}}_{11 \\cdot 2}^{-1} \\widehat{\\boldsymbol{Q}}_{12} \\widehat{\\boldsymbol{Q}}_{22}^{-1} \\widehat{\\boldsymbol{Q}}_{22 \\cdot 1} \\widehat{\\boldsymbol{Q}}_{22 \\cdot 1}^{-1}\\left(\\widehat{\\boldsymbol{Q}}_{2 y}-\\widehat{\\boldsymbol{Q}}_{21} \\widehat{\\boldsymbol{Q}}_{11}^{-1} \\widehat{\\boldsymbol{Q}}_{1 Y}\\right) \\\\\n& =\\widehat{\\boldsymbol{Q}}_{11 \\cdot 2}^{-1}\\left(\\widehat{\\boldsymbol{Q}}_{1 Y}-\\widehat{\\boldsymbol{Q}}_{12} \\widehat{\\boldsymbol{Q}}_{22}^{-1} \\widehat{\\mathbf{Q}}_{21} \\widehat{\\boldsymbol{Q}}_{11}^{-1} \\widehat{\\boldsymbol{Q}}_{1 Y}\\right) \\\\\n& =\\widehat{\\boldsymbol{Q}}_{11 \\cdot 2}^{-1}\\left(\\widehat{\\boldsymbol{Q}}_{11}-\\widehat{\\boldsymbol{Q}}_{12} \\widehat{\\boldsymbol{Q}}_{22}^{-1} \\widehat{\\boldsymbol{Q}}_{21}\\right) \\widehat{\\boldsymbol{Q}}_{11}^{-1} \\widehat{\\boldsymbol{Q}}_{1 Y} \\\\\n& =\\widehat{\\boldsymbol{Q}}_{11}^{-1} \\widehat{\\boldsymbol{Q}}_{1 Y}\n\\end{aligned}\n\\]\nwhich is (8.13) as originally claimed."
  },
  {
    "objectID": "chpt08-restricted-est.html#finite-sample-properties",
    "href": "chpt08-restricted-est.html#finite-sample-properties",
    "title": "8  Restricted Estimation",
    "section": "8.4 Finite Sample Properties",
    "text": "8.4 Finite Sample Properties\nIn this section we explore some of the properties of the CLS estimator in the linear regression model\n\\[\n\\begin{aligned}\nY &=X^{\\prime} \\beta+e \\\\\n\\mathbb{E}[e \\mid X] &=0 .\n\\end{aligned}\n\\]\nFirst, it is useful to write the estimator and the residuals as linear functions of the error vector. These are algebraic relationships and do not rely on the linear regression assumptions. Theorem 8.1 The CLS estimator satisfies\n\n\\(\\boldsymbol{R}^{\\prime} \\widehat{\\beta}-\\boldsymbol{c}=\\boldsymbol{R}^{\\prime}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\boldsymbol{X}^{\\prime} \\boldsymbol{e}\\)\n\\(\\widetilde{\\beta}_{\\mathrm{cls}}-\\beta=\\left(\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\boldsymbol{X}^{\\prime}-\\boldsymbol{A} \\boldsymbol{X}^{\\prime}\\right) \\boldsymbol{e}\\)\n\\(\\widetilde{\\boldsymbol{e}}=\\left(\\boldsymbol{I}-\\boldsymbol{P}+\\boldsymbol{X} \\boldsymbol{A} \\boldsymbol{X}^{\\prime}\\right) \\boldsymbol{e}\\)\n\\(\\boldsymbol{I}_{n}-\\boldsymbol{P}+\\boldsymbol{X} \\boldsymbol{A} \\boldsymbol{X}^{\\prime}\\) is symmetric and idempotent\n\\(\\operatorname{tr}\\left(\\boldsymbol{I}_{n}-\\boldsymbol{P}+\\boldsymbol{X} \\boldsymbol{A} \\boldsymbol{X}^{\\prime}\\right)=n-k+q\\)\n\nwhere \\(\\boldsymbol{P}=\\boldsymbol{X}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\boldsymbol{X}^{\\prime}\\) and \\(\\boldsymbol{A}=\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\boldsymbol{R}\\left(\\boldsymbol{R}^{\\prime}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\boldsymbol{R}\\right)^{-1} \\boldsymbol{R}^{\\prime}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1}\\)\nFor a proof see Exercise 8.6.\nGiven the linearity of Theorem 8.1.2 it is not hard to show that the CLS estimator is unbiased for \\(\\beta\\).\nTheorem 8.2 In the linear regression model (8.14)-(8.15) under (8.1), \\(\\mathbb{E}\\left[\\widetilde{\\beta}_{\\mathrm{cls}} \\mid \\boldsymbol{X}\\right]=\\beta\\)\nFor a proof see Exercise 8.7.\nWe can also calculate the covariance matrix of \\(\\widetilde{\\beta}_{\\text {cls }}\\). First, for simplicity take the case of conditional homoskedasticity.\nTheorem 8.3 In the homoskedastic linear regression model (8.14)-(8.15) with \\(\\mathbb{E}\\left[e^{2} \\mid X\\right]=\\sigma^{2}\\), under (8.1),\n\\[\n\\begin{aligned}\n\\boldsymbol{V}_{\\widetilde{\\beta}}^{0} &=\\operatorname{var}\\left[\\widetilde{\\beta}_{\\mathrm{cls}} \\mid \\boldsymbol{X}\\right] \\\\\n&=\\left(\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1}-\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\boldsymbol{R}\\left(\\boldsymbol{R}^{\\prime}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\boldsymbol{R}\\right)^{-1} \\boldsymbol{R}^{\\prime}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1}\\right) \\sigma^{2}\n\\end{aligned}\n\\]\nFor a proof see Exercise 8.8.\nWe use the \\(\\boldsymbol{V}_{\\tilde{\\beta}}^{0}\\) notation to emphasize that this is the covariance matrix under the assumption of conditional homoskedasticity.\nFor inference we need an estimate of \\(\\boldsymbol{V}_{\\widetilde{\\beta}}^{0}\\). A natural estimator is\n\\[\n\\widehat{\\boldsymbol{V}}_{\\widetilde{\\beta}}^{0}=\\left(\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1}-\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\boldsymbol{R}\\left(\\boldsymbol{R}^{\\prime}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\boldsymbol{R}\\right)^{-1} \\boldsymbol{R}^{\\prime}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1}\\right) s_{\\mathrm{cls}}^{2}\n\\]\nwhere\n\\[\ns_{\\mathrm{cls}}^{2}=\\frac{1}{n-k+q} \\sum_{i=1}^{n} \\widetilde{e}_{i}^{2}\n\\]\nis a biased-corrected estimator of \\(\\sigma^{2}\\). Standard errors for the components of \\(\\beta\\) are then found by taking the squares roots of the diagonal elements of \\(\\widehat{\\boldsymbol{V}}_{\\widetilde{\\beta}}\\), for example\n\\[\ns\\left(\\widehat{\\beta}_{j}\\right)=\\sqrt{\\left[\\widehat{\\boldsymbol{V}}_{\\widetilde{\\beta}}^{0}\\right]_{j j}} .\n\\]\nThe estimator (8.16) has the property that it is unbiased for \\(\\sigma^{2}\\) under conditional homoskedasticity. To see this, using the properties of Theorem 8.1,\n\\[\n\\begin{aligned}\n(n-k+q) s_{\\mathrm{cls}}^{2} &=\\widetilde{\\boldsymbol{e}}^{\\prime} \\widetilde{\\boldsymbol{e}} \\\\\n&=\\boldsymbol{e}^{\\prime}\\left(\\boldsymbol{I}_{n}-\\boldsymbol{P}+\\boldsymbol{X} \\boldsymbol{A} \\boldsymbol{X}^{\\prime}\\right)\\left(\\boldsymbol{I}_{n}-\\boldsymbol{P}+\\boldsymbol{X} \\boldsymbol{A} \\boldsymbol{X}^{\\prime}\\right) \\boldsymbol{e} \\\\\n&=\\boldsymbol{e}^{\\prime}\\left(\\boldsymbol{I}_{n}-\\boldsymbol{P}+\\boldsymbol{X} \\boldsymbol{A} \\boldsymbol{X}^{\\prime}\\right) \\boldsymbol{e} .\n\\end{aligned}\n\\]\nWe defer the remainder of the proof to Exercise 8.9.\nTheorem 8.4 In the homoskedastic linear regression model (8.14)-(8.15) with \\(\\mathbb{E}\\left[e^{2} \\mid X\\right]=\\sigma^{2}\\), under (8.1), \\(\\mathbb{E}\\left[s_{\\text {cls }}^{2} \\mid \\boldsymbol{X}\\right]=\\sigma^{2}\\) and \\(\\mathbb{E}\\left[\\widehat{\\boldsymbol{V}}_{\\widetilde{\\beta}}^{0} \\mid \\boldsymbol{X}\\right]=\\boldsymbol{V}_{\\widetilde{\\beta}}^{0} .\\)\nNow consider the distributional properties in the normal regression model \\(Y=X^{\\prime} \\beta+e\\) with \\(e \\sim\\) \\(\\mathrm{N}\\left(0, \\sigma^{2}\\right)\\). By the linearity of Theorem 8.1.2, conditional on \\(\\boldsymbol{X}, \\widetilde{\\beta}_{\\text {cls }}-\\beta\\) is normal. Given Theorems \\(8.2\\) and \\(8.3\\) we deduce that \\(\\widetilde{\\beta}_{\\mathrm{cls}} \\sim \\mathrm{N}\\left(\\beta, \\boldsymbol{V}_{\\widetilde{\\beta}}^{0}\\right)\\).\nSimilarly, from Exericise \\(8.1\\) we know \\(\\widetilde{\\boldsymbol{e}}=\\left(\\boldsymbol{I}_{n}-\\boldsymbol{P}+\\boldsymbol{X} \\boldsymbol{A} \\boldsymbol{X}^{\\prime}\\right) \\boldsymbol{e}\\) is linear in \\(\\boldsymbol{e}\\) so is also conditionally normal. Furthermore, since \\(\\left(\\boldsymbol{I}_{n}-\\boldsymbol{P}+\\boldsymbol{X} \\boldsymbol{A} \\boldsymbol{X}^{\\prime}\\right)\\left(\\boldsymbol{X}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1}-\\boldsymbol{X} \\boldsymbol{A}\\right)=0, \\widetilde{\\boldsymbol{e}}\\) and \\(\\widetilde{\\beta}_{\\text {cls }}\\) are uncorrelated and thus independent. Thus \\(s_{\\text {cls }}^{2}\\) and \\(\\widetilde{\\beta}_{\\text {cls }}\\) are independent.\nFrom (8.17) and the fact that \\(\\boldsymbol{I}_{n}-\\boldsymbol{P}+\\boldsymbol{X} \\boldsymbol{A} \\boldsymbol{X}^{\\prime}\\) is idempotent with rank \\(n-k+q\\) it follows that\n\\[\ns_{\\text {cls }}^{2} \\sim \\sigma^{2} \\chi_{n-k+q}^{2} /(n-k+q) .\n\\]\nIt follows that the \\(\\mathrm{t}\\)-statistic has the exact distribution\n\\[\nT=\\frac{\\widehat{\\beta}_{j}-\\beta_{j}}{s\\left(\\widehat{\\beta}_{j}\\right)} \\sim \\frac{\\mathrm{N}(0,1)}{\\sqrt{\\chi_{n-k+q}^{2} /(n-k+q)}} \\sim t_{n-k+q}\n\\]\na student \\(t\\) distribution with \\(n-k+q\\) degrees of freedom.\nThe relevance of this calculation is that the “degrees of freedom” for CLS regression equal \\(n-k+q\\) rather than \\(n-k\\) as in OLS. Essentially the model has \\(k-q\\) free parameters instead of \\(k\\). Another way of thinking about this is that estimation of a model with \\(k\\) coefficients and \\(q\\) restrictions is equivalent to estimation with \\(k-q\\) coefficients.\nWe summarize the properties of the normal regression model. Theorem 8.5 In the normal linear regression model (8.14)-(8.15) with constraint (8.1),\n\\[\n\\begin{aligned}\n\\widetilde{\\beta}_{\\mathrm{cls}} \\sim \\mathrm{N}\\left(\\beta, \\boldsymbol{V}_{\\widetilde{\\beta}}^{0}\\right) \\\\\n\\frac{(n-k+q) s_{\\mathrm{cls}}^{2}}{\\sigma^{2}} \\sim \\chi_{n-k+q}^{2} \\\\\nT & \\sim t_{n-k+q} .\n\\end{aligned}\n\\]\nAn interesting relationship is that in the homoskedastic regression model\n\\[\n\\begin{aligned}\n\\operatorname{cov}\\left(\\widehat{\\beta}_{\\mathrm{ols}}-\\widetilde{\\beta}_{\\mathrm{cls}}, \\widetilde{\\beta}_{\\mathrm{cls}} \\mid \\boldsymbol{X}\\right) &=\\mathbb{E}\\left[\\left(\\widehat{\\beta}_{\\mathrm{ols}}-\\widetilde{\\beta}_{\\mathrm{cls}}\\right)\\left(\\widetilde{\\beta}_{\\mathrm{cls}}-\\beta\\right)^{\\prime} \\mid \\boldsymbol{X}\\right] \\\\\n&=\\mathbb{E}\\left[\\boldsymbol{A} \\boldsymbol{X}^{\\prime} \\boldsymbol{e} \\boldsymbol{e}^{\\prime}\\left(\\boldsymbol{X}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1}-\\boldsymbol{X} \\boldsymbol{A}\\right) \\mid \\boldsymbol{X}\\right] \\\\\n&=\\boldsymbol{A} \\boldsymbol{X}^{\\prime}\\left(\\boldsymbol{X}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1}-\\boldsymbol{X} \\boldsymbol{A}\\right) \\sigma^{2}=0 .\n\\end{aligned}\n\\]\nThis means that \\(\\widehat{\\beta}_{\\text {ols }}-\\widetilde{\\beta}_{\\text {cls }}\\) and \\(\\widetilde{\\beta}_{\\text {cls }}\\) are conditionally uncorrelated and hence independent. A corollary is\n\\[\n\\operatorname{cov}\\left(\\widehat{\\beta}_{\\text {ols }}, \\widetilde{\\beta}_{\\text {cls }} \\mid \\boldsymbol{X}\\right)=\\operatorname{var}\\left[\\widetilde{\\beta}_{\\text {cls }} \\mid \\boldsymbol{X}\\right] .\n\\]\nA second corollary is\n\\[\n\\begin{aligned}\n\\operatorname{var}\\left[\\widehat{\\beta}_{\\mathrm{ols}}-\\widetilde{\\beta}_{\\mathrm{cls}} \\mid \\boldsymbol{X}\\right] &=\\operatorname{var}\\left[\\widehat{\\beta}_{\\mathrm{ols}} \\mid \\boldsymbol{X}\\right]-\\operatorname{var}\\left[\\widetilde{\\beta}_{\\mathrm{cls}} \\mid \\boldsymbol{X}\\right] \\\\\n&=\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\boldsymbol{R}\\left(\\boldsymbol{R}^{\\prime}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\boldsymbol{R}\\right)^{-1} \\boldsymbol{R}^{\\prime}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\sigma^{2}\n\\end{aligned}\n\\]\nThis also shows that the difference between the CLS and OLS variances matrices equals\n\\[\n\\operatorname{var}\\left[\\widehat{\\beta}_{\\text {ols }} \\mid \\boldsymbol{X}\\right]-\\operatorname{var}\\left[\\widetilde{\\beta}_{\\mathrm{cls}} \\mid \\boldsymbol{X}\\right]=\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\boldsymbol{R}\\left(\\boldsymbol{R}^{\\prime}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\boldsymbol{R}\\right)^{-1} \\boldsymbol{R}^{\\prime}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\sigma^{2} \\geq 0\n\\]\nthe final equality meaning positive semi-definite. It follows that \\(\\operatorname{var}\\left[\\widehat{\\beta}_{\\text {ols }} \\mid \\boldsymbol{X}\\right] \\geq \\operatorname{var}\\left[\\widetilde{\\beta}_{\\text {cls }} \\mid \\boldsymbol{X}\\right]\\) in the positive definite sense, and thus CLS is more efficient than OLS. Both estimators are unbiased (in the linear regression model) and CLS has a lower covariance matrix (in the linear homoskedastic regression model).\nThe relationship (8.18) is rather interesting and will appear again. The expression says that the variance of the difference between the estimators is equal to the difference between the variances. This is rather special. It occurs generically when we are comparing an efficient and an inefficient estimator. We call (8.18) the Hausman Equality as it was first pointed out in econometrics by Hausman (1978)."
  },
  {
    "objectID": "chpt08-restricted-est.html#minimum-distance",
    "href": "chpt08-restricted-est.html#minimum-distance",
    "title": "8  Restricted Estimation",
    "section": "8.5 Minimum Distance",
    "text": "8.5 Minimum Distance\nThe previous section explored the finite sample distribution theory under the assumptions of the linear regression model, homoskedastic regression model, and normal regression model. We now return to the general projection model where we do not impose linearity, homoskedasticity, nor normality. We are interested in the question: Can we do better than CLS in this setting?\nA minimum distance estimator tries to find a parameter value satisfying the constraint which is as close as possible to the unconstrained estimator. Let \\(\\widehat{\\beta}\\) be the unconstrained least squares estimator, and for some \\(k \\times k\\) positive definite weight matrix \\(\\widehat{W}\\) define the quadratic criterion function\n\\[\nJ(\\beta)=n(\\widehat{\\beta}-\\beta)^{\\prime} \\widehat{\\boldsymbol{W}}(\\widehat{\\beta}-\\beta) .\n\\]\nThis is a (squared) weighted Euclidean distance between \\(\\widehat{\\beta}\\) and \\(\\beta . J(\\beta)\\) is small if \\(\\beta\\) is close to \\(\\widehat{\\beta}\\), and is minimized at zero only if \\(\\beta=\\widehat{\\beta}\\). A minimum distance estimator \\(\\widetilde{\\beta}_{\\mathrm{md}}\\) for \\(\\beta\\) minimizes \\(J(\\beta)\\) subject to the constraint (8.1), that is,\n\\[\n\\widetilde{\\beta}_{\\mathrm{md}}=\\underset{\\boldsymbol{R}^{\\prime} \\beta=\\boldsymbol{c}}{\\operatorname{argmin}} J(\\beta) .\n\\]\nThe CLS estimator is the special case when \\(\\widehat{\\boldsymbol{W}}=\\widehat{\\boldsymbol{Q}}_{X X}\\) and we write this criterion function as\n\\[\nJ^{0}(\\beta)=n(\\widehat{\\beta}-\\beta)^{\\prime} \\widehat{\\boldsymbol{Q}}_{X X}(\\widehat{\\beta}-\\beta) .\n\\]\nTo see the equality of CLS and minimum distance rewrite the least squares criterion as follows. Substitute the unconstrained least squares fitted equation \\(Y_{i}=X_{i}^{\\prime} \\widehat{\\beta}+\\widehat{e}_{i}\\) into \\(\\operatorname{SSE}(\\beta)\\) to obtain\n\\[\n\\begin{aligned}\n\\operatorname{SSE}(\\beta) &=\\sum_{i=1}^{n}\\left(Y_{i}-X_{i}^{\\prime} \\beta\\right)^{2} \\\\\n&=\\sum_{i=1}^{n}\\left(X_{i}^{\\prime} \\widehat{\\beta}+\\widehat{e}_{i}-X_{i}^{\\prime} \\beta\\right)^{2} \\\\\n&=\\sum_{i=1}^{n} \\widehat{e}_{i}^{2}+(\\widehat{\\beta}-\\beta)^{\\prime}\\left(\\sum_{i=1}^{n} X_{i} X_{i}^{\\prime}\\right)(\\widehat{\\beta}-\\beta) \\\\\n&=n \\widehat{\\sigma}^{2}+J^{0}(\\beta)\n\\end{aligned}\n\\]\nwhere the third equality uses the fact that \\(\\sum_{i=1}^{n} X_{i} \\widehat{e}_{i}=0\\), and the last line uses \\(\\sum_{i=1}^{n} X_{i} X_{i}^{\\prime}=n \\widehat{\\mathbf{Q}}_{X X}\\). The expression (8.21) only depends on \\(\\beta\\) through \\(J^{0}(\\beta)\\). Thus minimization of \\(\\operatorname{SSE}(\\beta)\\) and \\(J^{0}(\\beta)\\) are equivalent, and hence \\(\\widetilde{\\beta}_{\\mathrm{md}}=\\widetilde{\\widetilde{\\beta}}_{\\text {cls }}\\) when \\(\\widehat{\\boldsymbol{W}}=\\widehat{\\boldsymbol{Q}}_{X X}\\).\nWe can solve for \\(\\widetilde{\\beta}_{\\mathrm{md}}\\) explicitly by the method of Lagrange multipliers. The Lagrangian is\n\\[\n\\mathscr{L}(\\beta, \\lambda)=\\frac{1}{2} J(\\beta, \\widehat{\\boldsymbol{W}})+\\lambda^{\\prime}\\left(\\boldsymbol{R}^{\\prime} \\beta-\\boldsymbol{c}\\right) .\n\\]\nThe solution to the pair of first order conditions is\n\\[\n\\begin{aligned}\n&\\widetilde{\\lambda}_{\\mathrm{md}}=n\\left(\\boldsymbol{R}^{\\prime} \\widehat{\\boldsymbol{W}}^{-1} \\boldsymbol{R}\\right)^{-1}\\left(\\boldsymbol{R}^{\\prime} \\widehat{\\boldsymbol{\\beta}}-\\boldsymbol{c}\\right) \\\\\n&\\widetilde{\\beta}_{\\mathrm{md}}=\\widehat{\\boldsymbol{\\beta}}-\\widehat{\\boldsymbol{W}}^{-1} \\boldsymbol{R}\\left(\\boldsymbol{R}^{\\prime} \\widehat{\\boldsymbol{W}}^{-1} \\boldsymbol{R}\\right)^{-1}\\left(\\boldsymbol{R}^{\\prime} \\widehat{\\beta}-\\boldsymbol{c}\\right) .\n\\end{aligned}\n\\]\n(See Exercise 8.10.) Comparing (8.23) with (8.9) we can see that \\(\\widetilde{\\beta}_{\\mathrm{md}}\\) specializes to \\(\\widetilde{\\beta}_{\\text {cls }}\\) when we set \\(\\widehat{\\boldsymbol{W}}=\\widehat{\\boldsymbol{Q}}_{X X}\\)\nAn obvious question is which weight matrix \\(\\widehat{\\boldsymbol{W}}\\) is best. We will address this question after we derive the asymptotic distribution for a general weight matrix."
  },
  {
    "objectID": "chpt08-restricted-est.html#asymptotic-distribution",
    "href": "chpt08-restricted-est.html#asymptotic-distribution",
    "title": "8  Restricted Estimation",
    "section": "8.6 Asymptotic Distribution",
    "text": "8.6 Asymptotic Distribution\nWe first show that the class of minimum distance estimators are consistent for the population parameters when the constraints are valid.\nAssumption 8.1 \\(\\boldsymbol{R}^{\\prime} \\beta=\\boldsymbol{c}\\) where \\(\\boldsymbol{R}\\) is \\(k \\times q\\) with \\(\\operatorname{rank}(\\boldsymbol{R})=q\\). Assumption 8.2 \\(\\widehat{W} \\underset{p}{\\longrightarrow} W>0\\).\nTheorem 8.6 Consistency Under Assumptions 7.1, 8.1, and 8.2, \\(\\widetilde{\\beta}_{\\mathrm{md}} \\underset{p}{\\longrightarrow} \\beta\\) as \\(n \\rightarrow \\infty\\).\nFor a proof see Exercise 8.11.\nTheorem \\(8.6\\) shows that consistency holds for any weight matrix with a positive definite limit so includes the CLS estimator.\nSimilarly, the constrained estimators are asymptotically normally distributed.\nTheorem 8.7 Asymptotic Normality Under Assumptions 7.2, 8.1, and 8.2,\n\\[\n\\sqrt{n}\\left(\\widetilde{\\beta}_{\\mathrm{md}}-\\beta\\right) \\underset{d}{\\longrightarrow} \\mathrm{N}\\left(0, \\boldsymbol{V}_{\\beta}(\\boldsymbol{W})\\right)\n\\]\nas \\(n \\rightarrow \\infty\\), where\n\\[\n\\begin{gathered}\n\\boldsymbol{V}_{\\beta}(\\boldsymbol{W})=\\boldsymbol{V}_{\\beta}-\\boldsymbol{W}^{-1} \\boldsymbol{R}\\left(\\boldsymbol{R}^{\\prime} \\boldsymbol{W}^{-1} \\boldsymbol{R}\\right)^{-1} \\boldsymbol{R}^{\\prime} \\boldsymbol{V}_{\\beta} \\\\\n-\\boldsymbol{V}_{\\beta} \\boldsymbol{R}\\left(\\boldsymbol{R}^{\\prime} \\boldsymbol{W}^{-1} \\boldsymbol{R}\\right)^{-1} \\boldsymbol{R}^{\\prime} \\boldsymbol{W}^{-1} \\\\\n+\\boldsymbol{W}^{-1} \\boldsymbol{R}\\left(\\boldsymbol{R}^{\\prime} \\boldsymbol{W}^{-1} \\boldsymbol{R}\\right)^{-1} \\boldsymbol{R}^{\\prime} \\boldsymbol{V}_{\\beta} \\boldsymbol{R}\\left(\\boldsymbol{R}^{\\prime} \\boldsymbol{W}^{-1} \\boldsymbol{R}\\right)^{-1} \\boldsymbol{R}^{\\prime} \\boldsymbol{W}^{-1}\n\\end{gathered}\n\\]\nand \\(\\boldsymbol{V}_{\\beta}=\\boldsymbol{Q}_{X X}^{-1} \\Omega \\boldsymbol{Q}_{X X}^{-1}\\)\nFor a proof see Exercise 8.12.\nTheorem \\(8.7\\) shows that the minimum distance estimator is asymptotically normal for all positive definite weight matrices. The asymptotic variance depends on \\(\\boldsymbol{W}\\). The theorem includes the CLS estimator as a special case by setting \\(\\boldsymbol{W}=\\boldsymbol{Q}_{X X}\\).\nTheorem 8.8 Asymptotic Distribution of CLS Estimator Under Assumptions \\(7.2\\) and 8.1, as \\(n \\rightarrow \\infty\\)\n\\[\n\\sqrt{n}\\left(\\widetilde{\\beta}_{\\mathrm{cls}}-\\beta\\right) \\underset{d}{\\longrightarrow} \\mathrm{N}\\left(0, \\boldsymbol{V}_{\\mathrm{cls}}\\right)\n\\]\nwhere\n\\[\n\\begin{aligned}\n\\boldsymbol{V}_{\\mathrm{cls}} &=\\boldsymbol{V}_{\\beta}-\\boldsymbol{Q}_{X X}^{-1} \\boldsymbol{R}\\left(\\boldsymbol{R}^{\\prime} \\boldsymbol{Q}_{X X}^{-1} \\boldsymbol{R}\\right)^{-1} \\boldsymbol{R}^{\\prime} \\boldsymbol{V}_{\\beta} \\\\\n&-\\boldsymbol{V}_{\\beta} \\boldsymbol{R}\\left(\\boldsymbol{R}^{\\prime} \\boldsymbol{Q}_{X X}^{-1} \\boldsymbol{R}\\right)^{-1} \\boldsymbol{R}^{\\prime} \\boldsymbol{Q}_{X X}^{-1} \\\\\n&+\\boldsymbol{Q}_{X X}^{-1} \\boldsymbol{R}\\left(\\boldsymbol{R}^{\\prime} \\boldsymbol{Q}_{X X}^{-1} \\boldsymbol{R}\\right)^{-1} \\boldsymbol{R}^{\\prime} \\boldsymbol{V}_{\\beta} \\boldsymbol{R}\\left(\\boldsymbol{R}^{\\prime} \\boldsymbol{Q}_{X X}^{-1} \\boldsymbol{R}\\right)^{-1} \\boldsymbol{R}^{\\prime} \\boldsymbol{Q}_{X X}^{-1}\n\\end{aligned}\n\\]\nFor a proof see Exercise 8.13."
  },
  {
    "objectID": "chpt08-restricted-est.html#variance-estimation-and-standard-errors",
    "href": "chpt08-restricted-est.html#variance-estimation-and-standard-errors",
    "title": "8  Restricted Estimation",
    "section": "8.7 Variance Estimation and Standard Errors",
    "text": "8.7 Variance Estimation and Standard Errors\nEarlier we introduced the covariance matrix estimator under the assumption of conditional homoskedasticity. We now introduce an estimator which does not impose homoskedasticity.\nThe asymptotic covariance matrix \\(\\boldsymbol{V}_{\\text {cls }}\\) may be estimated by replacing \\(\\boldsymbol{V}_{\\beta}\\) with a consistent estimator such as \\(\\widehat{\\boldsymbol{V}}_{\\beta}\\). A more efficient estimator can be obtained by using the restricted coefficient estimator which we now show. Given the constrained least squares squares residuals \\(\\widetilde{e}_{i}=Y_{i}-X_{i}^{\\prime} \\widetilde{\\beta}_{\\text {cls }}\\) we can estimate the matrix \\(\\Omega=\\mathbb{E}\\left[X X^{\\prime} e^{2}\\right]\\) by\n\\[\n\\widetilde{\\Omega}=\\frac{1}{n-k+q} \\sum_{i=1}^{n} X_{i} X_{i}^{\\prime} \\widetilde{e}_{i}^{2} .\n\\]\nNotice that we have used an adjusted degrees of freedom. This is an \\(a d\\) hoc adjustment designed to mimic that used for estimation of the error variance \\(\\sigma^{2}\\). The moment estimator of \\(\\boldsymbol{V}_{\\beta}\\) is\n\\[\n\\widetilde{\\boldsymbol{V}}_{\\beta}=\\widehat{\\boldsymbol{Q}}_{X X}^{-1} \\widetilde{\\Omega} \\widehat{\\boldsymbol{Q}}_{X X}^{-1}\n\\]\nand that for \\(\\boldsymbol{V}_{\\mathrm{cls}}\\) is\n\\[\n\\begin{aligned}\n\\widetilde{\\boldsymbol{V}}_{\\mathrm{cls}} &=\\widetilde{\\boldsymbol{V}}_{\\beta}-\\widehat{\\boldsymbol{Q}}_{X X}^{-1} \\boldsymbol{R}\\left(\\boldsymbol{R}^{\\prime} \\widehat{\\boldsymbol{Q}}_{X X}^{-1} \\boldsymbol{R}\\right)^{-1} \\boldsymbol{R}^{\\prime} \\widetilde{\\boldsymbol{V}}_{\\beta} \\\\\n&-\\widetilde{\\boldsymbol{V}}_{\\beta} \\boldsymbol{R}\\left(\\boldsymbol{R}^{\\prime} \\widehat{\\boldsymbol{Q}}_{X X}^{-1} \\boldsymbol{R}\\right)^{-1} \\boldsymbol{R}^{\\prime} \\widehat{\\boldsymbol{Q}}_{\\boldsymbol{x x}}^{-1} \\\\\n&+\\widehat{\\boldsymbol{Q}}_{X X}^{-1} \\boldsymbol{R}\\left(\\boldsymbol{R}^{\\prime} \\widehat{\\boldsymbol{Q}}_{X X}^{-1} \\boldsymbol{R}\\right)^{-1} \\boldsymbol{R}^{\\prime} \\widetilde{\\boldsymbol{V}}_{\\beta} \\boldsymbol{R}\\left(\\boldsymbol{R}^{\\prime} \\widehat{\\boldsymbol{Q}}_{X X}^{-1} \\boldsymbol{R}\\right)^{-1} \\boldsymbol{R}^{\\prime} \\widehat{\\boldsymbol{Q}}_{X X}^{-1}\n\\end{aligned}\n\\]\nWe can calculate standard errors for any linear combination \\(h^{\\prime} \\widetilde{\\beta}_{\\text {cls }}\\) such that \\(h\\) does not lie in the range space of \\(\\boldsymbol{R}\\). A standard error for \\(h^{\\prime} \\widetilde{\\beta}\\) is\n\\[\ns\\left(h^{\\prime} \\widetilde{\\boldsymbol{\\beta}}_{\\mathrm{cls}}\\right)=\\left(n^{-1} h^{\\prime} \\tilde{\\boldsymbol{V}}_{\\mathrm{cls}} h\\right)^{1 / 2} .\n\\]"
  },
  {
    "objectID": "chpt08-restricted-est.html#efficient-minimum-distance-estimator",
    "href": "chpt08-restricted-est.html#efficient-minimum-distance-estimator",
    "title": "8  Restricted Estimation",
    "section": "8.8 Efficient Minimum Distance Estimator",
    "text": "8.8 Efficient Minimum Distance Estimator\nTheorem \\(8.7\\) shows that minimum distance estimators, which include CLS as a special case, are asymptotically normal with an asymptotic covariance matrix which depends on the weight matrix \\(\\boldsymbol{W}\\). The asymptotically optimal weight matrix is the one which minimizes the asymptotic variance \\(\\boldsymbol{V}_{\\beta}(\\boldsymbol{W})\\). This turns out to be \\(\\boldsymbol{W}=\\boldsymbol{V}_{\\beta}^{-1}\\) as is shown in Theorem \\(8.9\\) below. Since \\(\\boldsymbol{V}_{\\beta}^{-1}\\) is unknown this weight matrix cannot be used for a feasible estimator but we can replace \\(\\boldsymbol{V}_{\\beta}^{-1}\\) with a consistent estimator \\(\\widehat{\\boldsymbol{V}}_{\\beta}^{-1}\\) and the asymptotic distribution (and efficiency) are unchanged. We call the minimum distance estimator with \\(\\widehat{\\boldsymbol{W}}=\\widehat{\\boldsymbol{V}}_{\\beta}^{-1}\\) the efficient minimum distance estimator and takes the form\n\\[\n\\widetilde{\\beta}_{\\text {emd }}=\\widehat{\\beta}-\\widehat{\\boldsymbol{V}}_{\\beta} \\boldsymbol{R}\\left(\\boldsymbol{R}^{\\prime} \\widehat{\\boldsymbol{V}}_{\\beta} \\boldsymbol{R}\\right)^{-1}\\left(\\boldsymbol{R}^{\\prime} \\widehat{\\beta}-\\boldsymbol{c}\\right) .\n\\]\nThe asymptotic distribution of (8.25) can be deduced from Theorem 8.7. (See Exercises \\(8.14\\) and 8.15, and the proof in Section 8.16.)\nTheorem 8.9 Efficient Minimum Distance Estimator Under Assumptions \\(7.2\\) and 8.1,\n\\[\n\\sqrt{n}\\left(\\widetilde{\\beta}_{\\mathrm{emd}}-\\beta\\right) \\underset{d}{\\longrightarrow} \\mathrm{N}\\left(0, \\boldsymbol{V}_{\\beta, \\mathrm{emd}}\\right)\n\\]\nas \\(n \\rightarrow \\infty\\), where\n\\[\n\\boldsymbol{V}_{\\beta, \\mathrm{emd}}=\\boldsymbol{V}_{\\beta}-\\boldsymbol{V}_{\\beta} \\boldsymbol{R}\\left(\\boldsymbol{R}^{\\prime} \\boldsymbol{V}_{\\beta} \\boldsymbol{R}\\right)^{-1} \\boldsymbol{R}^{\\prime} \\boldsymbol{V}_{\\beta}\n\\]\nSince\n\\[\n\\boldsymbol{V}_{\\beta, \\mathrm{emd}} \\leq \\boldsymbol{V}_{\\beta}\n\\]\nthe estimator (8.25) has lower asymptotic variance than the unrestricted estimator. Furthermore, for any \\(\\boldsymbol{W}\\),\n\\[\n\\boldsymbol{V}_{\\beta, \\mathrm{emd}} \\leq \\boldsymbol{V}_{\\beta}(\\boldsymbol{W})\n\\]\nso (8.25) is asymptotically efficient in the class of minimum distance estimators.\nTheorem \\(8.9\\) shows that the minimum distance estimator with the smallest asymptotic variance is (8.25). One implication is that the constrained least squares estimator is generally inefficient. The interesting exception is the case of conditional homoskedasticity in which case the optimal weight matrix is \\(\\boldsymbol{W}=\\left(\\boldsymbol{V}_{\\beta}^{0}\\right)^{-1}\\) so in this case CLS is an efficient minimum distance estimator. Otherwise when the error is conditionally heteroskedastic there are asymptotic efficiency gains by using minimum distance rather than least squares.\nThe fact that CLS is generally inefficient is counter-intuitive and requires some reflection. Standard intuition suggests to apply the same estimation method (least squares) to the unconstrained and constrained models and this is the common empirical practice. But Theorem \\(8.9\\) shows that this is inefficient. Why? The reason is that the least squares estimator does not make use of the regressor \\(X_{2}\\). It ignores the information \\(\\mathbb{E}\\left[X_{2} e\\right]=0\\). This information is relevant when the error is heteroskedastic and the excluded regressors are correlated with the included regressors.\nInequality (8.27) shows that the efficient minimum distance estimator \\(\\widetilde{\\beta}_{\\text {emd }}\\) has a smaller asymptotic variance than the unrestricted least squares estimator \\(\\widehat{\\beta}\\). This means that efficient estimation is attained by imposing correct restrictions when we use the minimum distance method."
  },
  {
    "objectID": "chpt08-restricted-est.html#exclusion-restriction-revisited",
    "href": "chpt08-restricted-est.html#exclusion-restriction-revisited",
    "title": "8  Restricted Estimation",
    "section": "8.9 Exclusion Restriction Revisited",
    "text": "8.9 Exclusion Restriction Revisited\nWe return to the example of estimation with a simple exclusion restriction. The model is\n\\[\nY=X_{1}^{\\prime} \\beta_{1}+X_{2}^{\\prime} \\beta_{2}+e\n\\]\nwith the exclusion restriction \\(\\beta_{2}=0\\). We have introduced three estimators of \\(\\beta_{1}\\). The first is unconstrained least squares applied to (8.10) which can be written as \\(\\widehat{\\beta}_{1}=\\widehat{\\boldsymbol{Q}}_{11 \\cdot 2}^{-1} \\widehat{\\boldsymbol{Q}}_{1 Y \\cdot 2}\\). From Theorem \\(7.25\\) and equation (7.14) its asymptotic variance is\n\\[\n\\operatorname{avar}\\left[\\widehat{\\beta}_{1}\\right]=\\boldsymbol{Q}_{11 \\cdot 2}^{-1}\\left(\\Omega_{11}-\\boldsymbol{Q}_{12} \\boldsymbol{Q}_{22}^{-1} \\Omega_{21}-\\Omega_{12} \\boldsymbol{Q}_{22}^{-1} \\boldsymbol{Q}_{21}+\\boldsymbol{Q}_{12} \\boldsymbol{Q}_{22}^{-1} \\Omega_{22} \\boldsymbol{Q}_{22}^{-1} \\boldsymbol{Q}_{21}\\right) \\boldsymbol{Q}_{11 \\cdot 2}^{-1}\n\\]\nThe second estimator of \\(\\beta_{1}\\) is CLS, which can be written as \\(\\widetilde{\\beta}_{1}=\\widehat{\\boldsymbol{Q}}_{11}^{-1} \\widehat{\\boldsymbol{Q}}_{1 Y}\\). Its asymptotic variance can be deduced from Theorem 8.8, but it is simpler to apply the CLT directly to show that\n\\[\n\\operatorname{avar}\\left[\\widetilde{\\beta}_{1}\\right]=\\boldsymbol{Q}_{11}^{-1} \\Omega_{11} \\boldsymbol{Q}_{11}^{-1} .\n\\]\nThe third estimator of \\(\\beta_{1}\\) is efficient minimum distance. Applying (8.25), it equals\n\\[\n\\bar{\\beta}_{1}=\\widehat{\\beta}_{1}-\\widehat{\\boldsymbol{V}}_{12} \\widehat{\\boldsymbol{V}}_{22}^{-1} \\widehat{\\beta}_{2}\n\\]\nwhere we have partitioned\n\\[\n\\widehat{\\boldsymbol{V}}_{\\beta}=\\left[\\begin{array}{ll}\n\\widehat{\\boldsymbol{V}}_{11} & \\widehat{\\boldsymbol{V}}_{12} \\\\\n\\widehat{\\boldsymbol{V}}_{21} & \\widehat{\\boldsymbol{V}}_{22}\n\\end{array}\\right]\n\\]\nFrom Theorem \\(8.9\\) its asymptotic variance is\n\\[\n\\operatorname{avar}\\left[\\bar{\\beta}_{1}\\right]=\\boldsymbol{V}_{11}-\\boldsymbol{V}_{12} \\boldsymbol{V}_{22}^{-1} \\boldsymbol{V}_{21}\n\\]\nSee Exercise \\(8.16\\) to verify equations (8.29), (8.30), and (8.31).\nIn general the three estimators are different and they have different asymptotic variances. It is instructive to compare the variances to assess whether or not the constrained estimator is more efficient than the unconstrained estimator.\nFirst, assume conditional homoskedasticity. In this case the two covariance matrices simplify to \\(\\operatorname{avar}\\left[\\widehat{\\beta}_{1}\\right]=\\sigma^{2} \\boldsymbol{Q}_{11 \\cdot 2}^{-1}\\) and \\(\\operatorname{avar}\\left[\\widetilde{\\beta}_{1}\\right]=\\sigma^{2} \\boldsymbol{Q}_{11}^{-1}\\). If \\(\\boldsymbol{Q}_{12}=0\\) (so \\(X_{1}\\) and \\(X_{2}\\) are uncorrelated) then these two variance matrices are equal and the two estimators have equal asymptotic efficiency. Otherwise, since \\(\\boldsymbol{Q}_{12} \\boldsymbol{Q}_{22}^{-1} \\boldsymbol{Q}_{21} \\geq 0\\), then \\(\\boldsymbol{Q}_{11} \\geq \\boldsymbol{Q}_{11}-\\boldsymbol{Q}_{12} \\boldsymbol{Q}_{22}^{-1} \\boldsymbol{Q}_{21}\\) and consequently\n\\[\n\\boldsymbol{Q}_{11}^{-1} \\sigma^{2} \\leq\\left(\\boldsymbol{Q}_{11}-\\boldsymbol{Q}_{12} \\boldsymbol{Q}_{22}^{-1} \\boldsymbol{Q}_{21}\\right)^{-1} \\sigma^{2} .\n\\]\nThis means that under conditional homoskedasticity \\(\\widetilde{\\beta}_{1}\\) has a lower asymptotic covariance matrix than \\(\\widehat{\\beta}_{1}\\). Therefore in this context constrained least squares is more efficient than unconstrained least squares. This is consistent with our intuition that imposing a correct restriction (excluding an irrelevant regressor) improves estimation efficiency.\nHowever, in the general case of conditional heteroskedasticity this ranking is not guaranteed. In fact what is really amazing is that the variance ranking can be reversed. The CLS estimator can have a larger asymptotic variance than the unconstrained least squares estimator.\nTo see this let’s use the simple heteroskedastic example from Section 7.4. In that example, \\(Q_{11}=\\) \\(Q_{22}=1, Q_{12}=\\frac{1}{2}, \\Omega_{11}=\\Omega_{22}=1\\), and \\(\\Omega_{12}=\\frac{7}{8}\\). We can calculate (see Exercise 8.17) that \\(Q_{11 \\cdot 2}=\\frac{3}{4}\\) and\n\\[\n\\begin{aligned}\n\\operatorname{avar}\\left[\\widehat{\\beta}_{1}\\right] &=\\frac{2}{3} \\\\\n\\operatorname{avar}\\left[\\widetilde{\\beta}_{1}\\right] &=1 \\\\\n\\operatorname{avar}\\left[\\bar{\\beta}_{1}\\right] &=\\frac{5}{8} .\n\\end{aligned}\n\\]\nThus the CLS estimator \\(\\widetilde{\\beta}_{1}\\) has a larger variance than the unrestricted least squares estimator \\(\\widehat{\\beta}_{1}\\) ! The minimum distance estimator has the smallest variance of the three, as expected.\nWhat we have found is that when the estimation method is least squares, deleting the irrelevant variable \\(X_{2}\\) can actually increase estimation variance, or equivalently, adding an irrelevant variable can decrease the estimation variance. To repeat this unexpected finding, we have shown that it is possible for least squares applied to the short regression (8.11) to be less efficient for estimation of \\(\\beta_{1}\\) than least squares applied to the long regression (8.10) even though the constraint \\(\\beta_{2}=0\\) is valid! This result is strongly counter-intuitive. It seems to contradict our initial motivation for pursuing constrained estimation - to improve estimation efficiency.\nIt turns out that a more refined answer is appropriate. Constrained estimation is desirable but not necessarily CLS. While least squares is asymptotically efficient for estimation of the unconstrained projection model it is not an efficient estimator of the constrained projection model."
  },
  {
    "objectID": "chpt08-restricted-est.html#variance-and-standard-error-estimation",
    "href": "chpt08-restricted-est.html#variance-and-standard-error-estimation",
    "title": "8  Restricted Estimation",
    "section": "8.10 Variance and Standard Error Estimation",
    "text": "8.10 Variance and Standard Error Estimation\nWe have discussed covariance matrix estimation for CLS but not yet for the EMD estimator.\nThe asymptotic covariance matrix (8.26) may be estimated by replacing \\(\\boldsymbol{V}_{\\beta}\\) with a consistent estimator. It is best to construct the variance estimate using \\(\\widetilde{\\beta}_{\\text {emd. }}\\). The EMD residuals are \\(\\widetilde{e}_{i}=Y_{i}-X_{i}^{\\prime} \\widetilde{\\beta}_{\\text {emd }}\\). Using these we can estimate the matrix \\(\\Omega=\\mathbb{E}\\left[X X^{\\prime} e^{2}\\right]\\) by\n\\[\n\\widetilde{\\Omega}=\\frac{1}{n-k+q} \\sum_{i=1}^{n} X_{i} X_{i}^{\\prime} \\widetilde{e}_{i}^{2} \\text {. }\n\\]\nFollowing the formula for CLS we recommend an adjusted degrees of freedom. Given \\(\\widetilde{\\Omega}\\) the moment estimator of \\(\\boldsymbol{V}_{\\beta}\\) is \\(\\widetilde{\\boldsymbol{V}}_{\\beta}=\\widehat{\\boldsymbol{Q}}_{X X}^{-1} \\widetilde{\\Omega} \\widehat{\\boldsymbol{Q}}_{X X}^{-1}\\). Given this, we construct the variance estimator\n\\[\n\\widetilde{\\boldsymbol{V}}_{\\beta, \\mathrm{emd}}=\\widetilde{\\boldsymbol{V}}_{\\beta}-\\widetilde{\\boldsymbol{V}}_{\\beta} \\boldsymbol{R}\\left(\\boldsymbol{R}^{\\prime} \\widetilde{\\boldsymbol{V}}_{\\beta} \\boldsymbol{R}\\right)^{-1} \\boldsymbol{R}^{\\prime} \\widetilde{\\boldsymbol{V}}_{\\beta} .\n\\]\nA standard error for \\(h^{\\prime} \\widetilde{\\beta}\\) is then\n\\[\ns\\left(h^{\\prime} \\widetilde{\\beta}\\right)=\\left(n^{-1} h^{\\prime} \\widetilde{\\boldsymbol{V}}_{\\beta, \\text { emd }} h\\right)^{1 / 2} .\n\\]"
  },
  {
    "objectID": "chpt08-restricted-est.html#hausman-equality",
    "href": "chpt08-restricted-est.html#hausman-equality",
    "title": "8  Restricted Estimation",
    "section": "8.11 Hausman Equality",
    "text": "8.11 Hausman Equality\nForm (8.25) we have\n\\[\n\\begin{aligned}\n\\sqrt{n}\\left(\\widehat{\\beta}_{\\mathrm{ols}}-\\widetilde{\\beta}_{\\mathrm{emd}}\\right) &=\\widehat{\\boldsymbol{V}}_{\\beta} \\boldsymbol{R}\\left(\\boldsymbol{R}^{\\prime} \\widehat{\\boldsymbol{V}}_{\\beta} \\boldsymbol{R}\\right)^{-1} \\sqrt{n}\\left(\\boldsymbol{R}^{\\prime} \\widehat{\\beta}_{\\mathrm{ols}}-\\boldsymbol{c}\\right) \\\\\n& \\underset{d}{\\mathrm{~N}}\\left(0, \\boldsymbol{V}_{\\beta} \\boldsymbol{R}\\left(\\boldsymbol{R}^{\\prime} \\boldsymbol{V}_{\\beta} \\boldsymbol{R}\\right)^{-1} \\boldsymbol{R}^{\\prime} \\boldsymbol{V}_{\\beta}\\right)\n\\end{aligned}\n\\]\nIt follows that the asymptotic variances of the estimators satisfy the relationship\n\\[\n\\operatorname{avar}\\left[\\widehat{\\beta}_{\\mathrm{ols}}-\\widetilde{\\beta}_{\\mathrm{emd}}\\right]=\\operatorname{avar}\\left[\\widehat{\\beta}_{\\mathrm{ols}}\\right]-\\operatorname{avar}\\left[\\widetilde{\\beta}_{\\mathrm{emd}}\\right] .\n\\]\nWe call (8.37) the Hausman Equality: the asymptotic variance of the difference between an efficient and another estimator is the difference in the asymptotic variances."
  },
  {
    "objectID": "chpt08-restricted-est.html#example-mankiw-romer-and-weil-1992",
    "href": "chpt08-restricted-est.html#example-mankiw-romer-and-weil-1992",
    "title": "8  Restricted Estimation",
    "section": "8.12 Example: Mankiw, Romer and Weil (1992)",
    "text": "8.12 Example: Mankiw, Romer and Weil (1992)\nWe illustrate the methods by replicating some of the estimates reported in a well-known paper by Mankiw, Romer, and Weil (1992). The paper investigates the implications of the Solow growth model using cross-country regressions. A key equation in their paper regresses the change between 1960 and 1985 in \\(\\log\\) GDP per capita on (1) \\(\\log\\) GDP in 1960, (2) the log of the ratio of aggregate investment to Table 8.1: Estimates of Solow Growth Model\n\n\n\n\n\n\n\n\n\n\n\\(\\widehat{\\beta}_{\\text {ols }}\\)\n\\(\\widehat{\\beta}_{\\text {cls }}\\)\n\\(\\widehat{\\beta}_{\\mathrm{emd}}\\)\n\n\n\n\n\\(\\log G D P_{1960}\\)\n\\(-0.29\\)\n\\(-0.30\\)\n\\(-0.30\\)\n\n\n\n\\((0.05)\\)\n\\((0.05)\\)\n\\((0.05)\\)\n\n\n\\(\\log \\frac{I}{\\text { GDP }}\\)\n\\(0.52\\)\n\\(0.50\\)\n\\(0.46\\)\n\n\n\n\\((0.11)\\)\n\\((0.09)\\)\n\\((0.08)\\)\n\n\n\\(\\log (n+g+\\delta)\\)\n\\(-0.51\\)\n\\(-0.74\\)\n\\(-0.71\\)\n\n\n\n\\((0.24)\\)\n\\((0.08)\\)\n\\((0.07)\\)\n\n\n\\(\\log (\\) School \\()\\)\n\\(0.23\\)\n\\(0.24\\)\n\\(0.25\\)\n\n\n\n\\((0.07)\\)\n\\((0.07)\\)\n\\((0.06)\\)\n\n\nIntercept\n\\(3.02\\)\n\\(2.46\\)\n\\(2.48\\)\n\n\n\n\\((0.74)\\)\n\\((0.44)\\)\n\\((0.44)\\)\n\n\n\nStandard errors are heteroskedasticity-consistent\nGDP, (3) the log of the sum of the population growth rate \\(n\\), the technological growth rate \\(g\\), and the rate of depreciation \\(\\delta\\), and (4) the log of the percentage of the working-age population that is in secondary school (School), the latter a proxy for human-capital accumulation.\nThe data is available on the textbook webpage in the file MRW1992.\nThe sample is 98 non-oil-producing countries and the data was reported in the published paper. As \\(g\\) and \\(\\delta\\) were unknown the authors set \\(g+\\delta=0.05\\). We report least squares estimates in the first column of Table 8.1. The estimates are consistent with the Solow theory due to the positive coefficients on investment and human capital and negative coefficient for population growth. The estimates are also consistent with the convergence hypothesis (that income levels tend towards a common mean over time) as the coefficient on intial GDP is negative.\nThe authors show that in the Solow model the \\(2^{n d}, 3^{r d}\\) and \\(4^{t h}\\) coefficients sum to zero. They reestimated the equation imposing this constraint. We present constrained least squares estimates in the second column of Table \\(8.1\\) and efficient minimum distance estimates in the third column. Most of the coefficients and standard errors only exhibit small changes by imposing the constraint. The one exception is the coefficient on log population growth which increases in magnitude and its standard error decreases substantially. The differences between the CLS and EMD estimates are modest.\nWe now present Stata, R and MATLAB code which implements these estimates.\nYou may notice that the Stata code has a section which uses the Mata matrix programming language. This is used because Stata does not implement the efficient minimum distance estimator, so needs to be separately programmed. As illustrated here, the Mata language allows a Stata user to implement methods using commands which are quite similar to MATLAB."
  },
  {
    "objectID": "chpt08-restricted-est.html#misspecification",
    "href": "chpt08-restricted-est.html#misspecification",
    "title": "8  Restricted Estimation",
    "section": "8.13 Misspecification",
    "text": "8.13 Misspecification\nWhat are the consequences for a constrained estimator \\(\\widetilde{\\beta}\\) if the constraint (8.1) is incorrect? To be specific suppose that the truth is\n\\[\n\\boldsymbol{R}^{\\prime} \\beta=\\boldsymbol{c}^{*}\n\\]\nwhere \\(\\boldsymbol{c}^{*}\\) is not necessarily equal to \\(\\boldsymbol{c}\\).\nThis situation is a generalization of the analysis of “omitted variable bias” from Section \\(2.24\\) where we found that the short regression (e.g. (8.12)) is estimating a different projection coefficient than the long regression (e.g. (8.10)).\nOne answer is to apply formula (8.23) to find that\n\\[\n\\widetilde{\\beta}_{\\mathrm{md}} \\underset{p}{\\rightarrow} \\beta_{\\mathrm{md}}^{*}=\\beta-\\boldsymbol{W}^{-1} \\boldsymbol{R}\\left(\\boldsymbol{R}^{\\prime} \\boldsymbol{W}^{-1} \\boldsymbol{R}\\right)^{-1}\\left(\\boldsymbol{c}^{*}-\\boldsymbol{c}\\right) .\n\\]\nThe second term, \\(\\boldsymbol{W}^{-1} \\boldsymbol{R}\\left(\\boldsymbol{R}^{\\prime} \\boldsymbol{W}^{-1} \\boldsymbol{R}\\right)^{-1}\\left(\\boldsymbol{c}^{*}-\\boldsymbol{c}\\right)\\), shows that imposing an incorrect constraint leads to inconsistency - an asymptotic bias. We can call the limiting value \\(\\beta_{\\mathrm{md}}^{*}\\) the minimum-distance projection coefficient or the pseudo-true value implied by the restriction.\nHowever, we can say more.\nFor example, we can describe some characteristics of the approximating projections. The CLS estimator projection coefficient has the representation\n\\[\n\\beta_{\\mathrm{cls}}^{*}=\\underset{\\boldsymbol{R}^{\\prime} \\beta=\\boldsymbol{c}}{\\operatorname{argmin}} \\mathbb{E}\\left[\\left(Y-X^{\\prime} \\beta\\right)^{2}\\right],\n\\]\nthe best linear predictor subject to the constraint (8.1). The minimum distance estimator converges in probability to\n\\[\n\\beta_{\\mathrm{md}}^{*}=\\underset{\\boldsymbol{R}^{\\prime} \\beta=\\boldsymbol{c}}{\\operatorname{argmin}}\\left(\\beta-\\beta_{0}\\right)^{\\prime} \\boldsymbol{W}\\left(\\beta-\\beta_{0}\\right)\n\\]\nwhere \\(\\beta_{0}\\) is the true coefficient. That is, \\(\\beta_{\\mathrm{md}}^{*}\\) is the coefficient vector satisfying (8.1) closest to the true value in the weighted Euclidean norm. These calculations show that the constrained estimators are still reasonable in the sense that they produce good approximations to the true coefficient conditional on being required to satisfy the constraint.\nWe can also show that \\(\\widetilde{\\beta}_{\\mathrm{md}}\\) has an asymptotic normal distribution. The trick is to define the pseudotrue value\n\\[\n\\beta_{n}^{*}=\\beta-\\widehat{\\boldsymbol{W}}^{-1} \\boldsymbol{R}\\left(\\boldsymbol{R}^{\\prime} \\widehat{\\boldsymbol{W}}^{-1} \\boldsymbol{R}\\right)^{-1}\\left(\\boldsymbol{c}^{*}-\\boldsymbol{c}\\right) .\n\\]\n(Note that (8.38) and (8.39) are different!) Then\n\\[\n\\begin{aligned}\n\\sqrt{n}\\left(\\widetilde{\\beta}_{\\mathrm{md}}-\\beta_{n}^{*}\\right)=& \\sqrt{n}(\\widehat{\\beta}-\\beta)-\\widehat{\\boldsymbol{W}}^{-1} \\boldsymbol{R}\\left(\\boldsymbol{R}^{\\prime} \\widehat{\\boldsymbol{W}}^{-1} \\boldsymbol{R}\\right)^{-1} \\sqrt{n}\\left(\\boldsymbol{R}^{\\prime} \\widehat{\\beta}-\\boldsymbol{c}^{*}\\right) \\\\\n&=\\left(\\boldsymbol{I}_{k}-\\widehat{\\boldsymbol{W}}^{-1} \\boldsymbol{R}\\left(\\boldsymbol{R}^{\\prime} \\widehat{\\boldsymbol{W}}^{-1} \\boldsymbol{R}\\right)^{-1} \\boldsymbol{R}^{\\prime}\\right) \\sqrt{n}(\\widehat{\\beta}-\\beta) \\\\\n& \\underset{d}{\\longrightarrow}\\left(\\boldsymbol{I}_{k}-\\boldsymbol{W}^{-1} \\boldsymbol{R}\\left(\\boldsymbol{R}^{\\prime} \\boldsymbol{W}^{-1} \\boldsymbol{R}\\right)^{-1} \\boldsymbol{R}^{\\prime}\\right) \\mathrm{N}\\left(0, \\boldsymbol{V}_{\\beta}\\right) \\\\\n&=\\mathrm{N}\\left(0, \\boldsymbol{V}_{\\beta}(\\boldsymbol{W})\\right)\n\\end{aligned}\n\\]\nIn particular\n\\[\n\\sqrt{n}\\left(\\widetilde{\\beta}_{\\mathrm{emd}}-\\beta_{n}^{*}\\right) \\underset{d}{\\longrightarrow} \\mathrm{N}\\left(0, \\boldsymbol{V}_{\\beta}^{*}\\right) .\n\\]\nThis means that even when the constraint (8.1) is misspecified the conventional covariance matrix estimator (8.35) and standard errors (8.36) are appropriate measures of the sampling variance though the distributions are centered at the pseudo-true values (projections) \\(\\beta_{n}^{*}\\) rather than \\(\\beta\\). The fact that the estimators are biased is an unavoidable consequence of misspecification.\nAn alternative approach to the asymptotic distribution theory under misspecification uses the concept of local alternatives. It is a technical device which might seem a bit artificial but it is a powerful method to derive useful distributional approximations in a wide variety of contexts. The idea is to index the true coefficient \\(\\beta_{n}\\) by \\(n\\) via the relationship\n\\[\n\\boldsymbol{R}^{\\prime} \\beta_{n}=\\boldsymbol{c}+\\delta n^{-1 / 2} .\n\\]\nfor some \\(\\delta \\in \\mathbb{R}^{q}\\). Equation (8.41) specifies that \\(\\beta_{n}\\) violates (8.1) and thus the constraint is misspecified. However, the constraint is “close” to correct as the difference \\(\\boldsymbol{R}^{\\prime} \\beta_{n}-\\boldsymbol{c}=\\delta n^{-1 / 2}\\) is “small” in the sense that it decreases with the sample size \\(n\\). We call (8.41) local misspecification.\nThe asymptotic theory is derived as \\(n \\rightarrow \\infty\\) under the sequence of probability distributions with the coefficients \\(\\beta_{n}\\). The way to think about this is that the true value of the parameter is \\(\\beta_{n}\\) and it is “close” to satisfying (8.1). The reason why the deviation is proportional to \\(n^{-1 / 2}\\) is because this is the only choice under which the localizing parameter \\(\\delta\\) appears in the asymptotic distribution but does not dominate it. The best way to see this is to work through the asymptotic approximation.\nSince \\(\\beta_{n}\\) is the true coefficient value, then \\(Y=X^{\\prime} \\beta_{n}+e\\) and we have the standard representation for the unconstrained estimator, namely\n\\[\n\\sqrt{n}\\left(\\widehat{\\beta}-\\beta_{n}\\right)=\\left(\\frac{1}{n} \\sum_{i=1}^{n} X_{i} X_{i}^{\\prime}\\right)^{-1}\\left(\\frac{1}{\\sqrt{n}} \\sum_{i=1}^{n} X_{i} e_{i}\\right) \\underset{d}{\\longrightarrow} \\mathrm{N}\\left(0, \\boldsymbol{V}_{\\beta}\\right) .\n\\]\nThere is no difference under fixed (classical) or local asymptotics since the right-hand-side is independent of the coefficient \\(\\beta_{n}\\).\nA difference arises for the constrained estimator. Using (8.41), \\(\\boldsymbol{c}=\\boldsymbol{R}^{\\prime} \\beta_{n}-\\delta n^{-1 / 2}\\) so\n\\[\n\\boldsymbol{R}^{\\prime} \\widehat{\\beta}-\\boldsymbol{c}=\\boldsymbol{R}^{\\prime}\\left(\\widehat{\\beta}-\\beta_{n}\\right)+\\delta n^{-1 / 2}\n\\]\nand\n\\[\n\\begin{aligned}\n\\widetilde{\\beta}_{\\mathrm{md}} &=\\widehat{\\beta}-\\widehat{\\boldsymbol{W}}^{-1} \\boldsymbol{R}\\left(\\boldsymbol{R}^{\\prime} \\widehat{\\boldsymbol{W}}^{-1} \\boldsymbol{R}\\right)^{-1}\\left(\\boldsymbol{R}^{\\prime} \\widehat{\\beta}-\\boldsymbol{c}\\right) \\\\\n&=\\widehat{\\beta}-\\widehat{\\boldsymbol{W}}^{-1} \\boldsymbol{R}\\left(\\boldsymbol{R}^{\\prime} \\widehat{\\boldsymbol{W}}^{-1} \\boldsymbol{R}\\right)^{-1} \\boldsymbol{R}^{\\prime}\\left(\\widehat{\\beta}-\\beta_{n}\\right)+\\widehat{\\boldsymbol{W}}^{-1} \\boldsymbol{R}\\left(\\boldsymbol{R}^{\\prime} \\widehat{\\boldsymbol{W}}^{-1} \\boldsymbol{R}\\right)^{-1} \\delta n^{-1 / 2} .\n\\end{aligned}\n\\]\nIt follows that\n\\[\n\\sqrt{n}\\left(\\widetilde{\\beta}_{\\mathrm{md}}-\\beta_{n}\\right)=\\left(\\boldsymbol{I}_{k}-\\widehat{\\boldsymbol{W}}^{-1} \\boldsymbol{R}\\left(\\boldsymbol{R}^{\\prime} \\widehat{\\boldsymbol{W}}^{-1} \\boldsymbol{R}\\right)^{-1} \\boldsymbol{R}^{\\prime}\\right) \\sqrt{n}\\left(\\widehat{\\beta}-\\beta_{n}\\right)+\\widehat{\\boldsymbol{W}}^{-1} \\boldsymbol{R}\\left(\\boldsymbol{R}^{\\prime} \\widehat{\\boldsymbol{W}}^{-1} \\boldsymbol{R}\\right)^{-1} \\delta .\n\\]\nThe first term is asymptotically normal (from 8.42)). The second term converges in probability to a constant. This is because the \\(n^{-1 / 2}\\) local scaling in (8.41) is exactly balanced by the \\(\\sqrt{n}\\) scaling of the estimator. No alternative rate would have produced this result.\nConsequently we find that the asymptotic distribution equals\n\\[\n\\sqrt{n}\\left(\\widetilde{\\beta}_{\\mathrm{md}}-\\beta_{n}\\right) \\underset{d}{\\longrightarrow} \\mathrm{N}\\left(0, \\boldsymbol{V}_{\\beta}\\right)+\\boldsymbol{W}^{-1} \\boldsymbol{R}\\left(\\boldsymbol{R}^{\\prime} \\boldsymbol{W}^{-1} \\boldsymbol{R}\\right)^{-1} \\delta=\\mathrm{N}\\left(\\delta^{*}, \\boldsymbol{V}_{\\beta}(\\boldsymbol{W})\\right)\n\\]\nwhere \\(\\delta^{*}=\\boldsymbol{W}^{-1} \\boldsymbol{R}\\left(\\boldsymbol{R}^{\\prime} \\boldsymbol{W}^{-1} \\boldsymbol{R}\\right)^{-1} \\delta\\)\nThe asymptotic distribution (8.43) is an approximation of the sampling distribution of the restricted estimator under misspecification. The distribution (8.43) contains an asymptotic bias component \\(\\delta^{*}\\). The approximation is not fundamentally different from (8.40) - they both have the same asymptotic variances and both reflect the bias due to misspecification. The difference is that (8.40) puts the bias on the left-side of the convergence arrow while (8.43) has the bias on the right-side. There is no substantive difference between the two. However, (8.43) is more convenient for some purposes such as the analysis of the power of tests as we will explore in the next chapter."
  },
  {
    "objectID": "chpt08-restricted-est.html#nonlinear-constraints",
    "href": "chpt08-restricted-est.html#nonlinear-constraints",
    "title": "8  Restricted Estimation",
    "section": "8.14 Nonlinear Constraints",
    "text": "8.14 Nonlinear Constraints\nIn some cases it is desirable to impose nonlinear constraints on the parameter vector \\(\\beta\\). They can be written as\n\\[\nr(\\beta)=0\n\\]\nwhere \\(r: \\mathbb{R}^{k} \\rightarrow \\mathbb{R}^{q}\\). This includes the linear constraints (8.1) as a special case. An example of (8.44) which cannot be written as (8.1) is \\(\\beta_{1} \\beta_{2}=1\\), which is (8.44) with \\(r(\\beta)=\\beta_{1} \\beta_{2}-1\\).\nThe constrained least squares and minimum distance estimators of \\(\\beta\\) subject to (8.44) solve the minimization problems\n\\[\n\\begin{gathered}\n\\widetilde{\\beta}_{\\mathrm{cls}}=\\underset{r(\\beta)=0}{\\operatorname{argmin} \\operatorname{SSE}(\\beta)} \\\\\n\\widetilde{\\beta}_{\\mathrm{md}}=\\underset{r(\\beta)=0}{\\operatorname{argmin}} J(\\beta)\n\\end{gathered}\n\\]\nwhere \\(\\operatorname{SSE}(\\beta)\\) and \\(J(\\beta)\\) are defined in (8.4) and (8.19), respectively. The solutions solve the Lagrangians\n\\[\n\\mathscr{L}(\\beta, \\lambda)=\\frac{1}{2} \\operatorname{SSE}(\\beta)+\\lambda^{\\prime} r(\\beta)\n\\]\nor\n\\[\n\\mathscr{L}(\\beta, \\lambda)=\\frac{1}{2} J(\\beta)+\\lambda^{\\prime} r(\\beta)\n\\]\n\\(\\operatorname{over}(\\beta, \\lambda)\\)\nComputationally there is no general closed-form solution so they must be found numerically. Algorithms to numerically solve (8.45) and (8.46) are known as constrained optimization methods and are available in programming languages including MATLAB and R. See Chapter 12 of Probability and Statistics for Economists.\nAssumption 8.3\n\n\\(r(\\beta)=0\\).\n\\(r(\\beta)\\) is continuously differentiable at the true \\(\\beta\\).\n\\(\\operatorname{rank}(\\boldsymbol{R})=q\\), where \\(\\boldsymbol{R}=\\frac{\\partial}{\\partial \\beta} r(\\beta)^{\\prime}\\).\n\nThe asymptotic distribution is a simple generalization of the case of a linear constraint but the proof is more delicate. Theorem 8.10 Under Assumptions 7.2, 8.2, and 8.3, for \\(\\widetilde{\\beta}=\\widetilde{\\beta}_{\\mathrm{md}}\\) and \\(\\widetilde{\\beta}=\\widetilde{\\beta}_{\\text {cls }}\\) defined in (8.45) and (8.46),\n\\[\n\\sqrt{n}(\\widetilde{\\beta}-\\beta) \\underset{d}{\\longrightarrow} \\mathrm{N}\\left(0, \\boldsymbol{V}_{\\beta}(\\boldsymbol{W})\\right)\n\\]\nas \\(n \\rightarrow \\infty\\) where \\(\\boldsymbol{V}_{\\beta}(\\boldsymbol{W})\\) is defined in (8.24). For \\(\\widetilde{\\beta}_{\\text {cls }}, \\boldsymbol{W}=\\boldsymbol{Q}_{X X}\\) and \\(\\boldsymbol{V}_{\\beta}(\\boldsymbol{W})=\\) \\(\\boldsymbol{V}_{\\text {cls }}\\) as defined in Theorem 8.8. \\(\\boldsymbol{V}_{\\beta}(\\boldsymbol{W})\\) is minimized with \\(\\boldsymbol{W}=\\boldsymbol{V}_{\\beta}^{-1}\\) in which case the asymptotic variance is\n\\[\n\\boldsymbol{V}_{\\beta}^{*}=\\boldsymbol{V}_{\\beta}-\\boldsymbol{V}_{\\beta} \\boldsymbol{R}\\left(\\boldsymbol{R}^{\\prime} \\boldsymbol{V}_{\\beta} \\boldsymbol{R}\\right)^{-1} \\boldsymbol{R}^{\\prime} \\boldsymbol{V}_{\\beta} .\n\\]\nThe asymptotic covariance matrix for the efficient minimum distance estimator can be estimated by\n\\[\n\\widehat{\\boldsymbol{V}}_{\\beta}^{*}=\\widehat{\\boldsymbol{V}}_{\\beta}-\\widehat{\\boldsymbol{V}}_{\\beta} \\widehat{\\boldsymbol{R}}\\left(\\widehat{\\boldsymbol{R}}^{\\prime} \\widehat{\\boldsymbol{V}}_{\\beta} \\widehat{\\boldsymbol{R}}^{-1} \\widehat{\\boldsymbol{R}}^{\\prime} \\widehat{\\boldsymbol{V}}_{\\beta}\\right.\n\\]\nwhere\n\\[\n\\widehat{\\boldsymbol{R}}=\\frac{\\partial}{\\partial \\beta} r\\left(\\widetilde{\\beta}_{\\mathrm{md}}\\right)^{\\prime} .\n\\]\nStandard errors for the elements of \\(\\widetilde{\\beta}_{\\mathrm{md}}\\) are the square roots of the diagonal elements of \\(\\widehat{\\boldsymbol{V}}_{\\widetilde{\\beta}}^{*}=n^{-1} \\widehat{\\boldsymbol{V}}_{\\beta}^{*}\\)."
  },
  {
    "objectID": "chpt08-restricted-est.html#inequality-restrictions",
    "href": "chpt08-restricted-est.html#inequality-restrictions",
    "title": "8  Restricted Estimation",
    "section": "8.15 Inequality Restrictions",
    "text": "8.15 Inequality Restrictions\nInequality constraints on the parameter vector \\(\\beta\\) take the form\n\\[\nr(\\beta) \\geq 0\n\\]\nfor some function \\(r: \\mathbb{R}^{k} \\rightarrow \\mathbb{R}^{q}\\). The most common example is a non-negative constraint \\(\\beta_{1} \\geq 0\\).\nThe constrained least squares and minimum distance estimators can be written as\n\\[\n\\widetilde{\\beta}_{\\mathrm{cls}}=\\underset{r(\\beta) \\geq 0}{\\operatorname{argmin}} \\operatorname{SSE}(\\beta)\n\\]\nand\n\\[\n\\widetilde{\\beta}_{\\mathrm{md}}=\\underset{r(\\beta) \\geq 0}{\\operatorname{argmin}} J(\\beta) .\n\\]\nExcept in special cases the constrained estimators do not have simple algebraic solutions. An important exception is when there is a single non-negativity constraint, e.g. \\(\\beta_{1} \\geq 0\\) with \\(q=1\\). In this case the constrained estimator can be found by the following approach. Compute the uncontrained estimator \\(\\widehat{\\beta}\\). If \\(\\widehat{\\beta}_{1} \\geq 0\\) then \\(\\widetilde{\\beta}=\\widehat{\\beta}\\). Otherwise if \\(\\widehat{\\beta}_{1}<0\\) then impose \\(\\beta_{1}=0\\) (eliminate the regressor \\(X_{1}\\) ) and re-estimate. This method yields the constrained least squares estimator. While this method works when there is a single non-negativity constraint, it does not immediately generalize to other contexts.\nThe computation problems (8.50) and (8.51) are examples of quadratic programming. Quick computer algorithms are available in programming languages including MATLAB and R.\nInference on inequality-constrained estimators is unfortunately quite challenging. The conventional asymptotic theory gives rise to the following dichotomy. If the true parameter satisfies the strict inequality \\(r(\\beta)>0\\) then asymptotically the estimator is not subject to the constraint and the inequalityconstrained estimator has an asymptotic distribution equal to the unconstrained case. However if the true parameter is on the boundary, e.g., \\(r(\\beta)=0\\), then the estimator has a truncated structure. This is easiest to see in the one-dimensional case. If we have an estimator \\(\\widehat{\\beta}\\) which satisfies \\(\\sqrt{n}(\\widehat{\\beta}-\\beta) \\underset{d}{\\rightarrow} Z=\\) \\(\\mathrm{N}\\left(0, V_{\\beta}\\right)\\) and \\(\\beta=0\\), then the constrained estimator \\(\\widetilde{\\beta}=\\max [\\widehat{\\beta}, 0]\\) will have the asymptotic distribution \\(\\sqrt{n} \\widetilde{\\beta} \\underset{d}{\\longrightarrow} \\max [Z, 0]\\), a “half-normal” distribution."
  },
  {
    "objectID": "chpt08-restricted-est.html#technical-proofs",
    "href": "chpt08-restricted-est.html#technical-proofs",
    "title": "8  Restricted Estimation",
    "section": "8.16 Technical Proofs*",
    "text": "8.16 Technical Proofs*\nProof of Theorem 8.9, equation (8.28) Let \\(\\boldsymbol{R}_{\\perp}\\) be a full rank \\(k \\times(k-q)\\) matrix satisfying \\(\\boldsymbol{R}_{\\perp}^{\\prime} \\boldsymbol{V}_{\\beta} \\boldsymbol{R}=0\\) and then set \\(\\boldsymbol{C}=\\left[\\boldsymbol{R}, \\boldsymbol{R}_{\\perp}\\right]\\) which is full rank and invertible. Then we can calculate that\n\\[\n\\boldsymbol{C}^{\\prime} \\boldsymbol{V}_{\\beta}^{*} \\boldsymbol{C}=\\left[\\begin{array}{cc}\n\\boldsymbol{R}^{\\prime} \\boldsymbol{V}_{\\beta}^{*} \\boldsymbol{R} & \\boldsymbol{R}^{\\prime} \\boldsymbol{V}_{\\beta}^{*} \\boldsymbol{R}_{\\perp} \\\\\n\\boldsymbol{R}_{\\perp}^{\\prime} \\boldsymbol{V}_{\\beta}^{*} \\boldsymbol{R} & \\boldsymbol{R}_{\\perp}^{\\prime} \\boldsymbol{V}_{\\beta}^{*} \\boldsymbol{R}_{\\perp}\n\\end{array}\\right]=\\left[\\begin{array}{cc}\n0 & 0 \\\\\n0 & \\boldsymbol{R}_{\\perp}^{\\prime} \\boldsymbol{V}_{\\beta} \\boldsymbol{R}_{\\perp}\n\\end{array}\\right]\n\\]\nand\n\\[\n\\begin{aligned}\n&\\boldsymbol{C}^{\\prime} \\boldsymbol{V}_{\\beta}(\\boldsymbol{W}) \\boldsymbol{C} \\\\\n&=\\left[\\begin{array}{cc}\n\\boldsymbol{R}^{\\prime} \\boldsymbol{V}_{\\beta}^{*}(\\boldsymbol{W}) \\boldsymbol{R} & \\boldsymbol{R}^{\\prime} \\boldsymbol{V}_{\\beta}^{*}(\\boldsymbol{W}) \\boldsymbol{R}_{\\perp} \\\\\n\\boldsymbol{R}_{\\perp}^{\\prime} \\boldsymbol{V}_{\\beta}^{*}(\\boldsymbol{W}) \\boldsymbol{R} & \\boldsymbol{R}_{\\perp}^{\\prime} \\boldsymbol{V}_{\\beta}^{*}(\\boldsymbol{W}) \\boldsymbol{R}_{\\perp}\n\\end{array}\\right] \\\\\n&=\\left[\\begin{array}{cc}\n0 & 0 \\\\\n0 & \\boldsymbol{R}_{\\perp}^{\\prime} \\boldsymbol{V}_{\\beta} \\boldsymbol{R}_{\\perp}+\\boldsymbol{R}_{\\perp}^{\\prime} \\boldsymbol{W} \\boldsymbol{R}\\left(\\boldsymbol{R}^{\\prime} \\boldsymbol{W} \\boldsymbol{R}\\right)^{-1} \\boldsymbol{R}^{\\prime} \\boldsymbol{V}_{\\beta} \\boldsymbol{R}\\left(\\boldsymbol{R}^{\\prime} \\boldsymbol{W} \\boldsymbol{R}\\right)^{-1} \\boldsymbol{R}^{\\prime} \\boldsymbol{W} \\boldsymbol{R}_{\\perp}\n\\end{array}\\right] .\n\\end{aligned}\n\\]\nThus\n\\[\n\\begin{aligned}\n&\\boldsymbol{C}^{\\prime}\\left(\\boldsymbol{V}_{\\beta}(\\boldsymbol{W})-\\boldsymbol{V}_{\\beta}^{*}\\right) \\boldsymbol{C} \\\\\n&=\\boldsymbol{C}^{\\prime} \\boldsymbol{V}_{\\beta}(\\boldsymbol{W}) \\boldsymbol{C}-\\boldsymbol{C}^{\\prime} \\boldsymbol{V}_{\\beta}^{*} \\boldsymbol{C} \\\\\n&=\\left[\\begin{array}{cc}\n0 & 0 \\\\\n0 & \\boldsymbol{R}_{\\perp}^{\\prime} \\boldsymbol{W} \\boldsymbol{R}\\left(\\boldsymbol{R}^{\\prime} \\boldsymbol{W} \\boldsymbol{R}\\right)^{-\\mathbf{1}} \\boldsymbol{R}^{\\prime} \\boldsymbol{V}_{\\beta} \\boldsymbol{R}\\left(\\boldsymbol{R}^{\\prime} \\boldsymbol{W} \\boldsymbol{R}\\right)^{-\\mathbf{1}} \\boldsymbol{R}^{\\prime} \\boldsymbol{W} \\boldsymbol{R}_{\\perp}\n\\end{array}\\right] \\\\\n&\\geq 0\n\\end{aligned}\n\\]\nSince \\(\\boldsymbol{C}\\) is invertible it follows that \\(\\boldsymbol{V}_{\\beta}(\\boldsymbol{W})-\\boldsymbol{V}_{\\beta}^{*} \\geq 0\\) which is (8.28).\nProof of Theorem 8.10 We show the result for the minimum distance estimator \\(\\widetilde{\\beta}=\\widetilde{\\beta}_{\\mathrm{md}}\\) as the proof for the constrained least squares estimator is similar. For simplicity we assume that the constrained estimator is consistent \\(\\widetilde{\\beta} \\underset{p}{\\vec{p}} \\beta\\). This can be shown with more effort, but requires a deeper treatment than appropriate for this textbook.\nFor each element \\(r_{j}(\\beta)\\) of the \\(q\\)-vector \\(r(\\beta)\\), by the mean value theorem there exists a \\(\\beta_{j}^{*}\\) on the line segment joining \\(\\widetilde{\\beta}\\) and \\(\\beta\\) such that\n\\[\nr_{j}(\\widetilde{\\beta})=r_{j}(\\beta)+\\frac{\\partial}{\\partial \\beta} r_{j}\\left(\\beta_{j}^{*}\\right)^{\\prime}(\\widetilde{\\beta}-\\beta) .\n\\]\nLet \\(\\boldsymbol{R}_{n}^{*}\\) be the \\(k \\times q\\) matrix\n\\[\n\\boldsymbol{R}^{*}=\\left[\\begin{array}{llll}\n\\frac{\\partial}{\\partial \\beta} r_{1}\\left(\\beta_{1}^{*}\\right) & \\frac{\\partial}{\\partial \\beta} r_{2}\\left(\\beta_{2}^{*}\\right) & \\cdots & \\frac{\\partial}{\\partial \\beta} r_{q}\\left(\\beta_{q}^{*}\\right)\n\\end{array}\\right]\n\\]\nSince \\(\\widetilde{\\beta} \\underset{p}{\\vec{p}} \\beta\\) it follows that \\(\\beta_{j}^{*} \\vec{p} \\beta\\), and by the CMT, \\(\\boldsymbol{R}^{*} \\underset{p}{\\rightarrow} \\boldsymbol{R}\\). Stacking the (8.52), we obtain\n\\[\nr(\\widetilde{\\beta})=r(\\beta)+\\boldsymbol{R}^{* \\prime}(\\widetilde{\\beta}-\\beta) .\n\\]\nSince \\(r(\\widetilde{\\beta})=0\\) by construction and \\(r(\\beta)=0\\) by Assumption \\(8.1\\) this implies\n\\[\n0=\\boldsymbol{R}^{* \\prime}(\\widetilde{\\beta}-\\beta) .\n\\]\nThe first-order condition for (8.47) is \\(\\widehat{\\boldsymbol{W}}(\\widehat{\\beta}-\\widetilde{\\beta})=\\widehat{\\boldsymbol{R}} \\widetilde{\\lambda}\\) where \\(\\widehat{\\boldsymbol{R}}\\) is defined in (8.48). Premultiplying by \\(\\boldsymbol{R}^{* \\prime} \\widehat{\\boldsymbol{W}}^{-1}\\), inverting, and using (8.53), we find\n\\[\n\\tilde{\\lambda}=\\left(\\boldsymbol{R}^{* \\prime} \\widehat{\\boldsymbol{W}}^{-1} \\widehat{\\boldsymbol{R}}\\right)^{-1} \\boldsymbol{R}^{* \\prime}(\\widehat{\\beta}-\\widetilde{\\beta})=\\left(\\boldsymbol{R}^{* \\prime} \\widehat{\\boldsymbol{W}}^{-1} \\widehat{\\boldsymbol{R}}\\right)^{-1} \\boldsymbol{R}^{* \\prime}(\\widehat{\\beta}-\\beta) .\n\\]\nThus\n\\[\n\\widetilde{\\beta}-\\beta=\\left(\\boldsymbol{I}_{k}-\\widehat{\\boldsymbol{W}}^{-1} \\widehat{\\boldsymbol{R}}\\left(\\boldsymbol{R}_{n}^{* \\prime} \\widehat{\\boldsymbol{W}}^{-1} \\widehat{\\boldsymbol{R}}\\right)^{-1} \\boldsymbol{R}_{n}^{* \\prime}\\right)(\\widehat{\\beta}-\\beta) .\n\\]\nFrom Theorem \\(7.3\\) and Theorem \\(7.6\\) we find\n\\[\n\\begin{aligned}\n\\sqrt{n}(\\widetilde{\\beta}-\\beta) &=\\left(\\boldsymbol{I}_{k}-\\widehat{\\boldsymbol{W}}^{-1} \\widehat{\\boldsymbol{R}}\\left(\\boldsymbol{R}_{n}^{* \\prime} \\widehat{\\boldsymbol{W}}^{-1} \\widetilde{\\boldsymbol{R}}\\right)^{-1} \\boldsymbol{R}_{n}^{* \\prime}\\right) \\sqrt{n}(\\widehat{\\beta}-\\beta) \\\\\n& \\underset{d}{\\longrightarrow}\\left(\\boldsymbol{I}_{k}-\\boldsymbol{W}^{-1} \\boldsymbol{R}\\left(\\boldsymbol{R}^{\\prime} \\boldsymbol{W}^{-1} \\boldsymbol{R}\\right)^{-1} \\boldsymbol{R}^{\\prime}\\right) \\mathrm{N}\\left(0, \\boldsymbol{V}_{\\beta}\\right) \\\\\n&=\\mathrm{N}\\left(0, \\boldsymbol{V}_{\\beta}(\\boldsymbol{W})\\right)\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "chpt08-restricted-est.html#exercises",
    "href": "chpt08-restricted-est.html#exercises",
    "title": "8  Restricted Estimation",
    "section": "8.17 Exercises",
    "text": "8.17 Exercises\nExercise 8.1 In the model \\(Y=X_{1}^{\\prime} \\beta_{1}+X_{2}^{\\prime} \\beta_{2}+e\\), show directly from definition (8.3) that the CLS estimator of \\(\\beta=\\left(\\beta_{1}, \\beta_{2}\\right)\\) subject to the constraint that \\(\\beta_{2}=0\\) is the OLS regression of \\(Y\\) on \\(X_{1}\\).\nExercise 8.2 In the model \\(Y=X_{1}^{\\prime} \\beta_{1}+X_{2}^{\\prime} \\beta_{2}+e\\), show directly from definition (8.3) that the CLS estimator of \\(\\beta=\\left(\\beta_{1}, \\beta_{2}\\right)\\) subject to the constraint \\(\\beta_{1}=\\boldsymbol{c}\\) (where \\(\\boldsymbol{c}\\) is some given vector) is OLS of \\(Y-X_{1}^{\\prime} \\boldsymbol{c}\\) on \\(X_{2}\\).\nExercise 8.3 In the model \\(Y=X_{1}^{\\prime} \\beta_{1}+X_{2}^{\\prime} \\beta_{2}+e\\), with \\(\\beta_{1}\\) and \\(\\beta_{2}\\) each \\(k \\times 1\\), find the CLS estimator of \\(\\beta=\\left(\\beta_{1}, \\beta_{2}\\right)\\) subject to the constraint that \\(\\beta_{1}=-\\beta_{2}\\).\nExercise 8.4 In the linear projection model \\(Y=\\alpha+X^{\\prime} \\beta+e\\) consider the restriction \\(\\beta=0\\).\n\nFind the CLS estimator of \\(\\alpha\\) under the restriction \\(\\beta=0\\).\nFind an expression for the efficient minimum distance estimator of \\(\\alpha\\) under the restriction \\(\\beta=0\\).\n\nExercise 8.5 Verify that for \\(\\widetilde{\\beta}_{\\mathrm{cls}}\\) defined in (8.8) that \\(\\boldsymbol{R}^{\\prime} \\widetilde{\\beta}_{\\mathrm{cls}}=\\boldsymbol{c}\\).\nExercise 8.6 Prove Theorem 8.1.\nExercise 8.7 Prove Theorem 8.2, that is, \\(\\mathbb{E}\\left[\\widetilde{\\beta}_{\\text {cls }} \\mid \\boldsymbol{X}\\right]=\\beta\\), under the assumptions of the linear regression regression model and (8.1). (Hint: Use Theorem 8.1.) Exercise 8.8 Prove Theorem 8.3.\nExercise 8.9 Prove Theorem 8.4. That is, show \\(\\mathbb{E}\\left[s_{\\mathrm{cls}}^{2} \\mid \\boldsymbol{X}\\right]=\\sigma^{2}\\) under the assumptions of the homoskedastic regression model and (8.1).\nExercise 8.10 Verify (8.22), (8.23), and that the minimum distance estimator \\(\\widetilde{\\beta}_{\\mathrm{md}}\\) with \\(\\widehat{\\boldsymbol{W}}=\\widehat{\\boldsymbol{Q}}_{X X}\\) equals the CLS estimator.\nExercise 8.11 Prove Theorem 8.6.\nExercise 8.12 Prove Theorem 8.7.\nExercise 8.13 Prove Theorem 8.8. (Hint: Use that CLS is a special case of Theorem 8.7.)\nExercise 8.14 Verify that (8.26) is \\(\\boldsymbol{V}_{\\beta}(\\boldsymbol{W})\\) with \\(\\boldsymbol{W}=\\boldsymbol{V}_{\\beta}^{-1}\\).\nExercise 8.15 Prove (8.27). Hint: Use (8.26).\nExercise 8.16 Verify (8.29), (8.30) and (8.31).\nExercise 8.17 Verify (8.32), (8.33), and (8.34).\nExercise 8.18 Suppose you have two independent samples each with \\(n\\) observations which satisfy the models \\(Y_{1}=X_{1}^{\\prime} \\beta_{1}+e_{1}\\) with \\(\\mathbb{E}\\left[X_{1} e_{1}\\right]=0\\) and \\(Y_{2}=X_{2}^{\\prime} \\beta_{2}+e_{2}\\) with \\(\\mathbb{E}\\left[X_{2} e_{2}\\right]=0\\) where \\(\\beta_{1}\\) and \\(\\beta_{2}\\) are both \\(k \\times 1\\). You estimate \\(\\beta_{1}\\) and \\(\\beta_{2}\\) by OLS on each sample, with consistent asymptotic covariance matrix estimators \\(\\widehat{\\boldsymbol{V}}_{\\beta_{1}}\\) and \\(\\widehat{\\boldsymbol{V}}_{\\beta_{2}}\\). Consider efficient minimum distance estimation under the restriction \\(\\beta_{1}=\\beta_{2}\\).\n\nFind the estimator \\(\\widetilde{\\beta}\\) of \\(\\beta=\\beta_{1}=\\beta_{2}\\).\nFind the asymptotic distribution of \\(\\widetilde{\\beta}\\).\nHow would you approach the problem if the sample sizes are different, say \\(n_{1}\\) and \\(n_{2}\\) ?\n\nExercise 8.19 Use the cps09mar dataset and the subsample of white male Hispanics.\n\nEstimate the regression\n\n\\[\n\\begin{aligned}\n& \\widehat{\\log (\\text { wage })}=\\beta_{1} \\text { education }+\\beta_{2} \\text { experience }+\\beta_{3} \\text { experience }^{2} / 100+\\beta_{4} \\text { married }_{1} \\\\\n& +\\beta_{5} \\text { married }_{2}+\\beta_{6} \\text { married }_{3}+\\beta_{7} \\text { widowed }+\\beta_{8} \\text { divorced }+\\beta_{9} \\text { separated }+\\beta_{10}\n\\end{aligned}\n\\]\nwhere married \\(_{1}\\), married \\(_{2}\\), and married \\(_{3}\\) are the first three marital codes listed in Section 3.22.\n\nEstimate the equation by CLS imposing the constraints \\(\\beta_{4}=\\beta_{7}\\) and \\(\\beta_{8}=\\beta_{9}\\). Report the estimates and standard errors.\nEstimate the equation using efficient minimum distance imposing the same constraints. Report the estimates and standard errors.\nUnder what constraint on the coefficients is the wage equation non-decreasing in experience for experience up to 50 ?\nEstimate the equation imposing \\(\\beta_{4}=\\beta_{7}, \\beta_{8}=\\beta_{9}\\), and the inequality from part (d). Exercise 8.20 Take the model\n\n\\[\n\\begin{aligned}\nY &=m(X)+e \\\\\nm(x) &=\\beta_{0}+\\beta_{1} x+\\beta_{2} x^{2}+\\cdots+\\beta_{p} x^{p} \\\\\n\\mathbb{E}\\left[X^{j} e\\right] &=0, \\quad j=0, \\ldots, p \\\\\ng(x) &=\\frac{d}{d x} m(x)\n\\end{aligned}\n\\]\nwith i.i.d. observations \\(\\left(Y_{i}, X_{i}\\right), i=1, \\ldots, n\\). The order of the polynomial \\(p\\) is known.\n\nHow should we interpret the function \\(m(x)\\) given the projection assumption? How should we interpret \\(g(x)\\) ? (Briefly)\nDescribe an estimator \\(\\widehat{g}(x)\\) of \\(g(x)\\).\nFind the asymptotic distribution of \\(\\sqrt{n}(\\widehat{g}(x)-g(x))\\) as \\(n \\rightarrow \\infty\\).\nShow how to construct an asymptotic 95% confidence interval for \\(g(x)\\) (for a single \\(x\\) ).\nAssume \\(p=2\\). Describe how to estimate \\(g(x)\\) imposing the constraint that \\(m(x)\\) is concave.\nAssume \\(p=2\\). Describe how to estimate \\(g(x)\\) imposing the constraint that \\(m(u)\\) is increasing on the region \\(u \\in\\left[x_{L}, x_{U}\\right]\\)\n\nExercise 8.21 Take the linear model with restrictions \\(Y=X^{\\prime} \\beta+e\\) with \\(\\mathbb{E}[X e]=0\\) and \\(\\boldsymbol{R}^{\\prime} \\beta=\\boldsymbol{c}\\). Consider three estimators for \\(\\beta\\) :\n\n\\(\\widehat{\\beta}\\) the unconstrained least squares estimator\n\\(\\widetilde{\\beta}\\) the constrained least squares estimator\n\\(\\bar{\\beta}\\) the constrained efficient minimum distance estimator\n\nFor the three estimator define the residuals \\(\\widehat{e}_{i}=Y_{i}-X_{i}^{\\prime} \\widehat{\\beta}, \\widetilde{e}_{i}=Y_{i}-X_{i}^{\\prime} \\widetilde{\\beta}, \\bar{e}_{i}=Y_{i}-X_{i}^{\\prime} \\bar{\\beta}\\), and variance estimators \\(\\widehat{\\sigma}^{2}=n^{-1} \\sum_{i=1}^{n} \\widehat{e}_{i}^{2}, \\widetilde{\\sigma}^{2}=n^{-1} \\sum_{i=1}^{n} \\widetilde{e}_{i}^{2}\\), and \\(\\bar{\\sigma}^{2}=n^{-1} \\sum_{i=1}^{n} \\bar{e}_{i}^{2}\\).\n\nAs \\(\\bar{\\beta}\\) is the most efficient estimator and \\(\\widehat{\\beta}\\) the least, do you expect \\(\\bar{\\sigma}^{2}<\\widetilde{\\sigma}^{2}<\\widehat{\\sigma}^{2}\\) in large samples?\nConsider the statistic\n\n\\[\nT_{n}=\\widehat{\\sigma}^{-2} \\sum_{i=1}^{n}\\left(\\widehat{e}_{i}-\\widetilde{e}_{i}\\right)^{2} .\n\\]\nFind the asymptotic distribution for \\(T_{n}\\) when \\(\\boldsymbol{R}^{\\prime} \\beta=\\boldsymbol{c}\\) is true.\n\nDoes the result of the previous question simplify when the error \\(e_{i}\\) is homoskedastic?\n\nExercise 8.22 Take the linear model \\(Y=X_{1} \\beta_{1}+X_{2} \\beta_{2}+e\\) with \\(\\mathbb{E}[X e]=0\\). Consider the restriction \\(\\frac{\\beta_{1}}{\\beta_{2}}=2\\).\n\nFind an explicit expression for the CLS estimator \\(\\widetilde{\\beta}=\\left(\\widetilde{\\beta}_{1}, \\widetilde{\\beta}_{2}\\right)\\) of \\(\\beta=\\left(\\beta_{1}, \\beta_{2}\\right)\\) under the restriction. Your answer should be specific to the restriction. It should not be a generic formula for an abstract general restriction.\nDerive the asymptotic distribution of \\(\\widetilde{\\beta}_{1}\\) under the assumption that the restriction is true."
  },
  {
    "objectID": "chpt09-hypothesit-test.html#hypotheses",
    "href": "chpt09-hypothesit-test.html#hypotheses",
    "title": "9  Hypothesis Testing",
    "section": "9.1 Hypotheses",
    "text": "9.1 Hypotheses\nIn Chapter 8 we discussed estimation subject to restrictions, including linear restrictions (8.1), nonlinear restrictions (8.44), and inequality restrictions (8.49). In this chapter we discuss tests of such restrictions.\nHypothesis tests attempt to assess whether there is evidence contrary to a proposed restriction. Let \\(\\theta=r(\\beta)\\) be a \\(q \\times 1\\) parameter of interest where \\(r: \\mathbb{R}^{k} \\rightarrow \\Theta \\subset \\mathbb{R}^{q}\\) is some transformation. For example, \\(\\theta\\) may be a single coefficient, e.g. \\(\\theta=\\beta_{j}\\), the difference between two coefficients, e.g. \\(\\theta=\\beta_{j}-\\beta_{\\ell}\\), or the ratio of two coefficients, e.g. \\(\\theta=\\beta_{j} / \\beta_{\\ell}\\).\nA point hypothesis concerning \\(\\theta\\) is a proposed restriction such as\n\\[\n\\theta=\\theta_{0}\n\\]\nwhere \\(\\theta_{0}\\) is a hypothesized (known) value.\nMore generally, letting \\(\\beta \\in B \\subset \\mathbb{R}^{k}\\) be the parameter space, a hypothesis is a restriction \\(\\beta \\in B_{0}\\) where \\(B_{0}\\) is a proper subset of \\(B\\). This specializes to (9.1) by setting \\(B_{0}=\\left\\{\\beta \\in B: r(\\beta)=\\theta_{0}\\right\\}\\).\nIn this chapter we will focus exclusively on point hypotheses of the form (9.1) as they are the most common and relatively simple to handle.\nThe hypothesis to be tested is called the null hypothesis.\nDefinition 9.1 The null hypothesis \\(\\mathbb{M}_{0}\\) is the restriction \\(\\theta=\\theta_{0}\\) or \\(\\beta \\in B_{0}\\).\nWe often write the null hypothesis as \\(\\mathbb{M}_{0}: \\theta=\\theta_{0}\\) or \\(\\mathbb{M}_{0}: r(\\beta)=\\theta_{0}\\).\nThe complement of the null hypothesis (the collection of parameter values which do not satisfy the null hypothesis) is called the alternative hypothesis.\nDefinition 9.2 The alternative hypothesis \\(\\mathbb{M}_{1}\\) is the set \\(\\left\\{\\theta \\in \\Theta: \\theta \\neq \\theta_{0}\\right\\}\\) or \\(\\left\\{\\beta \\in B: \\beta \\notin B_{0}\\right\\}\\) We often write the alternative hypothesis as \\(\\mathbb{M}_{1}: \\theta \\neq \\theta_{0}\\) or \\(\\mathbb{M}_{1}: r(\\beta) \\neq \\theta_{0}\\). For simplicity, we often refer to the hypotheses as “the null” and “the alternative”. Figure 9.1(a) illustrates the division of the parameter space into null and alternative hypotheses.\n\n\nNull and Alternative Hypotheses\n\n\n\nAcceptance and Rejection Regions\n\nFigure 9.1: Hypothesis Testing\nIn hypothesis testing, we assume that there is a true (but unknown) value of \\(\\theta\\) and this value either satisfies \\(\\mathbb{M}_{0}\\) or does not satisfy \\(\\mathbb{M}_{0}\\). The goal of hypothesis testing is to assess whether or not \\(\\mathbb{H}_{0}\\) is true by asking if \\(\\mathbb{M}_{0}\\) is consistent with the observed data.\nTo be specific, take our example of wage determination and consider the question: Does union membership affect wages? We can turn this into a hypothesis test by specifying the null as the restriction that a coefficient on union membership is zero in a wage regression. Consider, for example, the estimates reported in Table 4.1. The coefficient for “Male Union Member” is \\(0.095\\) (a wage premium of \\(9.5 %\\) ) and the coefficient for “Female Union Member” is \\(0.022\\) (a wage premium of \\(2.2 %\\) ). These are estimates, not the true values. The question is: Are the true coefficients zero? To answer this question the testing method asks the question: Are the observed estimates compatible with the hypothesis, in the sense that the deviation from the hypothesis can be reasonably explained by stochastic variation? Or are the observed estimates incompatible with the hypothesis, in the sense that that the observed estimates would be highly unlikely if the hypothesis were true?"
  },
  {
    "objectID": "chpt09-hypothesit-test.html#acceptance-and-rejection",
    "href": "chpt09-hypothesit-test.html#acceptance-and-rejection",
    "title": "9  Hypothesis Testing",
    "section": "9.2 Acceptance and Rejection",
    "text": "9.2 Acceptance and Rejection\nA hypothesis test either accepts the null hypothesis or rejects the null hypothesis in favor of the alternative hypothesis. We can describe these two decisions as “Accept \\(\\mathbb{H}_{0}\\)” and “Reject \\(\\mathbb{H}_{0}\\)”. In the example given in the previous section the decision is either to accept the hypothesis that union membership does not affect wages, or to reject the hypothesis in favor of the alternative that union membership does affect wages.\nThe decision is based on the data and so is a mapping from the sample space to the decision set. This splits the sample space into two regions \\(S_{0}\\) and \\(S_{1}\\) such that if the observed sample falls into \\(S_{0}\\) we accept \\(\\mathbb{M}_{0}\\), while if the sample falls into \\(S_{1}\\) we reject \\(\\mathbb{M}_{0}\\). The set \\(S_{0}\\) is called the acceptance region and the set \\(S_{1}\\) the rejection or critical region.\nIt is convenient to express this mapping as a real-valued function called a test statistic\n\\[\nT=T\\left(\\left(Y_{1}, X_{1}\\right), \\ldots,\\left(Y_{n}, X_{n}\\right)\\right)\n\\]\nrelative to a critical value \\(c\\). The hypothesis test then consists of the decision rule:\n\nAccept \\(\\mathbb{H}_{0}\\) if \\(T \\leq c\\).\nReject \\(\\mathbb{M}_{0}\\) if \\(T>c\\).\n\nFigure 9.1(b) illustrates the division of the sample space into acceptance and rejection regions.\nA test statistic \\(T\\) should be designed so that small values are likely when \\(\\mathbb{H}_{0}\\) is true and large values are likely when \\(\\mathbb{M}_{1}\\) is true. There is a well developed statistical theory concerning the design of optimal tests. We will not review that theory here, but instead refer the reader to Lehmann and Romano (2005). In this chapter we will summarize the main approaches to the design of test statistics.\nThe most commonly used test statistic is the absolute value of the t-statistic\n\\[\nT=\\left|T\\left(\\theta_{0}\\right)\\right|\n\\]\nwhere\n\\[\nT(\\theta)=\\frac{\\widehat{\\theta}-\\theta}{s(\\widehat{\\theta})}\n\\]\nis the t-statistic from (7.33), \\(\\widehat{\\theta}\\) is a point estimator, and \\(s(\\widehat{\\theta})\\) its standard error. \\(T\\) is an appropriate statistic when testing hypotheses on individual coefficients or real-valued parameters \\(\\theta=h(\\beta)\\) and \\(\\theta_{0}\\) is the hypothesized value. Quite typically \\(\\theta_{0}=0\\), as interest focuses on whether or not a coefficient equals zero, but this is not the only possibility. For example, interest may focus on whether an elasticity \\(\\theta\\) equals 1 , in which case we may wish to test \\(\\mathbb{H}_{0}: \\theta=1\\)."
  },
  {
    "objectID": "chpt09-hypothesit-test.html#type-i-error",
    "href": "chpt09-hypothesit-test.html#type-i-error",
    "title": "9  Hypothesis Testing",
    "section": "9.3 Type I Error",
    "text": "9.3 Type I Error\nA false rejection of the null hypothesis \\(\\mathbb{H}_{0}\\) (rejecting \\(\\mathbb{M}_{0}\\) when \\(\\mathbb{H}_{0}\\) is true) is called a Type I error. The probability of a Type I error is called the size of the test.\n\\[\n\\mathbb{P}\\left[\\text { Reject } \\mathbb{H}_{0} \\mid \\mathbb{H}_{0} \\text { true }\\right]=\\mathbb{P}\\left[T>c \\mid \\mathbb{H}_{0} \\text { true }\\right] .\n\\]\nThe uniform size of the test is the supremum of (9.4) across all data distributions which satisfy \\(\\mathbb{H}_{0}\\). A primary goal of test construction is to limit the incidence of Type I error by bounding the size of the test.\nFor the reasons discussed in Chapter 7 , in typical econometric models the exact sampling distributions of estimators and test statistics are unknown and hence we cannot explicitly calculate (9.4). Instead, we typically rely on asymptotic approximations. Suppose that the test statistic has an asymptotic distribution under \\(\\mathbb{H}_{0}\\). That is, when \\(\\mathbb{H}_{0}\\) is true\n\\[\nT \\longrightarrow \\underset{d}{\\xi}\n\\]\nas \\(n \\rightarrow \\infty\\) for some continuously-distributed random variable \\(\\xi\\). This is not a substantive restriction as most conventional econometric tests satisfy (9.5). Let \\(G(u)=\\mathbb{P}[\\xi \\leq u]\\) denote the distribution of \\(\\xi\\). We call \\(\\xi\\) (or \\(G\\) ) the asymptotic null distribution. It is desirable to design test statistics \\(T\\) whose asymptotic null distribution \\(G\\) is known and does not depend on unknown parameters. In this case we say that \\(T\\) is asymptotically pivotal.\nFor example, if the test statistic equals the absolute \\(t\\)-statistic from (9.2), then we know from Theorem \\(7.11\\) that if \\(\\theta=\\theta_{0}\\) (that is, the null hypothesis holds), then \\(T \\underset{d}{\\rightarrow}|Z|\\) as \\(n \\rightarrow \\infty\\) where \\(Z \\sim \\mathrm{N}(0,1)\\). This means that \\(G(u)=\\mathbb{P}[|Z| \\leq u]=2 \\Phi(u)-1\\), the distribution of the absolute value of the standard normal as shown in (7.34). This distribution does not depend on unknowns and is pivotal.\nWe define the asymptotic size of the test as the asymptotic probability of a Type I error:\n\\[\n\\lim _{n \\rightarrow \\infty} \\mathbb{P}\\left[T>c \\mid \\mathbb{M}_{0} \\text { true }\\right]=\\mathbb{P}[\\xi>c]=1-G(c) .\n\\]\nWe see that the asymptotic size of the test is a simple function of the asymptotic null distribution \\(G\\) and the critical value \\(c\\). For example, the asymptotic size of a test based on the absolute t-statistic with critical value \\(c\\) is \\(2(1-\\Phi(c))\\).\nIn the dominant approach to hypothesis testing the researcher pre-selects a significance level \\(\\alpha \\epsilon\\) \\((0,1)\\) and then selects \\(c\\) so the asymptotic size is no larger than \\(\\alpha\\). When the asymptotic null distribution \\(G\\) is pivotal we accomplish this by setting \\(c\\) equal to the \\(1-\\alpha\\) quantile of the distribution \\(G\\). (If the distribution \\(G\\) is not pivotal more complicated methods must be used.) We call \\(c\\) the asymptotic critical value because it has been selected from the asymptotic null distribution. For example, since \\(2(1-\\Phi(1.96))=0.05\\) it follows that the \\(5 %\\) asymptotic critical value for the absolute t-statistic is \\(c=1.96\\). Calculation of normal critical values is done numerically in statistical software. For example, in MATLAB the command is norminv \\((1-\\alpha / 2)\\)."
  },
  {
    "objectID": "chpt09-hypothesit-test.html#t-tests",
    "href": "chpt09-hypothesit-test.html#t-tests",
    "title": "9  Hypothesis Testing",
    "section": "9.4 t tests",
    "text": "9.4 t tests\nAs we mentioned earlier, the most common test of the one-dimensional hypothesis \\(\\mathbb{H}_{0}: \\theta=\\theta_{0} \\in \\mathbb{R}\\) against the alternative \\(\\mathbb{M}_{1}: \\theta \\neq \\theta_{0}\\) is the absolute value of the \\(\\mathrm{t}\\)-statistic (9.3). We now formally state its asymptotic null distribution, which is a simple application of Theorem 7.11.\nTheorem 9.1 Under Assumptions 7.2, 7.3, and \\(\\mathbb{H}_{0}: \\theta=\\theta_{0} \\in \\mathbb{R}, T\\left(\\theta_{0}\\right) \\underset{d}{\\longrightarrow} Z \\sim\\) \\(\\mathrm{N}(0,1)\\). For \\(c\\) satisfying \\(\\alpha=2(1-\\Phi(c)), \\mathbb{P}\\left[\\left|T\\left(\\theta_{0}\\right)\\right|>c \\mid \\mathbb{H}_{0}\\right] \\rightarrow \\alpha\\), and the test “Reject \\(\\mathbb{H}_{0}\\) if \\(\\left|T\\left(\\theta_{0}\\right)\\right|>c\\)” has asymptotic size \\(\\alpha\\).\nTheorem 9.1 shows that asymptotic critical values can be taken from the normal distribution. As in our discussion of asymptotic confidence intervals (Section 7.13) the critical value could alternatively be taken from the student \\(t\\) distribution, which would be the exact test in the normal regression model (Section 5.12). Indeed, \\(t\\) critical values are the default in packages such as Stata. Since the critical values from the student \\(t\\) distribution are (slightly) larger than those from the normal distribution, student \\(t\\) critical values slightly decrease the rejection probability of the test. In practical applications the difference is typically unimportant unless the sample size is quite small (in which case the asymptotic approximation should be questioned as well).\nThe alternative hypothesis \\(\\theta \\neq \\theta_{0}\\) is sometimes called a “two-sided” alternative. In contrast, sometimes we are interested in testing for one-sided alternatives such as \\(\\mathbb{M}_{1}: \\theta>\\theta_{0}\\) or \\(\\mathbb{H}_{1}: \\theta<\\theta_{0}\\). Tests of \\(\\theta=\\theta_{0}\\) against \\(\\theta>\\theta_{0}\\) or \\(\\theta<\\theta_{0}\\) are based on the signed t-statistic \\(T=T\\left(\\theta_{0}\\right)\\). The hypothesis \\(\\theta=\\theta_{0}\\) is rejected in favor of \\(\\theta>\\theta_{0}\\) if \\(T>c\\) where \\(c\\) satisfies \\(\\alpha=1-\\Phi(c)\\). Negative values of \\(T\\) are not taken as evidence against \\(\\mathbb{M}_{0}\\), as point estimates \\(\\widehat{\\theta}\\) less than \\(\\theta_{0}\\) do not point to \\(\\theta>\\theta_{0}\\). Since the critical values are taken from the single tail of the normal distribution they are smaller than for two-sided tests. Specifically, the asymptotic \\(5 %\\) critical value is \\(c=1.645\\). Thus, we reject \\(\\theta=\\theta_{0}\\) in favor of \\(\\theta>\\theta_{0}\\) if \\(T>1.645\\).\nConversely, tests of \\(\\theta=\\theta_{0}\\) against \\(\\theta<\\theta_{0}\\) reject \\(\\mathbb{M}_{0}\\) for negative t-statistics, e.g. if \\(T<-c\\). Large positive values of \\(T\\) are not evidence for \\(\\mathbb{H}_{1}: \\theta<\\theta_{0}\\). An asymptotic \\(5 %\\) test rejects if \\(T<-1.645\\).\nThere seems to be an ambiguity. Should we use the two-sided critical value \\(1.96\\) or the one-sided critical value 1.645? The answer is that in most cases the two-sided critical value is appropriate. We should use the one-sided critical values only when the parameter space is known to satisfy a one-sided restriction such as \\(\\theta \\geq \\theta_{0}\\). This is when the test of \\(\\theta=\\theta_{0}\\) against \\(\\theta>\\theta_{0}\\) makes sense. If the restriction \\(\\theta \\geq \\theta_{0}\\) is not known a priori then imposing this restriction to test \\(\\theta=\\theta_{0}\\) against \\(\\theta>\\theta_{0}\\) does not makes sense. Since linear regression coefficients typically do not have a priori sign restrictions, the standard convention is to use two-sided critical values.\nThis may seem contrary to the way testing is presented in statistical textbooks which often focus on one-sided alternative hypotheses. The latter focus is primarily for pedagogy as the one-sided theoretical problem is cleaner and easier to understand."
  },
  {
    "objectID": "chpt09-hypothesit-test.html#type-ii-error-and-power",
    "href": "chpt09-hypothesit-test.html#type-ii-error-and-power",
    "title": "9  Hypothesis Testing",
    "section": "9.5 Type II Error and Power",
    "text": "9.5 Type II Error and Power\nA false acceptance of the null hypothesis \\(\\mathbb{H}_{0}\\) (accepting \\(\\mathbb{M}_{0}\\) when \\(\\mathbb{H}_{1}\\) is true) is called a Type II error. The rejection probability under the alternative hypothesis is called the power of the test, and equals 1 minus the probability of a Type II error:\n\\[\n\\pi(\\theta)=\\mathbb{P}\\left[\\text { Reject } \\mathbb{H}_{0} \\mid \\mathbb{H}_{1} \\text { true }\\right]=\\mathbb{P}\\left[T>c \\mid \\mathbb{M}_{1} \\text { true }\\right] .\n\\]\nWe call \\(\\pi(\\theta)\\) the power function and is written as a function of \\(\\theta\\) to indicate its dependence on the true value of the parameter \\(\\theta\\).\nIn the dominant approach to hypothesis testing the goal of test construction is to have high power subject to the constraint that the size of the test is lower than the pre-specified significance level. Generally, the power of a test depends on the true value of the parameter \\(\\theta\\), and for a well-behaved test the power is increasing both as \\(\\theta\\) moves away from the null hypothesis \\(\\theta_{0}\\) and as the sample size \\(n\\) increases.\nGiven the two possible states of the world \\(\\left(\\mathbb{M}_{0}\\right.\\) or \\(\\left.\\mathbb{H}_{1}\\right)\\) and the two possible decisions (Accept \\(\\mathbb{M}_{0}\\) or Reject \\(\\mathbb{M}_{0}\\) ) there are four possible pairings of states and decisions as is depicted in Table 9.1.\nTable 9.1: Hypothesis Testing Decisions\n\n\n\n\n\n\n\n\n\n{Accept \\(\\mathbb{H}_{0}\\)\n{Reject \\(\\mathbb{M}_{0}\\)\n\n\n\n\n\\(\\mathbb{M}_{0}\\) true\nCorrect Decision\nType I Error\n\n\n\\(\\mathbb{H}_{1}\\) true\nType II Error\nCorrect Decision\n\n\n\nGiven a test statistic \\(T\\), increasing the critical value \\(c\\) increases the acceptance region \\(S_{0}\\) while decreasing the rejection region \\(S_{1}\\). This decreases the likelihood of a Type I error (decreases the size) but increases the likelihood of a Type II error (decreases the power). Thus the choice of \\(c\\) involves a trade-off between size and the power. This is why the significance level \\(\\alpha\\) of the test cannot be set arbitrarily small. Otherwise the test will not have meaningful power.\nIt is important to consider the power of a test when interpreting hypothesis tests as an overly narrow focus on size can lead to poor decisions. For example, it is easy to design a test which has perfect size yet has trivial power. Specifically, for any hypothesis we can use the following test: Generate a random variable \\(U \\sim U[0,1]\\) and reject \\(\\mathbb{M}_{0}\\) if \\(U<\\alpha\\). This test has exact size of \\(\\alpha\\). Yet the test also has power precisely equal to \\(\\alpha\\). When the power of a test equals the size we say that the test has trivial power. Nothing is learned from such a test."
  },
  {
    "objectID": "chpt09-hypothesit-test.html#statistical-significance",
    "href": "chpt09-hypothesit-test.html#statistical-significance",
    "title": "9  Hypothesis Testing",
    "section": "9.6 Statistical Significance",
    "text": "9.6 Statistical Significance\nTesting requires a pre-selected choice of significance level \\(\\alpha\\) yet there is no objective scientific basis for choice of \\(\\alpha\\). Nevertheless, the common practice is to set \\(\\alpha=0.05\\) (5%). Alternative common values are \\(\\alpha=0.10(10 %)\\) and \\(\\alpha=0.01(1 %)\\). These choices are somewhat the by-product of traditional tables of critical values and statistical software.\nThe informal reasoning behind the \\(5 %\\) critical value is to ensure that Type I errors should be relatively unlikely - that the decision “Reject \\(\\mathbb{H}_{0}\\)” has scientific strength - yet the test retains power against reasonable alternatives. The decision “Reject \\(\\mathbb{M}_{0}\\)” means that the evidence is inconsistent with the null hypothesis in the sense that it is relatively unlikely ( 1 in 20) that data generated by the null hypothesis would yield the observed test result.\nIn contrast, the decision “Accept \\(\\mathbb{H}_{0}\\)” is not a strong statement. It does not mean that the evidence supports \\(\\mathbb{M}_{0}\\), only that there is insufficient evidence to reject \\(\\mathbb{M}_{0}\\). Because of this it is more accurate to use the label “Do not Reject \\(\\mathbb{M}_{0}\\)” instead of “Accept \\(\\mathbb{H}_{0}\\)”.\nWhen a test rejects \\(\\mathbb{M}_{0}\\) at the \\(5 %\\) significance level it is common to say that the statistic is statistically significant and if the test accepts \\(\\mathbb{M}_{0}\\) it is common to say that the statistic is not statistically significant or that it is statistically insignificant. It is helpful to remember that this is simply a compact way of saying “Using the statistic \\(T\\) the hypothesis \\(\\mathbb{H}_{0}\\) can [cannot] be rejected at the asymptotic \\(5 %\\) level.” Furthermore, when the null hypothesis \\(\\mathbb{M}_{0}: \\theta=0\\) is rejected it is common to say that the coefficient \\(\\theta\\) is statistically significant, because the test has rejected the hypothesis that the coefficient is equal to zero.\nLet us return to the example about the union wage premium as measured in Table 4.1. The absolute \\(\\mathrm{t}\\)-statistic for the coefficient on “Male Union Member” is \\(0.095 / 0.020=4.7\\), which is greater than the \\(5 %\\) asymptotic critical value of \\(1.96\\). Therefore we reject the hypothesis that union membership does not affect wages for men. In this case we can say that union membership is statistically significant for men. However, the absolute t-statistic for the coefficient on “Female Union Member” is \\(0.023 / 0.020=1.2\\), which is less than \\(1.96\\) and therefore we do not reject the hypothesis that union membership does not affect wages for women. In this case we find that membership for women is not statistically significant.\nWhen a test accepts a null hypothesis (when a test is not statistically significant) a common misinterpretation is that this is evidence that the null hypothesis is true. This is incorrect. Failure to reject is by itself not evidence. Without an analysis of power we do not know the likelihood of making a Type II error and thus are uncertain. In our wage example it would be a mistake to write that “the regression finds that female union membership has no effect on wages”. This is an incorrect and most unfortunate interpretation. The test has failed to reject the hypothesis that the coefficient is zero but that does not mean that the coefficient is actually zero.\nWhen a test rejects a null hypothesis (when a test is statistically significant) it is strong evidence against the hypothesis (because if the hypothesis were true then rejection is an unlikely event). Rejection should be taken as evidence against the null hypothesis. However, we can never conclude that the null hypothesis is indeed false as we cannot exclude the possibility that we are making a Type I error.\nPerhaps more importantly, there is an important distinction between statistical and economic significance. If we correctly reject the hypothesis \\(\\mathbb{M}_{0}: \\theta=0\\) it means that the true value of \\(\\theta\\) is non-zero. This includes the possibility that \\(\\theta\\) may be non-zero but close to zero in magnitude. This only makes sense if we interpret the parameters in the context of their relevant models. In our wage regression example we might consider wage effects of \\(1 %\\) magnitude or less as being “close to zero”. In a log wage regression this corresponds to a dummy variable with a coefficient less than \\(0.01\\). If the standard error is sufficiently small (less than \\(0.005\\) ) then a coefficient estimate of \\(0.01\\) will be statistically significant but not economically significant. This occurs frequently in applications with very large sample sizes where standard errors can be quite small.\nThe solution is to focus whenever possible on confidence intervals and the economic meaning of the coefficients. For example, if the coefficient estimate is \\(0.005\\) with a standard error of \\(0.002\\) then a \\(95 %\\) confidence interval would be \\([0.001,0.009]\\) indicating that the true effect is likely between \\(0 %\\) and \\(1 %\\), and hence is slightly positive but small. This is much more informative than the misleading statement “the effect is statistically positive”."
  },
  {
    "objectID": "chpt09-hypothesit-test.html#p-values",
    "href": "chpt09-hypothesit-test.html#p-values",
    "title": "9  Hypothesis Testing",
    "section": "9.7 P-Values",
    "text": "9.7 P-Values\nContinuing with the wage regression estimates reported in Table 4.1, consider another question: Does marriage status affect wages? To test the hypothesis that marriage status has no effect on wages, we examine the t-statistics for the coefficients on “Married Male” and “Married Female” in Table 4.1, which are \\(0.211 / 0.010=22\\) and \\(0.016 / 0.010=1.7\\), respectively. The first exceeds the asymptotic \\(5 %\\) critical value of \\(1.96\\) so we reject the hypothesis for men. The second is smaller than \\(1.96\\) so we fail to reject the hypothesis for women. Taking a second look at the statistics we see that the statistic for men (22) is exceptionally high and that for women (1.7) is only slightly below the critical value. Suppose that the \\(\\mathrm{t}\\)-statistic for women were slightly increased to 2.0. This is larger than the critical value so would lead to the decision “Reject \\(\\mathbb{M}_{0}\\)” rather than “Accept \\(\\mathbb{M}_{0}\\)”. Should we really be making a different decision if the \\(\\mathrm{t}\\)-statistic is \\(2.0\\) rather than 1.7? The difference in values is small, shouldn’t the difference in the decision be also small? Thinking through these examples it seems unsatisfactory to simply report “Accept \\(\\mathbb{M}_{0}\\)” or “Reject \\(\\mathbb{H}_{0}\\)”. These two decisions do not summarize the evidence. Instead, the magnitude of the statistic \\(T\\) suggests a “degree of evidence” against \\(\\mathbb{H}_{0}\\). How can we take this into account?\nThe answer is to report what is known as the asymptotic p-value\n\\[\np=1-G(T) .\n\\]\nSince the distribution function \\(G\\) is monotonically increasing, the p-value is a monotonically decreasing function of \\(T\\) and is an equivalent test statistic. Instead of rejecting \\(\\mathbb{R}_{0}\\) at the significance level \\(\\alpha\\) if \\(T>c\\), we can reject \\(\\mathbb{M}_{0}\\) if \\(p<\\alpha\\). Thus it is sufficient to report \\(p\\), and let the reader decide. In practice, the p-value is calculated numerically. For example, in MATLAB the command is \\(2 *(1-\\operatorname{normal} c d f(\\mathrm{abs}(\\mathrm{t})))\\).\nIt is instructive to interpret \\(p\\) as the marginal significance level: the smallest value of \\(\\alpha\\) for which the test \\(T\\) “rejects” the null hypothesis. That is, \\(p=0.11\\) means that \\(T\\) rejects \\(\\mathbb{H}_{0}\\) for all significance levels greater than \\(0.11\\), but fails to reject \\(\\mathbb{M}_{0}\\) for significance levels less than \\(0.11\\).\nFurthermore, the asymptotic p-value has a very convenient asymptotic null distribution. Since \\(T-\\vec{d}\\) \\(\\xi\\) under \\(\\mathbb{M}_{0}\\), then \\(p=1-G(T) \\underset{d}{\\longrightarrow} 1-G(\\xi)\\), which has the distribution\n\\[\n\\begin{aligned}\n\\mathbb{P}[1-G(\\xi) \\leq u] &=\\mathbb{P}[1-u \\leq G(\\xi)] \\\\\n&=1-\\mathbb{P}\\left[\\xi \\leq G^{-1}(1-u)\\right] \\\\\n&=1-G\\left(G^{-1}(1-u)\\right) \\\\\n&=1-(1-u) \\\\\n&=u,\n\\end{aligned}\n\\]\nwhich is the uniform distribution on \\([0,1]\\). (This calculation assumes that \\(G(u)\\) is strictly increasing which is true for conventional asymptotic distributions such as the normal.) Thus \\(p \\underset{d}{\\longrightarrow} U[0,1]\\). This means that the “unusualness” of \\(p\\) is easier to interpret than the “unusualness” of \\(T\\).\nAn important caveat is that the \\(\\mathrm{p}\\)-value \\(p\\) should not be interpreted as the probability that either hypothesis is true. A common mis-interpretation is that \\(p\\) is the probability “that the null hypothesis is true.” This is incorrect. Rather, \\(p\\) is the marginal significance level-a measure of the strength of information against the null hypothesis. For a t-statistic the p-value can be calculated either using the normal distribution or the student \\(t\\) distribution, the latter presented in Section 5.12. p-values calculated using the student \\(t\\) will be slightly larger, though the difference is small when the sample size is large.\nReturning to our empirical example, for the test that the coefficient on “Married Male” is zero the pvalue is \\(0.000\\). This means that it would be nearly impossible to observe a t-statistic as large as 22 when the true value of the coefficient is zero. When presented with such evidence we can say that we “strongly reject” the null hypothesis, that the test is “highly significant”, or that “the test rejects at any conventional critical value”. In contrast, the p-value for the coefficient on “Married Female” is \\(0.094\\). In this context it is typical to say that the test is “close to significant”, meaning that the p-value is larger than \\(0.05\\), but not too much larger.\nA related but inferior empirical practice is to append asterisks \\((*)\\) to coefficient estimates or test statistics to indicate the level of significance. A common practice to to append a single asterisk (\\textit{) for an estimate or test statistic which exceeds the \\(10 %\\) critical value (i.e., is significant at the \\(10 %\\) level), append a double asterisk () for a test which exceeds the \\(5 %\\) critical value, and append a triple asterisk (}) for a test which exceeds the \\(1 %\\) critical value. Such a practice can be better than a table of raw test statistics as the asterisks permit a quick interpretation of significance. On the other hand, asterisks are inferior to p-values, which are also easy and quick to interpret. The goal is essentially the same; it is wiser to report p-values whenever possible and avoid the use of asterisks.\nOur recommendation is that the best empirical practice is to compute and report the asymptotic pvalue \\(p\\) rather than simply the test statistic \\(T\\), the binary decision Accept/Reject, or appending asterisks. The p-value is a simple statistic, easy to interpret, and contains more information than the other choices.\nWe now summarize the main features of hypothesis testing.\n\nSelect a significance level \\(\\alpha\\).\nSelect a test statistic \\(T\\) with asymptotic distribution \\(T \\underset{d}{\\rightarrow} \\xi\\) under \\(\\mathbb{H}_{0}\\).\nSet the asymptotic critical value \\(c\\) so that \\(1-G(c)=\\alpha\\), where \\(G\\) is the distribution function of \\(\\xi\\).\nCalculate the asymptotic p-value \\(p=1-G(T)\\).\nReject \\(\\mathbb{R}_{0}\\) if \\(T>c\\), or equivalently \\(p<\\alpha\\).\nAccept \\(\\mathbb{H}_{0}\\) if \\(T \\leq c\\), or equivalently \\(p \\geq \\alpha\\).\nReport \\(p\\) to summarize the evidence concerning \\(\\mathbb{M}_{0}\\) versus \\(\\mathbb{M}_{1}\\)."
  },
  {
    "objectID": "chpt09-hypothesit-test.html#t-ratios-and-the-abuse-of-testing",
    "href": "chpt09-hypothesit-test.html#t-ratios-and-the-abuse-of-testing",
    "title": "9  Hypothesis Testing",
    "section": "9.8 t-ratios and the Abuse of Testing",
    "text": "9.8 t-ratios and the Abuse of Testing\nIn Section \\(4.19\\) we argued that a good applied practice is to report coefficient estimates \\(\\widehat{\\theta}\\) and standard errors \\(s(\\widehat{\\theta})\\) for all coefficients of interest in estimated models. With \\(\\widehat{\\theta}\\) and \\(s(\\widehat{\\theta})\\) the reader can easily construct confidence intervals \\([\\widehat{\\theta} \\pm 2 s(\\widehat{\\theta})]\\) and t-statistics \\(\\left(\\widehat{\\theta}-\\theta_{0}\\right) / s(\\widehat{\\theta})\\) for hypotheses of interest.\nSome applied papers (especially older ones) report t-ratios \\(T=\\widehat{\\theta} / s(\\widehat{\\theta})\\) instead of standard errors. This is poor econometric practice. While the same information is being reported (you can back out standard errors by division, e.g. \\(s(\\widehat{\\theta})=\\widehat{\\theta} / T)\\), standard errors are generally more helpful to readers than t-ratios. Standard errors help the reader focus on the estimation precision and confidence intervals, while t-ratios focus attention on statistical significance. While statistical significance is important, it is less important that the parameter estimates themselves and their confidence intervals. The focus should be on the meaning of the parameter estimates, their magnitudes, and their interpretation, not on listing which variables have significant (e.g. non-zero) coefficients. In many modern applications sample sizes are very large so standard errors can be very small. Consequently t-ratios can be large even if the coefficient estimates are economically small. In such contexts it may not be interesting to announce “The coefficient is non-zero!” Instead, what is interesting to announce is that “The coefficient estimate is economically interesting!”\nIn particular, some applied papers report coefficient estimates and t-ratios and limit their discussion of the results to describing which variables are “significant” (meaning that their t-ratios exceed 2) and the signs of the coefficient estimates. This is very poor empirical work and should be studiously avoided. It is also a recipe for banishment of your work to lower tier economics journals.\nFundamentally, the common t-ratio is a test for the hypothesis that a coefficient equals zero. This should be reported and discussed when this is an interesting economic hypothesis of interest. But if this is not the case it is distracting.\nOne problem is that standard packages, such as Stata, by default report t-statistics and p-values for every estimated coefficient. While this can be useful (as a user doesn’t need to explicitly ask to test a desired coefficient) it can be misleading as it may unintentionally suggest that the entire list of t-statistics and p-values are important. Instead, a user should focus on tests of scientifically motivated hypotheses.\nIn general, when a coefficient \\(\\theta\\) is of interest it is constructive to focus on the point estimate, its standard error, and its confidence interval. The point estimate gives our “best guess” for the value. The standard error is a measure of precision. The confidence interval gives us the range of values consistent with the data. If the standard error is large then the point estimate is not a good summary about \\(\\theta\\). The endpoints of the confidence interval describe the bounds on the likely possibilities. If the confidence interval embraces too broad a set of values for \\(\\theta\\) then the dataset is not sufficiently informative to render useful inferences about \\(\\theta\\). On the other hand if the confidence interval is tight then the data have produced an accurate estimate and the focus should be on the value and interpretation of this estimate. In contrast, the statement “the t-ratio is highly significant” has little interpretive value.\nThe above discussion requires that the researcher knows what the coefficient \\(\\theta\\) means (in terms of the economic problem) and can interpret values and magnitudes, not just signs. This is critical for good applied econometric practice.\nFor example, consider the question about the effect of marriage status on mean log wages. We had found that the effect is “highly significant” for men and “close to significant” for women. Now, let’s construct asymptotic \\(95 %\\) confidence intervals for the coefficients. The one for men is \\([0.19,0.23]\\) and that for women is \\([-0.00,0.03]\\). This shows that average wages for married men are about \\(19-23 %\\) higher than for unmarried men, which is substantial, while the difference for women is about 0-3%, which is small. These magnitudes are more informative than the results of the hypothesis tests."
  },
  {
    "objectID": "chpt09-hypothesit-test.html#wald-tests",
    "href": "chpt09-hypothesit-test.html#wald-tests",
    "title": "9  Hypothesis Testing",
    "section": "9.9 Wald Tests",
    "text": "9.9 Wald Tests\nThe t-test is appropriate when the null hypothesis is a real-valued restriction. More generally there may be multiple restrictions on the coefficient vector \\(\\beta\\). Suppose that we have \\(q>1\\) restrictions which can be written in the form (9.1). It is natural to estimate \\(\\theta=r(\\beta)\\) by the plug-in estimator \\(\\widehat{\\theta}=r(\\widehat{\\beta})\\). To test \\(\\mathbb{H}_{0}: \\theta=\\theta_{0}\\) against \\(\\mathbb{H}_{1}: \\theta \\neq \\theta_{0}\\) one approach is to measure the magnitude of the discrepancy \\(\\widehat{\\theta}-\\theta_{0}\\). As this is a vector there is more than one measure of its length. One simple measure is the weighted quadratic form known as the Wald statistic. This is (7.37) evaluated at the null hypothesis\n\\[\nW=W\\left(\\theta_{0}\\right)=\\left(\\widehat{\\theta}-\\theta_{0}\\right)^{\\prime} \\widehat{\\boldsymbol{V}}_{\\widehat{\\theta}}^{-1}\\left(\\widehat{\\theta}-\\theta_{0}\\right)\n\\]\nwhere \\(\\widehat{\\boldsymbol{V}}_{\\widehat{\\theta}}=\\widehat{\\boldsymbol{R}}^{\\prime} \\widehat{\\boldsymbol{V}}_{\\widehat{\\beta}} \\widehat{\\boldsymbol{R}}\\) is an estimator of \\(\\boldsymbol{V}_{\\widehat{\\theta}}\\) and \\(\\widehat{\\boldsymbol{R}}=\\frac{\\partial}{\\partial \\beta} r(\\widehat{\\beta})^{\\prime}\\). Notice that we can write \\(W\\) alternatively as\n\\[\nW=n\\left(\\widehat{\\theta}-\\theta_{0}\\right)^{\\prime} \\widehat{\\boldsymbol{V}}_{\\theta}^{-1}\\left(\\widehat{\\theta}-\\theta_{0}\\right)\n\\]\nusing the asymptotic variance estimator \\(\\widehat{\\boldsymbol{V}}_{\\theta}\\), or we can write it directly as a function of \\(\\widehat{\\beta}\\) as\n\\[\nW=\\left(r(\\widehat{\\beta})-\\theta_{0}\\right)^{\\prime}\\left(\\widehat{\\boldsymbol{R}}^{\\prime} \\widehat{\\boldsymbol{V}}_{\\widehat{\\beta}} \\widehat{\\boldsymbol{R}}\\right)^{-1}\\left(r(\\widehat{\\beta})-\\theta_{0}\\right) .\n\\]\nAlso, when \\(r(\\beta)=\\boldsymbol{R}^{\\prime} \\beta\\) is a linear function of \\(\\beta\\), then the Wald statistic simplifies to\n\\[\nW=\\left(\\boldsymbol{R}^{\\prime} \\widehat{\\beta}-\\theta_{0}\\right)^{\\prime}\\left(\\boldsymbol{R}^{\\prime} \\widehat{\\boldsymbol{V}}_{\\widehat{\\beta}} \\boldsymbol{R}\\right)^{-1}\\left(\\boldsymbol{R}^{\\prime} \\widehat{\\beta}-\\theta_{0}\\right) .\n\\]\nThe Wald statistic \\(W\\) is a weighted Euclidean measure of the length of the vector \\(\\widehat{\\theta}-\\theta_{0}\\). When \\(q=1\\) then \\(W=T^{2}\\), the square of the t-statistic, so hypothesis tests based on \\(W\\) and \\(|T|\\) are equivalent. The Wald statistic (9.6) is a generalization of the t-statistic to the case of multiple restrictions. As the Wald statistic is symmetric in the argument \\(\\widehat{\\theta}-\\theta_{0}\\) it treats positive and negative alternatives symmetrically. Thus the inherent alternative is always two-sided.\nAs shown in Theorem 7.13, when \\(\\beta\\) satisfies \\(r(\\beta)=\\theta_{0}\\) then \\(W \\underset{d}{\\rightarrow} \\chi_{q}^{2}\\), a chi-square random variable with \\(q\\) degrees of freedom. Let \\(G_{q}(u)\\) denote the \\(\\chi_{q}^{2}\\) distribution function. For a given significance level \\(\\alpha\\) the asymptotic critical value \\(c\\) satisfies \\(\\alpha=1-G_{q}(c)\\). For example, the \\(5 %\\) critical values for \\(q=1, q=2\\), and \\(q=3\\) are \\(3.84,5.99\\), and \\(7.82\\), respectively, and in general the level \\(\\alpha\\) critical value can be calculated in MATLAB as chi2inv \\((1-\\alpha, q)\\). An asymptotic test rejects \\(\\mathbb{M}_{0}\\) in favor of \\(\\mathbb{M}_{1}\\) if \\(W>c\\). As with t-tests, it is conventional to describe a Wald test as “significant” if \\(W\\) exceeds the \\(5 %\\) asymptotic critical value.\nTheorem 9.2 Under Assumptions 7.2, 7.3, 7.4, and \\(\\mathbb{M}_{0}: \\theta=\\theta_{0} \\in \\mathbb{R}^{q}\\), then \\(W \\vec{d}\\) \\(\\chi_{q}^{2}\\). For \\(c\\) satisfying \\(\\alpha=1-G_{q}(c), \\mathbb{P}\\left(W>c \\mid \\mathbb{H}_{0}\\right) \\longrightarrow \\alpha\\) so the test “Reject \\(\\mathbb{H}_{0}\\) if \\(W>c\\)” has asymptotic size \\(\\alpha\\).\nNotice that the asymptotic distribution in Theorem \\(9.2\\) depends solely on \\(q\\), the number of restrictions being tested. It does not depend on \\(k\\), the number of parameters estimated.\nThe asymptotic p-value for \\(W\\) is \\(p=1-G_{q}(W)\\), and this is particularly useful when testing multiple restrictions. For example, if you write that a Wald test on eight restrictions ( \\(q=8\\) ) has the value \\(W=\\) \\(11.2\\) it is difficult for a reader to assess the magnitude of this statistic unless they have quick access to a statistical table or software. Instead, if you write that the p-value is \\(p=0.19\\) (as is the case for \\(W=11.2\\) and \\(q=8\\) ) then it is simple for a reader to interpret its magnitude as “insignificant”. To calculate the asymptotic p-value for a Wald statistic in MATLAB use the command \\(1-\\operatorname{ch} i 2 c d f(w, q)\\).\nSome packages (including Stata) and papers report \\(F\\) versions of Wald statistics. For any Wald statistic \\(W\\) which tests a \\(q\\)-dimensional restriction, the \\(F\\) version of the test is\n\\[\nF=W / q .\n\\]\nWhen \\(F\\) is reported, it is conventional to use \\(F_{q, n-k}\\) critical values and \\(\\mathrm{p}\\)-values rather than \\(\\chi_{q}^{2}\\) values. The connection between Wald and F statistics is demonstrated in Section \\(9.14\\) where we show that when Wald statistics are calculated using a homoskedastic covariance matrix then \\(F=W / q\\) is identicial to the F statistic of (5.19). While there is no formal justification to using the \\(F_{q, n-k}\\) distribution for nonhomoskedastic covariance matrices, the \\(F_{q, n-k}\\) distribution provides continuity with the exact distribution theory under normality and is a bit more conservative than the \\(\\chi_{q}^{2}\\) distribution. (Furthermore, the difference is small when \\(n-k\\) is moderately large.)\nTo implement a test of zero restrictions in Stata an easy method is to use the command test X1 X2 where X1 and X2 are the names of the variables whose coefficients are hypothesized to equal zero. The \\(F\\) version of the Wald statistic is reported using the covariance matrix calculated by the method specified in the regression command. A p-value is reported, calculated using the \\(F_{q, n-k}\\) distribution.\nTo illustrate, consider the empirical results presented in Table 4.1. The hypothesis “Union membership does not affect wages” is the joint restriction that both coefficients on “Male Union Member” and “Female Union Member” are zero. We calculate the Wald statistic for this joint hypothesis and find \\(W=23\\) (or \\(F=12.5\\) ) with a p-value of \\(p=0.000\\). Thus we reject the null hypothesis in favor of the alternative that at least one of the coefficients is non-zero. This does not mean that both coefficients are non-zero, just that one of the two is non-zero. Therefore examining both the joint Wald statistic and the individual t-statistics is useful for interpretation.\nAs a second example from the same regression, take the hypothesis that married status has no effect on mean wages for women. This is the joint restriction that the coefficients on “Married Female” and “Formerly Married Female” are zero. The Wald statistic for this hypothesis is \\(W=6.4(F=3.2)\\) with a p-value of \\(0.04\\). Such a p-value is typically called “marginally significant” in the sense that it is slightly smaller than \\(0.05\\).\nThe Wald statistic was proposed by Wald (1943)."
  },
  {
    "objectID": "chpt09-hypothesit-test.html#homoskedastic-wald-tests",
    "href": "chpt09-hypothesit-test.html#homoskedastic-wald-tests",
    "title": "9  Hypothesis Testing",
    "section": "9.10 Homoskedastic Wald Tests",
    "text": "9.10 Homoskedastic Wald Tests\nIf the error is known to be homoskedastic then it is appropriate to use the homoskedastic Wald statistic (7.38) which replaces \\(\\widehat{\\boldsymbol{V}}_{\\widehat{\\theta}}\\) with the homoskedastic estimator \\(\\widehat{\\boldsymbol{V}}_{\\widehat{\\theta}}^{0}\\). This statistic equals\n\\[\n\\begin{aligned}\nW^{0} &=\\left(\\widehat{\\theta}-\\theta_{0}\\right)^{\\prime}\\left(\\widehat{\\boldsymbol{V}}_{\\widehat{\\theta}}^{0}\\right)^{-1}\\left(\\widehat{\\theta}-\\theta_{0}\\right) \\\\\n&=\\left(r(\\widehat{\\beta})-\\theta_{0}\\right)^{\\prime}\\left(\\boldsymbol{R}^{\\prime}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\widehat{\\boldsymbol{R}}\\right)^{-1}\\left(r(\\widehat{\\beta})-\\theta_{0}\\right) / s^{2} .\n\\end{aligned}\n\\]\nIn the case of linear hypotheses \\(\\mathbb{M}_{0}: \\boldsymbol{R}^{\\prime} \\beta=\\theta_{0}\\) we can write this as\n\\[\nW^{0}=\\left(\\boldsymbol{R}^{\\prime} \\widehat{\\beta}-\\theta_{0}\\right)^{\\prime}\\left(\\boldsymbol{R}^{\\prime}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\boldsymbol{R}\\right)^{-1}\\left(\\boldsymbol{R}^{\\prime} \\widehat{\\beta}-\\theta_{0}\\right) / s^{2} .\n\\]\nWe call \\(W^{0}\\) a homoskedastic Wald statistic as it is appropriate when the errors are conditionally homoskedastic.\nWhen \\(q=1\\) then \\(W^{0}=T^{2}\\), the square of the t-statistic where the latter is computed with a homoskedastic standard error. Theorem 9.3 Under Assumptions \\(7.2\\) and 7.3, \\(\\mathbb{E}\\left[e^{2} \\mid X\\right]=\\sigma^{2}>0\\), and \\(\\mathbb{M}_{0}: \\theta=\\) \\(\\theta_{0} \\in \\mathbb{R}^{q}\\), then \\(W^{0} \\underset{d}{\\longrightarrow} \\chi_{q}^{2}\\). For \\(c\\) satisfying \\(\\alpha=1-G_{q}(c), \\mathbb{P}\\left[W^{0}>c \\mid \\mathbb{H}_{0}\\right] \\longrightarrow \\alpha\\) so the test “Reject \\(\\mathbb{M}_{0}\\) if \\(W^{0}>c\\)” has asymptotic size \\(\\alpha\\)."
  },
  {
    "objectID": "chpt09-hypothesit-test.html#criterion-based-tests",
    "href": "chpt09-hypothesit-test.html#criterion-based-tests",
    "title": "9  Hypothesis Testing",
    "section": "9.11 Criterion-Based Tests",
    "text": "9.11 Criterion-Based Tests\nThe Wald statistic is based on the length of the vector \\(\\widehat{\\theta}-\\theta_{0}\\) : the discrepancy between the estimator \\(\\widehat{\\theta}=r(\\widehat{\\beta})\\) and the hypothesized value \\(\\theta_{0}\\). An alternative class of tests is based on the discrepancy between the criterion function minimized with and without the restriction.\nCriterion-based testing applies when we have a criterion function, say \\(J(\\beta)\\) with \\(\\beta \\in B\\), which is minimized for estimation, and the goal is to test \\(\\mathbb{M}_{0}: \\beta \\in B_{0}\\) versus \\(\\mathbb{M}_{1}: \\beta \\notin B_{0}\\) where \\(B_{0} \\subset \\beta\\). Minimizing the criterion function over \\(B\\) and \\(B_{0}\\) we obtain the unrestricted and restricted estimators\n\\[\n\\begin{aligned}\n&\\widehat{\\beta}=\\underset{\\beta \\in B}{\\operatorname{argmin}} J(\\beta) \\\\\n&\\widetilde{\\beta}=\\underset{\\beta \\in B_{0}}{\\operatorname{argmin}} J(\\beta) .\n\\end{aligned}\n\\]\nThe criterion-based statistic for \\(\\mathbb{H}_{0}\\) versus \\(\\mathbb{H}_{1}\\) is proportional to\n\\[\nJ=\\min _{\\beta \\in B_{0}} J(\\beta)-\\min _{\\beta \\in B} J(\\beta)=J(\\widetilde{\\beta})-J(\\widehat{\\beta}) .\n\\]\nThe criterion-based statistic \\(J\\) is sometimes called a distance statistic, a minimum-distance statistic, or a likelihood-ratio-like statistic.\nSince \\(B_{0}\\) is a subset of \\(B, J(\\widetilde{\\beta}) \\geq J(\\widehat{\\beta})\\) and thus \\(J \\geq 0\\). The statistic \\(J\\) measures the cost on the criterion of imposing the null restriction \\(\\beta \\in B_{0}\\)."
  },
  {
    "objectID": "chpt09-hypothesit-test.html#minimum-distance-tests",
    "href": "chpt09-hypothesit-test.html#minimum-distance-tests",
    "title": "9  Hypothesis Testing",
    "section": "9.12 Minimum Distance Tests",
    "text": "9.12 Minimum Distance Tests\nThe minimum distance test is based on the minimum distance criterion (8.19)\n\\[\nJ(\\beta)=n(\\widehat{\\beta}-\\beta)^{\\prime} \\widehat{\\boldsymbol{W}}(\\widehat{\\beta}-\\beta)\n\\]\nwith \\(\\widehat{\\beta}\\) the unrestricted least squares estimator. The restricted estimator \\(\\widetilde{\\beta}_{\\text {md }}\\) minimizes (9.8) subject to \\(\\beta \\in B_{0}\\). Observing that \\(J(\\widehat{\\beta})=0\\), the minimum distance statistic simplifies to\n\\[\nJ=J\\left(\\widetilde{\\beta}_{\\mathrm{md}}\\right)=n\\left(\\widehat{\\beta}-\\widetilde{\\beta}_{\\mathrm{md}}\\right)^{\\prime} \\widehat{\\boldsymbol{W}}\\left(\\widehat{\\beta}-\\widetilde{\\beta}_{\\mathrm{md}}\\right) .\n\\]\nThe efficient minimum distance estimator \\(\\widetilde{\\beta}_{\\mathrm{emd}}\\) is obtained by setting \\(\\widehat{\\boldsymbol{W}}=\\widehat{\\boldsymbol{V}}_{\\beta}^{-1}\\) in (9.8) and (9.9). The efficient minimum distance statistic for \\(\\mathbb{H}_{0}: \\beta \\in B_{0}\\) is therefore\n\\[\nJ^{*}=n\\left(\\widehat{\\beta}-\\widetilde{\\beta}_{\\mathrm{emd}}\\right)^{\\prime} \\widehat{\\boldsymbol{V}}_{\\beta}^{-1}\\left(\\widehat{\\beta}-\\widetilde{\\beta}_{\\mathrm{emd}}\\right) .\n\\]\nConsider the class of linear hypotheses \\(\\mathbb{M}_{0}: \\boldsymbol{R}^{\\prime} \\beta=\\theta_{0}\\). In this case we know from (8.25) that the efficient minimum distance estimator \\(\\widetilde{\\beta}_{\\mathrm{emd}}\\) subject to the constraint \\(\\boldsymbol{R}^{\\prime} \\beta=\\theta_{0}\\) is\n\\[\n\\widetilde{\\beta}_{\\mathrm{emd}}=\\widehat{\\beta}-\\widehat{\\boldsymbol{V}}_{\\beta} \\boldsymbol{R}\\left(\\boldsymbol{R}^{\\prime} \\widehat{\\boldsymbol{V}}_{\\beta} \\boldsymbol{R}\\right)^{-1}\\left(\\boldsymbol{R}^{\\prime} \\widehat{\\beta}-\\theta_{0}\\right)\n\\]\nand thus\n\\[\n\\widehat{\\beta}-\\widetilde{\\beta}_{\\mathrm{emd}}=\\widehat{\\boldsymbol{V}}_{\\beta} \\boldsymbol{R}\\left(\\boldsymbol{R}^{\\prime} \\widehat{\\boldsymbol{V}}_{\\beta} \\boldsymbol{R}\\right)^{-1}\\left(\\boldsymbol{R}^{\\prime} \\widehat{\\beta}-\\theta_{0}\\right) .\n\\]\nSubstituting into (9.10) we find\n\\[\n\\begin{aligned}\nJ^{*} &=n\\left(\\boldsymbol{R}^{\\prime} \\widehat{\\beta}-\\theta_{0}\\right)^{\\prime}\\left(\\boldsymbol{R}^{\\prime} \\widehat{\\boldsymbol{V}}_{\\beta} \\boldsymbol{R}\\right)^{-1} \\boldsymbol{R}^{\\prime} \\widehat{\\boldsymbol{V}}_{\\beta} \\widehat{\\boldsymbol{V}}_{\\boldsymbol{\\beta}}^{-1} \\widehat{\\boldsymbol{V}}_{\\beta} \\boldsymbol{R}\\left(\\boldsymbol{R}^{\\prime} \\widehat{\\boldsymbol{V}}_{\\beta} \\boldsymbol{R}\\right)^{-1}\\left(\\boldsymbol{R}^{\\prime} \\widehat{\\beta}-\\theta_{0}\\right) \\\\\n&=n\\left(\\boldsymbol{R}^{\\prime} \\widehat{\\beta}-\\theta_{0}\\right)^{\\prime}\\left(\\boldsymbol{R}^{\\prime} \\widehat{\\boldsymbol{V}}_{\\beta} \\boldsymbol{R}\\right)^{-1}\\left(\\boldsymbol{R}^{\\prime} \\widehat{\\beta}-\\theta_{0}\\right) \\\\\n&=W,\n\\end{aligned}\n\\]\nwhich is the Wald statistic (9.6).\nThus for linear hypotheses \\(\\mathbb{H}_{0}: \\boldsymbol{R}^{\\prime} \\beta=\\theta_{0}\\), the efficient minimum distance statistic \\(J^{*}\\) is identical to the Wald statistic (9.6). For nonlinear hypotheses, however, the Wald and minimum distance statistics are different.\nNewey and West (1987a) established the asymptotic null distribution of \\(J^{*}\\).\nTheorem 9.4 Under Assumptions \\(7.2,7.3,7.4\\), and \\(\\mathbb{H}_{0}: \\theta=\\theta_{0} \\in \\mathbb{R}^{q}, J^{*} \\underset{d}{\\longrightarrow} \\chi_{q}^{2}\\).\nTesting using the minimum distance statistic \\(J^{*}\\) is similar to testing using the Wald statistic \\(W\\). Critical values and p-values are computed using the \\(\\chi_{q}^{2}\\) distribution. \\(\\mathbb{H}_{0}\\) is rejected in favor of \\(\\mathbb{H}_{1}\\) if \\(J^{*}\\) exceeds the level \\(\\alpha\\) critical value, which can be calculated in MATLAB as chi2inv \\((1-\\alpha, q)\\). The asymptotic pvalue is \\(p=1-G_{q}\\left(J^{*}\\right)\\). In MATLAB, use the command \\(1-\\operatorname{chi} 2 \\mathrm{cdf}(\\mathrm{J}, \\mathrm{q})\\).\nWe now demonstrate Theorem 9.4. The conditions of Theorem \\(8.10\\) hold, because \\(\\mathbb{H}_{0}\\) implies Assumption 8.1. From (8.54) with \\(\\widehat{\\boldsymbol{W}}=\\widehat{\\boldsymbol{V}}_{\\beta}\\), we see that\n\\[\n\\begin{aligned}\n\\sqrt{n}\\left(\\widehat{\\beta}-\\widetilde{\\beta}_{\\mathrm{emd}}\\right) &=\\widehat{\\boldsymbol{V}}_{\\beta} \\widehat{\\boldsymbol{R}}\\left(\\boldsymbol{R}_{n}^{* \\prime} \\widehat{\\boldsymbol{V}}_{\\beta} \\widehat{\\boldsymbol{R}}\\right)^{-1} \\boldsymbol{R}_{n}^{* \\prime} \\sqrt{n}(\\widehat{\\beta}-\\beta) \\\\\n& \\underset{d}{\\longrightarrow} \\boldsymbol{V}_{\\beta} \\boldsymbol{R}\\left(\\boldsymbol{R}^{\\prime} \\boldsymbol{V}_{\\beta} \\boldsymbol{R}\\right)^{-1} \\boldsymbol{R}^{\\prime} \\mathrm{N}\\left(0, \\boldsymbol{V}_{\\beta}\\right)=\\boldsymbol{V}_{\\beta} \\boldsymbol{R} Z\n\\end{aligned}\n\\]\nwhere \\(Z \\sim \\mathrm{N}\\left(0,\\left(\\boldsymbol{R}^{\\prime} \\boldsymbol{V}_{\\beta} \\boldsymbol{R}\\right)^{-1}\\right)\\). Thus\n\\[\nJ^{*}=n\\left(\\widehat{\\beta}-\\widetilde{\\beta}_{\\mathrm{emd}}\\right)^{\\prime} \\widehat{\\boldsymbol{V}}_{\\beta}^{-1}\\left(\\widehat{\\beta}-\\widetilde{\\beta}_{\\mathrm{emd}}\\right) \\underset{d}{\\longrightarrow} Z^{\\prime} \\boldsymbol{R}^{\\prime} \\boldsymbol{V}_{\\beta} \\boldsymbol{V}_{\\beta}^{-1} \\boldsymbol{V}_{\\beta} \\boldsymbol{R} Z=Z^{\\prime}\\left(\\boldsymbol{R}^{\\prime} \\boldsymbol{V}_{\\beta} \\boldsymbol{R}\\right) Z=\\chi_{q}^{2}\n\\]\nas claimed."
  },
  {
    "objectID": "chpt09-hypothesit-test.html#minimum-distance-tests-under-homoskedasticity",
    "href": "chpt09-hypothesit-test.html#minimum-distance-tests-under-homoskedasticity",
    "title": "9  Hypothesis Testing",
    "section": "9.13 Minimum Distance Tests Under Homoskedasticity",
    "text": "9.13 Minimum Distance Tests Under Homoskedasticity\nIf we set \\(\\widehat{\\boldsymbol{W}}=\\widehat{\\boldsymbol{Q}}_{X X} / s^{2}\\) in (9.8) we obtain the criterion (8.20)\n\\[\nJ^{0}(\\beta)=n(\\widehat{\\beta}-\\beta)^{\\prime} \\widehat{\\boldsymbol{Q}}_{X X}(\\widehat{\\beta}-\\beta) / s^{2} .\n\\]\nA minimum distance statistic for \\(\\mathbb{\\Perp}_{0}: \\beta \\in B_{0}\\) is\n\\[\nJ^{0}=\\min _{\\beta \\in B_{0}} J^{0}(\\beta) .\n\\]\nEquation (8.21) showed that \\(\\operatorname{SSE}(\\beta)=n \\widehat{\\sigma}^{2}+s^{2} J^{0}(\\beta)\\). So the minimizers of \\(\\operatorname{SSE}(\\beta)\\) and \\(J^{0}(\\beta)\\) are identical. Thus the constrained minimizer of \\(J^{0}(\\beta)\\) is constrained least squares\n\\[\n\\widetilde{\\beta}_{\\text {cls }}=\\underset{\\beta \\in B_{0}}{\\operatorname{argmin}} J^{0}(\\beta)=\\underset{\\beta \\in B_{0}}{\\operatorname{argmin}} \\operatorname{SSE}(\\beta)\n\\]\nand therefore\n\\[\nJ_{n}^{0}=J_{n}^{0}\\left(\\widetilde{\\beta}_{\\mathrm{cls}}\\right)=n\\left(\\widehat{\\beta}-\\widetilde{\\beta}_{\\mathrm{cls}}\\right)^{\\prime} \\widehat{\\boldsymbol{Q}}_{X X}\\left(\\widehat{\\beta}-\\widetilde{\\beta}_{\\mathrm{cls}}\\right) / s^{2} .\n\\]\nIn the special case of linear hypotheses \\(\\mathbb{M}_{0}: \\boldsymbol{R}^{\\prime} \\beta=\\theta_{0}\\), the constrained least squares estimator subject to \\(\\boldsymbol{R}^{\\prime} \\beta=\\theta_{0}\\) has the solution (8.9)\n\\[\n\\widetilde{\\beta}_{\\mathrm{cls}}=\\widehat{\\beta}-\\widehat{\\boldsymbol{Q}}_{X X}^{-1} \\boldsymbol{R}\\left(\\boldsymbol{R}^{\\prime} \\widehat{\\boldsymbol{Q}}_{X X}^{-1} \\boldsymbol{R}\\right)^{-1}\\left(\\boldsymbol{R}^{\\prime} \\widehat{\\beta}-\\theta_{0}\\right)\n\\]\nand solving we find\n\\[\nJ^{0}=n\\left(\\boldsymbol{R}^{\\prime} \\widehat{\\beta}-\\theta_{0}\\right)^{\\prime}\\left(\\boldsymbol{R}^{\\prime} \\widehat{\\boldsymbol{Q}}_{X X}^{-1} \\boldsymbol{R}\\right)^{-1}\\left(\\boldsymbol{R}^{\\prime} \\widehat{\\beta}-\\theta_{0}\\right) / s^{2}=W^{0} .\n\\]\nThis is the homoskedastic Wald statistic (9.7). Thus for testing linear hypotheses, homoskedastic minimum distance and Wald statistics agree.\nFor nonlinear hypotheses they disagree, but have the same null asymptotic distribution.\nTheorem 9.5 Under Assumptions \\(7.2\\) and \\(7.3, \\mathbb{E}\\left[e^{2} \\mid X\\right]=\\sigma^{2}>0\\), and \\(\\mathbb{M}_{0}: \\theta=\\) \\(\\theta_{0} \\in \\mathbb{R}^{q}\\), then \\(J^{0} \\underset{d}{\\longrightarrow} \\chi_{q}^{2}\\)"
  },
  {
    "objectID": "chpt09-hypothesit-test.html#f-tests",
    "href": "chpt09-hypothesit-test.html#f-tests",
    "title": "9  Hypothesis Testing",
    "section": "9.14 F Tests",
    "text": "9.14 F Tests\nIn Section \\(5.13\\) we introduced the \\(F\\) test for exclusion restrictions in the normal regression model. In this section we generalize this test to a broader set of restrictions. Let \\(B_{0} \\subset \\mathbb{R}^{k}\\) be a constrained parameter space which imposes \\(q\\) restrictions on \\(\\beta\\).\nLet \\(\\widehat{\\beta}_{\\text {ols }}\\) be the unrestricted least squares estimator and let \\(\\widehat{\\sigma}^{2}=n^{-1} \\sum_{i=1}^{n}\\left(Y_{i}-X_{i}^{\\prime} \\widehat{\\beta}_{\\text {ols }}\\right)^{2}\\) be the associated estimator of \\(\\sigma^{2}\\). Let \\(\\widetilde{\\beta}_{\\text {cls }}\\) be the CLS estimator (9.11) satisfying \\(\\widetilde{\\beta}_{\\text {cls }} \\in B_{0}\\) and let \\(\\widetilde{\\sigma}^{2}=n^{-1} \\sum_{i=1}^{n}\\left(Y_{i}-X_{i}^{\\prime} \\widetilde{\\beta}_{\\text {cls }}\\right)^{2}\\) be the associated estimator of \\(\\sigma^{2}\\). The \\(F\\) statistic for testing \\(\\mathbb{M}_{0}: \\beta \\in B_{0}\\) is\n\\[\nF=\\frac{\\left(\\tilde{\\sigma}^{2}-\\widehat{\\sigma}^{2}\\right) / q}{\\widehat{\\sigma}^{2} /(n-k)} .\n\\]\nWe can alternatively write\n\\[\nF=\\frac{\\operatorname{SSE}\\left(\\widetilde{\\beta}_{\\mathrm{cls}}\\right)-\\operatorname{SSE}\\left(\\widehat{\\beta}_{\\mathrm{ols}}\\right)}{q s^{2}}\n\\]\nwhere \\(\\operatorname{SSE}(\\beta)=\\sum_{i=1}^{n}\\left(Y_{i}-X_{i}^{\\prime} \\beta\\right)^{2}\\) is the sum-of-squared errors.\nThis shows that \\(F\\) is a criterion-based statistic. Using (8.21) we can also write \\(F=J^{0} / q\\), so the \\(F\\) statistic is identical to the homoskedastic minimum distance statistic divided by the number of restrictions \\(q\\).\nAs we discussed in the previous section, in the special case of linear hypotheses \\(\\mathbb{M}_{0}: \\boldsymbol{R}^{\\prime} \\beta=\\theta_{0}, J^{0}=\\) \\(W^{0}\\). It follows that in this case \\(F=W^{0} / q\\). Thus for linear restrictions the \\(F\\) statistic equals the homoskedastic Wald statistic divided by \\(q\\). It follows that they are equivalent tests for \\(\\mathbb{H}_{0}\\) against \\(\\mathbb{H}_{1}\\). Theorem 9.6 For tests of linear hypotheses \\(\\mathbb{H}_{0}: \\boldsymbol{R}^{\\prime} \\beta=\\theta_{0} \\in \\mathbb{R}^{q}\\), the \\(\\mathrm{F}\\) statistic equals \\(F=W^{0} / q\\) where \\(W^{0}\\) is the homoskedastic Wald statistic. Thus under 7.2, \\(\\mathbb{E}\\left[e^{2} \\mid X\\right]=\\sigma^{2}>0\\), and \\(\\mathbb{M}_{0}: \\theta=\\theta_{0}\\), then \\(F \\underset{d}{\\longrightarrow} \\chi_{q}^{2} / q\\).\nWhen using an \\(F\\) statistic it is conventional to use the \\(F_{q, n-k}\\) distribution for critical values and pvalues. Critical values are given in MATLAB by \\(f\\) inv \\((1-\\alpha, q, n-k)\\) and \\(p\\)-values by \\(1-f c d f(F, q, n-k)\\). Alternatively, the \\(\\chi_{q}^{2} / q\\) distribution can be used, using chi2inv \\((1-\\alpha, q) / q\\) and \\(1-\\operatorname{chi} 2 c d f(F * q, q)\\), respectively. Using the \\(F_{q, n-k}\\) distribution is a prudent small sample adjustment which yields exact answers if the errors are normal and otherwise slightly increasing the critical values and p-values relative to the asymptotic approximation. Once again, if the sample size is small enough that the choice makes a difference then probably we shouldn’t be trusting the asymptotic approximation anyway!\nAn elegant feature about (9.12) or (9.13) is that they are directly computable from the standard output from two simple OLS regressions, as the sum of squared errors (or regression variance) is a typical printed output from statistical packages and is often reported in applied tables. Thus \\(F\\) can be calculated by hand from standard reported statistics even if you don’t have the original data (or if you are sitting in a seminar and listening to a presentation!).\nIf you are presented with an \\(F\\) statistic (or a Wald statistic, as you can just divide by \\(q\\) ) but don’t have access to critical values, a useful rule of thumb is to know that for large \\(n\\) the \\(5 %\\) asymptotic critical value is decreasing as \\(q\\) increases and is less than 2 for \\(q \\geq 7\\).\nA word of warning: In many statistical packages when an OLS regression is estimated an “F-statistic” is automatically reported even though no hypothesis test was requested. What the package is reporting is an \\(F\\) statistic of the hypothesis that all slope coefficients \\({ }^{1}\\) are zero. This was a popular statistic in the early days of econometric reporting when sample sizes were very small and researchers wanted to know if there was “any explanatory power” to their regression. This is rarely an issue today as sample sizes are typically sufficiently large that this \\(F\\) statistic is nearly always highly significant. While there are special cases where this \\(F\\) statistic is useful these cases are not typical. As a general rule there is no reason to report this \\(F\\) statistic."
  },
  {
    "objectID": "chpt09-hypothesit-test.html#hausman-tests",
    "href": "chpt09-hypothesit-test.html#hausman-tests",
    "title": "9  Hypothesis Testing",
    "section": "9.15 Hausman Tests",
    "text": "9.15 Hausman Tests\nHausman (1978) introduced a general idea about how to test a hypothesis \\(\\mathbb{M}_{0}\\). If you have two estimators, one which is efficient under \\(\\mathbb{M}_{0}\\) but inconsistent under \\(\\mathbb{H}_{1}\\), and another which is consistent under \\(\\mathbb{H}_{1}\\), then construct a test as a quadratic form in the differences of the estimators. In the case of testing a hypothesis \\(\\mathbb{M}_{0}: r(\\beta)=\\theta_{0}\\) let \\(\\widehat{\\beta}_{\\text {ols }}\\) denote the unconstrained least squares estimator and let \\(\\widetilde{\\beta}_{\\text {emd }}\\) denote the efficient minimum distance estimator which imposes \\(r(\\beta)=\\theta_{0}\\). Both estimators are consistent under \\(\\mathbb{M}_{0}\\) but \\(\\widetilde{\\beta}_{\\mathrm{emd}}\\) is asymptotically efficient. Under \\(\\mathbb{H}_{1}, \\widehat{\\beta}_{\\mathrm{ols}}\\) is consistent for \\(\\beta\\) but \\(\\widetilde{\\beta}_{\\mathrm{emd}}\\) is inconsistent. The difference has the asymptotic distribution\n\\[\n\\sqrt{n}\\left(\\widehat{\\beta}_{\\mathrm{ols}}-\\widetilde{\\beta}_{\\mathrm{emd}}\\right) \\underset{d}{\\longrightarrow} \\mathrm{N}\\left(0, \\boldsymbol{V}_{\\beta} \\boldsymbol{R}\\left(\\boldsymbol{R}^{\\prime} \\boldsymbol{V}_{\\beta} \\boldsymbol{R}\\right)^{-1} \\boldsymbol{R}^{\\prime} \\boldsymbol{V}_{\\beta}\\right) .\n\\]\nLet \\(\\boldsymbol{A}^{-}\\)denote the Moore-Penrose generalized inverse. The Hausman statistic for \\(\\mathbb{H}_{0}\\) is\n\\[\n\\begin{aligned}\n& H=\\left(\\widehat{\\beta}_{\\mathrm{ols}}-\\widetilde{\\beta}_{\\mathrm{emd}}\\right)^{\\prime} \\widehat{\\operatorname{avar}}\\left(\\widehat{\\beta}_{\\mathrm{ols}}-\\widetilde{\\beta}_{\\mathrm{emd}}\\right)^{-}\\left(\\widehat{\\beta}_{\\mathrm{ols}}-\\widetilde{\\beta}_{\\mathrm{emd}}\\right) \\\\\n& =n\\left(\\widehat{\\beta}_{\\mathrm{ols}}-\\widetilde{\\beta}_{\\mathrm{emd}}\\right)^{\\prime}\\left(\\widehat{\\boldsymbol{V}}_{\\beta} \\widehat{\\boldsymbol{R}}\\left(\\widehat{\\boldsymbol{R}}^{\\prime} \\widehat{\\boldsymbol{V}}_{\\beta} \\widehat{\\boldsymbol{R}}\\right)^{-1} \\widehat{\\boldsymbol{R}}^{\\prime} \\widehat{\\boldsymbol{V}}_{\\beta}\\right)^{-}\\left(\\widehat{\\beta}_{\\mathrm{ols}}-\\widetilde{\\beta}_{\\mathrm{emd}}\\right) .\n\\end{aligned}\n\\]\n\\({ }^{1}\\) All coefficients except the intercept. The matrix \\(\\widehat{\\boldsymbol{V}}_{\\beta}^{1 / 2} \\widehat{\\boldsymbol{R}}\\left(\\widehat{\\boldsymbol{R}}^{\\prime} \\widehat{\\boldsymbol{V}}_{\\beta} \\widehat{\\boldsymbol{R}}\\right)^{-1} \\widehat{\\boldsymbol{R}}^{\\prime} \\widehat{\\boldsymbol{V}}_{\\beta}^{1 / 2}\\) idempotent so its generalized inverse is itself. (See Section A.11.) It follows that\n\\[\n\\begin{aligned}\n& =\\widehat{\\boldsymbol{V}}_{\\beta}^{-1 / 2} \\widehat{\\boldsymbol{V}}_{\\beta}^{1 / 2} \\widehat{\\boldsymbol{R}}\\left(\\widehat{\\boldsymbol{R}}^{\\prime} \\widehat{\\boldsymbol{V}}_{\\beta} \\widehat{\\boldsymbol{R}}\\right)^{-1} \\widehat{\\boldsymbol{R}}^{\\prime} \\widehat{\\boldsymbol{V}}_{\\beta}^{1 / 2} \\widehat{\\boldsymbol{V}}_{\\beta}^{-1 / 2} \\\\\n& =\\widehat{\\boldsymbol{R}}\\left(\\widehat{\\boldsymbol{R}}^{\\prime} \\widehat{\\boldsymbol{V}}_{\\beta} \\widehat{\\boldsymbol{R}}\\right)^{-1} \\widehat{\\boldsymbol{R}}^{\\prime} .\n\\end{aligned}\n\\]\nThus the Hausman statistic is\n\\[\nH=n\\left(\\widehat{\\beta}_{\\mathrm{ols}}-\\widetilde{\\beta}_{\\mathrm{emd}}\\right)^{\\prime} \\widehat{\\boldsymbol{R}}\\left(\\widehat{\\boldsymbol{R}}^{\\prime} \\widehat{\\boldsymbol{V}}_{\\beta} \\widehat{\\boldsymbol{R}}\\right)^{-1} \\widehat{\\boldsymbol{R}}^{\\prime}\\left(\\widehat{\\beta}_{\\mathrm{ols}}-\\widetilde{\\beta}_{\\mathrm{emd}}\\right) .\n\\]\nIn the context of linear restrictions, \\(\\widehat{\\boldsymbol{R}}=\\boldsymbol{R}\\) and \\(\\boldsymbol{R}^{\\prime} \\widetilde{\\beta}=\\theta_{0}\\) so the statistic takes the form\n\\[\nH=n\\left(\\boldsymbol{R}^{\\prime} \\widehat{\\beta}_{\\mathrm{ols}}-\\theta_{0}\\right)^{\\prime} \\widehat{\\boldsymbol{R}}\\left(\\boldsymbol{R}^{\\prime} \\widehat{\\boldsymbol{V}}_{\\beta} \\boldsymbol{R}\\right)^{-1}\\left(\\boldsymbol{R}^{\\prime} \\widehat{\\beta}_{\\mathrm{ols}}-\\theta_{0}\\right),\n\\]\nwhich is precisely the Wald statistic. With nonlinear restrictions \\(W\\) and \\(H\\) can differ.\nIn either case we see that that the asymptotic null distribution of the Hausman statistic \\(H\\) is \\(\\chi_{q}^{2}\\), so the appropriate test is to reject \\(\\mathbb{M}_{0}\\) in favor of \\(\\mathbb{H}_{1}\\) if \\(H>c\\) where \\(c\\) is a critical value taken from the \\(\\chi_{q}^{2}\\) distribution.\nTheorem 9.7 For general hypotheses the Hausman test statistic is\n\\[\nH=n\\left(\\widehat{\\beta}_{\\mathrm{ols}}-\\widetilde{\\beta}_{\\mathrm{emd}}\\right)^{\\prime} \\widehat{\\boldsymbol{R}}\\left(\\widehat{\\boldsymbol{R}}^{\\prime} \\widehat{\\boldsymbol{V}}_{\\beta} \\widehat{\\boldsymbol{R}}\\right)^{-1} \\widehat{\\boldsymbol{R}}^{\\prime}\\left(\\widehat{\\beta}_{\\mathrm{ols}}-\\widetilde{\\beta}_{\\mathrm{emd}}\\right) .\n\\]\nUnder Assumptions \\(7.2,7.3,7.4\\), and \\(\\mathbb{M}_{0}: r(\\beta)=\\theta_{0} \\in \\mathbb{R}^{q}, H \\underset{d}{\\longrightarrow} \\chi_{q}^{2}\\)"
  },
  {
    "objectID": "chpt09-hypothesit-test.html#score-tests",
    "href": "chpt09-hypothesit-test.html#score-tests",
    "title": "9  Hypothesis Testing",
    "section": "9.16 Score Tests",
    "text": "9.16 Score Tests\nScore tests are traditionally derived in likelihood analysis but can more generally be constructed from first-order conditions evaluated at restricted estimates. We focus on the likelihood derivation.\nGiven the log likelihood function \\(\\ell_{n}\\left(\\beta, \\sigma^{2}\\right)\\), a restriction \\(\\mathbb{H}_{0}: r(\\beta)=\\theta_{0}\\), and restricted estimators \\(\\widetilde{\\beta}\\) and \\(\\widetilde{\\sigma}^{2}\\), the score statistic for \\(\\mathbb{H}_{0}\\) is defined as\n\\[\nS=\\left(\\frac{\\partial}{\\partial \\beta} \\ell_{n}\\left(\\widetilde{\\beta}, \\widetilde{\\sigma}^{2}\\right)\\right)^{\\prime}\\left(-\\frac{\\partial^{2}}{\\partial \\beta \\partial \\beta^{\\prime}} \\ell_{n}\\left(\\widetilde{\\beta}, \\widetilde{\\sigma}^{2}\\right)\\right)^{-1}\\left(\\frac{\\partial}{\\partial \\beta} \\ell_{n}\\left(\\widetilde{\\beta}, \\widetilde{\\sigma}^{2}\\right)\\right) .\n\\]\nThe idea is that if the restriction is true then the restricted estimators should be close to the maximum of the log-likelihood where the derivative is zero. However if the restriction is false then the restricted estimators should be distant from the maximum and the derivative should be large. Hence small values of \\(S\\) are expected under \\(\\mathbb{H}_{0}\\) and large values under \\(\\mathbb{H}_{1}\\). Tests of \\(\\mathbb{M}_{0}\\) reject for large values of \\(S\\).\nWe explore the score statistic in the context of the normal regression model and linear hypotheses \\(r(\\beta)=\\boldsymbol{R}^{\\prime} \\beta\\). Recall that in the normal regression log-likelihood function is\n\\[\n\\ell_{n}\\left(\\beta, \\sigma^{2}\\right)=-\\frac{n}{2} \\log \\left(2 \\pi \\sigma^{2}\\right)-\\frac{1}{2 \\sigma^{2}} \\sum_{i=1}^{n}\\left(Y_{i}-X_{i}^{\\prime} \\beta\\right)^{2} .\n\\]\n\nThe constrained MLE under linear hypotheses is constrained least squares\n\\[\n\\begin{aligned}\n\\widetilde{\\beta} &=\\widehat{\\beta}-\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\boldsymbol{R}\\left[\\boldsymbol{R}^{\\prime}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\boldsymbol{R}\\right]^{-1}\\left(\\boldsymbol{R}^{\\prime} \\widehat{\\beta}-\\boldsymbol{c}\\right) \\\\\n\\widetilde{e}_{i} &=Y_{i}-X_{i}^{\\prime} \\widetilde{\\beta} \\\\\n\\widetilde{\\sigma}^{2} &=\\frac{1}{n} \\sum_{i=1}^{n} \\widetilde{e}_{i}^{2}\n\\end{aligned}\n\\]\nWe can calculate that the derivative and Hessian are\n\\[\n\\begin{aligned}\n\\frac{\\partial}{\\partial \\beta} \\ell_{n}\\left(\\widetilde{\\beta}, \\widetilde{\\sigma}^{2}\\right) &=\\frac{1}{\\widetilde{\\sigma}^{2}} \\sum_{i=1}^{n} X_{i}\\left(Y_{i}-X_{i}^{\\prime} \\widetilde{\\beta}\\right)=\\frac{1}{\\widetilde{\\sigma}^{2}} \\boldsymbol{X}^{\\prime} \\widetilde{\\boldsymbol{e}} \\\\\n-\\frac{\\partial^{2}}{\\partial \\beta \\partial \\beta^{\\prime}} \\ell_{n}\\left(\\widetilde{\\beta}, \\widetilde{\\sigma}^{2}\\right) &=\\frac{1}{\\widetilde{\\sigma}^{2}} \\sum_{i=1}^{n} X_{i} X_{i}^{\\prime}=\\frac{1}{\\widetilde{\\sigma}^{2}} \\boldsymbol{X}^{\\prime} \\boldsymbol{X}\n\\end{aligned}\n\\]\nSince \\(\\widetilde{\\boldsymbol{e}}=\\boldsymbol{Y}-\\boldsymbol{X} \\widetilde{\\beta}\\) we can further calculate that\n\\[\n\\begin{aligned}\n\\frac{\\partial}{\\partial \\beta} \\ell_{n}\\left(\\widetilde{\\beta}, \\widetilde{\\sigma}^{2}\\right) &=\\frac{1}{\\widetilde{\\sigma}^{2}}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)\\left(\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\boldsymbol{X}^{\\prime} \\boldsymbol{Y}-\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\boldsymbol{X}^{\\prime} \\boldsymbol{X} \\widetilde{\\beta}\\right) \\\\\n&=\\frac{1}{\\widetilde{\\sigma}^{2}}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)(\\widehat{\\beta}-\\widetilde{\\beta}) \\\\\n&=\\frac{1}{\\widetilde{\\sigma}^{2}} \\boldsymbol{R}\\left[\\boldsymbol{R}^{\\prime}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\boldsymbol{R}\\right]^{-1}\\left(\\boldsymbol{R}^{\\prime} \\widehat{\\beta}-\\boldsymbol{c}\\right) .\n\\end{aligned}\n\\]\nTogether we find that\n\\[\nS=\\left(\\boldsymbol{R}^{\\prime} \\widehat{\\boldsymbol{\\beta}}-\\boldsymbol{c}\\right)^{\\prime}\\left(\\boldsymbol{R}^{\\prime}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\boldsymbol{R}\\right)^{-1}\\left(\\boldsymbol{R}^{\\prime} \\widehat{\\boldsymbol{\\beta}}-\\boldsymbol{c}\\right) / \\widetilde{\\sigma}^{2} .\n\\]\nThis is identical to the homoskedastic Wald statistic with \\(s^{2}\\) replaced by \\(\\widetilde{\\sigma}^{2}\\). We can also write \\(S\\) as a monotonic transformation of the \\(F\\) statistic, as\n\\[\nS=n \\frac{\\left(\\widetilde{\\sigma}^{2}-\\widehat{\\sigma}^{2}\\right)}{\\widetilde{\\sigma}^{2}}=n\\left(1-\\frac{\\widehat{\\sigma}^{2}}{\\widetilde{\\sigma}^{2}}\\right)=n\\left(1-\\frac{1}{1+\\frac{q}{n-k} F}\\right) .\n\\]\nThe test “Reject \\(\\mathbb{M}_{0}\\) for large values of \\(S\\)” is identical to the test “Reject \\(\\mathbb{M}_{0}\\) for large values of \\(F\\)” so they are identical tests. Since for the normal regression model the exact distribution of \\(F\\) is known, it is better to use the \\(F\\) statistic with \\(F\\) p-values.\nIn more complicated settings a potential advantage of score tests is that they are calculated using the restricted parameter estimates \\(\\widetilde{\\beta}\\) rather than the unrestricted estimates \\(\\widehat{\\beta}\\). Thus when \\(\\widetilde{\\beta}\\) is relatively easy to calculate there can be a preference for score statistics. This is not a concern for linear restrictions.\nMore generally, score and score-like statistics can be constructed from first-order conditions evaluated at restricted parameter estimates. Also, when test statistics are constructed using covariance matrix estimators which are calculated using restricted parameter estimates (e.g. restricted residuals) then these are often described as score tests.\nAn example of the latter is the Wald-type statistic\n\\[\nW=\\left(r(\\widehat{\\beta})-\\theta_{0}\\right)^{\\prime}\\left(\\widehat{\\boldsymbol{R}}^{\\prime} \\widetilde{\\boldsymbol{V}}_{\\widehat{\\beta}} \\widehat{\\boldsymbol{R}}\\right)^{-1}\\left(r(\\widehat{\\beta})-\\theta_{0}\\right)\n\\]\nwhere the covariance matrix estimate \\(\\widetilde{\\boldsymbol{V}}_{\\widehat{\\beta}}\\) is calculated using the restricted residuals \\(\\widetilde{e}_{i}=Y_{i}-X_{i}^{\\prime} \\widetilde{\\beta}\\). This may be a good choice when \\(\\beta\\) and \\(\\theta\\) are high-dimensional as in this context there may be worry that the estimator \\(\\widehat{\\boldsymbol{V}}_{\\widehat{\\beta}}\\) is imprecise."
  },
  {
    "objectID": "chpt09-hypothesit-test.html#problems-with-tests-of-nonlinear-hypotheses",
    "href": "chpt09-hypothesit-test.html#problems-with-tests-of-nonlinear-hypotheses",
    "title": "9  Hypothesis Testing",
    "section": "9.17 Problems with Tests of Nonlinear Hypotheses",
    "text": "9.17 Problems with Tests of Nonlinear Hypotheses\nWhile the \\(t\\) and Wald tests work well when the hypothesis is a linear restriction on \\(\\beta\\), they can work quite poorly when the restrictions are nonlinear. This can be seen by a simple example introduced by Lafontaine and White (1986). Take the model \\(Y \\sim \\mathrm{N}\\left(\\beta, \\sigma^{2}\\right.\\) ) and consider the hypothesis \\(\\mathbb{H}_{0}: \\beta=1\\). Let \\(\\widehat{\\beta}\\) and \\(\\widehat{\\sigma}^{2}\\) be the sample mean and variance of \\(Y\\). The standard Wald statistic to test \\(\\mathbb{H}_{0}\\) is\n\\[\nW=n \\frac{(\\widehat{\\beta}-1)^{2}}{\\widehat{\\sigma}^{2}} .\n\\]\nNotice that \\(\\mathbb{M}_{0}\\) is equivalent to the hypothesis \\(\\mathbb{M}_{0}(s): \\beta^{s}=1\\) for any positive integer \\(s\\). Letting \\(r(\\beta)=\\) \\(\\beta^{s}\\), and noting \\(\\boldsymbol{R}=s \\beta^{s-1}\\), we find that the Wald statistic to test \\(\\mathbb{M}_{0}(s)\\) is\n\\[\nW_{s}=n \\frac{\\left(\\widehat{\\beta}^{s}-1\\right)^{2}}{\\widehat{\\sigma}^{2} s^{2} \\widehat{\\beta}^{2 s-2}} .\n\\]\nWhile the hypothesis \\(\\beta^{s}=1\\) is unaffected by the choice of \\(s\\), the statistic \\(W_{s}\\) varies with \\(s\\). This is an unfortunate feature of the Wald statistic.\nTo demonstrate this effect, we have plotted in Figure \\(9.2\\) the Wald statistic \\(W_{s}\\) as a function of \\(s\\), setting \\(n / \\widehat{\\sigma}^{2}=10\\). The increasing line is for the case \\(\\widehat{\\beta}=0.8\\). The decreasing line is for the case \\(\\widehat{\\beta}=1.6\\). It is easy to see that in each case there are values of \\(s\\) for which the test statistic is significant relative to asymptotic critical values, while there are other values of \\(s\\) for which the test statistic is insignificant. This is distressing because the choice of \\(s\\) is arbitrary and irrelevant to the actual hypothesis.\nOur first-order asymptotic theory is not useful to help pick \\(s\\), as \\(W_{s} \\underset{d}{\\longrightarrow} \\chi_{1}^{2}\\) under \\(\\mathbb{H}_{0}\\) for any \\(s\\). This is a context where Monte Carlo simulation can be quite useful as a tool to study and compare the exact distributions of statistical procedures in finite samples. The method uses random simulation to create artificial datasets to which we apply the statistical tools of interest. This produces random draws from the statistic’s sampling distribution. Through repetition, features of this distribution can be calculated.\nIn the present context of the Wald statistic, one feature of importance is the Type I error of the test using the asymptotic \\(5 %\\) critical value \\(3.84\\) - the probability of a false rejection, \\(\\mathbb{P}\\left[W_{s}>3.84 \\mid \\beta=1\\right]\\). Given the simplicity of the model this probability depends only on \\(s, n\\), and \\(\\sigma^{2}\\). In Table \\(9.2\\) we report the results of a Monte Carlo simulation where we vary these three parameters. The value of \\(s\\) is varied from 1 to \\(10, n\\) is varied among 20,100 , and 500 , and \\(\\sigma\\) is varied among 1 and 3 . The table reports the simulation estimate of the Type I error probability from 50,000 random samples. Each row of the table corresponds to a different value of \\(s\\) - and thus corresponds to a particular choice of test statistic. The second through seventh columns contain the Type I error probabilities for different combinations of \\(n\\) and \\(\\sigma\\). These probabilities are calculated as the percentage of the 50,000 simulated Wald statistics \\(W_{s}\\) which are larger than 3.84. The null hypothesis \\(\\beta^{s}=1\\) is true so these probabilities are Type I error.\nTo interpret the table remember that the ideal Type I error probability is \\(5 %(.05)\\) with deviations indicating distortion. Type I error rates between \\(3 %\\) and \\(8 %\\) are considered reasonable. Error rates above \\(10 %\\) are considered excessive. Rates above \\(20 %\\) are unacceptable. When comparing statistical procedures we compare the rates row by row, looking for tests for which rejection rates are close to \\(5 %\\) and rarely fall outside of the \\(3 %-8 %\\) range. For this particular example the only test which meets this criterion is the conventional \\(W=W_{1}\\) test. Any other \\(s\\) leads to a test with unacceptable Type I error probabilities.\nIn Table \\(9.2\\) you can also see the impact of variation in sample size. In each case the Type I error probability improves towards \\(5 %\\) as the sample size \\(n\\) increases. There is, however, no magic choice of \\(n\\) for which all tests perform uniformly well. Test performance deteriorates as \\(s\\) increases which is not surprising given the dependence of \\(W_{s}\\) on \\(s\\) as shown in Figure 9.2.\n\nFigure 9.2: Wald Statistic as a Function of \\(s\\)\nIn this example it is not surprising that the choice \\(s=1\\) yields the best test statistic. Other choices are arbitrary and would not be used in practice. While this is clear in this particular example, in other examples natural choices are not obvious and the best choices may be counter-intuitive.\nThis point can be illustrated through an example based on Gregory and Veall (1985). Take the model\n\\[\n\\begin{aligned}\nY &=\\beta_{0}+X_{1} \\beta_{1}+X_{2} \\beta_{2}+e \\\\\n\\mathbb{E}[X e] &=0\n\\end{aligned}\n\\]\nand the hypothesis \\(\\mathbb{M}_{0}: \\frac{\\beta_{1}}{\\beta_{2}}=\\theta_{0}\\) where \\(\\theta_{0}\\) is a known constant. Equivalently, define \\(\\theta=\\beta_{1} / \\beta_{2}\\) so the hypothesis can be stated as \\(\\mathbb{M}_{0}: \\theta=\\theta_{0}\\).\nLet \\(\\widehat{\\beta}=\\left(\\widehat{\\beta}_{0}, \\widehat{\\beta}_{1}, \\widehat{\\beta}_{2}\\right)\\) be the least squares estimator of \\((9.14)\\), let \\(\\widehat{\\boldsymbol{V}}_{\\widehat{\\beta}}\\) be an estimator of the covariance matrix for \\(\\widehat{\\beta}\\) and set \\(\\widehat{\\theta}=\\widehat{\\beta}_{1} / \\widehat{\\beta}_{2}\\). Define\n\\[\n\\widehat{\\boldsymbol{R}}_{1}=\\left(\\begin{array}{c}\n0 \\\\\n\\frac{1}{\\widehat{\\beta}_{2}} \\\\\n-\\frac{\\widehat{\\beta}_{1}}{\\widehat{\\beta}_{2}^{2}}\n\\end{array}\\right)\n\\]\nTable 9.2: Type I Error Probability of Asymptotic \\(5 % W(s)\\) Test\n\nRejection frequencies from 50,000 simulated random samples.\nso that the standard error for \\(\\widehat{\\theta}\\) is \\(s(\\widehat{\\theta})=\\left(\\widehat{\\boldsymbol{R}}_{1}^{\\prime} \\widehat{\\boldsymbol{V}}_{\\widehat{\\beta}} \\widehat{\\boldsymbol{R}}_{1}\\right)^{1 / 2}\\). In this case a t-statistic for \\(\\mathbb{M}_{0}\\) is\n\\[\nT_{1}=\\frac{\\left(\\frac{\\widehat{\\beta}_{1}}{\\widehat{\\beta}_{2}}-\\theta_{0}\\right)}{s(\\widehat{\\theta})} .\n\\]\nAn alternative statistic can be constructed through reformulating the null hypothesis as\n\\[\n\\mathbb{M}_{0}: \\beta_{1}-\\theta_{0} \\beta_{2}=0 .\n\\]\nA t-statistic based on this formulation of the hypothesis is\n\\[\nT_{2}=\\frac{\\widehat{\\beta}_{1}-\\theta_{0} \\widehat{\\beta}_{2}}{\\left(\\boldsymbol{R}_{2}^{\\prime} \\widehat{\\boldsymbol{V}}_{\\widehat{\\beta}} \\boldsymbol{R}_{2}\\right)^{1 / 2}}\n\\]\nwhere\n\\[\n\\boldsymbol{R}_{2}=\\left(\\begin{array}{c}\n0 \\\\\n1 \\\\\n-\\theta_{0}\n\\end{array}\\right) \\text {. }\n\\]\nTo compare \\(T_{1}\\) and \\(T_{2}\\) we perform another simple Monte Carlo simulation. We let \\(X_{1}\\) and \\(X_{2}\\) be mutually independent \\(\\mathrm{N}(0,1)\\) variables, \\(e\\) be an independent \\(\\mathrm{N}\\left(0, \\sigma^{2}\\right)\\) draw with \\(\\sigma=3\\), and normalize \\(\\beta_{0}=0\\) and \\(\\beta_{1}=1\\). This leaves \\(\\beta_{2}\\) as a free parameter along with sample size \\(n\\). We vary \\(\\beta_{2}\\) among \\(0.1\\), \\(0.25,0.50,0.75\\), and \\(1.0\\) and \\(n\\) among 100 and 500 .\nThe one-sided Type I error probabilities \\(\\mathbb{P}[T<-1.645]\\) and \\(\\mathbb{P}[T>1.645]\\) are calculated from 50,000 simulated samples. The results are presented in Table 9.3. Ideally, the entries in the table should be \\(0.05\\). However, the rejection rates for the \\(T_{1}\\) statistic diverge greatly from this value, especially for small values of \\(\\beta_{2}\\). The left tail probabilities \\(\\mathbb{P}\\left[T_{1}<-1.645\\right]\\) greatly exceed \\(5 %\\), while the right tail probabilities \\(\\mathbb{P}\\left[T_{1}>1.645\\right]\\) are close to zero in most cases. In contrast, the rejection rates for the \\(T_{2}\\) statistic are invariant to the value of \\(\\beta_{2}\\) and equal \\(5 %\\) for both sample sizes. The implication of Table \\(9.3\\) is that the two t-ratios have dramatically different sampling behavior.\nThe common message from both examples is that Wald statistics are sensitive to the algebraic formulation of the null hypothesis. Table 9.3: Type I Error Probability of Asymptotic 5% t-tests\n\nRejection frequencies from 50,000 simulated random samples.\nA simple solution is to use the minimum distance statistic \\(J\\) which equals \\(W\\) with \\(r=1\\) in the first example, and \\(\\left|T_{2}\\right|\\) in the second example. The minimum distance statistic is invariant to the algebraic formulation of the null hypothesis so is immune to this problem. Whenever possible, the Wald statistic should not be used to test nonlinear hypotheses.\nTheoretical investigations of these issues include Park and Phillips (1988) and Dufour (1997)."
  },
  {
    "objectID": "chpt09-hypothesit-test.html#monte-carlo-simulation",
    "href": "chpt09-hypothesit-test.html#monte-carlo-simulation",
    "title": "9  Hypothesis Testing",
    "section": "9.18 Monte Carlo Simulation",
    "text": "9.18 Monte Carlo Simulation\nIn Section \\(9.17\\) we introduced the method of Monte Carlo simulation to illustrate the small sample problems with tests of nonlinear hypotheses. In this section we describe the method in more detail.\nRecall, our data consist of observations \\(\\left(Y_{i}, X_{i}\\right)\\) which are random draws from a population distribution \\(F\\). Let \\(\\theta\\) be a parameter and let \\(T=T\\left(\\left(Y_{1}, X_{1}\\right), \\ldots,\\left(Y_{n}, X_{n}\\right), \\theta\\right)\\) be a statistic of interest, for example an estimator \\(\\widehat{\\theta}\\) or a t-statistic \\((\\widehat{\\theta}-\\theta) / s(\\widehat{\\theta})\\). The exact distribution of \\(T\\) is\n\\[\nG(u, F)=\\mathbb{P}[T \\leq u \\mid F] .\n\\]\nWhile the asymptotic distribution of \\(T\\) might be known, the exact (finite sample) distribution \\(G\\) is generally unknown.\nMonte Carlo simulation uses numerical simulation to compute \\(G(u, F)\\) for selected choices of \\(F\\). This is useful to investigate the performance of the statistic \\(T\\) in reasonable situations and sample sizes. The basic idea is that for any given \\(F\\) the distribution function \\(G(u, F)\\) can be calculated numerically through simulation. The name Monte Carlo derives from the Mediterranean gambling resort where games of chance are played.\nThe method of Monte Carlo is simple to describe. The researcher chooses \\(F\\) (the distribution of the pseudo data) and the sample size \\(n\\). A “true” value of \\(\\theta\\) is implied by this choice, or equivalently the value \\(\\theta\\) is selected directly by the researcher which implies restrictions on \\(F\\).\nThen the following experiment is conducted by computer simulation:\n\n\\(n\\) independent random pairs \\(\\left(Y_{i}^{*}, X_{i}^{*}\\right), i=1, \\ldots, n\\), are drawn from the distribution \\(F\\) using the computer’s random number generator.\nThe statistic \\(T=T\\left(\\left(Y_{1}^{*}, X_{1}^{*}\\right), \\ldots,\\left(Y_{n}^{*}, X_{n}^{*}\\right), \\theta\\right)\\) is calculated on this pseudo data.\n\nFor step 1, computer packages have built-in random number procedures including \\(U[0,1]\\) and \\(N(0,1)\\). From these most random variables can be constructed. (For example, a chi-square can be generated by sums of squares of normals.) For step 2, it is important that the statistic be evaluated at the “true” value of \\(\\theta\\) corresponding to the choice of \\(F\\).\nThe above experiment creates one random draw \\(T\\) from the distribution \\(G(u, F)\\). This is one observation from an unknown distribution. Clearly, from one observation very little can be said. So the researcher repeats the experiment \\(B\\) times where \\(B\\) is a large number. Typically, we set \\(B \\geq 1000\\). We will discuss this choice later.\nNotationally, let the \\(b^{t h}\\) experiment result in the draw \\(T_{b}, b=1, \\ldots, B\\). These results are stored. After all \\(B\\) experiments have been calculated these results constitute a random sample of size \\(B\\) from the distribution of \\(G(u, F)=\\mathbb{P}\\left[T_{b} \\leq u\\right]=\\mathbb{P}[T \\leq u \\mid F]\\).\nFrom a random sample we can estimate any feature of interest using (typically) a method of moments estimator. We now describe some specific examples.\nSuppose we are interested in the bias, mean-squared error (MSE), and/or variance of the distribution of \\(\\widehat{\\theta}-\\theta\\). We then set \\(T=\\widehat{\\theta}-\\theta\\), run the above experiment, and calculate\n\\[\n\\begin{aligned}\n\\widehat{\\operatorname{bias}}[\\widehat{\\theta}] &=\\frac{1}{B} \\sum_{b=1}^{B} T_{b}=\\frac{1}{B} \\sum_{b=1}^{B} \\widehat{\\theta}_{b}-\\theta \\\\\n\\widehat{\\operatorname{mse}}[\\widehat{\\theta}] &=\\frac{1}{B} \\sum_{b=1}^{B}\\left(T_{b}\\right)^{2}=\\frac{1}{B} \\sum_{b=1}^{B}\\left(\\widehat{\\theta}_{b}-\\theta\\right)^{2} \\\\\n\\widehat{\\operatorname{var}}[\\widehat{\\theta}] &=\\widehat{\\operatorname{mse}}[\\widehat{\\theta}]-(\\widehat{\\operatorname{bias}}[\\hat{\\theta}])^{2}\n\\end{aligned}\n\\]\nSuppose we are interested in the Type I error associated with an asymptotic 5% two-sided t-test. We would then set \\(T=|\\widehat{\\theta}-\\theta| / s(\\widehat{\\theta})\\) and calculate\n\\[\n\\widehat{P}=\\frac{1}{B} \\sum_{b=1}^{B} \\mathbb{1}\\left\\{T_{b} \\geq 1.96\\right\\},\n\\]\nthe percentage of the simulated t-ratios which exceed the asymptotic \\(5 %\\) critical value.\nSuppose we are interested in the \\(5 %\\) and \\(95 %\\) quantile of \\(T=\\widehat{\\theta}\\) or \\(T=(\\widehat{\\theta}-\\theta) / s(\\widehat{\\theta})\\). We then compute the \\(5 %\\) and \\(95 %\\) sample quantiles of the sample \\(\\left\\{T_{b}\\right\\}\\). For details on quantile estimation see Section \\(11.13\\) of Probability and Statistics for Economists.\nThe typical purpose of a Monte Carlo simulation is to investigate the performance of a statistical procedure in realistic settings. Generally, the performance will depend on \\(n\\) and \\(F\\). In many cases an estimator or test may perform wonderfully for some values and poorly for others. It is therefore useful to conduct a variety of experiments for a selection of choices of \\(n\\) and \\(F\\).\nAs discussed above the researcher must select the number of experiments \\(B\\). Often this is called the number of replications. Quite simply, a larger \\(B\\) results in more precise estimates of the features of interest of \\(G\\) but requires more computational time. In practice, therefore, the choice of \\(B\\) is often guided by the computational demands of the statistical procedure. Since the results of a Monte Carlo experiment are estimates computed from a random sample of size \\(B\\) it is straightforward to calculate standard errors for any quantity of interest. If the standard error is too large to make a reliable inference then \\(B\\) will have to be increased. A useful rule-of-thumb is to set \\(B=10,000\\) whenever possible.\nIn particular, it is simple to make inferences about rejection probabilities from statistical tests, such as the percentage estimate reported in (9.15). The random variable \\(\\mathbb{1}\\left\\{T_{b} \\geq 1.96\\right\\}\\) is i.i.d. Bernoulli, equalling 1 with probability \\(p=\\mathbb{E}\\left[\\mathbb{1}\\left\\{T_{b} \\geq 1.96\\right\\}\\right]\\). The average (9.15) is therefore an unbiased estimator of \\(p\\) with standard error \\(s(\\widehat{p})=\\sqrt{p(1-p) / B}\\). As \\(p\\) is unknown, this may be approximated by replacing \\(p\\) with \\(\\widehat{p}\\) or with an hypothesized value. For example, if we are assessing an asymptotic \\(5 %\\) test, then we can set \\(s(\\widehat{p})=\\sqrt{(.05)(.95) / B} \\simeq .22 / \\sqrt{B}\\). Hence, standard errors for \\(B=100,1000\\), and 5000, are, respectively, \\(s(\\widehat{p})=.022, .007\\), and \\(.003 .\\) Most papers in econometric methods and some empirical papers include the results of Monte Carlo simulations to illustrate the performance of their methods. When extending existing results it is good practice to start by replicating existing (published) results. This may not be exactly possible in the case of simulation results as they are inherently random. For example suppose a paper investigates a statistical test and reports a simulated rejection probability of \\(0.07\\) based on a simulation with \\(B=100\\) replications. Suppose you attempt to replicate this result and find a rejection probability of \\(0.03\\) (again using \\(B=100\\) simulation replications). Should you conclude that you have failed in your attempt? Absolutely not! Under the hypothesis that both simulations are identical you have two independent estimates, \\(\\widehat{p}_{1}=0.07\\) and \\(\\widehat{p}_{2}=0.03\\), of a common probability \\(p\\). The asymptotic (as \\(B \\rightarrow \\infty\\) ) distribution of their difference is \\(\\sqrt{B}\\left(\\widehat{p}_{1}-\\widehat{p}_{2}\\right) \\underset{d}{\\longrightarrow} \\mathrm{N}(0,2 p(1-p))\\), so a standard error for \\(\\widehat{p}_{1}-\\widehat{p}_{2}=0.04\\) is \\(\\widehat{s}=\\sqrt{2 \\bar{p}(1-\\bar{p}) / B} \\simeq 0.03\\), using the estimate \\(\\bar{p}=\\left(\\widehat{p}_{1}+\\widehat{p}_{2}\\right) / 2\\). Since the t-ratio \\(0.04 / 0.03=1.3\\) is not statistically significant it is incorrect to reject the null hypothesis that the two simulations are identical. The difference between the results \\(\\widehat{p}_{1}=0.07\\) and \\(\\widehat{p}_{2}=0.03\\) is consistent with random variation.\nWhat should be done? The first mistake was to copy the previous paper’s choice of \\(B=100\\). Instead, suppose you set \\(B=10,000\\) and now obtain \\(\\widehat{p}_{2}=0.04\\). Then \\(\\widehat{p}_{1}-\\widehat{p}_{2}=0.03\\) and a standard error is \\(\\widehat{s}=\\) \\(\\sqrt{\\bar{p}(1-\\bar{p})(1 / 100+1 / 10000)} \\simeq 0.02\\). Still we cannot reject the hypothesis that the two simulations are different. Even though the estimates ( \\(0.07\\) and \\(0.04)\\) appear to be quite different, the difficulty is that the original simulation used a very small number of replications \\((B=100)\\) so the reported estimate is quite imprecise. In this case it is appropriate to conclude that your results “replicate” the previous study as there is no statistical evidence to reject the hypothesis that they are equivalent.\nMost journals have policies requiring authors to make available their data sets and computer programs required for empirical results. Most do not have similar policies regarding simulations. Nevertheless, it is good professional practice to make your simulations available. The best practice is to post your simulation code on your webpage. This invites others to build on and use your results, leading to possible collaboration, citation, and/or advancement."
  },
  {
    "objectID": "chpt09-hypothesit-test.html#confidence-intervals-by-test-inversion",
    "href": "chpt09-hypothesit-test.html#confidence-intervals-by-test-inversion",
    "title": "9  Hypothesis Testing",
    "section": "9.19 Confidence Intervals by Test Inversion",
    "text": "9.19 Confidence Intervals by Test Inversion\nThere is a close relationship between hypothesis tests and confidence intervals. We observed in Section \\(7.13\\) that the standard \\(95 %\\) asymptotic confidence interval for a parameter \\(\\theta\\) is\n\\[\n\\widehat{C}=[\\widehat{\\theta}-1.96 \\times s(\\widehat{\\theta}), \\quad \\widehat{\\theta}+1.96 \\times s(\\widehat{\\theta})]=\\{\\theta:|T(\\theta)| \\leq 1.96\\} .\n\\]\nThat is, we can describe \\(\\widehat{C}\\) as “The point estimate plus or minus 2 standard errors” or “The set of parameter values not rejected by a two-sided t-test.” The second definition, known as test statistic inversion, is a general method for finding confidence intervals, and typically produces confidence intervals with excellent properties.\nGiven a test statistic \\(T(\\theta)\\) and critical value \\(c\\), the acceptance region “Accept if \\(T(\\theta) \\leq c\\)” is identical to the confidence interval \\(\\widehat{C}=\\{\\theta: T(\\theta) \\leq c\\}\\). Since the regions are identical the probability of coverage \\(\\mathbb{P}[\\theta \\in \\widehat{C}]\\) equals the probability of correct acceptance \\(\\mathbb{P}[\\) Accept \\(\\mid \\theta]\\) which is exactly 1 minus the Type I error probability. Thus inverting a test with good Type I error probabilities yields a confidence interval with good coverage probabilities.\nNow suppose that the parameter of interest \\(\\theta=r(\\beta)\\) is a nonlinear function of the coefficient vector \\(\\beta\\). In this case the standard confidence interval for \\(\\theta\\) is the set \\(\\widehat{C}\\) as in (9.16) where \\(\\widehat{\\theta}=r(\\widehat{\\beta})\\) is the point estimator and \\(s(\\widehat{\\theta})=\\sqrt{\\widehat{\\boldsymbol{R}}^{\\prime} \\widehat{\\boldsymbol{V}}_{\\widehat{\\beta}} \\widehat{\\boldsymbol{R}}}\\) is the delta method standard error. This confidence interval is inverting the t-test based on the nonlinear hypothesis \\(r(\\beta)=\\theta\\). The trouble is that in Section \\(9.17\\) we learned that there is no unique t-statistic for tests of nonlinear hypotheses and that the choice of parameterization matters greatly. For example, if \\(\\theta=\\beta_{1} / \\beta_{2}\\) then the coverage probability of the standard interval (9.16) is 1 minus the probability of the Type I error, which as shown in Table \\(8.2\\) can be far from the nominal \\(5 %\\).\nIn this example a good solution is the same as discussed in Section \\(9.17\\) - to rewrite the hypothesis as a linear restriction. The hypothesis \\(\\theta=\\beta_{1} / \\beta_{2}\\) is the same as \\(\\theta \\beta_{2}=\\beta_{1}\\). The t-statistic for this restriction is\n\\[\nT(\\theta)=\\frac{\\widehat{\\beta}_{1}-\\widehat{\\beta}_{2} \\theta}{\\left(\\boldsymbol{R}^{\\prime} \\widehat{\\boldsymbol{V}}_{\\widehat{\\beta}} \\boldsymbol{R}\\right)^{1 / 2}}\n\\]\nwhere\n\\[\n\\boldsymbol{R}=\\left(\\begin{array}{c}\n1 \\\\\n-\\theta\n\\end{array}\\right)\n\\]\nand \\(\\widehat{V}_{\\widehat{\\beta}}\\) is the covariance matrix for \\(\\left(\\widehat{\\beta}_{1} \\widehat{\\beta}_{2}\\right)\\). A 95% confidence interval for \\(\\theta=\\beta_{1} / \\beta_{2}\\) is the set of values of \\(\\theta\\) such that \\(|T(\\theta)| \\leq 1.96\\). Since \\(T(\\theta)\\) is a nonlinear function of \\(\\theta\\) one method to find the confidence set is grid search over \\(\\theta\\).\nFor example, in the wage equation\n\\[\n\\log (\\text { wage })=\\beta_{1} \\text { experience }+\\beta_{2} \\text { experience }^{2} / 100+\\cdots\n\\]\nthe highest expected wage occurs at experience \\(=-50 \\beta_{1} / \\beta_{2}\\). From Table \\(4.1\\) we have the point estimate \\(\\widehat{\\theta}=29.8\\) and we can calculate the standard error \\(s(\\widehat{\\theta})=0.022\\) for a 95% confidence interval \\([29.8,29.9]\\). However, if we instead invert the linear form of the test we numerically find the interval \\([29.1,30.6]\\) which is much larger. From the evidence presented in Section \\(9.17\\) we know the first interval can be quite inaccurate and the second interval is greatly preferred."
  },
  {
    "objectID": "chpt09-hypothesit-test.html#multiple-tests-and-bonferroni-corrections",
    "href": "chpt09-hypothesit-test.html#multiple-tests-and-bonferroni-corrections",
    "title": "9  Hypothesis Testing",
    "section": "9.20 Multiple Tests and Bonferroni Corrections",
    "text": "9.20 Multiple Tests and Bonferroni Corrections\nIn most applications economists examine a large number of estimates, test statistics, and p-values. What does it mean (or does it mean anything) if one statistic appears to be “significant” after examining a large number of statistics? This is known as the problem of multiple testing or multiple comparisons.\nTo be specific, suppose we examine a set of \\(k\\) coefficients, standard errors and t-ratios, and consider the “significance” of each statistic. Based on conventional reasoning, for each coefficient we would reject the hypothesis that the coefficient is zero with asymptotic size \\(\\alpha\\) if the absolute t-statistic exceeds the \\(1-\\alpha\\) critical value of the normal distribution, or equivalently if the \\(\\mathrm{p}\\)-value for the t-statistic is smaller than \\(\\alpha\\). If we observe that one of the \\(k\\) statistics is “significant” based on this criterion, that means that one of the p-values is smaller than \\(\\alpha\\), or equivalently, that the smallest p-value is smaller than \\(\\alpha\\). We can then rephrase the question: Under the joint hypothesis that a set of \\(k\\) hypotheses are all true, what is the probability that the smallest \\(\\mathrm{p}\\)-value is smaller than \\(\\alpha\\) ? In general, we cannot provide a precise answer to this quesion, but the Bonferroni correction bounds this probability by \\(\\alpha k\\). The Bonferroni method furthermore suggests that if we want the familywise error probability (the probability that one of the tests falsely rejects) to be bounded below \\(\\alpha\\), then an appropriate rule is to reject only if the smallest p-value is smaller than \\(\\alpha / k\\). Equivalently, the Bonferroni familywise \\(\\mathrm{p}\\)-value is \\(k \\min _{j \\leq k} p_{j}\\).\nFormally, suppose we have \\(k\\) hypotheses \\(\\mathbb{M}_{j}, j=1, \\ldots, k\\). For each we have a test and associated pvalue \\(p_{j}\\) with the property that when \\(\\mathbb{H}_{j}\\) is true \\(\\lim _{n \\rightarrow \\infty} \\mathbb{P}\\left[p_{j}<\\alpha\\right]=\\alpha\\). We then observe that among the \\(k\\) tests, one of the \\(k\\) is “significant” if \\(\\min _{j \\leq k} p_{j}<\\alpha\\). This event can be written as\n\\[\n\\left\\{\\min _{j \\leq k} p_{j}<\\alpha\\right\\}=\\bigcup_{j=1}^{k}\\left\\{p_{j}<\\alpha\\right\\} .\n\\]\nBoole’s inequality states that for any \\(k\\) events \\(A_{j}, \\mathbb{P}\\left[\\bigcup_{j=1}^{k} A_{j}\\right] \\leq \\sum_{j=1}^{k} \\mathbb{P}\\left[A_{k}\\right]\\). Thus\n\\[\n\\mathbb{P}\\left[\\min _{j \\leq k} p_{j}<\\alpha\\right] \\leq \\sum_{j=1}^{k} \\mathbb{P}\\left[p_{j}<\\alpha\\right] \\rightarrow k \\alpha\n\\]\nas stated. This demonstates that the asymptotic familywise rejection probability is at most \\(k\\) times the individual rejection probability.\nFurthermore,\n\\[\n\\mathbb{P}\\left[\\min _{j \\leq k} p_{j}<\\frac{\\alpha}{k}\\right] \\leq \\sum_{j=1}^{k} \\mathbb{P}\\left[p_{j}<\\frac{\\alpha}{k}\\right] \\rightarrow \\alpha .\n\\]\nThis demonstrates that the asymptotic familywise rejection probability can be controlled (bounded below \\(\\alpha\\) ) if each individual test is subjected to the stricter standard that a p-value must be smaller than \\(\\alpha / k\\) to be labeled as “significant”.\nTo illustrate, suppose we have two coefficient estimates with individual p-values \\(0.04\\) and \\(0.15\\). Based on a conventional \\(5 %\\) level the standard individual tests would suggest that the first coefficient estimate is “significant” but not the second. A Bonferroni 5% test, however, does not reject as it would require that the smallest p-value be smaller than \\(0.025\\), which is not the case in this example. Alternatively, the Bonferroni familywise \\(\\mathrm{p}\\)-value is \\(0.04 \\times 2=0.08\\), which is not significant at the \\(5 %\\) level.\nIn contrast, if the two p-values were \\(0.01\\) and \\(0.15\\), then the Bonferroni familywise p-value would be \\(0.01 \\times 2=0.02\\), which is significant at the \\(5 %\\) level."
  },
  {
    "objectID": "chpt09-hypothesit-test.html#power-and-test-consistency",
    "href": "chpt09-hypothesit-test.html#power-and-test-consistency",
    "title": "9  Hypothesis Testing",
    "section": "9.21 Power and Test Consistency",
    "text": "9.21 Power and Test Consistency\nThe power of a test is the probability of rejecting \\(\\mathbb{M}_{0}\\) when \\(\\mathbb{M}_{1}\\) is true.\nFor simplicity suppose that \\(Y_{i}\\) is i.i.d. \\(\\mathrm{N}\\left(\\theta, \\sigma^{2}\\right)\\) with \\(\\sigma^{2}\\) known, consider the t-statistic \\(T(\\theta)=\\sqrt{n}(\\bar{Y}-\\theta) / \\sigma\\), and tests of \\(\\mathbb{M}_{0}: \\theta=0\\) against \\(\\mathbb{M}_{1}: \\theta>0\\). We reject \\(\\mathbb{H}_{0}\\) if \\(T=T(0)>c\\). Note that\n\\[\nT=T(\\theta)+\\sqrt{n} \\theta / \\sigma\n\\]\nand \\(T(\\theta)\\) has an exact \\(\\mathrm{N}(0,1)\\) distribution. This is because \\(T(\\theta)\\) is centered at the true mean \\(\\theta\\), while the test statistic \\(T(0)\\) is centered at the (false) hypothesized mean of 0 .\nThe power of the test is\n\\[\n\\mathbb{P}[T>c \\mid \\theta]=\\mathbb{P}[\\mathrm{Z}+\\sqrt{n} \\theta / \\sigma>c]=1-\\Phi(c-\\sqrt{n} \\theta / \\sigma) .\n\\]\nThis function is monotonically increasing in \\(\\mu\\) and \\(n\\), and decreasing in \\(\\sigma\\) and \\(c\\).\nNotice that for any \\(c\\) and \\(\\theta \\neq 0\\) the power increases to 1 as \\(n \\rightarrow \\infty\\). This means that for \\(\\theta \\in \\mathbb{H}_{1}\\) the test will reject \\(\\mathbb{M}_{0}\\) with probability approaching 1 as the sample size gets large. We call this property test consistency.\nDefinition 9.3 A test of \\(\\mathbb{H}_{0}: \\theta \\in \\Theta_{0}\\) is consistent against fixed alternatives if for all \\(\\theta \\in \\Theta_{1}, \\mathbb{P}\\left[\\right.\\) Reject \\(\\left.\\mathbb{M}_{0} \\mid \\theta\\right] \\rightarrow 1\\) as \\(n \\rightarrow \\infty\\).\nFor tests of the form “Reject \\(\\mathbb{H}_{0}\\) if \\(T>c\\)”, a sufficient condition for test consistency is that the \\(T\\) diverges to positive infinity with probability one for all \\(\\theta \\in \\Theta_{1}\\). Definition 9.4 We say that \\(T \\underset{p}{\\rightarrow}\\) as \\(n \\rightarrow \\infty\\) if for all \\(M<\\infty, \\mathbb{P}[T \\leq M] \\rightarrow 0\\) as \\(n \\rightarrow \\infty\\). Similarly, we say that \\(T \\underset{p}{\\rightarrow}-\\infty\\) as \\(n \\rightarrow \\infty\\) if for all \\(M<\\infty\\), \\(\\mathbb{P}[T \\geq-M] \\rightarrow 0\\) as \\(n \\rightarrow \\infty\\)\nIn general, t-tests and Wald tests are consistent against fixed alternatives. Take a t-statistic for a test of \\(\\mathbb{\\sharp}_{0}: \\theta=\\theta_{0}, T=\\left(\\widehat{\\theta}-\\theta_{0}\\right) / s(\\widehat{\\theta})\\) where \\(\\theta_{0}\\) is a known value and \\(s(\\widehat{\\theta})=\\sqrt{n^{-1} \\widehat{V}_{\\theta}}\\). Note that\n\\[\nT=\\frac{\\widehat{\\theta}-\\theta}{s(\\widehat{\\theta})}+\\frac{\\sqrt{n}\\left(\\theta-\\theta_{0}\\right)}{\\sqrt{\\widehat{V}_{\\theta}}} .\n\\]\nThe first term on the right-hand-side converges in distribution to \\(\\mathrm{N}(0,1)\\). The second term on the righthand-side equals zero if \\(\\theta=\\theta_{0}\\), converges in probability to \\(+\\infty\\) if \\(\\theta>\\theta_{0}\\), and converges in probability to \\(-\\infty\\) if \\(\\theta<\\theta_{0}\\). Thus the two-sided t-test is consistent against \\(\\mathbb{M}_{1}: \\theta \\neq \\theta_{0}\\), and one-sided t-tests are consistent against the alternatives for which they are designed.\nTheorem 9.8 Under Assumptions 7.2, 7.3, and 7.4, for \\(\\theta=r(\\beta) \\neq \\theta_{0}\\) and \\(q=1\\), then \\(|T| \\underset{p}{\\longrightarrow}\\). For any \\(c<\\infty\\) the test “Reject \\(\\mathbb{H}_{0}\\) if \\(|T|>c\\)” is consistent against fixed alternatives.\nThe Wald statistic for \\(\\mathbb{M}_{0}: \\theta=r(\\beta)=\\theta_{0}\\) against \\(\\mathbb{M}_{1}: \\theta \\neq \\theta_{0}\\) is \\(W=n\\left(\\widehat{\\theta}-\\theta_{0}\\right)^{\\prime} \\widehat{\\boldsymbol{V}}_{\\theta}^{-1}\\left(\\widehat{\\theta}-\\theta_{0}\\right)\\). Under \\(\\mathbb{H}_{1}\\), \\(\\widehat{\\theta} \\underset{p}{\\longrightarrow} \\theta \\neq \\theta_{0}\\). Thus \\(\\left(\\widehat{\\theta}-\\theta_{0}\\right)^{\\prime} \\widehat{\\boldsymbol{V}}_{\\theta}^{-1}\\left(\\widehat{\\theta}-\\theta_{0}\\right) \\underset{p}{\\longrightarrow}\\left(\\theta-\\theta_{0}\\right)^{\\prime} \\boldsymbol{V}_{\\theta}^{-1}\\left(\\theta-\\theta_{0}\\right)>0\\). Hence under \\(\\mathbb{H}_{1}, W \\underset{p}{\\longrightarrow}\\). Again, this implies that Wald tests are consistent.\nTheorem 9.9 Under Assumptions 7.2, 7.3, and 7.4, for \\(\\theta=r(\\beta) \\neq \\theta_{0}\\), then \\(W \\underset{p}{\\longrightarrow}\\). For any \\(c<\\infty\\) the test “Reject \\(\\mathbb{M}_{0}\\) if \\(W>c\\)” is consistent against fixed alternatives."
  },
  {
    "objectID": "chpt09-hypothesit-test.html#asymptotic-local-power",
    "href": "chpt09-hypothesit-test.html#asymptotic-local-power",
    "title": "9  Hypothesis Testing",
    "section": "9.22 Asymptotic Local Power",
    "text": "9.22 Asymptotic Local Power\nConsistency is a good property for a test but is does not provided a tool to calculate test power. To approximate the power function we need a distributional approximation.\nThe standard asymptotic method for power analysis uses what are called local alternatives. This is similar to our analysis of restriction estimation under misspecification (Section 8.13). The technique is to index the parameter by sample size so that the asymptotic distribution of the statistic is continuous in a localizing parameter. In this section we consider t-tests on real-valued parameters and in the next section Wald tests. Specifically, we consider parameter vectors \\(\\beta_{n}\\) which are indexed by sample size \\(n\\) and satisfy the real-valued relationship\n\\[\n\\theta_{n}=r\\left(\\beta_{n}\\right)=\\theta_{0}+n^{-1 / 2} h\n\\]\nwhere the scalar \\(h\\) is called a localizing parameter. We index \\(\\beta_{n}\\) and \\(\\theta_{n}\\) by sample size to indicate their dependence on \\(n\\). The way to think of (9.17) is that the true value of the parameters are \\(\\beta_{n}\\) and \\(\\theta_{n}\\). The parameter \\(\\theta_{n}\\) is close to the hypothesized value \\(\\theta_{0}\\), with deviation \\(n^{-1 / 2} h\\).\nThe specification (9.17) states that for any fixed \\(h, \\theta_{n}\\) approaches \\(\\theta_{0}\\) as \\(n\\) gets large. Thus \\(\\theta_{n}\\) is “close” or “local” to \\(\\theta_{0}\\). The concept of a localizing sequence (9.17) might seem odd since in the actual world the sample size cannot mechanically affect the value of the parameter. Thus (9.17) should not be interpreted literally. Instead, it should be interpreted as a technical device which allows the asymptotic distribution to be continuous in the alternative hypothesis.\nTo evaluate the asymptotic distribution of the test statistic we start by examining the scaled estimator centered at the hypothesized value \\(\\theta_{0}\\). Breaking it into a term centered at the true value \\(\\theta_{n}\\) and a remainder we find\n\\[\n\\sqrt{n}\\left(\\widehat{\\theta}-\\theta_{0}\\right)=\\sqrt{n}\\left(\\widehat{\\theta}-\\theta_{n}\\right)+\\sqrt{n}\\left(\\theta_{n}-\\theta_{0}\\right)=\\sqrt{n}\\left(\\widehat{\\theta}-\\theta_{n}\\right)+h\n\\]\nwhere the second equality is (9.17). The first term is asymptotically normal:\n\\[\n\\sqrt{n}\\left(\\widehat{\\theta}-\\theta_{n}\\right) \\underset{d}{\\longrightarrow} \\sqrt{V_{\\theta}} Z\n\\]\nwhere \\(Z \\sim \\mathrm{N}(0,1)\\). Therefore\n\\[\n\\sqrt{n}\\left(\\widehat{\\theta}-\\theta_{0}\\right) \\underset{d}{\\longrightarrow} \\sqrt{V_{\\theta}} Z+h \\sim \\mathrm{N}\\left(h, V_{\\theta}\\right) .\n\\]\nThis asymptotic distribution depends continuously on the localizing parameter \\(h\\).\nApplied to the \\(t\\) statistic we find\n\\[\nT=\\frac{\\widehat{\\theta}-\\theta_{0}}{s(\\widehat{\\theta})} \\underset{d}{\\longrightarrow} \\frac{\\sqrt{V_{\\theta}} Z+h}{\\sqrt{V_{\\theta}}} \\sim Z+\\delta\n\\]\nwhere \\(\\delta=h / \\sqrt{V_{\\theta}}\\). This generalizes Theorem \\(9.1\\) (which assumes \\(\\mathbb{M}_{0}\\) is true) to allow for local alternatives of the form (9.17).\nConsider a t-test of \\(\\mathbb{M}_{0}\\) against the one-sided alternative \\(\\mathbb{M}_{1}: \\theta>\\theta_{0}\\) which rejects \\(\\mathbb{H}_{0}\\) for \\(T>c\\) where \\(\\Phi(c)=1-\\alpha\\). The asymptotic local power of this test is the limit (as the sample size diverges) of the rejection probability under the local alternative (9.17)\n\\[\n\\begin{aligned}\n\\lim _{n \\rightarrow \\infty} \\mathbb{P}\\left[\\text { Reject } \\mathbb{M}_{0}\\right] &=\\lim _{n \\rightarrow \\infty} \\mathbb{P}[T>c] \\\\\n&=\\mathbb{P}[Z+\\delta>c] \\\\\n&=1-\\Phi(c-\\delta) \\\\\n&=\\Phi(\\delta-c) \\\\\n& \\stackrel{\\text { def }}{=} \\pi(\\delta) .\n\\end{aligned}\n\\]\nWe call \\(\\pi(\\delta)\\) the asymptotic local power function.\nIn Figure 9.3(a) we plot the local power function \\(\\pi(\\delta)\\) as a function of \\(\\delta \\in[-1,4]\\) for tests of asymptotic size \\(\\alpha=0.05\\) and \\(\\alpha=0.01 . \\delta=0\\) corresponds to the null hypothesis so \\(\\pi(\\delta)=\\alpha\\). The power functions are monotonically increasing in \\(\\delta\\). Note that the power is lower than \\(\\alpha\\) for \\(\\delta<0\\) due to the one-sided nature of the test.\nWe can see that the power functions are ranked by \\(\\alpha\\) so that the test with \\(\\alpha=0.05\\) has higher power than the test with \\(\\alpha=0.01\\). This is the inherent trade-off between size and power. Decreasing size induces a decrease in power, and conversely.\n\n\nOne-Sided t Test\n\n\n\nVector Case\n\nFigure 9.3: Asymptotic Local Power Function\nThe coefficient \\(\\delta\\) can be interpreted as the parameter deviation measured as a multiple of the standard error \\(s(\\widehat{\\theta})\\). To see this, recall that \\(s(\\widehat{\\theta})=n^{-1 / 2} \\sqrt{\\widehat{V}_{\\theta}} \\simeq n^{-1 / 2} \\sqrt{V_{\\theta}}\\) and then note that\n\\[\n\\delta=\\frac{h}{\\sqrt{V_{\\theta}}} \\simeq \\frac{n^{-1 / 2} h}{s(\\widehat{\\theta})}=\\frac{\\theta_{n}-\\theta_{0}}{s(\\widehat{\\theta})} .\n\\]\nThus \\(\\delta\\) approximately equals the deviation \\(\\theta_{n}-\\theta_{0}\\) expressed as multiples of the standard error \\(s(\\widehat{\\theta})\\). Thus as we examine Figure 9.3(a) we can interpret the power function at \\(\\delta=1\\) (e.g. \\(26 %\\) for a 5% size test) as the power when the parameter \\(\\theta_{n}\\) is one standard error above the hypothesized value. For example, from Table \\(4.2\\) the standard error for the coefficient on “Married Female” is \\(0.010\\). Thus, in this example \\(\\delta=1\\) corresponds to \\(\\theta_{n}=0.010\\) or an \\(1.0 %\\) wage premium for married females. Our calculations show that the asymptotic power of a one-sided \\(5 %\\) test against this alternative is about \\(26 %\\).\nThe difference between power functions can be measured either vertically or horizontally. For example, in Figure 9.3(a) there is a vertical dashed line at \\(\\delta=1\\), showing that the asymptotic local power function \\(\\pi(\\delta)\\) equals \\(26 %\\) for \\(\\alpha=0.0\\), and \\(9 %\\) for \\(\\alpha=0.01\\). This is the difference in power across tests of differing size, holding fixed the parameter in the alternative.\nA horizontal comparison can also be illuminating. To illustrate, in Figure 9.3(a) there is a horizontal dashed line at \\(50 %\\) power. \\(50 %\\) power is a useful benchmark as it is the point where the test has equal odds of rejection and acceptance. The dotted line crosses the two power curves at \\(\\delta=1.65(\\alpha=0.05)\\) and \\(\\delta=2.33(\\alpha=0.01)\\). This means that the parameter \\(\\theta\\) must be at least \\(1.65\\) standard errors above the hypothesized value for a one-sided \\(5 %\\) test to have \\(50 %\\) (approximate) power, and \\(2.33\\) standard errors for a one-sided \\(1 %\\) test.\nThe ratio of these values (e.g. 2.33/1.65 = 1.41) measures the relative parameter magnitude needed to achieve the same power. (Thus, for a \\(1 %\\) size test to achieve \\(50 %\\) power, the parameter must be \\(41 %\\) larger than for a \\(5 %\\) size test.) Even more interesting, the square of this ratio (e.g. \\(1.41^{2}=2\\) ) is the increase in sample size needed to achieve the same power under fixed parameters. That is, to achieve \\(50 %\\) power, a \\(1 %\\) size test needs twice as many observations as a \\(5 %\\) size test. This interpretation follows by the following informal argument. By definition and (9.17) \\(\\delta=h / \\sqrt{V_{\\theta}}=\\sqrt{n}\\left(\\theta_{n}-\\theta_{0}\\right) / \\sqrt{V_{\\theta}}\\). Thus holding \\(\\theta\\) and \\(V_{\\theta}\\) fixed, \\(\\delta^{2}\\) is proportional to \\(n\\).\nThe analysis of a two-sided t test is similar. (9.18) implies that\n\\[\nT=\\left|\\frac{\\widehat{\\theta}-\\theta_{0}}{s(\\widehat{\\theta})}\\right| \\vec{d}|Z+\\delta|\n\\]\nand thus the local power of a two-sided t test is\n\\[\n\\lim _{n \\rightarrow \\infty} \\mathbb{P}\\left[\\text { Reject } \\mathbb{H}_{0}\\right]=\\lim _{n \\rightarrow \\infty} \\mathbb{P}[T>c]=\\mathbb{P}[|Z+\\delta|>c]=\\Phi(\\delta-c)+\\Phi(-\\delta-c)\n\\]\nwhich is monotonically increasing in \\(|\\delta|\\).\nTheorem 9.10 Under Assumptions 7.2, 7.3,7.4, and \\(\\theta_{n}=r\\left(\\beta_{n}\\right)=r_{0}+n^{-1 / 2} h\\), then\n\\[\nT\\left(\\theta_{0}\\right)=\\frac{\\widehat{\\theta}-\\theta_{0}}{s(\\widehat{\\theta})} \\underset{d}{\\longrightarrow} Z+\\delta\n\\]\nwhere \\(Z \\sim \\mathrm{N}(0,1)\\) and \\(\\delta=h / \\sqrt{V_{\\theta}}\\). For \\(c\\) such that \\(\\Phi(c)=1-\\alpha\\),\n\\[\n\\mathbb{P}\\left[T\\left(\\theta_{0}\\right)>c\\right] \\longrightarrow \\Phi(\\delta-c) .\n\\]\nFurthermore, for \\(c\\) such that \\(\\Phi(c)=1-\\alpha / 2\\),\n\\[\n\\mathbb{P}\\left[\\left|T\\left(\\theta_{0}\\right)\\right|>c\\right] \\longrightarrow \\Phi(\\delta-c)+\\Phi(-\\delta-c) .\n\\]"
  },
  {
    "objectID": "chpt09-hypothesit-test.html#asymptotic-local-power-vector-case",
    "href": "chpt09-hypothesit-test.html#asymptotic-local-power-vector-case",
    "title": "9  Hypothesis Testing",
    "section": "9.23 Asymptotic Local Power, Vector Case",
    "text": "9.23 Asymptotic Local Power, Vector Case\nIn this section we extend the local power analysis of the previous section to the case of vector-valued alternatives. We generalize (9.17) to vector-valued \\(\\theta_{n}\\). The local parameterization is\n\\[\n\\theta_{n}=r\\left(\\beta_{n}\\right)=\\theta_{0}+n^{-1 / 2} h\n\\]\nwhere \\(h\\) is \\(q \\times 1\\).\nUnder (9.19),\n\\[\n\\sqrt{n}\\left(\\widehat{\\theta}-\\theta_{0}\\right)=\\sqrt{n}\\left(\\widehat{\\theta}-\\theta_{n}\\right)+h \\underset{d}{\\longrightarrow} Z_{h} \\sim \\mathrm{N}\\left(h, \\boldsymbol{V}_{\\theta}\\right),\n\\]\na normal random vector with mean \\(h\\) and covariance matrix \\(\\boldsymbol{V}_{\\theta}\\).\nApplied to the Wald statistic we find\n\\[\nW=n\\left(\\widehat{\\theta}-\\theta_{0}\\right)^{\\prime} \\widehat{\\boldsymbol{V}}_{\\theta}^{-1}\\left(\\widehat{\\theta}-\\theta_{0}\\right) \\underset{d}{\\longrightarrow} Z_{h}^{\\prime} \\boldsymbol{V}_{\\theta}^{-1} Z_{h} \\sim \\chi_{q}^{2}(\\lambda)\n\\]\nwhere \\(\\lambda=h^{\\prime} \\boldsymbol{V}^{-1} h . \\chi_{q}^{2}(\\lambda)\\) is a non-central chi-square random variable with non-centrality parameter \\(\\lambda\\). (Theorem 5.3.6.)\nThe convergence (9.20) shows that under the local alternatives (9.19), W \\(\\underset{d}{ } \\chi_{q}^{2}(\\lambda)\\). This generalizes the null asymptotic distribution which obtains as the special case \\(\\lambda=0\\). We can use this result to obtain a continuous asymptotic approximation to the power function. For any significance level \\(\\alpha>0\\) set the asymptotic critical value \\(c\\) so that \\(\\mathbb{P}\\left[\\chi_{q}^{2}>c\\right]=\\alpha\\). Then as \\(n \\rightarrow \\infty\\),\n\\[\n\\mathbb{P}[W>c] \\longrightarrow \\mathbb{P}\\left[\\chi_{q}^{2}(\\lambda)>c\\right] \\stackrel{\\text { def }}{=} \\pi(\\lambda) .\n\\]\nThe asymptotic local power function \\(\\pi(\\lambda)\\) depends only on \\(\\alpha, q\\), and \\(\\lambda\\).\nTheorem 9.11 Under Assumptions 7.2, 7.3, 7.4, and \\(\\theta_{n}=r\\left(\\beta_{n}\\right)=\\theta_{0}+n^{-1 / 2} h\\), then \\(W \\underset{d}{\\longrightarrow} \\chi_{q}^{2}(\\lambda)\\) where \\(\\lambda=h^{\\prime} \\boldsymbol{V}_{\\theta}^{-1} h\\). Furthermore, for \\(c\\) such that \\(\\mathbb{P}\\left[\\chi_{q}^{2}>c\\right]=\\) \\(\\alpha, \\mathbb{P}[W>c] \\longrightarrow \\mathbb{P}\\left[\\chi_{q}^{2}(\\lambda)>c\\right]\\)\nFigure 9.3(b) plots \\(\\pi(\\lambda)\\) as a function of \\(\\lambda\\) for \\(q=1, q=2\\), and \\(q=3\\), and \\(\\alpha=0.05\\). The asymptotic power functions are monotonically increasing in \\(\\lambda\\) and asymptote to one.\nFigure 9.3(b) also shows the power loss for fixed non-centrality parameter \\(\\lambda\\) as the dimensionality of the test increases. The power curves shift to the right as \\(q\\) increases, resulting in a decrease in power. This is illustrated by the dashed line at \\(50 %\\) power. The dashed line crosses the three power curves at \\(\\lambda=3.85(q=1), \\lambda=4.96(q=2)\\), and \\(\\lambda=5.77(q=3)\\). The ratio of these \\(\\lambda\\) values correspond to the relative sample sizes needed to obtain the same power. Thus increasing the dimension of the test from \\(q=1\\) to \\(q=2\\) requires a \\(28 %\\) increase in sample size, or an increase from \\(q=1\\) to \\(q=3\\) requires a \\(50 %\\) increase in sample size, to maintain \\(50 %\\) power."
  },
  {
    "objectID": "chpt09-hypothesit-test.html#exercises",
    "href": "chpt09-hypothesit-test.html#exercises",
    "title": "9  Hypothesis Testing",
    "section": "9.24 Exercises",
    "text": "9.24 Exercises\nExercise 9.1 Prove that if an additional regressor \\(\\boldsymbol{X}_{k+1}\\) is added to \\(\\boldsymbol{X}\\), Theil’s adjusted \\(\\bar{R}^{2}\\) increases if and only if \\(\\left|T_{k+1}\\right|>1\\), where \\(T_{k+1}=\\widehat{\\beta}_{k+1} / s\\left(\\widehat{\\beta}_{k+1}\\right)\\) is the t-ratio for \\(\\widehat{\\beta}_{k+1}\\) and\n\\[\ns\\left(\\widehat{\\beta}_{k+1}\\right)=\\left(s^{2}\\left[\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1}\\right]_{k+1, k+1}\\right)^{1 / 2}\n\\]\nis the homoskedasticity-formula standard error.\nExercise 9.2 You have two independent samples \\(\\left(Y_{1 i}, X_{1 i}\\right)\\) and \\(\\left(Y_{2 i}, X_{2 i}\\right)\\) both with sample sizes \\(n\\) which satisfy \\(Y_{1}=X_{1}^{\\prime} \\beta_{1}+e_{1}\\) and \\(Y_{2}=X_{2}^{\\prime} \\beta_{2}+e_{2}\\), where \\(\\mathbb{E}\\left[X_{1} e_{1}\\right]=0\\) and \\(\\mathbb{E}\\left[X_{2} e_{2}\\right]=0\\). Let \\(\\widehat{\\beta}_{1}\\) and \\(\\widehat{\\beta}_{2}\\) be the OLS estimators of \\(\\beta_{1} \\in \\mathbb{R}^{k}\\) and \\(\\beta_{2} \\in \\mathbb{R}^{k}\\).\n\nFind the asymptotic distribution of \\(\\sqrt{n}\\left(\\left(\\widehat{\\beta}_{2}-\\widehat{\\beta}_{1}\\right)-\\left(\\beta_{2}-\\beta_{1}\\right)\\right)\\) as \\(n \\rightarrow \\infty\\).\nFind an appropriate test statistic for \\(\\mathbb{H}_{0}: \\beta_{2}=\\beta_{1}\\).\nFind the asymptotic distribution of this statistic under \\(\\mathbb{H}_{0}\\).\n\nExercise 9.3 Let \\(T\\) be a t-statistic for \\(\\mathbb{H}_{0}: \\theta=0\\) versus \\(\\mathbb{H}_{1}: \\theta \\neq 0\\). Since \\(|T| \\rightarrow{ }_{d}|Z|\\) under \\(\\mathbb{H}_{0}\\), someone suggests the test “Reject \\(\\mathbb{M}_{0}\\) if \\(|T|<c_{1}\\) or \\(|T|>c_{2}\\), where \\(c_{1}\\) is the \\(\\alpha / 2\\) quantile of \\(|Z|\\) and \\(c_{2}\\) is the \\(1-\\alpha / 2\\) quantile of \\(|Z|\\).\n\nShow that the asymptotic size of the test is \\(\\alpha\\). (b) Is this a good test of \\(\\mathbb{M}_{0}\\) versus \\(\\mathbb{M}_{1}\\) ? Why or why not?\n\nExercise 9.4 Let \\(W\\) be a Wald statistic for \\(\\mathbb{M}_{0}: \\theta=0\\) versus \\(\\mathbb{M}_{1}: \\theta \\neq 0\\), where \\(\\theta\\) is \\(q \\times 1\\). Since \\(W \\underset{d}{\\rightarrow} \\chi_{q}^{2}\\) under \\(H_{0}\\), someone suggests the test “Reject \\(\\mathbb{H}_{0}\\) if \\(W<c_{1}\\) or \\(W>c_{2}\\), where \\(c_{1}\\) is the \\(\\alpha / 2\\) quantile of \\(\\chi_{q}^{2}\\) and \\(c_{2}\\) is the \\(1-\\alpha / 2\\) quantile of \\(\\chi_{q}^{2}\\).\n\nShow that the asymptotic size of the test is \\(\\alpha\\).\nIs this a good test of \\(\\mathbb{M}_{0}\\) versus \\(\\mathbb{H}_{1}\\) ? Why or why not?\n\nExercise 9.5 Take the linear model \\(Y=X_{1}^{\\prime} \\beta_{1}+X_{2}^{\\prime} \\beta_{2}+e\\) with \\(\\mathbb{E}[X e]=0\\) where both \\(X_{1}\\) and \\(X_{2}\\) are \\(q \\times 1\\). Show how to test the hypotheses \\(\\mathbb{M}_{0}: \\beta_{1}=\\beta_{2}\\) against \\(\\mathbb{M}_{1}: \\beta_{1} \\neq \\beta_{2}\\).\nExercise 9.6 Suppose a researcher wants to know which of a set of 20 regressors has an effect on a variable testscore. He regresses testscore on the 20 regressors and reports the results. One of the 20 regressors (studytime) has a large t-ratio (about 2.5), while the other t-ratios are insignificant (smaller than 2 in absolute value). He argues that the data show that studytime is the key predictor for testscore. Do you agree with this conclusion? Is there a deficiency in his reasoning?\nExercise 9.7 Take the model \\(Y=X \\beta_{1}+X^{2} \\beta_{2}+e\\) with \\(\\mathbb{E}[e \\mid X]=0\\) where \\(Y\\) is wages (dollars per hour) and \\(X\\) is age. Describe how you would test the hypothesis that the expected wage for a 40 -year-old worker is \\(\\$ 20\\) an hour.\nExercise 9.8 You want to test \\(\\mathbb{H}_{0}: \\beta_{2}=0\\) against \\(\\mathbb{H}_{1}: \\beta_{2} \\neq 0\\) in the model \\(Y=X_{1}^{\\prime} \\beta_{1}+X_{2}^{\\prime} \\beta_{2}+e\\) with \\(\\mathbb{E}[X e]=0\\). You read a paper which estimates the model\n\\[\nY=X_{1}^{\\prime} \\widehat{\\gamma}_{1}+\\left(X_{2}-X_{1}\\right)^{\\prime} \\widehat{\\gamma}_{2}+u\n\\]\nand reports a test of \\(\\mathbb{M}_{0}: \\gamma_{2}=0\\) against \\(\\mathbb{M}_{1}: \\gamma_{2} \\neq 0\\). Is this related to the test you wanted to conduct?\nExercise 9.9 Suppose a researcher uses one dataset to test a specific hypothesis \\(\\mathbb{H}_{0}\\) against \\(\\mathbb{H}_{1}\\) and finds that he can reject \\(\\mathbb{H}_{0}\\). A second researcher gathers a similar but independent dataset, uses similar methods and finds that she cannot reject \\(\\mathbb{M}_{0}\\). How should we (as interested professionals) interpret these mixed results?\nExercise 9.10 In Exercise \\(7.8\\) you showed that \\(\\sqrt{n}\\left(\\widehat{\\sigma}^{2}-\\sigma^{2}\\right) \\underset{d}{\\rightarrow} \\mathrm{N}(0, V)\\) as \\(n \\rightarrow \\infty\\) for some \\(V\\). Let \\(\\widehat{V}\\) be an estimator of \\(V\\).\n\nUsing this result construct a t-statistic for \\(\\mathbb{H}_{0}: \\sigma^{2}=1\\) against \\(\\mathbb{H}_{1}: \\sigma^{2} \\neq 1\\).\nUsing the Delta Method find the asymptotic distribution of \\(\\sqrt{n}(\\widehat{\\sigma}-\\sigma)\\).\nUse the previous result to construct a t-statistic for \\(\\mathbb{M}_{0}: \\sigma=1\\) against \\(\\mathbb{H}_{1}: \\sigma \\neq 1\\).\nAre the null hypotheses in (a) and (c) the same or are they different? Are the tests in (a) and (c) the same or are they different? If they are different, describe a context in which the two tests would give contradictory results.\n\nExercise 9.11 Consider a regression such as Table \\(4.1\\) where both experience and its square are included. A researcher wants to test the hypothesis that experience does not affect mean wages and does this by computing the t-statistic for experience. Is this the correct approach? If not, what is the appropriate testing method? Exercise 9.12 A researcher estimates a regression and computes a test of \\(\\mathbb{H}_{0}\\) against \\(\\mathbb{H}_{1}\\) and finds a pvalue of \\(p=0.08\\), or “not significant”. She says “I need more data. If I had a larger sample the test will have more power and then the test will reject.” Is this interpretation correct?\nExercise 9.13 A common view is that “If the sample size is large enough, any hypothesis will be rejected.” What does this mean? Interpret and comment.\nExercise 9.14 Take the model \\(Y=X^{\\prime} \\beta+e\\) with \\(\\mathbb{E}[X e]=0\\) and parameter of interest \\(\\theta=\\boldsymbol{R}^{\\prime} \\beta\\) with \\(\\boldsymbol{R} k \\times 1\\). Let \\(\\widehat{\\beta}\\) be the least squares estimator and \\(\\widehat{\\boldsymbol{V}}_{\\widehat{\\beta}}\\) its variance estimator.\n\nWrite down \\(\\widehat{C}\\), the \\(95 %\\) asymptotic confidence interval for \\(\\theta\\), in terms of \\(\\widehat{\\beta}, \\widehat{\\boldsymbol{V}} \\widehat{\\widehat{\\beta}}\\), \\(\\boldsymbol{R}\\), and \\(z=1.96\\) (the \\(97.5 %\\) quantile of \\(N(0,1))\\).\nShow that the decision “Reject \\(\\mathbb{M}_{0}\\) if \\(\\theta_{0} \\notin \\widehat{C}\\)” is an asymptotic \\(5 %\\) test of \\(\\mathbb{M}_{0}: \\theta=\\theta_{0}\\).\n\nExercise 9.15 You are at a seminar where a colleague presents a simulation study of a test of a hypothesis \\(\\mathbb{H}_{0}\\) with nominal size \\(5 %\\). Based on \\(B=100\\) simulation replications under \\(\\mathbb{H}_{0}\\) the estimated size is \\(7 %\\). Your colleague says: “Unfortunately the test over-rejects.”\n\nDo you agree or disagree with your colleague? Explain. Hint: Use an asymptotic (large B) approximation.\nSuppose the number of simulation replications were \\(B=1000\\) yet the estimated size is still \\(7 %\\). Does your answer change?\n\nExercise 9.16 Consider two alternative regression models\n\\[\n\\begin{aligned}\nY &=X_{1}^{\\prime} \\beta_{1}+e_{1} \\\\\n\\mathbb{E}\\left[X_{1} e_{1}\\right] &=0 \\\\\nY &=X_{2}^{\\prime} \\beta_{2}+e_{2} \\\\\n\\mathbb{E}\\left[X_{2} e_{2}\\right] &=0\n\\end{aligned}\n\\]\nwhere \\(X_{1}\\) and \\(X_{2}\\) have at least some different regressors. (For example, (9.21) is a wage regression on geographic variables and (2) is a wage regression on personal appearance measurements.) You want to know if model (9.21) or model (9.22) fits the data better. Define \\(\\sigma_{1}^{2}=\\mathbb{E}\\left[e_{1}^{2}\\right]\\) and \\(\\sigma_{2}^{2}=\\mathbb{E}\\left[e_{2}^{2}\\right]\\). You decide that the model with the smaller variance fit (e.g., model (9.21) fits better if \\(\\sigma_{1}^{2}<\\sigma_{2}^{2}\\).) You decide to test for this by testing the hypothesis of equal fit \\(\\mathbb{H}_{0}: \\sigma_{1}^{2}=\\sigma_{2}^{2}\\) against the alternative of unequal fit \\(\\mathbb{H}_{1}: \\sigma_{1}^{2} \\neq \\sigma_{2}^{2}\\). For simplicity, suppose that \\(e_{1 i}\\) and \\(e_{2 i}\\) are observed.\n\nConstruct an estimator \\(\\widehat{\\theta}\\) of \\(\\theta=\\sigma_{1}^{2}-\\sigma_{2}^{2}\\).\nFind the asymptotic distribution of \\(\\sqrt{n}(\\widehat{\\theta}-\\theta)\\) as \\(n \\rightarrow \\infty\\).\nFind an estimator of the asymptotic variance of \\(\\widehat{\\theta}\\).\nPropose a test of asymptotic size \\(\\alpha\\) of \\(\\mathbb{M}_{0}\\) against \\(\\mathbb{M}_{1}\\).\nSuppose the test accepts \\(\\mathbb{M}_{0}\\). Briefly, what is your interpretation? Exercise 9.17 You have two regressors \\(X_{1}\\) and \\(X_{2}\\) and estimate a regression with all quadratic terms included\n\n\\[\nY=\\alpha+\\beta_{1} X_{1}+\\beta_{2} X_{2}+\\beta_{3} X_{1}^{2}+\\beta_{4} X_{2}^{2}+\\beta_{5} X_{1} X_{2}+e .\n\\]\nOne of your advisors asks: Can we exclude the variable \\(X_{2}\\) from this regression?\nHow do you translate this question into a statistical test? When answering these questions, be specific, not general.\n\nWhat is the relevant null and alternative hypotheses?\nWhat is an appropriate test statistic?\nWhat is the appropriate asymptotic distribution for the statistic?\nWhat is the rule for acceptance/rejection of the null hypothesis?\n\nExercise 9.18 The observed data is \\(\\left\\{Y_{i}, X_{i}, Z_{i}\\right\\} \\in \\mathbb{R} \\times \\mathbb{R}^{k} \\times \\mathbb{R}^{\\ell}, k>1\\) and \\(\\ell>1, i=1, \\ldots, n\\). An econometrician first estimates \\(Y_{i}=X_{i}^{\\prime} \\widehat{\\beta}+\\widehat{e}_{i}\\) by least squares. The econometrician next regresses the residual \\(\\widehat{e}_{i}\\) on \\(Z_{i}\\), which can be written as \\(\\widehat{e}_{i}=Z_{i}^{\\prime} \\widetilde{\\gamma}+\\widetilde{u}_{i}\\).\n\nDefine the population parameter \\(\\gamma\\) being estimated in this second regression.\nFind the probability limit for \\(\\widetilde{\\gamma}\\).\nSuppose the econometrician constructs a Wald statistic \\(W\\) for \\(\\mathbb{H}_{0}: \\gamma=0\\) from the second regression, ignoring the two-stage estimation process. Write down the formula for \\(W\\).\nAssume \\(\\mathbb{E}\\left[Z X^{\\prime}\\right]=0\\). Find the asymptotic distribution for \\(W\\) under \\(\\mathbb{M}_{0}: \\gamma=0\\).\nIf \\(\\mathbb{E}\\left[Z X^{\\prime}\\right] \\neq 0\\) will your answer to (d) change?\n\nExercise 9.19 An economist estimates \\(Y=X_{1}^{\\prime} \\beta_{1}+X_{2} \\beta_{2}+e\\) by least squares and tests hypothesis \\(\\mathbb{H}_{0}: \\beta_{2}=0\\) against \\(\\mathbb{H}_{1}: \\beta_{2} \\neq 0\\). Assume \\(\\beta_{1} \\in \\mathbb{R}^{k}\\) and \\(\\beta_{2} \\in \\mathbb{R}\\). She obtains a Wald statistic \\(W=0.34\\). The sample size is \\(n=500\\).\n\nWhat is the correct degrees of freedom for the \\(\\chi^{2}\\) distribution to evaluate the significance of the Wald statistic?\nThe Wald statistic \\(W\\) is very small. Indeed, is it less than the \\(1 %\\) quantile of the appropriate \\(\\chi^{2}\\) distribution? If so, should you reject \\(\\mathbb{H}_{0}\\) ? Explain your reasoning.\n\nExercise 9.20 You are reading a paper, and it reports the results from two nested OLS regressions:\n\\[\n\\begin{aligned}\n&Y_{i}=X_{1 i}^{\\prime} \\widetilde{\\beta}_{1}+\\widetilde{e}_{i} \\\\\n&Y_{i}=X_{1 i}^{\\prime} \\widehat{\\beta}_{1}+X_{2 i}^{\\prime} \\widehat{\\beta}_{2}+\\widehat{e}_{i} .\n\\end{aligned}\n\\]\nSome summary statistics are reported:\n\\[\n\\begin{array}{ll}\n\\text { Short Regression } & \\text { Long Regression } \\\\\nR^{2}=.20 & R^{2}=.26 \\\\\n\\sum_{i=1}^{n} \\widetilde{e}_{i}^{2}=106 & \\sum_{i=1}^{n} \\widehat{e}_{i}^{2}=100 \\\\\n\\# \\text { of coefficients }=5 & \\# \\text { of coefficients }=8 \\\\\nn=50 & n=50\n\\end{array}\n\\]\nYou are curious if the estimate \\(\\widehat{\\beta}_{2}\\) is statistically different from the zero vector. Is there a way to determine an answer from this information? Do you have to make any assumptions (beyond the standard regularity conditions) to justify your answer? Exercise 9.21 Take the model \\(Y=X_{1} \\beta_{1}+X_{2} \\beta_{2}+X_{3} \\beta_{3}+X_{4} \\beta_{4}+e\\) with \\(\\mathbb{E}[X e]=0\\). Describe how to test\n\\[\n\\mathbb{M}_{0}: \\frac{\\beta_{1}}{\\beta_{2}}=\\frac{\\beta_{3}}{\\beta_{4}}\n\\]\nagainst\n\\[\n\\mathbb{M}_{1}: \\frac{\\beta_{1}}{\\beta_{2}} \\neq \\frac{\\beta_{3}}{\\beta_{4}} .\n\\]\nExercise 9.22 You have a random sample from the model \\(Y=X \\beta_{1}+X^{2} \\beta_{2}+e\\) with \\(\\mathbb{E}[e \\mid X]=0\\) where \\(Y\\) is wages (dollars per hour) and \\(X\\) is age. Describe how you would test the hypothesis that the expected wage for a 40 -year-old worker is \\(\\$ 20\\) an hour.\nExercise 9.23 Let \\(T\\) be a test statistic such that under \\(\\mathbb{M}_{0}, T \\underset{d}{\\longrightarrow} \\chi_{3}^{2}\\). Since \\(\\mathbb{P}\\left[\\chi_{3}^{2}>7.815\\right]=0.05\\), an asymptotic \\(5 %\\) test of \\(\\mathbb{H}_{0}\\) rejects when \\(T>7.815\\). An econometrician is interested in the Type I error of this test when \\(n=100\\) and the data structure is well specified. She performs the following Monte Carlo experiment.\n\n\\(B=200\\) samples of size \\(n=100\\) are generated from a distribution satisfying \\(\\mathbb{H}_{0}\\).\nOn each sample, the test statistic \\(T_{b}\\) is calculated.\nShe calculates \\(\\hat{p}=B^{-1} \\sum_{b=1}^{B} \\mathbb{1}\\left\\{T_{b}>7.815\\right\\}=0.070\\).\nThe econometrician concludes that the test \\(T\\) is oversized in this context-it rejects too frequently under \\(\\mathbb{M}_{0}\\).\n\nIs her conclusion correct, incorrect, or incomplete? Be specific in your answer.\nExercise 9.24 Do a Monte Carlo simulation. Take the model \\(Y=\\alpha+X \\beta+e\\) with \\(\\mathbb{E}[X e]=0\\) where the parameter of interest is \\(\\theta=\\exp (\\beta)\\). Your data generating process (DGP) for the simulation is: \\(X\\) is \\(U[0,1]\\), \\(e \\sim \\mathrm{N}(0,1)\\) is independent of \\(X\\), and \\(n=50\\). Set \\(\\alpha=0\\) and \\(\\beta=1\\). Generate \\(B=1000\\) independent samples with \\(\\alpha\\). On each, estimate the regression by least squares, calculate the covariance matrix using a standard (heteroskedasticity-robust) formula, and similarly estimate \\(\\theta\\) and its standard error. For each replication, store \\(\\widehat{\\beta}, \\widehat{\\theta}, T_{\\beta}=(\\widehat{\\beta}-\\beta) / s(\\widehat{\\beta})\\), and \\(T_{\\theta}=(\\widehat{\\theta}-\\theta) / s(\\widehat{\\theta})\\).\n\nDoes the value of \\(\\alpha\\) matter? Explain why the described statistics are invariant to \\(\\alpha\\) and thus setting \\(\\alpha=0\\) is irrelevant.\nFrom the 1000 replications estimate \\(\\mathbb{E}[\\widehat{\\beta}]\\) and \\(\\mathbb{E}[\\widehat{\\theta}]\\). Discuss if you see evidence if either estimator is biased or unbiased.\nFrom the 1000 replications estimate \\(\\mathbb{P}\\left[T_{\\beta}>1.645\\right]\\) and \\(\\mathbb{P}\\left[T_{\\theta}>1.645\\right]\\). What does asymptotic theory predict these probabilities should be in large samples? What do your simulation results indicate?\n\nExercise 9.25 The data set Invest1993 on the textbook website contains data on 1962 U.S. firms extracted from Compustat, assembled by Bronwyn Hall, and used in Hall and Hall (1993).\nThe variables we use in this exercise are in the table below. The flow variables are annual sums. The stock variables are beginning of year.\n\n\n\n\nyear\nyear of the observation\n\n\n\n\n\\(I\\)\ninva\nInvestment to Capital Ratio\n\n\n\\(Q\\)\nvala\nTotal Market Value to Asset Ratio (Tobin’s Q)\n\n\n\\(C\\)\ncfa\nCash Flow to Asset Ratio\n\n\n\\(D\\)\ndebta\nLong Term Debt to Asset Ratio\n\n\n\n\nExtract the sub-sample of observations for 1987. There should be 1028 observations. Estimate a linear regression of \\(I\\) (investment to capital ratio) on the other variables. Calculate appropriate standard errors.\nCalculate asymptotic confidence intervals for the coefficients.\nThis regression is related to Tobin’s \\(q\\) theory of investment, which suggests that investment should be predicted solely by \\(Q\\) (Tobin’s \\(Q\\) ). This theory predicts that the coefficient on \\(Q\\) should be positive and the others should be zero. Test the joint hypothesis that the coefficients on cash flow \\((C)\\) and debt \\((D)\\) are zero. Test the hypothesis that the coefficient on \\(Q\\) is zero. Are the results consistent with the predictions of the theory?\nNow try a nonlinear (quadratic) specification. Regress \\(I\\) on \\(Q, C, D, Q^{2}, C^{2}, D^{2}, Q \\times C, Q \\times D, C \\times D\\). Test the joint hypothesis that the six interaction and quadratic coefficients are zero.\n\nExercise 9.26 In a paper in 1963, Marc Nerlove analyzed a cost function for 145 American electric companies. Nerlov was interested in estimating a cost function: \\(C=f(Q, P L, P F, P K)\\) where the variables are listed in the table below. His data set Nerlove1963 is on the textbook website.\n\n\n\nC\nTotal Cost\n\n\n\n\nQ\nOutput\n\n\nPL\nUnit price of labor\n\n\nPK\nUnit price of capital\n\n\nPF\nUnit price of fuel\n\n\n\n\nFirst, estimate an unrestricted Cobb-Douglass specification\n\n\\[\n\\log C=\\beta_{1}+\\beta_{2} \\log Q+\\beta_{3} \\log P L+\\beta_{4} \\log P K+\\beta_{5} \\log P F+e .\n\\]\nReport parameter estimates and standard errors.\n\nWhat is the economic meaning of the restriction \\(\\mathbb{H}_{0}: \\beta_{3}+\\beta_{4}+\\beta_{5}=1\\) ?\nEstimate (9.23) by constrained least squares imposing \\(\\beta_{3}+\\beta_{4}+\\beta_{5}=1\\). Report your parameter estimates and standard errors.\nEstimate (9.23) by efficient minimum distance imposing \\(\\beta_{3}+\\beta_{4}+\\beta_{5}=1\\). Report your parameter estimates and standard errors.\nTest \\(\\mathbb{H}_{0}: \\beta_{3}+\\beta_{4}+\\beta_{5}=1\\) using a Wald statistic.\nTest \\(\\mathbb{H}_{0}: \\beta_{3}+\\beta_{4}+\\beta_{5}=1\\) using a minimum distance statistic. Exercise 9.27 In Section 8.12 we reported estimates from Mankiw, Romer and Weil (1992). We reported estimation both by unrestricted least squares and by constrained estimation, imposing the constraint that three coefficients ( \\(2^{n d}, 3^{r d}\\) and \\(4^{t h}\\) coefficients) sum to zero as implied by the Solow growth theory. Using the same dataset MRW1992 estimate the unrestricted model and test the hypothesis that the three coefficients sum to zero.\n\nExercise 9.28 Using the cps09mar dataset and the subsample of non-Hispanic Black individuals (race code \\(=2\\) ) test the hypothesis that marriage status does not affect mean wages.\n\nTake the regression reported in Table 4.1. Which variables will need to be omitted to estimate a regression for this subsample?\nExpress the hypothesis “marriage status does not affect mean wages” as a restriction on the coefficients. How many restrictions is this?\nFind the Wald (or F) statistic for this hypothesis. What is the appropriate distribution for the test statistic? Calculate the p-value of the test.\nWhat do you conclude?\n\nExercise 9.29 Using the cps09mar dataset and the subsample of non-Hispanic Black individuals (race code \\(=2\\) ) and white individuals (race code \\(=1\\) ) test the hypothesis that the returns to education is common across groups.\n\nAllow the return to education to vary across the four groups (white male, white female, Black male, Black female) by interacting dummy variables with education. Estimate an appropriate version of the regression reported in Table 4.1.\nFind the Wald (or F) statistic for this hypothessis. What is the appropriate distribution for the test statistic? Calculate the p-value of the test.\nWhat do you conclude?"
  },
  {
    "objectID": "chpt10-resample-method.html#introduction",
    "href": "chpt10-resample-method.html#introduction",
    "title": "10  Resampling Methods",
    "section": "10.1 Introduction",
    "text": "10.1 Introduction\nSo far in this textbook we have discussed two approaches to inference: exact and asymptotic. Both have their strengths and weaknesses. Exact theory provides a useful benchmark but is based on the unrealistic and stringent assumption of the homoskedastic normal regression model. Asymptotic theory provides a more flexible distribution theory but is an approximation with uncertain accuracy.\nIn this chapter we introduce a set of alternative inference methods which are based around the concept of resampling - which means using sampling information extracted from the empirical distribution of the data. These are powerful methods, widely applicable, and often more accurate than exact methods and asymptotic approximations. Two disadvantages, however, are (1) resampling methods typically require more computation power; and (2) the theory is considerably more challenging. A consequence of the computation requirement is that most empirical researchers use asymptotic approximations for routine calculations while resampling approximations are used for final reporting.\nWe will discuss two categories of resampling methods used in statistical and econometric practice: jackknife and bootstrap. Most of our attention will be given to the bootstrap as it is the most commonly used resampling method in econometric practice.\nThe jackknife is the distribution obtained from the \\(n\\) leave-one-out estimators (see Section 3.20). The jackknife is most commonly used for variance estimation.\nThe bootstrap is the distribution obtained by estimation on samples created by i.i.d. sampling with replacement from the dataset. (There are other variants of bootstrap sampling, including parametric sampling and residual sampling.) The bootstrap is commonly used for variance estimation, confidence interval construction, and hypothesis testing.\nThere is a third category of resampling methods known as sub-sampling which we will not cover in this textbook. Sub-sampling is the distribution obtained by estimation on sub-samples (sampling without replacement) of the dataset. Sub-sampling can be used for most of same purposes as the bootstrap. See the excellent monograph by Politis, Romano and Wolf (1999)."
  },
  {
    "objectID": "chpt10-resample-method.html#example",
    "href": "chpt10-resample-method.html#example",
    "title": "10  Resampling Methods",
    "section": "10.2 Example",
    "text": "10.2 Example\nTo motivate our discussion we focus on the application presented in Section 3.7, which is a bivariate regression applied to the CPS subsample of married Black female wage earners with 12 years potential work experience and displayed in Table 3.1. The regression equation is\n\\[\n\\log (\\text { wage })=\\beta_{1} \\text { education }+\\beta_{2}+e .\n\\]\nThe estimates as reported in (4.44) are\n\\[\n\\begin{aligned}\n& \\log (\\text { wage })=0.155 \\text { education }+0.698+\\widehat{e} \\\\\n& \\text { (0.031) } \\quad(0.493) \\\\\n& \\widehat{\\sigma}^{2}=0.144 \\\\\n& \\text { (0.043) } \\\\\n& n=20 \\text {. }\n\\end{aligned}\n\\]\nWe focus on four estimates constructed from this regression. The first two are the coefficient estimates \\(\\widehat{\\beta}_{1}\\) and \\(\\widehat{\\beta}_{2}\\). The third is the variance estimate \\(\\widehat{\\sigma}^{2}\\). The fourth is an estimate of the expected level of wages for an individual with 16 years of education (a college graduate), which turns out to be a nonlinear function of the parameters. Under the simplifying assumption that the error \\(e\\) is independent of the level of education and normally distributed we find that the expected level of wages is\n\\[\n\\begin{aligned}\n\\mu &=\\mathbb{E}[\\text { wage } \\mid \\text { education }=16] \\\\\n&=\\mathbb{E}\\left[\\exp \\left(16 \\beta_{1}+\\beta_{2}+e\\right)\\right] \\\\\n&=\\exp \\left(16 \\beta_{1}+\\beta_{2}\\right) \\mathbb{E}[\\exp (e)] \\\\\n&=\\exp \\left(16 \\beta_{1}+\\beta_{2}+\\sigma^{2} / 2\\right) .\n\\end{aligned}\n\\]\nThe final equality is \\(\\mathbb{E}[\\exp (e)]=\\exp \\left(\\sigma^{2} / 2\\right)\\) which can be obtained from the normal moment generating function. The parameter \\(\\mu\\) is a nonlinear function of the coefficients. The natural estimator of \\(\\mu\\) replaces the unknowns by the point estimators. Thus\n\\[\n\\widehat{\\mu}=\\exp \\left(16 \\widehat{\\beta}_{1}+\\widehat{\\beta}_{2}+\\widehat{\\sigma}^{2} / 2\\right)=25.80\n\\]\nThe standard error for \\(\\widehat{\\mu}\\) can be found by extending Exercise \\(7.8\\) to find the joint asymptotic distribution of \\(\\widehat{\\sigma}^{2}\\) and the slope estimates, and then applying the delta method.\nWe are interested in calculating standard errors and confidence intervals for the four estimates described above."
  },
  {
    "objectID": "chpt10-resample-method.html#jackknife-estimation-of-variance",
    "href": "chpt10-resample-method.html#jackknife-estimation-of-variance",
    "title": "10  Resampling Methods",
    "section": "10.3 Jackknife Estimation of Variance",
    "text": "10.3 Jackknife Estimation of Variance\nThe jackknife estimates moments of estimators using the distribution of the leave-one-out estimators. The jackknife estimators of bias and variance were introduced by Quenouille (1949) and Tukey (1958), respectively. The idea was expanded further in the monographs of Efron (1982) and Shao and Tu (1995).\nLet \\(\\widehat{\\theta}\\) be any estimator of a vector-valued parameter \\(\\theta\\) which is a function of a random sample of size \\(n\\). Let \\(\\boldsymbol{V}_{\\widehat{\\theta}}=\\operatorname{var}[\\widehat{\\theta}]\\) be the variance of \\(\\widehat{\\theta}\\). Define the leave-one-out estimators \\(\\widehat{\\theta}_{(-i)}\\) which are computed using the formula for \\(\\widehat{\\theta}\\) except that observation \\(i\\) is deleted. Tukey’s jackknife estimator for \\(\\boldsymbol{V}_{\\widehat{\\theta}}\\) is defined as a scale of the sample variance of the leave-one-out estimators:\n\\[\n\\widehat{\\boldsymbol{V}}_{\\widehat{\\theta}}^{\\text {jack }}=\\frac{n-1}{n} \\sum_{i=1}^{n}\\left(\\widehat{\\theta}_{(-i)}-\\bar{\\theta}\\right)\\left(\\widehat{\\theta}_{(-i)}-\\bar{\\theta}\\right)^{\\prime}\n\\]\nwhere \\(\\bar{\\theta}\\) is the sample mean of the leave-one-out estimators \\(\\bar{\\theta}=n^{-1} \\sum_{i=1}^{n} \\widehat{\\theta}_{(-i)}\\). For scalar estimators \\(\\widehat{\\theta}\\) the jackknife standard error is the square root of (10.1): \\(s_{\\widehat{\\theta}}^{\\text {jack }}=\\sqrt{\\widehat{V}_{\\widehat{\\theta}}^{\\text {jack }}}\\).\nA convenient feature of the jackknife estimator \\(\\widehat{V}_{\\widehat{\\theta}}^{\\text {jack }}\\) is that the formula (10.1) is quite general and does not require any technical (exact or asymptotic) calculations. A downside is that can require \\(n\\) separate estimations, which in some cases can be computationally costly.\nIn most cases \\(\\widehat{\\boldsymbol{V}}_{\\widehat{\\theta}}^{\\text {jack }}\\) will be similar to a robust asymptotic covariance matrix estimator. The main attractions of the jackknife estimator are that it can be used when an explicit asymptotic variance formula is not available and that it can be used as a check on the reliability of an asymptotic formula.\nThe formula (10.1) is not immediately intuitive so may benefit from some motivation. We start by examining the sample mean \\(\\bar{Y}=\\frac{1}{n} \\sum_{i=1}^{n} Y_{i}\\) for \\(Y \\in \\mathbb{R}^{m}\\). The leave-one-out estimator is\n\\[\n\\bar{Y}_{(-i)}=\\frac{1}{n-1} \\sum_{j \\neq i} Y_{j}=\\frac{n}{n-1} \\bar{Y}-\\frac{1}{n-1} Y_{i} .\n\\]\nThe sample mean of the leave-one-out estimators is\n\\[\n\\frac{1}{n} \\sum_{i=1}^{n} \\bar{Y}_{(-i)}=\\frac{n}{n-1} \\bar{Y}-\\frac{1}{n-1} \\bar{Y}=\\bar{Y}\n\\]\nThe difference is\n\\[\n\\bar{Y}_{(-i)}-\\bar{Y}=\\frac{1}{n-1}\\left(\\bar{Y}-Y_{i}\\right) .\n\\]\nThe jackknife estimate of variance (10.1) is then\n\\[\n\\begin{aligned}\n\\widehat{\\boldsymbol{V}}_{\\bar{Y}}^{\\text {jack }} &=\\frac{n-1}{n} \\sum_{i=1}^{n}\\left(\\frac{1}{n-1}\\right)^{2}\\left(\\bar{Y}-Y_{i}\\right)\\left(\\bar{Y}-Y_{i}\\right)^{\\prime} \\\\\n&=\\frac{1}{n}\\left(\\frac{1}{n-1}\\right) \\sum_{i=1}^{n}\\left(\\bar{Y}-Y_{i}\\right)\\left(\\bar{Y}-Y_{i}\\right)^{\\prime}\n\\end{aligned}\n\\]\nThis is identical to the conventional estimator for the variance of \\(\\bar{Y}\\). Indeed, Tukey proposed the \\((n-1) / n\\) scaling in (10.1) so that \\(\\widehat{V}_{\\bar{Y}}^{\\text {jack }}\\) precisely equals the conventional estimator.\nWe next examine the case of least squares regression coefficient estimator. Recall from (3.43) that the leave-one-out OLS estimator equals\n\\[\n\\widehat{\\beta}_{(-i)}=\\widehat{\\beta}-\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} X_{i} \\widetilde{e}_{i}\n\\]\nwhere \\(\\widetilde{e}_{i}=\\left(1-h_{i i}\\right)^{-1} \\widehat{e}_{i}\\) and \\(h_{i i}=X_{i}^{\\prime}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} X_{i}\\). The sample mean of the leave-one-out estimators is \\(\\bar{\\beta}=\\widehat{\\beta}-\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\widetilde{\\mu}\\) where \\(\\widetilde{\\mu}=n^{-1} \\sum_{i=1}^{n} X_{i} \\widetilde{e}_{i}\\). Thus \\(\\widehat{\\beta}_{(-i)}-\\bar{\\beta}=-\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1}\\left(X_{i} \\widetilde{e}_{i}-\\widetilde{\\mu}\\right)\\). The jackknife estimate of variance for \\(\\widehat{\\beta}\\) is\n\\[\n\\begin{aligned}\n\\widehat{\\boldsymbol{V}}_{\\widehat{\\beta}}^{\\text {jack }} &=\\frac{n-1}{n} \\sum_{i=1}^{n}\\left(\\widehat{\\beta}_{(-i)}-\\bar{\\beta}\\right)\\left(\\widehat{\\beta}_{(-i)}-\\bar{\\beta}\\right)^{\\prime} \\\\\n&=\\frac{n-1}{n}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1}\\left(\\sum_{i=1}^{n} X_{i} X_{i}^{\\prime} \\tilde{e}_{i}^{2}-n \\widetilde{\\mu} \\widetilde{\\mu}^{\\prime}\\right)\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\\\\n&=\\frac{n-1}{n} \\widehat{\\boldsymbol{V}}_{\\widehat{\\beta}}^{\\mathrm{HC} 3}-(n-1)\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\widetilde{\\mu} \\widetilde{\\mu}^{\\prime}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1}\n\\end{aligned}\n\\]\nwhere \\(\\widehat{\\boldsymbol{V}}_{\\widehat{\\beta}}^{\\mathrm{HC}}\\) is the HC3 covariance estimator (4.39) based on prediction errors. The second term in (10.5) is typically quite small since \\(\\widetilde{\\mu}\\) is typically small in magnitude. Thus \\(\\widehat{\\boldsymbol{V}}_{\\widehat{\\beta}}^{\\text {jack }} \\simeq \\widehat{\\boldsymbol{V}}_{\\widehat{\\beta}}^{\\mathrm{HC}}\\). Indeed the HC3 estimator was originally motivated as a simplification of the jackknife estimator. This shows that for regression coefficients the jackknife estimator of variance is similar to a conventional robust estimator. This is accomplished without the user “knowing” the form of the asymptotic covariance matrix. This is further confirmation that the jackknife is making a reasonable calculation.\nThird, we examine the jackknife estimator for a function \\(\\widehat{\\theta}=r(\\widehat{\\beta})\\) of a least squares estimator. The leave-one-out estimator of \\(\\theta\\) is\n\\[\n\\begin{aligned}\n\\widehat{\\theta}_{(-i)} &=r\\left(\\widehat{\\beta}_{(-i)}\\right) \\\\\n&=r\\left(\\widehat{\\beta}-\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} X_{i} \\widetilde{e}_{i}\\right) \\\\\n& \\simeq \\widehat{\\theta}-\\widehat{\\boldsymbol{R}}^{\\prime}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} X_{i} \\widetilde{e}_{i}\n\\end{aligned}\n\\]\nThe second equality is (10.4). The final approximation is obtained by a mean-value expansion, using \\(r(\\widehat{\\beta})=\\widehat{\\theta}\\) and setting \\(\\widehat{\\boldsymbol{R}}=(\\partial / \\partial \\beta) r(\\widehat{\\beta})^{\\prime}\\). This approximation holds in large samples because \\(\\widehat{\\beta}_{(-i)}\\) are uniformly consistent for \\(\\beta\\). The jackknife variance estimator for \\(\\widehat{\\theta}\\) thus equals\n\\[\n\\begin{aligned}\n\\widehat{\\boldsymbol{V}}_{\\widehat{\\theta}}^{\\mathrm{jack}} &=\\frac{n-1}{n} \\sum_{i=1}^{n}\\left(\\widehat{\\theta}_{(-i)}-\\bar{\\theta}\\right)\\left(\\widehat{\\theta}_{(-i)}-\\bar{\\theta}\\right)^{\\prime} \\\\\n& \\simeq \\frac{n-1}{n} \\widehat{\\boldsymbol{R}}^{\\prime}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1}\\left(\\sum_{i=1}^{n} X_{i} X_{i}^{\\prime} \\widehat{e}_{i}^{2}-n \\widetilde{\\mu} \\widetilde{\\mu}^{\\prime}\\right)\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\widehat{\\boldsymbol{R}} \\\\\n&=\\widehat{\\boldsymbol{R}}^{\\prime} \\widehat{\\boldsymbol{V}}_{\\widehat{\\beta}}^{\\mathrm{jack}} \\widehat{\\boldsymbol{R}} \\\\\n& \\simeq \\widehat{\\boldsymbol{R}}^{\\prime} \\widetilde{\\boldsymbol{V}}_{\\widehat{\\beta}} \\widehat{\\boldsymbol{R}} .\n\\end{aligned}\n\\]\nThe final line equals a delta-method estimator for the variance of \\(\\widehat{\\theta}\\) constructed with the covariance estimator (4.39). This shows that the jackknife estimator of variance for \\(\\widehat{\\theta}\\) is approximately an asymptotic delta-method estimator. While this is an asymptotic approximation, it again shows that the jackknife produces an estimator which is asymptotically similar to one produced by asymptotic methods. This is despite the fact that the jackknife estimator is calculated without reference to asymptotic theory and does not require calculation of the derivatives of \\(r(\\beta)\\).\nThis argument extends directly to any “smooth function” estimator. Most of the estimators discussed so far in this textbook take the form \\(\\widehat{\\theta}=g(\\bar{W})\\) where \\(\\bar{W}=n^{-1} \\sum_{i=1}^{n} W_{i}\\) and \\(W_{i}\\) is some vector-valued function of the data. For any such estimator \\(\\widehat{\\theta}\\) the leave-one-out estimator equals \\(\\widehat{\\theta}_{(-i)}=g\\left(\\bar{W}_{(-i)}\\right)\\) and its jackknife estimator of variance is (10.1). Using (10.2) and a mean-value expansion we have the largesample approximation\n\\[\n\\begin{aligned}\n\\widehat{\\theta}_{(-i)} &=g\\left(\\bar{W}_{(-i)}\\right) \\\\\n&=g\\left(\\frac{n}{n-1} \\bar{W}-\\frac{1}{n-1} W_{i}\\right) \\\\\n& \\simeq g(\\bar{W})-\\frac{1}{n-1} \\boldsymbol{G}(\\bar{W})^{\\prime} W_{i}\n\\end{aligned}\n\\]\nwhere \\(\\boldsymbol{G}(x)=(\\partial / \\partial x) g(x)^{\\prime}\\). Thus\n\\[\n\\widehat{\\theta}_{(-i)}-\\bar{\\theta} \\simeq-\\frac{1}{n-1} \\boldsymbol{G}(\\bar{W})^{\\prime}\\left(W_{i}-\\bar{W}\\right)\n\\]\nand the jackknife estimator of the variance of \\(\\widehat{\\theta}\\) approximately equals\n\\[\n\\begin{aligned}\n\\widehat{\\boldsymbol{V}}_{\\widehat{\\theta}}^{\\mathrm{jack}} &=\\frac{n-1}{n} \\sum_{i=1}^{n}\\left(\\widehat{\\theta}_{(-i)}-\\widehat{\\theta}_{(\\cdot)}\\right)\\left(\\widehat{\\theta}_{(-i)}-\\widehat{\\theta}_{(\\cdot)}\\right)^{\\prime} \\\\\n& \\simeq \\frac{n-1}{n} \\boldsymbol{G}(\\bar{W})^{\\prime}\\left(\\frac{1}{(n-1)^{2}} \\sum_{i=1}^{n}\\left(W_{i}-\\bar{W}\\right)\\left(W_{i}-\\bar{W}\\right)^{\\prime}\\right) \\boldsymbol{G}(\\bar{W}) \\\\\n&=\\boldsymbol{G}(\\bar{W})^{\\prime} \\widehat{\\boldsymbol{V}}_{\\bar{W}}^{\\mathrm{jack}} \\boldsymbol{G}(\\bar{W})\n\\end{aligned}\n\\]\nwhere \\(\\widehat{V}_{\\bar{W}}^{\\text {jack }}\\) as defined in (10.3) is the conventional (and jackknife) estimator for the variance of \\(\\bar{W}\\). Thus \\(\\widehat{\\boldsymbol{V}}_{\\widehat{\\theta}}^{\\text {jack }}\\) is approximately the delta-method estimator. Once again, we see that the jackknife estimator automatically calculates what is effectively the delta-method variance estimator, but without requiring the user to explicitly calculate the derivative of \\(g(x)\\)."
  },
  {
    "objectID": "chpt10-resample-method.html#example-1",
    "href": "chpt10-resample-method.html#example-1",
    "title": "10  Resampling Methods",
    "section": "10.4 Example",
    "text": "10.4 Example\nWe illustrate by reporting the asymptotic and jackknife standard errors for the four parameter estimates given earlier. In Table \\(10.1\\) we report the actual values of the leave-one-out estimates for each of the twenty observations in the sample. The jackknife standard errors are calculated as the scaled square roots of the sample variances of these leave-one-out estimates and are reported in the second-to-last row. For comparison the asymptotic standard errors are reported in the final row.\nFor all estimates the jackknife and asymptotic standard errors are quite similar. This reinforces the credibility of both standard error estimates. The largest differences arise for \\(\\widehat{\\beta}_{2}\\) and \\(\\widehat{\\mu}\\), whose jackknife standard errors are about \\(5 %\\) larger than the asymptotic standard errors.\nThe take-away from our presentation is that the jackknife is a simple and flexible method for variance and standard error calculation. Circumventing technical asymptotic and exact calculations, the jackknife produces estimates which in many cases are similar to asymptotic delta-method counterparts. The jackknife is especially appealing in cases where asymptotic standard errors are not available or are difficult to calculate. They can also be used as a double-check on the reasonableness of asymptotic delta-method calculations.\nIn Stata, jackknife standard errors for coefficient estimates in many models are obtained by the vce(jackknife) option. For nonlinear functions of the coefficients or other estimators the jackkn ife command can be combined with any other command to obtain jackknife standard errors.\nTo illustrate, below we list the Stata commands which calculate the jackknife standard errors listed above. The first line is least squares estimation with standard errors calculated by the jackknife. The second line calculates the error variance estimate \\(\\widehat{\\sigma}^{2}\\) with a jackknife standard error. The third line does the same for the estimate \\(\\widehat{\\mu}\\).\n\nTable 10.1: Leave-one-out Estimators and Jackknife Standard Errors\n\n\n\n\n\n\n\n\n\n\nObservation\n\\(\\widehat{\\beta}_{1(-i)}\\)\n\\(\\widehat{\\beta}_{2(-i)}\\)\n\\(\\widehat{\\sigma}_{(-i)}^{2}\\)\n\\(\\widehat{\\mu}_{(-i)}\\)\n\n\n\n\n1\n\\(0.150\\)\n\\(0.764\\)\n\\(0.150\\)\n\\(25.63\\)\n\n\n2\n\\(0.148\\)\n\\(0.798\\)\n\\(0.149\\)\n\\(25.48\\)\n\n\n3\n\\(0.153\\)\n\\(0.739\\)\n\\(0.151\\)\n\\(25.97\\)\n\n\n4\n\\(0.156\\)\n\\(0.695\\)\n\\(0.144\\)\n\\(26.31\\)\n\n\n5\n\\(0.154\\)\n\\(0.701\\)\n\\(0.146\\)\n\\(25.38\\)\n\n\n6\n\\(0.158\\)\n\\(0.655\\)\n\\(0.151\\)\n\\(26.05\\)\n\n\n7\n\\(0.152\\)\n\\(0.705\\)\n\\(0.114\\)\n\\(24.32\\)\n\n\n8\n\\(0.146\\)\n\\(0.822\\)\n\\(0.147\\)\n\\(25.37\\)\n\n\n9\n\\(0.162\\)\n\\(0.588\\)\n\\(0.151\\)\n\\(25.75\\)\n\n\n10\n\\(0.157\\)\n\\(0.693\\)\n\\(0.139\\)\n\\(26.40\\)\n\n\n11\n\\(0.168\\)\n\\(0.510\\)\n\\(0.141\\)\n\\(26.40\\)\n\n\n12\n\\(0.158\\)\n\\(0.691\\)\n\\(0.118\\)\n\\(26.48\\)\n\n\n13\n\\(0.139\\)\n\\(0.974\\)\n\\(0.141\\)\n\\(26.56\\)\n\n\n14\n\\(0.169\\)\n\\(0.451\\)\n\\(0.131\\)\n\\(26.26\\)\n\n\n15\n\\(0.146\\)\n\\(0.852\\)\n\\(0.150\\)\n\\(24.93\\)\n\n\n16\n\\(0.156\\)\n\\(0.696\\)\n\\(0.148\\)\n\\(26.06\\)\n\n\n17\n\\(0.165\\)\n\\(0.513\\)\n\\(0.140\\)\n\\(25.22\\)\n\n\n18\n\\(0.155\\)\n\\(0.698\\)\n\\(0.151\\)\n\\(25.90\\)\n\n\n19\n\\(0.152\\)\n\\(0.742\\)\n\\(0.151\\)\n\\(25.73\\)\n\n\n20\n\\(0.155\\)\n\\(0.697\\)\n\\(0.151\\)\n\\(25.95\\)\n\n\n\\(s^{\\text {jack }}\\)\n\\(0.032\\)\n\\(0.514\\)\n\\(0.046\\)\n\\(2.39\\)\n\n\n\\(s^{\\text {asy }}\\)\n\\(0.031\\)\n\\(0.493\\)\n\\(0.043\\)\n\\(2.29\\)"
  },
  {
    "objectID": "chpt10-resample-method.html#jackknife-for-clustered-observations",
    "href": "chpt10-resample-method.html#jackknife-for-clustered-observations",
    "title": "10  Resampling Methods",
    "section": "10.5 Jackknife for Clustered Observations",
    "text": "10.5 Jackknife for Clustered Observations\nIn Section \\(4.21\\) we introduced the clustered regression model, cluster-robust variance estimators, and cluster-robust standard errors. Jackknife variance estimation can also be used for clustered samples but with some natural modifications. Recall that the least squares estimator in the clustered sample context can be written as\n\\[\n\\widehat{\\beta}=\\left(\\sum_{g=1}^{G} \\boldsymbol{X}_{g}^{\\prime} \\boldsymbol{X}_{g}\\right)^{-1}\\left(\\sum_{g=1}^{G} \\boldsymbol{X}_{g}^{\\prime} \\boldsymbol{Y}_{g}\\right)\n\\]\nwhere \\(g=1, \\ldots, G\\) indexes the cluster. Instead of leave-one-out estimators, it is natural to use deletecluster estimators, which delete one cluster at a time. They take the form (4.58):\n\\[\n\\widehat{\\beta}_{(-g)}=\\widehat{\\beta}-\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\boldsymbol{X}_{g}^{\\prime} \\widetilde{\\boldsymbol{e}}_{g}\n\\]\nwhere\n\\[\n\\begin{aligned}\n&\\widetilde{\\boldsymbol{e}}_{g}=\\left(\\boldsymbol{I}_{n_{g}}-\\boldsymbol{X}_{g}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\boldsymbol{X}_{g}^{\\prime}\\right)^{-1} \\widehat{\\boldsymbol{e}}_{g} \\\\\n&\\widehat{\\boldsymbol{e}}_{g}=\\boldsymbol{Y}_{g}-\\boldsymbol{X}_{g} \\widehat{\\beta}\n\\end{aligned}\n\\]\nThe delete-cluster jackknife estimator of the variance of \\(\\widehat{\\beta}\\) is\n\\[\n\\begin{aligned}\n\\widehat{\\boldsymbol{V}}_{\\widehat{\\beta}}^{\\mathrm{jack}} &=\\frac{G-1}{G} \\sum_{g=1}^{G}\\left(\\widehat{\\beta}_{(-g)}-\\bar{\\beta}\\right)\\left(\\widehat{\\beta}_{(-g)}-\\bar{\\beta}\\right)^{\\prime} \\\\\n\\bar{\\beta} &=\\frac{1}{G} \\sum_{g=1}^{G} \\widehat{\\beta}_{(-g)} .\n\\end{aligned}\n\\]\nWe call \\(\\widehat{V}_{\\widehat{\\beta}}^{\\text {jack }}\\) a cluster-robust jackknife estimator of variance.\nUsing the same approximations as the previous section we can show that the delete-cluster jackknife estimator is asymptotically equivalent to the cluster-robust covariance matrix estimator (4.59) calculated with the delete-cluster prediction errors. This verifies that the delete-cluster jackknife is the appropriate jackknife approach for clustered dependence.\nFor parameters which are functions \\(\\widehat{\\theta}=r(\\widehat{\\beta})\\) of the least squares estimator, the delete-cluster jackknife estimator of the variance of \\(\\widehat{\\theta}\\) is\n\\[\n\\begin{aligned}\n\\widehat{\\boldsymbol{V}}_{\\widehat{\\theta}}^{\\text {jack }} &=\\frac{G-1}{G} \\sum_{g=1}^{G}\\left(\\widehat{\\theta}_{(-g)}-\\bar{\\theta}\\right)\\left(\\widehat{\\theta}_{(-g)}-\\bar{\\theta}\\right)^{\\prime} \\\\\n\\widehat{\\theta}_{(-i)} &=r\\left(\\widehat{\\beta}_{(-g)}\\right) \\\\\n\\bar{\\theta} &=\\frac{1}{G} \\sum_{g=1}^{G} \\widehat{\\theta}_{(-g)} .\n\\end{aligned}\n\\]\nUsing a mean-value expansion we can show that this estimator is asymptotically equivalent to the deltamethod cluster-robust covariance matrix estimator for \\(\\widehat{\\theta}\\). This shows that the jackknife estimator is appropriate for covariance matrix estimation.\nAs in the context of i.i.d. samples, one advantage of the jackknife covariance matrix estimators is that they do not require the user to make a technical calculation of the asymptotic distribution. A downside is an increase in computation cost, as \\(G\\) separate regressions are effectively estimated.\nIn Stata, jackknife standard errors for coefficient estimates with clustered observations are obtained by using the options cluster (id) vce(jackkn ife) where id denotes the cluster variable."
  },
  {
    "objectID": "chpt10-resample-method.html#the-bootstrap-algorithm",
    "href": "chpt10-resample-method.html#the-bootstrap-algorithm",
    "title": "10  Resampling Methods",
    "section": "10.6 The Bootstrap Algorithm",
    "text": "10.6 The Bootstrap Algorithm\nThe bootstrap is a powerful approach to inference and is due to the pioneering work of Efron (1979). There are many textbook and monograph treatments of the bootstrap, including Efron (1982), Hall (1992), Efron and Tibshirani (1993), Shao and Tu (1995), and Davison and Hinkley (1997). Reviews for econometricians are provided by Hall (1994) and Horowitz (2001)\nThere are several ways to describe or define the bootstrap and there are several forms of the bootstrap. We start in this section by describing the basic nonparametric bootstrap algorithm. In subsequent sections we give more formal definitions of the bootstrap as well as theoretical justifications.\nBriefly, the bootstrap distribution is obtained by estimation on independent samples created by i.i.d. sampling (sampling with replacement) from the original dataset.\nTo understand this it is useful to start with the concept of sampling with replacement from the dataset. To continue the empirical example used earlier in the chapter we focus on the dataset displayed in Table 3.1, which has \\(n=20\\) observations. Sampling from this distribution means randomly selecting one row from this table. Mathematically this is the same as randomly selecting an integer from the set \\(\\{1,2, \\ldots, 20\\}\\). To illustrate, MATLAB has a random integer generator (the function randi). Using the random number seed of 13 (an arbitrary choice) we obtain the random draw 16 . This means that we draw observation number 16 from Table 3.1. Examining the table we can see that this is an individual with wage \\(\\$ 18.75\\) and education of 16 years. We repeat by drawing another random integer on the set \\(\\{1,2, \\ldots, 20\\}\\) and this time obtain 5 . This means we take observation 5 from Table 3.1, which is an individual with wage \\(\\$ 33.17\\) and education of 16 years. We continue until we have \\(n=20\\) such draws. This random set of observations are \\(\\{16,5,17,20,20,10,13,16,13,15,1,6,2,18,8,14,6,7,1,8\\}\\). We call this the bootstrap sample.\nNotice that the observations \\(1,6,8,13,16,20\\) each appear twice in the bootstrap sample, and the observations \\(3,4,9,11,12,19\\) do not appear at all. That is okay. In fact, it is necessary for the bootstrap to work. This is because we are drawing with replacement. (If we instead made draws without replacement then the constructed dataset would have exactly the same observations as in Table 3.1, only in different order.) We can also ask the question “What is the probability that an individual observation will appear at least once in the bootstrap sample?” The answer is\n\\[\n\\begin{aligned}\n\\mathbb{P}[\\text { Observation in Bootstrap Sample }] &=1-\\left(1-\\frac{1}{n}\\right)^{n} \\\\\n& \\rightarrow 1-e^{-1} \\simeq 0.632 .\n\\end{aligned}\n\\]\nThe limit holds as \\(n \\rightarrow \\infty\\). The approximation \\(0.632\\) is excellent even for small \\(n\\). For example, when \\(n=20\\) the probability (10.6) is \\(0.641\\). These calculations show that an individual observation is in the bootstrap sample with probability near \\(2 / 3\\).\nOnce again, the bootstrap sample is the constructed dataset with the 20 observations drawn randomly from the original sample. Notationally, we write the \\(i^{\\text {th }}\\) bootstrap observation as \\(\\left(Y_{i}^{*}, X_{i}^{*}\\right)\\) and the bootstrap sample as \\(\\left\\{\\left(Y_{1}^{*}, X_{1}^{*}\\right), \\ldots,\\left(Y_{n}^{*}, X_{n}^{*}\\right)\\right\\}\\). In our present example with \\(Y\\) denoting the log wage the bootstrap sample is\n\\[\n\\left\\{\\left(Y_{1}^{*}, X_{1}^{*}\\right), \\ldots,\\left(Y_{n}^{*}, X_{n}^{*}\\right)\\right\\}=\\{(2.93,16),(3.50,16) \\ldots,(3.76,18)\\}\n\\]\nThe bootstrap estimate \\(\\widehat{\\beta}^{*}\\) is obtained by applying the least squares estimation formula to the bootstrap sample. Thus we regress \\(Y^{*}\\) on \\(X^{*}\\). The other bootstrap estimates, in our example \\(\\widehat{\\sigma}^{2 *}\\) and \\(\\widehat{\\mu}^{*}\\), are obtained by applying their estimation formulae to the bootstrap sample as well. Writing \\(\\widehat{\\theta}^{*}=\\) \\(\\left(\\widehat{\\beta}_{1}^{*}, \\widehat{\\beta}_{2}^{*}, \\widehat{\\sigma}^{* 2}, \\widehat{\\mu}^{*}\\right)^{\\prime}\\) we have the bootstrap estimate of the parameter vector \\(\\theta=\\left(\\beta_{1}, \\beta_{2}, \\sigma^{2}, \\mu\\right)^{\\prime}\\). In our example (the bootstrap sample described above) \\(\\widehat{\\theta}^{*}=(0.195,0.113,0.107,26.7)^{\\prime}\\). This is one draw from the bootstrap distribution of the estimates.\nThe estimate \\(\\widehat{\\theta}^{*}\\) as described is one random draw from the distribution of estimates obtained by i.i.d. sampling from the original data. With one draw we can say relatively little. But we can repeat this exercise to obtain multiple draws from this bootstrap distribution. To distinguish between these draws we index the bootstrap samples by \\(b=1, \\ldots, B\\), and write the bootstrap estimates as \\(\\widehat{\\theta}_{b}^{*}\\) or \\(\\widehat{\\theta}^{*}(b)\\).\nTo continue our illustration we draw 20 more random integers \\(\\{19,5,7,19,1,2,13,18,1,15,17,2\\), \\(14,11,10,20,1,5,15,7\\}\\) and construct a second bootstrap sample. On this sample we again estimate the parameters and obtain \\(\\widehat{\\theta}^{*}(2)=(0.175,0.52,0.124,29.3)^{\\prime}\\). This is a second random draw from the distribution of \\(\\widehat{\\theta}^{*}\\). We repeat this \\(B\\) times, storing the parameter estimates \\(\\widehat{\\theta}^{*}(b)\\). We have thus created a new dataset of bootstrap draws \\(\\left\\{\\widehat{\\theta}^{*}(b): b=1, \\ldots, B\\right\\}\\). By construction the draws are independent across \\(b\\) and identically distributed.\nThe number of bootstrap draws, \\(B\\), is often called the “number of bootstrap replications”. Typical choices for \\(B\\) are 1000,5000 , and 10,000. We discuss selecting \\(B\\) later, but roughly speaking, larger \\(B\\) results in a more precise estimate at an increased computation cost. For our application we set \\(B=\\) 10,000 . To illustrate, Figure \\(13.1\\) displays the densities of the distributions of the bootstrap estimates \\(\\widehat{\\beta}_{1}^{*}\\) and \\(\\widehat{\\mu}^{*}\\) across 10,000 draws. The dashed lines show the point estimate. You can notice that the density for \\(\\widehat{\\beta}_{1}^{*}\\) is slightly skewed to the left.\\\n\nFigure 10.1: Bootstrap Distributions of \\(\\widehat{\\beta}_{1}^{*}\\) and \\(\\widehat{\\mu}^{*}\\)"
  },
  {
    "objectID": "chpt10-resample-method.html#bootstrap-variance-and-standard-errors",
    "href": "chpt10-resample-method.html#bootstrap-variance-and-standard-errors",
    "title": "10  Resampling Methods",
    "section": "10.7 Bootstrap Variance and Standard Errors",
    "text": "10.7 Bootstrap Variance and Standard Errors\nGiven the bootstrap draws we can estimate features of the bootstrap distribution. The bootstrap estimator of variance of an estimator \\(\\widehat{\\theta}\\) is the sample variance across the bootstrap draws \\(\\widehat{\\theta}^{*}(b)\\). It equals\n\\[\n\\begin{aligned}\n\\widehat{\\boldsymbol{V}}_{\\widehat{\\theta}}^{\\text {boot }} &=\\frac{1}{B-1} \\sum_{b=1}^{B}\\left(\\widehat{\\theta}^{*}(b)-\\bar{\\theta}^{*}\\right)\\left(\\widehat{\\theta}^{*}(b)-\\bar{\\theta}^{*}\\right)^{\\prime} \\\\\n\\bar{\\theta}^{*} &=\\frac{1}{B} \\sum_{b=1}^{B} \\widehat{\\theta}^{*}(b)\n\\end{aligned}\n\\]\nFor a scalar estimator \\(\\hat{\\theta}\\) the bootstrap standard error is the square root of the bootstrap estimator of variance:\n\\[\ns_{\\widehat{\\widehat{\\theta}}}^{\\text {boot }}=\\sqrt{\\widehat{\\boldsymbol{V}}_{\\widehat{\\theta}}^{\\text {boot }}} .\n\\]\nThis is a very simple statistic to calculate and is the most common use of the bootstrap in applied econometric practice. A caveat (discussed in more detail in Section 10.15) is that in many cases it is better to use a trimmed estimator.\nStandard errors are conventionally reported to convey the precision of the estimator. They are also commonly used to construct confidence intervals. Bootstrap standard errors can be used for this purpose. The normal-approximation bootstrap confidence interval is\n\\[\nC^{\\mathrm{nb}}=\\left[\\widehat{\\theta}-z_{1-\\alpha / 2} s_{\\widehat{\\theta}}^{\\text {boot }}, \\quad \\widehat{\\theta}+z_{1-\\alpha / 2} s_{\\widehat{\\theta}}^{\\text {boot }}\\right]\n\\]\nwhere \\(z_{1-\\alpha / 2}\\) is the \\(1-\\alpha / 2\\) quantile of the \\(\\mathrm{N}(0,1)\\) distribution. This interval \\(C^{\\mathrm{nb}}\\) is identical in format to an asymptotic confidence interval, but with the bootstrap standard error replacing the asymptotic standard error. \\(C^{\\mathrm{nb}}\\) is the default confidence interval reported by Stata when the bootstrap has been used to calculate standard errors. However, the normal-approximation interval is in general a poor choice for confidence interval construction as it relies on the normal approximation to the t-ratio which can be inaccurate in finite samples. There are other methods - such as the bias-corrected percentile method to be discussed in Section \\(10.17\\) - which are just as simple to compute but have better performance. In general, bootstrap standard errors should be used as estimates of precision rather than as tools to construct confidence intervals.\nSince \\(B\\) is finite, all bootstrap statistics, such as \\(\\widehat{\\boldsymbol{V}}_{\\widehat{\\theta}}^{\\text {boot }}\\), are estimates and hence random. Their values will vary across different choices for \\(B\\) and simulation runs (depending on how the simulation seed is set). Thus you should not expect to obtain the exact same bootstrap standard errors as other researchers when replicating their results. They should be similar (up to simulation sampling error) but not precisely the same.\nIn Table \\(10.2\\) we report the four parameter estimates introduced in Section \\(10.2\\) along with asymptotic, jackknife and bootstrap standard errors. We also report four bootstrap confidence intervals which will be introduced in subsequent sections.\nFor these four estimators we can see that the bootstrap standard errors are quite similar to the asymptotic and jackknife standard errors. The most noticable difference arises for \\(\\widehat{\\beta}_{2}\\), where the bootstrap standard error is about \\(10 %\\) larger than the asymptotic standard error.\nTable 10.2: Comparison of Methods\n\n\n\n\n\n\n\n\n\n\n\n\\(\\widehat{\\beta}_{1}\\)\n\\(\\widehat{\\beta}_{2}\\)\n\\(\\widehat{\\sigma}^{2}\\)\n\\(\\widehat{\\mu}\\)\n\n\n\n\nEstimate\n\\(0.155\\)\n\\(0.698\\)\n\\(0.144\\)\n\\(25.80\\)\n\n\nAsymptotic s.e.\n\\((0.031)\\)\n\\((0.493)\\)\n\\((0.043)\\)\n\\((2.29)\\)\n\n\nJackknife s.e.\n\\((0.032)\\)\n\\((0.514)\\)\n\\((0.046)\\)\n\\((2.39)\\)\n\n\nBootstrap s.e.\n\\((0.034)\\)\n\\((0.548)\\)\n\\((0.041)\\)\n\\((2.38)\\)\n\n\n\\(95 %\\) Percentile Interval\n\\([0.08,0.21]\\)\n\\([-0.27,1.91]\\)\n\\([0.06,0.22]\\)\n\\([21.4,30.7]\\)\n\n\n\\(95 %\\) BC Percentile Interval\n\\([0.08,0.21]\\)\n\\([-0.25,1.93]\\)\n\\([0.09,0.28]\\)\n\\([22.0,31.5]\\)\n\n\n\\(95 %\\) BC\n\n\n\n\n\n\n\nIn Stata, bootstrap standard errors for coefficient estimates in many models are obtained by the vce(bootstrap, reps(#)) option, where # is the number of bootstrap replications. For nonlinear functions of the coefficients or other estimators the bootstrap command can be combined with any other command to obtain bootstrap standard errors. Synonyms for bootstrap are bstrap and bs.\nTo illustrate, below we list the Stata commands which will calculate \\({ }^{1}\\) the bootstrap standard errors listed above.\n\\({ }^{1}\\) They will not precisely replicate the standard errors since those in Table \\(10.2\\) were produced in Matlab which uses a different random number sequence.\nStata Commands reg wage education if \\(\\operatorname{mbf} 12==1\\), vce(bootstrap, reps \\((10000))\\)\nbs (e(rss)/e(N)), reps(10000): reg wage education if \\(\\mathrm{mbf} 12==1\\)\nbs ( \\(\\exp \\left(16^{*}\\right.\\) bb[education]+_b[_cons] \\(\\left.\\left.+\\mathrm{e}(\\mathrm{rss}) / \\mathrm{e}(\\mathrm{N}) / 2\\right)\\right)\\), reps(10000): ///\nreg wage education if \\(\\operatorname{mbf} 12==1\\)"
  },
  {
    "objectID": "chpt10-resample-method.html#percentile-interval",
    "href": "chpt10-resample-method.html#percentile-interval",
    "title": "10  Resampling Methods",
    "section": "10.8 Percentile Interval",
    "text": "10.8 Percentile Interval\nThe second most common use of bootstrap methods is for confidence intervals. There are multiple bootstrap methods to form confidence intervals. A popular and simple method is called the percentile interval. It is based on the quantiles of the bootstrap distribution.\nIn Section \\(10.6\\) we described the bootstrap algorithm which creates an i.i.d. sample of bootstrap estimates \\(\\left\\{\\widehat{\\theta}_{1}^{*}, \\widehat{\\theta}_{2}^{*}, \\ldots, \\widehat{\\theta}_{B}^{*}\\right\\}\\) corresponding to an estimator \\(\\widehat{\\theta}\\) of a parameter \\(\\theta\\). We focus on the case of a scalar parameter \\(\\theta\\).\nFor any \\(0<\\alpha<1\\) we can calculate the empirical quantile \\(q_{\\alpha}^{*}\\) of these bootstrap estimates. This is the number such that \\(n \\alpha\\) bootstrap estimates are smaller than \\(q_{\\alpha}^{*}\\), and is typically calculated by taking the \\(n \\alpha^{t h}\\) order statistic of the \\(\\widehat{\\theta}_{b}^{*}\\). See Section \\(11.13\\) of Probability and Statistics for Economists for a precise discussion of empirical quantiles and common quantile estimators.\nThe percentile bootstrap \\(100(1-\\alpha) %\\) confidence interval is\n\\[\nC^{\\mathrm{pc}}=\\left[q_{\\alpha / 2}^{*}, q_{1-\\alpha / 2}^{*}\\right] .\n\\]\nFor example, if \\(B=1000, \\alpha=0.05\\), and the empirical quantile estimator is used, then \\(C^{\\mathrm{pc}}=\\left[\\widehat{\\theta}_{(25)}^{*}, \\widehat{\\theta}_{(975)}^{*}\\right]\\).\nTo illustrate, the \\(0.025\\) and \\(0.975\\) quantiles of the bootstrap distributions of \\(\\widehat{\\beta}_{1}^{*}\\) and \\(\\widehat{\\mu}^{*}\\) are indicated in Figure \\(13.1\\) by the arrows. The intervals between the arrows are the \\(95 %\\) percentile intervals.\nThe percentile interval has the convenience that it does not require calculation of a standard error. This is particularly convenient in contexts where asymptotic standard error calculation is complicated, burdensome, or unknown. \\(C^{\\mathrm{pc}}\\) is a simple by-product of the bootstrap algorithm and does not require meaningful computational cost above that required to calculate the bootstrap standard error.\nThe percentile interval has the useful property that it is transformation-respecting. Take a monotone parameter transformation \\(m(\\theta)\\). The percentile interval for \\(m(\\theta)\\) is simply the percentile interval for \\(\\theta\\) mapped by \\(m(\\theta)\\). That is, if \\(\\left[q_{\\alpha / 2}^{*}, q_{1-\\alpha / 2}^{*}\\right]\\) is the percentile interval for \\(\\theta\\), then \\(\\left[m\\left(q_{\\alpha / 2}^{*}\\right), m\\left(q_{1-\\alpha / 2}^{*}\\right)\\right]\\) is the percentile interval for \\(m(\\theta)\\). This property follows directly from the equivariance property of sample quantiles. Many confidence-interval methods, such as the delta-method asymptotic interval and the normal-approximation interval \\(C^{\\mathrm{nb}}\\), do not share this property.\nTo illustrate the usefulness of the transformation-respecting property consider the variance \\(\\sigma^{2}\\). In some cases it is useful to report the variance \\(\\sigma^{2}\\) and in other cases it is useful to report the standard deviation \\(\\sigma\\). Thus we may be interested in confidence intervals for \\(\\sigma^{2}\\) or \\(\\sigma\\). To illustrate, the asymptotic \\(95 %\\) normal confidence interval for \\(\\sigma^{2}\\) which we calculate from Table \\(13.2\\) is \\([0.060,0.228]\\). Taking square roots we obtain an interval for \\(\\sigma\\) of [0.244,0.477]. Alternatively, the delta method standard error for \\(\\widehat{\\sigma}=0.379\\) is \\(0.057\\), leading to an asymptotic \\(95 %\\) confidence interval for \\(\\sigma\\) of \\([0.265,0.493]\\) which is different. This shows that the delta method is not transformation-respecting. In contrast, the \\(95 %\\) percentile interval for \\(\\sigma^{2}\\) is \\([0.062,0.220]\\) and that for \\(\\sigma\\) is \\([0.249,0.469]\\) which is identical to the square roots of the interval for \\(\\sigma^{2}\\).\nThe bootstrap percentile intervals for the four estimators are reported in Table 13.2. In Stata, percentile confidence intervals can be obtained by using the command estat bootstrap, percentile or the command estat bootstrap, all after an estimation command which calculates standard errors via the bootstrap."
  },
  {
    "objectID": "chpt10-resample-method.html#the-bootstrap-distribution",
    "href": "chpt10-resample-method.html#the-bootstrap-distribution",
    "title": "10  Resampling Methods",
    "section": "10.9 The Bootstrap Distribution",
    "text": "10.9 The Bootstrap Distribution\nFor applications it is often sufficient if one understands the bootstrap as an algorithm. However, for theory it is more useful to view the bootstrap as a specific estimator of the sampling distribution. For this it is useful to introduce some additional notation.\nThe key is that the distribution of any estimator or statistic is determined by the distribution of the data. While the latter is unknown it can be estimated by the empirical distribution of the data. This is what the bootstrap does.\nTo fix notation, let \\(F\\) denote the distribution of an individual observation \\(W\\). (In regression, \\(W\\) is the \\(\\operatorname{pair}(Y, X)\\).) Let \\(G_{n}(u, F)\\) denote the distribution of an estimator \\(\\widehat{\\theta}\\). That is,\n\\[\nG_{n}(u, F)=\\mathbb{P}[\\widehat{\\theta} \\leq u \\mid F] .\n\\]\nWe write the distribution \\(G_{n}\\) as a function of \\(n\\) and \\(F\\) since the latter (generally) affect the distribution of \\(\\widehat{\\theta}\\). We are interested in the distribution \\(G_{n}\\). For example, we want to know its variance to calculate a standard error or its quantiles to calculate a percentile interval.\nIn principle, if we knew the distribution \\(F\\) we should be able to determine the distribution \\(G_{n}\\). In practice there are two barriers to implementation. The first barrier is that the calculation of \\(G_{n}(u, F)\\) is generally infeasible except in certain special cases such as the normal regression model. The second barrier is that in general we do not know \\(F\\).\nThe bootstrap simultaneously circumvents these two barriers by two clever ideas. First, the bootstrap proposes estimation of \\(F\\) by the empirical distribution function (EDF) \\(F_{n}\\), which is the simplest nonparametric estimator of the joint distribution of the observations. The EDF is \\(F_{n}(w)=n^{-1} \\sum_{i=1}^{n} \\mathbb{1}\\left\\{W_{i} \\leq w\\right\\}\\). (See Section \\(11.2\\) of Probability and Statistics for Economists for details and properties.) Replacing \\(F\\) with \\(F_{n}\\) we obtain the idealized bootstrap estimator of the distribution of \\(\\widehat{\\theta}\\)\n\\[\nG_{n}^{*}(u)=G_{n}\\left(u, F_{n}\\right) .\n\\]\nThe bootstrap’s second clever idea is to estimate \\(G_{n}^{*}\\) by simulation. This is the bootstrap algorithm described in the previous sections. The essential idea is that simulation from \\(F_{n}\\) is sampling with replacement from the original data, which is computationally simple. Applying the estimation formula for \\(\\hat{\\theta}\\) we obtain i.i.d. draws from the distribution \\(G_{n}^{*}(u)\\). By making a large number \\(B\\) of such draws we can estimate any feature of \\(G_{n}^{*}\\) of interest. The bootstrap combines these two ideas: (1) estimate \\(G_{n}(u, F)\\) by \\(G_{n}\\left(u, F_{n}\\right)\\); (2) estimate \\(G_{n}\\left(u, F_{n}\\right)\\) by simulation. These ideas are intertwined. Only by considering these steps together do we obtain a feasible method.\nThe way to think about the connection between \\(G_{n}\\) and \\(G_{n}^{*}\\) is as follows. \\(G_{n}\\) is the distribution of the estimator \\(\\widehat{\\theta}\\) obtained when the observations are sampled i.i.d. from the population distribution \\(F\\). \\(G_{n}^{*}\\) is the distribution of the same statistic, denoted \\(\\widehat{\\theta}^{*}\\), obtained when the observations are sampled i.i.d. from the empirical distribution \\(F_{n}\\). It is useful to conceptualize the “universe” which separately generates the dataset and the bootstrap sample. The “sampling universe” is the population distribution \\(F\\). In this universe the true parameter is \\(\\theta\\). The “bootstrap universe” is the empircal distribution \\(F_{n}\\). When drawing from the bootstrap universe we are treating \\(F_{n}\\) as if it is the true distribution. Thus anything which is true about \\(F_{n}\\) should be treated as true in the bootstrap universe. In the bootstrap universe the “true” value of the parameter \\(\\theta\\) is the value determined by the EDF \\(F_{n}\\). In most cases this is the estimate \\(\\widehat{\\theta}\\). It is the true value of the coefficient when the true distribution is \\(F_{n}\\). We now carefully explain the connection with the bootstrap algorithm as previously described.\nFirst, observe that sampling with replacement from the sample \\(\\left\\{Y_{1}, \\ldots, Y_{n}\\right\\}\\) is identical to sampling from the EDF \\(F_{n}\\). This is because the EDF is the probability distribution which puts probability mass \\(1 / n\\) on each observation. Thus sampling from \\(F_{n}\\) means sampling an observation with probability \\(1 / n\\), which is sampling with replacement.\nSecond, observe that the bootstrap estimator \\(\\widehat{\\theta}^{*}\\) described here is identical to the bootstrap algorithm described in Section 10.6. That is, \\(\\widehat{\\theta}^{*}\\) is the random vector generated by applying the estimator formula \\(\\widehat{\\theta}\\) to samples obtained by random sampling from \\(F_{n}\\).\nThird, observe that the distribution of these bootstrap estimators is the bootstrap distribution (10.9). This is a precise equality. That is, the bootstrap algorithm generates i.i.d. samples from \\(F_{n}\\), and when the estimators are applied we obtain random variables \\(\\widehat{\\theta}^{*}\\) with the distribution \\(G_{n}^{*}\\).\nFourth, observe that the bootstrap statistics described earlier - bootstrap variance, standard error, and quantiles - are estimators of the corresponding features of the bootstrap distribution \\(G_{n}^{*}\\).\nThis discussion is meant to carefully describe why the notation \\(G_{n}^{*}(u)\\) is useful to help understand the properties of the bootstrap algorithm. Since \\(F_{n}\\) is the natural nonparametric estimator of the unknown distribution \\(F, G_{n}^{*}(u)=G_{n}\\left(u, F_{n}\\right)\\) is the natural plug-in estimator of the unknown \\(G_{n}(u, F)\\). Furthermore, because \\(F_{n}\\) is uniformly consistent for \\(F\\) by the Glivenko-Cantelli Lemma (Theorem \\(18.8\\) in Probability and Statistics for Economists) we also can expect \\(G_{n}^{*}(u)\\) to be consistent for \\(G_{n}(u)\\). Making this precise is a bit challenging since \\(F_{n}\\) and \\(G_{n}\\) are functions. In the next several sections we develop an asymptotic distribution theory for the bootstrap distribution based on extending asymptotic theory to the case of conditional distributions."
  },
  {
    "objectID": "chpt10-resample-method.html#the-distribution-of-the-bootstrap-observations",
    "href": "chpt10-resample-method.html#the-distribution-of-the-bootstrap-observations",
    "title": "10  Resampling Methods",
    "section": "10.10 The Distribution of the Bootstrap Observations",
    "text": "10.10 The Distribution of the Bootstrap Observations\nLet \\(Y^{*}\\) be a random draw from the sample \\(\\left\\{Y_{1}, \\ldots, Y_{n}\\right\\}\\). What is the distribution of \\(Y^{*}\\) ?\nSince we are fixing the observations, the correct question is: What is the conditional distribution of \\(Y^{*}\\), conditional on the observed data? The empirical distribution function \\(F_{n}\\) summarizes the information in the sample, so equivalently we are talking about the distribution conditional on \\(F_{n}\\). Consequently we will write the bootstrap probability function and expectation as\n\\[\n\\begin{aligned}\n\\mathbb{P}^{*}\\left[Y^{*} \\leq x\\right] &=\\mathbb{P}\\left[Y^{*} \\leq x \\mid F_{n}\\right] \\\\\n\\mathbb{E}^{*}\\left[Y^{*}\\right] &=\\mathbb{E}\\left[Y^{*} \\mid F_{n}\\right] .\n\\end{aligned}\n\\]\nNotationally, the starred distribution and expectation are conditional given the data.\nThe (conditional) distribution of \\(Y^{*}\\) is the empirical distribution function \\(F_{n}\\), which is a discrete distribution with mass points \\(1 / n\\) on each observation \\(Y_{i}\\). Thus even if the original data come from a continuous distribution, the bootstrap data distribution is discrete.\nThe (conditional) mean and variance of \\(Y^{*}\\) are calculated from the EDF, and equal the sample mean and variance of the data. The mean is\n\\[\n\\mathbb{E}^{*}\\left[Y^{*}\\right]=\\sum_{i=1}^{n} Y_{i} \\mathbb{P}^{*}\\left[Y^{*}=Y_{i}\\right]=\\sum_{i=1}^{n} Y_{i} \\frac{1}{n}=\\bar{Y}\n\\]\nand the variance is\n\\[\n\\begin{aligned}\n\\operatorname{var}^{*}\\left[Y^{*}\\right] &=\\mathbb{E}^{*}\\left[Y^{*} Y^{* \\prime}\\right]-\\left(\\mathbb{E}^{*}\\left[Y^{*}\\right]\\right)\\left(\\mathbb{E}^{*}\\left[Y^{*}\\right]\\right)^{\\prime} \\\\\n&=\\sum_{i=1}^{n} Y_{i} Y_{i}^{\\prime} \\mathbb{P}^{*}\\left[Y^{*}=Y_{i}\\right]-\\bar{Y} \\bar{Y}^{\\prime} \\\\\n&=\\sum_{i=1}^{n} Y_{i} Y_{i}^{\\prime} \\frac{1}{n}-\\bar{Y} \\bar{Y}^{\\prime} \\\\\n&=\\widehat{\\Sigma}\n\\end{aligned}\n\\]\nTo summarize, the conditional distribution of \\(Y^{*}\\), given \\(F_{n}\\), is the discrete distribution on \\(\\left\\{Y_{1}, \\ldots, Y_{n}\\right\\}\\) with mean \\(\\bar{Y}\\) and covariance matrix \\(\\widehat{\\Sigma}\\).\nWe can extend this analysis to any integer moment \\(r\\). Assume \\(Y\\) is scalar. The \\(r^{t h}\\) moment of \\(Y^{*}\\) is\n\\[\n\\mu_{r}^{* \\prime}=\\mathbb{E}^{*}\\left[Y^{* r}\\right]=\\sum_{i=1}^{n} Y_{i}^{r} \\mathbb{P}^{*}\\left[Y^{*}=Y_{i}\\right]=\\frac{1}{n} \\sum_{i=1}^{n} Y_{i}^{r}=\\widehat{\\mu}_{r}^{\\prime},\n\\]\nthe \\(r^{t h}\\) sample moment. The \\(r^{t h}\\) central moment of \\(Y^{*}\\) is\n\\[\n\\mu_{r}^{*}=\\mathbb{E}^{*}\\left[\\left(Y^{*}-\\bar{Y}\\right)^{r}\\right]=\\frac{1}{n} \\sum_{i=1}^{n}\\left(Y_{i}-\\bar{Y}\\right)^{r}=\\widehat{\\mu}_{r},\n\\]\nthe \\(r^{t h}\\) central sample moment. Similarly, the \\(r^{t h}\\) cumulant of \\(Y^{*}\\) is \\(\\kappa_{r}^{*}=\\widehat{\\kappa}_{r}\\), the \\(r^{t h}\\) sample cumulant."
  },
  {
    "objectID": "chpt10-resample-method.html#the-distribution-of-the-bootstrap-sample-mean",
    "href": "chpt10-resample-method.html#the-distribution-of-the-bootstrap-sample-mean",
    "title": "10  Resampling Methods",
    "section": "10.11 The Distribution of the Bootstrap Sample Mean",
    "text": "10.11 The Distribution of the Bootstrap Sample Mean\nThe bootstrap sample mean is\n\\[\n\\bar{Y}^{*}=\\frac{1}{n} \\sum_{i=1}^{n} Y_{i}^{*} .\n\\]\nWe can calculate its (conditional) mean and variance. The mean is\n\\[\n\\mathbb{E}^{*}\\left[\\bar{Y}^{*}\\right]=\\mathbb{E}^{*}\\left[\\frac{1}{n} \\sum_{i=1}^{n} Y_{i}^{*}\\right]=\\frac{1}{n} \\sum_{i=1}^{n} \\mathbb{E}^{*}\\left[Y_{i}^{*}\\right]=\\frac{1}{n} \\sum_{i=1}^{n} \\bar{Y}=\\bar{Y}\n\\]\nusing (10.10). Thus the bootstrap sample mean \\(\\bar{Y}^{*}\\) has a distribution centered at the sample mean \\(\\bar{Y}\\). This is because the bootstrap observations \\(Y_{i}^{*}\\) are drawn from the bootstrap universe, which treats the EDF as the truth, and the mean of the latter distribution is \\(\\bar{Y}\\).\nThe (conditional) variance of the bootstrap sample mean is\n\\[\n\\operatorname{var}^{*}\\left[\\bar{Y}^{*}\\right]=\\operatorname{var}^{*}\\left[\\frac{1}{n} \\sum_{i=1}^{n} Y_{i}^{*}\\right]=\\frac{1}{n^{2}} \\sum_{i=1}^{n} \\operatorname{var}^{*}\\left[Y_{i}^{*}\\right]=\\frac{1}{n^{2}} \\sum_{i=1}^{n} \\widehat{\\Sigma}=\\frac{1}{n} \\widehat{\\Sigma}\n\\]\nusing (10.11). In the scalar case, \\(\\operatorname{var}^{*}\\left[\\bar{Y}^{*}\\right]=\\widehat{\\sigma}^{2} / n\\). This shows that the bootstrap variance of \\(\\bar{Y}^{*}\\) is precisely described by the sample variance of the original observations. Again, this is because the bootstrap observations \\(Y_{i}^{*}\\) are drawn from the bootstrap universe.\nWe can extend this to any integer moment \\(r\\). Assume \\(Y\\) is scalar. Define the normalized bootstrap sample mean \\(Z_{n}^{*}=\\sqrt{n}\\left(\\bar{Y}^{*}-\\bar{Y}\\right)\\). Using expressions from Section \\(6.17\\) of Probability and Statistics for Economists, the \\(3^{r d}\\) through \\(6^{\\text {th }}\\) conditional moments of \\(Z_{n}^{*}\\) are\n\\[\n\\begin{aligned}\n&\\mathbb{E}^{*}\\left[Z_{n}^{* 3}\\right]=\\widehat{\\kappa}_{3} / n^{1 / 2} \\\\\n&\\mathbb{E}^{*}\\left[Z_{n}^{* 4}\\right]=\\widehat{\\kappa}_{4} / n+3 \\widehat{\\kappa}_{2}^{2} \\\\\n&\\mathbb{E}^{*}\\left[Z_{n}^{* 5}\\right]=\\widehat{\\kappa}_{5} / n^{3 / 2}+10 \\widehat{\\kappa}_{3} \\widehat{\\kappa}_{2} / n^{1 / 2} \\\\\n&\\mathbb{E}^{*}\\left[Z_{n}^{* 6}\\right]=\\widehat{\\kappa}_{6} / n^{2}+\\left(15 \\widehat{\\kappa}_{4} \\kappa_{2}+10 \\widehat{\\kappa}_{3}^{2}\\right) / n+15 \\widehat{\\kappa}_{2}^{3}\n\\end{aligned}\n\\]\nwhere \\(\\widehat{\\kappa}_{r}\\) is the \\(r^{t h}\\) sample cumulant. Similar expressions can be derived for higher moments. The moments (10.14) are exact, not approximations."
  },
  {
    "objectID": "chpt10-resample-method.html#bootstrap-asymptotics",
    "href": "chpt10-resample-method.html#bootstrap-asymptotics",
    "title": "10  Resampling Methods",
    "section": "10.12 Bootstrap Asymptotics",
    "text": "10.12 Bootstrap Asymptotics\nThe bootstrap mean \\(\\bar{Y}^{*}\\) is a sample average over \\(n\\) i.i.d. random variables, so we might expect it to converge in probability to its expectation. Indeed, this is the case, but we have to be a bit careful since the bootstrap mean has a conditional distribution (given the data) so we need to define convergence in probability for conditional distributions.\nDefinition \\(10.1\\) We say that a random vector \\(Z_{n}^{*}\\) converges in bootstrap probability to \\(Z\\) as \\(n \\rightarrow \\infty\\), denoted \\(Z_{n}^{*} \\underset{p^{*}}{\\longrightarrow} Z\\), if for all \\(\\epsilon>0\\)\n\\[\n\\mathbb{P}^{*}\\left[\\left\\|Z_{n}^{*}-Z\\right\\|>\\epsilon\\right] \\underset{p}{\\longrightarrow} 0\n\\]\nTo understand this definition recall that conventional convergence in probability \\(Z_{n} \\underset{p}{\\longrightarrow}\\) means that for a sufficiently large sample size \\(n\\), the probability is high that \\(Z_{n}\\) is arbitrarily close to its limit \\(Z\\). In contrast, Definition \\(10.1\\) says \\(Z_{n}^{*} \\underset{p^{*}}{ } Z\\) means that for a sufficiently large \\(n\\), the probability is high that the conditional probability that \\(Z_{n}^{*}\\) is close to its limit \\(Z\\) is high. Note that there are two uses of probability - both unconditional and conditional.\nOur label “convergence in bootstrap probability” is a bit unusual. The label used in much of the statistical literature is “convergence in probability, in probability” but that seems like a mouthful. That literature more often focuses on the related concept of “convergence in probability, almost surely” which holds if we replace the ” \\(\\underset{p}{\\text { \" }}\\) convergence with almost sure convergence. We do not use this concept in this chapter as it is an unnecessary complication.\nWhile we have stated Definition \\(10.1\\) for the specific conditional probability distribution \\(\\mathbb{P}^{*}\\), the idea is more general and can be used for any conditional distribution and any sequence of random vectors.\nThe following may seem obvious but it is useful to state for clarity. Its proof is given in Section \\(10.31 .\\)\nTheorem \\(10.1\\) If \\(Z_{n} \\underset{p}{\\longrightarrow} Z\\) as \\(n \\rightarrow \\infty\\) then \\(Z_{n} \\underset{p^{*}}{ } Z\\).\nGiven Definition 10.1, we can establish a law of large numbers for the bootstrap sample mean. Theorem \\(10.2\\) Bootstrap WLLN. If \\(Y_{i}\\) are independent and uniformly integrable then \\(\\bar{Y}^{*}-\\bar{Y} \\underset{p^{*}}{\\longrightarrow} 0\\) and \\(\\bar{Y}^{*} \\underset{p^{*}}{\\longrightarrow} \\mu=\\mathbb{E}[Y]\\) as \\(n \\rightarrow \\infty\\).\nThe proof (presented in Section 10.31) is somewhat different from the classical case as it is based on the Marcinkiewicz WLLN (Theorem 10.20, presented in Section 10.31).\nNotice that the conditions for the bootstrap WLLN are the same for the conventional WLLN. Notice as well that we state two related but slightly different results. The first is that the difference between the bootstrap sample mean \\(\\bar{Y}^{*}\\) and the sample mean \\(\\bar{Y}\\) diminishes as the sample size diverges. The second result is that the bootstrap sample mean converges to the population mean \\(\\mu\\). The latter is not surprising (since the sample mean \\(\\bar{Y}\\) converges in probability to \\(\\mu\\) ) but it is constructive to be precise since we are dealing with a new convergence concept.\nTheorem 10.3 Bootstrap Continuous Mapping Theorem. If \\(Z_{n}^{*} \\underset{p^{*}}{ } c\\) as \\(n \\rightarrow\\) \\(\\infty\\) and \\(g(\\cdot)\\) is continuous at \\(c\\), then \\(g\\left(Z_{n}^{*}\\right) \\underset{p^{*}}{ } g(c)\\) as \\(n \\rightarrow \\infty\\).\nThe proof is essentially identical to that of Theorem \\(6.6\\) so is omitted.\nWe next would like to show that the bootstrap sample mean is asymptotically normally distributed, but for that we need a definition of convergence for conditional distributions.\nDefinition \\(10.2\\) Let \\(Z_{n}^{*}\\) be a sequence of random vectors with conditional distributions \\(G_{n}^{*}(x)=\\mathbb{P}^{*}\\left[Z_{n}^{*} \\leq x\\right]\\). We say that \\(Z_{n}^{*}\\) converges in bootstrap distribution to \\(Z\\) as \\(n \\rightarrow \\infty\\), denoted \\(Z_{n}^{*} \\underset{d^{*}}{\\longrightarrow}\\), if for all \\(x\\) at which \\(G(x)=\\mathbb{P}[Z \\leq x]\\) is continuous, \\(G_{n}^{*}(x) \\underset{p}{\\longrightarrow} G(x)\\) as \\(n \\rightarrow \\infty\\).\nThe difference with the conventional definition is that Definition \\(10.2\\) treats the conditional distribution as random. An alternative label for Definition \\(10.2\\) is “convergence in distribution, in probability”.\nWe now state a CLT for the bootstrap sample mean, with a proof given in Section 10.31.\nTheorem 10.4 Bootstrap CLT. If \\(Y_{i}\\) are i.i.d., \\(\\mathbb{E}\\|Y\\|^{2}<\\infty\\), and \\(\\Sigma=\\operatorname{var}[Y]>0\\), then as \\(n \\rightarrow \\infty, \\sqrt{n}\\left(\\bar{Y}^{*}-\\bar{Y}\\right) \\underset{d^{*}}{\\longrightarrow} \\mathrm{N}(0, \\Sigma)\\).\nTheorem \\(10.4\\) shows that the normalized bootstrap sample mean has the same asymptotic distribution as the sample mean. Thus the bootstrap distribution is asymptotically the same as the sampling distribution. A notable difference, however, is that the bootstrap sample mean is normalized by centering at the sample mean, not at the population mean. This is because \\(\\bar{Y}\\) is the true mean in the bootstrap universe.\nWe next state the distributional form of the continuous mapping theorem for bootstrap distributions and the Bootstrap Delta Method. Theorem 10.5 Bootstrap Continuous Mapping Theorem\nIf \\(Z_{n}^{*} \\underset{d^{*}}{ } Z\\) as \\(n \\rightarrow \\infty\\) and \\(g: \\mathbb{R}^{m} \\rightarrow \\mathbb{R}^{k}\\) has the set of discontinuity points \\(D_{g}\\) such that \\(\\mathbb{P}^{*}\\left[Z^{*} \\in D_{g}\\right]=0\\), then \\(g\\left(Z_{n}^{*}\\right) \\underset{d^{*}}{\\rightarrow} g(Z)\\) as \\(n \\rightarrow \\infty\\).\nTheorem 10.6 Bootstrap Delta Method: If \\(\\widehat{\\mu} \\underset{p}{\\longrightarrow} \\mu, \\sqrt{n}\\left(\\widehat{\\mu}^{*}-\\widehat{\\mu}\\right) \\underset{d^{*}}{\\longrightarrow} \\xi\\), and \\(g(u)\\) is continuously differentiable in a neighborhood of \\(\\mu\\), then as \\(n \\rightarrow \\infty\\)\n\\[\n\\sqrt{n}\\left(g\\left(\\widehat{\\mu}^{*}\\right)-g(\\widehat{\\mu})\\right) \\underset{d^{*}}{\\longrightarrow} \\boldsymbol{G}^{\\prime} \\xi\n\\]\nwhere \\(\\boldsymbol{G}(x)=\\frac{\\partial}{\\partial x} g(x)^{\\prime}\\) and \\(\\boldsymbol{G}=\\boldsymbol{G}(\\mu)\\). In particular, if \\(\\xi \\sim \\mathrm{N}(0, \\boldsymbol{V})\\) then as \\(n \\rightarrow \\infty\\)\n\\[\n\\sqrt{n}\\left(g\\left(\\widehat{\\mu}^{*}\\right)-g(\\widehat{\\mu})\\right) \\underset{d^{*}}{\\longrightarrow} \\mathrm{N}\\left(0, \\boldsymbol{G}^{\\prime} \\boldsymbol{V} \\boldsymbol{G}\\right) .\n\\]\nFor a proof, see Exercise 10.7.\nWe state an analog of Theorem 6.10, which presented the asymptotic distribution for general smooth functions of sample means, which covers most econometric estimators.\nTheorem 10.7 Under the assumptions of Theorem 6.10, that is, if \\(Y_{i}\\) is i.i.d., \\(\\mu=\\mathbb{E}[h(Y)], \\theta=g(\\mu), \\mathbb{E}\\|h(Y)\\|^{2}<\\infty\\), and \\(\\boldsymbol{G}(x)=\\frac{\\partial}{\\partial x} g(x)^{\\prime}\\) is continuous in a neighborhood of \\(\\mu\\), for \\(\\widehat{\\theta}=g(\\widehat{\\mu})\\) with \\(\\widehat{\\mu}=\\frac{1}{n} \\sum_{i=1}^{n} h\\left(Y_{i}\\right)\\) and \\(\\widehat{\\theta}^{*}=g\\left(\\widehat{\\mu}^{*}\\right)\\) with \\(\\widehat{\\mu}^{*}=\\frac{1}{n} \\sum_{i=1}^{n} h\\left(Y_{i}^{*}\\right)\\), as \\(n \\rightarrow \\infty\\)\n\\[\n\\sqrt{n}\\left(\\widehat{\\theta}^{*}-\\widehat{\\theta}\\right) \\underset{d^{*}}{\\longrightarrow} \\mathrm{N}\\left(0, \\boldsymbol{V}_{\\theta}\\right)\n\\]\nwhere \\(\\boldsymbol{V}_{\\theta}=\\boldsymbol{G}^{\\prime} \\boldsymbol{V} \\boldsymbol{G}, \\boldsymbol{V}=\\mathbb{E}\\left[(h(Y)-\\mu)(h(Y)-\\mu)^{\\prime}\\right]\\) and \\(\\boldsymbol{G}=\\boldsymbol{G}(\\mu)\\).\nFor a proof, see Exercise 10.8.\nTheorem \\(10.7\\) shows that the asymptotic distribution of the bootstrap estimator \\(\\widehat{\\theta}^{*}\\) is identical to that of the sample estimator \\(\\widehat{\\theta}\\). This means that we can learn the distribution of \\(\\widehat{\\theta}\\) from the bootstrap distribution, and hence perform asymptotically correct inference.\nFor some bootstrap applications we use bootstrap estimates of variance. The plug-in estimator of \\(\\boldsymbol{V}_{\\boldsymbol{\\theta}}\\) is \\(\\widehat{\\boldsymbol{V}}_{\\theta}=\\widehat{\\boldsymbol{G}}^{\\prime} \\widehat{\\boldsymbol{V}} \\widehat{\\boldsymbol{G}}\\) where \\(\\widehat{\\boldsymbol{G}}=\\boldsymbol{G}(\\widehat{\\mu})\\) and\n\\[\n\\widehat{\\boldsymbol{V}}=\\frac{1}{n} \\sum_{i=1}^{n}\\left(h\\left(Y_{i}\\right)-\\widehat{\\mu}\\right)\\left(h\\left(Y_{i}\\right)-\\widehat{\\mu}\\right)^{\\prime} .\n\\]\nThe bootstrap version is\n\\[\n\\begin{aligned}\n&\\widehat{\\boldsymbol{V}}_{\\theta}^{*}=\\widehat{\\boldsymbol{G}}^{* \\prime} \\widehat{\\boldsymbol{V}}^{*} \\widehat{\\boldsymbol{G}}^{*} \\\\\n&\\widehat{\\boldsymbol{G}}^{*}=\\boldsymbol{G}\\left(\\widehat{\\mu}^{*}\\right) \\\\\n&\\widehat{\\boldsymbol{V}}^{*}=\\frac{1}{n} \\sum_{i=1}^{n}\\left(h\\left(Y_{i}^{*}\\right)-\\widehat{\\mu}^{*}\\right)\\left(h\\left(Y_{i}^{*}\\right)-\\widehat{\\mu}^{*}\\right)^{\\prime} .\n\\end{aligned}\n\\]\nApplication of the bootstrap WLLN and bootstrap CMT show that \\(\\widehat{\\boldsymbol{V}}_{\\theta}^{*}\\) is consistent for \\(\\boldsymbol{V}_{\\theta}\\).\nTheorem \\(10.8\\) Under the assumptions of Theorem 10.7, \\(\\widehat{\\boldsymbol{V}}_{\\theta}^{*} \\underset{p^{*}}{\\longrightarrow} \\boldsymbol{V}_{\\theta}\\) as \\(n \\rightarrow \\infty\\).\nFor a proof, see Exercise 10.9."
  },
  {
    "objectID": "chpt10-resample-method.html#consistency-of-the-bootstrap-estimate-of-variance",
    "href": "chpt10-resample-method.html#consistency-of-the-bootstrap-estimate-of-variance",
    "title": "10  Resampling Methods",
    "section": "10.13 Consistency of the Bootstrap Estimate of Variance",
    "text": "10.13 Consistency of the Bootstrap Estimate of Variance\nRecall the definition (10.7) of the bootstrap estimator of variance \\(\\widehat{\\boldsymbol{V}}_{\\widehat{\\theta}}^{\\text {boot }}\\) of an estimator \\(\\widehat{\\theta}\\). In this section we explore conditions under which \\(\\widehat{\\boldsymbol{V}}_{\\widehat{\\theta}}^{\\text {boot }}\\) is consistent for the asymptotic variance of \\(\\widehat{\\theta}\\).\nTo do so it is useful to focus on a normalized version of the estimator so that the asymptotic variance is not degenerate. Suppose that for some sequence \\(a_{n}\\) we have\n\\[\nZ_{n}=a_{n}(\\widehat{\\theta}-\\theta) \\underset{d}{\\longrightarrow} \\xi\n\\]\nand\n\\[\nZ_{n}^{*}=a_{n}\\left(\\widehat{\\theta}^{*}-\\widehat{\\theta}\\right) \\underset{d^{*}}{\\longrightarrow} \\xi\n\\]\nfor some limit distribution \\(\\xi\\). That is, for some normalization, both \\(\\hat{\\theta}\\) and \\(\\widehat{\\theta}^{*}\\) have the same asymptotic distribution. This is quite general as it includes the smooth function model. The conventional bootstrap estimator of the variance of \\(Z_{n}\\) is the sample variance of the bootstrap draws \\(\\left\\{Z_{n}^{*}(b): b=1, \\ldots, B\\right\\}\\). This equals the estimator (10.7) multiplied by \\(a_{n}^{2}\\). Thus it is equivalent (up to scale) whether we discuss estimating the variance of \\(\\widehat{\\theta}\\) or \\(Z_{n}\\).\nThe bootstrap estimator of variance of \\(Z_{n}\\) is\n\\[\n\\begin{aligned}\n\\widehat{\\boldsymbol{V}}_{\\theta}^{\\text {boot,B }} &=\\frac{1}{B-1} \\sum_{b=1}^{B}\\left(Z_{n}^{*}(b)-Z_{n}^{*}\\right)\\left(Z_{n}^{*}(b)-Z_{n}^{*}\\right)^{\\prime} \\\\\n\\bar{Z}_{n}^{*} &=\\frac{1}{B} \\sum_{b=1}^{B} Z_{n}^{*}(b)\n\\end{aligned}\n\\]\nNotice that we index the estimator by the number of bootstrap replications \\(B\\).\nSince \\(Z_{n}^{*}\\) converges in bootstrap distribution to the same asymptotic distribution as \\(Z_{n}\\), it seems reasonable to guess that the variance of \\(Z_{n}^{*}\\) will converge to that of \\(\\xi\\). However, convergence in distribution is not sufficient for convergence in moments. For the variance to converge it is also necessary for the sequence \\(Z_{n}^{*}\\) to be uniformly square integrable. Theorem \\(10.9\\) If (10.15) and (10.16) hold for some sequence \\(a_{n}\\) and \\(\\left\\|Z_{n}^{*}\\right\\|^{2}\\) is uniformly integrable, then as \\(B \\rightarrow \\infty\\)\n\\[\n\\widehat{\\boldsymbol{V}}_{\\theta}^{\\mathrm{boot}, \\mathrm{B}} \\underset{p^{*}}{\\longrightarrow} \\widehat{\\boldsymbol{V}}_{\\theta}^{\\text {boot }}=\\operatorname{var}\\left[Z_{n}^{*}\\right] \\text {, }\n\\]\nand as \\(n \\rightarrow \\infty\\)\n\\[\n\\widehat{\\boldsymbol{V}}_{\\theta}^{\\text {boot }} \\underset{p^{*}}{\\longrightarrow} \\boldsymbol{V}_{\\theta}=\\operatorname{var}[\\xi] .\n\\]\nThis raises the question: Is the normalized sequence \\(Z_{n}\\) uniformly integrable? We spend the remainder of this section exploring this question and turn in the next section to trimmed variance estimators which do not require uniform integrability.\nThis condition is reasonably straightforward to verify for the case of a scalar sample mean with a finite variance. That is, suppose \\(Z_{n}^{*}=\\sqrt{n}\\left(\\bar{Y}^{*}-\\bar{Y}\\right)\\) and \\(\\mathbb{E}\\left[Y^{2}\\right]<\\infty\\). In (10.14) we calculated the exact fourth central moment of \\(Z_{n}^{*}\\) :\n\\[\n\\mathbb{E}^{*}\\left[Z_{n}^{* 4}\\right]=\\frac{\\widehat{\\kappa}_{4}}{n}+3 \\widehat{\\sigma}^{4}=\\frac{\\widehat{\\mu}_{4}-3 \\widehat{\\sigma}^{4}}{n}+3 \\widehat{\\sigma}^{4}\n\\]\nwhere \\(\\widehat{\\sigma}^{2}=n^{-1} \\sum_{i=1}^{n}\\left(Y_{i}-\\bar{Y}\\right)^{2}\\) and \\(\\widehat{\\mu}_{4}=n^{-1} \\sum_{i=1}^{n}\\left(Y_{i}-\\bar{Y}\\right)^{4}\\). The assumption \\(\\mathbb{E}\\left[Y^{2}\\right]<\\infty\\) implies that \\(\\mathbb{E}\\left[\\widehat{\\sigma}^{2}\\right]=O(1)\\) so \\(\\widehat{\\sigma}^{2}=O_{p}(1)\\). Furthermore, \\(n^{-1} \\widehat{\\mu}_{4}=n^{-2} \\sum_{i=1}^{n}\\left(Y_{i}-\\bar{Y}\\right)^{4}=o_{p}(1)\\) by the Marcinkiewicz WLLN (Theorem 10.20). It follows that\n\\[\n\\mathbb{E}^{*}\\left[Z_{n}^{* 4}\\right]=n^{2} \\mathbb{E}^{*}\\left[\\left(\\bar{Y}^{*}-\\bar{Y}\\right)^{4}\\right]=O_{p}(1) .\n\\]\nTheorem \\(6.13\\) shows that this implies that \\(Z_{n}^{* 2}\\) is uniformly integrable. Thus if \\(Y\\) has a finite variance the normalized bootstrap sample mean is uniformly square integrable and the bootstrap estimate of variance is consistent by Theorem \\(10.9\\).\nNow consider the smooth function model of Theorem 10.7. We can establish the following result.\nTheorem 10.10 In the smooth function model of Theorem 10.7, if for some \\(p \\geq 1\\) the \\(p^{t h}\\)-order derivatives of \\(g(x)\\) are bounded, then \\(Z_{n}^{*}=\\sqrt{n}\\left(\\widehat{\\theta}^{*}-\\widehat{\\theta}\\right)\\) is uniformly square integrable and the bootstrap estimator of variance is consistent as in Theorem 10.9.\nFor a proof see Section \\(10.31\\).\nThis shows that the bootstrap estimate of variance is consistent for a reasonably broad class of estimators. The class of functions \\(g(x)\\) covered by this result includes all \\(p^{t h}\\)-order polynomials."
  },
  {
    "objectID": "chpt10-resample-method.html#trimmed-estimator-of-bootstrap-variance",
    "href": "chpt10-resample-method.html#trimmed-estimator-of-bootstrap-variance",
    "title": "10  Resampling Methods",
    "section": "10.14 Trimmed Estimator of Bootstrap Variance",
    "text": "10.14 Trimmed Estimator of Bootstrap Variance\nTheorem \\(10.10\\) showed that the bootstrap estimator of variance is consistent for smooth functions with a bounded \\(p^{t h}\\) order derivative. This is a fairly broad class but excludes many important applications. An example is \\(\\theta=\\mu_{1} / \\mu_{2}\\) where \\(\\mu_{1}=\\mathbb{E}\\left[Y_{1}\\right]\\) and \\(\\mu_{2}=\\mathbb{E}\\left[Y_{2}\\right]\\). This function does not have a bounded derivative (unless \\(\\mu_{2}\\) is bounded away from zero) so is not covered by Theorem 10.10. This is more than a technical issue. When \\(\\left(Y_{1}, Y_{2}\\right)\\) are jointly normally distributed then it is known that \\(\\widehat{\\theta}=\\bar{Y}_{1} / \\bar{Y}_{2}\\) does not possess a finite variance. Consequently we cannot expect the bootstrap estimator of variance to perform well. (It is attempting to estimate the variance of \\(\\widehat{\\theta}\\), which is infinity.)\nIn these cases it is preferred to use a trimmed estimator of bootstrap variance. Let \\(\\tau_{n} \\rightarrow \\infty\\) be a sequence of positive trimming numbers satisfying \\(\\tau_{n}=O\\left(e^{n / 8}\\right)\\). Define the trimmed statistic\n\\[\nZ_{n}^{* *}=Z_{n}^{*} \\mathbb{1}\\left\\{\\left\\|Z_{n}^{*}\\right\\| \\leq \\tau_{n}\\right\\} .\n\\]\nThe trimmed bootstrap estimator of variance is\n\\[\n\\begin{aligned}\n\\widehat{\\boldsymbol{V}}_{\\theta}^{\\text {boot, }, \\tau} &=\\frac{1}{B-1} \\sum_{b=1}^{B}\\left(Z_{n}^{* *}(b)-Z_{n}^{* *}\\right)\\left(Z_{n}^{* *}(b)-Z_{n}^{* *}\\right)^{\\prime} \\\\\nZ_{n}^{* *} &=\\frac{1}{B} \\sum_{b=1}^{B} Z_{n}^{* *}(b) .\n\\end{aligned}\n\\]\nWe first examine the behavior of \\(\\widehat{\\boldsymbol{V}}_{\\theta}^{\\text {boot, } \\mathrm{B}}\\) as the number of bootstrap replications \\(B\\) grows to infinity. It is a sample variance of independent bounded random vectors. Thus by the bootstrap WLLN (Theorem 10.2) \\(\\widehat{\\boldsymbol{V}}_{\\theta}^{\\mathrm{boot}, \\mathrm{B}, \\tau}\\) converges in bootstrap probability to the variance of \\(Z_{n}^{* *}\\).\n\nWe next examine the behavior of the bootstrap estimator \\(\\widehat{\\boldsymbol{V}}_{\\theta}^{\\text {boot, } \\tau}\\) as \\(n\\) grows to infinity. We focus on the smooth function model of Theorem 10.7, which showed that \\(Z_{n}^{*}=\\sqrt{n}\\left(\\widehat{\\theta}^{*}-\\widehat{\\theta}\\right) \\underset{d^{*}}{\\longrightarrow} \\sim \\mathrm{N}\\left(0, \\boldsymbol{V}_{\\theta}\\right)\\). Since the trimming is asymptotically negligible, it follows that \\(Z_{n}^{* *} \\underset{d^{*}}{\\longrightarrow}\\). If we can show that \\(Z_{n}^{* *}\\) is uniformly square integrable, Theorem \\(10.9\\) shows that \\(\\operatorname{var}\\left[Z_{n}^{* *}\\right] \\rightarrow \\operatorname{var}[Z]=\\boldsymbol{V}_{\\theta}\\) as \\(n \\rightarrow \\infty\\). This is shown in the following result, whose proof is presented in Section 10.31.\nTheorem \\(10.12\\) Under the assumptions of Theorem 10.7, \\(\\widehat{\\boldsymbol{V}}_{\\theta}^{\\mathrm{boot}, \\tau} \\underset{p^{*}}{\\longrightarrow} \\boldsymbol{V}_{\\theta} .\\)\nTheorems \\(10.11\\) and \\(10.12\\) show that the trimmed bootstrap estimator of variance is consistent for the asymptotic variance in the smooth function model, which includes most econometric estimators. This justifies bootstrap standard errors as consistent estimators for the asymptotic distribution.\nAn important caveat is that these results critically rely on the trimmed variance estimator. This is a critical caveat as conventional statistical packages (e.g. Stata) calculate bootstrap standard errors using the untrimmed estimator (10.7). Thus there is no guarantee that the reported standard errors are consistent. The untrimmed variance estimator works in the context of Theorem \\(10.10\\) and whenever the bootstrap statistic is uniformly square integrable, but not necessarily in general applications.\nIn practice, it may be difficult to know how to select the trimming sequence \\(\\tau_{n}\\). The rule \\(\\tau_{n}=O\\left(e^{n / 8}\\right)\\) does not provide practical guidance. Instead, it may be useful to think about trimming in terms of percentages of the bootstrap draws. Thus we can set \\(\\tau_{n}\\) so that a given small percentage \\(\\gamma_{n}\\) is trimmed. For theoretical interpretation we would set \\(\\gamma_{n} \\rightarrow 0\\) as \\(n \\rightarrow \\infty\\). In practice we might set \\(\\gamma_{n}=1 %\\)."
  },
  {
    "objectID": "chpt10-resample-method.html#unreliability-of-untrimmed-bootstrap-standard-errors",
    "href": "chpt10-resample-method.html#unreliability-of-untrimmed-bootstrap-standard-errors",
    "title": "10  Resampling Methods",
    "section": "10.15 Unreliability of Untrimmed Bootstrap Standard Errors",
    "text": "10.15 Unreliability of Untrimmed Bootstrap Standard Errors\nIn the previous section we presented a trimmed bootstrap variance estimator which should be used to form bootstrap standard errors for nonlinear estimators. Otherwise, the untrimmed estimator is potentially unreliable.\nThis is an unfortunate situation, because reporting of bootstrap standard errors is commonplace in contemporary applied econometric practice, and standard applications (including Stata) use the untrimmed estimator.\nTo illustrate the seriousness of the problem we use the simple wage regression (7.31) which we repeat here. This is the subsample of married Black women with 982 observations. The point estimates and standard errors are\n\nWe are interested in the experience level which maximizes expected log wages \\(\\theta_{3}=-50 \\beta_{2} / \\beta_{3}\\). The point estimate and standard errors calculated with different methods are reported in Table \\(10.3\\) below.\nThe point estimate of the experience level with maximum earnings is \\(\\widehat{\\theta}_{3}=35\\). The asymptotic and jackknife standard errors are about 7 . The bootstrap standard error, however, is 825 ! Confused by this unusual value we rerun the bootstrap and obtain a standard error of 544 . Each was computed with 10,000 bootstrap replications. The fact that the two bootstrap standard errors are considerably different when recomputed (with different starting seeds) is indicative of moment failure. When there is an enormous discrepancy like this between the asymptotic and bootstrap standard error, and between bootstrap runs, it is a signal that there may be moment failure and consequently bootstrap standard errors are unreliable.\nA trimmed bootstrap with \\(\\tau=25\\) (set to slightly exceed three asymptotic standard errors) produces a more reasonable standard error of \\(10 .\\)\nOne message from this application is that when different methods produce very different standard errors we should be cautious about trusting any single method. The large discrepancies indicate poor asymptotic approximations, rendering all methods inaccurate. Another message is to be cautious about reporting conventional bootstrap standard errors. Trimmed versions are preferred, especially for nonlinear functions of estimated coefficients.\nTable 10.3: Experience Level Which Maximizes Expected log Wages\n\n\n\nEstimate\n\\(35.2\\)\n\n\n\n\nAsymptotic s.e.\n\\((7.0)\\)\n\n\nJackknife s.e.\n\\((7.0)\\)\n\n\nBootstrap s.e. (standard)\n\\((825)\\)\n\n\nBootstrap s.e. (repeat)\n\\((544)\\)\n\n\nBootstrap s.e. (trimmed)\n\\((10.1)\\)"
  },
  {
    "objectID": "chpt10-resample-method.html#consistency-of-the-percentile-interval",
    "href": "chpt10-resample-method.html#consistency-of-the-percentile-interval",
    "title": "10  Resampling Methods",
    "section": "10.16 Consistency of the Percentile Interval",
    "text": "10.16 Consistency of the Percentile Interval\nRecall the percentile interval (10.8). We now provide conditions under which it has asymptotically correct coverage. Theorem \\(10.13\\) Assume that for some sequence \\(a_{n}\\)\n\\[\na_{n}(\\widehat{\\theta}-\\theta) \\underset{d}{\\longrightarrow} \\xi\n\\]\nand\n\\[\na_{n}\\left(\\widehat{\\theta}^{*}-\\widehat{\\theta}\\right) \\underset{d^{*}}{\\longrightarrow} \\xi\n\\]\nwhere \\(\\xi\\) is continuously distributed and symmetric about zero. Then \\(\\mathbb{P}\\left[\\theta \\in C^{\\mathrm{pc}}\\right] \\rightarrow 1-\\alpha\\) as \\(n \\rightarrow \\infty\\)\nThe assumptions (10.18)-(10.19) hold for the smooth function model of Theorem 10.7, so this result incorporates many applications. The beauty of Theorem \\(10.13\\) is that the simple confidence interval \\(C^{\\mathrm{pc}}\\) - which does not require technical calculation of asymptotic standard errors - has asymptotically valid coverage for any estimator which falls in the smooth function class, as well as any other estimator satisfying the convergence results (10.18)-(10.19) with \\(\\xi\\) symmetrically distributed. The conditions are weaker than those required for consistent bootstrap variance estimation (and normal-approximation confidence intervals) because it is not necessary to verify that \\(\\widehat{\\theta}^{*}\\) is uniformly integrable, nor necessary to employ trimming.\nThe proof of Theorem \\(10.7\\) is not difficult. The convergence assumption (10.19) implies that the \\(\\alpha^{t h}\\) quantile of \\(a_{n}\\left(\\widehat{\\theta}^{*}-\\widehat{\\theta}\\right)\\), which is \\(a_{n}\\left(q_{\\alpha}^{*}-\\widehat{\\theta}\\right)\\) by quantile equivariance, converges in probability to the \\(\\alpha^{t h}\\) quantile of \\(\\xi\\), which we can denote as \\(\\bar{q}_{\\alpha}\\). Thus\n\\[\na_{n}\\left(q_{\\alpha}^{*}-\\widehat{\\theta}\\right) \\underset{p}{\\longrightarrow} \\bar{q}_{\\alpha} .\n\\]\nLet \\(H(x)=\\mathbb{P}[\\xi \\leq x]\\) be the distribution function of \\(\\xi\\). The assumption of symmetry implies \\(H(-x)=\\) \\(1-H(x)\\). Then the percentile interval has coverage\n\\[\n\\begin{aligned}\n\\mathbb{P}\\left[\\theta \\in C^{\\mathrm{pc}}\\right] &=\\mathbb{P}\\left[q_{\\alpha / 2}^{*} \\leq \\theta \\leq q_{1-\\alpha / 2}^{*}\\right] \\\\\n&=\\mathbb{P}\\left[-a_{n}\\left(q_{\\alpha / 2}^{*}-\\widehat{\\theta}\\right) \\geq a_{n}(\\widehat{\\theta}-\\theta) \\geq-a_{n}\\left(q_{1-\\alpha / 2}^{*}-\\widehat{\\theta}\\right)\\right] \\\\\n& \\rightarrow \\mathbb{P}\\left[-\\bar{q}_{\\alpha / 2} \\geq \\xi \\geq-\\bar{q}_{1-\\alpha / 2}\\right] \\\\\n&=H\\left(-\\bar{q}_{\\alpha / 2}\\right)-H\\left(-\\bar{q}_{1-\\alpha / 2}\\right) \\\\\n&=H\\left(\\bar{q}_{1-\\alpha / 2}\\right)-H\\left(\\bar{q}_{\\alpha / 2}\\right) \\\\\n&=1-\\alpha .\n\\end{aligned}\n\\]\nThe convergence holds by (10.18) and (10.20). The following equality uses the definition of \\(H\\), the nextto-last is the symmetry of \\(H\\), and the final equality is the definition of \\(\\bar{q}_{\\alpha}\\). This establishes Theorem \\(10.13 .\\)\nTheorem \\(10.13\\) seems quite general, but it critically rests on the assumption that the asymptotic distribution \\(\\xi\\) is symmetrically distributed about zero. This may seem innocuous since conventional asymptotic distributions are normal and hence symmetric, but it deserves further scrutiny. It is not merely a technical assumption - an examination of the steps in the preceeding argument isolate quite clearly that if the symmetry assumption is violated then the asymptotic coverage will not be \\(1-\\alpha\\). While Theorem \\(10.13\\) does show that the percentile interval is asymptotically valid for a conventional asymptotically normal estimator, the reliance on symmetry in the argument suggests that the percentile method will work poorly when the finite sample distribution is asymmetric. This turns out to be the case and leads us to consider alternative methods in the following sections. It is also worthwhile to investigate a finite sample justification for the percentile interval based on a heuristic analogy due to Efron.\nAssume that there exists an unknown but strictly increasing transformation \\(\\psi(\\theta)\\) such that \\(\\psi(\\widehat{\\theta})-\\) \\(\\psi(\\theta)\\) has a pivotal distribution \\(H(u)\\) (does not vary with \\(\\theta\\) ) which is symmetric about zero. For example, if \\(\\widehat{\\theta} \\sim \\mathrm{N}\\left(\\theta, \\sigma^{2}\\right)\\) we can set \\(\\psi(\\theta)=\\theta / \\sigma\\). Alternatively, if \\(\\widehat{\\theta}=\\exp (\\widehat{\\mu})\\) and \\(\\widehat{\\mu} \\sim \\mathrm{N}\\left(\\mu, \\sigma^{2}\\right)\\) then we can set \\(\\psi(\\theta)=\\) \\(\\log (\\theta) / \\sigma\\)\nTo assess the coverage of the percentile interval, observe that since the distribution \\(H\\) is pivotal the bootstrap distribution \\(\\psi\\left(\\widehat{\\theta}^{*}\\right)-\\psi(\\widehat{\\theta})\\) also has distribution \\(H(u)\\). Let \\(\\bar{q}_{\\alpha}\\) be the \\(\\alpha^{\\text {th }}\\) quantile of the distribution \\(H\\). Since \\(q_{\\alpha}^{*}\\) is the \\(\\alpha^{t h}\\) quantile of the distribution of \\(\\widehat{\\theta}^{*}\\) and \\(\\psi\\left(\\widehat{\\theta}^{*}\\right)-\\psi(\\widehat{\\theta})\\) is a monotonic transformation of \\(\\widehat{\\theta}^{*}\\), by the quantile equivariance property we deduce that \\(\\bar{q}_{\\alpha}+\\psi(\\widehat{\\theta})=\\psi\\left(q_{\\alpha}^{*}\\right)\\). The percentile interval has coverage\n\\[\n\\begin{aligned}\n\\mathbb{P}\\left[\\theta \\in C^{\\mathrm{pc}}\\right] &=\\mathbb{P}\\left[q_{\\alpha / 2}^{*} \\leq \\theta \\leq q_{1-\\alpha / 2}^{*}\\right] \\\\\n&=\\mathbb{P}\\left[\\psi\\left(q_{\\alpha / 2}^{*}\\right) \\leq \\psi(\\theta) \\leq \\psi\\left(q_{1-\\alpha / 2}^{*}\\right)\\right] \\\\\n&=\\mathbb{P}\\left[\\psi(\\widehat{\\theta})-\\psi\\left(q_{\\alpha / 2}^{*}\\right) \\geq \\psi(\\widehat{\\theta})-\\psi(\\theta) \\geq \\psi(\\widehat{\\theta})-\\psi\\left(q_{1-\\alpha / 2}^{*}\\right)\\right] \\\\\n&=\\mathbb{P}\\left[-\\bar{q}_{\\alpha / 2} \\geq \\psi(\\widehat{\\theta})-\\psi(\\theta) \\geq-\\bar{q}_{1-\\alpha / 2}\\right] \\\\\n&=H\\left(-\\bar{q}_{\\alpha / 2}\\right)-H\\left(-\\bar{q}_{1-\\alpha / 2}\\right) \\\\\n&=H\\left(\\bar{q}_{1-\\alpha / 2}\\right)-H\\left(\\bar{q}_{\\alpha / 2}\\right) \\\\\n&=1-\\alpha .\n\\end{aligned}\n\\]\nThe second equality applies the monotonic transformation \\(\\psi(u)\\) to all elements. The fourth uses the relationship \\(\\bar{q}_{\\alpha}+\\psi(\\widehat{\\theta})=\\psi\\left(q_{\\alpha}^{*}\\right)\\). The fifth uses the defintion of \\(H\\). The sixth uses the symmetry property of \\(H\\), and the final is by the definition of \\(\\bar{q}_{\\alpha}\\) as the \\(\\alpha^{t h}\\) quantile of \\(H\\).\nThis calculation shows that under these assumptions the percentile interval has exact coverage \\(1-\\alpha\\). The nice thing about this argument is the introduction of the unknown transformation \\(\\psi(u)\\) for which the percentile interval automatically adapts. The unpleasant feature is the assumption of symmetry. Similar to the asymptotic argument the calculation strongly relies on the symmetry of distribution \\(H(x)\\). Without symmetry the coverage will be incorrect.\nIntuitively, we expect that when the assumptions are approximately true then the percentile interval will have approximately correct coverage. Thus so long as there is a transformation \\(\\psi(u)\\) such that \\(\\psi(\\widehat{\\theta})-\\) \\(\\psi(\\theta)\\) is approximately pivotal and symmetric about zero, then the percentile interval should work well.\nThis argument has the following application. Suppose that the parameter of interest is \\(\\theta=\\exp (\\mu)\\) where \\(\\mu=\\mathbb{E}[Y]\\) and suppose \\(Y\\) has a pivotal symmetric distribution about \\(\\mu\\). Then even though \\(\\widehat{\\theta}=\\) \\(\\exp (\\bar{Y})\\) does not have a symmetric distribution, the percentile interval applied to \\(\\widehat{\\theta}\\) will have the correct coverage, because the monotonic transformation \\(\\log (\\widehat{\\theta})\\) has a pivotal symmetric distribution."
  },
  {
    "objectID": "chpt10-resample-method.html#bias-corrected-percentile-interval",
    "href": "chpt10-resample-method.html#bias-corrected-percentile-interval",
    "title": "10  Resampling Methods",
    "section": "10.17 Bias-Corrected Percentile Interval",
    "text": "10.17 Bias-Corrected Percentile Interval\nThe accuracy of the percentile interval depends critically upon the assumption that the sampling distribution is approximately symmetrically distributed. This excludes finite sample bias, for an estimator which is biased cannot be symmetrically distributed. Many contexts in which we want to apply bootstrap methods (rather than asymptotic) are when the parameter of interest is a nonlinear function of the model parameters, and nonlinearity typically induces estimation bias. Consequently it is difficult to expect the percentile method to generally have accurate coverage.\nTo reduce the bias problem Efron (1982) introduced the bias-corrected (BC) percentile interval. The justification is heuristic but there is considerable evidence that the bias-corrected method is an important improvement on the percentile interval. The construction is based on the assumption is that there is a an unknown but strictly increasing transformation \\(\\psi(\\theta)\\) and unknown constant \\(z_{0}\\) such that\n\\[\nZ=\\psi(\\widehat{\\theta})-\\psi(\\theta)+z_{0} \\sim \\mathrm{N}(0,1) .\n\\]\n(The assumption that \\(Z\\) is normal is not critical. It could be replaced by any known symmetric and invertible distribution.) Let \\(\\Phi(x)\\) denote the normal distribution function, \\(\\Phi^{-1}(p)\\) its quantile function, and \\(z_{\\alpha}=\\Phi^{-1}(\\alpha)\\) the normal critical values. Then the BC interval can be constructed from the bootstrap estimators \\(\\widehat{\\theta}_{b}^{*}\\) and bootstrap quantiles \\(q_{\\alpha}^{*}\\) as follows. Set\n\\[\np^{*}=\\frac{1}{B} \\sum_{b=1}^{B} \\mathbb{1}\\left\\{\\widehat{\\theta}_{b}^{*} \\leq \\widehat{\\theta}\\right\\}\n\\]\nand\n\\[\nz_{0}=\\Phi^{-1}\\left(p^{*}\\right) .\n\\]\n\\(p^{*}\\) is a measure of median bias, and \\(z_{0}\\) is \\(p^{*}\\) transformed into normal units. If the bias of \\(\\widehat{\\theta}\\) is zero then \\(p^{*}=0.5\\) and \\(z_{0}=0\\). If \\(\\widehat{\\theta}\\) is upwards biased then \\(p^{*}<0.5\\) and \\(z_{0}<0\\). Conversely if \\(\\widehat{\\theta}\\) is dowward biased then \\(p^{*}>0.5\\) and \\(z_{0}>0\\). Define for any \\(\\alpha\\) an adjusted version\n\\[\nx(\\alpha)=\\Phi\\left(z_{\\alpha}+2 z_{0}\\right) .\n\\]\nIf \\(z_{0}=0\\) then \\(x(\\alpha)=\\alpha\\). If \\(z_{0}>0\\) then \\(x(\\alpha)>\\alpha\\), and conversely when \\(x(\\alpha)<0\\). The BC interval is\n\\[\nC^{\\mathrm{bc}}=\\left[q_{x(\\alpha / 2)}^{*}, q_{x(1-\\alpha / 2)}^{*}\\right] .\n\\]\nEssentially, rather than going from the \\(2.5 %\\) to \\(97.5 %\\) quantile, the BC interval uses adjusted quantiles, with the degree of adjustment depending on the extent of the bias.\nThe construction of the BC interval is not intuitive. We now show that assumption (10.21) implies that the BC interval has exact coverage. (10.21) implies that\n\\[\n\\mathbb{P}\\left[\\psi(\\widehat{\\theta})-\\psi(\\theta)+z_{0} \\leq x\\right]=\\Phi(x) .\n\\]\nSince the distribution is pivotal the result carries over to the bootstrap distribution\n\\[\n\\mathbb{P}^{*}\\left[\\psi\\left(\\widehat{\\theta}^{*}\\right)-\\psi(\\widehat{\\theta})+z_{0} \\leq x\\right]=\\Phi(x) .\n\\]\nEvaluating (10.26) at \\(x=z_{0}\\) we find \\(\\mathbb{P}^{*}\\left[\\psi\\left(\\widehat{\\theta}^{*}\\right)-\\psi(\\widehat{\\theta}) \\leq 0\\right]=\\Phi\\left(z_{0}\\right)\\) which implies \\(\\mathbb{P}^{*}\\left[\\widehat{\\theta}^{*} \\leq \\widehat{\\theta}\\right]=\\Phi\\left(z_{0}\\right)\\). Inverting, we obtain\n\\[\nz_{0}=\\Phi^{-1}\\left(\\mathbb{P}^{*}\\left[\\widehat{\\theta}^{*} \\leq \\widehat{\\theta}\\right]\\right)\n\\]\nwhich is the probability limit of (10.23) as \\(B \\rightarrow \\infty\\). Thus the unknown \\(z_{0}\\) is recoved by (10.23), and we can treat \\(z_{0}\\) as if it were known.\nFrom (10.26) we deduce that\n\\[\n\\begin{aligned}\nx(\\alpha) &=\\Phi\\left(z_{\\alpha}+2 z_{0}\\right) \\\\\n&\\left.=\\mathbb{P}^{*}\\left[\\psi\\left(\\widehat{\\theta}^{*}\\right)-\\psi(\\widehat{\\theta}) \\leq z_{\\alpha}+z_{0}\\right)\\right] \\\\\n&=\\mathbb{P}^{*}\\left[\\widehat{\\theta}^{*} \\leq \\psi^{-1}\\left(\\psi(\\widehat{\\theta})+z_{0}+z_{\\alpha}\\right)\\right] .\n\\end{aligned}\n\\]\nThis equation shows that \\(\\psi^{-1}\\left(\\psi(\\widehat{\\theta})+z_{0}+z_{\\alpha}\\right)\\) equals the \\(x(\\alpha)^{t h}\\) bootstrap quantile. That is, \\(q_{x(\\alpha)}^{*}=\\) \\(\\psi^{-1}\\left(\\psi(\\widehat{\\theta})+z_{0}+z_{\\alpha}\\right)\\). Hence we can write (10.25) as\n\\[\nC^{\\mathrm{bc}}=\\left[\\psi^{-1}\\left(\\psi(\\widehat{\\theta})+z_{0}+z_{\\alpha / 2}\\right), \\psi^{-1}\\left(\\psi(\\widehat{\\theta})+z_{0}+z_{1-\\alpha / 2}\\right)\\right] .\n\\]\nIt has coverage probability\n\\[\n\\begin{aligned}\n\\mathbb{P}\\left[\\theta \\in C^{\\mathrm{bc}}\\right] &=\\mathbb{P}\\left[\\psi^{-1}\\left(\\psi(\\widehat{\\theta})+z_{0}+z_{\\alpha / 2}\\right) \\leq \\theta \\leq \\psi^{-1}\\left(\\psi(\\widehat{\\theta})+z_{0}+z_{1-\\alpha / 2}\\right)\\right] \\\\\n&=\\mathbb{P}\\left[\\psi(\\widehat{\\theta})+z_{0}+z_{\\alpha / 2} \\leq \\psi(\\theta) \\leq \\psi(\\widehat{\\theta})+z_{0}+z_{1-\\alpha / 2}\\right] \\\\\n&=\\mathbb{P}\\left[-z_{\\alpha / 2} \\geq \\psi(\\widehat{\\theta})-\\psi(\\theta)+z_{0} \\geq-z_{1-\\alpha / 2}\\right] \\\\\n&=\\mathbb{P}\\left[z_{1-\\alpha / 2} \\geq Z \\geq z_{\\alpha / 2}\\right] \\\\\n&=\\Phi\\left(z_{1-\\alpha / 2}\\right)-\\Phi\\left(z_{\\alpha / 2}\\right) \\\\\n&=1-\\alpha .\n\\end{aligned}\n\\]\nThe second equality applies the transformation \\(\\psi(\\theta)\\). The fourth equality uses the model (10.21) and the fact \\(z_{\\alpha}=-z_{1-\\alpha}\\). This shows that the BC interval (10.25) has exact coverage under the assumption (10.21).\nFurthermore, under the assumptions of Theorem 10.13, the \\(\\mathrm{BC}\\) interval has asymptotic coverage probability \\(1-\\alpha\\), since the bias correction is asymptotically negligible.\nAn important property of the BC percentile interval is that it is transformation-respecting (like the percentile interval). To see this, observe that \\(p^{*}\\) is invariant to transformations because it is a probability, and thus \\(z_{0}^{*}\\) and \\(x(\\alpha)\\) are invariant. Since the interval is constructed from the \\(x(\\alpha / 2)\\) and \\(x(1-\\alpha / 2)\\) quantiles, the quantile equivariance property shows that the interval is transformation-respecting.\nThe bootstrap BC percentile intervals for the four estimators are reported in Table 13.2. They are generally similar to the percentile intervals, though the intervals for \\(\\sigma^{2}\\) and \\(\\mu\\) are somewhat shifted to the right.\nIn Stata, BC percentile confidence intervals can be obtained by using the command estat bootstrap after an estimation command which calculates standard errors via the bootstrap."
  },
  {
    "objectID": "chpt10-resample-method.html#mathrmbc_a-percentile-interval",
    "href": "chpt10-resample-method.html#mathrmbc_a-percentile-interval",
    "title": "10  Resampling Methods",
    "section": "10.18 \\(\\mathrm{BC}_{a}\\) Percentile Interval",
    "text": "10.18 \\(\\mathrm{BC}_{a}\\) Percentile Interval\nA further improvement on the BC interval was made by Efron (1987) to account for the skewness in the sampling distribution, which can be modeled by specifying that the variance of the estimator depends on the parameter. The resulting bootstrap accelerated bias-corrected percentile interval \\(\\left(\\mathrm{BC}_{a}\\right)\\) has improved performance on the BC interval, but requires a bit more computation and is less intuitive to understand.\nThe construction is a generalization of that for the BC intervals. The assumption is that there is an unknown but strictly increasing transformation \\(\\psi(\\theta)\\) and unknown constants \\(a\\) and \\(z_{0}\\) such that\n\\[\nZ=\\frac{\\psi(\\widehat{\\theta})-\\psi(\\theta)}{1+a \\psi(\\theta)}+z_{0} \\sim \\mathrm{N}(0,1) .\n\\]\n(As before, the assumption that \\(Z\\) is normal could be replaced by any known symmetric and invertible distribution.)\nThe constant \\(z_{0}\\) is estimated by (10.23) just as for the BC interval. There are several possible estimators of \\(a\\). Efron’s suggestion is a scaled jackknife estimator of the skewness of \\(\\widehat{\\theta}\\) :\n\\[\n\\begin{aligned}\n&\\widehat{a}=\\frac{\\sum_{i=1}^{n}\\left(\\bar{\\theta}-\\widehat{\\theta}_{(-i)}\\right)^{3}}{6\\left(\\sum_{i=1}^{n}\\left(\\bar{\\theta}-\\widehat{\\theta}_{(-i)}\\right)^{2}\\right)^{3 / 2}} \\\\\n&\\bar{\\theta}=\\frac{1}{n} \\sum_{i=1}^{n} \\widehat{\\theta}_{(-i)} .\n\\end{aligned}\n\\]\nThe jackknife estimator of \\(\\widehat{a}\\) makes the \\(\\mathrm{BC}_{a}\\) interval more computationally costly than other intervals.\nDefine for any \\(\\alpha\\) the adjusted version\n\\[\nx(\\alpha)=\\Phi\\left(z_{0}+\\frac{z_{\\alpha}+z_{0}}{1-a\\left(z_{\\alpha}+z_{0}\\right)}\\right) .\n\\]\nThe \\(\\mathrm{BC}_{a}\\) percentile interval is\n\\[\nC^{\\mathrm{bca}}=\\left[q_{x(\\alpha / 2)}^{*}, q_{x(1-\\alpha / 2)}^{*}\\right] .\n\\]\nNote that \\(x(\\alpha)\\) simplifies to (10.24) and \\(C^{\\text {bca }}\\) simplies to \\(C^{\\text {bc }}\\) when \\(a=0\\). While \\(C^{\\text {bc }}\\) improves on \\(C^{\\text {pc }}\\) by correcting the median bias, \\(C^{\\text {bca }}\\) makes a further correction for skewness.\nThe \\(\\mathrm{BC}_{a}\\) interval is only well-defined for values of \\(\\alpha\\) such that \\(a\\left(z_{\\alpha}+z_{0}\\right)<1\\). (Or equivalently, if \\(\\alpha<\\Phi\\left(a^{-1}-z_{0}\\right)\\) for \\(a>0\\) and \\(\\alpha>\\Phi\\left(a^{-1}-z_{0}\\right)\\) for \\(a<0\\).)\nThe \\(\\mathrm{BC}_{a}\\) interval, like the \\(\\mathrm{BC}\\) and percentile intervals, is transformation-respecting. Thus if \\(\\left[q_{x(\\alpha / 2)}^{*}, q_{x(1-\\alpha / 2)}^{*}\\right]\\) is the \\(\\mathrm{BC}_{a}\\) interval for \\(\\theta\\), then \\(\\left[m\\left(q_{x(\\alpha / 2)}^{*}\\right), m\\left(q_{x(1-\\alpha / 2)}^{*}\\right)\\right]\\) is the \\(\\mathrm{BC}_{\\alpha}\\) interval for \\(\\phi=m(\\theta)\\) when \\(m(\\theta)\\) is monotone.\nWe now give a justification for the \\(\\mathrm{BC}_{a}\\) interval. The most difficult feature to understand is the estimator \\(\\widehat{a}\\) for \\(a\\). This involves higher-order approximations which are too advanced for our treatment, so we instead refer readers to Chapter \\(4.1 .4\\) of Shao and Tu (1995) and simply assume that \\(a\\) is known.\nWe now show that assumption (10.28) with \\(a\\) known implies that \\(C^{\\text {bca }}\\) has exact coverage. The argument is essentially the same as that given in the previous section. Assumption (10.28) implies that the bootstrap distribution satisfies\n\\[\n\\mathbb{P}^{*}\\left[\\frac{\\psi\\left(\\widehat{\\theta}^{*}\\right)-\\psi(\\widehat{\\theta})}{1+a \\psi(\\widehat{\\theta})}+z_{0} \\leq x\\right]=\\Phi(x) .\n\\]\nEvaluating at \\(x=z_{0}\\) and inverting we obtain (10.27) which is the same as for the BC interval. Thus the estimator (10.23) is consistent as \\(B \\rightarrow \\infty\\) and we can treat \\(z_{0}\\) as if it were known.\nFrom (10.29) we deduce that\n\\[\n\\begin{aligned}\nx(\\alpha) &=\\mathbb{P}^{*}\\left[\\frac{\\psi\\left(\\widehat{\\theta}^{*}\\right)-\\psi(\\widehat{\\theta})}{1+a \\psi(\\widehat{\\theta})} \\leq \\frac{z_{\\alpha}+z_{0}}{1-a\\left(z_{\\alpha}+z_{0}\\right)}\\right] \\\\\n&=\\mathbb{P}^{*}\\left[\\widehat{\\theta}^{*} \\leq \\psi^{-1}\\left(\\frac{\\psi(\\widehat{\\theta})+z_{\\alpha}+z_{0}}{1-a\\left(z_{\\alpha}+z_{0}\\right)}\\right)\\right] .\n\\end{aligned}\n\\]\nThis shows that \\(\\psi^{-1}\\left(\\frac{\\psi(\\widehat{\\theta})+z_{\\alpha}+z_{0}}{1-a\\left(z_{\\alpha}+z_{0}\\right)}\\right)\\) equals the \\(x(\\alpha)^{t h}\\) bootstrap quantile. Hence we can write \\(C^{\\text {bca }}\\) as\n\\[\nC^{\\mathrm{bca}}=\\left[\\psi^{-1}\\left(\\frac{\\psi(\\widehat{\\theta})+z_{\\alpha / 2}+z_{0}}{1-a\\left(z_{\\alpha / 2}+z_{0}\\right)}\\right), \\quad \\psi^{-1}\\left(\\frac{\\psi(\\widehat{\\theta})+z_{1-\\alpha / 2}+z_{0}}{1-a\\left(z_{1-\\alpha / 2}+z_{0}\\right)}\\right)\\right] .\n\\]\nIt has coverage probability\n\\[\n\\begin{aligned}\n\\mathbb{P}\\left[\\theta \\in C^{\\mathrm{bca}}\\right] &=\\mathbb{P}\\left[\\psi^{-1}\\left(\\frac{\\psi(\\widehat{\\theta})+z_{\\alpha / 2}+z_{0}}{1-a\\left(z_{\\alpha / 2}+z_{0}\\right)}\\right) \\leq \\theta \\leq \\psi^{-1}\\left(\\frac{\\psi(\\widehat{\\theta})+z_{1-\\alpha / 2}+z_{0}}{1-a\\left(z_{1-\\alpha / 2}+z_{0}\\right)}\\right)\\right] \\\\\n&=\\mathbb{P}\\left[\\frac{\\psi(\\widehat{\\theta})+z_{\\alpha / 2}+z_{0}}{1-a\\left(z_{\\alpha / 2}+z_{0}\\right)} \\leq \\psi(\\theta) \\leq \\frac{\\psi(\\widehat{\\theta})+z_{1-\\alpha / 2}+z_{0}}{1-a\\left(z_{1-\\alpha / 2}+z_{0}\\right)}\\right] \\\\\n&=\\mathbb{P}\\left[-z_{\\alpha / 2} \\geq \\frac{\\psi(\\widehat{\\theta})-\\psi(\\theta)}{1+a \\psi(\\theta)}+z_{0} \\geq-z_{1-\\alpha / 2}\\right] \\\\\n&=\\mathbb{P}\\left[z_{1-\\alpha / 2} \\geq Z \\geq z_{\\alpha / 2}\\right] \\\\\n&=1-\\alpha .\n\\end{aligned}\n\\]\nThe second equality applies the transformation \\(\\psi(\\theta)\\). The fourth equality uses the model (10.28) and the fact \\(z_{\\alpha}=-z_{1-\\alpha}\\). This shows that the \\(\\mathrm{BC}_{a}\\) interval \\(C^{\\text {bca }}\\) has exact coverage under the assumption (10.28) with \\(a\\) known.\nThe bootstrap \\(\\mathrm{BC}_{a}\\) percentile intervals for the four estimators are reported in Table 13.2. They are generally similar to the BC intervals, though the intervals for \\(\\sigma^{2}\\) and \\(\\mu\\) are slightly shifted to the right.\nIn Stata, \\(\\mathrm{BC}_{a}\\) intervals can be obtained by using the command estat bootstrap, bca or the command estat bootstrap, all after an estimation command which calculates standard errors via the bootstrap using the bca option."
  },
  {
    "objectID": "chpt10-resample-method.html#percentile-t-interval",
    "href": "chpt10-resample-method.html#percentile-t-interval",
    "title": "10  Resampling Methods",
    "section": "10.19 Percentile-t Interval",
    "text": "10.19 Percentile-t Interval\nIn many cases we can obtain improvement in accuracy by bootstrapping a studentized statistic such as a t-ratio. Let \\(\\widehat{\\theta}\\) be an estimator of a scalar parameter \\(\\theta\\) and \\(s(\\widehat{\\theta})\\) a standard error. The sample t-ratio is\n\\[\nT=\\frac{\\widehat{\\theta}-\\theta}{s(\\widehat{\\theta})} .\n\\]\nThe bootstrap t-ratio is\n\\[\nT^{*}=\\frac{\\widehat{\\theta}^{*}-\\widehat{\\theta}}{s\\left(\\widehat{\\theta}^{*}\\right)}\n\\]\nwhere \\(s\\left(\\widehat{\\theta}^{*}\\right)\\) is the standard error calculated on the bootstrap sample. Notice that the bootstrap t-ratio is centered at the parameter estimator \\(\\widehat{\\theta}\\). This is because \\(\\widehat{\\theta}\\) is the “true value” in the bootstrap universe.\nThe percentile-t interval is formed using the distribution of \\(T^{*}\\). This can be calculated via the bootstrap algorithm. On each bootstrap sample the estimator \\(\\widehat{\\theta}^{*}\\) and its standard error \\(s\\left(\\widehat{\\theta}^{*}\\right)\\) are calculated, and the t-ratio \\(T^{*}=\\left(\\widehat{\\theta}^{*}-\\widehat{\\theta}\\right) / s\\left(\\widehat{\\theta}^{*}\\right)\\) calculated and stored. This is repeated \\(B\\) times. The \\(\\alpha^{t h}\\) quantile \\(q_{\\alpha}^{*}\\) is estimated by the \\(\\alpha^{t h}\\) empirical quantile (or any quantile estimator) from the \\(B\\) bootstrap draws of \\(T^{*}\\).\nThe bootstrap percentile-t confidence interval is defined as\n\\[\nC^{\\mathrm{pt}}=\\left[\\widehat{\\theta}-s(\\widehat{\\theta}) q_{1-\\alpha / 2}^{*}, \\widehat{\\theta}-s(\\widehat{\\theta}) q_{\\alpha / 2}^{*}\\right] .\n\\]\nThe form may appear unusual when compared with the percentile interval. The left endpoint is determined by the upper quantile of the distribution of \\(T^{*}\\), and the right endpoint is determined by the lower quantile. As we show below, this construction is important for the interval to have correct coverage when the distribution is not symmetric.\nWhen the estimator is asymptotically normal and the standard error a reliable estimator of the standard deviation of the distribution we would expect the t-ratio \\(T\\) to be roughly approximated by the normal distribution. In this case we would expect \\(q_{0.975}^{*} \\approx-q_{0.025}^{*} \\approx 2\\). Departures from this baseline occur as the distribution becomes skewed or fat-tailed. If the bootstrap quantiles depart substantially from this baseline it is evidence of substantial departure from normality. (It may also indicate a programming error, so in these cases it is wise to triple-check!)\nThe percentile-t has the following advantages. First, when the standard error \\(s(\\widehat{\\theta})\\) is reasonably reliable, the percentile-t bootstrap makes use of the information in the standard error, thereby reducing the role of the bootstrap. This can improve the precision of the method relative to other methods. Second, as we show later, the percentile-t intervals achieve higher-order accuracy than the percentile and BC percentile intervals. Third, the percentile-t intervals correspond to the set of parameter values “not rejected” by one-sided t-tests using bootstrap critical values (bootstrap tests are presented in Section 10.21).\nThe percentile-t interval has the following disadvantages. First, they may be infeasible when standard error formula are unknown. Second, they may be practically infeasible when standard error calculations are computationally costly (since the standard error calculation needs to be performed on each bootstrap sample). Third, the percentile-t may be unreliable if the standard errors \\(s(\\widehat{\\theta})\\) are unreliable and thus add more noise than clarity. Fourth, the percentile-t interval is not translation preserving, unlike the percentile, \\(\\mathrm{BC}\\) percentile, and \\(\\mathrm{BC}_{a}\\) percentile intervals.\nIt is typical to calculate percentile-t intervals with t-ratios constructed with conventional asymptotic standard errors. But this is not the only possible implementation. The percentile-t interval can be constructed with any data-dependent measure of scale. For example, if \\(\\widehat{\\theta}\\) is a two-step estimator for which it is unclear how to construct a correct asymptotic standard error, but we know how to calculate a standard error \\(s(\\widehat{\\theta})\\) appropriate for the second step alone, then \\(s(\\widehat{\\theta})\\) can be used for a percentile-t-type interval as described above. It will not possess the higher-order accuracy properties of the following section, but it will satisfy the conditions for first-order validity.\nFurthermore, percentile-t intervals can be constructed using bootstrap standard errors. That is, the statistics \\(T\\) and \\(T^{*}\\) can be computed using bootstrap standard errors \\(s_{\\widehat{\\theta}}^{\\text {boot }}\\). This is computationally costly as it requires what we call a “nested bootstrap”. Specifically, for each bootstrap replication, a random sample is drawn, the bootstrap estimate \\(\\widehat{\\theta}^{*}\\) computed, and then \\(B\\) additional bootstrap sub-samples drawn from the bootstrap sample to compute the bootstrap standard error for the bootstrap estimate \\(\\widehat{\\theta}^{*}\\). Effectively \\(B^{2}\\) bootstrap samples are drawn and estimated, which increases the computational requirement by an order of magnitude.\nWe now describe the distribution theory for first-order validity of the percentile-t bootstrap.\nFirst, consider the smooth function model, where \\(\\widehat{\\theta}=g(\\widehat{\\mu})\\) and \\(s(\\widehat{\\theta})=\\sqrt{\\frac{1}{n} \\widehat{\\boldsymbol{G}}^{\\prime} \\widehat{\\boldsymbol{V}} \\widehat{\\boldsymbol{G}}}\\) with bootstrap analogs \\(\\widehat{\\theta}^{*}=g\\left(\\widehat{\\mu}^{*}\\right)\\) and \\(s\\left(\\widehat{\\theta}^{*}\\right)=\\sqrt{\\frac{1}{n} \\widehat{\\boldsymbol{G}}^{* \\prime} \\widehat{\\boldsymbol{V}}^{*} \\widehat{\\boldsymbol{G}}^{*}}\\). From Theorems \\(6.10,10.7\\), and \\(10.8\\)\n\\[\nT=\\frac{\\sqrt{n}(\\widehat{\\theta}-\\theta)}{\\sqrt{\\widehat{\\boldsymbol{G}}^{\\prime} \\widehat{\\boldsymbol{V}} \\widehat{\\boldsymbol{G}}}} \\underset{d}{\\longrightarrow}\n\\]\nand\n\\[\nT^{*}=\\frac{\\sqrt{n}\\left(\\widehat{\\theta}^{*}-\\widehat{\\theta}\\right)}{\\sqrt{\\widehat{\\boldsymbol{G}}^{* \\prime} \\widehat{\\boldsymbol{V}}^{*} \\widehat{\\boldsymbol{G}}^{*}}} \\overrightarrow{d^{*}} Z\n\\]\nwhere \\(Z \\sim \\mathrm{N}(0,1)\\). This shows that the sample and bootstrap t-ratios have the same asymptotic distribution.\nThis motivates considering the broader situation where the sample and bootstrap t-ratios have the same asymptotic distribution but not necessarily normal. Thus assume that\n\\[\n\\begin{gathered}\nT \\underset{d}{\\longrightarrow} \\xi \\\\\nT^{*} \\underset{d^{*}}{\\longrightarrow} \\xi\n\\end{gathered}\n\\]\nfor some continuous distribution \\(\\xi\\). (10.31) implies that the quantiles of \\(T^{*}\\) converge in probability to those of \\(\\xi\\), that is \\(q_{\\alpha}^{*} \\longrightarrow \\underset{p}{\\longrightarrow} q_{\\alpha}\\) where \\(q_{\\alpha}\\) is the \\(\\alpha^{t h}\\) quantile of \\(\\xi\\). This and (10.30) imply\n\\[\n\\begin{aligned}\n\\mathbb{P}\\left[\\theta \\in C^{\\mathrm{pt}}\\right] &=\\mathbb{P}\\left[\\widehat{\\theta}-s(\\widehat{\\theta}) q_{1-\\alpha / 2}^{*} \\leq \\theta \\leq \\hat{\\theta}-s(\\widehat{\\theta}) q_{\\alpha / 2}^{*}\\right] \\\\\n&=\\mathbb{P}\\left[q_{\\alpha / 2}^{*} \\leq T \\leq q_{1-\\alpha / 2}^{*}\\right] \\\\\n& \\rightarrow \\mathbb{P}\\left[q_{\\alpha / 2} \\leq \\xi \\leq q_{1-\\alpha / 2}\\right] \\\\\n&=1-\\alpha .\n\\end{aligned}\n\\]\nThus the percentile-t is asymptotically valid. Theorem \\(10.14\\) If (10.30) and (10.31) hold where \\(\\xi\\) is continuously distributed, then \\(\\mathbb{P}\\left[\\theta \\in C^{\\mathrm{pt}}\\right] \\rightarrow 1-\\alpha\\) as \\(n \\rightarrow \\infty\\)\nThe bootstrap percentile-t intervals for the four estimators are reported in Table 13.2. They are similar but somewhat different from the percentile-type intervals, and generally wider. The largest difference arises with the interval for \\(\\sigma^{2}\\) which is noticably wider than the other intervals."
  },
  {
    "objectID": "chpt10-resample-method.html#percentile-t-asymptotic-refinement",
    "href": "chpt10-resample-method.html#percentile-t-asymptotic-refinement",
    "title": "10  Resampling Methods",
    "section": "10.20 Percentile-t Asymptotic Refinement",
    "text": "10.20 Percentile-t Asymptotic Refinement\nThis section uses the theory of Edgeworth and Cornish-Fisher expansions introduced in Chapter 9.8-9.10 of Probability and Statistics for Economists. This theory will not be familiar to most students. If you are interested in the following refinement theory it is advisable to start by reading these sections of Probability and Statistics for Economists.\nThe percentile-t interval can be viewed as the intersection of two one-sided confidence intervals. In our discussion of Edgeworth expansions for the coverage probability of one-sided asymptotic confidence intervals (following Theorem \\(7.15\\) in the context of functions of regression coefficients) we found that one-sided asymptotic confidence intervals have accuracy to order \\(O\\left(n^{-1 / 2}\\right)\\). We now show that the percentile-t interval has improved accuracy.\nTheorem \\(9.13\\) of Probability and Statistics for Economists showed that the Cornish-Fisher expansion for the quantile \\(q_{\\alpha}\\) of a t-ratio \\(T\\) in the smooth function model takes the form\n\\[\nq_{\\alpha}=z_{\\alpha}+n^{-1 / 2} p_{11}\\left(z_{\\alpha}\\right)+O\\left(n^{-1}\\right)\n\\]\nwhere \\(p_{11}(x)\\) is an even polynomial of order 2 with coefficients depending on the moments up to order 8. The bootstrap quantile \\(q_{\\alpha}^{*}\\) has a similar Cornish-Fisher expansion\n\\[\nq_{\\alpha}^{*}=z_{\\alpha}+n^{-1 / 2} p_{11}^{*}\\left(z_{\\alpha}\\right)+O_{p}\\left(n^{-1}\\right)\n\\]\nwhere \\(p_{11}^{*}(x)\\) is the same as \\(p_{11}(x)\\) except that the population moments are replaced by the corresponding sample moments. Sample moments are estimated at the rate \\(n^{-1 / 2}\\). Thus we can replace \\(p_{11}^{*}\\) with \\(p_{11}\\) without affecting the order of this expansion:\n\\[\nq_{\\alpha}^{*}=z_{\\alpha}+n^{-1 / 2} p_{11}\\left(z_{\\alpha}\\right)+O_{p}\\left(n^{-1}\\right)=q_{\\alpha}+O_{p}\\left(n^{-1}\\right) .\n\\]\nThis shows that the bootstrap quantiles \\(q_{\\alpha}^{*}\\) of the studentized t-ratio are within \\(O_{p}\\left(n^{-1}\\right)\\) of the exact quantiles \\(q_{\\alpha}\\).\nBy the Edgeworth expansion Delta method (Theorem \\(9.12\\) of Probability and Statistics for Economists), \\(T\\) and \\(T+\\left(q_{\\alpha}-q_{\\alpha}^{*}\\right)=T+O_{p}\\left(n^{-1}\\right)\\) have the same Edgeworth expansion to order \\(O\\left(n^{-1}\\right)\\). Thus\n\\[\n\\begin{aligned}\n\\mathbb{P}\\left[T \\leq q_{\\alpha}^{*}\\right] &=\\mathbb{P}\\left[T+\\left(q_{\\alpha}-q_{\\alpha}^{*}\\right) \\leq q_{\\alpha}\\right] \\\\\n&=\\mathbb{P}\\left[T \\leq q_{\\alpha}\\right]+O\\left(n^{-1}\\right) \\\\\n&=\\alpha+O\\left(n^{-1}\\right) .\n\\end{aligned}\n\\]\nThus the coverage of the percentile-t interval is\n\\[\n\\begin{aligned}\n\\mathbb{P}\\left[\\theta \\in C^{\\mathrm{pt}}\\right] &=\\mathbb{P}\\left[q_{\\alpha / 2}^{*} \\leq T \\leq q_{1-\\alpha / 2}^{*}\\right] \\\\\n&=\\mathbb{P}\\left[q_{\\alpha / 2} \\leq T \\leq q_{1-\\alpha / 2}\\right]+O\\left(n^{-1}\\right) \\\\\n&=1-\\alpha+O\\left(n^{-1}\\right) .\n\\end{aligned}\n\\]\nThis is an improved rate of convergence relative to the one-sided asymptotic confidence interval. Theorem \\(10.15\\) Under the assumptions of Theorem \\(9.11\\) of Probability and Statistics for Economists, \\(\\mathbb{P}\\left[\\theta \\in C^{\\mathrm{pt}}\\right]=1-\\alpha+O\\left(n^{-1}\\right)\\).\nThe following definition of the accuracy of a confidence interval is useful.\nDefinition 10.3 A confidence set \\(C\\) for \\(\\theta\\) is \\(k^{t h}\\)-order accurate if\n\\[\n\\mathbb{P}[\\theta \\in C]=1-\\alpha+O\\left(n^{-k / 2}\\right) .\n\\]\nExamining our results we find that one-sided asymptotic confidence intervals are first-order accurate but percentile-t intervals are second-order accurate. When a bootstrap confidence interval (or test) achieves high-order accuracy than the analogous asymptotic interval (or test), we say that the bootstrap method achieves an asymptotic refinement. Here, we have shown that the percentile-t interval achieves an asymptotic refinement.\nIn order to achieve this asymptotic refinement it is important that the t-ratio \\(T\\) (and its bootstrap counter-part \\(T^{*}\\) ) are constructed with asymptotically valid standard errors. This is because the first term in the Edgeworth expansion is the standard normal distribution and this requires that the t-ratio is asymptotically normal. This also has the practical finite-sample implication that the accuracy of the percentile-t interval in practice depends on the accuracy of the standard errors used to construct the t-ratio.\nWe do not go through the details, but normal-approximation bootstrap intervals, percentile bootstrap intervals, and bias-corrected percentile bootstrap intervals are all first-order accurate and do not achieve an asymptotic refinement.\nThe \\(\\mathrm{BC}_{a}\\) interval, however, can be shown to be asymptotically equivalent to the percentile-t interval, and thus achieves an asymptotic refinement. We do not make this demonstration here as it is advanced. See Section 3.10.4 of Hall (1992).\n\n\n\n\n\n\nPeter Hall\n\n\n\n\nPeter Gavin Hall (1951-2016) of Australia was one of the most influential and\n\n\nprolific theoretical statisticians in history. He made wide-ranging contributions.\n\n\nSome of the most relevant for econometrics are theoretical investigations of\n\n\nbootstrap methods and nonparametric kernel methods."
  },
  {
    "objectID": "chpt10-resample-method.html#bootstrap-hypothesis-tests",
    "href": "chpt10-resample-method.html#bootstrap-hypothesis-tests",
    "title": "10  Resampling Methods",
    "section": "10.21 Bootstrap Hypothesis Tests",
    "text": "10.21 Bootstrap Hypothesis Tests\nTo test the hypothesis \\(\\mathbb{M}_{0}: \\theta=\\theta_{0}\\) against \\(\\mathbb{M}_{1}: \\theta \\neq \\theta_{0}\\) the most common approach is a t-test. We reject \\(\\mathbb{H}_{0}\\) in favor of \\(\\mathbb{H}_{1}\\) for large absolute values of the t-statistic \\(T=\\left(\\widehat{\\theta}-\\theta_{0}\\right) / s(\\hat{\\theta})\\) where \\(\\widehat{\\theta}\\) is an estimator of \\(\\theta\\) and \\(s(\\widehat{\\theta})\\) is a standard error for \\(\\widehat{\\theta}\\). For a bootstrap test we use the bootstrap algorithm to calculate the critical value. The bootstrap algorithm samples with replacement from the dataset. Given a bootstrap sample the bootstrap estimator \\(\\widehat{\\theta}^{*}\\) and standard error \\(s\\left(\\widehat{\\theta}^{*}\\right)\\) are calculated. Given these values the bootstrap \\(\\mathrm{t}\\)-statistic is \\(T^{*}=\\left(\\widehat{\\theta}^{*}-\\widehat{\\theta}\\right) / s\\left(\\widehat{\\theta}^{*}\\right)\\). There are two important features about the bootstrap t-statistic. First, \\(T^{*}\\) is centered at the sample estimate \\(\\widehat{\\theta}\\), not at the hypothesized value \\(\\theta_{0}\\). This is done because \\(\\widehat{\\theta}\\) is the true value in the bootstrap universe, and the distribution of the t-statistic must be centered at the true value within the bootstrap sampling framework. Second, \\(T^{*}\\) is calculated using the bootstrap standard error \\(s\\left(\\widehat{\\theta}^{*}\\right)\\). This allows the bootstrap to incorporate the randomness in standard error estimation.\nThe failure to properly center the bootstrap statistic at \\(\\widehat{\\theta}\\) is a common error in applications. Often this is because the hypothesis to be tested is \\(\\mathbb{H}_{0}: \\theta=0\\), so the test statistic is \\(T=\\widehat{\\theta} / s(\\widehat{\\theta})\\). This intuitively suggests the bootstrap statistic \\(T^{*}=\\widehat{\\theta}^{*} / s\\left(\\widehat{\\theta}^{*}\\right)\\), but this is wrong. The correct bootstrap statistic is \\(T^{*}=\\) \\(\\left(\\widehat{\\theta}^{*}-\\widehat{\\theta}\\right) / s\\left(\\widehat{\\theta}^{*}\\right)\\)\nThe bootstrap algorithm creates \\(B\\) draws \\(T^{*}(b)=\\left(\\widehat{\\theta}^{*}(b)-\\widehat{\\theta}\\right) / s\\left(\\widehat{\\theta}^{*}(b)\\right), b=1, \\ldots, B\\). The bootstrap \\(100 \\alpha %\\) critical value is \\(q_{1-\\alpha}^{*}\\), where \\(q_{\\alpha}^{*}\\) is the \\(\\alpha^{\\text {th }}\\) quantile of the absolute values of the bootstrap t-ratios \\(\\left|T^{*}(b)\\right|\\). For a \\(100 \\alpha %\\) test we reject \\(\\mathbb{M}_{0}: \\theta=\\theta_{0}\\) in favor of \\(\\mathbb{H}_{1}: \\theta \\neq \\theta_{0}\\) if \\(|T|>q_{1-\\alpha}^{*}\\) and fail to reject if \\(|T| \\leq q_{1-\\alpha}^{*}\\).\nIt is generally better to report p-values rather than critical values. Recall that a p-value is \\(p=1-\\) \\(G_{n}(|T|)\\) where \\(G_{n}(u)\\) is the null distribution of the statistic \\(|T|\\). The bootstrap p-value is defined as \\(p^{*}=1-G_{n}^{*}(|T|)\\), where \\(G_{n}^{*}(u)\\) is the bootstrap distribution of \\(\\left|T^{*}\\right|\\). This is estimated from the bootstrap algorithm as\n\\[\np^{*}=\\frac{1}{B} \\sum_{b=1}^{B} \\mathbb{1}\\left\\{\\left|T^{*}(b)\\right|>|T|\\right\\},\n\\]\nthe percentage of bootstrap t-statistics that are larger than the observed t-statistic. Intuitively, we want to know how “unusual” is the observed statistic \\(T\\) when the null hypothesis is true. The bootstrap algorithm generates a large number of independent draws from the distribution \\(T^{*}\\) (which is an approximation to the unknown distribution of \\(T\\) ). If the percentage of the \\(\\left|T^{*}\\right|\\) that exceed \\(|T|\\) is very small (say \\(1 %\\) ) this tells us that \\(|T|\\) is an unusually large value. However, if the percentage is larger, say \\(15 %\\), then we cannot interpret \\(|T|\\) as unusually large.\nIf desired, the bootstrap test can be implemented as a one-sided test. In this case the statistic is the signed version of the t-ratio, and bootstrap critical values are calculated from the upper tail of the distribution for the alternative \\(\\mathbb{H}_{1}: \\theta>\\theta_{0}\\), and from the lower tail for the alternative \\(\\mathbb{H}_{1}: \\theta<\\theta_{0}\\). There is a connection between the one-sided tests and the percentile-t confidence interval. The latter is the set of parameter values \\(\\theta\\) which are not rejected by either one-sided \\(100 \\alpha / 2 %\\) bootstrap t-test.\nBootstrap tests can also be conducted with other statistics. When standard errors are not available or are not reliable we can use the non-studentized statistic \\(T=\\widehat{\\theta}-\\theta_{0}\\). The bootstrap version is \\(T^{*}=\\widehat{\\theta}^{*}-\\widehat{\\theta}\\). Let \\(q_{\\alpha}^{*}\\) be the \\(\\alpha^{\\text {th }}\\) quantile of the bootstrap statistics \\(\\left|\\widehat{\\theta}^{*}(b)-\\widehat{\\theta}\\right|\\). A bootstrap \\(100 \\alpha %\\) test rejects \\(\\mathbb{H}_{0}: \\theta=\\theta_{0}\\) if \\(\\left|\\widehat{\\theta}-\\theta_{0}\\right|>q_{1-\\alpha}^{*}\\). The bootstrap p-value is\n\\[\np^{*}=\\frac{1}{B} \\sum_{b=1}^{B} \\mathbb{1}\\left\\{\\left|\\widehat{\\theta}^{*}(b)-\\widehat{\\theta}\\right|>\\left|\\widehat{\\theta}-\\theta_{0}\\right|\\right\\} .\n\\]\nTheorem \\(10.16\\) If (10.30) and (10.31) hold where \\(\\xi\\) is continuously distributed, then the bootstrap critical value satisfies \\(q_{1-\\alpha}^{*} \\underset{p}{\\longrightarrow} q_{1-\\alpha}\\) where \\(q_{1-\\alpha}\\) is the \\(1-\\alpha^{t h}\\) quantile of \\(|\\xi|\\). The bootstrap test “Reject \\(\\mathbb{M}_{0}\\) in favor of \\(\\mathbb{M}_{1}\\) if \\(|T|>q_{1-\\alpha}^{*}\\)” has asymptotic size \\(\\alpha: \\mathbb{P}\\left[|T|>q_{1-\\alpha}^{*} \\mid \\mathbb{H}_{0}\\right] \\longrightarrow \\alpha\\) as \\(n \\rightarrow \\infty\\). In the smooth function model the t-test (with correct standard errors) has the following performance.\nTheorem \\(10.17\\) Under the assumptions of Theorem \\(9.11\\) of Probability and Statistics for Economists,\n\\[\nq_{1-\\alpha}^{*}=\\bar{z}_{1-\\alpha}+o_{p}\\left(n^{-1}\\right)\n\\]\nwhere \\(\\bar{z}_{\\alpha}=\\Phi^{-1}((1+\\alpha) / 2)\\) is the \\(\\alpha^{t h}\\) quantile of \\(|Z|\\). The asymptotic test “Reject \\(\\mathbb{M}_{0}\\) in favor of \\(\\mathbb{M}_{1}\\) if \\(|T|>\\bar{z}_{1-\\alpha}\\)” has accuracy\n\\[\n\\mathbb{P}\\left[|T|>\\bar{z}_{1-\\alpha} \\mid \\mathbb{H}_{0}\\right]=1-\\alpha+O\\left(n^{-1}\\right)\n\\]\nand the bootstrap test “Reject \\(\\mathbb{M}_{0}\\) in favor of \\(\\mathbb{M}_{1}\\) if \\(|T|>q_{1-\\alpha}^{*}\\)” has accuracy\n\\[\n\\mathbb{P}\\left[|T|>q_{1-\\alpha}^{*} \\mid \\mathbb{M}_{0}\\right]=1-\\alpha+o\\left(n^{-1}\\right) .\n\\]\nThis shows that the bootstrap test achieves a refinement relative to the asymptotic test.\nThe reasoning is as follows. We have shown that the Edgeworth expansion for the absolute t-ratio takes the form\n\\[\n\\mathbb{P}[|T| \\leq x]=2 \\Phi(x)-1+n^{-1} 2 p_{2}(x)+o\\left(n^{-1}\\right) .\n\\]\nThis means the asymptotic test has accuracy of order \\(O\\left(n^{-1}\\right)\\).\nGiven the Edgeworth expansion, the Cornish-Fisher expansion for the \\(\\alpha^{t h}\\) quantile \\(q_{\\alpha}\\) of the distribution of \\(|T|\\) takes the form\n\\[\nq_{\\alpha}=\\bar{z}_{\\alpha}+n^{-1} p_{21}\\left(\\bar{z}_{\\alpha}\\right)+o\\left(n^{-1}\\right) .\n\\]\nThe bootstrap quantile \\(q_{\\alpha}^{*}\\) has the Cornish-Fisher expansion\n\\[\n\\begin{aligned}\nq_{\\alpha}^{*} &=\\bar{z}_{\\alpha}+n^{-1} p_{21}^{*}\\left(\\bar{z}_{\\alpha}\\right)+o\\left(n^{-1}\\right) \\\\\n&=\\bar{z}_{\\alpha}+n^{-1} p_{21}\\left(\\bar{z}_{\\alpha}\\right)+o_{p}\\left(n^{-1}\\right) \\\\\n&=q_{\\alpha}+o_{p}\\left(n^{-1}\\right)\n\\end{aligned}\n\\]\nwhere \\(p_{21}^{*}(x)\\) is the same as \\(p_{21}(x)\\) except that the population moments are replaced by the corresponding sample moments. The bootstrap test has rejection probability, using the Edgeworth expansion Delta method (Theorem \\(11.12\\) of of Probability and Statistics for Economists)\n\\[\n\\begin{aligned}\n\\mathbb{P}\\left[|T|>q_{1-\\alpha}^{*} \\mid \\mathbb{B}_{0}\\right] &=\\mathbb{P}\\left[|T|+\\left(q_{1-\\alpha}-q_{1-\\alpha}^{*}\\right)>q_{1-\\alpha}\\right] \\\\\n&=\\mathbb{P}\\left[|T|>q_{1-\\alpha}\\right]+o\\left(n^{-1}\\right) \\\\\n&=1-\\alpha+o\\left(n^{-1}\\right)\n\\end{aligned}\n\\]\nas claimed."
  },
  {
    "objectID": "chpt10-resample-method.html#wald-type-bootstrap-tests",
    "href": "chpt10-resample-method.html#wald-type-bootstrap-tests",
    "title": "10  Resampling Methods",
    "section": "10.22 Wald-Type Bootstrap Tests",
    "text": "10.22 Wald-Type Bootstrap Tests\nIf \\(\\theta\\) is a vector then to test \\(\\mathbb{M}_{0}: \\theta=\\theta_{0}\\) against \\(\\mathbb{M}_{1}: \\theta \\neq \\theta_{0}\\) at size \\(\\alpha\\), a common test is based on the Wald statistic \\(W=\\left(\\widehat{\\theta}-\\theta_{0}\\right)^{\\prime} \\widehat{\\boldsymbol{V}}_{\\widehat{\\theta}}^{-1}\\left(\\widehat{\\theta}-\\theta_{0}\\right)\\) where \\(\\widehat{\\theta}\\) is an estimator of \\(\\theta\\) and \\(\\widehat{\\boldsymbol{V}}_{\\widehat{\\theta}}\\) is a covariance matrix estimator. For a bootstrap test we use the bootstrap algorithm to calculate the critical value. The bootstrap algorithm samples with replacement from the dataset. Given a bootstrap sample the bootstrap estimator \\(\\widehat{\\theta}^{*}\\) and covariance matrix estimator \\(\\widehat{\\boldsymbol{V}}_{\\widehat{\\theta}}^{*}\\) are calculated. Given these values the bootstrap Wald statistic is\n\\[\nW^{*}=\\left(\\widehat{\\theta}^{*}-\\widehat{\\theta}\\right)^{\\prime} \\widehat{\\boldsymbol{V}}_{\\widehat{\\theta}}^{*-1}\\left(\\widehat{\\theta}^{*}-\\widehat{\\theta}\\right) .\n\\]\nAs for the t-test it is essential that the bootstrap Wald statistic \\(W^{*}\\) is centered at the sample estimator \\(\\widehat{\\theta}\\) instead of the hypothesized value \\(\\theta_{0}\\). This is because \\(\\widehat{\\theta}\\) is the true value in the bootstrap universe.\nBased on \\(B\\) bootstrap replications we calculate the \\(\\alpha^{t h}\\) quantile \\(q_{\\alpha}^{*}\\) of the distribution of the bootstrap Wald statistics \\(W^{*}\\). The bootstrap test rejects \\(\\mathbb{M}_{0}\\) in favor of \\(\\mathbb{H}_{1}\\) if \\(W>q_{1-\\alpha}^{*}\\). More commonly, we calculate a bootstrap p-value. This is\n\\[\np^{*}=\\frac{1}{B} \\sum_{b=1}^{B} \\mathbb{1}\\left\\{W^{*}(b)>W\\right\\} .\n\\]\nThe asymptotic performance of the Wald test mimics that of the t-test. In general, the bootstrap Wald test is first-order correct (achieves the correct size asymptotically) and under conditions for which an Edgeworth expansion exists, has accuracy\n\\[\n\\mathbb{P}\\left[W>q_{1-\\alpha}^{*} \\mid \\mathbb{H}_{0}\\right]=1-\\alpha+o\\left(n^{-1}\\right)\n\\]\nand thus achieves a refinement relative to the asymptotic Wald test.\nIf a reliable covariance matrix estimator \\(\\widehat{\\boldsymbol{V}}_{\\widehat{\\theta}}\\) is not available a Wald-type test can be implemented with any positive-definite weight matrix instead of \\(\\widehat{\\boldsymbol{V}}_{\\widehat{\\theta}}\\). This includes simple choices such as the identity matrix. The bootstrap algorithm can be used to calculate critical values and \\(\\mathrm{p}\\)-values for the test. So long as the estimator \\(\\hat{\\theta}\\) has an asymptotic distribution this bootstrap test will be asymptotically firstorder valid. The test will not achieve an asymptotic refinement but provides a simple method to test hypotheses when covariance matrix estimates are not available."
  },
  {
    "objectID": "chpt10-resample-method.html#criterion-based-bootstrap-tests",
    "href": "chpt10-resample-method.html#criterion-based-bootstrap-tests",
    "title": "10  Resampling Methods",
    "section": "10.23 Criterion-Based Bootstrap Tests",
    "text": "10.23 Criterion-Based Bootstrap Tests\nA criterion-based estimator takes the form\n\\[\n\\widehat{\\beta}=\\underset{\\beta}{\\operatorname{argmin}} J(\\beta)\n\\]\nfor some criterion function \\(J(\\beta)\\). This includes least squares, maximum likelihood, and minimum distance. Given a hypothesis \\(\\mathbb{M}_{0}: \\theta=\\theta_{0}\\) where \\(\\theta=r(\\beta)\\), the restricted estimator which satisfies \\(\\mathbb{H}_{0}\\) is\n\\[\n\\widetilde{\\beta}=\\underset{r(\\beta)=\\theta_{0}}{\\operatorname{argmin}} J(\\beta) .\n\\]\nA criterion-based statistic to test \\(\\mathbb{H}_{0}\\) is\n\\[\nJ=\\min _{r(\\beta)=\\theta_{0}} J(\\beta)-\\min _{\\beta} J(\\beta)=J(\\widetilde{\\beta})-J(\\widehat{\\beta}) .\n\\]\nA criterion-based test rejects \\(\\mathbb{H}_{0}\\) for large values of \\(J\\). A bootstrap test uses the bootstrap algorithm to calculate the critical value.\nIn this context we need to be a bit thoughtful about how to construct bootstrap versions of \\(J\\). It might seem natural to construct the exact same statistic on the bootstrap samples as on the original sample, but this is incorrect. It makes the same error as calculating a t-ratio or Wald statistic centered at the hypothesized value. In the bootstrap universe, the true value of \\(\\theta\\) is not \\(\\theta_{0}\\), rather it is \\(\\widehat{\\theta}=r(\\widehat{\\beta})\\). Thus when using the nonparametric bootstrap, we want to impose the constraint \\(r(\\beta)=r(\\widehat{\\beta})=\\widehat{\\theta}\\) to obtain the bootstrap version of \\(J\\).\nThus, the correct way to calculate a bootstrap version of \\(J\\) is as follows. Generate a bootstrap sample by random sampling from the dataset. Let \\(J^{*}(\\beta)\\) be the the bootstrap version of the criterion. On a bootstrap sample calculate the unrestricted estimator \\(\\widehat{\\beta}^{*}=\\underset{\\beta}{\\operatorname{argmin}} J^{*}(\\beta)\\) and the restricted version \\(\\widetilde{\\beta}^{*}=\\) \\(\\operatorname{argmin} J^{*}(\\beta)\\) where \\(\\widehat{\\theta}=r(\\widehat{\\beta})\\). The bootstrap statistic is \\(r(\\beta)=\\hat{\\theta}\\)\n\\[\nJ^{*}=\\min _{r(\\beta)=\\widehat{\\theta}} J^{*}(\\beta)-\\min _{\\beta} J^{*}(\\beta)=J^{*}\\left(\\widetilde{\\beta}^{*}\\right)-J^{*}\\left(\\widehat{\\beta}^{*}\\right) .\n\\]\nCalculate \\(J^{*}\\) on each bootstrap sample. Take the \\(1-\\alpha^{\\text {th }}\\) quantile \\(q_{1-\\alpha}^{*}\\). The bootstrap test rejects \\(\\mathbb{H}_{0}\\) in favor of \\(\\mathbb{H}_{1}\\) if \\(J>q_{1-\\alpha}^{*}\\). The bootstrap p-value is\n\\[\np^{*}=\\frac{1}{B} \\sum_{b=1}^{B} \\mathbb{1}\\left\\{J^{*}(b)>J\\right\\} .\n\\]\nSpecial cases of criterion-based tests are minimum distance tests, \\(F\\) tests, and likelihood ratio tests. Take the F test for a linear hypothesis \\(\\boldsymbol{R}^{\\prime} \\beta=\\theta_{0}\\). The \\(F\\) statistic is\n\\[\n\\mathrm{F}=\\frac{\\left(\\widetilde{\\sigma}^{2}-\\widehat{\\sigma}^{2}\\right) / q}{\\widehat{\\sigma}^{2} /(n-k)}\n\\]\nwhere \\(\\widehat{\\sigma}^{2}\\) is the unrestricted estimator of the error variance, \\(\\widetilde{\\sigma}^{2}\\) is the restricted estimator, \\(q\\) is the number of restrictions and \\(k\\) is the number of estimated coefficients. The bootstrap version of the \\(F\\) statistic is\n\\[\n\\mathrm{F}^{*}=\\frac{\\left(\\widetilde{\\sigma}^{* 2}-\\widehat{\\sigma}^{* 2}\\right) / q}{\\widehat{\\sigma}^{* 2} /(n-k)}\n\\]\nwhere \\(\\widehat{\\sigma}^{* 2}\\) is the unrestricted estimator on the bootstrap sample, and \\(\\widetilde{\\sigma}^{* 2}\\) is the restricted estimator which imposes the restriction \\(\\widehat{\\theta}=\\boldsymbol{R}^{\\prime} \\widehat{\\beta}\\).\nTake the likelihood ratio (LR) test for the hypothesis \\(r(\\beta)=\\theta_{0}\\). The LR test statistic is\n\\[\n\\mathrm{LR}=2\\left(\\ell_{n}(\\widehat{\\beta})-\\ell_{n}(\\widetilde{\\beta})\\right)\n\\]\nwhere \\(\\widehat{\\beta}\\) is the unrestricted MLE and \\(\\widetilde{\\beta}\\) is the restricted MLE (imposing \\(r(\\beta)=\\theta_{0}\\) ). The bootstrap version is\n\\[\n\\operatorname{LR}^{*}=2\\left(\\ell_{n}^{*}\\left(\\widehat{\\beta}^{*}\\right)-\\ell_{n}^{*}\\left(\\widetilde{\\beta}^{*}\\right)\\right)\n\\]\nwhere \\(\\ell_{n}^{*}(\\beta)\\) is the log-likelihood function calculated on the bootstrap sample, \\(\\widehat{\\beta}^{*}\\) is the unrestricted maximizer, and \\(\\widetilde{\\beta}^{*}\\) is the restricted maximizer imposing the restriction \\(r(\\beta)=r(\\widehat{\\beta})\\)."
  },
  {
    "objectID": "chpt10-resample-method.html#parametric-bootstrap",
    "href": "chpt10-resample-method.html#parametric-bootstrap",
    "title": "10  Resampling Methods",
    "section": "10.24 Parametric Bootstrap",
    "text": "10.24 Parametric Bootstrap\nThroughout this chapter we have described the most popular form of the bootstrap known as the nonparametric bootstrap. However there are other forms of the bootstrap algorithm including the parametric bootstrap. This is appropriate when there is a full parametric model for the distribution as in likelihood estimation.\nFirst, consider the context where the model specifies the full distribution of the random vector \\(Y\\), e.g. \\(Y \\sim F(y \\mid \\beta)\\) where the distribution function \\(F\\) is known but the parameter \\(\\beta\\) is unknown. Let \\(\\widehat{\\beta}\\) be an estimator of \\(\\beta\\) such as the maximum likelihood estimator. The parametric bootstrap algorithm generates bootstrap observations \\(Y_{i}^{*}\\) by drawing random vectors from the distribution function \\(F(y \\mid \\widehat{\\beta})\\). When this is done, the true value of \\(\\beta\\) in the bootstrap universe is \\(\\widehat{\\beta}\\). Everything which has been discussed in the chapter can be applied using this bootstrap algorithm.\nSecond, consider the context where the model specifies the conditional distribution of the random vector \\(Y\\) given the random vector \\(X\\), e.g. \\(Y \\mid X \\sim F(y \\mid X, \\beta)\\). An example is the normal linear regression model, where \\(Y \\mid X \\sim \\mathrm{N}\\left(X^{\\prime} \\beta, \\sigma^{2}\\right)\\). In this context we can hold the regressors \\(X_{i}\\) fixed and then draw the bootstrap observations \\(Y_{i}^{*}\\) from the conditional distribution \\(F\\left(y \\mid X_{i}, \\widehat{\\beta}\\right)\\). In the example of the normal regression model this is equivalent to drawing a normal error \\(e_{i}^{*} \\sim \\mathrm{N}\\left(0, \\widehat{\\sigma}^{2}\\right)\\) and then setting \\(Y_{i}^{*}=X_{i}^{\\prime} \\widehat{\\beta}+\\) \\(e_{i}^{*}\\). Again, in this algorithm the true value of \\(\\beta\\) is \\(\\widehat{\\beta}\\) and everything which is discussed in this chapter can be applied as before.\nThird, consider tests of the hypothesis \\(r(\\beta)=\\theta_{0}\\). In this context we can also construct a restricted estimator \\(\\widetilde{\\beta}\\) (for example the restricted MLE) which satisfies the hypothesis \\(r(\\widetilde{\\beta})=\\theta_{0}\\). Then we can generate bootstrap samples by simulating from the distribution \\(Y_{i}^{*} \\sim F(y \\mid \\widetilde{\\beta})\\), or in the conditional context from \\(Y_{i}^{*} \\sim F\\left(y \\mid X_{i}, \\widetilde{\\beta}\\right)\\). When this is done the true value of \\(\\beta\\) in the bootstrap is \\(\\widetilde{\\beta}\\) which satisfies the hypothesis. So in this context the correct values of the bootstrap statistics are\n\\[\n\\begin{gathered}\nT^{*}=\\frac{\\widehat{\\theta}^{*}-\\theta_{0}}{s\\left(\\widehat{\\theta}^{*}\\right)} \\\\\nW^{*}=\\left(\\widehat{\\theta}^{*}-\\theta_{0}\\right)^{\\prime} \\widehat{\\boldsymbol{V}}_{\\widehat{\\theta}}^{*-1}\\left(\\widehat{\\theta}^{*}-\\theta_{0}\\right) \\\\\nJ^{*}=\\min _{r(\\beta)=\\theta_{0}} J^{*}(\\beta)-\\min _{\\beta} J^{*}(\\beta) \\\\\n\\mathrm{LR}^{*}=2\\left(\\max _{\\beta} \\ell_{n}^{*}(\\beta)-\\max _{r(\\beta)=\\theta_{0}} \\ell_{n}^{*}(\\beta)\\right)\n\\end{gathered}\n\\]\nand\n\\[\n\\mathrm{F}^{*}=\\frac{\\left(\\widetilde{\\sigma}^{* 2}-\\widehat{\\sigma}^{* 2}\\right) / q}{\\widehat{\\sigma}^{* 2} /(n-k)}\n\\]\nwhere \\(\\widehat{\\sigma}^{* 2}\\) is the unrestricted estimator on the bootstrap sample and \\(\\widetilde{\\sigma}^{* 2}\\) is the restricted estimator which imposes the restriction \\(\\boldsymbol{R}^{\\prime} \\beta=\\theta_{0}\\).\nThe primary advantage of the parametric bootstrap (relative to the nonparametric bootstrap) is that it will be more accurate when the parametric model is correct. This may be quite important in small samples. The primary disadvantage of the parametric bootstrap is that it can be inaccurate when the parametric model is incorrect."
  },
  {
    "objectID": "chpt10-resample-method.html#how-many-bootstrap-replications",
    "href": "chpt10-resample-method.html#how-many-bootstrap-replications",
    "title": "10  Resampling Methods",
    "section": "10.25 How Many Bootstrap Replications?",
    "text": "10.25 How Many Bootstrap Replications?\nHow many bootstrap replications should be used? There is no universally correct answer as there is a trade-off between accuracy and computation cost. Computation cost is essentially linear in \\(B\\). Accuracy (either standard errors or \\(p\\)-values) is proportional to \\(B^{-1 / 2}\\). Improved accuracy can be obtained but only at a higher computational cost.\nIn most empirical research, most calculations are quick and investigatory, not requiring full accuracy. But final results (those going into the final version of the paper) should be accurate. Thus it seems reasonable to use asymptotic and/or bootstrap methods with a modest number of replications for daily calculations, but use a much larger \\(B\\) for the final version.\nIn particular, for final calculations, \\(B=10,000\\) is desired, with \\(B=1000\\) a minimal choice. In contrast, for daily quick calculations values as low as \\(B=100\\) may be sufficient for rough estimates. A useful way to think about the accuracy of bootstrap methods stems from the calculation of pvalues. The bootstrap p-value \\(p^{*}\\) is an average of \\(B\\) Bernoulli draws. The variance of the simulation estimator of \\(p^{*}\\) is \\(p^{*}\\left(1-p^{*}\\right) / B\\), which is bounded above by \\(1 / 4 B\\). To calculate the \\(\\mathrm{p}\\)-value within, say, \\(0.01\\) of the true value with \\(95 %\\) probability requires a standard error below \\(0.005\\). This is ensured if \\(B \\geq 10,000\\).\nStata by default sets \\(B=50\\). This is useful for verification that a program runs but is a poor choice for empirical reporting. Make sure that you set \\(B\\) to the value you want."
  },
  {
    "objectID": "chpt10-resample-method.html#setting-the-bootstrap-seed",
    "href": "chpt10-resample-method.html#setting-the-bootstrap-seed",
    "title": "10  Resampling Methods",
    "section": "10.26 Setting the Bootstrap Seed",
    "text": "10.26 Setting the Bootstrap Seed\nComputers do not generate true random numbers but rather pseudo-random numbers generated by a deterministic algorithm. The algorithms generate sequences which are indistinguishable from random sequences so this is not a worry for bootstrap applications.\nThe methods, however, necessarily require a starting value known as a “seed”. Some packages (including Stata and MATLAB) implement this with a default seed which is reset each time the statistical package is started. This means if you start the package fresh, run a bootstrap program (e.g. a “do” file in Stata), exit the package, restart the package and then rerun the bootstrap program, you should obtain exactly the same results. If you instead run the bootstrap program (e.g. “do” file) twice sequentially without restarting the package, the seed is not reset so a different set of pseudo-random numbers will be generated and the results from the two runs will be different.\nThe R package has a different implementation. When \\(\\mathrm{R}\\) is loaded the random number seed is generated based on the computer’s clock (which results in an essentially random starting seed). Therefore if you run a bootstrap program in R, exit, restart, and rerun, you will obtain a different set of random draws and therefore a different bootstrap result.\nPackages allow users to set their own seed. (In Stata, the command is set seed #. In MATLAB the command is \\(r n g(\\#)\\). In \\(R\\) the command is set. seed (#).) If the seed is set to a specific number at the start of a file then the exact same pseudo-random numbers will be generated each time the program is run. If this is the case, the results of a bootstrap calculation (standard error or test) will be identical across computer runs.\nThe fact that the bootstrap results can be fixed by setting the seed in the replication file has motivated many researchers to follow this choice. They set the seed at the start of the replication file so that repeated executions result in the same numerical findings.\nFixing seeds, however, should be done cautiously. It may be a wise choice for a final calculation (when a paper is finished) but is an unwise choice for daily calculations. If you use a small number of replications in your preliminary work, say \\(B=100\\), the bootstrap calculations will be inaccurate. But as you run your results again and again (as is typical in empirical projects) you will obtain the same numerical standard errors and test results, giving you a false sense of stability and accuracy. If instead a different seed is used each time the program is run then the bootstrap results will vary across runs, and you will observe that the results vary across these runs, giving you important and meaningful information about the (lack of) accuracy in your results. One way to ensure this is to set the seed according to the current clock. In MATLAB use the command rng(‘shuffle’). In R use set. seed (seed=NULL). Stata does not have this option.\nThese considerations lead to a recommended hybrid approach. For daily empirical investigations do not fix the bootstrap seed in your program unless you have it set by the clock. For your final calculations set the seed to a specific arbitrary choice, and set \\(B=10,000\\) so that the results are insensitive to the seed."
  },
  {
    "objectID": "chpt10-resample-method.html#bootstrap-regression",
    "href": "chpt10-resample-method.html#bootstrap-regression",
    "title": "10  Resampling Methods",
    "section": "10.27 Bootstrap Regression",
    "text": "10.27 Bootstrap Regression\nA major focus of this textbook has been on the least squares estimator \\(\\widehat{\\beta}\\) in the projection model. The bootstrap can be used to calculate standard errors and confidence intervals for smooth functions of the coefficient estimates.\nThe nonparametric bootstrap algorithm, as described before, samples observations randomly with replacement from the dataset, creating the bootstrap sample \\(\\left\\{\\left(Y_{1}^{*}, X_{1}^{*}\\right), \\ldots,\\left(Y_{n}^{*}, X_{n}^{*}\\right)\\right\\}\\), or in matrix notation \\(\\left(\\boldsymbol{Y}^{*}, \\boldsymbol{X}^{*}\\right)\\) It is important to recognize that entire observations (pairs of \\(Y_{i}\\) and \\(X_{i}\\) ) are sampled. This is often called the pairs bootstrap.\nGiven this bootstrap sample, we calculate the regression estimator\n\\[\n\\widehat{\\beta}^{*}=\\left(\\boldsymbol{X}^{* \\prime} \\boldsymbol{X}^{*}\\right)^{-1}\\left(\\boldsymbol{X}^{* \\prime} \\boldsymbol{Y}^{*}\\right) .\n\\]\nThis is repeated \\(B\\) times. The bootstrap standard errors are the standard deviations across the draws and confidence intervals are constructed from the empirical quantiles across the draws.\nWhat is the nature of the bootstrap distribution of \\(\\widehat{\\beta}^{*}\\) ? It is useful to start with the distribution of the bootstrap observations \\(\\left(Y_{i}^{*}, X_{i}^{*}\\right)\\), which is the discrete distribution which puts mass \\(1 / n\\) on each observation pair \\(\\left(Y_{i}, X_{i}\\right)\\). The bootstrap universe can be thought of as the empirical scatter plot of the observations. The true value of the projection coefficient in this bootstrap universe is\n\\[\n\\left(\\mathbb{E}^{*}\\left[X_{i}^{*} X_{i}^{* \\prime}\\right]\\right)^{-1}\\left(\\mathbb{E}^{*}\\left[X_{i}^{*} Y_{i}^{*}\\right]\\right)=\\left(\\frac{1}{n} \\sum_{i=1}^{n} X_{i} X_{i}^{\\prime}\\right)^{-1}\\left(\\frac{1}{n} \\sum_{i=1}^{n} X_{i} Y_{i}\\right)=\\widehat{\\beta}\n\\]\nWe see that the true value in the bootstrap distribution is the least squares estimator \\(\\widehat{\\beta}\\).\nThe bootstrap observations satisfy the projection equation\n\\[\n\\begin{aligned}\nY_{i}^{*} &=X_{i}^{* \\prime} \\widehat{\\beta}+e_{i}^{*} \\\\\n\\mathbb{E}^{*}\\left[X_{i}^{*} e_{i}^{*}\\right] &=0 .\n\\end{aligned}\n\\]\nFor each bootstrap pair \\(\\left(Y_{i}^{*}, X_{i}^{*}\\right)=\\left(Y_{j}, X_{j}\\right)\\) the true error \\(e_{i}^{*}=\\widehat{e}_{j}\\) equals the least squares residual from the original dataset. This is because each bootstrap pair corresponds to an actual observation.\nA technical problem (which is typically ignored) is that it is possible for \\(\\boldsymbol{X}^{* \\prime} \\boldsymbol{X}^{*}\\) to be singular in a simulated bootstrap sample, in which case the least squares estimator \\(\\widehat{\\beta}^{*}\\) is not uniquely defined. Indeed, the probability is positive that \\(\\boldsymbol{X}^{* \\prime} \\boldsymbol{X}^{*}\\) is singular. For example, the probability that a bootstrap sample consists entirely of one observation repeated \\(n\\) times is \\(n^{-(n-1)}\\). This is a small probability, but positive. A more significant example is sparse dummy variable designs where it is possible to draw an entire sample with only one observed value for the dummy variable. For example, if a sample has \\(n=20\\) observations with a dummy variable with treatment (equals 1) for only three of the 20 observations, the probability is \\(4 %\\) that a bootstrap sample contains entirely non-treated values (all 0’s). \\(4 %\\) is quite high!\nThe standard approach to circumvent this problem is to compute \\(\\widehat{\\beta}^{*}\\) only if \\(\\boldsymbol{X}^{* \\prime} \\boldsymbol{X}^{*}\\) is non-singular as defined by a conventional numerical tolerance and treat it as missing otherwise. A better solution is to define a tolerance which bounds \\(\\boldsymbol{X}^{* \\prime} \\boldsymbol{X}^{*}\\) away from non-singularity. Define the ratio of the smallest eigenvalue of the bootstrap design matrix to that of the data design matrix\n\\[\n\\lambda^{*}=\\frac{\\lambda_{\\min }\\left(\\boldsymbol{X}^{* \\prime} \\boldsymbol{X}^{*}\\right)}{\\lambda_{\\min }\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)} .\n\\]\nIf, in a given bootstrap replication, \\(\\lambda^{*}<\\tau\\) is smaller than a given tolerance (Shao and Tu \\((1995, \\mathrm{p} .291)\\) recommend \\(\\tau=1 / 2\\) ) then the estimator can be treated as missing, or we can define the trimming rule\n\\[\n\\widehat{\\beta}^{*}=\\left\\{\\begin{array}{cc}\n\\widehat{\\beta}^{*} & \\text { if } \\lambda^{*} \\geq \\tau \\\\\n\\widehat{\\beta} & \\text { if } \\lambda^{*}<\\tau\n\\end{array}\\right.\n\\]\nThis ensures that the bootstrap estimator \\(\\widehat{\\beta}^{*}\\) will be well behaved."
  },
  {
    "objectID": "chpt10-resample-method.html#bootstrap-regression-asymptotic-theory",
    "href": "chpt10-resample-method.html#bootstrap-regression-asymptotic-theory",
    "title": "10  Resampling Methods",
    "section": "10.28 Bootstrap Regression Asymptotic Theory",
    "text": "10.28 Bootstrap Regression Asymptotic Theory\nDefine the least squares estimator \\(\\widehat{\\beta}\\), its bootstrap version \\(\\widehat{\\beta}^{*}\\) as in (10.32), and the transformations \\(\\widehat{\\theta}=g(\\widehat{\\beta})\\) and \\(\\widehat{\\theta}^{*}=r\\left(\\widehat{\\beta}^{*}\\right)\\) for some smooth transformation \\(r\\). Let \\(\\widehat{\\boldsymbol{V}}_{\\beta}\\) and \\(\\widehat{\\boldsymbol{V}}_{\\theta}\\) denote heteroskedasticityrobust covariance matrix estimators for \\(\\widehat{\\beta}\\) and \\(\\widehat{\\theta}\\), and let \\(\\widehat{V}_{\\beta}^{*}\\) and \\(\\widehat{\\boldsymbol{V}}_{\\theta}^{*}\\) be their bootstrap versions. When \\(\\theta\\) is scalar define the standard errors \\(s(\\widehat{\\theta})=\\sqrt{n^{-1} \\widehat{\\boldsymbol{V}}_{\\theta}}\\) and \\(s\\left(\\widehat{\\theta}^{*}\\right)=\\sqrt{n^{-1} \\widehat{\\boldsymbol{V}}_{\\theta^{*}}}\\). Define the t-ratios \\(T=\\) \\((\\widehat{\\theta}-\\theta) / s(\\widehat{\\theta})\\) and bootstrap version \\(T^{*}=\\left(\\widehat{\\theta}^{*}-\\widehat{\\theta}\\right) / s\\left(\\widehat{\\theta}^{*}\\right)\\). We are interested in the asymptotic distributions of \\(\\widehat{\\beta}^{*}, \\widehat{\\theta}^{*}\\) and \\(T^{*}\\)\nSince the bootstrap observations satisfy the model (10.33), we see by standard calculations that\n\\[\n\\sqrt{n}\\left(\\widehat{\\beta}^{*}-\\widehat{\\beta}\\right)=\\left(\\frac{1}{n} \\sum_{i=1}^{n} X_{i}^{*} X_{i}^{* \\prime}\\right)^{-1}\\left(\\frac{1}{\\sqrt{n}} \\sum_{i=1}^{n} X_{i}^{*} e_{i}^{*}\\right)\n\\]\nBy the bootstrap WLLN\n\\[\n\\frac{1}{n} \\sum_{i=1}^{n} X_{i}^{*} X_{i}^{* \\prime} \\underset{p^{*}}{\\longrightarrow} \\mathbb{E}\\left[X_{i} X_{i}^{\\prime}\\right]=\\boldsymbol{Q}\n\\]\nand by the bootstrap CLT\n\\[\n\\frac{1}{\\sqrt{n}} \\sum_{i=1}^{n} X_{i}^{*} e_{i}^{*} \\underset{d^{*}}{\\longrightarrow} \\mathrm{N}(0, \\Omega)\n\\]\nwhere \\(\\Omega=\\mathbb{E}\\left[X X^{\\prime} e^{2}\\right]\\). Again applying the bootstrap WLLN we obtain\n\\[\n\\widehat{\\boldsymbol{V}}_{\\beta} \\underset{p^{*}}{ } \\boldsymbol{V}_{\\beta}=\\boldsymbol{Q}^{-1} \\Omega \\boldsymbol{Q}^{-1}\n\\]\nand\n\\[\n\\widehat{\\boldsymbol{V}}_{\\theta} \\underset{p^{*}}{\\longrightarrow} \\boldsymbol{V}_{\\theta}=\\boldsymbol{R}^{\\prime} \\boldsymbol{V}_{\\beta} \\boldsymbol{R}\n\\]\nwhere \\(\\boldsymbol{R}=\\boldsymbol{R}(\\beta)\\).\nCombining with the bootstrap CMT and delta method we establish the asymptotic distribution of the bootstrap regression estimator.\nTheorem \\(10.18\\) Under Assumption 7.2, as \\(n \\rightarrow \\infty\\)\n\\[\n\\sqrt{n}\\left(\\widehat{\\theta}^{*}-\\widehat{\\theta}\\right) \\underset{d^{*}}{\\longrightarrow} \\mathrm{N}\\left(0, \\boldsymbol{V}_{\\beta}\\right) .\n\\]\nIf Assumption \\(7.3\\) also holds then\n\\[\n\\sqrt{n}\\left(\\widehat{\\theta}^{*}-\\widehat{\\theta}\\right) \\underset{d^{*}}{\\longrightarrow} \\mathrm{N}\\left(0, \\boldsymbol{V}_{\\theta}\\right) .\n\\]\nIf Assumption \\(7.4\\) also holds then\n\\[\nT^{*} \\underset{d^{*}}{\\longrightarrow} \\mathrm{N}(0,1) .\n\\]\nThis means that the bootstrap confidence interval and testing methods all apply for inference on \\(\\beta\\) and \\(\\theta\\). This includes the percentile, \\(\\mathrm{BC}\\) percentile, \\(\\mathrm{BC}_{a}\\), and percentile-t intervals, and hypothesis tests based on t-tests, Wald tests, MD tests, LR tests and F tests.\nTo justify bootstrap standard errors we also need to verify the uniform square integrability of \\(\\widehat{\\beta}^{*}\\) and \\(\\widehat{\\theta}^{*}\\). This is technically challenging because the least squares estimator involves matrix inversion which is not globally continuous. A partial solution is to use the trimmed estimator (10.34). This bounds the moments of \\(\\widehat{\\beta}^{*}\\) by those of \\(n^{-1} \\sum_{i=1}^{n} X_{i}^{*} e_{i}^{*}\\). Since this is a sample mean, Theorem \\(10.10\\) applies and \\(\\widehat{\\boldsymbol{V}}_{\\beta}^{*}\\) is bootstrap consistent for \\(\\boldsymbol{V}_{\\beta}\\). However, this does not ensure that \\(\\widehat{\\boldsymbol{V}}_{\\theta}^{*}\\) will be consistent for \\(\\widehat{\\boldsymbol{V}}_{\\theta}\\) unless the function \\(r(x)\\) satisfies the conditions of Theorem 10.10. For general applications use a trimmed estimator for the bootstrap variance. For some \\(\\tau_{n}=O\\left(e^{n / 8}\\right)\\) define\n\\[\n\\begin{aligned}\nZ_{n}^{*} &=\\sqrt{n}\\left(\\widehat{\\theta}^{*}-\\widehat{\\theta}\\right) \\\\\nZ^{* *} &=z^{*} \\mathbb{1}\\left\\{\\left\\|Z_{n}^{*}\\right\\| \\leq \\tau_{n}\\right\\} \\\\\nZ^{* *} &=\\frac{1}{B} \\sum_{b=1}^{B} Z^{* *}(b) \\\\\n\\widehat{\\mathbf{V}}_{\\theta}^{\\text {boot }, \\tau} &=\\frac{1}{B-1} \\sum_{b=1}^{B}\\left(Z^{* *}(b)-Z^{* *}\\right)\\left(Z^{* *}(b)-Z^{* *}\\right)^{\\prime} .\n\\end{aligned}\n\\]\nThe matrix \\(\\widehat{\\boldsymbol{V}}_{\\theta}^{\\text {boot }}\\) is a trimmed bootstrap estimator of the variance of \\(Z_{n}=\\sqrt{n}(\\widehat{\\theta}-\\theta)\\). The associated bootstrap standard error for \\(\\widehat{\\theta}\\) (in the scalar case) is \\(s(\\widehat{\\theta})=\\sqrt{n^{-1} \\widehat{\\boldsymbol{V}}_{\\theta}^{\\text {boot }}}\\).\nBy an application of Theorems \\(10.11\\) and 10.12, we find that this estimator \\(\\widehat{\\boldsymbol{V}}_{\\theta}^{\\text {boot }}\\) is consistent for the asymptotic variance.\nTheorem 10.19 Under Assumption \\(7.2\\) and \\(7.3\\), as \\(n \\rightarrow \\infty, \\widehat{\\boldsymbol{V}}_{\\theta}^{\\mathrm{boot}, \\tau} \\underset{p^{*}}{\\longrightarrow} \\boldsymbol{V}_{\\theta}\\)\nPrograms such as Stata use the untrimmed estimator \\(\\widehat{\\boldsymbol{V}}_{\\theta}^{\\text {boot }}\\) rather than the trimmed estimator \\(\\widehat{\\boldsymbol{V}}_{\\theta}^{\\text {boot }, \\tau}\\). This means that we should be cautious about interpreting reported bootstrap standard errors especially for nonlinear functions such as ratios."
  },
  {
    "objectID": "chpt10-resample-method.html#wild-bootstrap",
    "href": "chpt10-resample-method.html#wild-bootstrap",
    "title": "10  Resampling Methods",
    "section": "10.29 Wild Bootstrap",
    "text": "10.29 Wild Bootstrap\nTake the linear regression model\n\\[\n\\begin{aligned}\nY &=X^{\\prime} \\beta+e \\\\\n\\mathbb{E}[e \\mid X] &=0 .\n\\end{aligned}\n\\]\nWhat is special about this model is the conditional mean restriction. The nonparametric bootstrap (which samples the pairs \\(\\left(Y_{i}^{*}, X_{i}^{*}\\right)\\) i.i.d. from the original observations) does not make use of this restriction. Consequently the bootstrap distribution for \\(\\left(Y^{*}, X^{*}\\right)\\) does not satisfy the conditional mean restriction and therefore does not satisfy the linear regression assumption. To improve precision it seems reasonable to impose the conditional mean restriction on the bootstrap distribution.\nA natural approach is to hold the regressors \\(X_{i}\\) fixed and then draw the errors \\(e_{i}^{*}\\) in some way which imposes a conditional mean of zero. The simplest approach is to draw the errors independent from the regressors, perhaps from the empirical distribution of the residuals. This procedure is known as the residual bootstrap. However, this imposes independence of the errors from the regressors which is much stronger than the conditional mean assumption. This is generally undesirable.\nA method which imposes the conditional mean restriction while allowing general heteroskedasticity is the wild bootstrap. It was proposed by Liu (1988) and extended by Mammon (1993). The method uses auxiliary random variables \\(\\xi^{*}\\) which are i.i.d., mean zero, and variance 1 . The bootstrap observations are then generated as \\(Y_{i}^{*}=X_{i}^{\\prime} \\widehat{\\beta}+e_{i}^{*}\\) with \\(e_{i}^{*}=\\widehat{e}_{i} \\xi_{i}^{*}\\), where the regressors \\(X_{i}\\) are held fixed at their sample values, \\(\\widehat{\\beta}\\) is the sample least squares estimator, and \\(\\widehat{e}_{i}\\) are the least squares residuals, which are also held fixed at their sample values.\nThis algorithm generates bootstrap errors \\(e_{i}^{*}\\) which are conditionally mean zero. Thus the bootstrap pairs \\(\\left(Y_{i}^{*}, X_{i}\\right)\\) satisfy a linear regression with the “true” coefficient of \\(\\widehat{\\beta}\\). The conditional variance of the wild bootstrap errors \\(e_{i}^{*}\\) are \\(\\mathbb{E}^{*}\\left[e_{i}^{* 2} \\mid X_{i}\\right]=\\widehat{e}_{i}^{2}\\). This means that the conditional variance of the bootstrap estimator \\(\\widehat{\\beta}^{*}\\) is\n\\[\n\\mathbb{E}^{*}\\left[\\left(\\widehat{\\beta}^{*}-\\widehat{\\beta}\\right)\\left(\\widehat{\\beta}^{*}-\\widehat{\\beta}\\right)^{\\prime} \\mid \\boldsymbol{X}\\right]=\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1}\\left(\\sum_{i=1}^{n} X_{i} X_{i}^{\\prime} \\widehat{e}_{i}^{2}\\right)\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1}\n\\]\nwhich is the White estimator of the variance of \\(\\widehat{\\beta}\\). Thus the wild bootstrap replicates the appropriate first and second moments of the distribution.\nTwo distributions have been proposed for the auxilary variables \\(\\xi_{i}^{*}\\) both of which are two-point discrete distributions. The first are Rademacher random variables which satisfy \\(\\mathbb{P}\\left[\\xi^{*}=1\\right]=\\frac{1}{2}\\) and \\(\\mathbb{P}\\left[\\xi^{*}=-1\\right]=\\) \\(\\frac{1}{2}\\). The second is the Mammen (1993) two-point distribution\n\\[\n\\begin{aligned}\n&\\mathbb{P}\\left[\\xi^{*}=\\frac{1+\\sqrt{5}}{2}\\right]=\\frac{\\sqrt{5}-1}{2 \\sqrt{5}} \\\\\n&\\mathbb{P}\\left[\\xi^{*}=\\frac{1-\\sqrt{5}}{2}\\right]=\\frac{\\sqrt{5}+1}{2 \\sqrt{5}}\n\\end{aligned}\n\\]\nThe reasoning behind the Mammen distribution is that this choice implies \\(\\mathbb{E}\\left[\\xi^{* 3}\\right]=1\\), which implies that the third central moment of \\(\\widehat{\\beta}^{*}\\) matches the natural nonparametric estimator of the third central moment of \\(\\widehat{\\beta}\\). Since the wild bootstrap matches the first three moments, the percentile-t interval and one-sided t-tests can be shown to achieve asymptotic refinements.\nThe reasoning behind the Rademacher distribution is that this choice implies \\(\\mathbb{E}\\left[\\xi^{* 4}\\right]=1\\), which implies that the fourth central moment of \\(\\widehat{\\beta}^{*}\\) matches the natural nonparametric estimator of the fourth central moment of \\(\\widehat{\\beta}\\). If the regression errors \\(e\\) are symmetrically distributed (so the third moment is zero) then the first four moments are matched. In this case the wild bootstrap should have even better performance, and additionally two-sided t-tests can be shown to achieve an asymptotic refinement. When the regression error is not symmetrically distributed these asymptotic refinements are not achieved. Limited simulation evidence for one-sided t-tests presented in Davidson and Flachaire (2008) suggests that the Rademacher distribution (used with the restricted wild bootstrap) has better performance and is their recommendation.\nFor hypothesis testing improved precision can be obtained by the restricted wild bootstrap. Consider tests of the hypothesis \\(\\mathbb{H}_{0}: r(\\beta)=0\\). Let \\(\\widetilde{\\beta}\\) be a CLS or EMD estimator of \\(\\beta\\) subject to the restriction \\(r(\\widetilde{\\beta})=0\\). Let \\(\\widetilde{e}_{i}=Y_{i}-X_{i}^{\\prime} \\widetilde{\\beta}\\) be the constrained residuals. The restricted wild bootstrap algorithm generates observations as \\(Y_{i}^{*}=X_{i}^{\\prime} \\widetilde{\\beta}+e_{i}^{*}\\) with \\(e_{i}^{*}=\\widetilde{e}_{i} \\xi_{i}^{*}\\). With this modification \\(\\widetilde{\\beta}\\) is the true value in the bootstrap universe so the null hypothesis \\(\\mathbb{M}_{0}\\) holds. Thus bootstrap tests are constructed the same as for the parametric bootstrap using a restricted parameter estimator."
  },
  {
    "objectID": "chpt10-resample-method.html#bootstrap-for-clustered-observations",
    "href": "chpt10-resample-method.html#bootstrap-for-clustered-observations",
    "title": "10  Resampling Methods",
    "section": "10.30 Bootstrap for Clustered Observations",
    "text": "10.30 Bootstrap for Clustered Observations\nBootstrap methods can also be applied in to clustered samples though the methodological literature is relatively thin. Here we review methods discussed in Cameron, Gelbach and Miller (2008).\nLet \\(\\boldsymbol{Y}_{g}=\\left(Y_{1 g}, \\ldots, Y_{n_{g} g}\\right)^{\\prime}\\) and \\(\\boldsymbol{X}_{g}=\\left(X_{1 g}, \\ldots, X_{n_{g} g}\\right)^{\\prime}\\) denote the \\(n_{g} \\times 1\\) vector of dependent variables and \\(n_{g} \\times k\\) matrix of regressors for the \\(g^{t h}\\) cluster. A linear regression model using cluster notation is \\(\\boldsymbol{Y}_{g}=\\) \\(\\boldsymbol{X}_{g} \\beta+\\boldsymbol{e}_{g}\\) where \\(\\boldsymbol{e}_{g}=\\left(e_{1 g}, \\ldots, e_{n_{g} g}\\right)^{\\prime}\\) is an \\(n_{g} \\times 1\\) error vector. The sample has \\(G\\) cluster pairs \\(\\left(\\boldsymbol{Y}_{g}, \\boldsymbol{X}_{g}\\right)\\).\nThe pairs cluster bootstrap samples \\(G\\) cluster pairs \\(\\left(\\boldsymbol{Y}_{g}, \\boldsymbol{X}_{g}\\right)\\) to create the bootstrap sample. Least squares is applied to the bootstrap sample to obtain the coefficient estimators. By repeating \\(B\\) times bootstrap standard errors for coefficients estimates, or functions of the coefficient estimates, can be calculated. Percentile, \\(\\mathrm{BC}\\) percentile, and \\(\\mathrm{BC}_{a}\\) confidence intervals can be calculated.\nThe \\(\\mathrm{BC}_{a}\\) interval requires an estimator of the acceleration coefficient \\(a\\) which is a scaled jackknife estimate of the third moment of the estimator. In the context of clustered observations the delete-cluster jackknife should be used for estimation of \\(a\\).\nFurthermore, on each bootstrap sample the cluster-robust standard errors can be calculated and used to compute bootstrap t-ratios, from which percentile-t confidence intervals can be calculated. tions as\nThe wild cluster bootstrap fixes the clusters and regressors, and generates the bootstrap observa-\n\\[\n\\begin{aligned}\n\\boldsymbol{Y}_{g}^{*} &=\\boldsymbol{X}_{g} \\widehat{\\beta}+\\boldsymbol{e}_{g}^{*} \\\\\n\\boldsymbol{e}_{g}^{*} &=\\widehat{\\boldsymbol{e}}_{i} \\xi_{g}^{*}\n\\end{aligned}\n\\]\nwhere \\(\\xi_{g}^{*}\\) is a scalar auxilary random variable as described in the previous section. Notice that \\(\\xi_{g}^{*}\\) is interacted with the entire vector of residuals from cluster \\(g\\). Cameron, Gelbach and Miller (2008) follow the recommendation of Davidson and Flachaire (2008) and use Rademacher random variables for \\(\\xi_{g}^{*}\\).\nFor hypothesis testing, Cameron, Gelbach and Miller (2008) recommend the restricted wild cluster bootstrap. For tests of \\(\\mathbb{M}_{0}: r(\\beta)=0\\) let \\(\\widetilde{\\beta}\\) be a CLS or EMD estimator of \\(\\beta\\) subject to the restriction \\(r(\\widetilde{\\beta})=\\) 0. Let \\(\\widetilde{\\boldsymbol{e}}_{g}=\\boldsymbol{Y}_{g}-\\boldsymbol{X}_{g} \\widetilde{\\beta}\\) be the constrained cluster-level residuals. The restricted wild cluster bootstrap algorithm generates observations as\n\\[\n\\begin{aligned}\n\\boldsymbol{Y}_{g}^{*} &=\\boldsymbol{X}_{g} \\widetilde{\\beta}+\\boldsymbol{e}_{g}^{*} \\\\\n\\boldsymbol{e}_{g}^{*} &=\\widetilde{\\boldsymbol{e}}_{i} \\xi_{g}^{*}\n\\end{aligned}\n\\]\nOn each bootstrap sample the test statistic for \\(\\mathbb{M}_{0}\\) (t-ratio, Wald, LR, or F) is applied. Since the bootstrap algorithm satisfies \\(\\mathbb{M}_{0}\\) these statistics are centered at the hypothesized value. p-values are then calculated conventionally and used to assess the significance of the test statistic.\nThere are several reasons why conventional asymptotic approximations may work poorly with clustered observations. First, while the sample size \\(n\\) may be large, the effective sample size is the number of clusters \\(G\\). This is because when the dependence structure within each cluster is unconstrained the central limit theorem effectively treats each cluster as a single observation. Thus, if \\(G\\) is small we should treat inference as a small sample problem. Second, cluster-robust covariance matrix estimation explicitly treats each cluster as a single observation. Consequently the accuracy of normal approximations to tratios and Wald statistics is more accurately viewed as a small sample distribution problem. Third, when cluster sizes \\(n_{g}\\) are heterogeneous this means that the estimation problems just described also involve heterogeneous variances. Specifically, heterogeneous cluster sizes induces a high degree of effective heteroskedasticity (since the variance of a within-cluster sum is proportional to \\(n_{g}\\) ). When \\(G\\) is small this means that cluster-robust inference is similar to finite-sample inference with a small heteroskedastic sample. Fourth, interest often concerns treatment which is applied at the level of a cluster (such as the effect of tracking discussed in Section 4.21). If the number of treated clusters is small this is equivalent to estimation with a highly sparse dummy variable design in which case cluster-robust covariance matrix estimation can be unreliable.\nThese concerns suggest that conventional normal approximations may be poor in the context of clustered observations with a small number of groups \\(G\\), motivating the use of bootstrap methods. However, these concerns also can cause challenges with the accuracy of bootstrap approximations. When the number of clusters \\(G\\) is small, the cluster sizes \\(n_{g}\\) heterogeneous, or the number of treated clusters small, bootstrap methods may be inaccurate. In such cases inference should proceed cautiously.\nTo illustrate the use of the pairs cluster bootstrap, Table \\(10.4\\) reports the estimates of the example from Section \\(4.21\\) of the effect of tracking on testscores from Duflo, Dupas, and Kremer (2011). In addition to the asymptotic cluster standard error we report the cluster jackknife and cluster bootstrap standard errors as well as three percentile-type confidence intervals. We use 10,000 bootstrap replications. In this example the asymptotic, jackknife, and cluster bootstrap standard errors are identical, which reflects the good balance of this particular regression design.\nTable 10.4: Comparison of Methods for Estimate of Effect of Tracking\n\n\n\nCoefficient on Tracking\n\\(0.138\\)\n\n\n\n\nAsymptotic cluster s.e.\n\\((0.078)\\)\n\n\nJackknife cluster s.e.\n\\((0.078)\\)\n\n\nCluster Bootstrap s.e.\n\\((0.078)\\)\n\n\n\\(95 %\\) Percentile Interval\n\\([-0.013,0.291]\\)\n\n\n\\(95 % \\mathrm{BC}\\) Percentile Interval\n\\([-0.015,0.289]\\)\n\n\n\\(95 % \\mathrm{BC}_{a}\\) Percentile Interval\n\\([-0.018,0.286]\\)\n\n\n\nIn Stata, to obtain cluster bootstrap standard errors and confidence intervals use the options cluster (id) vce(bootstrap, reps \\(\\#\\) )) , where id is the cluster variable and # is the number of replications."
  },
  {
    "objectID": "chpt10-resample-method.html#technical-proofs",
    "href": "chpt10-resample-method.html#technical-proofs",
    "title": "10  Resampling Methods",
    "section": "10.31 Technical Proofs*",
    "text": "10.31 Technical Proofs*\nSome of the asymptotic results are facilitated by the following convergence result.\nTheorem 10.20 Marcinkiewicz WLLN If \\(u_{i}\\) are independent and uniformly integrable, then for any \\(r>\\) 1 , as \\(n \\rightarrow \\infty, n^{-r} \\sum_{i=1}^{n}\\left|u_{i}\\right|^{r} \\underset{p}{\\longrightarrow} 0\\).\nProof of Theorem \\(10.20\\)\n\\[\nn^{-r} \\sum_{i=1}^{n}\\left|u_{i}\\right|^{r} \\leq\\left(n^{-1} \\max _{1 \\leq i \\leq n}\\left|u_{i}\\right|\\right)^{r-1} \\frac{1}{n} \\sum_{i=1}^{n}\\left|u_{i}\\right| \\underset{p}{\\longrightarrow} 0\n\\]\nby the WLLN, Theorem \\(6.15\\), and \\(r>1\\).\nProof of Theorem 10.1 Fix \\(\\epsilon>0\\). Since \\(Z_{n} \\underset{p}{\\longrightarrow} Z\\) there is an \\(n\\) sufficiently large such that\n\\[\n\\mathbb{P}\\left[\\left\\|Z_{n}-Z\\right\\|>\\epsilon\\right]<\\epsilon .\n\\]\nSince the event \\(\\left\\|Z_{n}-Z\\right\\|>\\epsilon\\) is non-random under the conditional probability \\(\\mathbb{P}^{*}\\), for such \\(n\\),\n\\[\n\\mathbb{P}^{*}\\left[\\left\\|Z_{n}-Z\\right\\|>\\epsilon\\right]=\\left\\{\\begin{array}{cc}\n0 & \\text { with probability exceeding } 1-\\epsilon \\\\\n1 & \\text { with probability less than } \\epsilon .\n\\end{array}\\right.\n\\]\nSince \\(\\varepsilon\\) is arbitrary we conclude \\(\\mathbb{P}^{*}\\left[\\left\\|Z_{n}-Z\\right\\|>\\epsilon\\right] \\underset{p}{\\longrightarrow} 0\\) as required.\nProof of Theorem 10.2 Fix \\(\\epsilon>0\\). By Markov’s inequality (B.36), the facts (10.12) and (10.13), and finally the Marcinkiewicz WLLN (Theorem 10.20) with \\(r=2\\) and \\(u_{i}=\\left\\|Y_{i}\\right\\|\\),\n\\[\n\\begin{aligned}\n\\mathbb{P}^{*}\\left[\\left\\|\\bar{Y}^{*}-\\bar{Y}\\right\\|>\\epsilon\\right] & \\leq \\epsilon^{-2} \\mathbb{E}^{*}\\left\\|\\bar{Y}^{*}-\\bar{Y}\\right\\|^{2} \\\\\n&=\\epsilon^{-2} \\operatorname{tr}\\left(\\operatorname{var}^{*}\\left[\\bar{Y}^{*}\\right]\\right) \\\\\n&=\\epsilon^{-2} \\operatorname{tr}\\left(\\frac{1}{n} \\widehat{\\Sigma}\\right) \\\\\n& \\leq \\epsilon^{-2} n^{-2} \\sum_{i=1}^{n} Y_{i}^{\\prime} Y_{i} \\\\\n& \\underset{p}{ } 0\n\\end{aligned}\n\\]\nThis establishes that \\(\\bar{Y}^{*}-\\bar{Y} \\underset{p^{*}}{\\longrightarrow} 0\\).\nSince \\(\\bar{Y}-\\mu \\underset{p}{\\longrightarrow} 0\\) by the WLLN, \\(\\bar{Y}-\\mu \\underset{p^{*}}{\\longrightarrow} 0\\) by Theorem 10.1. Since \\(\\bar{Y}^{*}-\\mu=\\bar{Y}^{*}-\\bar{Y}+\\bar{Y}-\\mu\\), we deduce that \\(\\bar{Y}^{*}-\\mu \\underset{p^{*}}{ } 0\\).\nProof of Theorem 10.4 We verify conditions for the multivariate Lindeberg CLT (Theorem 6.4). (We cannot use the Lindeberg-Lévy CLT because the conditional distribution depends on \\(n\\).) Conditional on \\(F_{n}\\), the bootstrap draws \\(Y_{i}^{*}-\\bar{Y}\\) are i.i.d. with mean 0 and covariance matrix \\(\\widehat{\\Sigma}\\). Set \\(v_{n}^{2}=\\lambda_{\\min }(\\widehat{\\Sigma})\\). Note that by the WLLN, \\(v_{n}^{2} \\underset{p}{\\rightarrow} v^{2}=\\lambda_{\\min }(\\Sigma)>0\\). Thus for \\(n\\) sufficiently large, \\(v_{n}^{2}>0\\) with high probability. Fix \\(\\epsilon>0\\). Equation (6.2) equals\n\\[\n\\begin{aligned}\n\\frac{1}{n v_{n}^{2}} \\sum_{i=1}^{n} \\mathbb{E}^{*}\\left[\\left\\|Y_{i}^{*}-\\bar{Y}\\right\\|^{2} \\mathbb{1}\\left\\{\\left\\|Y_{i}^{*}-\\bar{Y}\\right\\|^{2} \\geq \\epsilon n v_{n}^{2}\\right\\}\\right] &=\\frac{1}{v_{n}^{2}} \\mathbb{E}^{*}\\left[\\left\\|Y_{i}^{*}-\\bar{Y}\\right\\|^{2} \\mathbb{1}\\left\\{\\left\\|Y_{i}^{*}-\\bar{Y}\\right\\|^{2} \\geq \\epsilon n v_{n}^{2}\\right\\}\\right] \\\\\n& \\leq \\frac{1}{\\epsilon n v_{n}^{4}} \\mathbb{E}^{*}\\left\\|Y_{i}^{*}-\\bar{Y}\\right\\|^{4} \\\\\n& \\leq \\frac{2^{4}}{\\epsilon n v_{n}^{4}} \\mathbb{E}^{*}\\left\\|Y_{i}^{*}\\right\\|^{4} \\\\\n&=\\frac{2^{4}}{\\epsilon n^{2} v_{n}^{4}} \\sum_{i=1}^{n}\\left\\|Y_{i}\\right\\|^{4} \\\\\n& \\longrightarrow 0 .\n\\end{aligned}\n\\]\nThe second inequality uses Minkowski’s inequality (B.34), Liapunov’s inequality (B.35), and the \\(c_{r}\\) inequality (B.6). The following equality is \\(\\mathbb{E}^{*}\\left\\|Y_{i}^{*}\\right\\|^{4}=n^{-1} \\sum_{i=1}^{n}\\left\\|Y_{i}\\right\\|^{4}\\), which is similar to (10.10). The final convergence holds by the Marcinkiewicz WLLN (Theorem 10.20) with \\(r=2\\) and \\(u_{i}=\\left\\|Y_{i}\\right\\|^{2}\\). The conditions for Theorem \\(6.4\\) hold and we conclude\n\\[\n\\widehat{\\Sigma}^{-1 / 2} \\sqrt{n}\\left(\\bar{Y}^{*}-\\bar{Y}\\right) \\underset{d^{*}}{\\longrightarrow} \\mathrm{N}(0, \\boldsymbol{I}) .\n\\]\nSince \\(\\widehat{\\Sigma} \\underset{p^{*}}{\\longrightarrow} \\Sigma\\) we deduce that \\(\\sqrt{n}\\left(\\bar{Y}^{*}-\\bar{Y}\\right) \\underset{d^{*}}{\\longrightarrow} \\mathrm{N}(0, \\Sigma)\\) as claimed.\nProof of Theorem \\(10.10\\) For notational simplicity assume \\(\\theta\\) and \\(\\mu\\) are scalar. Set \\(h_{i}=h\\left(Y_{i}\\right)\\). The assumption that the \\(p^{t h}\\) derivative of \\(g(u)\\) is bounded implies \\(\\left|g^{(p)}(u)\\right| \\leq C\\) for some \\(C<\\infty\\). Taking a \\(p^{\\text {th }}\\) order Taylor series expansion\n\\[\n\\widehat{\\theta}^{*}-\\widehat{\\theta}=g\\left(\\bar{h}^{*}\\right)-g(\\bar{h})=\\sum_{j=1}^{p-1} \\frac{g^{(j)}(\\bar{h})}{j !}\\left(\\bar{h}^{*}-\\bar{h}\\right)^{j}+\\frac{g^{(p)}\\left(\\zeta_{n}^{*}\\right)}{p !}\\left(\\bar{h}^{*}-\\bar{h}\\right)^{p}\n\\]\nwhere \\(\\zeta_{n}^{*}\\) lies between \\(\\bar{h}^{*}\\) and \\(\\bar{h}\\). This implies\n\\[\n\\left|z_{n}^{*}\\right|=\\sqrt{n}\\left|\\widehat{\\theta}^{*}-\\widehat{\\theta}\\right| \\leq \\sqrt{n} \\sum_{j=1}^{p} c_{j} \\mid \\bar{h}^{*}-\\bar{h}^{j}\n\\]\nwhere \\(c_{j}=\\left|g^{(j)}(\\bar{h})\\right| / j\\) ! for \\(j<p\\) and \\(c_{p}=C / p\\) !. We find that the fourth central moment of the normalized bootstrap estimator \\(Z_{n}^{*}=\\sqrt{n}\\left(\\widehat{\\theta}^{*}-\\widehat{\\theta}\\right)\\) satisfies the bound\n\\[\n\\mathbb{E}^{*}\\left[Z_{n}^{* 4}\\right] \\leq \\sum_{r=4}^{4 p} a_{r} n^{2} \\mathbb{E}^{*}\\left|\\bar{h}^{*}-\\bar{h}\\right|^{r}\n\\]\nwhere the coefficients \\(a_{r}\\) are products of the coefficients \\(c_{j}\\) and hence each \\(O_{p}(1)\\). We see that \\(\\mathbb{E}^{*}\\left[Z_{n}^{* 4}\\right]=\\) \\(O_{p}(1)\\) if \\(n^{2} \\mathbb{E}^{*}\\left|\\bar{h}^{*}-\\bar{h}\\right|^{r}=O_{p}(1)\\) for \\(r=4, \\ldots, 4 p\\).\nWe show this holds for any \\(r \\geq 4\\) using Rosenthal’s inequality (B.50), which states that for each \\(r\\) there is a constant \\(R_{r}<\\infty\\) such that\n\\[\n\\begin{aligned}\nn^{2} \\mathbb{E}^{*}\\left|\\bar{h}^{*}-\\bar{h}\\right|^{r} &=n^{2-r_{\\mathbb{E}}}\\left|\\sum_{i=1}^{n}\\left(h_{i}^{*}-\\bar{h}\\right)\\right|^{r} \\\\\n& \\leq n^{2-r} R_{r}\\left\\{\\left(n \\mathbb{E}^{*}\\left(h_{i}^{*}-\\bar{h}\\right)^{2}\\right)^{r / 2}+n \\mathbb{E}^{*}\\left|h_{i}^{*}-\\bar{h}\\right|^{r}\\right\\} \\\\\n&=R_{r}\\left\\{n^{2-r / 2} \\widehat{\\sigma}^{r}+\\frac{1}{n^{r-2}} \\sum_{i=1}^{n}\\left|h_{i}-\\bar{h}\\right|^{r}\\right\\}\n\\end{aligned}\n\\]\nSince \\(\\mathbb{E}\\left[h_{i}^{2}\\right]<\\infty, \\widehat{\\sigma}^{2}=O_{p}(1)\\), so the first term in (10.36) is \\(O_{p}(1)\\). Also, by the Marcinkiewicz WLLN (Theorem 10.20), \\(n^{-r / 2} \\sum_{i=1}^{n}\\left|h_{i}-\\bar{h}\\right|^{r}=o_{p}\\) (1) for any \\(r \\geq 1\\), so the second term in (10.36) is \\(o_{p}(1)\\) for \\(r \\geq 4\\). Thus for all \\(r \\geq 4,(10.36)\\) is \\(O_{p}(1)\\) and thus (10.35) is \\(O_{p}(1)\\). We deduce that \\(Z_{n}^{*}\\) is uniformly square integrable, and the bootstrap estimate of variance is consistent.\nThis argument can be extended to vector-valued means and estimates.\nProof of Theorem 10.12 We show that \\(\\mathbb{E}^{*}\\left\\|Z_{n}^{* *}\\right\\|^{4}=O_{p}(1)\\). Theorem \\(6.13\\) shows that \\(Z_{n}^{* *}\\) is uniformly square integrable. Since \\(Z_{n}^{* *} \\underset{d^{*}}{\\longrightarrow} Z\\), Theorem \\(6.14\\) implies that \\(\\operatorname{var}\\left[Z_{n}^{* *}\\right] \\rightarrow \\operatorname{var}[Z]=V_{\\beta}\\) as stated.\nSet \\(h_{i}=h\\left(Y_{i}\\right)\\). Since \\(\\boldsymbol{G}(x)=\\frac{\\partial}{\\partial x} g(x)^{\\prime}\\) is continuous in a neighborhood of \\(\\mu\\), there exists \\(\\eta>0\\) and \\(M<\\infty\\) such that \\(\\|x-\\mu\\| \\leq 2 \\eta\\) implies \\(\\operatorname{tr}\\left(\\boldsymbol{G}(x)^{\\prime} \\boldsymbol{G}(x)\\right) \\leq M\\). By the WLLN and bootstrap WLLN there is an \\(n\\) sufficiently large such that \\(\\left\\|\\bar{h}_{n}-\\mu\\right\\| \\leq \\eta\\) and \\(\\left\\|\\bar{h}_{n}^{*}-\\bar{h}_{n}\\right\\| \\leq \\eta\\) with probability exceeding \\(1-\\eta\\). On this event, \\(\\left\\|x-\\bar{h}_{n}\\right\\| \\leq \\eta\\) implies \\(\\operatorname{tr}\\left(\\boldsymbol{G}(x)^{\\prime} \\boldsymbol{G}(x)\\right) \\leq M\\). Using the mean-value theorem at a point \\(\\zeta_{n}^{*}\\) intermediate between \\(\\bar{h}_{n}^{*}\\) and \\(\\bar{h}_{n}\\)\n\\[\n\\begin{aligned}\n\\left\\|Z_{n}^{* *}\\right\\|^{4} \\mathbb{1}\\left\\{\\left\\|\\bar{h}_{n}^{*}-\\bar{h}_{n}\\right\\| \\leq \\eta\\right\\} & \\leq n^{2}\\left\\|g\\left(\\bar{h}_{n}^{*}\\right)-g\\left(\\bar{h}_{n}\\right)\\right\\|^{4} \\mathbb{1}\\left\\{\\left\\|\\bar{h}_{n}^{*}-\\bar{h}_{n}\\right\\| \\leq \\eta\\right\\} \\\\\n& \\leq n^{2}\\left\\|\\boldsymbol{G}\\left(\\zeta_{n}^{*}\\right)^{\\prime}\\left(\\bar{h}_{n}^{*}-\\bar{h}_{n}\\right)\\right\\|^{4} \\\\\n& \\leq M^{2} n^{2}\\left\\|\\bar{h}_{n}^{*}-\\bar{h}_{n}\\right\\|^{4} .\n\\end{aligned}\n\\]\nThen\n\\[\n\\begin{aligned}\n\\mathbb{E}^{*}\\left\\|Z_{n}^{* *}\\right\\|^{4} & \\leq \\mathbb{E}^{*}\\left[\\left\\|Z_{n}^{* *}\\right\\|^{4} \\mathbb{1}\\left\\{\\left\\|\\bar{h}_{n}^{*}-\\bar{h}_{n}\\right\\| \\leq \\eta\\right\\}\\right]+\\tau_{n}^{4} \\mathbb{E}^{*}\\left[\\mathbb{1}\\left\\{\\left\\|\\bar{h}_{n}^{*}-\\bar{h}_{n}\\right\\|>\\eta\\right\\}\\right] \\\\\n& \\leq M^{2} n^{2} \\mathbb{E}^{*}\\left\\|\\bar{h}_{n}^{*}-\\bar{h}_{n}\\right\\|^{4}+\\tau_{n}^{4} \\mathbb{P}^{*}\\left(\\left\\|\\bar{h}_{n}^{*}-\\bar{h}_{n}\\right\\|>\\eta\\right) .\n\\end{aligned}\n\\]\nIn (10.17) we showed that the first term in (10.37) is \\(O_{p}(1)\\) in the scalar case. The vector case follows by element-by-element expansion.\nNow take the second term in (10.37). We apply Bernstein’s inequality for vectors (B.41). Note that \\(\\bar{h}_{n}^{*}-\\bar{h}_{n}=n^{-1} \\sum_{i=1}^{n} u_{i}^{*}\\) with \\(u_{i}^{*}=h_{i}^{*}-\\bar{h}_{n}\\) and \\(j^{t h}\\) element \\(u_{j i}^{*}=h_{j i}^{*}-\\bar{h}_{j n}\\). The \\(u_{i}^{*}\\) are i.i.d., mean zero, \\(\\mathbb{E}^{*}\\left[u_{j i}^{* 2}\\right]=\\widehat{\\sigma}_{j}^{2}=O_{p}(1)\\), and satisfy the bound \\(\\left|u_{j i}^{*}\\right| \\leq 2 \\max _{i, j}\\left|h_{j i}\\right|=B_{n}\\), say. Bernstein’s inequality states that\n\\[\n\\mathbb{P}^{*}\\left[\\left\\|\\bar{h}_{n}^{*}-\\bar{h}_{n}\\right\\|>\\eta\\right] \\leq 2 m \\exp \\left(-n^{1 / 2} \\frac{\\eta^{2}}{2 m^{2} n^{-1 / 2} \\max _{j} \\widehat{\\sigma}_{j}^{2}+2 m n^{-1 / 2} B_{n} \\eta / 3}\\right) .\n\\]\nTheorem \\(6.15\\) shows that \\(n^{-1 / 2} B_{n}=o_{p}(1)\\). Thus the expression in the denominator of the parentheses in (10.38) is \\(o_{p}\\) (1) as \\(n \\rightarrow \\infty\\), . It follows that for \\(n\\) sufficiently large (10.38) is \\(O_{p}\\left(\\exp \\left(-n^{1 / 2}\\right)\\right)\\). Hence the second term in (10.37) is \\(O_{p}\\left(\\exp \\left(-n^{1 / 2}\\right)\\right) o_{p}\\left(\\exp \\left(-n^{1 / 2}\\right)\\right)=o_{p}(1)\\) by the assumption on \\(\\tau_{n}\\).\nWe have shown that the two terms in (10.37) are each \\(O_{p}(1)\\). This completes the proof."
  },
  {
    "objectID": "chpt10-resample-method.html#exercises",
    "href": "chpt10-resample-method.html#exercises",
    "title": "10  Resampling Methods",
    "section": "10.32 Exercises",
    "text": "10.32 Exercises\nExercise 10.1 Find the jackknife estimator of variance of the estimator \\(\\widehat{\\mu}_{r}=n^{-1} \\sum_{i=1}^{n} Y_{i}^{r}\\) for \\(\\mu_{r}=\\mathbb{E}\\left[Y_{i}^{r}\\right]\\).\nExercise 10.2 Show that if the jackknife estimator of variance of \\(\\widehat{\\beta}\\) is \\(\\widehat{V}_{\\widehat{\\beta}}^{\\text {jack }}\\), then the jackknife estimator of variance of \\(\\widehat{\\theta}=\\boldsymbol{a}+\\boldsymbol{C} \\widehat{\\beta}\\) is \\(\\widehat{\\boldsymbol{V}}_{\\widehat{\\theta}}^{\\mathrm{jack}}=\\boldsymbol{C} \\widehat{\\boldsymbol{V}}_{\\widehat{\\beta}}^{\\mathrm{jack}} \\boldsymbol{C}^{\\prime}\\).\nExercise 10.3 A two-step estimator such as (12.49) is \\(\\widehat{\\beta}=\\left(\\sum_{i=1}^{n} \\widehat{W}_{i} \\widehat{W}_{i}^{\\prime}\\right)^{-1}\\left(\\sum_{i=1}^{n} \\widehat{W}_{i} Y_{i}\\right)\\) where \\(\\widehat{W}_{i}=\\widehat{A}^{\\prime} Z_{i}\\) and \\(\\widehat{\\boldsymbol{A}}=\\left(\\boldsymbol{Z}^{\\prime} \\boldsymbol{Z}\\right)^{-1} \\boldsymbol{Z}^{\\prime} \\boldsymbol{X}\\). Describe how to construct the jackknife estimator of variance of \\(\\widehat{\\beta}\\).\nExercise 10.4 Show that if the bootstrap estimator of variance of \\(\\widehat{\\beta}\\) is \\(\\widehat{V}_{\\widehat{\\beta}}^{\\text {boot }}\\), then the bootstrap estimator of variance of \\(\\widehat{\\theta}=\\boldsymbol{a}+\\boldsymbol{C} \\widehat{\\beta}\\) is \\(\\widehat{\\boldsymbol{V}}_{\\widehat{\\theta}}^{\\text {boot }}=\\boldsymbol{C} \\widehat{\\boldsymbol{V}}_{\\widehat{\\beta}}^{\\text {boot }} \\boldsymbol{C}^{\\prime}\\).\nExercise \\(10.5\\) Show that if the percentile interval for \\(\\beta\\) is \\([L, U]\\) then the percentile interval for \\(a+c \\beta\\) is \\([a+c L, a+c U]\\).\nExercise \\(10.6\\) Consider the following bootstrap procedure. Using the nonparametric bootstrap, generate bootstrap samples, calculate the estimate \\(\\widehat{\\theta}^{*}\\) on these samples and then calculate\n\\[\nT^{*}=\\left(\\widehat{\\theta}^{*}-\\widehat{\\theta}\\right) / s(\\widehat{\\theta}),\n\\]\nwhere \\(s(\\hat{\\theta})\\) is the standard error in the original data. Let \\(q_{\\alpha / 2}^{*}\\) and \\(q_{1-\\alpha / 2}^{*}\\) denote the \\(\\alpha / 2^{t h}\\) and \\(1-\\alpha / 2^{t h}\\) quantiles of \\(T^{*}\\), and define the bootstrap confidence interval\n\\[\nC=\\left[\\widehat{\\theta}+s(\\widehat{\\theta}) q_{\\alpha / 2}^{*}, \\quad \\widehat{\\theta}+s(\\widehat{\\theta}) q_{1-\\alpha / 2}^{*}\\right] .\n\\]\nShow that \\(C\\) exactly equals the percentile interval. Exercise \\(10.7\\) Prove Theorem 10.6.\nExercise \\(10.8\\) Prove Theorem 10.7.\nExercise \\(10.9\\) Prove Theorem 10.8.\nExercise \\(10.10\\) Let \\(Y_{i}\\) be i.i.d., \\(\\mu=\\mathbb{E}[Y]>0\\), and \\(\\theta=\\mu^{-1}\\). Let \\(\\widehat{\\mu}=\\bar{Y}_{n}\\) be the sample mean and \\(\\widehat{\\theta}=\\widehat{\\mu}^{-1}\\).\n\nIs \\(\\hat{\\theta}\\) unbiased for \\(\\theta\\) ?\nIf \\(\\widehat{\\theta}\\) is biased, can you determine the direction of the bias \\(\\mathbb{E}[\\widehat{\\theta}-\\theta]\\) (up or down)?\nIs the percentile interval appropriate in this context for confidence interval construction?\n\nExercise \\(10.11\\) Consider the following bootstrap procedure for a regression of \\(Y\\) on \\(X\\). Let \\(\\widehat{\\beta}\\) denote the OLS estimator and \\(\\widehat{e}_{i}=Y_{i}-X_{i}^{\\prime} \\widehat{\\beta}\\) the OLS residuals.\n\nDraw a random vector \\(\\left(X^{*}, e^{*}\\right)\\) from the pair \\(\\left\\{\\left(X_{i}, \\widehat{e}_{i}\\right): i=1, \\ldots, n\\right\\}\\). That is, draw a random integer \\(i^{\\prime}\\) from \\([1,2, \\ldots, n]\\), and set \\(X^{*}=X_{i^{\\prime}}\\) and \\(e^{*}=\\widehat{e}_{i^{\\prime}}\\). Set \\(Y^{*}=X^{* \\prime} \\widehat{\\beta}+e^{*}\\). Draw (with replacement) \\(n\\) such vectors, creating a random bootstrap data set \\(\\left(\\boldsymbol{Y}^{*}, \\boldsymbol{X}^{*}\\right)\\).\nRegress \\(\\boldsymbol{Y}^{*}\\) on \\(\\boldsymbol{X}^{*}\\), yielding OLS estimator \\(\\widehat{\\beta}^{*}\\) and any other statistic of interest.\n\nShow that this bootstrap procedure is (numerically) identical to the nonparametric bootstrap.\nExercise \\(10.12\\) Take \\(p^{*}\\) as defined in (10.22) for the BC percentile interval. Show that it is invariant to replacing \\(\\theta\\) with \\(g(\\theta)\\) for any strictly monotonically increasing transformation \\(g(\\theta)\\). Does this extend to \\(z_{0}^{*}\\) as defined in (10.23)?\nExercise \\(10.13\\) Show that if the percentile-t interval for \\(\\beta\\) is \\([L, U]\\) then the percentile-t interval for \\(a+c \\beta\\) is \\([a+b L, a+b U]\\).\nExercise 10.14 You want to test \\(\\mathbb{M}_{0}: \\theta=0\\) against \\(\\mathbb{M}_{1}: \\theta>0\\). The test for \\(\\mathbb{M}_{0}\\) is to reject if \\(T_{n}=\\widehat{\\theta} / s(\\widehat{\\theta})>c\\) where \\(c\\) is picked so that Type I error is \\(\\alpha\\). You do this as follows. Using the nonparametric bootstrap, you generate bootstrap samples, calculate the estimates \\(\\widehat{\\theta}^{*}\\) on these samples and then calculate \\(T^{*}=\\) \\(\\widehat{\\theta}^{*} / s\\left(\\widehat{\\theta}^{*}\\right)\\). Let \\(q_{1-\\alpha}^{*}\\) denote the \\(1-\\alpha^{t h}\\) quantile of \\(T^{*}\\). You replace \\(c\\) with \\(q_{1-\\alpha}^{*}\\), and thus reject \\(\\mathbb{H}_{0}\\) if \\(T_{n}=\\widehat{\\theta} / s(\\widehat{\\theta})>q_{1-\\alpha}^{*}\\). What is wrong with this procedure?\nExercise 10.15 Suppose that in an application, \\(\\widehat{\\theta}=1.2\\) and \\(s(\\widehat{\\theta})=0.2\\). Using the nonparametric bootstrap, 1000 samples are generated from the bootstrap distribution, and \\(\\widehat{\\theta}^{*}\\) is calculated on each sample. The \\(\\widehat{\\theta}^{*}\\) are sorted, and the \\(0.025^{t h}\\) and \\(0.975^{t h}\\) quantiles of the \\(\\widehat{\\theta}^{*}\\) are \\(.75\\) and \\(1.3\\), respectively.\n\nReport the \\(95 %\\) percentile interval for \\(\\theta\\).\nWith the given information, can you calculate the 95% BC percentile interval or percentile-t interval for \\(\\theta\\) ?\n\nExercise 10.16 Take the normal regression model \\(Y=X^{\\prime} \\beta+e\\) with \\(e \\mid X \\sim \\mathrm{N}\\left(0, \\sigma^{2}\\right)\\) where we know the MLE equals the least squares estimators \\(\\widehat{\\beta}\\) and \\(\\widehat{\\sigma}^{2}\\).\n\nDescribe the parametric regression bootstrap for this model. Show that the conditional distribution of the bootstrap observations is \\(Y_{i}^{*} \\mid F_{n} \\sim \\mathrm{N}\\left(X_{i}^{\\prime} \\widehat{\\beta}, \\widehat{\\sigma}^{2}\\right)\\). (b) Show that the distribution of the bootstrap least squares estimator is \\(\\widehat{\\beta}^{*} \\mid F_{n} \\sim \\mathrm{N}\\left(\\widehat{\\beta},\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\widehat{\\sigma}^{2}\\right)\\).\nShow that the distribution of the bootstrap t-ratio with a homoskedastic standard error is \\(T^{*} \\sim\\) \\(t_{n-k}\\).\n\nExercise \\(10.17\\) Consider the model \\(Y=X^{\\prime} \\beta+e\\) with \\(\\mathbb{E}[e \\mid X]=0, Y\\) scalar, and \\(X\\) a \\(k\\) vector. You have a random sample \\(\\left(Y_{i}, X_{i}: i=1, \\ldots, n\\right)\\). You are interested in estimating the regression function \\(m(x)=\\) \\(\\mathbb{E}[Y \\mid X=x]\\) at a fixed vector \\(x\\) and constructing a \\(95 %\\) confidence interval.\n\nWrite down the standard estimator and asymptotic confidence interval for \\(m(x)\\).\nDescribe the percentile bootstrap confidence interval for \\(m(x)\\).\nDescribe the percentile-t bootstrap confidence interval for \\(m(x)\\).\n\nExercise 10.18 The observed data is \\(\\left\\{Y_{i}, X_{i}\\right\\} \\in \\mathbb{R} \\times \\mathbb{R}^{k}, k>1, i=1, \\ldots, n\\). Take the model \\(Y=X^{\\prime} \\beta+e\\) with \\(\\mathbb{E}[X e]=0 .\\)\n\nWrite down an estimator for \\(\\mu_{3}=\\mathbb{E}\\left[e^{3}\\right]\\).\nExplain how to use the percentile method to construct a 90% confidence interval for \\(\\mu_{3}\\) in this specific model.\n\nExercise \\(10.19\\) Take the model \\(Y=X^{\\prime} \\beta+e\\) with \\(\\mathbb{E}[X e]=0\\). Describe the bootstrap percentile confidence interval for \\(\\sigma^{2}=\\mathbb{E}\\left[e^{2}\\right]\\).\nExercise 10.20 The model is \\(Y=X_{1}^{\\prime} \\beta_{1}+X_{2}^{\\prime} \\beta_{2}+e\\) with \\(\\mathbb{E}[X e]=0\\) and \\(X_{2}\\) scalar. Describe how to test \\(\\mathbb{H}_{0}: \\beta_{2}=0\\) against \\(\\mathbb{H}_{1}: \\beta_{2} \\neq 0\\) using the nonparametric bootstrap.\nExercise 10.21 The model is \\(Y=X_{1}^{\\prime} \\beta_{1}+X_{2}^{\\prime} \\beta_{2}+e\\) with \\(\\mathbb{E}[X e]=0\\), and both \\(X_{1}\\) and \\(X_{2} k \\times 1\\). Describe how to test \\(\\mathbb{M}_{0}: \\beta_{1}=\\beta_{2}\\) against \\(\\mathbb{M}_{1}: \\beta_{1} \\neq \\beta_{2}\\) using the nonparametric bootstrap.\nExercise 10.22 Suppose a Ph.D. student has a sample \\(\\left(Y_{i}, X_{i}, Z_{i}: i=1, \\ldots, n\\right)\\) and estimates by OLS the equation \\(Y=Z \\alpha+X^{\\prime} \\beta+e\\) where \\(\\alpha\\) is the coefficient of interest. She is interested in testing \\(\\mathbb{H}_{0}: \\alpha=0\\) against \\(\\mathbb{H}_{1}: \\alpha \\neq 0\\). She obtains \\(\\widehat{\\alpha}=2.0\\) with standard error \\(s(\\widehat{\\alpha})=1.0\\) so the value of the t-ratio for \\(\\mathbb{H}_{0}\\) is \\(T=\\widehat{\\alpha} / s(\\widehat{\\alpha})=2.0\\). To assess significance, the student decides to use the bootstrap. She uses the following algorithm\n\nSamples \\(\\left(Y_{i}^{*}, X_{i}^{*}, Z_{i}^{*}\\right)\\) randomly from the observations. (Random sampling with replacement). Creates a random sample with \\(n\\) observations.\nOn this pseudo-sample, estimates the equation \\(Y_{i}^{*}=Z_{i}^{*} \\alpha+X_{i}^{* \\prime} \\beta+e_{i}^{*}\\) by OLS and computes standard errors, including \\(s\\left(\\widehat{\\alpha}^{*}\\right)\\). The t-ratio for \\(\\mathbb{H}_{0}, T^{*}=\\widehat{\\alpha}^{*} / s\\left(\\widehat{\\alpha}^{*}\\right)\\) is computed and stored.\nThis is repeated \\(B=10,000\\) times.\nThe \\(0.95^{t h}\\) empirical quantile \\(q_{.95}^{*}=3.5\\) of the bootstrap absolute t-ratios \\(\\left|T^{*}\\right|\\) is computed.\nThe student notes that while \\(|T|=2>1.96\\) (and thus an asymptotic \\(5 %\\) size test rejects \\(\\mathbb{M}_{0}\\) ), \\(|T|=\\) \\(2<q_{.95}^{*}=3.5\\) and thus the bootstrap test does not reject \\(\\mathbb{M}_{0}\\). As the bootstrap is more reliable, the student concludes that \\(\\mathbb{M}_{0}\\) cannot be rejected in favor of \\(\\mathbb{H}_{1}\\). Question: Do you agree with the student’s method and reasoning? Do you see an error in her method?\n\nExercise 10.23 Take the model \\(Y=X_{1} \\beta_{1}+X_{2} \\beta_{2}+e\\) with \\(\\mathbb{E}[X e]=0\\) and scalar \\(X_{1}\\) and \\(X_{2}\\). The parameter of interest is \\(\\theta=\\beta_{1} \\beta_{2}\\). Show how to construct a confidence interval for \\(\\theta\\) using the following three methods.\n\nAsymptotic Theory.\nPercentile Bootstrap.\nPercentile-t Bootstrap.\n\nYour answer should be specific to this problem, not general.\nExercise 10.24 Take the model \\(Y=X_{1} \\beta_{1}+X_{2} \\beta_{2}+e\\) with i.i.d observations, \\(\\mathbb{E}[X e]=0\\) and scalar \\(X_{1}\\) and \\(X_{2}\\). Describe how you would construct the percentile-t bootstrap confidence interval for \\(\\theta=\\beta_{1} / \\beta_{2}\\).\nExercise 10.25 The model is i.i.d. data, \\(i=1, \\ldots, n, Y=X^{\\prime} \\beta+e\\) and \\(\\mathbb{E}[e \\mid X]=0\\). Does the presence of conditional heteroskedasticity invalidate the application of the nonparametric bootstrap? Explain.\nExercise 10.26 The RESET specification test for nonlinearity in a random sample (due to Ramsey (1969)) is the following. The null hypothesis is a linear regression \\(Y=X^{\\prime} \\beta+e\\) with \\(\\mathbb{E}[e \\mid X]=0\\). The parameter \\(\\beta\\) is estimated by OLS yielding predicted values \\(\\widehat{Y}_{i}\\). Then a second-stage least squares regression is estimated including both \\(X_{i}\\) and \\(\\widehat{Y}_{i}\\)\n\\[\nY_{i}=X_{i}^{\\prime} \\widetilde{\\beta}+\\left(\\widehat{Y}_{i}\\right)^{2} \\widetilde{\\gamma}+\\widetilde{e}_{i}\n\\]\nThe RESET test statistic \\(R\\) is the squared t-ratio on \\(\\widetilde{\\gamma}\\).\nA colleague suggests obtaining the critical value for the test using the bootstrap. He proposes the following bootstrap implementation.\n\nDraw \\(n\\) observations \\(\\left(Y_{i}^{*}, X_{i}^{*}\\right)\\) randomly from the observed sample pairs \\(\\left(Y_{i}, X_{i}\\right)\\) to create a bootstrap sample.\nCompute the statistic \\(R^{*}\\) on this bootstrap sample as described above.\nRepeat this \\(B\\) times. Sort the bootstrap statistics \\(R^{*}\\), take the \\(0.95^{t h}\\) quantile and use this as the critical value.\nReject the null hypothesis if \\(R\\) exceeds this critical value, otherwise do not reject.\n\nIs this procedure a correct implementation of the bootstrap in this context? If not, propose a modification.\nExercise 10.27 The model is \\(Y=X^{\\prime} \\beta+e\\) with \\(\\mathbb{E}[X e] \\neq 0\\). We know that in this case, the least squares estimator may be biased for the parameter \\(\\beta\\). We also know that the nonparametric BC percentile interval is (generally) a good method for confidence interval construction in the presence of bias. Explain whether or not you expect the BC percentile interval applied to the least squares estimator will have accurate coverage in this context.\nExercise 10.28 In Exercise 9.26 you estimated a cost function for 145 electric companies and tested the restriction \\(\\theta=\\beta_{3}+\\beta_{4}+\\beta_{5}=1\\). (a) Estimate the regression by unrestricted least squares and report standard errors calculated by asymptotic, jackknife and the bootstrap.\n\nEstimate \\(\\theta=\\beta_{3}+\\beta_{4}+\\beta_{5}\\) and report standard errors calculated by asymptotic, jackknife and the bootstrap.\nReport confidence intervals for \\(\\theta\\) using the percentile and \\(\\mathrm{BC}_{a}\\) methods.\n\nExercise 10.29 In Exercise 9.27 you estimated the Mankiw, Romer, and Weil (1992) unrestricted regression. Let \\(\\theta\\) be the sum of the second, third, and fourth coefficients.\n\nEstimate the regression by unrestricted least squares and report standard errors calculated by asymptotic, jackknife and the bootstrap.\nEstimate \\(\\theta\\) and report standard errors calculated by asymptotic, jackknife and the bootstrap.\nReport confidence intervals for \\(\\theta\\) using the percentile and BC methods.\n\nExercise 10.30 In Exercise \\(7.28\\) you estimated a wage regression with the cps09mar dataset and the subsample of white Male Hispanics. Further restrict the sample to those never-married and live in the Midwest region. (This sample has 99 observations.) As in subquestion (b) let \\(\\theta\\) be the ratio of the return to one year of education to the return of one year of experience.\n\nEstimate \\(\\theta\\) and report standard errors calculated by asymptotic, jackknife and the bootstrap.\nExplain the discrepancy between the standard errors.\nReport confidence intervals for \\(\\theta\\) using the BC percentile method.\n\nExercise 10.31 In Exercise \\(4.26\\) you extended the work from Duflo, Dupas, and Kremer (2011). Repeat that regression, now calculating the standard error by cluster bootstrap. Report a \\(\\mathrm{BC}_{a}\\) confidence interval for each coefficient."
  },
  {
    "objectID": "chpt11-multi-reg.html#introduction",
    "href": "chpt11-multi-reg.html#introduction",
    "title": "11  Multivariate Regression",
    "section": "11.1 Introduction",
    "text": "11.1 Introduction\nMultivariate regression is a system of regression equations. Multivariate regression is used as reduced form models for instrumental variable estimation (Chaper 12), vector autoregressions (Chapter 15), demand systems (demand for multiple goods), and other contexts.\nMultivariate regression is also called by the name systems of regression equations. Closely related is the method of Seemingly Unrelated Regressions (SUR) introduced in Section 11.7.\nMost of the tools of single equation regression generalize to multivariate regression. A major difference is a new set of notation to handle matrix estimators."
  },
  {
    "objectID": "chpt11-multi-reg.html#regression-systems",
    "href": "chpt11-multi-reg.html#regression-systems",
    "title": "11  Multivariate Regression",
    "section": "11.2 Regression Systems",
    "text": "11.2 Regression Systems\nA univariate linear regression equation equals \\(Y=X^{\\prime} \\beta+e\\) where \\(Y\\) is scalar and \\(X\\) is a vector. Multivariate regression is a system of \\(m\\) linear regressions, and equals\n\\[\nY_{j}=X_{j}^{\\prime} \\beta_{j}+e_{j}\n\\]\nfor \\(j=1, \\ldots, m\\). Here we use the subscript \\(j\\) to denote the \\(j^{t h}\\) dependent variable, not the \\(i^{t h}\\) individual. As an example, \\(Y_{j}\\) could be expenditures by a household on good category \\(j\\) (e.g., food, housing, transportation, clothing, recreation). The regressor vectors \\(X_{j}\\) are \\(k_{j} \\times 1\\) and \\(e_{j}\\) is an error. The coefficient vectors \\(\\beta_{j}\\) are \\(k_{j} \\times 1\\). The total number of coefficients are \\(\\bar{k}=\\sum_{j=1}^{m} k_{j}\\). The regressors can be common across \\(j\\) or can vary across \\(j\\). In the household expenditure example the regressors \\(X_{j}\\) are typically common across \\(j\\), and include variables such as household income, number and ages of family members, and demographic characteristics. The regression system specializes to univariate regression when \\(m=1\\).\nDefine the \\(m \\times 1\\) error vector \\(e=\\left(e_{1}, \\ldots, e_{m}\\right)^{\\prime}\\) and its \\(m \\times m\\) covariance matrix \\(\\Sigma=\\mathbb{E}\\left[e e^{\\prime}\\right]\\). The diagonal elements are the variances of the errors \\(e_{j}\\) and the off-diagonals are the covariances across variables.\nWe can group the \\(m\\) equations (11.1) into a single equation as follows. Let \\(Y=\\left(Y_{1}, \\ldots, Y_{m}\\right)^{\\prime}\\) be the \\(m \\times 1\\) vector of dependent variables. Define the \\(m \\times \\bar{k}\\) matrix of regressors\n\\[\n\\bar{X}=\\left(\\begin{array}{cccc}\nX_{1}^{\\prime} & 0 & \\cdots & 0 \\\\\n\\vdots & X_{2}^{\\prime} & & \\vdots \\\\\n0 & 0 & \\cdots & X_{m}^{\\prime}\n\\end{array}\\right)\n\\]\nand the \\(\\bar{k} \\times 1\\) stacked coefficient vector\n\\[\n\\beta=\\left(\\begin{array}{c}\n\\beta_{1} \\\\\n\\vdots \\\\\n\\beta_{m}\n\\end{array}\\right)\n\\]\nThe \\(m\\) regression equations can be jointly written as\n\\[\nY=\\bar{X} \\beta+e .\n\\]\nThis is a system of \\(m\\) equations.\nFor \\(n\\) observations the joint system can be written in matrix notation by stacking. Define\n\\[\n\\boldsymbol{Y}=\\left(\\begin{array}{c}\nY_{1} \\\\\n\\vdots \\\\\nY_{n}\n\\end{array}\\right), \\quad \\boldsymbol{e}=\\left(\\begin{array}{c}\ne_{1} \\\\\n\\vdots \\\\\ne_{n}\n\\end{array}\\right), \\quad \\overline{\\boldsymbol{X}}=\\left(\\begin{array}{c}\n\\bar{X}_{1} \\\\\n\\vdots \\\\\n\\bar{X}_{n}\n\\end{array}\\right)\n\\]\nwhich are \\(m n \\times 1, m n \\times 1\\), and \\(m n \\times \\bar{k}\\), respectively. The system can be written as \\(\\boldsymbol{Y}=\\overline{\\boldsymbol{X}} \\beta+\\boldsymbol{e}\\).\nIn many applications the regressor vectors \\(X_{j}\\) are common across the variables \\(j\\), so \\(X_{j}=X\\) and \\(k_{j}=k\\). By this we mean that the same variables enter each equation with no exclusion restrictions. Several important simplifications occur in this context. One is that we can write (11.2) using the notation\n\\[\nY=\\boldsymbol{B}^{\\prime} X+e\n\\]\nwhere \\(\\boldsymbol{B}=\\left(\\beta_{1}, \\beta_{2}, \\cdots, \\beta_{m}\\right)\\) is \\(k \\times m\\). Another is that we can write the joint system of observations in the \\(n \\times m\\) matrix notation \\(\\boldsymbol{Y}=\\boldsymbol{X} \\boldsymbol{B}+\\boldsymbol{E}\\) where\n\\[\n\\boldsymbol{Y}=\\left(\\begin{array}{c}\nY_{1}^{\\prime} \\\\\n\\vdots \\\\\nY_{n}^{\\prime}\n\\end{array}\\right), \\quad \\boldsymbol{E}=\\left(\\begin{array}{c}\ne_{1}^{\\prime} \\\\\n\\vdots \\\\\ne_{n}^{\\prime}\n\\end{array}\\right), \\quad \\boldsymbol{X}=\\left(\\begin{array}{c}\nX_{1}^{\\prime} \\\\\n\\vdots \\\\\nX_{n}^{\\prime}\n\\end{array}\\right)\n\\]\nAnother convenient implication of common regressors is that we have the simplification\n\\[\n\\bar{X}=\\left(\\begin{array}{cccc}\nX^{\\prime} & 0 & \\cdots & 0 \\\\\n0 & X^{\\prime} & & 0 \\\\\n\\vdots & \\vdots & & \\vdots \\\\\n0 & 0 & \\cdots & X^{\\prime}\n\\end{array}\\right)=\\boldsymbol{I}_{m} \\otimes X^{\\prime}\n\\]\nwhere \\(\\otimes\\) is the Kronecker product (see Appendix A.21)."
  },
  {
    "objectID": "chpt11-multi-reg.html#least-squares-estimator",
    "href": "chpt11-multi-reg.html#least-squares-estimator",
    "title": "11  Multivariate Regression",
    "section": "11.3 Least Squares Estimator",
    "text": "11.3 Least Squares Estimator\nThe equations (11.1) can be estimated by least squares. This takes the form\n\\[\n\\widehat{\\beta}_{j}=\\left(\\sum_{i=1}^{n} X_{j i} X_{j i}^{\\prime}\\right)^{-1}\\left(\\sum_{i=1}^{n} X_{j i} Y_{j i}\\right) .\n\\]\nAn estimator of \\(\\beta\\) is the stacked vector\n\\[\n\\widehat{\\beta}=\\left(\\begin{array}{c}\n\\widehat{\\beta}_{1} \\\\\n\\vdots \\\\\n\\widehat{\\beta}_{m}\n\\end{array}\\right) .\n\\]\nWe can alternatively write this estimator using the systems notation\n\\[\n\\widehat{\\beta}=\\left(\\overline{\\boldsymbol{X}}^{\\prime} \\overline{\\boldsymbol{X}}\\right)^{-1}\\left(\\overline{\\boldsymbol{X}}^{\\prime} \\boldsymbol{Y}\\right)=\\left(\\sum_{i=1}^{n} \\bar{X}_{i}^{\\prime} \\bar{X}_{i}\\right)^{-1}\\left(\\sum_{i=1}^{n} \\bar{X}_{i}^{\\prime} Y_{i}\\right)\n\\]\nTo see this, observe that\n\\[\n\\begin{aligned}\n\\overline{\\boldsymbol{X}}^{\\prime} \\overline{\\boldsymbol{X}} &=\\left(\\begin{array}{ccc}\n\\bar{X}_{1}^{\\prime} & \\cdots & \\bar{X}_{n}^{\\prime}\n\\end{array}\\right)\\left(\\begin{array}{c}\n\\bar{X}_{1} \\\\\n\\vdots \\\\\n\\bar{X}_{n}\n\\end{array}\\right) \\\\\n&=\\sum_{i=1}^{n} \\bar{X}_{i}^{\\prime} \\bar{X}_{i} \\\\\n&=\\sum_{i=1}^{n}\\left(\\begin{array}{cccc}\nX_{1 i} & 0 & \\cdots & 0 \\\\\n\\vdots & X_{2 i} & & \\vdots \\\\\n0 & 0 & \\cdots & X_{m i}\n\\end{array}\\right)\\left(\\begin{array}{ccccc}\nX_{1 i}^{\\prime} & 0 & \\cdots & 0 \\\\\n\\vdots & X_{2 i}^{\\prime} & & \\vdots \\\\\n0 & 0 & \\cdots & X_{m i}^{\\prime}\n\\end{array}\\right) \\\\\n&=\\left(\\begin{array}{cccccc}\n\\sum_{i=1}^{n} X_{1 i} X_{1 i}^{\\prime} & & 0 \\\\\n\\vdots & & \\sum_{i=1}^{n} X_{2 i} X_{2 i}^{\\prime} & & & \\\\\n0 & & 0 & \\cdots & \\sum_{i=1}^{n} X_{m i} X_{m i}^{\\prime}\n\\end{array}\\right)\n\\end{aligned}\n\\]\nand\n\\[\n\\begin{aligned}\n\\overline{\\boldsymbol{X}}^{\\prime} \\boldsymbol{Y} &=\\left(\\begin{array}{ccc}\n\\bar{X}_{1}^{\\prime} & \\cdots & \\bar{X}_{n}^{\\prime}\n\\end{array}\\right)\\left(\\begin{array}{c}\nY_{1} \\\\\n\\vdots \\\\\nY_{n}\n\\end{array}\\right) \\\\\n&=\\sum_{i=1}^{n} \\bar{X}_{i}^{\\prime} Y_{i} \\\\\n&=\\sum_{i=1}^{n}\\left(\\begin{array}{cccc}\nX_{1 i} & 0 & \\cdots & 0 \\\\\n\\vdots & X_{2 i} & & \\vdots \\\\\n0 & 0 & \\cdots & X_{m i}\n\\end{array}\\right)\\left(\\begin{array}{c}\nY_{1 i} \\\\\n\\vdots \\\\\nY_{m i}\n\\end{array}\\right) \\\\\n&=\\left(\\begin{array}{c}\n\\sum_{i=1}^{n} X_{1 i} Y_{1 i} \\\\\n\\vdots \\\\\n\\sum_{i=1}^{n} X_{m i} Y_{m i}\n\\end{array}\\right) .\n\\end{aligned}\n\\]\nHence\n\\[\n\\begin{aligned}\n\\left(\\overline{\\boldsymbol{X}}^{\\prime} \\overline{\\boldsymbol{X}}\\right)^{-1}\\left(\\overline{\\boldsymbol{X}}^{\\prime} \\boldsymbol{Y}\\right) &=\\left(\\sum_{i=1}^{n} \\bar{X}_{i} \\bar{X}_{i}^{\\prime}\\right)^{-1}\\left(\\sum_{i=1}^{n} \\bar{X}_{i} Y_{i}\\right) \\\\\n&=\\left(\\begin{array}{c}\n\\left(\\sum_{i=1}^{n} X_{1 i} X_{1 i}^{\\prime}\\right)^{-1}\\left(\\sum_{i=1}^{n} X_{1 i} Y_{1 i}\\right) \\\\\n\\vdots \\\\\n\\left(\\sum_{i=1}^{n} X_{m i} X_{m i}^{\\prime}\\right)^{-1}\\left(\\sum_{i=1}^{n} X_{m i} Y_{m i}\\right)\n\\end{array}\\right) \\\\\n&=\\widehat{\\beta}\n\\end{aligned}\n\\]\nas claimed. The \\(m \\times 1\\) residual vector for the \\(i^{t h}\\) observation is \\(\\widehat{e}_{i}=Y_{i}-\\overline{\\boldsymbol{X}}_{i}^{\\prime} \\widehat{\\beta}\\). The least squares estimator of the \\(m \\times m\\) error covariance matrix is\n\\[\n\\widehat{\\Sigma}=\\frac{1}{n} \\sum_{i=1}^{n} \\widehat{e}_{i} \\widehat{e}_{i}^{\\prime} .\n\\]\nIn the case of common regressors, the least squares coefficients can be written as\n\\[\n\\widehat{\\beta}_{j}=\\left(\\sum_{i=1}^{n} X_{i} X_{i}^{\\prime}\\right)^{-1}\\left(\\sum_{i=1}^{n} X_{i} Y_{j i}\\right)\n\\]\nand\n\\[\n\\widehat{\\boldsymbol{B}}=\\left(\\widehat{\\beta}_{1}, \\widehat{\\beta}_{2}, \\cdots, \\widehat{\\beta}_{m}\\right)=\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{Y}\\right) .\n\\]\nIn Stata, multivariate regression can be implemented using the mvreg command."
  },
  {
    "objectID": "chpt11-multi-reg.html#expectation-and-variance-of-systems-least-squares",
    "href": "chpt11-multi-reg.html#expectation-and-variance-of-systems-least-squares",
    "title": "11  Multivariate Regression",
    "section": "11.4 Expectation and Variance of Systems Least Squares",
    "text": "11.4 Expectation and Variance of Systems Least Squares\nWe can calculate the finite-sample expectation and variance of \\(\\widehat{\\beta}\\) under the conditional expectation assumption\n\\[\n\\mathbb{E}[e \\mid X]=0\n\\]\nwhere \\(X\\) is the union of the regressors \\(X_{j}\\). Equation (11.7) is equivalent to \\(\\mathbb{E}\\left\\lfloor Y_{j} \\mid X\\right\\rfloor=X_{j}^{\\prime} \\beta_{j}\\), which means that the regression model is correctly specified.\nWe can center the estimator as\n\\[\n\\widehat{\\beta}-\\beta=\\left(\\overline{\\boldsymbol{X}}^{\\prime} \\overline{\\boldsymbol{X}}\\right)^{-1}\\left(\\overline{\\boldsymbol{X}}^{\\prime} \\boldsymbol{e}\\right)=\\left(\\sum_{i=1}^{n} \\bar{X}_{i}^{\\prime} \\bar{X}_{i}\\right)^{-1}\\left(\\sum_{i=1}^{n} \\bar{X}_{i}^{\\prime} e_{i}\\right)\n\\]\nTaking conditional expectations we find \\(\\mathbb{E}[\\widehat{\\beta} \\mid \\boldsymbol{X}]=\\beta\\). Consequently, systems least squares is unbiased under correct specification.\nTo compute the variance of the estimator, define the conditional covariance matrix of the errors of the \\(i^{t h}\\) observation \\(\\mathbb{E}\\left[e_{i} e_{i}^{\\prime} \\mid X_{i}\\right]=\\Sigma_{i}\\) which in general is a function of \\(X_{i}\\). If the observations are mutually independent then\n\\[\n\\mathbb{E}\\left[\\boldsymbol{e} \\boldsymbol{e}^{\\prime} \\mid \\boldsymbol{X}\\right]=\\mathbb{E}\\left[\\left(\\begin{array}{cccc}\ne_{1} e_{1}^{\\prime} & e_{1} e_{2}^{\\prime} & \\cdots & e_{1} e_{n}^{\\prime} \\\\\n\\vdots & \\ddots & & \\vdots \\\\\ne_{n} e_{1}^{\\prime} & e_{n} e_{2}^{\\prime} & \\cdots & e_{n} e_{n}^{\\prime}\n\\end{array}\\right) \\mid \\boldsymbol{X}\\right]=\\left(\\begin{array}{cccc}\n\\Sigma_{1} & 0 & \\cdots & 0 \\\\\n\\vdots & \\ddots & & \\vdots \\\\\n0 & 0 & \\cdots & \\Sigma_{n}\n\\end{array}\\right) \\text {. }\n\\]\nAlso, by independence across observations,\n\\[\n\\operatorname{var}\\left[\\sum_{i=1}^{n} \\bar{X}_{i}^{\\prime} e_{i} \\mid \\boldsymbol{X}\\right]=\\sum_{i=1}^{n} \\operatorname{var}\\left[\\bar{X}_{i}^{\\prime} e_{i} \\mid X_{i}\\right]=\\sum_{i=1}^{n} \\bar{X}_{i}^{\\prime} \\Sigma_{i} \\bar{X}_{i} .\n\\]\nIt follows that\n\\[\n\\operatorname{var}[\\widehat{\\beta} \\mid \\boldsymbol{X}]=\\left(\\overline{\\boldsymbol{X}}^{\\prime} \\overline{\\boldsymbol{X}}\\right)^{-1}\\left(\\sum_{i=1}^{n} \\bar{X}_{i}^{\\prime} \\Sigma_{i} \\bar{X}_{i}\\right)\\left(\\overline{\\boldsymbol{X}}^{\\prime} \\overline{\\boldsymbol{X}}\\right)^{-1} .\n\\]\nWhen the regressors are common so that \\(\\bar{X}_{i}=\\boldsymbol{I}_{m} \\otimes X_{i}^{\\prime}\\) then the covariance matrix can be written as\n\\[\n\\operatorname{var}[\\widehat{\\beta} \\mid \\boldsymbol{X}]=\\left(\\boldsymbol{I}_{m} \\otimes\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1}\\right)\\left(\\sum_{i=1}^{n}\\left(\\Sigma_{i} \\otimes X_{i} X_{i}^{\\prime}\\right)\\right)\\left(\\boldsymbol{I}_{m} \\otimes\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1}\\right)\n\\]\nIf the errors are conditionally homoskedastic\n\\[\n\\mathbb{E}\\left[e e^{\\prime} \\mid X\\right]=\\Sigma\n\\]\nthen the covariance matrix simplifies to\n\\[\n\\operatorname{var}[\\widehat{\\beta} \\mid \\boldsymbol{X}]=\\left(\\overline{\\boldsymbol{X}}^{\\prime} \\overline{\\boldsymbol{X}}\\right)^{-1}\\left(\\sum_{i=1}^{n} \\bar{X}_{i}^{\\prime} \\Sigma \\bar{X}_{i}\\right)\\left(\\overline{\\boldsymbol{X}}^{\\prime} \\overline{\\boldsymbol{X}}\\right)^{-1}\n\\]\nIf both simplifications (common regressors and conditional homoskedasticity) hold then we have the considerable simplication\n\\[\n\\operatorname{var}[\\widehat{\\beta} \\mid \\boldsymbol{X}]=\\Sigma \\otimes\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} .\n\\]"
  },
  {
    "objectID": "chpt11-multi-reg.html#asymptotic-distribution",
    "href": "chpt11-multi-reg.html#asymptotic-distribution",
    "title": "11  Multivariate Regression",
    "section": "11.5 Asymptotic Distribution",
    "text": "11.5 Asymptotic Distribution\nFor an asymptotic distribution it is sufficient to consider the equation-by-equation projection model in which case\n\\[\n\\mathbb{E}\\left[X_{j} e_{j}\\right]=0 .\n\\]\nFirst, consider consistency. Since \\(\\widehat{\\beta}_{j}\\) are the standard least squares estimators, they are consistent for the projection coefficients \\(\\beta_{j}\\).\nSecond, consider the asymptotic distribution. Our single equation theory implies that the \\(\\widehat{\\beta}_{j}\\) are asymptotically normal. But this theory does not provide a joint distribution of the \\(\\widehat{\\beta}_{j}\\) across \\(j\\), which we now derive. Since the vector\n\\[\n\\bar{X}_{i}^{\\prime} e_{i}=\\left(\\begin{array}{c}\nX_{1 i} e_{1 i} \\\\\n\\vdots \\\\\nX_{m i} e_{m i}\n\\end{array}\\right)\n\\]\nis i.i.d. across \\(i\\) and mean zero under (11.9), the central limit theorem implies\n\\[\n\\frac{1}{\\sqrt{n}} \\sum_{i=1}^{n} \\bar{X}_{i}^{\\prime} e_{i} \\underset{d}{\\longrightarrow} \\mathrm{N}(0, \\Omega)\n\\]\nwhere\n\\[\n\\Omega=\\mathbb{E}\\left[\\bar{X}_{i}^{\\prime} e_{i} e_{i}^{\\prime} \\bar{X}_{i}\\right]=\\mathbb{E}\\left[\\bar{X}_{i}^{\\prime} \\Sigma_{i} \\bar{X}_{i}\\right] .\n\\]\nThe matrix \\(\\Omega\\) is the covariance matrix of the variables \\(X_{j i} e_{j i}\\) across equations. Under conditional homoskedasticity (11.8) the matrix \\(\\Omega\\) simplifies to\n\\[\n\\Omega=\\mathbb{E}\\left[\\bar{X}_{i}^{\\prime} \\Sigma \\bar{X}_{i}\\right]\n\\]\n(see Exercise 11.1). When the regressors are common it simplies to\n\\[\n\\Omega=\\mathbb{E}\\left[e e^{\\prime} \\otimes X X^{\\prime}\\right]\n\\]\n(see Exercise 11.2). Under both conditions (homoskedasticity and common regressors) it simplifies to\n\\[\n\\Omega=\\Sigma \\otimes \\mathbb{E}\\left[X X^{\\prime}\\right]\n\\]\n(see Exercise 11.3).\nApplied to the centered and normalized estimator we obtain the asymptotic distribution. Theorem 11.1 Under Assumption 7.2, \\(\\sqrt{n}(\\widehat{\\beta}-\\beta) \\underset{d}{\\longrightarrow} \\mathrm{N}\\left(0, \\boldsymbol{V}_{\\beta}\\right)\\) where \\(\\boldsymbol{V}_{\\beta}=\\) \\(Q^{-1} \\Omega Q^{-1}\\) and\n\\[\n\\boldsymbol{Q}=\\mathbb{E}\\left[\\bar{X}^{\\prime} \\bar{X}\\right]=\\left(\\begin{array}{cccc}\n\\mathbb{E}\\left[X_{1} X_{1}^{\\prime}\\right] & 0 & \\cdots & 0 \\\\\n\\vdots & \\ddots & & \\vdots \\\\\n0 & 0 & \\cdots & \\mathbb{E}\\left[X_{m} X_{m}^{\\prime}\\right]\n\\end{array}\\right)\n\\]\nFor a proof, see Exercise 11.4.\nWhen the regressors are common the matrix \\(\\boldsymbol{Q}\\) simplifies as\n\\[\n\\boldsymbol{Q}=\\boldsymbol{I}_{m} \\otimes \\mathbb{E}\\left[X X^{\\prime}\\right]\n\\]\n(See Exercise 11.5).\nIf both the regressors are common and the errors are conditionally homoskedastic (11.8) then we have the simplification\n\\[\n\\boldsymbol{V}_{\\beta}=\\Sigma \\otimes\\left(\\mathbb{E}\\left[X X^{\\prime}\\right]\\right)^{-1}\n\\]\n(see Exercise 11.6).\nSometimes we are interested in parameters \\(\\theta=r\\left(\\beta_{1}, \\ldots, \\beta_{m}\\right)=r(\\beta)\\) which are functions of the coefficients from multiple equations. In this case the least squares estimator of \\(\\theta\\) is \\(\\widehat{\\theta}=r(\\widehat{\\beta})\\). The asymptotic distribution of \\(\\widehat{\\theta}\\) can be obtained from Theorem \\(11.1\\) by the delta method.\nTheorem 11.2 Under Assumptions \\(7.2\\) and \\(7.3, \\sqrt{n}(\\widehat{\\theta}-\\theta) \\underset{d}{\\longrightarrow} \\mathrm{N}\\left(0, \\boldsymbol{V}_{\\theta}\\right)\\) where \\(\\boldsymbol{V}_{\\theta}=\\boldsymbol{R}^{\\prime} \\boldsymbol{V}_{\\beta} \\boldsymbol{R}\\) and \\(\\boldsymbol{R}=\\frac{\\partial}{\\partial \\beta} r(\\beta)^{\\prime} .\\)\nFor a proof, see Exercise 11.7.\nTheorem \\(11.2\\) is an example where multivariate regression is fundamentally distinct from univariate regression. Only by treating least squares as a joint estimator can we obtain a distributional theory for a function of multiple equations. We can thereby construct standard errors, confidence intervals, and hypothesis tests."
  },
  {
    "objectID": "chpt11-multi-reg.html#covariance-matrix-estimation",
    "href": "chpt11-multi-reg.html#covariance-matrix-estimation",
    "title": "11  Multivariate Regression",
    "section": "11.6 Covariance Matrix Estimation",
    "text": "11.6 Covariance Matrix Estimation\nFrom the finite sample and asymptotic theory we can construct appropriate estimators for the variance of \\(\\widehat{\\beta}\\). In the general case we have\n\\[\n\\widehat{\\boldsymbol{V}}_{\\widehat{\\beta}}=\\left(\\overline{\\boldsymbol{X}}^{\\prime} \\overline{\\boldsymbol{X}}\\right)^{-1}\\left(\\sum_{i=1}^{n} \\bar{X}_{i}^{\\prime} \\widehat{e}_{i} \\widehat{e}_{i}^{\\prime} \\bar{X}_{i}\\right)\\left(\\overline{\\boldsymbol{X}}^{\\prime} \\overline{\\boldsymbol{X}}\\right)^{-1} .\n\\]\nUnder conditional homoskedasticity (11.8) an appropriate estimator is\n\\[\n\\widehat{\\boldsymbol{V}}_{\\widehat{\\beta}}^{0}=\\left(\\overline{\\boldsymbol{X}}^{\\prime} \\overline{\\boldsymbol{X}}\\right)^{-1}\\left(\\sum_{i=1}^{n} \\bar{X}_{i}^{\\prime} \\widehat{\\Sigma}_{i}\\right)\\left(\\overline{\\boldsymbol{X}}^{\\prime} \\overline{\\boldsymbol{X}}\\right)^{-1} .\n\\]\nWhen the regressors are common then these estimators equal\n\\[\n\\widehat{\\boldsymbol{V}}_{\\widehat{\\beta}}=\\left(\\boldsymbol{I}_{m} \\otimes\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1}\\right)\\left(\\sum_{i=1}^{n}\\left(\\widehat{e}_{i} \\widehat{e}_{i}^{\\prime} \\otimes X_{i} X_{i}^{\\prime}\\right)\\right)\\left(\\boldsymbol{I}_{m} \\otimes\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1}\\right)\n\\]\nand \\(\\widehat{\\boldsymbol{V}}_{\\widehat{\\beta}}^{0}=\\widehat{\\Sigma} \\otimes\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1}\\), respectively.\nCovariance matrix estimators for \\(\\widehat{\\theta}\\) are found as\n\\[\n\\begin{aligned}\n\\widehat{\\boldsymbol{V}}_{\\widehat{\\theta}} &=\\widehat{\\boldsymbol{R}}^{\\prime} \\widehat{\\boldsymbol{V}}_{\\widehat{\\beta}} \\widehat{\\boldsymbol{R}} \\\\\n\\widehat{\\boldsymbol{V}}_{\\widehat{\\theta}}^{0} &=\\widehat{\\boldsymbol{R}}^{\\prime} \\widehat{\\boldsymbol{V}}_{\\widehat{\\beta}}^{0} \\widehat{\\boldsymbol{R}} \\\\\n\\widehat{\\boldsymbol{R}} &=\\frac{\\partial}{\\partial \\beta} r(\\widehat{\\beta})^{\\prime} .\n\\end{aligned}\n\\]\nTheorem 11.3 Under Assumption 7.2, \\(n \\widehat{\\boldsymbol{V}}_{\\widehat{\\beta}} \\underset{p}{\\rightarrow} \\boldsymbol{V}_{\\beta}\\) and \\(n \\widehat{\\boldsymbol{V}}_{\\widehat{\\beta}}^{0} \\vec{p}^{0} \\boldsymbol{V}_{\\beta}^{0}\\)\nFor a proof, see Exercise 11.8."
  },
  {
    "objectID": "chpt11-multi-reg.html#seemingly-unrelated-regression",
    "href": "chpt11-multi-reg.html#seemingly-unrelated-regression",
    "title": "11  Multivariate Regression",
    "section": "11.7 Seemingly Unrelated Regression",
    "text": "11.7 Seemingly Unrelated Regression\nConsider the systems regression model under the conditional expectation and homoskedasticity assumptions\n\\[\n\\begin{aligned}\nY &=\\bar{X} \\beta+e \\\\\n\\mathbb{E}[e \\mid X] &=0 \\\\\n\\mathbb{E}\\left[e e^{\\prime} \\mid X\\right] &=\\Sigma .\n\\end{aligned}\n\\]\nSince the errors are correlated across equations we consider estimation by Generalized Least Squares (GLS). To derive the estimator, premultiply (11.15) by \\(\\Sigma^{-1 / 2}\\) so that the transformed error vector is i.i.d. with covariance matrix \\(\\boldsymbol{I}_{m}\\). Then apply least squares and rearrange to find\n\\[\n\\widehat{\\beta}_{\\mathrm{gls}}=\\left(\\sum_{i=1}^{n} \\bar{X}_{i}^{\\prime} \\Sigma^{-1} \\bar{X}_{i}\\right)^{-1}\\left(\\sum_{i=1}^{n} \\bar{X}_{i}^{\\prime} \\Sigma^{-1} Y_{i}\\right)\n\\]\n(see Exercise 11.9). Another approach is to take the vector representation\n\\[\n\\boldsymbol{Y}=\\overline{\\boldsymbol{X}} \\beta+\\boldsymbol{e}\n\\]\nand calculate that the equation error \\(\\boldsymbol{e}\\) has variance \\(\\mathbb{E}\\left[\\boldsymbol{e} \\boldsymbol{e}^{\\prime}\\right]=\\boldsymbol{I}_{n} \\otimes \\Sigma\\). Premultiply the equation by \\(\\boldsymbol{I}_{n} \\otimes\\) \\(\\Sigma^{-1 / 2}\\) so that the transformed error has covariance matrix \\(\\boldsymbol{I}_{n m}\\) and then apply least squares to find\n\\[\n\\widehat{\\beta}_{\\mathrm{gls}}=\\left(\\overline{\\boldsymbol{X}}^{\\prime}\\left(\\boldsymbol{I}_{n} \\otimes \\Sigma^{-1}\\right) \\overline{\\boldsymbol{X}}\\right)^{-1}\\left(\\overline{\\boldsymbol{X}}^{\\prime}\\left(\\boldsymbol{I}_{n} \\otimes \\Sigma^{-1}\\right) \\boldsymbol{Y}\\right)\n\\]\n(see Exercise 11.10). Expressions (11.16) and (11.17) are algebraically equivalent. To see the equivalence, observe that\n\\[\n\\begin{aligned}\n\\overline{\\boldsymbol{X}}^{\\prime}\\left(\\boldsymbol{I}_{n} \\otimes \\Sigma^{-1}\\right) \\overline{\\boldsymbol{X}} &=\\left(\\begin{array}{lll}\n\\bar{X}_{1}^{\\prime} & \\cdots & \\bar{X}_{n}^{\\prime}\n\\end{array}\\right)\\left(\\begin{array}{cccc}\n\\Sigma^{-1} & 0 & \\cdots & 0 \\\\\n\\vdots & \\Sigma^{-1} & & \\vdots \\\\\n0 & 0 & \\cdots & \\Sigma^{-1}\n\\end{array}\\right)\\left(\\begin{array}{c}\n\\bar{X}_{1} \\\\\n\\vdots \\\\\n\\bar{X}_{n}\n\\end{array}\\right) \\\\\n&=\\sum_{i=1}^{n} \\bar{X}_{i}^{\\prime} \\Sigma^{-1} \\bar{X}_{i}\n\\end{aligned}\n\\]\nand\n\\[\n\\begin{aligned}\n\\overline{\\boldsymbol{X}}^{\\prime}\\left(\\boldsymbol{I}_{n} \\otimes \\Sigma^{-1}\\right) \\boldsymbol{Y} &=\\left(\\begin{array}{lll}\n\\bar{X}_{1}^{\\prime} & \\cdots & \\bar{X}_{n}^{\\prime}\n\\end{array}\\right)\\left(\\begin{array}{cccc}\n\\Sigma^{-1} & 0 & \\cdots & 0 \\\\\n\\vdots & \\Sigma^{-1} & & \\vdots \\\\\n0 & 0 & \\cdots & 0^{-1}\n\\end{array}\\right)\\left(\\begin{array}{c}\nY_{1} \\\\\n\\vdots \\\\\nY_{n}\n\\end{array}\\right) \\\\\n&=\\sum_{i=1}^{n} \\bar{X}_{i}^{\\prime} \\Sigma^{-1} Y_{i} .\n\\end{aligned}\n\\]\nSince \\(\\Sigma\\) is unknown it must be replaced by an estimator. Using \\(\\widehat{\\Sigma}\\) from (11.5) we obtain a feasible GLS estimator.\n\\[\n\\begin{aligned}\n\\widehat{\\beta}_{\\mathrm{sur}} &=\\left(\\sum_{i=1}^{n} \\bar{X}_{i}^{\\prime} \\widehat{\\Sigma}^{-1} \\bar{X}_{i}\\right)^{-1}\\left(\\sum_{i=1}^{n} \\bar{X}_{i}^{\\prime} \\widehat{\\Sigma}^{-1} Y_{i}\\right) \\\\\n&=\\left(\\overline{\\boldsymbol{X}}^{\\prime}\\left(\\boldsymbol{I}_{n} \\otimes \\widehat{\\Sigma}^{-1}\\right) \\overline{\\boldsymbol{X}}\\right)^{-1}\\left(\\overline{\\boldsymbol{X}}^{\\prime}\\left(\\boldsymbol{I}_{n} \\otimes \\widehat{\\Sigma}^{-1}\\right) \\boldsymbol{Y}\\right) .\n\\end{aligned}\n\\]\nThis is the Seemingly Unrelated Regression (SUR) estimator as introduced by Zellner (1962).\nThe estimator \\(\\widehat{\\Sigma}\\) can be updated by calculating the SUR residuals \\(\\widehat{e}_{i}=Y_{i}-\\bar{X}_{i}^{\\prime} \\widehat{\\beta}_{\\text {sur }}\\) and the covariance matrix estimator \\(\\widehat{\\Sigma}=\\frac{1}{n} \\sum_{i=1}^{n} \\widehat{e}_{i} \\widehat{e}_{i}^{\\prime}\\). Substituted into (11.18) we obtain an iterated SUR estimator. This can be iterated until convergence.\nUnder conditional homoskedasticity (11.8) we can derive its asymptotic distribution.\nTheorem 11.4 Under Assumption \\(7.2\\) and (11.8)\n\\[\n\\sqrt{n}\\left(\\widehat{\\beta}_{\\text {sur }}-\\beta\\right) \\underset{d}{\\longrightarrow} \\mathrm{N}\\left(0, V_{\\beta}^{*}\\right)\n\\]\nwhere \\(\\boldsymbol{V}_{\\beta}^{*}=\\left(\\mathbb{E}\\left[\\bar{X}^{\\prime} \\Sigma^{-1} \\bar{X}\\right]\\right)^{-1}\\).\nFor a proof, see Exercise 11.11.\nUnder these assumptions, SUR is more efficient than least squares.\nTheorem 11.5 Under Assumption \\(7.2\\) and (11.8)\n\\[\n\\boldsymbol{V}_{\\beta}^{*}=\\left(\\mathbb{E}\\left[\\bar{X}^{\\prime} \\Sigma^{-1} \\bar{X}\\right]\\right)^{-1} \\leq\\left(\\mathbb{E}\\left[\\bar{X}^{\\prime} \\bar{X}\\right]\\right)^{-1} \\mathbb{E}\\left[\\bar{X}^{\\prime} \\Sigma \\bar{X}\\right]\\left(\\mathbb{E}\\left[\\bar{X}^{\\prime} \\bar{X}\\right]\\right)^{-1}=\\boldsymbol{V}_{\\beta}\n\\]\nand thus \\(\\widehat{\\beta}_{\\text {sur }}\\) is asymptotically more efficient than \\(\\widehat{\\beta}_{\\text {ols. }}\\). For a proof, see Exercise 11.12.\nAn appropriate estimator of the variance of \\(\\widehat{\\beta}_{\\text {sur }}\\) is\n\\[\n\\widehat{\\boldsymbol{V}}_{\\widehat{\\beta}}=\\left(\\sum_{i=1}^{n} \\bar{X}_{i}^{\\prime} \\widehat{\\Sigma}^{-1} \\bar{X}_{i}\\right)^{-1}\n\\]\nTheorem 11.6 Under Assumption \\(7.2\\) and (11.8) \\(n \\widehat{\\boldsymbol{V}}_{\\widehat{\\beta}} \\underset{p}{\\longrightarrow} \\boldsymbol{V}_{\\beta}\\).\nFor a proof, see Exercise 11.13.\nIn Stata, the seemingly unrelated regressions estimator is implemented using the sureg command."
  },
  {
    "objectID": "chpt11-multi-reg.html#equivalence-of-sur-and-least-squares",
    "href": "chpt11-multi-reg.html#equivalence-of-sur-and-least-squares",
    "title": "11  Multivariate Regression",
    "section": "11.8 Equivalence of SUR and Least Squares",
    "text": "11.8 Equivalence of SUR and Least Squares\nWhen the regressors are common across equations \\(X_{j}=X\\) it turns out that the SUR estimator simplifies to least squares.\nTo see this, recall that when regressors are common this implies that \\(\\bar{X}=\\boldsymbol{I}_{m} \\otimes X^{\\prime}\\). Then\n\\[\n\\begin{aligned}\n\\bar{X}_{i}^{\\prime} \\widehat{\\Sigma}^{-1} &=\\left(\\boldsymbol{I}_{m} \\otimes X_{i}\\right) \\widehat{\\Sigma}^{-1} \\\\\n&=\\widehat{\\Sigma}^{-1} \\otimes X_{i} \\\\\n&=\\left(\\widehat{\\Sigma}^{-1} \\otimes \\boldsymbol{I}_{k}\\right)\\left(\\boldsymbol{I}_{m} \\otimes X_{i}\\right) \\\\\n&=\\left(\\widehat{\\Sigma}^{-1} \\otimes \\boldsymbol{I}_{k}\\right) \\bar{X}_{i}^{\\prime} .\n\\end{aligned}\n\\]\nThus\n\\[\n\\begin{aligned}\n\\widehat{\\beta}_{\\mathrm{sur}} &=\\left(\\sum_{i=1}^{n} \\bar{X}_{i}^{\\prime} \\widehat{\\Sigma}^{-1} \\bar{X}_{i}\\right)^{-1}\\left(\\sum_{i=1}^{n} \\bar{X}_{i}^{\\prime} \\widehat{\\Sigma}^{-1} Y_{i}\\right) \\\\\n&=\\left(\\left(\\widehat{\\Sigma}^{-1} \\otimes \\boldsymbol{I}_{k}\\right) \\sum_{i=1}^{n} \\bar{X}_{i}^{\\prime} \\bar{X}_{i}\\right)^{-1}\\left(\\left(\\widehat{\\Sigma}^{-1} \\otimes \\boldsymbol{I}_{k}\\right) \\sum_{i=1}^{n} \\bar{X}_{i}^{\\prime} Y_{i}\\right) \\\\\n&=\\left(\\sum_{i=1}^{n} \\bar{X}_{i}^{\\prime} \\bar{X}_{i}\\right)^{-1}\\left(\\sum_{i=1}^{n} \\bar{X}_{i}^{\\prime} Y_{i}\\right)=\\widehat{\\beta}_{\\mathrm{ols}}\n\\end{aligned}\n\\]\nA model where regressors are not common across equations is nested within a model with the union of all regressors included in all equations. Thus the model with regressors common across equations is a fully unrestricted model, and a model where the regressors differ across equations is a restricted model. Thus the above result shows that the SUR estimator reduces to least squares in the absence of restrictions, but SUR can differ from least squares otherwise.\nAnother context where SUR=OLS is when the variance matrix is diagonal, \\(\\Sigma=\\operatorname{diag}\\left\\{\\sigma_{1}^{2}, \\ldots, \\sigma_{m}^{2}\\right\\}\\). In this case \\(\\Sigma^{-1 / 2} \\bar{X}_{i}=\\bar{X}_{i} \\operatorname{diag}\\left\\{\\boldsymbol{I}_{k_{1}} \\sigma_{1}^{-1 / 2}, \\ldots, \\boldsymbol{I}_{k_{m}} \\sigma_{m}^{-1 / 2}\\right\\}\\) from which you can calculate that \\(\\widehat{\\beta}_{\\text {sur }}=\\widehat{\\beta}_{\\text {ols }}\\). The intuition is that there is no difference in systems estimation when the equations are uncorrelated, which occurs when \\(\\Sigma\\) is diagonal."
  },
  {
    "objectID": "chpt11-multi-reg.html#maximum-likelihood-estimator",
    "href": "chpt11-multi-reg.html#maximum-likelihood-estimator",
    "title": "11  Multivariate Regression",
    "section": "11.9 Maximum Likelihood Estimator",
    "text": "11.9 Maximum Likelihood Estimator\nTake the linear model under the assumption that the error is independent of the regressors and multivariate normally distributed. Thus \\(Y=\\bar{X} \\beta+e\\) with \\(e \\sim \\mathrm{N}(0, \\Sigma)\\). In this case we can consider the maximum likelihood estimator (MLE) of the coefficients.\nIt is convenient to reparameterize the covariance matrix in terms of its inverse \\(S=\\Sigma^{-1}\\). With this reparameterization the conditional density of \\(Y\\) given \\(X=x\\) equals\n\\[\nf(y \\mid x)=\\frac{\\operatorname{det}(\\boldsymbol{S})^{1 / 2}}{(2 \\pi)^{m / 2}} \\exp \\left(-\\frac{1}{2}(y-x \\beta)^{\\prime} \\boldsymbol{S}(y-x \\beta)\\right) .\n\\]\nThe log-likelihood function for the sample is\n\\[\n\\ell_{n}(\\beta, \\boldsymbol{S})=-\\frac{n m}{2} \\log (2 \\pi)+\\frac{n}{2} \\log (\\operatorname{det}(\\boldsymbol{S}))-\\frac{1}{2} \\sum_{i=1}^{n}\\left(Y_{i}-\\bar{X}_{i} \\beta\\right)^{\\prime} S\\left(Y_{i}-\\bar{X}_{i} \\beta\\right) .\n\\]\nThe maximum likelihood estimator \\(\\left(\\widehat{\\beta}_{\\text {mle }}, \\widehat{S}_{\\text {mle }}\\right)\\) maximizes the log-likelihood function. The first order conditions are\n\\[\n0=\\left.\\frac{\\partial}{\\partial \\beta} \\ell_{n}(\\beta, \\boldsymbol{S})\\right|_{\\beta=\\widehat{\\beta}, \\boldsymbol{S}=\\widehat{\\boldsymbol{S}}}=\\sum_{i=1}^{n} \\bar{X}_{i} \\widehat{\\boldsymbol{S}}\\left(Y_{i}-\\bar{X}_{i} \\widehat{\\beta}\\right)\n\\]\nand\n\\[\n0=\\left.\\frac{\\partial}{\\partial \\boldsymbol{S}} \\ell_{n}(\\beta, \\Sigma)\\right|_{\\beta=\\widehat{\\beta}, \\boldsymbol{S}=\\widehat{\\boldsymbol{S}}}=\\frac{n}{2} \\widehat{\\boldsymbol{S}}^{-1}-\\frac{1}{2} \\operatorname{tr}\\left(\\sum_{i=1}^{n}\\left(Y_{i}-\\bar{X}_{i} \\widehat{\\beta}\\right)\\left(Y_{i}-\\bar{X}_{i} \\widehat{\\beta}\\right)^{\\prime}\\right)\n\\]\nThe second equation uses the matrix results \\(\\frac{\\partial}{\\partial S} \\log (\\operatorname{det}(\\boldsymbol{S}))=\\boldsymbol{S}^{-1}\\) and \\(\\frac{\\partial}{\\partial \\boldsymbol{B}} \\operatorname{tr}(\\boldsymbol{A B})=\\boldsymbol{A}^{\\prime}\\) from Appendix A.20.\nSolving and making the substitution \\(\\widehat{\\Sigma}=\\widehat{\\boldsymbol{S}}^{-1}\\) we obtain\n\\[\n\\begin{gathered}\n\\widehat{\\beta}_{\\mathrm{mle}}=\\left(\\sum_{i=1}^{n} \\bar{X}_{i}^{\\prime} \\widehat{\\Sigma}^{-1} \\bar{X}_{i}\\right)^{-1}\\left(\\sum_{i=1}^{n} \\bar{X}_{i}^{\\prime} \\widehat{\\Sigma}^{-1} Y_{i}\\right) \\\\\n\\widehat{\\Sigma}_{\\mathrm{mle}}=\\frac{1}{n} \\sum_{i=1}^{n}\\left(Y_{i}-\\bar{X}_{i} \\widehat{\\beta}\\right)\\left(Y_{i}-\\bar{X}_{i} \\widehat{\\beta}\\right)^{\\prime} .\n\\end{gathered}\n\\]\nNotice that each equation refers to the other. Hence these are not closed-form expressions but can be solved via iteration. The solution is identical to the iterated SUR estimator. Thus the iterated SUR estimator is identical to MLE under normality.\nRecall that the SUR estimator simplifies to OLS when the regressors are common across equations. The same occurs for the MLE. Thus when \\(\\bar{X}_{i}=\\boldsymbol{I}_{m} \\otimes X_{i}^{\\prime}\\) we find that \\(\\widehat{\\beta}_{\\mathrm{mle}}=\\widehat{\\beta}_{\\text {ols }}\\) and \\(\\widehat{\\Sigma}_{\\text {mle }}=\\widehat{\\Sigma}_{\\text {ols }}\\)."
  },
  {
    "objectID": "chpt11-multi-reg.html#restricted-estimation",
    "href": "chpt11-multi-reg.html#restricted-estimation",
    "title": "11  Multivariate Regression",
    "section": "11.10 Restricted Estimation",
    "text": "11.10 Restricted Estimation\nIn many multivariate regression applications it is desired to impose restrictions on the coefficients. In particular, cross-equation restrictions (for example, imposing Slutsky symmetry on a demand system) can be quite important and can only be imposed by a multivariate estimation method. Estimation subject to restrictions can be done by minimum distance, maximum likelihood, or the generalized method of moments.\nMinimum distance is a straightforward application of the methods of Chapter 8 to the estimators presented in this chapter, as such methods apply to any asymptotically normal estimator.\nImposing restrictions on maximum likelihood is also straightforward. The likelihood is maximized subject to the imposed restrictions. One important example is explored in detail in the following section.\nGeneralized method of moments estimation of multivariate regression subject to restrictions will be explored in Section 13.18. This is a particularly simple and straightforward way to estimate restricted multivariate regression models and is our generally preferred approach."
  },
  {
    "objectID": "chpt11-multi-reg.html#reduced-rank-regression",
    "href": "chpt11-multi-reg.html#reduced-rank-regression",
    "title": "11  Multivariate Regression",
    "section": "11.11 Reduced Rank Regression",
    "text": "11.11 Reduced Rank Regression\nOne context where systems estimation is important is when it is desired to impose or test restrictions across equations. Restricted systems are commonly estimated by maximum likelihood under normality. In this section we explore one important special case of restricted multivariate regression known as reduced rank regression. The model was originally proposed by Anderson (1951) and extended by Johansen (1995).\nThe unrestricted model is\n\\[\n\\begin{aligned}\nY &=\\boldsymbol{B}^{\\prime} X+\\boldsymbol{C}^{\\prime} Z+e \\\\\n\\mathbb{E}\\left[e e^{\\prime} \\mid X, Z\\right] &=\\Sigma\n\\end{aligned}\n\\]\nwhere \\(\\boldsymbol{B}\\) is \\(k \\times m, \\boldsymbol{C}\\) is \\(\\ell \\times m, Y \\in \\mathbb{R}^{m}, X \\in \\mathbb{R}^{k}\\), and \\(Z \\in \\mathbb{R}^{\\ell}\\). We separate the regressors as \\(X\\) and \\(Z\\) because the coefficient matrix \\(\\boldsymbol{B}\\) will be restricted while \\(\\boldsymbol{C}\\) will be unrestricted.\nThe matrix \\(\\boldsymbol{B}\\) is full rank if\n\\[\n\\operatorname{rank}(\\boldsymbol{B})=\\min (k, m) .\n\\]\nThe reduced rank restriction is \\(\\operatorname{rank}(\\boldsymbol{B})=r<\\min (k, m)\\) for some known \\(r\\).\nThe reduced rank restriction implies that we can write the coefficient matrix \\(\\boldsymbol{B}\\) in the factored form \\(\\boldsymbol{B}=\\boldsymbol{G} \\boldsymbol{A}^{\\prime}\\) where \\(\\boldsymbol{A}\\) is \\(m \\times r\\) and \\(\\boldsymbol{G}\\) is \\(k \\times r\\). This representation is not unique as we can replace \\(\\boldsymbol{G}\\) with \\(\\boldsymbol{G} \\boldsymbol{Q}\\) and \\(\\boldsymbol{A}\\) with \\(\\boldsymbol{A} \\boldsymbol{Q}^{-1 \\prime}\\) for any invertible \\(\\boldsymbol{Q}\\) and the same relation holds. Identification therefore requires a normalization of the coefficients. A conventional normalization is \\(\\boldsymbol{G}^{\\prime} \\boldsymbol{D} \\boldsymbol{G}=\\boldsymbol{I}_{r}\\) for given \\(\\boldsymbol{D}\\).\nEquivalently, the reduced rank restriction can be imposed by requiring that \\(\\boldsymbol{B}\\) satisfy the restriction \\(\\boldsymbol{B} \\boldsymbol{A}_{\\perp}=\\boldsymbol{G} \\boldsymbol{A}^{\\prime} \\boldsymbol{A}_{\\perp}=0\\) for some \\(m \\times(m-r)\\) coefficient matrix \\(\\boldsymbol{A}_{\\perp}\\). Since \\(\\boldsymbol{G}\\) is full rank this requires that \\(\\boldsymbol{A}^{\\prime} \\boldsymbol{A}_{\\perp}=0\\), hence \\(\\boldsymbol{A}_{\\perp}\\) is the orthogonal complement of \\(\\boldsymbol{A}\\). Note that \\(\\boldsymbol{A}_{\\perp}\\) is not unique as it can be replaced by \\(\\boldsymbol{A}_{\\perp} \\boldsymbol{Q}\\) for any \\((m-r) \\times(m-r)\\) invertible \\(\\boldsymbol{Q}\\). Thus if \\(\\boldsymbol{A}_{\\perp}\\) is to be estimated it requires a normalization.\nWe discuss methods for estimation of \\(\\boldsymbol{G}, \\boldsymbol{A}, \\Sigma, \\boldsymbol{C}\\), and \\(\\boldsymbol{A}_{\\perp}\\). The standard approach is maximum likelihood under the assumption that \\(e \\sim \\mathrm{N}(0, \\Sigma)\\). The log-likelihood function for the sample is\n\\[\n\\begin{aligned}\n\\ell_{n}(\\boldsymbol{G}, \\boldsymbol{A}, \\boldsymbol{C}, \\Sigma) &=-\\frac{n m}{2} \\log (2 \\pi)-\\frac{n}{2} \\log (\\operatorname{det}(\\Sigma)) \\\\\n&-\\frac{1}{2} \\sum_{i=1}^{n}\\left(Y_{i}-\\boldsymbol{A} \\boldsymbol{G}^{\\prime} X_{i}-\\boldsymbol{C}^{\\prime} Z_{i}\\right)^{\\prime} \\Sigma^{-1}\\left(Y_{i}-\\boldsymbol{A} \\boldsymbol{G}^{\\prime} X_{i}-\\boldsymbol{C}^{\\prime} Z_{i}\\right) .\n\\end{aligned}\n\\]\nAnderson (1951) derived the MLE by imposing the constraint \\(\\boldsymbol{B} \\boldsymbol{A}_{\\perp}=0\\) via the method of Lagrange multipliers. This turns out to be algebraically cumbersome.\nJohansen (1995) instead proposed the following straightforward concentration method. Treating \\(\\boldsymbol{G}\\) as if it is known, maximize the log-likelihood with respect to the other parameters. Resubstituting these estimators we obtain the concentrated log-likelihood function with respect to \\(\\boldsymbol{G}\\). This can be maximized to find the MLE for \\(\\boldsymbol{G}\\). The other parameter estimators are then obtain by substitution. We now describe these steps in detail.\nGiven \\(\\boldsymbol{G}\\) the likelihood is a normal multivariate regression in the variables \\(\\boldsymbol{G}^{\\prime} X\\) and \\(Z\\), so the MLE for \\(\\boldsymbol{A}, \\boldsymbol{C}\\) and \\(\\Sigma\\) are least squares. In particular, using the Frisch-Waugh-Lovell residual regression formula we can write the estimators for \\(\\boldsymbol{A}\\) and \\(\\Sigma\\) as\n\\[\n\\widehat{\\boldsymbol{A}}(\\boldsymbol{G})=\\left(\\widetilde{\\boldsymbol{Y}}^{\\prime} \\widetilde{\\boldsymbol{X}} \\boldsymbol{G}\\right)\\left(\\boldsymbol{G}^{\\prime} \\widetilde{\\boldsymbol{X}}^{\\prime} \\widetilde{\\boldsymbol{X}} \\boldsymbol{G}\\right)^{-1}\n\\]\nand\n\\[\n\\widehat{\\Sigma}(\\boldsymbol{G})=\\frac{1}{n}\\left(\\widetilde{\\boldsymbol{Y}}^{\\prime} \\tilde{\\boldsymbol{Y}}-\\widetilde{\\boldsymbol{Y}}^{\\prime} \\widetilde{\\boldsymbol{X}} \\boldsymbol{G}\\left(\\boldsymbol{G}^{\\prime} \\widetilde{\\boldsymbol{X}}^{\\prime} \\widetilde{\\boldsymbol{X}} \\boldsymbol{G}\\right)^{-1} \\boldsymbol{G}^{\\prime} \\widetilde{\\boldsymbol{X}}^{\\prime} \\widetilde{\\boldsymbol{Y}}\\right)\n\\]\nwhere \\(\\tilde{\\boldsymbol{Y}}=\\boldsymbol{Y}-\\boldsymbol{Z}\\left(\\boldsymbol{Z}^{\\prime} \\boldsymbol{Z}\\right)^{-1} \\boldsymbol{Z}^{\\prime} \\boldsymbol{Y}\\) and \\(\\widetilde{\\boldsymbol{X}}=\\boldsymbol{X}-\\boldsymbol{Z}\\left(\\boldsymbol{Z}^{\\prime} \\boldsymbol{Z}\\right)^{-1} \\boldsymbol{Z}^{\\prime} \\boldsymbol{X}\\).\nSubstituting these estimators into the log-likelihood function we obtain the concentrated likelihood function, which is a function of \\(G\\) only.\n\\[\n\\begin{aligned}\n\\widetilde{\\ell}_{n}(\\boldsymbol{G}) &=\\ell_{n}(\\boldsymbol{G}, \\widehat{\\boldsymbol{A}}(\\boldsymbol{G}), \\widehat{\\boldsymbol{C}}(\\boldsymbol{G}), \\widehat{\\Sigma}(\\boldsymbol{G})) \\\\\n&=\\frac{m}{2}(n \\log (2 \\pi)-1)-\\frac{n}{2} \\log \\left[\\operatorname{det}\\left(\\widetilde{\\boldsymbol{Y}}^{\\prime} \\widetilde{\\boldsymbol{Y}}-\\widetilde{\\boldsymbol{Y}}^{\\prime} \\widetilde{\\boldsymbol{X}} \\boldsymbol{G}\\left(\\boldsymbol{G}^{\\prime} \\widetilde{\\boldsymbol{X}}^{\\prime} \\widetilde{\\boldsymbol{X}} \\boldsymbol{G}\\right)^{-1} \\boldsymbol{G}^{\\prime} \\widetilde{\\boldsymbol{X}}^{\\prime} \\widetilde{\\boldsymbol{Y}}\\right)\\right] \\\\\n&=\\frac{m}{2}(n \\log (2 \\pi)-1)-\\frac{n}{2} \\log \\left(\\operatorname{det}\\left(\\widetilde{\\boldsymbol{Y}}^{\\prime} \\widetilde{\\boldsymbol{Y}}\\right)\\right)-\\frac{n}{2} \\log \\left[\\frac{\\operatorname{det}\\left(\\boldsymbol{G}^{\\prime}\\left(\\widetilde{\\boldsymbol{X}}^{\\prime} \\widetilde{\\boldsymbol{X}}-\\widetilde{\\boldsymbol{X}}^{\\prime} \\tilde{\\boldsymbol{Y}}\\left(\\widetilde{\\boldsymbol{Y}}^{\\prime} \\widetilde{\\boldsymbol{Y}}\\right)^{-1} \\boldsymbol{Y}^{\\prime} \\widetilde{\\boldsymbol{X}}\\right) \\boldsymbol{G}\\right)}{\\operatorname{det}\\left(\\boldsymbol{G}^{\\prime} \\widetilde{\\boldsymbol{X}}^{\\prime} \\widetilde{\\boldsymbol{X}} \\boldsymbol{G}\\right)} .\\right.\n\\end{aligned}\n\\]\nThe third equality uses Theorem A.1.8. The MLE \\(\\widehat{\\boldsymbol{G}}\\) for \\(\\boldsymbol{G}\\) is the maximizer of \\(\\widetilde{\\ell}_{n}(\\boldsymbol{G})\\), or equivalently equals\n\\[\n\\begin{aligned}\n\\widehat{\\boldsymbol{G}} &=\\underset{\\boldsymbol{G}}{\\operatorname{argmin}} \\frac{\\operatorname{det}\\left(\\boldsymbol{G}^{\\prime}\\left(\\widetilde{\\boldsymbol{X}}^{\\prime} \\widetilde{\\boldsymbol{X}}-\\widetilde{\\boldsymbol{X}}^{\\prime} \\widetilde{\\boldsymbol{Y}}\\left(\\tilde{\\boldsymbol{Y}}^{\\prime} \\widetilde{\\boldsymbol{Y}}\\right)^{-1} \\boldsymbol{Y}^{\\prime} \\tilde{\\boldsymbol{X}}\\right) \\boldsymbol{G}\\right)}{\\operatorname{det}\\left(\\boldsymbol{G}^{\\prime} \\widetilde{\\boldsymbol{X}}^{\\prime} \\widetilde{\\boldsymbol{X}} \\boldsymbol{G}\\right)} \\\\\n&=\\underset{\\boldsymbol{G}}{\\operatorname{argmax}} \\frac{\\operatorname{det}\\left(\\boldsymbol{G}^{\\prime} \\widetilde{\\boldsymbol{X}}^{\\prime} \\widetilde{\\boldsymbol{Y}}\\left(\\widetilde{\\boldsymbol{Y}}^{\\prime} \\widetilde{\\boldsymbol{Y}}\\right)^{-1} \\boldsymbol{Y}^{\\prime} \\widetilde{\\boldsymbol{X}} \\boldsymbol{G}\\right)}{\\operatorname{det}\\left(\\boldsymbol{G}^{\\prime} \\widetilde{\\boldsymbol{X}}^{\\prime} \\widetilde{\\boldsymbol{X}} \\boldsymbol{G}\\right)} \\\\\n&=\\left\\{v_{1}, \\ldots, v_{r}\\right\\}\n\\end{aligned}\n\\]\nwhich are the generalized eigenvectors of \\(\\widetilde{\\boldsymbol{X}}^{\\prime} \\tilde{\\boldsymbol{Y}}\\left(\\tilde{\\boldsymbol{Y}}^{\\prime} \\tilde{\\boldsymbol{Y}}\\right)^{-1} \\boldsymbol{Y}^{\\prime} \\tilde{\\boldsymbol{X}}\\) with respect to \\(\\widetilde{\\boldsymbol{X}}^{\\prime} \\widetilde{\\boldsymbol{X}}\\) corresponding to the \\(r\\) largest generalized eigenvalues. (Generalized eigenvalues and eigenvectors are discussed in Section A.14.) The estimator satisfies the normalization \\(\\widehat{\\boldsymbol{G}}^{\\prime} \\widetilde{\\boldsymbol{X}}^{\\prime} \\widetilde{\\boldsymbol{X}} \\widehat{\\boldsymbol{G}}=\\boldsymbol{I}_{r}\\). Letting \\(v_{j}^{*}\\) denote the eigenvectors of (11.20) we can also express \\(\\widehat{\\boldsymbol{G}}=\\left\\{v_{m}^{*}, \\ldots, v_{m-r+1}^{*}\\right\\}\\).\nThis is computationally straightforward. In MATLAB, for example, the generalized eigenvalues and eigenvectors of a matrix \\(\\boldsymbol{A}\\) with respect to \\(\\boldsymbol{B}\\) are found using the command eig \\((\\mathrm{A}, \\mathrm{B})\\).\nGiven \\(\\widehat{\\boldsymbol{G}}\\), the MLE \\(\\widehat{\\boldsymbol{A}}, \\widehat{\\boldsymbol{C}}, \\widehat{\\Sigma}\\) are found by least squares regression of \\(Y\\) on \\(\\widehat{\\boldsymbol{G}}^{\\prime} X\\) and \\(Z\\). In particular, \\(\\widehat{\\boldsymbol{A}}=\\widehat{\\boldsymbol{G}}^{\\prime} \\widetilde{\\boldsymbol{X}}^{\\prime} \\tilde{\\boldsymbol{Y}}\\) because \\(\\widehat{\\boldsymbol{G}}^{\\prime} \\widetilde{\\boldsymbol{X}}^{\\prime} \\widetilde{\\boldsymbol{X}} \\widehat{\\boldsymbol{G}}=\\boldsymbol{I}_{r}\\) We now discuss the estimator \\(\\widehat{\\boldsymbol{A}}_{\\perp}\\) of \\(\\boldsymbol{A}_{\\perp}\\). It turns out that\n\\[\n\\begin{aligned}\n\\widehat{\\boldsymbol{A}}_{\\perp} &=\\underset{\\boldsymbol{A}}{\\operatorname{argmax}} \\frac{\\operatorname{det}\\left(\\boldsymbol{A}^{\\prime}\\left(\\tilde{\\boldsymbol{Y}}^{\\prime} \\tilde{\\boldsymbol{Y}}-\\tilde{\\boldsymbol{Y}}^{\\prime} \\tilde{\\boldsymbol{X}}\\left(\\tilde{\\boldsymbol{X}}^{\\prime} \\tilde{\\boldsymbol{X}}\\right)^{-1} \\widetilde{\\boldsymbol{X}}^{\\prime} \\tilde{\\boldsymbol{Y}}\\right) \\boldsymbol{A}\\right)}{\\operatorname{det}\\left(\\boldsymbol{A}^{\\prime} \\tilde{\\boldsymbol{Y}}^{\\prime} \\tilde{\\boldsymbol{Y}} \\boldsymbol{A}\\right)} \\\\\n&=\\left\\{w_{1}, \\ldots, w_{m-r}\\right\\}\n\\end{aligned}\n\\]\nthe eigenvectors of \\(\\tilde{\\boldsymbol{Y}}^{\\prime} \\tilde{\\boldsymbol{Y}}-\\tilde{\\boldsymbol{Y}}^{\\prime} \\tilde{\\boldsymbol{X}}\\left(\\tilde{\\boldsymbol{X}}^{\\prime} \\tilde{\\boldsymbol{X}}\\right)^{-1} \\tilde{\\boldsymbol{X}}^{\\prime} \\tilde{\\boldsymbol{Y}}\\) with respect to \\(\\tilde{\\boldsymbol{Y}}^{\\prime} \\tilde{\\boldsymbol{Y}}\\) associated with the largest \\(m-r\\) eigenvalues.\nBy the dual eigenvalue relation (Theorem A.5), equations (11.20) and (11.21) have the same non-zero eigenvalues \\(\\lambda_{j}\\) and the associated eigenvectors \\(v_{j}^{*}\\) and \\(w_{j}\\) satisfy the relationship\n\\[\nw_{j}=\\lambda_{j}^{-1 / 2}\\left(\\tilde{\\boldsymbol{Y}}^{\\prime} \\tilde{\\boldsymbol{Y}}\\right)^{-1} \\tilde{\\boldsymbol{Y}}^{\\prime} \\widetilde{\\boldsymbol{X}} v_{j}^{*}\n\\]\nLetting \\(\\Lambda=\\operatorname{diag}\\left\\{\\lambda_{m}, \\ldots, \\lambda_{m-r+1}\\right\\}\\) this implies\n\\[\n\\left\\{w_{m}, \\ldots, w_{m-r+1}\\right\\}=\\left(\\widetilde{\\boldsymbol{Y}}^{\\prime} \\widetilde{\\boldsymbol{Y}}\\right)^{-1} \\tilde{\\boldsymbol{Y}}^{\\prime} \\widetilde{\\boldsymbol{X}}\\left\\{v_{m}^{*}, \\ldots, v_{m-r+1}^{*}\\right\\} \\Lambda=\\left(\\tilde{\\boldsymbol{Y}}^{\\prime} \\widetilde{\\boldsymbol{Y}}\\right)^{-1} \\widehat{\\boldsymbol{A}} \\Lambda .\n\\]\nThe second equality holds because \\(\\widehat{\\boldsymbol{G}}=\\left\\{v_{m}^{*}, \\ldots, v_{m-r+1}^{*}\\right\\}\\) and \\(\\widehat{\\boldsymbol{A}}=\\widetilde{\\boldsymbol{Y}}^{\\prime} \\widetilde{\\boldsymbol{X}} \\widehat{\\boldsymbol{G}}\\). Since the eigenvectors \\(w_{j}\\) satisfy the orthogonality property \\(w_{j}^{\\prime} \\widetilde{\\boldsymbol{Y}}^{\\prime} \\widetilde{\\boldsymbol{Y}} w_{\\ell}=0\\) for \\(j \\neq \\ell\\), it follows that\n\\[\n0=\\widehat{\\boldsymbol{A}}_{\\perp}^{\\prime} \\tilde{\\boldsymbol{Y}}^{\\prime} \\tilde{\\boldsymbol{Y}}\\left\\{w_{m}, \\ldots, w_{m-r+1}\\right\\}=\\widehat{\\boldsymbol{A}}_{\\perp}^{\\prime} \\widehat{\\boldsymbol{A}} \\Lambda .\n\\]\nSince \\(\\Lambda>0\\) we conclude that \\(\\widehat{A}_{\\perp}^{\\prime} \\widehat{A}=0\\) as desired.\nThe solution \\(\\widehat{A}_{\\perp}\\) in (11.21) can be represented several ways. One which is computationally convenient is to observe that\n\\[\n\\tilde{\\boldsymbol{Y}}^{\\prime} \\widetilde{\\boldsymbol{Y}}-\\widetilde{\\boldsymbol{Y}}^{\\prime} \\widetilde{\\boldsymbol{X}}\\left(\\widetilde{\\boldsymbol{X}}^{\\prime} \\widetilde{\\boldsymbol{X}}\\right)^{-1} \\widetilde{\\boldsymbol{Y}}^{\\prime} \\widetilde{\\boldsymbol{X}}=\\boldsymbol{Y}^{\\prime} \\boldsymbol{M}_{\\boldsymbol{X}, \\boldsymbol{Z}} \\boldsymbol{Y}=\\widetilde{\\boldsymbol{E}}^{\\prime} \\widetilde{\\boldsymbol{E}}\n\\]\nwhere \\(\\boldsymbol{M}_{\\boldsymbol{X}, \\boldsymbol{Z}}=\\boldsymbol{I}_{n}-(\\boldsymbol{X}, \\boldsymbol{Z})\\left((\\boldsymbol{X}, \\boldsymbol{Z})^{\\prime}(\\boldsymbol{X}, \\boldsymbol{Z})\\right)^{-1}(\\boldsymbol{X}, \\boldsymbol{Z})^{\\prime}\\) and \\(\\widetilde{\\boldsymbol{E}}=\\boldsymbol{M}_{\\boldsymbol{X}, \\boldsymbol{Z}} \\boldsymbol{Y}\\) is the residual matrix from the unrestricted multivariate least squares regression of \\(Y\\) on \\(X\\) and \\(\\boldsymbol{Z}\\). The first equality follows by the FrischWaugh-Lovell theorem. This shows that \\(\\widehat{\\boldsymbol{A}}_{\\perp}\\) are the generalized eigenvectors of \\(\\widetilde{\\boldsymbol{E}}^{\\prime} \\widetilde{\\boldsymbol{E}}\\) with respect to \\(\\widetilde{\\boldsymbol{Y}}^{\\prime} \\tilde{\\boldsymbol{Y}}\\) corresponding to the \\(m-r\\) largest eigenvalues. In MATLAB, for example, these can be computed using the eig \\((\\mathrm{A}, \\mathrm{B})\\) command.\nAnother representation is to write \\(\\boldsymbol{M}_{Z}=\\boldsymbol{I}_{n}-\\boldsymbol{Z}\\left(\\boldsymbol{Z}^{\\prime} \\boldsymbol{Z}\\right)^{-1} \\boldsymbol{Z}^{\\prime}\\) so that\n\\[\n\\widehat{A}_{\\perp}=\\underset{A}{\\operatorname{argmax}} \\frac{\\operatorname{det}\\left(\\boldsymbol{A}^{\\prime} \\boldsymbol{Y}^{\\prime} \\boldsymbol{M}_{\\boldsymbol{X}, \\boldsymbol{Z}} \\boldsymbol{Y} \\boldsymbol{A}\\right)}{\\operatorname{det}\\left(\\boldsymbol{A}^{\\prime} \\boldsymbol{Y}^{\\prime} \\boldsymbol{M}_{\\boldsymbol{Z}} \\boldsymbol{Y} \\boldsymbol{A}\\right)}=\\underset{\\boldsymbol{A}}{\\operatorname{argmin}} \\frac{\\operatorname{det}\\left(\\boldsymbol{A}^{\\prime} \\boldsymbol{Y}^{\\prime} \\boldsymbol{M}_{\\boldsymbol{Z}} \\boldsymbol{Y} \\boldsymbol{A}\\right)}{\\operatorname{det}\\left(\\boldsymbol{A}^{\\prime} \\boldsymbol{Y}^{\\prime} \\boldsymbol{M}_{\\boldsymbol{X}, \\boldsymbol{Z}} \\boldsymbol{Y} \\boldsymbol{A}\\right)} .\n\\]\nWe summarize our findings. Theorem 11.7 The MLE for the reduced rank model (11.19) under \\(e \\sim \\mathrm{N}(0, \\Sigma)\\) is given as follows. Let \\(\\tilde{\\boldsymbol{Y}}\\) and \\(\\widetilde{\\boldsymbol{X}}\\) be the residual matrices from multivariate regression of \\(\\boldsymbol{Y}\\) and \\(\\boldsymbol{X}\\) on \\(\\boldsymbol{Z}\\), respectively. Then \\(\\widehat{\\boldsymbol{G}}_{\\mathrm{mle}}=\\left\\{v_{1}, \\ldots, v_{r}\\right\\}\\), the generalized eigenvectors of \\(\\widetilde{\\boldsymbol{X}}^{\\prime} \\tilde{\\boldsymbol{Y}}\\left(\\widetilde{\\boldsymbol{Y}}^{\\prime} \\tilde{\\boldsymbol{Y}}\\right)^{-1} \\boldsymbol{Y}^{\\prime} \\widetilde{\\boldsymbol{X}}\\) with respect to \\(\\widetilde{\\boldsymbol{X}}^{\\prime} \\widetilde{\\boldsymbol{X}}\\) corresponding to the \\(r\\) largest eigenvalues \\(\\widehat{\\lambda}_{j} . \\widehat{\\boldsymbol{A}}_{\\text {mle }}, \\widehat{\\boldsymbol{C}}_{\\text {mle }}\\) and \\(\\widehat{\\Sigma}_{\\text {mle }}\\) are obtained by the least squares regression\n\\[\n\\begin{aligned}\nY_{i} &=\\widehat{\\boldsymbol{A}}_{\\mathrm{mle}} \\widehat{\\boldsymbol{G}}_{\\mathrm{mle}}^{\\prime} X_{i}+\\widehat{\\boldsymbol{C}}_{\\mathrm{mle}}^{\\prime} Z_{i}+\\widehat{e}_{i} \\\\\n\\widehat{\\Sigma}_{\\mathrm{mle}} &=\\frac{1}{n} \\sum_{i=1}^{n} \\widehat{e}_{i} \\widehat{e}_{i}^{\\prime}\n\\end{aligned}\n\\]\nLet \\(\\widetilde{\\boldsymbol{E}}\\) be the residual matrix from a multivariate regression of \\(\\boldsymbol{Y}\\) on \\(\\boldsymbol{X}\\) and \\(\\boldsymbol{Z}\\). Then \\(\\widehat{\\boldsymbol{A}}_{\\perp}\\) equals the generalized eigenvectors of \\(\\widetilde{\\boldsymbol{E}} \\widetilde{\\boldsymbol{E}}^{\\prime}\\) with respect to \\(\\widetilde{\\boldsymbol{Y}}^{\\prime} \\widetilde{\\boldsymbol{Y}}\\) corresponding to the \\(m-r\\) smallest eigenvalues. The maximized likelihood equals\n\\[\n\\ell_{n}=\\frac{m}{2}(n \\log (2 \\pi)-1)-\\frac{n}{2} \\log \\left(\\operatorname{det}\\left(\\widetilde{\\boldsymbol{Y}}^{\\prime} \\tilde{\\boldsymbol{Y}}\\right)\\right)-\\frac{n}{2} \\sum_{j=1}^{r} \\log \\left(1-\\widehat{\\lambda}_{j}\\right) .\n\\]\nAn R package for reduced rank regression is “RRR”. I am unaware of a Stata command."
  },
  {
    "objectID": "chpt11-multi-reg.html#principal-component-analysis",
    "href": "chpt11-multi-reg.html#principal-component-analysis",
    "title": "11  Multivariate Regression",
    "section": "11.12 Principal Component Analysis",
    "text": "11.12 Principal Component Analysis\nIn Section \\(4.21\\) we described the Duflo, Dupas, and Kremer (2011) dataset which is a sample of Kenyan first grade test scores. Following the authors we focused on the variable totalscore which is each student’s composite test score. If you examine the data file you will find other pieces of information about the students’ performance, including each student’s score on separate sections of the test, with the labels wordscore (word recognition), sentscore (sentence recognition), letterscore (letter recognition), spellscore (spelling), additions_score (addition), substractions_score (subtraction), multiplications_score (multiplication). The “total” score sums the scores from the individual sections. Perhaps there is more information in the section scores. How can we learn about this from the data?\nPrincipal component analysis (PCA) addresses this issue by ordering linear combinations by their contribution to variance. Definition \\(11.1\\) Let \\(X\\) be a \\(k \\times 1\\) random vector.\nThe first principal component is \\(U_{1}=h_{1}^{\\prime} X\\) where \\(h_{1}\\) satisfies\n\\[\nh_{1}=\\underset{h^{\\prime} h=1}{\\operatorname{argmax}} \\operatorname{var}\\left[h^{\\prime} X\\right] .\n\\]\nThe second principal component is \\(U_{2}=h_{2}^{\\prime} X\\) where\n\\[\nh_{2}=\\underset{h^{\\prime} h=1, h^{\\prime} h_{1}=0}{\\operatorname{argmax}} \\operatorname{var}\\left[h^{\\prime} X\\right] .\n\\]\nIn general, the \\(j^{t h}\\) principal component is \\(U_{j}=h_{j}^{\\prime} X\\) where\n\\[\nh_{j}=\\underset{h^{\\prime} h=1, h^{\\prime} h_{1}=0, \\ldots, h^{\\prime} h_{j-1}=0}{\\operatorname{argmax}} \\operatorname{var}\\left[h^{\\prime} X\\right] .\n\\]\nThe principal components of \\(X\\) are linear combinations \\(h^{\\prime} X\\) ranked by contribution to variance. By the properties of quadratic forms (Section A.15) the weight vectors \\(h_{j}\\) are the eigenvectors of \\(\\Sigma=\\operatorname{var}[X]\\).\nTheorem 11.8 The principal components of \\(X\\) are \\(U_{j}=h_{j}^{\\prime} X\\), where \\(h_{j}\\) is the eigenvector of \\(\\Sigma\\) associated with the \\(j^{\\text {th }}\\) ordered eigenvalue \\(\\lambda_{j}\\) of \\(\\Sigma\\).\nAnother way to see the PCA construction is as follows. Since \\(\\Sigma\\) is symmetric the spectral decomposition (Theorem A.3) states that \\(\\Sigma=\\boldsymbol{H} \\boldsymbol{D} \\boldsymbol{H}^{\\prime}\\) where \\(\\boldsymbol{H}=\\left[h_{1}, \\ldots, h_{k}\\right]\\) and \\(\\boldsymbol{D}=\\operatorname{diag}\\left(d_{1}, \\ldots, d_{k}\\right)\\) are the eigenvectors and eigenvalues of \\(\\Sigma\\). Since \\(\\Sigma\\) is positive semi-definite the eigenvalues are real, non-negative, and ordered \\(d_{1} \\geq d_{2} \\geq \\cdots \\geq d_{k}\\). Let \\(U=\\left(U_{1}, \\ldots, U_{k}\\right)\\) be the principal components of \\(X\\). By Theorem 11.8, \\(U=\\boldsymbol{H}^{\\prime} X\\). The covariance matrix of \\(U\\) is\n\\[\n\\operatorname{var}[U]=\\operatorname{var}\\left[\\boldsymbol{H}^{\\prime} X\\right]=\\boldsymbol{H}^{\\prime} \\Sigma \\boldsymbol{H}=\\boldsymbol{D}\n\\]\nwhich is diagonal. This shows that \\(\\operatorname{var}\\left[U_{j}\\right]=d_{j}\\) and the principal components are mutually uncorrelated. The relative variance contribution of the \\(j^{t h}\\) principal component is \\(d_{j} / \\operatorname{tr}(\\Sigma)\\).\nPrincipal components are sensitive to the scaling of \\(X\\). Consequently, it is recommended to first scale each element of \\(X\\) to have mean zero and unit variance. In this case \\(\\Sigma\\) is a correlation matrix.\nThe sample principal components are obtained by replacing the unknowns by sample estimators. Let \\(\\widehat{\\Sigma}\\) be the sample covariance or correlation matrix and \\(\\widehat{h}_{1}, \\widehat{h}_{2}, \\ldots, \\widehat{h}_{k}\\) its ordered eigenvectors. The sample principal components are \\(\\widehat{h}_{j}^{\\prime} X_{i}\\).\nTo illustrate we use the Duflo, Dupas, and Kremer (2011) dataset. In Table \\(11.1\\) we display the seven eigenvalues of the sample correlation matrix for the seven test scores described above. The seven eigenvalues sum to seven because we have applied PCA to the correlation matrix. The first eigenvalue is \\(4.0\\), implying that the first principal component explains \\(57 %\\) of the variance of the seven test scores. The second eigenvalue is \\(1.0\\), implying that the second principal component explains \\(15 %\\) of the variance. Together the first two components explain \\(72 %\\) of the variance of the seven test scores.\nIn Table \\(11.2\\) we display the weight vectors (eigenvectors) for the first two principal components. The weights for the first component are all positive and similar in magnitude. This means that the first Table 11.1: Eigenvalue Decomposition of Sample Correlation Matrix\n|Eigenvalue|Proportion|\n|:|———-|———-| |1| \\(4.02\\) | \\(0.57\\) | |2| \\(1.04\\) | \\(0.15\\) | |3| \\(0.57\\) | \\(0.08\\) | |4| \\(0.52\\) | \\(0.08\\) | |5| \\(0.37\\) | \\(0.05\\) | |6| \\(0.29\\) | \\(0.04\\) | |7| \\(0.19\\) | \\(0.03\\) |\nTable 11.2: Principal Component Weight Vectors\n\n\n\n\nFirst\nSecond\n\n\n\n\nwords\n\\(0.41\\)\n\\(-0.32\\)\n\n\nsentences\n\\(0.32\\)\n\\(-0.49\\)\n\n\nletters\n\\(0.40\\)\n\\(-0.13\\)\n\n\nspelling\n\\(0.43\\)\n\\(-0.28\\)\n\n\naddition\n\\(0.38\\)\n\\(0.41\\)\n\n\nsubtraction\n\\(0.35\\)\n\\(0.52\\)\n\n\nmultiplication\n\\(0.33\\)\n\\(0.36\\)\n\n\n\nprincipal component is similar to a simple average of the seven test scores. This is quite fascinating. This is consistent with our intuition that a simple average (e.g. the variable totalscore) captures most of the information contained in the seven test scores. The weights for the second component have a different pattern. The four literacy scores receive negative weight and the three math scores receive positive weight with similar magnitudes. This means that the second principal component is similar to the difference between a student’s math and verbal test scores. Taken together, the information in the first two principal components is equivalent to “average verbal” and “average math” test scores. What this shows is that \\(57 %\\) of the variation in the seven section test scores can be explained by a simple average (e.g. totalscore), and \\(72 %\\) can be explained by averages for the verbal and math halves of the test.\nIn Stata, principal components analysis can be implemented with the pca command. In \\(\\mathrm{R}\\) use prcomp or princomp. All three can be applied to either covariance matrices (unscaled data) or correlation matrices (normalized data) but they have different default settings. The Stata pca command by default normalizes the observations. The R commands by default do not normalize the observations."
  },
  {
    "objectID": "chpt11-multi-reg.html#factor-models",
    "href": "chpt11-multi-reg.html#factor-models",
    "title": "11  Multivariate Regression",
    "section": "11.13 Factor Models",
    "text": "11.13 Factor Models\nClosely related to principal components are factor models. These are statistical models which decompose random vectors into common factors and idiosyncratic errors. Factor models are popular throughout the social sciences. Consequently a variety of estimation methods have been developed. In the next few sections we focus on methods which are popular among economists.\nLet \\(X=\\left(X_{1}, \\ldots, X_{k}\\right)^{\\prime}\\) be a \\(k \\times 1\\) random vector (for example the seven test scores described in the previous section). Assume that the elements of \\(X\\) are scaled to have mean zero and unit variance.\nA single factor model for \\(X\\) is\n\\[\nX=\\lambda F+u\n\\]\nwhere \\(\\lambda \\in \\mathbb{R}^{k}\\) are factor loadings, \\(F \\in \\mathbb{R}\\) is a common factor, and \\(u \\in \\mathbb{R}^{k}\\) is a random error. The factor \\(F\\) is individual-specific while the coefficient \\(\\lambda\\) is common across individuals. The model (11.22) specifies that correlation between the elements of \\(X\\) is due to the common factor \\(F\\). In the student test score example it is intuitive to think of \\(F\\) as a student’s scholastic “aptitude”; in this case the vector \\(\\lambda\\) describes how scholastic aptitude affects the seven subject scores.\nA multiple factor model has \\(r<k\\) factors. We write the model as\n\\[\nX=\\Lambda F+u\n\\]\nwhere \\(\\Lambda\\) is a \\(k \\times r\\) matrix of factor loadings and \\(F=\\left(F_{1}, \\ldots, F_{r}\\right)^{\\prime}\\) is an \\(r \\times 1\\) vector of factors. In the student test score example possible factors could be “math aptitude”, “language skills”, “social skills”, “artistic ability”, “creativity”, etc. The factor loading matrix \\(\\Lambda\\) indicates the effect of each factor on each test score. The number of factors \\(r\\) is taken as known. We discuss selection of \\(r\\) later.\nThe error vector \\(u\\) is assumed to be mean zero, uncorrelated with \\(F\\), and (under correct specification) to have mutually uncorrelated elements. We write its covariance matrix as \\(\\Psi=\\mathbb{E}\\left[u u^{\\prime}\\right]\\). The factor vector \\(F\\) can either be treated as random or as a regressor. In this section we treat \\(F\\) as random; in the next we treat \\(F\\) as regressors. The random factors \\(F\\) are assumed mean zero and are normalized so that \\(\\mathbb{E}\\left[F F^{\\prime}\\right]=\\) \\(\\boldsymbol{I}_{r}\\)\nThe assumptions imply that the correlation matrix \\(\\Sigma=\\mathbb{E}\\left[X X^{\\prime}\\right]\\) equals\n\\[\n\\Sigma=\\Lambda \\Lambda^{\\prime}+\\Psi .\n\\]\nThe factor analysis literature often describes \\(\\Lambda \\Lambda^{\\prime}\\) as the communality and the idiosyncratic error matrix \\(\\Psi\\) as the uniqueness. The former is the portion of the variance which is explained by the factor model and the latter is the unexplained portion of the variance.\nThe model is often \\({ }^{1}\\) estimated by maximum likelihood. Under joint normality of \\((F, u)\\) the distribution of \\(X\\) is \\(\\mathrm{N}\\left(0, \\Lambda \\Lambda^{\\prime}+\\Psi\\right)\\). The parameters are \\(\\Lambda\\) and \\(\\Psi=\\operatorname{diag}\\left(\\psi_{1}, \\ldots, \\psi_{k}\\right)\\). The log-likelihood function of a random sample \\(\\left(X_{1}, \\ldots, X_{n}\\right)\\) is\n\\[\n\\ell_{n}(\\Lambda, \\Psi)=-\\frac{n k}{2} \\log (2 \\pi)-\\frac{n}{2} \\log \\operatorname{det}\\left(\\Lambda \\Lambda^{\\prime}+\\Psi\\right)-\\frac{n}{2} \\operatorname{tr}\\left(\\left(\\Lambda \\Lambda^{\\prime}+\\Psi\\right)^{-1} \\widehat{\\Sigma}\\right) .\n\\]\nThe \\(\\operatorname{MLE}(\\widehat{\\Lambda}, \\widehat{\\Psi})\\) maximizes \\(\\ell_{n}(\\Lambda, \\Psi)\\). There is not an algebraic solution so the estimator is found using numerical methods. Fortunately, computational algorithms are available in standard packages. A detailed description and analysis can be found in Anderson (2003, Chapter 14).\nThe form of the log-likelihood is intriguing. Notice that the log-likelihood is only a function of the observations through its correlation matrix \\(\\widehat{\\Sigma}\\), and only a function of the parameters through the population correlation matrix \\(\\Lambda \\Lambda^{\\prime}+\\Psi\\). The final term in (11.25) is a measure of the match between \\(\\widehat{\\Sigma}\\) and \\(\\Lambda \\Lambda^{\\prime}+\\Psi\\). Together, we see that the Gaussian log-likelihood is essentially a measure of the fit of the model and sample correlation matrices. It is therefore not reliant on the normality assumption.\nIt is often of interest to estimate the factors \\(F_{i}\\). Given \\(\\Lambda\\) the equation \\(X_{i}=\\Lambda F_{i}+u_{i}\\) can be viewed as a regression with coefficient \\(F_{i}\\). Its least squares estimator is \\(\\widehat{F}_{i}=\\left(\\Lambda^{\\prime} \\Lambda\\right)^{-1} \\Lambda^{\\prime} X_{i}\\). The GLS estimator (taking into account the covariance matrix of \\(\\left.u_{i}\\right)\\) is \\(\\widehat{F}_{i}=\\left(\\Lambda^{\\prime} \\Psi^{-1} \\Lambda\\right)^{-1} \\Lambda^{\\prime} \\Psi^{-1} X_{i}\\). This motivates the Bartlett scoring estimator\n\\[\n\\widetilde{F}_{i}=\\left(\\widehat{\\Lambda}^{\\prime} \\widehat{\\Psi}^{-1} \\widehat{\\Lambda}\\right)^{-1} \\widehat{\\Lambda}^{\\prime} \\widehat{\\Psi}^{-1} X_{i} .\n\\]\nThe idealized version satisfies\n\\[\n\\widehat{F}_{i}=\\left(\\Lambda^{\\prime} \\Psi^{-1} \\Lambda\\right)^{-1} \\Lambda^{\\prime} \\Psi^{-1}\\left(\\Lambda F_{i}+u_{i}\\right)=F_{i}+\\left(\\Lambda^{\\prime} \\Psi^{-1} \\Lambda\\right)^{-1} \\Lambda^{\\prime} \\Psi^{-1} u_{i}\n\\]\n\\({ }^{1}\\) There are other estimators used in applied factor analysis. However there is little reason to consider estimators beyond the MLE of this section and the principal components estimator of the next section. which is unbiased for \\(F_{i}\\) and has variance \\(\\left(\\Lambda^{\\prime} \\Psi^{-1} \\Lambda\\right)^{-1}\\). Thus the Barlett scoring estimator is typically described as “unbiased” though this is actually a property of its idealized version \\(\\widehat{F}_{i}\\).\nA second estimator for the factors can be constructed from the multivariate linear projection of \\(F\\) on \\(X\\). This is \\(F=A X+\\xi\\) where the coefficient matrix \\(\\boldsymbol{A}\\) is \\(r \\times k\\). The coefficient matrix equals\n\\[\n\\boldsymbol{A}=\\mathbb{E}\\left[F X^{\\prime}\\right] \\mathbb{E}\\left[X X^{\\prime}\\right]^{-1}=\\Lambda^{\\prime} \\Sigma^{-1},\n\\]\nthe second equation using \\(\\mathbb{E}\\left[F X^{\\prime}\\right]=\\mathbb{E}\\left[F(\\Lambda F+u)^{\\prime}\\right]=\\mathbb{E}\\left[F F^{\\prime}\\right] \\Lambda^{\\prime}+\\mathbb{E}\\left[F u^{\\prime}\\right]=\\Lambda^{\\prime}\\). The predicted value of \\(F_{i}\\) is \\(F_{i}^{*}=\\boldsymbol{A} X_{i}=\\Lambda^{\\prime} \\Sigma^{-1} X_{i}\\). This motivates the regression scoring estimator\n\\[\n\\bar{F}_{i}=\\widehat{\\Lambda}^{\\prime} \\widehat{\\Sigma}^{-1} X_{i} .\n\\]\nThe idealized version \\(F_{i}^{*}\\) has conditional expectation \\(\\Lambda^{\\prime} \\Sigma^{-1} \\Lambda F_{i}\\) and is thus biased for \\(F_{i}\\). Hence the regression scoring estimator \\(\\bar{F}_{i}\\) is often described as “biased”. Some algebraic manipulations reveal that \\(F_{i}^{*}\\) has MSE \\(\\boldsymbol{I}_{r}-\\Lambda^{\\prime}\\left(\\Lambda^{\\prime} \\Lambda+\\Psi\\right)^{-1} \\Lambda\\) which is smaller (in a positive definite sense) than the MSE of the idealized Bartlett estimator \\(\\widehat{F}_{i}\\).\nWhich estimator is preferred, Bartlett or regression scoring? The differences diminish when \\(k\\) is large so the choice is most relevant for small to moderate \\(k\\). The regression scoring estimator has lower approximate MSE, meaning that it is a more precise estimator. Thus based on estimation precision this is our recommended choice.\nThe factor loadings \\(\\Lambda\\) and factors \\(F\\) are not separately identified. To see this, notice that if you replace \\((\\Lambda, F)\\) with \\(\\Lambda^{*}=\\Lambda \\boldsymbol{G}\\) and \\(F^{*}=\\boldsymbol{G}^{\\prime} F\\) where \\(\\boldsymbol{G}\\) is \\(r \\times r\\) and orthonormal then the regression model is identical. Such replacements are called “rotations” in the factor analysis literature. Any orthogonal rotation of the factor loadings is an equally valid representation. The default MLE outputs are one specific rotation; others can be obtained by a variety of algorithms (which we do not review here). Consequently it is unwise to attribute meaning to the individual factor loading estimates.\nAnother important and tricky issue is selection of the number of factors \\(r\\). There is no clear guideline. One approach is to examine the principal component decomposition, look for a division between the “large” and “small eigenvalues, and set \\(r\\) to equal to the number of”large” eigenvalues. Another approach is based on testing. As a by-product of the MLE (and standard package implementations) we obtain the LR test for the null hypothesis of \\(r\\) factors against the alternative hypothesis of \\(k\\) factors. If the LR test rejects (has a small p-value) this is evidence that the given \\(r\\) may be too small.\nIn Stata, the \\(\\operatorname{MLE}(\\widehat{\\Lambda}, \\widehat{\\Psi})\\) can be calculated with the factor, ml factors (r) command. The factor estimates \\(\\widetilde{F}_{i}\\) and \\(\\bar{F}_{i}\\) can be calculated by the predict command with either the barlett or regression option, respectively. In \\(R\\), the command factanal ( \\(X\\), factors=r, rotation=“none”) calculates the \\(\\operatorname{MLE}(\\widehat{\\Lambda}, \\widehat{\\Psi})\\) and also calculates the factor estimates \\(\\widetilde{F}_{i}\\) and/or \\(\\bar{F}_{i}\\) using the scores option."
  },
  {
    "objectID": "chpt11-multi-reg.html#approximate-factor-models",
    "href": "chpt11-multi-reg.html#approximate-factor-models",
    "title": "11  Multivariate Regression",
    "section": "11.14 Approximate Factor Models",
    "text": "11.14 Approximate Factor Models\nThe MLE of the previous section is a good choice for factor estimation when the number of variables \\(k\\) is small and the factor model is believed to be correctly specified. In many economic applications of factor analysis, however, the number of variables is \\(k\\) is large. In such contexts the MLE can be computationally costly and/or unstable. Furthermore it is typically not credible to believe that the model is correctly specified; rather it is more reasonable to view the factor model as a useful approximation. In this section we explore an approach known as the approximate factor model with estimation by principal components. The estimation method is justified by an asymptotic framework where the number of variables \\(k \\rightarrow \\infty\\) The approximate factor model was introduced by Chamberlain and Rothschild (1983). It is the same as (11.23) but relaxes the assumption on the idiosyncratic error \\(u\\) so that the covariance matrix \\(\\Psi=\\) \\(\\mathbb{E}\\left[u u^{\\prime}\\right]\\) is left unrestricted. In this context the Gaussian MLE of the previous section is misspecified.\nChamberlain and Rothschild (and the literature which followed) proposed estimation by least squares. The idea is to treat the factors as unknown regressors and simultaneously estimate the factors \\(F_{i}\\) and factor loadings \\(\\Lambda\\). We first describe the estimation method.\nLet \\(\\left(X_{1}, \\ldots, X_{n}\\right)\\) be a sample centered at sample means. The least squares criterion is\n\\[\n\\frac{1}{n} \\sum_{i=1}^{n}\\left(X_{i}-\\Lambda F_{i}\\right)^{\\prime}\\left(X_{i}-\\Lambda F_{i}\\right) .\n\\]\nLet \\(\\left(\\widehat{\\Lambda}, \\widehat{F}_{1}, \\ldots, \\widehat{F}_{n}\\right)\\) be the joint minimizers. As \\(\\Lambda\\) and \\(F_{i}\\) are not separately identified a normalization is needed. For compatibility with the notation of the previous section we use \\(n^{-1} \\sum_{i=1}^{n} \\widehat{F}_{i} \\widehat{F}_{i}^{\\prime}=\\boldsymbol{I}_{r}\\).\nWe use a concentration argument to find the solution. As described in the previous section, each observation satisfies the multivariate equation \\(X_{i}=\\Lambda F_{i}+u_{i}\\). For fixed \\(\\Lambda\\) this is a set of \\(k\\) equations with \\(r\\) unknowns \\(F_{i}\\). The least squares solution is \\(\\widehat{F}_{i}(\\Lambda)=\\left(\\Lambda^{\\prime} \\Lambda\\right)^{-1} \\Lambda^{\\prime} X_{i}\\). Substituting this expression into the least squares criterion the concentrated least squares criterion for \\(\\Lambda\\) is\n\\[\n\\begin{aligned}\n\\frac{1}{n} \\sum_{i=1}^{n}\\left(X_{i}-\\Lambda \\widehat{F}_{i}(\\Lambda)\\right)^{\\prime}\\left(X_{i}-\\Lambda \\widehat{F}_{i}(\\Lambda)\\right) &=\\frac{1}{n} \\sum_{i=1}^{n}\\left(X_{i}-\\Lambda\\left(\\Lambda^{\\prime} \\Lambda\\right)^{-1} \\Lambda^{\\prime} X_{i}\\right)^{\\prime}\\left(X_{i}-\\Lambda\\left(\\Lambda^{\\prime} \\Lambda\\right)^{-1} \\Lambda^{\\prime} X_{i}\\right) \\\\\n&=\\frac{1}{n} \\sum_{i=1}^{n}\\left(X_{i}^{\\prime} X_{i}-X_{i}^{\\prime} \\Lambda\\left(\\Lambda^{\\prime} \\Lambda\\right)^{-1} \\Lambda^{\\prime} X_{i}\\right) \\\\\n&=\\operatorname{tr}[\\widehat{\\Sigma}]-\\operatorname{tr}\\left[\\left(\\Lambda^{\\prime} \\Lambda\\right)^{-1} \\Lambda^{\\prime} \\widehat{\\Sigma} \\Lambda\\right]\n\\end{aligned}\n\\]\nwhere \\(\\widehat{\\Sigma}=n^{-1} \\sum_{i=1}^{n} X_{i} X_{i}^{\\prime}\\) is the sample covariance matrix. The least squares estimator \\(\\widehat{\\Lambda}\\) minimizes this criterion. Let \\(\\widehat{\\boldsymbol{D}}\\) and \\(\\widehat{\\boldsymbol{H}}\\) be first \\(r\\) eigenvalues and eigenvectors of \\(\\widehat{\\Sigma}\\). Using the normalization \\(\\Lambda^{\\prime} \\Lambda=\\boldsymbol{I}_{r}\\), from the extrema results of Section A.15 the minimizer of the least squares criterion is \\(\\widehat{\\Lambda}=\\widehat{\\boldsymbol{H}}\\). More broadly any rotation of \\(\\widehat{\\boldsymbol{H}}\\) is valid. Consider \\(\\widehat{\\Lambda}=\\widehat{\\boldsymbol{H}} \\widehat{\\boldsymbol{D}}^{1 / 2}\\). Recall the expression for the factors \\(\\widehat{F}_{i}(\\Lambda)=\\) \\(\\left(\\Lambda^{\\prime} \\Lambda\\right)^{-1} \\Lambda^{\\prime} X_{i}\\). We find that the estimated factors are\n\\[\n\\widehat{F}_{i}=\\left(\\widehat{\\boldsymbol{D}}^{1 / 2} \\widehat{\\boldsymbol{H}}^{\\prime} \\widehat{\\boldsymbol{H}} \\widehat{\\boldsymbol{D}}^{1 / 2}\\right)^{-1} \\widehat{\\boldsymbol{D}}^{1 / 2} \\widehat{\\boldsymbol{H}}^{\\prime} X_{i}=\\widehat{\\boldsymbol{D}}^{-1 / 2} \\widehat{\\boldsymbol{H}}^{\\prime} X_{i}\n\\]\nWe calculate that\n\\[\nn^{-1} \\sum_{i=1}^{n} \\widehat{F}_{i} \\widehat{F}_{i}^{\\prime}=\\widehat{\\boldsymbol{D}}^{-1 / 2} \\widehat{\\boldsymbol{H}}^{\\prime} \\widehat{\\Sigma} \\widehat{\\boldsymbol{H}} \\widehat{\\boldsymbol{D}}^{-1 / 2 \\prime}=\\widehat{\\boldsymbol{D}}^{-1 / 2} \\widehat{\\boldsymbol{D}} \\widehat{\\boldsymbol{D}}^{-1 / 2 \\prime}=\\boldsymbol{I}_{r}\n\\]\nwhich is the desired normalization. This shows that the rotation \\(\\widehat{\\Lambda}=\\widehat{\\boldsymbol{H}} \\widehat{\\boldsymbol{D}}^{1 / 2}\\) produces factor estimates satisfying this normalization.\nWe have proven the following result.\nTheorem 11.9 The least squares estimator of the factor model (11.23) under the normalization \\(n^{-1} \\sum_{i=1}^{n} \\widehat{F}_{i} \\widehat{F}_{i}^{\\prime}=\\boldsymbol{I}_{r}\\) has the following solution:\n\nLet \\(\\widehat{\\boldsymbol{D}}=\\operatorname{diag}\\left[\\widehat{d}_{1}, \\ldots, \\widehat{d}_{r}\\right]\\) and \\(\\widehat{\\boldsymbol{H}}=\\left[\\widehat{h}_{1}, \\ldots, \\widehat{h}_{r}\\right]\\) be the first \\(r\\) eigenvalues and eigenvectors of the sample covariance matrix \\(\\widehat{\\Sigma}\\).\n\\(\\widehat{\\Lambda}=\\widehat{\\boldsymbol{H}} \\widehat{\\boldsymbol{D}}^{1 / 2}\\).\n\\(\\widehat{F}_{i}=\\widehat{\\boldsymbol{D}}^{-1 / 2} \\widehat{\\boldsymbol{H}}^{\\prime} X_{i}\\). Theorem \\(11.9\\) shows that the least squares estimator is based on an eigenvalue decomposition of the covariance matrix. This is computationally stable even in high dimensions.\n\nThe factor estimates are the principal components scaled by the eigenvalues of \\(\\widehat{\\Sigma}\\). Specifically, the \\(j^{t h}\\) factor estimate is \\(\\widehat{F}_{j i}=\\widehat{d}_{j}^{-1 / 2} \\widehat{h}_{j}^{\\prime} X\\). Consequently many authors call this estimator the “principalcomponent method”.\nUnfortunately, \\(\\widehat{\\Lambda}\\) is inconsistent for \\(\\Lambda\\) if \\(k\\) is fixed, as we now show. By the WLLN and CMT, \\(\\widehat{\\Sigma} \\underset{p}{\\longrightarrow}\\) and \\(\\widehat{\\boldsymbol{H}} \\underset{p}{\\longrightarrow} \\boldsymbol{H}\\), the first \\(r\\) eigenvectors of \\(\\Sigma\\). When \\(\\Psi\\) is diagonal, the eigenvectors of \\(\\Sigma=\\Lambda \\Lambda^{\\prime}+\\Psi\\) do not lie in the range space of \\(\\Lambda\\) except in the special case \\(\\Psi=\\sigma^{2} \\boldsymbol{I}_{k}\\). Consequently the estimator \\(\\widehat{\\Lambda}\\) is inconsistent.\nThis inconsistency should not be viewed as surprising. The sample has a total of \\(n k\\) observations and the model has a total of \\(n r+k r-r(r+1) / 2\\) parameters. Since the number of estimated pararameters is proportional to sample size we should not expect estimator consistency.\nAs first recognized by Chamberlain and Rothschild, this deficiency diminishes as \\(k\\) increases. Specifically, assume that \\(k \\rightarrow \\infty\\) as \\(n \\rightarrow \\infty\\). One implication is that the number of observations \\(n k\\) increase at a rate faster than \\(n\\), while the number of parameters increase at a rate proportional to \\(n\\). Another implication is that as \\(k\\) increases there is increasing information about the factors.\nTo make this precise we add the following assumption. Let \\(\\lambda_{\\min }(\\boldsymbol{A})\\) and \\(\\lambda_{\\max }(\\boldsymbol{A})\\) denote the smallest and largest eigenvalues of a positive semi-definite matrix \\(\\boldsymbol{A}\\).\nAssumption \\(11.1\\) As \\(k \\rightarrow \\infty\\)\n\n\\(\\lambda_{\\max }(\\Psi) \\leq B<\\infty\\).\n\\(\\lambda_{\\min }\\left(\\Lambda^{\\prime} \\Lambda\\right) \\rightarrow \\infty\\) as \\(k \\rightarrow \\infty\\).\n\nAssumption 11.1.1 bounds the covariance matrix of the idiosyncratic errors. When \\(\\Psi=\\operatorname{diag}\\left(\\sigma_{1}^{2}, \\ldots, \\sigma_{k}^{2}\\right)\\) this is the same as bounding the individual variances. Effectively Assumption 11.1.1 means that while the elements of \\(u\\) can be correlated they cannot have a correlation structure similar to that of a factor model. Assumption 11.1.2 requires the factor loading matrix to increase in magnitude as the number of variables increases. This is a fairly mild requirement. When the factor loadings are of similar magnitude across variables, \\(\\lambda_{\\min }\\left(\\Lambda^{\\prime} \\Lambda\\right) \\sim k \\rightarrow \\infty\\). Conceptually, Assumption 11.1.2 requires additional variables to add information about the unobserved factors.\nAssumption \\(11.1\\) implies that in the covariance matrix factorization \\(\\Sigma=\\Lambda \\Lambda^{\\prime}+\\Psi\\) the component \\(\\Lambda \\Lambda^{\\prime}\\) dominates as \\(k\\) increases. This means that for large \\(k\\) the first \\(r\\) eigenvectors of \\(\\Sigma\\) are equivalent to those of \\(\\Lambda \\Lambda^{\\prime}\\), which are in the range space of \\(\\Lambda\\). This observation led Chamberlain and Rothschild (1983) to deduce that the principal components estimator is an asymptotic (large \\(k\\) ) analog estimator for the factor loadings and factors. Bai (2003) demonstrated that the estimator is consistent as \\(n, k \\rightarrow \\infty\\) jointly. The conditions and proofs are technical so are not reviewed here.\nNow consider the estimated factors\n\\[\n\\widehat{F}_{i}=\\boldsymbol{D}^{-1 / 2} \\boldsymbol{H}^{\\prime} X_{i}=\\boldsymbol{D}^{-1} \\Lambda^{\\prime} X_{i}\n\\]\nwhere for simplicity we ignore estimation error. Since \\(X_{i}=\\Lambda F_{i}+u_{i}\\) and \\(\\Lambda^{\\prime} \\Lambda=\\boldsymbol{D}\\) we can write this as\n\\[\n\\widehat{F}_{i}=F_{i}+\\boldsymbol{D}^{-1} \\Lambda^{\\prime} u_{i} .\n\\]\nThis shows that \\(\\widehat{F}_{i}\\) is an unbiased estimator for \\(F_{i}\\) and has variance \\(\\operatorname{var}\\left[\\widehat{F}_{i}\\right]=\\boldsymbol{D}^{-1} \\Lambda^{\\prime} \\Psi \\Lambda \\boldsymbol{D}^{-1}\\). Under Assumption 11.1, \\(\\left\\|\\operatorname{var}\\left[\\widehat{F}_{i}\\right]\\right\\| \\leq B / \\lambda_{\\min }\\left(\\Lambda^{\\prime} \\Lambda\\right) \\rightarrow 0\\). Thus \\(\\widehat{F}_{i}\\) is consistent for \\(F_{i}\\) as \\(k \\rightarrow \\infty\\). Bai (2003) shows that this extends to the feasible estimator as \\(n, k \\rightarrow \\infty\\).\nIn Stata, the least squares estimator \\(\\widehat{\\Lambda}\\) and factors \\(\\widehat{F}_{i}\\) can be calculated with the factor, pcf factors (r) command followed by predict. In \\(\\mathrm{R}\\) a feasible estimation approach is to calculate the factors by eigenvalue decomposition."
  },
  {
    "objectID": "chpt11-multi-reg.html#factor-models-with-additional-regressors",
    "href": "chpt11-multi-reg.html#factor-models-with-additional-regressors",
    "title": "11  Multivariate Regression",
    "section": "11.15 Factor Models with Additional Regressors",
    "text": "11.15 Factor Models with Additional Regressors\nConsider the model\n\\[\nX=\\Lambda F+B Z+e\n\\]\nwhere \\(X\\) and \\(e\\) are \\(k \\times 1, \\Lambda\\) is \\(k \\times r, F\\) is \\(r \\times 1, B\\) is \\(k \\times \\ell\\), and \\(Z\\) is \\(\\ell \\times 1\\).\nThe coefficients \\(\\Lambda\\) and \\(\\boldsymbol{B}\\) can be estimated by a combination of factor regression (either MLE or principal components) and least squares. The key is the following two observations:\n\nGiven \\(\\boldsymbol{B}\\), the coefficient \\(\\Lambda\\) can be estimated by factor regression applied to \\(X-\\boldsymbol{B} Z\\).\nGiven the factors \\(F\\), the coefficients \\(\\Lambda\\) and \\(\\boldsymbol{B}\\) can be estimated by multivariate least squares of \\(X\\) on \\(F\\) and \\(Z\\).\n\nEstimation iterates between these two steps. Start with a preliminary estimator of \\(\\boldsymbol{B}\\) obtained by multivariate least squares of \\(X\\) on \\(Z\\). Then apply the above two steps and iterate under convergence."
  },
  {
    "objectID": "chpt11-multi-reg.html#factor-augmented-regression",
    "href": "chpt11-multi-reg.html#factor-augmented-regression",
    "title": "11  Multivariate Regression",
    "section": "11.16 Factor-Augmented Regression",
    "text": "11.16 Factor-Augmented Regression\nIn the previous sections we considered factor models which decompose a set of variables into common factors and idiosyncratic errors. In this section we consider factor-augmented regression, which uses such common factors as regressors for dimension reduction.\nSuppose we have the variables \\((Y, Z, X)\\) where \\(Y \\in \\mathbb{R}, Z \\in \\mathbb{R}^{\\ell}\\), and \\(X \\in \\mathbb{R}^{k}\\). In practice, \\(k\\) may be large and the elements of \\(X\\) may be highly correlated. The factor-augmented regression model is\n\\[\n\\begin{aligned}\nY &=F^{\\prime} \\beta+Z^{\\prime} \\gamma+e \\\\\nX &=\\Lambda F+u \\\\\n\\mathbb{E}[F e] &=0 \\\\\n\\mathbb{E}[Z e] &=0 \\\\\n\\mathbb{E}\\left[F u^{\\prime}\\right] &=0 \\\\\n\\mathbb{E}[u e] &=0,\n\\end{aligned}\n\\]\nThe random variables are \\(e \\in \\mathbb{R}, F \\in \\mathbb{R}^{r}\\), and \\(u \\in \\mathbb{R}^{k}\\). The regression coefficients are \\(\\beta \\in \\mathbb{R}^{k}\\) and \\(\\gamma \\in \\mathbb{R}^{\\ell}\\). The matrix \\(\\Lambda\\) are the factor loadings.\nThis model specifies that the influence of \\(X\\) on \\(Y\\) is through the common factors \\(F\\). The idea is that the variation in the regressors is mostly captured by the variation in the factors, so the influence of the regressors can be captured through these factors. This can be viewed as a dimension-reduction technique as we have reduced the \\(k\\)-dimensional \\(X\\) to the \\(r\\)-dimensional \\(F\\). Interest typically focuses on the regressors \\(Z\\) and its coefficients \\(\\gamma\\). The factors \\(F\\) are included in the regression as “controls” and its coefficient \\(\\beta\\) is less typically of interest. Since it is difficult to interpret the factors \\(F\\) only their range space is identified it is generally prudent to avoid intrepreting the coefficients \\(\\beta\\). The model is typically estimated in multiple steps. First, the factor loadings \\(\\Lambda\\) and factors \\(F_{i}\\) are estimated by factor regression. In the case of principal-components estimation the factor estimates are the scaled \\({ }^{2}\\) principal components \\(\\widehat{F}_{i}=\\widehat{\\boldsymbol{D}}^{-1} \\widehat{\\Lambda}^{\\prime} X_{i}\\). Second, \\(Y\\) is regressed on the estimated factors and the other regressors to obtain the estimator of \\(\\beta\\) and \\(\\gamma\\). This second-step estimator equals (for simplicity assume there is no \\(Z\\) )\n\\[\n\\begin{aligned}\n\\widehat{\\beta} &=\\left(\\sum_{i=1}^{n} \\widehat{F}_{i} \\widehat{F}_{i}^{\\prime}\\right)^{-1}\\left(\\sum_{i=1}^{n} \\widehat{F}_{i} Y_{i}\\right) \\\\\n&=\\left(\\widehat{\\boldsymbol{D}}^{-1} \\widehat{\\Lambda}^{\\prime} \\frac{1}{n} \\sum_{i=1}^{n} X_{i} X_{i}^{\\prime} \\widehat{\\Lambda}^{-1}\\right)^{-1}\\left(\\widehat{\\boldsymbol{D}}^{-1} \\widehat{\\Lambda}^{\\prime} \\frac{1}{n} \\sum_{i=1}^{n} X_{i} Y_{i}\\right) .\n\\end{aligned}\n\\]\nNow let’s investigate its asymptotic behavior. As \\(n \\rightarrow \\infty, \\widehat{\\Lambda} \\underset{p}{\\rightarrow} \\Lambda\\) and \\(\\widehat{\\boldsymbol{D}} \\underset{p}{\\rightarrow} \\boldsymbol{D}\\) so\n\\[\n\\widehat{\\beta} \\underset{p}{\\longrightarrow} \\beta^{*}=\\left(\\boldsymbol{D}^{-1} \\Lambda^{\\prime} \\mathbb{E}\\left[X X^{\\prime}\\right] \\Lambda \\boldsymbol{D}^{-1}\\right)^{-1}\\left(\\boldsymbol{D}^{-1} \\Lambda^{\\prime} \\mathbb{E}[X Y]\\right) .\n\\]\nRecall \\(\\mathbb{E}\\left[X X^{\\prime}\\right]=\\Lambda \\Lambda^{\\prime}+\\Psi\\) and \\(\\Lambda^{\\prime} \\Lambda=\\boldsymbol{D}\\). We calculate that\n\\[\n\\mathbb{E}[X Y]=\\mathbb{E}\\left[(\\Lambda F+u)\\left(F^{\\prime} \\beta+e\\right)\\right]=\\Lambda \\beta .\n\\]\nWe find that the right-hand-side of (11.26) equals\n\\[\n\\beta^{*}=\\left(D^{-1} \\Lambda^{\\prime}\\left(\\Lambda \\Lambda^{\\prime}+\\Psi\\right) \\Lambda \\boldsymbol{D}^{-1}\\right)^{-1}\\left(\\boldsymbol{D}^{-1} \\Lambda^{\\prime} \\Lambda \\beta\\right)=\\left(\\boldsymbol{I}_{r}+\\boldsymbol{D}^{-1} \\Lambda^{\\prime} \\Psi \\Lambda \\boldsymbol{D}^{-1}\\right)^{-1} \\beta\n\\]\nwhich does not equal \\(\\beta\\). Thus \\(\\widehat{\\beta}\\) has a probability limit but is inconsistent for \\(\\beta\\) as \\(n \\rightarrow \\infty\\).\nThis deficiency diminishes as \\(k \\rightarrow \\infty\\). Indeed,\n\\[\n\\left\\|\\boldsymbol{D}^{-1} \\Lambda^{\\prime} \\Psi \\Lambda \\boldsymbol{D}^{-1}\\right\\| \\leq B\\left\\|\\boldsymbol{D}^{-1}\\right\\| \\rightarrow 0\n\\]\nas \\(k \\rightarrow \\infty\\). This implies \\(\\beta^{*} \\rightarrow \\beta\\). Hence, if we take the sequential asymptotic limit \\(n \\rightarrow \\infty\\) followed by \\(k \\rightarrow\\) \\(\\infty\\), we find \\(\\widehat{\\beta} \\underset{p}{\\longrightarrow} \\beta\\). This implies that the estimator is consistent. Bai (2003) demonstrated consistency under the more rigorous but technically challenging setting where \\(n, k \\rightarrow \\infty\\) jointly. The implication of this result is that factor augmented regression is consistent if both the sample size and dimension of \\(X\\) are large.\nFor asymptotic normality of \\(\\widehat{\\beta}\\) it turns out that we need to strengthen Assumption 11.1.2. The relevant condition is \\(n^{-1 / 2} \\lambda_{\\min }\\left(\\Lambda^{\\prime} \\Lambda\\right) \\rightarrow \\infty\\). This is similar to the condition that \\(k^{2} / n \\rightarrow \\infty\\). This is technical but can be interpreted as meaning that \\(k\\) is large relative to \\(\\sqrt{n}\\). Intuitively, this requires that dimension of \\(X\\) is larger than sample size \\(n\\).\nIn Stata, estimation takes the following steps. First, the factor command is used to estimate the factor model. Either MLE or principal components estimation can be used. Second, the predict command is used to estimate the factors, either by Barlett or regression scoring. Third, the factors are treated as regressors in an estimated regression."
  },
  {
    "objectID": "chpt11-multi-reg.html#multivariate-normal",
    "href": "chpt11-multi-reg.html#multivariate-normal",
    "title": "11  Multivariate Regression",
    "section": "11.17 Multivariate Normal*",
    "text": "11.17 Multivariate Normal*\nSome interesting sampling results hold for matrix-valued normal variates. Let \\(\\boldsymbol{Y}\\) be an \\(n \\times m\\) matrix whose rows are independent and distributed \\(\\mathrm{N}(\\mu, \\Sigma)\\). We say that \\(\\boldsymbol{Y}\\) is multivariate matrix normal, and\n\\({ }^{2}\\) The unscaled principal components can equivalently be used if the coefficients \\(\\widehat{\\beta}\\) are not reported. The coefficient estimates \\(\\hat{\\gamma}\\) are unaffected by the choice of factor scaling. write \\(Y \\sim \\mathrm{N}\\left(\\bar{\\mu}, I_{n} \\otimes \\Sigma\\right)\\), where \\(\\bar{\\mu}\\) is \\(n \\times m\\) with each row equal to \\(\\mu^{\\prime}\\). The notation is due to the fact that \\(\\operatorname{vec}\\left((\\boldsymbol{Y}-\\mu)^{\\prime}\\right) \\sim \\mathrm{N}\\left(0, \\boldsymbol{I}_{n} \\otimes \\Sigma\\right)\\)\nDefinition 11.2 If \\(n \\times m Y \\sim \\mathrm{N}\\left(\\bar{\\mu}, I_{n} \\otimes \\Sigma\\right)\\) then \\(W=Y^{\\prime} Y\\) is distributed Wishart with \\(n\\) degress of freedom and covariance matrix \\(\\Sigma\\), and is written as \\(W \\sim\\) \\(W_{m}(n, \\Sigma)\\).\nThe Wishart is a multivariate generalization of the chi-square. If \\(W \\sim W_{1}\\left(n, \\sigma^{2}\\right)\\) then \\(W \\sim \\sigma^{2} \\chi_{n}^{2}\\).\nThe Wishart arises as the exact distribution of a sample covariance matrix in the normal sampling model. The bias-corrected estimator of \\(\\Sigma\\) is\n\\[\n\\widehat{\\Sigma}=\\frac{1}{n-1} \\sum_{i=1}^{n}\\left(Y_{i}-\\bar{Y}\\right)\\left(Y_{i}-\\bar{Y}\\right)^{\\prime} .\n\\]\nTheorem 11.10 If \\(Y_{i} \\sim \\mathrm{N}(\\mu, \\Sigma)\\) are independent then \\(\\widehat{\\Sigma} \\sim W_{m}\\left(n-1, \\frac{1}{n-1} \\Sigma\\right)\\).\nThe following manipulation is useful.\nTheorem 11.11 If \\(W \\sim W_{m}(n, \\Sigma)\\) then for \\(m \\times 1 \\alpha,\\left(\\alpha^{\\prime} W^{-1} \\alpha\\right)^{-1} \\sim \\frac{\\chi_{n-m+1}^{2}}{\\alpha^{\\prime} \\Sigma^{-1} \\alpha}\\)\nTo prove this, note that without loss of generality we can take \\(\\Sigma=\\boldsymbol{I}_{m}\\) and \\(\\alpha^{\\prime} \\alpha=1\\). Let \\(\\boldsymbol{H}\\) be \\(m \\times m\\) orthonormal with first row equal to \\(\\alpha\\). so that \\(\\boldsymbol{H} \\alpha=\\left(\\begin{array}{l}1 \\\\ 0\\end{array}\\right)\\). Since the distribution of \\(\\boldsymbol{Y}\\) and \\(\\boldsymbol{Y} \\boldsymbol{H}\\) are identical we can without loss of generality set \\(\\alpha=\\left(\\begin{array}{c}1 \\\\ 0\\end{array}\\right)\\). Partition \\(\\boldsymbol{Y}=\\left[\\boldsymbol{Y}_{1}, \\boldsymbol{Y}_{2}\\right]\\) where \\(\\boldsymbol{Y}_{1}\\) is \\(n \\times 1, \\boldsymbol{Y}_{2}\\) is \\(n \\times(m-1)\\), and they are independent. Then\n\\[\n\\begin{aligned}\n\\left(\\alpha^{\\prime} W^{-1} \\alpha\\right)^{-1} &=\\left(\\left(\\begin{array}{ll}\n1 & 0\n\\end{array}\\right)\\left(\\begin{array}{cc}\n\\boldsymbol{Y}_{1}^{\\prime} \\boldsymbol{Y}_{1} & \\boldsymbol{Y}_{1}^{\\prime} \\boldsymbol{Y}_{2} \\\\\n\\boldsymbol{Y}_{2}^{\\prime} \\boldsymbol{Y}_{1} & \\boldsymbol{Y}_{2}^{\\prime} \\boldsymbol{Y}_{2}\n\\end{array}\\right)^{-1}\\left(\\begin{array}{l}\n1 \\\\\n0\n\\end{array}\\right)\\right)^{-1} \\\\\n&=\\boldsymbol{Y}_{1}^{\\prime} \\boldsymbol{Y}_{1}-\\boldsymbol{Y}_{1}^{\\prime} \\boldsymbol{Y}_{2}\\left(\\boldsymbol{Y}_{2}^{\\prime} \\boldsymbol{Y}_{2}\\right)^{-1} \\boldsymbol{Y}_{2}^{\\prime} \\boldsymbol{Y}_{1} \\\\\n&=\\boldsymbol{Y}_{1}^{\\prime} \\boldsymbol{M}_{2} \\boldsymbol{Y}_{1} \\sim \\chi_{n-(m-1)}^{2}\n\\end{aligned}\n\\]\nwhere \\(\\boldsymbol{M}_{2}=\\boldsymbol{I}_{m-1}-\\boldsymbol{Y}_{2}\\left(\\boldsymbol{Y}_{2}^{\\prime} \\boldsymbol{Y}_{2}\\right)^{-1} \\boldsymbol{Y}_{2}^{\\prime}\\). The final distributional equality holds conditional on \\(\\boldsymbol{Y}_{2}\\) by the same argument in the proof of Theorem 5.7. Since this does not depend on \\(\\boldsymbol{Y}_{2}\\) it is the unconditional distribution as well. This establishes the stated result.\nTo test hypotheses about \\(\\mu\\) a classical statistic is known as Hotelling’s \\(T^{2}\\) :\n\\[\nT^{2}=n(\\bar{Y}-\\mu)^{\\prime} \\widehat{\\Sigma}^{-1}(\\bar{Y}-\\mu) .\n\\]\nTheorem 11.12 If \\(Y \\sim \\mathrm{N}(\\mu, \\Sigma)\\) then\n\\[\nT^{2} \\sim \\frac{m}{(n-m)(n-1)} F(m, n-m)\n\\]\na scaled F distribution.\nTo prove this recall that \\(\\bar{Y}\\) is independent of \\(\\widehat{\\Sigma}\\). Apply Theorem \\(11.11\\) with \\(\\alpha=\\bar{Y}-\\mu\\). Conditional on \\(\\bar{Y}\\) and using the fact that \\(\\widehat{\\Sigma} \\sim W_{m}\\left(n-1, \\frac{1}{n-1} \\Sigma\\right)\\),\n\\[\n\\begin{aligned}\n\\frac{n}{T^{2}} &=\\left((\\bar{Y}-\\Sigma)^{\\prime} \\widehat{\\Sigma}^{-1}(\\bar{Y}-\\Sigma)\\right)^{-1} \\\\\n& \\sim \\frac{\\chi_{n-1-m+1}^{2}}{(\\bar{Y}-\\mu)^{\\prime}\\left(\\frac{1}{n-1} \\Sigma\\right)^{-1}(\\bar{Y}-\\mu)} \\\\\n& \\sim n(n-1) \\frac{\\chi_{n-m}^{2}}{\\chi_{m}^{2}}\n\\end{aligned}\n\\]\nSince the two chi-square variables are independent, this is the stated result.\nA very interesting property of this result is that the \\(T^{2}\\) statistic is a multivariate quadratric form in normal random variables, yet it has the exact \\(F\\) distribution."
  },
  {
    "objectID": "chpt11-multi-reg.html#exercises",
    "href": "chpt11-multi-reg.html#exercises",
    "title": "11  Multivariate Regression",
    "section": "11.18 Exercises",
    "text": "11.18 Exercises\nExercise 11.1 Show (11.10) when the errors are conditionally homoskedastic (11.8).\nExercise 11.2 Show (11.11) when the regressors are common across equations \\(X_{j}=X\\).\nExercise 11.3 Show (11.12) when the regressors are common across equations \\(X_{j}=X\\) and the errors are conditionally homoskedastic (11.8).\nExercise 11.4 Prove Theorem 11.1.\nExercise 11.5 Show (11.13) when the regressors are common across equations \\(X_{j}=X\\).\nExercise 11.6 Show (11.14) when the regressors are common across equations \\(X_{j}=X\\) and the errors are conditionally homoskedastic (11.8).\nExercise 11.7 Prove Theorem 11.2.\nExercise 11.8 Prove Theorem 11.3.\nExercise \\(11.9\\) Show that (11.16) follows from the steps described.\nExercise 11.10 Show that (11.17) follows from the steps described.\nExercise \\(11.11\\) Prove Theorem 11.4. Exercise 11.12 Prove Theorem 11.5.\nHint: First, show that it is sufficient to show that\n\\[\n\\mathbb{E}\\left[\\bar{X}^{\\prime} \\bar{X}\\right]\\left(\\mathbb{E}\\left[\\bar{X}^{\\prime} \\Sigma^{-1} \\bar{X}\\right]\\right)^{-1} \\mathbb{E}\\left[\\bar{X}^{\\prime} \\bar{X}\\right] \\leq \\mathbb{E}\\left[\\bar{X}^{\\prime} \\Sigma \\bar{X}\\right] .\n\\]\nSecond, rewrite this equation using the transformations \\(U=\\Sigma^{1 / 2} \\bar{X}\\) and \\(V=\\Sigma^{1 / 2} \\bar{X}\\), and then apply the matrix Cauchy-Schwarz inequality (B.33).\nExercise 11.13 Prove Theorem 11.6.\nExercise \\(11.14\\) Take the model\n\\[\n\\begin{aligned}\nY &=\\pi^{\\prime} \\beta+e \\\\\n\\pi &=\\mathbb{E}[X \\mid Z]=\\Gamma^{\\prime} Z \\\\\n\\mathbb{E}[e \\mid Z] &=0\n\\end{aligned}\n\\]\nwhere \\(Y\\) is scalar, \\(X\\) is a \\(k\\) vector and \\(Z\\) is an \\(\\ell\\) vector. \\(\\beta\\) and \\(\\pi\\) are \\(k \\times 1\\) and \\(\\Gamma\\) is \\(\\ell \\times k\\). The sample is \\(\\left(Y_{i}, X_{i}, Z_{i}: i=1, \\ldots, n\\right)\\) with \\(\\pi_{i}\\) unobserved.\nConsider the estimator \\(\\widehat{\\beta}\\) for \\(\\beta\\) by OLS of \\(Y\\) on \\(\\widehat{\\pi}=\\widehat{\\Gamma}^{\\prime} Z\\) where \\(\\widehat{\\Gamma}\\) is the OLS coefficient from the multivariate regression of \\(X\\) on \\(Z\\).\n\nShow that \\(\\widehat{\\beta}\\) is consistent for \\(\\beta\\).\nFind the asymptotic distribution \\(\\sqrt{n}(\\widehat{\\beta}-\\beta)\\) as \\(n \\rightarrow \\infty\\) assuming that \\(\\beta=0\\).\nWhy is the assumption \\(\\beta=0\\) an important simplifying condition in part (b)?\nUsing the result in (c) construct an appropriate asymptotic test for the hypothesis \\(\\mathbb{H}_{0}: \\beta=0\\).\n\nExercise \\(11.15\\) The observations are i.i.d., \\(\\left(Y_{1 i}, Y_{2 i}, X_{i}: i=1, \\ldots, n\\right)\\). The dependent variables \\(Y_{1}\\) and \\(Y_{2}\\) are real-valued. The regressor \\(X\\) is a \\(k\\)-vector. The model is the two-equation system\n\\[\n\\begin{aligned}\nY_{1} &=X^{\\prime} \\beta_{1}+e_{1} \\\\\n\\mathbb{E}\\left[X e_{1}\\right] &=0 \\\\\nY_{2} &=X^{\\prime} \\beta_{2}+e_{2} \\\\\n\\mathbb{E}\\left[X e_{2}\\right] &=0 .\n\\end{aligned}\n\\]\n\nWhat are the appropriate estimators \\(\\widehat{\\beta}_{1}\\) and \\(\\widehat{\\beta}_{2}\\) for \\(\\beta_{1}\\) and \\(\\beta_{2}\\) ?\nFind the joint asymptotic distribution of \\(\\widehat{\\beta}_{1}\\) and \\(\\widehat{\\beta}_{2}\\).\nDescribe a test for \\(\\mathbb{H}_{0}: \\beta_{1}=\\beta_{2}\\)."
  },
  {
    "objectID": "chpt12-iv.html#introduction",
    "href": "chpt12-iv.html#introduction",
    "title": "12  Instrumental Variables",
    "section": "12.1 Introduction",
    "text": "12.1 Introduction\nThe concepts of endogeneity and instrumental variable are fundamental to econometrics, and mark a substantial departure from other branches of statistics. The ideas of endogeneity arise naturally in economics from models of simultaneous equations, most notably the classic supply/demand model of price determination.\nThe identification problem in simultaneous equations dates back to Philip Wright (1915) and Working (1927). The method of instrumental variables first appears in an Appendix of a 1928 book by Philip Wright, though the authorship is sometimes credited to his son Sewell Wright. The label “instrumental variables” was introduced by Reiersøl (1945). An excellent review of the history of instrumental variables is Stock and Trebbi (2003)."
  },
  {
    "objectID": "chpt12-iv.html#overview",
    "href": "chpt12-iv.html#overview",
    "title": "12  Instrumental Variables",
    "section": "12.2 Overview",
    "text": "12.2 Overview\nWe say that there is endogeneity in the linear model\n\\[\nY=X^{\\prime} \\beta+e\n\\]\nif \\(\\beta\\) is the parameter of interest and\n\\[\n\\mathbb{E}[X e] \\neq 0 \\text {. }\n\\]\nThis is a core problem in econometrics and largely differentiates the field from statistics. To distinguish (12.1) from the regression and projection models, we will call (12.1) a structural equation and \\(\\beta\\) a structural parameter. When (12.2) holds, it is typical to say that \\(X\\) is endogenous for \\(\\beta\\).\nEndogeneity cannot happen if the coefficient is defined by linear projection. Indeed, we can define the linear projection coefficient \\(\\beta^{*}=\\mathbb{E}\\left[X X^{\\prime}\\right]^{-1} \\mathbb{E}[X Y]\\) and linear projection equation\n\\[\n\\begin{aligned}\nY &=X^{\\prime} \\beta^{*}+e^{*} \\\\\n\\mathbb{E}\\left[X e^{*}\\right] &=0 .\n\\end{aligned}\n\\]\nHowever, under endogeneity (12.2) the projection coefficient \\(\\beta^{*}\\) does not equal the structural parameter \\(\\beta\\). Indeed,\n\\[\n\\begin{aligned}\n\\beta^{*} &=\\left(\\mathbb{E}\\left[X X^{\\prime}\\right]\\right)^{-1} \\mathbb{E}[X Y] \\\\\n&=\\left(\\mathbb{E}\\left[X X^{\\prime}\\right]\\right)^{-1} \\mathbb{E}\\left[X\\left(X^{\\prime} \\beta+e\\right)\\right] \\\\\n&=\\beta+\\left(\\mathbb{E}\\left[X X^{\\prime}\\right]\\right)^{-1} \\mathbb{E}[X e] \\neq \\beta\n\\end{aligned}\n\\]\nthe final relation because \\(\\mathbb{E}[X e] \\neq 0\\).\nThus endogeneity requires that the coefficient be defined differently than projection. We describe such definitions as structural. We will present three examples in the following section.\nEndogeneity implies that the least squares estimator is inconsistent for the structural parameter. Indeed, under i.i.d. sampling, least squares is consistent for the projection coefficient.\n\\[\n\\widehat{\\beta} \\underset{p}{\\longrightarrow}\\left(\\mathbb{E}\\left[X X^{\\prime}\\right]\\right)^{-1} \\mathbb{E}[X Y]=\\beta^{*} \\neq \\beta .\n\\]\nThe inconsistency of least squares is typically referred to as endogeneity bias or estimation bias due to endogeneity. This is an imperfect label as the actual issue is inconsistency, not bias.\nAs the structural parameter \\(\\beta\\) is the parameter of interest, endogeneity requires the development of alternative estimation methods. We discuss those in later sections."
  },
  {
    "objectID": "chpt12-iv.html#examples",
    "href": "chpt12-iv.html#examples",
    "title": "12  Instrumental Variables",
    "section": "12.3 Examples",
    "text": "12.3 Examples\nThe concept of endogeneity may be easiest to understand by example. We discuss three. In each case it is important to see how the structural parameter \\(\\beta\\) is defined independently from the linear projection model.\nExample: Measurement error in the regressor. Suppose that \\((Y, Z)\\) are joint random variables, \\(\\mathbb{E}[Y \\mid Z]=Z^{\\prime} \\beta\\) is linear, and \\(\\beta\\) is the structural parameter. \\(Z\\) is not observed. Instead we observe \\(X=Z+u\\) where \\(u\\) is a \\(k \\times 1\\) measurement error, independent of \\(e\\) and \\(Z\\). This is an example of a latent variable model, where “latent” refers to an unobserved structural variable.\nThe model \\(X=Z+u\\) with \\(Z\\) and \\(u\\) independent and \\(\\mathbb{E}[u]=0\\) is known as classical measurement error. This means that \\(X\\) is a noisy but unbiased measure of \\(Z\\).\nBy substitution we can express \\(Y\\) as a function of the observed variable \\(X\\).\n\\[\nY=Z^{\\prime} \\beta+e=(X-u)^{\\prime} \\beta+e=X^{\\prime} \\beta+v\n\\]\nwhere \\(v=e-u^{\\prime} \\beta\\). This means that \\((Y, X)\\) satisfy the linear equation\n\\[\nY=X^{\\prime} \\beta+v\n\\]\nwith an error \\(v\\). But this error is not a projection error. Indeed,\n\\[\n\\mathbb{E}[X v]=\\mathbb{E}\\left[(Z+u)\\left(e-u^{\\prime} \\beta\\right)\\right]=-\\mathbb{E}\\left[u u^{\\prime}\\right] \\beta \\neq 0\n\\]\nif \\(\\beta \\neq 0\\) and \\(\\mathbb{E}\\left[u u^{\\prime}\\right] \\neq 0\\). As we learned in the previous section, if \\(\\mathbb{E}[X \\nu] \\neq 0\\) then least squares estimation will be inconsistent.\nWe can calculate the form of the projection coefficient (which is consistently estimated by least squares). For simplicity suppose that \\(k=1\\). We find\n\\[\n\\beta^{*}=\\beta+\\frac{\\mathbb{E}[X \\nu]}{\\mathbb{E}\\left[X^{2}\\right]}=\\beta\\left(1-\\frac{\\mathbb{E}\\left[u^{2}\\right]}{\\mathbb{E}\\left[X^{2}\\right]}\\right) .\n\\]\nSince \\(\\mathbb{E}\\left[u^{2}\\right] / \\mathbb{E}\\left[X^{2}\\right]<1\\) the projection coefficient shrinks the structural parameter \\(\\beta\\) towards zero. This is called measurement error bias or attenuation bias.\nTo illustrate, Figure 12.1(a) displays the impact of measurement error on the regression line. The three solid points are pairs \\((Y, Z)\\) which are measured without error. The regression function drawn through these three points is marked as “No Measurement Error”. The six open circles mark pairs \\((Y, X)\\) where \\(X=Z+u\\) with \\(u=\\{+1,-1\\}\\). Thus \\(X\\) is a mis-measured version of \\(Z\\). The six open circles spread the joint distribution along the \\(\\mathrm{x}\\)-axis, but not along the \\(\\mathrm{y}\\)-axis. The regression line drawn for these six points is marked as “With Measurement Error”. You can see that the latter regression line is flattened relative to the original regression function. This is the attenuation bias due to measurement error.\n\n\nMeasurement Error\n\n\n\nSupply and Demand\n\nFigure 12.1: Examples of Endogeneity\nExample: Supply and Demand. The variables \\(Q\\) and \\(P\\) (quantity and price) are determined jointly by the demand equation\n\\[\nQ=-\\beta_{1} P+e_{1}\n\\]\nand the supply equation\n\\[\nQ=\\beta_{2} P+e_{2} \\text {. }\n\\]\nAssume that \\(e=\\left(e_{1}, e_{2}\\right)\\) satisfies \\(\\mathbb{E}[e]=0\\) and \\(\\mathbb{E}\\left[e e^{\\prime}\\right]=\\boldsymbol{I}_{2}\\) (the latter for simplicity). The question is: if we regress \\(Q\\) on \\(P\\), what happens?\nIt is helpful to solve for \\(Q\\) and \\(P\\) in terms of the errors. In matrix notation,\n\\[\n\\left[\\begin{array}{cc}\n1 & \\beta_{1} \\\\\n1 & -\\beta_{2}\n\\end{array}\\right]\\left(\\begin{array}{l}\nQ \\\\\nP\n\\end{array}\\right)=\\left(\\begin{array}{l}\ne_{1} \\\\\ne_{2}\n\\end{array}\\right)\n\\]\nso\n\\[\n\\begin{aligned}\n\\left(\\begin{array}{l}\nQ \\\\\nP\n\\end{array}\\right) &=\\left[\\begin{array}{cc}\n1 & \\beta_{1} \\\\\n1 & -\\beta_{2}\n\\end{array}\\right]^{-1}\\left(\\begin{array}{c}\ne_{1} \\\\\ne_{2}\n\\end{array}\\right) \\\\\n&=\\left[\\begin{array}{cc}\n\\beta_{2} & \\beta_{1} \\\\\n1 & -1\n\\end{array}\\right]\\left(\\begin{array}{l}\ne_{1} \\\\\ne_{2}\n\\end{array}\\right)\\left(\\frac{1}{\\beta_{1}+\\beta_{2}}\\right) \\\\\n&=\\left(\\begin{array}{c}\n\\left(\\beta_{2} e_{1}+\\beta_{1} e_{2}\\right) /\\left(\\beta_{1}+\\beta_{2}\\right) \\\\\n\\left(e_{1}-e_{2}\\right) /\\left(\\beta_{1}+\\beta_{2}\\right)\n\\end{array}\\right) .\n\\end{aligned}\n\\]\nThe projection of \\(Q\\) on \\(P\\) yields \\(Q=\\beta^{*} P+e^{*}\\) with \\(\\mathbb{E}\\left[P e^{*}\\right]=0\\) and the projection coefficient is\n\\[\n\\beta^{*}=\\frac{\\mathbb{E}[P Q]}{\\mathbb{E}\\left[P^{2}\\right]}=\\frac{\\beta_{2}-\\beta_{1}}{2} .\n\\]\nThe projection coefficient \\(\\beta^{*}\\) equals neither the demand slope \\(\\beta_{1}\\) nor the supply slope \\(\\beta_{2}\\), but equals an average of the two. (The fact that it is a simple average is an artifact of the covariance structure.)\nThe OLS estimator satisfies \\(\\widehat{\\beta} \\underset{p}{\\rightarrow} \\beta^{*}\\) and the limit does not equal either \\(\\beta_{1}\\) or \\(\\beta_{2}\\). This is called simultaneous equations bias. This occurs generally when \\(Y\\) and \\(X\\) are jointly determined, as in a market equilibrium.\nGenerally, when both the dependent variable and a regressor are simultaneously determined then the regressor should be treated as endogenous.\nTo illustrate, Figure 12.1(b) draws a supply/demand model with Quantity on the y-axis and Price on the \\(\\mathrm{x}\\)-axis. The supply and demand equations are \\(Q=P+\\varepsilon_{1}\\) and \\(Q=4-P-\\varepsilon_{2}\\), respectively. Suppose that the errors each have the Rademacher distribution \\(\\varepsilon \\in\\{-1,+1\\}\\). This model has four equilibrium outcomes, marked by the four points in the figure. The regression line through these four points has a slope of zero and is marked as “Regression”. This is what would be measured by a least squares regression of observed quantity on observed price. This is endogeneity bias due to simultaneity.\nExample: Choice Variables as Regressors. Take the classic wage equation\n\\[\n\\log (\\text { wage })=\\beta \\text { education }+e\n\\]\nwith \\(\\beta\\) the average causal effect of education on wages. If wages are affected by unobserved ability, and individuals with high ability self-select into higher education, then \\(e\\) contains unobserved ability, so education and \\(e\\) will be positively correlated. Hence education is endogenous. The positive correlation means that the linear projection coefficient \\(\\beta^{*}\\) will be upward biased relative to the structural coefficient \\(\\beta\\). Thus least squares (which is estimating the projection coefficient) will tend to over-estimate the causal effect of education on wages.\nThis type of endogeneity occurs generally when \\(Y\\) and \\(X\\) are both choices made by an economic agent, even if they are made at different points in time.\nGenerally, when both the dependent variable and a regressor are choice variables made by the same agent, the variables should be treated as endogenous.\nThis example was illustrated back in Figure \\(2.8\\) which displayed the joint distribution of wages and education of the population of Jennifers and Georges. In Figure 2.8, the plotted Average Causal Effect is the structural impact (on average in the population) of college education on wages. The plotted regression line has a larger slope, as it adds the endogeneity bias due to the fact that education is a choice variable."
  },
  {
    "objectID": "chpt12-iv.html#endogenous-regressors",
    "href": "chpt12-iv.html#endogenous-regressors",
    "title": "12  Instrumental Variables",
    "section": "12.4 Endogenous Regressors",
    "text": "12.4 Endogenous Regressors\nWe have defined endogeneity as the context where a regressor is correlated with the equation error. The converse of endogeneity is exogeneity. That is, we say a regressor \\(X\\) is exogenous for \\(\\beta\\) if \\(\\mathbb{E}[X e]=\\) 0 . In general the distinction in an economic model is that a regressor \\(X\\) is endogenous if it is jointly determined with \\(Y\\), while a regressor \\(X\\) is exogenous if it is determined separately from \\(Y\\).\nIn most applications only a subset of the regressors are treated as endogenous. Partition \\(X=\\left(X_{1}, X_{2}\\right)\\) with dimensions \\(\\left(k_{1}, k_{2}\\right)\\) so that \\(X_{1}\\) contains the exogenous regressors and \\(X_{2}\\) contains the endogenous regressors. As the dependent variable \\(Y\\) is also endogenous, we sometimes differentiate \\(X_{2}\\) by calling it the endogenous right-hand-side variable. Similarly partition \\(\\beta=\\left(\\beta_{1}, \\beta_{2}\\right)\\). With this notation the structural equation is\n\\[\nY=X_{1}^{\\prime} \\beta_{1}+X_{2}^{\\prime} \\beta_{2}+e .\n\\]\nAn alternative notation is as follows. Let \\(Y_{2}=X_{2}\\) be the endogenous regressors and rename the dependent variable \\(Y\\) as \\(Y_{1}\\). Then the structural equation is\n\\[\nY_{1}=X_{1}^{\\prime} \\beta_{1}+Y_{2}^{\\prime} \\beta_{2}+e .\n\\]\nThis is especially useful so that the notation clarifies which variables are endogenous and which exogenous. We also write \\(\\vec{Y}=\\left(Y_{1}, Y_{2}\\right)\\) as the set of endogenous variables. We use the notation \\(\\vec{Y}\\) so that there is no confusion with \\(Y\\) as defined in (12.3).\nThe assumptions regarding the regressors and regression error are\n\\[\n\\begin{aligned}\n&\\mathbb{E}\\left[X_{1} e\\right]=0 \\\\\n&\\mathbb{E}\\left[Y_{2} e\\right] \\neq 0 .\n\\end{aligned}\n\\]\nThe endogenous regressors \\(Y_{2}\\) are the critical variables discussed in the examples of the previous section - simultaneous variables, choice variables, mis-measured regressors - that are potentially correlated with the equation error \\(e\\). In many applications \\(k_{2}\\) is small (1 or 2 ). The exogenous variables \\(X_{1}\\) are the remaining regressors (including the equation intercept) and can be low or high dimensional."
  },
  {
    "objectID": "chpt12-iv.html#instruments",
    "href": "chpt12-iv.html#instruments",
    "title": "12  Instrumental Variables",
    "section": "12.5 Instruments",
    "text": "12.5 Instruments\nTo consistently estimate \\(\\beta\\) we require additional information. One type of information which is commonly used in economic applications are what we call instruments.\nDefinition \\(12.1\\) The \\(\\ell \\times 1\\) random vector \\(Z\\) is an instrumental variable for (12.3) if\n\\[\n\\begin{aligned}\n\\mathbb{E}[Z e] &=0 \\\\\n\\mathbb{E}\\left[Z Z^{\\prime}\\right] &>0 \\\\\n\\operatorname{rank}\\left(\\mathbb{E}\\left[Z X^{\\prime}\\right]\\right) &=k .\n\\end{aligned}\n\\]\nThere are three components to the definition as given. The first (12.5) is that the instruments are uncorrelated with the regression error. The second (12.6) is a normalization which excludes linearly redundant instruments. The third (12.7) is often called the relevance condition and is essential for the identification of the model, as we discuss later. A necessary condition for (12.7) is that \\(\\ell \\geq k\\).\nCondition (12.5) - that the instruments are uncorrelated with the equation error - is often described as that they are exogenous in the sense that they are determined outside the model for \\(Y\\).\nNotice that the regressors \\(X_{1}\\) satisfy condition (12.5) and thus should be included as instrumental variables. They are therefore a subset of the variables \\(Z\\). Notationally we make the partition\n\\[\nZ=\\left(\\begin{array}{l}\nZ_{1} \\\\\nZ_{2}\n\\end{array}\\right)=\\left(\\begin{array}{c}\nX_{1} \\\\\nZ_{2}\n\\end{array}\\right) \\begin{aligned}\n&k_{1} \\\\\n&\\ell_{2}\n\\end{aligned} .\n\\]\nHere, \\(X_{1}=Z_{1}\\) are the included exogenous variables and \\(Z_{2}\\) are the excluded exogenous variables. That is, \\(Z_{2}\\) are variables which could be included in the equation for \\(Y\\) (in the sense that they are uncorrelated with \\(e\\) ) yet can be excluded as they have true zero coefficients in the equation. With this notation we can also write the structural equation (12.4) as\n\\[\nY_{1}=Z_{1}^{\\prime} \\beta_{1}+Y_{2}^{\\prime} \\beta_{2}+e .\n\\]\nThis is useful notation as it clarifies that the variable \\(Z_{1}\\) is exogenous and the variable \\(Y_{2}\\) is endogenous.\nMany authors describe \\(Z_{1}\\) as the “exogenous variables”, \\(Y_{2}\\) as the “endogenous variables”, and \\(Z_{2}\\) as the “instrumental variables”.\nWe say that the model is just-identified if \\(\\ell=k\\) and over-identified if \\(\\ell>k\\).\nWhat variables can be used as instrumental variables? From the definition \\(\\mathbb{E}[Z e]=0\\) the instrument must be uncorrelated with the equation error, meaning that it is excluded from the structural equation as mentioned above. From the rank condition (12.7) it is also important that the instrumental variables be correlated with the endogenous variables \\(Y_{2}\\) after controlling for the other exogenous variables \\(Z_{1}\\). These two requirements are typically interpreted as requiring that the instruments be determined outside the system for \\(\\vec{Y}\\), causally determine \\(Y_{2}\\), but do not causally determine \\(Y_{1}\\) except through \\(Y_{2}\\).\nLet’s take the three examples given above.\nMeasurement error in the regressor. When \\(X\\) is a mis-measured version of \\(Z\\) a common choice for an instrument \\(Z_{2}\\) is an alternative measurement of \\(Z\\). For this \\(Z_{2}\\) to satisfy the property of an instrumental variable the measurement error in \\(Z_{2}\\) must be independent of that in \\(X\\).\nSupply and Demand. An appropriate instrument for price \\(P\\) in a demand equation is a variable \\(Z_{2}\\) which influences supply but not demand. Such a variable affects the equilibrium values of \\(P\\) and \\(Q\\) but does not directly affect price except through quantity. Variables which affect supply but not demand are typically related to production costs.\nAn appropriate instrument for price in a supply equation is a variable which influences demand but not supply. Such a variable affects the equilibrium values of price and quantity but only affects price through quantity.\nChoice Variable as Regressor. An ideal instrument affects the choice of the regressor (education) but does not directly influence the dependent variable (wages) except through the indirect effect on the regressor. We will discuss an example in the next section."
  },
  {
    "objectID": "chpt12-iv.html#example-college-proximity",
    "href": "chpt12-iv.html#example-college-proximity",
    "title": "12  Instrumental Variables",
    "section": "12.6 Example: College Proximity",
    "text": "12.6 Example: College Proximity\nIn a influential paper David Card (1995) suggested if a potential student lives close to a college this reduces the cost of attendence and thereby raises the likelihood that the student will attend college. However, college proximity does not directly affect a student’s skills or abilities so should not have a direct effect on his or her market wage. These considerations suggest that college proximity can be used as an instrument for education in a wage regression. We use the simplest model reported in Card’s paper to illustrate the concepts of instrumental variables throughout the chapter.\nCard used data from the National Longitudinal Survey of Young Men (NLSYM) for 1976. A baseline least squares wage regression for his data set is reported in the first column of Table 12.1. The dependent variable is the log of weekly earnings. The regressors are education (years of schooling), experience (years of work experience, calculated as age (years) less education \\(+6\\) ), experience \\({ }^{2} / 100\\), Black, south (an indicator for residence in the southern region of the U.S.), and urban (an indicator for residence in a standard metropolitan statistical area). We drop observations for which wage is missing. The remaining sample has 3,010 observations. His data is the file Card1995 on the textbook website. The point estimate obtained by least squares suggests an \\(7 %\\) increase in earnings for each year of education.\nTable 12.1: Instrumental Variable Wage Regressions\n\n\n\n\n\n\n\n\n\n\n\n\neducation\nOLS\nIV(a)\nIV(b)\n2SLS(a)\n2SLS(b)\nLIML\n\n\n\n\n\n\\(0.074\\)\n\\(0.132\\)\n\\(0.133\\)\n\\(0.161\\)\n\\(0.160\\)\n\\(0.164\\)\n\n\n\n\\((0.004)\\)\n\\((0.049)\\)\n\\((0.051)\\)\n\\((0.040)\\)\n\\((0.041)\\)\n\\((0.042)\\)\n\n\n\n\\(0.084\\)\n\\(0.107\\)\n\\(0.056\\)\n\\(0.119\\)\n\\(0.047\\)\n\\(0.120\\)\n\n\nexperience \\(2 / 100\\)\n\\(-0.224\\)\n\\(-0.228\\)\n\\(-0.080\\)\n\\(-0.231\\)\n\\(-0.032\\)\n\\(-0.231\\)\n\n\n\n\\((0.032)\\)\n\\((0.035)\\)\n\\((0.133)\\)\n\\((0.037)\\)\n\\((0.127)\\)\n\\((0.037)\\)\n\n\nBlack\n\\(-0.190\\)\n\\(-0.131\\)\n\\(-0.103\\)\n\\(-0.102\\)\n\\(-0.064\\)\n\\(-0.099\\)\n\n\n\n\\((0.017)\\)\n\\((0.051)\\)\n\\((0.075)\\)\n\\((0.044)\\)\n\\((0.061)\\)\n\\((0.045)\\)\n\n\nsouth\n\\(-0.125\\)\n\\(-0.105\\)\n\\(-0.098\\)\n\\(-0.095\\)\n\\(-0.086\\)\n\\(-0.094\\)\n\n\n\n\\((0.015)\\)\n\\((0.023)\\)\n\\((0.0284)\\)\n\\((0.022)\\)\n\\((0.026)\\)\n\\((0.022)\\)\n\n\nurban\n\\(0.161\\)\n\\(0.131\\)\n\\(0.108\\)\n\\(0.116\\)\n\\(0.083\\)\n\\(0.115\\)\n\n\n\n\\((0.015)\\)\n\\((0.030)\\)\n\\((0.049)\\)\n\\((0.026)\\)\n\\((0.041)\\)\n\\((0.027)\\)\n\n\nSargan\n\n\n\n\\(0.82\\)\n\\(0.52\\)\n\\(0.82\\)\n\n\np-value\n\n\n\n\\(0.37\\)\n\\(0.47\\)\n\\(0.37\\)\n\n\n\nNotes:\n\nIV(a) uses college as an instrument for education.\nIV(b) uses college, age, and age \\(^{2} / 100\\) as instruments for education, experience, and experience \\({ }^{2} / 100\\).\n2SLS(a) uses public and private as instruments for education.\n\\(2 \\mathrm{SLS}(\\mathrm{b})\\) uses public, private, age, and age \\({ }^{2}\\) as instruments for education, experience, and experience \\(^{2} / 100\\).\nLIML uses public and private as instruments for education.\n\nAs discussed in the previous sections it is reasonable to view years of education as a choice made by an individual and thus is likely endogenous for the structural return to education. This means that least squares is an estimate of a linear projection but is inconsistent for coefficient of a structural equation representing the causal impact of years of education on expected wages. Labor economics predicts that ability, education, and wages will be positively correlated. This suggests that the population projection coefficient estimated by least squares will be higher than the structural parameter (and hence upwards biased). However, the sign of the bias is uncertain because there are multiple regressors and there are other potential sources of endogeneity.\nTo instrument for the endogeneity of education, Card suggested that a reasonable instrument is a dummy variable indicating if the individual grew up near a college. We will consider three measures:\ncollege Grew up in same county as a 4-year college\npublic Grew up in same county as a 4-year public college\nprivate Grew up in same county as a 4-year private college."
  },
  {
    "objectID": "chpt12-iv.html#reduced-form",
    "href": "chpt12-iv.html#reduced-form",
    "title": "12  Instrumental Variables",
    "section": "12.7 Reduced Form",
    "text": "12.7 Reduced Form\nThe reduced form is the relationship between the endogenous regressors \\(Y_{2}\\) and the instruments \\(Z\\). A linear reduced form model for \\(Y_{2}\\) is\n\\[\nY_{2}=\\Gamma^{\\prime} Z+u_{2}=\\Gamma_{12}^{\\prime} Z_{1}+\\Gamma_{22}^{\\prime} Z_{2}+u_{2}\n\\]\nThis is a multivariate regression as introduced in Chapter 11 . The \\(\\ell \\times k_{2}\\) coefficient matrix \\(\\Gamma\\) is defined by linear projection:\n\\[\n\\Gamma=\\mathbb{E}\\left[Z Z^{\\prime}\\right]^{-1} \\mathbb{E}\\left[Z Y_{2}^{\\prime}\\right]\n\\]\nThis implies \\(\\mathbb{E}\\left[Z u_{2}^{\\prime}\\right]=0\\). The projection coefficient (12.11) is well defined and unique under (12.6).\nWe also construct the reduced form for \\(Y_{1}\\). Substitute (12.10) into (12.9) to obtain\n\\[\n\\begin{aligned}\nY_{1} &=Z_{1}^{\\prime} \\beta_{1}+\\left(\\Gamma_{12}^{\\prime} Z_{1}+\\Gamma_{22}^{\\prime} Z_{2}+u_{2}\\right)^{\\prime} \\beta_{2}+e \\\\\n&=Z_{1}^{\\prime} \\lambda_{1}+Z_{2}^{\\prime} \\lambda_{2}+u_{1} \\\\\n&=Z^{\\prime} \\lambda+u_{1}\n\\end{aligned}\n\\]\nwhere\n\\[\n\\begin{aligned}\n&\\lambda_{1}=\\beta_{1}+\\Gamma_{12} \\beta_{2} \\\\\n&\\lambda_{2}=\\Gamma_{22} \\beta_{2} \\\\\n&u_{1}=u_{2}^{\\prime} \\beta_{2}+e .\n\\end{aligned}\n\\]\nWe can also write\n\\[\n\\lambda=\\bar{\\Gamma} \\beta\n\\]\nwhere\n\\[\n\\bar{\\Gamma}=\\left[\\begin{array}{cc}\n\\boldsymbol{I}_{k_{1}} & \\Gamma_{12} \\\\\n0 & \\Gamma_{22}\n\\end{array}\\right]=\\left[\\begin{array}{cc}\n\\boldsymbol{I}_{k_{1}} & \\Gamma \\\\\n0 &\n\\end{array}\\right] .\n\\]\nTogether, the reduced form equations for the system are\n\\[\n\\begin{aligned}\n&Y_{1}=\\lambda^{\\prime} Z+u_{1} \\\\\n&Y_{2}=\\Gamma^{\\prime} Z+u_{2} .\n\\end{aligned}\n\\]\nor\n\\[\n\\vec{Y}=\\left[\\begin{array}{cc}\n\\lambda_{1}^{\\prime} & \\lambda_{2}^{\\prime} \\\\\n\\Gamma_{12}^{\\prime} & \\Gamma_{22}^{\\prime}\n\\end{array}\\right] Z+u\n\\]\nwhere \\(u=\\left(u_{1}, u_{2}\\right)\\).\nThe relationships (12.14)-(12.16) are critically important for understanding the identification of the structural parameters \\(\\beta_{1}\\) and \\(\\beta_{2}\\), as we discuss below. These equations show the tight relationship between the structural parameters \\(\\left(\\beta_{1}\\right.\\) and \\(\\left.\\beta_{2}\\right)\\) and the reduced form parameters \\((\\Gamma\\) and \\(\\lambda)\\).\nThe reduced form equations are projections so the coefficients may be estimated by least squares (see Chapter 11). The least squares estimators of (12.11) and (12.13) are\n\\[\n\\begin{aligned}\n&\\widehat{\\Gamma}=\\left(\\sum_{i=1}^{n} Z_{i} Z_{i}^{\\prime}\\right)^{-1}\\left(\\sum_{i=1}^{n} Z_{i} Y_{2 i}^{\\prime}\\right) \\\\\n&\\widehat{\\lambda}=\\left(\\sum_{i=1}^{n} Z_{i} Z_{i}^{\\prime}\\right)^{-1}\\left(\\sum_{i=1}^{n} Z_{i} Y_{1 i}\\right) .\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "chpt12-iv.html#identification",
    "href": "chpt12-iv.html#identification",
    "title": "12  Instrumental Variables",
    "section": "12.8 Identification",
    "text": "12.8 Identification\nA parameter is identified if it is a unique function of the probability distribution of the observables. One way to show that a parameter is identified is to write it as an explicit function of population moments. For example, the reduced form coefficient matrices \\(\\Gamma\\) and \\(\\lambda\\) are identified because they can be written as explicit functions of the moments of the variables \\((Y, X, Z)\\). That is,\n\\[\n\\begin{aligned}\n&\\Gamma=\\mathbb{E}\\left[Z Z^{\\prime}\\right]^{-1} \\mathbb{E}\\left[Z Y_{2}^{\\prime}\\right] \\\\\n&\\lambda=\\mathbb{E}\\left[Z Z^{\\prime}\\right]^{-1} \\mathbb{E}\\left[Z Y_{1}\\right] .\n\\end{aligned}\n\\]\nThese are uniquely determined by the probability distribution of \\(\\left(Y_{1}, Y_{2}, Z\\right)\\) if Definition \\(12.1\\) holds, because this includes the requirement that \\(\\mathbb{E}\\left[Z Z^{\\prime}\\right]\\) is invertible.\nWe are interested in the structural parameter \\(\\beta\\). It relates to \\((\\lambda, \\Gamma)\\) through (12.16). \\(\\beta\\) is identified if it uniquely determined by this relation. This is a set of \\(\\ell\\) equations with \\(k\\) unknowns with \\(\\ell \\geq k\\). From linear algebra we know that there is a unique solution if and only if \\(\\bar{\\Gamma}\\) has full rank \\(k\\).\n\\[\n\\operatorname{rank}(\\bar{\\Gamma})=k .\n\\]\nUnder (12.22) \\(\\beta\\) can be uniquely solved from (12.16). If (12.22) fails then (12.16) has fewer equations than coefficients so there is not a unique solution.\nWe can write \\(\\bar{\\Gamma}=\\mathbb{E}\\left[Z Z^{\\prime}\\right]^{-1} \\mathbb{E}\\left[Z X^{\\prime}\\right]\\). Combining this with (12.16) we obtain\n\\[\n\\mathbb{E}\\left[Z Z^{\\prime}\\right]^{-1} \\mathbb{E}\\left[Z Y_{1}\\right]=\\mathbb{E}\\left[Z Z^{\\prime}\\right]^{-1} \\mathbb{E}\\left[Z X^{\\prime}\\right] \\beta\n\\]\nor\n\\[\n\\mathbb{E}\\left[Z Y_{1}\\right]=\\mathbb{E}\\left[Z X^{\\prime}\\right] \\beta\n\\]\nwhich is a set of \\(\\ell\\) equations with \\(k\\) unknowns. This has a unique solution if (and only if)\n\\[\n\\operatorname{rank}\\left(\\mathbb{E}\\left[Z X^{\\prime}\\right]\\right)=k\n\\]\nwhich was listed in (12.7) as a condition of Definition 12.1. (Indeed, this is why it was listed as part of the definition.) We can also see that (12.22) and (12.23) are equivalent ways of expressing the same requirement. If this condition fails then \\(\\beta\\) will not be identified. The condition (12.22)-(12.23) is called the relevance condition.\nIt is useful to have explicit expressions for the solution \\(\\beta\\). The easiest case is when \\(\\ell=k\\). Then (12.22) implies \\(\\bar{\\Gamma}\\) is invertible so the structural parameter equals \\(\\beta=\\bar{\\Gamma}^{-1} \\lambda\\). It is a unique solution because \\(\\bar{\\Gamma}\\) and \\(\\lambda\\) are unique and \\(\\bar{\\Gamma}\\) is invertible.\nWhen \\(\\ell>k\\) we can solve for \\(\\beta\\) by applying least squares to the system of equations \\(\\lambda=\\bar{\\Gamma} \\beta\\). This is \\(\\ell\\) equations with \\(k\\) unknowns and no error. The least squares solution is \\(\\beta=\\left(\\bar{\\Gamma}^{\\prime} \\bar{\\Gamma}\\right)^{-1} \\bar{\\Gamma}^{\\prime} \\lambda\\). Under (12.22) the matrix \\(\\bar{\\Gamma}^{\\prime} \\bar{\\Gamma}\\) is invertible so the solution is unique.\n\\(\\beta\\) is identified if \\(\\operatorname{rank}(\\bar{\\Gamma})=k\\), which is true if and only if \\(\\operatorname{rank}\\left(\\Gamma_{22}\\right)=k_{2}\\) (by the upper-diagonal structure of \\(\\bar{\\Gamma})\\). Thus the key to identification of the model rests on the \\(\\ell_{2} \\times k_{2}\\) matrix \\(\\Gamma_{22}\\) in (12.10). To see this, recall the reduced form relationships (12.14)-(12.15). We can see that \\(\\beta_{2}\\) is identified from (12.15) alone, and the necessary and sufficient condition is \\(\\operatorname{rank}\\left(\\Gamma_{22}\\right)=k_{2}\\). If this is satisfied then the solution equals \\(\\beta_{2}=\\left(\\Gamma_{22}^{\\prime} \\Gamma_{22}\\right)^{-1} \\Gamma_{22}^{\\prime} \\lambda_{2} \\cdot \\beta_{1}\\) is identified from this and (12.14), with the explicit solution \\(\\beta_{1}=\\lambda_{1}-\\Gamma_{12}\\left(\\Gamma_{22}^{\\prime} \\Gamma_{22}\\right)^{-1} \\Gamma_{22}^{\\prime} \\lambda_{2}\\). In the just-identified case \\(\\left(\\ell_{2}=k_{2}\\right)\\) these equations simplify as \\(\\beta_{2}=\\Gamma_{22}^{-1} \\lambda_{2}\\) and \\(\\beta_{1}=\\lambda_{1}-\\Gamma_{12} \\Gamma_{22}^{-1} \\lambda_{2}\\)"
  },
  {
    "objectID": "chpt12-iv.html#instrumental-variables-estimator",
    "href": "chpt12-iv.html#instrumental-variables-estimator",
    "title": "12  Instrumental Variables",
    "section": "12.9 Instrumental Variables Estimator",
    "text": "12.9 Instrumental Variables Estimator\nIn this section we consider the special case where the model is just-identified so that \\(\\ell=k\\).\nThe assumption that \\(Z\\) is an instrumental variable implies that \\(\\mathbb{E}[Z e]=0\\). Making the substitution \\(e=Y_{1}-X^{\\prime} \\beta\\) we find \\(\\mathbb{E}\\left[Z\\left(Y_{1}-X^{\\prime} \\beta\\right)\\right]=0\\). Expanding,\n\\[\n\\mathbb{E}\\left[Z Y_{1}\\right]-\\mathbb{E}\\left[Z X^{\\prime}\\right] \\beta=0 .\n\\]\nThis is a system of \\(\\ell=k\\) equations and \\(k\\) unknowns. Solving for \\(\\beta\\) we find\n\\[\n\\beta=\\left(\\mathbb{E}\\left[Z X^{\\prime}\\right]\\right)^{-1} \\mathbb{E}\\left[Z Y_{1}\\right] .\n\\]\nThis requires that the matrix \\(\\mathbb{E}\\left[Z X^{\\prime}\\right]\\) is invertible, which holds under (12.7) or equivalently (12.23).\nThe instrumental variables (IV) estimator \\(\\beta\\) replaces population by sample moments. We find\n\\[\n\\begin{aligned}\n\\widehat{\\beta}_{\\mathrm{iv}} &=\\left(\\frac{1}{n} \\sum_{i=1}^{n} Z_{i} X_{i}^{\\prime}\\right)^{-1}\\left(\\frac{1}{n} \\sum_{i=1}^{n} Z_{i} Y_{1 i}\\right) \\\\\n&=\\left(\\sum_{i=1}^{n} Z_{i} X_{i}^{\\prime}\\right)^{-1}\\left(\\sum_{i=1}^{n} Z_{i} Y_{1 i}\\right) .\n\\end{aligned}\n\\]\nMore generally, given any variable \\(W \\in \\mathbb{R}^{k}\\) it is common to refer to the estimator\n\\[\n\\widehat{\\beta}_{\\mathrm{iv}}=\\left(\\sum_{i=1}^{n} W_{i} X_{i}^{\\prime}\\right)^{-1}\\left(\\sum_{i=1}^{n} W_{i} Y_{1 i}\\right)\n\\]\nas the IV estimator for \\(\\beta\\) using the instrument \\(W\\).\nAlternatively, recall that when \\(\\ell=k\\) the structural parameter can be written as a function of the reduced form parameters as \\(\\beta=\\bar{\\Gamma}^{-1} \\lambda\\). Replacing \\(\\bar{\\Gamma}\\) and \\(\\lambda\\) by their least squares estimators (12.18)-(12.19) we can construct what is called the Indirect Least Squares (ILS) estimator. Using the matrix algebra representations\n\\[\n\\begin{aligned}\n\\widehat{\\beta}_{\\mathrm{ils}} &=\\widehat{\\bar{\\Gamma}}^{-1} \\widehat{\\lambda} \\\\\n&=\\left(\\left(\\boldsymbol{Z}^{\\prime} \\boldsymbol{Z}\\right)^{-1}\\left(\\boldsymbol{Z}^{\\prime} \\boldsymbol{X}\\right)\\right)^{-1}\\left(\\left(\\boldsymbol{Z}^{\\prime} \\boldsymbol{Z}\\right)^{-1}\\left(\\boldsymbol{Z}^{\\prime} \\boldsymbol{Y}_{1}\\right)\\right) \\\\\n&=\\left(\\boldsymbol{Z}^{\\prime} \\boldsymbol{X}\\right)^{-1}\\left(\\boldsymbol{Z}^{\\prime} \\boldsymbol{Z}\\right)\\left(\\boldsymbol{Z}^{\\prime} \\boldsymbol{Z}\\right)^{-1}\\left(\\boldsymbol{Z}^{\\prime} \\boldsymbol{Y}_{1}\\right) \\\\\n&=\\left(\\boldsymbol{Z}^{\\prime} \\boldsymbol{X}\\right)^{-1}\\left(\\boldsymbol{Z}^{\\prime} \\boldsymbol{Y}_{1}\\right) .\n\\end{aligned}\n\\]\nWe see that this equals the IV estimator (12.24). Thus the ILS and IV estimators are identical.\nGiven the IV estimator we define the residual \\(\\widehat{e}_{i}=Y_{1 i}-X_{i}^{\\prime} \\widehat{\\beta}_{\\mathrm{iv}}\\). It satisfies\n\\[\n\\boldsymbol{Z}^{\\prime} \\widehat{\\boldsymbol{e}}=\\boldsymbol{Z}^{\\prime} \\boldsymbol{Y}_{1}-\\boldsymbol{Z}^{\\prime} \\boldsymbol{X}\\left(\\boldsymbol{Z}^{\\prime} \\boldsymbol{X}\\right)^{-1}\\left(\\boldsymbol{Z}^{\\prime} \\boldsymbol{Y}_{1}\\right)=0\n\\]\nSince \\(Z\\) includes an intercept this means that the residuals sum to zero and are uncorrelated with the included and excluded instruments.\nTo illustrate IV regression we estimate the reduced form equations, treating education as endogenous and using college as an instrumental variable. The reduced form equations for log(wage) and education are reported in the first and second columns of Table 12.2. Table 12.2: Reduced Form Regressions\n\nOf particular interest is the equation for the endogenous regressor education, and the coefficients for the excluded instruments - in this case college. The estimated coefficient equals \\(0.337\\) with a small standard error. This implies that growing up near a 4-year college increases average educational attainment by \\(0.3\\) years. This seems to be a reasonable magnitude.\nSince the structural equation is just-identified with one right-hand-side endogenous variable the ILS/IV estimate for the education coefficient is the ratio of the coefficient estimates for the instrument college in the two equations, e.g. \\(0.045 / 0.337=0.13\\), implying a \\(13 %\\) return to each year of education. This is substantially greater than the \\(7 %\\) least squares estimate from the first column of Table 12.1. The IV estimates of the full equation are reported in the second column of Table 12.1. One first reaction is surprise that the IV estimate is larger than the OLS estimate. The endogeneity of educational choice should lead to upward bias in the OLS estimator, which predicts that the IV estimate should have been smaller than the OLS estimator. An alternative explanation may be needed. One possibility is heterogeneous education effects (when the education coefficient \\(\\beta\\) is heterogenous across individuals). In Section \\(12.34\\) we show that in this context the IV estimator picks up this treatment effect for a subset of the population, and this may explain why IV estimation results in a larger estimated coefficient.\nCard (1995) also points out that if education is endogenous then so is our measure of experience as it is calculated by subtracting education from age. He suggests that we can use the variables age and age \\({ }^{2}\\) as instruments for experience and experience \\({ }^{2}\\). The age variables are exogenous (not choice variables) yet highly correlated with experience and experience \\({ }^{2}\\). Notice that this approach treats experience \\({ }^{2}\\) as a variable separate from experience. Indeed, this is the correct approach.\nFollowing this recommendation we now have three endogenous regressors and three instruments. We present the three reduced form equations for the three endogenous regressors in the third through fifth columns of Table 12.2. It is interesting to compare the equations for education and experience. The two sets of coefficients are simply the sign change of the other with the exception of the coefficient on age. Indeed this must be the case because the three variables are linearly related. Does this cause a problem for 2SLS? Fortunately, no. The fact that the coefficient on age is not simply a sign change means that the equations are not linearly singular. Hence Assumption (12.22) is not violated.\nThe IV estimates using the three instruments college, age, and age \\({ }^{2}\\) for the endogenous regressors education, experience, and experience \\({ }^{2}\\) is presented in the third column of Table 12.1. The estimate of the returns to schooling is not affected by this change in the instrument set, but the estimated return to experience profile flattens (the quadratic effect diminishes).\nThe IV estimator may be calculated in Stata using the ivregress 2 sls command."
  },
  {
    "objectID": "chpt12-iv.html#demeaned-representation",
    "href": "chpt12-iv.html#demeaned-representation",
    "title": "12  Instrumental Variables",
    "section": "12.10 Demeaned Representation",
    "text": "12.10 Demeaned Representation\nDoes the well-known demeaned representation for linear regression (3.18) carry over to the IV estimator? To see, write the linear projection equation in the format \\(Y_{1}=X^{\\prime} \\beta+\\alpha+e\\) where \\(\\alpha\\) is the intercept and \\(X\\) does not contain a constant. Similarly, partition the instrument as \\((1, Z)\\) where \\(Z\\) does not contain a constant. We can write the IV estimator for the \\(i^{t h}\\) equation as\n\\[\nY_{1 i}=X_{i}^{\\prime} \\widehat{\\beta}_{\\mathrm{iv}}+\\widehat{\\alpha}_{\\mathrm{iv}}+\\widehat{e}_{i} .\n\\]\nThe orthogonality (12.25) implies the two-equation system\n\\[\n\\begin{aligned}\n&\\sum_{i=1}^{n}\\left(Y_{1 i}-X_{i}^{\\prime} \\widehat{\\beta}_{\\mathrm{iv}}-\\widehat{\\alpha}_{\\mathrm{iv}}\\right)=0 \\\\\n&\\sum_{i=1}^{n} Z_{i}\\left(Y_{1 i}-X_{i}^{\\prime} \\widehat{\\beta}_{\\mathrm{iv}}-\\widehat{\\alpha}_{\\mathrm{iv}}\\right)=0 .\n\\end{aligned}\n\\]\nThe first equation implies \\(\\widehat{\\alpha}_{\\mathrm{iv}}=\\overline{Y_{1}}-\\bar{X}^{\\prime} \\widehat{\\beta}_{\\mathrm{iv}}\\). Substituting into the second equation\n\\[\n\\sum_{i=1}^{n} Z_{i}\\left(\\left(Y_{1 i}-\\overline{Y_{1}}\\right)-\\left(X_{i}-\\bar{X}\\right)^{\\prime} \\widehat{\\beta}_{\\mathrm{iv}}\\right)\n\\]\nand solving for \\(\\widehat{\\beta}_{\\text {iv }}\\) we find\n\\[\n\\begin{aligned}\n\\widehat{\\beta}_{\\mathrm{iv}} &=\\left(\\sum_{i=1}^{n} Z_{i}\\left(X_{i}-\\bar{X}\\right)^{\\prime}\\right)^{-1}\\left(\\sum_{i=1}^{n} Z_{i}\\left(Y_{1 i}-\\bar{Y}_{1}\\right)\\right) \\\\\n&=\\left(\\sum_{i=1}^{n}\\left(Z_{i}-\\bar{Z}\\right)\\left(X_{i}-\\bar{X}\\right)^{\\prime}\\right)^{-1}\\left(\\sum_{i=1}^{n}\\left(Z_{i}-\\bar{Z}\\right)\\left(Y_{1 i}-\\bar{Y}_{1}\\right)\\right) .\n\\end{aligned}\n\\]\nThus the demeaning equations for least squares carry over to the IV estimator. The coefficient estimator \\(\\widehat{\\beta}_{\\text {iv }}\\) is a function only of the demeaned data."
  },
  {
    "objectID": "chpt12-iv.html#wald-estimator",
    "href": "chpt12-iv.html#wald-estimator",
    "title": "12  Instrumental Variables",
    "section": "12.11 Wald Estimator",
    "text": "12.11 Wald Estimator\nIn many cases including the Card proximity example the excluded instrument is a binary (dummy) variable. Let’s focus on that case and suppose that the model has just one endogenous regressor and no other regressors beyond the intercept. The model can be written as \\(Y=X \\beta+\\alpha+e\\) with \\(\\mathbb{E}[e \\mid Z]=0\\) and \\(Z\\) binary. Take expectations of the structural equation given \\(Z=1\\) and \\(Z=0\\), respectively. We obtain\n\\[\n\\begin{aligned}\n&\\mathbb{E}[Y \\mid Z=1]=\\mathbb{E}[X \\mid Z=1] \\beta+\\alpha \\\\\n&\\mathbb{E}[Y \\mid Z=0]=\\mathbb{E}[X \\mid Z=0] \\beta+\\alpha .\n\\end{aligned}\n\\]\nSubtracting and dividing we obtain an expression for the slope coefficient\n\\[\n\\beta=\\frac{\\mathbb{E}[Y \\mid Z=1]-\\mathbb{E}[Y \\mid Z=0]}{\\mathbb{E}[X \\mid Z=1]-\\mathbb{E}[X \\mid Z=0]} .\n\\]\nThe natural moment estimator replaces the expectations by the averages within the “grouped data” where \\(Z_{i}=1\\) and \\(Z_{i}=0\\), respectively. That is, define the group means\n\\[\n\\begin{array}{ll}\n\\bar{Y}_{1}=\\frac{\\sum_{i=1}^{n} Z_{i} Y_{i}}{\\sum_{i=1}^{n} Z_{i}}, & \\bar{Y}_{0}=\\frac{\\sum_{i=1}^{n}\\left(1-Z_{i}\\right) Y_{i}}{\\sum_{i=1}^{n}\\left(1-Z_{i}\\right)} \\\\\n\\bar{X}_{1}=\\frac{\\sum_{i=1}^{n} Z_{i} X_{i}}{\\sum_{i=1}^{n} Z_{i}}, & \\bar{X}_{0}=\\frac{\\sum_{i=1}^{n}\\left(1-Z_{i}\\right) X_{i}}{\\sum_{i=1}^{n}\\left(1-Z_{i}\\right)}\n\\end{array}\n\\]\nand the moment estimator\n\\[\n\\widehat{\\beta}=\\frac{\\bar{Y}_{1}-\\bar{Y}_{0}}{\\bar{X}_{1}-\\bar{X}_{0}} .\n\\]\nThis is the “Wald estimator” of Wald (1940).\nThese expressions are rather insightful. (12.27) shows that the structural slope coefficient is the expected change in \\(Y\\) due to changing the instrument divided by the expected change in \\(X\\) due to changing the instrument. Informally, it is the change in \\(Y\\) (due to \\(Z\\) ) over the change in \\(X\\) (due to \\(Z\\) ). Equation (12.28) shows that the slope coefficient can be estimated by the ratio of differences in means.\nThe expression (12.28) may appear like a distinct estimator from the IV estimator \\(\\widehat{\\beta}_{\\text {iv }}\\) but it turns out that they are the same. That is, \\(\\widehat{\\beta}=\\widehat{\\beta}_{\\mathrm{iv}}\\). To see this, use (12.26) to find\n\\[\n\\widehat{\\beta}_{\\mathrm{iv}}=\\frac{\\sum_{i=1}^{n} Z_{i}\\left(Y_{i}-\\bar{Y}\\right)}{\\sum_{i=1}^{n} Z_{i}\\left(X_{i}-\\bar{X}\\right)}=\\frac{\\bar{Y}_{1}-\\bar{Y}}{\\bar{X}_{1}-\\bar{X}} .\n\\]\nThen notice\n\\[\n\\bar{Y}_{1}-\\bar{Y}=\\bar{Y}_{1}-\\left(\\frac{1}{n} \\sum_{i=1}^{n} Z_{i} \\bar{Y}_{1}+\\frac{1}{n} \\sum_{i=1}^{n}\\left(1-Z_{i}\\right) \\bar{Y}_{0}\\right)=(1-\\bar{Z})\\left(\\bar{Y}_{1}-\\bar{Y}_{0}\\right)\n\\]\nand similarly\n\\[\n\\bar{X}_{1}-\\bar{X}=(1-\\bar{Z})\\left(\\bar{X}_{1}-\\bar{X}_{0}\\right)\n\\]\nand hence\n\\[\n\\widehat{\\beta}_{\\mathrm{iv}}=\\frac{(1-\\bar{Z})\\left(\\bar{Y}_{1}-\\bar{Y}_{0}\\right)}{(1-\\bar{Z})\\left(\\bar{X}_{1}-\\bar{X}_{0}\\right)}=\\widehat{\\beta}\n\\]\nas defined in (12.28). Thus the Wald estimator equals the IV estimator.\nWe can illustrate using the Card proximity example. If we estimate a simple IV model with no covariates we obtain the estimate \\(\\widehat{\\beta}_{\\text {iv }}=0.19\\). If we estimate the group-mean of log wages and education based on the instrument college we find\n\n\n\n\nnear college\nnot near college\ndifference\n\n\n\n\n\\(\\log (\\) wage)\n\\(6.311\\)\n\\(6.156\\)\n\\(0.155\\)\n\n\neducation\n\\(13.527\\)\n\\(12.698\\)\n\\(0.829\\)\n\n\nratio\n\n\n\\(0.19\\)\n\n\n\nBased on these estimates the Wald estimator of the slope coefficient is \\((6.311-6.156) /(13.527-12.698)=\\) \\(0.155 / 0.829=0.19\\), the same as the IV estimator."
  },
  {
    "objectID": "chpt12-iv.html#two-stage-least-squares",
    "href": "chpt12-iv.html#two-stage-least-squares",
    "title": "12  Instrumental Variables",
    "section": "12.12 Two-Stage Least Squares",
    "text": "12.12 Two-Stage Least Squares\nThe IV estimator described in the previous section presumed \\(\\ell=k\\). Now we allow the general case of \\(\\ell \\geq k\\). Examining the reduced-form equation (12.13) we see\n\\[\n\\begin{aligned}\nY_{1} &=Z^{\\prime} \\bar{\\Gamma} \\beta+u_{1} \\\\\n\\mathbb{E}\\left[Z u_{1}\\right] &=0 .\n\\end{aligned}\n\\]\nDefining \\(W=\\bar{\\Gamma}^{\\prime} Z\\) we can write this as\n\\[\n\\begin{aligned}\nY_{1} &=W^{\\prime} \\beta+u_{1} \\\\\n\\mathbb{E}\\left[W u_{1}\\right] &=0 .\n\\end{aligned}\n\\]\nOne way of thinking about this is that \\(Z\\) is set of candidate instruments. The instrument vector \\(W=\\bar{\\Gamma}^{\\prime} Z\\) is a \\(k\\)-dimentional set of linear combinations.\nSuppose that \\(\\bar{\\Gamma}\\) were known. Then we would estimate \\(\\beta\\) by least squares of \\(Y_{1}\\) on \\(W=\\bar{\\Gamma}^{\\prime} Z\\)\n\\[\n\\widehat{\\beta}=\\left(\\boldsymbol{W}^{\\prime} \\boldsymbol{W}\\right)^{-1}\\left(\\boldsymbol{W}^{\\prime} \\boldsymbol{Y}\\right)=\\left(\\bar{\\Gamma}^{\\prime} \\boldsymbol{Z}^{\\prime} \\boldsymbol{Z} \\bar{\\Gamma}\\right)^{-1}\\left(\\bar{\\Gamma}^{\\prime} \\boldsymbol{Z}^{\\prime} \\boldsymbol{Y}_{1}\\right) .\n\\]\nWhile this is infeasible we can estimate \\(\\bar{\\Gamma}\\) from the reduced form regression. Replacing \\(\\bar{\\Gamma}\\) with its estimator \\(\\widehat{\\Gamma}=\\left(\\boldsymbol{Z}^{\\prime} \\boldsymbol{Z}\\right)^{-1}\\left(\\boldsymbol{Z}^{\\prime} \\boldsymbol{X}\\right)\\) we obtain\n\\[\n\\begin{aligned}\n\\widehat{\\beta}_{2 \\text { sls }} &=\\left(\\widehat{\\Gamma}^{\\prime} \\boldsymbol{Z}^{\\prime} \\boldsymbol{Z} \\widehat{\\Gamma}\\right)^{-1}\\left(\\widehat{\\Gamma}^{\\prime} \\boldsymbol{Z}^{\\prime} \\boldsymbol{Y}_{1}\\right) \\\\\n&=\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{Z}\\left(\\boldsymbol{Z}^{\\prime} \\boldsymbol{Z}\\right)^{-1} \\boldsymbol{Z}^{\\prime} \\boldsymbol{Z}\\left(\\boldsymbol{Z}^{\\prime} \\boldsymbol{Z}\\right)^{-\\mathbf{1}} \\boldsymbol{Z}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\boldsymbol{X}^{\\prime} \\boldsymbol{Z}\\left(\\boldsymbol{Z}^{\\prime} \\boldsymbol{Z}\\right)^{-1} \\boldsymbol{Z}^{\\prime} \\boldsymbol{Y}_{1} \\\\\n&=\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{Z}\\left(\\boldsymbol{Z}^{\\prime} \\boldsymbol{Z}\\right)^{-1} \\boldsymbol{Z}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\boldsymbol{X}^{\\prime} \\boldsymbol{Z}\\left(\\boldsymbol{Z}^{\\prime} \\boldsymbol{Z}\\right)^{-1} \\boldsymbol{Z}^{\\prime} \\boldsymbol{Y}_{1}\n\\end{aligned}\n\\]\nThis is called the two-stage-least squares (2SLS) estimator. It was originally proposed by Theil (1953) and Basmann (1957) and is a standard estimator for linear equations with instruments.\nIf the model is just-identified, so that \\(k=\\ell\\), then 2SLS simplifies to the IV estimator of the previous section. Since the matrices \\(\\boldsymbol{X}^{\\prime} \\boldsymbol{Z}\\) and \\(\\boldsymbol{Z}^{\\prime} \\boldsymbol{X}\\) are square we can factor\n\\[\n\\begin{aligned}\n\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{Z}\\left(\\boldsymbol{Z}^{\\prime} \\boldsymbol{Z}\\right)^{-1} \\boldsymbol{Z}^{\\prime} \\boldsymbol{X}\\right)^{-1} &=\\left(\\boldsymbol{Z}^{\\prime} \\boldsymbol{X}\\right)^{-1}\\left(\\left(\\boldsymbol{Z}^{\\prime} \\boldsymbol{Z}\\right)^{-1}\\right)^{-1}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{Z}\\right)^{-1} \\\\\n&=\\left(\\boldsymbol{Z}^{\\prime} \\boldsymbol{X}\\right)^{-1}\\left(\\boldsymbol{Z}^{\\prime} \\boldsymbol{Z}\\right)\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{Z}\\right)^{-1}\n\\end{aligned}\n\\]\n(Once again, this only works when \\(k=\\ell\\).) Then\n\\[\n\\begin{aligned}\n\\widehat{\\beta}_{2 \\text { sls }} &=\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{Z}\\left(\\boldsymbol{Z}^{\\prime} \\boldsymbol{Z}\\right)^{-1} \\boldsymbol{Z}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\boldsymbol{X}^{\\prime} \\boldsymbol{Z}\\left(\\boldsymbol{Z}^{\\prime} \\boldsymbol{Z}\\right)^{-1} \\boldsymbol{Z}^{\\prime} \\boldsymbol{Y}_{1} \\\\\n&=\\left(\\boldsymbol{Z}^{\\prime} \\boldsymbol{X}\\right)^{-1}\\left(\\boldsymbol{Z}^{\\prime} \\boldsymbol{Z}\\right)\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{Z}\\right)^{-1} \\boldsymbol{X}^{\\prime} \\boldsymbol{Z}\\left(\\boldsymbol{Z}^{\\prime} \\boldsymbol{Z}\\right)^{-1} \\boldsymbol{Z}^{\\prime} \\boldsymbol{Y}_{1} \\\\\n&=\\left(\\boldsymbol{Z}^{\\prime} \\boldsymbol{X}\\right)^{-1}\\left(\\boldsymbol{Z}^{\\prime} \\boldsymbol{Z}\\right)\\left(\\boldsymbol{Z}^{\\prime} \\boldsymbol{Z}\\right)^{-1} \\boldsymbol{Z}^{\\prime} \\boldsymbol{Y}_{1} \\\\\n&=\\left(\\boldsymbol{Z}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\boldsymbol{Z}^{\\prime} \\boldsymbol{Y}_{1}=\\widehat{\\beta}_{\\mathrm{iv}}\n\\end{aligned}\n\\]\nas claimed. This shows that the 2SLS estimator as defined in (12.29) is a generalization of the IV estimator defined in (12.24).\nThere are several alternative representations of the 2SLS estimator which we now describe. First, defining the projection matrix\n\\[\n\\boldsymbol{P}_{\\boldsymbol{Z}}=\\boldsymbol{Z}\\left(\\boldsymbol{Z}^{\\prime} \\boldsymbol{Z}\\right)^{-1} \\boldsymbol{Z}^{\\prime}\n\\]\nwe can write the 2SLS estimator more compactly as\n\\[\n\\widehat{\\beta}_{2 \\text { sls }}=\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{P}_{\\boldsymbol{Z}} \\boldsymbol{X}\\right)^{-1} \\boldsymbol{X}^{\\prime} \\boldsymbol{P}_{\\boldsymbol{Z}} \\boldsymbol{Y}_{1} .\n\\]\nThis is useful for representation and derivations but is not useful for computation as the \\(n \\times n\\) matrix \\(\\boldsymbol{P}_{\\boldsymbol{Z}}\\) is too large to compute when \\(n\\) is large.\nSecond, define the fitted values for \\(\\boldsymbol{X}\\) from the reduced form \\(\\widehat{\\boldsymbol{X}}=\\boldsymbol{P}_{\\boldsymbol{Z}} \\boldsymbol{X}=\\boldsymbol{Z} \\widehat{\\Gamma}\\). Then the 2SLS estimator can be written as\n\\[\n\\widehat{\\beta}_{2 \\text { sls }}=\\left(\\widehat{\\boldsymbol{X}}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\widehat{\\boldsymbol{X}}^{\\prime} \\boldsymbol{Y}_{1}\n\\]\nThis is an IV estimator as defined in the previous section using \\(\\widehat{X}\\) as an instrument for \\(X\\).\nThird, because \\(\\boldsymbol{P}_{Z}\\) is idempotent we can also write the 2SLS estimator as\n\\[\n\\widehat{\\beta}_{2 \\text { sls }}=\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{P}_{\\boldsymbol{Z}} \\boldsymbol{P}_{\\boldsymbol{Z}} \\boldsymbol{X}\\right)^{-1} \\boldsymbol{X}^{\\prime} \\boldsymbol{P}_{\\boldsymbol{Z}} \\boldsymbol{Y}_{1}=\\left(\\widehat{\\boldsymbol{X}}^{\\prime} \\widehat{\\boldsymbol{X}}\\right)^{-1} \\widehat{\\boldsymbol{X}}^{\\prime} \\boldsymbol{Y}_{1}\n\\]\nwhich is the least squares estimator obtained by regressing \\(Y_{1}\\) on the fitted values \\(\\widehat{X}\\).\nThis is the source of the “two-stage” name as it can be computed as follows.\n\nRegress \\(X\\) on \\(Z\\) to obtain the fitted \\(\\widehat{X}: \\widehat{\\Gamma}=\\left(\\boldsymbol{Z}^{\\prime} \\boldsymbol{Z}\\right)^{-1}\\left(\\boldsymbol{Z}^{\\prime} \\boldsymbol{X}\\right)\\) and \\(\\widehat{\\boldsymbol{X}}=\\boldsymbol{Z} \\widehat{\\Gamma}=\\boldsymbol{P}_{\\boldsymbol{Z}} \\boldsymbol{X}\\).\nRegress \\(Y_{1}\\) on \\(\\widehat{X}: \\widehat{\\beta}_{2 s l s}=\\left(\\widehat{\\boldsymbol{X}}^{\\prime} \\widehat{\\boldsymbol{X}}\\right)^{-1} \\widehat{\\boldsymbol{X}}^{\\prime} \\boldsymbol{Y}_{1}\\)\n\nIt is useful to scrutinize the projection \\(\\widehat{\\boldsymbol{X}}\\). Recall, \\(\\boldsymbol{X}=\\left[\\boldsymbol{Z}_{1}, \\boldsymbol{Y}_{2}\\right]\\) and \\(\\boldsymbol{Z}=\\left[\\boldsymbol{Z}_{1}, \\boldsymbol{Z}_{2}\\right]\\). Notice \\(\\widehat{\\boldsymbol{X}}_{1}=\\boldsymbol{P}_{\\boldsymbol{Z}} \\boldsymbol{Z}_{1}=\\) \\(Z_{1}\\) because \\(Z_{1}\\) lies in the span of \\(\\boldsymbol{Z}\\). Then \\(\\widehat{\\boldsymbol{X}}=\\left[\\widehat{\\boldsymbol{X}}_{1}, \\widehat{\\boldsymbol{Y}}_{2}\\right]=\\left[\\boldsymbol{Z}_{1}, \\widehat{\\boldsymbol{Y}}_{2}\\right]\\). This shows that in the second stage we regress \\(Y_{1}\\) on \\(Z_{1}\\) and \\(\\widehat{Y}_{2}\\). This means that only the endogenous variables \\(Y_{2}\\) are replaced by their fitted values, \\(\\widehat{Y}_{2}=\\widehat{\\Gamma}_{12}^{\\prime} Z_{1}+\\widehat{\\Gamma}_{22}^{\\prime} Z_{2}\\).\nA fourth representation of 2SLS can be obtained using the FWL Theorem. The third representation and following discussion showed that 2SLS is obtained as least squares of \\(Y_{1}\\) on the fitted values \\(\\left(Z_{1}, \\widehat{Y}_{2}\\right)\\). Hence the coefficient \\(\\widehat{\\beta}_{2}\\) on the endogenous variables can be found by residual regression. Set \\(\\boldsymbol{P}_{1}=\\) \\(Z_{1}\\left(Z_{1}^{\\prime} Z_{1}\\right)^{-1} Z_{1}^{\\prime}\\). Applying the FWL theorem we obtain\n\\[\n\\begin{aligned}\n\\widehat{\\beta}_{2} &=\\left(\\widehat{\\boldsymbol{Y}}_{2}^{\\prime}\\left(\\boldsymbol{I}_{n}-\\boldsymbol{P}_{1}\\right) \\widehat{\\boldsymbol{Y}}_{2}\\right)^{-1}\\left(\\widehat{\\boldsymbol{Y}}_{2}^{\\prime}\\left(\\boldsymbol{I}_{n}-\\boldsymbol{P}_{1}\\right) \\boldsymbol{Y}_{1}\\right) \\\\\n&=\\left(\\boldsymbol{Y}_{2}^{\\prime} \\boldsymbol{P}_{\\boldsymbol{Z}}\\left(\\boldsymbol{I}_{n}-\\boldsymbol{P}_{1}\\right) \\boldsymbol{P}_{\\boldsymbol{Z}} \\boldsymbol{Y}_{2}\\right)^{-1}\\left(\\boldsymbol{Y}_{2}^{\\prime} \\boldsymbol{P}_{\\boldsymbol{Z}}\\left(\\boldsymbol{I}_{n}-\\boldsymbol{P}_{1}\\right) \\boldsymbol{Y}_{1}\\right) \\\\\n&=\\left(\\boldsymbol{Y}_{2}^{\\prime}\\left(\\boldsymbol{P}_{\\boldsymbol{Z}}-\\boldsymbol{P}_{1}\\right) \\boldsymbol{Y}_{2}\\right)^{-1}\\left(\\boldsymbol{Y}_{2}^{\\prime}\\left(\\boldsymbol{P}_{\\boldsymbol{Z}}-\\boldsymbol{P}_{1}\\right) \\boldsymbol{Y}_{1}\\right)\n\\end{aligned}\n\\]\nbecause \\(\\boldsymbol{P}_{Z} \\boldsymbol{P}_{1}=\\boldsymbol{P}_{1}\\).\nA fifth representation can be obtained by a further projection. The projection matrix \\(\\boldsymbol{P}_{\\boldsymbol{Z}}\\) can be replaced by the projection onto the pair \\(\\left[\\boldsymbol{Z}_{1}, \\widetilde{\\boldsymbol{Z}}_{2}\\right.\\) ] where \\(\\widetilde{\\boldsymbol{Z}}_{2}=\\left(\\boldsymbol{I}_{n}-\\boldsymbol{P}_{1}\\right) \\boldsymbol{Z}_{2}\\) is \\(\\boldsymbol{Z}_{2}\\) projected orthogonal to \\(\\boldsymbol{Z}_{1}\\). Since \\(\\boldsymbol{Z}_{1}\\) and \\(\\widetilde{\\boldsymbol{Z}}_{2}\\) are orthogonal, \\(\\boldsymbol{P}_{\\boldsymbol{Z}}=\\boldsymbol{P}_{1}+\\boldsymbol{P}_{2}\\) where \\(\\boldsymbol{P}_{2}=\\widetilde{\\boldsymbol{Z}}_{2}\\left(\\widetilde{\\boldsymbol{Z}}_{2}^{\\prime} \\widetilde{\\boldsymbol{Z}}_{2}\\right)^{-1} \\widetilde{\\boldsymbol{Z}}_{2}^{\\prime}\\). Thus \\(\\boldsymbol{P}_{\\boldsymbol{Z}}-\\boldsymbol{P}_{1}=\\boldsymbol{P}_{2}\\) and\n\\[\n\\begin{aligned}\n\\widehat{\\beta}_{2} &=\\left(\\boldsymbol{Y}_{2}^{\\prime} \\boldsymbol{P}_{2} \\boldsymbol{Y}_{2}\\right)^{-1}\\left(\\boldsymbol{Y}_{2}^{\\prime} \\boldsymbol{P}_{2} \\boldsymbol{Y}_{1}\\right) \\\\\n&=\\left(\\boldsymbol{Y}_{2}^{\\prime} \\widetilde{\\boldsymbol{Z}}_{2}\\left(\\widetilde{\\boldsymbol{Z}}_{2}^{\\prime} \\widetilde{\\boldsymbol{Z}}_{2}\\right)^{-1} \\widetilde{\\boldsymbol{Z}}_{2}^{\\prime} \\boldsymbol{Y}_{2}\\right)^{-1}\\left(\\boldsymbol{Y}_{2}^{\\prime} \\widetilde{\\boldsymbol{Z}}_{2}\\left(\\widetilde{\\boldsymbol{Z}}_{2}^{\\prime} \\widetilde{\\boldsymbol{Z}}_{2}\\right)^{-1} \\widetilde{\\boldsymbol{Z}}_{2}^{\\prime} \\boldsymbol{Y}_{1}\\right) .\n\\end{aligned}\n\\]\nGiven the 2SLS estimator we define the residual \\(\\widehat{e}_{i}=Y_{1 i}-X_{i}^{\\prime} \\widehat{\\beta}_{2 s l s}\\). When the model is overidentified the instruments and residuals are not orthogonal. That is, \\(\\boldsymbol{Z}^{\\prime} \\widehat{\\boldsymbol{e}} \\neq 0\\). It does, however, satisfy\n\\[\n\\begin{aligned}\n\\widehat{\\boldsymbol{X}}^{\\prime} \\widehat{\\boldsymbol{e}} &=\\widehat{\\boldsymbol{\\Gamma}}^{\\prime} \\boldsymbol{Z}^{\\prime} \\widehat{\\boldsymbol{e}} \\\\\n&=\\boldsymbol{X}^{\\prime} \\boldsymbol{Z}\\left(\\boldsymbol{Z}^{\\prime} \\boldsymbol{Z}\\right)^{-1} \\boldsymbol{Z}^{\\prime} \\widehat{\\boldsymbol{e}} \\\\\n&=\\boldsymbol{X}^{\\prime} \\boldsymbol{Z}\\left(\\boldsymbol{Z}^{\\prime} \\boldsymbol{Z}\\right)^{-1} \\boldsymbol{Z}^{\\prime} \\boldsymbol{Y}_{1}-\\boldsymbol{X}^{\\prime} \\boldsymbol{Z}\\left(\\boldsymbol{Z}^{\\prime} \\boldsymbol{Z}\\right)^{-1} \\boldsymbol{Z}^{\\prime} \\boldsymbol{X} \\widehat{\\beta}_{2 \\text { sls }}=0 .\n\\end{aligned}\n\\]\nReturning to Card’s college proximity example suppose that we treat experience as exogeneous but that instead of using the single instrument college (grew up near a 4-year college) we use the two instruments (public, private) (grew up near a public/private 4-year college, respectively). In this case we have one endogenous variable (education) and two instruments (public, private). The estimated reduced form equation for education is presented in the sixth column of Table 12.2. In this specification the coefficient on public - growing up near a public 4-year college - is somewhat larger than that found for the variable college in the previous specification (column 2). Furthermore, the coefficient on private-growing up near a private 4-year college - is much smaller. This indicates that the key impact of proximity on education is via public colleges rather than private colleges.\nThe 2SLS estimates obtained using these two instruments are presented in the fourth column of Table 12.1. The coefficient on education increases to \\(0.161\\), indicating a \\(16 %\\) return to a year of education. This is roughly twice as large as the estimate obtained by least squares in the first column.\nAdditionally, if we follow Card and treat experience as endogenous and use age as an instrument we now have three endogenous variables (education, experience, experience \\({ }^{2} / 100\\) ) and four instruments (public, private, age, \\(a g e^{2}\\) ). We present the 2SLS estimates using this specification in the fifth column of Table 12.1. The estimate of the return to education remains \\(16 %\\) and the return to experience flattens.\nYou might wonder if we could use all three instruments - college, public, and private. The answer is no. This is because college \\(=\\) public \\(+\\) private so the three variables are colinear. Since the instruments are linearly related the three together would violate the full-rank condition (12.6).\nThe 2SLS estimator may be calculated in Stata using the ivregress 2 sls command."
  },
  {
    "objectID": "chpt12-iv.html#limited-information-maximum-likelihood",
    "href": "chpt12-iv.html#limited-information-maximum-likelihood",
    "title": "12  Instrumental Variables",
    "section": "12.13 Limited Information Maximum Likelihood",
    "text": "12.13 Limited Information Maximum Likelihood\nAn alternative method to estimate the parameters of the structural equation is by maximum likelihood. Anderson and Rubin (1949) derived the maximum likelihood estimator for the joint distribution of \\(\\vec{Y}=\\left(Y_{1}, Y_{2}\\right)\\). The estimator is known as limited information maximum likelihood (LIML).\nThis estimator is called “limited information” because it is based on the structural equation for \\(Y\\) combined with the reduced form equation for \\(X_{2}\\). If maximum likelihood is derived based on a structural equation for \\(X_{2}\\) as well this leads to what is known as full information maximum likelihood (FIML). The advantage of LIML relative to FIML is that the former does not require a structural model for \\(X_{2}\\) and thus allows the researcher to focus on the structural equation of interest - that for \\(Y\\). We do not describe the FIML estimator as it is not commonly used in applied econometrics.\nWhile the LIML estimator is less widely used among economists than 2SLS it has received a resurgence of attention from econometric theorists.\nTo derive the LIML estimator recall the definition \\(\\vec{Y}=\\left(Y_{1}, Y_{2}\\right)\\) and the reduced form (12.17)\n\\[\n\\begin{aligned}\n\\vec{Y} &=\\left[\\begin{array}{cc}\n\\lambda_{1}^{\\prime} & \\lambda_{2} \\\\\n\\Gamma_{12}^{\\prime} & \\Gamma_{22}^{\\prime}\n\\end{array}\\right]\\left(\\begin{array}{l}\nZ_{1} \\\\\nZ_{2}\n\\end{array}\\right)+u \\\\\n&=\\Pi_{1}^{\\prime} Z_{1}+\\Pi_{2}^{\\prime} Z_{2}+u\n\\end{aligned}\n\\]\nwhere \\(\\Pi_{1}=\\left[\\begin{array}{cc}\\lambda_{1} & \\Gamma_{12}\\end{array}\\right]\\) and \\(\\Pi_{2}=\\left[\\begin{array}{cc}\\lambda_{2} & \\Gamma_{22}\\end{array}\\right]\\). The LIML estimator is derived under the assumption that \\(u\\) is multivariate normal.\nDefine \\(\\gamma^{\\prime}=\\left[\\begin{array}{ll}1 & -\\beta_{2}^{\\prime}\\end{array}\\right]\\). From (12.15) we find\n\\[\n\\Pi_{2} \\gamma=\\lambda_{2}-\\Gamma_{22} \\beta_{2}=0 .\n\\]\nThus the \\(\\ell_{2} \\times\\left(k_{2}+1\\right)\\) coefficient matrix \\(\\Pi_{2}\\) in (12.33) has deficient rank. Indeed, its rank must be \\(k_{2}\\) because \\(\\Gamma_{22}\\) has full rank.\nThis means that the model (12.33) is precisely the reduced rank regression model of Section \\(11.11 .\\) Theorem \\(11.7\\) presents the maximum likelihood estimators for the reduced rank parameters. In particular, the MLE for \\(\\gamma\\) is\n\\[\n\\widehat{\\gamma}=\\underset{\\gamma}{\\operatorname{argmin}} \\frac{\\gamma^{\\prime} \\overrightarrow{\\boldsymbol{Y}}^{\\prime} \\boldsymbol{M}_{1} \\overrightarrow{\\boldsymbol{Y}} \\gamma}{\\gamma^{\\prime} \\overrightarrow{\\boldsymbol{Y}}^{\\prime} \\boldsymbol{M}_{\\boldsymbol{Z}} \\overrightarrow{\\boldsymbol{Y}} \\gamma}\n\\]\nwhere \\(\\boldsymbol{M}_{1}=\\boldsymbol{I}_{n}-\\boldsymbol{Z}_{1}\\left(\\boldsymbol{Z}_{1}^{\\prime} \\boldsymbol{Z}_{1}\\right)^{-1} \\boldsymbol{Z}_{1}^{\\prime}\\) and \\(\\boldsymbol{M}_{\\boldsymbol{Z}}=\\boldsymbol{I}_{n}-\\boldsymbol{Z}\\left(\\boldsymbol{Z}^{\\prime} \\boldsymbol{Z}\\right)^{-1} \\boldsymbol{Z}^{\\prime}\\). The minimization (12.34) is sometimes called the “least variance ratio” problem.\nThe minimization problem (12.34) is invariant to the scale of \\(\\gamma\\) (that is, \\(\\widehat{\\gamma} c\\) is equivalently the argmin for any c) so normalization is required. A convenient choice is \\(\\gamma^{\\prime} \\overrightarrow{\\boldsymbol{Y}}^{\\prime} \\boldsymbol{M}_{Z} \\overrightarrow{\\boldsymbol{Y}} \\gamma=1\\). Using this normalization and the theory of the minimum of quadratic forms (Section A.15) \\(\\widehat{\\gamma}\\) is the generalized eigenvector of \\(\\overrightarrow{\\boldsymbol{Y}}^{\\prime} \\boldsymbol{M}_{1} \\overrightarrow{\\boldsymbol{Y}}\\) with respect to \\(\\overrightarrow{\\boldsymbol{Y}}^{\\prime} \\boldsymbol{M}_{Z} \\overrightarrow{\\boldsymbol{Y}}\\) associated with the smallest generalized eigenvalue. (See Section A.14 for the definition of generalized eigenvalues and eigenvectors.) Computationally this is straightforward. For example, in MATLAB the generalized eigenvalues and eigenvectors of the matrix \\(\\boldsymbol{A}\\) with respect to \\(\\boldsymbol{B}\\) is found by the command eig \\((\\boldsymbol{A}, \\boldsymbol{B})\\). Once this \\(\\widehat{\\gamma}\\) is found any other normalization can be obtained by rescaling. For example, to obtain the MLE for \\(\\beta_{2}\\) make the partition \\(\\widehat{\\gamma}^{\\prime}=\\left[\\begin{array}{cc}\\widehat{\\gamma}_{1} & \\widehat{\\gamma}_{2}^{\\prime}\\end{array}\\right]\\) and set \\(\\widehat{\\beta}_{2}=-\\widehat{\\gamma}_{2} / \\widehat{\\gamma}_{1}\\).\nTo obtain the MLE for \\(\\beta_{1}\\) recall the structural equation \\(Y_{1}=Z_{1}^{\\prime} \\beta_{1}+Y_{2}^{\\prime} \\beta_{2}+e\\). Replace \\(\\beta_{2}\\) with the MLE \\(\\widehat{\\beta}_{2}\\) and apply regression. This yields\n\\[\n\\widehat{\\beta}_{1}=\\left(Z_{1}^{\\prime} Z_{1}\\right)^{-1} Z_{1}^{\\prime}\\left(Y_{1}-Y_{2} \\widehat{\\beta}_{2}\\right) .\n\\]\nThese solutions are the MLE for the structural parameters \\(\\beta_{1}\\) and \\(\\beta_{2}\\).\nPrevious econometrics textbooks did not present a derivation of the LIML estimator as the original derivation by Anderson and Rubin (1949) is lengthy and not particularly insightful. In contrast the derivation given here based on reduced rank regression is simple.\nThere is an alternative (and traditional) expression for the LIML estimator. Define the minimum obtained in (12.34)\n\\[\n\\widehat{\\boldsymbol{\\kappa}}=\\min _{\\gamma} \\frac{\\gamma^{\\prime} \\overrightarrow{\\boldsymbol{Y}}^{\\prime} \\boldsymbol{M}_{1} \\overrightarrow{\\boldsymbol{Y}} \\gamma}{\\gamma^{\\prime} \\overrightarrow{\\boldsymbol{Y}}^{\\prime} \\boldsymbol{M}_{\\boldsymbol{Z}} \\overrightarrow{\\boldsymbol{Y}} \\gamma}\n\\]\nwhich is the smallest generalized eigenvalue of \\(\\overrightarrow{\\boldsymbol{Y}}^{\\prime} \\boldsymbol{M}_{1} \\overrightarrow{\\boldsymbol{Y}}\\) with respect to \\(\\overrightarrow{\\boldsymbol{Y}}^{\\prime} \\boldsymbol{M}_{\\boldsymbol{Z}} \\overrightarrow{\\boldsymbol{Y}}\\). The LIML estimator can be written as\n\\[\n\\widehat{\\beta}_{\\text {liml }}=\\left(\\boldsymbol{X}^{\\prime}\\left(\\boldsymbol{I}_{n}-\\widehat{\\kappa} \\boldsymbol{M}_{\\boldsymbol{Z}}\\right) \\boldsymbol{X}\\right)^{-1}\\left(\\boldsymbol{X}^{\\prime}\\left(\\boldsymbol{I}_{n}-\\widehat{\\kappa} \\boldsymbol{M}_{\\boldsymbol{Z}}\\right) \\boldsymbol{Y}_{1}\\right) .\n\\]\nWe defer the derivation of (12.37) until the end of this section. Expression (12.37) does not simplify computation (because \\(\\widehat{\\kappa}\\) requires solving the same eigenvector problem that yields \\(\\widehat{\\beta}_{2}\\) ). However (12.37) is important for the distribution theory. It also helps reveal the algebraic connection between LIML, least squares, and 2SLS.\nThe estimator (12.37) with arbitrary \\(\\kappa\\) is known as a k-class estimator of \\(\\beta\\). While the LIML estimator obtains by setting \\(\\kappa=\\widehat{\\kappa}\\), the least squares estimator is obtained by setting \\(\\kappa=0\\) and 2SLS is obtained by setting \\(\\kappa=1\\). It is worth observing that the LIML solution satisfies \\(\\widehat{\\kappa} \\geq 1\\). When the model is just-identified the LIML estimator is identical to the IV and 2SLS estimators. They are only different in the over-identified setting. (One corollary is that under just-identification and normal errors the IV estimator is MLE.)\nFor inference it is useful to observe that (12.37) shows that \\(\\widehat{\\beta}_{\\mathrm{liml}}\\) can be written as an IV estimator\n\\[\n\\widehat{\\beta}_{\\mathrm{liml}}=\\left(\\widetilde{\\boldsymbol{X}}^{\\prime} \\boldsymbol{X}\\right)^{-1}\\left(\\widetilde{\\boldsymbol{X}}^{\\prime} \\boldsymbol{Y}_{1}\\right)\n\\]\nusing the instrument\n\\[\n\\widetilde{\\boldsymbol{X}}=\\left(\\boldsymbol{I}_{n}-\\widehat{\\kappa} \\boldsymbol{M}_{\\boldsymbol{Z}}\\right) \\boldsymbol{X}=\\left(\\begin{array}{c}\n\\boldsymbol{X}_{1} \\\\\n\\boldsymbol{X}_{2}-\\widehat{\\kappa} \\widehat{\\boldsymbol{U}}_{2}\n\\end{array}\\right)\n\\]\nwhere \\(\\widehat{\\boldsymbol{U}}_{2}=\\boldsymbol{M}_{\\boldsymbol{Z}} \\boldsymbol{X}_{2}\\) are the reduced-form residuals from the multivariate regression of the endogenous regressors \\(Y_{2}\\) on the instruments \\(Z\\). Expressing LIML using this IV formula is useful for variance estimation.\nThe LIML estimator has the same asymptotic distribution as 2SLS. However, they have quite different behaviors in finite samples. There is considerable evidence that the LIML estimator has reduced finite sample bias relative to 2 SLS when there are many instruments or the reduced form is weak. (We review these cases in the following sections.) However, on the other hand LIML has wider finite sample dispersion.\nWe now derive the expression (12.37). Use the normalization \\(\\gamma^{\\prime}=\\left[\\begin{array}{ll}1 & -\\beta_{2}^{\\prime}\\end{array}\\right]\\) to write (12.34) as\n\\[\n\\widehat{\\beta}_{2}=\\underset{\\beta_{2}}{\\operatorname{argmin}} \\frac{\\left(\\boldsymbol{Y}_{1}-\\boldsymbol{Y}_{2} \\beta_{2}\\right)^{\\prime} \\boldsymbol{M}_{1}\\left(\\boldsymbol{Y}_{1}-\\boldsymbol{Y}_{2} \\beta_{2}\\right)}{\\left(\\boldsymbol{Y}_{1}-\\boldsymbol{Y} \\beta_{2}\\right)^{\\prime} \\boldsymbol{M}_{\\boldsymbol{Z}}\\left(\\boldsymbol{Y}_{1}-\\boldsymbol{Y}_{2} \\beta_{2}\\right)} .\n\\]\nThe first-order-condition for minimization is \\(2 /\\left(\\boldsymbol{Y}_{1}-\\boldsymbol{Y}_{2} \\widehat{\\beta}_{2}\\right)^{\\prime} \\boldsymbol{M}_{\\boldsymbol{Z}}\\left(\\boldsymbol{Y}_{1}-\\boldsymbol{Y}_{2} \\widehat{\\beta}_{2}\\right)\\) times\n\\[\n\\begin{aligned}\n0 &=\\boldsymbol{Y}_{2}^{\\prime} \\boldsymbol{M}_{1}\\left(\\boldsymbol{Y}_{1}-\\boldsymbol{Y}_{2} \\widehat{\\beta}_{2}\\right)-\\frac{\\left(\\boldsymbol{Y}_{1}-\\boldsymbol{Y}_{2} \\widehat{\\beta}_{2}\\right)^{\\prime} \\boldsymbol{M}_{1}\\left(\\boldsymbol{Y}_{1}-\\boldsymbol{Y}_{2} \\widehat{\\beta}_{2}\\right)}{\\left(\\boldsymbol{Y}_{1}-\\boldsymbol{Y}_{2} \\widehat{\\beta}_{2}\\right)^{\\prime} \\boldsymbol{M}_{\\boldsymbol{Z}}\\left(\\boldsymbol{Y}_{1}-\\boldsymbol{Y}_{2} \\widehat{\\beta}_{2}\\right)} \\boldsymbol{X}_{2}^{\\prime} \\boldsymbol{M}_{\\boldsymbol{Z}}\\left(\\boldsymbol{Y}_{1}-\\boldsymbol{Y}_{2} \\widehat{\\beta}_{2}\\right) \\\\\n&=\\boldsymbol{Y}_{2}^{\\prime} \\boldsymbol{M}_{1}\\left(\\boldsymbol{Y}_{1}-\\boldsymbol{Y}_{2} \\widehat{\\beta}_{2}\\right)-\\widehat{\\kappa} \\boldsymbol{X}_{2}^{\\prime} \\boldsymbol{M}_{\\boldsymbol{Z}}\\left(\\boldsymbol{Y}_{1}-\\boldsymbol{Y}_{2} \\widehat{\\beta}_{2}\\right)\n\\end{aligned}\n\\]\nusing definition (12.36). Rewriting,\n\\[\n\\boldsymbol{Y}_{2}^{\\prime}\\left(\\boldsymbol{M}_{1}-\\widehat{\\kappa} \\boldsymbol{M}_{\\boldsymbol{Z}}\\right) \\boldsymbol{X}_{2} \\widehat{\\beta}_{2}=\\boldsymbol{X}_{2}^{\\prime}\\left(\\boldsymbol{M}_{1}-\\widehat{\\kappa} \\boldsymbol{M}_{\\boldsymbol{Z}}\\right) \\boldsymbol{Y}_{1} .\n\\]\nEquation (12.37) is the same as the two equation system\n\\[\n\\begin{aligned}\n\\boldsymbol{Z}_{1}^{\\prime} \\boldsymbol{Z}_{1} \\widehat{\\beta}_{1}+\\boldsymbol{Z}_{1}^{\\prime} \\boldsymbol{Y}_{2} \\widehat{\\beta}_{2} &=\\boldsymbol{Z}_{1}^{\\prime} \\boldsymbol{Y}_{1} \\\\\n\\boldsymbol{Y}_{2}^{\\prime} \\boldsymbol{Z}_{1} \\widehat{\\beta}_{1}+\\left(\\boldsymbol{Y}_{2}^{\\prime}\\left(\\boldsymbol{I}_{n}-\\widehat{\\kappa} \\boldsymbol{M}_{\\boldsymbol{Z}}\\right) \\boldsymbol{Y}_{2}\\right) \\widehat{\\beta}_{2} &=\\boldsymbol{Y}_{2}^{\\prime}\\left(\\boldsymbol{I}_{n}-\\widehat{\\kappa} \\boldsymbol{M}_{\\boldsymbol{Z}}\\right) \\boldsymbol{Y}_{1} .\n\\end{aligned}\n\\]\nThe first equation is (12.35). Using (12.35), the second is\n\\[\n\\boldsymbol{Y}_{2}^{\\prime} \\boldsymbol{Z}_{1}\\left(\\boldsymbol{Z}_{1}^{\\prime} \\boldsymbol{Z}_{1}\\right)^{-1} \\boldsymbol{Z}_{1}^{\\prime}\\left(\\boldsymbol{Y}_{1}-\\boldsymbol{Y}_{2} \\widehat{\\beta}_{2}\\right)+\\left(\\boldsymbol{Y}_{2}^{\\prime}\\left(\\boldsymbol{I}_{n}-\\widehat{\\kappa} \\boldsymbol{M}_{\\boldsymbol{Z}}\\right) \\boldsymbol{Y}_{2}\\right) \\widehat{\\beta}_{2}=\\boldsymbol{Y}_{2}^{\\prime}\\left(\\boldsymbol{I}_{n}-\\widehat{\\kappa} \\boldsymbol{M}_{\\boldsymbol{Z}}\\right) \\boldsymbol{Y}_{1}\n\\]\nwhich is (12.38) when rearranged. We have thus shown that (12.37) is equivalent to (12.35) and (12.38) and is thus a valid expression for the LIML estimator.\nReturning to the Card college proximity example we now present the LIML estimates of the equation with the two instruments (public, private). They are reported in the final column of Table 12.1. They are quite similar to the 2SLS estimates.\nThe LIML estimator may be calculated in Stata using the ivregress liml command.\n\n\n\n\n\n\nTheodore Anderson\n\n\n\n\nTheodore (Ted) Anderson (1918-2016) was a American statistician and econo-\n\n\nmetrician, who made fundamental contributions to multivariate statistical the-\n\n\nory. Important contributions include the Anderson-Darling distribution test, the\n\n\nAnderson-Rubin statistic, the method of reduced rank regression, and his most\n\n\nfamous econometrics contribution - the LIML estimator. He continued working\n\n\nthroughout his long life, even publishing theoretical work at the age of 97 !"
  },
  {
    "objectID": "chpt12-iv.html#split-sample-iv-and-jive",
    "href": "chpt12-iv.html#split-sample-iv-and-jive",
    "title": "12  Instrumental Variables",
    "section": "12.14 Split-Sample IV and JIVE",
    "text": "12.14 Split-Sample IV and JIVE\nThe ideal instrument for estimation of \\(\\beta\\) is \\(W=\\Gamma^{\\prime} Z\\). We can write the ideal IV estimator as\n\\[\n\\widehat{\\beta}_{\\text {ideal }}=\\left(\\sum_{i=1}^{n} W_{i} X_{i}^{\\prime}\\right)^{-1}\\left(\\sum_{i=1}^{n} W_{i} Y_{i}\\right) .\n\\]\nThis estimator is not feasible since \\(\\Gamma\\) is unknown. The 2SLS estimator replaces \\(\\Gamma\\) with the multivariate least squares estimator \\(\\widehat{\\Gamma}\\) and \\(W_{i}\\) with \\(\\widehat{W}_{i}=\\widehat{\\Gamma}^{\\prime} Z_{i}\\) leading to the following representation for 2SLS\n\\[\n\\widehat{\\beta}_{2 \\text { sls }}=\\left(\\sum_{i=1}^{n} \\widehat{W}_{i} X_{i}^{\\prime}\\right)^{-1}\\left(\\sum_{i=1}^{n} \\widehat{W}_{i} Y_{i}\\right) .\n\\]\nSince \\(\\widehat{\\Gamma}\\) is estimated on the full sample including observation \\(i\\) it is a function of the reduced form error \\(u\\) which is correlated with the structural error \\(e\\). It follows that \\(\\widehat{W}\\) and \\(e\\) are correlated, which means that \\(\\widehat{\\beta}_{2 s l s}\\) is biased for \\(\\beta\\). This correlation and bias disappears asymptotically but it can be important in applications.\nA possible solution to this problem is to replace \\(\\widehat{W}\\) with a predicted value which is uncorrelated with the error \\(e\\). One method is the split-sample IV (SSIV) estimator of Angrist and Krueger (1995). Divide the sample randomly into two independent halves \\(A\\) and \\(B\\). Use \\(A\\) to estimate the reduced form and \\(B\\) to estimate the structural coefficient. Specifically, use sample \\(A\\) to construct \\(\\widehat{\\Gamma}_{A}=\\left(\\boldsymbol{Z}_{A}^{\\prime} \\boldsymbol{Z}_{A}\\right)^{-1}\\left(\\boldsymbol{Z}_{A}^{\\prime} \\boldsymbol{X}_{A}\\right)\\). Combine this with sample \\(B\\) to create the predicted values \\(\\widehat{\\boldsymbol{W}}_{B}=Z_{B} \\widehat{\\Gamma}_{A}\\). The SSIV estimator is \\(\\widehat{\\beta}_{\\text {ssiv }}=\\) \\(\\left(\\widehat{\\boldsymbol{W}}_{B}^{\\prime} \\boldsymbol{X}_{B}\\right)^{-1}\\left(\\widehat{\\boldsymbol{W}}_{B}^{\\prime} \\boldsymbol{Y}_{B}\\right)\\). This has lower bias than \\(\\widehat{\\beta}_{2 \\text { sls. }}\\)\nA limitation of SSIV is that the results will be sensitive to the sample spliting. One split will produce one estimator; another split will produce a different estimator. Any specific split is arbitrary, so the estimator depends on the specific random sorting of the observations into the samples \\(A\\) and \\(B\\). A second limitation of SSIV is that it is unlikely to work well when the sample size \\(n\\) is small.\nA much better solution is obtained by a leave-one-out estimator for \\(\\Gamma\\). Specifically, let\n\\[\n\\widehat{\\Gamma}_{(-i)}=\\left(\\boldsymbol{Z}^{\\prime} \\boldsymbol{Z}-Z_{i} Z_{i}^{\\prime}\\right)^{-1}\\left(\\boldsymbol{Z}^{\\prime} \\boldsymbol{X}-Z_{i} X_{i}^{\\prime}\\right)\n\\]\nbe the least squares leave-one-out estimator of the reduced form matrix \\(\\Gamma\\), and let \\(\\widehat{W}_{i}=\\widehat{\\Gamma}_{(-i)}^{\\prime} Z_{i}\\) be the reduced form predicted values. Using \\(\\widehat{W}_{i}=\\widehat{\\Gamma}_{(-i)}^{\\prime} Z_{i}\\) as an instrument we obtain the estimator\n\\[\n\\widehat{\\beta}_{\\mathrm{jive1}}=\\left(\\sum_{i=1}^{n} \\widehat{W}_{i} X_{i}^{\\prime}\\right)^{-1}\\left(\\sum_{i=1}^{n} \\widehat{W}_{i} Y_{i}\\right)=\\left(\\sum_{i=1}^{n} \\widehat{\\Gamma}_{(-i)}^{\\prime} Z_{i} X_{i}^{\\prime}\\right)^{-1}\\left(\\sum_{i=1}^{n} \\widehat{\\Gamma}_{(-i)}^{\\prime} Z_{i} Y_{i}\\right) .\n\\]\nThis was called the jackknife instrumental variables (JIVE1) estimator by Angrist, Imbens, and Krueger (1999). It first appeared in Phillips and Hale (1977).\nAngrist, Imbens, and Krueger (1999) pointed out that a somewhat simpler adjustment also removes the correlation and bias. Define the estimator and predicted value\n\\[\n\\begin{aligned}\n\\widetilde{\\Gamma}_{(-i)} &=\\left(\\boldsymbol{Z}^{\\prime} \\boldsymbol{Z}\\right)^{-1}\\left(\\boldsymbol{Z}^{\\prime} \\boldsymbol{X}-Z_{i} X_{i}^{\\prime}\\right) \\\\\n\\widetilde{W}_{i} &=\\widetilde{\\Gamma}_{(-i)}^{\\prime} Z_{i}\n\\end{aligned}\n\\]\nwhich only adjusts the \\(\\boldsymbol{Z}^{\\prime} \\boldsymbol{X}\\) component. Their JIVE2 estimator is\n\\[\n\\widehat{\\beta}_{\\mathrm{jive} 2}=\\left(\\sum_{i=1}^{n} \\widetilde{W}_{i} X_{i}^{\\prime}\\right)^{-1}\\left(\\sum_{i=1}^{n} \\widetilde{W}_{i} Y_{i}\\right)=\\left(\\sum_{i=1}^{n} \\widetilde{\\Gamma}_{(-i)}^{\\prime} Z_{i} X_{i}^{\\prime}\\right)^{-1}\\left(\\sum_{i=1}^{n} \\widetilde{\\Gamma}_{(-i)}^{\\prime} Z_{i} Y_{i}\\right) .\n\\]\nUsing the formula for leave-one-out estimators (Theorem 3.7), the JIVE1 and JIVE2 estimators use two linear operations: the first to create the predicted values \\(\\widehat{W}_{i}\\) or \\(\\widetilde{W}_{i}\\), and the second to calculate the IV estimator. Thus the estimators do not require significantly more computation than 2SLS.\nAn asymptotic distribution theory for JIVE1 and JIVE2 was developed by Chao, Swanson, Hausman, Newey, and Woutersen (2012).\nThe JIVE1 and JIVE2 estimators may be calculated in Stata using the \\(j\\) ive command. It is not a part of the standard package but can be easily added."
  },
  {
    "objectID": "chpt12-iv.html#consistency-of-2sls",
    "href": "chpt12-iv.html#consistency-of-2sls",
    "title": "12  Instrumental Variables",
    "section": "12.15 Consistency of 2SLS",
    "text": "12.15 Consistency of 2SLS\nWe now demonstrate the consistency of the 2SLS estimator for the structural parameter. The following is a set of regularity conditions.\nAssumption $12.1\n\nThe variables \\(\\left(Y_{1 i}, X_{i}, Z_{i}\\right), i=1, \\ldots, n\\), are independent and identically distributed.\n\\(\\mathbb{E}\\left[Y_{1}^{2}\\right]<\\infty\\).\n\\(\\mathbb{E}\\|X\\|^{2}<\\infty\\).\n\\(\\mathbb{E}\\|Z\\|^{2}<\\infty\\)\n\\(\\mathbb{E}\\left[Z Z^{\\prime}\\right]\\) is positive definite.\n\\(\\mathbb{E}\\left[Z X^{\\prime}\\right]\\) has full rank \\(k\\).\n\\(\\mathbb{E}[Z e]=0\\).\n\nAssumptions 12.1.2-4 state that all variables have finite variances. Assumption 12.1.5 states that the instrument vector has an invertible design matrix, which is identical to the core assumption about regressors in the linear regression model. This excludes linearly redundant instruments. Assumptions 12.1.6 and 12.1.7 are the key identification conditions for instrumental variables. Assumption 12.1.6 states that the instruments and regressors have a full-rank cross-moment matrix. This is often called the relevance condition. Assumption 12.1.7 states that the instrumental variables and structural error are uncorrelated. Assumptions 12.1.5-7 are identical to Definition 12.1.\nTheorem 12.1 Under Assumption 12.1, \\(\\widehat{\\beta}_{2 s l s} \\underset{p}{\\longrightarrow} \\beta\\) as \\(n \\rightarrow \\infty\\).\nThe proof of the theorem is provided below.\nThis theorem shows that the 2SLS estimator is consistent for the structural coefficient \\(\\beta\\) under similar moment conditions as the least squares estimator. The key differences are the instrumental variables assumption \\(\\mathbb{E}[Z e]=0\\) and the relevance condition \\(\\operatorname{rank}\\left(\\mathbb{E}\\left[Z X^{\\prime}\\right]\\right)=k\\).\nThe result includes the IV estimator (when \\(\\ell=k\\) ) as a special case.\nThe proof of this consistency result is similar to that for least squares. Take the structural equation \\(\\boldsymbol{Y}=\\boldsymbol{X} \\beta+\\boldsymbol{e}\\) in matrix format and substitute it into the expression for the estimator. We obtain\n\\[\n\\begin{aligned}\n\\widehat{\\beta}_{2 \\text { sls }} &=\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{Z}\\left(\\boldsymbol{Z}^{\\prime} \\boldsymbol{Z}\\right)^{-1} \\boldsymbol{Z}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\boldsymbol{X}^{\\prime} \\boldsymbol{Z}\\left(\\boldsymbol{Z}^{\\prime} \\boldsymbol{Z}\\right)^{-1} \\boldsymbol{Z}^{\\prime}(\\boldsymbol{X} \\beta+\\boldsymbol{e}) \\\\\n&=\\beta+\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{Z}\\left(\\boldsymbol{Z}^{\\prime} \\boldsymbol{Z}\\right)^{-1} \\boldsymbol{Z}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\boldsymbol{X}^{\\prime} \\boldsymbol{Z}\\left(\\boldsymbol{Z}^{\\prime} \\boldsymbol{Z}\\right)^{-1} \\boldsymbol{Z}^{\\prime} \\boldsymbol{e} .\n\\end{aligned}\n\\]\nThis separates out the stochastic component. Re-writing and applying the WLLN and CMT\n\\[\n\\begin{aligned}\n\\widehat{\\beta}_{2 s l s}-\\beta &=\\left(\\left(\\frac{1}{n} \\boldsymbol{X}^{\\prime} \\boldsymbol{Z}\\right)\\left(\\frac{1}{n} \\boldsymbol{Z}^{\\prime} \\boldsymbol{Z}\\right)^{-1}\\left(\\frac{1}{n} \\boldsymbol{Z}^{\\prime} \\boldsymbol{X}\\right)\\right)^{-1} \\\\\n& \\times\\left(\\frac{1}{n} \\boldsymbol{X}^{\\prime} \\boldsymbol{Z}\\right)\\left(\\frac{1}{n} \\boldsymbol{Z}^{\\prime} \\boldsymbol{Z}\\right)^{-1}\\left(\\frac{1}{n} \\boldsymbol{Z}^{\\prime} \\boldsymbol{e}\\right) \\\\\n\\underset{p}{\\rightarrow}\\left(\\boldsymbol{Q}_{X Z} \\boldsymbol{Q}_{Z Z}^{-1} \\boldsymbol{Q}_{Z X}\\right)^{-1} \\boldsymbol{Q}_{X Z} \\boldsymbol{Q}_{Z Z}^{-1} \\mathbb{E}[Z e]=0\n\\end{aligned}\n\\]\nwhere\n\\[\n\\begin{aligned}\n&\\boldsymbol{Q}_{X Z}=\\mathbb{E}\\left[X Z^{\\prime}\\right] \\\\\n&\\boldsymbol{Q}_{Z Z}=\\mathbb{E}\\left[Z Z^{\\prime}\\right] \\\\\n&\\boldsymbol{Q}_{Z X}=\\mathbb{E}\\left[Z X^{\\prime}\\right] .\n\\end{aligned}\n\\]\nThe WLLN holds under Assumptions 12.1.1 and 12.1.2-4. The continuous mapping theorem applies if the matrices \\(\\boldsymbol{Q}_{Z Z}\\) and \\(\\boldsymbol{Q}_{X Z} \\boldsymbol{Q}_{Z Z}^{-1} \\boldsymbol{Q}_{Z X}\\) are invertible, which hold under Assumptions 12.1.5 and 12.1.6. The final equality uses Assumption 12.1.7."
  },
  {
    "objectID": "chpt12-iv.html#asymptotic-distribution-of-2sls",
    "href": "chpt12-iv.html#asymptotic-distribution-of-2sls",
    "title": "12  Instrumental Variables",
    "section": "12.16 Asymptotic Distribution of 2SLS",
    "text": "12.16 Asymptotic Distribution of 2SLS\nWe now show that the 2SLS estimator satisfies a central limit theorem. We first state a set of sufficient regularity conditions. Assumption 12.2 In addition to Assumption 12.1,\n\n\\(\\mathbb{E}\\left[Y_{1}^{4}\\right]<\\infty\\).\n\\(\\mathbb{E}\\|X\\|^{4}<\\infty\\).\n\\(\\mathbb{E}\\|Z\\|^{4}<\\infty\\).\n\\(\\Omega=\\mathbb{E}\\left[Z Z^{\\prime} e^{2}\\right]\\) is positive definite.\n\nAssumption \\(12.2\\) strengthens Assumption \\(12.1\\) by requiring that the dependent variable and instruments have finite fourth moments. This is used to establish the central limit theorem.\nTheorem 12.2 Under Assumption 12.2, as \\(n \\rightarrow \\infty\\).\n\\[\n\\sqrt{n}\\left(\\widehat{\\beta}_{2 \\text { sls }}-\\beta\\right) \\underset{d}{\\longrightarrow} \\mathrm{N}\\left(0, V_{\\beta}\\right)\n\\]\nwhere\n\\[\n\\boldsymbol{V}_{\\beta}=\\left(\\boldsymbol{Q}_{X Z} \\boldsymbol{Q}_{Z Z}^{-1} \\boldsymbol{Q}_{Z X}\\right)^{-1}\\left(\\boldsymbol{Q}_{X Z} \\boldsymbol{Q}_{Z Z}^{-1} \\Omega \\boldsymbol{Q}_{Z Z}^{-1} \\boldsymbol{Q}_{Z X}\\right)\\left(\\boldsymbol{Q}_{X Z} \\boldsymbol{Q}_{Z Z}^{-1} \\boldsymbol{Q}_{Z X}\\right)^{-1}\n\\]\nThis shows that the 2 SLS estimator converges at a \\(\\sqrt{n}\\) rate to a normal random vector. It shows as well the form of the covariance matrix. The latter takes a substantially more complicated form than the least squares estimator.\nAs in the case of least squares estimation the asymptotic variance simplifies under a conditional homoskedasticity condition. For 2SLS the simplification occurs when \\(\\mathbb{E}\\left[e^{2} \\mid Z\\right]=\\sigma^{2}\\). This holds when \\(Z\\) and \\(e\\) are independent. It may be reasonable in some contexts to conceive that the error \\(e\\) is independent of the excluded instruments \\(Z_{2}\\), since by assumption the impact of \\(Z_{2}\\) on \\(Y\\) is only through \\(X\\), but there is no reason to expect \\(e\\) to be independent of the included exogenous variables \\(X_{1}\\). Hence heteroskedasticity should be equally expected in 2SLS and least squares regression. Nevertheless, under homoskedasticity we have the simplifications \\(\\Omega=\\boldsymbol{Q}_{Z Z} \\sigma^{2}\\) and \\(\\boldsymbol{V}_{\\beta}=\\boldsymbol{V}_{\\beta}^{0} \\stackrel{\\text { def }}{=}\\left(\\boldsymbol{Q}_{X Z} \\boldsymbol{Q}_{Z Z}^{-1} \\boldsymbol{Q}_{Z X}\\right)^{-1} \\sigma^{2}\\).\nThe derivation of the asymptotic distribution builds on the proof of consistency. Using equation (12.39) we have\n\\[\n\\sqrt{n}\\left(\\widehat{\\beta}_{2 \\mathrm{sls}}-\\beta\\right)=\\left(\\left(\\frac{1}{n} \\boldsymbol{X}^{\\prime} \\boldsymbol{Z}\\right)\\left(\\frac{1}{n} \\boldsymbol{Z}^{\\prime} \\boldsymbol{Z}\\right)^{-1}\\left(\\frac{1}{n} \\boldsymbol{Z}^{\\prime} \\boldsymbol{X}\\right)\\right)^{-1}\\left(\\frac{1}{n} \\boldsymbol{X}^{\\prime} \\boldsymbol{Z}\\right)\\left(\\frac{1}{n} \\boldsymbol{Z}^{\\prime} \\boldsymbol{Z}\\right)^{-1}\\left(\\frac{1}{\\sqrt{n}} \\boldsymbol{Z}^{\\prime} \\boldsymbol{e}\\right) \\text {. }\n\\]\nWe apply the WLLN and CMT for the moment matrices involving \\(X\\) and \\(\\boldsymbol{Z}\\) the same as in the proof of consistency. In addition, by the CLT for i.i.d. observations\n\\[\n\\frac{1}{\\sqrt{n}} \\boldsymbol{Z}^{\\prime} \\boldsymbol{e}=\\frac{1}{\\sqrt{n}} \\sum_{i=1}^{n} Z_{i} e_{i} \\underset{d}{\\longrightarrow} \\mathrm{N}(0, \\Omega)\n\\]\nbecause the vector \\(Z_{i} e_{i}\\) is i.i.d. and mean zero under Assumptions 12.1.1 and 12.1.7, and has a finite second moment as we verify below. We obtain\n\\[\n\\begin{aligned}\n\\sqrt{n}\\left(\\widehat{\\beta}_{2 \\text { sls }}-\\beta\\right) &=\\left(\\left(\\frac{1}{n} \\boldsymbol{X}^{\\prime} \\boldsymbol{Z}\\right)\\left(\\frac{1}{n} \\boldsymbol{Z}^{\\prime} \\boldsymbol{Z}\\right)^{-1}\\left(\\frac{1}{n} \\boldsymbol{Z}^{\\prime} \\boldsymbol{X}\\right)\\right)^{-1}\\left(\\frac{1}{n} \\boldsymbol{X}^{\\prime} \\boldsymbol{Z}\\right)\\left(\\frac{1}{n} \\boldsymbol{Z}^{\\prime} \\boldsymbol{Z}\\right)^{-1}\\left(\\frac{1}{\\sqrt{n}} \\boldsymbol{Z}^{\\prime} \\boldsymbol{e}\\right) \\\\\n& \\underset{d}{\\rightarrow}\\left(\\boldsymbol{Q}_{X Z} \\boldsymbol{Q}_{Z Z}^{-1} \\boldsymbol{Q}_{Z X}\\right)^{-1} \\boldsymbol{Q}_{X Z} \\boldsymbol{Q}_{Z Z}^{-1} \\mathrm{~N}(0, \\Omega)=\\mathrm{N}\\left(0, \\boldsymbol{V}_{\\beta}\\right)\n\\end{aligned}\n\\]\nas stated.\nTo complete the proof we demonstrate that \\(Z e\\) has a finite second moment under Assumption 12.2. To see this, note that by Minkowski’s inequality (B.34)\n\\[\n\\left(\\mathbb{E}\\left[e^{4}\\right]\\right)^{1 / 4}=\\left(\\mathbb{E}\\left[\\left(Y_{1}-X^{\\prime} \\beta\\right)^{4}\\right]\\right)^{1 / 4} \\leq\\left(\\mathbb{E}\\left[Y_{1}^{4}\\right]\\right)^{1 / 4}+\\|\\beta\\|\\left(\\mathbb{E}\\|X\\|^{4}\\right)^{1 / 4}<\\infty\n\\]\nunder Assumptions 12.2.1 and 12.2.2. Then by the Cauchy-Schwarz inequality (B.32)\n\\[\n\\mathbb{E}\\|Z e\\|^{2} \\leq\\left(\\mathbb{E}\\|Z\\|^{4}\\right)^{1 / 2}\\left(\\mathbb{E}\\left[e^{4}\\right]\\right)^{1 / 2}<\\infty\n\\]\nusing Assumptions 12.2.3."
  },
  {
    "objectID": "chpt12-iv.html#determinants-of-2-sls-variance",
    "href": "chpt12-iv.html#determinants-of-2-sls-variance",
    "title": "12  Instrumental Variables",
    "section": "12.17 Determinants of 2 SLS Variance",
    "text": "12.17 Determinants of 2 SLS Variance\nIt is instructive to examine the asymptotic variance of the 2SLS estimator to understand the factors which determine the precision (or lack thereof) of the estimator. As in the least squares case it is more transparent to examine the variance under the assumption of homoskedasticity. In this case the asymptotic variance takes the form\n\\[\n\\begin{aligned}\n\\boldsymbol{V}_{\\beta}^{0} &=\\left(\\boldsymbol{Q}_{X Z} \\boldsymbol{Q}_{Z Z}^{-1} \\boldsymbol{Q}_{Z X}\\right)^{-1} \\sigma^{2} \\\\\n&=\\left(\\mathbb{E}\\left[X Z^{\\prime}\\right]\\left(\\mathbb{E}\\left[Z Z^{\\prime}\\right]\\right)^{-1} \\mathbb{E}\\left[Z X^{\\prime}\\right]\\right)^{-1} \\mathbb{E}\\left[e^{2}\\right] .\n\\end{aligned}\n\\]\nAs in the least squares case we can see that the variance of \\(\\widehat{\\beta}_{2 \\text { sls }}\\) is increasing in the variance of the error \\(e\\) and decreasing in the variance of \\(X\\). What is different is that the variance is decreasing in the (matrixvalued) correlation between \\(X\\) and \\(Z\\).\nIt is also useful to observe that the variance expression is not affected by the variance structure of \\(Z\\). Indeed, \\(\\boldsymbol{V}_{\\beta}^{0}\\) is invariant to rotations of \\(Z\\) (if you replace \\(Z\\) with \\(\\boldsymbol{C Z}\\) for invertible \\(\\boldsymbol{C}\\) the expression does not change). This means that the variance expression is not affected by the scaling of \\(Z\\) and is not directly affected by correlation among the \\(Z\\).\nWe can also use this expression to examine the impact of increasing the instrument set. Suppose we partition \\(Z=\\left(Z_{a}, Z_{b}\\right)\\) where \\(\\operatorname{dim}\\left(Z_{a}\\right) \\geq k\\) so we can construct a 2SLS estimator using \\(Z_{a}\\) alone. Let \\(\\widehat{\\beta}_{a}\\) and \\(\\widehat{\\beta}\\) denote the 2SLS estimators constructed using the instrument sets \\(Z_{a}\\) and \\(\\left(Z_{a}, Z_{b}\\right)\\), respectively. Without loss of generality we can assume that \\(Z_{a}\\) and \\(Z_{b}\\) are uncorrelated (if not, replace \\(Z_{b}\\) with the projection error after projecting onto \\(Z_{a}\\) ). In this case both \\(\\mathbb{E}\\left[Z Z^{\\prime}\\right]\\) and \\(\\left(\\mathbb{E}\\left[Z Z^{\\prime}\\right]\\right)^{-1}\\) are block diagonal so\n\\[\n\\begin{aligned}\n\\operatorname{avar}[\\widehat{\\beta}] &=\\left(\\mathbb{E}\\left[X Z^{\\prime}\\right]\\left(\\mathbb{E}\\left[Z Z^{\\prime}\\right]\\right)^{-1} \\mathbb{E}\\left[Z X^{\\prime}\\right]\\right)^{-1} \\sigma^{2} \\\\\n&=\\left(\\mathbb{E}\\left[X Z_{a}^{\\prime}\\right]\\left(\\mathbb{E}\\left[Z_{a} Z_{a}^{\\prime}\\right]\\right)^{-1} \\mathbb{E}\\left[Z_{a} X^{\\prime}\\right]+\\mathbb{E}\\left[X Z_{b}^{\\prime}\\right]\\left(\\mathbb{E}\\left[Z_{b} Z_{b}^{\\prime}\\right]\\right)^{-1} \\mathbb{E}\\left[Z_{b} X^{\\prime}\\right]\\right)^{-1} \\sigma^{2} \\\\\n& \\leq\\left(\\mathbb{E}\\left[X Z_{a}^{\\prime}\\right]\\left(\\mathbb{E}\\left[Z_{a} Z_{a}^{\\prime}\\right]\\right)^{-1} \\mathbb{E}\\left[Z_{a} X^{\\prime}\\right]\\right)^{-1} \\sigma^{2} \\\\\n&=\\operatorname{avar}\\left[\\widehat{\\beta}_{a}\\right]\n\\end{aligned}\n\\]\nwith strict inequality if \\(\\mathbb{E}\\left[X Z_{b}^{\\prime}\\right] \\neq 0\\). Thus the 2SLS estimator with the full instrument set has a smaller asymptotic variance than the estimator with the smaller instrument set.\nWhat we have shown is that the asymptotic variance of the 2SLS estimator is decreasing as the number of instruments increases. From the viewpoint of asymptotic efficiency this means that it is better to use more instruments (when they are available and are all known to be valid instruments).\nUnfortunately there is a catch. It turns out that the finite sample bias of the 2SLS estimator (which cannot be calculated exactly but can be approximated using asymptotic expansions) is generically increasing linearily as the number of instruments increases. We will see some calculations illustrating this phenomenon in Section 12.37. Thus the choice of instruments in practice induces a trade-off between bias and variance."
  },
  {
    "objectID": "chpt12-iv.html#covariance-matrix-estimation",
    "href": "chpt12-iv.html#covariance-matrix-estimation",
    "title": "12  Instrumental Variables",
    "section": "12.18 Covariance Matrix Estimation",
    "text": "12.18 Covariance Matrix Estimation\nEstimation of the asymptotic covariance matrix \\(\\boldsymbol{V}_{\\beta}\\) is done using similar techniques as for least squares estimation. The estimator is constructed by replacing the population moment matrices by sample counterparts. Thus\n\\[\n\\widehat{\\boldsymbol{V}}_{\\beta}=\\left(\\widehat{\\mathbf{Q}}_{X Z} \\widehat{\\mathbf{Q}}_{Z Z}^{-1} \\widehat{\\mathbf{Q}}_{Z X}\\right)^{-1}\\left(\\widehat{\\mathbf{Q}}_{X Z} \\widehat{\\mathbf{Q}}_{Z Z}^{-1} \\widehat{\\Omega} \\widehat{\\mathbf{Q}}_{Z Z}^{-1} \\widehat{\\mathbf{Q}}_{Z X}\\right)\\left(\\widehat{\\mathbf{Q}}_{X Z} \\widehat{\\mathbf{Q}}_{Z Z}^{-1} \\widehat{\\mathbf{Q}}_{Z X}\\right)^{-1}\n\\]\nwhere\n\\[\n\\begin{aligned}\n\\widehat{\\boldsymbol{Q}}_{Z Z} &=\\frac{1}{n} \\sum_{i=1}^{n} Z_{i} Z_{i}^{\\prime}=\\frac{1}{n} \\boldsymbol{Z}^{\\prime} \\boldsymbol{Z} \\\\\n\\widehat{\\boldsymbol{Q}}_{X Z} &=\\frac{1}{n} \\sum_{i=1}^{n} X_{i} Z_{i}^{\\prime}=\\frac{1}{n} \\boldsymbol{X}^{\\prime} \\boldsymbol{Z} \\\\\n\\widehat{\\Omega} &=\\frac{1}{n} \\sum_{i=1}^{n} Z_{i} Z_{i}^{\\prime} \\widehat{e}_{i}^{2} \\\\\n\\widehat{e}_{i} &=Y_{i}-X_{i}^{\\prime} \\widehat{\\beta}_{2 s l s}\n\\end{aligned}\n\\]\nThe homoskedastic covariance matrix can be estimated by\n\\[\n\\begin{aligned}\n\\widehat{\\boldsymbol{V}}_{\\beta}^{0} &=\\left(\\widehat{\\boldsymbol{Q}}_{X Z} \\widehat{\\boldsymbol{Q}}_{Z Z}^{-1} \\widehat{\\boldsymbol{Q}}_{Z X}\\right)^{-1} \\widehat{\\sigma}^{2} \\\\\n\\widehat{\\sigma}^{2} &=\\frac{1}{n} \\sum_{i=1}^{n} \\widehat{e}_{i}^{2}\n\\end{aligned}\n\\]\nStandard errors for the coefficients are obtained as the square roots of the diagonal elements of \\(n^{-1} \\widehat{\\boldsymbol{V}}_{\\beta}\\). Confidence intervals, t-tests, and Wald tests may all be constructed from the coefficient and covariance matrix estimates exactly as for least squares regression.\nIn Stata the ivregress command by default calculates the covariance matrix estimator using the homoskedastic covariance matrix. To obtain covariance matrix estimation and standard errors with the robust estimator \\(\\widehat{\\boldsymbol{V}}_{\\beta}\\), use the “,r” option.\nTheorem 12.3 Under Assumption 12.2, as \\(n \\rightarrow \\infty, \\widehat{\\boldsymbol{V}}_{\\beta}^{0}{\\underset{p}{\\longrightarrow}}^{\\boldsymbol{V}_{\\beta}^{0}}\\) and \\(\\widehat{\\boldsymbol{V}}_{\\beta} \\vec{p}^{\\boldsymbol{V}_{\\beta}}\\) To prove Theorem \\(12.3\\) the key is to show \\(\\widehat{\\Omega} \\vec{p} \\Omega\\) as the other convergence results were established in the proof of consistency. We defer this to Exercise 12.6.\nIt is important that the covariance matrix be constructed using the correct residual formula \\(\\widehat{e}_{i}=Y_{i}-\\) \\(X_{i}^{\\prime} \\widehat{\\beta}_{2 \\text { sls }}\\). This is different than what would be obtained if the “two-stage” computation method were used. To see this let’s walk through the two-stage method. First, we estimate the reduced form \\(X_{i}=\\widehat{\\Gamma}^{\\prime} Z_{i}+\\widehat{u}_{i}\\) to obtain the predicted values \\(\\widehat{X}_{i}=\\widehat{\\Gamma}^{\\prime} Z_{i}\\). Second, we regress \\(Y\\) on \\(\\widehat{X}\\) to obtain the 2SLS estimator \\(\\widehat{\\beta}_{2 \\text { sls }}\\). This latter regression takes the form\n\\[\nY_{i}=\\widehat{X}_{i}^{\\prime} \\widehat{\\beta}_{2 \\mathrm{sls}}+\\widehat{v}_{i}\n\\]\nwhere \\(\\widehat{v}_{i}\\) are least squares residuals. The covariance matrix (and standard errors) reported by this regression are constructed using the residual \\(\\widehat{v}_{i}\\). For example, the homoskedastic formula is\n\\[\n\\begin{aligned}\n\\widehat{\\boldsymbol{V}}_{\\beta} &=\\left(\\frac{1}{n} \\widehat{\\boldsymbol{X}}^{\\prime} \\widehat{\\boldsymbol{X}}\\right)^{-1} \\widehat{\\sigma}_{v}^{2}=\\left(\\widehat{\\boldsymbol{Q}}_{X Z} \\widehat{\\boldsymbol{Q}}_{Z Z}^{-1} \\widehat{\\mathbf{Q}}_{Z X}\\right)^{-1} \\widehat{\\sigma}_{v}^{2} \\\\\n\\widehat{\\sigma}_{v}^{2} &=\\frac{1}{n} \\sum_{i=1}^{n} \\widehat{v}_{i}^{2}\n\\end{aligned}\n\\]\nwhich is proportional to the variance estimator \\(\\widehat{\\sigma}_{v}^{2}\\) rather than \\(\\widehat{\\sigma}^{2}\\). This is important because the residual \\(\\widehat{v}\\) differs from \\(\\widehat{e}\\). We can see this because the regression (12.41) uses the regressor \\(\\widehat{X}\\) rather than \\(X\\). Indeed, we calculate that\n\\[\n\\widehat{v}_{i}=Y_{i}-X_{i}^{\\prime} \\widehat{\\beta}_{2 \\mathrm{sls}}+\\left(X_{i}-\\widehat{X}_{i}\\right)^{\\prime} \\widehat{\\beta}_{2 \\mathrm{sls}}=\\widehat{e}_{i}+\\widehat{u}_{i}^{\\prime} \\widehat{\\beta}_{2 \\mathrm{sls}} \\neq \\widehat{e}_{i} \\text {. }\n\\]\nThis means that standard errors reported by the regression (12.41) will be incorrect.\nThis problem is avoided if the 2SLS estimator is constructed directly and the standard errors calculated with the correct formula rather than taking the “two-step” shortcut."
  },
  {
    "objectID": "chpt12-iv.html#liml-asymptotic-distribution",
    "href": "chpt12-iv.html#liml-asymptotic-distribution",
    "title": "12  Instrumental Variables",
    "section": "12.19 LIML Asymptotic Distribution",
    "text": "12.19 LIML Asymptotic Distribution\nIn this section we show that the LIML estimator is asymptotically equivalent to the 2SLS estimator. We recommend, however, a different covariance matrix estimator based on the IV representation.\nWe start by deriving the asymptotic distribution. Recall that the LIML estimator has several representations including\n\\[\n\\widehat{\\beta}_{\\mathrm{liml}}=\\left(\\boldsymbol{X}^{\\prime}\\left(\\boldsymbol{I}_{n}-\\widehat{\\kappa} \\boldsymbol{M}_{\\boldsymbol{Z}}\\right) \\boldsymbol{X}\\right)^{-1}\\left(\\boldsymbol{X}^{\\prime}\\left(\\boldsymbol{I}_{n}-\\widehat{\\kappa} \\boldsymbol{M}_{\\boldsymbol{Z}}\\right) \\boldsymbol{Y}_{1}\\right)\n\\]\nwhere\n\\[\n\\widehat{\\boldsymbol{\\kappa}}=\\min _{\\gamma} \\frac{\\gamma^{\\prime} \\overrightarrow{\\boldsymbol{Y}}^{\\prime} \\boldsymbol{M}_{1} \\overrightarrow{\\boldsymbol{Y}} \\gamma}{\\gamma^{\\prime} \\overrightarrow{\\boldsymbol{Y}}^{\\prime} \\boldsymbol{M}_{\\boldsymbol{Z}} \\overrightarrow{\\boldsymbol{Y}} \\gamma}\n\\]\nand \\(\\gamma=\\left(1,-\\beta_{2}^{\\prime}\\right)^{\\prime}\\). For the distribution theory it is useful to rewrite the slope coefficient as\n\\[\n\\widehat{\\beta}_{\\mathrm{liml}}=\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{P}_{Z} \\boldsymbol{X}-\\widehat{\\mu} \\boldsymbol{X}^{\\prime} \\boldsymbol{M}_{Z} \\boldsymbol{X}\\right)^{-1}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{P}_{\\boldsymbol{Z}} \\boldsymbol{Y}_{1}-\\widehat{\\mu} \\boldsymbol{X}^{\\prime} \\boldsymbol{M}_{Z} \\boldsymbol{Y}_{1}\\right)\n\\]\nwhere\n\\[\n\\widehat{\\mu}=\\widehat{\\kappa}-1=\\min _{\\gamma} \\frac{\\gamma^{\\prime} \\overrightarrow{\\boldsymbol{Y}}^{\\prime} \\boldsymbol{M}_{1} \\boldsymbol{Z}_{2}\\left(\\boldsymbol{Z}_{2}^{\\prime} \\boldsymbol{M}_{1} \\boldsymbol{Z}_{2}\\right)^{-1} \\boldsymbol{Z}_{2}^{\\prime} \\boldsymbol{M}_{1} \\overrightarrow{\\boldsymbol{Y}} \\gamma}{\\gamma^{\\prime} \\overrightarrow{\\boldsymbol{Y}}^{\\prime} \\boldsymbol{M}_{\\boldsymbol{Z}} \\overrightarrow{\\boldsymbol{Y}} \\gamma} \\text {. }\n\\]\nThis second equality holds because the span of \\(\\boldsymbol{Z}=\\left[\\boldsymbol{Z}_{1}, \\boldsymbol{Z}_{2}\\right]\\) equals the span of \\(\\left[\\boldsymbol{Z}_{1}, \\boldsymbol{M}_{1} \\boldsymbol{Z}_{2}\\right]\\). This implies\n\\[\n\\boldsymbol{P}_{\\boldsymbol{Z}}=\\boldsymbol{Z}\\left(\\boldsymbol{Z}^{\\prime} \\boldsymbol{Z}\\right)^{-1} \\boldsymbol{Z}^{\\prime}=\\boldsymbol{Z}_{1}\\left(\\boldsymbol{Z}_{1}^{\\prime} \\boldsymbol{Z}_{1}\\right)^{-1} \\boldsymbol{Z}_{1}^{\\prime}+\\boldsymbol{M}_{1} \\boldsymbol{Z}_{2}\\left(\\boldsymbol{Z}_{2}^{\\prime} \\boldsymbol{M}_{1} \\boldsymbol{Z}_{2}\\right)^{-1} \\boldsymbol{Z}_{2}^{\\prime} \\boldsymbol{M}_{1}\n\\]\nWe now show that \\(n \\widehat{\\mu}=O_{p}(1)\\). The reduced form (12.33) implies that\n\\[\n\\boldsymbol{Y}=\\boldsymbol{Z}_{1} \\Pi_{1}+\\boldsymbol{Z}_{2} \\Pi_{2}+\\boldsymbol{e} .\n\\]\nIt will be important to note that\n\\[\n\\Pi_{2}=\\left[\\lambda_{2}, \\Gamma_{22}\\right]=\\left[\\Gamma_{22} \\beta_{2}, \\Gamma_{22}\\right]\n\\]\nusing (12.15). It follows that \\(\\Pi_{2} \\gamma=0\\). Note \\(\\boldsymbol{U} \\gamma=\\boldsymbol{e}\\). Then \\(\\boldsymbol{M}_{\\boldsymbol{Z}} \\boldsymbol{Y} \\gamma=\\boldsymbol{M}_{\\boldsymbol{Z}} \\boldsymbol{e}\\) and \\(\\boldsymbol{M}_{1} \\boldsymbol{Y} \\gamma=\\boldsymbol{M}_{1} \\boldsymbol{e}\\). Hence\n\\[\n\\begin{aligned}\nn \\widehat{\\mu} &=\\min _{\\gamma} \\frac{\\gamma^{\\prime} \\overrightarrow{\\boldsymbol{Y}}^{\\prime} \\boldsymbol{M}_{1} \\boldsymbol{Z}_{2}\\left(\\boldsymbol{Z}_{2}^{\\prime} \\boldsymbol{M}_{1} \\boldsymbol{Z}_{2}\\right)^{-1} \\boldsymbol{Z}_{2}^{\\prime} \\boldsymbol{M}_{1} \\overrightarrow{\\boldsymbol{Y}} \\gamma}{\\gamma^{\\prime} \\frac{1}{n} \\overrightarrow{\\boldsymbol{Y}}^{\\prime} \\boldsymbol{M}_{\\boldsymbol{Z}} \\overrightarrow{\\boldsymbol{Y}} \\gamma} \\\\\n& \\leq \\frac{\\left(\\frac{1}{\\sqrt{n}} \\boldsymbol{e}^{\\prime} \\boldsymbol{M}_{1} \\boldsymbol{Z}_{2}\\right)\\left(\\frac{1}{n} \\boldsymbol{Z}_{2}^{\\prime} \\boldsymbol{M}_{1} \\boldsymbol{Z}_{2}\\right)^{-1}\\left(\\frac{1}{\\sqrt{n}} \\boldsymbol{Z}_{2}^{\\prime} \\boldsymbol{M}_{1} \\boldsymbol{e}\\right)}{\\frac{1}{n} \\boldsymbol{e}^{\\prime} \\boldsymbol{M}_{\\boldsymbol{Z}} \\boldsymbol{e}} \\\\\n&=O_{p}(1) .\n\\end{aligned}\n\\]\nIt follows that\n\\[\n\\begin{aligned}\n\\sqrt{n}\\left(\\widehat{\\beta}_{\\mathrm{liml}}-\\beta\\right) &=\\left(\\frac{1}{n} \\boldsymbol{X}^{\\prime} \\boldsymbol{P}_{Z} \\boldsymbol{X}-\\widehat{\\mu} \\frac{1}{n} \\boldsymbol{X}^{\\prime} \\boldsymbol{M}_{\\boldsymbol{Z}} \\boldsymbol{X}\\right)^{-1}\\left(\\frac{1}{\\sqrt{n}} \\boldsymbol{X}^{\\prime} \\boldsymbol{P}_{Z} \\boldsymbol{e}-\\sqrt{n} \\widehat{\\mu} \\frac{1}{n} \\boldsymbol{X}^{\\prime} \\boldsymbol{M}_{Z} \\boldsymbol{e}\\right) \\\\\n&=\\left(\\frac{1}{n} \\boldsymbol{X}^{\\prime} \\boldsymbol{P}_{Z} \\boldsymbol{X}-o_{p}(1)\\right)^{-1}\\left(\\frac{1}{\\sqrt{n}} \\boldsymbol{X}^{\\prime} \\boldsymbol{P}_{Z} \\boldsymbol{e}-o_{p}(1)\\right) \\\\\n&=\\sqrt{n}\\left(\\widehat{\\beta}_{2 \\mathrm{sls}}-\\beta\\right)+o_{p}(1)\n\\end{aligned}\n\\]\nwhich means that LIML and 2SLS have the same asymptotic distribution. This holds under the same assumptions as for 2SLS.\nConsequently, one method to obtain an asymptotically valid covariance estimator for LIML is to use the 2SLS formula. However, this is not the best choice. Rather, consider the IV representation for LIML\n\\[\n\\widehat{\\beta}_{\\mathrm{liml}}=\\left(\\widetilde{\\boldsymbol{X}}^{\\prime} \\boldsymbol{X}\\right)^{-1}\\left(\\widetilde{\\boldsymbol{X}}^{\\prime} \\boldsymbol{Y}_{1}\\right)\n\\]\nwhere\n\\[\n\\widetilde{\\boldsymbol{X}}=\\left(\\begin{array}{c}\n\\boldsymbol{X}_{1} \\\\\n\\boldsymbol{X}_{2}-\\widehat{\\boldsymbol{K}}_{2}\n\\end{array}\\right)\n\\]\nand \\(\\widehat{\\boldsymbol{U}}_{2}=\\boldsymbol{M}_{\\boldsymbol{Z}} \\boldsymbol{X}_{2}\\). The asymptotic covariance matrix formula for an IV estimator is\n\\[\n\\widehat{\\boldsymbol{V}}_{\\beta}=\\left(\\frac{1}{n} \\widetilde{\\boldsymbol{X}}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\widehat{\\Omega}\\left(\\frac{1}{n} \\boldsymbol{X}^{\\prime} \\tilde{\\boldsymbol{X}}\\right)^{-1}\n\\]\nwhere\n\\[\n\\begin{aligned}\n\\widehat{\\Omega} &=\\frac{1}{n} \\sum_{i=1}^{n} \\widetilde{X}_{i} \\widetilde{X}_{i} \\widehat{e}_{i}^{2} \\\\\n\\widehat{e}_{i} &=Y_{1 i}-X_{i}^{\\prime} \\widehat{\\beta}_{\\text {liml }} .\n\\end{aligned}\n\\]\nThis simplifies to the 2SLS formula when \\(\\widehat{\\kappa}=1\\) but otherwise differs. The estimator (12.42) is a better choice than the 2SLS formula for covariance matrix estimation as it takes advantage of the LIML estimator structure."
  },
  {
    "objectID": "chpt12-iv.html#functions-of-parameters",
    "href": "chpt12-iv.html#functions-of-parameters",
    "title": "12  Instrumental Variables",
    "section": "12.20 Functions of Parameters",
    "text": "12.20 Functions of Parameters\nGiven the distribution theory in Theorems \\(12.2\\) and \\(12.3\\) it is straightforward to derive the asymptotic distribution of smooth nonlinear functions of the coefficient estimators.\nSpecifically, given a function \\(r(\\beta): \\mathbb{R}^{k} \\rightarrow \\Theta \\subset \\mathbb{R}^{q}\\) we define the parameter \\(\\theta=r(\\beta)\\). Given \\(\\widehat{\\beta}_{2 \\text { sls }}\\) a natural estimator of \\(\\theta\\) is \\(\\widehat{\\theta}_{2 \\text { sls }}=r\\left(\\widehat{\\beta}_{2 \\text { sls }}\\right)\\).\nConsistency follows from Theorem \\(12.1\\) and the continuous mapping theorem.\nTheorem 12.4 Under Assumptions \\(12.1\\) and 7.3, as \\(n \\rightarrow \\infty, \\widehat{\\theta}_{2 s l s} \\underset{p}{\\longrightarrow} \\theta\\)\nIf \\(r(\\beta)\\) is differentiable then an estimator of the asymptotic covariance matrix for \\(\\widehat{\\theta}_{2 \\text { sls }}\\) is\n\\[\n\\begin{aligned}\n\\widehat{\\boldsymbol{V}}_{\\theta} &=\\widehat{\\boldsymbol{R}}^{\\prime} \\widehat{\\boldsymbol{V}}_{\\beta} \\widehat{\\boldsymbol{R}} \\\\\n\\widehat{\\boldsymbol{R}} &=\\frac{\\partial}{\\partial \\beta} r\\left(\\widehat{\\beta}_{2 s l s}\\right)^{\\prime} .\n\\end{aligned}\n\\]\nWe similarly define the homoskedastic variance estimator as \\(\\widehat{\\boldsymbol{V}}_{\\theta}^{0}=\\widehat{\\boldsymbol{R}}^{\\prime} \\widehat{\\boldsymbol{V}}_{\\beta}^{0} \\widehat{\\boldsymbol{R}}\\).\nThe asymptotic distribution theory follows from Theorems \\(12.2\\) and \\(12.3\\) and the delta method.\nTheorem 12.5 Under Assumptions \\(12.2\\) and \\(7.3\\), as \\(n \\rightarrow \\infty\\),\n\\[\n\\sqrt{n}\\left(\\widehat{\\theta}_{2 s l s}-\\theta\\right) \\underset{d}{\\longrightarrow} \\mathrm{N}\\left(0, \\boldsymbol{V}_{\\theta}\\right)\n\\]\nand \\(\\widehat{\\boldsymbol{V}}_{\\theta} \\underset{p}{\\longrightarrow} \\boldsymbol{V}_{\\theta}\\) where \\(\\boldsymbol{V}_{\\theta}=\\boldsymbol{R}^{\\prime} \\boldsymbol{V}_{\\beta} \\boldsymbol{R}\\) and \\(\\boldsymbol{R}=\\frac{\\partial}{\\partial \\beta} \\boldsymbol{r}(\\beta)^{\\prime}\\)\nWhen \\(q=1\\), a standard error for \\(\\widehat{\\theta}_{2 \\text { sls }}\\) is \\(s\\left(\\widehat{\\theta}_{2 \\text { sls }}\\right)=\\sqrt{n^{-1} \\widehat{\\boldsymbol{V}}_{\\theta}}\\).\nFor example, let’s take the parameter estimates from the fifth column of Table 12.1, which are the 2SLS estimates with three endogenous regressors and four excluded instruments. Suppose we are interested in the return to experience, which depends on the level of experience. The estimated return at experience \\(=10\\) is \\(0.047-0.032 \\times 2 \\times 10 / 100=0.041\\) and its standard error is \\(0.003\\). This implies a \\(4 %\\) increase in wages per year of experience and is precisely estimated. Or suppose we are interested in the level of experience at which the function maximizes. The estimate is \\(50 \\times 0.047 / 0.032=73\\). This has a standard error of 249 . The large standard error implies that the estimate (73 years of experience) is without precision and is thus uninformative."
  },
  {
    "objectID": "chpt12-iv.html#hypothesis-tests",
    "href": "chpt12-iv.html#hypothesis-tests",
    "title": "12  Instrumental Variables",
    "section": "12.21 Hypothesis Tests",
    "text": "12.21 Hypothesis Tests\nAs in the previous section, for a given function \\(r(\\beta): \\mathbb{R}^{k} \\rightarrow \\Theta \\subset \\mathbb{R}^{q}\\) we define the parameter \\(\\theta=r(\\beta)\\) and consider tests of hypotheses of the form \\(\\mathbb{M}_{0}: \\theta=\\theta_{0}\\) against \\(\\mathbb{H}_{1}: \\theta \\neq \\theta_{0}\\). The Wald statistic for \\(\\mathbb{M}_{0}\\) is\n\\[\nW=n\\left(\\widehat{\\theta}-\\theta_{0}\\right)^{\\prime} \\widehat{\\boldsymbol{V}}_{\\widehat{\\theta}}^{-1}\\left(\\widehat{\\theta}-\\theta_{0}\\right) .\n\\]\nFrom Theorem \\(12.5\\) we deduce that \\(W\\) is asymptotically chi-square distributed. Let \\(G_{q}(u)\\) denote the \\(\\chi_{q}^{2}\\) distribution function.\nTheorem 12.6 Under Assumption 12.2, Assumption 7.3, and \\(\\mathbb{H}_{0}\\), then as \\(n \\rightarrow\\) \\(\\infty, W \\underset{d}{\\rightarrow} \\chi_{q}^{2}\\). For \\(c\\) satisfying \\(\\alpha=1-G_{q}(c), \\mathbb{P}\\left[W>c \\mid \\mathbb{M}_{0}\\right] \\longrightarrow \\alpha\\) so the test “Reject \\(\\mathbb{M}_{0}\\) if \\(W>c\\)” has asymptotic size \\(\\alpha\\).\nIn linear regression we often report the \\(F\\) version of the Wald statistic (by dividing by degrees of freedom) and use the \\(F\\) distribution for inference as this is justified in the normal sampling model. For 2SLS estimation, however, this is not done as there is no finite sample \\(F\\) justification for the \\(F\\) version of the Wald statistic.\nTo illustrate, once again let’s take the parameter estimates from the fifth column of Table \\(12.1\\) and again consider the return to experience which is determined by the coefficients on experience and experience \\(^{2} / 100\\). Neither coefficient is statisticially significant at the \\(5 %\\) level and it is unclear if the overall effect is statistically significant. We can assess this by testing the joint hypothesis that both coefficients are zero. The Wald statistic for this hypothesis is \\(W=244\\) which is highly significant with an asymptotic p-value of \\(0.0000\\). Thus by examining the joint test in contrast to the individual tests is quite clear that experience has a non-zero effect."
  },
  {
    "objectID": "chpt12-iv.html#finite-sample-theory",
    "href": "chpt12-iv.html#finite-sample-theory",
    "title": "12  Instrumental Variables",
    "section": "12.22 Finite Sample Theory",
    "text": "12.22 Finite Sample Theory\nIn Chapter 5 we reviewed the rich exact distribution available for the linear regression model under the assumption of normal innovations. There is a similarly rich literature in econometrics for IV, 2SLS and LIML estimators. An excellent review of the theory, mostly developed in the 1970s and early 1980s, is provided by Peter Phillips (1983).\nThis theory was developed under the assumption that the structural error vector \\(e\\) and reduced form error \\(u_{2}\\) are multivariate normally distributed. Even though the errors are normal, IV-type estimators are nonlinear functions of these errors and are thus non-normally distributed. Formulae for the exact distributions have been derived but are unfortunately functions of model parameters and hence are not directly useful for finite sample inference.\nOne important implication of this literature is that even in this optimal context of exact normal innovations the finite sample distributions of the IV estimators are non-normal and the finite sample distributions of test statistics are not chi-squared. The normal and chi-squared approximations hold asymptotically but there is no reason to expect these approximations to be accurate in finite samples.\nA second important result is that under the assumption of normal errors most of the estimators do not have finite moments in any finite sample. A clean statement concerning the existence of moments for the 2SLS estimator was obtained by Kinal (1980) for the case of joint normality. Let \\(\\widehat{\\beta}_{2 s l s, 2}\\) be the 2SLS estimators of the coefficients on the endogeneous regressors.\nTheorem \\(12.7\\) If \\((Y, X, Z)\\) are jointly normal, then for any \\(r, \\mathbb{E}\\left\\|\\widehat{\\beta}_{2 s l s, 2}\\right\\|^{r}<\\infty\\) if and only if \\(r<\\ell_{2}-k_{2}+1\\). This result states that in the just-identified case the IV estimator does not have any finite order integer moments. In the over-identified case the number of finite moments corresponds to the number of overidentifying restrictions \\(\\left(\\ell_{2}-k_{2}\\right)\\). Thus if there is one over-identifying restriction 2 SLS has a finite expectation and if there are two over-identifying restrictions then the 2SLS estimator has a finite variance.\nThe LIML estimator has a more severe moment problem as it has no finite integer moments (Mariano, 1982) regardless of the number of over-identifying restrictions. Due to this lack of moments Fuller (1977) proposed the following modification of LIML. His estimator is\n\\[\n\\begin{aligned}\n\\widehat{\\beta}_{\\text {Fuller }} &=\\left(\\boldsymbol{X}^{\\prime}\\left(\\boldsymbol{I}_{n}-K \\boldsymbol{M}_{\\boldsymbol{Z}}\\right) \\boldsymbol{X}\\right)^{-1}\\left(\\boldsymbol{X}^{\\prime}\\left(\\boldsymbol{I}_{n}-K \\boldsymbol{M}_{\\boldsymbol{Z}}\\right) \\boldsymbol{Y}_{1}\\right) \\\\\nK &=\\widehat{\\kappa}-\\frac{C}{n-k}\n\\end{aligned}\n\\]\nfor some \\(C \\geq 1\\). Fuller showed that his estimator has all moments finite under suitable conditions.\nHausman, Newey, Woutersen, Chao and Swanson (2012) propose an estimator they call HFUL which combines the ideas of JIVE and Fuller which has excellent finite sample properties."
  },
  {
    "objectID": "chpt12-iv.html#bootstrap-for-2sls",
    "href": "chpt12-iv.html#bootstrap-for-2sls",
    "title": "12  Instrumental Variables",
    "section": "12.23 Bootstrap for 2SLS",
    "text": "12.23 Bootstrap for 2SLS\nThe standard bootstrap algorithm for IV, 2SLS, and GMM generates bootstrap samples by sampling the triplets \\(\\left(Y_{1 i}^{*}, X_{i}^{*}, Z_{i}^{*}\\right)\\) independently and with replacement from the original sample \\(\\left\\{\\left(Y_{1 i}, X_{i}, Z_{i}\\right): i=\\right.\\) \\(1, \\ldots, n\\}\\). Sampling \\(n\\) such observations and stacking into observation matrices \\(\\left(\\boldsymbol{Y}_{1}^{*}, \\boldsymbol{X}^{*}, \\boldsymbol{Z}^{*}\\right)\\), the bootstrap 2SLS estimator is\n\\[\n\\widehat{\\beta}_{2 \\mathrm{sls}}^{*}=\\left(\\boldsymbol{X}^{* \\prime} \\boldsymbol{Z}^{*}\\left(\\boldsymbol{Z}^{* \\prime} \\boldsymbol{Z}^{*}\\right)^{-1} \\boldsymbol{Z}^{* \\prime} \\boldsymbol{X}^{*}\\right)^{-1} \\boldsymbol{X}^{* \\prime} \\boldsymbol{Z}^{*}\\left(\\boldsymbol{Z}^{* \\prime} \\boldsymbol{Z}^{*}\\right)^{-1} \\boldsymbol{Z}^{* \\prime} \\boldsymbol{Y}_{1}^{*}\n\\]\nThis is repeated \\(B\\) times to create a sample of \\(B\\) bootstrap draws. Given these draws bootstrap statistics can be calculated. This includes the bootstrap estimate of variance, standard errors, and confidence intervals, including percentile, \\(\\mathrm{BC}\\) percentile, \\(\\mathrm{BC}_{a}\\) and percentile-t.\nWe now show that the bootstrap estimator has the same asymptotic distribution as the sample estimator. For overidentified cases this demonstration requires a bit of extra care. This was first shown by Hahn (1996).\nThe sample observations satisfy the model \\(Y_{1}=X^{\\prime} \\beta+e\\) with \\(\\mathbb{E}[Z e]=0\\). The true value of \\(\\beta\\) in the population can be written as\n\\[\n\\beta=\\left(\\mathbb{E}\\left[X Z^{\\prime}\\right] \\mathbb{E}\\left[Z Z^{\\prime}\\right]^{-1} \\mathbb{E}\\left[Z X^{\\prime}\\right]\\right)^{-1} \\mathbb{E}\\left[X Z^{\\prime}\\right] \\mathbb{E}\\left[Z Z^{\\prime}\\right]^{-1} \\mathbb{E}\\left[Z Y_{1}\\right]\n\\]\nThe true value in the bootstrap universe is obtained by replacing the population moments by the sample moments, which equals the 2SLS estimator\n\\[\n\\begin{aligned}\n&\\left(\\mathbb{E}^{*}\\left[X^{*} Z^{* \\prime}\\right] \\mathbb{E}^{*}\\left[Z^{*} Z^{* \\prime}\\right]^{-1} \\mathbb{E}^{*}\\left[Z^{*} X^{* \\prime}\\right]\\right)^{-1} \\mathbb{E}^{*}\\left[X^{*} Z^{* \\prime}\\right] \\mathbb{E}^{*}\\left[Z^{*} Z^{* \\prime}\\right]^{-1} \\mathbb{E}^{*}\\left[Z^{*} Y_{1}^{*}\\right] \\\\\n&=\\left(\\left(\\frac{1}{n} \\boldsymbol{X}^{\\prime} \\boldsymbol{Z}\\right)\\left(\\frac{1}{n} \\boldsymbol{Z}^{\\prime} \\boldsymbol{Z}\\right)^{-1}\\left(\\frac{1}{n} \\boldsymbol{Z}^{\\prime} \\boldsymbol{X}\\right)\\right)^{-1}\\left(\\frac{1}{n} \\boldsymbol{X}^{\\prime} \\boldsymbol{Z}\\right)\\left(\\frac{1}{n} \\boldsymbol{Z}^{\\prime} \\boldsymbol{Z}\\right)^{-1}\\left[\\frac{1}{n} \\boldsymbol{Z}^{\\prime} \\boldsymbol{Y}_{1}\\right] \\\\\n&=\\widehat{\\beta}_{2 \\text { sls }} .\n\\end{aligned}\n\\]\nThe bootstrap observations thus satisfy the equation \\(Y_{1 i}^{*}=X_{i}^{* \\prime} \\widehat{\\beta}_{2 s l s}+e_{i}^{*}\\). In matrix notation for the sample this is\n\\[\n\\boldsymbol{Y}_{1}^{*}=\\boldsymbol{X}^{* \\prime} \\widehat{\\beta}_{2 \\mathrm{sls}}+\\boldsymbol{e}^{*} .\n\\]\nGiven a bootstrap triple \\(\\left(Y_{1 i}^{*}, X_{i}^{*}, Z_{i}^{*}\\right)=\\left(Y_{1 j}, X_{j}, Z_{j}\\right)\\) for some observation \\(j\\) the true bootstrap error is\n\\[\ne_{i}^{*}=Y_{1 j}-X_{j}^{\\prime} \\widehat{\\beta}_{2 s l s}=\\widehat{e}_{j} .\n\\]\nIt follows that\n\\[\n\\mathbb{E}^{*}\\left[Z^{*} e^{*}\\right]=n^{-1} \\boldsymbol{Z}^{\\prime} \\widehat{\\boldsymbol{e}} .\n\\]\nThis is generally not equal to zero in the over-identified case.\nThis an an important complication. In over-identified models the true observations satisfy the population condition \\(\\mathbb{E}[Z e]=0\\) but in the bootstrap sample \\(\\mathbb{E}^{*}\\left[Z^{*} e^{*}\\right] \\neq 0\\). This means that to apply the central limit theorem to the bootstrap estimator we first have to recenter the moment condition. That is, (12.44) and the bootstrap CLT imply\n\\[\n\\frac{1}{\\sqrt{n}}\\left(\\boldsymbol{Z}^{* \\prime} \\boldsymbol{e}^{*}-\\boldsymbol{Z}^{\\prime} \\widehat{\\boldsymbol{e}}\\right)=\\frac{1}{\\sqrt{n}} \\sum_{i=1}^{n}\\left(Z_{i}^{*} e_{i}^{*}-\\mathbb{E}^{*}\\left[Z^{*} e^{*}\\right]\\right) \\underset{d^{*}}{\\longrightarrow} \\mathrm{N}(0, \\Omega)\n\\]\nwhere\n\\[\n\\Omega=\\mathbb{E}\\left[Z Z^{\\prime} e^{2}\\right] .\n\\]\nUsing (12.43) we can normalize the bootstrap estimator as\n\\[\n\\begin{aligned}\n\\sqrt{n}\\left(\\widehat{\\boldsymbol{\\beta}}_{2 \\mathrm{sls}}^{*}-\\widehat{\\boldsymbol{\\beta}}_{2 \\mathrm{sls}}\\right) &=\\sqrt{n}\\left(\\boldsymbol{X}^{* \\prime} \\boldsymbol{Z}^{*}\\left(\\boldsymbol{Z}^{* \\prime} \\boldsymbol{Z}^{*}\\right)^{-1} \\boldsymbol{Z}^{* \\prime} \\boldsymbol{X}^{*}\\right)^{-1} \\boldsymbol{X}^{* \\prime} \\boldsymbol{Z}^{*}\\left(\\boldsymbol{Z}^{* \\prime} \\boldsymbol{Z}^{*}\\right)^{-1} \\boldsymbol{Z}^{* \\prime} \\boldsymbol{e}^{*} \\\\\n&=\\left(\\left(\\frac{1}{n} \\boldsymbol{X}^{* \\prime} \\boldsymbol{Z}^{*}\\right)\\left(\\frac{1}{n} \\boldsymbol{Z}^{* \\prime} \\boldsymbol{Z}^{*}\\right)^{-1}\\left(\\frac{1}{n} \\boldsymbol{Z}^{* \\prime} \\boldsymbol{X}^{*}\\right)\\right)^{-1} \\\\\n& \\times\\left(\\frac{1}{n} \\boldsymbol{X}^{* \\prime} \\boldsymbol{Z}^{*}\\right)\\left(\\frac{1}{n} \\boldsymbol{Z}^{* \\prime} \\boldsymbol{Z}^{*}\\right)^{-1} \\frac{1}{\\sqrt{n}}\\left(\\boldsymbol{Z}^{* \\prime} \\boldsymbol{e}^{*}-\\boldsymbol{Z}^{\\prime} \\widehat{\\boldsymbol{e}}\\right) \\\\\n&+\\left(\\left(\\frac{1}{n} \\boldsymbol{X}^{* \\prime} \\boldsymbol{Z}^{*}\\right)\\left(\\frac{1}{n} \\boldsymbol{Z}^{* \\prime} \\boldsymbol{Z}^{*}\\right)^{-1}\\left(\\frac{1}{n} \\boldsymbol{Z}^{* \\prime} \\boldsymbol{X}^{*}\\right)\\right)^{-1} \\\\\n& \\times\\left(\\frac{1}{n} \\boldsymbol{X}^{* \\prime} \\boldsymbol{Z}^{*}\\right)\\left(\\frac{1}{n} \\boldsymbol{Z}^{* \\prime} \\boldsymbol{Z}^{*}\\right)^{-1}\\left(\\frac{1}{\\sqrt{n}} \\boldsymbol{Z}^{\\prime} \\widehat{\\boldsymbol{e}}\\right) .\n\\end{aligned}\n\\]\nUsing the bootstrap WLLN,\n\\[\n\\begin{aligned}\n\\frac{1}{n} \\boldsymbol{X}^{* \\prime} \\boldsymbol{Z}^{*} &=\\frac{1}{n} \\boldsymbol{X}^{\\prime} \\boldsymbol{Z}+o_{p}(1) \\\\\n\\frac{1}{n} \\boldsymbol{Z}^{* \\prime} \\boldsymbol{Z}^{*} &=\\frac{1}{n} \\boldsymbol{Z}^{\\prime} \\boldsymbol{Z}+o_{p}(1) .\n\\end{aligned}\n\\]\nThis implies (12.47) is equal to\n\\[\n\\sqrt{n}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{Z}\\left(\\boldsymbol{Z}^{\\prime} \\boldsymbol{Z}\\right)^{-1}\\left(\\boldsymbol{Z}^{\\prime} \\boldsymbol{X}\\right)\\right)^{-1} \\boldsymbol{X}^{\\prime} \\boldsymbol{Z}\\left(\\boldsymbol{Z}^{\\prime} \\boldsymbol{Z}\\right)^{-1} \\boldsymbol{Z}^{\\prime} \\widehat{\\boldsymbol{e}}+o_{p}(1)=0+o_{p}(1)\n\\]\nThe equality holds because the 2SLS first-order condition implies \\(\\boldsymbol{X}^{\\prime} \\boldsymbol{Z}\\left(\\boldsymbol{Z}^{\\prime} \\boldsymbol{Z}\\right)^{-1} \\boldsymbol{Z}^{\\prime} \\widehat{\\boldsymbol{e}}=0\\). Also, combined with (12.45) we see that (12.46) converges in bootstrap distribution to\n\\[\n\\left(\\boldsymbol{Q}_{X Z} \\boldsymbol{Q}_{Z Z}^{-1} \\boldsymbol{Q}_{Z X}\\right)^{-1} \\boldsymbol{Q}_{X Z} \\boldsymbol{Q}_{Z Z}^{-1} \\mathrm{~N}(0, \\Omega)=\\mathrm{N}\\left(0, \\boldsymbol{V}_{\\beta}\\right)\n\\]\nwhere \\(\\boldsymbol{V}_{\\beta}\\) is the 2SLS asymptotic variance from Theorem 12.2. This is the asymptotic distribution of \\(\\sqrt{n}\\left(\\widehat{\\beta}_{2 s l s}^{*}-\\widehat{\\beta}_{2 s l s}\\right)\\)\nBy standard calculations we can also show that bootstrap t-ratios are asymptotically normal. Theorem 12.8 Under Assumption 12.2, as \\(n \\rightarrow \\infty\\)\n\\[\n\\sqrt{n}\\left(\\widehat{\\beta}_{2 s l s}^{*}-\\widehat{\\beta}_{2 s l s}\\right) \\underset{d^{*}}{\\longrightarrow} \\mathrm{N}\\left(0, \\boldsymbol{V}_{\\beta}\\right)\n\\]\nwhere \\(\\boldsymbol{V}_{\\beta}\\) is the \\(2 \\mathrm{SLS}\\) asymptotic variance from Theorem 12.2. Furthermore,\n\\[\nT^{*}=\\frac{\\sqrt{n}\\left(\\widehat{\\beta}_{2 s l s}^{*}-\\widehat{\\beta}_{2 s l s}\\right)}{s\\left(\\widehat{\\beta}_{2 \\text { sls }}^{*}\\right)} \\underset{d^{*}}{\\longrightarrow} \\mathrm{N}(0,1) .\n\\]\nThis shows that percentile-type and percentile-t confidence intervals are asymptotically valid.\nOne might expect that the asymptotic refinement arguments extend to the \\(\\mathrm{BC}_{a}\\) and percentile-t methods but this does not appear to be the case. While \\(\\sqrt{n}\\left(\\widehat{\\beta}_{2 \\text { sls }}^{*}-\\widehat{\\beta}_{2 s l s}\\right)\\) and \\(\\sqrt{n}\\left(\\widehat{\\beta}_{2 s l s}-\\beta\\right)\\) have the same asymptotic distribution they differ in finite samples by an \\(O_{p}\\left(n^{-1 / 2}\\right)\\) term. This means that they have distinct Edgeworth expansions. Consequently, unadjusted bootstrap methods will not achieve an asymptotic refinement.\nAn alternative suggested by Hall and Horowitz (1996) is to recenter the bootstrap 2SLS estimator so that it satisfies the correct orthogonality condition. Define\n\\[\n\\widehat{\\beta}_{2 \\text { sls }}^{* *}=\\left(\\boldsymbol{X}^{* \\prime} \\boldsymbol{Z}^{*}\\left(\\boldsymbol{Z}^{* \\prime} \\boldsymbol{Z}^{*}\\right)^{-1} \\boldsymbol{Z}^{* \\prime} \\boldsymbol{X}^{*}\\right)^{-1} \\boldsymbol{X}^{* \\prime} \\boldsymbol{Z}^{*}\\left(\\boldsymbol{Z}^{* \\prime} \\boldsymbol{Z}^{*}\\right)^{-1}\\left(\\boldsymbol{Z}^{* \\prime} \\boldsymbol{Y}_{1}^{*}-\\boldsymbol{Z}^{\\prime} \\widehat{\\boldsymbol{e}}\\right) .\n\\]\nWe can see that\n\\[\n\\begin{aligned}\n\\sqrt{n}\\left(\\widehat{\\beta}_{2 \\text { sls }}^{* *}-\\widehat{\\beta}_{2 \\mathrm{sls}}\\right) &=\\left(\\frac{1}{n} \\boldsymbol{X}^{* \\prime} \\boldsymbol{Z}^{*}\\left(\\frac{1}{n} \\boldsymbol{Z}^{* \\prime} \\boldsymbol{Z}^{*}\\right)^{-1} \\frac{1}{n} \\boldsymbol{Z}^{* \\prime} \\boldsymbol{X}^{*}\\right)^{-1} \\\\\n& \\times\\left(\\frac{1}{n} \\boldsymbol{X}^{* \\prime} \\boldsymbol{Z}^{*}\\right)\\left(\\frac{1}{n} \\boldsymbol{Z}^{* \\prime} \\boldsymbol{Z}^{*}\\right)^{-1}\\left(\\frac{1}{\\sqrt{n}} \\sum_{i=1}^{n}\\left(Z_{i}^{*} e_{i}^{*}-\\mathbb{E}^{*}\\left[Z^{*} e^{*}\\right]\\right)\\right)\n\\end{aligned}\n\\]\nwhich converges to the \\(\\mathrm{N}\\left(0, \\boldsymbol{V}_{\\beta}\\right)\\) distribution without special handling. Hall and Horowitz (1996) show that percentile-t methods applied to \\(\\widehat{\\beta}_{2 \\text { sls }}^{* *}\\) achieve an asymptotic refinement and are thus preferred to the unadjusted bootstrap estimator.\nThis recentered estimator, however, is not the standard implementation of the bootstrap for 2SLS as used in empirical practice."
  },
  {
    "objectID": "chpt12-iv.html#the-peril-of-bootstrap-2sls-standard-errors",
    "href": "chpt12-iv.html#the-peril-of-bootstrap-2sls-standard-errors",
    "title": "12  Instrumental Variables",
    "section": "12.24 The Peril of Bootstrap 2SLS Standard Errors",
    "text": "12.24 The Peril of Bootstrap 2SLS Standard Errors\nIt is tempting to use the bootstrap algorithm to estimate variance matrices and standard errors for the 2SLS estimator. In fact this is one of the most common uses of bootstrap methods in current econometric practice. Unfortunately this is an unjustified and ill-conceived idea and should not be done. In finite samples the 2SLS estimator may not have a finite second moment, meaning that bootstrap variance estimates are unstable and unreliable.\nTheorem \\(12.7\\) shows that under joint normality the 2SLS estimator will have a finite variance if and only if the number of overidentifying restrictions is two or larger. Thus for just-identified IV, and 2SLS with one degree of overidentification, the finite sample variance is infinite. The bootstrap will be attempting to estimate this value - infinity - and will yield nonsensical answers. When the observations are not jointly normal there is no finite sample theory (so it is possible that the finite sample variance is actually finite) but this is unknown and unverifiable. In overidentified settings when the number of overidentifying restrictions is two or larger the bootstrap can be applied for standard error estimation. However this is not the most common application of IV methods in econometric practice and thus should be viewed as the exception rather than the norm.\nTo understand what is going on consider the simplest case of a just-identified model with a single endogenous regressor and no included exogenous regressors. In this case the estimator can be written as a ratio of means\n\\[\n\\widehat{\\beta}_{\\mathrm{iv}}-\\beta=\\frac{\\sum_{i=1}^{n} Z_{i} e_{i}}{\\sum_{i=1}^{n} Z_{i} X_{i}} .\n\\]\nUnder joint normality of \\((e, X)\\) this has a Cauchy-like distribution which does not possess any finite integer moments. The trouble is that the denominator can be either positive or negative, and arbitrarily close to zero. This means that the ratio can take arbitrarily large values.\nTo illustrate let us return to the basic Card IV wage regression from column 2 of Table \\(12.1\\) which uses college as an instrument for education. We estimate this equation for the subsample of Black men which has \\(n=703\\) observations, and focus on the coefficient for the return to education. The coefficient estimate is reported in Table 12.3, along with asymptotic, jackknife, and two bootstrap standard errors each calculated with 10,000 bootstrap replications.\nTable 12.3: Instrumental Variable Return to Education for Black Men\n\n\n\nEstimate\n\\(0.11\\)\n\n\n\n\nAsymptotic s.e.\n\\((0.11)\\)\n\n\nJackknife s.e.\n\\((0.11)\\)\n\n\nBootstrap s.e. (standard)\n\\((1.42)\\)\n\n\nBootstrap s.e. (repeat)\n\\((4.79)\\)\n\n\n\nThe bootstrap standard errors are an order of magnitude larger than the asymptotic standard errors, and vary substantially across the bootstrap runs despite using 10,000 bootstrap replications. This indicates moment failure and unreliability of the bootstrap standard errors.\nThis is a strong message that bootstrap standard errors should not be computed for IV estimators. Instead, report percentile-type confidence intervals."
  },
  {
    "objectID": "chpt12-iv.html#clustered-dependence",
    "href": "chpt12-iv.html#clustered-dependence",
    "title": "12  Instrumental Variables",
    "section": "12.25 Clustered Dependence",
    "text": "12.25 Clustered Dependence\nIn Section \\(4.21\\) we introduced clustered dependence. We can also use the methods of clustered dependence for 2SLS estimation. Recall, the \\(g^{t h}\\) cluster has the observations \\(\\boldsymbol{Y}_{g}=\\left(Y_{1 g}, \\ldots, Y_{n_{g} g}\\right)^{\\prime}, \\boldsymbol{X}_{g}=\\) \\(\\left(X_{1 g}, \\ldots, X_{n_{g} g}\\right)^{\\prime}\\), and \\(Z_{g}=\\left(Z_{1 g}, \\ldots, Z_{n_{g} g}\\right)^{\\prime}\\). The structural equation for the \\(g^{t h}\\) cluster can be written as the matrix system \\(\\boldsymbol{Y}_{g}=\\boldsymbol{X}_{g} \\beta+\\boldsymbol{e}_{g}\\). Using this notation the centered 2SLS estimator can be written as\n\\[\n\\begin{aligned}\n\\widehat{\\beta}_{2 \\text { sls }}-\\beta &=\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{Z}\\left(\\boldsymbol{Z}^{\\prime} \\boldsymbol{Z}\\right)^{-1} \\boldsymbol{Z}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\boldsymbol{X}^{\\prime} \\boldsymbol{Z}\\left(\\boldsymbol{Z}^{\\prime} \\boldsymbol{Z}\\right)^{-1} \\boldsymbol{Z}^{\\prime} \\boldsymbol{e} \\\\\n&=\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{Z}\\left(\\boldsymbol{Z}^{\\prime} \\boldsymbol{Z}\\right)^{-1} \\boldsymbol{Z}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\boldsymbol{X}^{\\prime} \\boldsymbol{Z}\\left(\\boldsymbol{Z}^{\\prime} \\boldsymbol{Z}\\right)^{-1}\\left(\\sum_{g=1}^{G} \\boldsymbol{Z}_{g}^{\\prime} \\boldsymbol{e}_{g}\\right)\n\\end{aligned}\n\\]\nThe cluster-robust covariance matrix estimator for \\(\\widehat{\\beta}_{2 s l s}\\) thus takes the form\n\\[\n\\widehat{\\boldsymbol{V}}_{\\beta}=\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{Z}\\left(\\boldsymbol{Z}^{\\prime} \\boldsymbol{Z}\\right)^{-1} \\boldsymbol{Z}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\boldsymbol{X}^{\\prime} \\boldsymbol{Z}\\left(\\boldsymbol{Z}^{\\prime} \\boldsymbol{Z}\\right)^{-1} \\widehat{\\boldsymbol{S}}\\left(\\boldsymbol{Z}^{\\prime} \\boldsymbol{Z}\\right)^{-1} \\boldsymbol{Z}^{\\prime} \\boldsymbol{X}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{Z}\\left(\\boldsymbol{Z}^{\\prime} \\boldsymbol{Z}\\right)^{-1} \\boldsymbol{Z}^{\\prime} \\boldsymbol{X}\\right)^{-1}\n\\]\nwith\n\\[\n\\widehat{\\boldsymbol{S}}=\\sum_{g=1}^{G} \\boldsymbol{Z}_{g}^{\\prime} \\widehat{\\boldsymbol{e}}_{g} \\widehat{\\boldsymbol{e}}_{g}^{\\prime} \\boldsymbol{Z}_{g}\n\\]\nand the clustered residuals \\(\\widehat{\\boldsymbol{e}}_{g}=\\boldsymbol{Y}_{g}-\\boldsymbol{X}_{g} \\widehat{\\beta}_{2 \\text { sls }}\\)\nThe difference between the heteroskedasticity-robust estimator and the cluster-robust estimator is the covariance estimator \\(\\widehat{\\boldsymbol{S}}\\)."
  },
  {
    "objectID": "chpt12-iv.html#generated-regressors",
    "href": "chpt12-iv.html#generated-regressors",
    "title": "12  Instrumental Variables",
    "section": "12.26 Generated Regressors",
    "text": "12.26 Generated Regressors\nThe “two-stage” form of the 2SLS estimator is an example of what is called “estimation with generated regressors”. We say a regressor is a generated if it is an estimate of an idealized regressor or if it is a function of estimated parameters. Typically, a generated regressor \\(\\widehat{W}\\) is an estimate of an unobserved ideal regressor \\(W\\). As an estimate, \\(\\widehat{W}_{i}\\) is a function of the full sample not just observation \\(i\\). Hence it is not “i.i.d.” as it is dependent across observations which invalidates the conventional regression assumptions. Consequently, the sampling distribution of regression estimates is affected. Unless this is incorporated into our inference methods, covariance matrix estimates and standard errors will be incorrect.\nThe econometric theory of generated regressors was developed by Pagan (1984) for linear models and extended to nonlinear models and more general two-step estimators by Pagan (1986). Independently, similar results were obtained by Murphy and Topel (1985). Here we focus on the linear model:\n\\[\n\\begin{aligned}\nY &=W^{\\prime} \\beta+v \\\\\nW &=\\boldsymbol{A}^{\\prime} Z \\\\\n\\mathbb{E}[Z v] &=0 .\n\\end{aligned}\n\\]\nThe observables are \\((Y, Z)\\). We also have an estimate \\(\\widehat{\\boldsymbol{A}}\\) of \\(\\boldsymbol{A}\\).\nGiven \\(\\widehat{A}\\) we construct the estimate \\(\\widehat{W}_{i}=\\widehat{A}^{\\prime} Z_{i}\\) of \\(W_{i}\\), replace \\(W_{i}\\) in (12.48) with \\(\\widehat{W}_{i}\\), and then estimate \\(\\beta\\) by least squares, resulting in the estimator\n\\[\n\\widehat{\\beta}=\\left(\\sum_{i=1}^{n} \\widehat{W}_{i} \\widehat{W}_{i}^{\\prime}\\right)^{-1}\\left(\\sum_{i=1}^{n} \\widehat{W}_{i} Y_{i}\\right)\n\\]\nThe regressors \\(\\widehat{W}_{i}\\) are called generated regressors. The properties of \\(\\widehat{\\beta}\\) are different than least squares with i.i.d. observations because the generated regressors are themselves estimates.\nThis framework includes 2SLS as well as other common estimators. The 2SLS model can be written as (12.48) by looking at the reduced form equation (12.13), with \\(W=\\Gamma^{\\prime} Z, A=\\Gamma\\), and \\(\\widehat{A}=\\widehat{\\Gamma}\\).\nThe examples which motivated Pagan (1984) and Murphy and Topel (1985) emerged from the macroeconomics literature, in particular the work of Barro (1977) which examined the impact of inflation expectations and expectation errors on economic output. Let \\(\\pi\\) denote realized inflation and \\(Z\\) be variables available to economic agents. A model of inflation expectations sets \\(W=\\mathbb{E}[\\pi \\mid Z]=\\gamma^{\\prime} Z\\) and a model of expectation error sets \\(W=\\pi-\\mathbb{E}[\\pi \\mid Z]=\\pi-\\gamma^{\\prime} Z\\). Since expectations and errors are not observed they are replaced in applications with the fitted values \\(\\widehat{W}_{i}=\\widehat{\\gamma}^{\\prime} Z_{i}\\) and residuals \\(\\widehat{W}_{i}=\\pi_{i}-\\widehat{\\gamma}^{\\prime} Z_{i}\\) where \\(\\widehat{\\gamma}\\) is the coefficient from a regression of \\(\\pi\\) on \\(Z\\).\nThe generated regressor framework includes all of these examples.\nThe goal is to obtain a distributional approximation for \\(\\widehat{\\beta}\\) in order to construct standard errors, confidence intervals, and tests. Start by substituting equation (12.48) into (12.49). We obtain\n\\[\n\\widehat{\\beta}=\\left(\\sum_{i=1}^{n} \\widehat{W}_{i} \\widehat{W}_{i}^{\\prime}\\right)^{-1}\\left(\\sum_{i=1}^{n} \\widehat{W}_{i}\\left(W_{i}^{\\prime} \\beta+v_{i}\\right)\\right) .\n\\]\nNext, substitute \\(W_{i}^{\\prime} \\beta=\\widehat{W}_{i}^{\\prime} \\beta+\\left(W_{i}-\\widehat{W}_{i}\\right)^{\\prime} \\beta\\). We obtain\n\\[\n\\widehat{\\beta}-\\beta=\\left(\\sum_{i=1}^{n} \\widehat{W}_{i} \\widehat{W}_{i}^{\\prime}\\right)^{-1}\\left(\\sum_{i=1}^{n} \\widehat{W}_{i}\\left(\\left(W_{i}-\\widehat{W}_{i}\\right)^{\\prime} \\beta+v_{i}\\right)\\right) .\n\\]\nEffectively, this shows that the distribution of \\(\\widehat{\\beta}-\\beta\\) has two random components, one due to conventional regression component and the second due to the generated regressor. Conventional variance estimators do not address this second component and thus will be biased.\nInterestingly, the distribution in (12.50) dramatically simplifies in the special case that the “generated regressor term” \\(\\left(W_{i}-\\widehat{W}_{i}\\right)^{\\prime} \\beta\\) disappears. This occurs when the slope coefficients on the generated regressors are zero. To be specific, partition \\(W_{i}=\\left(W_{1 i}, W_{2 i}\\right), \\widehat{W}_{i}=\\left(W_{1 i}, \\widehat{W}_{2 i}\\right)\\), and \\(\\beta=\\left(\\beta_{1}, \\beta_{2}\\right)\\) so that \\(W_{1 i}\\) are the conventional observed regressors and \\(\\widehat{W}_{2 i}\\) are the generated regressors. Then \\(\\left(W_{i}-\\widehat{W}_{i}\\right)^{\\prime} \\beta=\\) \\(\\left(W_{2 i}-\\widehat{W}_{2 i}\\right)^{\\prime} \\beta_{2}\\). Thus if \\(\\beta_{2}=0\\) this term disappears. In this case (12.50) equals\n\\[\n\\widehat{\\beta}-\\widehat{\\beta}=\\left(\\sum_{i=1}^{n} \\widehat{W}_{i} \\widehat{W}_{i}^{\\prime}\\right)^{-1}\\left(\\sum_{i=1}^{n} \\widehat{W}_{i} v_{i}\\right) .\n\\]\nThis is a dramatic simplification.\nFurthermore, since \\(\\widehat{W}_{i}=\\widehat{A}^{\\prime} Z_{i}\\) we can write the estimator as a function of sample moments:\n\\[\n\\sqrt{n}(\\widehat{\\beta}-\\beta)=\\left(\\widehat{\\boldsymbol{A}}^{\\prime}\\left(\\frac{1}{n} \\sum_{i=1}^{n} Z_{i} Z_{i}^{\\prime}\\right) \\widehat{\\boldsymbol{A}}\\right)^{-1} \\widehat{\\boldsymbol{A}}^{\\prime}\\left(\\frac{1}{\\sqrt{n}} \\sum_{i=1}^{n} Z_{i} v_{i}\\right)\n\\]\nIf \\(\\widehat{\\boldsymbol{A}} \\underset{p}{\\longrightarrow} \\boldsymbol{A}\\) we find from standard manipulations that \\(\\sqrt{n}(\\widehat{\\beta}-\\beta) \\underset{d}{\\longrightarrow} \\mathrm{N}\\left(0, \\boldsymbol{V}_{\\beta}\\right)\\) where\n\\[\n\\boldsymbol{V}_{\\boldsymbol{\\beta}}=\\left(\\boldsymbol{A}^{\\prime} \\mathbb{E}\\left[Z Z^{\\prime}\\right] \\boldsymbol{A}\\right)^{-1}\\left(\\boldsymbol{A}^{\\prime} \\mathbb{E}\\left[Z Z^{\\prime} v^{2}\\right] \\boldsymbol{A}\\right)\\left(\\boldsymbol{A}^{\\prime} \\mathbb{E}\\left[Z Z^{\\prime}\\right] \\boldsymbol{A}\\right)^{-1} .\n\\]\nThe conventional asymptotic covariance matrix estimator for \\(\\widehat{\\beta}\\) takes the form\n\\[\n\\widehat{\\boldsymbol{V}}_{\\beta}=\\left(\\frac{1}{n} \\sum_{i=1}^{n} \\widehat{W}_{i} \\widehat{W}_{i}^{\\prime}\\right)^{-1}\\left(\\frac{1}{n} \\sum_{i=1}^{n} \\widehat{W}_{i} \\widehat{W}_{i}^{\\prime} \\widehat{v}_{i}^{2}\\right)\\left(\\frac{1}{n} \\sum_{i=1}^{n} \\widehat{W}_{i} \\widehat{W}_{i}^{\\prime}\\right)^{-1}\n\\]\nwhere \\(\\widehat{v}_{i}=Y_{i}-\\widehat{W}_{i}^{\\prime} \\widehat{\\beta}\\). Under the given assumptions, \\(\\widehat{\\boldsymbol{V}}_{\\beta} \\underset{p}{\\longrightarrow} \\boldsymbol{V}_{\\beta}\\). Thus inference using \\(\\widehat{\\boldsymbol{V}}_{\\beta}\\) is asymptotically valid. This is useful when we are interested in tests of \\(\\beta_{2}=0\\). Often this is of major interest in applications.\nTo test \\(\\mathbb{M}_{0}: \\beta_{2}=0\\) we partition \\(\\widehat{\\beta}=\\left(\\widehat{\\beta}_{1}, \\widehat{\\beta}_{2}\\right)\\) and construct a conventional Wald statistic\n\\[\nW=n \\widehat{\\beta}_{2}^{\\prime}\\left(\\left[\\widehat{\\boldsymbol{V}}_{\\beta}\\right]_{22}\\right)^{-1} \\widehat{\\beta}_{2} .\n\\]\nTheorem 12.9 Take model (12.48) with \\(\\mathbb{E}\\left[Y^{4}\\right]<\\infty, \\mathbb{E}\\|Z\\|^{4}<\\infty, A^{\\prime} \\mathbb{E}\\left[Z Z^{\\prime}\\right] \\boldsymbol{A}>\\) \\(0, \\widehat{\\boldsymbol{A}} \\underset{p}{\\longrightarrow} \\boldsymbol{A}\\), and \\(\\widehat{W}_{i}=\\left(W_{1 i}, \\widehat{W}_{2 i}\\right)\\). Under \\(\\mathbb{H}_{0}: \\beta_{2}=0\\), as \\(n \\rightarrow \\infty, \\sqrt{n}(\\widehat{\\beta}-\\beta) \\underset{d}{\\longrightarrow}\\) \\(\\mathrm{N}\\left(0, \\boldsymbol{V}_{\\beta}\\right)\\) where \\(\\boldsymbol{V}_{\\beta}\\) is given in (12.51). For \\(\\widehat{\\boldsymbol{V}}_{\\beta}\\) given in (12.52), \\(\\widehat{\\boldsymbol{V}}_{\\beta} \\underset{p}{\\longrightarrow} \\boldsymbol{V}_{\\beta}\\). Furthermore, \\(W \\underset{d}{\\longrightarrow} \\chi_{q}^{2}\\) where \\(q=\\operatorname{dim}\\left(\\beta_{2}\\right)\\). For \\(c\\) satisfying \\(\\alpha=1-G_{q}(c)\\), \\(\\mathbb{P}\\left[W>c \\mid \\mathbb{M}_{0}\\right] \\rightarrow \\alpha\\), so the test “Reject \\(\\mathbb{M}_{0}\\) if \\(W>c\\)” has asymptotic size \\(\\alpha\\). In the special case that \\(\\widehat{\\boldsymbol{A}}=\\boldsymbol{A}(\\boldsymbol{X}, \\boldsymbol{Z})\\) and \\(v \\mid X, Z \\sim \\mathrm{N}\\left(0, \\sigma^{2}\\right)\\) there is a finite sample version of the previous result. Let \\(W^{0}\\) be the Wald statistic constructed with a homoskedastic covariance matrix estimator, and let\n\\[\nF=W / q\n\\]\nbe the the \\(F\\) statistic, where \\(q=\\operatorname{dim}\\left(\\beta_{2}\\right)\\)\nTheorem 12.10 Take model (12.48) with \\(\\widehat{\\boldsymbol{A}}=\\boldsymbol{A}(\\boldsymbol{X}, \\boldsymbol{Z}), v \\mid X, Z \\sim \\mathrm{N}\\left(0, \\sigma^{2}\\right)\\) and \\(\\widehat{W}=\\left(W_{1}, \\widehat{W}_{2}\\right)\\). Under \\(\\mathbb{M}_{0}: \\beta_{2}=0\\), t-statistics have exact \\(\\mathrm{N}(0,1)\\) distributions, and the \\(F\\) statistic (12.53) has an exact \\(F_{q, n-k}\\) distribution where \\(q=\\operatorname{dim}\\left(\\beta_{2}\\right)\\) and \\(k=\\operatorname{dim}(\\beta)\\)\nTo summarize, in the model \\(Y=W_{1}^{\\prime} \\beta_{1}+W_{2}^{\\prime} \\beta_{2}+v\\) where \\(W_{2}\\) is not observed but replaced with an estimate \\(\\widehat{W}_{2}\\), conventional significance tests for \\(\\mathbb{M}_{0}: \\beta_{2}=0\\) are asymptotically valid without adjustment.\nWhile this theory allows tests of \\(\\mathbb{M}_{0}: \\beta_{2}=0\\) it unfortunately does not justify conventional standard errors or confidence intervals. For this, we need to work out the distribution without imposing the simplification \\(\\beta_{2}=0\\). This often needs to be worked out case-by-case or by using methods based on the generalized method of moments to be introduced in Chapter 13. However, in one important set of examples it is straightforward to work out the asymptotic distribution.\nFor the remainder of this section we examine the setting where the estimators \\(\\widehat{A}\\) take a least squares form so for some \\(\\boldsymbol{X}\\) can be written as \\(\\widehat{\\boldsymbol{A}}=\\left(\\boldsymbol{Z}^{\\prime} \\boldsymbol{Z}\\right)^{-1}\\left(\\boldsymbol{Z}^{\\prime} \\boldsymbol{X}\\right)\\). Such estimators correspond to the multivariate projection model\n\\[\n\\begin{aligned}\nX &=\\boldsymbol{A}^{\\prime} Z+u \\\\\n\\mathbb{E}\\left[Z u^{\\prime}\\right] &=0 .\n\\end{aligned}\n\\]\nThis class of estimators includes 2SLS and the expectation model described above. We can write the matrix of generated regressors as \\(\\widehat{W}=Z \\widehat{A}\\) and then (12.50) as\n\\[\n\\begin{aligned}\n\\widehat{\\beta}-\\beta &=\\left(\\widehat{\\boldsymbol{W}}^{\\prime} \\widehat{\\boldsymbol{W}}\\right)^{-1}\\left(\\widehat{\\boldsymbol{W}}^{\\prime}((\\boldsymbol{W}-\\widehat{\\boldsymbol{W}}) \\beta+\\boldsymbol{v})\\right) \\\\\n&=\\left(\\widehat{\\boldsymbol{A}}^{\\prime} \\boldsymbol{Z}^{\\prime} \\boldsymbol{Z} \\widehat{\\boldsymbol{A}}\\right)^{-1}\\left(\\widehat{\\boldsymbol{A}}^{\\prime} \\boldsymbol{Z}^{\\prime}\\left(-\\boldsymbol{Z}\\left(\\boldsymbol{Z}^{\\prime} \\boldsymbol{Z}\\right)^{-1}\\left(\\boldsymbol{Z}^{\\prime} \\boldsymbol{U}\\right) \\beta+\\boldsymbol{v}\\right)\\right) \\\\\n&=\\left(\\widehat{\\boldsymbol{A}}^{\\prime} \\boldsymbol{Z}^{\\prime} \\boldsymbol{Z} \\widehat{\\boldsymbol{A}}\\right)^{-1}\\left(\\widehat{\\boldsymbol{A}}^{\\prime} \\boldsymbol{Z}^{\\prime}(-\\boldsymbol{U} \\beta+\\boldsymbol{v})\\right) \\\\\n&=\\left(\\widehat{\\boldsymbol{A}}^{\\prime} \\boldsymbol{Z}^{\\prime} \\boldsymbol{Z} \\widehat{\\boldsymbol{A}}\\right)^{-1}\\left(\\widehat{\\boldsymbol{A}}^{\\prime} \\boldsymbol{Z}^{\\prime} \\boldsymbol{e}\\right)\n\\end{aligned}\n\\]\nwhere\n\\[\ne=v-u^{\\prime} \\beta=Y-X^{\\prime} \\beta .\n\\]\nThis estimator has the asymptotic distribution \\(\\sqrt{n}(\\widehat{\\beta}-\\beta) \\underset{d}{\\longrightarrow} \\mathrm{N}\\left(0, V_{\\beta}\\right)\\) where\n\\[\n\\boldsymbol{V}_{\\beta}=\\left(\\boldsymbol{A}^{\\prime} \\mathbb{E}\\left[Z Z^{\\prime}\\right] \\boldsymbol{A}\\right)^{-1}\\left(\\boldsymbol{A}^{\\prime} \\mathbb{E}\\left[Z Z^{\\prime} e^{2}\\right] \\boldsymbol{A}\\right)\\left(\\boldsymbol{A}^{\\prime} \\mathbb{E}\\left[Z Z^{\\prime}\\right] \\boldsymbol{A}\\right)^{-1} .\n\\]\nUnder conditional homoskedasticity the covariance matrix simplifies to\n\\[\n\\boldsymbol{V}_{\\boldsymbol{\\beta}}=\\left(\\boldsymbol{A}^{\\prime} \\mathbb{E}\\left[Z Z^{\\prime}\\right] \\boldsymbol{A}\\right)^{-1} \\mathbb{E}\\left[e^{2}\\right] .\n\\]\nAn appropriate estimator of \\(\\boldsymbol{V}_{\\beta}\\) is\n\\[\n\\begin{aligned}\n\\widehat{\\boldsymbol{V}}_{\\beta} &=\\left(\\frac{1}{n} \\widehat{\\boldsymbol{W}}^{\\prime} \\widehat{\\boldsymbol{W}}\\right)^{-1}\\left(\\frac{1}{n} \\sum_{i=1}^{n} \\widehat{W}_{i} \\widehat{W}_{i}^{\\prime} \\widehat{e}_{i}^{2}\\right)\\left(\\frac{1}{n} \\widehat{\\boldsymbol{W}}^{\\prime} \\widehat{\\boldsymbol{W}}\\right)^{-1} \\\\\n\\widehat{e}_{i} &=Y_{i}-X_{i}^{\\prime} \\widehat{\\beta}\n\\end{aligned}\n\\]\nUnder the assumption of conditional homoskedasticity this can be simplified as usual.\nThis appears to be the usual covariance matrix estimator, but it is not because the least squares residuals \\(\\widehat{v}_{i}=Y_{i}-\\widehat{W_{i}^{\\prime}} \\widehat{\\beta}\\) have been replaced with \\(\\widehat{e}_{i}\\). This is exactly the substitution made by the 2SLS covariance matrix formula. Indeed, the covariance matrix estimator \\(\\widehat{\\boldsymbol{V}}_{\\beta}\\) precisely equals (12.40).\nTheorem 12.11 Take model (12.48) and (12.54) with \\(\\mathbb{E}\\left[Y^{4}\\right]<\\infty, \\mathbb{E}\\|Z\\|^{4}<\\infty\\), \\(\\boldsymbol{A}^{\\prime} \\mathbb{E}\\left[Z Z^{\\prime}\\right] \\boldsymbol{A}>0\\), and \\(\\widehat{\\boldsymbol{A}}=\\left(\\boldsymbol{Z}^{\\prime} \\boldsymbol{Z}\\right)^{-1}\\left(\\boldsymbol{Z}^{\\prime} \\boldsymbol{X}\\right)\\). As \\(n \\rightarrow \\infty, \\sqrt{n}(\\widehat{\\beta}-\\beta) \\underset{d}{\\rightarrow} \\mathrm{N}\\left(0, \\boldsymbol{V}_{\\beta}\\right)\\) where \\(\\boldsymbol{V}_{\\beta}\\) is given in (12.56) with \\(e\\) defined in (12.55). For \\(\\widehat{\\boldsymbol{V}}_{\\beta}\\) given in (12.57), \\(\\widehat{\\boldsymbol{V}}_{\\beta} \\underset{p}{\\longrightarrow} \\boldsymbol{V}_{\\beta}\\).\nSince the parameter estimators are asymptotically normal and the covariance matrix is consistently estimated, standard errors and test statistics constructed from \\(\\widehat{\\boldsymbol{V}}_{\\beta}\\) are asymptotically valid with conventional interpretations.\nWe now summarize the results of this section. In general, care needs to be exercised when estimating models with generated regressors. As a general rule, generated regressors and two-step estimation affect sampling distributions and variance matrices. An important simplication occurs for tests that the generated regressors have zero slopes. In this case conventional tests have conventional distributions, both asymptotically and in finite samples. Another important special case occurs when the generated regressors are least squares fitted values. In this case the asymptotic distribution takes a conventional form but the conventional residual needs to be replaced by one constructed with the forecasted variable. With this one modification asymptotic inference using the generated regressors is conventional."
  },
  {
    "objectID": "chpt12-iv.html#regression-with-expectation-errors",
    "href": "chpt12-iv.html#regression-with-expectation-errors",
    "title": "12  Instrumental Variables",
    "section": "12.27 Regression with Expectation Errors",
    "text": "12.27 Regression with Expectation Errors\nIn this section we examine a generated regressor model which includes expectation errors in the regression. This is an important class of generated regressor models and is relatively straightforward to characterize. The model is\n\\[\n\\begin{aligned}\nY &=X^{\\prime} \\beta+u^{\\prime} \\alpha+v \\\\\nW &=\\boldsymbol{A}^{\\prime} Z \\\\\nX &=W+u \\\\\n\\mathbb{E}[Z v] &=0 \\\\\n\\mathbb{E}[u v] &=0 \\\\\n\\mathbb{E}\\left[Z u^{\\prime}\\right] &=0 .\n\\end{aligned}\n\\]\nThe observables are \\((Y, X, Z)\\). This model states that \\(W\\) is the expectation of \\(X\\) (or more generally, the projection of \\(X\\) on \\(Z\\) ) and \\(u\\) is its expectation error. The model allows for exogenous regressors as in the standard IV model if they are listed in \\(W, X\\), and \\(Z\\). This model is used, for example, to decompose the effect of expectations from expectation errors. In some cases it is desired to include only the expectation error \\(u\\), not the expectation \\(W\\). This does not change the results described here.\nThe model is estimated as follows. First, \\(\\boldsymbol{A}\\) is estimated by multivariate least squares of \\(X\\) on \\(Z\\), \\(\\widehat{\\boldsymbol{A}}=\\left(\\boldsymbol{Z}^{\\prime} \\boldsymbol{Z}\\right)^{-1}\\left(\\boldsymbol{Z}^{\\prime} \\boldsymbol{X}\\right)\\), which yields as by-products the fitted values \\(\\widehat{W}_{i}=\\widehat{\\boldsymbol{A}}^{\\prime} Z_{i}\\) and residuals \\(\\widehat{u}_{i}=\\widehat{X}_{i}-\\widehat{W}_{i}\\). Second, the coefficients are estimated by least squares of \\(Y\\) on the fitted values \\(\\widehat{W}\\) and residuals \\(\\widehat{u}\\)\n\\[\nY_{i}=\\widehat{W}_{i}^{\\prime} \\widehat{\\beta}+\\widehat{u}_{i}^{\\prime} \\widehat{\\alpha}+\\widehat{v}_{i} .\n\\]\nWe now examine the asymptotic distributions of these estimators.\nBy the first-step regression \\(\\boldsymbol{Z}^{\\prime} \\widehat{\\boldsymbol{U}}=0, \\widehat{\\boldsymbol{W}}^{\\prime} \\widehat{\\boldsymbol{U}}=0\\) and \\(\\boldsymbol{W}^{\\prime} \\widehat{\\boldsymbol{U}}=0\\). This means that \\(\\widehat{\\beta}\\) and \\(\\widehat{\\alpha}\\) can be computed separately. Notice that\n\\[\n\\widehat{\\beta}=\\left(\\widehat{\\boldsymbol{W}}^{\\prime} \\widehat{\\boldsymbol{W}}\\right)^{-1} \\widehat{\\boldsymbol{W}}^{\\prime} \\boldsymbol{Y}\n\\]\nand\n\\[\n\\boldsymbol{Y}=\\widehat{\\boldsymbol{W}} \\beta+\\boldsymbol{U} \\alpha+(\\boldsymbol{W}-\\widehat{\\boldsymbol{W}}) \\beta+\\boldsymbol{v} .\n\\]\nSubstituting, using \\(\\widehat{\\boldsymbol{W}}^{\\prime} \\widehat{\\boldsymbol{U}}=0\\) and \\(\\boldsymbol{W}-\\widehat{\\boldsymbol{W}}=-\\boldsymbol{Z}\\left(\\boldsymbol{Z}^{\\prime} \\boldsymbol{Z}\\right)^{-1} \\boldsymbol{Z}^{\\prime} \\boldsymbol{U}\\) we find\n\\[\n\\begin{aligned}\n\\widehat{\\beta}-\\beta &=\\left(\\widehat{\\boldsymbol{W}}^{\\prime} \\widehat{\\boldsymbol{W}}\\right)^{-1} \\widehat{\\boldsymbol{W}}^{\\prime}(\\boldsymbol{U} \\alpha+(\\boldsymbol{W}-\\widehat{\\boldsymbol{W}}) \\beta+\\boldsymbol{v}) \\\\\n&=\\left(\\widehat{\\boldsymbol{A}}^{\\prime} \\boldsymbol{Z}^{\\prime} \\boldsymbol{Z} \\widehat{\\boldsymbol{A}}\\right)^{-1} \\widehat{\\boldsymbol{A}}^{\\prime} \\boldsymbol{Z}^{\\prime}(\\boldsymbol{U} \\alpha-\\boldsymbol{U} \\beta+\\boldsymbol{v}) \\\\\n&=\\left(\\widehat{\\boldsymbol{A}}^{\\prime} \\boldsymbol{Z}^{\\prime} \\boldsymbol{Z} \\widehat{\\boldsymbol{A}}\\right)^{-1} \\widehat{\\boldsymbol{A}}^{\\prime} \\boldsymbol{Z}^{\\prime} \\boldsymbol{e}\n\\end{aligned}\n\\]\nwhere\n\\[\ne_{i}=v_{i}+u_{i}^{\\prime}(\\alpha-\\beta)=Y_{i}-X_{i}^{\\prime} \\beta .\n\\]\nWe also find\n\\[\n\\widehat{\\alpha}=\\left(\\widehat{\\boldsymbol{U}}^{\\prime} \\widehat{\\boldsymbol{U}}\\right)^{-1} \\widehat{\\boldsymbol{U}}^{\\prime} \\boldsymbol{Y} .\n\\]\nSince \\(\\widehat{\\boldsymbol{U}}^{\\prime} \\boldsymbol{W}=0, \\boldsymbol{U}-\\widehat{\\boldsymbol{U}}=\\boldsymbol{Z}\\left(\\boldsymbol{Z}^{\\prime} \\boldsymbol{Z}\\right)^{-1} \\boldsymbol{Z}^{\\prime} \\boldsymbol{U}\\) and \\(\\widehat{\\boldsymbol{U}}^{\\prime} \\boldsymbol{Z}=0\\) then\n\\[\n\\begin{aligned}\n\\widehat{\\alpha}-\\alpha &=\\left(\\widehat{\\boldsymbol{U}}^{\\prime} \\widehat{\\boldsymbol{U}}\\right)^{-1} \\widehat{\\boldsymbol{U}}^{\\prime}(\\boldsymbol{W} \\beta+(\\boldsymbol{U}-\\widehat{\\boldsymbol{U}}) \\alpha+\\boldsymbol{v}) \\\\\n&=\\left(\\widehat{\\boldsymbol{U}}^{\\prime} \\widehat{\\boldsymbol{U}}\\right)^{-1} \\widehat{\\boldsymbol{U}}^{\\prime} \\boldsymbol{v}\n\\end{aligned}\n\\]\nTogether, we establish the following distributional result. Theorem 12.12 For the model and estimators described in this section, with \\(\\mathbb{E}\\left[Y^{4}\\right]<\\infty, \\mathbb{E}\\|Z\\|^{4}<\\infty, \\mathbb{E}\\|X\\|^{4}<\\infty, A^{\\prime} \\mathbb{E}\\left[Z Z^{\\prime}\\right] A>0\\), and \\(\\mathbb{E}\\left[u u^{\\prime}\\right]>0\\), as \\(n \\rightarrow \\infty\\)\n\\[\n\\sqrt{n}\\left(\\begin{array}{c}\n\\widehat{\\beta}-\\beta \\\\\n\\widehat{\\alpha}-\\alpha\n\\end{array}\\right) \\underset{d}{\\longrightarrow} \\mathrm{N}(0, \\boldsymbol{V})\n\\]\nwhere\n\\[\n\\boldsymbol{V}=\\left(\\begin{array}{ll}\n\\boldsymbol{V}_{\\beta \\beta} & \\boldsymbol{V}_{\\beta \\alpha} \\\\\n\\boldsymbol{V}_{\\alpha \\beta} & \\boldsymbol{V}_{\\alpha \\alpha}\n\\end{array}\\right)\n\\]\nand\n\\[\n\\begin{aligned}\n\\boldsymbol{V}_{\\beta \\beta} &=\\left(\\boldsymbol{A}^{\\prime} \\mathbb{E}\\left[Z Z^{\\prime}\\right] \\boldsymbol{A}\\right)^{-1}\\left(\\boldsymbol{A}^{\\prime} \\mathbb{E}\\left[Z Z^{\\prime} e^{2}\\right] \\boldsymbol{A}\\right)\\left(\\boldsymbol{A}^{\\prime} \\mathbb{E}\\left[Z Z^{\\prime}\\right] \\boldsymbol{A}\\right)^{-1} \\\\\n\\boldsymbol{V}_{\\alpha \\beta} &=\\left(\\mathbb{E}\\left[u u^{\\prime}\\right]\\right)^{-1}\\left(\\mathbb{E}\\left[u Z^{\\prime} e v\\right] \\boldsymbol{A}\\right)\\left(\\boldsymbol{A}^{\\prime} \\mathbb{E}\\left[Z Z^{\\prime}\\right] \\boldsymbol{A}\\right)^{-1} \\\\\n\\boldsymbol{V}_{\\alpha \\alpha} &=\\left(\\mathbb{E}\\left[u u^{\\prime}\\right]\\right)^{-1} \\mathbb{E}\\left[u u^{\\prime} v^{2}\\right]\\left(\\mathbb{E}\\left[u u^{\\prime}\\right]\\right)^{-1}\n\\end{aligned}\n\\]\nThe asymptotic covariance matrix is estimated by\n\\[\n\\begin{aligned}\n&\\widehat{\\boldsymbol{V}}_{\\beta \\beta}=\\left(\\frac{1}{n} \\widehat{\\boldsymbol{W}}^{\\prime} \\widehat{\\boldsymbol{W}}\\right)^{-1}\\left(\\frac{1}{n} \\sum_{i=1}^{n} \\widehat{W}_{i} \\widehat{W}_{i}^{\\prime} \\widehat{e}_{i}^{2}\\right)\\left(\\frac{1}{n} \\widehat{\\boldsymbol{W}}^{\\prime} \\widehat{\\boldsymbol{W}}\\right)^{-1} \\\\\n&\\widehat{\\boldsymbol{V}}_{\\alpha \\beta}=\\left(\\frac{1}{n} \\widehat{\\boldsymbol{U}}^{\\prime} \\widehat{\\boldsymbol{U}}\\right)^{-1}\\left(\\frac{1}{n} \\sum_{i=1}^{n} \\widehat{u}_{i} \\widehat{W}_{i}^{\\prime} \\widehat{e}_{i} \\widehat{v}_{i}\\right)\\left(\\frac{1}{n} \\widehat{\\boldsymbol{W}}^{\\prime} \\widehat{\\boldsymbol{W}}\\right)^{-1} \\\\\n&\\widehat{\\boldsymbol{V}}_{\\alpha \\alpha}=\\left(\\frac{1}{n} \\widehat{\\boldsymbol{U}}^{\\prime} \\widehat{\\boldsymbol{U}}\\right)^{-1}\\left(\\frac{1}{n} \\sum_{i=1}^{n} \\widehat{U}_{i} \\widehat{U}_{i}^{\\prime} \\widehat{v}_{i}^{2}\\right)\\left(\\frac{1}{n} \\widehat{\\boldsymbol{U}}^{\\prime} \\widehat{\\boldsymbol{U}}\\right)^{-1}\n\\end{aligned}\n\\]\nwhere\n\\[\n\\begin{aligned}\n\\widehat{W}_{i} &=\\widehat{A}^{\\prime} Z_{i} \\\\\n\\widehat{u}_{i} &=\\widehat{X}_{i}-\\widehat{W}_{i} \\\\\n\\widehat{e}_{i} &=Y_{i}-X_{i}^{\\prime} \\widehat{\\beta} \\\\\n\\widehat{v}_{i} &=Y_{i}-\\widehat{W}_{i}^{\\prime} \\widehat{\\beta}-\\widehat{u}_{i}^{\\prime} \\widehat{\\alpha} .\n\\end{aligned}\n\\]\nUnder conditional homoskedasticity, specifically\n\\[\n\\mathbb{E}\\left[\\left(\\begin{array}{cc}\ne_{i}^{2} & e_{i} v_{i} \\\\\ne_{i} v_{i} & v_{i}^{2}\n\\end{array}\\right) \\mid Z_{i}\\right]=\\boldsymbol{C}\n\\]\nthen \\(\\boldsymbol{V}_{\\alpha \\beta}=0\\) and the coefficient estimates \\(\\widehat{\\beta}\\) and \\(\\widehat{\\alpha}\\) are asymptotically independent. The variance components also simplify to\n\\[\n\\begin{aligned}\n\\boldsymbol{V}_{\\beta \\beta} &=\\left(\\boldsymbol{A}^{\\prime} \\mathbb{E}\\left[Z Z^{\\prime}\\right] \\boldsymbol{A}\\right)^{-1} \\mathbb{E}\\left[e_{i}^{2}\\right] \\\\\n\\boldsymbol{V}_{\\alpha \\alpha} &=\\left(\\mathbb{E}\\left[u u^{\\prime}\\right]\\right)^{-1} \\mathbb{E}\\left[v^{2}\\right] .\n\\end{aligned}\n\\]\nIn this case we have the covariance matrix estimators\n\\[\n\\begin{aligned}\n&\\widehat{\\boldsymbol{V}}_{\\beta \\beta}^{0}=\\left(\\frac{1}{n} \\widehat{\\boldsymbol{W}}^{\\prime} \\widehat{\\boldsymbol{W}}\\right)^{-1}\\left(\\frac{1}{n} \\sum_{i=1}^{n} \\widehat{e}_{i}^{2}\\right) \\\\\n&\\widehat{\\boldsymbol{V}}_{\\alpha \\alpha}^{0}=\\left(\\frac{1}{n} \\widehat{\\boldsymbol{U}}^{\\prime} \\widehat{\\boldsymbol{U}}\\right)^{-1}\\left(\\frac{1}{n} \\sum_{i=1}^{n} \\widehat{v}_{i}^{2}\\right)\n\\end{aligned}\n\\]\nand \\(\\widehat{\\boldsymbol{V}}_{\\alpha \\beta}^{0}=0\\)"
  },
  {
    "objectID": "chpt12-iv.html#control-function-regression",
    "href": "chpt12-iv.html#control-function-regression",
    "title": "12  Instrumental Variables",
    "section": "12.28 Control Function Regression",
    "text": "12.28 Control Function Regression\nIn this section we present an alternative way of computing the 2SLS estimator by least squares. It is useful in nonlinear contexts, and also in the linear model to construct tests for endogeneity.\nThe structural and reduced form equations for the standard IV model are\n\\[\n\\begin{aligned}\nY &=X_{1}^{\\prime} \\beta_{1}+X_{2}^{\\prime} \\beta_{2}+e \\\\\nX_{2} &=\\Gamma_{12}^{\\prime} Z_{1}+\\Gamma_{22}^{\\prime} Z_{2}+u_{2} .\n\\end{aligned}\n\\]\nSince the instrumental variable assumption specifies that \\(\\mathbb{E}[Z e]=0, X_{2}\\) is endogenous (correlated with \\(e)\\) if \\(u_{2}\\) and \\(e\\) are correlated. We can therefore consider the linear projection of \\(e\\) on \\(u_{2}\\)\n\\[\n\\begin{aligned}\ne &=u_{2}^{\\prime} \\alpha+v \\\\\n\\alpha &=\\left(\\mathbb{E}\\left[u_{2} u_{2}^{\\prime}\\right]\\right)^{-1} \\mathbb{E}\\left[u_{2} e\\right] \\\\\n\\mathbb{E}\\left[u_{2} v\\right] &=0 .\n\\end{aligned}\n\\]\nSubstituting this into the structural form equation we find\n\\[\n\\begin{aligned}\nY &=X_{1}^{\\prime} \\beta_{1}+X_{2}^{\\prime} \\beta_{2}+u_{2}^{\\prime} \\alpha+v \\\\\n\\mathbb{E}\\left[X_{1} v\\right] &=0 \\\\\n\\mathbb{E}\\left[X_{2} v\\right] &=0 \\\\\n\\mathbb{E}\\left[u_{2} v\\right] &=0 .\n\\end{aligned}\n\\]\nNotice that \\(X_{2}\\) is uncorrelated with \\(v\\). This is because \\(X_{2}\\) is correlated with \\(e\\) only through \\(u_{2}\\), and \\(v\\) is the error after \\(e\\) has been projected orthogonal to \\(u_{2}\\).\nIf \\(u_{2}\\) were observed we could then estimate (12.59) by least squares. Since it is not observed we estimate it by the reduced-form residual \\(\\widehat{u}_{2 i}=X_{2 i}-\\widehat{\\Gamma}_{12}^{\\prime} Z_{1 i}-\\widehat{\\Gamma}_{22}^{\\prime} Z_{2 i}\\). Then the coefficients \\(\\left(\\beta_{1}, \\beta_{2}, \\alpha\\right)\\) can be estimated by least squares of \\(Y\\) on \\(\\left(X_{1}, X_{2}, \\widehat{u}_{2}\\right)\\). We can write this as\n\\[\nY_{i}=X_{i}^{\\prime} \\widehat{\\beta}+\\widehat{u}_{2 i}^{\\prime} \\widehat{\\alpha}+\\widehat{v}_{i}\n\\]\nor in matrix notation as\n\\[\n\\boldsymbol{Y}=\\boldsymbol{X} \\widehat{\\beta}+\\widehat{\\boldsymbol{U}}_{2} \\widehat{\\alpha}+\\widehat{\\boldsymbol{v}} .\n\\]\nThis turns out to be an alternative algebraic expression for the 2SLS estimator.\nIndeed, we now show that \\(\\widehat{\\beta}=\\widehat{\\beta}_{2 s l s}\\). First, note that the reduced form residual can be written as\n\\[\n\\widehat{\\boldsymbol{U}}_{2}=\\left(\\boldsymbol{I}_{n}-\\boldsymbol{P}_{\\boldsymbol{Z}}\\right) \\boldsymbol{X}_{2}\n\\]\nwhere \\(\\boldsymbol{P}_{\\boldsymbol{Z}}\\) is defined in (12.30). By the FWL representation\n\\[\n\\widehat{\\beta}=\\left(\\widetilde{\\boldsymbol{X}}^{\\prime} \\widetilde{\\boldsymbol{X}}\\right)^{-1}\\left(\\widetilde{\\boldsymbol{X}}^{\\prime} \\boldsymbol{Y}\\right)\n\\]\nwhere \\(\\widetilde{\\boldsymbol{X}}=\\left[\\widetilde{\\boldsymbol{X}}_{1}, \\widetilde{\\boldsymbol{X}}_{2}\\right]\\) with\n\\[\n\\widetilde{\\boldsymbol{X}}_{1}=\\boldsymbol{X}_{1}-\\widehat{\\boldsymbol{U}}_{2}\\left(\\widehat{\\boldsymbol{U}}_{2}^{\\prime} \\widehat{\\boldsymbol{U}}_{2}\\right)^{-1} \\widehat{\\boldsymbol{U}}_{2}^{\\prime} \\boldsymbol{X}_{1}=\\boldsymbol{X}_{1}\n\\]\n(since \\(\\left.\\widehat{\\boldsymbol{U}}_{2}^{\\prime} \\boldsymbol{X}_{1}=0\\right)\\) and\n\\[\n\\begin{aligned}\n\\widetilde{\\boldsymbol{X}}_{2} &=\\boldsymbol{X}_{2}-\\widehat{\\boldsymbol{U}}_{2}\\left(\\widehat{\\boldsymbol{U}}_{2}^{\\prime} \\widehat{\\boldsymbol{U}}_{2}\\right)^{-1} \\widehat{\\boldsymbol{U}}_{2}^{\\prime} \\boldsymbol{X}_{2} \\\\\n&=\\boldsymbol{X}_{2}-\\widehat{\\boldsymbol{U}}_{2}\\left(\\boldsymbol{X}_{2}^{\\prime}\\left(\\boldsymbol{I}_{n}-\\boldsymbol{P}_{\\boldsymbol{Z}}\\right) \\boldsymbol{X}_{2}\\right)^{-1} \\boldsymbol{X}_{2}^{\\prime}\\left(\\boldsymbol{I}_{n}-\\boldsymbol{P}_{\\boldsymbol{Z}}\\right) \\boldsymbol{X}_{2} \\\\\n&=\\boldsymbol{X}_{2}-\\widehat{\\boldsymbol{U}}_{2} \\\\\n&=\\boldsymbol{P}_{\\boldsymbol{Z}} \\boldsymbol{X}_{2} .\n\\end{aligned}\n\\]\nThus \\(\\tilde{\\boldsymbol{X}}=\\left[\\boldsymbol{X}_{1}, \\boldsymbol{P}_{\\boldsymbol{Z}} \\boldsymbol{X}_{2}\\right]=\\boldsymbol{P}_{\\boldsymbol{Z}} \\boldsymbol{X}\\). Substituted into (12.61) we find\n\\[\n\\widehat{\\beta}=\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{P}_{\\boldsymbol{Z}} \\boldsymbol{X}\\right)^{-1}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{P}_{\\boldsymbol{Z}} \\boldsymbol{Y}\\right)=\\widehat{\\beta}_{2 \\text { sls }}\n\\]\nwhich is (12.31) as claimed.\nAgain, what we have found is that OLS estimation of equation (12.60) yields algebraically the 2SLS estimator \\(\\widehat{\\beta}_{2 \\text { sls }}\\).\nWe now consider the distribution of the control function estimator \\((\\widehat{\\beta}, \\widehat{\\alpha})\\). It is a generated regression model, and in fact is covered by the model examined in Section \\(12.27\\) after a slight reparametrization. Let \\(W=\\bar{\\Gamma}^{\\prime} Z\\). Note \\(u=X-W\\). Then the main equation (12.59) can be written as \\(Y=W^{\\prime} \\beta+u_{2}^{\\prime} \\gamma+v\\) where \\(\\gamma=\\alpha+\\beta_{2}\\). This is the model in Section 12.27.\nSet \\(\\widehat{\\gamma}=\\widehat{\\alpha}+\\widehat{\\beta}_{2}\\). It follows from (12.58) that as \\(n \\rightarrow \\infty\\) we have the joint distribution\n\\[\n\\sqrt{n}\\left(\\begin{array}{c}\n\\widehat{\\beta}_{2}-\\beta_{2} \\\\\n\\widehat{\\gamma}-\\gamma\n\\end{array}\\right) \\vec{d} \\mathrm{~N}(0, \\boldsymbol{V})\n\\]\nwhere\n\\[\n\\begin{aligned}\n\\boldsymbol{V}=\\left(\\begin{array}{ll}\n\\boldsymbol{V}_{22} & \\boldsymbol{V}_{2 \\gamma} \\\\\n\\boldsymbol{V}_{\\gamma 2} & \\boldsymbol{V}_{\\gamma \\gamma}\n\\end{array}\\right) \\\\\n\\boldsymbol{V}_{22} &=\\left[\\left(\\bar{\\Gamma}^{\\prime} \\mathbb{E}\\left[Z Z^{\\prime}\\right] \\bar{\\Gamma}\\right)^{-1} \\bar{\\Gamma}^{\\prime} \\mathbb{E}\\left[Z Z^{\\prime} e^{2}\\right] \\bar{\\Gamma}\\left(\\bar{\\Gamma}^{\\prime} \\mathbb{E}\\left[Z Z^{\\prime}\\right] \\bar{\\Gamma}\\right)^{-1}\\right]_{22} \\\\\n\\boldsymbol{V}_{\\gamma 2} &=\\left[\\left(\\mathbb{E}\\left[u_{2} u_{2}^{\\prime}\\right]\\right)^{-1} \\mathbb{E}\\left[u Z^{\\prime} e v\\right] \\bar{\\Gamma}\\left(\\bar{\\Gamma}^{\\prime} \\mathbb{E}\\left[Z Z^{\\prime}\\right] \\bar{\\Gamma}\\right)^{-1}\\right]_{\\cdot 2} \\\\\n\\boldsymbol{V}_{\\gamma \\gamma} &=\\left(\\mathbb{E}\\left[u_{2} u_{2}^{\\prime}\\right]\\right)^{-1} \\mathbb{E}\\left[u_{2} u_{2}^{\\prime} v^{2}\\right]\\left(\\mathbb{E}\\left[u_{2} u_{2}^{\\prime}\\right]\\right)^{-1} \\\\\ne &=Y-X^{\\prime} \\beta .\n\\end{aligned}\n\\]\nThe asymptotic distribution of \\(\\widehat{\\gamma}=\\widehat{\\alpha}-\\widehat{\\beta}_{2}\\) can be deduced.\nTheorem 12.13 If \\(\\mathbb{E}\\left[Y^{4}\\right]<\\infty, \\mathbb{E}\\|Z\\|^{4}<\\infty, \\mathbb{E}\\|X\\|^{4}<\\infty, A^{\\prime} \\mathbb{E}\\left[Z Z^{\\prime}\\right] A>0\\), and \\(\\mathbb{E}\\left[u u^{\\prime}\\right]>0\\), as \\(n \\rightarrow \\infty\\)\n\\[\n\\sqrt{n}(\\widehat{\\alpha}-\\alpha) \\underset{d}{\\longrightarrow} \\mathrm{N}\\left(0, \\boldsymbol{V}_{\\alpha}\\right)\n\\]\nwhere\n\\[\n\\boldsymbol{V}_{\\alpha}=\\boldsymbol{V}_{22}+\\boldsymbol{V}_{\\gamma \\gamma}-\\boldsymbol{V}_{\\gamma 2}-\\boldsymbol{V}_{\\gamma 2}^{\\prime} .\n\\]\nUnder conditional homoskedasticity we have the important simplifications\n\\[\n\\begin{aligned}\n\\boldsymbol{V}_{22} &=\\left[\\left(\\bar{\\Gamma}^{\\prime} \\mathbb{E}\\left[Z Z^{\\prime}\\right] \\bar{\\Gamma}\\right)^{-1}\\right]_{22} \\mathbb{E}\\left[e^{2}\\right] \\\\\n\\boldsymbol{V}_{\\gamma \\gamma} &=\\left(\\mathbb{E}\\left[u_{2} u_{2}^{\\prime}\\right]\\right)^{-1} \\mathbb{E}\\left[v^{2}\\right] \\\\\n\\boldsymbol{V}_{\\gamma 2} &=0 \\\\\n\\boldsymbol{V}_{\\alpha} &=\\boldsymbol{V}_{22}+\\boldsymbol{V}_{\\gamma \\gamma} .\n\\end{aligned}\n\\]\nAn estimator for \\(\\boldsymbol{V}_{\\alpha}\\) in the general case is\n\\[\n\\widehat{\\boldsymbol{V}}_{\\alpha}=\\widehat{\\boldsymbol{V}}_{22}+\\widehat{\\boldsymbol{V}}_{\\gamma \\gamma}-\\widehat{\\boldsymbol{V}}_{\\gamma 2}-\\widehat{\\boldsymbol{V}}_{\\gamma 2}^{\\prime}\n\\]\nwhere\n\\[\n\\begin{aligned}\n\\widehat{\\boldsymbol{V}}_{22} &=\\left[\\frac{1}{n}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{P}_{\\boldsymbol{Z}} \\boldsymbol{X}\\right)^{-1} \\boldsymbol{X}^{\\prime} \\boldsymbol{Z}\\left(\\boldsymbol{Z}^{\\prime} \\boldsymbol{Z}\\right)^{-1}\\left(\\sum_{i=1}^{n} Z_{i} Z_{i}^{\\prime} \\widehat{e}_{i}^{2}\\right)\\left(\\boldsymbol{Z}^{\\prime} \\boldsymbol{Z}\\right)^{-1} \\boldsymbol{Z}^{\\prime} \\boldsymbol{X}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{P}_{\\boldsymbol{Z}} \\boldsymbol{X}\\right)^{-1}\\right]_{22} \\\\\n\\widehat{\\boldsymbol{V}}_{\\gamma 2} &=\\left[\\frac{1}{n}\\left(\\widehat{\\boldsymbol{U}}^{\\prime} \\widehat{\\boldsymbol{U}}\\right)^{-1}\\left(\\sum_{i=1}^{n} \\widehat{u}_{i} \\widehat{W}_{i}^{\\prime} \\widehat{e}_{i} \\widehat{v}_{i}\\right)\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{P}_{\\boldsymbol{Z}} \\boldsymbol{X}\\right)^{-1}\\right]_{-2} \\\\\n\\widehat{e}_{i} &=Y_{i}-X_{i}^{\\prime} \\widehat{\\beta} \\\\\n\\widehat{v}_{i} &=Y_{i}-X_{i}^{\\prime} \\widehat{\\beta}-\\widehat{u}_{2 i}^{\\prime} \\widehat{\\gamma}\n\\end{aligned}\n\\]\nUnder the assumption of conditional homoskedasticity we have the estimator\n\\[\n\\begin{aligned}\n\\widehat{\\boldsymbol{V}}_{\\alpha}^{0} &=\\widehat{\\boldsymbol{V}}_{\\beta \\beta}^{0}+\\widehat{\\boldsymbol{V}}_{\\gamma \\gamma}^{0} \\\\\n\\widehat{\\boldsymbol{V}}_{\\beta \\beta} &=\\left[\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{P}_{\\boldsymbol{Z}} \\boldsymbol{X}\\right)^{-1}\\right]_{22}\\left(\\sum_{i=1}^{n} \\widehat{e}_{i}^{2}\\right) \\\\\n\\widehat{\\boldsymbol{V}}_{\\gamma \\gamma} &=\\left(\\widehat{\\boldsymbol{U}}^{\\prime} \\widehat{\\boldsymbol{U}}\\right)^{-1}\\left(\\sum_{i=1}^{n} \\widehat{v}_{i}^{2}\\right) .\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "chpt12-iv.html#endogeneity-tests",
    "href": "chpt12-iv.html#endogeneity-tests",
    "title": "12  Instrumental Variables",
    "section": "12.29 Endogeneity Tests",
    "text": "12.29 Endogeneity Tests\nThe 2SLS estimator allows the regressor \\(X_{2}\\) to be endogenous, meaning that \\(X_{2}\\) is correlated with the structural error \\(e\\). If this correlation is zero then \\(X_{2}\\) is exogenous and the structural equation can be estimated by least squares. This is a testable restriction. Effectively, the null hypothesis is\n\\[\n\\mathbb{H}_{0}: \\mathbb{E}\\left[X_{2} e\\right]=0\n\\]\nwith the alternative\n\\[\n\\mathbb{M}_{1}: \\mathbb{E}\\left[X_{2} e\\right] \\neq 0 .\n\\]\nThe maintained hypothesis is \\(\\mathbb{E}[Z e]=0\\). Since \\(X_{1}\\) is a component of \\(Z\\) this implies \\(\\mathbb{E}\\left[X_{1} e\\right]=0\\). Consequently we could alternatively write the null as \\(\\mathbb{H}_{0}: \\mathbb{E}[X e]=0\\) (and some authors do so).\nRecall the control function regression (12.59)\n\\[\n\\begin{aligned}\n&Y=X_{1}^{\\prime} \\beta_{1}+X_{2}^{\\prime} \\beta_{2}+u_{2}^{\\prime} \\alpha+v \\\\\n&\\alpha=\\left(\\mathbb{E}\\left[u_{2} u_{2}^{\\prime}\\right]\\right)^{-1} \\mathbb{E}\\left[u_{2} e\\right]\n\\end{aligned}\n\\]\nNotice that \\(\\mathbb{E}\\left[X_{2} e\\right]=0\\) if and only if \\(\\mathbb{E}\\left[u_{2} e\\right]=0\\), so the hypothesis can be restated as \\(\\mathbb{H}_{0}: \\alpha=0\\) against \\(\\mathbb{H}_{1}: \\alpha \\neq 0\\). Thus a natural test is based on the Wald statistic \\(W\\) for \\(\\alpha=0\\) in the control function regression (12.28). Under Theorem 12.9, Theorem \\(12.10\\), and \\(\\mathbb{M}_{0}, W\\) is asymptotically chi-square with \\(k_{2}\\) degrees of freedom. In addition, under the normal regression assumption the \\(F\\) statistic has an exact \\(F\\left(k_{2}, n-\\right.\\) \\(k_{1}-2 k_{2}\\) ) distribution. We accept the null hypothesis that \\(X_{2}\\) is exogenous if \\(W\\) (or F) is smaller than the critical value, and reject in favor of the hypothesis that \\(X_{2}\\) is endogenous if the statistic is larger than the critical value.\nSpecifically, estimate the reduced form by least squares\n\\[\nX_{2 i}=\\widehat{\\Gamma}_{12}^{\\prime} Z_{1 i}+\\widehat{\\Gamma}_{22}^{\\prime} Z_{2 i}+\\widehat{u}_{2 i}\n\\]\nto obtain the residuals. Then estimate the control function by least squares\n\\[\nY_{i}=X_{i}^{\\prime} \\widehat{\\beta}+\\widehat{u}_{2 i}^{\\prime} \\widehat{\\alpha}+\\widehat{v}_{i} .\n\\]\nLet \\(W, W^{0}\\) and \\(F=W^{0} / k_{2}\\) denote the Wald, homoskedastic Wald, and \\(F\\) statistics for \\(\\alpha=0\\).\nTheorem 12.14 Under \\(\\mathbb{M}_{0}, W \\underset{d}{\\longrightarrow} \\chi_{k_{2}}^{2}\\). Let \\(c_{1-\\alpha}\\) solve \\(\\mathbb{P}\\left[\\chi_{k_{2}}^{2} \\leq c_{1-\\alpha}\\right]=1-\\alpha\\). The test “Reject \\(\\mathbb{M}_{0}\\) if \\(W>c_{1-\\alpha}\\)” has asymptotic size \\(\\alpha\\).\nTheorem 12.15 Suppose \\(e \\mid X, Z \\sim \\mathrm{N}\\left(0, \\sigma^{2}\\right)\\). Under \\(\\mathbb{H}_{0}, \\mathrm{~F} \\sim F\\left(k_{2}, n-k_{1}-2 k_{2}\\right)\\). Let \\(c_{1-\\alpha}\\) solve \\(\\mathbb{P}\\left[F\\left(k_{2}, n-k_{1}-2 k_{2}\\right) \\leq c_{1-\\alpha}\\right]=1-\\alpha\\). The test “Reject \\(\\mathbb{H}_{0}\\) if \\(\\mathrm{F}>\\) \\(c_{1-\\alpha}\\)” has exact size \\(\\alpha\\).\nSince in general we do not want to impose homoskedasticity these results suggest that the most appropriate test is the Wald statistic constructed with the robust heteroskedastic covariance matrix. This can be computed in Stata using the command estat endogenous after ivregress when the latter uses a robust covariance option. Stata reports the Wald statistic in \\(F\\) form (and thus uses the \\(F\\) distribution to calculate the p-value) as “Robust regression F”. Using the \\(F\\) rather than the \\(\\chi^{2}\\) is not formally justified but is a reasonable finite sample adjustment. If the command estat endogenous is applied after ivregress without a robust covariance option Stata reports the \\(F\\) statistic as “Wu-Hausman F”.\nThere is an alternative (and traditional) way to derive a test for endogeneity. Under \\(\\mathbb{M}_{0}\\), both OLS and 2 SLS are consistent estimators. But under \\(\\mathbb{M}_{1}\\) they converge to different values. Thus the difference between the OLS and 2SLS estimators is a valid test statistic for endogeneity. It also measures what we often care most about - the impact of endogeneity on the parameter estimates. This literature was developed under the assumption of conditional homoskedasticity (and it is important for these results) so we assume this condition for the development of the statistic.\nLet \\(\\widehat{\\beta}=\\left(\\widehat{\\beta}_{1}, \\widehat{\\beta}_{2}\\right)\\) be the OLS estimator and let \\(\\widetilde{\\beta}=\\left(\\widetilde{\\beta}_{1}, \\widetilde{\\beta}_{2}\\right)\\) be the 2SLS estimator. Under \\(\\mathbb{H}_{0}\\) and homoskedasticity the OLS estimator is Gauss-Markov efficient so by the Hausman equality\n\\[\n\\begin{aligned}\n\\operatorname{var}\\left[\\widehat{\\beta}_{2}-\\widetilde{\\beta}_{2}\\right] &=\\operatorname{var}\\left[\\widetilde{\\beta}_{2}\\right]-\\operatorname{var}\\left[\\widehat{\\beta}_{2}\\right] \\\\\n&=\\left(\\left(\\boldsymbol{X}_{2}^{\\prime}\\left(\\boldsymbol{P}_{\\boldsymbol{Z}}-\\boldsymbol{P}_{1}\\right) \\boldsymbol{X}_{2}\\right)^{-1}-\\left(\\boldsymbol{X}_{2}^{\\prime} \\boldsymbol{M}_{1} \\boldsymbol{X}_{2}\\right)^{-1}\\right) \\sigma^{2}\n\\end{aligned}\n\\]\nwhere \\(\\boldsymbol{P}_{\\boldsymbol{Z}}=\\boldsymbol{Z}\\left(\\boldsymbol{Z}^{\\prime} \\boldsymbol{Z}\\right)^{-1} \\boldsymbol{Z}^{\\prime}, \\boldsymbol{P}_{1}=\\boldsymbol{X}_{1}\\left(\\boldsymbol{X}_{1}^{\\prime} \\boldsymbol{X}_{1}\\right)^{-1} \\boldsymbol{X}_{1}^{\\prime}\\), and \\(\\boldsymbol{M}_{1}=\\boldsymbol{I}_{n}-\\boldsymbol{P}_{1}\\). Thus a valid test statistic for \\(\\mathbb{H}_{0}\\) is\n\\[\nT=\\frac{\\left(\\widehat{\\beta}_{2}-\\widetilde{\\beta}_{2}\\right)^{\\prime}\\left(\\left(\\boldsymbol{X}_{2}^{\\prime}\\left(\\boldsymbol{P}_{\\boldsymbol{Z}}-\\boldsymbol{P}_{1}\\right) \\boldsymbol{X}_{2}\\right)^{-1}-\\left(\\boldsymbol{X}_{2}^{\\prime} \\boldsymbol{M}_{1} \\boldsymbol{X}_{2}\\right)^{-1}\\right)^{-1}\\left(\\widehat{\\beta}_{2}-\\widetilde{\\beta}_{2}\\right)}{\\widehat{\\sigma}^{2}}\n\\]\nfor some estimator \\(\\widehat{\\sigma}^{2}\\) of \\(\\sigma^{2}\\). Durbin (1954) first proposed \\(T\\) as a test for endogeneity in the context of IV estimation setting \\(\\widehat{\\sigma}^{2}\\) to be the least squares estimator of \\(\\sigma^{2}\\). Wu (1973) proposed \\(T\\) as a test for endogeneity in the context of 2SLS estimation, considering a set of possible estimators \\(\\widehat{\\sigma}^{2}\\) including the regression estimator from (12.63). Hausman (1978) proposed a version of \\(T\\) based on the full contrast \\(\\widehat{\\beta}-\\widetilde{\\beta}\\), and observed that it equals the regression Wald statistic \\(W^{0}\\) described earlier. In fact, when \\(\\widehat{\\sigma}^{2}\\) is the regression estimator from (12.63) the statistic (12.64) algebraically equals both \\(W^{0}\\) and the version of (12.64) based on the full contrast \\(\\widehat{\\beta}-\\widetilde{\\beta}\\). We show these equalities below. Thus these three approaches yield exactly the same statistic except for possible differences regarding the choice of \\(\\widehat{\\sigma}^{2}\\). Since the regression \\(F\\) test described earlier has an exact \\(F\\) distribution in the normal sampling model and thus can exactly control test size, this is the preferred version of the test. The general class of tests are called Durbin-Wu-Hausman tests, Wu-Hausman tests, or Hausman tests, depending on the author.\nWhen \\(k_{2}=1\\) (there is one right-hand-side endogenous variable), which is quite common in applications, the endogeneity test can be equivalently expressed at the t-statistic for \\(\\widehat{\\alpha}\\) in the estimated control function. Thus it is sufficient to estimate the control function regression and check the t-statistic for \\(\\widehat{\\alpha}\\). If \\(|\\widehat{\\alpha}|>2\\) then we can reject the hypothesis that \\(X_{2}\\) is exogenous for \\(\\beta\\).\nWe illustrate using the Card proximity example using the two instruments public and private. We first estimate the reduced form for education, obtain the residual, and then estimate the control function regression. The residual has a coefficient \\(-0.088\\) with a standard error of \\(0.037\\) and a t-statistic of 2.4. Since the latter exceeds the \\(5 %\\) critical value (its p-value is \\(0.017\\) ) we reject exogeneity. This means that the 2SLS estimates are statistically different from the least squares estimates of the structural equation and supports our decision to treat education as an endogenous variable. (Alternatively, the \\(F\\) statistic is \\(2.4^{2}=5.7\\) with the same p-value).\nWe now show the equality of the various statistics.\nWe first show that the statistic (12.64) is not altered if based on the full contrast \\(\\widehat{\\beta}-\\widetilde{\\beta}\\). Indeed, \\(\\widehat{\\beta}_{1}-\\widetilde{\\beta}_{1}\\) is a linear function of \\(\\widehat{\\beta}_{2}-\\widetilde{\\beta}_{2}\\), so there is no extra information in the full contrast. To see this, observe that given \\(\\widehat{\\beta}_{2}\\) we can solve by least squares to find\n\\[\n\\widehat{\\beta}_{1}=\\left(\\boldsymbol{X}_{1}^{\\prime} \\boldsymbol{X}_{1}\\right)^{-1}\\left(\\boldsymbol{X}_{1}^{\\prime}\\left(\\boldsymbol{Y}-\\boldsymbol{X}_{2} \\widehat{\\beta}_{2}\\right)\\right)\n\\]\nand similarly\n\\[\n\\widetilde{\\beta}_{1}=\\left(\\boldsymbol{X}_{1}^{\\prime} \\boldsymbol{X}_{1}\\right)^{-1}\\left(\\boldsymbol{X}_{1}^{\\prime}\\left(\\boldsymbol{Y}-\\boldsymbol{P}_{\\boldsymbol{Z}} \\boldsymbol{X}_{2} \\widetilde{\\beta}\\right)\\right)=\\left(\\boldsymbol{X}_{1}^{\\prime} \\boldsymbol{X}_{1}\\right)^{-1}\\left(\\boldsymbol{X}_{1}^{\\prime}\\left(\\boldsymbol{Y}-\\boldsymbol{X}_{2} \\widetilde{\\beta}\\right)\\right)\n\\]\nthe second equality because \\(\\boldsymbol{P}_{\\boldsymbol{Z}} \\boldsymbol{X}_{1}=\\boldsymbol{X}_{1}\\). Thus\n\\[\n\\begin{aligned}\n\\widehat{\\beta}_{1}-\\widetilde{\\beta}_{1} &=\\left(\\boldsymbol{X}_{1}^{\\prime} \\boldsymbol{X}_{1}\\right)^{-1} \\boldsymbol{X}_{1}^{\\prime}\\left(\\boldsymbol{Y}-\\boldsymbol{X}_{2} \\widehat{\\beta}_{2}\\right)-\\left(\\boldsymbol{X}_{1}^{\\prime} \\boldsymbol{X}_{1}\\right)^{-1} \\boldsymbol{X}_{1}^{\\prime}\\left(\\boldsymbol{Y}-\\boldsymbol{P}_{\\boldsymbol{Z}} \\boldsymbol{X}_{2} \\widetilde{\\beta}\\right) \\\\\n&=\\left(\\boldsymbol{X}_{1}^{\\prime} \\boldsymbol{X}_{1}\\right)^{-1} \\boldsymbol{X}_{1}^{\\prime} \\boldsymbol{X}_{2}\\left(\\widetilde{\\beta}_{2}-\\widehat{\\beta}_{2}\\right)\n\\end{aligned}\n\\]\nas claimed.\nWe next show that \\(T\\) in (12.64) equals the homoskedastic Wald statistic \\(W^{0}\\) for \\(\\widehat{\\alpha}\\) from the regression (12.63). Consider the latter regression. Since \\(\\boldsymbol{X}_{2}\\) is contained in \\(\\boldsymbol{X}\\) the coefficient estimate \\(\\widehat{\\alpha}\\) is invariant to replacing \\(\\widehat{\\boldsymbol{U}}_{2}=\\boldsymbol{X}_{2}-\\widehat{\\boldsymbol{X}}_{2}\\) with \\(-\\widehat{\\boldsymbol{X}}_{2}=-\\boldsymbol{P}_{\\boldsymbol{Z}} \\boldsymbol{X}_{2}\\). By the FWL representation, setting \\(\\boldsymbol{M}_{\\boldsymbol{X}}=\\) \\(\\boldsymbol{I}_{n}-\\boldsymbol{X}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\boldsymbol{X}^{\\prime}\\)\n\\[\n\\widehat{\\alpha}=-\\left(\\widehat{\\boldsymbol{X}}_{2}^{\\prime} \\boldsymbol{M}_{\\boldsymbol{X}} \\widehat{\\boldsymbol{X}}_{2}\\right)^{-1} \\widehat{\\boldsymbol{X}}_{2}^{\\prime} \\boldsymbol{M}_{\\boldsymbol{X}} \\boldsymbol{Y}=-\\left(\\boldsymbol{X}_{2}^{\\prime} \\boldsymbol{P}_{\\boldsymbol{Z}} \\boldsymbol{M}_{\\boldsymbol{X}} \\boldsymbol{P}_{\\boldsymbol{Z}} \\boldsymbol{X}_{2}\\right)^{-1} \\boldsymbol{X}_{2}^{\\prime} \\boldsymbol{P}_{\\boldsymbol{Z}} \\boldsymbol{M}_{\\boldsymbol{X}} \\boldsymbol{Y}\n\\]\nIt follows that\n\\[\nW^{0}=\\frac{\\boldsymbol{Y}^{\\prime} \\boldsymbol{M}_{\\boldsymbol{X}} \\boldsymbol{P}_{\\boldsymbol{Z}} \\boldsymbol{X}_{2}\\left(\\boldsymbol{X}_{2}^{\\prime} \\boldsymbol{P}_{\\boldsymbol{Z}} \\boldsymbol{M}_{\\boldsymbol{X}} \\boldsymbol{P}_{\\boldsymbol{Z}} \\boldsymbol{X}_{2}\\right)^{-1} \\boldsymbol{X}_{2}^{\\prime} \\boldsymbol{P}_{\\boldsymbol{Z}} \\boldsymbol{M}_{\\boldsymbol{X}} \\boldsymbol{Y}}{\\widehat{\\sigma}^{2}} .\n\\]\nOur goal is to show that \\(T=W^{0}\\). Define \\(\\widetilde{\\boldsymbol{X}}_{2}=\\left(\\boldsymbol{I}_{n}-\\boldsymbol{P}_{1}\\right) \\boldsymbol{X}_{2}\\) so \\(\\widehat{\\beta}_{2}=\\left(\\widetilde{\\boldsymbol{X}}_{2}^{\\prime} \\widetilde{\\boldsymbol{X}}_{2}\\right)^{-1} \\widetilde{\\boldsymbol{X}}_{2}^{\\prime} \\boldsymbol{Y}\\). Then using \\(\\left(\\boldsymbol{P}_{\\boldsymbol{Z}}-\\boldsymbol{P}_{1}\\right)\\left(\\boldsymbol{I}_{n}-\\boldsymbol{P}_{1}\\right)=\\left(\\boldsymbol{P}_{\\boldsymbol{Z}}-\\boldsymbol{P}_{1}\\right)\\) and defining \\(\\boldsymbol{Q}=\\widetilde{\\boldsymbol{X}}_{2}\\left(\\widetilde{\\boldsymbol{X}}_{2}^{\\prime} \\tilde{\\boldsymbol{X}}_{2}\\right)^{-1} \\widetilde{\\boldsymbol{X}}_{2}^{\\prime}\\) we find\n\\[\n\\begin{aligned}\n&\\boldsymbol{\\Delta} \\stackrel{\\text { def }}{=}\\left(\\boldsymbol{X}_{2}^{\\prime}\\left(\\boldsymbol{P}_{\\boldsymbol{Z}}-\\boldsymbol{P}_{1}\\right) \\boldsymbol{X}_{2}\\right)\\left(\\widetilde{\\beta}_{2}-\\widehat{\\beta}_{2}\\right) \\\\\n&=\\boldsymbol{X}_{2}^{\\prime}\\left(\\boldsymbol{P}_{\\boldsymbol{Z}}-\\boldsymbol{P}_{1}\\right) \\boldsymbol{Y}-\\left(\\boldsymbol{X}_{2}^{\\prime}\\left(\\boldsymbol{P}_{\\boldsymbol{Z}}-\\boldsymbol{P}_{1}\\right) \\boldsymbol{X}_{2}\\right)\\left(\\widetilde{\\boldsymbol{X}}_{2}^{\\prime} \\widetilde{\\boldsymbol{X}}_{2}\\right)^{-1} \\widetilde{\\boldsymbol{X}}_{2}^{\\prime} \\boldsymbol{Y} \\\\\n&=\\boldsymbol{X}_{2}^{\\prime}\\left(\\boldsymbol{P}_{\\boldsymbol{Z}}-\\boldsymbol{P}_{1}\\right)\\left(\\boldsymbol{I}_{n}-\\boldsymbol{Q}\\right) \\boldsymbol{Y} \\\\\n&=\\boldsymbol{X}_{2}^{\\prime}\\left(\\boldsymbol{P}_{\\boldsymbol{Z}}-\\boldsymbol{P}_{1}-\\boldsymbol{P}_{\\boldsymbol{Z}} \\boldsymbol{Q}\\right) \\boldsymbol{Y} \\\\\n&=\\boldsymbol{X}_{2}^{\\prime} \\boldsymbol{P}_{\\boldsymbol{Z}}\\left(\\boldsymbol{I}_{n}-\\boldsymbol{P}_{1}-\\boldsymbol{Q}\\right) \\boldsymbol{Y} \\\\\n&=\\boldsymbol{X}_{2}^{\\prime} \\boldsymbol{P}_{\\boldsymbol{Z}} \\boldsymbol{M}_{\\boldsymbol{X}} \\boldsymbol{Y} .\n\\end{aligned}\n\\]\nThe third-to-last equality is \\(\\boldsymbol{P}_{1} \\boldsymbol{Q}=0\\) and the final uses \\(\\boldsymbol{M}_{\\boldsymbol{X}}=\\boldsymbol{I}_{n}-\\boldsymbol{P}_{1}-\\boldsymbol{Q}\\). We also calculate that\n\\[\n\\begin{aligned}\n&\\boldsymbol{Q}^{*} \\stackrel{\\text { def }}{=}\\left(\\boldsymbol{X}_{2}^{\\prime}\\left(\\boldsymbol{P}_{\\boldsymbol{Z}}-\\boldsymbol{P}_{1}\\right) \\boldsymbol{X}_{2}\\right)\\left(\\left(\\boldsymbol{X}_{2}^{\\prime}\\left(\\boldsymbol{P}_{\\boldsymbol{Z}}-\\boldsymbol{P}_{1}\\right) \\boldsymbol{X}_{2}\\right)^{-1}-\\left(\\boldsymbol{X}_{2}^{\\prime} \\boldsymbol{M}_{1} \\boldsymbol{X}_{2}\\right)^{-1}\\right)\\left(\\boldsymbol{X}_{2}^{\\prime}\\left(\\boldsymbol{P}_{\\boldsymbol{Z}}-\\boldsymbol{P}_{1}\\right) \\boldsymbol{X}_{2}\\right) \\\\\n&=\\boldsymbol{X}_{2}^{\\prime}\\left(\\boldsymbol{P}_{\\boldsymbol{Z}}-\\boldsymbol{P}_{1}-\\left(\\boldsymbol{P}_{\\boldsymbol{Z}}-\\boldsymbol{P}_{1}\\right) \\boldsymbol{Q}\\left(\\boldsymbol{P}_{\\boldsymbol{Z}}-\\boldsymbol{P}_{1}\\right)\\right) \\boldsymbol{X}_{2} \\\\\n&=\\boldsymbol{X}_{2}^{\\prime}\\left(\\boldsymbol{P}_{\\boldsymbol{Z}}-\\boldsymbol{P}_{1}-\\boldsymbol{P}_{\\boldsymbol{Z}} \\boldsymbol{Q} \\boldsymbol{P}_{\\boldsymbol{Z}}\\right) \\boldsymbol{X}_{2} \\\\\n&=\\boldsymbol{X}_{2}^{\\prime} \\boldsymbol{P}_{\\boldsymbol{Z}} \\boldsymbol{M}_{\\boldsymbol{X}} \\boldsymbol{P}_{\\boldsymbol{Z}} \\boldsymbol{X}_{2} .\n\\end{aligned}\n\\]\nThus\n\\[\n\\begin{aligned}\nT &=\\frac{\\boldsymbol{\\Delta}^{\\prime} \\boldsymbol{Q}^{*-1} \\boldsymbol{\\Delta}}{\\widehat{\\sigma}^{2}} \\\\\n&=\\frac{\\boldsymbol{Y}^{\\prime} \\boldsymbol{M}_{\\boldsymbol{X}} \\boldsymbol{P}_{\\boldsymbol{Z}} \\boldsymbol{X}_{2}\\left(\\boldsymbol{X}_{2}^{\\prime} \\boldsymbol{P}_{\\boldsymbol{Z}} \\boldsymbol{M}_{\\boldsymbol{X}} \\boldsymbol{P}_{\\boldsymbol{Z}} \\boldsymbol{X}_{2}\\right)^{-1} \\boldsymbol{X}_{2}^{\\prime} \\boldsymbol{P}_{\\boldsymbol{Z}} \\boldsymbol{M}_{\\boldsymbol{X}} \\boldsymbol{Y}}{\\widehat{\\sigma}^{2}} \\\\\n&=W^{0}\n\\end{aligned}\n\\]\nas claimed."
  },
  {
    "objectID": "chpt12-iv.html#subset-endogeneity-tests",
    "href": "chpt12-iv.html#subset-endogeneity-tests",
    "title": "12  Instrumental Variables",
    "section": "12.30 Subset Endogeneity Tests",
    "text": "12.30 Subset Endogeneity Tests\nIn some cases we may only wish to test the endogeneity of a subset of the variables. In the Card proximity example we may wish test the exogeneity of education separately from experience and its square. To execute a subset endogeneity test it is useful to partition the regressors into three groups so that the structural model is\n\\[\n\\begin{aligned}\nY &=X_{1}^{\\prime} \\beta_{1}+X_{2}^{\\prime} \\beta_{2}+X_{3}^{\\prime} \\beta_{3}+e \\\\\n\\mathbb{E}[Z e] &=0 .\n\\end{aligned}\n\\]\nAs before, the instrument vector \\(Z\\) includes \\(X_{1}\\). The vector \\(X_{3}\\) is treated as endogenous and \\(X_{2}\\) is treated as potentially endogenous. The hypothesis to test is that \\(X_{2}\\) is exogenous, or \\(\\mathbb{H}_{0}: \\mathbb{E}\\left[X_{2} e\\right]=0\\) against \\(\\mathbb{H}_{1}: \\mathbb{E}\\left[X_{2} e\\right] \\neq 0\\)\nUnder homoskedasticity a straightfoward test can be constructed by the Durbin-Wu-Hausman principle. Under \\(\\mathbb{M}_{0}\\) the appropriate estimator is \\(2 \\mathrm{SLS}\\) using the instruments \\(\\left(Z, X_{2}\\right)\\). Let this estimator of \\(\\beta_{2}\\) be denoted \\(\\widehat{\\beta}_{2}\\). Under \\(\\mathbb{H}_{1}\\) the appropriate estimator is 2SLS using the smaller instrument set \\(Z\\). Let this estimator of \\(\\beta_{2}\\) be denoted \\(\\widetilde{\\beta}_{2}\\). A Durbin-Wu-Hausman statistic for \\(\\mathbb{H}_{0}\\) against \\(\\mathbb{H}_{1}\\) is\n\\[\nT=\\left(\\widehat{\\beta}_{2}-\\widetilde{\\beta}_{2}\\right)^{\\prime}\\left(\\widehat{\\operatorname{var}}\\left[\\widetilde{\\beta}_{2}\\right]-\\widehat{\\operatorname{var}}\\left[\\widehat{\\beta}_{2}\\right]\\right)^{-1}\\left(\\widehat{\\beta}_{2}-\\widetilde{\\beta}_{2}\\right) .\n\\]\nThe asymptotic distribution under \\(\\mathbb{H}_{0}\\) is \\(\\chi_{k_{2}}^{2}\\) where \\(k_{2}=\\operatorname{dim}\\left(X_{2}\\right)\\), so we reject the hypothesis that the variables \\(X_{2}\\) are exogenous if \\(T\\) exceeds an upper critical value from the \\(\\chi_{k_{2}}^{2}\\) distribution.\nInstead of using the Wald statistic one could use the \\(F\\) version of the test by dividing by \\(k_{2}\\) and using the \\(F\\) distribution for critical values. There is no finite sample justification for this modification, however, since \\(X_{3}\\) is endogenous under the null hypothesis.\nIn Stata, the command estat endogenous (adding the variable name to specify which variable to test for exogeneity) after ivregress without a robust covariance option reports the \\(F\\) version of this statistic as “Wu-Hausman F”. For example, in the Card proximity example using the four instruments public, private, age, and \\(a g e^{2}\\), if we estimate the equation by 2SLS with a non-robust covariance matrix and then compute the endogeneity test for education we find \\(F=272\\) with a p-value of \\(0.0000\\), but if we compute the test for experience and its square we find \\(F=2.98\\) with a p-value of \\(0.051\\). In this model, the assumption of exogeneity with homogenous coefficients is rejected for education but the result for experience is unclear.\nA heteroskedasticity or cluster-robust test cannot be constructed easily by the Durbin-Wu-Hausman approach since the covariance matrix does not take a simple form. To allow for non-homoskedastic errors it is recommended to use GMM estimation. See Section 13.24."
  },
  {
    "objectID": "chpt12-iv.html#overidentification-tests",
    "href": "chpt12-iv.html#overidentification-tests",
    "title": "12  Instrumental Variables",
    "section": "12.31 OverIdentification Tests",
    "text": "12.31 OverIdentification Tests\nWhen \\(\\ell>k\\) the model is overidentified meaning that there are more moments than free parameters. This is a restriction and is testable. Such tests are called overidentification tests.\nThe instrumental variables model specifies \\(\\mathbb{E}[Z e]=0\\). Equivalently, since \\(e=Y-X^{\\prime} \\beta\\) this is\n\\[\n\\mathbb{E}[Z Y]-\\mathbb{E}\\left[Z X^{\\prime}\\right] \\beta=0 .\n\\]\nThis is an \\(\\ell \\times 1\\) vector of restrictions on the moment matrices \\(\\mathbb{E}[Z Y]\\) and \\(\\mathbb{E}\\left[Z X^{\\prime}\\right]\\). Yet since \\(\\beta\\) is of dimension \\(k\\) which is less than \\(\\ell\\) it is not certain if indeed such a \\(\\beta\\) exists.\nTo make things a bit more concrete, suppose there is a single endogenous regressor \\(X_{2}\\), no \\(X_{1}\\), and two instruments \\(Z_{1}\\) and \\(Z_{2}\\). Then the model specifies that\n\\[\n\\mathbb{E}\\left(\\left[Z_{1} Y\\right]=\\mathbb{E}\\left[Z_{1} X_{2}\\right] \\beta\\right.\n\\]\nand\n\\[\n\\mathbb{E}\\left[Z_{2} Y\\right]=\\mathbb{E}\\left[Z_{2} X_{2}\\right] \\beta .\n\\]\nThus \\(\\beta\\) solves both equations. This is rather special.\nAnother way of thinking about this is we could solve for \\(\\beta\\) using either one equation or the other. In terms of estimation this is equivalent to estimating by IV using just the instrument \\(Z_{1}\\) or instead just using the instrument \\(Z_{2}\\). These two estimators (in finite samples) are different. If the overidentification hypothesis is correct both are estimating the same parameter and both are consistent for \\(\\beta\\). In contrast, if the overidentification hypothesis is false then the two estimators will converge to different probability limits and it is unclear if either probability limit is interesting.\nFor example, take the 2SLS estimates in the fourth column of Table \\(12.1\\) which use public and private as instruments for education. Suppose we instead estimate by IV using just public as an instrument and then repeat using private. The IV coefficient for education in the first case is \\(0.16\\) and in the second case 0.27. These appear to be quite different. However, the second estimate has a large standard error (0.16) so the difference may be sampling variation. An overidentification test addresses this question.\nFor a general overidentification test the null and alternative hypotheses are \\(\\mathbb{H}_{0}: \\mathbb{E}[Z e]=0\\) against \\(\\mathbb{H}_{1}: \\mathbb{E}[Z e] \\neq 0\\). We will also add the conditional homoskedasticity assumption\n\\[\n\\mathbb{E}\\left[e^{2} \\mid Z\\right]=\\sigma^{2} .\n\\]\nTo avoid (12.65) it is best to take a GMM approach which we defer until Chapter \\(13 .\\)\nTo implement a test of \\(\\mathbb{M}_{0}\\) consider a linear regression of the error \\(e\\) on the instruments \\(Z\\)\n\\[\ne=Z^{\\prime} \\alpha+v\n\\]\nwith \\(\\alpha=\\left(\\mathbb{E}\\left[Z Z^{\\prime}\\right]\\right)^{-1} \\mathbb{E}[Z e]\\). We can rewrite \\(\\mathbb{H}_{0}\\) as \\(\\alpha=0\\). While \\(e\\) is not observed we can replace it with the 2SLS residual \\(\\widehat{e}_{i}\\) and estimate \\(\\alpha\\) by least squares regression, e.g. \\(\\widehat{\\alpha}=\\left(Z^{\\prime} \\boldsymbol{Z}\\right)^{-1} Z^{\\prime} \\widehat{\\boldsymbol{e}}\\). Sargan (1958) proposed testing \\(\\mathbb{M}_{0}\\) via a score test, which equals\n\\[\nS=\\widehat{\\alpha}^{\\prime}(\\widehat{\\operatorname{var}}[\\widehat{\\alpha}])^{-} \\widehat{\\alpha}=\\frac{\\widehat{\\boldsymbol{e}}^{\\prime} \\boldsymbol{Z}\\left(\\boldsymbol{Z}^{\\prime} \\boldsymbol{Z}\\right)^{-1} \\boldsymbol{Z}^{\\prime} \\widehat{\\boldsymbol{e}}}{\\widehat{\\sigma}^{2}} .\n\\]\nwhere \\(\\widehat{\\sigma}^{2}=\\frac{1}{n} \\widehat{\\boldsymbol{e}} \\widehat{\\boldsymbol{e}}\\). Basmann (1960) independently proposed a Wald statistic for \\(\\mathbb{H}_{0}\\), which is \\(S\\) with \\(\\widehat{\\sigma}^{2}\\) replaced with \\(\\widetilde{\\sigma}^{2}=n^{-1} \\widehat{\\boldsymbol{v}} ' \\widehat{\\boldsymbol{v}}\\) where \\(\\widehat{\\boldsymbol{v}}=\\widehat{\\boldsymbol{e}}-\\boldsymbol{Z} \\widehat{\\alpha}\\). By the equivalence of homoskedastic score and Wald tests (see Section 9.16) Basmann’s statistic is a monotonic function of Sargan’s statistic and hence they yield equivalent tests. Sargan’s version is more typically reported.\nThe Sargan test rejects \\(\\mathbb{H}_{0}\\) in favor of \\(\\mathbb{M}_{1}\\) if \\(S>c\\) for some critical value \\(c\\). An asymptotic test sets \\(c\\) as the \\(1-\\alpha\\) quantile of the \\(\\chi_{\\ell-k}^{2}\\) distribution. This is justified by the asymptotic null distribution of \\(S\\) which we now derive.\nTheorem 12.16 Under Assumption \\(12.2\\) and \\(\\mathbb{E}\\left[e^{2} \\mid Z\\right]=\\sigma^{2}\\), then as \\(n \\rightarrow \\infty\\), \\(S \\underset{d}{\\longrightarrow} \\chi_{\\ell-k}^{2}\\). For \\(c\\) satisfying \\(\\alpha=1-G_{\\ell-k}(c), \\mathbb{P}\\left[S>c \\mid \\mathbb{H}_{0}\\right] \\rightarrow \\alpha\\) so the test “Reject\n\\(\\mathbb{M}_{0}\\) if \\(S>c\\) ” has asymptotic size \\(\\alpha\\).\nWe prove Theorem \\(12.16\\) below.\nThe Sargan statistic \\(S\\) is an asymptotic test of the overidentifying restrictions under the assumption of conditional homoskedasticity. It has some limitations. First, it is an asymptotic test and does not have a finite sample (e.g. F) counterpart. Simulation evidence suggests that the test can be oversized (reject too frequently) in small and moderate sample sizes. Consequently, p-values should be interpreted cautiously. Second, the assumption of conditional homoskedasticity is unrealistic in applications. The best way to generalize the Sargan statistic to allow heteroskedasticity is to use the GMM overidentification statistic - which we will examine in Chapter 13. For 2SLS, Wooldrige (1995) suggested a robust score test, but Baum, Schaffer and Stillman (2003) point out that it is numerically equivalent to the GMM overidentification statistic. Hence the bottom line appears to be that to allow heteroskedasticity or clustering it is best to use a GMM approach.\nIn overidentified applications it is always prudent to report an overidentification test. If the test is insignificant it means that the overidentifying restrictions are not rejected, supporting the estimated model. If the overidentifying test statistic is highly significant (if the p-value is very small) this is evidence that the overidentifying restrictions are violated. In this case we should be concerned that the model is misspecified and interpreting the parameter estimates should be done cautiously.\nWhen reporting the results of an overidentification test it seems reasonable to focus on very small significance levels such as \\(1 %\\). This means that we should only treat a model as “rejected” if the Sargan p-value is very small, e.g. less than \\(0.01\\). The reason to focus on very small significance levels is because it is very difficult to interpret the result “The model is rejected”. Stepping back a bit it does not seem credible that any overidentified model is literally true; rather what seems potentially credible is that an overidentified model is a reasonable approximation. A test is asking the question “Is there evidence that a model is not true” when we really want to know the answer to “Is there evidence that the model is a poor approximation”. Consequently it seems reasonable to require strong evidence to lead to the conclusion “Let’s reject this model”. The recommendation is that mild rejections ( \\(\\mathrm{p}\\)-values between \\(1 %\\) and 5%) should be viewed as mildly worrisome but not critical evidence against a model. The results of an overidentification test should be integrated with other information before making a strong decision.\nWe illustrate the methods with the Card college proximity example. We have estimated two overidentified models by 2SLS in columns 4 & 5 of Table 12.1. In each case the number of overidentifying restrictions is 1 . We report the Sargan statistic and its asymptotic \\(p\\)-value (calculated using the \\(\\chi_{1}^{2}\\) distribution) in the table. Both p-values (0.37 and \\(0.47)\\) are far from significant indicating that there is no evidence that the models are misspecified.\nWe now prove Theorem 12.16. The statistic \\(S\\) is invariant to rotations of \\(\\boldsymbol{Z}\\) (replacing \\(\\boldsymbol{Z}\\) with \\(\\boldsymbol{Z} \\boldsymbol{C}\\) ) so without loss of generality we assume \\(\\mathbb{E}\\left[Z Z^{\\prime}\\right]=\\boldsymbol{I}_{\\ell}\\). As \\(n \\rightarrow \\infty, n^{-1 / 2} \\boldsymbol{Z}^{\\prime} \\boldsymbol{e} \\underset{d}{\\rightarrow} Z\\) where \\(Z \\sim \\mathrm{N}\\left(0, \\boldsymbol{I}_{\\ell}\\right)\\). Also \\(\\frac{1}{n} \\boldsymbol{Z}^{\\prime} \\boldsymbol{Z} \\underset{p}{\\longrightarrow} \\boldsymbol{I}_{\\ell}\\) and \\(\\frac{1}{n} \\boldsymbol{Z}^{\\prime} \\boldsymbol{X} \\underset{p}{\\longrightarrow} \\boldsymbol{Q}\\), say. Then\n\\[\n\\begin{aligned}\nn^{-1 / 2} \\boldsymbol{Z}^{\\prime} \\widehat{\\boldsymbol{e}} &=\\left(\\boldsymbol{I}_{\\ell}-\\left(\\frac{1}{n} \\boldsymbol{Z}^{\\prime} \\boldsymbol{X}\\right)\\left(\\frac{1}{n} \\boldsymbol{X}^{\\prime} \\boldsymbol{P}_{\\boldsymbol{Z}} \\boldsymbol{X}\\right)^{-1}\\left(\\frac{1}{n} \\boldsymbol{X}^{\\prime} \\boldsymbol{Z}\\right)\\left(\\frac{1}{n} \\boldsymbol{Z}^{\\prime} \\boldsymbol{Z}\\right)^{-1}\\right) n^{-1 / 2} \\boldsymbol{Z}^{\\prime} \\boldsymbol{e} \\\\\n& \\underset{d}{\\rightarrow} \\sigma\\left(\\boldsymbol{I}_{\\ell}-\\boldsymbol{Q}\\left(\\boldsymbol{Q}^{\\prime} \\boldsymbol{Q}\\right)^{-1} \\boldsymbol{Q}^{\\prime}\\right) Z\n\\end{aligned}\n\\]\nSince \\(\\widehat{\\sigma}^{2} \\underset{p}{\\rightarrow} \\sigma^{2}\\) it follows that\n\\[\nS \\underset{d}{\\rightarrow} Z^{\\prime}\\left(\\boldsymbol{I}_{\\ell}-\\boldsymbol{Q}\\left(\\boldsymbol{Q}^{\\prime} \\boldsymbol{Q}\\right)^{-1} \\boldsymbol{Q}^{\\prime}\\right) \\mathrm{Z} \\sim \\chi_{\\ell-k}^{2} .\n\\]\nThe distribution is \\(\\chi_{\\ell-k}^{2}\\) because \\(\\boldsymbol{I}_{\\ell}-\\boldsymbol{Q}\\left(\\boldsymbol{Q}^{\\prime} \\boldsymbol{Q}\\right)^{-1} \\boldsymbol{Q}^{\\prime}\\) is idempotent with rank \\(\\ell-k\\).\nThe Sargan statistic test can be implemented in Stata using the command estat overid after ivregress 2sls or ivregres liml if a standard (non-robust) covariance matrix has been specified (that is, without the ‘, r’ option), or otherwise by the command estat overid, forcenonrobust."
  },
  {
    "objectID": "chpt12-iv.html#subset-overidentification-tests",
    "href": "chpt12-iv.html#subset-overidentification-tests",
    "title": "12  Instrumental Variables",
    "section": "12.32 Subset OverIdentification Tests",
    "text": "12.32 Subset OverIdentification Tests\nTests of \\(\\mathbb{H}_{0}: \\mathbb{E}[Z e]=0\\) are typically interpreted as tests of model specification. The alternative \\(\\mathbb{H}_{1}\\) : \\(\\mathbb{E}[Z e] \\neq 0\\) means that at least one element of \\(Z\\) is correlated with the error \\(e\\) and is thus an invalid instrumental variable. In some cases it may be reasonable to test only a subset of the moment conditions.\nAs in the previous section we restrict attention to the homoskedastic case \\(\\mathbb{E}\\left[e^{2} \\mid Z\\right]=\\sigma^{2}\\).\nPartition \\(Z=\\left(Z_{a}, Z_{b}\\right)\\) with dimensions \\(\\ell_{a}\\) and \\(\\ell_{b}\\), respectively, where \\(Z_{a}\\) contains the instruments which are believed to be uncorrelated with \\(e\\) and \\(Z_{b}\\) contains the instruments which may be correlated with \\(e\\). It is necessary to select this partition so that \\(\\ell_{a}>k\\), or equivalently \\(\\ell_{b}<\\ell-k\\). This means that the model with just the instruments \\(Z_{a}\\) is over-identified, or that \\(\\ell_{b}\\) is smaller than the number of overidentifying restrictions. (If \\(\\ell_{a}=k\\) then the tests described here exist but reduce to the Sargan test so are not interesting.) Hence the tests require that \\(\\ell-k>1\\), that the number of overidentifying restrictions exceeds one.\nGiven this partition the maintained hypothesis is \\(\\mathbb{E}\\left[Z_{a} e\\right]=0\\). The null and alternative hypotheses are \\(\\mathbb{H}_{0}: \\mathbb{E}\\left[Z_{b} e\\right]=0\\) against \\(\\mathbb{H}_{1}: \\mathbb{E}\\left[Z_{b} e\\right] \\neq 0\\). That is, the null hypothesis is that the full set of moment conditions are valid while the alternative hypothesis is that the instrument subset \\(Z_{b}\\) is correlated with \\(e\\) and thus an invalid instrument. Rejection of \\(\\mathbb{H}_{0}\\) in favor of \\(\\mathbb{M}_{1}\\) is then interpreted as evidence that \\(Z_{b}\\) is misspecified as an instrument.\nBased on the same reasoning as described in the previous section, to test \\(\\mathbb{H}_{0}\\) against \\(\\mathbb{H}_{1}\\) we consider a partitioned version of the regression (12.66)\n\\[\ne=Z_{a}^{\\prime} \\alpha_{a}+Z_{b}^{\\prime} \\alpha_{b}+v\n\\]\nbut now focus on the coefficient \\(\\alpha_{b}\\). Given \\(\\mathbb{E}\\left[Z_{a} e\\right]=0, \\mathbb{H}_{0}\\) is equivalent to \\(\\alpha_{b}=0\\). The equation is estimated by least squares replacing the unobserved \\(e_{i}\\) with the 2 SLS residual \\(\\widehat{e}_{i}\\). The estimate of \\(\\alpha_{b}\\) is\n\\[\n\\widehat{\\alpha}_{b}=\\left(\\boldsymbol{Z}_{b}^{\\prime} \\boldsymbol{M}_{a} \\boldsymbol{Z}_{b}\\right)^{-1} \\boldsymbol{Z}_{b}^{\\prime} \\boldsymbol{M}_{a} \\widehat{\\boldsymbol{e}}\n\\]\nwhere \\(\\boldsymbol{M}_{a}=\\boldsymbol{I}_{n}-\\boldsymbol{Z}_{a}\\left(\\boldsymbol{Z}_{a}^{\\prime} \\boldsymbol{Z}_{a}\\right)^{-1} \\boldsymbol{Z}_{a}^{\\prime}\\). Newey (1985) showed that an optimal (asymptotically most powerful) test of \\(\\mathbb{M}_{0}\\) against \\(\\mathbb{M}_{1}\\) is to reject for large values of the score statistic\n\\[\nN=\\widehat{\\alpha}_{b}^{\\prime}\\left(\\widehat{\\operatorname{var}}\\left[\\widehat{\\alpha}_{b}\\right]\\right)^{-} \\widehat{\\alpha}_{b}=\\frac{\\widehat{\\boldsymbol{e}}^{\\prime} \\boldsymbol{R}\\left(\\boldsymbol{R}^{\\prime} \\boldsymbol{R}-\\boldsymbol{R}^{\\prime} \\widehat{\\boldsymbol{X}}\\left(\\widehat{\\boldsymbol{X}}^{\\prime} \\widehat{\\boldsymbol{X}}\\right)^{-1} \\widehat{\\boldsymbol{X}}^{\\prime} \\boldsymbol{R}\\right)^{-1} \\boldsymbol{R}^{\\prime} \\widehat{\\boldsymbol{e}}}{\\widehat{\\sigma}^{2}}\n\\]\nwhere \\(\\widehat{\\boldsymbol{X}}=\\boldsymbol{P} \\boldsymbol{X}, \\boldsymbol{P}=\\boldsymbol{Z}\\left(\\boldsymbol{Z}^{\\prime} \\boldsymbol{Z}\\right)^{-1} \\boldsymbol{Z}^{\\prime}, \\boldsymbol{R}=\\boldsymbol{M}_{a} \\boldsymbol{Z}_{b}\\), and \\(\\widehat{\\sigma}^{2}=\\frac{1}{n} \\widehat{\\boldsymbol{e}}^{\\prime} \\widehat{\\boldsymbol{e}}\\).\nIndependently from Newey (1985), Eichenbaum, L. Hansen, and Singleton (1988) proposed a test based on the difference of Sargan statistics. Let \\(S\\) be the Sargan test statistic (12.67) based on the full instrument set and \\(S_{a}\\) be the Sargan statistic based on the instrument set \\(Z_{a}\\). The Sargan difference statistic is \\(C=S-S_{a}\\). Specifically, let \\(\\widetilde{\\beta}_{2 \\text { sls }}\\) be the 2SLS estimator using the instruments \\(Z_{a}\\) only, set \\(\\widetilde{e}_{i}=Y_{i}-X_{i}^{\\prime} \\widetilde{\\beta}_{2 s l s}\\), and set \\(\\widetilde{\\sigma}^{2}=\\frac{1}{n} \\widetilde{\\boldsymbol{e}}^{\\prime} \\widetilde{\\boldsymbol{e}}\\). Then\n\\[\nS_{a}=\\frac{\\widetilde{\\boldsymbol{e}}^{\\prime} \\boldsymbol{Z}_{a}\\left(\\boldsymbol{Z}_{a}^{\\prime} \\boldsymbol{Z}_{a}\\right)^{-1} \\boldsymbol{Z}_{a}^{\\prime} \\widetilde{\\boldsymbol{e}}}{\\widetilde{\\sigma}^{2}}\n\\]\nAn advantage of the \\(C\\) statistic is that it is quite simple to calculate from the standard regression output.\nAt this point it is useful to reflect on our stated requirement that \\(\\ell_{a}>k\\). Indeed, if \\(\\ell_{a}<k\\) then \\(Z_{a}\\) fails the order condition for identification and \\(\\widetilde{\\beta}_{2 \\text { sls }}\\) cannot be calculated. Thus \\(\\ell_{a} \\geq k\\) is necessary to compute \\(S_{a}\\) and hence \\(S\\). Furthermore, if \\(\\ell_{a}=k\\) then model \\(a\\) is just identified so while \\(\\widetilde{\\beta}_{2 \\text { sls }}\\) can be calculated, the statistic \\(S_{a}=0\\) so \\(C=S\\). Thus when \\(\\ell_{a}=k\\) the subset test equals the full overidentification test so there is no gain from considering subset tests.\nThe \\(C\\) statistic \\(S_{a}\\) is asymptotically equivalent to replacing \\(\\widetilde{\\sigma}^{2}\\) in \\(S_{a}\\) with \\(\\widehat{\\sigma}^{2}\\), yielding the statistic\n\\[\nC^{*}=\\frac{\\widehat{\\boldsymbol{e}}^{\\prime} \\boldsymbol{Z}\\left(\\boldsymbol{Z}^{\\prime} \\boldsymbol{Z}\\right)^{-1} \\boldsymbol{Z}^{\\prime} \\widehat{\\boldsymbol{e}}}{\\widehat{\\sigma}^{2}}-\\frac{\\widetilde{\\boldsymbol{e}}^{\\prime} \\boldsymbol{Z}_{a}\\left(\\boldsymbol{Z}_{a}^{\\prime} \\boldsymbol{Z}_{a}\\right)^{-1} \\boldsymbol{Z}_{a}^{\\prime} \\widetilde{\\boldsymbol{e}}}{\\widehat{\\sigma}^{2}} .\n\\]\nIt turns out that this is Newey’s statistic \\(N\\). These tests have chi-square asymptotic distributions.\nLet \\(c\\) satisfy \\(\\alpha=1-G_{\\ell_{b}}(c)\\).\nTheorem 12.17 Algebraically, \\(N=C^{*}\\). Under Assumption \\(12.2\\) and \\(\\mathbb{E}\\left[e^{2} \\mid Z\\right]=\\) \\(\\sigma^{2}\\), as \\(n \\rightarrow \\infty, N \\underset{d}{\\longrightarrow} \\chi_{\\ell_{b}}^{2}\\) and \\(C \\underset{d}{\\longrightarrow} \\chi_{\\ell_{b}}^{2}\\). Thus the tests “Reject \\(\\mathbb{H}_{0}\\) if \\(N>c\\)” and\n“Reject \\(\\mathbb{H}_{0}\\) if \\(C>c\\)” are asymptotically equivalent and have asymptotic size \\(\\alpha\\).\nTheorem \\(12.17\\) shows that \\(N\\) and \\(C^{*}\\) are identical and are near equivalents to the convenient statistic C. The appropriate asymptotic distribution is \\(\\chi_{\\ell_{b}}^{2}\\). Computationally, the easiest method to implement a subset overidentification test is to estimate the model twice by 2SLS, first using the full instrument set \\(Z\\) and the second using the partial instrument set \\(Z_{a}\\). Compute the Sargan statistics for both 2SLS regressions and compute \\(C\\) as the difference in the Sargan statistics. In Stata, for example, this is simple to implement with a few lines of code.\nWe illustrate using the Card college proximity example. Our reported 2SLS estimates have \\(\\ell-k=1\\) so there is no role for a subset overidentification test. (Recall, the number of overidentifying restrictions must exceed one.) To illustrate we add extra instruments to the estimates in column 5 of Table \\(12.1\\) (the 2SLS estimates using public, private, age, and age \\({ }^{2}\\) as instruments for education, experience, and experience \\(\\left.{ }^{2} / 100\\right)\\). We add two instruments: the years of education of the father and the mother of the worker. These variables had been used in the earlier labor economics literature as instruments but Card did not. (He used them as regression controls in some specifications.) The motivation for using parent’s education as instruments is the hypothesis that parental education influences children’s educational attainment but does not directly influence their ability. The more modern labor economics literature has disputed this idea, arguing that children are educated in part at home and thus parent’s education has a direct impact on the skill attainment of children (and not just an indirect impact via educational attainment). The older view was that parent’s education is a valid instrument, the modern view is that it is not valid. We can test this dispute using a overidentification subset test.\nWe do this by estimating the wage equation by 2SLS using public, private, age, age \\(^{2}\\), father, and \\(^{2}\\) mother, as instruments for education, experience, and experience \\(\\left.{ }^{2} / 100\\right)\\). We do not report the parameter estimates here but observe that this model is overidentified with 3 overidentifying restrictions. We calculate the Sargan overidentification statistic. It is \\(7.9\\) with an asymptotic p-value (calculated using \\(\\chi_{3}^{2}\\) ) of \\(0.048\\). This is a mild rejection of the null hypothesis of correct specification. As we argued in the previous section this by itself is not reason to reject the model. Now we consider a subset overidentification test. We are interested in testing the validity of the two instruments father and mother, not the instruments public, private, age, \\(a g e^{2}\\). To test the hypothesis that these two instruments are uncorrelated with the structural error we compute the difference in Sargan statistic, \\(C=7.9-0.5=7.4\\), which has a p-value (calculated using \\(\\chi_{2}^{2}\\) ) of \\(0.025\\). This is marginally statistically significant, meaning that there is evidence that father and mother are not valid instruments for the wage equation. Since the \\(\\mathrm{p}\\)-value is not smaller than \\(1 %\\) it is not overwhelming evidence but it still supports Card’s decision to not use parental education as instruments for the wage equation. We now prove the results in Theorem 12.17.\nWe first show that \\(N=C^{*}\\). Define \\(\\boldsymbol{P}_{a}=\\boldsymbol{Z}_{a}\\left(\\boldsymbol{Z}_{a}^{\\prime} \\boldsymbol{Z}_{a}\\right)^{-1} \\boldsymbol{Z}_{a}^{\\prime}\\) and \\(\\boldsymbol{P}_{\\boldsymbol{R}}=\\boldsymbol{R}\\left(\\boldsymbol{R}^{\\prime} \\boldsymbol{R}\\right)^{-1} \\boldsymbol{R}^{\\prime}\\). Since \\(\\left[\\boldsymbol{Z}_{a}, \\boldsymbol{R}\\right]\\) span \\(\\boldsymbol{Z}\\) we find \\(\\boldsymbol{P}=\\boldsymbol{P}_{\\boldsymbol{R}}+\\boldsymbol{P}_{a}\\) and \\(\\boldsymbol{P}_{\\boldsymbol{R}} \\boldsymbol{P}_{a}=0\\). It will be useful to note that\n\\[\n\\begin{aligned}\n\\boldsymbol{P}_{R} \\widehat{\\boldsymbol{X}} &=\\boldsymbol{P}_{\\boldsymbol{R}} \\boldsymbol{P} \\boldsymbol{X}=\\boldsymbol{P}_{\\boldsymbol{R}} \\boldsymbol{X} \\\\\n\\widehat{\\boldsymbol{X}}^{\\prime} \\widehat{\\boldsymbol{X}}-\\widehat{\\boldsymbol{X}}^{\\prime} \\boldsymbol{P}_{\\boldsymbol{R}} \\widehat{\\boldsymbol{X}} &=\\boldsymbol{X}^{\\prime}\\left(\\boldsymbol{P}-\\boldsymbol{P}_{\\boldsymbol{R}}\\right) \\boldsymbol{X}=\\boldsymbol{X}^{\\prime} \\boldsymbol{P}_{a} \\boldsymbol{X}\n\\end{aligned}\n\\]\nThe fact that \\(\\boldsymbol{X}^{\\prime} \\boldsymbol{P} \\widehat{\\boldsymbol{e}}=\\widehat{\\boldsymbol{X}}^{\\prime} \\widehat{\\boldsymbol{e}}=0\\) implies \\(\\boldsymbol{X}^{\\prime} \\boldsymbol{P}_{\\boldsymbol{R}} \\widehat{\\boldsymbol{e}}=-\\boldsymbol{X}^{\\prime} \\boldsymbol{P}_{a} \\widehat{\\boldsymbol{e}}\\). Finally, since \\(\\boldsymbol{Y}=\\boldsymbol{X} \\widehat{\\boldsymbol{\\beta}}+\\widehat{\\boldsymbol{e}}\\),\n\\[\n\\widetilde{\\boldsymbol{e}}=\\left(\\boldsymbol{I}_{n}-\\boldsymbol{X}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{P}_{a} \\boldsymbol{X}\\right)^{-1} \\boldsymbol{X}^{\\prime} \\boldsymbol{P}_{a}\\right) \\widehat{\\boldsymbol{e}}\n\\]\nso\n\\[\n\\widetilde{\\boldsymbol{e}}^{\\prime} \\boldsymbol{P}_{a} \\widetilde{\\boldsymbol{e}}=\\widehat{\\boldsymbol{e}}^{\\prime}\\left(\\boldsymbol{P}_{a}-\\boldsymbol{P}_{a} \\boldsymbol{X}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{P}_{a} \\boldsymbol{X}\\right)^{-1} \\boldsymbol{X}^{\\prime} \\boldsymbol{P}_{a}\\right) \\widehat{\\boldsymbol{e}} .\n\\]\nApplying the Woodbury matrix equality to the definition of \\(N\\) and the above algebraic relationships,\n\\[\n\\begin{aligned}\nN &=\\frac{\\widehat{\\boldsymbol{e}}^{\\prime} \\boldsymbol{P}_{\\boldsymbol{R}} \\widehat{\\boldsymbol{e}}+\\widehat{\\boldsymbol{e}}^{\\prime} \\boldsymbol{P}_{\\boldsymbol{R}} \\widehat{\\boldsymbol{X}}\\left(\\widehat{\\boldsymbol{X}}^{\\prime} \\widehat{\\boldsymbol{X}}-\\widehat{\\boldsymbol{X}}^{\\prime} \\boldsymbol{P}_{\\boldsymbol{R}} \\widehat{\\boldsymbol{X}}\\right)^{-1} \\widehat{\\boldsymbol{X}}^{\\prime} \\boldsymbol{P}_{\\boldsymbol{R}} \\widehat{\\boldsymbol{e}}}{\\widehat{\\sigma}^{2}} \\\\\n&=\\frac{\\widehat{\\boldsymbol{e}}^{\\prime} \\boldsymbol{P} \\widehat{\\boldsymbol{e}}-\\widehat{\\boldsymbol{e}}^{\\prime} \\boldsymbol{P}_{a} \\widehat{\\boldsymbol{e}}+\\widehat{\\boldsymbol{e}}^{\\prime} \\boldsymbol{P}_{a} \\boldsymbol{X}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{P}_{a} \\boldsymbol{X}\\right)^{-1} \\boldsymbol{X}^{\\prime} \\boldsymbol{P}_{a} \\widehat{\\boldsymbol{e}}}{\\widehat{\\sigma}^{2}} \\\\\n&=\\frac{\\widehat{\\boldsymbol{e}}^{\\prime} \\boldsymbol{P} \\widehat{\\boldsymbol{e}}-\\widetilde{\\boldsymbol{e}}^{\\prime} \\boldsymbol{P}_{a} \\widetilde{\\boldsymbol{e}}}{\\widehat{\\sigma}^{2}} \\\\\n&=C^{*}\n\\end{aligned}\n\\]\nas claimed.\nWe next establish the asymptotic distribution. Since \\(\\boldsymbol{Z}_{a}\\) is a subset of \\(\\boldsymbol{Z}, \\boldsymbol{P}_{a}=\\boldsymbol{M}_{a} \\boldsymbol{P}\\), thus \\(\\boldsymbol{P} \\boldsymbol{R}=\\boldsymbol{R}\\) and \\(\\boldsymbol{R}^{\\prime} \\boldsymbol{X}=\\boldsymbol{R}^{\\prime} \\widehat{\\boldsymbol{X}}\\). Consequently\n\\[\n\\begin{aligned}\n\\frac{1}{\\sqrt{n}} \\boldsymbol{R}^{\\prime} \\widehat{\\boldsymbol{e}} &=\\frac{1}{\\sqrt{n}} \\boldsymbol{R}^{\\prime}(\\boldsymbol{Y}-\\boldsymbol{X} \\widehat{\\boldsymbol{\\beta}}) \\\\\n&=\\frac{1}{\\sqrt{n}} \\boldsymbol{R}^{\\prime}\\left(\\boldsymbol{I}_{n}-\\boldsymbol{X}\\left(\\widehat{\\boldsymbol{X}}^{\\prime} \\widehat{\\boldsymbol{X}}\\right)^{-1} \\widehat{\\boldsymbol{X}}^{\\prime}\\right) \\boldsymbol{e} \\\\\n&=\\frac{1}{\\sqrt{n}} \\boldsymbol{R}^{\\prime}\\left(\\boldsymbol{I}_{n}-\\widehat{\\boldsymbol{X}}\\left(\\widehat{\\boldsymbol{X}}^{\\prime} \\widehat{\\boldsymbol{X}}\\right)^{-1} \\widehat{\\boldsymbol{X}}^{\\prime}\\right) \\boldsymbol{e} \\\\\n& \\underset{d}{\\longrightarrow} \\mathrm{N}\\left(0, \\boldsymbol{V}_{2}\\right)\n\\end{aligned}\n\\]\nwhere\n\\[\n\\boldsymbol{V}_{2}=\\operatorname{plim}_{n \\rightarrow \\infty}\\left(\\frac{1}{n} \\boldsymbol{R}^{\\prime} \\boldsymbol{R}-\\frac{1}{n} \\boldsymbol{R}^{\\prime} \\widehat{\\boldsymbol{X}}\\left(\\frac{1}{n} \\widehat{\\boldsymbol{X}}^{\\prime} \\widehat{\\boldsymbol{X}}\\right)^{-1} \\frac{1}{n} \\widehat{\\boldsymbol{X}}^{\\prime} \\boldsymbol{R}\\right) .\n\\]\nIt follows that \\(N=C^{*} \\underset{d}{\\longrightarrow} \\chi_{\\ell_{b}}^{2}\\) as claimed. Since \\(C=C^{*}+o_{p}(1)\\) it has the same limiting distribution."
  },
  {
    "objectID": "chpt12-iv.html#bootstrap-overidentification-tests",
    "href": "chpt12-iv.html#bootstrap-overidentification-tests",
    "title": "12  Instrumental Variables",
    "section": "12.33 Bootstrap Overidentification Tests",
    "text": "12.33 Bootstrap Overidentification Tests\nIn small to moderate sample sizes the overidentification tests are not well approximated by the asymptotic chi-square distributions. For improved accuracy it is advised to use bootstrap critical values. The bootstrap for 2SLS (Section 12.23) can be used for this purpose but the bootstrap version of the overidentification statistic must be adjusted. This is because in the bootstrap universe the overidentified moment conditions are not satisfied. One solution is to center the moment conditions. For the 2SLS estimator the standard overidentification test is based on the Sargan statistic\n\\[\n\\begin{aligned}\n&S=n \\frac{\\widehat{\\boldsymbol{e}}^{\\prime} \\boldsymbol{Z}\\left(\\boldsymbol{Z}^{\\prime} \\boldsymbol{Z}\\right)^{-1} \\boldsymbol{Z}^{\\prime} \\widehat{\\boldsymbol{e}}}{\\widehat{\\boldsymbol{e}}^{\\prime} \\widehat{\\boldsymbol{e}}} \\\\\n&\\widehat{\\boldsymbol{e}}=\\boldsymbol{Y}-\\boldsymbol{X} \\widehat{\\beta}_{2 s l s}\n\\end{aligned}\n\\]\nThe recentered bootstrap analog is\n\\[\n\\begin{aligned}\nS^{* *} &=n \\frac{\\left(\\widehat{\\boldsymbol{e}}^{* \\prime} \\boldsymbol{Z}^{*}-\\boldsymbol{Z}^{\\prime} \\widehat{\\boldsymbol{e}}\\right)\\left(\\boldsymbol{Z}^{* \\prime} \\boldsymbol{Z}^{*}\\right)^{-1}\\left(\\boldsymbol{Z}^{* \\prime} \\widehat{\\boldsymbol{e}}^{*}-\\boldsymbol{Z}^{\\prime} \\widehat{\\boldsymbol{e}}\\right)}{\\widehat{\\boldsymbol{e}}^{*} \\widehat{\\boldsymbol{e}}^{*}} \\\\\n\\widehat{\\boldsymbol{e}}^{*} &=\\boldsymbol{Y}^{*}-\\boldsymbol{X}^{*} \\widehat{\\beta}_{2 \\mathrm{sls}}^{*}\n\\end{aligned}\n\\]\nOn each bootstrap sample \\(S^{* *}(b)\\) is calculated and stored. The bootstrap p-value is\n\\[\np^{*}=\\frac{1}{B} \\sum_{b=1}^{B} \\mathbb{1}\\left\\{S^{* *}(b)>S\\right\\} .\n\\]\nThis bootstrap \\(\\mathrm{p}\\)-value is valid because the statistic \\(S^{* *}\\) satisfies the overidentified moment conditions."
  },
  {
    "objectID": "chpt12-iv.html#local-average-treatment-effects",
    "href": "chpt12-iv.html#local-average-treatment-effects",
    "title": "12  Instrumental Variables",
    "section": "12.34 Local Average Treatment Effects",
    "text": "12.34 Local Average Treatment Effects\nIn a pair of influential papers, Imbens and Angrist (1994) and Angrist, Imbens and Rubin (1996) proposed an new interpretation of the instrumental variables estimator using the potential outcomes model introduced in Section 2.30.\nWe will restrict attention to the case that the endogenous regressor \\(X\\) and excluded instrument \\(Z\\) are binary variables. We write the model as a pair of potential outcome functions. The dependent variable \\(Y\\) is a function of the regressor and an unobservable vector \\(U, Y=h(X, U)\\), and the endogenous regressor \\(X\\) is a function of the instrument \\(Z\\) and \\(U, X=g(Z, U)\\). By specifying \\(U\\) as a vector there is no loss of generality in letting both equations depend on \\(U\\).\nIn this framework the outcomes are determined by the random vector \\(U\\) and the exogenous instrument \\(Z\\). This determines \\(X\\) which determines \\(Y\\). To put this in the context of the college proximity example the variable \\(U\\) is everything specific about an individual. Given college proximity \\(Z\\) the person decides to attend college or not. The person’s wage is determined by the individual attributes \\(U\\) as well as college attendence \\(X\\) but is not directly affected by college proximity \\(Z\\).\nWe can omit the random variable \\(U\\) from the notation as follows. An individual has a realization \\(U\\). We then set \\(Y(x)=h(x, U)\\) and \\(X(z)=g(z, U)\\). Also, given a realization \\(Z\\) the observables are \\(X=X(Z)\\) and \\(Y=Y(X)\\).\nIn this model the causal effect of college for an individual is \\(C=Y(1)-Y(0)\\). As discussed in Section \\(2.30\\), this is individual-specific and random.\nWe would like to learn about the distribution of the causal effects, or at least features of the distribution. A common feature of interest is the average treatment effect (ATE)\n\\[\n\\operatorname{ATE}=\\mathbb{E}[C]=\\mathbb{E}[Y(1)-Y(0)] .\n\\]\nThis, however, it typically not feasible to estimate allowing for endogenous \\(X\\) without strong assumptions (such as that the causal effect \\(C\\) is constant across individuals). The treatment effect literature has explored what features of the distribution of \\(C\\) can be estimated. One particular feature of interest emphasized by Imbens and Angrist (1994) is the local average treatment effect (LATE). Roughly, this is the average effect upon those effected by the instrumental variable. To understand LATE, consider the college proximity example. In the potential outcomes framework each person is fully characterized by their individual unobservable \\(U\\). Given \\(U\\), their decision to attend college is a function of the proximity indicator \\(Z\\). For some students, proximity has no effect on their decision. For other students, it has an effect in the specific sense that given \\(Z=1\\) they choose to attend college while if \\(Z=0\\) they choose to not attend. We can summarize the possibilites with the following chart which is based on labels developed by Angrist, Imbens and Rubin (1996).\n\\[\n\\begin{array}{ccc}\n& X(0)=0 & X(0)=1 \\\\\nX(1)=0 & \\text { Never Takers } & \\text { Defiers } \\\\\nX(1)=1 & \\text { Compliers } & \\text { Always Takers }\n\\end{array}\n\\]\nThe columns indicate the college attendence decision given \\(Z=0\\) (not close to a college). The rows indicate the college attendence decision given \\(Z=1\\) (close to a college). The four entries are labels for the four types of individuals based on these decisions. The upper-left entry are the individuals who do not attend college regardless of \\(Z\\). They are called “Never Takers”. The lower-right entry are the individuals who conversely attend college regardless of \\(Z\\). They are called “Always Takers”. The bottom left are the individuals who only attend college if they live close to one. They are called “Compliers”. The upper right entry is a bit of a challenge. These are individuals who attend college only if they do not live close to one. They are called “Dediers”. Imbens and Angrist discovered that to identify the parameters of interest we need to assume that there are no Dediers, or equivalently that \\(X(1) \\geq X(0)\\). They call this a “monotonicity” condition - increasing the instrument does not decrease \\(X\\) for any individual.\nAs another example, suppose we are interested in the effect of wearing a face mask \\(X\\) on health \\(Y\\) during a virus pandemic. Wearing a face mask is a choice made by the individual so should be viewed as endogenous. For an instrument \\(Z\\) consider a government policy that requires face masks to be worn in public. The “Compliers” are those who wear a face mask if there is a policy but otherwise do not. The “Deniers” are those who do the converse. That is, these individuals would have worn a face mask based on the evidence of a pandemic but rebel against a government policy. Once again, identification requires that there are no Deniers.\nWe can distinguish the types in the table by the relative values of \\(X(1)-X(0)\\). For Never-Takers and Always-Takers \\(X(1)-X(0)=0\\), while for Compliers \\(X(1)-X(0)=1\\).\nWe are interested in the causal effect \\(C=h(1, U)-h(0, U)\\) of college on wages. The average causal effect (ACE) is its expectation \\(\\mathbb{E}[Y(1)-Y(0)]\\). To estimate the ACE we need observations of both \\(Y(0)\\) and \\(Y\\) (1) which means we need to observe some individuals who attend college and some who do not attend college. Consider the group “Never-Takers”. They never attend college so we only observe \\(Y(0)\\). It is thus impossible to estimate the ACE of college for this group. Similarly consider the group “Always-Takers”. They always attend college so we only observe \\(Y(1)\\) and again we cannot estimate the ACE of college for this group. The group for which we can estimate the ACE are the “Compliers”. The ACE for this group is\n\\[\n\\text { LATE }=\\mathbb{E}[Y(1)-Y(0) \\mid X(1)>X(0)] .\n\\]\nImbens and Angrist call this the local average treatment effect (LATE) as it is the average treatment effect for the sub-population whose endogenous regressor is affected by the instrument. Examining the definition, the LATE is the average causal effect of college attendence on wages for the sub-sample of individuals who choose to attend college if (and only if) they live close to one.\nInterestingly, we show below that\n\\[\n\\text { LATE }=\\frac{\\mathbb{E}[Y \\mid Z=1]-\\mathbb{E}[Y \\mid Z=0]}{\\mathbb{E}[X \\mid Z=1]-\\mathbb{E}[X \\mid Z=0]} .\n\\]\nThat is, LATE equals the Wald expression (12.27) for the slope coefficient in the IV regression model. This means that the standard IV estimator is an estimator of LATE. Thus when treatment effects are potentially heterogeneous we can interpret IV as an estimator of LATE. The equality (12.68) occurs under the following conditions.\nAssumption 12.3 \\(U\\) and \\(Z\\) are independent and \\(\\mathbb{P}[X(1)-X(0)<0]=0 .\\)\nOne interesting feature about LATE is that its value can depend on the instrument \\(Z\\) and the distribution of causal effects \\(C\\) in the population. To make this concrete suppose that instead of the Card proximity instrument we consider an instrument based on the financial cost of local college attendence. It is reasonable to expect that while the set of students affected by these two instruments are similar the two sets of students will not be the same. That is, some students may be responsive to proximity but not finances, and conversely. If the causal effect \\(C\\) has a different average in these two groups of students then LATE will be different when calculated with these two instruments. Thus LATE can vary by the choice of instrument.\nHow can that be? How can a well-defined parameter depend on the choice of instrument? Doesn’t this contradict the basic IV regression model? The answer is that the basic IV regression model is restrictive - it specifies that the causal effect \\(\\beta\\) is common across all individuals. Its value is the same regardless of the choice of specific instrument (so long as it satisfies the instrumental variables assumptions). In contrast, the potential outcomes framework is more general allowing for the causal effect to vary across individuals. What this analysis shows us is that in this context is quite possible for the LATE coefficient to vary by instrument. This occurs when causal effects are heterogeneous.\nOne implication of the LATE framework is that IV estimates should be interpreted as causal effects only for the population of compliers. Interpretation should focus on the population of potential compliers and extension to other populations should be done with caution. For example, in the Card proximity model the IV estimates of the causal return to schooling presented in Table \\(12.1\\) should be interpreted as applying to the population of students who are incentivized to attend college by the presence of a college within their home county. The estimates should not be applied to other students.\nFormally, the analysis of this section examined the case of a binary instrument and endogenous regressor. How does this generalize? Suppose that the regressor \\(X\\) is discrete, taking \\(J+1\\) discrete values. We can then rewrite the model as one with \\(J\\) binary endogenous regressors. If we then have \\(J\\) binary instruments we are back in the Imbens-Angrist framework (assuming the instruments have a monotonic impact on the endogenous regressors). A benefit is that with a larger set of instruments it is plausible that the set of compliers in the population is expanded.\nWe close this section by showing (12.68) under Assumption 12.3. The realized value of \\(X\\) can be written as\n\\[\nX=(1-Z) X(0)+Z X(1)=X(0)+Z(X(1)-X(0))\n\\]\nSimilarly\n\\[\nY=Y(0)+X(Y(1)-Y(0))=Y(0)+X C .\n\\]\nCombining,\n\\[\nY=Y(0)+X(0) C+Z(X(1)-Y(0)) C .\n\\]\nThe independence of \\(u\\) and \\(Z\\) implies independence of \\((Y(0), Y(1), X(0), X(1), C)\\) and \\(Z\\). Thus\n\\[\n\\mathbb{E}[Y \\mid Z=1]=\\mathbb{E}[Y(0)]+\\mathbb{E}[X(0) C]+\\mathbb{E}[(X(1)-X(0)) C]\n\\]\nand\n\\[\n\\mathbb{E}[Y \\mid Z=0]=\\mathbb{E}[Y(0)]+\\mathbb{E}[X(0) C] .\n\\]\nSubtracting we obtain\n\\[\n\\begin{aligned}\n\\mathbb{E}[Y \\mid Z=1]-\\mathbb{E}[Y \\mid Z=0] &=\\mathbb{E}[(X(1)-X(0)) C] \\\\\n&=1 \\times \\mathbb{E}[C \\mid X(1)-X(0)=1] \\mathbb{P}[X(1)-X(0)=1] \\\\\n&+0 \\times \\mathbb{E}[C \\mid X(1)-X(0)=0] \\mathbb{P}[X(1)-X(0)=0] \\\\\n&+(-1) \\times \\mathbb{E}[C \\mid X(1)-X(0)=-1] \\mathbb{P}[X(1)-X(0)=-1] \\\\\n&=\\mathbb{E}[C \\mid X(1)-X(0)=1](\\mathbb{E}[X \\mid X=1]-\\mathbb{E}[X \\mid Z=0])\n\\end{aligned}\n\\]\nwhere the final equality uses \\(\\mathbb{P}[X(1)-X(0)<0]=0\\) and\n\\[\n\\mathbb{P}[X(1)-X(0)=1]=\\mathbb{E}[X(1)-X(0)]=\\mathbb{E}[X \\mid Z=1]-\\mathbb{E}[X \\mid Z=0] .\n\\]\nRearranging\n\\[\n\\mathrm{LATE}=\\mathbb{E}[C \\mid X(1)-X(0)=1]=\\frac{\\mathbb{E}[Y \\mid Z=1]-\\mathbb{E}[Y \\mid Z=0]}{\\mathbb{E}[X \\mid Z=1]-\\mathbb{E}[X \\mid Z=0]}\n\\]\nas claimed."
  },
  {
    "objectID": "chpt12-iv.html#identification-failure",
    "href": "chpt12-iv.html#identification-failure",
    "title": "12  Instrumental Variables",
    "section": "12.35 Identification Failure",
    "text": "12.35 Identification Failure\nRecall the reduced form equation\n\\[\nX_{2}=\\Gamma_{12}^{\\prime} Z_{1}+\\Gamma_{22}^{\\prime} Z_{2}+u_{2} .\n\\]\nThe parameter \\(\\beta\\) fails to be identified if \\(\\Gamma_{22}\\) has deficient rank. The consequences of identification failure for inference are quite severe.\nTake the simplest case where \\(k_{1}=0\\) and \\(k_{2}=\\ell_{2}=1\\). Then the model may be written as\n\\[\n\\begin{aligned}\n&Y=X \\beta+e \\\\\n&X=Z \\gamma+u\n\\end{aligned}\n\\]\nand \\(\\Gamma_{22}=\\gamma=\\mathbb{E}[Z X] / \\mathbb{E}\\left[Z^{2}\\right]\\). We see that \\(\\beta\\) is identified if and only if \\(\\gamma \\neq 0\\), which occurs when \\(\\mathbb{E}[X Z] \\neq 0\\). Thus identification hinges on the existence of correlation between the excluded exogenous variable and the included endogenous variable.\nSuppose this condition fails. In this case \\(\\gamma=0\\) and \\(\\mathbb{E}[X Z]=0\\). We now analyze the distribution of the least squares and IV estimators of \\(\\beta\\). For simplicity we assume conditional homoskedasticity and normalize the variances of \\(e, u\\), and \\(Z\\) to unity. Thus\n\\[\n\\operatorname{var}\\left[\\left(\\begin{array}{c}\ne \\\\\nu\n\\end{array}\\right) \\mid Z\\right]=\\left(\\begin{array}{ll}\n1 & \\rho \\\\\n\\rho & 1\n\\end{array}\\right) .\n\\]\nThe errors have non-zero correlation \\(\\rho \\neq 0\\) when the variables are endogenous.\nBy the CLT we have the joint convergence\n\\[\n\\frac{1}{\\sqrt{n}} \\sum_{i=1}^{n}\\left(\\begin{array}{c}\nZ_{i} e_{i} \\\\\nZ_{i} u_{i}\n\\end{array}\\right) \\underset{d}{ }\\left(\\begin{array}{l}\n\\xi_{1} \\\\\n\\xi_{2}\n\\end{array}\\right) \\sim \\mathrm{N}\\left(0,\\left(\\begin{array}{cc}\n1 & \\rho \\\\\n\\rho & 1\n\\end{array}\\right)\\right) .\n\\]\nIt is convenient to define \\(\\xi_{0}=\\xi_{1}-\\rho \\xi_{2}\\) which is normal and independent of \\(\\xi_{2}\\). As a benchmark it is useful to observe that the least squares estimator of \\(\\beta\\) satisfies\n\\[\n\\widehat{\\beta}_{\\mathrm{ols}}-\\beta=\\frac{n^{-1} \\sum_{i=1}^{n} u_{i} e_{i}}{n^{-1} \\sum_{i=1}^{n} u_{i}^{2}} \\underset{p}{\\longrightarrow} \\rho \\neq 0\n\\]\nso endogeneity causes \\(\\widehat{\\beta}_{\\text {ols }}\\) to be inconsistent for \\(\\beta\\).\nUnder identification failure \\(\\gamma=0\\) the asymptotic distribution of the IV estimator is\n\\[\n\\widehat{\\beta}_{\\mathrm{iv}}-\\beta=\\frac{\\frac{1}{\\sqrt{n}} \\sum_{i=1}^{n} Z_{i} e_{i}}{\\frac{1}{\\sqrt{n}} \\sum_{i=1}^{n} Z_{i} X_{i}} \\underset{\\mathrm{d}}{\\xi_{2}}=\\rho+\\frac{\\xi_{0}}{\\xi_{2}} .\n\\]\nThis asymptotic convergence result uses the continuous mapping theorem which applies since the function \\(\\xi_{1} / \\xi_{2}\\) is continuous everywhere except at \\(\\xi_{2}=0\\), which occurs with probability equal to zero.\nThis limiting distribution has several notable features.\nFirst, \\(\\widehat{\\beta}_{\\mathrm{iv}}\\) does not converge in probability to a limit, rather it converges in distribution to a random variable. Thus the IV estimator is inconsistent. Indeed, it is not possible to consistently estimate an unidentified parameter and \\(\\beta\\) is not identified when \\(\\gamma=0\\).\nSecond, the ratio \\(\\xi_{0} / \\xi_{2}\\) is symmetrically distributed about zero so the median of the limiting distribution of \\(\\widehat{\\beta}_{\\text {iv }}\\) is \\(\\beta+\\rho\\). This means that the IV estimator is median biased under endogeneity. Thus under identification failure the IV estimator does not correct the centering (median bias) of least squares.\nThird, the ratio \\(\\xi_{0} / \\xi_{2}\\) of two independent normal random variables is Cauchy distributed. This is particularly nasty as the Cauchy distribution does not have a finite mean. The distribution has thick tails meaning that extreme values occur with higher frequency than the normal. Inferences based on the normal distribution can be quite incorrect.\nTogether, these results show that \\(\\gamma=0\\) renders the IV estimator particularly poorly behaved - it is inconsistent, median biased, and non-normally distributed.\nWe can also examine the behavior of the t-statistic. For simplicity consider the classical (homoskedastic) t-statistic. The error variance estimate has the asymptotic distribution\n\\[\n\\begin{aligned}\n& \\widehat{\\sigma}^{2}=\\frac{1}{n} \\sum_{i=1}^{n}\\left(Y_{i}-X_{i} \\widehat{\\beta}_{\\mathrm{iv}}\\right)^{2} \\\\\n& =\\frac{1}{n} \\sum_{i=1}^{n} e_{i}^{2}-\\frac{2}{n} \\sum_{i=1}^{n} e_{i} X_{i}\\left(\\widehat{\\beta}_{\\mathrm{iv}}-\\beta\\right)+\\frac{1}{n} \\sum_{i=1}^{n} X_{i}^{2}\\left(\\widehat{\\beta}_{\\mathrm{iv}}-\\beta\\right)^{2} \\\\\n& \\underset{d}{\\longrightarrow} 1-2 \\rho \\frac{\\xi_{1}}{\\xi_{2}}+\\left(\\frac{\\xi_{1}}{\\xi_{2}}\\right)^{2} \\text {. }\n\\end{aligned}\n\\]\nThus the t-statistic has the asymptotic distribution\n\nThe limiting distribution is non-normal, meaning that inference using the normal distribution will be (considerably) incorrect. This distribution depends on the correlation \\(\\rho\\). The distortion is increasing in \\(\\rho\\). Indeed as \\(\\rho \\rightarrow 1\\) we have \\(\\xi_{1} / \\xi_{2} \\rightarrow p 1\\) and the unexpected finding \\(\\widehat{\\sigma}^{2} \\rightarrow{ }_{p} 0\\). The latter means that the conventional standard error \\(s\\left(\\widehat{\\beta}_{\\text {iv }}\\right)\\) for \\(\\widehat{\\beta}_{\\text {iv }}\\) also converges in probability to zero. This implies that the t-statistic diverges in the sense \\(|T| \\rightarrow p \\infty\\). In this situations users may incorrectly interpret estimates as precise despite the fact that they are highly imprecise."
  },
  {
    "objectID": "chpt12-iv.html#weak-instruments",
    "href": "chpt12-iv.html#weak-instruments",
    "title": "12  Instrumental Variables",
    "section": "12.36 Weak Instruments",
    "text": "12.36 Weak Instruments\nIn the previous section we examined the extreme consequences of full identification failure. Similar problems occur when identification is weak in the sense that the reduced form coefficients are of small magnitude. In this section we derive the asymptotic distribution of the OLS, 2SLS, and LIML estimators when the reduced form coefficients are treated as weak. We show that the estimators are inconsistent and the 2SLS and LIML estimators remain random in large samples.\nTo simplify the exposition we assume that there are no included exogenous variables (no \\(X_{1}\\) ) so we write \\(X_{2}, Z_{2}\\), and \\(\\beta_{2}\\) simply as \\(X, Z\\), and \\(\\beta\\). The model is\n\\[\n\\begin{aligned}\n&Y=X^{\\prime} \\beta+e \\\\\n&X=\\Gamma^{\\prime} Z+u_{2} .\n\\end{aligned}\n\\]\nRecall the reduced form error vector \\(u=\\left(u_{1}, u_{2}\\right)\\) and its covariance matrix\n\\[\n\\mathbb{E}\\left[u u^{\\prime}\\right]=\\Sigma=\\left[\\begin{array}{cc}\n\\Sigma_{11} & \\Sigma_{12} \\\\\n\\Sigma_{21} & \\Sigma_{22}\n\\end{array}\\right] .\n\\]\nRecall that the structural error is \\(e=u_{1}-\\beta^{\\prime} u_{2}=\\gamma^{\\prime} u\\) where \\(\\gamma=(1,-\\beta)\\) which has variance \\(\\mathbb{E}\\left[e^{2} \\mid Z\\right]=\\) \\(\\gamma^{\\prime} \\Sigma \\gamma\\). Also define the covariance \\(\\Sigma_{2 e}=\\mathbb{E}\\left[u_{2} e \\mid Z\\right]=\\Sigma_{21}-\\Sigma_{22} \\beta\\).\nIn Section \\(12.35\\) we assumed complete identification failure in the sense that \\(\\Gamma=0\\). We now want to assume that identification does not completely fail but is weak in the sense that \\(\\Gamma\\) is small. A rich asymptotic distribution theory has been developed to understand this setting by modeling \\(\\Gamma\\) as “localto-zero”. The seminal contribution is Staiger and Stock (1997). The theory was extended to nonlinear GMM estimation by Stock and Wright (2000).\nThe technical device introduced by Staiger and Stock (1997) is to assume that the reduced form parameter is local-to-zero, specifically\n\\[\n\\Gamma=n^{-1 / 2} \\boldsymbol{C}\n\\]\nwhere \\(\\boldsymbol{C}\\) is a free matrix. The \\(n^{-1 / 2}\\) scaling is picked because it provides just the right balance to allow a useful distribution theory. The local-to-zero assumption (12.71) is not meant to be taken literally but rather is meant to be a useful distributional approximation. The parameter \\(\\boldsymbol{C}\\) indexes the degree of identification. Larger \\(\\|\\boldsymbol{C}\\|\\) implies stronger identification; smaller \\(\\|\\boldsymbol{C}\\|\\) implies weaker identification.\nWe now derive the asymptotic distribution of the least squares, 2SLS, and LIML estimators under the local-to-unity assumption (12.71).\nThe least squares estimator satisfies\n\\[\n\\begin{aligned}\n\\widehat{\\beta}_{\\mathrm{ols}}-\\beta &=\\left(n^{-1} \\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1}\\left(n^{-1} \\boldsymbol{X}^{\\prime} \\boldsymbol{e}\\right) \\\\\n&=\\left(n^{-1} \\boldsymbol{U}_{2}^{\\prime} \\boldsymbol{U}_{2}\\right)^{-1}\\left(n^{-1} \\boldsymbol{U}_{2}^{\\prime} \\boldsymbol{e}\\right)+o_{p}(1) \\\\\n& \\longrightarrow \\underset{22}{-1} \\Sigma_{2 e} .\n\\end{aligned}\n\\]\nThus the least squares estimator is inconsistent for \\(\\beta\\).\nTo examine the 2SLS estimator, by the central limit theorem\n\\[\n\\frac{1}{\\sqrt{n}} \\sum_{i=1}^{n} Z_{i} u_{i}^{\\prime} \\underset{d}{\\longrightarrow} \\xi=\\left[\\xi_{1}, \\xi_{2}\\right]\n\\]\nwhere\n\\[\n\\operatorname{vec}(\\xi) \\sim \\mathrm{N}\\left(0, \\mathbb{E}\\left[u u^{\\prime} \\otimes Z Z^{\\prime}\\right]\\right)\n\\]\nThis implies\n\\[\n\\frac{1}{\\sqrt{n}} \\boldsymbol{Z}^{\\prime} \\boldsymbol{e} \\longrightarrow \\underset{d}{\\xi_{e}}=\\xi \\gamma\n\\]\nWe also find that\n\\[\n\\frac{1}{\\sqrt{n}} \\boldsymbol{Z}^{\\prime} \\boldsymbol{X}=\\frac{1}{n} \\boldsymbol{Z}^{\\prime} \\boldsymbol{Z} \\boldsymbol{C}+\\frac{1}{\\sqrt{n}} \\boldsymbol{Z}^{\\prime} \\boldsymbol{U}_{2} \\underset{d}{\\longrightarrow} \\boldsymbol{Q}_{Z} \\boldsymbol{C}+\\xi_{2} .\n\\]\nThus\n\\[\n\\boldsymbol{X}^{\\prime} \\boldsymbol{P}_{\\boldsymbol{Z}} \\boldsymbol{X}=\\left(\\frac{1}{\\sqrt{n}} \\boldsymbol{X}^{\\prime} \\boldsymbol{Z}\\right)\\left(\\frac{1}{n} \\boldsymbol{Z}^{\\prime} \\boldsymbol{Z}\\right)^{-1}\\left(\\frac{1}{\\sqrt{n}} \\boldsymbol{Z}^{\\prime} \\boldsymbol{X}\\right) \\underset{d}{\\longrightarrow}\\left(\\boldsymbol{Q}_{Z} \\boldsymbol{C}+\\xi_{2}\\right)^{\\prime} \\boldsymbol{Q}_{Z}^{-1}\\left(\\boldsymbol{Q}_{Z} \\boldsymbol{C}+\\xi_{2}\\right)\n\\]\nand\n\\[\n\\boldsymbol{X}^{\\prime} \\boldsymbol{P}_{\\boldsymbol{Z}} \\boldsymbol{e}=\\left(\\frac{1}{\\sqrt{n}} \\boldsymbol{X}^{\\prime} \\boldsymbol{Z}\\right)\\left(\\frac{1}{n} \\boldsymbol{Z}^{\\prime} \\boldsymbol{Z}\\right)^{-1}\\left(\\frac{1}{\\sqrt{n}} \\boldsymbol{Z}^{\\prime} \\boldsymbol{e}\\right) \\underset{d}{\\longrightarrow}\\left(\\boldsymbol{Q}_{Z} \\boldsymbol{C}+\\xi_{2}\\right)^{\\prime} \\boldsymbol{Q}_{Z}^{-1} \\xi_{e}\n\\]\nWe find that the 2SLS estimator has the asymptotic distribution\n\\[\n\\begin{aligned}\n\\widehat{\\beta}_{2 \\text { sls }}-\\beta &=\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{P}_{\\boldsymbol{Z}} \\boldsymbol{X}\\right)^{-1}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{P}_{\\boldsymbol{Z}} \\boldsymbol{e}\\right) \\\\\n& \\longrightarrow\\left(\\left(\\boldsymbol{Q}_{Z} \\boldsymbol{C}+\\xi_{2}\\right)^{\\prime} \\boldsymbol{Q}_{Z}^{-1}\\left(\\boldsymbol{Q}_{Z} \\boldsymbol{C}+\\xi_{2}\\right)\\right)^{-1}\\left(\\boldsymbol{Q}_{Z} \\boldsymbol{C}+\\xi_{2}\\right)^{\\prime} \\boldsymbol{Q}_{Z}^{-1} \\xi_{e} .\n\\end{aligned}\n\\]\nAs in the case of complete identification failure we find that \\(\\widehat{\\beta}_{2 s l s}\\) is inconsistent for \\(\\beta\\), it is asymptotically random, and its asymptotic distribution is non-normal. The distortion is affected by the coefficient \\(\\boldsymbol{C}\\). As \\(\\|\\boldsymbol{C}\\| \\rightarrow \\infty\\) the distribution in (12.72) converges in probability to zero suggesting that \\(\\widehat{\\beta}_{2 \\text { sls }}\\) is consistent for \\(\\beta\\). This corresponds to the classic “strong identification” context.\nNow consider the LIML estimator. The reduced form is \\(\\overrightarrow{\\boldsymbol{Y}}=\\boldsymbol{Z \\Pi}+\\boldsymbol{U}\\). This implies \\(\\boldsymbol{M}_{Z} \\overrightarrow{\\boldsymbol{Y}}=\\boldsymbol{M}_{Z} \\boldsymbol{U}\\) and by standard asymptotic theory\n\\[\n\\frac{1}{n} \\overrightarrow{\\boldsymbol{Y}}^{\\prime} \\boldsymbol{M}_{\\boldsymbol{Z}} \\overrightarrow{\\boldsymbol{Y}}=\\frac{1}{n} \\boldsymbol{U}^{\\prime} \\boldsymbol{M}_{\\boldsymbol{Z}} \\boldsymbol{U} \\underset{p}{\\longrightarrow}=\\mathbb{E}\\left[u u^{\\prime}\\right] .\n\\]\nDefine \\(\\bar{\\beta}=\\left[\\beta, \\boldsymbol{I}_{k}\\right]\\) so that the reduced form coefficients equal \\(\\Pi=[\\boldsymbol{\\Gamma} \\beta, \\boldsymbol{\\Gamma}]=n^{-1 / 2} \\boldsymbol{C} \\bar{\\beta}\\). Then\n\\[\n\\frac{1}{\\sqrt{n}} \\boldsymbol{Z}^{\\prime} \\overrightarrow{\\boldsymbol{Y}}=\\frac{1}{n} \\boldsymbol{Z}^{\\prime} \\boldsymbol{Z} \\boldsymbol{C} \\bar{\\beta}+\\frac{1}{\\sqrt{n}} \\boldsymbol{Z}^{\\prime} \\boldsymbol{U} \\underset{d}{\\longrightarrow} \\boldsymbol{Q}_{Z} \\boldsymbol{C} \\bar{\\beta}+\\xi\n\\]\nand\n\\[\n\\overrightarrow{\\boldsymbol{Y}}^{\\prime} \\boldsymbol{Z}\\left(\\boldsymbol{Z}^{\\prime} \\boldsymbol{Z}\\right)^{-1} \\boldsymbol{Z}^{\\prime} \\overrightarrow{\\boldsymbol{Y}} \\underset{d}{\\longrightarrow}\\left(\\boldsymbol{Q}_{Z} \\boldsymbol{C} \\bar{\\beta}+\\xi\\right)^{\\prime} \\boldsymbol{Q}_{Z}^{-1}\\left(\\boldsymbol{Q}_{Z} \\boldsymbol{C} \\bar{\\beta}+\\xi\\right) .\n\\]\nThis allows us to calculate that by the continuous mapping theorem\n\\[\n\\begin{aligned}\nn \\widehat{\\mu} &=\\min _{\\gamma} \\frac{\\gamma^{\\prime} \\overrightarrow{\\boldsymbol{Y}}^{\\prime} \\boldsymbol{Z}\\left(\\boldsymbol{Z}^{\\prime} \\boldsymbol{Z}\\right)^{-1} \\boldsymbol{Z}^{\\prime} \\overrightarrow{\\boldsymbol{Y}} \\gamma}{\\gamma^{\\prime} \\frac{1}{n} \\overrightarrow{\\boldsymbol{Y}}^{\\prime} \\boldsymbol{M}_{\\boldsymbol{Z}} \\overrightarrow{\\boldsymbol{Y}} \\gamma} \\\\\n& \\underset{d}{\\longrightarrow} \\min _{\\gamma} \\frac{\\gamma^{\\prime}\\left(\\boldsymbol{Q}_{Z} \\boldsymbol{C} \\bar{\\beta}+\\xi\\right)^{\\prime} \\boldsymbol{Q}_{Z}^{-1}\\left(\\boldsymbol{Q}_{Z} \\boldsymbol{C} \\bar{\\beta}+\\xi\\right) \\gamma}{\\gamma^{\\prime} \\Sigma \\gamma} \\\\\n&=\\mu^{*}\n\\end{aligned}\n\\]\nsay, which is a function of \\(\\xi\\) and thus random. We deduce that the asymptotic distribution of the LIML estimator is\n\\[\n\\begin{aligned}\n\\widehat{\\beta}_{\\mathrm{liml}}-\\beta=&\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{P}_{\\boldsymbol{Z}} \\boldsymbol{X}-n \\widehat{\\mu} \\frac{1}{n} \\boldsymbol{X}^{\\prime} \\boldsymbol{M}_{\\boldsymbol{Z}} \\boldsymbol{X}\\right)^{-1}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{P}_{\\boldsymbol{Z}} \\boldsymbol{e}-n \\widehat{\\mu} \\frac{1}{n} \\boldsymbol{X}^{\\prime} \\boldsymbol{M}_{\\boldsymbol{Z}} \\boldsymbol{e}\\right) \\\\\n\\underset{d}{\\longrightarrow}\\left(\\left(\\boldsymbol{Q}_{Z} \\boldsymbol{C}+\\xi_{2}\\right)^{\\prime} \\boldsymbol{Q}_{Z}^{-1}\\left(\\boldsymbol{Q}_{Z} \\boldsymbol{C}+\\xi_{2}\\right)-\\mu^{*} \\Sigma_{22}\\right)^{-1}\\left(\\left(\\boldsymbol{Q}_{Z} \\boldsymbol{C}+\\xi_{2}\\right)^{\\prime} \\boldsymbol{Q}_{Z}^{-1} \\xi_{e}-\\mu^{*} \\Sigma_{2 e}\\right) .\n\\end{aligned}\n\\]\nSimilarly to 2SLS, the LIML estimator is inconsistent for \\(\\beta\\), is asymptotically random, and non-normally distributed.\nWe summarize.\nTheorem 12.18 Under (12.71),\n\\[\n\\begin{gathered}\n\\widehat{\\beta}_{\\mathrm{ols}}-\\beta \\underset{p}{\\longrightarrow} \\Sigma_{22}^{-1} \\Sigma_{2 e} \\\\\n\\widehat{\\beta}_{2 \\mathrm{sls}}-\\beta \\underset{d}{\\longrightarrow}\\left(\\left(\\boldsymbol{Q}_{Z} \\boldsymbol{C}+\\xi_{2}\\right)^{\\prime} \\boldsymbol{Q}_{Z}^{-1}\\left(\\boldsymbol{Q}_{Z} \\boldsymbol{C}+\\xi_{2}\\right)\\right)^{-1}\\left(\\boldsymbol{Q}_{Z} \\boldsymbol{C}+\\xi_{2}\\right)^{\\prime} \\boldsymbol{Q}_{Z}^{-1} \\xi_{e}\n\\end{gathered}\n\\]\nand\n\\[\n\\begin{aligned}\n&\\widehat{\\beta}_{\\mathrm{liml}}-\\beta \\underset{d}{\\longrightarrow}\\left(\\left(\\boldsymbol{Q}_{Z} \\boldsymbol{C}+\\xi_{2}\\right)^{\\prime} \\boldsymbol{Q}_{Z}^{-1}\\left(\\boldsymbol{Q}_{Z} \\boldsymbol{C}+\\xi_{2}\\right)-\\mu^{*} \\Sigma_{22}\\right)^{-1} \\\\\n&\\times\\left(\\left(\\boldsymbol{Q}_{Z} \\boldsymbol{C}+\\xi_{2}\\right)^{\\prime} \\boldsymbol{Q}_{Z}^{-1} \\xi_{e}-\\mu^{*} \\boldsymbol{\\Sigma}_{2 e}\\right)\n\\end{aligned}\n\\]\nwhere\n\\[\n\\mu^{*}=\\min _{\\gamma} \\frac{\\gamma^{\\prime}\\left(\\boldsymbol{Q}_{Z} \\boldsymbol{C} \\bar{\\beta}+\\xi\\right)^{\\prime} \\boldsymbol{Q}_{Z}^{-1}\\left(\\boldsymbol{Q}_{Z} \\boldsymbol{C} \\bar{\\beta}+\\xi\\right) \\gamma}{\\gamma^{\\prime} \\Sigma \\gamma}\n\\]\nand \\(\\bar{\\beta}=\\left[\\beta, I_{k}\\right]\\)\nAll three estimators are inconsistent. The 2SLS and LIML estimators are asymptotically random with non-standard distributions, similar to the asymptotic distribution of the IV estimator under complete identification failure explored in the previous section. The difference under weak identification is the presence of the coefficient matrix \\(\\boldsymbol{C}\\)."
  },
  {
    "objectID": "chpt12-iv.html#many-instruments",
    "href": "chpt12-iv.html#many-instruments",
    "title": "12  Instrumental Variables",
    "section": "12.37 Many Instruments",
    "text": "12.37 Many Instruments\nSome applications have available a large number \\(\\ell\\) of instruments. If they are all valid, using a large number should reduce the asymptotic variance relative to estimation with a smaller number of instruments. Is it then good practice to use many instruments? Or is there a cost to this practice? Bekker (1994) initiated a large literature investigating this question by formalizing the idea of “many instruments”. Bekker proposed an asymptotic approximation which treats the number of instruments \\(\\ell\\) as proportional to the sample size, that is \\(\\ell=\\alpha n\\), or equivalently that \\(\\ell / n \\rightarrow \\alpha \\in[0,1)\\). The distributional theory obtained is similar in many respects to the weak instrument theory outlined in the previous section. Consequently the impact of “weak” and “many” instruments is similar.\nAgain for simplicity we assume that there are no included exogenous regressors so that the model is\n\\[\n\\begin{aligned}\n&Y=X^{\\prime} \\beta+e \\\\\n&X=\\Gamma^{\\prime} Z+u_{2}\n\\end{aligned}\n\\]\nwith \\(Z \\ell \\times 1\\). We also make the simplifying assumption that the reduced form errors are conditionally homoskedastic. Specifically,\n\\[\n\\mathbb{E}\\left[u u^{\\prime} \\mid Z\\right]=\\Sigma=\\left[\\begin{array}{cc}\n\\Sigma_{11} & \\Sigma_{12} \\\\\n\\Sigma_{21} & \\Sigma_{22}\n\\end{array}\\right] .\n\\]\nIn addition we assume that the conditional fourth moments are bounded\n\\[\n\\mathbb{E}\\left[\\|u\\|^{4} \\mid Z\\right] \\leq B<\\infty .\n\\]\nThe idea that there are “many instruments” is formalized by the assumption that the number of instruments is increasing proportionately with the sample size\n\\[\n\\frac{\\ell}{n} \\longrightarrow \\alpha .\n\\]\nThe best way to think about this is to view \\(\\alpha\\) as the ratio of \\(\\ell\\) to \\(n\\) in a given sample. Thus if an application has \\(n=100\\) observations and \\(\\ell=10\\) instruments, then we should treat \\(\\alpha=0.10\\).\nSuppose that there is a single endogenous regressor \\(X\\). Calculate its variance using the reduced form: \\(\\operatorname{var}[X]=\\operatorname{var}\\left[Z^{\\prime} \\Gamma\\right]+\\operatorname{var}[u]\\). Suppose as well that \\(\\operatorname{var}[X]\\) and \\(\\operatorname{var}[u]\\) are unchanging as \\(\\ell\\) increases. This implies that \\(\\operatorname{var}\\left[Z^{\\prime} \\Gamma\\right]\\) is unchanging even though the dimension \\(\\ell\\) is increasing. This is a useful assumption as it implies that the population \\(R^{2}\\) of the reduced form is not changing with \\(\\ell\\). We don’t need this exact condition, rather we simply assume that the sample version converges in probability to a fixed constant. Specifically, we assume that\n\\[\n\\frac{1}{n} \\sum_{i=1}^{n} \\Gamma^{\\prime} Z_{i} Z_{i}^{\\prime} \\Gamma \\underset{p}{\\longrightarrow} \\boldsymbol{H}\n\\]\nfor some matrix \\(\\boldsymbol{H}>0\\). Again, this essentially implies that the \\(R^{2}\\) of the reduced form regressions for each component of \\(X\\) converge to constants.\nAs a baseline it is useful to examine the behavior of the least squares estimator of \\(\\beta\\). First, observe that the variance of \\(\\operatorname{vec}\\left(n^{-1} \\sum_{i=1}^{n} \\Gamma^{\\prime} Z_{i} u_{i}^{\\prime}\\right)\\), conditional on \\(Z\\), is\n\\[\n\\Sigma \\otimes n^{-2} \\sum_{i=1}^{n} \\Gamma^{\\prime} Z_{i} Z_{i}^{\\prime} \\Gamma \\underset{p}{\\longrightarrow} 0\n\\]\nby (12.77). Thus it converges in probability to zero:\n\\[\nn^{-1} \\sum_{i=1}^{n} \\Gamma^{\\prime} Z_{i} u_{i}^{\\prime} \\underset{p}{\\longrightarrow} 0 .\n\\]\nCombined with (12.77) and the WLLN we find\n\\[\n\\frac{1}{n} \\sum_{i=1}^{n} X_{i} e_{i}=\\frac{1}{n} \\sum_{i=1}^{n} \\Gamma^{\\prime} Z_{i} e_{i}+\\frac{1}{n} \\sum_{i=1}^{n} u_{2 i} e_{i} \\underset{p}{\\longrightarrow} \\Sigma_{2 e}\n\\]\nand\n\\[\n\\frac{1}{n} \\sum_{i=1}^{n} X_{i} X_{i}^{\\prime}=\\frac{1}{n} \\sum_{i=1}^{n} \\Gamma^{\\prime} Z_{i} Z_{i}^{\\prime} \\Gamma+\\frac{1}{n} \\sum_{i=1}^{n} \\Gamma^{\\prime} Z_{i} u_{2 i}^{\\prime}+\\frac{1}{n} \\sum_{i=1}^{n} u_{2 i} Z_{i}^{\\prime} \\Gamma+\\frac{1}{n} \\sum_{i=1}^{n} u_{2 i} u_{2 i}^{\\prime} \\underset{p}{\\rightarrow} \\boldsymbol{H}+\\Sigma_{22}\n\\]\nHence\n\\[\n\\widehat{\\beta}_{\\mathrm{ols}}=\\beta+\\left(\\frac{1}{n} \\sum_{i=1}^{n} X_{i} X_{i}^{\\prime}\\right)^{-1}\\left(\\frac{1}{n} \\sum_{i=1}^{n} X_{i} e_{i}\\right) \\underset{p}{\\longrightarrow} \\beta+\\left(\\boldsymbol{H}+\\Sigma_{22}\\right)^{-1} \\Sigma_{2 e}\n\\]\nThus least squares is inconsistent for \\(\\beta\\).\nNow consider the 2SLS estimator. In matrix notation, setting \\(\\boldsymbol{P}_{Z}=\\boldsymbol{Z}\\left(\\boldsymbol{Z}^{\\prime} \\boldsymbol{Z}\\right)^{-1} \\boldsymbol{Z}^{\\prime}\\),\n\\[\n\\begin{aligned}\n\\widehat{\\beta}_{2 \\mathrm{sls}}-\\beta &=\\left(\\frac{1}{n} \\boldsymbol{X}^{\\prime} \\boldsymbol{P}_{\\boldsymbol{Z}} \\boldsymbol{X}\\right)^{-1}\\left(\\frac{1}{n} \\boldsymbol{X}^{\\prime} \\boldsymbol{P}_{\\boldsymbol{Z}} \\boldsymbol{e}\\right) \\\\\n&=\\left(\\frac{1}{n} \\bar{\\Gamma}^{\\prime} \\boldsymbol{Z}^{\\prime} \\boldsymbol{Z} \\bar{\\Gamma}+\\frac{1}{n} \\bar{\\Gamma}^{\\prime} \\boldsymbol{Z}^{\\prime} \\boldsymbol{u}_{2}+\\frac{1}{n} \\boldsymbol{u}_{2}^{\\prime} \\boldsymbol{Z} \\bar{\\Gamma}+\\frac{1}{n} \\boldsymbol{u}_{2}^{\\prime} \\boldsymbol{P}_{Z} \\boldsymbol{u}_{2}\\right)^{-1}\\left(\\frac{1}{n} \\Gamma^{\\prime} \\boldsymbol{Z}^{\\prime} \\boldsymbol{e}+\\frac{1}{n} \\boldsymbol{u}_{2}^{\\prime} \\boldsymbol{P}_{Z} \\boldsymbol{e}\\right)\n\\end{aligned}\n\\]\nIn the expression on the right-side of (12.79) several of the components have been examined in (12.77) and (12.78). We now examine the remaining components \\(\\frac{1}{n} \\boldsymbol{u}_{2}^{\\prime} \\boldsymbol{P}_{\\boldsymbol{Z}} \\boldsymbol{e}\\) and \\(\\frac{1}{n} \\boldsymbol{u}_{2}^{\\prime} \\boldsymbol{P}_{\\boldsymbol{Z}} \\boldsymbol{u}_{2}\\) which are sub-components of the matrix \\(\\frac{1}{n} \\boldsymbol{u}^{\\prime} \\boldsymbol{P}_{\\boldsymbol{Z}} \\boldsymbol{u}\\). Take the \\(j k^{t h}\\) element \\(\\frac{1}{n} \\boldsymbol{u}_{j}^{\\prime} \\boldsymbol{P}_{\\boldsymbol{Z}} \\boldsymbol{u}_{k}\\).\nFirst, take its expectation. We have (given under the conditional homoskedasticity assumption (12.74))\n\\[\n\\mathbb{E}\\left[\\frac{1}{n} \\boldsymbol{u}_{j}^{\\prime} \\boldsymbol{P}_{\\boldsymbol{Z}} \\boldsymbol{u}_{k} \\mid \\boldsymbol{Z}\\right]=\\frac{1}{n} \\operatorname{tr}\\left(\\mathbb{E}\\left[\\boldsymbol{P}_{\\boldsymbol{Z}} \\boldsymbol{u}_{k} \\boldsymbol{u}_{j}^{\\prime} \\mid \\boldsymbol{Z}\\right]\\right)=\\frac{1}{n} \\operatorname{tr}\\left(\\boldsymbol{P}_{\\boldsymbol{Z}}\\right) \\Sigma_{j k}=\\frac{\\ell}{n} \\Sigma_{j k} \\rightarrow \\alpha \\Sigma_{j k}\n\\]\nusing \\(\\operatorname{tr}\\left(\\boldsymbol{P}_{Z}\\right)=\\ell\\).\nSecond, we calculate its variance which is a more cumbersome exercise. Let \\(P_{i m}=Z_{i}^{\\prime}\\left(\\boldsymbol{Z}^{\\prime} \\boldsymbol{Z}\\right)^{-1} Z_{m}\\) be the \\(i m^{t h}\\) element of \\(\\boldsymbol{P}_{\\boldsymbol{Z}}\\). Then \\(\\boldsymbol{u}_{j}^{\\prime} \\boldsymbol{P}_{\\boldsymbol{Z}} \\boldsymbol{u}_{k}=\\sum_{i=1}^{n} \\sum_{m=1}^{n} u_{j i} u_{k m} P_{i m}\\). The matrix \\(\\boldsymbol{P}_{\\boldsymbol{Z}}\\) is idempotent. It therefore has the properties \\(\\sum_{i=1}^{n} P_{i i}=\\operatorname{tr}\\left(\\boldsymbol{P}_{Z}\\right)=\\ell\\) and \\(0 \\leq P_{i i} \\leq 1\\). The property \\(\\boldsymbol{P}_{Z} \\boldsymbol{P}_{Z}=\\boldsymbol{P}_{Z}\\) also implies \\(\\sum_{m=1}^{n} P_{i m}^{2}=P_{i i}\\). Then\n\\[\n\\begin{aligned}\n\\operatorname{var}\\left[\\frac{1}{n} \\boldsymbol{u}_{j}^{\\prime} \\boldsymbol{P}_{\\boldsymbol{Z}} \\boldsymbol{u}_{k} \\mid \\boldsymbol{Z}\\right] &=\\frac{1}{n^{2}} \\mathbb{E}\\left[\\sum_{i=1}^{n} \\sum_{m=1}^{n}\\left(u_{j i} u_{k m}-\\mathbb{E}\\left[u_{j i} u_{k m}\\right] \\mathbb{1}\\{i=m\\}\\right) P_{i m} \\mid \\boldsymbol{Z}\\right]^{2} \\\\\n&=\\frac{1}{n^{2}} \\mathbb{E}\\left[\\sum_{i=1}^{n} \\sum_{m=1}^{n} \\sum_{q=1}^{n} \\sum_{r=1}^{n}\\left(u_{j i} u_{k m}-\\Sigma_{j k} \\mathbb{1}\\{i=m\\}\\right) P_{i m}\\left(u_{j q} u_{k r}-\\Sigma_{j k} \\mathbb{1}\\{q=r\\}\\right) P_{q r}\\right] \\\\\n&=\\frac{1}{n^{2}} \\sum_{i=1}^{n} \\mathbb{E}\\left[\\left(u_{j i} u_{k i}-\\Sigma_{j k}\\right)^{2}\\right] P_{i i}^{2} \\\\\n&+\\frac{1}{n^{2}} \\sum_{i=1}^{n} \\sum_{m \\neq i} \\mathbb{E}\\left[u_{j i}^{2} u_{k m}^{2}\\right] P_{i m}^{2}+\\frac{1}{n^{2}} \\sum_{i=1}^{n} \\sum_{m \\neq i} \\mathbb{E}\\left[u_{j i} u_{k m} u_{j m} u_{k i}\\right] P_{i m}^{2} \\\\\n& \\leq \\frac{B}{n^{2}}\\left(\\sum_{i=1}^{n} P_{i i}^{2}+2 \\sum_{i=1}^{n} \\sum_{m=1}^{n} P_{i m}^{2}\\right) \\\\\n& \\leq \\frac{3 B}{n^{2}} \\sum_{i=1}^{n} P_{i i} \\\\\n&=3 B \\frac{\\ell}{n^{2}} \\rightarrow 0 .\n\\end{aligned}\n\\]\nThe third equality holds because the remaining cross-products have zero expectation as the observations are independent and the errors have zero mean. The first inequality is (12.75). The second uses \\(P_{i i}^{2} \\leq P_{i i}\\) and \\(\\sum_{m=1}^{n} P_{i m}^{2}=P_{i i}\\). The final equality is \\(\\sum_{i=1}^{n} P_{i i}=\\ell\\).\nUsing (12.76), (12.80), Markov’s inequality (B.36), and combining across all \\(j\\) and \\(k\\) we deduce that\n\\[\n\\frac{1}{n} \\boldsymbol{u}^{\\prime} \\boldsymbol{P}_{\\boldsymbol{Z}} \\boldsymbol{u} \\underset{p}{\\longrightarrow} \\alpha \\Sigma .\n\\]\nReturning to the 2SLS estimator (12.79) and combining (12.77), (12.78), and (12.81), we find\n\\[\n\\widehat{\\beta}_{2 \\text { sls }}-\\beta \\underset{p}{\\longrightarrow}\\left(\\boldsymbol{H}+\\alpha \\Sigma_{22}\\right)^{-1} \\alpha \\Sigma_{2 e} .\n\\]\nThus 2SLS is also inconsistent for \\(\\beta\\). The limit, however, depends on the magnitude of \\(\\alpha\\).\nWe finally examine the LIML estimator. (12.81) implies\n\\[\n\\frac{1}{n} \\boldsymbol{Y}^{\\prime} \\boldsymbol{M}_{Z} \\boldsymbol{Y}=\\frac{1}{n} \\boldsymbol{u}^{\\prime} \\boldsymbol{u}-\\frac{1}{n} \\boldsymbol{u}^{\\prime} \\boldsymbol{P}_{\\boldsymbol{Z}} \\boldsymbol{u} \\underset{p}{\\longrightarrow}(1-\\alpha) \\Sigma .\n\\]\nSimilarly\n\\[\n\\begin{aligned}\n\\frac{1}{n} \\boldsymbol{Y}^{\\prime} \\boldsymbol{Z}\\left(\\boldsymbol{Z}^{\\prime} \\boldsymbol{Z}\\right)^{-1} \\boldsymbol{Z}^{\\prime} \\boldsymbol{Y} &=\\bar{\\beta}^{\\prime} \\Gamma^{\\prime}\\left(\\frac{1}{n} \\boldsymbol{Z}^{\\prime} \\boldsymbol{Z}\\right) \\Gamma \\bar{\\beta}+\\bar{\\beta}^{\\prime} \\Gamma^{\\prime}\\left(\\frac{1}{n} \\boldsymbol{Z}^{\\prime} \\boldsymbol{u}\\right)+\\left(\\frac{1}{n} \\boldsymbol{u}^{\\prime} \\boldsymbol{Z}\\right) \\Gamma \\bar{\\beta}+\\frac{1}{n} \\boldsymbol{u}^{\\prime} \\boldsymbol{P}_{\\boldsymbol{Z}} \\boldsymbol{u} \\\\\n& \\underset{d}{\\longrightarrow} \\bar{\\beta}^{\\prime} \\boldsymbol{H} \\bar{\\beta}+\\alpha \\Sigma .\n\\end{aligned}\n\\]\nHence\n\\[\n\\widehat{\\mu}=\\min _{\\gamma} \\frac{\\gamma^{\\prime} \\boldsymbol{Y}^{\\prime} \\boldsymbol{Z}\\left(\\boldsymbol{Z}^{\\prime} \\boldsymbol{Z}\\right)^{-1} \\boldsymbol{Z}^{\\prime} \\boldsymbol{Y} \\gamma}{\\gamma^{\\prime} \\boldsymbol{Y}^{\\prime} \\boldsymbol{M}_{\\boldsymbol{Z}} \\boldsymbol{Y} \\gamma} \\underset{d}{\\longrightarrow} \\min _{\\gamma} \\frac{\\gamma^{\\prime}\\left(\\bar{\\beta}^{\\prime} \\boldsymbol{H} \\bar{\\beta}+\\alpha \\Sigma\\right) \\gamma}{\\gamma^{\\prime}(1-\\alpha) \\Sigma \\gamma}=\\frac{\\alpha}{1-\\alpha}\n\\]\nand\n\\[\n\\begin{aligned}\n\\widehat{\\beta}_{\\mathrm{liml}}-\\beta &=\\left(\\frac{1}{n} \\boldsymbol{X}^{\\prime} \\boldsymbol{P}_{\\boldsymbol{Z}} \\boldsymbol{X}-\\widehat{\\mu} \\frac{1}{n} \\boldsymbol{X}^{\\prime} \\boldsymbol{M}_{\\boldsymbol{Z}} \\boldsymbol{X}\\right)^{-1}\\left(\\frac{1}{n} \\boldsymbol{X}^{\\prime} \\boldsymbol{P}_{Z} \\boldsymbol{e}-\\widehat{\\mu} \\frac{1}{n} \\boldsymbol{X}^{\\prime} \\boldsymbol{M}_{\\boldsymbol{Z}} \\boldsymbol{e}\\right) \\\\\n& \\underset{d}{\\longrightarrow}\\left(\\boldsymbol{H}+\\alpha \\Sigma_{22}-\\frac{\\alpha}{1-\\alpha}(1-\\alpha) \\Sigma_{22}\\right)^{-1}\\left(\\alpha \\Sigma_{2 e}-\\frac{\\alpha}{1-\\alpha}(1-\\alpha) \\Sigma_{2 e}\\right) \\\\\n&=\\boldsymbol{H}^{-1} 0 \\\\\n&=0 .\n\\end{aligned}\n\\]\nThus LIML is consistent for \\(\\beta\\), unlike 2SLS.\nWe state these results formally.\nTheorem 12.19 In model (12.73), under assumptions (12.74), (12.75) and (12.76), then as \\(n \\rightarrow \\infty\\).\n\\[\n\\begin{aligned}\n&\\widehat{\\beta}_{\\text {ols }} \\underset{p}{\\longrightarrow} \\beta+\\left(\\boldsymbol{H}+\\Sigma_{22}\\right)^{-1} \\Sigma_{2 e} \\\\\n&\\widehat{\\beta}_{2 \\text { sls }} \\underset{p}{\\longrightarrow} \\beta+\\left(\\boldsymbol{H}+\\alpha \\Sigma_{22}\\right)^{-1} \\alpha \\Sigma_{2 e} \\\\\n&\\widehat{\\beta}_{\\text {liml }} \\underset{p}{\\longrightarrow} \\beta .\n\\end{aligned}\n\\]\nThis result is quite insightful. It shows that while endogeneity \\(\\left(\\Sigma_{2 e} \\neq 0\\right)\\) renders the least squares estimator inconsistent, the 2SLS estimator is also inconsistent if the number of instruments diverges proportionately with \\(n\\). The limit in Theorem \\(12.19\\) shows a continuity between least squares and 2 SLS. The probability limit of the 2SLS estimator is continuous in \\(\\alpha\\), with the extreme case \\((\\alpha=1)\\) implying that 2SLS and least squares have the same probability limit. The general implication is that the inconsistency of 2 SLS is increasing in \\(\\alpha\\).\nThe theorem also shows that unlike 2SLS the LIML estimator is consistent under the many instruments assumption. Effectively, LIML makes a bias-correction.\nTheorems \\(12.18\\) (weak instruments) and \\(12.19\\) (many instruments) tell a cautionary tale. They show that when instruments are weak and/or many the 2SLS estimator is inconsistent. The degree of inconsistency depends on the weakness of the instruments (the magnitude of the matrix \\(\\boldsymbol{C}\\) in Theorem 12.18) and the degree of overidentification (the ratio \\(\\alpha\\) in Theorem 12.19). The Theorems also show that the LIML estimator is inconsistent under the weak instrument assumption but with a bias-correction, and is consistent under the many instrument assumption. This suggests that LIML is more robust than 2SLS to weak and many instruments.\nAn important limitation of the results in Theorem \\(12.19\\) is the assumption of conditional homoskedasticity. It appears likely that the consistency of LIML fails in the many instrument setting if the errors are heteroskedastic.\nIn applications users should be aware of the potential consequences of the many instrument framework. It is useful to calculate the “many instrument ratio” \\(\\alpha=\\ell / n\\). While there is no specific rule-ofthumb for \\(\\alpha\\) which leads to acceptable inference a minimum criterion is that if \\(\\alpha \\geq 0.05\\) you should be seriously concerned about the many-instrument problem. In general, when \\(\\alpha\\) is large it seems preferable to use LIML instead of 2SLS."
  },
  {
    "objectID": "chpt12-iv.html#testing-for-weak-instruments",
    "href": "chpt12-iv.html#testing-for-weak-instruments",
    "title": "12  Instrumental Variables",
    "section": "12.38 Testing for Weak Instruments",
    "text": "12.38 Testing for Weak Instruments\nIn the previous sections we found that weak instruments results in non-standard asymptotic distributions for the 2SLS and LIML estimators. In practice how do we know if this is a problem? Is there a way to check if the instruments are weak?\nThis question was addressed in an influential paper by Stock and Yogo (2005) as an extension of Staiger and Stock (1997). Stock-Yogo focus on two implications of weak instruments: (1) estimation bias and (2) inference distortion. They show how to test the hypothesis that these distortions are not “too big”. They propose \\(F\\) tests for the excluded instruments in the reduced form regressions with non-standard critical values. In particular, when there is one endogenous regressor and a single instrument the StockYogo test rejects the null of weak instruments when this \\(F\\) statistic exceeds 10 . While Stock and Yogo explore two types of distortions, we focus exclusively on inference as that is the more challenging problem. In this section we describe the Stock-Yogo theory and tests for the case of a single endogenous regressor \\(\\left(k_{2}=1\\right)\\). In the following section we describe their method for the case of multiple endogeneous regressors.\nWhile the theory in Stock and Yogo allows for an arbitrary number of exogenous regressors and instruments, for the sake of clear exposition we will focus on the very simple case of no included exogenous variables \\(\\left(k_{1}=0\\right)\\) and just one exogenous instrument \\(\\left(\\ell_{2}=1\\right)\\) which is model (12.69) from Section 12.35.\n\\[\n\\begin{aligned}\n&Y=X \\beta+e \\\\\n&X=Z \\Gamma+u .\n\\end{aligned}\n\\]\nFurthermore, as in Section \\(12.35\\) we assume conditional homoskedasticity and normalize the variances as in (12.70). Since the model is just-identified the 2SLS, LIML, and IV estimators are all equivalent.\nThe question of primary interest is to determine conditions on the reduced form under which the IV estimator of the structural equation is well behaved, and secondly, what statistical tests can be used to learn if these conditions are satisfied. As in Section \\(12.36\\) we assume that the reduced form coefficient \\(\\Gamma\\) is local-to-zero, specifically \\(\\Gamma=n^{-1 / 2} \\mu\\). The asymptotic distribution of the IV estimator is presented in Theorem 12.18. Given the simplifying assumptions the result is\n\\[\n\\widehat{\\beta}_{\\mathrm{iv}}-\\beta \\underset{d}{\\longrightarrow} \\frac{\\xi_{e}}{\\mu+\\xi_{2}}\n\\]\nwhere \\(\\left(\\xi_{e}, \\xi_{2}\\right)\\) are bivariate normal. For inference we also examine the behavior of the classical (ho- moskedastic) t-statistic for the IV estimator. Note\n\\[\n\\begin{aligned}\n\\widehat{\\sigma}^{2} &=\\frac{1}{n} \\sum_{i=1}^{n}\\left(Y_{i}-X_{i} \\widehat{\\beta}_{\\mathrm{iv}}\\right)^{2} \\\\\n&=\\frac{1}{n} \\sum_{i=1}^{n} e_{i}^{2}-\\frac{2}{n} \\sum_{i=1}^{n} e_{i} X_{i}\\left(\\widehat{\\beta}_{\\mathrm{iv}}-\\beta\\right)+\\frac{1}{n} \\sum_{i=1}^{n} X_{i}^{2}\\left(\\widehat{\\beta}_{\\mathrm{iv}}-\\beta\\right)^{2} \\\\\n& \\underset{d}{\\longrightarrow} 1-2 \\rho \\frac{\\xi_{e}}{\\mu+\\xi_{2}}+\\left(\\frac{\\xi_{e}}{\\mu+\\xi_{2}}\\right)^{2} .\n\\end{aligned}\n\\]\nThus\n\nIn general, \\(S\\) is non-normal and its distribution depends on the parameters \\(\\rho\\) and \\(\\mu\\).\nCan we use the distribution \\(S\\) for inference on \\(\\beta\\) ? The distribution depends on two unknown parameters and neither is consistently estimable. This means we cannot use the distribution in (12.82) with \\(\\rho\\) and \\(\\mu\\) replaced with estimates. To eliminate the dependence on \\(\\rho\\) one possibility is to use the “worst case” value which turns out to be \\(\\rho=1\\). By worst-case we mean the value which causes the greatest distortion away from normal critical values. Setting \\(\\rho=1\\) we have the considerable simplification\n\\[\nS=S_{1}=\\xi\\left|1+\\frac{\\xi}{\\mu}\\right|\n\\]\nwhere \\(\\xi \\sim \\mathrm{N}(0,1)\\). When the model is strongly identified (so \\(|\\mu|\\) is very large) then \\(S_{1} \\approx \\xi\\) is standard normal, consistent with classical theory. However when \\(|\\mu|\\) is very small (but non-zero) \\(\\left|S_{1}\\right| \\approx \\xi^{2} / \\mu\\) (in the sense that this term dominates), which is a scaled \\(\\chi_{1}^{2}\\) and quite far from normal. As \\(|\\mu| \\rightarrow 0\\) we find the extreme case \\(\\left|S_{1}\\right| \\rightarrow p \\infty\\).\nWhile (12.83) is a convenient simplification it does not yield a useful approximation for inference as the distribution in (12.83) is highly dependent on the unknown \\(\\mu\\). If we take the worst-case value of \\(\\mu\\), which is \\(\\mu=0\\), we find that \\(\\left|S_{1}\\right|\\) diverges and all distributional approximations fail.\nTo break this impasse Stock and Yogo (2005) recommended a constructive alternative. Rather than using the worst-case \\(\\mu\\) they suggested finding a threshold such that if \\(\\mu\\) exceeds this threshold then the distribution (12.83) is not “too badly” distorted from the normal distribution.\nSpecifically, the Stock-Yogo recommendation can be summarized by two steps. First, the distribution result (12.83) can be used to find a threshold value \\(\\tau^{2}\\) such that if \\(\\mu^{2} \\geq \\tau^{2}\\) then the size of the nominal \\({ }^{1}\\) 5% test “Reject if \\(|T| \\geq 1.96\\)” has asymptotic size \\(\\mathbb{P}\\left[\\left|S_{1}\\right| \\geq 1.96\\right] \\leq 0.15\\). This means that while the goal is to obtain a test with size \\(5 %\\), we recognize that there may be size distortion due to weak instruments and are willing to tolerate a specific distortion. For example, a \\(10 %\\) distortion means we allow the actual size to be up to \\(15 %\\). Second, they use the asymptotic distribution of the reduced-form (first stage) \\(F\\) statistic to test if the actual unknown value of \\(\\mu^{2}\\) exceeds the threshold \\(\\tau^{2}\\). These two steps together give rise to the rule-of-thumb that the first-stage \\(F\\) statistic should exceed 10 in order to achieve reliable IV inference. (This is for the case of one instrumental variable. If there is more than one instrument then the rule-of-thumb changes.) We now describe the steps behind this reasoning in more detail.\nThe first step is to use the distribution (12.82) to determine the threshold \\(\\tau^{2}\\). Formally, the goal is to find the value of \\(\\tau^{2}=\\mu^{2}\\) at which the asymptotic size of a nominal \\(5 %\\) test is actually a given \\(r\\) (e.g.\n\\({ }^{1}\\) The term “nominal size” of a test is the official intended size - the size which would obtain under ideal circumstances. In this context the test “Reject if \\(|T| \\geq 1.96\\)” has nominal size \\(0.05\\) as this would be the asymptotic rejection probability in the ideal context of strong instruments. \\(r=0.15)\\), thus \\(\\mathbb{P}\\left[\\left|S_{1}\\right| \\geq 1.96\\right] \\leq r\\). By some algebra and the quadratic formula the event \\(|\\xi(1+\\xi / \\mu)|<x\\) is the same as\n\\[\n\\frac{\\mu^{2}}{4}-x \\mu<\\left(\\xi+\\frac{\\mu}{2}\\right)^{2}<\\frac{\\mu^{2}}{4}+x \\mu .\n\\]\nThe random variable between the inequalities is distributed \\(\\chi_{1}^{2}\\left(\\mu^{2} / 4\\right)\\), a noncentral chi-square with one degree of freedom and noncentrality parameter \\(\\mu^{2} / 4\\). Thus\n\\[\n\\begin{aligned}\n\\mathbb{P}\\left[\\left|S_{1}\\right| \\geq x\\right] &=\\mathbb{P}\\left[\\chi_{1}^{2}\\left(\\frac{\\mu^{2}}{4}\\right) \\geq \\frac{\\mu^{2}}{4}+x \\mu\\right]+\\mathbb{P}\\left[\\chi_{1}^{2}\\left(\\frac{\\mu^{2}}{4}\\right) \\leq \\frac{\\mu^{2}}{4}-x \\mu\\right] \\\\\n&=1-G\\left(\\frac{\\mu^{2}}{4}+x \\mu, \\frac{\\mu^{2}}{4}\\right)+G\\left(\\frac{\\mu^{2}}{4}-x \\mu, \\frac{\\mu^{2}}{4}\\right)\n\\end{aligned}\n\\]\nwhere \\(G(u, \\lambda)\\) is the distribution function of \\(\\chi_{1}^{2}(\\lambda)\\). Hence the desired threshold \\(\\tau^{2}\\) solves\n\\[\n1-G\\left(\\frac{\\tau^{2}}{4}+1.96 \\tau, \\frac{\\tau^{2}}{4}\\right)+G\\left(\\frac{\\tau^{2}}{4}-1.96 \\tau, \\frac{\\tau^{2}}{4}\\right)=r\n\\]\nor effectively\n\\[\nG\\left(\\frac{\\tau^{2}}{4}+1.96 \\tau, \\frac{\\tau^{2}}{4}\\right)=1-r\n\\]\nbecause \\(\\tau^{2} / 4-1.96 \\tau<0\\) for relevant values of \\(\\tau\\). The numerical solution (computed with the non-central chi-square distribution function, e.g. ncx \\(2 c d f\\) in MATLAB) is \\(\\tau^{2}=1.70\\) when \\(r=0.15\\). (That is, the command\n\\[\n\\operatorname{ncx} 2 \\mathrm{cdf}(1.7 / 4+1.96 * \\operatorname{sqrt}(1.7), 1,1.7 / 4)\n\\]\nyields the answer \\(0.8500\\). Stock and Yogo (2005) approximate the same calculation using simulation methods and report \\(\\tau^{2}=1.82\\).)\nThis calculation means that if the reduced form satisfies \\(\\mu^{2} \\geq 1.7\\), or equivalently if \\(\\Gamma^{2} \\geq 1.7 / n\\), then the asymptotic size of a nominal \\(5 %\\) test on the structural parameter is no larger than \\(15 %\\).\nTo summarize the Stock-Yogo first step, we calculate the minimum value \\(\\tau^{2}\\) for \\(\\mu^{2}\\) sufficient to ensure that the asymptotic size of a nominal 5% t-test does not exceed \\(r\\), and find that \\(\\tau^{2}=1.70\\) for \\(r=0.15\\).\nThe Stock-Yogo second step is to find a critical value for the first-stage \\(F\\) statistic sufficient to reject the hypothesis that \\(\\mathbb{M}_{0}: \\mu^{2}=\\tau^{2}\\) against \\(\\mathbb{M}_{1}: \\mu^{2}>\\tau^{2}\\). We now describe this procedure.\nThey suggest testing \\(\\mathbb{M}_{0}: \\mu^{2}=\\tau^{2}\\) at the \\(5 %\\) size using the first stage \\(F\\) statistic. If the \\(F\\) statistic is small so that the test does not reject then we should be worried that the true value of \\(\\mu^{2}\\) is small and there is a weak instrument problem. On the other hand if the \\(F\\) statistic is large so that the test rejects then we can have some confidence that the true value of \\(\\mu^{2}\\) is sufficiently large that the weak instrument problem is not too severe.\nTo implement the test we need to calculate an appropriate critical value. It should be calculated under the null hypothesis \\(\\mathbb{H}_{0}: \\mu^{2}=\\tau^{2}\\). This is different from a conventional \\(F\\) test which is calculated under \\(\\mathbb{M}_{0}: \\mu^{2}=0\\).\nWe start by calculating the asymptotic distribution of \\(\\mathrm{F}\\). Since there is one regressor and one instrument in our simplified setting the first-stage \\(F\\) statistic is the squared t-statistic from the reduced form. Given our previous calculations it has the asymptotic distribution\n\\[\n\\mathrm{F}=\\frac{\\widehat{\\gamma}^{2}}{s(\\widehat{\\gamma})^{2}}=\\frac{\\left(\\sum_{i=1}^{n} Z_{i} X_{i}\\right)^{2}}{\\left(\\sum_{i=1}^{n} X_{i}^{2}\\right) \\widehat{\\sigma}_{u}^{2}} \\underset{d}{ }\\left(\\mu+\\xi_{2}\\right)^{2} \\sim \\chi_{1}^{2}\\left(\\mu^{2}\\right) .\n\\]\nThis is a non-central chi-square distribution \\(G\\left(u, \\mu^{2}\\right)\\) with one degree of freedom and non-centrality parameter \\(\\mu^{2}\\). To test \\(\\mathbb{M}_{0}: \\mu^{2}=\\tau^{2}\\) against \\(\\mathbb{M}_{1}: \\mu^{2}>\\tau^{2}\\) we reject for \\(\\mathrm{F} \\geq c\\) where \\(c\\) is selected so that the asymptotic rejection probability satisfies\n\\[\n\\mathbb{P}\\left[\\mathrm{F} \\geq c \\mid \\mu^{2}=\\tau^{2}\\right] \\rightarrow \\mathbb{P}\\left[\\chi_{1}^{2}\\left(\\tau^{2}\\right) \\geq c\\right]=1-G\\left(c, \\tau^{2}\\right)=0.05\n\\]\nfor \\(\\tau^{2}=1.70\\), or equivalently \\(G(c, 1.7)=0.95\\). This is found by inverting the non-central chi-square quantile function, e.g. the function \\(Q(p, d)\\) which solves \\(G(Q(p, d), d)=p\\). We find that \\(c=Q(0.95,1.7)=8.7\\). In MATLAB, this can be computed by ncx2inv \\((.95,1.7\\) ). Stock and Yogo (2005) report \\(c=9.0\\) because they used \\(\\tau^{2}=1.82\\).\nThis means that if \\(\\mathrm{F}>8.7\\) we can reject \\(\\mathbb{M}_{0}: \\mu^{2}=1.7\\) against \\(\\mathbb{H}_{1}: \\mu^{2}>1.7\\) with an asymptotic \\(5 %\\) test. In this context we should expect the IV estimator and tests to be reasonably well behaved. However, if \\(\\mathrm{F}<8.7\\) then we should be cautious about the IV estimator, confidence intervals, and tests. This finding led Staiger and Stock (1997) to propose the informal “rule of thumb” that the first stage \\(F\\) statistic should exceed 10. Notice that \\(\\mathrm{F}\\) exceeding \\(8.7\\) (or 10) is equivalent to the reduced form t-statistic exceeding \\(2.94\\) (or 3.16), which is considerably larger than a conventional check if the t-statistic is “significant”. Equivalently, the recommended rule-of-thumb for the case of a single instrument is to estimate the reduced form and verify that the t-statistic for exclusion of the instrumental variable exceeds 3 in absolute value.\nDoes the proposed procedure control the asymptotic size of a 2SLS test? The first step has asymptotic size bounded below \\(r\\) (e.g. 15%). The second step has asymptotic size 5%. By the Bonferroni bound (see Section 9.20) the two steps together have asymptotic size bounded below \\(r+0.05\\) (e.g. 20%). We can thus call the Stock-Yogo procedure a rigorous test with asymptotic size \\(r+0.05\\) (or 20%).\nOur analysis has been confined to the case \\(k_{2}=\\ell_{2}=1\\). Stock and Yogo (2005) also examine the case \\(\\ell_{2}>1\\) (which requires numerical simulation to solve) and both the 2SLS and LIML estimators. They show that the \\(F\\) statistic critical values depend on the number of instruments \\(\\ell_{2}\\) as well as the estimator. Their critical values (calculated by simulation) are in their paper and posted on Motohiro Yogo’s webpage. We report a subset in Table 12.4.\nTable 12.4: 5% Critical Value for Weak Instruments, \\(k_{2}=1\\)\n|\\(\\ell_{2}\\)|\\(0.10\\)|\\(0.15\\)|\\(0.20\\)|\\(0.25\\)||\\(0.10\\)|\\(0.15\\)|\\(0.20\\)|\\(0.25\\)| |———:|—–:|—–:|—–:|—–:||—–:|—–:|—–:|—–:| | 1|\\(16.4\\)| \\(9.0\\)| \\(6.7\\)| \\(5.5\\)||\\(16.4\\)| \\(9.0\\)| \\(6.7\\)| \\(5.5\\)| | 2|\\(19.9\\)|\\(11.6\\)| \\(8.7\\)| \\(7.2\\)|| \\(8.7\\)| \\(5.3\\)| \\(4.4\\)| \\(3.9\\)| | 3|\\(22.3\\)|\\(12.8\\)| \\(9.5\\)| \\(7.8\\)|| \\(6.5\\)| \\(4.4\\)| \\(3.7\\)| \\(3.3\\)| | 4|\\(24.6\\)|\\(14.0\\)|\\(10.3\\)| \\(8.3\\)|| \\(5.4\\)| \\(3.9\\)| \\(3.3\\)| \\(3.0\\)| | 5|\\(26.9\\)|\\(15.1\\)|\\(11.0\\)| \\(8.8\\)|| \\(4.8\\)| \\(3.6\\)| \\(3.0\\)| \\(2.8\\)| | 6|\\(29.2\\)|\\(16.2\\)|\\(11.7\\)| \\(9.4\\)|| \\(4.4\\)| \\(3.3\\)| \\(2.9\\)| \\(2.6\\)| | 7|\\(31.5\\)|\\(17.4\\)|\\(12.5\\)| \\(9.9\\)|| \\(4.2\\)| \\(3.2\\)| \\(2.7\\)| \\(2.5\\)| | 8|\\(33.8\\)|\\(18.5\\)|\\(13.2\\)|\\(10.5\\)|| \\(4.0\\)| \\(3.0\\)| \\(2.6\\)| \\(2.4\\)| | 9|\\(36.2\\)|\\(19.7\\)|\\(14.0\\)|\\(11.1\\)|| \\(3.8\\)| \\(2.9\\)| \\(2.5\\)| \\(2.3\\)| | 10|\\(38.5\\)|\\(20.9\\)|\\(14.8\\)|\\(11.6\\)|| \\(3.7\\)| \\(2.8\\)| \\(2.5\\)| \\(2.2\\)| | 15|\\(50.4\\)|\\(26.8\\)|\\(18.7\\)|\\(12.2\\)|| \\(3.3\\)| \\(2.5\\)| \\(2.2\\)| \\(2.0\\)| | 20|\\(62.3\\)|\\(32.8\\)|\\(22.7\\)|\\(17.6\\)|| \\(3.2\\)| \\(2.3\\)| \\(2.1\\)| \\(1.9\\)| | 25|\\(74.2\\)|\\(38.8\\)|\\(26.7\\)|\\(20.6\\)|| \\(3.8\\)| \\(2.2\\)| \\(2.0\\)| \\(1.8\\)| | 30|\\(86.2\\)|\\(44.8\\)|\\(30.7\\)|\\(23.6\\)|| \\(3.9\\)| \\(2.2\\)| \\(1.9\\)| \\(1.7\\)|\nSource: . One striking feature about these critical values is that those for the 2SLS estimator are strongly increasing in \\(\\ell_{2}\\) while those for the LIML estimator are decreasing in \\(\\ell_{2}\\). This means that when the number of instruments \\(\\ell_{2}\\) is large, 2SLS requires a much stronger reduced form (larger \\(\\mu^{2}\\) ) in order for inference to be reliable, but this is not the case for LIML. This is direct evidence that LIML inference is less sensitive to weak instruments than 2SLS. This makes a strong case for LIML over 2SLS, especially when \\(\\ell_{2}\\) is large or the instruments are potentially weak.\nWe now summarize the recommended Staiger-Stock/Stock-Yogo procedure for \\(k_{1} \\geq 1, k_{2}=1\\), and \\(\\ell_{2} \\geq 1\\). The structural equation and reduced form equations are\n\\[\n\\begin{aligned}\n&Y_{1}=Z_{1}^{\\prime} \\beta_{1}+Y_{2} \\beta_{2}+e \\\\\n&Y_{2}=Z_{1}^{\\prime} \\gamma_{1}+Z_{2}^{\\prime} \\gamma_{2}+u .\n\\end{aligned}\n\\]\nThe structural equation is estimated by either 2 SLS or LIML. Let \\(\\mathrm{F}\\) be the \\(F\\) statistic for \\(\\mathbb{H}_{0}: \\gamma_{2}=0\\) in the reduced form equation. Let \\(s\\left(\\widehat{\\beta}_{2}\\right)\\) be a standard error for \\(\\beta_{2}\\) in the structural equation. The procedure is:\n\nCompare \\(F\\) with the critical values \\(c\\) in Table \\(12.4\\) with the row selected to match the number of excluded instruments \\(\\ell_{2}\\) and the columns to match the estimation method (2SLS or LIML) and the desired size \\(r\\).\nIf \\(F>c\\) then report the 2 SLS or LIML estimates with conventional inference.\n\nThe Stock-Yogo test can be implemented in Stata using the command estat firststage after ivregress 2 sls or ivregres liml if a standard (non-robust) covariance matrix has been specified (that is, without the ‘, r’ option).\nThere are possible extensions to the Stock-Yogo procedure.\nOne modest extension is to use the information to convey the degree of confidence in the accuracy of a confidence interval. Suppose in an application you have \\(\\ell_{2}=5\\) excluded instruments and have estimated your equation by 2SLS. Now suppose that your reduced form \\(F\\) statistic equals 12 . You check Table \\(12.4\\) and find that \\(\\mathrm{F}=12\\) is significant with \\(r=0.20\\). Thus we can interpret the conventional 2SLS confidence interval as having coverage of \\(80 %\\) (or \\(75 %\\) if we make the Bonferroni correction). On the other hand if \\(\\mathrm{F}=27\\) we would conclude that the test for weak instruments is significant with \\(r=0.10\\), meaning that the conventional 2SLS confidence interval can be interpreted as having coverage of \\(90 %\\) (or \\(85 %\\) after Bonferroni correction). Thus the value of the \\(\\mathrm{F}\\) statistic can be used to calibrate the coverage accuracy.\nA more substantive extension, which we now discuss, reverses the steps. Unfortunately this discussion will be limited to the case \\(\\ell_{2}=1\\). First, use the reduced form \\(F\\) statistic to find a one-sided confidence interval for \\(\\mu^{2}\\) of the form \\(\\left[\\mu_{L}^{2}, \\infty\\right)\\). Second, use the lower bound \\(\\mu_{L}^{2}\\) to calculate a critical value \\(c\\) for \\(S_{1}\\) such that the 2SLS test has asymptotic size bounded below \\(0.05\\). This produces better size control than the Stock-Yogo procedure and produces more informative confidence intervals for \\(\\beta_{2}\\). We now describe the steps in detail.\nThe first goal is to find a one-sided confidence interval for \\(\\mu^{2}\\). This is found by test inversion. As we described earlier, for any \\(\\tau^{2}\\) we reject \\(\\mathbb{M}_{0}: \\mu^{2}=\\tau^{2}\\) in favor of \\(\\mathbb{H}_{1}: \\mu^{2}>\\tau^{2}\\) if \\(\\mathrm{F}>c\\) where \\(G\\left(c, \\tau^{2}\\right)=0.95\\). Equivalently, we reject if \\(G\\left(\\mathrm{~F}, \\tau^{2}\\right)>0.95\\). By the test inversion principle an asymptotic \\(95 %\\) confidence interval \\(\\left[\\mu_{L}^{2}, \\infty\\right)\\) is the set of all values of \\(\\tau^{2}\\) which are not rejected. Since \\(G\\left(\\mathrm{~F}, \\tau^{2}\\right) \\geq 0.95\\) for all \\(\\tau^{2}\\) in this set, the lower bound \\(\\mu_{L}^{2}\\) satisfies \\(G\\left(\\mathrm{~F}, \\mu_{L}^{2}\\right)=0.95\\), and is found numerically. In MATLAB, the solution is mu2 when \\(n c x 2 c d f(F, 1, m u 2)\\) returns \\(0.95 .\\)\nThe second goal is to find the critical value \\(c\\) such that \\(\\mathbb{P}\\left(\\left|S_{1}\\right| \\geq c\\right)=0.05\\) when \\(\\mu^{2}=\\mu_{L}^{2}\\). From (12.84) this is achieved when\n\\[\n1-G\\left(\\frac{\\mu_{L}^{2}}{4}+c \\mu_{L}, \\frac{\\mu_{L}^{2}}{4}\\right)+G\\left(\\frac{\\mu_{L}^{2}}{4}-c \\mu_{L}, \\frac{\\mu_{L}^{2}}{4}\\right)=0.05 .\n\\]\nThis can be solved as\n\\[\nG\\left(\\frac{\\mu_{L}^{2}}{4}+c \\mu_{L}, \\frac{\\mu_{L}^{2}}{4}\\right)=0.95 \\text {. }\n\\]\n(The third term on the left-hand-side of (12.85) is zero for all solutions so can be ignored.) Using the non-central chi-square quantile function \\(Q(p, d)\\), this \\(C\\) equals\n\\[\nc=\\frac{Q\\left(0.95, \\frac{\\mu_{L}^{2}}{4}\\right)-\\frac{\\mu_{L}^{2}}{4}}{\\mu_{L}} .\n\\]\nFor example, in MATLAB this is found as \\(c=(n c x 2 i n v ~(.95,1, \\mathrm{mu} 2 / 4)-\\mathrm{mu} 2 / 4) / \\mathrm{sqrt}(\\mathrm{mu} 2) .95 %\\) confidence intervals for \\(\\beta_{2}\\) are then calculated as \\(\\widehat{\\beta}_{\\mathrm{iv}} \\pm c s\\left(\\widehat{\\beta}_{\\mathrm{iv}}\\right)\\).\nWe can also calculate a p-value for the t-statistic \\(T\\) for \\(\\beta_{2}\\). This is\n\\[\np=1-G\\left(\\frac{\\mu_{L}^{2}}{4}+|T| \\mu_{L}, \\frac{\\mu_{L}^{2}}{4}\\right)+G\\left(\\frac{\\mu_{L}^{2}}{4}-|T| \\mu_{L}, \\frac{\\mu_{L}^{2}}{4}\\right)\n\\]\nwhere the third term equals zero if \\(|T| \\geq \\mu_{L} / 4\\). In MATLAB, for example, this can be calculated by the commands\n\\(\\mathrm{T} 1=\\mathrm{mu} 2 / 4+\\operatorname{abs}(\\mathrm{T}) * \\operatorname{sqrt}(\\mathrm{mu} 2)\\)\n\\(\\mathrm{T} 2=\\mathrm{mu} 2 / 4-\\operatorname{abs}(\\mathrm{T}) * \\operatorname{sqrt}(\\mathrm{mu} 2) ;\\)\n\\(\\mathrm{p}=-\\mathrm{ncx} 2 \\mathrm{cdf}(\\mathrm{T} 1,1, \\mathrm{mu} 2 / 4)+\\mathrm{ncx} 2 \\mathrm{cdf}(\\mathrm{T} 2,1, \\mathrm{mu} 2 / 4)\\);\nThese confidence intervals and p-values will be larger than the conventional intervals and p-values, reflecting the incorporation of information about the strength of the instruments through the first-stage \\(F\\) statistic. Also, by the Bonferroni bound these tests have asymptotic size bounded below \\(10 %\\) and the confidence intervals have asymptotic converage exceeding \\(90 %\\), unlike the Stock-Yogo method which has size of \\(20 %\\) and coverage of \\(80 %\\).\nThe augmented procedure suggested here, only for the \\(\\ell_{2}=1\\) case, is\n\nFind \\(\\mu_{L}^{2}\\) which solves \\(G\\left(\\mathrm{~F}, \\mu_{L}^{2}\\right)=0.95\\). In MATLAB, the solution is mu2 when \\(\\operatorname{cx} 2 \\operatorname{cdf}(\\mathrm{F}, 1, \\operatorname{mu} 2)\\) returns \\(0.95 .\\)\nFind \\(c\\) which solves \\(G\\left(\\mu_{L}^{2} / 4+c \\mu_{L}, \\mu_{L}^{2} / 4\\right)=0.95\\). In MATLAB, the command is \\(c=(n c x 2 \\operatorname{inv}(.95,1, \\mathrm{mu} 2 / 4)-\\mathrm{mu} 2 / 4) / \\mathrm{sqrt}(\\mathrm{mu} 2)\\)\nReport the confidence interval \\(\\widehat{\\beta}_{2} \\pm c s\\left(\\widehat{\\beta}_{2}\\right)\\) for \\(\\beta_{2}\\).\nFor the \\(\\mathrm{t}\\) statistic \\(T=\\left(\\widehat{\\beta}_{2}-\\beta_{2}\\right) / s\\left(\\widehat{\\beta}_{2}\\right)\\) the asymptotic \\(\\mathrm{p}\\)-value is\n\n\\[\np=1-G\\left(\\frac{\\mu_{L}^{2}}{4}+|T| \\mu_{L}, \\frac{\\mu_{L}^{2}}{4}\\right)+G\\left(\\frac{\\mu_{L}^{2}}{4}-|T| \\mu_{L}, \\frac{\\mu_{L}^{2}}{4}\\right)\n\\]\nwhich is computed in MATLAB by \\(\\mathrm{T} 1=\\mathrm{mu} 2 / 4+\\mathrm{abs}(\\mathrm{T}) * \\operatorname{sqrt}(\\mathrm{mu} 2) ; \\mathrm{T} 2=\\mathrm{mu} 2 / 4-\\mathrm{abs}(\\mathrm{T}) * \\mathrm{sqrt}(\\mathrm{mu} 2)\\); and \\(\\mathrm{p}=1-\\mathrm{ncx} 2 \\mathrm{cdf}(\\mathrm{T} 1,1, \\mathrm{mu} 2 / 4)+\\mathrm{ncx} 2 \\mathrm{cdf}(\\mathrm{T} 2,1, \\mathrm{mu} 2 / 4)\\).\nWe have described an extension to the Stock-Yogo procedure for the case of one instrumental variable \\(\\ell_{2}=1\\). This restriction was due to the use of the analytic formula (12.85) for the asymptotic distribution which is only available when \\(\\ell_{2}=1\\). In principle the procedure could be extended using simulation or bootstrap methods but this has not been done to my knowledge.\nTo illustrate the Stock-Yogo and extended procedures let us return to the Card proximity example. Take the IV estimates reported in the second column of Table \\(12.1\\) which used college proximity as a single instrument. The reduced form estimates for the endogenous variable education are reported in the second column of Table 12.2. The excluded instrument college has a t-ratio of \\(4.2\\) which implies an \\(F\\) statistic of 17.8. The \\(F\\) statistic exceeds the rule-of thumb of 10 so the structural estimates pass the Stock-Yogo threshold. Based on their recommendation this means that we can interpret the estimates conventionally. However, the conventional confidence interval, e.g. for the returns to education \\(0.132 \\pm\\) \\(0.049 \\times 1.96=[0.04,0.23]\\), has an asymptotic coverage of \\(80 %\\) rather than the nominal \\(95 %\\) rate.\nNow consider the extended procedure. Given \\(\\mathrm{F}=17.8\\) we calculate the lower bound \\(\\mu_{L}^{2}=6.6\\). This implies a critical value of \\(C=2.7\\). Hence an improved confidence interval for the returns to education in this equation is \\(0.132 \\pm 0.049 \\times 2.7=[0.01,0.26]\\). This is a wider confidence interval but has improved asymptotic coverage of \\(90 %\\). The p-value for \\(\\beta_{2}=0\\) is \\(p=0.012\\).\nNext, take the 2SLS estimates reported in the fourth column of Table \\(11.1\\) which use the two instruments public and private. The reduced form equation is reported in column six of Table 12.2. An \\(F\\) statistic for exclusion of the two instruments is \\(F=13.9\\) which exceeds the \\(15 %\\) size threshold for 2SLS and all thresholds for LIML, indicating that the structural estimates pass the Stock-Yogo threshold test and can be interpreted conventionally.\nThe weak instrument methods described here are important for applied econometrics as they discipline researchers to assess the quality of their reduced form relationships before reporting structural estimates. The theory, however, has limitations and shortcomings, in particular the strong assumption of conditional homoskedasticity. Despite this limitation, in practice researchers apply the Stock-Yogo recommendations to estimates computed with heteroskedasticity-robust standard errors. This is an active area of research so the recommended methods may change in the years ahead."
  },
  {
    "objectID": "chpt12-iv.html#weak-instruments-with-k_21",
    "href": "chpt12-iv.html#weak-instruments-with-k_21",
    "title": "12  Instrumental Variables",
    "section": "12.39 Weak Instruments with \\(k_{2}>1\\)",
    "text": "12.39 Weak Instruments with \\(k_{2}>1\\)\nWhen there is more than one endogenous regressor \\(\\left(k_{2}>1\\right)\\) it is better to examine the reduced form as a system. Staiger and Stock (1997) and Stock and Yogo (2005) provided an analysis of this case and constructed a test for weak instruments. The theory is considerably more involved than the \\(k_{2}=1\\) case so we briefly summarize it here excluding many details, emphasizing their suggested methods.\nThe structural equation and reduced form equations are\n\\[\n\\begin{aligned}\n&Y_{1}=Z_{1}^{\\prime} \\beta_{1}+Y_{2}^{\\prime} \\beta_{2}+e \\\\\n&Y_{2}=\\Gamma_{12}^{\\prime} Z_{1}+\\Gamma_{22}^{\\prime} Z_{2}+u_{2} .\n\\end{aligned}\n\\]\nAs in the previous section we assume that the errors are conditionally homoskedastic.\nIdentification of \\(\\beta_{2}\\) requires the matrix \\(\\Gamma_{22}\\) to be full rank. A necessary condition is that each row of \\(\\Gamma_{22}^{\\prime}\\) is non-zero but this is not sufficient.\nWe focus on the size performance of the homoskedastic Wald statistic for the 2SLS estimator of \\(\\beta_{2}\\). For simplicity assume that the variance of \\(e\\) is known and normalized to one. Using representation (12.32), the Wald statistic can be written as\n\\[\nW=\\boldsymbol{e}^{\\prime} \\widetilde{\\boldsymbol{Z}}_{2}\\left(\\widetilde{\\boldsymbol{Z}}_{2}^{\\prime} \\widetilde{\\boldsymbol{Z}}_{2}\\right)^{-1} \\widetilde{\\boldsymbol{Z}}_{2}^{\\prime} \\boldsymbol{Y}_{2}\\left(\\boldsymbol{Y}_{2}^{\\prime} \\widetilde{\\boldsymbol{Z}}_{2}\\left(\\widetilde{\\boldsymbol{Z}}_{2}^{\\prime} \\widetilde{\\boldsymbol{Z}}_{2}\\right)^{-1} \\widetilde{\\boldsymbol{Z}}_{2}^{\\prime} \\boldsymbol{Y}_{2}\\right)^{-1}\\left(\\boldsymbol{Y}_{2}^{\\prime} \\widetilde{\\boldsymbol{Z}}_{2}\\left(\\widetilde{\\boldsymbol{Z}}_{2}^{\\prime} \\widetilde{\\boldsymbol{Z}}_{2}\\right)^{-1} \\widetilde{\\boldsymbol{Z}}_{2}^{\\prime} \\boldsymbol{e}\\right)\n\\]\nwhere \\(\\widetilde{\\boldsymbol{Z}}_{2}=\\left(\\boldsymbol{I}_{n}-\\boldsymbol{P}_{1}\\right) \\boldsymbol{Z}_{2}\\) and \\(\\boldsymbol{P}_{1}=\\boldsymbol{Z}_{1}\\left(\\boldsymbol{Z}_{1}^{\\prime} \\boldsymbol{Z}_{1}\\right)^{-1} \\boldsymbol{Z}_{1}^{\\prime}\\).\nRecall from Section \\(12.36\\) that Stock and Staiger model the excluded instruments \\(Z_{2}\\) as weak by setting \\(\\Gamma_{22}=n^{-1 / 2} \\boldsymbol{C}\\) for some matrix \\(\\boldsymbol{C}\\). In this framework we have the asymptotic distribution results\n\\[\n\\frac{1}{n} \\widetilde{\\boldsymbol{Z}}_{2}^{\\prime} \\widetilde{\\boldsymbol{Z}}_{2} \\underset{p}{\\longrightarrow} \\boldsymbol{Q}=\\mathbb{E}\\left[Z_{2} Z_{2}^{\\prime}\\right]-\\mathbb{E}\\left[Z_{2} Z_{1}^{\\prime}\\right]\\left(\\mathbb{E}\\left[Z_{1} Z_{1}^{\\prime}\\right]\\right)^{-1} \\mathbb{E}\\left[Z_{1} Z_{2}^{\\prime}\\right]\n\\]\nand\n\\[\n\\frac{1}{\\sqrt{n}} \\widetilde{\\boldsymbol{Z}}_{2}^{\\prime} \\boldsymbol{e} \\underset{d}{\\longrightarrow} \\boldsymbol{Q}^{1 / 2} \\xi_{0}\n\\]\nwhere \\(\\xi_{0}\\) is a matrix normal variate whose columns are independent \\(\\mathrm{N}(0, \\boldsymbol{I})\\). Furthermore, setting \\(\\Sigma=\\) \\(\\mathbb{E}\\left[u_{2} u_{2}^{\\prime}\\right]\\) and \\(\\overline{\\boldsymbol{C}}=\\boldsymbol{Q}^{1 / 2} \\boldsymbol{C} \\Sigma^{-1 / 2}\\),\n\\[\n\\frac{1}{\\sqrt{n}} \\widetilde{\\boldsymbol{Z}}_{2}^{\\prime} \\boldsymbol{Y}_{2}=\\frac{1}{n} \\widetilde{\\boldsymbol{Z}}_{2}^{\\prime} \\widetilde{\\boldsymbol{Z}}_{2} \\boldsymbol{C}+\\frac{1}{\\sqrt{n}} \\widetilde{\\boldsymbol{Z}}_{2}^{\\prime} \\boldsymbol{U}_{2} \\underset{d}{\\longrightarrow} \\boldsymbol{Q}^{1 / 2} \\overline{\\boldsymbol{C}} \\Sigma^{1 / 2}+\\boldsymbol{Q}^{1 / 2} \\xi_{2} \\Sigma^{1 / 2}\n\\]\nwhere \\(\\xi_{2}\\) is a matrix normal variate whose columns are independent \\(\\mathrm{N}(0, \\boldsymbol{I})\\). The variables \\(\\xi_{0}\\) and \\(\\xi_{2}\\) are correlated. Together we obtain the asymptotic distribution of the Wald statistic\n\\[\nW \\underset{d}{\\longrightarrow} S=\\xi_{0}^{\\prime}\\left(\\overline{\\boldsymbol{C}}+\\xi_{2}\\right)\\left(\\overline{\\boldsymbol{C}}^{\\prime} \\overline{\\boldsymbol{C}}\\right)^{-1}\\left(\\overline{\\boldsymbol{C}}+\\xi_{2}\\right)^{\\prime} \\xi_{0}\n\\]\nUsing the spectral decomposition, \\(\\overline{\\boldsymbol{C}}^{\\prime} \\overline{\\boldsymbol{C}}=\\boldsymbol{H}^{\\prime} \\Lambda \\boldsymbol{H}\\) where \\(\\boldsymbol{H}^{\\prime} \\boldsymbol{H}=\\boldsymbol{I}\\) and \\(\\Lambda\\) is diagonal. Thus we can write \\(S=\\xi_{0}^{\\prime} \\bar{\\xi}_{2} \\Lambda^{-1} \\bar{\\xi}_{2}^{\\prime} \\xi_{0}\\) where \\(\\bar{\\xi}_{2}=\\overline{\\boldsymbol{C}} \\boldsymbol{H}^{\\prime}+\\xi_{2} \\boldsymbol{H}^{\\prime}\\). The matrix \\(\\xi^{*}=\\left(\\xi_{0}, \\bar{\\xi}_{2}\\right)\\) is multivariate normal, so \\(\\xi^{* \\prime} \\xi^{*}\\) has what is called a non-central Wishart distribution. It only depends on the matrix \\(\\overline{\\boldsymbol{C}}\\) through \\(\\boldsymbol{H} \\overline{\\boldsymbol{C}}^{\\prime} \\overline{\\boldsymbol{C}} \\boldsymbol{H}^{\\prime}=\\Lambda\\) which are the eigenvalues of \\(\\overline{\\boldsymbol{C}}^{\\prime} \\overline{\\boldsymbol{C}}\\). Since \\(S\\) is a function of \\(\\xi^{*}\\) only through \\(\\bar{\\xi}_{2}^{\\prime} \\xi_{0}\\) we conclude that \\(S\\) is a function of \\(\\overline{\\boldsymbol{C}}\\) only through these eigenvalues.\nThis is a very quick derivation of a rather involved derivation but the conclusion drawn by Stock and Yogo is that the asymptotic distribution of the Wald statistic is non-standard and a function of the model parameters only through the eigenvalues of \\(\\overline{\\boldsymbol{C}} \\overline{\\bar{C}}\\) and the correlations between the normal variates \\(\\xi_{0}\\) and \\(\\bar{\\xi}_{2}\\). The worst-case can be summarized by the maximal correlation between \\(\\xi_{0}\\) and \\(\\bar{\\xi}_{2}\\) and the smallest eigenvalue of \\(\\overline{\\boldsymbol{C}}^{\\prime} \\overline{\\boldsymbol{C}}\\). For convenience they rescale the latter by dividing by the number of endogenous variables. Define\n\\[\n\\boldsymbol{G}=\\overline{\\boldsymbol{C}}^{\\prime} \\overline{\\boldsymbol{C}} / k_{2}=\\Sigma^{-1 / 2} \\boldsymbol{C}^{\\prime} \\boldsymbol{Q} \\boldsymbol{C} \\Sigma^{-1 / 2} / k_{2}\n\\]\nand\n\\[\ng=\\lambda_{\\min }(\\boldsymbol{G})=\\lambda_{\\min }\\left(\\Sigma^{-1 / 2} \\boldsymbol{C}^{\\prime} \\boldsymbol{Q} \\boldsymbol{C} \\Sigma^{-1 / 2}\\right) / k_{2} .\n\\]\nThis can be estimated from the reduced-form regression\n\\[\nX_{2 i}=\\widehat{\\Gamma}_{12}^{\\prime} Z_{1 i}+\\widehat{\\Gamma}_{22}^{\\prime} Z_{2 i}+\\widehat{u}_{2 i} .\n\\]\nThe estimator is\n\\[\n\\begin{aligned}\n\\widehat{\\boldsymbol{G}} &=\\widehat{\\Sigma}^{-1 / 2} \\widehat{\\Gamma}_{22}^{\\prime}\\left(\\widetilde{\\boldsymbol{Z}}_{2}^{\\prime} \\widetilde{\\boldsymbol{Z}}_{2}\\right) \\widehat{\\Gamma}_{22} \\widehat{\\Sigma}^{-1 / 2} / k_{2}=\\widehat{\\Sigma}^{-1 / 2}\\left(\\boldsymbol{X}_{2}^{\\prime} \\widetilde{\\boldsymbol{Z}}_{2}\\left(\\widetilde{\\boldsymbol{Z}}_{2}^{\\prime} \\widetilde{\\boldsymbol{Z}}_{2}\\right)^{-1} \\widetilde{\\boldsymbol{Z}}_{2}^{\\prime} \\boldsymbol{X}_{2}\\right) \\widehat{\\Sigma}^{-1 / 2} / k_{2} \\\\\n\\widehat{\\Sigma} &=\\frac{1}{n-k} \\sum_{i=1}^{n} \\widehat{u}_{2 i} \\widehat{u}_{2 i}^{\\prime} \\\\\n\\widehat{g} &=\\lambda_{\\min }(\\widehat{\\boldsymbol{G}})\n\\end{aligned}\n\\]\n\\(\\widehat{\\boldsymbol{G}}\\) is a matrix \\(F\\)-type statistic for the coefficient matrix \\(\\widehat{\\Gamma}_{22}\\).\nThe statistic \\(\\widehat{g}\\) was proposed by Cragg and Donald (1993) as a test for underidentification. Stock and Yogo (2005) use it as a test for weak instruments. Using simulation methods they determined critical values for \\(\\widehat{g}\\) similar to those for \\(k_{2}=1\\). For given size \\(r>0.05\\) there is a critical value \\(c\\) (reported in the table below) such that if \\(\\widehat{g}>c\\) then the 2SLS (or LIML) Wald statistic \\(W\\) for \\(\\widehat{\\beta}_{2}\\) has asymptotic size bounded below \\(r\\). On the other hand, if \\(\\widehat{g} \\leq c\\) then we cannot bound the asymptotic size below \\(r\\) and we cannot reject the hypothesis of weak instruments. Critical values (calculated by simulation) are reported in their paper and posted on Motohiro Yogo’s webpage. We report a subset for the case \\(k_{2}=2\\) in Table 12.5. The methods and theory applies to the cases \\(k_{2}>2\\) as well but those critical values have not been calculated. As for the \\(k_{2}=1\\) case the critical values for 2 SLS are dramatically increasing in \\(\\ell_{2}\\). Thus when the model is over-identified, we need a large value of \\(\\widehat{g}\\) to reject the hypothesis of weak instruments. This is a strong cautionary message to check the \\(\\widehat{g}\\) statistic in applications. Furthermore, the critical values for LIML are generally decreasing in \\(\\ell_{2}\\) (except for \\(r=0.10\\) where the critical values are increasing for large \\(\\ell_{2}\\) ). This means that for over-identified models LIML inference is less sensitive to weak instruments than 2SLS and may be the preferred estimation method.\nThe Stock-Yogo test can be implemented in Stata using the command estat firststage after ivregress 2sls or ivregres \\(\\operatorname{liml}\\) if a standard (non-robust) covariance matrix has been specified (that is, without the ‘, r’ option). Critical values which control for size are only available for \\(k_{2} \\leq 2\\). For for \\(k_{2}>2\\) critical values which control for relative bias are reported.\nRobust versions of the test have been proposed by Kleibergen and Paap (2006). These can be implemented in Stata using the downloadable command ivreg2.\nTable 12.5: 5% Critical Value for Weak Instruments, \\(k_{2}=2\\)\n|\\(\\ell_{2}\\)|\\(0.10\\)|\\(0.15\\)|\\(0.20\\)|\\(0.25\\)||\\(0.10\\)|\\(0.15\\)|\\(0.20\\)|\\(0.25\\)| |———:|—–:|—–:|—–:|—–:||—–:|—–:|—–:|—–:| | 2| \\(7.0\\)| \\(4.6\\)| \\(3.9\\)| \\(3.6\\)|| \\(7.0\\)| \\(4.6\\)| \\(3.9\\)| \\(3.6\\)| | 3|\\(13.4\\)| \\(8.2\\)| \\(6.4\\)| \\(5.4\\)|| \\(5.4\\)| \\(3.8\\)| \\(3.3\\)| \\(3.1\\)| | 4|\\(16.9\\)| \\(9.9\\)| \\(7.5\\)| \\(6.3\\)|| \\(4.7\\)| \\(3.4\\)| \\(3.0\\)| \\(2.8\\)| | 5|\\(19.4\\)|\\(11.2\\)| \\(8.4\\)| \\(6.9\\)|| \\(4.3\\)| \\(3.1\\)| \\(2.8\\)| \\(2.6\\)| | 6|\\(21.7\\)|\\(12.3\\)| \\(9.1\\)| \\(7.4\\)|| \\(4.1\\)| \\(2.9\\)| \\(2.6\\)| \\(2.5\\)| | 7|\\(23.7\\)|\\(13.3\\)| \\(9.8\\)| \\(7.9\\)|| \\(3.9\\)| \\(2.8\\)| \\(2.5\\)| \\(2.4\\)| | 8|\\(25.6\\)|\\(14.3\\)|\\(10.4\\)| \\(8.4\\)|| \\(3.8\\)| \\(2.7\\)| \\(2.4\\)| \\(2.3\\)| | 9|\\(27.5\\)|\\(15.2\\)|\\(11.0\\)| \\(8.8\\)|| \\(3.7\\)| \\(2.7\\)| \\(2.4\\)| \\(2.2\\)| | 10|\\(29.3\\)|\\(16.2\\)|\\(11.6\\)| \\(9.3\\)|| \\(3.6\\)| \\(2.6\\)| \\(2.3\\)| \\(2.1\\)| | 15|\\(38.0\\)|\\(20.6\\)|\\(14.6\\)|\\(11.6\\)|| \\(3.5\\)| \\(2.4\\)| \\(2.1\\)| \\(2.0\\)| | 20|\\(46.6\\)|\\(25.0\\)|\\(17.6\\)|\\(13.8\\)|| \\(3.6\\)| \\(2.4\\)| \\(2.0\\)| \\(1.9\\)| | 25|\\(55.1\\)|\\(29.3\\)|\\(20.6\\)|\\(16.1\\)|| \\(3.6\\)| \\(2.4\\)|\\(1.97\\)| \\(1.8\\)| | 30|\\(63.5\\)|\\(33.6\\)|\\(23.5\\)|\\(18.3\\)|| \\(4.1\\)| \\(2.4\\)|\\(1.95\\)| \\(1.7\\)|\nSource: ."
  },
  {
    "objectID": "chpt12-iv.html#example-acemoglu-johnson-and-robinson-2001",
    "href": "chpt12-iv.html#example-acemoglu-johnson-and-robinson-2001",
    "title": "12  Instrumental Variables",
    "section": "12.40 Example: Acemoglu, Johnson, and Robinson (2001)",
    "text": "12.40 Example: Acemoglu, Johnson, and Robinson (2001)\nOne particularly well-cited instrumental variable regression is in Acemoglu, Johnson, and Robinson (2001) with additional details published in (2012). They are interested in the effect of political institutions on economic performance. The theory is that good institutions (rule-of-law, property rights) should result in a country having higher long-term economic output than if the same country had poor institutions. To investigate this question they focus on a sample of 64 former European colonies. Their data is in the file AJR2001 on the textbook website.\nThe authors’ premise is that modern political institutions have been influenced by colonization. In particular they argue that colonizing countries tended to set up colonies as either an “extractive state” or as a “migrant colony”. An extractive state was used by the colonizer to extract resources for the colonizing country but was not largely settled by the European colonists. In this case the colonists had no incentive to set up good political institutions. In contrast, if a colony was set up as a “migrant colony” then large numbers of European settlers migrated to the colony to live. These settlers desired institutions similar to those in their home country and hence had an incentive to set up good political institutions. The nature of institutions is quite persistent over time so these \\(19^{t h}\\)-century foundations affect the nature of modern institutions. The authors conclude that the \\(19^{t h}\\)-century nature of the colony is predictive of the nature of modern institutions and hence modern economic growth.\nTo start the investigation they report an OLS regression of log GDP per capita in 1995 on a measure of political institutions they call risk which is a measure of legal protection against expropriation. This variable ranges from 0 to 10 , with 0 the lowest protection against appropriation and 10 the highest. For each country the authors take the average value of the index over 1985 to 1995 (the mean is \\(6.5\\) with a standard deviation of 1.5). Their reported OLS estimates (intercept omitted) are\n\nThese estimates imply a \\(52 %\\) difference in GDP between countries with a 1 -unit difference in risk.\nThe authors argue that the risk is endogenous since economic output influences political institutions and because the variable risk is undoubtedly measured with error. These issues induce least-square bias in different directions and thus the overall bias effect is unclear.\nTo correct for endogeneity bias the authors argue the need for an instrumental variable which does not directly affect economic performance yet is associated with political institutions. Their innovative suggestion was to use the mortality rate which faced potential European settlers in the \\(19^{t h}\\) century. Colonies with high expected mortality were less attractive to European settlers resulting in lower levels of European migrants. As a consequence the authors expect such colonies to be more likely structured as an extractive state rather than a migrant colony. To measure the expected mortality rate the authors use estimates provided by historical research of the annualized deaths per 1000 soldiers, labeled mortality. (They used military mortality rates as the military maintained high-quality records.) The first-stage regression is\n\\[\n\\text { risk }=\\underset{(0.13)}{-0.61} \\log (\\text { mortality })+\\widehat{u} .\n\\]\nThese estimates confirm that \\(19^{t h}\\)-century high mortality rates are associated with lower quality modern institutions. Using \\(\\log\\) (mortality) as an instrument for risk, they estimate the structural equation using 2SLS and report\n\\[\n\\log (\\text { GDP per Capita })=\\begin{gathered}\n0.94 \\text { risk. } \\\\\n(0.16)\n\\end{gathered}\n\\]\nThis estimate is much higher than the OLS estimate from (12.86). The estimate is consistent with a near doubling of GDP due to a 1-unit difference in the risk index.\nThese are simple regressions involving just one right-hand-side variable. The authors considered a range of other models. Included in these results are a reversal of a traditional finding. In a conventional least squares regression two relevant variables for output are latitude (distance from the equator) and africa (a dummy variable for countries from Africa) both of which are difficult to interpret causally. But in the proposed instrumental variables regression the variables latitude and africa have much smaller and statistically insignificant - coefficients. To assess the specification we can use the Stock-Yogo and endogeneity tests. The Stock-Yogo test is from the reduced form (12.87). The instrument has a t-ratio of \\(4.8\\) (or \\(F=23\\) ) which exceeds the StockYogo critical value and hence can be treated as strong. For an endogeneity test we take the least squares residual \\(\\widehat{u}\\) from this equation and include it in the structural equation and estimate by least squares. We find a coefficient on \\(\\widehat{u}\\) of \\(-0.57\\) with a t-ratio of \\(4.7\\) which is highly significant. We conclude that the least squares and 2SLS estimates are statistically different and reject the hypothesis that the variable risk is exogenous for the GDP structural equation.\nIn Exercise \\(12.22\\) you will replicate and extend these results using the authors’ data.\nThis paper is a creative and careful use of instrumental variables. The creativity stems from the historical analysis which lead to the focus on mortality as a potential predictor of migration choices. The care comes in the implementation as the authors needed to gather country-level data on political institutions and mortality from distinct sources. Putting these pieces together is the art of the project."
  },
  {
    "objectID": "chpt12-iv.html#example-angrist-and-krueger-1991",
    "href": "chpt12-iv.html#example-angrist-and-krueger-1991",
    "title": "12  Instrumental Variables",
    "section": "12.41 Example: Angrist and Krueger (1991)",
    "text": "12.41 Example: Angrist and Krueger (1991)\nAnother influential instrument variable regression is Angrist and Krueger (1991). Their concern, similar to Card (1995), is estimation of the structural returns to education while treating educational attainment as endogenous. Like Card, their goal is to find an instrument which is exogenous for wages yet has an impact on education. A subset of their data in the file AK1991 on the textbook website.\nTheir creative suggestion was to focus on compulsory school attendance policies and their interaction with birthdates. Compulsory schooling laws vary across states in the United States, but typically require that youth remain in school until their sixteenth or seventeenth birthday. Angrist and Krueger argue that compulsory schooling has a causal effect on wages - youth who would have chosen to drop out of school stay in school for more years - and thus have more education which causally impacts their earnings as adults.\nAngrist and Krueger observe that these policies have differential impact on youth who are born early or late in the school year. Students who are born early in the calendar year are typically older when they enter school. Consequently when they attain the legal dropout age they have attended less school than those born near the end of the year. This means that birthdate (early in the calendar year versus late) exogenously impacts educational attainment and thus wages through education. Yet birthdate must be exogenous for the structural wage equation as there is no reason to believe that birthdate itself has a causal impact on a person’s ability or wages. These considerations together suggest that birthdate is a valid instrumental variable for education in a causal wage equation.\nTypical wage datasets include age but not birthdates. To obtain information on birthdate, Angrist and Krueger used U.S. Census data which includes an individual’s quarter of birth (January-March, AprilJune, etc.). They use this variable to construct 2SLS estimates of the return to education.\nTheir paper carefully documents that educational attainment varies by quarter of birth (as predicted by the above discussion), and reports a large set of least squares and 2SLS estimates. We focus on two estimates at the core of their analysis, reported in column (6) of their Tables \\(\\mathrm{V}\\) and VII. This involves data from the 1980 census with men born in 1930-1939, with 329,509 observations. The first equation is\n\nwhere \\(e d u\\) is years of education and Black, urban, and married are dummy variables indicating race (1 if Black, 0 otherwise), lives in a metropolitan area, and if married. In addition to the reported coefficients the equation also includes as regressors nine year-of-birth dummies and eight region-of-residence dummies. The equation is estimated by 2 SLS. The instrumental variables are the 30 interactions of three quarter-of-birth times ten year-of-birth dummy variables.\nThis equation indicates an \\(8 %\\) increase in wages due to each year of education.\nAngrist and Krueger observe that the effect of compulsory education laws are likely to vary across states, so expand the instrument set to include interactions with state-of-birth. They estimate the following equation by 2 SLS\n\nThis equation also adds fifty state-of-birth dummy variables as regressors. The instrumental variables are the 180 interactions of quarter-of-birth times year-of-birth dummy variables, plus quarter-of-birth times state-of-birth interactions.\nThis equation shows a similar estimated causal effect of education on wages as in (12.89). More notably, the standard error is smaller in (12.90) suggesting improved precision by the expanded instrumental variable set.\nHowever, these estimates seem excellent candidates for weak instruments and many instruments. Indeed, this paper (published in 1991) helped spark these two literatures. We can use the Stock-Yogo tools to explore the instrument strength and the implications for the Angrist-Krueger estimates.\nWe first take equation (12.89). Using the original Angrist-Krueger data we estimate the corresponding reduced form and calculate the \\(F\\) statistic for the 30 excluded instruments. We find \\(F=4.8\\). It has an asymptotic p-value of \\(0.000\\) suggesting that we can reject (at any significance level) the hypothesis that the coefficients on the excluded instruments are zero. Thus Angrist and Krueger appear to be correct that quarter of birth helps to explain educational attainment and are thus a valid instrumental variable set. However, using the Stock-Yogo test, \\(F=4.8\\) is not high enough to reject the hypothesis that the instruments are weak. Specifically, for \\(\\ell_{2}=30\\) and \\(15 %\\) size the critical value for the \\(F\\) statistic is 45 . The actual value of \\(4.8\\) is far below 45. Since we cannot reject that the instruments are weak this indicates that we cannot interpret the 2SLS estimates and test statistics in (12.89) as reliable.\nSecond, take (12.90) with the expanded regressor and instrument set. Estimating the corresponding reduced form we find the \\(F\\) statistic for the 180 excluded instruments is \\(\\mathrm{F}=2.43\\) which also has an asymptotic p-value of \\(0.000\\) indicating that we can reject at any significance level the hypothesis that the excluded instruments have no effect on educational attainment. However, using the Stock-Yogo test we also cannot reject the hypothesis that the instruments are weak. While Stock and Yogo did not calculate the critical values for \\(\\ell_{2}=180\\), the 2 SLS critical values are increasing in \\(\\ell_{2}\\) so we can use those for \\(\\ell_{2}=30\\) as a lower bound. The observed value of \\(\\mathrm{F}=2.43\\) is far below the level needed for significance. Consequently the results in (12.90) cannot be viewed as reliable. In particular, the observation that the standard errors in (12.90) are smaller than those in (12.89) should not be interpreted as evidence of greater precision. Rather, they should be viewed as evidence of unreliability due to weak instruments.\nWhen instruments are weak one constructive suggestion is to use LIML estimation rather than 2SLS. Another constructive suggestion is to alter the instrument set. While Angrist and Krueger used a large number of instrumental variables we can consider a smaller set. Take equation (12.89). Rather than estimating it using the 30 interaction instruments consider using only the three quarter-of-birth dummy variables. We report the reduced form estimates here:\n\nwhere \\(Q_{2}, Q_{3}\\), and \\(Q_{4}\\) are dummy variables for birth in the \\(2^{n d}, 3^{r d}\\), and \\(4^{t h}\\) quarter. The regression also includes nine year-of-birth and eight region-of-residence dummy variables.\nThe reduced form coefficients in (12.91) on the quarter-of-birth dummies are instructive. The coefficients are positive and increasing, consistent with the Angrist-Krueger hypothesis that individuals born later in the year achieve higher average education. Focusing on the weak instrument problem the \\(F\\) test for exclusion of these three variables is \\(\\mathrm{F}=31\\). The Stock-Yogo critical value is \\(12.8\\) for \\(\\ell_{2}=3\\) and a size of \\(15 %\\), and is \\(22.3\\) for a size of \\(10 %\\). Since \\(F=31\\) exceeds both these thresholds we can reject the hypothesis that this reduced form is weak. Estimating the model by 2SLS with these three instruments we find\n\nThese estimates indicate a slightly larger (10%) causal impact of education on wages but with a larger standard error. The Stock-Yogo analysis indicates that we can interpret the confidence intervals from these estimates as having asymptotic coverage \\(85 %\\).\nWhile the original Angrist-Krueger estimates suffer due to weak instruments their paper is a very creative and thoughtful application of the natural experiment methodology. They discovered a completely exogenous variation present in the world - birthdate - and showed how this has a small but measurable effect on educational attainment and thereby on earnings. Their crafting of this natural experiment regression is clever and demonstrates a style of analysis which can successfully underlie an effective instrumental variables empirical analysis."
  },
  {
    "objectID": "chpt12-iv.html#programming",
    "href": "chpt12-iv.html#programming",
    "title": "12  Instrumental Variables",
    "section": "12.42 Programming",
    "text": "12.42 Programming\nWe now present Stata code for some of the empirical work reported in this chapter.\nStata do File for Card Example use Card1995.dta, clear\nset more off\ngen exp = age \\(76-\\) ed \\(76-6\\)\ngen \\(\\exp 2=\\left(\\exp ^{\\wedge} 2\\right) / 100\\)\n\nDrop observations with missing wage\n\ndrop if lwage \\(76==.\\)\n\nTable \\(12.1\\) regressions\n\nreg lwage76 ed76 exp exp2 black reg76r smsa76r, \\(r\\)\nivregress 2 sls lwage76 exp exp2 black reg76r smsa76r (ed76=nearc4), r\nivregress 2 sls lwage76 black reg76r smsa76r (ed76 exp exp2 \\(=\\) nearc4 age76 age2), r perfect\nivregress 2sls lwage76 exp exp2 black reg76r smsa76r (ed76=nearc4a nearc4b), \\(\\mathrm{r}\\)\nivregress 2 sls lwage76 black reg76r smsa76r (ed76 exp exp2 \\(=\\) nearc4a nearc4b age76 age2), \\(r\\) perfect\nivregress liml lwage76 exp exp2 black reg76r smsa76r (ed76=nearc4a nearc4b), \\(r\\)\n\nTable \\(12.2\\) regressions\n\nreg lwage76 exp exp2 black reg76r smsa76r nearc4, \\(r\\)\nreg ed76 exp exp2 black reg76r smsa76r nearc4, \\(r\\)\nreg ed76 black reg76r smsa76r nearc4 age76 age2, \\(r\\)\nreg exp black reg76r smsa76r nearc4 age76 age2, \\(r\\)\nreg exp2 black reg76r smsa76r nearc4 age76 age2, \\(r\\)\nreg ed76 exp exp2 black reg76r smsa76r nearc4a nearc4b, \\(r\\)\nreg lwage76 ed76 exp exp2 smsa76r reg76r, \\(r\\)\nreg lwage76 nearc4 exp exp2 smsa76r reg76r, \\(r\\)\nreg ed76 nearc4 exp exp2 smsa76r reg76r, \\(r\\)\nStata do File for Acemoglu-Johnson-Robinson Example use AJR2001.dta, clear\nreg loggdp risk\nreg risk logmort0\npredict \\(u\\), residual\nivregress 2sls loggdp (risk=logmort0)\nreg loggdp risk \\(u\\)\n\n\n\n\n\n\nStata do File for Angrist-Krueger Example\n\n\n\n\nuse AK1991.dta, clear\n\n\nivregress 2sls logwage black smsa married i.yob i.region (edu = i.qob#i.yob)\n\n\nivregress 2sls logwage black smsa married i.yob i.region i.state (edu \\(=\\)\n\n\ni.qob#i.yob i.qob#i.state)\n\n\nreg edu black smsa married i.yob i.region i.qob#i.yob\n\n\ntestparm i.qob#i.yob\n\n\nreg edu black smsa married i.yob i.region i.state i.qob#i.yob i.qob#i.state\n\n\ntestparm i.qob#i.yob i.qob#i.state\n\n\nreg edu black smsa married i.yob i.region i.qob\n\n\ntestparm i.qob\n\n\nivregress 2sls logwage black smsa married i.yob i.region (edu = i.qob)"
  },
  {
    "objectID": "chpt12-iv.html#exercises",
    "href": "chpt12-iv.html#exercises",
    "title": "12  Instrumental Variables",
    "section": "12.43 Exercises",
    "text": "12.43 Exercises\nExercise 12.1 Consider the single equation model \\(Y=Z \\beta+e\\) where \\(Y\\) and \\(Z\\) are both real-valued \\((1 \\times 1)\\). Let \\(\\widehat{\\beta}\\) denote the IV estimator of \\(\\beta\\) using as an instrument a dummy variable \\(D\\) (takes only the values 0 and 1). Find a simple expression for the IV estimator in this context.\nExercise 12.2 Take the linear model \\(Y=X^{\\prime} \\beta+e\\) with \\(\\mathbb{E}[e \\mid X]=0\\). Suppose \\(\\sigma^{2}(x)=\\mathbb{E}\\left[e^{2} \\mid X=x\\right]\\) is known. Show that the GLS estimator of \\(\\beta\\) can be written as an IV estimator using some instrument \\(Z\\). (Find an expression for \\(Z\\).)\nExercise 12.3 Take the linear model \\(Y=X^{\\prime} \\beta+e\\). Let the OLS estimator for \\(\\beta\\) be \\(\\widehat{\\beta}\\) with OLS residual \\(\\widehat{e}_{i}\\). Let the IV estimator for \\(\\beta\\) using some instrument \\(Z\\) be \\(\\widetilde{\\beta}\\) with IV residual \\(\\widetilde{e}_{i}=Y_{i}-X_{i}^{\\prime} \\widetilde{\\beta}\\). If \\(X\\) is indeed endogenous, will IV “fit” better than OLS in the sense that \\(\\sum_{i=1}^{n} \\widetilde{e}_{i}^{2}<\\sum_{i=1}^{n} \\widehat{e}_{i}^{2}\\), at least in large samples?\nExercise 12.4 The reduced form between the regressors \\(X\\) and instruments \\(Z\\) takes the form \\(X=\\Gamma^{\\prime} Z+u\\) where \\(X\\) is \\(k \\times 1, Z\\) is \\(\\ell \\times 1\\), and \\(\\Gamma\\) is \\(\\ell \\times k\\). The parameter \\(\\Gamma\\) is defined by the population moment condition \\(\\mathbb{E}\\left[Z u^{\\prime}\\right]=0\\). Show that the method of moments estimator for \\(\\Gamma\\) is \\(\\widehat{\\Gamma}=\\left(\\boldsymbol{Z}^{\\prime} \\boldsymbol{Z}\\right)^{-1}\\left(\\boldsymbol{Z}^{\\prime} \\boldsymbol{X}\\right)\\).\nExercise 12.5 In the structural model \\(Y=X^{\\prime} \\beta+e\\) with \\(X=\\Gamma^{\\prime} Z+u\\) and \\(\\Gamma \\ell \\times k, \\ell \\geq k\\), we claim that a necessary condition for \\(\\beta\\) to be identified (can be recovered from the reduced form) is \\(\\operatorname{rank}(\\Gamma)=k\\). Explain why this is true. That is, show that if \\(\\operatorname{rank}(\\Gamma)<k\\) then \\(\\beta\\) is not identified.\nExercise 12.6 For Theorem \\(12.3\\) establish that \\(\\widehat{\\boldsymbol{V}}_{\\beta} \\underset{p}{\\longrightarrow} \\boldsymbol{V}_{\\beta}\\)\nExercise 12.7 Take the linear model \\(Y=X^{\\prime} \\beta+e\\) with \\(\\mathbb{E}[e \\mid X]=0\\) where \\(X\\) and \\(\\beta\\) are \\(1 \\times 1\\).\n\nShow that \\(\\mathbb{E}[X e]=0\\) and \\(\\mathbb{E}\\left[X^{2} e\\right]=0\\). Is \\(Z=\\left(\\begin{array}{ll}X & X^{2}\\end{array}\\right)^{\\prime}\\) a valid instrument for estimation of \\(\\beta\\) ?\nDefine the 2SLS estimator of \\(\\beta\\) using \\(Z\\) as an instrument for \\(X\\). How does this differ from OLS? Exercise 12.8 Suppose that price and quantity are determined by the intersection of the linear demand and supply curves\n\n\\[\n\\begin{aligned}\n\\text { Demand: } & Q=a_{0}+a_{1} P+a_{2} Y+e_{1} \\\\\n\\text { Supply: } & Q=b_{0}+b_{1} P+b_{2} W+e_{2}\n\\end{aligned}\n\\]\nwhere income \\((Y)\\) and wage \\((W)\\) are determined outside the market. In this model are the parameters identified?\nExercise 12.9 Consider the model \\(Y=X^{\\prime} \\beta+e\\) with \\(\\mathbb{E}[e \\mid Z]=0\\) with \\(Y\\) scalar and \\(X\\) and \\(Z\\) each a \\(k\\) vector. You have a random sample \\(\\left(Y_{i}, X_{i}, Z_{i}: i=1, \\ldots, n\\right)\\).\n\nAssume that \\(X\\) is exogenous in the sense that \\(\\mathbb{E}[e \\mid Z, X]=0\\). Is the IV estimator \\(\\widehat{\\beta}_{\\mathrm{iv}}\\) unbiased?\nContinuing to assume that \\(X\\) is exogenous, find the conditional covariance matrix \\(\\operatorname{var}\\left[\\widehat{\\beta}_{\\text {iv }} \\mid \\boldsymbol{X}, \\boldsymbol{Z}\\right]\\).\n\nExercise 12.10 Consider the model\n\\[\n\\begin{aligned}\nY &=X^{\\prime} \\beta+e \\\\\nX &=\\Gamma^{\\prime} Z+u \\\\\n\\mathbb{E}[Z e] &=0 \\\\\n\\mathbb{E}\\left[Z u^{\\prime}\\right] &=0\n\\end{aligned}\n\\]\nwith \\(Y\\) scalar and \\(X\\) and \\(Z\\) each a \\(k\\) vector. You have a random sample \\(\\left(Y_{i}, X_{i}, Z_{i}: i=1, \\ldots, n\\right)\\). Take the control function equation \\(e=u^{\\prime} \\gamma+v\\) with \\(\\mathbb{E}[u v]=0\\) and assume for simplicity that \\(u\\) is observed. Inserting into the structural equation we find \\(Y=Z^{\\prime} \\beta+u^{\\prime} \\gamma+v\\). The control function estimator \\((\\widehat{\\beta}, \\widehat{\\gamma})\\) is OLS estimation of this equation.\n\nShow that \\(\\mathbb{E}[X v]=0\\) (algebraically).\nDerive the asymptotic distribution of \\((\\widehat{\\beta}, \\widehat{\\gamma})\\).\n\nExercise 12.11 Consider the structural equation\n\\[\nY=\\beta_{0}+\\beta_{1} X+\\beta_{2} X^{2}+e\n\\]\nwith \\(X \\in \\mathbb{R}\\) treated as endogenous so that \\(\\mathbb{E}[X e] \\neq 0\\). We have an instrument \\(Z \\in \\mathbb{R}\\) which satisfies \\(\\mathbb{E}[e \\mid Z]=0\\) so in particular \\(\\mathbb{E}[e]=0, \\mathbb{E}[Z e]=0\\) and \\(\\mathbb{E}\\left[Z^{2} e\\right]=0\\).\n\nShould \\(X^{2}\\) be treated as endogenous or exogenous?\nSuppose we have a scalar instrument \\(Z\\) which satisfies\n\n\\[\nX=\\gamma_{0}+\\gamma_{1} Z+u\n\\]\nwith \\(u\\) independent of \\(Z\\) and mean zero.\nConsider using \\(\\left(1, Z, Z^{2}\\right.\\) ) as instruments. Is this a sufficient number of instruments? Is (12.93) just-identified, over-identified, or under-identified?\n\nWrite out the reduced form equation for \\(X^{2}\\). Under what condition on the reduced form parameters (12.94) are the parameters in (12.93) identified? Exercise 12.12 Consider the structural equation and reduced form\n\n\\[\n\\begin{aligned}\nY &=\\beta X^{2}+e \\\\\nX &=\\gamma Z+u \\\\\n\\mathbb{E}[Z e] &=0 \\\\\n\\mathbb{E}[Z u] &=0\n\\end{aligned}\n\\]\nwith \\(X^{2}\\) treated as endogenous so that \\(\\mathbb{E}\\left[X^{2} e\\right] \\neq 0\\). For simplicity assume no intercepts. \\(Y, Z\\), and \\(X\\) are scalar. Assume \\(\\gamma \\neq 0\\). Consider the following estimator. First, estimate \\(\\gamma\\) by OLS of \\(X\\) on \\(Z\\) and construct the fitted values \\(\\widehat{X}_{i}=\\widehat{\\gamma} Z_{i}\\). Second, estimate \\(\\beta\\) by OLS of \\(Y_{i}\\) on \\(\\left(\\widehat{X}_{i}\\right)^{2}\\).\n\nWrite out this estimator \\(\\widehat{\\beta}\\) explicitly as a function of the sample.\nFind its probability limit as \\(n \\rightarrow \\infty\\).\nIn general, is \\(\\widehat{\\beta}\\) consistent for \\(\\beta\\) ? Is there a reasonable condition under which \\(\\widehat{\\beta}\\) is consistent?\n\nExercise 12.13 Consider the structural equation \\(Y_{1}=Z_{1}^{\\prime} \\beta_{1}+Y_{2}^{\\prime} \\beta_{2}+e\\) with \\(\\mathbb{E}[Z e]=0\\) where \\(Y_{2}\\) is \\(k_{2} \\times 1\\) and treated as endogenous. The variables \\(Z=\\left(Z_{1}, Z_{2}\\right)\\) are treated as exogenous where \\(Z_{2}\\) is \\(\\ell_{2} \\times 1\\) and \\(\\ell_{2} \\geq k_{2}\\). You are interested in testing the hypothesis \\(\\mathbb{H}_{0}: \\beta_{2}=0\\).\nConsider the reduced form equation for \\(Y_{1}\\)\n\\[\nY_{1}=Z_{1}^{\\prime} \\lambda_{1}+Z_{2}^{\\prime} \\lambda_{2}+u_{1} .\n\\]\nShow how to test \\(\\mathbb{M}_{0}\\) using only the OLS estimates of (12.95).\nHint: This will require an analysis of the reduced form equations and their relation to the structural equation.\nExercise 12.14 Take the linear instrumental variables equation \\(Y_{1}=Z_{1}^{\\prime} \\beta_{1}+Y_{2}^{\\prime} \\beta_{2}+e\\) with \\(\\mathbb{E}[Z e]=0\\) where \\(Z_{1}\\) is \\(k_{1} \\times 1, Y_{2}\\) is \\(k_{2} \\times 1\\), and \\(Z\\) is \\(\\ell \\times 1\\), with \\(\\ell \\geq k=k_{1}+k_{2}\\). The sample size is \\(n\\). Assume that \\(\\boldsymbol{Q}_{Z Z}=\\) \\(\\mathbb{E}\\left[Z Z^{\\prime}\\right]>0\\) and \\(Q_{Z X}=\\mathbb{E}\\left[Z X^{\\prime}\\right]\\) has full rank \\(k\\).\nSuppose that only \\(\\left(Y_{1}, Z_{1}, Z_{2}\\right)\\) are available and \\(Y_{2}\\) is missing from the dataset.\nConsider the 2SLS estimator \\(\\widehat{\\beta}_{1}\\) of \\(\\beta_{1}\\) obtained from the misspecified IV regression of \\(Y_{1}\\) on \\(Z_{1}\\) only, using \\(Z_{2}\\) as an instrument for \\(Z_{1}\\).\n\nFind a stochastic decomposition \\(\\widehat{\\beta}_{1}=\\beta_{1}+b_{1 n}+r_{1 n}\\) where \\(r_{1 n}\\) depends on the error \\(e\\) and \\(b_{1 n}\\) does not depend on the error \\(e\\).\nShow that \\(r_{1 n} \\rightarrow p 0\\) as \\(n \\rightarrow \\infty\\).\nFind the probability limit of \\(b_{1 n}\\) and \\(\\widehat{\\beta}_{1}\\) as \\(n \\rightarrow \\infty\\).\nDoes \\(\\widehat{\\beta}_{1}\\) suffer from “omitted variables bias”? Explain. Under what conditions is there no omitted variables bias?\nFind the asymptotic distribution as \\(n \\rightarrow \\infty\\) of \\(\\sqrt{n}\\left(\\widehat{\\beta}_{1}-\\beta_{1}-b_{1 n}\\right)\\).\n\nExercise 12.15 Take the linear instrumental variables equation \\(Y_{1}=Z \\beta_{1}+Y_{2} \\beta_{2}+e\\) with \\(\\mathbb{E}[e \\mid Z]=0\\) where both \\(X\\) and \\(Z\\) are scalar \\(1 \\times 1\\).\n\nCan the coefficients \\(\\left(\\beta_{1}, \\beta_{2}\\right)\\) be estimated by 2 SLS using \\(Z\\) as an instrument for \\(Y_{2}\\) ?\n\nWhy or why not? (b) Can the coefficients \\(\\left(\\beta_{1}, \\beta_{2}\\right)\\) be estimated by 2SLS using \\(Z\\) and \\(Z^{2}\\) as instruments?\n\nFor the 2SLS estimator suggested in (b), what is the implicit exclusion restriction?\nIn (b) what is the implicit assumption about instrument relevance?\n\n[Hint: Write down the implied reduced form equation for \\(Y_{2}\\).]\n\nIn a generic application would you be comfortable with the assumptions in (c) and (d)?\n\nExercise 12.16 Take a linear equation with endogeneity and a just-identified linear reduced form \\(Y=\\) \\(X \\beta+e\\) with \\(X=\\gamma Z+u_{2}\\) where both \\(X\\) and \\(Z\\) are scalar \\(1 \\times 1\\). Assume that \\(\\mathbb{E}[Z e]=0\\) and \\(\\mathbb{E}\\left[Z u_{2}\\right]=0\\).\n\nDerive the reduced form equation \\(Y=Z \\lambda+u_{1}\\). Show that \\(\\beta=\\lambda / \\gamma\\) if \\(\\gamma \\neq 0\\), and that \\(\\mathbb{E}[Z u]=0\\).\nLet \\(\\widehat{\\lambda}\\) denote the OLS estimate from linear regression of \\(Y\\) on \\(Z\\), and let \\(\\widehat{\\gamma}\\) denote the OLS estimate from linear regression of \\(X\\) on \\(Z\\). Write \\(\\theta=(\\lambda, \\gamma)^{\\prime}\\) and let \\(\\widehat{\\theta}=(\\widehat{\\lambda}, \\widehat{\\gamma})^{\\prime}\\). Define \\(u=\\left(u_{1}, u_{2}\\right)\\). Write \\(\\sqrt{n}(\\widehat{\\theta}-\\theta)\\) using a single expression as a function of the error \\(u\\).\nShow that \\(\\mathbb{E}[Z u]=0\\).\nDerive the joint asymptotic distribution of \\(\\sqrt{n}(\\widehat{\\theta}-\\theta)\\) as \\(n \\rightarrow \\infty\\). Hint: Define \\(\\Omega_{u}=\\mathbb{E}\\left[Z^{2} u u^{\\prime}\\right]\\).\nUsing the previous result and the Delta Method find the asymptotic distribution of the Indirect Least Squares estimator \\(\\widehat{\\beta}=\\widehat{\\lambda} / \\widehat{\\gamma}\\).\nIs the answer in (e) the same as the asymptotic distribution of the 2SLS estimator in Theorem 12.2? Hint: Show that \\(\\left(\\begin{array}{ll}1 & -\\beta\\end{array}\\right) u=e\\) and \\(\\left(\\begin{array}{cc}1 & -\\beta\\end{array}\\right) \\Omega_{u}\\left(\\begin{array}{c}1 \\\\ -\\beta\\end{array}\\right)=\\mathbb{E}\\left[Z^{2} e^{2}\\right]\\).\n\nExercise 12.17 Take the model \\(Y=X^{\\prime} \\beta+e\\) with \\(\\mathbb{E}[Z e]=0\\) and consider the two-stage least squares estimator. The first-stage estimate is least squares of \\(X\\) on \\(Z\\) with least squares fitted values \\(\\widehat{X}\\). The second-stage is least squares of \\(Y\\) on \\(\\widehat{X}\\) with coefficient estimator \\(\\widehat{\\beta}\\) and least squares residuals \\(\\widehat{e}_{i}=\\) \\(Y_{i}-\\widehat{X}_{i} \\widehat{\\beta}\\). Consider \\(\\widehat{\\sigma}^{2}=\\frac{1}{n} \\sum_{i=1}^{n} \\widehat{e}_{i}^{2}\\) as an estimator for \\(\\sigma^{2}=\\mathbb{E}\\left[e_{i}^{2}\\right]\\). Is this appropriate? If not, propose an alternative estimator.\nExercise 12.18 You have two independent i.i.d. samples \\(\\left(Y_{1 i}, X_{1 i}, Z_{1 i}: i=1, \\ldots, n\\right)\\) and \\(\\left(Y_{2 i}, X_{2 i}, Z_{2 i}: i=\\right.\\) \\(1, \\ldots, n\\) ). The dependent variables \\(Y_{1}\\) and \\(Y_{2}\\) are real-valued. The regressors \\(X_{1}\\) and \\(X_{2}\\) and instruments \\(Z_{1}\\) and \\(Z_{2}\\) are \\(k\\)-vectors. The model is standard just-identified linear instrumental variables\n\\[\n\\begin{aligned}\nY_{1} &=X_{1}^{\\prime} \\beta_{1}+e_{1} \\\\\n\\mathbb{E}\\left[Z_{1} e_{1}\\right] &=0 \\\\\nY_{2} &=X_{2}^{\\prime} \\beta_{2}+e_{2} \\\\\n\\mathbb{E}\\left[Z_{2} e_{2}\\right] &=0 .\n\\end{aligned}\n\\]\nFor concreteness, sample 1 are women and sample 2 are men. You want to test \\(\\mathbb{M}_{0}: \\beta_{1}=\\beta_{2}\\), that the two samples have the same coefficients.\n\nDevelop a test statistic for \\(\\mathbb{H}_{0}\\).\nDerive the asymptotic distribution of the test statistic. (c) Describe (in brief) the testing procedure.\n\nExercise 12.19 You want to use household data to estimate \\(\\beta\\) in the model \\(Y=X \\beta+e\\) with \\(X\\) scalar and endogenous, using as an instrument the state of residence.\n\nWhat are the assumptions needed to justify this choice of instrument?\nIs the model just identified or overidentified?\n\nExercise 12.20 The model is \\(Y=X^{\\prime} \\beta+e\\) with \\(\\mathbb{E}[Z e]=0\\). An economist wants to obtain the 2 SLS estimates and standard errors for \\(\\beta\\). He uses the following steps\n\nRegresses \\(X\\) on \\(Z\\), obtains the predicted values \\(\\widehat{X}\\).\nRegresses \\(Y\\) on \\(\\widehat{X}\\), obtains the coefficient estimate \\(\\widehat{\\beta}\\) and standard error \\(s(\\widehat{\\beta})\\) from this regression. Is this correct? Does this produce the 2SLS estimates and standard errors?\n\nExercise 12.21 In the linear model \\(Y=X \\beta+e\\) with \\(X \\in \\mathbb{R}\\) suppose \\(\\sigma^{2}(x)=\\mathbb{E}\\left[e^{2} \\mid X=x\\right]\\) is known. Show that the GLS estimator of \\(\\beta\\) can be written as an instrumental variables estimator using some instrument \\(Z\\). (Find an expression for \\(Z\\).)\nExercise 12.22 You will replicate and extend the work reported in Acemoglu, Johnson, and Robinson (2001). The authors provided an expanded set of controls when they published their 2012 extension and posted the data on the AER website. This dataset is A JR2001 on the textbook website.\n\nEstimate the OLS regression (12.86), the reduced form regression (12.87), and the 2SLS regression (12.88). (Which point estimate is different by \\(0.01\\) from the reported values? This is a common phenomenon in empirical replication).\nFor the above estimates calculate both homoskedastic and heteroskedastic-robust standard errors. Which were used by the authors (as reported in (12.86)-(12.87)-(12.88)?)\nCalculate the 2SLS estimates by the Indirect Least Squares formula. Are they the same?\nCalculate the 2SLS estimates by the two-stage approach. Are they the same?\nCalculate the 2SLS estimates by the control variable approach. Are they the same?\nAcemoglu, Johnson, and Robinson (2001) reported many specifications including alternative regressor controls, for example latitude and africa. Estimate by least squares the equation for logGDP adding latitude and africa as regressors. Does this regression suggest that latitude and africa are predictive of the level of GDP?\nNow estimate the same equation as in (f) but by 2SLS using log(mortality) as an instrument for risk. How does the interpretation of the effect of latitude and africa change?\nReturn to our baseline model (without including latitude and africa). The authors’ reduced form equation uses \\(\\log\\) (mortality) as the instrument, rather than, say, the level of mortality. Estimate the reduced form for risk with mortality as the instrument. (This variable is not provided in the dataset so you need to take the exponential of \\(\\log\\) (mortality).) Can you explain why the authors preferred the equation with \\(\\log (\\) mortality) ? (i) Try an alternative reduced form including both \\(\\log\\) (mortality) and the square of \\(\\log (\\) mortality). Interpret the results. Re-estimate the structural equation by 2 SLS using both \\(\\log (\\) mortality) and its square as instruments. How do the results change?\nFor the estimates in (i) are the instruments strong or weak using the Stock-Yogo test?\nCalculate and interpret a test for exogeneity of the instruments.\nEstimate the equation by LIML using the instruments \\(\\log (\\) mortality) and the square of \\(\\log (\\) mortality).\n\nExercise 12.23 In Exercise 12.22 you extended the work reported in Acemoglu, Johnson, and Robinson (2001). Consider the 2SLS regression (12.88). Compute the standard errors both by the asymptotic formula and by the bootstrap using a large number \\((10,000)\\) of bootstrap replications. Re-calculate the bootstrap standard errors. Comment on the reliability of bootstrap standard errors for IV regression.\nExercise 12.24 You will replicate and extend the work reported in the chapter relating to Card (1995). The data is from the author’s website and is posted as Card1995. The model we focus on is labeled 2SLS(a) in Table \\(12.1\\) which uses public and private as instruments for edu. The variables you will need for this exercise include lwage76, ed76, age76, smsa76r, reg76r, nearc2, nearc4, nearc4a, nearc4b. See the description file for definitions. Experience is not in the dataset, so needs to be generated as age-edu-6.\n\nFirst, replicate the reduced form regression presented in the final column of Table 12.2, and the 2SLS regression described above (using public and private as instruments for \\(e d u\\) ) to verify that you have the same variable defintions.\nTry a different reduced form model. The variable nearc2 means “grew up near a 2-year college”. See if adding it to the reduced form equation is useful.\nTry more interactions in the reduced form. Create the interactions nearc \\(4 a^{*}\\) age 76 and nearc \\(4 a^{*}\\) age \\(76^{2} / 100\\), and add them to the reduced form equation. Estimate this by least squares. Interpret the coefficients on the two new variables.\nEstimate the structural equation by 2SLS using the expanded instrument set \\(\\left\\{\\right.\\) nearc \\(4 a\\), nearc \\(4 b\\), nearc \\(4 a^{*}\\) age 76 , nearc \\(4 a^{*}\\) age \\(\\left.76^{2} / 100\\right\\}\\).\n\nWhat is the impact on the structural estimate of the return to schooling?\n\nUsing the Stock-Yogo test are the instruments strong or weak?\nTest the hypothesis that \\(e d u\\) is exogenous for the structural return to schooling.\nRe-estimate the last equation by LIML. Do the results change meaningfully?\n\nExercise 12.25 In Exercise 12.24 you extended the work reported in Card (1995). Now, estimate the IV equation corresponding to the IV(a) column of Table 12.1 which is the baseline specification considered in Card. Use the bootstrap to calculate a BC percentile confidence interval. In this example should we also report the bootstrap standard error?\nExercise 12.26 You will extend Angrist and Krueger (1991) using the data file AK1991 on the textbook website.. Their Table VIII reports estimates of an analog of (12.90) for the subsample of 26,913 Black men. Use this sub-sample for the following analysis. (a) Estimate an equation which is identical in form to (12.90) with the same additional regressors (year-of-birth, region-of-residence, and state-of-birth dummy variables) and 180 excluded instrumental variables (the interactions of quarter-of-birth times year-of-birth dummy variables and quarter-of-birth times state-of-birth interactions) but use the subsample of Black men. One regressor must be omitted to achieve identification. Which variable is this?\n\nEstimate the reduced form for the above equation by least squares. Calculate the \\(F\\) statistic for the excluded instruments. What do you conclude about the strength of the instruments?\nRepeat, estimating the reduced form for the analog of (12.89) which has 30 excluded instrumental variables and does not include the state-of-birth dummy variables in the regression. What do you conclude about the strength of the instruments?\nRepeat, estimating the reduced form for the analog of (12.92) which has only 3 excluded instrumental variables. Are the instruments sufficiently strong for 2SLS estimation? For LIML estimation?\nEstimate the structural wage equation using what you believe is the most appropriate set of regressors, instruments, and the most appropriate estimation method. What is the estimated return to education (for the subsample of Black men) and its standard error? Without doing a formal hypothesis test, do these results (or in which way?) appear meaningfully different from the results for the full sample?\n\nExercise 12.27 In Exercise 12.26 you extended the work reported in Angrist and Krueger (1991) by estimating wage equations for the subsample of Black men. Re-estimate equation (12.92) for this group using as instruments only the three quarter-of-birth dummy variables. Calculate the standard error for the return to education by asymptotic and bootstrap methods. Calculate a BC percentile interval. In this application of 2SLS is it appropriate to report the bootstrap standard error?"
  },
  {
    "objectID": "chpt13-gmm.html#introduction",
    "href": "chpt13-gmm.html#introduction",
    "title": "13  Generalized Method of Moments",
    "section": "13.1 Introduction",
    "text": "13.1 Introduction\nOne of the most popular estimation methods in applied econometrics is the Generalized Method of Moments (GMM). GMM generalizes classical method of moments by allowing for more equations than unknown parameters (so are overidentified) and by allowing general nonlinear functions of the observations and parameters. Together this allows for a fairly rich and flexible estimation framework. GMM includes as special cases OLS, IV, multivariate regression, and 2SLS. It includes both linear and nonlinear models. In this chapter we focus primarily on linear models.\nThe GMM label and methods were introduced to econometrics in a seminal paper by Lars Hansen (1982). The ideas and methods build on the work of Amemiya \\((1974,1977)\\), Gallant (1977), and Gallant and Jorgenson (1979). The ideas are closely related to the contemporeneous work of Halbert White (1980, 1982) and White and Domowitz (1984). The methods are also related to what are called estimating equations in the statistics literature. For a review of the latter see Godambe (1991)."
  },
  {
    "objectID": "chpt13-gmm.html#moment-equation-models",
    "href": "chpt13-gmm.html#moment-equation-models",
    "title": "13  Generalized Method of Moments",
    "section": "13.2 Moment Equation Models",
    "text": "13.2 Moment Equation Models\nAll of the models that have been introduced so far can be written as moment equation models where the population parameters solve a system of moment equations. Moment equation models are broader than the models so far considered and understanding their common structure opens up straightforward techniques to handle new econometric models.\nMoment equation models take the following form. Let \\(g_{i}(\\beta)\\) be a known \\(\\ell \\times 1\\) function of the \\(i^{\\text {th }}\\) observation and a \\(k \\times 1\\) parameter \\(\\beta\\). A moment equation model is summarized by the moment equations\n\\[\n\\mathbb{E}\\left[g_{i}(\\beta)\\right]=0\n\\]\nand a parameter space \\(\\beta \\in B\\). For example, in the instrumental variables model \\(g_{i}(\\beta)=Z_{i}\\left(Y_{i}-X_{i}^{\\prime} \\beta\\right)\\).\nIn general, we say that a parameter \\(\\beta\\) is identified if there is a unique mapping from the data distribution to \\(\\beta\\). In the context of the model (13.1) this means that there is a unique \\(\\beta\\) satisfying (13.1). Since (13.1) is a system of \\(\\ell\\) equations with \\(k\\) unknowns, then it is necessary that \\(\\ell \\geq k\\) for there to be a unique solution. If \\(\\ell=k\\) we say that the model is just identified, meaning that there is just enough information to identify the parameters. If \\(\\ell>k\\) we say that the model is overidentified, meaning that there is excess information. If \\(\\ell<k\\) we say that the model is underidentified, meaning that there is insufficient information to identify the parameters. In general, we assume that \\(\\ell \\geq k\\) so the model is either just identified or overidentified."
  },
  {
    "objectID": "chpt13-gmm.html#method-of-moments-estimators",
    "href": "chpt13-gmm.html#method-of-moments-estimators",
    "title": "13  Generalized Method of Moments",
    "section": "13.3 Method of Moments Estimators",
    "text": "13.3 Method of Moments Estimators\nIn this section we consider the just-identified case \\(\\ell=k\\).\nDefine the sample analog of (13.5)\n\\[\n\\bar{g}_{n}(\\beta)=\\frac{1}{n} \\sum_{i=1}^{n} g_{i}(\\beta) .\n\\]\nThe method of moments estimator (MME) \\(\\widehat{\\beta}_{\\mathrm{mm}}\\) is the parameter value which sets \\(\\bar{g}_{n}(\\beta)=0\\). Thus\n\\[\n\\bar{g}_{n}\\left(\\widehat{\\beta}_{\\mathrm{mm}}\\right)=\\frac{1}{n} \\sum_{i=1}^{n} g_{i}\\left(\\widehat{\\beta}_{\\mathrm{mm}}\\right)=0 .\n\\]\nThe equations (13.3) are known as the estimating equations as they are the equations which determine the estimator \\(\\widehat{\\beta}_{\\mathrm{mm}}\\).\nIn some contexts (such as those discussed in the examples below) there is an explicit solution for \\(\\widehat{\\beta}_{\\mathrm{mm}}\\). In other cases the solution must be found numerically.\nWe now show how most of the estimators discussed so far in the textbook can be written as method of moments estimators.\nMean: Set \\(g_{i}(\\mu)=Y_{i}-\\mu\\). The MME is \\(\\widehat{\\mu}=\\frac{1}{n} \\sum_{i=1}^{n} Y_{i}\\).\nMean and Variance: Set\n\\[\ng_{i}\\left(\\mu, \\sigma^{2}\\right)=\\left(\\begin{array}{c}\nY_{i}-\\mu \\\\\n\\left(Y_{i}-\\mu\\right)^{2}-\\sigma^{2}\n\\end{array}\\right) .\n\\]\nThe MME are \\(\\widehat{\\mu}=\\frac{1}{n} \\sum_{i=1}^{n} Y_{i}\\) and \\(\\widehat{\\sigma}^{2}=\\frac{1}{n} \\sum_{i=1}^{n}\\left(Y_{i}-\\widehat{\\mu}\\right)^{2}\\).\nOLS: Set \\(g_{i}(\\beta)=X_{i}\\left(Y_{i}-X_{i}^{\\prime} \\beta\\right)\\). The MME is \\(\\widehat{\\beta}=\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{Y}\\right)\\).\nOLS and Variance: Set\n\\[\ng_{i}\\left(\\beta, \\sigma^{2}\\right)=\\left(\\begin{array}{c}\nX_{i}\\left(Y_{i}-X_{i}^{\\prime} \\beta\\right) \\\\\n\\left(Y_{i}-X_{i}^{\\prime} \\beta\\right)^{2}-\\sigma^{2}\n\\end{array}\\right) \\text {. }\n\\]\nThe MME is \\(\\widehat{\\beta}=\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{Y}\\right)\\) and \\(\\widehat{\\sigma}^{2}=\\frac{1}{n} \\sum_{i=1}^{n}\\left(Y_{i}-X_{i}^{\\prime} \\widehat{\\beta}\\right)^{2}\\).\nMultivariate Least Squares, vector form: \\(\\operatorname{Set} g_{i}(\\beta)=\\bar{X}_{i}^{\\prime}\\left(Y_{i}-\\bar{X}_{i} \\beta\\right)\\). The MME is \\(\\widehat{\\beta}=\\left(\\sum_{i=1}^{n} \\bar{X}_{i}^{\\prime} \\bar{X}_{i}\\right)^{-1}\\left(\\sum_{i=1}^{n} \\bar{X}_{i} Y_{i}\\right)\\) which is (11.4).\nMultivariate Least Squares, matrix form: Set \\(g_{i}(\\boldsymbol{B})=\\operatorname{vec}\\left(X_{i}\\left(Y_{i}^{\\prime}-X_{i}^{\\prime} \\boldsymbol{B}\\right)\\right)\\). The MME is \\(\\widehat{\\boldsymbol{B}}=\\left(\\sum_{i=1}^{n} X_{i} X_{i}^{\\prime}\\right)^{-1}\\left(\\sum_{i=1}^{n} X_{i} Y_{i}^{\\prime}\\right)\\) which is (11.6).\nSeemingly Unrelated Regression: Set\n\\[\ng_{i}(\\beta, \\Sigma)=\\left(\\begin{array}{c}\n\\bar{X}_{i} \\Sigma^{-1}\\left(Y_{i}-\\bar{X}_{i}^{\\prime} \\beta\\right) \\\\\n\\operatorname{vec}\\left(\\Sigma-\\left(Y_{i}-\\bar{X}_{i}^{\\prime} \\beta\\right)\\left(Y_{i}-\\bar{X}_{i}^{\\prime} \\beta\\right)^{\\prime}\\right)\n\\end{array}\\right)\n\\]\nThe MME is \\(\\widehat{\\beta}=\\left(\\sum_{i=1}^{n} \\bar{X}_{i} \\widehat{\\Sigma}^{-1} \\bar{X}_{i}^{\\prime}\\right)^{-1}\\left(\\sum_{i=1}^{n} \\bar{X}_{i} \\widehat{\\Sigma}^{-1} Y_{i}\\right)\\) and \\(\\widehat{\\Sigma}=n^{-1} \\sum_{i=1}^{n}\\left(Y_{i}-\\bar{X}_{i}^{\\prime} \\widehat{\\beta}\\right)\\left(Y_{i}-\\bar{X}_{i}^{\\prime} \\widehat{\\beta}\\right)^{\\prime}\\).\nIV: Set \\(g_{i}(\\beta)=Z_{i}\\left(Y_{i}-X_{i}^{\\prime} \\beta\\right)\\). The MME is \\(\\widehat{\\beta}=\\left(\\boldsymbol{Z}^{\\prime} \\boldsymbol{X}\\right)^{-1}\\left(\\boldsymbol{Z}^{\\prime} \\boldsymbol{Y}\\right)\\). Generated Regressors: Set\n\\[\ng_{i}(\\beta, \\boldsymbol{A})=\\left(\\begin{array}{c}\n\\boldsymbol{A}^{\\prime} Z_{i}\\left(Y_{i}-Z_{i}^{\\prime} \\boldsymbol{A} \\beta\\right) \\\\\n\\operatorname{vec}\\left(Z_{i}\\left(X_{i}^{\\prime}-Z_{i}^{\\prime} \\boldsymbol{A}\\right)\\right)\n\\end{array}\\right)\n\\]\nThe MME is \\(\\widehat{\\boldsymbol{A}}=\\left(\\sum_{i=1}^{n} Z_{i} Z_{i}^{\\prime}\\right)^{-1}\\left(\\sum_{i=1}^{n} Z_{i} X_{i}^{\\prime}\\right)\\) and \\(\\widehat{\\beta}=\\left(\\widehat{\\boldsymbol{A}}^{\\prime} \\boldsymbol{Z}^{\\prime} \\boldsymbol{Z} \\widehat{\\boldsymbol{A}}\\right)^{-1}\\left(\\widehat{\\boldsymbol{A}}^{\\prime} \\boldsymbol{Z}^{\\prime} \\boldsymbol{Y}\\right)\\).\nA common feature of these examples is that the estimator can be written as the solution to a set of estimating equations (13.3). This provides a common framework which enables a convenient development of a unified distribution theory."
  },
  {
    "objectID": "chpt13-gmm.html#overidentified-moment-equations",
    "href": "chpt13-gmm.html#overidentified-moment-equations",
    "title": "13  Generalized Method of Moments",
    "section": "13.4 Overidentified Moment Equations",
    "text": "13.4 Overidentified Moment Equations\nIn the instrumental variables model \\(g_{i}(\\beta)=Z_{i}\\left(Y_{i}-X_{i}^{\\prime} \\beta\\right)\\). Thus (13.2) is\n\\[\n\\bar{g}_{n}(\\beta)=\\frac{1}{n} \\sum_{i=1}^{n} g_{i}(\\beta)=\\frac{1}{n} \\sum_{i=1}^{n} Z_{i}\\left(Y_{i}-X_{i}^{\\prime} \\beta\\right)=\\frac{1}{n}\\left(\\boldsymbol{Z}^{\\prime} \\boldsymbol{Y}-\\boldsymbol{Z}^{\\prime} \\boldsymbol{X} \\beta\\right) .\n\\]\nWe have defined the method of moments estimator for \\(\\beta\\) as the parameter value which sets \\(\\bar{g}_{n}(\\beta)=\\) 0 . However, when the model is overidentified (if \\(\\ell>k\\) ) this is generally impossible as there are more equations than free parameters. Equivalently, there is no choice of \\(\\beta\\) which sets (13.4) to zero. Thus the method of moments estimator is not defined for the overidentified case.\nWhile we cannot find an estimator which sets \\(\\bar{g}_{n}(\\beta)\\) equal to zero we can try to find an estimator which makes \\(\\bar{g}_{n}(\\beta)\\) as close to zero as possible.\nOne way to think about this is to define the vector \\(\\mu=\\boldsymbol{Z}^{\\prime} \\boldsymbol{Y}\\), the matrix \\(\\boldsymbol{G}=\\boldsymbol{Z}^{\\prime} \\boldsymbol{X}\\) and the “error” \\(\\eta=\\mu-\\boldsymbol{G} \\beta\\). Then we can write (13.4) as \\(\\mu=\\boldsymbol{G} \\beta+\\eta\\). This looks like a regression equation with the \\(\\ell \\times 1\\) dependent variable \\(\\mu\\), the \\(\\ell \\times k\\) regressor matrix \\(\\boldsymbol{G}\\), and the \\(\\ell \\times 1\\) error vector \\(\\eta\\). The goal is to make the error vector \\(\\eta\\) as small as possible. Recalling our knowledge about least squares we deduct that a simple method is to regress \\(\\mu\\) on \\(\\boldsymbol{G}\\), obtaining \\(\\widehat{\\beta}=\\left(\\boldsymbol{G}^{\\prime} \\boldsymbol{G}\\right)^{-1}\\left(\\boldsymbol{G}^{\\prime} \\mu\\right)\\). This minimizes the sum-of-squares \\(\\eta^{\\prime} \\eta\\). This is certainly one way to make \\(\\eta\\) “small”.\nMore generally we know that when errors are non-homogeneous it can be more efficient to estimate by weighted least squares. Thus for some weight matrix \\(\\boldsymbol{W}\\) consider the estimator\n\\[\n\\widehat{\\beta}=\\left(\\boldsymbol{G}^{\\prime} \\boldsymbol{W} \\boldsymbol{G}\\right)^{-1}\\left(\\boldsymbol{G}^{\\prime} \\boldsymbol{W} \\boldsymbol{\\mu}\\right)=\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{Z} \\boldsymbol{W} \\boldsymbol{Z}^{\\prime} \\boldsymbol{X}\\right)^{-1}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{Z} \\boldsymbol{W} \\boldsymbol{Z}^{\\prime} \\boldsymbol{Y}\\right) .\n\\]\nThis minimizes the weighted sum of squares \\(\\eta^{\\prime} \\boldsymbol{W} \\eta\\). This solution is known as the generalized method of moments (GMM).\nThe estimator is typically defined as follows. Given a set of moment equations (13.2) and an \\(\\ell \\times \\ell\\) weight matrix \\(\\boldsymbol{W}>0\\) the GMM criterion function is defined as\n\\[\nJ(\\beta)=n \\bar{g}_{n}(\\beta)^{\\prime} \\boldsymbol{W} \\bar{g}_{n}(\\beta) .\n\\]\nThe factor ” \\(n\\) ” is not important for the definition of the estimator but is convenient for the distribution theory. The criterion \\(J(\\beta)\\) is the weighted sum of squared moment equation errors. When \\(\\boldsymbol{W}=\\boldsymbol{I}_{\\ell}\\) then \\(J(\\beta)=n \\bar{g}_{n}(\\beta)^{\\prime} \\bar{g}_{n}(\\beta)=n\\left\\|\\bar{g}_{n}(\\beta)\\right\\|^{2}\\), the square of the Euclidean length. Since we restrict attention to positive definite weight matrices \\(\\boldsymbol{W}\\) the criterion \\(J(\\beta)\\) is non-negative.\nDefinition 13.1 The Generalized Method of Moments (GMM) estimator is\n\\[\n\\widehat{\\beta}_{\\mathrm{gmm}}=\\underset{\\beta}{\\operatorname{argmin}} J(\\beta) .\n\\]\nRecall that in the just-identified case \\(k=\\ell\\) the method of moments estimator \\(\\widehat{\\beta}_{\\mathrm{mm}}\\) solves \\(\\bar{g}_{n}\\left(\\widehat{\\beta}_{\\mathrm{mm}}\\right)=\\) 0 . Hence in this case \\(J\\left(\\widehat{\\beta}_{\\mathrm{mm}}\\right)=0\\) which means that \\(\\widehat{\\beta}_{\\mathrm{mm}}\\) minimizes \\(J(\\beta)\\) and equals \\(\\widehat{\\beta}_{\\mathrm{gmm}}=\\widehat{\\beta}_{\\mathrm{mm}}\\). This means that GMM includes MME as a special case. This implies that all of our results for GMM apply to any method of moments estimator.\nIn the over-identified case the GMM estimator depends on the choice of weight matrix \\(\\boldsymbol{W}\\) and so this is an important focus of the theory. In the just-identified case the GMM estimator simplifies to the MME which does not depend on \\(\\boldsymbol{W}\\).\nThe method and theory of the generalized method of moments was developed in an influential paper by Lars Hansen (1982). This paper introduced the method, its asymptotic distribution, the form of the efficient weight matrix, and tests for overidentification."
  },
  {
    "objectID": "chpt13-gmm.html#linear-moment-models",
    "href": "chpt13-gmm.html#linear-moment-models",
    "title": "13  Generalized Method of Moments",
    "section": "13.5 Linear Moment Models",
    "text": "13.5 Linear Moment Models\nOne of the great advantages of the moment equation framework is that it allows both linear and nonlinear models. However, when the moment equations are linear in the parameters then we have explicit solutions for the estimates and a straightforward asymptotic distribution theory. Hence we start by confining attention to linear moment equations and return to nonlinear moment equations later. In the examples listed earlier the estimators which have linear moment equations include the sample mean, OLS, multivariate least squares, IV, and 2SLS. The estimates which have nonlinear moment equations include the sample variance, SUR, and generated regressors.\nIn particular, we focus on the overidentified IV model with moment equations\n\\[\ng_{i}(\\beta)=Z_{i}\\left(Y_{i}-X_{i}^{\\prime} \\beta\\right)\n\\]\nwhere \\(Z_{i}\\) is \\(\\ell \\times 1\\) and \\(X_{i}\\) is \\(k \\times 1\\)."
  },
  {
    "objectID": "chpt13-gmm.html#gmm-estimator",
    "href": "chpt13-gmm.html#gmm-estimator",
    "title": "13  Generalized Method of Moments",
    "section": "13.6 GMM Estimator",
    "text": "13.6 GMM Estimator\nGiven (13.5) the sample moment equations are (13.4). The GMM criterion can be written as\n\\[\nJ(\\beta)=n\\left(\\boldsymbol{Z}^{\\prime} \\boldsymbol{Y}-\\boldsymbol{Z}^{\\prime} \\boldsymbol{X} \\beta\\right)^{\\prime} \\boldsymbol{W}\\left(\\boldsymbol{Z}^{\\prime} \\boldsymbol{Y}-\\boldsymbol{Z}^{\\prime} \\boldsymbol{X} \\beta\\right) .\n\\]\nThe GMM estimator minimizes \\(J(\\beta)\\). The first order conditions are\n\\[\n\\begin{aligned}\n0 &=\\frac{\\partial}{\\partial \\beta} J(\\widehat{\\beta}) \\\\\n&=2 \\frac{\\partial}{\\partial \\beta} \\bar{g}_{n}(\\widehat{\\beta})^{\\prime} \\boldsymbol{W} \\bar{g}_{n}(\\widehat{\\beta}) \\\\\n&=-2\\left(\\frac{1}{n} \\boldsymbol{X}^{\\prime} \\boldsymbol{Z}\\right) \\boldsymbol{W}\\left(\\frac{1}{n} \\boldsymbol{Z}^{\\prime}(\\boldsymbol{Y}-\\boldsymbol{X} \\widehat{\\beta})\\right) .\n\\end{aligned}\n\\]\nThe solution is given as follows.\nTheorem 13.1 For the overidentified IV model\n\\[\n\\widehat{\\beta}_{\\mathrm{gmm}}=\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{Z} \\boldsymbol{W} \\boldsymbol{Z}^{\\prime} \\boldsymbol{X}\\right)^{-1}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{Z} \\boldsymbol{W} \\boldsymbol{Z}^{\\prime} \\boldsymbol{Y}\\right) .\n\\]\nWhile the estimator depends on \\(\\boldsymbol{W}\\) the dependence is only up to scale. This is because if \\(\\boldsymbol{W}\\) is replaced by \\(c W\\) for some \\(c>0, \\widehat{\\beta}_{\\text {gmm }}\\) does not change. When \\(W\\) is fixed by the user we call \\(\\widehat{\\beta}_{\\text {gmm }}\\) ane-step GMM estimator. The formula (13.6) applies for the over-identified \\((\\ell>k)\\) and the just-identified \\((\\ell=k)\\) case. When the model is just-identified then \\(\\boldsymbol{X}^{\\prime} \\boldsymbol{Z}\\) is \\(k \\times k\\) so expression (13.6) simplifies to\n\\[\n\\widehat{\\beta}_{\\mathrm{gmm}}=\\left(\\boldsymbol{Z}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\boldsymbol{W}^{-1}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{Z}\\right)^{-1}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{Z} \\boldsymbol{W} \\boldsymbol{Z}^{\\prime} \\boldsymbol{Y}\\right)=\\left(\\boldsymbol{Z}^{\\prime} \\boldsymbol{X}\\right)^{-1}\\left(\\boldsymbol{Z}^{\\prime} \\boldsymbol{Y}\\right)=\\widehat{\\beta}_{\\mathrm{iv}}\n\\]\nthe IV estimator.\nThe GMM estimator (13.6) resembles the 2SLS estimator (12.29). In fact they are equal when \\(\\boldsymbol{W}=\\) \\(\\left(\\boldsymbol{Z}^{\\prime} \\boldsymbol{Z}\\right)^{-1}\\). This means that the 2SLS estimator is a one-step GMM estimator for the linear model.\nTheorem 13.2 If \\(\\boldsymbol{W}=\\left(\\boldsymbol{Z}^{\\prime} \\boldsymbol{Z}\\right)^{-1}\\) then \\(\\widehat{\\beta}_{\\mathrm{gmm}}=\\widehat{\\beta}_{2 \\text { sls. }}\\) Furthermore, if \\(k=\\ell\\) then \\(\\widehat{\\beta}_{\\mathrm{gmm}}=\\widehat{\\beta}_{\\mathrm{iv}}\\)"
  },
  {
    "objectID": "chpt13-gmm.html#distribution-of-gmm-estimator",
    "href": "chpt13-gmm.html#distribution-of-gmm-estimator",
    "title": "13  Generalized Method of Moments",
    "section": "13.7 Distribution of GMM Estimator",
    "text": "13.7 Distribution of GMM Estimator\nLet \\(\\boldsymbol{Q}=\\mathbb{E}\\left[Z X^{\\prime}\\right]\\) and \\(\\Omega=\\mathbb{E}\\left[Z Z^{\\prime} e^{2}\\right]\\). Then\n\\[\n\\left(\\frac{1}{n} \\boldsymbol{X}^{\\prime} \\boldsymbol{Z}\\right) \\boldsymbol{W}\\left(\\frac{1}{n} \\boldsymbol{Z}^{\\prime} \\boldsymbol{X}\\right) \\underset{p}{\\longrightarrow} \\boldsymbol{Q}^{\\prime} \\boldsymbol{W} \\boldsymbol{Q}\n\\]\nand\n\\[\n\\left(\\frac{1}{n} \\boldsymbol{X}^{\\prime} \\boldsymbol{Z}\\right) \\boldsymbol{W}\\left(\\frac{1}{\\sqrt{n}} \\boldsymbol{Z}^{\\prime} \\boldsymbol{e}\\right) \\underset{d}{\\longrightarrow} \\boldsymbol{Q}^{\\prime} \\boldsymbol{W} \\mathrm{N}(0, \\Omega)\n\\]\nWe conclude:\nTheorem 13.3 Asymptotic Distribution of GMM Estimator. Under Assumption 12.2, as \\(n \\rightarrow \\infty, \\sqrt{n}\\left(\\widehat{\\beta}_{\\mathrm{gmm}}-\\beta\\right) \\underset{d}{\\longrightarrow} \\mathrm{N}\\left(0, \\boldsymbol{V}_{\\beta}\\right)\\) where\n\\[\n\\boldsymbol{V}_{\\boldsymbol{\\beta}}=\\left(\\boldsymbol{Q}^{\\prime} \\boldsymbol{W} \\boldsymbol{Q}\\right)^{-1}\\left(\\boldsymbol{Q}^{\\prime} \\boldsymbol{W} \\Omega \\boldsymbol{W} \\boldsymbol{Q}\\right)\\left(\\boldsymbol{Q}^{\\prime} \\boldsymbol{W} \\boldsymbol{Q}\\right)^{-1} .\n\\]\nThe GMM estimator is asymptotically normal with a “sandwich form” asymptotic variance.\nOur derivation treated the weight matrix \\(W\\) as if it is non-random but Theorem \\(13.3\\) applies to the random weight matrix case so long as \\(\\widehat{\\boldsymbol{W}}\\) converges in probability to a positive definite limit \\(\\boldsymbol{W}\\). This may require scaling the weight matrix, for example replacing \\(\\widehat{\\boldsymbol{W}}=\\left(\\boldsymbol{Z}^{\\prime} \\boldsymbol{Z}\\right)^{-1}\\) with \\(\\widehat{\\boldsymbol{W}}=\\left(n^{-1} \\boldsymbol{Z}^{\\prime} \\boldsymbol{Z}\\right)^{-1}\\). Since rescaling the weight matrix does not affect the estimator this is ignored in implementation."
  },
  {
    "objectID": "chpt13-gmm.html#efficient-gmm",
    "href": "chpt13-gmm.html#efficient-gmm",
    "title": "13  Generalized Method of Moments",
    "section": "13.8 Efficient GMM",
    "text": "13.8 Efficient GMM\nThe asymptotic distribution of the GMM estimator \\(\\widehat{\\beta}_{\\mathrm{gmm}}\\) depends on the weight matrix \\(\\boldsymbol{W}\\) through the asymptotic variance \\(\\boldsymbol{V}_{\\beta}\\). The asymptotically optimal weight matrix \\(\\boldsymbol{W}_{0}\\) is that which minimizes \\(\\boldsymbol{V}_{\\beta}\\). This turns out to be \\(\\boldsymbol{W}_{0}=\\Omega^{-1}\\). The proof is left to Exercise 13.4.\nWhen the GMM estimator \\(\\widehat{\\beta}\\) is constructed with \\(\\boldsymbol{W}=\\boldsymbol{W}_{0}=\\Omega^{-1}\\) (or a weight matrix which is a consistent estimator of \\(\\boldsymbol{W}_{0}\\) ) we call it the Efficient GMM estimator:\n\\[\n\\widehat{\\beta}_{\\mathrm{gmm}}=\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{Z} \\Omega^{-1} \\boldsymbol{Z}^{\\prime} \\boldsymbol{X}\\right)^{-1}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{Z} \\Omega^{-1} \\boldsymbol{Z}^{\\prime} \\boldsymbol{Y}\\right) .\n\\]\nIts asymptotic distribution takes a simpler form than in Theorem 13.3. By substituting \\(\\boldsymbol{W}=\\boldsymbol{W}_{0}=\\Omega^{-1}\\) into (13.7) we find\n\\[\n\\boldsymbol{V}_{\\beta}=\\left(\\boldsymbol{Q}^{\\prime} \\Omega^{-1} \\boldsymbol{Q}\\right)^{-1}\\left(\\boldsymbol{Q}^{\\prime} \\Omega^{-1} \\Omega \\Omega^{-1} \\boldsymbol{Q}\\right)\\left(\\boldsymbol{Q}^{\\prime} \\Omega^{-1} \\boldsymbol{Q}\\right)^{-1}=\\left(\\boldsymbol{Q}^{\\prime} \\Omega^{-1} \\boldsymbol{Q}\\right)^{-1} .\n\\]\nThis is the asymptotic variance of the efficient GMM estimator.\nTheorem 13.4 Asymptotic Distribution of GMM with Efficient Weight Ma-\\ trix. Under Assumption \\(12.2\\) and \\(\\boldsymbol{W}=\\Omega^{-1}\\), as \\(n \\rightarrow \\infty, \\sqrt{n}\\left(\\widehat{\\beta}_{\\mathrm{gmm}}-\\beta\\right) \\underset{d}{\\mathrm{~d}}\\)\\ \\(\\mathrm{~N}\\left(0, \\boldsymbol{V}_{\\beta}\\right)\\) where \\(\\boldsymbol{V}_{\\beta}=\\left(\\boldsymbol{Q}^{\\prime} \\Omega^{-1} \\boldsymbol{Q}\\right)^{-1}\\).\nTheorem 13.5 Efficient GMM. Under Assumption 12.2, for any \\(\\boldsymbol{W}>0\\),\n\\[\n\\left(\\boldsymbol{Q}^{\\prime} \\boldsymbol{W} \\boldsymbol{Q}\\right)^{-1}\\left(\\boldsymbol{Q}^{\\prime} \\boldsymbol{W} \\Omega \\boldsymbol{W} \\boldsymbol{Q}\\right)\\left(\\boldsymbol{Q}^{\\prime} \\boldsymbol{W} \\boldsymbol{Q}\\right)^{-1}-\\left(\\boldsymbol{Q}^{\\prime} \\Omega^{-1} \\boldsymbol{Q}\\right)^{-1} \\geq 0\n\\]\nThe inequality ” \\(\\geq\\) ” can be replaced with ” \\(>\\) ” if \\(W \\neq \\Omega^{-1}\\). Thus if \\(\\widehat{\\beta}_{\\mathrm{gmm}}\\) is the efficient GMM estimator and \\(\\widetilde{\\beta}_{\\text {gmm }}\\) is another GMM estimator, then\n\\[\n\\operatorname{avar}\\left[\\widehat{\\beta}_{\\mathrm{gmm}}\\right] \\leq \\operatorname{avar}\\left[\\widetilde{\\beta}_{\\mathrm{gmm}}\\right] .\n\\]\nFor a proof see Exercise 13.4.\nThis means that the smallest possible GMM covariance matrix (in the positive definite sense) is achieved by the efficient GMM weight matrix.\n\\(\\boldsymbol{W}_{0}=\\Omega^{-1}\\) is not known in practice but it can be estimated consistently as we discuss in Section \\(13.10 .\\) For any \\(\\widehat{\\boldsymbol{W}} \\underset{p}{\\rightarrow} \\boldsymbol{W}_{0}\\) the asymptotic distribution in Theorem \\(13.4\\) is unaffected. Consequently we call any \\(\\widehat{\\beta}_{\\mathrm{gmm}}\\) constructed with an estimate of the efficient weight matrix an efficient GMM estimator.\nBy “efficient” we mean that this estimator has the smallest asymptotic variance in the class of GMM estimators with this set of moment conditions. This is a weak concept of optimality as we are only considering alternative weight matrices \\(\\widehat{\\boldsymbol{W}}\\). However, it turns out that the GMM estimator is semiparametrically efficient as shown by Gary Chamberlain (1987). If it is known that \\(\\mathbb{E}\\left[g_{i}(\\beta)\\right]=0\\) and this is all that is known this is a semi-parametric problem as the distribution of the data is unknown. Chamberlain showed that in this context no semiparametric estimator (one which is consistent globally for the class of models considered) can have a smaller asymptotic variance than \\(\\left(\\boldsymbol{G}^{\\prime} \\Omega^{-1} \\boldsymbol{G}\\right)^{-1}\\) where \\(\\boldsymbol{G}=\\mathbb{E}\\left[\\frac{\\partial}{\\partial \\beta^{\\prime}} g_{i}(\\beta)\\right]\\). Since the GMM estimator has this asymptotic variance it is semiparametrically efficient.\nThe results in this section show that in the linear model no estimator has better asymptotic efficiency than the efficient linear GMM estimator. No estimator can do better (in this first-order asymptotic sense) without imposing additional assumptions."
  },
  {
    "objectID": "chpt13-gmm.html#efficient-gmm-versus-2sls",
    "href": "chpt13-gmm.html#efficient-gmm-versus-2sls",
    "title": "13  Generalized Method of Moments",
    "section": "13.9 Efficient GMM versus 2SLS",
    "text": "13.9 Efficient GMM versus 2SLS\nFor the linear model we introduced 2SLS as a standard estimator for \\(\\beta\\). Now we have introduced GMM which includes 2SLS as a special case. Is there a context where 2SLS is efficient?\nTo answer this question recall that 2SLS is GMM given the weight matrix \\(\\widehat{\\boldsymbol{W}}=\\left(\\boldsymbol{Z}^{\\prime} \\boldsymbol{Z}\\right)^{-1}\\) or equivalently \\(\\widehat{\\boldsymbol{W}}=\\left(n^{-1} \\boldsymbol{Z}^{\\prime} \\boldsymbol{Z}\\right)^{-1}\\) since scaling doesn’t matter. Since \\(\\widehat{\\boldsymbol{W}} \\underset{p}{\\longrightarrow}\\left(\\mathbb{E}\\left[Z Z^{\\prime}\\right]\\right)^{-1}\\) this is asymptotically equivalent to the weight matrix \\(\\boldsymbol{W}=\\left(\\mathbb{E}\\left[Z Z^{\\prime}\\right]\\right)^{-1}\\). In contrast, the efficient weight matrix takes the form \\(\\left(\\mathbb{E}\\left[Z Z^{\\prime} e^{2}\\right]\\right)^{-1}\\). Now suppose that the structural equation error \\(e\\) is conditionally homoskedastic in the sense that \\(\\mathbb{E}\\left[e^{2} \\mid Z\\right]=\\sigma^{2}\\). Then the efficient weight matrix equals \\(\\boldsymbol{W}=\\left(\\mathbb{E}\\left[Z Z^{\\prime}\\right]\\right)^{-1} \\sigma^{-2}\\) or equivalently \\(W=\\left(\\mathbb{E}\\left[Z Z^{\\prime}\\right]\\right)^{-1}\\) since scaling doesn’t matter. The latter weight matrix is the same as the 2SLS asymptotic weight matrix. This shows that the 2SLS weight matrix is the efficient weight matrix under conditional homoskedasticity.\nTheorem 13.6 Under Assumption \\(12.2\\) and \\(\\mathbb{E}\\left[e^{2} \\mid Z\\right]=\\sigma^{2}, \\widehat{\\beta}_{2 \\text { sls }}\\) is efficient GMM.\nThis shows that 2SLS is efficient under homoskedasticity. When homoskedasticity holds there is no reason to use efficient GMM over 2SLS. More broadly, when homoskedasticity is a reasonable approximation then 2SLS will be a reasonable estimator. However, this result also shows that in the general case where the error is conditionally heteroskedastic, 2SLS is inefficient relative to efficient GMM."
  },
  {
    "objectID": "chpt13-gmm.html#estimation-of-the-efficient-weight-matrix",
    "href": "chpt13-gmm.html#estimation-of-the-efficient-weight-matrix",
    "title": "13  Generalized Method of Moments",
    "section": "13.10 Estimation of the Efficient Weight Matrix",
    "text": "13.10 Estimation of the Efficient Weight Matrix\nTo construct the efficient GMM estimator we need a consistent estimator \\(\\widehat{\\boldsymbol{W}}\\) of \\(\\boldsymbol{W}_{0}=\\Omega^{-1}\\). The convention is to form an estimator \\(\\widehat{\\Omega}\\) of \\(\\Omega\\) and then set \\(\\widehat{\\boldsymbol{W}}=\\widehat{\\Omega}^{-1}\\).\nThe two-step GMM estimator proceeds by using a one-step consistent estimator of \\(\\beta\\) to construct the weight matrix estimator \\(\\widehat{\\boldsymbol{W}}\\). In the linear model the natural one-step estimator for \\(\\beta\\) is 2 SLS. Set \\(\\widetilde{e}_{i}=Y_{i}-X_{i}^{\\prime} \\widehat{\\beta}_{2 s l s}, \\widetilde{g}_{i}=g_{i}(\\widetilde{\\beta})=Z_{i} \\widetilde{e}_{i}\\), and \\(\\bar{g}_{n}=n^{-1} \\sum_{i=1}^{n} \\widetilde{g}_{i}\\). Two moment estimators of \\(\\Omega\\) are\n\\[\n\\widehat{\\Omega}=\\frac{1}{n} \\sum_{i=1}^{n} \\widetilde{g}_{i} \\widetilde{g}_{i}^{\\prime}\n\\]\nand\n\\[\n\\widehat{\\Omega}^{*}=\\frac{1}{n} \\sum_{i=1}^{n}\\left(\\widetilde{g}_{i}-\\bar{g}_{n}\\right)\\left(\\widetilde{g}_{i}-\\bar{g}_{n}\\right)^{\\prime} .\n\\]\nThe estimator (13.8) is an uncentered covariance matrix estimator while the estimator (13.9) is a centered version. Either is consistent when \\(\\mathbb{E}[Z e]=0\\) which holds under correct specification. However under misspecification we may have \\(\\mathbb{E}[Z e] \\neq 0\\). In the latter context \\(\\widehat{\\Omega}^{*}\\) remains an estimator of var \\([Z e]\\) while \\(\\widehat{\\Omega}\\) is an estimator of \\(\\mathbb{E}\\left[Z Z^{\\prime} e^{2}\\right]\\). In this sense \\(\\widehat{\\Omega}^{*}\\) is a robust variance estimator. For some testing problems it turns out to be preferable to use a covariance matrix estimator which is robust to the alternative hypothesis. For these reasons estimator (13.9) is generally preferred. The uncentered estimator (13.8) is more commonly seen in practice since it is the default choice by most packages. It is also worth observing that when the model is just identified then \\(\\bar{g}_{n}=0\\) so the two are algebraically identical. The choice of weight matrix may also impact covariance matrix estimation as discussed in Section 13.12.\nGiven the choice of covariance matrix estimator we set \\(\\widehat{W}=\\widehat{\\Omega}^{-1}\\) or \\(\\widehat{W}=\\widehat{\\Omega}^{*-1}\\). Given this weight matrix we construct the two-step GMM estimator as (13.6) using the weight matrix \\(\\widehat{\\boldsymbol{W}}\\).\nSince the 2SLS estimator is consistent for \\(\\beta\\), by arguments nearly identical to those used for covariance matrix estimation we can show that \\(\\widehat{\\Omega}\\) and \\(\\widehat{\\Omega}^{*}\\) are consistent for \\(\\Omega\\) and thus \\(\\widehat{\\boldsymbol{W}}\\) is consistent for \\(\\Omega^{-1}\\). See Exercise 13.3.\nThis also means that the two-step GMM estimator satisfies the conditions for Theorem 13.4.\nTheorem \\(13.7\\) Under Assumption \\(12.2\\) and \\(\\Omega>0\\), if \\(\\widehat{W}=\\widehat{\\Omega}^{-1}\\) or \\(\\widehat{W}=\\) \\(\\widehat{\\Omega}^{*-1}\\) where the latter are defined in (13.8) and (13.9) then as \\(n \\rightarrow \\infty\\), \\(\\sqrt{n}\\left(\\widehat{\\beta}_{\\mathrm{gmm}}-\\beta\\right) \\underset{d}{\\longrightarrow} \\mathrm{N}\\left(0, \\boldsymbol{V}_{\\beta}\\right)\\) where \\(\\boldsymbol{V}_{\\beta}=\\left(\\boldsymbol{Q}^{\\prime} \\Omega^{-1} \\boldsymbol{Q}\\right)^{-1}\\).\nThis shows that the two-step GMM estimator is asymptotically efficient.\nThe two-step GMM estimator of the IV regression equation can be computed in Stata using the ivregress gmm command. By default it uses formula (13.8). The centered version (13.9) may be selected using the center option."
  },
  {
    "objectID": "chpt13-gmm.html#iterated-gmm",
    "href": "chpt13-gmm.html#iterated-gmm",
    "title": "13  Generalized Method of Moments",
    "section": "13.11 Iterated GMM",
    "text": "13.11 Iterated GMM\nThe asymptotic distribution of the two-step GMM estimator does not depend on the choice of the preliminary one-step estimator. However, the actual value of the estimator depends on this choice and so will the finite sample distribution. This is undesirable and likely inefficient. To remove this dependence we can iterate the estimation sequence. Specifically, given \\(\\widehat{\\beta}_{\\mathrm{gmm}}\\) we can construct an updated weight matrix estimate \\(\\widehat{\\boldsymbol{W}}\\) and then re-estimate \\(\\widehat{\\beta}_{\\mathrm{gmm}}\\). This updating can be iterated until convergence \\({ }^{1}\\). The result is called the iterated GMM estimator and is a common implementation of efficient GMM.\nInterestingly, B. E. Hansen and Lee (2021) show that the iterated GMM estimator is unaffected if the weight matrix is computed with or without centering. Standard errors and test statistics, however, will be affected by the choice.\nThe iterated GMM estimator of the IV regression equation can be computed in Stata using the ivregress gmm command using the igmm option."
  },
  {
    "objectID": "chpt13-gmm.html#covariance-matrix-estimation",
    "href": "chpt13-gmm.html#covariance-matrix-estimation",
    "title": "13  Generalized Method of Moments",
    "section": "13.12 Covariance Matrix Estimation",
    "text": "13.12 Covariance Matrix Estimation\nAn estimator of the asymptotic variance of \\(\\widehat{\\beta}_{\\mathrm{gmm}}\\) can be obtained by replacing the matrices in the asymptotic variance formula by consistent estimators.\n\\({ }^{1}\\) In practice, “convergence” obtains when the difference between the estimates at subsequent steps is smaller than a prespecified tolerance. A sufficient condition for convergence is that the sequence is a contraction mapping. Indeed, B. Hansen and Lee (2021) have shown that the iterated GMM estimator generally satisfies this condition in large samples. For the one-step or two-step GMM estimator the covariance matrix estimator is\n\\[\n\\widehat{\\boldsymbol{V}}_{\\beta}=\\left(\\widehat{\\boldsymbol{Q}}^{\\prime} \\widehat{\\boldsymbol{W}} \\widehat{\\boldsymbol{Q}}\\right)^{-1}\\left(\\widehat{\\boldsymbol{Q}}^{\\prime} \\widehat{\\boldsymbol{W}} \\widehat{\\Omega} \\widehat{\\boldsymbol{W}} \\widehat{\\boldsymbol{Q}}\\right)\\left(\\widehat{\\boldsymbol{Q}}^{\\prime} \\widehat{\\boldsymbol{W}} \\widehat{\\boldsymbol{Q}}\\right)^{-1}\n\\]\nwhere \\(\\widehat{\\boldsymbol{Q}}=\\frac{1}{n} \\sum_{i=1}^{n} Z_{i} X_{i}^{\\prime}\\). The weight matrix is constructed using either the uncentered estimator (13.8) or centered estimator (13.9) with the residuals \\(\\widehat{e}_{i}=Y_{i}-X_{i}^{\\prime} \\widehat{\\beta}_{\\mathrm{gmm}}\\).\nFor the efficient iterated GMM estimator the covariance matrix estimator is\n\\[\n\\widehat{\\boldsymbol{V}}_{\\beta}=\\left(\\widehat{\\boldsymbol{Q}}^{\\prime} \\widehat{\\Omega}^{-1} \\widehat{\\boldsymbol{Q}}\\right)^{-1}=\\left(\\left(\\frac{1}{n} \\boldsymbol{X}^{\\prime} \\boldsymbol{Z}\\right) \\widehat{\\Omega}^{-1}\\left(\\frac{1}{n} \\boldsymbol{Z}^{\\prime} \\boldsymbol{X}\\right)\\right)^{-1} .\n\\]\n\\(\\widehat{\\Omega}\\) can be computed using either the uncentered estimator (13.8) or centered estimator (13.9). Based on the asymptotic approximation the estimator (13.11) can be used as well for the two-step estimator but should use the final residuals \\(\\widehat{e}_{i}=Y_{i}-X_{i}^{\\prime} \\widehat{\\beta}_{\\mathrm{gmm}}\\).\nAsymptotic standard errors are given by the square roots of the diagonal elements of \\(n^{-1} \\widehat{\\boldsymbol{V}}_{\\beta}\\).\nIt is unclear if it is preferred to use the covariance matrix estimator based on the centered or uncentered estimator of \\(\\Omega\\) to construct the covariance matrix estimator. Using the centered estimator results in a smaller covariance matrix and standard errors and thus more “significant” tests based on asymptotic critical values. In contrast the uncentered estimator of \\(\\Omega\\) will result in larger standard errors and will thus be more “conservative”.\nIn Stata, the default covariance matrix estimation method is determined by the choice of weight matrix. Thus if the centered estimator (13.9) is used for the weight matrix it is also used for the covariance matrix estimator."
  },
  {
    "objectID": "chpt13-gmm.html#clustered-dependence",
    "href": "chpt13-gmm.html#clustered-dependence",
    "title": "13  Generalized Method of Moments",
    "section": "13.13 Clustered Dependence",
    "text": "13.13 Clustered Dependence\nIn Section \\(4.21\\) we introduced clustered dependence and in Section \\(12.25\\) described covariance matrix estimation for 2SLS. The methods extend naturally to GMM but with the additional complication of potentially altering weight matrix calculation.\nThe structural equation for the \\(g^{t h}\\) cluster can be written as the matrix system \\(\\boldsymbol{Y}_{g}=\\boldsymbol{X}_{g} \\beta+\\boldsymbol{e}_{g}\\). Using this notation the centered GMM estimator with weight matrix \\(\\boldsymbol{W}\\) can be written as\n\\[\n\\widehat{\\beta}_{\\mathrm{gmm}}-\\beta=\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{Z} \\boldsymbol{W} \\boldsymbol{Z}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\boldsymbol{X}^{\\prime} \\boldsymbol{Z} \\boldsymbol{W}\\left(\\sum_{g=1}^{G} \\boldsymbol{Z}_{g}^{\\prime} \\boldsymbol{e}_{g}\\right)\n\\]\nThe cluster-robust covariance matrix estimator for \\(\\widehat{\\beta}_{\\mathrm{gmm}}\\) is\n\\[\n\\widehat{\\boldsymbol{V}}_{\\beta}=\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{Z} \\boldsymbol{W} \\boldsymbol{Z}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\boldsymbol{X}^{\\prime} \\boldsymbol{Z} \\boldsymbol{W} \\widehat{\\boldsymbol{S}} \\boldsymbol{W} \\boldsymbol{Z}^{\\prime} \\boldsymbol{X}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{Z} \\boldsymbol{W} \\boldsymbol{Z}^{\\prime} \\boldsymbol{X}\\right)^{-1}\n\\]\nwith\n\\[\n\\widehat{\\boldsymbol{S}}=\\sum_{g=1}^{G} \\boldsymbol{Z}_{g}^{\\prime} \\widehat{\\boldsymbol{e}}_{g} \\widehat{\\boldsymbol{e}}_{g}^{\\prime} \\boldsymbol{Z}_{g}\n\\]\nand the clustered residuals\n\\[\n\\widehat{\\boldsymbol{e}}_{g}=\\boldsymbol{Y}_{g}-\\boldsymbol{X}_{g} \\widehat{\\beta}_{\\mathrm{gmm}} .\n\\]\nThe cluster-robust estimator (13.12) is appropriate for the one-step or two-step GMM estimator. It is also appropriate for the iterated estimator when the latter uses a conventional (non-clustered) efficient weight matrix. However in the clustering context it is more natural to use a cluster-robust weight matrix such as \\(\\boldsymbol{W}=\\widehat{\\boldsymbol{S}}^{-1}\\) where \\(\\widehat{\\boldsymbol{S}}\\) is a cluster-robust covariance estimator as in (13.13) based on a one-step or iterated residual. This gives rise to the cluster-robust GMM estimator\n\\[\n\\widehat{\\beta}_{\\mathrm{gmm}}=\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{Z} \\widehat{\\boldsymbol{S}}^{-1} \\boldsymbol{Z}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\boldsymbol{X}^{\\prime} \\boldsymbol{Z} \\widehat{\\boldsymbol{S}}^{-1} \\boldsymbol{Z}^{\\prime} \\boldsymbol{Y}\n\\]\nAn appropriate cluster-robust covariance matrix estimator is\n\\[\n\\widehat{\\boldsymbol{V}}_{\\beta}=\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{Z} \\widehat{\\boldsymbol{S}}^{-1} \\boldsymbol{Z}^{\\prime} \\boldsymbol{X}\\right)^{-1}\n\\]\nwhere \\(\\widehat{S}\\) is calculated using the final residuals.\nTo implement a cluster-robust weight matrix use the 2SLS estimator for first step. Compute the cluster residuals (13.14) and covariance matrix (13.13). Then (13.15) is the two-step GMM estimator. Iterating the residuals and covariance matrix until convergence we obtain the iterated GMM estimator.\nIn Stata, using the ivregress gmm command with the cluster option implements the two-step GMM estimator using the cluster-robust weight matrix and cluster-robust covariance matrix estimator. To use the centered covariance matrix use the center option and to implement the iterated GMM estimator use the igmm option. Alternatively, you can use the wmatrix and vce options to separately specify the weight matrix and covariance matrix estimation methods."
  },
  {
    "objectID": "chpt13-gmm.html#wald-test",
    "href": "chpt13-gmm.html#wald-test",
    "title": "13  Generalized Method of Moments",
    "section": "13.14 Wald Test",
    "text": "13.14 Wald Test\nFor a given function \\(r(\\beta): \\mathbb{R}^{k} \\rightarrow \\Theta \\subset \\mathbb{R}^{q}\\) we define the parameter \\(\\theta=r(\\beta)\\). The GMM estimator of \\(\\theta\\) is \\(\\widehat{\\theta}_{\\mathrm{gmm}}=r\\left(\\widehat{\\beta}_{\\mathrm{gmm}}\\right)\\). By the delta method it is asymptotically normal with covariance matrix \\(\\boldsymbol{V}_{\\theta}=\\) \\(\\boldsymbol{R}^{\\prime} \\boldsymbol{V}_{\\beta} \\boldsymbol{R}\\) where \\(\\boldsymbol{R}=\\frac{\\partial}{\\partial \\beta} r(\\beta)^{\\prime}\\). An estimator of the asymptotic covariance matrix is \\(\\widehat{\\boldsymbol{V}}_{\\theta}=\\widehat{\\boldsymbol{R}}^{\\prime} \\widehat{\\boldsymbol{V}}_{\\beta} \\widehat{\\boldsymbol{R}}^{\\text {where }}\\) \\(\\widehat{\\boldsymbol{R}}=\\frac{\\partial}{\\partial \\beta} r\\left(\\widehat{\\beta}_{\\mathrm{gmm}}\\right)^{\\prime}\\). When \\(\\theta\\) is scalar then an asymptotic standard error for \\(\\widehat{\\theta}_{\\mathrm{gmm}}\\) is formed as \\(\\sqrt{n^{-1} \\widehat{\\boldsymbol{V}}_{\\theta}} .\\)\nA standard test of the hypothesis \\(\\mathbb{M}_{0}: \\theta=\\theta_{0}\\) against \\(\\mathbb{M}_{1}: \\theta \\neq \\theta_{0}\\) is based on the Wald statistic\n\\[\nW=n\\left(\\widehat{\\theta}-\\theta_{0}\\right)^{\\prime} \\widehat{\\boldsymbol{V}}_{\\widehat{\\theta}}^{-1}\\left(\\widehat{\\theta}-\\theta_{0}\\right) .\n\\]\nLet \\(G_{q}(u)\\) denote the \\(\\chi_{q}^{2}\\) distribution function.\nTheorem \\(13.8\\) Under Assumption 12.2, Assumption 7.3, and \\(\\mathbb{H}_{0}\\), as \\(n \\rightarrow \\infty\\), \\(W \\underset{d}{\\longrightarrow} \\chi_{q}^{2}\\). For \\(c\\) satisfying \\(\\alpha=1-G_{q}(c), \\mathbb{P}\\left[W>c \\mid \\mathbb{H}_{0}\\right] \\longrightarrow \\alpha\\) so the test “Reject \\(\\mathbb{H}_{0}\\) if \\(W>c\\)” has asymptotic size \\(\\alpha\\).\nFor a proof see Exercise 13.5.\nIn Stata, the commands test and testparm can be used after ivregress gmm to implement Wald tests of linear hypotheses. The commands nlcom and testnl can be used after ivregress gmm to implement Wald tests of nonlinear hypotheses."
  },
  {
    "objectID": "chpt13-gmm.html#restricted-gmm",
    "href": "chpt13-gmm.html#restricted-gmm",
    "title": "13  Generalized Method of Moments",
    "section": "13.15 Restricted GMM",
    "text": "13.15 Restricted GMM\nIt is often desirable to impose restrictions on the coefficients. In this section we consider estimation subject to the linear constraints \\(\\boldsymbol{R}^{\\prime} \\beta=\\boldsymbol{c}\\). In the following section we consider nonlinear constraints.\nThe constrained GMM estimator minimizes the GMM criterion subject to the constraint. It is\n\\[\n\\widehat{\\beta}_{\\mathrm{cgmm}}=\\underset{\\boldsymbol{R}^{\\prime} \\beta=\\boldsymbol{c}}{\\operatorname{argmin}} J(\\beta) .\n\\]\nThis is the parameter vector which makes the estimating equations as close to zero as possible with respect to the weighted quadratic distance while imposing the restriction on the parameters.\nSuppose the weight matrix \\(\\boldsymbol{W}\\) is fixed. Using the methods of Chapter 8 it is straightforward to derive that the constrained GMM estimator is\n\\[\n\\widehat{\\beta}_{\\mathrm{cgmm}}=\\widehat{\\beta}_{\\mathrm{gmm}}-\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{Z} \\boldsymbol{W} \\boldsymbol{Z}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\boldsymbol{R}\\left(\\boldsymbol{R}^{\\prime}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{Z} \\boldsymbol{W} \\boldsymbol{Z}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\boldsymbol{R}\\right)^{-1}\\left(\\boldsymbol{R}^{\\prime} \\widehat{\\beta}_{\\mathrm{gmm}}-\\boldsymbol{c}\\right) .\n\\]\n(For details, see Exercise 13.6.)\nWe derive the asymptotic distribution under the assumption that the restriction is true. Make the substitution \\(\\boldsymbol{c}=\\boldsymbol{R}^{\\prime} \\beta\\) in (13.16) and reorganize to find\n\\[\n\\sqrt{n}\\left(\\widehat{\\beta}_{\\mathrm{cgmm}}-\\beta\\right)=\\left(\\boldsymbol{I}_{k}-\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{Z} \\boldsymbol{W} \\boldsymbol{Z}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\boldsymbol{R}\\left(\\boldsymbol{R}^{\\prime}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{Z} \\boldsymbol{W} \\boldsymbol{Z}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\boldsymbol{R}\\right)^{-1} \\boldsymbol{R}^{\\prime}\\right) \\sqrt{n}\\left(\\widehat{\\beta}_{\\mathrm{gmm}}-\\beta\\right)\n\\]\nThis is a linear function of \\(\\sqrt{n}\\left(\\widehat{\\beta}_{\\mathrm{gmm}}-\\beta\\right)\\). Since the asymptotic distribution of the latter is known the asymptotic distribution of \\(\\sqrt{n}\\left(\\widehat{\\beta}_{\\mathrm{cgmm}}-\\beta\\right)\\) is a linear function of the former.\n\\[\n\\begin{aligned}\n&\\text { Theorem 13.9 Under Assumptions } 12.2 \\text { and 8.3, for the constrained GMM es- } \\\\\n&\\text { timator (13.16), } \\sqrt{n}\\left(\\widehat{\\beta}_{\\mathrm{cgmm}}-\\beta\\right) \\underset{d}{\\longrightarrow} \\mathrm{N}\\left(0, \\boldsymbol{V}_{\\mathrm{cgmm}}\\right) \\text { as } n \\rightarrow \\infty \\text {, where } \\\\\n&\\boldsymbol{V}_{\\mathrm{cgmm}}=\\boldsymbol{V}_{\\beta}-\\left(\\boldsymbol{Q}^{\\prime} \\boldsymbol{W} \\boldsymbol{Q}\\right)^{-1} \\boldsymbol{R}\\left(\\boldsymbol{R}^{\\prime}\\left(\\boldsymbol{Q}^{\\prime} \\boldsymbol{W} \\boldsymbol{Q}\\right)^{-1} \\boldsymbol{R}\\right)^{-1} \\boldsymbol{R}^{\\prime} \\boldsymbol{V}_{\\beta} \\\\\n&\\quad-\\boldsymbol{V}_{\\beta} \\boldsymbol{R}\\left(\\boldsymbol{R}^{\\prime}\\left(\\boldsymbol{Q}^{\\prime} \\boldsymbol{W} \\boldsymbol{Q}\\right)^{-1} \\boldsymbol{R}\\right)^{-1} \\boldsymbol{R}^{\\prime}\\left(\\boldsymbol{Q}^{\\prime} \\boldsymbol{W} \\boldsymbol{Q}\\right)^{-1} \\\\\n& \\\\\n&+\\left(\\boldsymbol{Q}^{\\prime} \\boldsymbol{W} \\boldsymbol{Q}\\right)^{-1} \\boldsymbol{R}\\left(\\boldsymbol{R}^{\\prime}\\left(\\boldsymbol{Q}^{\\prime} \\boldsymbol{W} \\boldsymbol{Q}\\right)^{-1} \\boldsymbol{R}\\right)^{-1} \\boldsymbol{R}^{\\prime} \\boldsymbol{V}_{\\beta} \\boldsymbol{R}\\left(\\boldsymbol{R}^{\\prime}\\left(\\boldsymbol{Q}^{\\prime} \\boldsymbol{W} \\boldsymbol{Q}\\right)^{-1} \\boldsymbol{R}\\right)^{-1} \\boldsymbol{R}^{\\prime}\\left(\\boldsymbol{Q}^{\\prime} \\boldsymbol{W} \\boldsymbol{Q}\\right)^{-1}\n\\end{aligned}\n\\]\nFor a proof, see Exercise 13.8. Unfortunately the asymptotic covariance matrix formula (13.18) is quite tedious!\nNow suppose that the the weight matrix is set as \\(W=\\widehat{\\Omega}^{-1}\\), the efficient weight matrix from unconstrained estimation. In this case the constrained GMM estimator can be written as\n\\[\n\\widehat{\\beta}_{\\mathrm{cgmm}}=\\widehat{\\beta}_{\\mathrm{gmm}}-\\widehat{\\boldsymbol{V}}_{\\beta} \\boldsymbol{R}\\left(\\boldsymbol{R}^{\\prime} \\widehat{\\boldsymbol{V}}_{\\boldsymbol{\\beta}} \\boldsymbol{R}\\right)^{-1}\\left(\\boldsymbol{R}^{\\prime} \\widehat{\\beta}_{\\mathrm{gmm}}-\\boldsymbol{c}\\right)\n\\]\nwhich is the same formula (8.25) as efficient minimum distance. (For details, see Exercise 13.7.) We find that the asymptotic covariance matrix simplifies considerably. Theorem 13.10 Under Assumptions \\(12.2\\) and 8.3, for the efficient constrained GMM estimator (13.19), \\(\\sqrt{n}\\left(\\widehat{\\beta}_{\\mathrm{cgmm}}-\\beta\\right) \\underset{d}{\\longrightarrow} \\mathrm{N}\\left(0, \\boldsymbol{V}_{\\mathrm{cgmm}}\\right)\\) as \\(n \\rightarrow \\infty\\), where\n\\[\n\\boldsymbol{V}_{\\mathrm{cgmm}}=\\boldsymbol{V}_{\\beta}-\\boldsymbol{V}_{\\beta} \\boldsymbol{R}\\left(\\boldsymbol{R}^{\\prime} \\boldsymbol{V}_{\\beta} \\boldsymbol{R}\\right)^{-1} \\boldsymbol{R}^{\\prime} \\boldsymbol{V}_{\\beta} .\n\\]\nFor a proof, see Exercise 13.9.\nThe asymptotic covariance matrix (13.20) can be estimated by\n\\[\n\\begin{aligned}\n\\widehat{\\boldsymbol{V}}_{\\mathrm{cgmm}} &=\\widetilde{\\boldsymbol{V}}_{\\beta}-\\widetilde{\\boldsymbol{V}}_{\\beta} \\boldsymbol{R}\\left(\\boldsymbol{R}^{\\prime} \\widetilde{\\boldsymbol{V}}_{\\beta} \\boldsymbol{R}\\right)^{-1} \\boldsymbol{R}^{\\prime} \\widetilde{\\boldsymbol{V}}_{\\beta} . \\\\\n\\widetilde{\\boldsymbol{V}}_{\\beta} &=\\left(\\widehat{\\boldsymbol{Q}}^{\\prime} \\widetilde{\\Omega}^{-1} \\widehat{\\boldsymbol{Q}}\\right)^{-1} \\\\\n\\widetilde{\\Omega} &=\\frac{1}{n} \\sum_{i=1}^{n} Z_{i} Z_{i}^{\\prime} \\widetilde{e}_{i}^{2} \\\\\n\\widetilde{e}_{i} &=Y_{i}-X_{i}^{\\prime} \\widehat{\\beta}_{\\mathrm{cgmm}} .\n\\end{aligned}\n\\]\nThe covariance matrix (13.18) can be estimated similarly, though using (13.10) to estimate \\(\\boldsymbol{V}_{\\beta}\\). The covariance matrix estimator \\(\\widetilde{\\Omega}\\) can also be replaced with a centered version.\nA constrained iterated GMM estimator can be implemented by setting \\(\\boldsymbol{W}=\\widetilde{\\Omega}^{-1}\\) where \\(\\widetilde{\\Omega}\\) is defined in (13.22) and then iterating until convergence. This is a natural estimator as it is the appropriate implementation of iterated GMM.\nSince both \\(\\widehat{\\Omega}\\) and \\(\\widetilde{\\Omega}\\) converge to the same limit \\(\\Omega\\) under the assumption that the constraint is true the constrained iterated GMM estimator has the asymptotic distribution given in Theorem 13.10."
  },
  {
    "objectID": "chpt13-gmm.html#nonlinear-restricted-gmm",
    "href": "chpt13-gmm.html#nonlinear-restricted-gmm",
    "title": "13  Generalized Method of Moments",
    "section": "13.16 Nonlinear Restricted GMM",
    "text": "13.16 Nonlinear Restricted GMM\nNonlinear constraints on the parameters can be written as \\(r(\\beta)=0\\) for some function \\(r: \\mathbb{R}^{k} \\rightarrow \\mathbb{R}^{q}\\). The constraint is nonlinear if \\(r(\\beta)\\) cannot be written as a linear function of \\(\\beta\\). Least squares estimation subject to nonlinear constraints was explored in Section 8.14. In this section we introduce GMM estimation subject to nonlinear constraints.\nThe constrained GMM estimator minimizes the GMM criterion subject to the constraint. It is\n\\[\n\\widehat{\\beta}_{\\mathrm{cgmm}}=\\underset{r(\\beta)=0}{\\operatorname{argmin}} J(\\beta) .\n\\]\nThis is the parameter vector which makes the estimating equations as close to zero as possible with respect to the weighted quadratic distance while imposing the restriction on the parameters.\nIn general there is no explicit solution for \\(\\widehat{\\beta}_{\\mathrm{cgmm}}\\). Instead the solution is found numerically. Fortunately there are excellent nonlinear constrained optimization solvers implemented in standard software packages.\nFor the asymptotic distribution assume that the restriction \\(r(\\beta)=0\\) is true. Using the same methods as in the proof of Theorem \\(8.10\\) we can show that (13.17) approximately holds in the sense that\n\\[\n\\sqrt{n}\\left(\\widehat{\\beta}_{\\mathrm{cgmm}}-\\beta\\right)=\\left(\\boldsymbol{I}_{k}-\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{Z} \\boldsymbol{W} \\boldsymbol{Z}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\boldsymbol{R}\\left(\\boldsymbol{R}^{\\prime}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{Z} \\boldsymbol{W} \\boldsymbol{Z}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\boldsymbol{R}\\right)^{-1} \\boldsymbol{R}^{\\prime}\\right) \\sqrt{n}\\left(\\widehat{\\beta}_{\\mathrm{gmm}}-\\beta\\right)+o_{p}(1)\n\\]\nwhere \\(\\boldsymbol{R}=\\frac{\\partial}{\\partial \\beta} r(\\beta)^{\\prime}\\). Thus the asymptotic distribution of the constrained estimator takes the same form as in the linear case. Theorem \\(13.11\\) Under Assumptions \\(12.2\\) and 8.3, for the constrained GMM estimator (13.23), \\(\\sqrt{n}\\left(\\widehat{\\beta}_{\\mathrm{cgmm}}-\\beta\\right) \\underset{d}{\\longrightarrow} \\mathrm{N}\\left(0, V_{\\mathrm{cgmm}}\\right)\\) as \\(n \\rightarrow \\infty\\), where \\(\\boldsymbol{V}_{\\mathrm{cgmm}}\\) equals (13.18). If \\(W=\\widehat{\\Omega}^{-1}\\), then \\(V_{\\text {cgmm }}\\) equals (13.20).\nThe asymptotic covariance matrix in the efficient case is estimated by (13.21) with \\(\\boldsymbol{R}\\) replaced with \\(\\widehat{\\boldsymbol{R}}=\\frac{\\partial}{\\partial \\beta} r\\left(\\widehat{\\beta}_{\\text {cgmm }}\\right)^{\\prime}\\). The asymptotic covariance matrix (13.18) in the general case is estimated similarly.\nTo implement an iterated restricted GMM estimator the weight matrix may be set as \\(\\boldsymbol{W}=\\widetilde{\\Omega}^{-1}\\) where \\(\\widetilde{\\Omega}\\) is defined in (13.22), and then iterated until convergence."
  },
  {
    "objectID": "chpt13-gmm.html#constrained-regression",
    "href": "chpt13-gmm.html#constrained-regression",
    "title": "13  Generalized Method of Moments",
    "section": "13.17 Constrained Regression",
    "text": "13.17 Constrained Regression\nTake the conventional projection model \\(Y=X^{\\prime} \\beta+e\\) with \\(\\mathbb{E}[X e]=0\\). This is a special case of GMM as it is model (13.5) with \\(Z=X\\). The just-identified GMM estimator equals least squares \\(\\widehat{\\beta}_{\\mathrm{gmm}}=\\widehat{\\beta}_{\\mathrm{ols}}\\).\nIn Chapter 8 we discussed estimation of the projection model subject to linear constraints \\(\\boldsymbol{R}^{\\prime} \\beta=\\boldsymbol{c}\\), which includes exclusion restrictions. Since the projection model is a special case of GMM the constrained projection model is also constrained GMM. From the results of Section \\(13.15\\) we find that the efficient constrained GMM estimator is\n\\[\n\\widehat{\\beta}_{\\mathrm{cgmm}}=\\widehat{\\beta}_{\\mathrm{ols}}-\\widehat{\\boldsymbol{V}}_{\\beta} \\boldsymbol{R}\\left(\\boldsymbol{R}^{\\prime} \\widehat{\\boldsymbol{V}}_{\\beta} \\boldsymbol{R}\\right)^{-1}\\left(\\boldsymbol{R}^{\\prime} \\widehat{\\beta}_{\\mathrm{ols}}-\\boldsymbol{c}\\right)=\\widehat{\\beta}_{\\mathrm{emd}},\n\\]\nthe efficient minimum distance estimator. Thus for linear constraints on the linear projection model efficient GMM equals efficient minimum distance. Thus one convenient method to implement efficient minimum distance is GMM."
  },
  {
    "objectID": "chpt13-gmm.html#multivariate-regression",
    "href": "chpt13-gmm.html#multivariate-regression",
    "title": "13  Generalized Method of Moments",
    "section": "13.18 Multivariate Regression",
    "text": "13.18 Multivariate Regression\nGMM methods can simplify estimation and inference for multivariate regressions such as those introduced in Chapter \\(11 .\\)\nThe general multivariate regression (projection) model is\n\\[\n\\begin{aligned}\nY_{j} &=X_{j}^{\\prime} \\beta_{j}+e_{j} \\\\\n\\mathbb{E}\\left[X_{j} e_{j}\\right] &=0\n\\end{aligned}\n\\]\nfor \\(j=1, \\ldots, m\\). Using the notation from Section \\(11.2\\) the equations can be written jointly as \\(Y=\\bar{X} \\beta+e\\) and for the full sample as \\(\\boldsymbol{Y}=\\overline{\\boldsymbol{X}} \\beta+\\boldsymbol{e}\\). The \\(\\bar{k}\\) moment conditions are\n\\[\n\\mathbb{E}\\left[\\bar{X}^{\\prime}(Y-\\bar{X} \\beta)\\right]=0 .\n\\]\nGiven a \\(\\bar{k} \\times \\bar{k}\\) weight matrix \\(\\boldsymbol{W}\\) the GMM criterion is\n\\[\nJ(\\beta)=n(\\boldsymbol{Y}-\\overline{\\boldsymbol{X}} \\beta)^{\\prime} \\overline{\\boldsymbol{X}} \\boldsymbol{W} \\overline{\\boldsymbol{X}}^{\\prime}(\\boldsymbol{Y}-\\overline{\\boldsymbol{X}} \\beta) .\n\\]\nThe GMM estimator \\(\\widehat{\\beta}_{\\text {gmm }}\\) minimizes \\(J(\\beta)\\). Since this is a just-identified model the estimator solves the sample equations\n\\[\n\\overline{\\boldsymbol{X}}^{\\prime}\\left(\\boldsymbol{Y}-\\overline{\\boldsymbol{X}} \\widehat{\\beta}_{\\mathrm{gmm}}\\right)=0\n\\]\nThe solution is\n\\[\n\\widehat{\\beta}_{\\mathrm{gmm}}=\\left(\\sum_{i=1}^{n} \\bar{X}_{i}^{\\prime} \\bar{X}_{i}\\right)^{-1}\\left(\\sum_{i=1}^{n} \\bar{X}_{i}^{\\prime} Y_{i}\\right)=\\left(\\overline{\\boldsymbol{X}}^{\\prime} \\overline{\\boldsymbol{X}}\\right)^{-1}\\left(\\overline{\\boldsymbol{X}}^{\\prime} \\boldsymbol{Y}\\right)=\\widehat{\\beta}_{\\mathrm{ols}}\n\\]\nthe multivariate least squares estimator.\nThus the unconstrained GMM estimator of the multivariate regression model is least squares. The estimator does not depend on the weight matrix since the model is just-identified.\nA important advantage of the GMM framework is the ability to incorporate cross-equation constraints. Consider the class of restrictions \\(\\boldsymbol{R}^{\\prime} \\beta=\\boldsymbol{c}\\). Minimization of the GMM criterion subject to this restriction has solutions as described in (13.15). The restricted GMM estimator is\n\\[\n\\widehat{\\beta}_{\\mathrm{gmm}}=\\widehat{\\beta}_{\\mathrm{ols}}-\\left(\\overline{\\boldsymbol{X}}^{\\prime} \\overline{\\boldsymbol{X}} \\boldsymbol{W} \\overline{\\boldsymbol{X}}^{\\prime} \\overline{\\boldsymbol{X}}\\right)^{-1} \\boldsymbol{R}\\left(\\boldsymbol{R}^{\\prime}\\left(\\overline{\\boldsymbol{X}}^{\\prime} \\overline{\\boldsymbol{X}} \\boldsymbol{W} \\overline{\\boldsymbol{X}}^{\\prime} \\overline{\\boldsymbol{X}}\\right)^{-1} \\boldsymbol{R}\\right)^{-1}\\left(\\boldsymbol{R}^{\\prime} \\widehat{\\beta}_{\\mathrm{ols}}-\\boldsymbol{c}\\right)\n\\]\nThis estimator depends on the weight matrix because it is over-identified.\nA simple choice for weight matrix is \\(\\boldsymbol{W}=\\overline{\\boldsymbol{X}}^{\\prime} \\overline{\\boldsymbol{X}}\\). This leads to the one-step estimator\n\\[\n\\widehat{\\beta}_{1}=\\widehat{\\beta}_{\\mathrm{ols}}-\\left(\\overline{\\boldsymbol{X}}^{\\prime} \\overline{\\boldsymbol{X}}\\right)^{-1} \\boldsymbol{R}\\left(\\boldsymbol{R}^{\\prime}\\left(\\overline{\\boldsymbol{X}}^{\\prime} \\overline{\\boldsymbol{X}}\\right)^{-1} \\boldsymbol{R}\\right)^{-1}\\left(\\boldsymbol{R}^{\\prime} \\widehat{\\beta}_{\\mathrm{ols}}-\\boldsymbol{c}\\right) .\n\\]\nThe asymptotically efficient choice sets \\(\\boldsymbol{W}=\\widehat{\\Omega}^{-1}\\) where \\(\\widehat{\\Omega}=n^{-1} \\sum_{i=1}^{n} \\bar{X}_{i}^{\\prime} \\widehat{e}_{i} \\widehat{e}_{i}^{\\prime} \\bar{X}_{i}\\) and \\(\\widehat{e}_{i}=Y_{i}-\\bar{X}_{i} \\widehat{\\beta}_{1}\\). This leads to the two-step estimator\n\\[\n\\widehat{\\beta}_{2}=\\widehat{\\beta}_{\\text {ols }}-\\left(\\overline{\\boldsymbol{X}}^{\\prime} \\overline{\\boldsymbol{X}} \\widehat{\\Omega}^{-1} \\overline{\\boldsymbol{X}}^{\\prime} \\overline{\\boldsymbol{X}}\\right)^{-1} \\boldsymbol{R}\\left(\\boldsymbol{R}^{\\prime}\\left(\\overline{\\boldsymbol{X}}^{\\prime} \\overline{\\boldsymbol{X}} \\widehat{\\Omega}^{-1} \\overline{\\boldsymbol{X}}^{\\prime} \\overline{\\boldsymbol{X}}\\right)^{-1} \\boldsymbol{R}\\right)^{-1}\\left(\\boldsymbol{R}^{\\prime} \\widehat{\\beta}_{\\mathrm{ols}}-\\boldsymbol{c}\\right)\n\\]\nWhen the regressors \\(X\\) are common across all equations the multivariate regression model can be written conveniently as in (11.3): \\(Y=\\boldsymbol{B}^{\\prime} X+e\\) with \\(\\mathbb{E}\\left[X e^{\\prime}\\right]=0\\). The moment restrictions can be written as the matrix system \\(\\mathbb{E}\\left[X\\left(Y^{\\prime}-X^{\\prime} \\boldsymbol{B}\\right)\\right]=0\\). Written as a vector system this is (13.24) and leads to the same restricted GMM estimators.\nThese are general formula for imposing restrictions. In specific cases (such as an exclusion restriction) direct methods may be more convenient. In all cases the solution is found by minimization of the GMM criterion \\(J(\\beta)\\) subject to the restriction."
  },
  {
    "objectID": "chpt13-gmm.html#distance-test",
    "href": "chpt13-gmm.html#distance-test",
    "title": "13  Generalized Method of Moments",
    "section": "13.19 Distance Test",
    "text": "13.19 Distance Test\nIn Section \\(13.14\\) we introduced Wald tests of the hypothesis \\(\\mathbb{M}_{0}: \\theta=\\theta_{0}\\) where \\(\\theta=r(\\beta)\\) for a given function \\(r(\\beta): \\mathbb{R}^{k} \\rightarrow \\Theta \\subset \\mathbb{R}^{q}\\). When \\(r(\\beta)\\) is nonlinear an alternative is to use a criterion-based statistic. This is sometimes called the GMM Distance statistic and sometimes called a LR-like statistic (the LR is for likelihood-ratio). The idea was first put forward by Newey and West (1987a).\nThe idea is to compare the unrestricted and restricted estimators by contrasting the criterion functions. The unrestricted estimator takes the form\n\\[\n\\widehat{\\beta}_{\\mathrm{gmm}}=\\underset{\\beta}{\\operatorname{argmin}} \\widehat{J}(\\beta)\n\\]\nwhere\n\\[\n\\widehat{J}(\\beta)=n \\bar{g}_{n}(\\beta)^{\\prime} \\widehat{\\Omega}^{-1} \\bar{g}_{n}(\\beta)\n\\]\nis the unrestricted GMM criterion with an efficient weight matrix estimate \\(\\widehat{\\Omega}\\). The minimized value of the criterion is \\(\\widehat{J}=\\widehat{J}\\left(\\widehat{\\beta}_{\\mathrm{gmm}}\\right)\\). As in Section 13.15, the estimator subject to \\(r(\\beta)=\\theta_{0}\\) is\n\\[\n\\widehat{\\beta}_{\\mathrm{cgmm}}=\\underset{r(\\beta)=\\theta_{0}}{\\operatorname{argmin}} \\widetilde{J}(\\beta)\n\\]\nwhere\n\\[\n\\widetilde{J}(\\beta)=n \\bar{g}_{n}(\\beta)^{\\prime} \\widetilde{\\Omega}^{-1} \\bar{g}_{n}(\\beta)\n\\]\nwhich depends on an efficient weight matrix estimator, either \\(\\widehat{\\Omega}\\) (the same as the unrestricted estimator) or \\(\\widetilde{\\Omega}\\) (the iterated weight matrix from constrained estimation). The minimized value of the criterion is \\(\\widetilde{J}=\\widetilde{J}\\left(\\widehat{\\beta}_{\\operatorname{cgmm}}\\right)\\)\nThe GMM distance (or LR-like) statistic is the difference in the criterion functions: \\(D=\\widetilde{J}-\\widehat{J}\\). The distance test shares the useful feature of LR tests in that it is a natural by-product of the computation of alternative models.\nThe test has the following large sample distribution.\nTheorem \\(13.12\\) Under Assumption 12.2, Assumption 7.3, and \\(\\mathbb{H}_{0}\\), then as \\(n \\rightarrow\\) \\(\\infty, D \\longrightarrow \\chi_{q}^{2}\\). For \\(c\\) satisfying \\(\\alpha=1-G_{q}(c), \\mathbb{P}\\left[D>c \\mid \\mathbb{H}_{0}\\right] \\longrightarrow \\alpha\\). The test “Reject \\(\\mathbb{H}_{0}\\) if \\(D>c\\)” has asymptotic size \\(\\alpha\\).\nThe proof is given in Section 13.28.\nTheorem \\(13.12\\) shows that the distance statistic has the same asymptotic distribution as Wald and likelihood ratio statistics and can be interpreted similarly. Small values of \\(D\\) mean that imposing the restriction does not result in a large value of the moment equations. Hence the restriction appears to be compatible with the data. On the other hand, large values of \\(D\\) mean that imposing the restriction results in a much larger value of the moment equations, implying that the restriction is not compatible with the data. The finding that the asymptotic distribution is chi-squared allows the calculation of asymptotic critical values and p-values.\nWe now discuss the choice of weight matrix. As mentioned above one simple choice is to set \\(\\widetilde{\\Omega}=\\widehat{\\Omega}\\). In this case we have the following result.\nTheorem 13.13 If \\(\\widetilde{\\Omega}=\\widehat{\\Omega}\\) then \\(D \\geq 0\\). Furthermore, if \\(r\\) is linear in \\(\\beta\\) then \\(D\\) equals the Wald statistic.\nThe statement that \\(\\widetilde{\\Omega}=\\widehat{\\Omega}\\) implies \\(D \\geq 0\\) follows from the fact that in this case the criterion functions \\(\\widehat{J}(\\beta)=\\widetilde{J}(\\beta)\\) are identical so the constrained minimum cannot be smaller than the unconstrained. The statement that linear hypotheses and \\(\\widetilde{\\Omega}=\\widehat{\\Omega}\\) implies \\(D=W\\) follows from applying the expression for the constrained GMM estimator (13.19) and using the covariance matrix formula (13.11).\nThe fact that \\(D \\geq 0\\) when \\(\\widetilde{\\Omega}=\\widehat{\\Omega}\\) motivated Newey and West (1987a) to recommend this choice. However, \\(\\widetilde{\\Omega}=\\widehat{\\Omega}\\) is not necessary. Instead, setting \\(\\widetilde{\\Omega}\\) to equal the constrained efficient weight matrix is natural for efficient estimation of \\(\\widehat{\\beta}_{\\mathrm{cgmm}}\\). In the event that \\(D<0\\) the test simply fails to reject \\(\\mathbb{H}_{0}\\) at any significance level.\nAs discussed in Section \\(9.17\\) for tests of nonlinear hypotheses the Wald statistic can work quite poorly. In particular, the Wald statistic is affected by how the hypothesis \\(r(\\beta)\\) is formulated. In contrast, the distance statistic \\(D\\) is not affected by the algebraic formulation of the hypothesis. Current evidence suggests that the \\(D\\) statistic appears to have good sampling properties, and is a preferred test statistic relative to the Wald statistic for nonlinear hypotheses. (See B. E. Hansen (2006).)\nIn Stata the command estat overid after ivregress gmm can be used to report the value of the GMM criterion \\(J\\). By estimating the two nested GMM regressions the values \\(\\widehat{J}\\) and \\(\\widetilde{J}\\) can be obtained and \\(D\\) computed."
  },
  {
    "objectID": "chpt13-gmm.html#continuously-updated-gmm",
    "href": "chpt13-gmm.html#continuously-updated-gmm",
    "title": "13  Generalized Method of Moments",
    "section": "13.20 Continuously-Updated GMM",
    "text": "13.20 Continuously-Updated GMM\nAn alternative to the two-step GMM estimator can be constructed by letting the weight matrix be an explicit function of \\(\\beta\\). These leads to the criterion function\n\\[\nJ(\\beta)=n \\bar{g}_{n}(\\beta)^{\\prime}\\left(\\frac{1}{n} \\sum_{i=1}^{n} g_{i}(\\beta) g_{i}(\\beta)^{\\prime}\\right)^{-1} \\bar{g}_{n}(\\beta) .\n\\]\nThe \\(\\widehat{\\beta}\\) which minimizes this function is called the continuously-updated GMM (CU-GMM) estimator and was introduced by L. Hansen, Heaton and Yaron (1996).\nA complication is that the continuously-updated criterion \\(J(\\beta)\\) is not quadratic in \\(\\beta\\). This means that minimization requires numerical methods. It may appear that the CU-GMM estimator is the same as the iterated GMM estimator but this is not the case at all. They solve distinct first-order conditions and can be quite different in applications.\nRelative to traditional GMM the CU-GMM estimator has lower bias but thicker distributional tails. While it has received considerable theoretical attention it is not used commonly in applications."
  },
  {
    "objectID": "chpt13-gmm.html#overidentification-test",
    "href": "chpt13-gmm.html#overidentification-test",
    "title": "13  Generalized Method of Moments",
    "section": "13.21 OverIdentification Test",
    "text": "13.21 OverIdentification Test\nIn Section \\(12.31\\) we introduced the Sargan (1958) overidentification test for the 2SLS estimator under the assumption of homoskedasticity. L. Hansen (1982) generalized the test to cover the GMM estimator allowing for general heteroskedasticity.\nRecall, overidentified models \\((\\ell>k)\\) are special in the sense that there may not be a parameter value \\(\\beta\\) such that the moment condition \\(\\mathbb{H}_{0}: \\mathbb{E}[Z e]=0\\) holds. Thus the model-the overidentifying restrictions - are testable.\nFor example, take the linear model \\(Y=\\beta_{1}^{\\prime} X_{1}+\\beta_{2}^{\\prime} X_{2}+e\\) with \\(\\mathbb{E}\\left[X_{1} e\\right]=0\\) and \\(\\mathbb{E}\\left[X_{2} e\\right]=0\\). It is possible that \\(\\beta_{2}=0\\) so that the linear equation may be written as \\(Y=\\beta_{1}^{\\prime} X_{1}+e\\). However, it is possible that \\(\\beta_{2} \\neq 0\\). In this case it is impossible to find a value of \\(\\beta_{1}\\) such that both \\(\\mathbb{E}\\left[X_{1}\\left(Y-X_{1}^{\\prime} \\beta_{1}\\right)\\right]=0\\) and \\(\\mathbb{E}\\left[X_{2}\\left(Y-X_{1}^{\\prime} \\beta_{1}\\right)\\right]=0\\) hold simultaneously. In this sense an exclusion restriction can be seen as an overidentifying restriction.\nNote that \\(\\bar{g}_{n} \\underset{p}{\\mathbb{E}}[Z e]\\) and thus \\(\\bar{g}_{n}\\) can be used to assess the hypothesis \\(\\mathbb{E}[Z e]=0\\). Assuming that an efficient weight matrix estimator is used the criterion function at the parameter estimator is \\(J=\\) \\(J\\left(\\widehat{\\beta}_{\\mathrm{gmm}}\\right)=n \\bar{g}_{n}^{\\prime} \\widehat{\\Omega}^{-1} \\bar{g}_{n}\\). This is a quadratic form in \\(\\bar{g}_{n}\\) and is thus a natural test statistic for \\(\\mathbb{H}_{0}: \\mathbb{E}[Z e]=0\\). Note that we assume that the criterion function is constructed with an efficient weight matrix estimator. This is important for the distribution theory.\nTheorem 13.14 Under Assumption \\(12.2\\) then as \\(n \\rightarrow \\infty, J=J\\left(\\widehat{\\beta}_{\\mathrm{gmm}}\\right) \\underset{d}{\\rightarrow} \\chi_{\\ell-k}^{2} \\cdot\\) For \\(c\\) satisfying \\(\\alpha=1-G_{\\ell-k}(c), \\mathbb{P}\\left[J>c \\mid \\mathbb{M}_{0}\\right] \\longrightarrow \\alpha\\) so the test “Reject \\(\\mathbb{H}_{0}\\) if \\(J>c\\)” has asymptotic size \\(\\alpha\\). The proof of the theorem is left to Exercise 13.13.\nThe degrees of freedom of the asymptotic distribution are the number of overidentifying restrictions. If the statistic \\(J\\) exceeds the chi-square critical value we can reject the model. Based on this information alone it is unclear what is wrong but it is typically cause for concern. The GMM overidentification test is a useful by-product of the GMM methodology and it is advisable to report the statistic \\(J\\) whenever GMM is the estimation method. When over-identified models are estimated by GMM it is customary to report the \\(J\\) statistic as a general test of model adequacy.\nIn Stata the command estat overid afer ivregress gmm can be used to implement the overidentification test. The GMM criterion \\(J\\) and its asymptotic \\(\\mathrm{p}\\)-value using the \\(\\chi_{\\ell-k}^{2}\\) distribution are reported."
  },
  {
    "objectID": "chpt13-gmm.html#subset-overidentification-tests",
    "href": "chpt13-gmm.html#subset-overidentification-tests",
    "title": "13  Generalized Method of Moments",
    "section": "13.22 Subset OverIdentification Tests",
    "text": "13.22 Subset OverIdentification Tests\nIn Section \\(12.32\\) we introduced subset overidentification tests for the 2SLS estimator under the assumption of homoskedasticity. In this section we describe how to construct analogous tests for the GMM estimator under general heteroskedasticity.\nRecall, subset overidentification tests are used when it is desired to focus attention on a subset of instruments whose validity is questioned. Partition \\(Z=\\left(Z_{a}, Z_{b}\\right)\\) with dimensions \\(\\ell_{a}\\) and \\(\\ell_{b}\\), respectively, where \\(Z_{a}\\) contains the instruments which are believed to be uncorrelated with \\(e\\) and \\(Z_{b}\\) contains the instruments which may be correlated with \\(e\\). It is necessary to select this partition so that \\(\\ell_{a}>k\\), so that the instruments \\(Z_{a}\\) alone identify the parameters.\nGiven this partition the maintained hypothesis is \\(\\mathbb{E}\\left[Z_{a} e\\right]=0\\). The null and alternative hypotheses are \\(\\mathbb{H}_{0}: \\mathbb{E}\\left[Z_{b} e\\right]=0\\) and \\(\\mathbb{M}_{1}: \\mathbb{E}\\left[Z_{b} e\\right] \\neq 0\\). The GMM test is constructed as follows. First, estimate the model by efficient GMM with only the smaller set \\(Z_{a}\\) of instruments. Let \\(\\widetilde{J}\\) denote the resulting GMM criterion. Second, estimate the model by efficient GMM with the full set \\(Z=\\left(Z_{a}, Z_{b}\\right)\\) of instruments. Let \\(\\widehat{J}\\) denote the resulting GMM criterion. The test statistic is the difference in the criterion functions: \\(C=\\widehat{J}-\\widetilde{J}\\). This is similar to the GMM distance statistic presented in Section 13.19. The difference is that the distance statistic compares models which differ based on the parameter restrictions while the \\(C\\) statistic compares models based on different instrument sets.\nTypically \\(C \\geq 0\\). However, this is not necessary and \\(C<0\\) can arise. If this occurs it leads to a nonrejection of \\(\\mathbb{H}_{0}\\).\nIf the smaller instrument set \\(Z_{a}\\) is just-identified so that \\(\\ell_{a}=k\\) then \\(\\widetilde{J}=0\\) so \\(C=\\widehat{J}\\) is simply the standard overidentification test. This is why we have restricted attention to the case \\(\\ell_{a}>k\\).\nThe test has the following large sample distribution.\nTheorem 13.15 Under Assumption \\(12.2\\) and \\(\\mathbb{E}\\left[Z_{a} X^{\\prime}\\right]\\) has full rank \\(k\\), then as \\(n \\rightarrow \\infty, C \\rightarrow \\underset{d}{\\rightarrow} \\chi_{\\ell_{b}}^{2} .\\) For \\(c\\) satisfying \\(\\alpha=1-G_{\\ell_{b}}(c), \\mathbb{P}\\left[C>c \\mid \\mathbb{H}_{0}\\right] \\longrightarrow \\alpha .\\) The test “Reject \\(\\mathbb{H}_{0}\\) if \\(C>c\\)” has asymptotic size \\(\\alpha\\).\nThe proof of Theorem \\(13.15\\) is presented in Section 13.28.\nIn Stata the command estat overid zb afer ivregress gmm can be used to implement a subset overidentification test where \\(\\mathrm{zb}\\) is the name(s) of the instruments(s) tested for validity. The statistic \\(C\\) and its asymptotic \\(p\\)-value using the \\(\\chi_{\\ell_{2}}^{2}\\) distribution are reported."
  },
  {
    "objectID": "chpt13-gmm.html#endogeneity-test",
    "href": "chpt13-gmm.html#endogeneity-test",
    "title": "13  Generalized Method of Moments",
    "section": "13.23 Endogeneity Test",
    "text": "13.23 Endogeneity Test\nIn Section \\(12.29\\) we introduced tests for endogeneity in the context of 2SLS estimation. Endogeneity tests are simple to implement in the GMM framework as a subset overidentification test. The model is \\(Y=Z_{1}^{\\prime} \\beta_{1}+Y_{2}^{\\prime} \\beta_{2}+e\\) where the maintained assumption is that the regressors \\(Z_{1}\\) and excluded instruments \\(Z_{2}\\) are exogenous so that \\(\\mathbb{E}\\left[Z_{1} e\\right]=0\\) and \\(\\mathbb{E}\\left[Z_{2} e\\right]=0\\). The question is whether or not \\(Y_{2}\\) is endogenous. The null hypothesis is \\(\\mathbb{M}_{0}: \\mathbb{E}\\left[Y_{2} e\\right]=0\\) with the alternative \\(\\mathbb{H}_{1}: \\mathbb{E}\\left[Y_{2} e\\right] \\neq 0\\).\nThe GMM test is constructed as follows. First, estimate the model by efficient GMM using \\(\\left(Z_{1}, Z_{2}\\right)\\) as instruments for \\(\\left(Z_{1}, Y_{2}\\right)\\). Let \\(\\widetilde{J}\\) denote the resulting GMM criterion. Second, estimate the model by efficient \\(\\mathrm{GMM}^{2}\\) using \\(\\left(Z_{1}, Z_{2}, Y_{2}\\right)\\) as instruments for \\(\\left(Z_{1}, Y_{2}\\right)\\). Let \\(\\widehat{J}\\) denote the resulting GMM criterion. The test statistic is the difference in the criterion functions: \\(C=\\widehat{J}-\\widetilde{J}\\).\nThe distribution theory for the test is a special case of overidentification testing.\nTheorem \\(13.16\\) Under Assumption \\(12.2\\) and \\(\\mathbb{E}\\left[Z_{2} Y_{2}^{\\prime}\\right]\\) has full rank \\(k_{2}\\), then as \\(n \\rightarrow \\infty, C \\underset{d}{\\rightarrow} \\chi_{k_{2}}^{2}\\). For \\(c\\) satisfying \\(\\alpha=1-G_{k_{2}}(c), \\mathbb{P}\\left[C>c \\mid \\mathbb{H}_{0}\\right] \\rightarrow \\alpha\\). The test “Reject \\(\\mathbb{H}_{0}\\) if \\(C>c\\)” has asymptotic size \\(\\alpha\\).\nIn Stata the command estat endogenous afer ivregress gmm can be used to implement the test for endogeneity. The statistic \\(C\\) and its asymptotic \\(p\\)-value using the \\(\\chi_{k_{2}}^{2}\\) distribution are reported."
  },
  {
    "objectID": "chpt13-gmm.html#subset-endogeneity-test",
    "href": "chpt13-gmm.html#subset-endogeneity-test",
    "title": "13  Generalized Method of Moments",
    "section": "13.24 Subset Endogeneity Test",
    "text": "13.24 Subset Endogeneity Test\nIn Section \\(12.30\\) we introduced subset endogeneity tests for 2SLS estimation. GMM tests are simple to implement as subset overidentification tests. The model is \\(Y=Z_{1}^{\\prime} \\beta_{1}+Y_{2}^{\\prime} \\beta_{2}+Y_{3}^{\\prime} \\beta_{3}+e\\) with \\(\\mathbb{E}[Z e]=0\\) where the instrument vector is \\(Z=\\left(Z_{1}, Z_{2}\\right)\\). The \\(k_{3} \\times 1\\) variables \\(Y_{3}\\) are treated as endogenous and the \\(k_{2} \\times 1\\) variables \\(Y_{2}\\) are treated as potentially endogenous. The hypothesis to test is that \\(Y_{2}\\) is exogenous, or \\(\\mathbb{M}_{0}: \\mathbb{E}\\left[Y_{2} e\\right]=0\\) against \\(\\mathbb{H}_{1}: \\mathbb{E}\\left[Y_{2} e\\right] \\neq 0\\). The test requires that \\(\\ell_{2} \\geq\\left(k_{2}+k_{3}\\right)\\) so that the model can be estimated under \\(\\mathbb{H}_{1}\\).\nThe GMM test is constructed as follows. First, estimate the model by efficient GMM using \\(\\left(Z_{1}, Z_{2}\\right.\\) ) as instruments for \\(\\left(Z_{1}, Y_{2}, Y_{3}\\right)\\). Let \\(\\widetilde{J}\\) denote the resulting GMM criterion. Second, estimate the model by efficient GMM using \\(\\left(Z_{1}, Z_{2}, Y_{2}\\right)\\) as instruments for \\(\\left(Z_{1}, Y_{2}, Y_{3}\\right)\\). Let \\(\\widehat{J}\\) denote the resulting GMM criterion. The test statistic is the difference in the criterion functions: \\(C=\\widehat{J}-\\widetilde{J}\\).\nThe distribution theory for the test is a special case of the theory of overidentification testing.\nTheorem \\(13.17\\) Under Assumption \\(12.2\\) and \\(\\mathbb{E}\\left[Z_{2}\\left(Y_{2}^{\\prime}, Y_{3}^{\\prime}\\right)\\right]\\) has full rank \\(k_{2}+k_{3}\\), then as \\(n \\rightarrow \\infty, C \\underset{d}{\\longrightarrow} \\chi_{k_{2}}^{2}\\). For \\(c\\) satisfying \\(\\alpha=1-G_{k_{2}}(c), \\mathbb{P}\\left[C>c \\mid \\mathbb{H}_{0}\\right] \\longrightarrow \\alpha\\). The test “Reject \\(\\mathbb{H}_{0}\\) if \\(C>c\\)” has asymptotic size \\(\\alpha\\).\nIn Stata, the command estat endogenous \\(\\mathrm{x} 2\\) afer ivregress gmm can be used to implement the test for endogeneity where \\(\\mathrm{x} 2\\) is the name(s) of the variable(s) tested for endogeneity. The statistic \\(C\\) and its asymptotic \\(\\mathrm{p}\\)-value using the \\(\\chi_{k_{2}}^{2}\\) distribution are reported.\n\\({ }^{2}\\) If the homoskedastic weight matrix is used this GMM estimator equals least squares, but when the weight matrix allows for heteroskedasticity the efficient GMM estimator does not equal least squares as the model is overidentified."
  },
  {
    "objectID": "chpt13-gmm.html#nonlinear-gmm",
    "href": "chpt13-gmm.html#nonlinear-gmm",
    "title": "13  Generalized Method of Moments",
    "section": "13.25 Nonlinear GMM",
    "text": "13.25 Nonlinear GMM\nGMM applies whenever an economic or statistical model implies the \\(\\ell \\times 1\\) moment condition\n\\[\n\\mathbb{E}\\left[g_{i}(\\beta)\\right]=0 .\n\\]\nwhere \\(g_{i}(\\beta)\\) is a possibly nonlinear function of the parameters \\(\\beta\\). Often, this is all that is known. Identification requires \\(\\ell \\geq k=\\operatorname{dim}(\\beta)\\). The GMM estimator minimizes\n\\[\nJ(\\beta)=n \\bar{g}_{n}(\\beta)^{\\prime} \\widehat{\\boldsymbol{W}} \\bar{g}_{n}(\\beta)\n\\]\nfor some weight matrix \\(\\widehat{\\boldsymbol{W}}\\) where\n\\[\n\\bar{g}_{n}(\\beta)=\\frac{1}{n} \\sum_{i=1}^{n} g_{i}(\\beta) .\n\\]\nThe efficient GMM estimator can be constructed by setting\n\\[\n\\widehat{\\boldsymbol{W}}=\\left(\\frac{1}{n} \\sum_{i=1}^{n} \\widehat{g}_{i} \\widehat{g}_{i}^{\\prime}-\\bar{g}_{n} \\bar{g}_{n}^{\\prime}\\right)^{-1},\n\\]\nwith \\(\\widehat{g}_{i}=g_{i}(\\widetilde{\\beta})\\) constructed using a preliminary consistent estimator \\(\\widetilde{\\beta}\\), perhaps obtained with \\(\\widehat{\\boldsymbol{W}}=\\boldsymbol{I}_{\\ell}\\). As in the case of the linear model the weight matrix can be iterated until convergence to obtain the iterated GMM estimator.\nProposition 13.1 Distribution of Nonlinear GMM Estimator Under general regularity conditions, \\(\\sqrt{n}\\left(\\widehat{\\beta}_{\\mathrm{gmm}}-\\beta\\right) \\underset{d}{\\longrightarrow} \\mathrm{N}\\left(0, \\boldsymbol{V}_{\\beta}\\right)\\) where\n\\[\n\\boldsymbol{V}_{\\beta}=\\left(\\boldsymbol{Q}^{\\prime} \\boldsymbol{W} \\boldsymbol{Q}\\right)^{-1}\\left(\\boldsymbol{Q}^{\\prime} \\boldsymbol{W} \\Omega \\boldsymbol{W} \\boldsymbol{Q}\\right)\\left(\\boldsymbol{Q}^{\\prime} \\boldsymbol{W} \\boldsymbol{Q}\\right)^{-1}\n\\]\nwith \\(\\Omega=\\mathbb{E}\\left[g_{i} g_{i}^{\\prime}\\right]\\) and\n\\[\n\\boldsymbol{Q}=\\mathbb{E}\\left[\\frac{\\partial}{\\partial \\beta^{\\prime}} g_{i}(\\beta)\\right]\n\\]\nIf the efficient weight matrix is used then \\(\\boldsymbol{V}_{\\beta}=\\left(\\boldsymbol{Q}^{\\prime} \\Omega^{-1} \\boldsymbol{Q}\\right)^{-1}\\).\nThe proof of this result is omitted as it uses more advanced techniques.\nThe asymptotic covariance matrices can be estimated by sample counterparts of the population matrices. For the case of a general weight matrix,\n\\[\n\\widehat{\\boldsymbol{V}}_{\\beta}=\\left(\\widehat{\\boldsymbol{Q}}^{\\prime} \\widehat{\\boldsymbol{W}} \\widehat{\\boldsymbol{Q}}\\right)^{-1}\\left(\\widehat{\\boldsymbol{Q}}^{\\prime} \\widehat{\\boldsymbol{W}} \\widehat{\\Omega} \\widehat{\\boldsymbol{W}} \\widehat{\\boldsymbol{Q}}\\right)\\left(\\widehat{\\boldsymbol{Q}}^{\\prime} \\widehat{\\boldsymbol{W}} \\widehat{\\boldsymbol{Q}}\\right)^{-1}\n\\]\nwhere\n\\[\n\\begin{gathered}\n\\widehat{\\Omega}=\\frac{1}{n} \\sum_{i=1}^{n}\\left(g_{i}(\\widehat{\\beta})-\\bar{g}\\right)\\left(g_{i}(\\widehat{\\beta})-\\bar{g}\\right)^{\\prime} \\\\\n\\bar{g}=n^{-1} \\sum_{i=1}^{n} g_{i}(\\widehat{\\beta})\n\\end{gathered}\n\\]\nand\n\\[\n\\widehat{\\boldsymbol{Q}}=\\frac{1}{n} \\sum_{i=1}^{n} \\frac{\\partial}{\\partial \\beta^{\\prime}} g_{i}(\\widehat{\\beta}) .\n\\]\nFor the case of the iterated efficient weight matrix,\n\\[\n\\widehat{\\boldsymbol{V}}_{\\beta}=\\left(\\widehat{\\boldsymbol{Q}}^{\\prime} \\widehat{\\Omega}^{-1} \\widehat{\\boldsymbol{Q}}\\right)^{-1} .\n\\]\nAll of the methods discussed in this chapter - Wald tests, constrained estimation, distance tests, overidentification tests, endogeneity tests - apply similarly to the nonlinear GMM estimator."
  },
  {
    "objectID": "chpt13-gmm.html#bootstrap-for-gmm",
    "href": "chpt13-gmm.html#bootstrap-for-gmm",
    "title": "13  Generalized Method of Moments",
    "section": "13.26 Bootstrap for GMM",
    "text": "13.26 Bootstrap for GMM\nThe bootstrap for 2SLS (Section 12.23) can be used for GMM estimation. The standard bootstrap algorithm generates bootstrap samples by sampling the triplets \\(\\left(Y_{i}^{*}, X_{i}^{*}, Z_{i}^{*}\\right)\\) independently and with replacement from the original sample. The GMM estimator is applied to the bootstrap sample to obtain the bootstrap estimates \\(\\widehat{\\beta}_{\\mathrm{gmm}}^{*}\\). This is repeated \\(B\\) times to create a sample of \\(B\\) bootstrap draws. Given these draws, bootstrap confidence intervals, including percentile, \\(\\mathrm{BC}\\) percentile, \\(\\mathrm{BC}_{a}\\) and percentile-t, are calculated conventionally.\nFor variance and standard error estimation the same cautions apply as for 2SLS. It is difficult to know if the GMM estimator has a finite variance in a given application. It is best to avoid using the bootstrap to calculate standard errors. Instead, use the bootstrap for percentile and percentile-t confidence intervals.\nWhen the model is overidentified, as discussed for 2SLS, bootstrap GMM inference will not achieve an asymptotic refinement unless the bootstrap estimator is recentered to satisfy the orthogonality condition. We now describe the recentering recommended by Hall and Horowitz (1996).\nFor linear GMM wth weight matrix \\(\\boldsymbol{W}\\) the recentered GMM bootstrap estimator is\n\\[\n\\widehat{\\beta}_{\\mathrm{gmm}}^{* *}=\\left(\\boldsymbol{X}^{* \\prime} \\boldsymbol{Z}^{*} \\boldsymbol{W}^{*} \\boldsymbol{Z}^{* \\prime} \\boldsymbol{X}^{*}\\right)^{-1}\\left(\\boldsymbol{X}^{* \\prime} \\boldsymbol{Z}^{*} \\boldsymbol{W}^{*}\\left(\\boldsymbol{Z}^{* \\prime} \\boldsymbol{Y}^{*}-\\boldsymbol{Z}^{\\prime} \\widehat{\\boldsymbol{e}}\\right)\\right)\n\\]\nwhere \\(\\boldsymbol{W}^{*}\\) is the bootstrap version of \\(\\boldsymbol{W}\\) and \\(\\widehat{\\boldsymbol{e}}=\\boldsymbol{Y}-\\boldsymbol{X} \\widehat{\\beta}_{\\mathrm{gmm}}\\). For efficient GMM,\n\\[\n\\boldsymbol{W}^{*}=\\left(\\frac{1}{n} \\sum_{i=1}^{n} Z_{i}^{*} Z_{i}^{* \\prime}\\left(Y_{i}^{*}-X_{i}^{* \\prime} \\widetilde{\\beta}^{*}\\right)^{2}\\right)^{-1}\n\\]\nfor preliminary estimator \\(\\widetilde{\\beta}^{*}\\).\nFor nonlinear GMM (Section 13.25) the bootstrap criterion function is modified. The recentered bootstrap criterion is\n\\[\n\\begin{aligned}\n&J^{* *}(\\beta)=n\\left(\\bar{g}_{n}^{*}(\\beta)-\\bar{g}_{n}\\left(\\widehat{\\beta}_{\\mathrm{gmm}}\\right)\\right)^{\\prime} \\boldsymbol{W}^{*}\\left(\\bar{g}_{n}^{*}(\\beta)-\\bar{g}_{n}\\left(\\widehat{\\beta}_{\\mathrm{gmm}}\\right)\\right) \\\\\n&\\bar{g}_{n}^{*}(\\beta)=\\frac{1}{n} \\sum_{i=1}^{n} g_{i}^{*}(\\beta)\n\\end{aligned}\n\\]\nwhere \\(\\bar{g}_{n}\\left(\\widehat{\\beta}_{\\mathrm{gmm}}\\right)\\) is from the sample not from the bootstrap data. The bootstrap estimator is\n\\[\n\\widehat{\\beta}_{\\mathrm{gmm}}^{* *}=\\operatorname{argmin} J^{* *}(\\beta) .\n\\]\nThe bootstrap can be used to calculate the p-value of the GMM overidentification test. For the GMM estimator with an efficient weight matrix the standard overidentification test is the Hansen \\(J\\) statistic\n\\[\nJ=n \\bar{g}_{n}\\left(\\widehat{\\beta}_{\\mathrm{gmm}}\\right)^{\\prime} \\widehat{\\Omega}^{-1} \\bar{g}_{n}\\left(\\widehat{\\beta}_{\\mathrm{gmm}}\\right) .\n\\]\nThe recentered bootstrap analog is\n\\[\nJ^{* *}=n\\left(\\bar{g}_{n}^{*}\\left(\\widehat{\\beta}_{\\mathrm{gmm}}^{* *}\\right)-\\bar{g}_{n}\\left(\\widehat{\\beta}_{\\mathrm{gmm}}\\right)\\right)^{\\prime} \\widehat{\\Omega}^{*-1}\\left(\\bar{g}_{n}^{*}\\left(\\widehat{\\beta}_{\\mathrm{gmm}}^{* *}\\right)-\\bar{g}_{n}\\left(\\widehat{\\beta}_{\\mathrm{gmm}}\\right)\\right)\n\\]\nOn each bootstrap sample \\(J^{* *}(b)\\) is calculated and stored. The bootstrap p-value is\n\\[\np^{*}=\\frac{1}{B} \\sum_{b=1}^{B} \\mathbb{1}\\left\\{J^{* *}(b)>S\\right\\} .\n\\]\nThis bootstrap p-value is asymptotically valid since \\(J^{* *}\\) satisfies the overidentified moment conditions."
  },
  {
    "objectID": "chpt13-gmm.html#conditional-moment-equation-models",
    "href": "chpt13-gmm.html#conditional-moment-equation-models",
    "title": "13  Generalized Method of Moments",
    "section": "13.27 Conditional Moment Equation Models",
    "text": "13.27 Conditional Moment Equation Models\nIn many contexts, an economic model implies conditional moment restriction of the form\n\\[\n\\mathbb{E}\\left[e_{i}(\\beta) \\mid Z_{i}\\right]=0\n\\]\nwhere \\(e_{i}(\\beta)\\) is some \\(s \\times 1\\) function of the observation and the parameters. In many cases \\(s=1\\). It turns out that this conditional moment restriction is more powerful than the unconditional moment equation model discussed throughout this chapter.\nFor example, the linear model \\(Y=X^{\\prime} \\beta+e\\) with instruments \\(Z\\) falls into this class under the assumption \\(\\mathbb{E}[e \\mid Z]=0\\). In this case \\(e_{i}(\\beta)=Y_{i}-X_{i}^{\\prime} \\beta\\).\nIt is also helpful to realize that conventional regression models also fall into this class except that in this case \\(X=Z\\). For example, in linear regression \\(e_{i}(\\beta)=Y_{i}-X_{i}^{\\prime} \\beta\\), while in a nonlinear regression model \\(e_{i}(\\beta)=Y_{i}-m\\left(X_{i}, \\beta\\right)\\). In a joint model of the conditional expectation \\(\\mathbb{E}[Y \\mid X=x]=x^{\\prime} \\beta\\) and variance \\(\\operatorname{var}[Y \\mid X=x]=f(x)^{\\prime} \\gamma\\), then\n\\[\ne_{i}(\\beta, \\gamma)=\\left\\{\\begin{array}{c}\nY_{i}-X_{i}^{\\prime} \\beta \\\\\n\\left(Y_{i}-X_{i}^{\\prime} \\beta\\right)^{2}-f\\left(X_{i}\\right)^{\\prime} \\gamma\n\\end{array} .\\right.\n\\]\nHere \\(s=2\\).\nGiven a conditional moment restriction an unconditional moment restriction can always be constructed. That is for any \\(\\ell \\times 1\\) function \\(\\phi(Z, \\beta)\\) we can set \\(g_{i}(\\beta)=\\phi\\left(Z_{i}, \\beta\\right) e_{i}(\\beta)\\) which satisfies \\(\\mathbb{E}\\left[g_{i}(\\beta)\\right]=\\) 0 and hence defines an unconditional moment equation model. The obvious problem is that the class of functions \\(\\phi\\) is infinite. Which should be selected?\nThis is equivalent to the problem of selection of the best instruments. If \\(Z \\in \\mathbb{R}\\) is a valid instrument satisfying \\(\\mathbb{E}[e \\mid Z]=0\\), then \\(Z, Z^{2}, Z^{3}\\),…, etc., are all valid instruments. Which should be used?\nOne solution is to construct an infinite list of potent instruments and then use the first \\(\\ell\\). How is \\(\\ell\\) to be determined? This is an area of theory still under development. One study of this problem is Donald and Newey (2001).\nAnother approach is to construct the optimal instrument which minimizes the asymptotic variance. The form was uncovered by Chamberlain (1987). Take the case \\(s=1\\). Let\n\\[\nR_{i}=\\mathbb{E}\\left[\\frac{\\partial}{\\partial \\beta} e_{i}(\\beta) \\mid Z_{i}\\right]\n\\]\nand \\(\\sigma_{i}^{2}=\\mathbb{E}\\left[e_{i}(\\beta)^{2} \\mid Z_{i}\\right]\\). Then the optimal instrument is \\(A_{i}=-\\sigma_{i}^{-2} R_{i}\\). The optimal moment is \\(g_{i}(\\beta)=\\) \\(A_{i} e_{i}(\\beta)\\). Setting \\(g_{i}(\\beta)\\) to be this choice (which is \\(k \\times 1\\), so is just-identified) yields the GMM estimator with lowest asymptotic variance. In practice \\(A_{i}\\) is unknown, but its form helps us think about construction of good instruments. In the linear model \\(e_{i}(\\beta)=Y_{i}-X_{i}^{\\prime} \\beta\\) note that \\(R_{i}=-\\mathbb{E}\\left[X_{i} \\mid Z_{i}\\right]\\) and \\(\\sigma_{i}^{2}=\\mathbb{E}\\left[e_{i}^{2} \\mid Z_{i}\\right]\\). This means the optimal instrument is \\(A_{i}=\\sigma_{i}^{-2} \\mathbb{E}\\left[X_{i} \\mid Z_{i}\\right]\\). In the case of linear regression \\(X_{i}=Z_{i}\\) so \\(A_{i}=\\sigma_{i}^{-2} Z_{i}\\). Hence efficient GMM is equivalent to GLS!\nIn the case of endogenous variables note that the efficient instrument \\(A_{i}\\) involves the estimation of the conditional mean of \\(X\\) given \\(Z\\). In other words, to get the best instrument for \\(X\\) we need the best conditional mean model for \\(X\\) given \\(Z\\) not just an arbitrary linear projection. The efficient instrument is also inversely proportional to the conditional variance of \\(e\\). This is the same as the GLS estimator; namely that improved efficiency can be obtained if the observations are weighted inversely to the conditional variance of the errors."
  },
  {
    "objectID": "chpt13-gmm.html#technical-proofs",
    "href": "chpt13-gmm.html#technical-proofs",
    "title": "13  Generalized Method of Moments",
    "section": "13.28 Technical Proofs*",
    "text": "13.28 Technical Proofs*\nProof of Theorem 13.12 Set \\(\\widetilde{e}_{i}=Y_{i}-X_{i}^{\\prime} \\widehat{\\beta}_{\\mathrm{cgmm}}\\) and \\(\\widehat{e}_{i}=Y_{i}-X_{i}^{\\prime} \\widehat{\\beta}_{\\mathrm{gmm}}\\). By standard covariance matrix analysis \\(\\widehat{\\Omega} \\vec{p} \\Omega\\) and \\(\\widetilde{\\Omega} \\vec{p} \\Omega\\). Thus we can replace \\(\\widehat{\\Omega}\\) and \\(\\widetilde{\\Omega}\\) in the criteria without affecting the asymptotic distribution. In particular\n\\[\n\\begin{aligned}\n\\widetilde{J}\\left(\\widehat{\\beta}_{\\mathrm{cgmm}}\\right) &=\\frac{1}{n} \\widetilde{\\boldsymbol{e}}^{\\prime} \\boldsymbol{Z} \\widetilde{\\Omega}^{-1} \\boldsymbol{Z}^{\\prime} \\widetilde{\\boldsymbol{e}} \\\\\n&=\\frac{1}{n} \\widetilde{\\boldsymbol{e}}^{\\prime} \\boldsymbol{Z} \\widehat{\\Omega}^{-1} \\boldsymbol{Z}^{\\prime} \\widetilde{\\boldsymbol{e}}+o_{p}(1) .\n\\end{aligned}\n\\]\nNow observe that\n\\[\n\\boldsymbol{Z}^{\\prime} \\widetilde{\\boldsymbol{e}}=\\boldsymbol{Z}^{\\prime} \\widehat{\\boldsymbol{e}}-\\boldsymbol{Z}^{\\prime} \\boldsymbol{X}\\left(\\widehat{\\beta}_{\\mathrm{cgmm}}-\\widehat{\\beta}_{\\mathrm{gmm}}\\right) .\n\\]\nThus\n\\[\n\\begin{aligned}\n\\frac{1}{n} \\widetilde{\\boldsymbol{e}}^{\\prime} \\boldsymbol{Z} \\widehat{\\Omega}^{-1} \\boldsymbol{Z}^{\\prime} \\widetilde{\\boldsymbol{e}} &=\\frac{1}{n} \\widehat{\\boldsymbol{e}}^{\\prime} \\boldsymbol{Z} \\widehat{\\Omega}^{-1} \\boldsymbol{Z}^{\\prime} \\widehat{\\boldsymbol{e}}-\\frac{2}{n}\\left(\\widehat{\\beta}_{\\mathrm{cgmm}}-\\widehat{\\beta}_{\\mathrm{gmm}}\\right)^{\\prime} \\boldsymbol{X}^{\\prime} \\boldsymbol{Z} \\widehat{\\Omega}^{-1} \\boldsymbol{Z}^{\\prime} \\widehat{\\boldsymbol{e}} \\\\\n&+\\frac{1}{n}\\left(\\widehat{\\beta}_{\\mathrm{cgmm}}-\\widehat{\\beta}_{\\mathrm{gmm}}\\right)^{\\prime} \\boldsymbol{X}^{\\prime} \\boldsymbol{Z} \\widehat{\\Omega}^{-1} \\boldsymbol{Z}^{\\prime} \\boldsymbol{X}\\left(\\widehat{\\beta}_{\\mathrm{cgmm}}-\\widehat{\\beta}_{\\mathrm{gmm}}\\right) \\\\\n&=\\widehat{J}\\left(\\widehat{\\beta}_{\\mathrm{gmm}}\\right)+\\frac{1}{n}\\left(\\widehat{\\beta}_{\\mathrm{cgmm}}-\\widehat{\\beta}_{\\mathrm{gmm}}\\right)^{\\prime} \\boldsymbol{X}^{\\prime} \\boldsymbol{Z} \\widehat{\\Omega}^{-1} \\boldsymbol{Z}^{\\prime} \\boldsymbol{X}\\left(\\widehat{\\beta}_{\\mathrm{cgmm}}-\\widehat{\\beta}_{\\mathrm{gmm}}\\right)\n\\end{aligned}\n\\]\nwhere the second equality holds because \\(\\boldsymbol{X}^{\\prime} \\boldsymbol{Z} \\widehat{\\Omega}^{-1} Z^{\\prime} \\widehat{\\boldsymbol{e}}=0\\) is the first-order condition for \\(\\widehat{\\beta}_{\\mathrm{gmm}}\\). By (13.16) and Theorem 13.4, under \\(\\mathbb{M}_{0}\\)\n\\[\n\\begin{aligned}\n\\sqrt{n}\\left(\\widehat{\\beta}_{\\mathrm{cgmm}}-\\widehat{\\beta}_{\\mathrm{gmm}}\\right) &=-\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{Z} \\Omega^{-1} \\boldsymbol{Z}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\boldsymbol{R}\\left(\\boldsymbol{R}^{\\prime}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{Z} \\Omega^{-1} \\boldsymbol{Z}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\boldsymbol{R}\\right)^{-1} \\boldsymbol{R}^{\\prime} \\sqrt{n}\\left(\\widehat{\\beta}_{\\mathrm{gmm}}-\\beta\\right)+o_{p}(1) \\\\\n& \\underset{d}{\\longrightarrow}\\left(\\boldsymbol{Q}^{\\prime} \\Omega^{-1} \\boldsymbol{Q}\\right)^{-1} \\boldsymbol{R} Z\n\\end{aligned}\n\\]\nwhere\n\\[\n\\begin{aligned}\nZ & \\sim \\mathrm{N}\\left(0, \\boldsymbol{V}_{\\boldsymbol{R}}\\right) \\\\\n\\boldsymbol{V}_{\\boldsymbol{R}} &=\\left(\\boldsymbol{R} \\boldsymbol{V}^{\\prime}\\left(\\boldsymbol{Q}^{\\prime} \\Omega^{-1} \\boldsymbol{Q}\\right)^{-1} \\boldsymbol{R}\\right)^{-1} .\n\\end{aligned}\n\\]\nPutting together (13.25), (13.26), (13.27) and (13.28),\n\\[\n\\begin{aligned}\nD &=\\widetilde{J}\\left(\\widehat{\\beta}_{\\mathrm{cgmm}}\\right)-\\widehat{J}\\left(\\widehat{\\beta}_{\\mathrm{gmm}}\\right) \\\\\n&=\\sqrt{n}\\left(\\widehat{\\beta}_{\\mathrm{cgmm}}-\\widehat{\\beta}_{\\mathrm{gmm}}\\right)^{\\prime} \\frac{1}{n} \\boldsymbol{X}^{\\prime} \\boldsymbol{Z} \\widehat{\\Omega}^{-1} \\frac{1}{n} \\boldsymbol{Z}^{\\prime} \\boldsymbol{X} \\sqrt{n}\\left(\\widehat{\\beta}_{\\mathrm{cgmm}}-\\widehat{\\beta}_{\\mathrm{gmm}}\\right) \\\\\n& \\underset{d}{\\longrightarrow} Z^{\\prime} \\boldsymbol{V}_{\\boldsymbol{R}}^{-1} Z \\sim \\chi_{q}^{2}\n\\end{aligned}\n\\]\nbecause \\(V_{R}>0\\) and \\(\\mathrm{Z}\\) is \\(q \\times 1\\).\nProof of Theorem 13.15 Let \\(\\widetilde{\\beta}\\) denote the GMM estimator obtained with the instrument set \\(Z_{a}\\) and let \\(\\widehat{\\beta}\\) denote the GMM estimator obtained with the instrument set \\(Z\\). Set \\(\\widetilde{e}_{i}=Y_{i}-X_{i}^{\\prime} \\widetilde{\\beta}, \\widehat{e}{ }_{i}=Y_{i}-X_{i}^{\\prime} \\widehat{\\beta}\\),\n\\[\n\\begin{aligned}\n&\\widetilde{\\Omega}=n^{-1} \\sum_{i=1}^{n} Z_{a i} Z_{a i}^{\\prime} \\widetilde{e}_{i}^{2} \\\\\n&\\widehat{\\Omega}=n^{-1} \\sum_{i=1}^{n} Z_{i} Z_{i}^{\\prime} \\widehat{e}_{i}^{2}\n\\end{aligned}\n\\]\nLet \\(\\boldsymbol{R}\\) be the \\(\\ell \\times \\ell_{a}\\) selector matrix so that \\(Z_{a}=\\boldsymbol{R}^{\\prime} Z\\). Note that\n\\[\n\\widetilde{\\Omega}=\\boldsymbol{R}^{\\prime} n^{-1} \\sum_{i=1}^{n} Z_{i} Z_{i}^{\\prime} \\widetilde{e}_{i}^{2} \\boldsymbol{R} .\n\\]\nBy standard covariance matrix analysis, \\(\\widehat{\\Omega} \\underset{p}{\\rightarrow} \\Omega\\) and \\(\\widetilde{\\Omega} \\underset{p}{\\rightarrow} \\boldsymbol{R}^{\\prime} \\Omega \\boldsymbol{R}\\). Also, \\(\\frac{1}{n} \\boldsymbol{Z}^{\\prime} \\boldsymbol{X} \\underset{p}{\\rightarrow} \\boldsymbol{Q}\\), say. By the CLT, \\(n^{-1 / 2} \\boldsymbol{Z}^{\\prime} \\boldsymbol{e} \\underset{d}{\\longrightarrow} Z\\) where \\(Z \\sim \\mathrm{N}(0, \\Omega)\\). Then\n\\[\n\\begin{aligned}\nn^{-1 / 2} \\boldsymbol{Z}^{\\prime} \\widehat{\\boldsymbol{e}} &=\\left(\\boldsymbol{I}_{\\ell}-\\left(\\frac{1}{n} \\boldsymbol{Z}^{\\prime} \\boldsymbol{X}\\right)\\left(\\frac{1}{n} \\boldsymbol{X}^{\\prime} \\boldsymbol{Z} \\widehat{\\Omega}^{-1} \\frac{1}{n} \\boldsymbol{Z}^{\\prime} \\boldsymbol{X}\\right)^{-1}\\left(\\frac{1}{n} \\boldsymbol{X}^{\\prime} \\boldsymbol{Z}\\right) \\widehat{\\Omega}^{-1}\\right) n^{-1 / 2} \\boldsymbol{Z}^{\\prime} \\boldsymbol{e} \\\\\n& \\rightarrow\\left(\\boldsymbol{I}_{\\ell}-\\boldsymbol{Q}\\left(\\boldsymbol{Q}^{\\prime} \\Omega^{-1} \\boldsymbol{Q}\\right)^{-1} \\boldsymbol{Q}^{\\prime} \\Omega^{-1}\\right) Z\n\\end{aligned}\n\\]\nand\n\\[\n\\begin{aligned}\nn^{-1 / 2} \\boldsymbol{Z}_{a}^{\\prime} \\widetilde{\\boldsymbol{e}} &=\\boldsymbol{R}^{\\prime}\\left(\\boldsymbol{I}_{\\ell}-\\left(\\frac{1}{n} \\boldsymbol{Z}^{\\prime} \\boldsymbol{X}\\right)\\left(\\frac{1}{n} \\boldsymbol{X}^{\\prime} \\boldsymbol{Z} \\boldsymbol{R}^{-1} \\boldsymbol{R}^{\\prime} \\frac{1}{n} \\boldsymbol{Z}^{\\prime} \\boldsymbol{X}\\right)^{-1}\\left(\\frac{1}{n} \\boldsymbol{X}^{\\prime} \\boldsymbol{Z}\\right) \\boldsymbol{R}^{-1} \\boldsymbol{R}^{\\prime}\\right) n^{-1 / 2} \\boldsymbol{Z}^{\\prime} \\boldsymbol{e} \\\\\n& \\underset{d}{\\longrightarrow} \\boldsymbol{R}^{\\prime}\\left(\\boldsymbol{I}_{\\ell}-\\boldsymbol{Q}\\left(\\boldsymbol{Q}^{\\prime} \\boldsymbol{R}\\left(\\boldsymbol{R}^{\\prime} \\Omega \\boldsymbol{R}\\right)^{-1} \\boldsymbol{R}^{\\prime} \\boldsymbol{Q}\\right)^{-1} \\boldsymbol{Q}^{\\prime} \\boldsymbol{R}\\left(\\boldsymbol{R}^{\\prime} \\Omega \\boldsymbol{R}\\right)^{-1} \\boldsymbol{R}^{\\prime}\\right) Z\n\\end{aligned}\n\\]\njointly.\nBy linear rotations of \\(Z\\) and \\(\\boldsymbol{R}\\) we can set \\(\\Omega=\\boldsymbol{I}_{\\ell}\\) to simplify the notation. Thus setting \\(\\boldsymbol{P}_{\\boldsymbol{Q}}=\\boldsymbol{Q}\\left(\\boldsymbol{Q}^{\\prime} \\boldsymbol{Q}\\right)^{-1} \\boldsymbol{Q}^{\\prime}\\), \\(\\boldsymbol{P}_{\\boldsymbol{R}}=\\boldsymbol{R}\\left(\\boldsymbol{R}^{\\prime} \\boldsymbol{R}\\right)^{-1} \\boldsymbol{R}^{\\prime}\\) and \\(Z \\sim \\mathrm{N}\\left(0, \\boldsymbol{I}_{\\ell}\\right)\\) we have\n\\[\n\\widehat{J} \\underset{d}{\\longrightarrow} Z^{\\prime}\\left(I_{\\ell}-\\boldsymbol{P}_{\\mathbf{Q}}\\right) Z\n\\]\nand\n\\[\n\\widetilde{J} \\underset{d}{\\rightarrow} Z^{\\prime}\\left(\\boldsymbol{P}_{\\boldsymbol{R}}-\\boldsymbol{P}_{\\boldsymbol{R}} \\boldsymbol{Q}\\left(\\boldsymbol{Q}^{\\prime} \\boldsymbol{P}_{R} \\boldsymbol{Q}\\right)^{-1} \\boldsymbol{Q}^{\\prime} \\boldsymbol{P}_{\\boldsymbol{R}}\\right) Z\n\\]\nIt follows that\n\\[\nC=\\widehat{J}-\\widetilde{J} \\underset{d}{\\longrightarrow} \\mathrm{Z}^{\\prime} A \\mathrm{Z}\n\\]\nwhere\n\\[\n\\boldsymbol{A}=\\left(\\boldsymbol{I}_{\\ell}-\\boldsymbol{P}_{Q}-\\boldsymbol{P}_{\\boldsymbol{R}}+\\boldsymbol{P}_{R} \\boldsymbol{Q}\\left(\\boldsymbol{Q}^{\\prime} \\boldsymbol{P}_{R} \\boldsymbol{Q}\\right)^{-1} \\boldsymbol{Q}^{\\prime} \\boldsymbol{P}_{R}\\right) .\n\\]\nThis is a quadratic form in a standard normal vector and the matrix \\(\\boldsymbol{A}\\) is idempotent (this is straightforward to check). \\(Z^{\\prime} A Z\\) is thus distributed \\(\\chi_{d}^{2}\\) with degrees of freedom \\(d\\) equal to\n\\[\n\\begin{aligned}\n\\operatorname{rank}(\\boldsymbol{A}) &=\\operatorname{tr}\\left(\\boldsymbol{I}_{\\ell}-\\boldsymbol{P}_{\\boldsymbol{Q}}-\\boldsymbol{P}_{\\boldsymbol{R}}+\\boldsymbol{P}_{\\boldsymbol{R}} \\boldsymbol{Q}\\left(\\boldsymbol{Q}^{\\prime} \\boldsymbol{P}_{\\boldsymbol{R}} \\boldsymbol{Q}\\right)^{-1} \\boldsymbol{Q}^{\\prime} \\boldsymbol{P}_{\\boldsymbol{R}}\\right) \\\\\n&=\\ell-k-\\ell_{a}+k=\\ell_{b}\n\\end{aligned}\n\\]\nThus the asymptotic distribution of \\(C\\) is \\(\\chi_{\\ell_{b}}^{2}\\) as claimed."
  },
  {
    "objectID": "chpt13-gmm.html#exercises",
    "href": "chpt13-gmm.html#exercises",
    "title": "13  Generalized Method of Moments",
    "section": "13.29 Exercises",
    "text": "13.29 Exercises\nExercise 13.1 Take the model\n\\[\n\\begin{aligned}\nY &=X^{\\prime} \\beta+e \\\\\n\\mathbb{E}[X e] &=0 \\\\\ne^{2} &=Z^{\\prime} \\gamma+\\eta \\\\\n\\mathbb{E}[Z \\eta] &=0 .\n\\end{aligned}\n\\]\nFind the method of moments estimators \\((\\widehat{\\beta}, \\widehat{\\gamma})\\) for \\((\\beta, \\gamma)\\)\nExercise 13.2 Take the model \\(Y=X^{\\prime} \\beta+e\\) with \\(\\mathbb{E}[e \\mid Z]=0\\). Let \\(\\widehat{\\beta}_{\\mathrm{gmm}}\\) be the GMM estimator using the weight matrix \\(\\boldsymbol{W}_{n}=\\left(\\boldsymbol{Z}^{\\prime} \\boldsymbol{Z}\\right)^{-1}\\). Under the assumption \\(\\mathbb{E}\\left[e^{2} \\mid Z\\right]=\\sigma^{2}\\) show that\n\\[\n\\sqrt{n}(\\widehat{\\beta}-\\beta) \\underset{d}{\\longrightarrow} \\mathrm{N}\\left(0, \\sigma^{2}\\left(\\boldsymbol{Q}^{\\prime} \\boldsymbol{M}^{-1} \\boldsymbol{Q}\\right)^{-1}\\right)\n\\]\nwhere \\(\\boldsymbol{Q}=\\mathbb{E}\\left[Z X^{\\prime}\\right]\\) and \\(\\boldsymbol{M}=\\mathbb{E}\\left[Z Z^{\\prime}\\right]\\)\nExercise 13.3 Take the model \\(Y=X^{\\prime} \\beta+e\\) with \\(\\mathbb{E}[Z e]=0\\). Let \\(\\widetilde{e}_{i}=Y_{i}-X_{i}^{\\prime} \\widetilde{\\beta}\\) where \\(\\widetilde{\\beta}\\) is consistent for \\(\\beta\\) (e.g. a GMM estimator with some weight matrix). An estimator of the optimal GMM weight matrix is\n\\[\n\\widehat{\\boldsymbol{W}}=\\left(\\frac{1}{n} \\sum_{i=1}^{n} Z_{i} Z_{i}^{\\prime} \\widetilde{e}_{i}^{2}\\right)^{-1} .\n\\]\nShow that \\(\\widehat{\\boldsymbol{W}} \\underset{p}{\\longrightarrow} \\Omega^{-1}\\) where \\(\\Omega=\\mathbb{E}\\left[Z Z^{\\prime} e^{2}\\right]\\).\nExercise \\(13.4\\) In the linear model estimated by GMM with general weight matrix \\(\\boldsymbol{W}\\) the asymptotic variance of \\(\\widehat{\\beta}_{\\mathrm{gmm}}\\) is\n\\[\n\\boldsymbol{V}=\\left(\\boldsymbol{Q}^{\\prime} \\boldsymbol{W} \\boldsymbol{Q}\\right)^{-1} \\boldsymbol{Q}^{\\prime} \\boldsymbol{W} \\Omega \\boldsymbol{W} \\boldsymbol{Q}\\left(\\boldsymbol{Q}^{\\prime} \\boldsymbol{W} \\boldsymbol{Q}\\right)^{-1}\n\\]\n\nLet \\(\\boldsymbol{V}_{0}\\) be this matrix when \\(\\boldsymbol{W}=\\Omega^{-1}\\). Show that \\(\\boldsymbol{V}_{0}=\\left(\\boldsymbol{Q}^{\\prime} \\Omega^{-1} \\boldsymbol{Q}\\right)^{-1}\\).\nWe want to show that for any \\(\\boldsymbol{W}, \\boldsymbol{V}-\\boldsymbol{V}_{0}\\) is positive semi-definite (for then \\(\\boldsymbol{V}_{0}\\) is the smaller possible covariance matrix and \\(W=\\Omega^{-1}\\) is the efficient weight matrix). To do this start by finding matrices \\(\\boldsymbol{A}\\) and \\(\\boldsymbol{B}\\) such that \\(\\boldsymbol{V}=\\boldsymbol{A}^{\\prime} \\Omega \\boldsymbol{A}\\) and \\(\\boldsymbol{V}_{0}=\\boldsymbol{B}^{\\prime} \\Omega \\boldsymbol{B}\\).\nShow that \\(\\boldsymbol{B}^{\\prime} \\Omega \\boldsymbol{A}=\\boldsymbol{B}^{\\prime} \\Omega \\boldsymbol{B}\\) and therefore that \\(\\boldsymbol{B}^{\\prime} \\Omega(\\boldsymbol{A}-\\boldsymbol{B})=0\\).\nUse the expressions \\(\\boldsymbol{V}=\\boldsymbol{A}^{\\prime} \\mathbf{\\Omega} \\boldsymbol{A}, \\boldsymbol{A}=\\boldsymbol{B}+(\\boldsymbol{A}-\\boldsymbol{B})\\), and \\(\\boldsymbol{B}^{\\prime} \\boldsymbol{\\Omega}(\\boldsymbol{A}-\\boldsymbol{B})=0\\) to show that \\(\\boldsymbol{V} \\geq \\boldsymbol{V}_{0}\\).\n\nExercise \\(13.5\\) Prove Theorem 13.8.\nExercise 13.6 Derive the constrained GMM estimator (13.16).\nExercise 13.7 Show that the constrained GMM estimator (13.16) with the efficient weight matrix is (13.19).\nExercise \\(13.8\\) Prove Theorem 13.9.\nExercise 13.9 Prove Theorem 13.10. Exercise \\(13.10\\) The equation of interest is \\(Y=m(X, \\beta)+e\\) with \\(\\mathbb{E}[Z e]=0\\) where \\(m(x, \\beta)\\) is a known function, \\(\\beta\\) is \\(k \\times 1\\) and \\(Z\\) is \\(\\ell \\times 1\\). Show how to construct an efficient GMM estimator for \\(\\beta\\).\nExercise 13.11 As a continuation of Exercise \\(12.7\\) derive the efficient GMM estimator using the instrument \\(Z=\\left(\\begin{array}{ll}X & X^{2}\\end{array}\\right)^{\\prime}\\). Does this differ from 2SLS and/or OLS?\nExercise 13.12 In the linear model \\(Y=X^{\\prime} \\beta+e\\) with \\(\\mathbb{E}[X e]=0\\) the GMM criterion function for \\(\\beta\\) is\n\\[\nJ(\\beta)=\\frac{1}{n}(\\boldsymbol{Y}-\\boldsymbol{X} \\beta)^{\\prime} \\boldsymbol{X} \\widehat{\\Omega}^{-1} \\boldsymbol{X}^{\\prime}(\\boldsymbol{Y}-\\boldsymbol{X} \\beta)\n\\]\nwhere \\(\\widehat{\\Omega}=n^{-1} \\sum_{i=1}^{n} X_{i} X_{i}^{\\prime} \\widehat{e}_{i}^{2}, \\widehat{e}_{i}=Y_{i}-X_{i}^{\\prime} \\widehat{\\beta}\\) are the OLS residuals, and \\(\\widehat{\\beta}=\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\boldsymbol{X}^{\\prime} \\boldsymbol{Y}\\) is least squares. The GMM estimator of \\(\\beta\\) subject to the restriction \\(r(\\beta)=0\\) is\n\\[\n\\widetilde{\\beta}=\\underset{r(\\beta)=0}{\\operatorname{argmin}} J_{n}(\\beta) .\n\\]\nThe GMM test statistic (the distance statistic) of the hypothesis \\(r(\\beta)=0\\) is\n\\[\nD=J(\\tilde{\\beta})=\\min _{r(\\beta)=0} J(\\beta) .\n\\]\n\nShow that you can rewrite \\(J(\\beta)\\) in (13.29) as\n\n\\[\nJ(\\beta)=n(\\beta-\\widehat{\\beta})^{\\prime} \\widehat{\\boldsymbol{V}}_{\\beta}^{-1}(\\beta-\\widehat{\\beta})\n\\]\nand thus \\(\\widetilde{\\beta}\\) is the same as the minimum distance estimator.\n\nShow that under linear hypotheses the distance statistic \\(D\\) in (13.30) equals the Wald statistic.\n\nExercise 13.13 Take the linear model \\(Y=X^{\\prime} \\beta+e\\) with \\(\\mathbb{E}[Z e]=0\\). Consider the GMM estimator \\(\\widehat{\\beta}\\) of \\(\\beta\\). Let \\(J=n \\bar{g}_{n}(\\widehat{\\beta})^{\\prime} \\widehat{\\Omega}^{-1} \\bar{g}_{n}(\\widehat{\\beta})\\) denote the test of overidentifying restrictions. Show that \\(J \\underset{d}{\\longrightarrow} \\chi_{\\ell-k}^{2}\\) as \\(n \\rightarrow \\infty\\) by demonstrating each of the following.\n\nSince \\(\\Omega>0\\), we can write \\(\\Omega^{-1}=\\boldsymbol{C} \\boldsymbol{C}^{\\prime}\\) and \\(\\Omega=\\boldsymbol{C}^{\\prime-1} \\boldsymbol{C}^{-1}\\) for some matrix \\(\\boldsymbol{C}\\).\n\\(J=n\\left(\\boldsymbol{C}^{\\prime} \\bar{g}_{n}(\\widehat{\\beta})\\right)^{\\prime}\\left(\\boldsymbol{C}^{\\prime} \\widehat{\\Omega} \\boldsymbol{C}\\right)^{-1} \\boldsymbol{C}^{\\prime} \\bar{g}_{n}(\\widehat{\\beta})\\).\n\\(\\boldsymbol{C}^{\\prime} \\bar{g}_{n}(\\widehat{\\beta})=\\boldsymbol{D}_{n} \\boldsymbol{C}^{\\prime} \\bar{g}_{n}(\\beta)\\) where \\(\\bar{g}_{n}(\\beta)=\\frac{1}{n} \\boldsymbol{Z}^{\\prime} \\boldsymbol{e}\\) and\n\n\\[\n\\boldsymbol{D}_{n}=\\boldsymbol{I}_{\\ell}-\\boldsymbol{C}^{\\prime}\\left(\\frac{1}{n} \\boldsymbol{Z}^{\\prime} \\boldsymbol{X}\\right)\\left(\\left(\\frac{1}{n} \\boldsymbol{X}^{\\prime} \\boldsymbol{Z}\\right) \\widehat{\\Omega}^{-1}\\left(\\frac{1}{n} \\boldsymbol{Z}^{\\prime} \\boldsymbol{X}\\right)\\right)^{-1}\\left(\\frac{1}{n} \\boldsymbol{X}^{\\prime} \\boldsymbol{Z}\\right) \\widehat{\\Omega}^{-1} \\boldsymbol{C}^{\\prime-1}\n\\]\n\n\\(\\boldsymbol{D}_{n} \\underset{p}{\\longrightarrow} \\boldsymbol{I}_{\\ell}-\\boldsymbol{R}\\left(\\boldsymbol{R}^{\\prime} \\boldsymbol{R}\\right)^{-1} \\boldsymbol{R}^{\\prime}\\) where \\(\\boldsymbol{R}=\\boldsymbol{C}^{\\prime} \\mathbb{E}\\left[Z X^{\\prime}\\right]\\).\n\\(n^{1 / 2} \\boldsymbol{C}^{\\prime} \\bar{g}_{n}(\\beta) \\underset{d}{\\longrightarrow} u \\sim \\mathrm{N}\\left(0, \\boldsymbol{I}_{\\ell}\\right)\\).\n\\(J \\underset{d}{\\longrightarrow} u^{\\prime}\\left(I_{\\ell}-\\boldsymbol{R}\\left(\\boldsymbol{R}^{\\prime} \\boldsymbol{R}\\right)^{-1} \\boldsymbol{R}^{\\prime}\\right) u\\).\n\\(u^{\\prime}\\left(\\boldsymbol{I}_{\\ell}-\\boldsymbol{R}\\left(\\boldsymbol{R}^{\\prime} \\boldsymbol{R}\\right)^{-1} \\boldsymbol{R}^{\\prime}\\right) u \\sim \\chi_{\\ell-k}^{2}\\).\n\nHint: \\(\\boldsymbol{I}_{\\ell}-\\boldsymbol{R}\\left(\\boldsymbol{R}^{\\prime} \\boldsymbol{R}\\right)^{-1} \\boldsymbol{R}^{\\prime}\\) is a projection matrix. Exercise 13.14 Take the model \\(Y=X^{\\prime} \\beta+e\\) with \\(\\mathbb{E}[Z e]=0, Y \\in \\mathbb{R}, X \\in \\mathbb{R}^{k}, Z \\in \\mathbb{R}^{\\ell}, \\ell \\geq k\\). Consider the statistic\n\\[\n\\begin{aligned}\nJ(\\beta) &=n \\bar{m}_{n}(\\beta)^{\\prime} \\boldsymbol{W} \\bar{m}_{n}(\\beta) \\\\\n\\bar{m}_{n}(\\beta) &=\\frac{1}{n} \\sum_{i=1}^{n} Z_{i}\\left(Y_{i}-X_{i}^{\\prime} \\beta\\right)\n\\end{aligned}\n\\]\nfor some weight matrix \\(W>0\\).\n\nTake the hypothesis \\(\\mathbb{I}_{0}: \\beta=\\beta_{0}\\). Derive the asymptotic distribution of \\(J\\left(\\beta_{0}\\right)\\) under \\(\\mathbb{H}_{0}\\) as \\(n \\rightarrow \\infty\\).\nWhat choice for \\(W\\) yields a known asymptotic distribution in part (a)? (Be specific about degrees of freedom.)\nWrite down an appropriate estimator \\(\\widehat{\\boldsymbol{W}}\\) for \\(W\\) which takes advantage of \\(\\mathbb{M}_{0}\\). (You do not need to demonstrate consistency or unbiasedness.)\nDescribe an asymptotic test of \\(\\mathbb{H}_{0}\\) against \\(\\mathbb{M}_{1}: \\beta \\neq \\beta_{0}\\) based on this statistic.\nUse the result in part (d) to construct a confidence region for \\(\\beta\\). What can you say about the form of this region? For example, does the confidence region take the form of an ellipse, similar to conventional confidence regions?\n\nExercise 13.15 Consider the model \\(Y=X^{\\prime} \\beta+e\\) with \\(\\mathbb{E}[Z e]=0\\) and\n\\[\n\\boldsymbol{R}^{\\prime} \\beta=0\n\\]\nwith \\(Y \\in \\mathbb{R}, X \\in \\mathbb{R}^{k}, Z \\in \\mathbb{R}^{\\ell}, \\ell>k\\). The matrix \\(\\boldsymbol{R}\\) is \\(k \\times q\\) with \\(1 \\leq q<k\\). You have a random sample \\(\\left(Y_{i}, X_{i}, Z_{i}: i=1, \\ldots, n\\right)\\).\nFor simplicity, assume the efficient weight matrix \\(\\boldsymbol{W}=\\left(\\mathbb{E}\\left[Z Z^{\\prime} e^{2}\\right]\\right)^{-1}\\) is known.\n\nWrite out the GMM estimator \\(\\widehat{\\beta}\\) ignoring constraint (13.31).\nWrite out the GMM estimator \\(\\widetilde{\\beta}\\) adding the constraint (13.31).\nFind the asymptotic distribution of \\(\\sqrt{n}(\\widetilde{\\beta}-\\beta)\\) as \\(n \\rightarrow \\infty\\) under Assumption (13.31).\n\nExercise \\(13.16\\) The observed data is \\(\\left\\{Y_{i}, X_{i}, Z_{i}\\right\\} \\in \\mathbb{R} \\times \\mathbb{R}^{k} \\times \\mathbb{R}^{\\ell}, k>1\\) and \\(\\ell>k>1, i=1, \\ldots, n\\). The model is \\(Y=X^{\\prime} \\beta+e\\) with \\(\\mathbb{E}[Z e]=0\\).\n\nGiven a weight matrix \\(\\boldsymbol{W}>0\\) write down the GMM estimator \\(\\widehat{\\beta}\\) for \\(\\beta\\).\nSuppose the model is misspecified. Specifically, assume that for some \\(\\delta \\neq 0\\),\n\n\\[\n\\begin{aligned}\ne &=\\delta n^{-1 / 2}+u \\\\\n\\mathbb{E}[u \\mid Z] &=0\n\\end{aligned}\n\\]\nwith \\(\\mu_{Z}=\\mathbb{E}[Z] \\neq 0\\). Show that (13.32) implies that \\(\\mathbb{E}[Z e] \\neq 0\\).\n\nExpress \\(\\sqrt{n}(\\widehat{\\beta}-\\beta)\\) as a function of \\(\\boldsymbol{W}, n, \\delta\\), and the variables \\(\\left(X_{i}, Z_{i}, u_{i}\\right)\\).\nFind the asymptotic distribution of \\(\\sqrt{n}(\\widehat{\\beta}-\\beta)\\) under Assumption (13.32). Exercise \\(13.17\\) The model is \\(Y=Z \\beta+X \\gamma+e\\) with \\(\\mathbb{E}[e \\mid Z]=0, X \\in \\mathbb{R}\\) and \\(Z \\in \\mathbb{R}\\). \\(X\\) is potentially endogenous and \\(Z\\) is exogenous. Someone suggests estimating \\((\\beta, \\gamma)\\) by GMM using the pair \\(\\left(Z, Z^{2}\\right)\\) as instruments. Is this feasible? Under what conditions is this a valid estimator?\n\nExercise \\(13.18\\) The observations are i.i.d., \\(\\left(Y_{i}, X_{i}, Q_{i}: i=1, \\ldots, n\\right)\\), where \\(X\\) is \\(k \\times 1\\) and \\(Q\\) is \\(m \\times 1\\). The model is \\(Y=X^{\\prime} \\beta+e\\) with \\(\\mathbb{E}[X e]=0\\) and \\(\\mathbb{E}[Q e]=0\\). Find the efficient GMM estimator for \\(\\beta\\).\nExercise 13.19 You want to estimate \\(\\mu=\\mathbb{E}[Y]\\) under the assumption that \\(\\mathbb{E}[X]=0\\), where \\(Y\\) and \\(X\\) are scalar and observed from a random sample. Find an efficient GMM estimator for \\(\\mu\\).\nExercise 13.20 Consider the model \\(Y=X^{\\prime} \\beta+e\\) given \\(\\mathbb{E}[Z e]=0\\) and \\(\\boldsymbol{R}^{\\prime} \\beta=0\\). The dimensions are \\(X \\in R^{k}\\) and \\(Z \\in R^{\\ell}\\) with \\(\\ell>k\\). The matrix \\(\\boldsymbol{R}\\) is \\(k \\times q, 1 \\leq q<k\\). Derive an efficient GMM estimator for \\(\\beta\\).\nExercise 13.21 Take the linear equation \\(Y=X^{\\prime} \\beta+e\\) and consider the following estimators of \\(\\beta\\).\n\n\\(\\widehat{\\beta}\\) : 2SLS using the instruments \\(Z_{1}\\).\n\\(\\widetilde{\\beta}: 2\\) SLS using the instruments \\(Z_{2}\\).\n\\(\\bar{\\beta}\\) : GMM using the instruments \\(Z=\\left(Z_{1}, Z_{2}\\right)\\) and the weight matrix\n\n\\[\n\\boldsymbol{W}=\\left(\\begin{array}{cc}\n\\left(\\boldsymbol{Z}_{1}^{\\prime} \\boldsymbol{Z}_{1}\\right)^{-1} \\lambda & 0 \\\\\n0 & \\left(\\boldsymbol{Z}_{2}^{\\prime} \\boldsymbol{Z}_{2}\\right)^{-1}(1-\\lambda)\n\\end{array}\\right)\n\\]\nfor \\(\\lambda \\in(0,1)\\).\nFind an expression for \\(\\bar{\\beta}\\) which shows that it is a specific weighted average of \\(\\widehat{\\beta}\\) and \\(\\widetilde{\\beta}\\).\nExercise 13.22 Consider the just-identified model \\(Y=X_{1}^{\\prime} \\beta_{1}+X_{2}^{\\prime} \\beta_{2}+e\\) with \\(\\mathbb{E}[Z e]=0\\) where \\(X=\\left(X_{1}^{\\prime}\\right.\\) \\(\\left.X_{2}^{\\prime}\\right)^{\\prime} \\in \\mathbb{R}^{k}\\) and \\(Z \\in \\mathbb{R}^{k}\\). We want to test \\(\\mathbb{H}_{0}: \\beta_{1}=0\\). Three econometricians are called for advice.\n\nEconometrician 1 proposes testing \\(\\mathbb{M}_{0}\\) by a Wald statistic.\nEconometrician 2 suggests testing \\(\\mathbb{M}_{0}\\) by the GMM Distance Statistic.\nEconometrician 3 suggests testing \\(\\mathbb{M}_{0}\\) using the test of overidentifying restrictions.\n\nYou are asked to settle this dispute. Explain the advantages and/or disadvantages of the different procedures in this specific context.\nExercise 13.23 Take the model \\(Y=X^{\\prime} \\beta+e\\) with \\(\\mathbb{E}[X e]=0\\) and \\(\\beta=\\boldsymbol{Q} \\theta\\), where \\(\\beta\\) is \\(k \\times 1, \\boldsymbol{Q}\\) is \\(k \\times m\\) with \\(m<k, \\boldsymbol{Q}\\) is known, and \\(\\theta\\) is \\(m \\times 1\\). The observations \\(\\left(Y_{i}, X_{i}\\right)\\) are i.i.d. across \\(i=1, \\ldots, n\\).\nUnder these assumptions what is the efficient estimator of \\(\\theta\\) ?\nExercise 13.24 Take the model \\(Y=\\theta+e\\) with \\(\\mathbb{E}[X e]=0, Y \\in \\mathbb{R}, X \\in \\mathbb{R}^{k}\\) and \\(\\left(Y_{i}, X_{i}\\right)\\) a random sample.\n\nFind the efficient GMM estimator of \\(\\theta\\).\nIs this model over-identified or just-identified?\nFind the GMM test statistic for over-identification. Exercise 13.25 Take the model \\(Y=X^{\\prime} \\beta+e\\) with \\(\\mathbb{E}[X e]=0\\) where \\(X\\) contains an intercept so \\(\\mathbb{E}[e]=0\\). An enterprising econometrician notices that this implies the \\(n\\) moment conditions\n\n\\[\n\\mathbb{E}\\left[e_{i}\\right]=0, i=1, \\ldots, n .\n\\]\nGiven an \\(n \\times n\\) weight matrix \\(\\boldsymbol{W}\\), this implies a GMM criterion\n\\[\nJ(\\beta)=(\\boldsymbol{Y}-\\boldsymbol{X} \\beta)^{\\prime} \\boldsymbol{W}(\\boldsymbol{Y}-\\boldsymbol{X} \\beta) .\n\\]\n\nUnder i.i.d. sampling, show that the efficient weight matrix is \\(\\boldsymbol{W}=\\sigma^{-2} \\boldsymbol{I}_{n}\\) where \\(\\sigma^{2}=\\mathbb{E}\\left[e^{2}\\right]\\).\nUsing the weight matrix \\(\\boldsymbol{W}=\\sigma^{-2} \\boldsymbol{I}_{n}\\) find the GMM estimator \\(\\widehat{\\beta}\\) that minimizes \\(J(\\beta)\\).\nFind a simple expression for the minimized criteria \\(J(\\widehat{\\beta})\\).\nTheorem \\(13.14\\) says that criterion such as \\(J(\\widehat{\\beta})\\) are asymptotically \\(\\chi_{\\ell-k}^{2}\\) where \\(\\ell\\) is the number of moments. While the assumptions of Theorem \\(13.14\\) do not apply to this context, what is \\(\\ell\\) here? That is, which \\(\\chi^{2}\\) distribution is the asserted asymptotic distribution?\nDoes the answer in (d) make sense? Explain your reasoning.\n\nExercise 13.26 Take the model \\(Y=X^{\\prime} \\beta+e\\) with \\(\\mathbb{E}[e \\mid X]=0\\) and \\(\\mathbb{E}\\left[e^{2} \\mid X\\right]=\\sigma^{2}\\). An econometrician more enterprising than the one in previous question notices that this implies the \\(n k\\) moment conditions\n\\[\n\\mathbb{E}\\left[X_{i} e_{i}\\right]=0, i=1, \\ldots, n .\n\\]\nWe can write the moments using matrix notation as \\(\\mathbb{E}\\left[\\bar{X}^{\\prime}(\\boldsymbol{Y}-\\boldsymbol{X} \\beta)\\right]\\) where\n\\[\n\\overline{\\boldsymbol{X}}=\\left(\\begin{array}{cccc}\nX_{1}^{\\prime} & 0 & \\cdots & 0 \\\\\n0 & X_{2}^{\\prime} & & 0 \\\\\n\\vdots & \\vdots & & \\vdots \\\\\n0 & 0 & \\cdots & X_{n}^{\\prime}\n\\end{array}\\right) \\text {. }\n\\]\nGiven an \\(n k \\times n k\\) weight matrix \\(\\boldsymbol{W}\\) this implies a GMM criterion\n\\[\nJ(\\beta)=(\\boldsymbol{Y}-\\boldsymbol{X} \\beta)^{\\prime} \\overline{\\boldsymbol{X}} \\boldsymbol{W} \\overline{\\boldsymbol{X}}^{\\prime}(\\boldsymbol{Y}-\\boldsymbol{X} \\beta) .\n\\]\n\nCalculate \\(\\Omega=\\mathbb{E}\\left[\\overline{\\boldsymbol{X}}^{\\prime} \\boldsymbol{e} \\boldsymbol{e}^{\\prime} \\overline{\\boldsymbol{X}}\\right]\\).\nThe econometrician decides to set \\(\\boldsymbol{W}=\\Omega^{-}\\), the Moore-Penrose generalized inverse of \\(\\Omega\\). (See Section A.6.) Note: A useful fact is that for a vector \\(\\boldsymbol{a},\\left(\\boldsymbol{a} \\boldsymbol{a}^{\\prime}\\right)^{-}=\\boldsymbol{a} \\boldsymbol{a}^{\\prime}\\left(\\boldsymbol{a}^{\\prime} \\boldsymbol{a}\\right)^{-2}\\).\nFind the GMM estimator \\(\\widehat{\\beta}\\) that minimizes \\(J(\\beta)\\).\nFind a simple expression for the minimized criterion \\(J(\\widehat{\\beta})\\).\nComment on whether the \\(\\chi^{2}\\) approximation from Theorem \\(13.14\\) is appropriate for \\(J(\\widehat{\\beta})\\).\n\nExercise 13.27 Continuation of Exercise 12.22, based on the empirical work reported in Acemoglu, Johnson, and Robinson (2001).\n\nRe-estimate the model estimated in part (j) by efficient GMM. Use the 2SLS estimates as the firststep for the weight matrix and then calculate the GMM estimator using this weight matrix without further iteration. Report the estimates and standard errors. (b) Calculate and report the \\(J\\) statistic for overidentification.\nCompare the GMM and 2SLS estimates. Discuss your findings.\n\nExercise 13.28 Continuation of Exercise 12.24, which involved estimation of a wage equation by 2 SLS.\n\nRe-estimate the model in part (a) by efficient GMM. Do the results change meaningfully?\nRe-estimate the model in part (d) by efficient GMM. Do the results change meaningfully?\nReport the \\(J\\) statistic for overidentification."
  },
  {
    "objectID": "chpt14-time-series.html#introduction",
    "href": "chpt14-time-series.html#introduction",
    "title": "14  Time Series",
    "section": "14.1 Introduction",
    "text": "14.1 Introduction\nA time series \\(Y_{t} \\in \\mathbb{R}^{m}\\) is a process which is sequentially ordered over time. In this textbook we focus on discrete time series where \\(t\\) is an integer, though there is also a considerable literature on continuoustime processes. To denote the time period it is typical to use the subscript \\(t\\). The time series is univariate if \\(m=1\\) and multivariate if \\(m>1\\). This chapter is primarily focused on univariate time series models, though we describe the concepts for the multivariate case when the added generality does not add extra complication.\nMost economic time series are recorded at discrete intervals such as annual, quarterly, monthly, weekly, or daily. The number of observaed periods \\(s\\) per year is called the frequency. In most cases we will denote the observed sample by the periods \\(t=1, \\ldots, n\\).\nBecause of the sequential nature of time series we expect that observations close in calender time, e.g. \\(Y_{t}\\) and its lagged value \\(Y_{t-1}\\), will be dependent. This type of dependence structure requires a different distributional theory than for cross-sectional and clustered observations since we cannot divide the sample into independent groups. Many of the issues which distinguish time series from cross-section econometrics concern the modeling of these dependence relationships.\nThere are many excellent textbooks for time series analysis. The encyclopedic standard is Hamilton (1994). Others include Harvey (1990), Tong (1990), Brockwell and Davis (1991), Fan and Yao (2003), Lütkepohl (2005), Enders (2014), and Kilian and Lütkepohl (2017). For textbooks on the related subject of forecasting see Granger and Newbold (1986), Granger (1989), and Elliott and Timmermann (2016)."
  },
  {
    "objectID": "chpt14-time-series.html#examples",
    "href": "chpt14-time-series.html#examples",
    "title": "14  Time Series",
    "section": "14.2 Examples",
    "text": "14.2 Examples\nMany economic time series are macroeconomic variables. An excellent resource for U.S. macroeconomic data are the FRED-MD and FRED-QD databases which contain a wide set of monthly and quarterly variables, assembled and maintained by the St. Louis Federal Reserve Bank. See McCracken and Ng (2016, 2021). The datasets FRED-MD and FRED-QD for 1959-2017 are posted on the textbook website. FRED-MD has 129 variables over 708 months. FRED-QD has 248 variables over 236 quarters.\nWhen working with time series data one of the first tasks is to plot the series against time. In Figures 14.1-14.2 we plot eight example time series from FRED-QD and FRED-MD. As is conventional, the x-axis displays calendar dates (in this case years) and the y-axis displays the level of the series. The series plotted are: (1a) Real U.S. GDP ( \\(g d p c 1)\\); (1b) U.S.-Canada exchange rate (excausx); (1c) Interest rate on U.S. 10-year Treasury bond (gs10); (1d) Real crude oil price (oilpricex); (2a) U.S. unemployment rate (unrate); (2b) U.S. real non-durables consumption growth rate (growth rate of \\(p c n d x\\) ); (2c) U.S. CPI inflation rate\n\n\nU.S. Real GDP\n\n\n\nInterest Rate on 10-Year Treasury\n\n\n\nU.S.-Canada Exchange Rate\n\n\n\nReal Crude Oil Price\n\nFigure 14.1: GDP, Exchange Rate, Interest Rate, Oil Price\n(growth rate of cpiaucsl); (2d) S&P 500 return (growth rate of \\(s p 500\\) ). (1a) and (2b) are quarterly series, the rest are monthly.\nMany of the plots are smooth, meaning that the neighboring values (in calendar time) are similar to one another and hence are serially correlated. Some of the plots are non-smooth, meaning that the neighboring values are less similar and hence less correlated. At least one plot (real GDP) displays an upward trend.\n\n\nU.S. Unemployment Rate\n\n\n\nU.S. Inflation Rate\n\n\n\nConsumption Growth Rate\n\n\n\nS&P 500 Return\n\nFigure 14.2: Unemployment Rate, Consumption Growth Rate, Inflation Rate, and S&P 500 Return"
  },
  {
    "objectID": "chpt14-time-series.html#differences-and-growth-rates",
    "href": "chpt14-time-series.html#differences-and-growth-rates",
    "title": "14  Time Series",
    "section": "14.3 Differences and Growth Rates",
    "text": "14.3 Differences and Growth Rates\nIt is common to transform series by taking logarithms, differences, and/or growth rates. Three of the series in Figure \\(14.2\\) (consumption growth, inflation [growth rate of CPI index], and S&P 500 return) are displayed as growth rates. This may be done for a number of reasons. The most credible is that this is the suitable transformation for the desired analysis.\nMany aggregate series such as real GDP are transformed by taking natural logarithms. This flattens the apparent exponential growth and makes fluctuations proportionate.\nThe first difference of a series \\(Y_{t}\\) is\n\\[\n\\Delta Y_{t}=Y_{t}-Y_{t-1}\n\\]\nThe second difference is\n\\[\n\\Delta^{2} Y_{t}=\\Delta Y_{t}-\\Delta Y_{t-1} .\n\\]\nHigher-order differences can be defined similarly but are not used in practice. The annual, or year-onyear, change of a series \\(Y_{t}\\) with frequency \\(s\\) is\n\\[\n\\Delta_{s} Y_{t}=Y_{t}-Y_{t-s} .\n\\]\nThere are several methods to calculate growth rates. The one-period growth rate is the percentage change from period \\(t-1\\) to period \\(t\\) :\n\\[\nQ_{t}=100\\left(\\frac{\\Delta Y_{t}}{Y_{t-1}}\\right)=100\\left(\\frac{Y_{t}}{Y_{t-1}}-1\\right) .\n\\]\nThe multiplication by 100 is not essential but scales \\(Q_{t}\\) so that it is a percentage. This is the transformation used for the plots in Figures \\(14.2\\) (b)-(d). For quarterly data, \\(Q_{t}\\) is the quarterly growth rate. For monthly data, \\(Q_{t}\\) is the monthly growth rate.\nFor non-annual data the one-period growth rate (14.1) may be unappealing for interpretation. Consequently, statistical agencies commonly report “annualized” growth rates which is the annual growth which would occur if the one-period growth rate is compounded for a full year. For a series with frequency \\(s\\) the annualized growth rate is\n\\[\nA_{t}=100\\left(\\left(\\frac{Y_{t}}{Y_{t-1}}\\right)^{s}-1\\right) .\n\\]\nNotice that \\(A_{t}\\) is a nonlinear function of \\(Q_{t}\\).\nYear-on-year growth rates are\n\\[\nG_{t}=100\\left(\\frac{\\Delta_{s} Y_{t}}{Y_{t-s}}\\right)=100\\left(\\frac{Y_{t}}{Y_{t-s}}-1\\right) .\n\\]\nThese do not need annualization.\nGrowth rates are closely related to logarithmic transformations. For small growth rates, \\(Q_{t}, A_{t}\\) and \\(G_{t}\\) are approximately first differences in logarithms:\n\\[\n\\begin{aligned}\nQ_{t} & \\simeq 100 \\Delta \\log Y_{t} \\\\\nA_{t} & \\simeq s \\times 100 \\Delta \\log Y_{t} \\\\\nG_{t} & \\simeq 100 \\Delta_{s} \\log Y_{t} .\n\\end{aligned}\n\\]\nFor analysis using growth rates I recommend the one-period growth rates (14.1) or differenced logarithms rather than the annualized growth rates (14.2). While annualized growth rates are preferred for reporting, they are a highly nonlinear transformation which is unnatural for statistical analysis. Differenced logarithms are the most common choice and are recommended for models which combine log-levels and growth rates for then the models are linear in all variables."
  },
  {
    "objectID": "chpt14-time-series.html#stationarity",
    "href": "chpt14-time-series.html#stationarity",
    "title": "14  Time Series",
    "section": "14.4 Stationarity",
    "text": "14.4 Stationarity\nRecall that cross-sectional observations are conventionally treated as random draws from an underlying population. This is not an appropriate model for time series processes due to serial dependence. Instead, we treat the observed sample \\(\\left\\{Y_{1}, \\ldots, Y_{n}\\right\\}\\) as a realization of a dependent stochastic process. It is often useful to view \\(\\left\\{Y_{1}, \\ldots, Y_{n}\\right\\}\\) as a subset of an underlying doubly-infinite sequence \\(\\left\\{\\ldots, Y_{t-1}, Y_{t}, Y_{t+1}, \\ldots\\right\\}\\).\nA random vector \\(Y_{t}\\) can be characterized by its distribution. A set such as \\(\\left(Y_{t}, Y_{t+1}, \\ldots, Y_{t+\\ell}\\right)\\) can be characterized by its joint distribution. Important features of these distributions are their means, variances, and covariances. Since there is only one observed time series sample, in order to learn about these distributions there needs to be some sort of constancy. This may only hold after a suitable transformation such as growth rates (as discussed in the previous section).\nThe most commonly assumed form of constancy is stationarity. There are two definitions. The first is sufficient for construction of linear models.\nDefinition \\(14.1\\left\\{Y_{t}\\right\\}\\) is covariance or weakly stationary if the expectation \\(\\mu=\\) \\(\\mathbb{E}\\left[Y_{t}\\right]\\) and covariance matrix \\(\\Sigma=\\operatorname{var}\\left[Y_{t}\\right]=\\mathbb{E}\\left[\\left(Y_{t}-\\mu\\right)\\left(Y_{t}-\\mu\\right)^{\\prime}\\right]\\) are finite and are independent of \\(t\\), and the autocovariances\n\\[\n\\Gamma(k)=\\operatorname{cov}\\left(Y_{t}, Y_{t-k}\\right)=\\mathbb{E}\\left[\\left(Y_{t}-\\mu\\right)\\left(Y_{t-k}-\\mu\\right)^{\\prime}\\right]\n\\]\nare independent of \\(t\\) for all \\(k\\)\nIn the univariate case we typically write the variance as \\(\\sigma^{2}\\) and autocovariances as \\(\\gamma(k)\\).\nThe expectation \\(\\mu\\) and variance \\(\\Sigma\\) are features of the marginal distribution of \\(Y_{t}\\) (the distribution of \\(Y_{t}\\) at a specific time period \\(t\\) ). Their constancy as stated in the above definition means that these features of the distribution are stable over time.\nThe autocovariances \\(\\Gamma(k)\\) are features of the bivariate distributions of \\(\\left(Y_{t}, Y_{t-k}\\right)\\). Their constancy as stated in the definition means that the correlation patterns between adjacent \\(Y_{t}\\) are stable over time and only depend on the number of time periods \\(k\\) separating the variables. By symmetry we have \\(\\Gamma(-k)=\\) \\(\\Gamma(k)^{\\prime}\\). In the univariate case this simplifies to \\(\\gamma(-k)=\\gamma(k)\\). The autocovariances \\(\\Gamma(k)\\) are finite under the assumption that the covariance matrix \\(\\Sigma\\) is finite by the Cauchy-Schwarz inequality.\nThe autocovariances summarize the linear dependence between \\(Y_{t}\\) and its lags. A scale-free measure of linear dependence in the univariate case are the autocorrelations\n\\[\n\\rho(k)=\\operatorname{corr}\\left(Y_{t}, Y_{t-k}\\right)=\\frac{\\operatorname{cov}\\left(Y_{t}, Y_{t-k}\\right)}{\\sqrt{\\operatorname{var}\\left[Y_{t}\\right] \\operatorname{var}\\left[Y_{t-1}\\right]}}=\\frac{\\gamma(k)}{\\sigma^{2}}=\\frac{\\gamma(k)}{\\gamma(0)} .\n\\]\nNotice by symmetry that \\(\\rho(-k)=\\rho(k)\\).\nThe second definition of stationarity concerns the entire joint distribution.\nDefinition 14.2 \\(\\left\\{Y_{t}\\right\\}\\) is strictly stationary if the joint distribution of \\(\\left(Y_{t}, \\ldots, Y_{t+\\ell}\\right)\\) is independent of \\(t\\) for all \\(\\ell\\). This is the natural generalization of the cross-section definition of identical distributions. Strict stationarity implies that the (marginal) distribution of \\(Y_{t}\\) does not vary over time. It also implies that the bivariate distributions of \\(\\left(Y_{t}, Y_{t+1}\\right)\\) and multivariate distributions of \\(\\left(Y_{t}, \\ldots, Y_{t+\\ell}\\right)\\) are stable over time. Under the assumption of a bounded variance a strictly stationary process is covariance stationary \\({ }^{1}\\).\nFor formal statistical theory we generally require the stronger assumption of strict stationarity. Therefore if we label a process as “stationary” you should interpret it as meaning “strictly stationary”.\nThe core meaning of both weak and strict stationarity is the same - that the distribution of \\(Y_{t}\\) is stable over time. To understand the concept it may be useful to review the plots in Figures 14.1-14.2. Are these stationary processes? If so, we would expect that the expectation and variance to be stable over time. This seems unlikely to apply to the series in Figure 14.1, as in each case it is difficult to describe what is the “typical” value of the series. Stationarity may be appropriate for the series in Figure \\(14.2\\) as each oscillates with a fairly regular pattern. It is difficult, however, to know whether or not a given time series is stationary simply by examining a time series plot.\nA straightforward but essential relationship is that an i.i.d. process is strictly stationary.\nTheorem 14.1 If \\(Y_{t}\\) is i.i.d., then it strictly stationary.\nHere are some examples of strictly stationary scalar processes. In each, \\(e_{t}\\) is i.i.d. and \\(\\mathbb{E}\\left[e_{t}\\right]=0\\).\nExample 14.1 \\(Y_{t}=e_{t}+\\theta e_{t-1}\\).\nExample 14.2 \\(Y_{t}=Z\\) for some random variable \\(Z\\).\nExample 14.3 \\(Y_{t}=(-1)^{t} Z\\) for a random variable \\(Z\\) which is symmetrically distributed about 0 .\nHere are some examples of processes which are not stationary.\nExample 14.4 \\(Y_{t}=t\\).\nExample 14.5 \\(Y_{t}=(-1)^{t}\\).\nExample 14.6 \\(Y_{t}=\\cos (\\theta t)\\).\nExample 14.7 \\(Y_{t}=\\sqrt{t} e_{t}\\).\nExample 14.8 \\(Y_{t}=e_{t}+t^{-1 / 2} e_{t-1}\\).\nExample 14.9 \\(Y_{t}=Y_{t-1}+e_{t}\\) with \\(Y_{0}=0\\).\nFrom the examples we can see that stationarity means that the distribution is constant over time. It does not mean, however, that the process has some sort of limited dependence, nor that there is an absence of periodic patterns. These restrictions are associated with the concepts of ergodicity and mixing which we shall introduce in subsequent sections.\n\\({ }^{1}\\) More generally, the two classes are non-nested since strictly stationary infinite variance processes are not covariance stationary."
  },
  {
    "objectID": "chpt14-time-series.html#transformations-of-stationary-processes",
    "href": "chpt14-time-series.html#transformations-of-stationary-processes",
    "title": "14  Time Series",
    "section": "14.5 Transformations of Stationary Processes",
    "text": "14.5 Transformations of Stationary Processes\nOne of the important properties of strict stationarity is that it is preserved by transformation. That is, transformations of strictly stationary processes are also strictly stationary. This includes transformations which include the full history of \\(Y_{t}\\).\nTheorem 14.2 If \\(Y_{t}\\) is strictly stationary and \\(X_{t}=\\phi\\left(Y_{t}, Y_{t-1}, Y_{t-2}, \\ldots\\right) \\in \\mathbb{R}^{q}\\) is a random vector then \\(X_{t}\\) is strictly stationary.\nTheorem \\(14.2\\) is extremely useful both for the study of stochastic processes which are constructed from underlying errors and for the study of sample statistics such as linear regression estimators which are functions of sample averages of squares and cross-products of the original data.\nWe give the proof of Theorem \\(14.2\\) in Section 14.47."
  },
  {
    "objectID": "chpt14-time-series.html#convergent-series",
    "href": "chpt14-time-series.html#convergent-series",
    "title": "14  Time Series",
    "section": "14.6 Convergent Series",
    "text": "14.6 Convergent Series\nA transformation which includes the full past history is an infinite-order moving average. For scalar \\(Y\\) and coefficients \\(a_{j}\\) define the vector process\n\\[\nX_{t}=\\sum_{j=0}^{\\infty} a_{j} Y_{t-j} .\n\\]\nMany time-series models involve representations and transformations of the form (14.3).\nThe infinite series (14.3) exists if it is convergent, meaning that the sequence \\(\\sum_{j=0}^{N} a_{j} Y_{t-j}\\) has a finite limit as \\(N \\rightarrow \\infty\\). Since the inputs \\(Y_{t}\\) are random we define this as a probability limit.\nDefinition 14.3 The infinite series (14.3) converges almost surely if \\(\\sum_{j=0}^{N} a_{j} Y_{t-j}\\) has a finite limit as \\(N \\rightarrow \\infty\\) with probability one. In this case we describe \\(X_{t}\\) as convergent.\nTheorem 14.3 If \\(Y_{t}\\) is strictly stationary, \\(\\mathbb{E}|Y|<\\infty\\), and \\(\\sum_{j=0}^{\\infty}\\left|a_{j}\\right|<\\infty\\), then (14.3) converges almost surely. Furthermore, \\(X_{t}\\) is strictly stationary.\nThe proof of Theorem \\(14.3\\) is provided in Section \\(14.47\\)."
  },
  {
    "objectID": "chpt14-time-series.html#ergodicity",
    "href": "chpt14-time-series.html#ergodicity",
    "title": "14  Time Series",
    "section": "14.7 Ergodicity",
    "text": "14.7 Ergodicity\nStationarity alone is not sufficient for the weak law of large numbers as there are strictly stationary processes with no time series variation. As we described earlier, an example of a stationary process is \\(Y_{t}=Z\\) for some random variable \\(Z\\). This is random but constant over all time. An implication is that the sample mean of \\(Y_{t}=Z\\) will be inconsistent for the population expectation.\nWhat is a minimal assumption beyond stationarity so that the law of large numbers applies? This topic is called ergodicity. It is sufficiently important that it is treated as a separate area of study. We mention only a few highlights here. For a rigorous treatment see a standard textbook such as Walters (1982).\nA time series \\(Y_{t}\\) is ergodic if all invariant events are trivial, meaning that any event which is unaffected by time-shifts has probability either zero or one. This definition is rather abstract and difficult to grasp but fortunately it is not needed by most economists.\nA useful intuition is that if \\(Y_{t}\\) is ergodic then its sample paths will pass through all parts of the sample space never getting “stuck” in a subregion.\nWe will first describe the properties of ergodic series which are relevant for our needs and follow with the more rigorous technical definitions. For proofs of the results see Section 14.47.\nFirst, many standard time series processes can be shown to be ergodic. A useful starting point is the observation that an i.i.d. sequence is ergodic.\nTheorem 14.4 If \\(Y_{t} \\in \\mathbb{R}^{m}\\) is i.i.d. then it strictly stationary and ergodic.\nSecond, ergodicity, like stationarity, is preserved by transformation.\nTheorem 14.5 If \\(Y_{t} \\in \\mathbb{R}^{m}\\) is strictly stationary and ergodic and \\(X_{t}=\\) \\(\\phi\\left(Y_{t}, Y_{t-1}, Y_{t-2}, \\ldots\\right)\\) is a random vector, then \\(X_{t}\\) is strictly stationary and ergodic.\nAs an example, the infinite-order moving average transformation (14.3) is ergodic if the input is ergodic and the coefficients are absolutely convergent.\nTheorem 14.6 If \\(Y_{t}\\) is strictly stationary, ergodic, \\(\\mathbb{E}|Y|<\\infty\\), and \\(\\sum_{j=0}^{\\infty}\\left|a_{j}\\right|<\\infty\\) then \\(X_{t}=\\sum_{j=0}^{\\infty} a_{j} Y_{t-j}\\) is strictly stationary and ergodic.\nWe now present a useful property. It is that the Cesàro sum of the autocovariances limits to zero.\nTheorem 14.7 If \\(Y_{t} \\in \\mathbb{R}\\) is strictly stationary, ergodic, and \\(\\mathbb{E}\\left[Y^{2}\\right]<\\infty\\), then\n\\[\n\\lim _{n \\rightarrow \\infty} \\frac{1}{n} \\sum_{\\ell=1}^{n} \\operatorname{cov}\\left(Y_{t}, Y_{t+\\ell}\\right)=0 .\n\\]\nThe result (14.4) can be interpreted as that the autocovariances “on average” tend to zero. Some authors have mis-stated ergodicity as implying that the covariances tend to zero but this is not correct, as (14.4) allows, for example, the non-convergent sequence \\(\\operatorname{cov}\\left(Y_{t}, Y_{t+\\ell}\\right)=(-1)^{\\ell}\\). The reason why (14.4) is particularly useful is because it is sufficient for the WLLN as we discover later in Theorem 14.9.\nWe now give the formal definition of ergodicity for interested readers. As the concepts will not be used again most readers can safely skip this discussion.\nAs we stated above, by definition the series \\(Y_{t} \\in \\mathbb{R}^{m}\\) is ergodic if all invariant events are trivial. To understand this we introduce some technical definitions. First, we can write an event as \\(A=\\left\\{\\widetilde{Y}_{t} \\in G\\right\\}\\) where \\(\\widetilde{Y}_{t}=\\left(\\ldots, Y_{t-1}, Y_{t}, Y_{t+1}, \\ldots\\right)\\) is an infinite history and \\(G \\subset \\mathbb{R}^{m \\infty}\\). Second, the \\(\\ell^{t h}\\) time-shift of \\(\\widetilde{Y}_{t}\\) is defined as \\(\\widetilde{Y}_{t+\\ell}=\\left(\\ldots, Y_{t-1+\\ell}, Y_{t+\\ell}, Y_{t+1+\\ell}, \\ldots\\right)\\). Thus \\(\\widetilde{Y}_{t+\\ell}\\) replaces each observation in \\(\\widetilde{Y}_{t}\\) by its \\(\\ell^{t h}\\) shifted value \\(Y_{t+\\ell}\\). A time-shift of the event \\(A=\\left\\{\\widetilde{Y}_{t} \\in G\\right\\}\\) is \\(A_{\\ell}=\\left\\{\\widetilde{Y}_{t+\\ell} \\in G\\right\\}\\). Third, an event \\(A\\) is called invariant if it is unaffected by a time-shift, so that \\(A_{\\ell}=A\\). Thus replacing any history \\(\\widetilde{Y}_{t}\\) with its shifted history \\(\\widetilde{Y}_{t+\\ell}\\) doesn’t change the event. Invariant events are rather special. An example of an invariant event is \\(A=\\left\\{\\max _{-\\infty<t<\\infty} Y_{t} \\leq 0\\right\\}\\). Fourth, an event \\(A\\) is called trivial if either \\(\\mathbb{P}[A]=0\\) or \\(\\mathbb{P}[A]=1\\). You can think of trivial events as essentially non-random. Recall, by definition \\(Y_{t}\\) is ergodic if all invariant events are trivial. This means that any event which is unaffected by a time shift is trivial-is essentially non-random. For example, again consider the invariant event \\(A=\\left\\{\\max _{-\\infty<t<\\infty} Y_{t} \\leq 0\\right\\}\\). If \\(Y_{t}=Z \\sim \\mathrm{N}(0,1)\\) for all \\(t\\) then \\(\\mathbb{P}[A]=\\mathbb{P}[Z \\leq 0]=0.5\\). Since this does not equal 0 or 1 then \\(Y_{t}=Z\\) is not ergodic. However, if \\(Y_{t}\\) is i.i.d. \\(\\mathrm{N}(0,1)\\) then \\(\\mathbb{P}\\left[\\max _{-\\infty<t<\\infty} Y_{t} \\leq 0\\right]=0\\). This is a trivial event. For \\(Y_{t}\\) to be ergodic (it is in this case) all such invariant events must be trivial.\nAn important technical result is that ergodicity is equivalent to the following property.\nTheorem 14.8 A stationary series \\(Y_{t} \\in \\mathbb{R}^{m}\\) is ergodic iff for all events \\(A\\) and \\(B\\)\n\\[\n\\lim _{n \\rightarrow \\infty} \\frac{1}{n} \\sum_{\\ell=1}^{n} \\mathbb{P}\\left[A_{\\ell} \\cap B\\right]=\\mathbb{P}[A] \\mathbb{P}[B] .\n\\]\nThis result is rather deep so we do not prove it here. See Walters (1982), Corollary 1.14.2, or Davidson (1994), Theorem 14.7. The limit in (14.5) is the Cesàro sum of \\(\\mathbb{P}\\left[A_{\\ell} \\cap B\\right]\\). The Theorem of Cesàro Means (Theorem A.4 of Probability and Statistics for Economists) shows that a sufficient condition for (14.5) is that \\(\\mathbb{P}\\left[A_{\\ell} \\cap B\\right] \\rightarrow \\mathbb{P}[A] \\mathbb{P}[B]\\) which is known as mixing. Thus mixing implies ergodicity. Mixing, roughly, means that separated events are asymptotically independent. Ergodicity is weaker, only requiring that the events are asymptotically independent “on average”. We discuss mixing in Section 14.12."
  },
  {
    "objectID": "chpt14-time-series.html#ergodic-theorem",
    "href": "chpt14-time-series.html#ergodic-theorem",
    "title": "14  Time Series",
    "section": "14.8 Ergodic Theorem",
    "text": "14.8 Ergodic Theorem\nThe ergodic theorem is one of the most famous results in time series theory. There are actually several forms of the theorem, most of which concern almost sure convergence. For simplicity we state the theorem in terms of convergence in probability. Theorem 14.9 Ergodic Theorem.\nIf \\(Y_{t} \\in \\mathbb{R}^{m}\\) is strictly stationary, ergodic, and \\(\\mathbb{E}\\|Y\\|<\\infty\\), then as \\(n \\rightarrow \\infty\\),\n\\[\n\\mathbb{E}\\|\\bar{Y}-\\mu\\| \\longrightarrow 0\n\\]\nand\n\\[\n\\bar{Y} \\underset{p}{\\longrightarrow} \\mu\n\\]\nwhere \\(\\mu=\\mathbb{E}[Y]\\).\nThe ergodic theorem shows that ergodicity is sufficient for consistent estimation. The moment condition \\(\\mathbb{E}\\|Y\\|<\\infty\\) is the same as in the WLLN for i.i.d. observations.\nWe now provide a proof of the ergodic theorem for the scalar case under the additional assumption that \\(\\operatorname{var}[Y]=\\sigma^{2}<\\infty\\). A proof which relaxes this assumption is provided in Section 14.47.\nBy direct calculation\n\\[\n\\operatorname{var}[\\bar{Y}]=\\frac{1}{n^{2}} \\sum_{t=1}^{n} \\sum_{j=1}^{n} \\gamma(t-j)\n\\]\nwhere \\(\\gamma(\\ell)=\\operatorname{cov}\\left(Y_{t}, Y_{t+\\ell}\\right)\\). The double sum is over all elements of an \\(n \\times n\\) matrix whose \\(t j^{t h}\\) element is \\(\\gamma(t-j)\\). The diagonal elements are \\(\\gamma(0)=\\sigma^{2}\\), the first off-diagonal elements are \\(\\gamma(1)\\), the second offdiagonal elements are \\(\\gamma(2)\\) and so on. This means that there are precisely \\(n\\) diagonal elements equalling \\(\\sigma^{2}, 2(n-1)\\) equalling \\(\\gamma(1)\\), etc. Thus the above equals\n\\[\n\\begin{aligned}\n\\operatorname{var}[\\bar{Y}] &=\\frac{1}{n^{2}}\\left(n \\sigma^{2}+2(n-1) \\gamma(1)+2(n-2) \\gamma(2)+\\cdots+2 \\gamma(n-1)\\right) \\\\\n&=\\frac{\\sigma^{2}}{n}+\\frac{2}{n} \\sum_{\\ell=1}^{n}\\left(1-\\frac{\\ell}{n}\\right) \\gamma(\\ell) .\n\\end{aligned}\n\\]\nThis is a rather intruiging expression. It shows that the variance of the sample mean precisely equals \\(\\sigma^{2} / n\\) (which is the variance of the sample mean under i.i.d. sampling) plus a weighted Cesàro mean of the autocovariances. The latter is zero under i.i.d. sampling but is non-zero otherwise. Theorem \\(14.7\\) shows that the Cesàro mean of the autocovariances converges to zero. Let \\(w_{n \\ell}=2\\left(\\ell / n^{2}\\right)\\), which satisfy the conditions of the Toeplitz Lemma (Theorem A.5 of Probability and Statistics for Economists). Then\n\\[\n\\frac{2}{n} \\sum_{\\ell=1}^{n}\\left(1-\\frac{\\ell}{n}\\right) \\gamma(\\ell)=\\frac{2}{n^{2}} \\sum_{\\ell=1}^{n-1} \\sum_{j=1}^{\\ell} \\gamma(j)=\\sum_{\\ell=1}^{n-1} w_{n \\ell}\\left(\\frac{1}{\\ell} \\sum_{j=1}^{\\ell} \\gamma(j)\\right) \\longrightarrow 0\n\\]\nTogether, we have shown that (14.8) is \\(o(1)\\) under ergodicity. Hence \\(\\operatorname{var}[\\bar{Y}] \\rightarrow 0\\). Markov’s inequality establishes that \\(\\bar{Y} \\underset{p}{\\longrightarrow} \\mu\\)."
  },
  {
    "objectID": "chpt14-time-series.html#conditioning-on-information-sets",
    "href": "chpt14-time-series.html#conditioning-on-information-sets",
    "title": "14  Time Series",
    "section": "14.9 Conditioning on Information Sets",
    "text": "14.9 Conditioning on Information Sets\nIn the past few sections we have introduced the concept of the infinite histories. We now consider conditional expectations given infinite histories.\nFirst, some basics. Recall from probability theory that an outcome is an element of a sample space. An event is a set of outcomes. A probability law is a rule which assigns non-negative real numbers to events. When outcomes are infinite histories then events are collections of such histories and a probability law is a rule which assigns numbers to collections of infinite histories.\nNow we wish to define a conditional expectation given an infinite past history. Specifically, we wish to define\n\\[\n\\mathbb{E}_{t-1}\\left[Y_{t}\\right]=\\mathbb{E}\\left[Y_{t} \\mid Y_{t-1}, Y_{t-2}, \\ldots\\right] \\text {. }\n\\]\nthe expected value of \\(Y_{t}\\) given the history \\(\\widetilde{Y}_{t-1}=\\left(Y_{t-1}, Y_{t-2}, \\ldots\\right)\\) up to time \\(t\\). Intuitively, \\(\\mathbb{E}_{t-1}\\left[Y_{t}\\right]\\) is the mean of the conditional distribution, the latter reflecting the information in the history. Mathematically this cannot be defined using (2.6) as the latter requires a joint density for \\(\\left(Y_{t}, Y_{t-1}, Y_{t-2}, \\ldots\\right)\\) which does not make much sense. Instead, we can appeal to Theorem \\(2.13\\) which states that the conditional expectation (14.10) exists if \\(\\mathbb{E}\\left|Y_{t}\\right|<\\infty\\) and the probabilities \\(\\mathbb{P}\\left[\\widetilde{Y}_{t-1} \\in A\\right]\\) are defined. The latter events are discussed in the previous paragraph. Thus the conditional expectation is well defined.\nIn this textbook we have avoided measure-theoretic terminology to keep the presentation accessible, and because it is my belief that measure theory is more distracting than helpful. However, it is standard in the time series literature to follow the measure-theoretic convention of writing (14.10) as the conditional expectation given a \\(\\sigma\\)-field. So at the risk of being overly-technical we will follow this convention and write the expectation (14.10) as \\(\\mathbb{E}\\left[Y_{t} \\mid \\mathscr{F}_{t-1}\\right]\\) where \\(\\mathscr{F}_{t-1}=\\sigma\\left(\\widetilde{Y}_{t-1}\\right)\\) is the \\(\\sigma\\)-field generated by the history \\(\\widetilde{Y}_{t-1}\\). A \\(\\sigma\\)-field (also known as a \\(\\sigma\\)-algebra) is a collection of sets satisfying certain regularity conditions \\({ }^{2}\\). See Probability and Statistics for Economists, Section 1.14. The \\(\\sigma\\)-field generated by a random variable \\(Y\\) is the collection of measurable events involving \\(Y\\). Similarly, the \\(\\sigma\\)-field generated by an infinite history is the collection of measurable events involving this history. Intuitively, \\(\\mathscr{F}_{t-1}\\) contains all the information available in the history \\(\\widetilde{Y}_{t-1}\\). Consequently, economists typically call \\(\\mathscr{F}_{t-1}\\) an information set rather than a \\(\\sigma\\)-field. As I said, in this textbook we endeavor to avoid measure theoretic complications so will follow the economists’ label rather than the probabilists’, but use the latter’s notation as is conventional. To summarize, we will write \\(\\mathscr{F}_{t}=\\sigma\\left(Y_{t}, Y_{t-1}, \\ldots\\right)\\) to indicate the information set generated by an infinite history \\(\\left(Y_{t}, Y_{t-1}, \\ldots\\right)\\), and will write \\((14.10)\\) as \\(\\mathbb{E}\\left[Y_{t} \\mid \\mathscr{F}_{t-1}\\right]\\).\nWe now describe some properties about information sets \\(\\mathscr{F}_{t}\\).\nFirst, they are nested: \\(\\mathscr{F}_{t-1} \\subset \\mathscr{F}\\). This means that information accumulates over time. Information is not lost.\nSecond, it is important to be precise about which variables are contained in the information set. Some economists are sloppy and refer to “the information set at time \\(t\\)” without specifying which variables are in the information set. It is better to be specific. For example, the information sets \\(\\mathscr{F}_{1 t}=\\) \\(\\sigma\\left(Y_{t}, Y_{t-1}, \\ldots\\right)\\) and \\(\\mathscr{F}_{2 t}=\\sigma\\left(Y_{t}, X_{t}, Y_{t-1}, X_{t-1} \\ldots\\right)\\) are distinct even though they are both dated at time \\(t\\).\nThird, the conditional expectations (14.10) follow the law of iterated expectations and the conditioning theorem, thus\n\\[\n\\begin{aligned}\n\\mathbb{E}\\left[\\mathbb{E}\\left[Y_{t} \\mid \\mathscr{F}_{t-1}\\right] \\mid \\mathscr{F}_{t-2}\\right] &=\\mathbb{E}\\left[Y_{t} \\mid \\mathscr{F}_{t-2}\\right] \\\\\n\\mathbb{E}\\left[\\mathbb{E}\\left[Y_{t} \\mid \\mathscr{F}_{t-1}\\right]\\right] &=\\mathbb{E}\\left[Y_{t}\\right]\n\\end{aligned}\n\\]\nand\n\\[\n\\mathbb{E}\\left[Y_{t-1} Y_{t} \\mid \\mathscr{F}_{t-1}\\right]=Y_{t-1} \\mathbb{E}\\left[Y_{t} \\mid \\mathscr{F}_{t-1}\\right]\n\\]"
  },
  {
    "objectID": "chpt14-time-series.html#martingale-difference-sequences",
    "href": "chpt14-time-series.html#martingale-difference-sequences",
    "title": "14  Time Series",
    "section": "14.10 Martingale Difference Sequences",
    "text": "14.10 Martingale Difference Sequences\nAn important concept in economics is unforecastability, meaning that the conditional expectation is the unconditional expectation. This is similar to the properties of a regression error. An unforecastable process is called a martingale difference sequence (MDS).\n\\({ }^{2} \\mathrm{~A} \\sigma\\)-field contains the universal set, is closed under complementation, and closed under countable unions. A MDS \\(e_{t}\\) is defined with respect to a specific sequence of information sets \\(\\mathscr{F}_{t}\\). Most commonly the latter are the natural filtration \\(\\mathscr{F}_{t}=\\sigma\\left(e_{t}, e_{t-1}, \\ldots\\right)\\) (the past history of \\(\\left.e_{t}\\right)\\) but it could be a larger information set. The only requirement is that \\(e_{t}\\) is adapted to \\(\\mathscr{F}_{t}\\), meaning that \\(\\mathbb{E}\\left[e_{t} \\mid \\mathscr{F}_{t}\\right]=e_{t}\\).\nDefinition 14.4 The process \\(\\left(e_{t}, \\mathscr{F}_{t}\\right)\\) is a Martingale Difference Sequence (MDS) if \\(e_{t}\\) is adapted to \\(\\mathscr{F}_{t}\\), EE \\(\\left|e_{t}\\right|<\\infty\\), and \\(\\mathbb{E}\\left[e_{t} \\mid \\mathscr{F}_{t-1}\\right]=0\\).\nIn words, a MDS \\(e_{t}\\) is unforecastable in the mean. It is useful to notice that if we apply iterated expectations \\(\\mathbb{E}\\left[e_{t}\\right]=\\mathbb{E}\\left[\\mathbb{E}\\left[e_{t} \\mid \\mathscr{F}_{t-1}\\right]\\right]=0\\). Thus a MDS is mean zero.\nThe definition of a MDS requires the information sets \\(\\mathscr{F}_{t}\\) to contain the information in \\(e_{t}\\), but is broader in the sense that it can contain more information. When no explicit definition is given it is standard to assume that \\(\\mathscr{F}_{t}\\) is the natural filtration. However, it is best to explicitly specify the information sets so there is no confusion.\nThe term “martingale difference sequence” refers to the fact that the summed process \\(S_{t}=\\sum_{j=1}^{t} e_{j}\\) is a martingale and \\(e_{t}\\) is its first-difference. A martingale \\(S_{t}\\) is a process which has a finite mean and \\(\\mathbb{E}\\left[S_{t} \\mid \\mathscr{F}_{t-1}\\right]=S_{t-1}\\)\nIf \\(e_{t}\\) is i.i.d. and mean zero it is a MDS but the reverse is not the case. To see this, first suppose that \\(e_{t}\\) is i.i.d. and mean zero. It is then independent of \\(\\mathscr{F}_{t-1}=\\sigma\\left(e_{t-1}, e_{t-2}, \\ldots\\right)\\) so \\(\\mathbb{E}\\left[e_{t} \\mid \\mathscr{F}_{t-1}\\right]=\\mathbb{E}\\left[e_{t}\\right]=0\\). Thus an i.i.d. shock is a MDS as claimed.\nTo show that the reverse is not true let \\(u_{t}\\) be i.i.d. \\(\\mathrm{N}(0,1)\\) and set\n\\[\ne_{t}=u_{t} u_{t-1}\n\\]\nBy the conditioning theorem\n\\[\n\\mathbb{E}\\left[e_{t} \\mid \\mathscr{F}_{t-1}\\right]=u_{t-1} \\mathbb{E}\\left[u_{t} \\mid \\mathscr{F}_{t-1}\\right]=0\n\\]\nso \\(e_{t}\\) is a MDS. The process (14.11) is not, however, i.i.d. One way to see this is to calculate the first autocovariance of \\(e_{t}^{2}\\), which is\n\\[\n\\begin{aligned}\n\\operatorname{cov}\\left(e_{t}^{2}, e_{t-1}^{2}\\right) &=\\mathbb{E}\\left[e_{t}^{2} e_{t-1}^{2}\\right]-\\mathbb{E}\\left[e_{t}^{2}\\right] \\mathbb{E}\\left[e_{t-1}^{2}\\right] \\\\\n&=\\mathbb{E}\\left[u_{t}^{2}\\right] \\mathbb{E}\\left[u_{t-1}^{4}\\right] \\mathbb{E}\\left[u_{t-2}^{2}\\right]-1 \\\\\n&=2 \\neq 0 .\n\\end{aligned}\n\\]\nSince the covariance is non-zero, \\(e_{t}\\) is not an independent sequence. Thus \\(e_{t}\\) is a MDS but not i.i.d.\nAn important property of a square integrable MDS is that it is serially uncorrelated. To see this, observe that by iterated expectations, the conditioning theorem, and the definition of a MDS, for \\(k>0\\),\n\\[\n\\begin{aligned}\n\\operatorname{cov}\\left(e_{t}, e_{t-k}\\right) &=\\mathbb{E}\\left[e_{t} e_{t-k}\\right] \\\\\n&=\\mathbb{E}\\left[\\mathbb{E}\\left[e_{t} e_{t-k} \\mid \\mathscr{F}_{t-1}\\right]\\right] \\\\\n&=\\mathbb{E}\\left[\\mathbb{E}\\left[e_{t} \\mid \\mathscr{F}_{t-1}\\right] e_{t-k}\\right] \\\\\n&=\\mathbb{E}\\left[0 e_{t-k}\\right] \\\\\n&=0 .\n\\end{aligned}\n\\]\nThus the autocovariances and autocorrelations are zero. A process that is serially uncorrelated, however, is not necessarily a MDS. Take the process \\(e_{t}=u_{t}+\\) \\(u_{t-1} u_{t-2}\\) with \\(u_{t}\\) i.i.d. \\(\\mathrm{N}(0,1)\\). The process \\(e_{t}\\) is not a MDS because \\(\\mathbb{E}\\left[e_{t} \\mid \\mathscr{F}_{t-1}\\right]=u_{t-1} u_{t-2} \\neq 0\\). However,\n\\[\n\\begin{aligned}\n\\operatorname{cov}\\left(e_{t}, e_{t-1}\\right) &=\\mathbb{E}\\left[e_{t} e_{t-1}\\right] \\\\\n&=\\mathbb{E}\\left[\\left(u_{t}+u_{t-1} u_{t-2}\\right)\\left(u_{t-1}+u_{t-2} u_{t-3}\\right)\\right] \\\\\n&=\\mathbb{E}\\left[u_{t} u_{t-1}+u_{t} u_{t-2} u_{t-3}+u_{t-1}^{2} u_{t-2}+u_{t-1} u_{t-2}^{2} u_{t-3}\\right] \\\\\n&=\\mathbb{E}\\left[u_{t}\\right] \\mathbb{E}\\left[u_{t-1}\\right]+\\mathbb{E}\\left[u_{t}\\right] \\mathbb{E}\\left[u_{t-2}\\right] \\mathbb{E}\\left[u_{t-3}\\right] \\\\\n&+\\mathbb{E}\\left[u_{t-1}^{2}\\right] \\mathbb{E}\\left[u_{t-2}\\right]+\\mathbb{E}\\left[u_{t-1}\\right] \\mathbb{E}\\left[u_{t-2}^{2}\\right] \\mathbb{E}\\left[u_{t-3}\\right] \\\\\n&=0 .\n\\end{aligned}\n\\]\nSimilarly, \\(\\operatorname{cov}\\left(e_{t}, e_{t-k}\\right)=0\\) for \\(k \\neq 0\\). Thus \\(e_{t}\\) is serially uncorrelated. We have proved the following.\nTheorem 14.10 If \\(\\left(e_{t}, \\mathscr{F}_{t}\\right)\\) is a MDS and \\(\\mathbb{E}\\left[e_{t}^{2}\\right]<\\infty\\) then \\(e_{t}\\) is serially uncorrelated.\nAnother important special case is a homoskedastic martingale difference sequence.\nDefinition 14.5 The MDS \\(\\left(e_{t}, \\mathscr{F}_{t}\\right)\\) is a Homoskedastic Martingale Difference Sequence if \\(\\mathbb{E}\\left[e_{t}^{2} \\mid \\mathscr{F}_{t-1}\\right]=\\sigma^{2}\\).\nA homoskedastic MDS should more properly be called a conditionally homoskedastic MDS because the property concerns the conditional distribution rather than the unconditional. That is, any strictly stationary MDS satisfies a constant variance \\(\\mathbb{E}\\left[e_{t}^{2}\\right]\\) but only a homoskedastic MDS has a constant conditional variance \\(\\mathbb{E}\\left[e_{t}^{2} \\mid \\mathscr{F}_{t-1}\\right]\\)\nA homoskedatic MDS is analogous to a conditionally homoskedastic regression error. It is intermediate between a MDS and an i.i.d. sequence. Specifically, a square integrable and mean zero i.i.d. sequence is a homoskedastic MDS and the latter is a MDS.\nThe reverse is not the case. First, a MDS is not necessarily conditionally homoskedastic. Consider the example \\(e_{t}=u_{t} u_{t-1}\\) given previously which we showed is a MDS. It is not conditionally homoskedastic, however, because\n\\[\n\\mathbb{E}\\left[e_{t}^{2} \\mid \\mathscr{F}_{t-1}\\right]=u_{t-1}^{2} \\mathbb{E}\\left[u_{t}^{2} \\mid \\mathscr{F}_{t-1}\\right]=u_{t-1}^{2}\n\\]\nwhich is time-varying. Thus this MDS \\(e_{t}\\) is conditionally heteroskedastic. Second, a homoskedastic MDS is not necessarily i.i.d. Consider the following example. Set \\(e_{t}=\\sqrt{1-2 / \\eta_{t-1}} T_{t}\\), where \\(T_{t}\\) is distributed as student \\(t\\) with degree of freedom parameter \\(\\eta_{t-1}=2+e_{t-1}^{2}\\). This is scaled so that \\(\\mathbb{E}\\left[e_{t} \\mid \\mathscr{F}_{t-1}\\right]=0\\) and \\(\\mathbb{E}\\left[e_{t}^{2} \\mid \\mathscr{F}_{t-1}\\right]=1\\), and is thus a homoskedastic MDS. The conditional distribution of \\(e_{t}\\) depends on \\(e_{t-1}\\) through the degree of freedom parameter. Hence \\(e_{t}\\) is not an independent sequence.\nOne way to think about the difference between MDS and i.i.d. shocks is in terms of forecastability. An i.i.d. process is fully unforecastable in that no function of an i.i.d. process is forecastable. A MDS is unforecastable in the mean but other moments may be forecastable.\nAs we mentioned above, the definition of a MDS \\(e_{t}\\) allows for conditional heteroskedasticity, meaning that the conditional variance \\(\\sigma_{t}^{2}=\\mathbb{E}\\left[e_{t}^{2} \\mid \\mathscr{F}_{t-1}\\right]\\) may be time-varying. In financial econometrics there are many models for conditional heteroskedasticity, including autoregressive conditional heteroskedasticity (ARCH), generalized ARCH (GARCH), and stochastic volatility. A good reference for this class of models is Campbell, Lo, and MacKinlay (1997)."
  },
  {
    "objectID": "chpt14-time-series.html#clt-for-martingale-differences",
    "href": "chpt14-time-series.html#clt-for-martingale-differences",
    "title": "14  Time Series",
    "section": "14.11 CLT for Martingale Differences",
    "text": "14.11 CLT for Martingale Differences\nWe are interested in an asymptotic approximation for the distribution of the normalized sample mean\n\\[\nS_{n}=\\frac{1}{\\sqrt{n}} \\sum_{t=1}^{n} u_{t}\n\\]\nwhere \\(u_{t}\\) is mean zero with variance \\(\\mathbb{E}\\left[u_{t} u_{t}^{\\prime}\\right]=\\Sigma<\\infty\\). In this section we present a CLT for the case where \\(u_{t}\\) is a martingale difference sequence.\nTheorem 14.11 MDS CLT If \\(u_{t}\\) is a strictly stationary and ergodic martingale difference sequence and \\(\\mathbb{E}\\left[u_{t} u_{t}^{\\prime}\\right]=\\Sigma<\\infty\\), then as \\(n \\rightarrow \\infty\\),\n\\[\nS_{n}=\\frac{1}{\\sqrt{n}} \\sum_{t=1}^{n} u_{t} \\underset{d}{\\longrightarrow} \\mathrm{N}(0, \\Sigma) \\text {. }\n\\]\nThe conditions for Theorem \\(14.11\\) are similar to the Lindeberg-Lévy CLT. The only difference is that the i.i.d. assumption has been replaced by the assumption of a strictly stationarity and ergodic MDS.\nThe proof of Theorem \\(14.11\\) is technically advanced so we do not present the full details, but instead refer readers to Theorem \\(3.2\\) of Hall and Heyde (1980) or Theorem \\(25.3\\) of Davidson (1994) (which are more general than Theorem 14.11, not requiring strict stationarity). To illustrate the role of the MDS assumption we give a sketch of the proof in Section 14.47."
  },
  {
    "objectID": "chpt14-time-series.html#mixing",
    "href": "chpt14-time-series.html#mixing",
    "title": "14  Time Series",
    "section": "14.12 Mixing",
    "text": "14.12 Mixing\nFor many results, including a CLT for correlated (non-MDS) series, we need a stronger restriction on the dependence between observations than ergodicity.\nRecalling the property (14.5) of ergodic sequences we can measure the dependence between two events \\(A\\) and \\(B\\) by the discrepancy\n\\[\n\\alpha(A, B)=|\\mathbb{P}[A \\cap B]-\\mathbb{P}[A] \\mathbb{P}[B]| .\n\\]\nThis equals 0 when \\(A\\) and \\(B\\) are independent and is positive otherwise. In general, \\(\\alpha(A, B)\\) can be used to measure the degree of dependence between the events \\(A\\) and \\(B\\).\nNow consider the two information sets ( \\(\\sigma\\)-fields)\n\\[\n\\begin{aligned}\n\\mathscr{F}_{-\\infty}^{t} &=\\sigma\\left(\\ldots, Y_{t-1}, Y_{t}\\right) \\\\\n\\mathscr{F}_{t}^{\\infty} &=\\sigma\\left(Y_{t}, Y_{t+1}, \\ldots\\right) .\n\\end{aligned}\n\\]\nThe first is the history of the series up until period \\(t\\) and the second is the history of the series starting in period \\(t\\) and going forward. We then separate the information sets by \\(\\ell\\) periods, that is, take \\(\\mathscr{F}_{-\\infty}^{t-\\ell}\\) and \\(\\mathscr{F}_{t}^{\\infty}\\). We can measure the degree of dependence between the information sets by taking all events in each and then taking the largest discrepancy (14.13). This is\n\\[\n\\alpha(\\ell)=\\sup _{A \\in \\mathscr{F}_{-\\infty}^{t-\\ell}, B \\in \\mathscr{F}_{t}^{\\infty}} \\alpha(A, B) .\n\\]\nThe constants \\(\\alpha(\\ell)\\) are known as the strong mixing coefficients. We say that \\(Y_{t}\\) is strong mixing if \\(\\alpha(\\ell) \\rightarrow 0\\) as \\(\\ell \\rightarrow \\infty\\). This means that as the time separation increases between the information sets, the degree of dependence decreases, eventually reaching independence.\nFrom the Theorem of Cesàro Means (Theorem A.4 of Probability and Statistics for Economists), strong mixing implies (14.5) which is equivalent to ergodicity. Thus a mixing process is ergodic.\nAn intuition concerning mixing can be colorfully illustrated by the following example due to Halmos (1956). A martini is a drink consisting of a large portion of gin and a small part of vermouth. Suppose that you pour a serving of gin into a martini glass, pour a small amount of vermouth on top, and then stir the drink with a swizzle stick. If your stirring process is mixing, with each turn of the stick the vermouth will become more evenly distributed throughout the gin, and asymptotically (as the number of stirs tends to infinity) the vermouth and gin distributions will become independent \\({ }^{3}\\). If so, this is a mixing process.\nFor applications, mixing is often useful when we can characterize the rate at which the coefficients \\(\\alpha(\\ell)\\) decline to zero. There are two types of conditions which are seen in asymptotic theory: rates and summation. Rate conditions take the form \\(\\alpha(\\ell)=O\\left(\\ell^{-r}\\right)\\) or \\(\\alpha(\\ell)=o\\left(\\ell^{-r}\\right)\\). Summation conditions take the form \\(\\sum_{\\ell=0}^{\\infty} \\alpha(\\ell)^{r}<\\infty\\) or \\(\\sum_{\\ell=0}^{\\infty} \\ell^{s} \\alpha(\\ell)^{r}<\\infty\\).\nThere are alternative measures of dependence beyond (14.13) and many have been proposed. Strong mixing is one of the weakest (and thus embraces a wide set of time series processes) but is insufficiently strong for some applications. Another popular dependence measure is known as absolute regularity or \\(\\beta\\)-mixing. The \\(\\beta\\)-mixing coefficients are\n\\[\n\\beta(\\ell)=\\sup _{A \\in \\mathscr{F}_{t}^{\\infty}} \\mathbb{E}\\left|\\mathbb{P}\\left[A \\mid \\mathscr{F}_{-\\infty}^{t-\\ell}\\right]-\\mathbb{P}[A]\\right| .\n\\]\nAbsolute regularity is stronger than strong mixing in the sense that \\(\\beta(\\ell) \\rightarrow 0\\) implies \\(\\alpha(\\ell) \\rightarrow 0\\), and rate conditions for the \\(\\beta\\)-mixing coefficients imply the same rates for the strong mixing coefficients.\nOne reason why mixing is useful for applications is that it is preserved by transformations.\nTheorem 14.12 If \\(Y_{t}\\) has mixing coefficients \\(\\alpha_{Y}(\\ell)\\) and \\(X_{t}=\\) \\(\\phi\\left(Y_{t}, Y_{t-1}, Y_{t-2}, \\ldots, Y_{t-q}\\right)\\) then \\(X_{t}\\) has mixing coefficients \\(\\alpha_{X}(\\ell) \\leq \\alpha_{Y}(\\ell-q)\\) (for \\(\\ell \\geq q)\\). The coefficients \\(\\alpha_{X}(\\ell)\\) satisfy the same summation and rate conditions as \\(\\alpha_{Y}(\\ell)\\).\nA limitation of the above result is that it is confined to a finite number of lags unlike the transformation results for stationarity and ergodicity.\nMixing can be a useful tool because of the following inequalities.\n\\({ }^{3}\\) Of course, if you really make an asymptotic number of stirs you will never finish stirring and you won’t be able to enjoy the martini. Hence in practice it is advised to stop stirring before the number of stirs reaches infinity. Theorem 14.13 Let \\(\\mathscr{F}_{-\\infty}^{t}\\) and \\(\\mathscr{F}_{t}^{\\infty}\\) be constructed from the pair \\(\\left(X_{t}, Z_{t}\\right)\\).\n\nIf \\(\\left|X_{t}\\right| \\leq C_{1}\\) and \\(\\left|Z_{t}\\right| \\leq C_{2}\\) then\n\n\\[\n\\left|\\operatorname{cov}\\left(X_{t-\\ell}, Z_{t}\\right)\\right| \\leq 4 C_{1} C_{2} \\alpha(\\ell) .\n\\]\n 1. If \\(\\mathbb{E}\\left|X_{t}\\right|^{r}<\\infty\\) and \\(\\mathbb{E}\\left|Z_{t}\\right|^{q}<\\infty\\) for \\(1 / r+1 / q<1\\) then\n\\[\n\\left|\\operatorname{cov}\\left(X_{t-\\ell}, Z_{t}\\right)\\right| \\leq 8\\left(\\mathbb{E}\\left|X_{t}\\right|^{r}\\right)^{1 / r}\\left(\\mathbb{E}\\left|Z_{t}\\right|^{q}\\right)^{1 / q} \\alpha(\\ell)^{1-1 / r-1 / q} .\n\\]\n 1. If \\(\\mathbb{E}\\left[Z_{t}\\right]=0\\) and \\(\\mathbb{E}\\left|Z_{t}\\right|^{r}<\\infty\\) for \\(r \\geq 1\\) then\n\\[\n\\mathbb{E}\\left|\\mathbb{E}\\left[Z_{t} \\mid \\mathscr{F}_{-\\infty}^{t-\\ell}\\right]\\right| \\leq 6\\left(\\mathbb{E}\\left|Z_{t}\\right|^{r}\\right)^{1 / r} \\alpha(\\ell)^{1-1 / r} .\n\\]\nThe proof is given in Section 14.47. Our next result follows fairly directly from the definition of mixing.\nTheorem 14.14 If \\(Y_{t}\\) is i.i.d. then it is strong mixing and ergodic."
  },
  {
    "objectID": "chpt14-time-series.html#clt-for-correlated-observations",
    "href": "chpt14-time-series.html#clt-for-correlated-observations",
    "title": "14  Time Series",
    "section": "14.13 CLT for Correlated Observations",
    "text": "14.13 CLT for Correlated Observations\nIn this section we develop a CLT for the normalized mean \\(S_{n}\\) defined in (14.12) allowing the variables \\(u_{t}\\) to be serially correlated.\nIn (14.8) we found that in the scalar case\n\\[\n\\operatorname{var}\\left[S_{n}\\right]=\\sigma^{2}+2 \\sum_{\\ell=1}^{n}\\left(1-\\frac{\\ell}{n}\\right) \\gamma(\\ell)\n\\]\nwhere \\(\\sigma^{2}=\\operatorname{var}\\left[u_{t}\\right]\\) and \\(\\gamma(\\ell)=\\operatorname{cov}\\left(u_{t}, u_{t-\\ell}\\right)\\). Since \\(\\gamma(-\\ell)=\\gamma(\\ell)\\) this can be written as\n\\[\n\\operatorname{var}\\left[S_{n}\\right]=\\sum_{\\ell=-n}^{n}\\left(1-\\frac{|\\ell|}{n}\\right) \\gamma(\\ell) .\n\\]\nIn the vector case define the variance \\(\\Sigma=\\mathbb{E}\\left[u_{t} u_{t}^{\\prime}\\right]\\) and the matrix covariance \\(\\Gamma(\\ell)=\\mathbb{E}\\left[u_{t} u_{t-\\ell}^{\\prime}\\right]\\) which satisfies \\(\\Gamma(-\\ell)=\\Gamma(\\ell)^{\\prime}\\). We obtain by a calculation analogous to (14.14)\n\\[\n\\operatorname{var}\\left[S_{n}\\right]=\\Sigma+\\sum_{\\ell=1}^{n}\\left(1-\\frac{\\ell}{n}\\right)\\left(\\Gamma(\\ell)+\\Gamma(\\ell)^{\\prime}\\right)=\\sum_{\\ell=-n}^{n}\\left(1-\\frac{|\\ell|}{n}\\right) \\Gamma(\\ell) .\n\\]\nA necessary condition for \\(S_{n}\\) to converge to a normal distribution is that the variance (14.15) converges to a limit. Indeed, as \\(n \\rightarrow \\infty\\)\n\\[\n\\sum_{\\ell=1}^{n}\\left(1-\\frac{\\ell}{n}\\right) \\Gamma(\\ell)=\\frac{1}{n} \\sum_{\\ell=1}^{n-1} \\sum_{j=1}^{\\ell} \\Gamma(j) \\rightarrow \\sum_{\\ell=0}^{\\infty} \\Gamma(\\ell)\n\\]\nwhere the convergence holds by the Theorem of Cesàro Means if the limit in (14.16) is convergent. A necessary condition for this to hold is that the covariances \\(\\Gamma(\\ell)\\) decline to zero as \\(\\ell \\rightarrow \\infty\\). A sufficient condition is that the covariances are absolutely summable which can be verified using a mixing inequality. Using the triangle inequality (B.16) and Theorem 14.13.2, for any \\(r>2\\)\n\\[\n\\sum_{\\ell=0}^{\\infty}\\|\\Gamma(\\ell)\\| \\leq 8\\left(\\mathbb{E}\\left\\|u_{t}\\right\\|^{r}\\right)^{2 / r} \\sum_{\\ell=0}^{\\infty} \\alpha(\\ell)^{1-2 / r} .\n\\]\nThis implies that (14.15) converges if \\(\\mathbb{E}\\left\\|u_{t}\\right\\|^{r}<\\infty\\) and \\(\\sum_{\\ell=0}^{\\infty} \\alpha(\\ell)^{1-2 / r}<\\infty\\). We conclude that under these assumptions\n\\[\n\\operatorname{var}\\left[S_{n}\\right] \\rightarrow \\sum_{\\ell=-\\infty}^{\\infty} \\Gamma(\\ell) \\stackrel{\\text { def }}{=} \\Omega\n\\]\nThe matrix \\(\\Omega\\) plays a special role in the inference theory for tme series. It is often called the long-run variance of \\(u_{t}\\) as it is the variance of sample means in large samples.\nIt turns out that these conditions are sufficient for the CLT.\nTheorem 14.15 If \\(u_{t}\\) is strictly stationary with mixing coefficients \\(\\alpha(\\ell), \\mathbb{E}\\left[u_{t}\\right]=\\) 0 , for some \\(r>2\\), \\(\\mathbb{E}\\left\\|u_{t}\\right\\|^{r}<\\infty\\) and \\(\\sum_{\\ell=0}^{\\infty} \\alpha(\\ell)^{1-2 / r}<\\infty\\), then (14.17) is convergent and \\(S_{n}=n^{-1 / 2} \\sum_{t=1}^{n} u_{t} \\underset{d}{\\longrightarrow} \\mathrm{N}(0, \\Omega)\\)\nThe proof is in Section \\(14.47\\).\nThe theorem requires \\(r>2\\) finite moments which is stronger than the MDS CLT. This \\(r\\) does not need to be an integer, meaning that the theorem holds under slightly more than two finite moments. The summability condition on the mixing coefficients in Theorem \\(14.15\\) is considerably stronger than ergodicity. There is a trade-off involving the choice of \\(r\\). A larger \\(r\\) means more moments are required finite but a slower decay in the coefficients \\(\\alpha(\\ell)\\) is allowed. Smaller \\(r\\) is less restrictive regarding moments but requires a faster decay rate in the mixing coefficients."
  },
  {
    "objectID": "chpt14-time-series.html#linear-projection",
    "href": "chpt14-time-series.html#linear-projection",
    "title": "14  Time Series",
    "section": "14.14 Linear Projection",
    "text": "14.14 Linear Projection\nIn Chapter 2 we extensively studied the properties of linear projection models. In the context of stationary time series we can use similar tools. An important extension is to allow for projections onto infinite dimensional random vectors. For this analysis we assume that \\(Y_{t}\\) is covariance stationary.\nRecall that when \\((Y, X)\\) have a joint distribution with bounded variances the linear projection of \\(Y\\) onto \\(X\\) (the best linear predictor) is the minimizer of \\(S(\\beta)=\\mathbb{E}\\left[\\left(Y-\\beta^{\\prime} X\\right)^{2}\\right]\\) and has the solution\n\\[\n\\mathscr{P}[Y \\mid X]=X^{\\prime}\\left(\\mathbb{E}\\left[X X^{\\prime}\\right]\\right)^{-1} \\mathbb{E}[X Y] \\text {. }\n\\]\nThis projection is unique and has a unique projection error \\(e=Y-\\mathscr{P}[Y \\mid X]\\).\nThis idea extends to any Hilbert space including the infinite past history \\(\\widetilde{Y}_{t-1}=\\left(\\ldots, Y_{t-2}, Y_{t-1}\\right)\\). From the projection theorem for Hilbert spaces (see Theorem 2.3.1 of Brockwell and Davis (1991)) the projection \\(\\mathscr{P}_{t-1}\\left[Y_{t}\\right]=\\mathscr{P}\\left[Y_{t} \\mid \\tilde{Y}_{t-1}\\right]\\) of \\(Y_{t}\\) onto \\(\\widetilde{Y}_{t-1}\\) is unique and has a unique projection error\n\\[\ne_{t}=Y_{t}-\\mathscr{P}_{t-1}\\left[Y_{t}\\right] .\n\\]\nThe projection error is mean zero, has finite variance \\(\\sigma^{2}=\\mathbb{E}\\left[e_{t}^{2}\\right] \\leq \\mathbb{E}\\left[Y_{t}^{2}\\right]<\\infty\\), and is serially uncorrelated. By Theorem 14.2, if \\(Y_{t}\\) is strictly stationary then \\(\\mathscr{P}_{t-1}\\left[Y_{t}\\right]\\) and \\(e_{t}\\) are strictly stationary.\nThe property (14.18) implies that the projection errors are serially uncorrelated. We state these results formally.\nTheorem 14.16 If \\(Y_{t} \\in \\mathbb{R}\\) is covariance stationary it has the projection equation\n\\[\nY_{t}=\\mathscr{P}_{t-1}\\left[Y_{t}\\right]+e_{t} .\n\\]\nThe projection error \\(e_{t}\\) satisfies\n\\[\n\\begin{aligned}\n\\mathbb{E}\\left[e_{t}\\right] &=0 \\\\\n\\mathbb{E}\\left[e_{t-j} e_{t}\\right] &=0 \\quad j \\geq 1\n\\end{aligned}\n\\]\nand\n\\[\n\\sigma^{2}=\\mathbb{E}\\left[e_{t}^{2}\\right] \\leq \\mathbb{E}\\left[Y_{t}^{2}\\right]<\\infty .\n\\]\nIf \\(Y_{t}\\) is strictly stationary then \\(e_{t}\\) is strictly stationary."
  },
  {
    "objectID": "chpt14-time-series.html#white-noise",
    "href": "chpt14-time-series.html#white-noise",
    "title": "14  Time Series",
    "section": "14.15 White Noise",
    "text": "14.15 White Noise\nThe projection error \\(e_{t}\\) is mean zero, has a finite variance, and is serially uncorrelated. This describes what is known as a white noise process.\nDefinition 14.6 The process \\(e_{t}\\) is white noise if \\(\\mathbb{E}\\left[e_{t}\\right]=0, \\mathbb{E}\\left[e_{t}^{2}\\right]=\\sigma^{2}<\\infty\\), and \\(\\operatorname{cov}\\left(e_{t}, e_{t-k}\\right)=0\\) for \\(k \\neq 0\\).\nA MDS is white noise (Theorem 14.10) but the reverse is not true as shown by the example \\(e_{t}=\\) \\(u_{t}+u_{t-1} u_{t-2}\\) given in Section 14.10, which is white noise but not a MDS. Therefore, the following types of shocks are nested: i.i.d., MDS, and white noise, with i.i.d. being the most narrow class and white noise the broadest. It is helpful to observe that a white noise process can be conditionally heteroskedastic as the conditional variance is unrestricted."
  },
  {
    "objectID": "chpt14-time-series.html#the-wold-decomposition",
    "href": "chpt14-time-series.html#the-wold-decomposition",
    "title": "14  Time Series",
    "section": "14.16 The Wold Decomposition",
    "text": "14.16 The Wold Decomposition\nIn Section \\(14.14\\) we showed that a covariance stationary process has a white noise projection error. This result can be used to express the series as an infinite linear function of the projection errors. This is a famous result known as the Wold decomposition. Theorem 14.17 The Wold Decomposition If \\(Y_{t}\\) is covariance stationary and \\(\\sigma^{2}>0\\) where \\(\\sigma^{2}\\) is the projection error variance (14.19), then \\(Y_{t}\\) has the linear representation\n\\[\nY_{t}=\\mu_{t}+\\sum_{j=0}^{\\infty} b_{j} e_{t-j}\n\\]\nwhere \\(e_{t}\\) are the white noise projection errors (14.18), \\(b_{0}=1\\),\n\\[\n\\sum_{j=1}^{\\infty} b_{j}^{2}<\\infty,\n\\]\nand\n\\[\n\\mu_{t}=\\lim _{m \\rightarrow \\infty} \\mathscr{P}_{t-m}\\left[Y_{t}\\right]\n\\]\nThe Wold decomposition shows that \\(Y_{t}\\) can be written as a linear function of the white noise projection errors plus \\(\\mu_{t}\\). The infinite sum in (14.20) is also known as a linear process. The Wold decomposition is a foundational result for linear time series analysis. Since any covariance stationary process can be written in this format this justifies linear models as approximations.\nThe series \\(\\mu_{t}\\) is the projection of \\(Y_{t}\\) on the history from the infinite past. It is the part of \\(Y_{t}\\) which is perfectly predictable from its past values and is called the deterministic component. In most cases \\(\\mu_{t}=\\mu\\), the unconditional mean of \\(Y_{t}\\). However, it is possible for stationary processes to have more substantive deterministic components. An example is\n\\[\n\\mu_{t}=\\left\\{\\begin{array}{cc}\n(-1)^{t} & \\text { with probability } 1 / 2 \\\\\n(-1)^{t+1} & \\text { with probability } 1 / 2 .\n\\end{array}\\right.\n\\]\nThis series is strictly stationary, mean zero, and variance one. However, it is perfectly predictable given the previous history as it simply oscillates between \\(-1\\) and 1 .\nIn practical applied time series analysis, deterministic components are typically excluded by assumption. We call a stationary time series non-deterministic \\({ }^{4}\\) if \\(\\mu_{t}=\\mu\\), a constant. In this case the Wold decomposition has a simpler form.\nTheorem 14.18 If \\(Y_{t}\\) is covariance stationary and non-deterministic then \\(Y_{t}\\) has the linear representation\n\\[\nY_{t}=\\mu+\\sum_{j=0}^{\\infty} b_{j} e_{t-j},\n\\]\nwhere \\(b_{j}\\) satisfy (14.21) and \\(e_{t}\\) are the white noise projection errors (14.18).\nA limitation of the Wold decomposition is the restriction to linearity. While it shows that there is a valid linear approximation, it may be that a nonlinear model provides a better approximation.\nFor a proof of Theorem \\(14.17\\) see Section 14.47.\n\\({ }^{4}\\) Most authors define purely non-deterministic as the case \\(\\mu_{t}=0\\). We allow for a non-zero mean so to accomodate practical time series applications."
  },
  {
    "objectID": "chpt14-time-series.html#lag-operator",
    "href": "chpt14-time-series.html#lag-operator",
    "title": "14  Time Series",
    "section": "14.17 Lag Operator",
    "text": "14.17 Lag Operator\nAn algebraic construct which is useful for the analysis of time series models is the lag operator.\nDefinition 14.7 The lag operator L satisfies L \\(Y_{t}=Y_{t-1}\\).\nDefining \\(\\mathrm{L}^{2}=\\mathrm{LL}\\), we see that \\(\\mathrm{L}^{2} Y_{t}=\\mathrm{L} Y_{t-1}=Y_{t-2}\\). In general, \\(\\mathrm{L}^{k} Y_{t}=Y_{t-k}\\).\nUsing the lag operator the Wold decomposition can be written in the format\n\\[\n\\begin{aligned}\nY_{t} &=\\mu+b_{0} e_{t}+b_{1} \\mathrm{~L} e_{t}+b_{2} \\mathrm{~L}^{2} e_{t}+\\cdots \\\\\n&=\\mu+\\left(b_{0}+b_{1} \\mathrm{~L}+b_{2} \\mathrm{~L}^{2}+\\cdots\\right) e_{t} \\\\\n&=\\mu+b(\\mathrm{~L}) e_{t}\n\\end{aligned}\n\\]\nwhere \\(b(z)=b_{0}+b_{1} z+b_{2} z^{2}+\\cdots\\) is an infinite-order polynomial. The expression \\(Y_{t}=\\mu+b(\\mathrm{~L}) e_{t}\\) is compact way to write the Wold representation."
  },
  {
    "objectID": "chpt14-time-series.html#autoregressive-wold-representation",
    "href": "chpt14-time-series.html#autoregressive-wold-representation",
    "title": "14  Time Series",
    "section": "14.18 Autoregressive Wold Representation",
    "text": "14.18 Autoregressive Wold Representation\nFrom Theorem 14.16, \\(Y_{t}\\) satisfies a projection onto its infinite past. Theorem \\(14.18\\) shows that this projection equals a linear function of the lagged projection errors. An alternative is to write the projection as a linear function of the lagged \\(Y_{t}\\). It turns out that to obtain a unique and convergent representation we need a strengthening of the conditions.\nTheorem 14.19 If \\(Y_{t}\\) is covariance stationary, non-deterministic, with Wold representation \\(Y_{t}=b(\\mathrm{~L}) e_{t}\\), such that \\(|b(z)| \\geq \\delta>0\\) for all complex \\(|z| \\leq 1\\), and for some integer \\(s \\geq 0\\) the Wold coefficients satisfy \\(\\sum_{j=0}^{\\infty}\\left(\\sum_{k=0}^{\\infty} k^{s} b_{j+k}\\right)^{2}<\\infty\\), then \\(Y_{t}\\) has the representation\n\\[\nY_{t}=\\mu+\\sum_{j=1}^{\\infty} a_{j} Y_{t-j}+e_{t}\n\\]\nfor some coefficients \\(\\mu\\) and \\(a_{j}\\). The coefficients satisfy \\(\\sum_{k=0}^{\\infty} k^{s}\\left|a_{k}\\right|<\\infty\\) so (14.23) is convergent.\nEquation (14.23) is known as an infinite-order autoregressive representation with autoregressive coefficients \\(a_{j}\\).\nA solution to the equation \\(b(z)=0\\) is a root of the polynomial \\(b(z)\\). The assumption \\(|b(z)|>0\\) for \\(|z| \\leq 1\\) means that the roots of \\(b(z)\\) lie outside the unit circle \\(|z|=1\\) (the circle in the complex plane with radius one). Theorem \\(14.19\\) makes the stronger restriction that \\(|b(z)|\\) is bounded away from 0 for \\(z\\) on or within the unit circle. The need for this strengthening is less intuitive but essentially excludes the possibility of an infinite number of roots outside but arbitrarily close to the unit circle. The summability assumption on the Wold coefficients ensures convergence of the autoregressive coefficients \\(a_{j}\\). To understand the restriction on the roots of \\(b(z)\\) consider the simple case \\(b(z)=1-b_{1} z\\). (Below we call this a MA(1) model.) The requirement \\(|b(z)| \\geq \\delta\\) for \\(|z| \\leq 1\\) means \\({ }^{5}\\left|b_{1}\\right| \\leq 1-\\delta\\). Thus the assumption in Theorem \\(14.19\\) bounds the coefficient strictly below 1 . Now consider an infinite polynomial case \\(b(z)=\\prod_{j=1}^{\\infty}\\left(1-b_{j} z\\right)\\). The assumption in Theorem \\(14.19\\) requires \\(\\sup _{j}\\left|b_{j}\\right|<1\\).\nTheorem \\(14.19\\) is attributed to Wiener and Masani (1958). For a recent treatment and proof see Corollary 6.1.17 of Politis and McElroy (2020). These authors (as is common in the literature) state their assumptions differently than we do in Theorem 14.19. First, instead of the condition on \\(b(z)\\) they bound from below the spectral density function \\(f(\\lambda)\\) of \\(Y_{t}\\). We do not define the spectral density in this text so we restate their condition in terms of the linear process polynomial \\(b(z)\\). Second, instead of the condition on the Wold coefficients they require that the autocovariances satisfy \\(\\sum_{k=0}^{\\infty} k^{s}|\\gamma(k)|<\\infty\\). This is implied by our stated summability condition on the \\(b_{j}\\) (using the expression for \\(\\gamma(k)\\) in Section \\(14.21\\) below and simplifying)."
  },
  {
    "objectID": "chpt14-time-series.html#linear-models",
    "href": "chpt14-time-series.html#linear-models",
    "title": "14  Time Series",
    "section": "14.19 Linear Models",
    "text": "14.19 Linear Models\nIn the previous two sections we showed that any non-deterministic covariance stationary time series has the projection representation\n\\[\nY_{t}=\\mu+\\sum_{j=0}^{\\infty} b_{j} e_{t-j}\n\\]\nand under a restriction on the projection coefficients satisfies the autoregressive representation\n\\[\nY_{t}=\\mu+\\sum_{j=1}^{\\infty} a_{j} Y_{t-j}+e_{t} .\n\\]\nIn both equations the errors \\(e_{t}\\) are white noise projection errors. These representations help us understand that linear models can be used as approximations for stationary time series.\nFor the next several sections we reverse the analysis. We will assume a specific linear model and then study the properties of the resulting time series. In particular we will be seeking conditions under which the stated process is stationary. This helps us understand the properties of linear models. Throughout, we assume that the error \\(e_{t}\\) is a strictly stationary and ergodic white noise process. This allows as a special case the stronger assumption that \\(e_{t}\\) is i.i.d. but is less restrictive. In particular, it allows for conditional heteroskedasticity."
  },
  {
    "objectID": "chpt14-time-series.html#moving-average-processes",
    "href": "chpt14-time-series.html#moving-average-processes",
    "title": "14  Time Series",
    "section": "14.20 Moving Average Processes",
    "text": "14.20 Moving Average Processes\nThe first-order moving average process, denoted MA(1), is\n\\[\nY_{t}=\\mu+e_{t}+\\theta e_{t-1}\n\\]\nwhere \\(e_{t}\\) is a strictly stationary and ergodic white noise process with var \\(\\left[e_{t}\\right]=\\sigma^{2}\\). The model is called a “moving average” because \\(Y_{t}\\) is a weighted average of the shocks \\(e_{t}\\) and \\(e_{t-1}\\).\n\\({ }^{5}\\) To see this, focus on the case \\(b_{1} \\geq 0\\). The requirement \\(\\left|1-b_{1} z\\right| \\geq \\delta\\) for \\(|z| \\leq 1\\) means \\(\\min _{|z| \\leq 1}\\left|1-b_{1} z\\right|=1-b_{1} \\geq \\delta\\) or \\(b_{1} \\leq 1-\\delta\\). It is straightforward to calculate that a MA(1) has the following moments.\n\\[\n\\begin{aligned}\n\\mathbb{E}\\left[Y_{t}\\right] &=\\mu \\\\\n\\operatorname{var}\\left[Y_{t}\\right] &=\\left(1+\\theta^{2}\\right) \\sigma^{2} \\\\\n\\gamma(1) &=\\theta \\sigma^{2} \\\\\n\\rho(1) &=\\frac{\\theta}{1+\\theta^{2}} \\\\\n\\gamma(k) &=\\rho(k)=0, \\quad k \\geq 2 .\n\\end{aligned}\n\\]\nThus the MA(1) process has a non-zero first autocorrelation with the remainder zero.\nA MA(1) process with \\(\\theta \\neq 0\\) is serially correlated with each pair of adjacent observations \\(\\left(Y_{t-1}, Y_{t}\\right)\\) correlated. If \\(\\theta>0\\) the pair are positively correlated, while if \\(\\theta<0\\) they are negatively correlated. The serial correlation is limited in that observations separated by multiple periods are mutually independent.\nThe \\(\\mathbf{q}^{t h}\\)-order moving average process, denoted \\(\\mathbf{M A}(\\mathbf{q})\\), is\n\\[\nY_{t}=\\mu+\\theta_{0} e_{t}+\\theta_{1} e_{t-1}+\\theta_{2} e_{t-2}+\\cdots+\\theta_{q} e_{t-q}\n\\]\nwhere \\(\\theta_{0}=1\\). It is straightforward to calculate that a MA(q) has the following moments.\n\\[\n\\begin{aligned}\n\\mathbb{E}\\left[Y_{t}\\right] &=\\mu \\\\\n\\operatorname{var}\\left[Y_{t}\\right] &=\\left(\\sum_{j=0}^{q} \\theta_{j}^{2}\\right) \\sigma^{2} \\\\\n\\gamma(k) &=\\left(\\sum_{j=0}^{q-k} \\theta_{j+k} \\theta_{j}\\right) \\sigma^{2}, \\quad k \\leq q \\\\\n\\rho(k) &=\\frac{\\sum_{j=0}^{q-k} \\theta_{j+k} \\theta_{j}}{\\sum_{j=0}^{q} \\theta_{j}^{2}} \\\\\n\\gamma(k) &=\\rho(k)=0, \\quad k>q .\n\\end{aligned}\n\\]\nIn particular, a MA(q) has \\(q\\) non-zero autocorrelations with the remainder zero.\nA MA(q) process \\(Y_{t}\\) is strictly stationary and ergodic.\nA MA(q) process with moderately large \\(q\\) can have considerably more complicated dependence relations than a MA(1) process. One specific pattern which can be induced by a MA process is smoothing. Suppose that the coefficients \\(\\theta_{j}\\) all equal 1. Then \\(Y_{t}\\) is a smoothed version of the shocks \\(e_{t}\\).\nTo illustrate, Figure \\(14.3(\\) a) displays a plot of a simulated white noise (i.i.d. \\(\\mathrm{N}(0,1)\\) ) process with \\(n=120\\) observations. Figure 14.3(b) displays a plot of a MA(8) process constructed with the same innovations, with \\(\\theta_{j}=1, j=1, \\ldots, 8\\). You can see that the white noise has no predictable behavior while the \\(\\mathrm{MA}(8)\\) is smooth."
  },
  {
    "objectID": "chpt14-time-series.html#infinite-order-moving-average-process",
    "href": "chpt14-time-series.html#infinite-order-moving-average-process",
    "title": "14  Time Series",
    "section": "14.21 Infinite-Order Moving Average Process",
    "text": "14.21 Infinite-Order Moving Average Process\nAn infinite-order moving average process, denoted MA( \\(\\infty\\) ), also known as a linear process, is\n\\[\nY_{t}=\\mu+\\sum_{j=0}^{\\infty} \\theta_{j} e_{t-j}\n\\]\n\n\nWhite Noise\n\n\n\nMA(8)\n\nFigure 14.3: White Noise and MA(8)\nwhere \\(e_{t}\\) is a strictly stationary and ergodic white noise process, \\(\\operatorname{var}\\left[e_{t}\\right]=\\sigma^{2}\\), and \\(\\sum_{j=0}^{\\infty}\\left|\\theta_{j}\\right|<\\infty\\). From Theorem 14.6, \\(Y_{t}\\) is strictly stationary and ergodic. A linear process has the following moments:\n\\[\n\\begin{aligned}\n\\mathbb{E}\\left[Y_{t}\\right] &=\\mu \\\\\n\\operatorname{var}\\left[Y_{t}\\right] &=\\left(\\sum_{j=0}^{\\infty} \\theta_{j}^{2}\\right) \\sigma^{2} \\\\\n\\gamma(k) &=\\left(\\sum_{j=0}^{\\infty} \\theta_{j+k} \\theta_{j}\\right) \\sigma^{2} \\\\\n\\rho(k) &=\\frac{\\sum_{j=0}^{\\infty} \\theta_{j+k} \\theta_{j}}{\\sum_{j=0}^{\\infty} \\theta_{j}^{2}} .\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "chpt14-time-series.html#first-order-autoregressive-process",
    "href": "chpt14-time-series.html#first-order-autoregressive-process",
    "title": "14  Time Series",
    "section": "14.22 First-Order Autoregressive Process",
    "text": "14.22 First-Order Autoregressive Process\nThe first-order autoregressive process, denoted AR(1), is\n\\[\nY_{t}=\\alpha_{0}+\\alpha_{1} Y_{t-1}+e_{t}\n\\]\nwhere \\(e_{t}\\) is a strictly stationary and ergodic white noise process with var \\(\\left[e_{t}\\right]=\\sigma^{2}\\). The AR(1) model is probably the single most important model in econometric time series analysis.\nAs a simple motivating example let \\(Y_{t}\\) be is the employment level (number of jobs) in an economy. Suppose that a fixed fraction \\(1-\\alpha_{1}\\) of employees lose their job and a random number \\(u_{t}\\) of new employees are hired each period. Setting \\(\\alpha_{0}=\\mathbb{E}\\left[u_{t}\\right]\\) and \\(e_{t}=u_{t}-\\alpha_{0}\\), this implies the law of motion (14.25).\nTo illustrate the behavior of the AR(1) process, Figure \\(14.4\\) plots two simulated AR(1) processes. Each is generated using the white noise process \\(e_{t}\\) displayed in Figure 14.3(a). The plot in Figure 14.4(a) sets\n\n\n\\(\\operatorname{AR}(1)\\) with \\(\\alpha_{1}=0.5\\)\n\n\n\n\\(\\operatorname{AR}(1)\\) with \\(\\alpha_{1}=0.95\\)\n\nFigure 14.4: AR(1) Processes\n\\(\\alpha_{1}=0.5\\) and the plot in Figure 14.4(b) sets \\(\\alpha_{1}=0.95\\). You can see how both are more smooth than the white noise process and that the smoothing increases with \\(\\alpha\\).\nOur first goal is to obtain conditions under which (14.25) is stationary. We can do so by showing that \\(Y_{t}\\) can be written as a convergent linear process and then appealing to Theorem 14.5. To find a linear process representation for \\(Y_{t}\\) we can use backward recursion. Notice that \\(Y_{t}\\) in (14.25) depends on its previous value \\(Y_{t-1}\\). If we take (14.25) and lag it one period we find \\(Y_{t-1}=\\alpha_{0}+\\alpha_{1} Y_{t-2}+e_{t-1}\\). Substituting this into (14.25) we find\n\\[\n\\begin{aligned}\nY_{t} &=\\alpha_{0}+\\alpha_{1}\\left(\\alpha_{0}+\\alpha_{1} Y_{t-2}+e_{t-1}\\right)+e_{t} \\\\\n&=\\alpha_{0}+\\alpha_{1} \\alpha_{0}+\\alpha_{1}^{2} Y_{t-2}+\\alpha_{1} e_{t-1}+e_{t} .\n\\end{aligned}\n\\]\nSimilarly we can lag (14.31) twice to find \\(Y_{t-2}=\\alpha_{0}+\\alpha_{1} Y_{t-3}+e_{t-2}\\) and can be used to substitute out \\(Y_{t-2}\\). Continuing recursively \\(t\\) times, we find\n\\[\n\\begin{aligned}\nY_{t} &=\\alpha_{0}\\left(1+\\alpha_{1}+\\alpha_{1}^{2}+\\cdots+\\alpha_{1}^{t-1}\\right)+\\alpha_{1}^{t} Y_{0}+\\alpha_{1}^{t-1} e_{1}+\\alpha_{1}^{t-2} e_{2}+\\cdots+e_{t} \\\\\n&=\\alpha_{0} \\sum_{j=0}^{t-1} \\alpha_{1}^{j}+\\alpha_{1}^{t} Y_{0}+\\sum_{j=0}^{t-1} \\alpha_{1}^{j} e_{t-j} .\n\\end{aligned}\n\\]\nThus \\(Y_{t}\\) equals an intercept plus the scaled initial condition \\(\\alpha_{1}^{t} Y_{0}\\) and the moving average \\(\\sum_{j=0}^{t-1} \\alpha_{1}^{j} e_{t-j}\\).\nNow suppose we continue this recursion into the infinite past. By Theorem \\(14.3\\) this converges if \\(\\sum_{j=0}^{\\infty}\\left|\\alpha_{1}\\right|^{j}<\\infty\\). The limit is provided by the following well-known result.\nTheorem \\(14.20 \\sum_{k=0}^{\\infty} \\beta^{k}=\\frac{1}{1-\\beta}\\) is absolutely convergent if \\(|\\beta|<1\\) The series converges by the ratio test (see Theorem A.3 of Probability and Statistics for Economists). To find the limit,\n\\[\nA=\\sum_{k=0}^{\\infty} \\beta^{k}=1+\\sum_{k=1}^{\\infty} \\beta^{k}=1+\\beta \\sum_{k=0}^{\\infty} \\beta^{k}=1+\\beta A .\n\\]\nSolving, we find \\(A=1 /(1-\\beta)\\).\nThus the intercept in (14.26) converges to \\(\\alpha_{0} /\\left(1-\\alpha_{1}\\right)\\). We deduce the following:\nTheorem 14.21 If \\(\\mathbb{E}\\left|e_{t}\\right|<\\infty\\) and \\(\\left|\\alpha_{1}\\right|<1\\) then the AR(1) process (14.25) has the convergent representation\n\\[\nY_{t}=\\mu+\\sum_{j=0}^{\\infty} \\alpha_{1}^{j} e_{t-j}\n\\]\nwhere \\(\\mu=\\alpha_{0} /\\left(1-\\alpha_{1}\\right)\\). The AR(1) process \\(Y_{t}\\) is strictly stationary and ergodic.\nWe can compute the moments of \\(Y_{t}\\) from (14.27)\n\\[\n\\begin{gathered}\n\\mathbb{E}\\left[Y_{t}\\right]=\\mu+\\sum_{k=0}^{\\infty} \\alpha_{1}^{k} \\mathbb{E}\\left[e_{t-k}\\right]=\\mu \\\\\n\\operatorname{var}\\left[Y_{t}\\right]=\\sum_{k=0}^{\\infty} \\alpha_{1}^{2 k} \\operatorname{var}\\left[e_{t-k}\\right]=\\frac{\\sigma^{2}}{1-\\alpha_{1}^{2}} .\n\\end{gathered}\n\\]\nOne way to calculate the moments is as follows. Apply expectations to both sides of (14.25)\n\\[\n\\mathbb{E}\\left[Y_{t}\\right]=\\alpha_{0}+\\alpha_{1} \\mathbb{E}\\left[Y_{t-1}\\right]+\\mathbb{E}\\left[e_{t}\\right]=\\alpha_{0}+\\alpha_{1} \\mathbb{E}\\left[Y_{t-1}\\right] .\n\\]\nStationarity implies \\(\\mathbb{E}\\left[Y_{t-1}\\right]=\\mathbb{E}\\left[Y_{t}\\right]\\). Solving we find \\(\\mathbb{E}\\left[Y_{t}\\right]=\\alpha_{0} /\\left(1-\\alpha_{1}\\right)\\). Similarly,\n\\[\n\\operatorname{var}\\left[Y_{t}\\right]=\\operatorname{var}\\left[\\alpha Y_{t-1}+e_{t}\\right]=\\alpha_{1}^{2} \\operatorname{var}\\left[Y_{t-1}\\right]+\\operatorname{var}\\left[e_{t}\\right]=\\alpha_{1}^{2} \\operatorname{var}\\left[Y_{t-1}\\right]+\\sigma^{2} .\n\\]\nStationarity implies \\(\\operatorname{var}\\left[Y_{t-1}\\right]=\\operatorname{var}\\left[Y_{t}\\right]\\). Solving we find \\(\\operatorname{var}\\left[Y_{t}\\right]=\\sigma^{2} /\\left(1-\\alpha_{1}^{2}\\right)\\). This method is useful for calculation of autocovariances and autocorrelations. For simplicity set \\(\\alpha_{0}=0\\) so that \\(\\mathbb{E}\\left[Y_{t}\\right]=0\\) and \\(\\mathbb{E}\\left[Y_{t}^{2}\\right]=\\operatorname{var}\\left[Y_{t}\\right]\\). We find\n\\[\n\\gamma(1)=\\mathbb{E}\\left[Y_{t-1} Y_{t}\\right]=\\mathbb{E}\\left[Y_{t-1}\\left(\\alpha_{1} Y_{t-1}+e_{t}\\right)\\right]=\\alpha_{1} \\operatorname{var}\\left[Y_{t}\\right]\n\\]\nso\n\\[\n\\rho(1)=\\gamma(1) / \\operatorname{var}\\left[Y_{t}\\right]=\\alpha_{1} .\n\\]\nFurthermore,\n\\[\n\\gamma(k)=\\mathbb{E}\\left[Y_{t-k} Y_{t}\\right]=\\mathbb{E}\\left[Y_{t-k}\\left(\\alpha_{1} Y_{t-1}+e_{t}\\right)\\right]=\\alpha_{1} \\gamma(k-1)\n\\]\nBy recursion we obtain\n\\[\n\\begin{aligned}\n&\\gamma(k)=\\alpha_{1}^{k} \\operatorname{var}\\left[Y_{t}\\right] \\\\\n&\\rho(k)=\\alpha_{1}^{k} .\n\\end{aligned}\n\\]\nThus the AR(1) process with \\(\\alpha_{1} \\neq 0\\) has non-zero autocorrelations of all orders which decay to zero geometrically as \\(k\\) increases. For \\(\\alpha_{1}>0\\) the autocorrelations are all positive. For \\(\\alpha_{1}<0\\) the autocorrelations alternate in sign.\nWe can also express the AR(1) process using the lag operator notation:\n\\[\n\\left(1-\\alpha_{1} \\mathrm{~L}\\right) Y_{t}=\\alpha_{0}+e_{t}\n\\]\nWe can write this as \\(\\alpha(\\mathrm{L}) Y_{t}=\\alpha_{0}+e_{t}\\) where \\(\\alpha(\\mathrm{L})=1-\\alpha_{1} \\mathrm{~L}\\). We call \\(\\alpha(z)=1-\\alpha_{1} z\\) the autoregressive polynomial of \\(Y_{t}\\).\nThis suggests an alternative way of obtaining the representation (14.27). We can invert the operator (1- \\(\\left.\\alpha_{1} \\mathrm{~L}\\right)\\) to write \\(Y_{t}\\) as a function of lagged \\(e_{t}\\). That is, suppose that the inverse operator \\(\\left(1-\\alpha_{1} \\mathrm{~L}\\right)^{-1}\\) exists. Then we can use this operator on (14.28) to find\n\\[\nY_{t}=\\left(1-\\alpha_{1} \\mathrm{~L}\\right)^{-1}\\left(1-\\alpha_{1} \\mathrm{~L}\\right) Y_{t}=\\left(1-\\alpha_{1} \\mathrm{~L}\\right)^{-1}\\left(\\alpha_{0}+e_{t}\\right) .\n\\]\nWhat is the operator \\(\\left(1-\\alpha_{1} \\mathrm{~L}\\right)^{-1}\\) ? Recall from Theorem \\(14.20\\) that for \\(|x|<1\\),\n\\[\n\\sum_{j=0}^{\\infty} x^{j}=\\frac{1}{1-x}=(1-x)^{-1} .\n\\]\nEvaluate this expression at \\(x=\\alpha_{1} z\\). We find\n\\[\n\\left(1-\\alpha_{1} z\\right)^{-1}=\\sum_{j=0}^{\\infty} \\alpha_{1}^{j} z^{j} .\n\\]\nSetting \\(z=\\mathrm{L}\\) this is\n\\[\n\\left(1-\\alpha_{1} \\mathrm{~L}\\right)^{-1}=\\sum_{j=0}^{\\infty} \\alpha_{1}^{j} \\mathrm{~L}^{j} .\n\\]\nSubstituted into (14.29) we obtain\n\\[\n\\begin{aligned}\nY_{t} &=\\left(1-\\alpha_{1} \\mathrm{~L}\\right)^{-1}\\left(\\alpha_{0}+e_{t}\\right) \\\\\n&=\\left(\\sum_{j=0}^{\\infty} \\alpha^{j} \\mathrm{~L}^{j}\\right)\\left(\\alpha_{0}+e_{t}\\right) \\\\\n&=\\sum_{j=0}^{\\infty} \\alpha_{1}^{j} \\mathrm{~L}^{j}\\left(\\alpha_{0}+e_{t}\\right) \\\\\n&=\\sum_{j=0}^{\\infty} \\alpha_{1}^{j}\\left(\\alpha_{0}+e_{t-j}\\right) \\\\\n&=\\frac{\\alpha_{0}}{1-\\alpha_{1}}+\\sum_{j=0}^{\\infty} \\alpha_{1}^{j} e_{t-j}\n\\end{aligned}\n\\]\nwhich is (14.27). This is valid for \\(\\left|\\alpha_{1}\\right|<1\\).\nThis illustrates another important concept. We say that a polynomial \\(\\alpha(z)\\) is invertible if\n\\[\n\\alpha(z)^{-1}=\\sum_{j=0}^{\\infty} a_{j} z^{j}\n\\]\nis absolutely convergent. In particular, the \\(\\operatorname{AR}(1)\\) autoregressive polynomial \\(\\alpha(z)=1-\\alpha_{1} z\\) is invertible if \\(\\left|\\alpha_{1}\\right|<1\\). This is the same condition as for stationarity of the AR(1) process. Invertibility turns out to be a useful property."
  },
  {
    "objectID": "chpt14-time-series.html#unit-root-and-explosive-ar1-processes",
    "href": "chpt14-time-series.html#unit-root-and-explosive-ar1-processes",
    "title": "14  Time Series",
    "section": "14.23 Unit Root and Explosive AR(1) Processes",
    "text": "14.23 Unit Root and Explosive AR(1) Processes\nThe AR(1) process (14.25) is stationary if \\(\\left|\\alpha_{1}\\right|<1\\). What happens otherwise?\nIf \\(\\alpha_{0}=0\\) and \\(\\alpha_{1}=1\\) the model is known as a random walk.\n\\[\nY_{t}=Y_{t-1}+e_{t} .\n\\]\nThis is also called a unit root process, a martingale, or an integrated process. By back-substitution\n\\[\nY_{t}=Y_{0}+\\sum_{j=1}^{t} e_{j} .\n\\]\nThus the initial condition does not disappear for large \\(t\\). Consequently the series is non-stationary. The autoregressive polynomial \\(\\alpha(z)=1-z\\) is not invertible, meaning that \\(Y_{t}\\) cannot be written as a convergent function of the infinite past history of \\(e_{t}\\).\nThe stochastic behavior of a random walk is noticably different from a stationary AR(1) process. It wanders up and down with equal likelihood and is not mean-reverting. While it has no tendency to return to its previous values the wandering nature of a random walk can give the illusion of mean reversion. The difference is that a random walk will take a very large number of time periods to “revert”.\n\n\nExample 1\n\n\n\nExample 2\n\nFigure 14.5: Random Walk Processes\nTo illustrate, Figure \\(14.5\\) plots two independent random walk processes. The plot in panel (a) uses the innovations from Figure 14.3(a). The plot in panel (b) uses an independent set of i.i.d. \\(N(0,1)\\) errors. You can see that the plot in panel (a) appears similar to the MA(8) and AR(1) plots in the sense that the series is smooth with long swings, but the difference is that the series does not return to a longterm mean. It appears to have drifted down over time. The plot in panel (b) appears to have quite different behavior, falling dramatically over a 5-year period, and then appearing to stabilize. These are both common behaviors of random walk processes. If \\(\\alpha_{1}>1\\) the process is explosive. The model (14.25) with \\(\\alpha_{1}>1\\) exhibits exponential growth and high sensitivity to initial conditions. Explosive autoregressive processes do not seem to be good descriptions for most economic time series. While aggregate time series such as the GDP process displayed in Figure 14.1 (a) exhibit a similar exponential growth pattern, the exponential growth can typically be removed by taking logarithms.\nThe case \\(\\alpha_{1}<-1\\) induces explosive oscillating growth and does not appear to be empirically relevant for economic applications."
  },
  {
    "objectID": "chpt14-time-series.html#second-order-autoregressive-process",
    "href": "chpt14-time-series.html#second-order-autoregressive-process",
    "title": "14  Time Series",
    "section": "14.24 Second-Order Autoregressive Process",
    "text": "14.24 Second-Order Autoregressive Process\nThe second-order autoregressive process, denoted \\(\\mathbf{A R}(2)\\), is\n\\[\nY_{t}=\\alpha_{0}+\\alpha_{1} Y_{t-1}+\\alpha_{2} Y_{t-2}+e_{t}\n\\]\nwhere \\(e_{t}\\) is a strictly stationary and ergodic white noise process. The dynamic patterns of an AR(2) process are more complicated than an AR(1) process.\nAs a motivating example consider the multiplier-accelerator model of Samuelson (1939). It might be a bit dated as a model but it is simple so hopefully makes the point. Aggregate output (in an economy with no trade) is defined as \\(Y_{t}=\\) Consumption \\(_{t}+\\) Investment \\(_{t}+\\) Gov \\(_{t}\\). Suppose that individuals make their consumption decisions on the previous period’s income Consumption \\(t=b Y_{t-1}\\), firms make their investment decisions on the change in consumption Investment \\(t_{t}=d \\Delta C_{t}\\), and government spending is random \\(G o v_{t}=a+e_{t}\\). Then aggregate output follows\n\\[\nY_{t}=a+b(1+d) Y_{t-1}-b d Y_{t-2}+e_{t}\n\\]\nwhich is an \\(\\operatorname{AR}(2)\\) process.\nUsing the lag operator we can write (14.31) as\n\\[\nY_{t}-\\alpha_{1} \\mathrm{~L} Y_{t}-\\alpha_{2} \\mathrm{~L}^{2} Y_{t}=\\alpha_{0}+e_{t}\n\\]\nor \\(\\alpha(\\mathrm{L}) Y_{t}=\\alpha_{0}+e_{t}\\) where \\(\\alpha(\\mathrm{L})=1-\\alpha_{1} \\mathrm{~L}-\\alpha_{2} \\mathrm{~L}^{2}\\). We call \\(\\alpha(z)\\) the autoregressive polynomial of \\(Y_{t}\\).\nWe would like to find the conditions for the stationarity of \\(Y_{t}\\). It turns out that it is convenient to transform the process (14.31) into a VAR(1) process (to be studied in the next chapter). Set \\(\\widetilde{Y}_{t}=\\left(Y_{t}, Y_{t-1}\\right)^{\\prime}\\), which is stationary if and only if \\(Y_{t}\\) is stationary. Equation (14.31) implies that \\(\\widetilde{Y}_{t}\\) satisfies\n\\[\n\\left(\\begin{array}{c}\nY_{t} \\\\\nY_{t-1}\n\\end{array}\\right)=\\left(\\begin{array}{cc}\n\\alpha_{1} & \\alpha_{2} \\\\\n1 & 0\n\\end{array}\\right)\\left(\\begin{array}{c}\nY_{t-1} \\\\\nY_{t-2}\n\\end{array}\\right)+\\left(\\begin{array}{c}\na_{0}+e_{t} \\\\\n0\n\\end{array}\\right)\n\\]\nor\n\\[\n\\widetilde{Y}_{t}=\\boldsymbol{A} \\widetilde{Y}_{t-1}+\\widetilde{e}_{t}\n\\]\nwhere \\(\\boldsymbol{A}=\\left(\\begin{array}{cc}\\alpha_{1} & \\alpha_{2} \\\\ 1 & 0\\end{array}\\right)\\) and \\(\\widetilde{e}_{t}=\\left(a_{0}+e_{t}, 0\\right)^{\\prime}\\). Equation (14.33) falls in the class of VAR(1) models studied in Section 15.6. Theorem \\(15.6\\) shows that the \\(\\operatorname{VAR}(1)\\) process is strictly stationary and ergodic if the innovations satisfy \\(\\mathbb{E}\\left\\|\\widetilde{e}_{t}\\right\\|<\\infty\\) and all eigenvalues \\(\\lambda\\) of \\(\\boldsymbol{A}\\) are less than one in absolute value. The eigenvalues satisfy \\(\\operatorname{det}\\left(\\boldsymbol{A}-\\boldsymbol{I}_{2} \\lambda\\right)=0\\), where\n\\[\n\\operatorname{det}\\left(\\boldsymbol{A}-\\boldsymbol{I}_{2} \\lambda\\right)=\\operatorname{det}\\left(\\begin{array}{cc}\n\\alpha_{1}-\\lambda & \\alpha_{2} \\\\\n1 & -\\lambda\n\\end{array}\\right)=\\lambda^{2}-\\lambda \\alpha_{1}-\\alpha_{2}=\\lambda^{2} \\alpha(1 / \\lambda)\n\\]\nand \\(\\alpha(z)=1-\\alpha_{1} z-\\alpha_{2} z^{2}\\) is the autoregressive polynomial. Thus the eigenvalues satisfy \\(\\alpha(1 / \\lambda)=0\\). Factoring the autoregressive polynomial as \\(\\alpha(z)=\\left(1-\\lambda_{1} z\\right)\\left(1-\\lambda_{2} z\\right)\\) the solutions \\(\\alpha(1 / \\lambda)=0\\) must equal \\(\\lambda_{1}\\) and \\(\\lambda_{2}\\). The quadratic formula shows that these equal\n\\[\n\\lambda_{j}=\\frac{\\alpha_{1} \\pm \\sqrt{\\alpha_{1}^{2}+4 \\alpha_{2}}}{2} .\n\\]\nThese eigenvalues are real if \\(\\alpha_{1}^{2}+4 \\alpha_{2} \\geq 0\\) and are complex conjugates otherwise. The AR(2) process is stationary if the solutions (14.34) satisfy \\(\\left|\\lambda_{j}\\right|<1\\).\n\nFigure 14.6: Stationarity Region for \\(\\operatorname{AR}(2)\\)\nUsing (14.34) to solve for the AR coefficients in terms of the eigenvalues we find \\(\\alpha_{1}=\\lambda_{1}+\\lambda_{2}\\) and \\(\\alpha_{2}=-\\lambda_{1} \\lambda_{2}\\). With some algebra (the details are deferred to Section 14.47) we can show that \\(\\left|\\lambda_{1}\\right|<1\\) and \\(\\left|\\lambda_{2}\\right|<1\\) iff the following restrictions hold on the autoregressive coefficients:\n\\[\n\\begin{aligned}\n\\alpha_{1}+\\alpha_{2} &<1 \\\\\n\\alpha_{2}-\\alpha_{1}<1 \\\\\n\\alpha_{2} &>-1 .\n\\end{aligned}\n\\]\nThese restrictions describe a triangle in \\(\\left(\\alpha_{1}, \\alpha_{2}\\right)\\) space which is shown in Figure 14.6. Coefficients within this triangle correspond to a stationary \\(\\operatorname{AR}(2)\\) process.\nTake the Samuelson multiplier-accelerator model (14.32). You can calculate that (14.35)-(14.37) are satisfied (and thus the process is strictly stationary) if \\(0 \\leq b<1\\) and \\(0 \\leq d \\leq 1\\), which are reasonable restrictions on the model parameters. The most important restriction is \\(b<1\\), which in the language of old-school macroeconomics is that the marginal propensity to consume out of income is less than one.\nFurthermore, the triangle is divided into two regions as marked in Figure 14.6: the region above the parabola \\(\\alpha_{1}^{2}+4 \\alpha_{2}=0\\) producing real eigenvalues \\(\\lambda_{j}\\), and the region below the parabola producing complex eigenvalues \\(\\lambda_{j}\\). This is interesting because when the eigenvalues are complex the autocorrelations of \\(Y_{t}\\) display damped oscillations. For this reason the dynamic patterns of an AR(2) can be much more complicated than those of an AR(1).\nAgain, take the Samuelson multiplier-accelerator model (14.32). You can calculate that if \\(b \\geq 0\\), the model has real eigenvalues iff \\(b \\geq 4 d /(1+d)^{2}\\), which holds for \\(b\\) large and \\(d\\) small, which are “stable” parameterizations. On the other hand, the model has complex eigenvalues (and thus oscillations) for sufficiently small \\(b\\) and large \\(d\\).\nTheorem 14.22 If \\(\\mathbb{E}\\left|e_{t}\\right|<\\infty\\) and \\(\\left|\\lambda_{j}\\right|<1\\) for \\(\\lambda_{j}\\) defined in (14.34), or equivalently if the inequalities (14.35)-(14.37) hold, then the \\(\\mathrm{AR}(2)\\) process (14.31) is absolutely convergent, strictly stationary, and ergodic.\nThe proof is presented in Section \\(14.47\\).\n\n\n\\(\\operatorname{AR}(2)\\)\n\n\n\n\\(\\operatorname{AR}(2)\\) with Complex Roots\n\nFigure 14.7: AR(2) Processes\nTo illustrate, Figure \\(14.7\\) displays two simulated AR(2) processes. The plot in panel (a) sets \\(\\alpha_{1}=\\alpha_{2}=\\) 0.4. These coefficients produce real factors so the process displays behavior similar to that of the AR(1) processes. The plot in panel (b) sets \\(\\alpha_{1}=1.3\\) and \\(\\alpha_{2}=-0.8\\). These coefficients produce complex factors so the process displays oscillations."
  },
  {
    "objectID": "chpt14-time-series.html#arp-processes",
    "href": "chpt14-time-series.html#arp-processes",
    "title": "14  Time Series",
    "section": "14.25 AR(p) Processes",
    "text": "14.25 AR(p) Processes\nThe \\(\\mathbf{p}^{\\text {th }}\\)-order autoregressive process, denoted \\(\\mathbf{A R}(\\mathbf{p})\\), is\n\\[\nY_{t}=\\alpha_{0}+\\alpha_{1} Y_{t-1}+\\alpha_{2} Y_{t-2}+\\cdots+\\alpha_{p} Y_{t-p}+e_{t}\n\\]\nwhere \\(e_{t}\\) is a strictly stationary and ergodic white noise process.\nUsing the lag operator,\n\\[\nY_{t}-\\alpha_{1} \\mathrm{~L} Y_{t}-\\alpha_{2} \\mathrm{~L}^{2} Y_{t}-\\cdots-\\alpha_{p} \\mathrm{~L}^{p} Y_{t}=\\alpha_{0}+e_{t}\n\\]\nor \\(\\alpha(\\mathrm{L}) Y_{t}=\\alpha_{0}+e_{t}\\) where\n\\[\n\\alpha(\\mathrm{L})=1-\\alpha_{1} \\mathrm{~L}-\\alpha_{2} \\mathrm{~L}^{2}-\\cdots-\\alpha_{p} \\mathrm{~L}^{p} .\n\\]\nWe call \\(\\alpha(z)\\) the autoregressive polynomial of \\(Y_{t}\\).\nWe find conditions for the stationarity of \\(Y_{t}\\) by a technique similar to that used for the AR(2) process. Set \\(\\widetilde{Y}_{t}=\\left(Y_{t}, Y_{t-1}, \\ldots, Y_{t-p+1}\\right)^{\\prime}\\) and \\(\\widetilde{e}_{t}=\\left(a_{0}+e_{t}, 0, \\ldots, 0\\right)^{\\prime}\\). Equation (14.38) implies that \\(\\widetilde{Y}_{t}\\) satisfies the VAR(1) equation (14.33) with\n\\[\n\\boldsymbol{A}=\\left(\\begin{array}{ccccc}\n\\alpha_{1} & \\alpha_{2} & \\cdots & \\alpha_{p-1} & \\alpha_{p} \\\\\n1 & 0 & \\cdots & 0 & 0 \\\\\n0 & 1 & \\cdots & 0 & 0 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots & \\vdots \\\\\n0 & 0 & \\cdots & 1 & 0\n\\end{array}\\right)\n\\]\nAs shown in the proof of Theorem \\(14.23\\) below, the eigenvalues \\(\\lambda_{j}\\) of \\(\\boldsymbol{A}\\) are the reciprocals of the roots \\(r_{j}\\) of the autoregressive polynomial (14.39). The roots \\(r_{j}\\) are the solutions to \\(\\alpha\\left(r_{j}\\right)=0\\). Theorem \\(15.6\\) shows that stationarity of \\(\\widetilde{Y}_{t}\\) holds if the eigenvalues \\(\\lambda_{j}\\) are less than one in absolute value, or equivalently when the roots \\(r_{j}\\) are greater than one in absolute value. For complex numbers the equation \\(|z|=1\\) defines the unit circle (the circle with radius of unity). We therefore say that ” \\(z\\) lies outside the unit circle” if \\(|z|>1\\).\nTheorem 14.23 If \\(\\mathbb{E}\\left|e_{t}\\right|<\\infty\\) and all roots of \\(\\alpha(z)\\) lie outside the unit circle then the AR(p) process (14.38) is absolutely convergent, strictly stationary, and ergodic.\nWhen the roots of \\(\\alpha(z)\\) lie outside the unit circle then the polynomial \\(\\alpha(z)\\) is invertible. Inverting the autoregressive representation \\(\\alpha(\\mathrm{L}) Y_{t}=\\alpha_{0}+e_{t}\\) we obtain an infinite-order moving average representation\n\\[\nY_{t}=\\mu+b(\\mathrm{~L}) e_{t}\n\\]\nwhere\n\\[\nb(z)=\\alpha(z)^{-1}=\\sum_{j=0}^{\\infty} b_{j} z^{j}\n\\]\nand \\(\\mu=\\alpha(1)^{-1} a_{0}\\).\nWe have the following characterization of the moving average coefficients. Theorem 14.24 If all roots \\(r_{j}\\) of the autoregressive polynomial \\(\\alpha(z)\\) satisfy \\(\\left|r_{j}\\right|>1\\) then (14.41) holds with \\(\\left|b_{j}\\right| \\leq(j+1)^{p} \\lambda^{j}\\) and \\(\\sum_{j=0}^{\\infty}\\left|b_{j}\\right|<\\infty\\) where \\(\\lambda=\\max _{1 \\leq j \\leq p}\\left|r_{j}^{-1}\\right|<1\\)\nThe proof is presented in Section \\(14.47\\)."
  },
  {
    "objectID": "chpt14-time-series.html#impulse-response-function",
    "href": "chpt14-time-series.html#impulse-response-function",
    "title": "14  Time Series",
    "section": "14.26 Impulse Response Function",
    "text": "14.26 Impulse Response Function\nThe coefficients of the moving average representation\n\\[\n\\begin{aligned}\nY_{t} &=b(\\mathrm{~L}) e_{t} \\\\\n&=\\sum_{j=0}^{\\infty} b_{j} e_{t-j} \\\\\n&=b_{0} e_{t}+b_{1} e_{t-1}+b_{2} e_{t-2}+\\cdots\n\\end{aligned}\n\\]\nare known among economists as the impulse response function (IRF). Often the IRF is scaled by the standard deviation of \\(e_{t}\\). We discuss this scaling at the end of the section. In linear models the impulse response function is defined as the change in \\(Y_{t+j}\\) due to a shock at time \\(t\\). This is\n\\[\n\\frac{\\partial}{\\partial e_{t}} Y_{t+j}=b_{j} .\n\\]\nThis means that the coefficient \\(b_{j}\\) can be interpreted as the magnitude of the impact of a time \\(t\\) shock on the time \\(t+j\\) variable. Plots of \\(b_{j}\\) can be used to assess the time-propagation of shocks.\nIt is desirable to have a convenient method to calculate the impulse responses \\(b_{j}\\) from the coefficients of an autoregressive model (14.38). There are two methods which we now describe.\nThe first uses a simple recursion. In the linear \\(\\operatorname{AR}(\\mathrm{p})\\) model, we can see that the coefficient \\(b_{j}\\) is the simple derivative\n\\[\nb_{j}=\\frac{\\partial}{\\partial e_{t}} Y_{t+j}=\\frac{\\partial}{\\partial e_{0}} Y_{j}\n\\]\nWe can calculate \\(b_{j}\\) by generating a history and perturbing the shock \\(e_{0}\\). Since this calculation is unaffected by all other shocks we can simply set \\(e_{t}=0\\) for \\(t \\neq 0\\) and set \\(e_{0}=1\\). This implies the recursion\n\\[\n\\begin{aligned}\nb_{0} &=1 \\\\\nb_{1} &=\\alpha_{1} b_{0} \\\\\nb_{2} &=\\alpha_{1} b_{1}+\\alpha_{2} b_{0} \\\\\n& \\vdots \\\\\nb_{j} &=\\alpha_{1} b_{j-1}+\\alpha_{2} b_{j-2}+\\cdots+\\alpha_{p} b_{j-p} .\n\\end{aligned}\n\\]\nThis recursion is conveniently calculated by the following simulation. Set \\(Y_{t}=0\\) for \\(t \\leq 0\\). Set \\(e_{0}=1\\) and \\(e_{t}=0\\) for \\(t \\geq 1\\). Generate \\(Y_{t}\\) for \\(t \\geq 0\\) by \\(Y_{t}=\\alpha_{1} Y_{t-1}+\\alpha_{2} Y_{t-2}+\\cdots+\\alpha_{p} Y_{t-p}+e_{t}\\). Then \\(Y_{j}=b_{j}\\).\nA second method uses the vector representation (14.33) of the AR(p) model with coefficient matrix (14.40). By recursion\n\\[\n\\widetilde{Y}_{t}=\\sum_{j=0}^{\\infty} \\boldsymbol{A}^{j} \\widetilde{e}_{t-j}\n\\]\nHere, \\(\\boldsymbol{A}^{j}=\\boldsymbol{A} \\cdots \\boldsymbol{A}\\) means the \\(\\boldsymbol{j}^{t h}\\) matrix product of \\(\\boldsymbol{A}\\) with itself. Setting \\(S=(1,0, \\ldots 0)^{\\prime}\\) we find\n\\[\nY_{t}=\\sum_{j=0}^{\\infty} S^{\\prime} A^{j} S e_{t-j} .\n\\]\nBy linearity\n\\[\nb_{j}=\\frac{\\partial}{\\partial e_{t}} Y_{t+j}=S^{\\prime} A^{j} S .\n\\]\nThus the coefficient \\(b_{j}\\) can be calculated by forming the matrix \\(\\boldsymbol{A}\\), its \\(j\\)-fold product \\(\\boldsymbol{A}^{j}\\), and then taking the upper-left element.\nAs mentioned at the beginning of the section it is often desirable to scale the IRF so that it is the response to a one-deviation shock. Let \\(\\sigma^{2}=\\operatorname{var}\\left[e_{t}\\right]\\) and define \\(\\varepsilon_{t}=e_{t} / \\sigma\\) which has unit variance. Then the IRF at lag \\(j\\) is\n\\[\n\\operatorname{IRF}_{j}=\\frac{\\partial}{\\partial \\varepsilon_{t}} Y_{t+j}=\\sigma b_{j} .\n\\]"
  },
  {
    "objectID": "chpt14-time-series.html#arma-and-arima-processes",
    "href": "chpt14-time-series.html#arma-and-arima-processes",
    "title": "14  Time Series",
    "section": "14.27 ARMA and ARIMA Processes",
    "text": "14.27 ARMA and ARIMA Processes\nThe autoregressive-moving-average process, denoted ARMA(p,q), is\n\\[\nY_{t}=\\alpha_{0}+\\alpha_{1} Y_{t-1}+\\alpha_{2} Y_{t-2}+\\cdots+\\alpha_{p} Y_{t-p}+\\theta_{0} e_{t}+\\theta_{1} e_{t-1}+\\theta_{2} e_{t-2}+\\cdots+\\theta_{q} e_{t-q}\n\\]\nwhere \\(e_{t}\\) is a strictly stationary and erogodic white noise process. It can be written using lag operator notation as \\(\\alpha(\\mathrm{L}) Y_{t}=\\alpha_{0}+\\theta(\\mathrm{L}) e_{t}\\).\nTheorem 14.25 The ARMA(p,q) process (14.43) is strictly stationary and ergodic if all roots of \\(\\alpha(z)\\) lie outside the unit circle. In this case we can write\n\\[\nY_{t}=\\mu+b(\\mathrm{~L}) e_{t}\n\\]\nwhere \\(b_{j}=O\\left(j^{p} \\beta^{j}\\right)\\) and \\(\\sum_{j=0}^{\\infty}\\left|b_{j}\\right|<\\infty\\).\nThe process \\(Y_{t}\\) follows an autoregressive-integrated moving-average process, denoted ARIMA(p,d,q), if \\(\\Delta^{d} Y_{t}\\) is ARMA(p,q). It can be written using lag operator notation as \\(\\alpha(\\mathrm{L})(1-\\mathrm{L})^{d} Y_{t}=\\alpha_{0}+\\theta(\\mathrm{L}) e_{t}\\)."
  },
  {
    "objectID": "chpt14-time-series.html#mixing-properties-of-linear-processes",
    "href": "chpt14-time-series.html#mixing-properties-of-linear-processes",
    "title": "14  Time Series",
    "section": "14.28 Mixing Properties of Linear Processes",
    "text": "14.28 Mixing Properties of Linear Processes\nThere is a considerable probability literature investigating the mixing properties of time series processes. One challenge is that as autoregressive processes depend on the infinite past sequence of innovations \\(e_{t}\\) it is not immediately obvious if they satisfy the mixing conditions.\nIn fact, a simple AR(1) is not necessarily mixing. A counter-example was developed by Andrews (1984). He showed that if the error \\(e_{t}\\) has a two-point discrete distribution then an AR(1) is not strong mixing. The reason is that a discrete innovation combined with the autoregressive structure means that by observing \\(Y_{t}\\) you can deduce with near certainty the past history of the shocks \\(e_{t}\\). The example seems rather special but shows the need to be careful with the theory. The intuition stemming from Andrews’ example is that for an autoregressive process to be mixing it is necessary for the errors \\(e_{t}\\) to be continuous.\nA useful characterization was provided by Pham and Tran (1985).\nTheorem 14.26 Suppose that \\(Y_{t}=\\mu+\\sum_{j=0}^{\\infty} \\theta_{j} e_{t-j}\\) satisfies the following conditions:\n\n\\(e_{t}\\) is i.i.d. with \\(\\mathbb{E}\\left|e_{t}\\right|^{r}<\\infty\\) for some \\(r>0\\) and density \\(f(x)\\) which satisfies\n\n\\[\n\\int_{-\\infty}^{\\infty}|f(x-u)-f(x)| d x \\leq C|u|\n\\]\nfor some \\(C<\\infty\\).\n 1. All roots of \\(\\theta(z)=0\\) lie outside the unit circle and \\(\\sum_{j=0}^{\\infty}\\left|\\theta_{j}\\right|<\\infty\\).\n\n\\(\\sum_{k=1}^{\\infty}\\left(\\sum_{j=k}^{\\infty}\\left|\\theta_{j}\\right|\\right)^{r /(1+r)}<\\infty\\).\n\nThen for some \\(B<\\infty\\)\n\\[\n\\alpha(\\ell) \\leq 4 \\beta(\\ell) \\leq B \\sum_{k=\\ell}^{\\infty}\\left(\\sum_{j=k}^{\\infty}\\left|\\theta_{j}\\right|\\right)^{r /(1+r)}\n\\]\nand \\(Y_{t}\\) is absolutely regular and strong mixing.\nThe condition (14.44) is rather unusual, but specifies that \\(e_{t}\\) has a smooth density. This rules out Andrews’ counter-example.\nThe summability condition on the coefficients in part 3 involves a trade-off with the number of moments \\(r\\). If \\(e_{t}\\) has all moments finite (e.g. normal errors) then we can set \\(r=\\infty\\) and this condition simplifies to \\(\\sum_{k=1}^{\\infty} k\\left|\\theta_{k}\\right|<\\infty\\). For any finite \\(r\\) the summability condition holds if \\(\\theta_{j}\\) has geometric decay.\nIt is instructive to deduce how the decay in the coefficients \\(\\theta_{j}\\) affects the rate for the mixing coefficients \\(\\alpha(\\ell)\\). If \\(\\left|\\theta_{j}\\right| \\leq O\\left(j^{-\\eta}\\right)\\) then \\(\\sum_{j=k}^{\\infty}\\left|\\theta_{j}\\right| \\leq O\\left(k^{-(\\eta-1)}\\right)\\) so the rate is \\(\\alpha(\\ell) \\leq 4 \\beta(\\ell) \\leq O\\left(\\ell^{-s}\\right)\\) for \\(s=(\\eta-1) r /(1+r)-1\\). Mixing requires \\(s>0\\), which holds for sufficiently large \\(\\eta\\). For example, if \\(r=4\\) it holds for \\(\\eta>9 / 4\\).\nThe primary message from this section is that linear processes, including autoregressive and ARMA processes, are mixing if the innovations satisfy suitable conditions. The mixing coefficients decay at rates related to the decay rates of the moving average coefficients."
  },
  {
    "objectID": "chpt14-time-series.html#identification",
    "href": "chpt14-time-series.html#identification",
    "title": "14  Time Series",
    "section": "14.29 Identification",
    "text": "14.29 Identification\nThe parameters of a model are identified if the parameters are uniquely determined by the probability distribution of the observations. In the case of linear time series analysis we typically focus on the first two moments of the observations (means, variances, covariances). We therefore say that the coefficients of a stationary MA, AR, or ARMA model are identified if they are uniquely determined by the autocorrelation function. That is, given the autocorrelation function \\(\\rho(k)\\), are the coefficients unique? It turns out that the answer is that MA and ARMA models are generally not identified. Identification is achieved by restricting the class of polynomial operators. In contrast, AR models are generally identified.\nLet us start with the MA(1) model\n\\[\nY_{t}=e_{t}+\\theta e_{t-1} .\n\\]\nIt has first-order autocorrelation\n\\[\n\\rho(1)=\\frac{\\theta}{1+\\theta^{2}} .\n\\]\nSet \\(\\omega=1 / \\theta\\). Then\n\\[\n\\frac{\\omega}{1+\\omega^{2}}=\\frac{1 / \\omega}{1+(1 / \\omega)^{2}}=\\frac{\\theta}{1+\\theta^{2}}=\\rho(1) .\n\\]\nThus the MA(1) model with coefficient \\(\\omega=1 / \\theta\\) produces the same autocorrelations as the MA(1) model with coefficient \\(\\theta\\). For example, \\(\\theta=1 / 2\\) and \\(\\omega=2\\) each yield \\(\\rho(1)=2 / 5\\). There is no empirical way to distinguish between the models \\(Y_{t}=e_{t}+\\theta e_{t-1}\\) and \\(Y_{t}=e_{t}+\\omega e_{t-1}\\). Thus the coefficient \\(\\theta\\) is not identified.\nThe standard solution is to select the parameter which produces an invertible moving average polynomial. Since there is only one such choice this yields a unique solution. This may be sensible when there is reason to believe that shocks have their primary impact in the contemporaneous period and secondary (lesser) impact in the second period.\nNow consider the MA(2) model\n\\[\nY_{t}=e_{t}+\\theta_{1} e_{t-1}+\\theta_{2} e_{t-2} .\n\\]\nThe moving average polynomial can be factored as\n\\[\n\\theta(z)=\\left(1-\\beta_{1} z\\right)\\left(1-\\beta_{2} z\\right)\n\\]\nso that \\(\\beta_{1} \\beta_{2}=\\theta_{2}\\) and \\(\\beta_{1}+\\beta_{2}=-\\theta_{1}\\). The process has first- and second-order autocorrelations\n\\[\n\\begin{aligned}\n&\\rho(1)=\\frac{\\theta_{1}+\\theta_{1} \\theta_{2}}{1+\\theta_{1}^{2}+\\theta_{2}^{2}}=\\frac{-\\beta_{1}-\\beta_{2}-\\beta_{1}^{2} \\beta_{2}-\\beta_{1} \\beta_{2}^{2}}{1+\\beta_{1}^{2}+\\beta_{2}^{2}+2 \\beta_{1} \\beta_{2}+\\beta_{1}^{2} \\beta_{2}^{2}} \\\\\n&\\rho(2)=\\frac{\\theta_{2}}{1+\\theta_{1}^{2}+\\theta_{2}^{2}}=\\frac{\\beta_{1} \\beta_{2}}{1+\\beta_{1}^{2}+\\beta_{2}^{2}+2 \\beta_{1} \\beta_{2}+\\beta_{1}^{2} \\beta_{2}^{2}} .\n\\end{aligned}\n\\]\nIf we replace \\(\\beta_{1}\\) with \\(\\omega_{1}=1 / \\beta_{1}\\) we obtain\n\\[\n\\begin{aligned}\n&\\rho(1)=\\frac{-1 / \\beta_{1}-\\beta_{2}-\\beta_{2} / \\beta_{1}^{2}-\\beta_{2}^{2} / \\beta_{1}}{1+1 / \\beta_{1}^{2}+\\beta_{2}^{2}+2 \\beta_{2} / \\beta_{1}+\\beta_{2}^{2} / \\beta_{1}^{2}}=\\frac{-\\beta_{1}-\\beta_{2} \\beta_{1}^{2}-\\beta_{2}-\\beta_{2}^{2} \\beta_{1}}{\\beta_{1}^{2}+1+\\beta_{2}^{2} \\beta_{1}^{2}+2 \\beta_{2} \\beta_{1}+\\beta_{2}^{2}} \\\\\n&\\rho(2)=\\frac{\\beta_{2} / \\beta_{1}}{1+1 / \\beta_{1}^{2}+\\beta_{2}^{2}+2 \\beta_{2} / \\beta_{1}+\\beta_{2}^{2} / \\beta_{1}^{2}}=\\frac{\\beta_{1} \\beta_{2}}{\\beta_{1}^{2}+1+\\beta_{1}^{2} \\beta_{2}^{2}+2 \\beta_{1} \\beta_{2}+\\beta_{2}^{2}}\n\\end{aligned}\n\\]\nwhich is unchanged. Similarly if we replace \\(\\beta_{2}\\) with \\(\\omega_{2}=1 / \\beta_{2}\\) we obtain unchanged first- and secondorder autocorrelations. It follows that in the MA(2) model the factors \\(\\beta_{1}\\) and \\(\\beta_{2}\\) nor the coefficients \\(\\theta_{1}\\) and \\(\\theta_{2}\\) are identified. Consequently there are four distinct \\(\\mathrm{MA}(2)\\) models which are identifiably indistinguishable.\nThis analysis extends to the MA(q) model. The factors of the MA polynomial can be replaced by their inverses and consequently the coefficients are not identified.\nThe standard solution is to confine attention to MA(q) models with invertible roots. This technically solves the identification dilemma. This solution corresponds to the Wold decomposition, as it is defined in terms of the projection errors which correspond to the invertible representation.\nA deeper identification failure occurs in ARMA models. Consider an ARMA(1,1) model\n\\[\nY_{t}=\\alpha Y_{t-1}+e_{t}+\\theta e_{t-1} .\n\\]\nWritten in lag operator notation\n\\[\n(1-\\alpha \\mathrm{L}) Y_{t}=(1+\\theta \\mathrm{L}) e_{t} .\n\\]\nThe identification failure is that when \\(\\alpha=-\\theta\\) then the model simplifies to \\(Y_{t}=e_{t}\\). This means that the continuum of models with \\(\\alpha=-\\theta\\) are all identical and the coefficients are not identified.\nThis extends to higher order ARMA models. Take the ARMA \\((2,2)\\) model written in factored lag operator notation\n\\[\n\\left(1-\\alpha_{1} \\mathrm{~L}\\right)\\left(1-\\alpha_{2} \\mathrm{~L}\\right) Y_{t}=\\left(1+\\theta_{1} \\mathrm{~L}\\right)\\left(1+\\theta_{2} \\mathrm{~L}\\right) e_{t} .\n\\]\nThe models with \\(\\alpha_{1}=-\\theta_{1}, \\alpha_{1}=-\\theta_{2}, \\alpha_{2}=-\\theta_{1}\\), or \\(\\alpha_{2}=-\\theta_{2}\\) all simplify to an ARMA(1,1). Thus all these models are identical and hence the coefficients are not identified.\nThe problem is called “cancelling roots” due to the fact that it arises when there are two identical lag polynomial factors in the AR and MA polynomials.\nThe standard solution in the ARMA literature is to assume that there are no cancelling roots. The trouble with this solution is that this is an assumption about the true process which is unknown. Thus it is not really a solution to the identification problem. One recommendation is to be careful when using ARMA models and be aware that highly parameterized models may not have unique coefficients.\nNow consider the \\(\\operatorname{AR}(\\mathrm{p})\\) model (14.38). It can be written as\n\\[\nY_{t}=X_{t}^{\\prime} \\alpha+e_{t}\n\\]\nwhere \\(\\alpha=\\left(\\alpha_{0}, \\alpha_{1}, \\ldots \\alpha_{p}\\right)^{\\prime}\\) and \\(X_{t}=\\left(1, Y_{t-1}, \\ldots, Y_{t-p}\\right)^{\\prime}\\). The MDS assumption implies that \\(\\mathbb{E}\\left[e_{t}\\right]=0\\) and \\(\\mathbb{E}\\left[X_{t} e_{t}\\right]=0\\). This means that the coefficient \\(\\alpha\\) satisfies\n\\[\n\\alpha=\\left(\\mathbb{E}\\left[X_{t} X_{t}^{\\prime}\\right]\\right)^{-1}\\left(\\mathbb{E}\\left[X_{t} Y_{t}\\right]\\right) .\n\\]\nThis equation is unique if \\(\\boldsymbol{Q}=\\mathbb{E}\\left[X_{t} X_{t}^{\\prime}\\right]\\) is positive definite. It turns out that this is generically true so \\(\\alpha\\) is unique and identified.\nTheorem 14.27 In the AR(p) model (14.38), if \\(0<\\sigma^{2}<\\infty\\) then \\(\\boldsymbol{Q}>0\\) and \\(\\alpha\\) is unique and identified.\nThe assumption \\(\\sigma^{2}>0\\) means that \\(Y_{t}\\) is not purely deterministic.\nWe can extend this result to approximating \\(\\operatorname{AR}(\\mathrm{p})\\) models. That is, consider the equation (14.45) without the assumption that \\(Y_{t}\\) is necessarily a true AR(p) with a MDS error. Instead, suppose that \\(Y_{t}\\) is a non-deterministic stationary process. (Recall, non-deterministic means that \\(\\sigma^{2}>0\\) where \\(\\sigma^{2}\\) is the projection error variance (14.19).) We then define the coefficient \\(\\alpha\\) as the best linear predictor, which is (14.46). The error \\(e_{t}\\) is defined by the equation (14.45). This is a linear projection model.\nAs in the case of any linear projection, the error \\(e_{t}\\) satisfies \\(\\mathbb{E}\\left[X_{t} e_{t}\\right]=0\\). This means that \\(\\mathbb{E}\\left[e_{t}\\right]=0\\) and \\(\\mathbb{E}\\left[Y_{t-j} e_{t}\\right]=0\\) for \\(j=1, \\ldots, p\\). However, the error \\(e_{t}\\) is not necessarily a MDS nor white noise.\nThe coefficient \\(\\alpha\\) is identified if \\(\\boldsymbol{Q}>0\\). The proof of Theorem \\(14.27\\) (presented in Section 14.47) does not make use of the assumption that \\(Y_{t}\\) is an \\(\\operatorname{AR}(\\mathrm{p})\\) with a MDS error. Rather, it only uses the assumption that \\(\\sigma^{2}>0\\). This holds in the approximate \\(\\operatorname{AR}(\\mathrm{p})\\) model as well under the assumption that \\(Y_{t}\\) is nondeterministic. We conclude that any approximating AR(p) is identified.\nTheorem 14.28 If \\(Y_{t}\\) is strictly stationary, not purely deterministic, and \\(\\mathbb{E}\\left[Y_{t}^{2}\\right]<\\infty\\), then for any \\(p, \\boldsymbol{Q}=\\mathbb{E}\\left[X_{t} X_{t}^{\\prime}\\right]>0\\) and thus the coefficient vector (14.46) is identified."
  },
  {
    "objectID": "chpt14-time-series.html#estimation-of-autoregressive-models",
    "href": "chpt14-time-series.html#estimation-of-autoregressive-models",
    "title": "14  Time Series",
    "section": "14.30 Estimation of Autoregressive Models",
    "text": "14.30 Estimation of Autoregressive Models\nWe consider estimation of an \\(\\mathrm{AR}(\\mathrm{p})\\) model for stationary, ergodic, and non-deterministic \\(Y_{t}\\). The model is (14.45) where \\(X_{t}=\\left(1, Y_{t-1}, \\ldots, Y_{t-p}\\right)^{\\prime}\\). The coefficient \\(\\alpha\\) is defined by projection in (14.46). The error is defined by (14.45) and has variance \\(\\sigma^{2}=\\mathbb{E}\\left[e_{t}^{2}\\right]\\). This allows \\(Y_{t}\\) to follow a true AR(p) process but it is not necessary.\nThe least squares estimator is\n\\[\n\\widehat{\\alpha}=\\left(\\sum_{t=1}^{n} X_{t} X_{t}^{\\prime}\\right)^{-1}\\left(\\sum_{t=1}^{n} X_{t} Y_{t}\\right) .\n\\]\nThis notation presumes that there are \\(n+p\\) total observations on \\(Y_{t}\\) from which the first \\(p\\) are used as initial conditions so that \\(X_{1}=\\left(1, Y_{0}, Y_{-1}, \\ldots, Y_{-p+1}\\right)\\) is defined. Effectively, this redefines the sample period. (An alternative notational choice is to define the periods so the sums range from observations \\(p+1\\) to \\(n\\).)\nThe least squares residuals are \\(\\widehat{e}_{t}=Y_{t}-X_{t}^{\\prime} \\widehat{\\alpha}\\). The error variance can be estimated by \\(\\widehat{\\sigma}^{2}=n^{-1} \\sum_{t=1}^{n} \\widehat{e}_{t}^{2}\\) or \\(s^{2}=(n-p-1)^{-1} \\sum_{t=1}^{n} \\widehat{e}_{t}^{2}\\).\nIf \\(Y_{t}\\) is strictly stationary and ergodic then so are \\(X_{t} X_{t}^{\\prime}\\) and \\(X_{t} Y_{t}\\). They have finite means if \\(\\mathbb{E}\\left[Y_{t}^{2}\\right]<\\infty\\). Under these assumptions the Ergodic Theorem implies that\n\\[\n\\frac{1}{n} \\sum_{t=1}^{n} X_{t} Y_{t} \\underset{p}{\\longrightarrow} \\mathbb{E}\\left[X_{t} Y_{t}\\right]\n\\]\nand\n\\[\n\\frac{1}{n} \\sum_{t=1}^{n} X_{t} X_{t}^{\\prime} \\underset{p}{\\longrightarrow} \\mathbb{E}\\left[X_{t} X_{t}^{\\prime}\\right]=\\boldsymbol{Q} .\n\\]\nTheorem \\(14.28\\) shows that \\(\\boldsymbol{Q}>0\\). Combined with the continuous mapping theorem we see that\n\\[\n\\widehat{\\alpha}=\\left(\\frac{1}{n} \\sum_{t=1}^{n} X_{t} X_{t}^{\\prime}\\right)^{-1}\\left(\\frac{1}{n} \\sum_{t=1}^{n} X_{t} Y_{t}\\right) \\underset{p}{\\longrightarrow}\\left(\\mathbb{E}\\left[X_{t} X_{t}^{\\prime}\\right]\\right)^{-1} \\mathbb{E}\\left[X_{t} Y_{t}\\right]=\\alpha .\n\\]\nIt is straightforward to show that \\(\\widehat{\\sigma}^{2}\\) is consistent as well.\nTheorem 14.29 If \\(Y_{t}\\) is strictly stationary, ergodic, not purely deterministic, and \\(\\mathbb{E}\\left[Y_{t}^{2}\\right]<\\infty\\), then for any \\(p, \\widehat{\\alpha} \\underset{p}{\\longrightarrow} \\alpha\\) and \\(\\widehat{\\sigma}^{2} \\underset{p}{\\longrightarrow} \\sigma^{2}\\) as \\(n \\rightarrow \\infty\\).\nThis shows that under very mild conditions the coefficients of an AR(p) model can be consistently estimated by least squares. Once again, this does not require that the series \\(Y_{t}\\) is actually an \\(\\mathrm{AR}(\\mathrm{p})\\) process. It holds for any stationary process with the coefficient defined by projection."
  },
  {
    "objectID": "chpt14-time-series.html#asymptotic-distribution-of-least-squares-estimator",
    "href": "chpt14-time-series.html#asymptotic-distribution-of-least-squares-estimator",
    "title": "14  Time Series",
    "section": "14.31 Asymptotic Distribution of Least Squares Estimator",
    "text": "14.31 Asymptotic Distribution of Least Squares Estimator\nThe asymptotic distribution of the least squares estimator \\(\\widehat{\\alpha}\\) depends on the stochastic assumptions. In this section we derive the asymptotic distribution under the assumption of correct specification.\nSpecifically, we assume that the error \\(e_{t}\\) is a MDS. An important implication of the MDS assumption is that since \\(X_{t}=\\left(1, Y_{t-1}, \\ldots, Y_{t-p}\\right)^{\\prime}\\) is part of the information set \\(\\mathscr{F}_{t-1}\\), by the conditioning theorem,\n\\[\n\\mathbb{E}\\left[X_{t} e_{t} \\mid \\mathscr{F}_{t-1}\\right]=X_{t} \\mathbb{E}\\left[e_{t} \\mid \\mathscr{F}_{t-1}\\right]=0 .\n\\]\nThus \\(X_{t} e_{t}\\) is a MDS. It has a finite variance if \\(e_{t}\\) has a finite fourth moment. To see this, by Theorem \\(14.24, Y_{t}=\\mu+\\sum_{j=0}^{\\infty} b_{j} e_{t-j}\\) with \\(\\sum_{j=0}^{\\infty}\\left|b_{j}\\right|<\\infty\\). Using Minkowski’s Inequality,\n\\[\n\\left(\\mathbb{E}\\left|Y_{t}\\right|^{4}\\right)^{1 / 4} \\leq \\sum_{j=0}^{\\infty}\\left|b_{j}\\right|\\left(\\mathbb{E}\\left|e_{t-j}\\right|^{4}\\right)^{1 / 4}<\\infty .\n\\]\nThus \\(\\mathbb{E}\\left[Y_{t}^{4}\\right]<\\infty\\). The Cauchy-Schwarz inequality then shows that \\(\\mathbb{E}\\left\\|X_{t} e_{t}\\right\\|^{2}<\\infty\\). We can then apply the martingale difference CLT (Theorem 14.11) to see that\n\\[\n\\frac{1}{\\sqrt{n}} \\sum_{t=1}^{n} X_{t} e_{t} \\underset{d}{\\longrightarrow} \\mathrm{N}(0, \\Sigma)\n\\]\nwhere \\(\\Sigma=\\mathbb{E}\\left[X_{t} X_{t}^{\\prime} e_{t}^{2}\\right]\\)\nTheorem 14.30 If \\(Y_{t}\\) follows the AR(p) model (14.38), all roots of \\(a(z)\\) lie outside the unit circle, \\(\\mathbb{E}\\left[e_{t} \\mid \\mathscr{F}_{t-1}\\right]=0, \\mathbb{E}\\left[e_{t}^{4}\\right]<\\infty\\), and \\(\\mathbb{E}\\left[e_{t}^{2}\\right]>0\\), then as \\(n \\rightarrow \\infty\\), \\(\\sqrt{n}(\\widehat{\\alpha}-\\alpha) \\underset{d}{\\rightarrow} \\mathrm{N}(0, \\boldsymbol{V})\\) where \\(\\boldsymbol{V}=\\boldsymbol{Q}^{-1} \\Sigma \\boldsymbol{Q}^{-1}\\).\nThis is identical in form to the asymptotic distribution of least squares in cross-section regression. The implication is that asymptotic inference is the same. In particular, the asymptotic covariance matrix is estimated just as in the cross-section case."
  },
  {
    "objectID": "chpt14-time-series.html#distribution-under-homoskedasticity",
    "href": "chpt14-time-series.html#distribution-under-homoskedasticity",
    "title": "14  Time Series",
    "section": "14.32 Distribution Under Homoskedasticity",
    "text": "14.32 Distribution Under Homoskedasticity\nIn cross-section regression we found that the covariance matrix simplifies under the assumption of conditional homoskedasticity. The same occurs in the time series context. Assume that the error is a homoskedastic MDS:\n\\[\n\\begin{aligned}\n\\mathbb{E}\\left[e_{t} \\mid \\mathscr{F}_{t-1}\\right] &=0 \\\\\n\\mathbb{E}\\left[e_{t}^{2} \\mid \\mathscr{F}_{t-1}\\right] &=\\sigma^{2} .\n\\end{aligned}\n\\]\nIn this case\n\\[\n\\Sigma=\\mathbb{E}\\left[X_{t} X_{t}^{\\prime} \\mathbb{E}\\left[e_{t}^{2} \\mid \\mathscr{F}_{t-1}\\right]\\right]=\\boldsymbol{Q} \\sigma^{2}\n\\]\nand the asymptotic distribution simplifies.\nTheorem 14.31 Under the assumptions of Theorem 14.30, if in addition \\(\\mathbb{E}\\left[e_{t}^{2} \\mid \\mathscr{F}_{t-1}\\right]=\\sigma^{2}\\), then as \\(n \\rightarrow \\infty, \\sqrt{n}(\\widehat{\\alpha}-\\alpha) \\underset{d}{\\longrightarrow} \\mathrm{N}\\left(0, \\boldsymbol{V}^{0}\\right)\\) where \\(\\boldsymbol{V}^{0}=\\sigma^{2} \\boldsymbol{Q}^{-1}\\).\nThese results show that under correct specification (a MDS error) the format of the asymptotic distribution of the least squares estimator exactly parallels the cross-section case. In general the covariance matrix takes a sandwich form with components exactly equal to the cross-section case. Under conditional homoskedasticity the covariance matrix simplies exactly as in the cross-section case. A particularly useful insight which can be derived from Theorem \\(14.31\\) is to focus on the simple AR(1) with no intercept. In this case \\(Q=\\mathbb{E}\\left[Y_{t}^{2}\\right]=\\sigma^{2} /\\left(1-\\alpha_{1}^{2}\\right)\\) so the asymptotic distribution simplifies to\n\\[\n\\sqrt{n}\\left(\\widehat{\\alpha}_{1}-\\alpha_{1}\\right) \\underset{d}{\\longrightarrow} \\mathrm{N}\\left(0,1-\\alpha_{1}^{2}\\right) .\n\\]\nThus the asymptotic variance depends only on \\(\\alpha_{1}\\) and is decreasing with \\(\\alpha_{1}^{2}\\). An intuition is that larger \\(\\alpha_{1}^{2}\\) means greater signal and hence greater estimation precision. This result also shows that the asymptotic distribution is non-similar: the variance is a function of the parameter of interest. This means that we can expect (from advanced statistical theory) asymptotic inference to be less accurate than indicated by nominal levels.\nIn the context of cross-section data we argued that the homoskedasticity assumption was dubious except for occassional theoretical insight. For practical applications it is recommended to use heteroskedasticityrobust theory and methods when possible. The same argument applies to the time series case. While the distribution theory simplifies under conditional homoskedasticity there is no reason to expect homoskedasticity to hold in practice. Therefore in applications it is better to use the heteroskedasticityrobust distributional theory when possible.\nUnfortunately, many existing time series textbooks report the distribution theory from (14.31). This has influenced computer software packages many of which also by default (or exclusively) use the homoskedastic distribution theory. This is unfortunate."
  },
  {
    "objectID": "chpt14-time-series.html#asymptotic-distribution-under-general-dependence",
    "href": "chpt14-time-series.html#asymptotic-distribution-under-general-dependence",
    "title": "14  Time Series",
    "section": "14.33 Asymptotic Distribution Under General Dependence",
    "text": "14.33 Asymptotic Distribution Under General Dependence\nIf the \\(\\mathrm{AR}(\\mathrm{p})\\) model (14.38) holds with white noise errors or if the \\(\\mathrm{AR}(\\mathrm{p})\\) is an approximation with \\(\\alpha\\) defined as the best linear predictor then the MDS central limit theory does not apply. Instead, if \\(Y_{t}\\) is strong mixing we can use the central limit theory for mixing processes (Theorem 14.15).\nTheorem 14.32 Assume that \\(Y_{t}\\) is strictly stationary, ergodic, and for some \\(r>\\) \\(4, \\mathbb{E}\\left|Y_{t}\\right|^{r}<\\infty\\) and the mixing coefficients satisfy \\(\\sum_{\\ell=1}^{\\infty} \\alpha(\\ell)^{1-4 / r}<\\infty\\). Let \\(\\alpha\\) be defined as the best linear projection coefficients (14.46) from an AR(p) model with projection errors \\(e_{t}\\). Let \\(\\widehat{\\alpha}\\) be the least squares estimator of \\(\\alpha\\). Then\n\\[\n\\Omega=\\sum_{\\ell=-\\infty}^{\\infty} \\mathbb{E}\\left[X_{t-\\ell} X_{t}^{\\prime} e_{t} e_{t-\\ell}\\right]\n\\]\nis convergent and \\(\\sqrt{n}(\\widehat{\\alpha}-\\alpha) \\underset{d}{\\longrightarrow} \\mathrm{N}(0, \\boldsymbol{V})\\) as \\(n \\rightarrow \\infty\\), where \\(\\boldsymbol{V}=\\boldsymbol{Q}^{-1} \\Omega \\boldsymbol{Q}^{-1}\\).\nThis result is substantially different from the cross-section case. It shows that model misspecification (including misspecifying the order of the autoregression) renders invalid the conventional “heteroskedasticityrobust” covariance matrix formula. Misspecified models do not have unforecastable (martingale difference) errors so the regression scores \\(X_{t} e_{t}\\) are potentially serially correlated. The asymptotic variance takes a sandwich form with the central component \\(\\Omega\\) the long-run variance (recall Section 14.13) of the regression scores \\(X_{t} e_{t}\\)."
  },
  {
    "objectID": "chpt14-time-series.html#covariance-matrix-estimation",
    "href": "chpt14-time-series.html#covariance-matrix-estimation",
    "title": "14  Time Series",
    "section": "14.34 Covariance Matrix Estimation",
    "text": "14.34 Covariance Matrix Estimation\nUnder the assumption of correct specification covariance matrix estimation is identical to the crosssection case. The asymptotic covariance matrix estimator under homoskedasticity is\n\\[\n\\begin{aligned}\n\\widehat{\\boldsymbol{V}}^{0} &=\\widehat{\\sigma}^{2} \\widehat{\\boldsymbol{Q}}^{-1} \\\\\n\\widehat{\\boldsymbol{Q}} &=\\frac{1}{n} \\sum_{t=1}^{n} X_{t} X_{t}^{\\prime}\n\\end{aligned}\n\\]\nThe estimator \\(s^{2}\\) may be used instead of \\(\\widehat{\\sigma}^{2}\\).\nThe heteroskedasticity-robust asymptotic covariance matrix estimator is\n\\[\n\\widehat{\\boldsymbol{V}}=\\widehat{\\boldsymbol{Q}}^{-1} \\widehat{\\Sigma} \\widehat{\\boldsymbol{Q}}^{-1}\n\\]\nwhere\n\\[\n\\widehat{\\Sigma}=\\frac{1}{n} \\sum_{t=1}^{n} X_{t} X_{t}^{\\prime} \\widehat{e}_{t}^{2} .\n\\]\nDegree-of-freedom adjustments may be made as in the cross-section case though a theoretical justification has not been developed.\nStandard errors \\(s\\left(\\widehat{\\alpha}_{j}\\right)\\) for individual coefficient estimates can be formed by taking the scaled diagonal elements of \\(\\widehat{\\boldsymbol{V}}\\)\nTheorem 14.33 Under the assumptions of Theorem 14.32, as \\(n \\rightarrow \\infty, \\widehat{\\boldsymbol{V}} \\underset{p}{\\rightarrow} \\boldsymbol{V}\\) and \\(\\left(\\widehat{\\alpha}_{j}-\\alpha_{j}\\right) / s\\left(\\widehat{\\alpha}_{j}\\right) \\longrightarrow \\underset{d}{\\longrightarrow} \\mathrm{N}(0,1)\\)\nTheorem \\(14.33\\) shows that standard covariance matrix estimation is consistent and the resulting tratios are asymptotically normal. This means that for stationary autoregressions, inference can proceed using conventional regression methods."
  },
  {
    "objectID": "chpt14-time-series.html#covariance-matrix-estimation-under-general-dependence",
    "href": "chpt14-time-series.html#covariance-matrix-estimation-under-general-dependence",
    "title": "14  Time Series",
    "section": "14.35 Covariance Matrix Estimation Under General Dependence",
    "text": "14.35 Covariance Matrix Estimation Under General Dependence\nUnder the assumptions of Theorem \\(14.32\\) the conventional covariance matrix estimators are inconsistent as they do not capture the serial dependence in the regression scores \\(X_{t} e_{t}\\). To consistently estimate the covariance matrix we need an estimator of the long-run variance \\(\\Omega\\). The appropriate class of estimators are called Heteroskedasticity and Autocorrelation Consistent (HAC) or Heteroskedasticity and Autocorrelation Robust (HAR) covariance matrix estimators.\nTo understand the methods it is helpful to define the vector series \\(u_{t}=X_{t} e_{t}\\) and autocovariance matrices \\(\\Gamma(\\ell)=\\mathbb{E}\\left[u_{t-\\ell} u_{t}^{\\prime}\\right]\\) so that\n\\[\n\\Omega=\\sum_{\\ell=-\\infty}^{\\infty} \\Gamma(\\ell) .\n\\]\nSince this sum is convergent the autocovariance matrices converge to zero as \\(\\ell \\rightarrow \\infty\\). Therefore \\(\\Omega\\) can be approximated by taking a finite sum of autocovariances such as\n\\[\n\\Omega_{M}=\\sum_{\\ell=-M}^{M} \\Gamma(\\ell) .\n\\]\nThe number \\(M\\) is sometimes called the lag truncation number. Other authors call it the bandwidth. An estimator of \\(\\Gamma(\\ell)\\) is\n\\[\n\\widehat{\\Gamma}(\\ell)=\\frac{1}{n} \\sum_{1 \\leq t-\\ell \\leq n} \\widehat{u}_{t-\\ell} \\widehat{u}_{t}^{\\prime}\n\\]\nwhere \\(\\widehat{u}_{t}=X_{t} \\widehat{e}_{t}\\). By the ergodic theorem we can show that for any \\(\\ell, \\widehat{\\Gamma}(\\ell) \\underset{p}{\\longrightarrow} \\Gamma(\\ell)\\). Thus for any fixed \\(M\\), the estimator\n\\[\n\\widehat{\\Omega}_{M}=\\sum_{\\ell=-M}^{M} \\widehat{\\Gamma}(\\ell)\n\\]\nis consistent for \\(\\Omega_{M}\\).\nIf the serial correlation in \\(X_{t} e_{t}\\) is known to be zero after \\(M\\) lags, then \\(\\Omega_{M}=\\Omega\\) and the estimator (14.49) is consistent for \\(\\Omega\\). This estimator was proposed by L. Hansen and Hodrick (1980) in the context of multiperiod forecasts and by L. Hansen (1982) for the generalized method of moments.\nIn the general case we can select \\(M\\) to increase with sample size \\(n\\). If the rate at which \\(M\\) increases is sufficiently slow then \\(\\widehat{\\Omega}_{M}\\) will be consistent for \\(\\Omega\\), as first shown by White and Domowitz (1984).\nOnce we view the lag truncation number \\(M\\) as a choice the estimator (14.49) has two potential deficiencies. One is that \\(\\widehat{\\Omega}_{M}\\) can change non-smoothly with \\(M\\) which makes estimation results sensitive to the choice of \\(M\\). The other is that \\(\\widehat{\\Omega}_{M}\\) may not be positive semi-definite and is therefore not a valid covariance matrix estimator. We can see this in the simple case of scalar \\(u_{t}\\) and \\(M=1\\). In this case \\(\\widehat{\\Omega}_{1}=\\widehat{\\gamma}(0)(1+2 \\widehat{\\rho}(1))\\) which is negative when \\(\\widehat{\\rho}(1)<-1 / 2\\). Thus if the data are strongly negatively autocorrelated the variance estimator can be negative. A negative variance estimator means that standard errors are ill-defined (a naïve computation will produce a complex standard error which makes no sense \\({ }^{6}\\) ).\nThese two deficiencies can be resolved if we amend (14.49) by a weighted sum of autocovariances. Newey and West (1987b) proposed\n\\[\n\\widehat{\\Omega}_{\\mathrm{nW}}=\\sum_{\\ell=-M}^{M}\\left(1-\\frac{|\\ell|}{M+1}\\right) \\widehat{\\Gamma}(\\ell)\n\\]\nThis is a weighted sum of the autocovariances. Other weight functions can be used; the one in (14.50) is known as the Bartlett kernel \\({ }^{7}\\). Newey and West (1987b) showed that this estimator has the algebraic property that \\(\\widehat{\\Omega}_{\\mathrm{nw}} \\geq 0\\) (it is positive semi-definite), solving the negative variance problem, and it is also a smooth function of \\(M\\). Thus this estimator solves the two problems described above.\nFor \\(\\widehat{\\Omega}_{n w}\\) to be consistent for \\(\\Omega\\) the lag trunction number \\(M\\) must increase to infinity with \\(n\\). Sufficient conditions were established by B. E. Hansen (1992).\nTheorem 14.34 Under the assumptions of Theorem \\(14.32\\) plus \\(\\sum_{\\ell=1}^{\\infty} \\alpha(\\ell)^{1 / 2-4 / r}<\\infty\\), if \\(M \\rightarrow \\infty\\) yet \\(M^{3} / n=O(1)\\), then as \\(n \\rightarrow \\infty, \\widehat{\\Omega}_{\\mathrm{nw}} \\underset{p}{\\rightarrow} \\Omega\\)\nThe assumption \\(M^{3} / n=O(1)\\) technically means that \\(M\\) grows no faster than \\(n^{1 / 3}\\) but this does not have a practical counterpart other than the implication that ” \\(M\\) should be much smaller than \\(n\\) “. The assumption on the mixing coefficients is slightly stronger than in Theorem 14.32, due to the technical nature of the derivation.\n\\({ }^{6}\\) A common computational mishap is a complex standard error. This occurs when a covariance matrix estimator has negative elements on the diagonal.\n\\({ }^{7}\\) See Andrews (1991b) for a description of popular options. In practice, the choice of weight function is much less important than the choice of lag truncation number \\(M\\). A important practical issue is how to select \\(M\\). One way to think about it is that \\(M\\) impacts the precision of the estimator \\(\\widehat{\\Omega}_{\\mathrm{nw}}\\) through its bias and variance. Since \\(\\widehat{\\Gamma}(\\ell)\\) is a sample average its variance is \\(O(1 / n)\\) so we expect the variance of \\(\\widehat{\\Omega}_{M}\\) to be of order \\(O(M / n)\\). The bias of \\(\\widehat{\\Omega}_{\\mathrm{nw}}\\) for \\(\\Omega\\) is harder to calculate but depends on the rate at which the covariances \\(\\Gamma(\\ell)\\) decay to zero. Andrews (1991b) found that the \\(M\\) which minimizes the mean squared error of \\(\\widehat{\\Omega}_{\\mathrm{nw}}\\) satisfies the rate \\(M=C n^{1 / 3}\\) where the constant \\(C\\) depends on the autocovariances. Practical rules to estimate and implement this optimal lag truncation parameter have been proposed by Andrews (1991b) and Newey and West (1994). Andrews’ rule for the Newey-West estimator (14.50) can be written as\n\\[\nM=\\left(6 \\frac{\\rho^{2}}{\\left(1-\\rho^{2}\\right)^{2}}\\right)^{1 / 3} n^{1 / 3}\n\\]\nwhere \\(\\rho\\) is a serial correlation parameter. When \\(u_{t}\\) is scalar, \\(\\rho\\) is the first autocorrelation of \\(u_{t}\\). Andrews suggested using an estimator of \\(\\rho\\) to plug into this formula to find \\(M\\). An alternative is to use a default value of \\(\\rho\\). For example, if we set \\(\\rho=0.5\\) then the Andrews rule is \\(M=1.4 n^{1 / 3}\\), which is a useful benchmark."
  },
  {
    "objectID": "chpt14-time-series.html#testing-the-hypothesis-of-no-serial-correlation",
    "href": "chpt14-time-series.html#testing-the-hypothesis-of-no-serial-correlation",
    "title": "14  Time Series",
    "section": "14.36 Testing the Hypothesis of No Serial Correlation",
    "text": "14.36 Testing the Hypothesis of No Serial Correlation\nIn some cases it may be of interest to test the hypothesis that the series \\(Y_{t}\\) is serially uncorrelated against the alternative that it is serially correlated. There have been many proposed tests of this hypothesis. The most appropriate is based on the least squares regression of an AR(p) model. Take the model\n\\[\nY_{t}=\\alpha_{0}+\\alpha_{1} Y_{t-1}+\\alpha_{2} Y_{t-2}+\\cdots+\\alpha_{p} Y_{t-p}+e_{t}\n\\]\nwith \\(e_{t}\\) a MDS. In this model the series \\(Y_{t}\\) is serially uncorrelated if the slope coefficients are all zero. Thus the hypothesis of interest is\n\\[\n\\begin{aligned}\n&\\mathbb{H}_{0}: \\alpha_{1}=\\cdots=\\alpha_{p}=0 \\\\\n&\\mathbb{H}_{1}: \\alpha_{j} \\neq 0 \\text { for some } j \\geq 1 .\n\\end{aligned}\n\\]\nThe test can be implemented by a Wald or F test. Estimate the AR(p) model by least squares. Form the Wald or F statistic using the variance estimator (14.48). (The Newey-West estimator should not be used as there is no serial correlation under the null hypothesis.) Accept the hypothesis if the test statistic is smaller than a conventional critical value (or if the p-value exceeds the significance level) and reject the hypothesis otherwise.\nImplementation of this test requires a choice of autoregressive order \\(p\\). This choice affects the power of the test. A sufficient number of lags should be included so to pick up potential serial correlation patterns but not so many that the power of the test is diluted. A reasonable choice in many applications is to set \\(p\\) to equals \\(s\\), the seasonal periodicity. Thus include four lags for quarterly data or twelve lags for monthly data."
  },
  {
    "objectID": "chpt14-time-series.html#testing-for-omitted-serial-correlation",
    "href": "chpt14-time-series.html#testing-for-omitted-serial-correlation",
    "title": "14  Time Series",
    "section": "14.37 Testing for Omitted Serial Correlation",
    "text": "14.37 Testing for Omitted Serial Correlation\nWhen using an AR(p) model it may be of interest to know if there is any remaining serial correlation. This can be expressed as a test for serial correlation in the error or equivalently as a test for a higher-order autogressive model. Take the \\(\\operatorname{AR}(\\mathrm{p})\\) model\n\\[\nY_{t}=\\alpha_{0}+\\alpha_{1} Y_{t-1}+\\alpha_{2} Y_{t-2}+\\cdots+\\alpha_{p} Y_{t-p}+u_{t} .\n\\]\nThe null hypothesis is that \\(u_{t}\\) is serially uncorrelated and the alternative hypothesis is that it is serially correlated. We can model the latter as a mean-zero autoregressive process\n\\[\nu_{t}=\\theta_{1} u_{t-1}+\\cdots+\\theta_{q} u_{t-q}+e_{t} .\n\\]\nThe hypothesis is\n\\[\n\\begin{aligned}\n&\\mathbb{H}_{0}: \\theta_{1}=\\cdots=\\theta_{q}=0 \\\\\n&\\mathbb{H}_{1}: \\theta_{j} \\neq 0 \\text { for some } j \\geq 1 .\n\\end{aligned}\n\\]\nA seemingly natural test for \\(\\mathbb{H}_{0}\\) uses a two-step method. First estimate (14.52) by least squares and obtain the residuals \\(\\widehat{u}_{t}\\). Second, estimate (14.53) by least squares by regressing \\(\\widehat{u}_{t}\\) on its lagged values and obtain the Wald or \\(F\\) test for \\(\\mathbb{M}_{0}\\). This seems like a natural approach but it is muddled by the fact that the distribution of the Wald statistic is distorted by the two-step procedure. The Wald statistic is not asymptotically chi-square so it is inappropriate to make a decision based on the conventional critical values. One approach to obtain the correct asymptotic distribution is to use the generalized method of moments, treating (14.52)-(14.53) as a two-equation just-identified system.\nAn easier solution is to re-write (14.52)-(14.53) as a higher-order autoregression so that we can use a standard test statistic. To illustrate how this works take the case \\(q=1\\). Take (14.52) and lag the equation once:\n\\[\nY_{t-1}=\\alpha_{0}+\\alpha_{1} Y_{t-2}+\\alpha_{2} Y_{t-3}+\\cdots+\\alpha_{p} Y_{t-p-1}+u_{t-1} .\n\\]\nMultiply this by \\(\\theta_{1}\\) and subtract from (14.52) to find\n\\[\n\\begin{aligned}\nY_{t}-\\theta_{1} Y_{t-1} &=\\alpha_{0}+\\alpha_{1} Y_{t-1}+\\alpha_{2} Y_{t-2}+\\cdots+\\alpha_{p} Y_{t-p}+u_{t} \\\\\n&-\\theta_{1} \\alpha_{0}-\\theta_{1} \\alpha_{1} Y_{t-2}-\\theta_{1} \\alpha_{2} Y_{t-3}-\\cdots-\\theta_{1} \\alpha_{p} Y_{t-p-1}-\\theta_{1} u_{t-1}\n\\end{aligned}\n\\]\nor\n\\[\nY_{t}=\\alpha_{0}\\left(1-\\theta_{1}\\right)+\\left(\\alpha_{1}+\\theta_{1}\\right) Y_{t-1}+\\left(\\alpha_{2}-\\theta_{1} \\alpha_{1}\\right) Y_{t-2}+\\cdots-\\theta_{1} \\alpha_{p} Y_{t-p-1}+e_{t} .\n\\]\nThis is an \\(\\operatorname{AR}(\\mathrm{p}+1)\\). It simplifies to an \\(\\operatorname{AR}(\\mathrm{p})\\) when \\(\\theta_{1}=0\\). Thus \\(\\mathbb{H}_{0}\\) is equivalent to the restriction that the coefficient on \\(Y_{t-p-1}\\) is zero.\nThus testing the null hypothesis of an \\(\\operatorname{AR}(\\mathrm{p})\\) (14.52) against the alternative that the error is an \\(\\operatorname{AR}(1)\\) is equivalent to testing an \\(\\operatorname{AR}(\\mathrm{p})\\) against an \\(\\operatorname{AR}(\\mathrm{p}+1)\\). The latter test is implemented as a t test on the coefficient on \\(Y_{t-p-1}\\).\nMore generally, testing the null hypothesis of an \\(\\operatorname{AR}(\\mathrm{p})\\) (14.52) against the alternative that the error is an \\(\\operatorname{AR}(\\mathrm{q})\\) is equivalent to testing that \\(Y_{t}\\) is an \\(\\mathrm{AR}(\\mathrm{p})\\) against the alternative that \\(Y_{t}\\) is an \\(\\mathrm{AR}(\\mathrm{p}+\\mathrm{q})\\). The latter test is implemented as a Wald (or F) test on the coefficients on \\(Y_{t-p-1}, \\ldots, Y_{t-p-q}\\). If the statistic is smaller than the critical values (or the p-value is larger than the significance level) then we reject the hypothesis that the \\(\\operatorname{AR}(\\mathrm{p})\\) is correctly specified in favor of the alternative that there is omitted serial correlation. Otherwise we accept the hypothesis that the AR(p) model is correctly specified.\nAnother way of deriving the test is as follows. Write (14.52) and (14.53) using lag operator notation \\(\\alpha(\\mathrm{L}) Y_{t}=\\alpha_{0}+u_{t}\\) with \\(\\theta(\\mathrm{L}) u_{t}=e_{t}\\). Applying the operator \\(\\theta(\\mathrm{L})\\) to the first equation we obtain \\(\\theta(\\mathrm{L}) \\alpha(\\mathrm{L}) Y_{t}=\\) \\(\\alpha_{0}^{*}+e_{t}\\) where \\(\\alpha_{0}^{*}=\\theta(1) \\alpha_{0}\\). The product \\(\\theta(\\mathrm{L}) \\alpha(\\mathrm{L})\\) is a polynomial of order \\(p+q\\) so \\(Y_{t}\\) is an AR(p+q).\nWhile this discussion is all good fun, it is unclear if there is good reason to use the test described in this section. Economic theory does not typically produce hypotheses concerning the autoregressive order. Consequently there is rarely a case where there is scientific interest in testing, say, the hypothesis that a series is an AR(4) or any other specific autoregressive order. Instead, practitioners tend to use hypothesis tests for another purpose - model selection. That is, in practice users want to know “What autoregressive model should be used” in a specific application and resort to hypothesis tests to aid in this decision. This is an inappropriate use of hypothesis tests because tests are designed to provide answers to scientific questions rather than being designed to select models with good approximation properties. Instead, model selection should be based on model selection tools. One is described in the following section."
  },
  {
    "objectID": "chpt14-time-series.html#model-selection",
    "href": "chpt14-time-series.html#model-selection",
    "title": "14  Time Series",
    "section": "14.38 Model Selection",
    "text": "14.38 Model Selection\nWhat is an appropriate choice of autoregressive order \\(p\\) ? This is the problem of model selection. A good choice is to minimize the Akaike information criterion (AIC)\n\\[\n\\operatorname{AIC}(p)=n \\log \\widehat{\\sigma}^{2}(p)+2 p\n\\]\nwhere \\(\\widehat{\\sigma}^{2}(p)\\) is the estimated residual variance from an \\(\\operatorname{AR}(\\mathrm{p})\\). The AIC is a penalized version of the Gaussian log-likelihood function for the estimated regression model. It is an estimator of the divergence between the fitted model and the true conditional density (see Section 28.4). By selecting the model with the smallest value of the AIC you select the model with the smallest estimated divergence - the highest estimated fit between the estimated and true densities.\nThe AIC is also a monotonic transformation of an estimator of the one-step-ahead forecast mean squared error. Thus selecting the model with the smallest value of the AIC you are selecting the model with the smallest estimated forecast error.\nOne possible hiccup in computing the AIC criterion for multiple models is that the sample size available for estimation changes as \\(p\\) changes. (If you increase \\(p\\), you need more initial conditions.) This renders AIC comparisons inappropriate. The same sample - the same number of observations - should be used for estimation of all models. This is because AIC is a penalized likelihood, and if the samples are different then the likelihoods are not the same. The appropriate remedy is to fix a upper value \\(\\bar{p}\\), and then reserve the first \\(\\bar{p}\\) as initial conditions. Then estimate the models \\(\\operatorname{AR}(1), \\operatorname{AR}(2), \\ldots, \\operatorname{AR}(\\bar{p})\\) on this (unified) sample.\nThe AIC of an estimated regression model can be displayed in Stata by using the estimates stats command."
  },
  {
    "objectID": "chpt14-time-series.html#illustrations",
    "href": "chpt14-time-series.html#illustrations",
    "title": "14  Time Series",
    "section": "14.39 Illustrations",
    "text": "14.39 Illustrations\nWe illustrate autoregressive estimation with three empirical examples using U.S. quarterly time series from the FRED-QD data file.\nThe first example is real GDP growth rates (growth rate of \\(g d p c 1\\) ). We estimate autoregressive models of order 0 through 4 using the sample from \\(1980-2017^{8}\\). This is a commonly estimated model in applied macroeconomic practice and is the empirical version of the Samuelson multiplier-accelerator model discussed in Section 14.24. The coefficient estimates, conventional (heteroskedasticity-robust) standard errors, Newey-West (with \\(M=5\\) ) standard errors, and AIC, are displayed in Table 14.1. This sample has 152 observations. The model selected by the AIC criterion is the AR(2). The estimated model has positive and small values for the first two autoregressive coefficients. This means that quarterly output growth\n\\({ }^{8}\\) This sub-sample was used for estimation as it has been argued that the growth rate of U.S. GDP slowed around this period. The goal was to estimate the model over a period of time when the series is plausibly stationary. Table 14.1: U.S. GDP AR Models\n\n\n\n\\(\\alpha_{0}\\)\nAR(0)\nAR(1)\nAR(2)\nAR(3)\nAR(4)\n\n\n\n\n\n\\(0.65\\)\n\\(0.40\\)\n\\(0.34\\)\n\\(0.34\\)\n\\(0.34\\)\n\n\n\n\\((0.06)\\)\n\\((0.08)\\)\n\\((0.10)\\)\n\\((0.10)\\)\n\\((0.11)\\)\n\n\n\n\\([0.09]\\)\n\\([0.08]\\)\n\\([0.09]\\)\n\\([0.09]\\)\n\\([0.09]\\)\n\n\n\n\n\\(0.39\\)\n\\(0.34\\)\n\\(0.33\\)\n\\(0.34\\)\n\n\n\n\n\\((0.09)\\)\n\\((0.10)\\)\n\\((0.10)\\)\n\\((0.10)\\)\n\n\n\\(\\alpha_{2}\\)\n\n\\([0.10]\\)\n\\([0.10]\\)\n\\([0.10]\\)\n\\([0.10]\\)\n\n\n\n\n\n\\(0.14\\)\n\\(0.13\\)\n\\(0.13\\)\n\n\n\n\n\n\\((0.11)\\)\n\\((0.13)\\)\n\\((0.14)\\)\n\n\n\\(\\alpha_{3}\\)\n\n\n\\([0.10]\\)\n\\([0.10]\\)\n\\([0.11]\\)\n\n\n\n\n\n\n\\(0.02\\)\n\\(0.03\\)\n\n\n\n\n\n\n\\((0.11)\\)\n\\((0.12)\\)\n\n\n\\(\\alpha_{4}\\)\n\n\n\n\\([0.07]\\)\n\\([0.09]\\)\n\n\n\n\n\n\n\n\\(-0.02\\)\n\n\n\n\n\n\n\n\\((0.12)\\)\n\n\nAIC\n329\n306\n305\n307\n309\n\n\n\n\nStandard errors robust to heteroskedasticity in parenthesis.\nNewey-West standard errors in square brackets, with \\(M=5\\).\n\nrates are positively correlated from quarter to quarter, but only mildly so, and most of the correlation is captured by the first lag. The coefficients of this model are in the real section of Figure 14.6, meaning that the dynamics of the estimated model do not display oscillations. The coefficients of the estimated AR(4) model are nearly identical to the AR(2) model. The conventional and Newey-West standard errors are somewhat different from one another for the AR(0) and AR(4) models, but are nearly identical to one another for the \\(\\operatorname{AR}(1)\\) and \\(\\operatorname{AR}(2)\\) models\nOur second example is real non-durables consumption growth rates \\(C_{t}\\) (growth rate of \\(p c n d x\\) ). This is motivated by an influential paper by Robert Hall (1978) who argued that the permanent income hypothesis implies that changes in consumption should be unpredictable (martingale differences). To test this model Hall (1978) estimated an AR(4) model. Our estimated regression using the full sample \\((n=231)\\) is reported in the following equation.\n\nHere, we report heteroskedasticity-robust standard errors. Hall’s hypothesis is that all autoregressive coefficients should be zero. We test this joint hypothesis with an \\(F\\) statistic and find \\(\\mathrm{F}=3.32\\) with a p-value of \\(p=0.012\\). This is significant at the \\(5 %\\) level and close to the \\(1 %\\) level. The first three autoregressive coefficients appear to be positive, but small, indicating positive serial correlation. This evidence is (mildly) inconsistent with Hall’s hypothesis. We report heteroskedasticity-robust standard errors (not Newey-West standard errors) since the purpose was to test the hypothesis of no serial correlation. Table 14.2: U.S. Inflation AR Models\n\n\n\n\\(\\alpha_{0}\\)\nAR(1)\nAR(2)\nAR(3)\nAR(4)\nAR(5)\n\n\n\n\n\n\\(0.004\\)\n\\(0.003\\)\n\\(0.003\\)\n\\(0.003\\)\n\\(0.003\\)\n\n\n\n\\((0.034)\\)\n\\((0.032)\\)\n\\((0.032)\\)\n\\((0.032)\\)\n\\((0.032)\\)\n\n\n\n\\([0.023]\\)\n\\([0.028]\\)\n\\([0.029]\\)\n\\([0.031]\\)\n\\([0.032]\\)\n\n\n\\(\\alpha_{1}\\)\n\\(-0.26\\)\n\\(-0.36\\)\n\\(-0.36\\)\n\\(-0.36\\)\n\\(-0.37\\)\n\n\n\n\\((0.08)\\)\n\\((0.07)\\)\n\\((0.07)\\)\n\\((0.07)\\)\n\\((0.07)\\)\n\n\n\n\\([0.05]\\)\n\\([0.07]\\)\n\\([0.07]\\)\n\\([0.07]\\)\n\\([0.07]\\)\n\n\n\\(\\alpha_{2}\\)\n\n\\(-0.36\\)\n\\(-0.37\\)\n\\(-0.42\\)\n\\(-0.43\\)\n\n\n\n\n\\((0.07)\\)\n\\((0.06)\\)\n\\((0.06)\\)\n\\((0.06)\\)\n\n\n\n\n\\([0.06]\\)\n\\([0.05]\\)\n\\([0.07]\\)\n\\([0.07]\\)\n\n\n\\(\\alpha_{3}\\)\n\n\n\\(-0.00\\)\n\\(-0.06\\)\n\\(-0.08\\)\n\n\n\n\n\n\\((0.09)\\)\n\\((0.10)\\)\n\\((0.11)\\)\n\n\n\n\n\n\\([0.09]\\)\n\\([0.12]\\)\n\\([0.13]\\)\n\n\n\\(\\alpha_{4}\\)\n\n\n\n\\(-0.16\\)\n\\(-0.18\\)\n\n\n\n\n\n\n\\((0.08)\\)\n\\((0.08)\\)\n\n\n\n\n\n\n\\([0.09]\\)\n\\([0.09]\\)\n\n\n\\(\\alpha_{5}\\)\n\n\n\n\n\\(-0.04\\)\n\n\n\n\n\n\n\n\\((0.07)\\)\n\n\n\n\n\n\n\n\\([0.06]\\)\n\n\nAIC\n342\n312\n314\n310\n312\n\n\n\n\nStandard errors robust to heteroskedasticity in parenthesis.\nNewey-West standard errors in square brackets, with \\(M=5\\).\n\nThe third example is the first difference of CPI inflation (first difference of growth rate of cpiaucsl). This is motivated by Stock and Watson (2007) who examined forecasting models for inflation rates. We estimate autoregressive models of order 1 through 8 using the full sample ( \\(n=226)\\); we report models 1 through 5 in Table 14.2. The model with the lowest AIC is the AR(4). All four estimated autoregressive coefficients are negative, most particularly the first two. The two sets of standard errors are quite similar for the AR(4) model. There are meaningful differences only for the lower order AR models."
  },
  {
    "objectID": "chpt14-time-series.html#time-series-regression-models",
    "href": "chpt14-time-series.html#time-series-regression-models",
    "title": "14  Time Series",
    "section": "14.40 Time Series Regression Models",
    "text": "14.40 Time Series Regression Models\nLeast squares regression methods can be used broadly with stationary time series. Interpretation and usefulness can depend, however, on constructive dynamic specifications. Furthermore, it is necessary to be aware of the serial correlation properties of the series involved, and to use the appropriate covariance matrix estimator when the dynamics have not been explicitly modeled.\nLet \\(\\left(Y_{t}, X_{t}\\right)\\) be paired observations with \\(Y_{t}\\) the dependent variable and \\(X_{t}\\) a vector of regressors including an intercept. The regressors can contain lagged \\(Y_{t}\\) so this framework includes the autoregressive model as a special case. A linear regression model takes the form\n\\[\nY_{t}=X_{t}^{\\prime} \\beta+e_{t} .\n\\]\nThe coefficient vector is defined by projection and therefore equals\n\\[\n\\beta=\\left(\\mathbb{E}\\left[X_{t} X_{t}^{\\prime}\\right]\\right)^{-1} \\mathbb{E}\\left[X_{t} Y_{t}\\right] .\n\\]\nThe error \\(e_{t}\\) is defined by (14.54) and thus its properties are determined by that relationship. Implicitly the model assumes that the variables have finite second moments and \\(\\mathbb{E}\\left[X_{t} X_{t}^{\\prime}\\right]>0\\), otherwise the model is not uniquely defined and a regressor could be eliminated. By the property of projection the error is uncorrelated with the regressors \\(\\mathbb{E}\\left[X_{t} e_{t}\\right]=0\\).\nThe least squares estimator of \\(\\beta\\) is\n\\[\n\\widehat{\\beta}=\\left(\\sum_{t=1}^{n} X_{t} X_{t}^{\\prime}\\right)^{-1}\\left(\\sum_{t=1}^{n} X_{t} Y_{t}\\right) .\n\\]\nUnder the assumption that the joint series \\(\\left(Y_{t}, X_{t}\\right)\\) is strictly stationary and ergodic the estimator is consistent. Under the mixing and moment conditions of Theorem \\(14.32\\) the estimator is asymptotically normal with a general covariance matrix\nHowever, under the stronger assumption that the error is a MDS the asymptotic covariance matrix simplifies. It is worthwhile investigating this condition further. The necessary condition is \\(\\mathbb{E}\\left[e_{t} \\mid \\mathscr{F}_{t-1}\\right]=\\) 0 where \\(\\mathscr{F}_{t-1}\\) is an information set to which \\(\\left(e_{t-1}, X_{t}\\right)\\) is adapted. This notation may appear somewhat odd but recall in the autoregessive context that \\(X_{t}=\\left(1, Y_{t-1}, \\ldots, Y_{t-p}\\right)\\) contains variables dated time \\(t-1\\) and previously, thus \\(X_{t}\\) in this context is a “time \\(t-1\\)” variable. The reason why we need \\(\\left(e_{t-1}, X_{t}\\right)\\) to be adapted to \\(\\mathscr{F}_{t-1}\\) is that for the regression function \\(X_{t}^{\\prime} \\beta\\) to be the conditional mean of \\(Y_{t}\\) given \\(\\mathscr{F}_{t-1}, X_{t}\\) must be part of the information set \\(\\mathscr{F}_{t-1}\\). Under this assumption\n\\[\n\\mathbb{E}\\left[X_{t} e_{t} \\mid \\mathscr{F}_{t-1}\\right]=X_{t} \\mathbb{E}\\left[e_{t} \\mid \\mathscr{F}_{t-1}\\right]=0\n\\]\nso \\(\\left(X_{t} e_{t}, \\mathscr{F}_{t}\\right)\\) is a MDS. This means we can apply the MDS CLT to obtain the asymptotic distribution.\nWe summarize this discussion with the following formal statement.\nTheorem 14.35 If \\(\\left(Y_{t}, X_{t}\\right)\\) is strictly stationary, ergodic, with finite second moments, and \\(\\boldsymbol{Q}=\\mathbb{E}\\left[X_{t} X_{t}^{\\prime}\\right]>0\\), then \\(\\beta\\) in (14.55) is uniquely defined and the least squares estimator is consistent, \\(\\widehat{\\beta} \\underset{p}{\\longrightarrow} \\beta\\).\nIf in addition, \\(\\mathbb{E}\\left[e_{t} \\mid \\mathscr{F}_{t-1}\\right]=0\\), where \\(\\mathscr{F}_{t-1}\\) is an information set to which \\(\\left(e_{t-1}, X_{t}\\right)\\) is adapted, \\(\\mathbb{E}\\left|Y_{t}\\right|^{4}<\\infty\\), and \\(\\mathbb{E}\\left\\|X_{t}\\right\\|^{4}<\\infty\\), then\n\\[\n\\sqrt{n}(\\widehat{\\beta}-\\beta) \\underset{d}{\\longrightarrow} \\mathrm{N}\\left(0, \\boldsymbol{Q}^{-1} \\Omega \\boldsymbol{Q}^{-1}\\right)\n\\]\nas \\(n \\rightarrow \\infty\\), where \\(\\Omega=\\mathbb{E}\\left[X_{t} X_{t}^{\\prime} e_{t}^{2}\\right]\\)\nAlternatively, if for some \\(r>4\\), \\(\\mathbb{E}\\left|Y_{t}\\right|^{r}<\\infty\\), \\(\\mathbb{E}\\left\\|X_{t}\\right\\|^{r}<\\infty\\), and the mixing coefficients for \\(\\left(Y_{t}, X_{t}\\right)\\) satisfy \\(\\sum_{\\ell=1}^{\\infty} \\alpha(\\ell)^{1-4 / r}<\\infty\\), then (14.56) holds with\n\\[\n\\Omega=\\sum_{\\ell=-\\infty}^{\\infty} \\mathbb{E}\\left[X_{t-\\ell} X_{t}^{\\prime} e_{t} e_{t-\\ell}\\right] .\n\\]"
  },
  {
    "objectID": "chpt14-time-series.html#static-distributed-lag-and-autoregressive-distributed-lag-models",
    "href": "chpt14-time-series.html#static-distributed-lag-and-autoregressive-distributed-lag-models",
    "title": "14  Time Series",
    "section": "14.41 Static, Distributed Lag, and Autoregressive Distributed Lag Models",
    "text": "14.41 Static, Distributed Lag, and Autoregressive Distributed Lag Models\nIn this section we describe standard linear time series regression models.\nLet \\(\\left(Y_{t}, Z_{t}\\right)\\) be paired observations with \\(Y_{t}\\) the dependent variable and \\(Z_{t}\\) an observed regressor vector which does not include lagged \\(Y_{t}\\).\nThe simplest regression model is the static equation\n\\[\nY_{t}=\\alpha+Z_{t}^{\\prime} \\beta+e_{t} .\n\\]\nThis is (14.54) by setting \\(X_{t}=\\left(1, Z_{t}^{\\prime}\\right)^{\\prime}\\). Static models are motivated to describe how \\(Y_{t}\\) and \\(Z_{t}\\) co-move. Their advantage is their simplicity. The disadvantage is that they are difficult to interpret. The coefficient is the best linear predictor (14.55) but almost certainly is dynamically misspecified. The regression of \\(Y_{t}\\) on contemporeneous \\(Z_{t}\\) is difficult to interpret without a causal framework since the two may be simultaneous. If this regression is estimated it is important that the standard errors be calculated using the Newey-West method to account for serial correlation in the error.\nA model which allows the regressor to have impact over several periods is called a distributed lag (DL) model. It takes the form\n\\[\nY_{t}=\\alpha+Z_{t-1}^{\\prime} \\beta_{1}+Z_{t-2}^{\\prime} \\beta_{2}+\\cdots+Z_{t-q}^{\\prime} \\beta_{q}+e_{t} .\n\\]\nIt is also possible to include the contemporenous regressor \\(Z_{t}\\). In this model the leading coefficient \\(\\beta_{1}\\) represents the initial impact of \\(Z_{t}\\) on \\(Y_{t}, \\beta_{2}\\) represents the impact in the second period, and so on. The cumulative impact is the sum of the coefficients \\(\\beta_{1}+\\cdots+\\beta_{q}\\) which is called the long-run multiplier.\nThe distributed lag model falls in the class (14.54) by setting \\(X_{t}=\\left(1, Z_{t-1}^{\\prime}, Z_{t-2}^{\\prime}, \\ldots, Z_{t-q}^{\\prime}\\right)^{\\prime}\\). While it allows for a lagged impact of \\(Z_{t}\\) on \\(Y_{t}\\), the model does not incorporate serial correlation so the error \\(e_{t}\\) should be expected to be serially correlated. Thus the model is (typically) dynamically misspecified which can make interpretation difficult. It is also necessary to use Newey-West standard errors to account for the serial correlation.\nA more complete model combines autoregressive and distributed lags. It takes the form\n\\[\nY_{t}=\\alpha_{0}+\\alpha_{1} Y_{t-1}+\\cdots+\\alpha_{p} Y_{t-p}+Z_{t-1}^{\\prime} \\beta_{1}+\\cdots+Z_{t-q}^{\\prime} \\beta_{q}+e_{t} .\n\\]\nThis is called an autoregressive distributed lag (AR-DL) model. It nests both the autoregressive and distributed lag models thereby combining serial correlation and dynamic impact. The AR-DL model falls in the class (14.54) by setting \\(X_{t}=\\left(1, Y_{t-1}, \\ldots, Y_{t-p}, Z_{t-1}^{\\prime}, \\ldots, Z_{t-q}^{\\prime}\\right)^{\\prime}\\).\nIf the lag orders \\(p\\) and \\(q\\) are selected sufficiently large the AR-DL model will have an error which is approximately white noise in which case the model can be interpreted as dynamically well-specified and conventional standard error methods can be used.\nIn an AR-DL specification the long-run multiplier is\n\\[\n\\frac{\\beta_{1}+\\cdots+\\beta_{q}}{1-\\alpha_{1}-\\cdots-\\alpha_{p}}\n\\]\nwhich is a nonlinear function of the coefficients."
  },
  {
    "objectID": "chpt14-time-series.html#time-trends",
    "href": "chpt14-time-series.html#time-trends",
    "title": "14  Time Series",
    "section": "14.42 Time Trends",
    "text": "14.42 Time Trends\nMany economic time series have means which change over time. A useful way to think about this is the components model\n\\[\nY_{t}=T_{t}+u_{t}\n\\]\nwhere \\(T_{t}\\) is the trend component and \\(u_{t}\\) is the stochastic component. The latter can be modeled by a linear process or autoregression\n\\[\n\\alpha(\\mathrm{L}) u_{t}=e_{t}\n\\]\nThe trend component is often modeled as a linear function in the time index\n\\[\nT_{t}=\\beta_{0}+\\beta_{1} t\n\\]\nor a quadratic function in time\n\\[\nT_{t}=\\beta_{0}+\\beta_{1} t+\\beta_{2} t^{2} .\n\\]\nThese models are typically not thought of as being literally true but rather as useful approximations.\nWhen we write down time series models we write the index as \\(t=1, \\ldots, n\\). But in practical applications the time index corresponds to a date, e.g. \\(t=1960,1961, \\ldots, 2017\\). Furthermore, if the data is at a higher frequency than annual then it is incremented in fractional units. This is not of fundamental importance; it merely changes the meaning of the intercept \\(\\beta_{0}\\) and slope \\(\\beta_{1}\\). Consequently these should not be interpreted outside of how the time index is defined.\nOne traditional way of dealing with time trends is to “detrend” the data. This means using an estimation method to estimate the trend and subtract it off. The simplest method is least squares linear detrending. Given the linear model\n\\[\nY_{t}=\\beta_{0}+\\beta_{1} t+u_{t}\n\\]\nthe coefficients are estimated by least squares. The detrended series is the residual \\(\\widehat{u}_{t}\\). More intricate methods can be used but they have a similar flavor.\nTo understand the properties of the detrending method we can apply an asymptotic approximation. A time trend is not a stationary process so we should be thoughtful before applying standard theory. We will study asymptotics for non-stationary processes in more detail in Chapter 16 so our treatment here will be brief. It turns out that most of our conventional procedures work just fine with time trends (and quadratics in time) as regressors. The rates of convergence change but this does not affect anything of practical importance.\nLet us demonstrate that the least squares estimator of the coefficients in (14.57) is consistent. We can write the estimator as\n\\[\n\\left(\\begin{array}{c}\n\\widehat{\\beta}_{0}-\\beta_{0} \\\\\n\\widehat{\\beta}_{1}-\\beta_{1}\n\\end{array}\\right)=\\left(\\begin{array}{cc}\nn & \\sum_{t=1}^{n} t \\\\\n\\sum_{t=1}^{n} t & \\sum_{t=1}^{n} t^{2}\n\\end{array}\\right)^{-1}\\left(\\begin{array}{c}\n\\sum_{t=1}^{n} u_{t} \\\\\n\\sum_{t=1}^{n} t u_{t}\n\\end{array}\\right) .\n\\]\nWe need to study the behavior of the sums in the design matrix. For this the following result is useful, which follows by taking the limit of the Riemann sum for the integral \\(\\int_{0}^{1} x^{r} d x=1 /(1+r)\\).\nTheorem 14.36 For any \\(r>0\\), as \\(n \\rightarrow \\infty, n^{-1-r} \\sum_{t=1}^{n} t^{r} \\longrightarrow 1 /(1+r)\\).\nTheorem \\(14.36\\) implies that\n\\[\n\\frac{1}{n^{2}} \\sum_{t=1}^{n} t \\rightarrow \\frac{1}{2}\n\\]\nand\n\\[\n\\frac{1}{n^{3}} \\sum_{t=1}^{n} t^{2} \\rightarrow \\frac{1}{3} .\n\\]\nWhat is interesting about these results is that the sums require normalizations other than \\(n^{-1}\\) ! To handle this in multiple regression it is convenient to define a scaling matrix which normalizes each element in the regression by its convergence rate. Define the matrix \\(D_{n}=\\left[\\begin{array}{ll}1 & 0 \\\\ 0 & n\\end{array}\\right]\\). The first diagonal element is the intercept and second for the time trend. Then\n\\[\n\\begin{aligned}\nD_{n}\\left(\\begin{array}{c}\n\\widehat{\\beta}_{0}-\\beta_{0} \\\\\n\\widehat{\\beta}_{1}-\\beta_{1}\n\\end{array}\\right) &=D_{n}\\left(\\begin{array}{cc}\nn & \\sum_{t=1}^{n} t \\\\\n\\sum_{t=1}^{n} t & \\sum_{t=1}^{n} t^{2}\n\\end{array}\\right)^{-1} D_{n} D_{n}^{-1}\\left(\\begin{array}{c}\n\\sum_{t=1}^{n} u_{t} \\\\\n\\sum_{t=1}^{n} t u_{t}\n\\end{array}\\right) \\\\\n&=\\left(D_{n}^{-1}\\left(\\begin{array}{cc}\nn & \\sum_{t=1}^{n} t \\\\\n\\sum_{t=1}^{n} t & \\sum_{t=1}^{n} t^{2}\n\\end{array}\\right)_{n}^{-1}\\right)^{-1}\\left(\\begin{array}{c}\n\\sum_{t=1}^{n} u_{t} \\\\\n\\frac{1}{n} \\sum_{t=1}^{n} t u_{t}\n\\end{array}\\right) \\\\\n&=\\left(\\begin{array}{cc}\nn & \\frac{1}{n} \\sum_{t=1}^{n} t \\\\\n\\frac{1}{n} \\sum_{t=1}^{n} t & \\frac{1}{n^{2}} \\sum_{t=1}^{n} t^{2}\n\\end{array}\\right)^{-1}\\left(\\begin{array}{c}\n\\sum_{i=1}^{n} u_{t} \\\\\n\\frac{1}{n} \\sum_{i=1}^{n} t u_{t}\n\\end{array}\\right)\n\\end{aligned}\n\\]\nMultiplying by \\(n^{1 / 2}\\) we obtain\n\\[\n\\left(\\begin{array}{c}\nn^{1 / 2}\\left(\\widehat{\\beta}_{0}-\\beta_{0}\\right) \\\\\nn^{3 / 2}\\left(\\widehat{\\beta}_{1}-\\beta_{1}\\right)\n\\end{array}\\right)=\\left(\\begin{array}{cc}\n1 & \\frac{1}{n^{2}} \\sum_{t=1}^{n} t \\\\\n\\frac{1}{n^{2}} \\sum_{t=1}^{n} t & \\frac{1}{n^{3}} \\sum_{t=1}^{n} t^{2}\n\\end{array}\\right)^{-1}\\left(\\begin{array}{c}\n\\frac{1}{n_{1}^{1 / 2}} \\sum_{t=1}^{n} u_{t} \\\\\n\\frac{n^{3 / 2}}{n} \\sum_{t=1}^{n} t u_{t}\n\\end{array}\\right)\n\\]\nThe denominator matrix satisfies\n\\[\n\\left(\\begin{array}{cc}\n1 & \\frac{1}{n^{2}} \\sum_{t=1}^{n} t \\\\\n\\frac{1}{n^{2}} \\sum_{t=1}^{n} t & \\frac{1}{n^{3}} \\sum_{t=1}^{n} t^{2}\n\\end{array}\\right) \\rightarrow\\left(\\begin{array}{cc}\n1 & \\frac{1}{2} \\\\\n\\frac{1}{2} & \\frac{1}{3}\n\\end{array}\\right)\n\\]\nwhich is invertible. Setting \\(X_{n t}=(t / n, 1)\\), the numerator vector can be written as \\(n^{-1 / 2} \\sum_{t=1}^{n} X_{n t} u_{t}\\). It has variance\n\\[\n\\begin{aligned}\n\\left\\|\\operatorname{var}\\left[\\frac{1}{n^{1 / 2}} \\sum_{t=1}^{n} X_{n t} u_{t}\\right]\\right\\| &=\\left\\|\\frac{1}{n} \\sum_{t=1}^{n} \\sum_{j=1}^{n} X_{n t} X_{n j}^{\\prime} \\mathbb{E}\\left[u_{t} u_{j}\\right]\\right\\| \\\\\n& \\leq \\sqrt{2} \\sum_{\\ell=-\\infty}^{\\infty}\\left\\|\\mathbb{E}\\left[u_{t} u_{j}\\right]\\right\\|<\\infty\n\\end{aligned}\n\\]\nby Theorem \\(14.15\\) if \\(u_{t}\\) satisfies the mixing and moment conditions for the central limit theorem. This means that the numerator vector is \\(O_{p}(1)\\). (It is also asymptotically normal but we defer this demonstration for now.) We conclude that\n\\[\n\\left(\\begin{array}{c}\nn^{1 / 2}\\left(\\widehat{\\beta}_{0}-\\beta_{0}\\right) \\\\\nn^{3 / 2}\\left(\\widehat{\\beta}_{1}-\\beta_{1}\\right)\n\\end{array}\\right)=O_{p}(1)\n\\]\nThis shows that both coefficients are consistent, \\(\\widehat{\\beta}_{0}\\) converges at the standard \\(n^{1 / 2}\\) rate, and \\(\\widehat{\\beta}_{1}\\) converges at the faster \\(n^{3 / 2}\\) rate.\nThe consistency of the coefficient estimators (and their rates of convergence) can be used to show that linear detrending (regression of \\(Y_{t}\\) on an intercept and time trend to obtain a residual \\(\\widehat{u}_{t}\\) ) is consistent for the error \\(u_{t}\\) in (14.57).\nAn alternative is to include a time trend in the estimated regression. If we have an autoregression, a distributed lag, or an AL-DL model, we add a time index to obtain a model of the form\n\\[\nY_{t}=\\alpha_{0}+\\alpha_{1} Y_{t-1}+\\cdots+\\alpha_{p} Y_{t-p}+Z_{t-1}^{\\prime} \\beta_{1}+\\cdots+Z_{t-q}^{\\prime} \\beta_{q}+\\gamma t+e_{t} .\n\\]\nEstimation by least squares is equivalent to estimation after linear detrending by the FWL theorem. Inclusion of a linear (and possibly quadratic) time trend in a regression model is typically the easiest method to incorporate time trends."
  },
  {
    "objectID": "chpt14-time-series.html#illustration",
    "href": "chpt14-time-series.html#illustration",
    "title": "14  Time Series",
    "section": "14.43 Illustration",
    "text": "14.43 Illustration\nWe illustrate the models described in the previous section using a classical Phillips curve for inflation prediction. A. W. Phillips (1958) famously observed that the unemployment rate and the wage inflation rate are negatively correlated over time. Equations relating the inflation rate, or the change in the inflation rate, to macroeconomic indicators such as the unemployment rate are typically described as “Phillips curves”. A simple Phillips curve takes the form\n\\[\n\\Delta \\pi_{t}=\\alpha+\\beta U_{t}+e_{t}\n\\]\nwhere \\(\\pi_{t}\\) is price inflation and \\(U_{t}\\) is the unemployment rate. This specification relates the change in inflation in a given period to the level of the unemployment rate in the previous period.\nThe least squares estimate of (14.58) using U.S. quarterly series from FRED-QD is reported in the first column of Table 14.3. Both heteroskedasticity-robust and Newey-West standard errors are reported. The Newey-West standard errors are the appropriate choice since the estimated equation is static - no modeling of the serial correlation. In this example the measured impact of the unemployment rate on inflation appears minimal. The estimate is consistent with a small effect of the unemployment rate on the inflation rate but it is not precisely estimated.\nA distributed lag (DL) model takes the form\n\\[\n\\Delta \\pi_{t}=\\alpha+\\beta_{1} U_{t-1}+\\beta_{2} U_{t-2}+\\cdots+\\beta_{q} U_{t-q}+e_{t} .\n\\]\nThe least squares estimate of (14.59) is reported in the second column of Table 14.3. The estimates are quite different from the static model. We see large negative impacts in the first and third periods, countered by a large positive impact in the second period. The model suggests that the unemployment rate has a strong impact on the inflation rate but the long-run impact is mitigated. The long-run multiplier is reported at the bottom of the column. The point estimate of \\(-0.022\\) is quite small and similar to the static estimate. It implies that an increase in the unemployment rate by 5 percentage points (a typical recession) decreases the long-run annual inflation rate by about a half of a percentage point.\nAn AR-DL takes the form\n\\[\n\\Delta \\pi_{t}=\\alpha_{0}+\\alpha_{1} \\Delta \\pi_{t-1}+\\cdots+\\alpha_{p} \\Delta \\pi_{t-p}+\\beta_{1} U_{t-1}+\\cdots+\\beta_{q} U_{t-q}+e_{t} .\n\\]\nThe least squares estimate of \\((14.60)\\) is reported in the third column of Table 14.3. The coefficient estimates are similar to those from the distributed lag model. The point estimate of the long-run multiplier is also nearly identical but with a smaller standard error."
  },
  {
    "objectID": "chpt14-time-series.html#granger-causality",
    "href": "chpt14-time-series.html#granger-causality",
    "title": "14  Time Series",
    "section": "14.44 Granger Causality",
    "text": "14.44 Granger Causality\nIn the AR-DL model (14.60) the unemployment rate has no predictive impact on the inflation rate under the coefficient restriction \\(\\beta_{1}=\\cdots=\\beta_{q}=0\\). This restriction is called Granger non-causality. When the coefficients are non-zero we say that the unemployment rate “Granger causes” the inflation rate. This definition of causality was developed by Granger (1969) and Sims (1972).\nThe reason why we call this “Granger causality” rather than “causality” is because this is not a structural definition. An alternative label is “predictive causality”.\nTo be precise, assume that we have two series \\(\\left(Y_{t}, Z_{t}\\right)\\). Consider the projection of \\(Y_{t}\\) onto the lagged history of both series\n\\[\n\\begin{aligned}\nY_{t} &=\\mathscr{P}_{t-1}\\left(Y_{t}\\right)+e_{t} \\\\\n&=\\alpha_{0}+\\sum_{j=1}^{\\infty} \\alpha_{j} Y_{t-j}+\\sum_{j=1}^{\\infty} \\beta_{j} Z_{t-j}+e_{t}\n\\end{aligned}\n\\]\nTable 14.3: Phillips Curve Regressions\n\n\nStandard errors robust to heteroskedasticity in parenthesis.\nNewey-West standard errors in square brackets with \\(M=5\\). We say that \\(Z_{t}\\) does not Granger-cause \\(Y_{t}\\) if \\(\\beta_{j}=0\\) for all \\(j\\). If \\(\\beta_{j} \\neq 0\\) for some \\(j\\) then we say that \\(Z_{t}\\) Granger-causes \\(Y_{t}\\).\n\nIt is important that the definition includes the projection on the past history of \\(Y_{t}\\). Granger causality means that \\(Z_{t}\\) helps to predict \\(Y_{t}\\) even after the past history of \\(Y_{t}\\) has been accounted for.\nThe definition can alternatively be written in terms of conditional expectations rather than projections. We can say that \\(Z_{t}\\) does not Granger-cause \\(Y_{t}\\) if\n\\[\n\\mathbb{E}\\left[Y_{t} \\mid Y_{t-1}, Y_{t-2} \\ldots ; Z_{t-1}, Z_{t-2}, \\ldots\\right]=\\mathbb{E}\\left[Y_{t} \\mid Y_{t-1}, Y_{t-2}, \\ldots\\right] .\n\\]\nGranger causality can be tested in AR-DL models using a standard Wald or F test. In the context of model (14.60) we report the F statistic for \\(\\beta_{1}=\\cdots=\\beta_{q}=0\\). The test rejects the hypothesis (and thus finds evidence of Granger causality) if the statistic is larger than the critical value (if the p-value is small) and fails to reject the hypothesis (and thus finds no evidence of causality) if the statistic is smaller than the critical value.\nFor example, in the results presented in Table \\(14.3\\) the F statistic for the hypothesis \\(\\beta_{1}=\\cdots=\\beta_{4}=0\\) using the Newey-West covariance matrix is \\(\\mathrm{F}=6.98\\) with a p-value of \\(0.000\\). This is statistically significant at any conventional level so we can conclude that the unemployment rate has a predictively causal impact on inflation.\nGranger causality should not be interpreted structurally outside the context of an economic model. For example consider the regression of GDP growth rates \\(Y_{t}\\) on stock price growth rates \\(R_{t}\\). We use the quarterly series from FRED-QD, estimating an AR-DL specification with two lags\n\nThe coefficients on the lagged stock price growth rates are small in magnitude but the first lag appears statistically significant. The \\(\\mathrm{F}\\) statistic for exclusion of \\(\\left(R_{t-1}, R_{t-2}\\right)\\) is \\(F=9.3\\) with a \\(\\mathrm{p}\\)-value of \\(0.0002\\), which is highly significant. We can therefore reject the hypothesis of no Granger causality and deduce that stock prices Granger-cause GDP growth. We should be wary of concluding that this is structurally causal - that stock market movements cause output fluctuations. A more reasonable explanation from economic theory is that stock prices are forward-looking measures of expected future profits. When corporate profits are forecasted to rise the value of corporate stock rises, bidding up stock prices. Thus stock prices move in advance of actual economic activity but are not necessarily structurally causal."
  },
  {
    "objectID": "chpt14-time-series.html#testing-for-serial-correlation-in-regression-models",
    "href": "chpt14-time-series.html#testing-for-serial-correlation-in-regression-models",
    "title": "14  Time Series",
    "section": "14.45 Testing for Serial Correlation in Regression Models",
    "text": "14.45 Testing for Serial Correlation in Regression Models\nConsider the problem of testing for omitted serial correlation in an AR-DL model such as\n\\[\nY_{t}=\\alpha_{0}+\\alpha_{1} Y_{t-1}+\\cdots+\\alpha_{p} Y_{t-p}+\\beta_{1} Z_{t-1}+\\cdots+\\beta_{q} Z_{t-q}+u_{t} .\n\\]\nThe null hypothesis is that \\(u_{t}\\) is serially uncorrelated and the alternative hypothesis is that it is serially correlated. We can model the latter as a mean-zero autoregressive process\n\\[\nu_{t}=\\theta_{1} u_{t-1}+\\cdots+\\theta_{r} u_{t-r}+e_{t} .\n\\]\nThe hypothesis is\n\\[\n\\begin{aligned}\n&\\mathbb{H}_{0}: \\theta_{1}=\\cdots=\\theta_{r}=0 \\\\\n&\\mathbb{H}_{1}: \\theta_{j} \\neq 0 \\text { for some } j \\geq 1\n\\end{aligned}\n\\]\nThere are two ways to implement a test of \\(\\mathbb{H}_{0}\\) against \\(\\mathbb{H}_{1}\\). The first is to estimate equations (14.61)(14.62) sequentially by least squares and construct a test for \\(\\mathbb{H}_{0}\\) on the second equation. This test is complicated by the two-step estimation. Therefore this approach is not recommended.\nThe second approach is to combine equations (14.61)-(14.62) into a single model and execute the test as a restriction within this model. One way to make this combination is by using lag operator notation. Write (14.61)-(14.62) as\n\\[\n\\begin{aligned}\n&\\alpha(\\mathrm{L}) Y_{t}=\\alpha_{0}+\\beta(\\mathrm{L}) Z_{t-1}+u_{t} \\\\\n&\\theta(\\mathrm{L}) u_{t}=e_{t}\n\\end{aligned}\n\\]\nApplying the operator \\(\\theta(\\mathrm{L})\\) to the first equation we obtain\n\\[\n\\theta(\\mathrm{L}) \\alpha(\\mathrm{L}) Y_{t}=\\theta(\\mathrm{L}) \\alpha_{0}+\\theta(\\mathrm{L}) \\beta(\\mathrm{L}) Z_{t-1}+\\theta(\\mathrm{L}) u_{t}\n\\]\nor\n\\[\n\\alpha^{*}(\\mathrm{~L}) Y_{t}=\\alpha_{0}^{*}+\\beta^{*}(\\mathrm{~L}) Z_{t-1}+e_{t}\n\\]\nwhere \\(\\alpha^{*}(\\mathrm{~L})\\) is a \\(p+r\\) order polynomial and \\(\\beta^{*}(\\mathrm{~L})\\) is a \\(q+r\\) order polynomial. The restriction \\(\\mathbb{H}_{0}\\) is that these are \\(p\\) and \\(q\\) order polynomials. Thus we can implement a test of \\(\\mathbb{H}_{0}\\) against \\(\\mathbb{H}_{1}\\) by estimating an AR-DL model with \\(p+r\\) and \\(q+r\\) lags, and testing the exclusion of the final \\(r\\) lags of \\(Y_{t}\\) and \\(Z_{t}\\). This test has a conventional asymptotic distribution so is simple to implement.\nThe basic message is that testing for omitted serial correlation can be implement in regression models by estimating and contrasting different dynamic specifications."
  },
  {
    "objectID": "chpt14-time-series.html#bootstrap-for-time-series",
    "href": "chpt14-time-series.html#bootstrap-for-time-series",
    "title": "14  Time Series",
    "section": "14.46 Bootstrap for Time Series",
    "text": "14.46 Bootstrap for Time Series\nRecall that the bootstrap approximates the sampling distribution of estimators and test statistics by the empirical distribution of the observations. The traditional nonparametric bootstrap is appropriate for independent observations. For dependent observations alternative methods should be used.\nBootstrapping for time series is considerably more complicated than the cross section case. Many methods have been proposed. One of the challenges is that theoretical justifications are more difficult to establish than in the independent observation case.\nIn this section we describe the most popular methods to implement bootstrap resampling for time series data."
  },
  {
    "objectID": "chpt14-time-series.html#recursive-bootstrap",
    "href": "chpt14-time-series.html#recursive-bootstrap",
    "title": "14  Time Series",
    "section": "14.47 Recursive Bootstrap",
    "text": "14.47 Recursive Bootstrap\n\nEstimate a complete model such as an \\(\\mathrm{AR}(\\mathrm{p})\\) producing coefficient estimates \\(\\widehat{\\alpha}\\) and residuals \\(\\widehat{e}_{t}\\).\nFix the initial condition \\(\\left(Y_{-p+1}, Y_{-p+2}, \\ldots, Y_{0}\\right)\\).\nSimulate i.i.d. draws \\(e_{t}^{*}\\) from the empirical distribution of the residuals \\(\\left\\{\\widehat{e}_{1}, \\ldots, \\widehat{e}_{n}\\right\\}\\).\nCreate the bootstrap series \\(Y_{t}^{*}\\) by the recursive formula\n\n\\[\nY_{t}^{*}=\\widehat{\\alpha}_{0}+\\widehat{\\alpha}_{1} Y_{t-1}^{*}+\\widehat{\\alpha}_{2} Y_{t-2}^{*}+\\cdots+\\widehat{\\alpha}_{p} Y_{t-p}^{*}+e_{t}^{*} .\n\\]\nThis construction creates bootstrap samples \\(Y_{t}^{*}\\) with the stochastic properties of the estimated AR(p) model including the auxiliary assumption that the errors are i.i.d. This method can work well if the true process is an \\(\\mathrm{AR}(\\mathrm{p})\\). One flaw is that it imposes homoskedasticity on the errors \\(e_{t}^{*}\\) which may be different than the properties of the actual \\(e_{t}\\). Another limitation is that it is inappropriate for AR-DL models unless the conditioning variables are strictly exogenous.\nThere are alternative versions of this basic method. First, instead of fixing the initial conditions at the sample values a random block can be drawn from the sample. The difference is that this produces an unconditional distribution rather than a conditional one. Second, instead of drawing the errors from the residuals a parametric (typically normal) distribution can be used. This can improve precision when sample sizes are small but otherwise is not recommended."
  },
  {
    "objectID": "chpt14-time-series.html#pairwise-bootstrap",
    "href": "chpt14-time-series.html#pairwise-bootstrap",
    "title": "14  Time Series",
    "section": "14.48 Pairwise Bootstrap",
    "text": "14.48 Pairwise Bootstrap\n\nWrite the sample as \\(\\left\\{Y_{t}, X_{t}\\right\\}\\) where \\(X_{t}=\\left(Y_{t-1}, \\ldots, Y_{t-p}\\right)^{\\prime}\\) contains the lagged values used in estimation.\nApply the traditional nonparametric bootstrap which samples pairs \\(\\left(Y_{t}^{*}, X_{t}^{*}\\right)\\) i.i.d. from \\(\\left\\{Y_{t}, X_{t}\\right\\}\\) with replacement to create the bootstrap sample.\nCreate the bootstrap estimates on this bootstrap sample, e.g. regress \\(Y_{t}^{*}\\) on \\(X_{t}^{*}\\).\n\nThis construction is essentially the traditional nonparametric bootstrap but applied to the paired sample \\(\\left\\{Y_{t}, X_{t}\\right\\}\\). It does not mimic the time series correlations across observations. However, it does produce bootstrap statistics with the correct first-order asymptotic distribution under MDS errors. This method may be useful when we are interested in the distribution of nonlinear functions of the coefficient estimates and therefore desire an improvement on the Delta Method approximation."
  },
  {
    "objectID": "chpt14-time-series.html#fixed-design-residual-bootstrap",
    "href": "chpt14-time-series.html#fixed-design-residual-bootstrap",
    "title": "14  Time Series",
    "section": "14.49 Fixed Design Residual Bootstrap",
    "text": "14.49 Fixed Design Residual Bootstrap\n\nWrite the sample as \\(\\left\\{Y_{t}, X_{t}, \\widehat{e}_{t}\\right\\}\\) where \\(X_{t}=\\left(Y_{t-1}, \\ldots, Y_{t-p}\\right)^{\\prime}\\) contains the lagged values used in estimation and \\(\\widehat{e}_{t}\\) are the residuals.\nFix the regressors \\(X_{t}\\) at their sample values.\nSimulate i.i.d. draws \\(e_{t}^{*}\\) from the empirical distribution of the residuals \\(\\left\\{\\widehat{e}_{1}, \\ldots, \\widehat{e}_{n}\\right\\}\\).\nSet \\(Y_{t}^{*}=X_{t}^{\\prime} \\widehat{\\beta}+e_{t}^{*}\\).\n\nThis construction is similar to the pairwise bootstrap but imposes an i.i.d. error. It is therefore only valid when the errors are i.i.d. (and thus excludes heteroskedasticity)."
  },
  {
    "objectID": "chpt14-time-series.html#fixed-design-wild-bootstrap",
    "href": "chpt14-time-series.html#fixed-design-wild-bootstrap",
    "title": "14  Time Series",
    "section": "14.50 Fixed Design Wild Bootstrap",
    "text": "14.50 Fixed Design Wild Bootstrap\n\nWrite the sample as \\(\\left\\{Y_{t}, X_{t}, \\widehat{e}_{t}\\right\\}\\) where \\(X_{t}=\\left(Y_{t-1}, \\ldots, Y_{t-p}\\right)^{\\prime}\\) contains the lagged values used in estimation and \\(\\widehat{e}_{t}\\) are the residuals.\nFix the regressors \\(X_{t}\\) and residuals \\(\\widehat{e}_{t}\\) at their sample values.\nSimulate i.i.d. auxiliary random variables \\(\\xi_{t}^{*}\\) with mean zero and variance one. See Section \\(10.29\\) for a discussion of choices.\nSet \\(e_{t}^{*}=\\xi_{t}^{*} \\widehat{e}_{t}\\) and \\(Y_{t}^{*}=X_{t}^{\\prime} \\widehat{\\beta}+e_{t}^{*}\\)\n\nThis construction is similar to the pairwise and fixed design bootstrap combined with the wild bootstrap. This imposes the conditional mean assumption on the error but allows heteroskedasticity."
  },
  {
    "objectID": "chpt14-time-series.html#block-bootstrap",
    "href": "chpt14-time-series.html#block-bootstrap",
    "title": "14  Time Series",
    "section": "14.51 Block Bootstrap",
    "text": "14.51 Block Bootstrap\n\nWrite the sample as \\(\\left\\{Y_{t}, X_{t}\\right\\}\\) where \\(X_{t}=\\left(Y_{t-1}, \\ldots, Y_{t-p}\\right)^{\\prime}\\) contains the lagged values used in estimation.\nDivide the sample of paired observations \\(\\left\\{Y_{t}, X_{t}\\right\\}\\) into \\(n / m\\) blocks of length \\(m\\).\nResample complete blocks. For each simulated sample draw \\(n / m\\) blocks.\nPaste the blocks together to create the bootstrap time series \\(\\left\\{Y_{t}^{*}, X_{t}^{*}\\right\\}\\).\n\nThis construction allows for arbitrary stationary serial correlation, heteroskedasticity, and modelmisspecification. One challenge is that the block bootstrap is sensitive to the block length and the way that the data are partitioned into blocks. The method may also work less well in small samples. Notice that the block bootstrap with \\(m=1\\) is equal to the pairwise bootstrap and the latter is the traditional nonparametric bootstrap. Thus the block bootstrap is a natural generalization of the nonparametric bootstrap."
  },
  {
    "objectID": "chpt14-time-series.html#technical-proofs",
    "href": "chpt14-time-series.html#technical-proofs",
    "title": "14  Time Series",
    "section": "14.52 Technical Proofs*",
    "text": "14.52 Technical Proofs*\nProof of Theorem 14.2 Define \\(\\tilde{Y}_{t}=\\left(Y_{t}, Y_{t-1}, Y_{t-2}, \\ldots\\right) \\in \\mathbb{R}^{m \\times \\infty}\\) as the history of \\(Y_{t}\\) up to time \\(t\\). Write \\(X_{t}=\\phi\\left(\\widetilde{Y}_{t}\\right)\\). Let \\(B\\) be the pre-image of \\(\\left\\{X_{t} \\leq x\\right\\}\\) (the vectors \\(\\widetilde{Y} \\in \\mathbb{R}^{m \\times \\infty}\\) such that \\(\\left.\\phi(\\widetilde{Y}) \\leq x\\right)\\). Then\n\\[\n\\mathbb{P}\\left[X_{t} \\leq x\\right]=\\mathbb{P}\\left[\\phi\\left(\\widetilde{Y}_{t}\\right) \\leq x\\right]=\\mathbb{P}\\left[\\tilde{Y}_{t} \\in B\\right] .\n\\]\nSince \\(Y_{t}\\) is strictly stationary, \\(\\mathbb{P}\\left[\\tilde{Y}_{t} \\in B\\right]\\) is independent \\({ }^{9}\\) of \\(t\\). This means that the distribution of \\(X_{t}\\) is independent of \\(t\\). This argument can be extended to show that the distribution of \\(\\left(X_{t}, \\ldots, X_{t+\\ell}\\right)\\) is independent of \\(t\\). This means that \\(X_{t}\\) is strictly stationary as claimed.\nProof of Theorem 14.3 By the Cauchy criterion for convergence (see Theorem A.2 of Probability and Statistics for Economists), \\(S_{N}=\\sum_{j=0}^{N} a_{j} Y_{t-j}\\) converges almost surely if for all \\(\\epsilon>0\\),\n\\[\n\\inf _{N} \\sup _{j>N}\\left|S_{N+j}-S_{N}\\right| \\leq \\epsilon .\n\\]\n\\({ }^{9}\\) An astute reader may notice that the independence of \\(\\mathbb{P}\\left[\\widetilde{Y}_{t} \\in B\\right]\\) from \\(t\\) does not follow directly from the definition of strict stationarity. Indeed, a full derivation requires a measure-theoretic treatment. See Section 1.2.B of Petersen (1983) or Section \\(3.5\\) of Stout (1974). Let \\(A_{\\epsilon}\\) be this event. Its complement is\n\\[\nA_{\\epsilon}^{c}=\\bigcap_{N=1}^{\\infty}\\left\\{\\sup _{j>N}\\left|\\sum_{i=N+1}^{N+j} a_{i} Y_{t-i}\\right|>\\epsilon\\right\\} .\n\\]\nThis has probability\n\\[\n\\mathbb{P}\\left[A_{\\epsilon}^{c}\\right] \\leq \\lim _{N \\rightarrow \\infty} \\mathbb{P}\\left[\\sup _{j>N}\\left|\\sum_{i=N+1}^{N+j} a_{i} Y_{t-i}\\right|>\\epsilon\\right] \\leq \\lim _{N \\rightarrow \\infty} \\frac{1}{\\epsilon} \\mathbb{E}\\left[\\sup _{j>N}\\left|\\sum_{i=N+1}^{N+j} a_{i} Y_{t-i}\\right|\\right] \\leq \\frac{1}{\\epsilon} \\lim _{N \\rightarrow \\infty} \\sum_{i=N+1}^{\\infty}\\left|a_{i}\\right| \\mathbb{E}\\left|Y_{t-i}\\right|=0 .\n\\]\nThe second equality is Markov’s inequality (B.36) and the following is the triangle inequality (B.1). The limit is zero because \\(\\sum_{i=0}^{\\infty}\\left|a_{i}\\right|<\\infty\\) and \\(\\mathbb{E}\\left|Y_{t}\\right|<\\infty\\). Hence for all \\(\\epsilon>0, \\mathbb{P}\\left[A_{\\varepsilon}^{c}\\right]=0\\) and \\(\\mathbb{P}\\left[A_{\\epsilon}\\right]=1\\). This means that \\(S_{N}\\) converges with probability one, as claimed.\nSince \\(Y_{t}\\) is strictly stationary then \\(X_{t}\\) is as well by Theorem \\(14.2\\).\nProof of Theorem 14.4 See Theorem 14.14.\nProof of Theorem 14.5 Strict stationarity follows from Theorem 14.2. Let \\(\\widetilde{Y}_{t}\\) and \\(\\widetilde{X}_{t}\\) be the histories of \\(Y_{t}\\) and \\(X_{t}\\). Write \\(X_{t}=\\phi\\left(\\widetilde{Y}_{t}\\right)\\). Let \\(A\\) be an invariant event for \\(X_{t}\\). We want to show \\(\\mathbb{P}[A]=0\\) or 1 . The event \\(A\\) is a collection of \\(\\widetilde{X}_{t}\\) histories, and occurs if and and only if an associated collection of \\(\\widetilde{Y}_{t}\\) histories occur. That is, for some sets \\(G\\) and \\(H\\),\n\\[\nA=\\left\\{\\widetilde{X}_{t} \\in G\\right\\}=\\left\\{\\phi\\left(\\widetilde{Y}_{t}\\right) \\in G\\right\\}=\\left\\{\\widetilde{Y}_{t} \\in H\\right\\} .\n\\]\nThe assumption that \\(A\\) is invariant means it is unaffected by the time shift, thus can be written as\n\\[\nA=\\left\\{\\widetilde{X}_{t+\\ell} \\in G\\right\\}=\\left\\{\\widetilde{Y}_{t+\\ell} \\in H\\right\\} .\n\\]\nThis means the event \\(\\left\\{\\widetilde{Y}_{t+\\ell} \\in H\\right\\}\\) is invariant. Since \\(Y_{t}\\) is ergodic the event has probability 0 or 1. Hence \\(\\mathbb{P}[A]=0\\) or 1 , as desired.\nProof of Theorem 14.7 Suppose \\(Y_{t}\\) is discrete with support on \\(\\left(\\tau_{1}, \\ldots, \\tau_{N}\\right)\\) and without loss of generality assume \\(\\mathbb{E}\\left[Y_{t}\\right]=0\\). Then by Theorem \\(14.8\\)\n\\[\n\\begin{aligned}\n\\lim _{n \\rightarrow \\infty} \\frac{1}{n} \\sum_{\\ell=1}^{n} \\operatorname{cov}\\left(Y_{t}, Y_{t+\\ell}\\right) &=\\lim _{n \\rightarrow \\infty} \\frac{1}{n} \\sum_{\\ell=1}^{n} \\mathbb{E}\\left[Y_{t} Y_{t+\\ell}\\right] \\\\\n&=\\lim _{n \\rightarrow \\infty} \\frac{1}{n} \\sum_{\\ell=1}^{n} \\sum_{j=1}^{N} \\sum_{k=1}^{N} \\tau_{j} \\tau_{k} \\mathbb{P}\\left[Y_{t}=\\tau_{j}, Y_{t+\\ell}=\\tau_{k}\\right] \\\\\n&=\\sum_{j=1}^{N} \\sum_{k=1}^{N} \\tau_{j} \\tau_{k} \\lim _{n \\rightarrow \\infty} \\frac{1}{n} \\sum_{\\ell=1}^{n} \\mathbb{P}\\left[Y_{t}=\\tau_{j}, Y_{t+\\ell}=\\tau_{k}\\right] \\\\\n&=\\sum_{j=1}^{N} \\sum_{k=1}^{N} \\tau_{j} \\tau_{k} \\mathbb{P}\\left[y_{t}=\\tau_{j}\\right] \\mathbb{P}\\left[Y_{t+\\ell}=\\tau_{k}\\right] \\\\\n&=\\mathbb{E}\\left[Y_{t}\\right] \\mathbb{E}\\left[Y_{t+\\ell}\\right] \\\\\n&=0 .\n\\end{aligned}\n\\]\nwhich is (14.4). This can be extended to the case of continuous distributions using the monotone convergence theorem. See Corollary \\(14.8\\) of Davidson (1994).\nProof of Theorem 14.9 We show (14.6). (14.7) follows by Markov’s inequality (B.36). Without loss of generality we focus on the scalar case and assume \\(\\mathbb{E}\\left[Y_{t}\\right]=0\\). Fix \\(\\epsilon>0\\). Pick \\(B\\) large enough such that\n\\[\n\\mathbb{E}\\left|Y_{t} \\mathbb{1}\\left\\{\\left|Y_{t}\\right|>B\\right\\}\\right| \\leq \\frac{\\epsilon}{4}\n\\]\nwhich is feasible because \\(\\mathbb{E}\\left|Y_{t}\\right|<\\infty\\). Define\n\\[\n\\begin{aligned}\nW_{t} &=Y_{t} \\mathbb{1}\\left\\{\\left|Y_{t}\\right| \\leq B\\right\\}-\\mathbb{E}\\left[Y_{t} \\mathbb{1}\\left\\{\\left|Y_{t}\\right| \\leq B\\right\\}\\right] \\\\\nZ_{t} &=Y_{t} \\mathbb{1}\\left\\{\\left|Y_{t}\\right|>B\\right\\}-\\mathbb{E}\\left[Y_{t} \\mathbb{1}\\left\\{\\left|Y_{t}\\right|>B\\right\\}\\right] .\n\\end{aligned}\n\\]\nNotice that \\(W_{t}\\) is a bounded transformation of the ergodic series \\(Y_{t}\\). Thus by (14.4) and (14.9) there is an \\(n\\) sufficiently large so that\n\\[\n\\frac{\\operatorname{var}\\left[W_{t}\\right]}{n}+\\frac{2}{n} \\sum_{m=1}^{n}\\left(1-\\frac{m}{n}\\right) \\operatorname{cov}\\left(W_{t}, W_{j}\\right) \\leq \\frac{\\epsilon^{2}}{4}\n\\]\nBy the triangle inequality (B.1)\n\\[\n\\mathbb{E}|\\bar{Y}|=\\mathbb{E}|\\bar{W}+\\bar{Z}| \\leq \\mathbb{E}|\\bar{W}|+\\mathbb{E}|\\bar{Z}| .\n\\]\nBy another application of the triangle inequality and (14.63)\n\\[\n\\mathbb{E}|\\bar{Z}| \\leq \\mathbb{E}\\left|Z_{t}\\right| \\leq 2 \\mathbb{E}\\left|Y_{t} \\mathbb{1}\\left(\\left|Y_{t}\\right|>B\\right)\\right| \\leq \\frac{\\epsilon}{2} .\n\\]\nBy Jensen’s inequality (B.27), direct calculation, and (14.64)\n\\[\n\\begin{aligned}\n(\\mathbb{E}|\\bar{W}|)^{2} & \\leq \\mathbb{E}\\left[|\\bar{W}|^{2}\\right] \\\\\n&=\\frac{1}{n^{2}} \\sum_{t=1}^{n} \\sum_{j=1}^{n} \\mathbb{E}\\left[W_{t} W_{j}\\right] \\\\\n&=\\frac{\\operatorname{var}\\left[W_{t}\\right]}{n}+\\frac{2}{n} \\sum_{m=1}^{n}\\left(1-\\frac{m}{n}\\right) \\operatorname{cov}\\left(W_{t}, W_{j}\\right) \\\\\n& \\leq \\frac{\\epsilon^{2}}{4} .\n\\end{aligned}\n\\]\nThus\n\\[\n\\mathbb{E}|\\bar{W}| \\leq \\frac{\\epsilon}{2} .\n\\]\nTogether, (14.65), (14.66) and (14.67) show that \\(\\mathbb{E}|\\bar{Y}| \\leq \\epsilon\\). Since \\(\\varepsilon\\) is arbitrary, this establishes (14.6) as claimed.\nProof of Theorem 14.11 (sketch) By the Cramér-Wold device (Theorem \\(8.4\\) from Probability and Statistics for Economists) it is sufficient to establish the result for scalar \\(u_{t}\\). Let \\(\\sigma^{2}=\\mathbb{E}\\left[u_{t}^{2}\\right]\\). By a Taylor series expansion, for \\(x\\) small \\(\\log (1+x) \\simeq x-x^{2} / 2\\). Taking exponentials and rearranging we obtain the approximation\nFix \\(\\lambda\\). Define\n\\[\n\\exp (x) \\simeq(1+x) \\exp \\left(\\frac{x^{2}}{2}\\right) .\n\\]\n\\[\n\\begin{aligned}\nT_{j} &=\\prod_{i=1}^{j}\\left(1+\\frac{\\lambda}{\\sqrt{n}} u_{t}\\right) \\\\\nV_{n} &=\\frac{1}{n} \\sum_{t=1}^{n} u_{t}^{2} .\n\\end{aligned}\n\\]\nSince \\(u_{t}\\) is strictly stationary and ergodic, \\(V_{n} \\stackrel{p}{\\rightarrow} \\sigma^{2}\\) by the Ergodic Theorem (Theorem 14.9). Since \\(u_{t}\\) is a MDS\n\\[\n\\mathbb{E}\\left[T_{n}\\right]=1 .\n\\]\nTo see this, define \\(\\mathscr{F}_{t}=\\sigma\\left(\\ldots, u_{t-1}, u_{t}\\right)\\). Note \\(T_{j}=T_{j-1}\\left(1+\\frac{\\lambda}{\\sqrt{n}} u_{j}\\right)\\). By iterated expectations\n\\[\n\\begin{aligned}\n\\mathbb{E}\\left[T_{n}\\right] &=\\mathbb{E}\\left[\\mathbb{E}\\left[T_{n} \\mid \\mathscr{F}_{n-1}\\right]\\right] \\\\\n&=\\mathbb{E}\\left[T_{n-1} \\mathbb{E}\\left[1+\\frac{\\lambda}{\\sqrt{n}} u_{n} \\mid \\mathscr{F}_{n-1}\\right]\\right] \\\\\n&=\\mathbb{E}\\left[T_{n-1}\\right]=\\cdots=\\mathbb{E}\\left[T_{1}\\right] \\\\\n&=1 .\n\\end{aligned}\n\\]\nThis is (14.69).\nThe moment generating function of \\(S_{n}\\) is\n\\[\n\\begin{aligned}\n\\mathbb{E}\\left[\\exp \\left(\\frac{\\lambda}{\\sqrt{n}} \\sum_{t=1}^{n} u_{t}\\right)\\right] &=\\mathbb{E}\\left[\\prod_{i=1}^{n} \\exp \\left(\\frac{\\lambda}{\\sqrt{n}} u_{t}\\right)\\right] \\\\\n& \\simeq \\mathbb{E}\\left[\\prod_{i=1}^{n}\\left[1+\\frac{\\lambda}{\\sqrt{n}} u_{t}\\right] \\exp \\left(\\frac{\\lambda^{2}}{2 n} u_{t}^{2}\\right)\\right] \\\\\n&=\\mathbb{E}\\left[T_{n} \\exp \\left(\\frac{\\lambda^{2} V_{n}}{2}\\right)\\right] \\\\\n& \\simeq \\mathbb{E}\\left[T_{n} \\exp \\left(\\frac{\\lambda^{2} \\sigma^{2}}{2}\\right)\\right] \\\\\n&=\\exp \\left(\\frac{\\lambda^{2} \\sigma^{2}}{2}\\right) .\n\\end{aligned}\n\\]\nThe approximation in (14.70) is (14.68). The approximation (14.71) is \\(V_{n} \\vec{p} \\sigma^{2}\\). (A rigorous justification which allows this substitution in the expectation is technical.) The final equality is (14.69). This shows that the moment generating function of \\(S_{n}\\) is approximately that of \\(\\mathrm{N}\\left(0, \\sigma^{2}\\right)\\), as claimed.\nThe assumption that \\(u_{t}\\) is a MDS is critical for (14.69). \\(T_{n}\\) is a nonlinear function of the errors \\(u_{t}\\) so a white noise assumption cannot be used instead. The MDS assumption is exactly the minimal condition needed to obtain (14.69). This is why the MDS assumption cannot be easily replaced by a milder assumption such as white noise.\nProof of Theorem 14.13.1 Without loss of generality suppose \\(\\mathbb{E}\\left[X_{t}\\right]=0\\) and \\(\\mathbb{E}\\left[Z_{t}\\right]=0\\). Set \\(\\eta_{t-m}=\\) \\(\\operatorname{sgn}\\left(\\mathbb{E}\\left[Z_{t} \\mid \\mathscr{F}_{-\\infty}^{t-m}\\right]\\right)\\). By iterated expectations, \\(\\left|X_{t}\\right| \\leq C_{1},\\left|\\mathbb{E}\\left[Z_{t} \\mid \\mathscr{F}_{-\\infty}^{t-m}\\right]\\right|=\\eta_{t-m} \\mathbb{E}\\left[Z_{t} \\mid \\mathscr{F}_{-\\infty}^{t-m}\\right]\\), and again using iterated expectations\n\\[\n\\begin{aligned}\n\\left|\\operatorname{cov}\\left(X_{t-m}, Z_{t}\\right)\\right| &=\\left|\\mathbb{E}\\left[\\mathbb{E}\\left[X_{t-m} Z_{t} \\mid \\mathscr{F}_{-\\infty}^{t-m}\\right]\\right]\\right| \\\\\n&=\\left|\\mathbb{E}\\left(X_{t-m} \\mathbb{E}\\left[Z_{t} \\mid \\mathscr{F}_{-\\infty}^{t-m}\\right]\\right)\\right| \\\\\n& \\leq C_{1} \\mathbb{E}\\left|\\mathbb{E}\\left[Z_{t} \\mid \\mathscr{F}_{-\\infty}^{t-m}\\right]\\right| \\\\\n&=C_{1} \\mathbb{E}\\left[\\eta_{t-m} \\mathbb{E}\\left[Z_{t} \\mid \\mathscr{F}_{-\\infty}^{t-m}\\right]\\right] \\\\\n&=C_{1} \\mathbb{E}\\left[\\mathbb{E}\\left[\\eta_{t-m} Z_{t} \\mid \\mathscr{F}_{-\\infty}^{t-m}\\right]\\right] \\\\\n&=C_{1} \\mathbb{E}\\left[\\eta_{t-m} Z_{t}\\right] \\\\\n&=C_{1} \\operatorname{cov}\\left(\\eta_{t-m}, Z_{t}\\right) .\n\\end{aligned}\n\\]\nSetting \\(\\xi_{t}=\\operatorname{sgn}\\left(\\mathbb{E}\\left[X_{t-m} \\mid \\mathscr{F}_{t}^{\\infty}\\right]\\right)\\), by a similar argument (14.72) is bounded by \\(C_{1} C_{2} \\operatorname{cov}\\left(\\eta_{t-m}, \\xi_{t}\\right)\\). Set \\(A_{1}=\\mathbb{1}\\left\\{\\eta_{t-m}=1\\right\\}, A_{2}=\\mathbb{1}\\left\\{\\eta_{t-m}=-1\\right\\}, B_{1}=\\mathbb{1}\\left\\{\\xi_{t}=1\\right\\}, B_{2}=\\mathbb{1}\\left\\{\\xi_{t}=-1\\right\\}\\). We calculate\n\\[\n\\begin{aligned}\n\\left|\\operatorname{cov}\\left(\\eta_{t-m}, \\xi_{t}\\right)\\right| &=\\mid \\mathbb{P}\\left[A_{1} \\cap B_{1}\\right]+\\mathbb{P}\\left[A_{2} \\cap B_{2}\\right]-\\mathbb{P}\\left[A_{2} \\cap B_{1}\\right]-\\mathbb{P}\\left[A_{1} \\cap B_{2}\\right] \\\\\n&-\\mathbb{P}\\left[A_{1}\\right] \\mathbb{P}\\left[B_{1}\\right]-\\mathbb{P}\\left[A_{2}\\right] \\mathbb{P}\\left[B_{2}\\right]+\\mathbb{P}\\left[A_{2}\\right] \\mathbb{P}\\left[B_{1}\\right]+\\mathbb{P}\\left[A_{1}\\right] \\mathbb{P}\\left[B_{2}\\right] \\mid \\\\\n& \\leq 4 \\alpha(m) .\n\\end{aligned}\n\\]\nTogether, \\(\\left|\\operatorname{cov}\\left(X_{t-m}, z_{t}\\right)\\right| \\leq 4 C_{1} C_{2} \\alpha(m)\\) as claimed.\nProof of Theorem 14.13.2 Assume \\(\\mathbb{E}\\left[X_{t}\\right]=0\\) and \\(\\mathbb{E}\\left[Z_{t}\\right]=0\\). We first show that if \\(\\left|X_{t}\\right| \\leq C\\) then\n\\[\n\\left|\\operatorname{cov}\\left(X_{t-\\ell}, Z_{t}\\right)\\right| \\leq 6 C\\left(\\mathbb{E}\\left|Z_{t}\\right|^{r}\\right)^{1 / r} \\alpha(\\ell)^{1-1 / r} .\n\\]\nIndeed, if \\(\\alpha(\\ell)=0\\) the result is immediate so assume \\(\\alpha(\\ell)>0\\). Set \\(D=\\alpha(\\ell)^{-1 / r}\\left(\\mathbb{E}\\left|Z_{t}\\right|^{r}\\right)^{1 / r}, V_{t}=Z_{t} \\mathbb{1}\\left\\{\\left|Z_{t}\\right| \\leq D\\right\\}\\) and \\(W_{t}=Z_{t} \\mathbb{1}\\left\\{\\left|Z_{t}\\right|>D\\right\\}\\). Using the triangle inequality (B.1) and then part 1, because \\(\\left|X_{t}\\right| \\leq C\\) and \\(\\left|V_{t}\\right| \\leq D\\),\n\\[\n\\left|\\operatorname{cov}\\left(X_{t-\\ell}, Z_{t}\\right)\\right| \\leq\\left|\\operatorname{cov}\\left(X_{t-\\ell}, V_{t}\\right)\\right|+\\left|\\operatorname{cov}\\left(X_{t-\\ell}, W_{t}\\right)\\right| \\leq 4 C D \\alpha(\\ell)+2 C \\mathbb{E}\\left|w_{t}\\right| .\n\\]\nAlso,\n\\[\n\\mathbb{E}\\left|W_{t}\\right|=\\mathbb{E}\\left|Z_{t} \\mathbb{1}\\left\\{\\left|Z_{t}\\right|>D\\right\\}\\right|=\\mathbb{E}\\left|\\frac{\\left|Z_{t}\\right|^{r}}{\\left|Z_{t}\\right|^{r-1}} \\mathbb{1}\\left\\{\\left|Z_{t}\\right|>D\\right\\}\\right| \\leq \\frac{\\mathbb{E}\\left|Z_{t}\\right|^{r}}{D^{r-1}}=\\alpha(\\ell)^{(r-1) / r}\\left(\\mathbb{E}\\left|Z_{t}\\right|^{r}\\right)^{1 / r}\n\\]\nusing the definition of \\(D\\). Together we have\n\\[\n\\left|\\operatorname{cov}\\left(X_{t-\\ell}, Z_{t}\\right)\\right| \\leq 6 C\\left(\\mathbb{E}\\left|X_{t}\\right|^{r}\\right)^{1 / r} \\alpha(\\ell)^{1-1 / r} .\n\\]\nwhich is (14.73) as claimed.\nNow set \\(C=\\alpha(\\ell)^{-1 / r}\\left(\\mathbb{E}\\left|X_{t}\\right|^{r}\\right)^{1 / r}, V_{t}=X_{t} \\mathbb{1}\\left\\{\\left|X_{t}\\right| \\leq C\\right\\}\\) and \\(W_{t}=X_{t} \\mathbb{1}\\left\\{\\left|X_{t}\\right|>C\\right\\}\\). Using the triangle inequality and (14.73)\n\\[\n\\left|\\operatorname{cov}\\left(X_{t-\\ell}, Z_{t}\\right)\\right| \\leq\\left|\\operatorname{cov}\\left(V_{t-\\ell}, Z_{t}\\right)\\right|+\\left|\\operatorname{cov}\\left(W_{t-\\ell}, Z_{t}\\right)\\right| .\n\\]\nSince \\(\\left|V_{t}\\right| \\leq C\\), using (14.73) and the definition of \\(C\\)\n\\[\n\\left|\\operatorname{cov}\\left(V_{t-\\ell}, Z_{t}\\right)\\right| \\leq 6 C\\left(\\mathbb{E}\\left|Z_{t}\\right|^{q}\\right)^{1 / q} \\alpha(\\ell)^{1-1 / q}=6\\left(\\mathbb{E}\\left|X_{t}\\right|^{r}\\right)^{1 / r}\\left(\\mathbb{E}\\left|Z_{t}\\right|^{q}\\right)^{1 / q} \\alpha(\\ell)^{1-1 / q-1 / r} .\n\\]\nUsing Hölder’s inequality (B.31) and the definition of \\(C\\)\n\\[\n\\begin{aligned}\n\\left|\\operatorname{cov}\\left(W_{t-\\ell}, Z_{t}\\right)\\right| & \\leq 2\\left(\\mathbb{E}\\left|W_{t}\\right|^{q /(q-1)}\\right)^{(q-1) / q}\\left(\\mathbb{E}\\left|Z_{t}\\right|^{q}\\right)^{1 / q} \\\\\n&=2\\left(\\mathbb{E}\\left[\\left|X_{t}\\right|^{q /(q-1)} \\mathbb{1}\\left\\{\\left|X_{t}\\right|>C\\right\\}\\right]\\right)^{(q-1) / q}\\left(\\mathbb{E}\\left|Z_{t}\\right|^{q}\\right)^{1 / q} \\\\\n&=2\\left(\\mathbb{E}\\left[\\frac{\\left|X_{t}\\right|^{r}}{\\left|X_{t}\\right|^{r-q /(q-1)}} \\mathbb{1}\\left\\{\\left|X_{t}\\right|>C\\right\\}\\right]\\right)^{(q-1) / q}\\left(\\mathbb{E}\\left|Z_{t}\\right|^{q}\\right)^{1 / q} \\\\\n& \\leq \\frac{2}{C^{r(q-1) / q-1}}\\left(\\mathbb{E}\\left|X_{t}\\right|^{r}\\right)^{(q-1) / q}\\left(\\mathbb{E}\\left|Z_{t}\\right|^{q}\\right)^{1 / q} \\\\\n&=2\\left(\\mathbb{E}\\left|X_{t}\\right|^{r}\\right)^{1 / r}\\left(\\mathbb{E}\\left|Z_{t}\\right|^{q}\\right)^{1 / q} \\alpha(\\ell)^{1-1 / q-1 / r} .\n\\end{aligned}\n\\]\nTogether we have\n\\[\n\\left|\\operatorname{cov}\\left(X_{t-\\ell}, Z_{t}\\right)\\right| \\leq 8\\left(\\mathbb{E}\\left|X_{t}\\right|^{r}\\right)^{1 / r}\\left(\\mathbb{E}\\left|Z_{t}\\right|^{q}\\right)^{1 / q} \\alpha(\\ell)^{1-1 / r-1 / q}\n\\]\nas claimed. Proof of Theorem 14.13.3 Set \\(\\eta_{t-\\ell}=\\operatorname{sgn}\\left(\\mathbb{E}\\left[Z_{t} \\mid \\mathscr{F}_{-\\infty}^{t-\\ell}\\right]\\right)\\) which satisfies \\(\\left|\\eta_{t-\\ell}\\right| \\leq 1\\). Since \\(\\eta_{t-\\ell}\\) is \\(\\mathscr{F}_{-\\infty}^{t-\\ell}-\\) measurable, iterated expectations, using (14.73) with \\(C=1\\), the conditional Jensen’s inequality (B.28), and iterated expectations,\n\\[\n\\begin{aligned}\n\\mathbb{E}\\left|\\mathbb{E}\\left[Z_{t} \\mid \\mathscr{F}_{-\\infty}^{t-\\ell}\\right]\\right| &=\\mathbb{E}\\left[\\eta_{t-\\ell} \\mathbb{E}\\left[Z_{t} \\mid \\mathscr{F}_{-\\infty}^{t-\\ell}\\right]\\right] \\\\\n&=\\mathbb{E}\\left[\\mathbb{E}\\left[\\eta_{t-\\ell} Z_{t} \\mid \\mathscr{F}_{-\\infty}^{t-\\ell}\\right]\\right] \\\\\n&=\\mathbb{E}\\left[\\eta_{t-\\ell} Z_{t}\\right] \\\\\n& \\leq 6\\left(\\mathbb{E}\\left|\\mathbb{E}\\left[Z_{t} \\mid \\mathscr{F}_{-\\infty}^{t-\\ell}\\right]\\right|^{r}\\right)^{1 / r} \\alpha(\\ell)^{1-1 / r} \\\\\n& \\leq 6\\left(\\mathbb{E}\\left(\\mathbb{E}\\left[\\left|Z_{t}\\right|^{r} \\mid \\mathscr{F}_{-\\infty}^{t-\\ell}\\right]\\right)\\right)^{1 / r} \\alpha(\\ell)^{1-1 / r} \\\\\n&=6\\left(\\mathbb{E}\\left|Z_{t}\\right|^{r} \\mid\\right)^{1 / r} \\alpha(\\ell)^{1-1 / r}\n\\end{aligned}\n\\]\nas claimed.\nProof of Theorem 14.15 By the Cramér-Wold device (Theorem \\(8.4\\) of Probability and Statistics for Economists) it is sufficient to prove the result for the scalar case. Our proof method is based on a MDS approximation. The trick is to establish the relationship\n\\[\nu_{t}=e_{t}+Z_{t}-Z_{t+1}\n\\]\nwhere \\(e_{t}\\) is a strictly stationary and ergodic MDS with \\(\\mathbb{E}\\left[e_{t}^{2}\\right]=\\Omega\\) and \\(\\mathbb{E}\\left|Z_{t}\\right|<\\infty\\). Defining \\(S_{n}^{e}=\\frac{1}{\\sqrt{n}} \\sum_{t=1}^{n} e_{t}\\), we have\n\\[\nS_{n}=\\frac{1}{\\sqrt{n}} \\sum_{t=1}^{n}\\left(e_{t}+Z_{t}-Z_{t+1}\\right)=S_{n}^{e}+\\frac{Z_{1}}{\\sqrt{n}}-\\frac{Z_{n+1}}{\\sqrt{n}} .\n\\]\nThe first component on the right side is asymptotically \\(\\mathrm{N}(0, \\Omega)\\) by the MDS CLT (Theorem 14.11). The second and third terms are \\(o_{p}(1)\\) by Markov’s inequality (B.36).\nThe desired relationship (14.74) holds as follows. Set \\(\\mathscr{F}_{t}=\\sigma\\left(\\ldots, u_{t-1}, u_{t}\\right)\\),\n\\[\ne_{t}=\\sum_{\\ell=0}^{\\infty}\\left(\\mathbb{E}\\left[u_{t+\\ell} \\mid \\mathscr{F}_{t}\\right]-\\mathbb{E}\\left[u_{t+\\ell} \\mid \\mathscr{F}_{t-1}\\right]\\right)\n\\]\nand\n\\[\nZ_{t}=\\sum_{\\ell=0}^{\\infty} \\mathbb{E}\\left[u_{t+\\ell} \\mid \\mathscr{F}_{t-1}\\right] .\n\\]\nYou can verify that these definitions satisfy (14.74) given \\(\\mathbb{E}\\left[u_{t} \\mid \\mathscr{F}_{t}\\right]=u_{t}\\). The variable \\(Z_{t}\\) has a finite expectation because by the triangle inequality (B.1), Theorem 14.13.3, and the assumptions\n\\[\n\\mathbb{E}\\left|Z_{t}\\right|=\\mathbb{E}\\left|\\sum_{\\ell=0}^{\\infty} \\mathbb{E}\\left[u_{t+\\ell} \\mid \\mathscr{F}_{t-1}\\right]\\right| \\leq 6\\left(\\mathbb{E}\\left|u_{t}\\right|^{r}\\right)^{1 / r} \\sum_{\\ell=0}^{\\infty} \\alpha(\\ell)^{1-1 / r}<\\infty\n\\]\nthe final inequality because \\(\\sum_{\\ell=0}^{\\infty} \\alpha(\\ell)^{1-2 / r}<\\infty\\) implies \\(\\sum_{\\ell=0}^{\\infty} \\alpha(\\ell)^{1-1 / r}<\\infty\\).\nThe series \\(e_{t}\\) in (14.76) has a finite expectation by the same calculation as for \\(Z_{t}\\). It is a MDS since by iterated expectations\n\\[\n\\begin{aligned}\n\\mathbb{E}\\left[e_{t} \\mid \\mathscr{F}_{t-1}\\right] &=\\mathbb{E}\\left[\\sum_{\\ell=0}^{\\infty}\\left(\\mathbb{E}\\left[u_{t+\\ell} \\mid \\mathscr{F}_{t}\\right]-\\mathbb{E}\\left[u_{t+\\ell} \\mid \\mathscr{F}_{t-1}\\right]\\right) \\mid \\mathscr{F}_{t-1}\\right] \\\\\n&=\\sum_{\\ell=0}^{\\infty}\\left(\\mathbb{E}\\left[\\mathbb{E}\\left[u_{t+\\ell} \\mid \\mathscr{F}_{t}\\right] \\mid \\mathscr{F}_{t-1}\\right]-\\mathbb{E}\\left[\\mathbb{E}\\left[u_{t+\\ell} \\mid \\mathscr{F}_{t-1}\\right] \\mid \\mathscr{F}_{t-1}\\right]\\right) \\\\\n&=\\sum_{\\ell=0}^{\\infty}\\left(\\mathbb{E}\\left[u_{t+\\ell} \\mid \\mathscr{F}_{t-1}\\right]-\\mathbb{E}\\left[u_{t+\\ell} \\mid \\mathscr{F}_{t-1}\\right]\\right) \\\\\n&=0 .\n\\end{aligned}\n\\]\nIt is strictly stationary and ergodic by Theorem \\(14.2\\) because it is a function of the history \\(\\left(\\ldots, u_{t-1}, u_{t}\\right)\\).\nThe proof is completed by showing that \\(e_{t}\\) has a finite variance which equals \\(\\Omega\\). The trickiest step is to show that \\(\\operatorname{var}\\left[e_{t}\\right]<\\infty\\). Since\n\\[\n\\mathbb{E}\\left|S_{n}\\right| \\leq \\sqrt{\\operatorname{var}\\left[S_{n}\\right]} \\rightarrow \\sqrt{\\Omega}\n\\]\n(as shown in (14.17)) it follows that \\(\\mathbb{E}\\left|S_{n}\\right| \\leq 2 \\sqrt{\\Omega}\\) for \\(n\\) sufficiently large. Using (14.75) and \\(\\mathbb{E}\\left|Z_{t}\\right|<\\infty\\), for \\(n\\) sufficiently large,\n\\[\n\\mathbb{E}\\left|S_{n}^{e}\\right| \\leq \\mathbb{E}\\left|S_{n}\\right|+\\frac{\\mathbb{E}\\left|Z_{1}\\right|}{\\sqrt{n}}+\\frac{\\mathbb{E}\\left|Z_{n+1}\\right|}{\\sqrt{n}} \\leq 3 \\sqrt{\\Omega} .\n\\]\nNow define \\(e_{B t}=e_{t} \\mathbb{1}\\left\\{\\left|e_{t}\\right| \\leq B\\right\\}-\\mathbb{E}\\left[e_{t} \\mathbb{1}\\left\\{\\left|e_{t}\\right| \\leq B\\right\\} \\mid \\mathscr{F}_{t-1}\\right]\\) which is a bounded MDS. By Theorem 14.11, \\(\\frac{1}{\\sqrt{n}} \\sum_{t=1}^{n} e_{B t} \\stackrel{d}{\\longrightarrow} \\mathrm{N}\\left(0, \\sigma_{B}^{2}\\right)\\) where \\(\\sigma_{B}^{2}=\\mathbb{E}\\left[e_{B t}^{2}\\right]\\). Since the sequence is uniformly integrable this implies\n\\[\n\\mathbb{E}\\left|\\frac{1}{\\sqrt{n}} \\sum_{t=1}^{n} e_{B t}\\right| \\longrightarrow \\mathbb{E}\\left|\\mathrm{N}\\left(0, \\sigma_{B}^{2}\\right)\\right|=\\sqrt{\\frac{2}{\\pi}} \\sigma_{B}\n\\]\nusing \\(\\mathbb{E}|\\mathrm{N}(0,1)|=2 / \\pi\\). We want to show that \\(\\operatorname{var}\\left[e_{t}\\right]<\\infty\\). Suppose not. Then \\(\\sigma_{B} \\rightarrow \\infty\\) as \\(B \\rightarrow \\infty\\), so there will be some \\(B\\) sufficiently large such that the right-side of (14.78) exceeds the right-side of (14.77). This is a contradiction. We deduce that \\(\\operatorname{var}\\left[e_{t}\\right]<\\infty\\).\nExamining (14.75), we see that since var \\(\\left[S_{n}\\right] \\rightarrow \\Omega<\\infty\\) and \\(\\operatorname{var}\\left[S_{n}^{e}\\right]=\\operatorname{var}\\left[e_{t}\\right]<\\infty\\) then \\(\\operatorname{var}\\left[Z_{1}-Z_{n+1}\\right] / n<\\) \\(\\infty\\). Since \\(Z_{t}\\) is stationary, we deduce that \\(\\operatorname{var}\\left[Z_{1}-Z_{n+1}\\right]<\\infty\\). Equation (14.75) implies var \\(\\left[e_{t}\\right]=\\operatorname{var}\\left[S_{n}^{e}\\right]=\\) \\(\\operatorname{var}\\left[S_{n}\\right]+o(1) \\rightarrow \\Omega\\). We deduce that \\(\\operatorname{var}\\left[e_{t}\\right]=\\Omega\\) as claimed.\nProof of Theorem 14.17 (Sketch) Consider the projection of \\(Y_{t}\\) onto \\(\\left(\\ldots, e_{t-1}, e_{t}\\right)\\). Since the projection errors \\(e_{t}\\) are uncorrelated, the coefficients of this projection are the bivariate projection coefficients \\(b_{j}=\\) \\(\\mathbb{E}\\left[Y_{t} e_{t-j}\\right] / \\mathbb{E}\\left[e_{t-j}^{2}\\right]\\). The leading coefficient is\n\\[\nb_{0}=\\frac{\\mathbb{E}\\left[Y_{t} e_{t}\\right]}{\\sigma^{2}}=\\frac{\\sum_{j=1}^{\\infty} \\alpha_{j} \\mathbb{E}\\left[Y_{t-j} e_{t}\\right]+\\mathbb{E}\\left[e_{t}^{2}\\right]}{\\sigma^{2}}=1\n\\]\nusing Theorem 14.16. By Bessel’s Inequality (Brockwell and Davis, 1991, Corollary 2.4.1),\n\\[\n\\sum_{j=1}^{\\infty} b_{j}^{2}=\\sigma^{-4} \\sum_{j=1}^{\\infty}\\left(\\mathbb{E}\\left[Y_{t} e_{t}\\right]\\right)^{2} \\leq \\sigma^{-4}\\left(\\mathbb{E}\\left[Y_{t}^{2}\\right]\\right)^{2}<\\infty\n\\]\nbecause \\(\\mathbb{E}\\left[Y_{t}^{2}\\right]<\\infty\\) by the assumption of covariance stationarity.\nThe error from the projection of \\(Y_{t}\\) onto \\(\\left(\\ldots, e_{t-1}, e_{t}\\right)\\) is \\(\\mu_{t}=Y_{t}-\\sum_{j=0}^{\\infty} b_{j} e_{t-j}\\). The fact that this can be written as (14.22) is technical. See Theorem 5.7.1 of Brockwell and Davis (1991). Proof of Theorem 14.22 In the text we showed that \\(\\left|\\lambda_{j}\\right|<1\\) is sufficient for \\(Y_{t}\\) to be strictly stationary and ergodic. We now verify that \\(\\left|\\lambda_{j}\\right|<1\\) is equivalent to (14.35)-(14.37). The roots \\(\\lambda_{j}\\) are defined in (14.34). Consider separately the cases of real roots and complex roots.\nSuppose that the roots are real, which occurs when \\(\\alpha_{1}^{2}+4 \\alpha_{2} \\geq 0\\). Then \\(\\left|\\lambda_{j}\\right|<1\\) iff \\(\\left|\\alpha_{1}\\right|<2\\) and\n\\[\n\\frac{\\alpha_{1}+\\sqrt{\\alpha_{1}^{2}+4 \\alpha_{2}}}{2}<1 \\quad \\text { and } \\quad-1<\\frac{\\alpha_{1}-\\sqrt{\\alpha_{1}^{2}+4 \\alpha_{2}}}{2} .\n\\]\nEquivalently, this holds iff\n\\[\n\\alpha_{1}^{2}+4 \\alpha_{2}<\\left(2-\\alpha_{1}\\right)^{2}=4-4 \\alpha_{1}+\\alpha_{1}^{2} \\quad \\text { and } \\quad \\alpha_{1}^{2}+4 \\alpha_{2}<\\left(2+\\alpha_{1}\\right)^{2}=4+4 \\alpha_{1}+\\alpha_{1}^{2}\n\\]\nor equivalently iff\n\\[\n\\alpha_{2}<1-\\alpha_{1} \\quad \\text { and } \\quad \\alpha_{2}<1+\\alpha_{1}\n\\]\nwhich are (14.35) and (14.36). \\(\\alpha_{1}^{2}+4 \\alpha_{2} \\geq 0\\) and \\(\\left|\\alpha_{1}\\right|<2\\) imply \\(\\alpha_{2} \\geq-\\alpha_{1}^{2} / 4 \\geq-1\\), which is (14.37).\nNow suppose the roots are complex, which occurs when \\(\\alpha_{1}^{2}+4 \\alpha_{2}<0\\). The squared modulus of the roots \\(\\lambda_{j}=\\left(\\alpha_{1} \\pm \\sqrt{\\alpha_{1}^{2}+4 \\alpha_{2}}\\right) / 2\\) are\n\\[\n\\left|\\lambda_{j}\\right|^{2}=\\left(\\frac{\\alpha_{1}}{2}\\right)^{2}-\\left(\\frac{\\sqrt{\\alpha_{1}^{2}+4 \\alpha_{2}}}{2}\\right)^{2}=-\\alpha_{2} .\n\\]\nThus the requirement \\(\\left|\\lambda_{j}\\right|<1\\) is satisfied iff \\(\\alpha_{2}>-1\\), which is (14.37). \\(\\alpha_{1}^{2}+4 \\alpha_{2}<0\\) and \\(\\alpha_{2}>-1\\) imply \\(\\alpha_{1}^{2}<\\) \\(-4 \\alpha_{2}<4\\), so \\(\\left|\\alpha_{1}\\right|<2\\). \\(\\alpha_{1}^{2}+4 \\alpha_{2}<0\\) and \\(\\left|\\alpha_{1}\\right|<2\\) imply \\(\\alpha_{1}+\\alpha_{2}<\\alpha_{1}-\\alpha_{1}^{2} / 4<1\\) and \\(\\alpha_{2}-\\alpha_{1}<-\\alpha_{1}^{2} / 4-\\alpha_{1}<1\\) which are (14.35) and (14.36).\nProof of Theorem 14.23 To complete the proof we need to establish that the eigenvalues \\(\\lambda_{j}\\) of \\(\\boldsymbol{A}\\) defined in (14.40) equal the reciprocals of the roots \\(r_{j}\\) of the autoregressive polynomial \\(\\alpha(z)\\) of (14.39). Our goal is therefore to show that if \\(\\lambda\\) satisfies \\(\\operatorname{det}\\left(\\boldsymbol{A}-\\boldsymbol{I}_{p} \\lambda\\right)=0\\) then it satisfies \\(\\alpha(1 / \\lambda)=0\\).\nNotice that\n\\[\n\\boldsymbol{A}-\\boldsymbol{I}_{p} \\lambda=\\left(\\begin{array}{cc}\n-\\lambda+\\alpha_{1} & \\widetilde{\\alpha}^{\\prime} \\\\\na & \\boldsymbol{B}\n\\end{array}\\right)\n\\]\nwhere \\(\\widetilde{\\alpha}^{\\prime}=\\left(\\alpha_{2}, \\ldots, \\alpha_{p}\\right), a^{\\prime}=(1,0, \\ldots, 0)\\), and \\(\\boldsymbol{B}\\) is a lower-diagonal matrix with \\(-\\lambda\\) on the diagonal and 1 immediately below the diagonal. Notice that \\(\\operatorname{det}(\\boldsymbol{B})=(-\\lambda)^{p-1}\\) and by direct calculation\n\\[\n\\boldsymbol{B}^{-1}=-\\left(\\begin{array}{ccccc}\n\\lambda^{-1} & 0 & \\cdots & 0 & 0 \\\\\n\\lambda^{-2} & \\lambda^{-1} & \\cdots & 0 & 0 \\\\\n\\lambda^{-3} & \\lambda^{-2} & \\cdots & 0 & 0 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots & \\vdots \\\\\n\\lambda^{-p+1} & \\lambda^{-p+2} & \\cdots & \\lambda^{-2} & \\lambda^{-1}\n\\end{array}\\right) .\n\\]\nUsing the properties of the determinant (Theorem A.1.5)\n\\[\n\\begin{aligned}\n\\operatorname{det}\\left(\\boldsymbol{A}-\\boldsymbol{I}_{p} \\lambda\\right) &=\\operatorname{det}\\left(\\begin{array}{cc}\n-\\lambda+\\alpha_{1} & \\widetilde{\\alpha}^{\\prime} \\\\\na & \\boldsymbol{B}\n\\end{array}\\right) \\\\\n&=\\operatorname{det}(\\boldsymbol{B})\\left(-\\lambda+\\alpha_{1}-\\widetilde{\\alpha}^{\\prime} \\boldsymbol{B}^{-1} a\\right) \\\\\n&=(-\\lambda)^{p}\\left(1-\\alpha_{1} \\lambda^{-1}-\\alpha_{2} \\lambda^{-2}-\\alpha_{3} \\lambda^{-3}-\\cdots-\\alpha_{p} \\lambda^{-p}\\right) \\\\\n&=(-\\lambda)^{p} \\alpha(1 / \\lambda) .\n\\end{aligned}\n\\]\nThus if \\(\\lambda\\) satisfies \\(\\operatorname{det}\\left(\\boldsymbol{A}-\\boldsymbol{I}_{p} \\lambda\\right)=0\\) then \\(\\alpha(1 / \\lambda)=0\\) as required.\nProof of Theorem 14.24 By the Fundamental Theorem of Algebra we can factor the autoregressive polynomial as \\(\\alpha(z)=\\prod_{\\ell=1}^{p}\\left(1-\\lambda_{\\ell} z\\right)\\) where \\(\\lambda_{\\ell}=r_{\\ell}^{-1}\\). By assumption \\(\\left|\\lambda_{\\ell}\\right|<1\\). Inverting the autoregressive polynomial we obtain\n\\[\n\\begin{aligned}\n\\alpha(z)^{-1} &=\\prod_{\\ell=1}^{p}\\left(1-\\lambda_{\\ell} z\\right)^{-1} \\\\\n&=\\prod_{\\ell=1}^{p}\\left(\\sum_{j=0}^{\\infty} \\lambda_{\\ell}^{j} z^{j}\\right) \\\\\n&=\\sum_{j=0}^{\\infty}\\left(\\sum_{i_{1}+\\cdots+i_{p}=j} \\lambda_{1}^{i_{1}} \\cdots \\lambda_{p}^{i_{p}}\\right) z^{j} \\\\\n&=\\sum_{j=0}^{\\infty} b_{j} z^{j}\n\\end{aligned}\n\\]\nwith \\(b_{j}=\\sum_{i_{1}+\\cdots+i_{p}=j} \\lambda_{1}^{i_{1}} \\cdots \\lambda_{p}^{i_{p}}\\)\nUsing the triangle inequality and the stars and bars theorem (Theorem \\(1.10\\) of Probability and Statistics for Economists)\n\\[\n\\begin{aligned}\n\\left|b_{j}\\right| & \\leq \\sum_{i_{1}+\\cdots+i_{p}=j}\\left|\\lambda_{1}\\right|^{i_{1}} \\cdots\\left|\\lambda_{p}\\right|^{i_{p}} \\\\\n& \\leq \\sum_{i_{1}+\\cdots+i_{p}=j} \\lambda^{j} \\\\\n& \\leq\\left(\\begin{array}{c}\np+j-1 \\\\\nj\n\\end{array}\\right) \\lambda^{j} \\\\\n&=\\frac{(p+j-1) !}{(p-1) ! j !} \\lambda^{j} \\\\\n& \\leq(j+1)^{p} \\lambda^{j}\n\\end{aligned}\n\\]\nas claimed. We next verify the convergence of \\(\\sum_{j=0}^{\\infty}\\left|b_{j}\\right| \\leq \\sum_{j=0}^{\\infty}(j+1)^{p} \\lambda^{j}\\). Note that\n\\[\n\\lim _{j \\rightarrow \\infty} \\frac{(j+1)^{p} \\lambda^{j}}{(j)^{p} \\lambda^{j-1}}=\\lambda<1\n\\]\nBy the ratio test (Theorem A.3.2 of Probability and Statistics for Economists) \\(\\sum_{j=0}^{\\infty}(j+1)^{p} \\lambda^{j}\\) is convergent.\nProof of Theorem 14.27 If \\(\\boldsymbol{Q}\\) is singular then there is some \\(\\gamma\\) such that \\(\\gamma^{\\prime} \\boldsymbol{Q} \\gamma=0\\). We can normalize \\(\\gamma\\) to have a unit coefficient on \\(Y_{t-1}\\) (or the first non-zero coefficient other than the intercept). We then have that \\(\\mathbb{E}\\left[\\left(Y_{t-1}-\\left(1, Y_{t-2}, \\ldots, Y_{t-p)}\\right)^{\\prime} \\phi\\right)^{2}\\right]=0\\) for some \\(\\phi\\), or equivalently \\(\\mathbb{E}\\left[\\left(Y_{t}-\\left(1, Y_{t-1}, \\ldots, Y_{t-p+1)}\\right)^{\\prime} \\phi\\right)^{2}\\right]=\\) 0. Setting \\(\\beta=\\left(\\phi^{\\prime}, 0\\right)^{\\prime}\\) this implies \\(\\mathbb{E}\\left[\\left(Y_{t}-\\beta^{\\prime} X_{t}\\right)^{2}\\right]=0\\). Since \\(\\alpha\\) is the best linear predictor we must have \\(\\beta=\\alpha\\). This implies \\(\\sigma^{2}=\\mathbb{E}\\left[\\left(Y_{t}-\\alpha^{\\prime} X_{t}\\right)^{2}\\right]=0\\). This contradicts the assumption \\(\\sigma^{2}>0\\). We conclude that \\(\\boldsymbol{Q}\\) is not singular."
  },
  {
    "objectID": "chpt14-time-series.html#exercises",
    "href": "chpt14-time-series.html#exercises",
    "title": "14  Time Series",
    "section": "14.53 Exercises",
    "text": "14.53 Exercises\nExercise 14.1 For a scalar time series \\(Y_{t}\\) define the sample autocovariance and autocorrelation\n\\[\n\\begin{aligned}\n&\\widehat{\\gamma}(k)=n^{-1} \\sum_{t=k+1}^{n}\\left(Y_{t}-\\bar{Y}\\right)\\left(Y_{t-k}-\\bar{Y}\\right) \\\\\n&\\widehat{\\rho}(k)=\\frac{\\widehat{\\gamma}(k)}{\\widehat{\\gamma}(0)}=\\frac{\\sum_{t=k+1}^{n}\\left(Y_{t}-\\bar{Y}\\right)\\left(Y_{t-k}-\\bar{Y}\\right)}{\\sum_{t=1}^{n}\\left(Y_{t}-\\bar{Y}\\right)^{2}} .\n\\end{aligned}\n\\]\nAssume the series is strictly stationary, ergodic, strictly stationary, and \\(\\mathbb{E}\\left[Y_{t}^{2}\\right]<\\infty\\).\nShow that \\(\\widehat{\\gamma}(k) \\underset{p}{\\longrightarrow} \\gamma(k)\\) and \\(\\widehat{\\rho}(k) \\underset{p}{\\longrightarrow} \\gamma(k)\\) as \\(n \\rightarrow \\infty\\). (Use the Ergodic Theorem.)\nExercise 14.2 Show that if \\(\\left(e_{t}, \\mathscr{F}_{t}\\right)\\) is a MDS and \\(X_{t}\\) is \\(\\mathscr{F}_{t}\\)-measurable then \\(u_{t}=X_{t-1} e_{t}\\) is a MDS.\nExercise 14.3 Let \\(\\sigma_{t}^{2}=\\mathbb{E}\\left[e_{t}^{2} \\mid \\mathscr{F}_{t-1}\\right]\\). Show that \\(u_{t}=e_{t}^{2}-\\sigma_{t}^{2}\\) is a MDS.\nExercise 14.4 Continuing the previous exercise, show that if \\(\\mathbb{E}\\left[e_{t}^{4}\\right]<\\infty\\) then\n\\[\nn^{-1 / 2} \\sum_{t=1}^{n}\\left(e_{t}^{2}-\\sigma_{t}^{2}\\right) \\underset{d}{\\longrightarrow} \\mathrm{N}\\left(0, v^{2}\\right) \\text {. }\n\\]\nExpress \\(v^{2}\\) in terms of the moments of \\(e_{t}\\).\nExercise 14.5 A stochastic volatility model is\n\\[\n\\begin{aligned}\nY_{t} &=\\sigma_{t} e_{t} \\\\\n\\log \\sigma_{t}^{2} &=\\omega+\\beta \\log \\sigma_{t-1}^{2}+u_{t}\n\\end{aligned}\n\\]\nwhere \\(e_{t}\\) and \\(u_{t}\\) are independent i.i.d. \\(\\mathrm{N}(0,1)\\) shocks.\n\nWrite down an information set for which \\(Y_{t}\\) is a MDS.\nShow that if \\(|\\beta|<1\\) then \\(Y_{t}\\) is strictly stationary and ergodic.\n\nExercise 14.6 Verify the formula \\(\\rho(1)=\\theta /\\left(1+\\theta^{2}\\right)\\) for a MA(1) process.\nExercise 14.7 Verify the formula \\(\\rho(k)=\\left(\\sum_{j=0}^{\\infty} \\theta_{j+k} \\theta_{j}\\right) /\\left(\\sum_{j=0}^{q} \\theta_{j}^{2}\\right)\\) for a \\(\\mathrm{MA}(\\infty)\\) process.\nExercise 14.8 Suppose \\(Y_{t}=Y_{t-1}+e_{t}\\) with \\(e_{t}\\) i.i.d. \\((0,1)\\) and \\(Y_{0}=0\\). Find var \\(\\left[Y_{t}\\right]\\). Is \\(Y_{t}\\) stationary?\nExercise 14.9 Take the AR(1) model with no intercept \\(Y_{t}=\\alpha_{1} Y_{t-1}+e_{t}\\).\n\nFind the impulse response function \\(b_{j}=\\frac{\\partial}{\\partial e_{t}} Y_{t+j}\\).\nLet \\(\\widehat{\\alpha}_{1}\\) be the least squares estimator of \\(\\alpha_{1}\\). Find an estimator of \\(b_{j}\\).\nLet \\(s\\left(\\widehat{\\alpha}_{1}\\right)\\) be a standard error for \\(\\widehat{\\alpha}_{1}\\). Use the delta method to find a 95% asymptotic confidence interval for \\(b_{j}\\)\n\nExercise 14.10 Take the AR(2) model \\(Y_{t}=\\alpha_{1} Y_{t-1}+\\alpha_{2} Y_{t-1}+e_{t}\\). (a) Find expressions for the impulse responses \\(b_{1}, b_{2}, b_{3}\\) and \\(b_{4}\\).\n\nLet \\(\\left(\\widehat{\\alpha}_{1}, \\widehat{\\alpha}_{2}\\right)\\) be the least squares estimator. Find an estimator of \\(b_{2}\\).\nLet \\(\\widehat{\\boldsymbol{V}}\\) be the estimated covariance matrix for the coefficients. Use the delta method to find a \\(95 %\\) asymptotic confidence interval for \\(b_{2}\\).\n\nExercise 14.11 Show that the models\n\\[\n\\alpha(\\mathrm{L}) Y_{t}=\\alpha_{0}+e_{t}\n\\]\nand\n\\[\n\\begin{aligned}\n&\\alpha(\\mathrm{L}) Y_{t}=\\mu+u_{t} \\\\\n&\\alpha(\\mathrm{L}) u_{t}=e_{t}\n\\end{aligned}\n\\]\nare identical. Find an expression for \\(\\mu\\) in terms of \\(\\alpha_{0}\\) and \\(\\alpha(\\mathrm{L})\\).\nExercise 14.12 Take the model\n\\[\n\\begin{aligned}\n\\alpha(\\mathrm{L}) Y_{t} &=u_{t} \\\\\n\\beta(\\mathrm{L}) u_{t} &=e_{t}\n\\end{aligned}\n\\]\nwhere \\(\\alpha(\\mathrm{L})\\) and \\(\\beta(\\mathrm{L})\\) are \\(p\\) and \\(q\\) order lag polynomials. Show that these equations imply that\n\\[\n\\gamma(\\mathrm{L}) Y_{t}=e_{t}\n\\]\nfor some lag polynomial \\(\\gamma(\\mathrm{L})\\). What is the order of \\(\\gamma(\\mathrm{L})\\) ?\nExercise 14.13 Suppose that \\(Y_{t}=e_{t}+u_{t}+\\theta u_{t-1}\\) where \\(u_{t}\\) and \\(e_{t}\\) are mutually independent i.i.d. \\((0,1)\\) processes.\n\nShow that \\(Y_{t}\\) is a MA(1) process \\(Y_{t}=\\eta_{t}+\\psi \\eta_{t-1}\\) for a white noise error \\(\\eta_{t}\\).\n\nHint: Calculate the autocorrelation function of \\(Y_{t}\\).\n\nFind an expression for \\(\\psi\\) in terms of \\(\\theta\\).\n\\(\\operatorname{Suppose} \\theta=1\\). Find \\(\\psi\\).\n\nExercise 14.14 Suppose that\n\\[\n\\begin{aligned}\nY_{t} &=X_{t}+e_{t} \\\\\nX_{t} &=\\alpha X_{t-1}+u_{t}\n\\end{aligned}\n\\]\nwhere the errors \\(e_{t}\\) and \\(u_{t}\\) are mutually independent i.i.d. processes. Show that \\(Y_{t}\\) is an ARMA(1,1) process.\nExercise 14.15 A Gaussian AR model is an autoregression with i.i.d. \\(\\mathrm{N}\\left(0, \\sigma^{2}\\right)\\) errors. Consider the Gaussian AR(1) model\n\\[\n\\begin{aligned}\nY_{t} &=\\alpha_{0}+\\alpha_{1} Y_{t-1}+e_{t} \\\\\ne_{t} \\sim \\mathrm{N}\\left(0, \\sigma^{2}\\right)\n\\end{aligned}\n\\]\nwith \\(\\left|\\alpha_{1}\\right|<1\\). Show that the marginal distribution of \\(Y_{t}\\) is also normal:\n\\[\nY_{t} \\sim \\mathrm{N}\\left(\\frac{\\alpha_{0}}{1-\\alpha_{1}}, \\frac{\\sigma^{2}}{1-\\alpha_{1}^{2}}\\right) .\n\\]\nHint: Use the MA representation of \\(Y_{t}\\). Exercise 14.16 Assume that \\(Y_{t}\\) is a Gaussian \\(\\operatorname{AR}(1)\\) as in the previous exercise. Calculate the moments\n\\[\n\\begin{aligned}\n\\mu &=\\mathbb{E}\\left[Y_{t}\\right] \\\\\n\\sigma_{Y}^{2} &=\\mathbb{E}\\left[\\left(Y_{t}-\\mu\\right)^{2}\\right] \\\\\n\\kappa &=\\mathbb{E}\\left[\\left(Y_{t}-\\mu\\right)^{4}\\right]\n\\end{aligned}\n\\]\nA colleague suggests estimating the parameters \\(\\left(\\alpha_{0}, \\alpha_{1}, \\sigma^{2}\\right)\\) of the Gaussian AR(1) model by GMM applied to the corresponding sample moments. He points out that there are three moments and three parameters, so it should be identified. Can you find a flaw in his approach?\nHint: This is subtle.\nExercise 14.17 Take the nonlinear process\n\\[\nY_{t}=Y_{t-1}^{\\alpha} u_{t}^{1-\\alpha}\n\\]\nwhere \\(u_{t}\\) is i.i.d. with strictly positive support.\n\nFind the condition under which \\(Y_{t}\\) is strictly stationary and ergodic.\nFind an explicit expression for \\(Y_{t}\\) as a function of \\(\\left(u_{t}, u_{t-1}, \\ldots\\right)\\).\n\nExercise 14.18 Take the quarterly series pnfix (nonresidential real private fixed investment) from FRED-QD.\n\nTransform the series into quarterly growth rates.\nEstimate an AR(4) model. Report using heteroskedastic-consistent standard errors.\nRepeat using the Newey-West standard errors, using \\(M=5\\).\nComment on the magnitude and interpretation of the coefficients.\nCalculate (numerically) the impulse responses for \\(j=1, \\ldots, 10\\).\n\nExercise 14.19 Take the quarterly series oilpricex (real price of crude oil) from FRED-QD.\n\nTransform the series by taking first differences.\nEstimate an AR(4) model. Report using heteroskedastic-consistent standard errors.\nTest the hypothesis that the real oil prices is a random walk by testing that the four AR coefficients jointly equal zero.\nInterpret the coefficient estimates and test result.\n\nExercise 14.20 Take the monthly series unrate (unemployment rate) from FRED-MD.\n\nEstimate AR(1) through AR(8) models, using the sample starting in \\(1960 \\mathrm{~m} 1\\) so that all models use the same observations.\nCompute the AIC for each AR model and report.\nWhich AR model has the lowest AIC? (d) Report the coefficient estimates and standard errors for the selected model.\n\nExercise 14.21 Take the quarterly series unrate (unemployment rate) and claimsx (initial claims) from FRED-QD. “Initial claims” are the number of individuals who file for unemployment insurance.\n\nEstimate a distributed lag regression of the unemployment rate on initial claims. Use lags 1 through 4. Which standard error method is appropriate?\nEstimate an autoregressive distributed lag regression of the unemployment rate on initial claims. Use lags 1 through 4 for both variables.\nTest the hypothesis that initial claims does not Granger cause the unemployment rate.\nInterpret your results.\n\nExercise 14.22 Take the quarterly series gdpcl (real GDP) and houst (housing starts) from FRED-QD. “Housing starts” are the number of new houses on which construction is started.\n\nTransform the real GDP series into its one quarter growth rate.\nEstimate a distributed lag regression of GDP growth on housing starts. Use lags 1 through 4. Which standard error method is appropriate?\nEstimate an autoregressive distributed lag regression of GDP growth on housing starts. Use lags 1 through 2 for GDP growth and 1 through 4 for housing starts.\nTest the hypothesis that housing starts does not Granger cause GDP growth.\nInterpret your results."
  },
  {
    "objectID": "chpt15-multiple-time-series.html#introduction",
    "href": "chpt15-multiple-time-series.html#introduction",
    "title": "15  Multivariate Time Series",
    "section": "15.1 Introduction",
    "text": "15.1 Introduction\nA multivariate time series \\(Y_{t}=\\left(Y_{1 t}, \\ldots, Y_{m t}\\right)^{\\prime}\\) is an \\(m \\times 1\\) vector process observed in sequence over time, \\(t=1, \\ldots, n\\). Multivariate time series models primarily focus on the joint modeling of the vector series \\(Y_{t}\\). The most common multivariate time series models used by economists are vector autoregressions (VARs). VARs were introduced to econometrics by Sims (1980).\nSome excellent textbooks and review articles on multivariate time series include Hamilton (1994), Watson (1994), Canova (1995), Lütkepohl (2005), Ramey (2016), Stock and Watson (2016), and Kilian and Lütkepohl (2017)."
  },
  {
    "objectID": "chpt15-multiple-time-series.html#multiple-equation-time-series-models",
    "href": "chpt15-multiple-time-series.html#multiple-equation-time-series-models",
    "title": "15  Multivariate Time Series",
    "section": "15.2 Multiple Equation Time Series Models",
    "text": "15.2 Multiple Equation Time Series Models\nTo motivate vector autoregressions let us start by reviewing the autoregressive distributed lag model of Section \\(14.41\\) for the case of two series \\(Y_{t}=\\left(Y_{1 t}, Y_{2 t}\\right)^{\\prime}\\) with a single lag. An AR-DL model for \\(Y_{1 t}\\) is\n\\[\nY_{1 t}=\\alpha_{0}+\\alpha_{1} Y_{1 t-1}+\\beta_{1} Y_{2 t-1}+e_{1 t} .\n\\]\nSimilarly, an AR-DL model for \\(Y_{2 t}\\) is\n\\[\nY_{2 t}=\\gamma_{0}+\\gamma_{1} Y_{2 t-1}+\\delta_{1} Y_{1 t-1}+e_{2 t} .\n\\]\nThese two equations specify that each variable is a linear function of its own lag and the lag of the other variable. In so doing we find that the variables on the right hand side of each equation are \\(Y_{t-1}\\).\nWe can simplify the equations by combining the regressors stacking the two equations together and writing the vector error as \\(e_{t}=\\left(e_{1 t}, e_{2 t}\\right)^{\\prime}\\) to find\n\\[\nY_{t}=a_{0}+\\boldsymbol{A}_{1} Y_{t-1}+e_{t}\n\\]\nwhere \\(a_{0}\\) is \\(2 \\times 1\\) and \\(\\boldsymbol{A}_{1}\\) is \\(2 \\times 2\\). This is a bivariate vector autoregressive model for \\(Y_{t}\\). It specifies that the multivariate process \\(Y_{t}\\) is a linear function of its own lag \\(Y_{t-1}\\) plus \\(e_{t}\\). It is the combination of two equations each of which is an autoregressive distributed lag model. Thus a multivariate autoregression is a set of autoregressive distributed lag models.\nThe above derivation assumed a single lag. If the equations include \\(p\\) lags of each variable we obtain the \\(p^{t h}\\) order vector autoregressive (VAR) model\n\\[\nY_{t}=a_{0}+\\boldsymbol{A}_{1} Y_{t-1}+\\boldsymbol{A}_{2} Y_{t-2}+\\cdots+\\boldsymbol{A}_{p} Y_{t-p}+e_{t} .\n\\]\nFurthermore, there is nothing special about the two variable case. The notation in (15.1) allows \\(Y_{t}=\\) \\(\\left(Y_{1 t}, \\ldots, Y_{m t}\\right)^{\\prime}\\) to be a vector of dimension \\(m\\) in which case the matrices \\(\\boldsymbol{A}_{\\ell}\\) are \\(m \\times m\\) and the error \\(\\boldsymbol{e}_{t}\\) is \\(m \\times 1\\). We will denote the elements of \\(\\boldsymbol{A}_{\\ell}\\) using the notation\n\\[\n\\boldsymbol{A}_{\\ell}=\\left[\\begin{array}{cccc}\na_{11, \\ell} & a_{12, \\ell} & \\cdots & a_{1 m, \\ell} \\\\\na_{21, \\ell} & a_{22, \\ell} & \\cdots & a_{2 m, \\ell} \\\\\n\\vdots & \\vdots & & \\vdots \\\\\na_{m 1, \\ell} & a_{m 2, \\ell} & \\cdots & a_{m m, \\ell}\n\\end{array}\\right]\n\\]\nThe error \\(e_{t}=\\left(e_{1 t}, \\ldots, e_{m t}\\right)^{\\prime}\\) is the component of \\(Y_{t}\\) which is unforecastable at time \\(t-1\\). However, the components of \\(Y_{t}\\) are contemporaneously correlated. Therefore the contemporaneous covariance matrix \\(\\Sigma=\\mathbb{E}\\left[e e^{\\prime}\\right]\\) is non-diagonal.\nThe VAR model falls in the class of multivariate regression models studied in Chapter 11.\nIn the following several sections we take a step back and provide a rigorous foundation for vector autoregressions for stationary time series."
  },
  {
    "objectID": "chpt15-multiple-time-series.html#linear-projection",
    "href": "chpt15-multiple-time-series.html#linear-projection",
    "title": "15  Multivariate Time Series",
    "section": "15.3 Linear Projection",
    "text": "15.3 Linear Projection\nIn Section \\(14.14\\) we derived the linear projection of the univariate series \\(Y_{t}\\) on its infinite past history. We now extend this to the multivariate case. Define the multivariate infinite past history \\(\\widetilde{Y}_{t-1}=\\) \\(\\left(\\ldots, Y_{t-2}, Y_{t-1}\\right)\\). The projection of \\(Y_{t}\\) onto \\(\\widetilde{Y}_{t-1}\\), written \\(\\mathscr{P}_{t-1}\\left[Y_{t}\\right]=\\mathscr{P}\\left[Y_{t} \\mid \\widetilde{Y}_{t-1}\\right]\\), is unique and has a unique projection error\n\\[\ne_{t}=Y_{t}-\\mathscr{P}_{t-1}\\left[Y_{t}\\right] .\n\\]\nWe will call the projection errors \\(e_{t}\\) the “innnovations”.\nThe innovations \\(e_{t}\\) are mean zero and serially uncorrelated. We state this formally.\nTheorem 15.1 If \\(Y_{t}\\) is covariance stationary it has the projection equation\n\\[\nY_{t}=\\mathscr{P}_{t-1}\\left[Y_{t}\\right]+e_{t} .\n\\]\nThe innovations \\(e_{t}\\) satisfy \\(\\mathbb{E}\\left[e_{t}\\right]=0\\), \\(\\mathbb{E}\\left[e_{t-\\ell} e_{t}^{\\prime}\\right]=0\\) for \\(\\ell \\geq 1\\), and \\(\\Sigma=\\mathbb{E}\\left[e e^{\\prime}\\right]<\\infty\\). If \\(Y_{t}\\) is strictly stationary then \\(e_{t}\\) is strictly stationary.\nThe uncorrelatedness of the projection errors is a property of a multivariate white noise process.\nDefinition 15.1 The vector process \\(e_{t}\\) is multivariate white noise if \\(\\mathbb{E}\\left[e_{t}\\right]=0\\), \\(\\mathbb{E}\\left[e_{t} e_{t}^{\\prime}\\right]=\\Sigma<\\infty\\), and \\(\\mathbb{E}\\left[e_{t} e_{t-\\ell}^{\\prime}\\right]=0\\) for \\(\\ell \\neq 0\\)"
  },
  {
    "objectID": "chpt15-multiple-time-series.html#multivariate-wold-decomposition",
    "href": "chpt15-multiple-time-series.html#multivariate-wold-decomposition",
    "title": "15  Multivariate Time Series",
    "section": "15.4 Multivariate Wold Decomposition",
    "text": "15.4 Multivariate Wold Decomposition\nBy projecting \\(Y_{t}\\) onto the past history of the white noise innovations \\(e_{t}\\) we obtain a multivariate version of the Wold decomposition.\nTheorem 15.2 If \\(Y_{t}\\) is covariance stationary and non-deterministic then it has the linear representation\n\\[\nY_{t}=\\mu+\\sum_{\\ell=0}^{\\infty} \\Theta_{\\ell} e_{t-\\ell}\n\\]\nwhere \\(e_{t}\\) are the white noise projection errors and \\(\\Theta_{0}=\\boldsymbol{I}_{m}\\). The coefficient matrices \\(\\Theta_{\\ell}\\) are \\(m \\times m\\).\nWe can write the moving average representation using the lag operator notation as\n\\[\nY_{t}=\\mu+\\Theta(\\mathrm{L}) e_{t}\n\\]\nwhere\n\\[\n\\Theta(z)=\\sum_{\\ell=0}^{\\infty} \\Theta_{\\ell} z^{\\ell} .\n\\]\nA multivariate version of Theorem \\(14.19\\) can also be established.\nTheorem 15.3 If \\(Y_{t}\\) is covariance stationary, non-deterministic, with Wold representation \\(Y_{t}=\\Theta(\\mathrm{L}) e_{t}\\), such that \\(\\lambda_{\\min }\\left(\\Theta^{*}(z) \\Theta(z)\\right) \\geq \\delta>0\\) for all complex \\(|z| \\leq 1\\), and for some integer \\(s \\geq 0\\) the Wold coefficients satisfy \\(\\sum_{j=0}^{\\infty}\\left\\|\\sum_{k=0}^{\\infty} k^{s} \\Theta_{j+k}\\right\\|^{2}<\\infty\\), then \\(Y_{t}\\) has an infinite-order autoregressive representation\n\\[\n\\boldsymbol{A} \\text { (L) } Y_{t}=a_{0}+e_{t}\n\\]\nwhere\n\\[\n\\boldsymbol{A}(z)=\\boldsymbol{I}_{m}-\\sum_{\\ell=1}^{\\infty} \\boldsymbol{A}_{\\ell} z^{\\ell}\n\\]\nand the coefficients satisfy \\(\\sum_{k=1}^{\\infty} k^{s}\\left\\|\\boldsymbol{A}_{k}\\right\\|<\\infty\\). The series in (15.4) is convergent.\nFor a proof see Section 2 of Meyer and Kreiss (2015).\nWe can also provide an analog of Theorem 14.6.\nTheorem 15.4 If \\(e_{t} \\in \\mathbb{R}^{m}\\) is strictly stationary, ergodic, \\(\\mathbb{E}\\left\\|e_{t}\\right\\|<\\infty\\), and \\(\\sum_{\\ell=0}^{\\infty}\\left\\|\\Theta_{\\ell}\\right\\|<\\infty\\), then \\(Y_{t}=\\sum_{\\ell=0}^{\\infty} \\Theta_{\\ell} e_{t-\\ell}\\) is strictly stationary and ergodic. The proof of Theorem \\(15.4\\) is a straightforward extension of Theorem \\(14.6\\) so is omitted.\nThe moving average and autoregressive lag polynomials satisfy the relationship \\(\\Theta(z)=\\boldsymbol{A}(z)^{-1}\\).\nFor some purposes (such as impulse response calculations) we need to calculate the moving average coefficient matrices \\(\\Theta_{\\ell}\\) from the autoregressive coefficient matrices \\(\\boldsymbol{A}_{\\ell}\\). While there is not a closed-form solution there is a simple recursion by which the coefficients may be calculated.\nTheorem 15.5 For \\(j \\geq 1, \\Theta_{j}=\\sum_{\\ell=1}^{j} A_{\\ell} \\Theta_{j-\\ell}\\).\nTo see this, suppose for simplicity \\(a_{0}=0\\) and that the innovations satisfy \\(e_{t}=0\\) for \\(t \\neq 0\\). Then \\(Y_{t}=0\\) for \\(t<0\\). Using the regression equation (15.4) for \\(t \\geq 0\\) we solve for each \\(Y_{t}\\). For \\(t=0\\)\n\\[\nY_{0}=e_{0}=\\Theta_{0} e_{0}\n\\]\nwhere \\(\\Theta_{0}=\\boldsymbol{I}_{m}\\). For \\(t=1\\)\n\\[\nY_{1}=\\boldsymbol{A}_{1} Y_{0}=\\boldsymbol{A}_{1} \\Theta_{0} e_{0}=\\Theta_{1} e_{0}\n\\]\nwhere \\(\\Theta_{1}=A_{1} \\Theta_{0}\\). For \\(t=2\\)\n\\[\nY_{2}=\\boldsymbol{A}_{1} Y_{1}+\\boldsymbol{A}_{2} Y_{0}=\\boldsymbol{A}_{1} \\Theta_{1} e_{0}+\\boldsymbol{A}_{2} \\Theta_{0} e_{0}=\\Theta_{2} e_{0}\n\\]\nwhere \\(\\Theta_{2}=A_{1} \\Theta_{1}+A_{2} \\Theta_{0}\\). For \\(t=3\\)\n\\[\nY_{3}=\\boldsymbol{A}_{1} Y_{2}+\\boldsymbol{A}_{2} Y_{1}+\\boldsymbol{A}_{3} Y_{0}=\\boldsymbol{A}_{1} \\Theta_{2} e_{0}+\\boldsymbol{A}_{2} \\Theta_{1} e_{0}+\\boldsymbol{A}_{3} \\Theta_{0} e_{0}=\\Theta_{3} e_{0}\n\\]\nwhere \\(\\Theta_{3}=\\boldsymbol{A}_{1} \\Theta_{2}+\\boldsymbol{A}_{2} \\Theta_{2}+\\boldsymbol{A}_{2} \\Theta_{0}\\). The coefficients satisfy the stated recursion as claimed."
  },
  {
    "objectID": "chpt15-multiple-time-series.html#impulse-response",
    "href": "chpt15-multiple-time-series.html#impulse-response",
    "title": "15  Multivariate Time Series",
    "section": "15.5 Impulse Response",
    "text": "15.5 Impulse Response\nOne of the most important concepts in applied multivariate time series is the impulse response function (IRF) which is defined as the change in \\(Y_{t}\\) due to a change in an innovation or shock. In this section we define the baseline IRF - the unnormalized non-orthogonalized impulse response function - which is the change in \\(Y_{t}\\) due to a change in an innovation \\(e_{t}\\). Specifically, we define the impulse response of variable \\(i\\) with respect to innovation \\(j\\) as the change in the time \\(t\\) projection of the \\(i^{t h}\\) variable \\(Y_{i t+h}\\) due to the \\(j^{t h}\\) innovation \\(e_{j t}\\)\n\\[\n\\operatorname{IRF}_{i j}(h)=\\frac{\\partial}{\\partial e_{j t}} \\mathscr{P}_{t}\\left[Y_{i t+h}\\right] .\n\\]\nThere are \\(m^{2}\\) such responses for each horizon \\(h\\). We can write them as an \\(m \\times m\\) matrix\n\\[\n\\operatorname{IRF}(h)=\\frac{\\partial}{\\partial e_{t}^{\\prime}} \\mathscr{P}_{t}\\left[Y_{t+h}\\right] .\n\\]\nRecall the multivariate Wold representation\n\\[\nY_{t}=\\mu+\\sum_{\\ell=0}^{\\infty} \\Theta_{\\ell} e_{t-\\ell} .\n\\]\nWe can calculate that the projection onto the history at time \\(t\\) is\n\\[\n\\mathscr{P}_{t}\\left[Y_{t+h}\\right]=\\mu+\\sum_{\\ell=h}^{\\infty} \\Theta_{\\ell} e_{t+h-\\ell}=\\mu+\\sum_{\\ell=0}^{\\infty} \\Theta_{h+\\ell} e_{t-\\ell} .\n\\]\nWe deduce that the impulse response matrix is \\(\\operatorname{IRF}(h)=\\Theta_{h}\\), the \\(h^{t h}\\) moving average coefficient matrix. The invididual impulse response is \\(\\operatorname{IRF}_{i j}(h)=\\Theta_{h, i j}\\), the \\(i j^{t h}\\) element of \\(\\Theta_{h}\\).\nHere we have defined the impulse response in terms of the linear projection operator. An alternative is to define the impulse response in terms of the conditional expectation operator. The two coincide when the innovations \\(e_{t}\\) are a martingale difference sequence (and thus when the true process is linear) but otherwise will not coincide.\nTypically we view impulse responses as a function of the horizon \\(h\\) and plot them as a function of \\(h\\) for each pair \\((i, j)\\). The impulse response function \\(\\operatorname{IRF}_{i j}(h)\\) is interpreted as how the \\(i^{t h}\\) variable responds over time to the \\(j^{t h}\\) innovation.\nIn a linear vector autoregression the impulse response function is symmetric in negative and positive innovations. That is, the impact on \\(Y_{i t+h}\\) of a positive innovation \\(e_{j t}=1\\) is \\(\\operatorname{IRF}_{i j}(h)\\) and the impact of a negative innovation \\(e_{j t}=-1\\) is \\(-\\operatorname{IRF}_{i j}(h)\\). Furthermore, the magnitude of the impact is linear in the magnitude of the innovation. Thus the impact of the innovation \\(e_{j t}=2\\) is \\(2 \\times \\operatorname{IRF}_{i j}(h)\\) and the impact of the innovation \\(e_{j t}=-2\\) is \\(-2 \\times \\operatorname{IRF}_{i j}(h)\\). This means that the shape of the impulse response function is unaffected by the magnitude of the innovation. (These are consequences of the linearity of the vector autoregressive model, not necessarily features of the true world.)\nThe impulse response functions can be scaled as desired. One standard choice is to scale so that the innovations correspond to one unit of the impulse variable. Thus if the impulse variable is measured in dollars the impulse response can be scaled to correspond to a change in \\(\\$ 1\\) or some multiple such as a million dollars. If the impulse variable is measured in percentage points (e.g. an interest rate) then the impulse response can be scaled to correspond to a change of one percentage point (e.g. from 3% to \\(4 %\\) ) or to correspond to a change of one basis point (e.g. from 3.05% to 3.06%). Another standard choice is to scale the impulse responses to correspond to a “one standard deviation” innovation. This occurs when the innovations have been scaled to have unit variances. In this latter case impulse response functions can be interpreted as responses due to a “typical” sized (one standard deviation) innovation.\nClosely related to the IRF is the cumulative impulse response function (CIRF) defined as\n\\[\n\\operatorname{CIRF}(h)=\\sum_{\\ell=1}^{h} \\frac{\\partial}{\\partial e_{t}^{\\prime}} \\mathscr{P}_{t}\\left[Y_{t+\\ell}\\right]=\\sum_{\\ell=1}^{h} \\Theta_{\\ell} .\n\\]\nThe cumulative impulse response is the accumulated (summed) responses on \\(Y_{t}\\) from time \\(t\\) to \\(t+h\\). The limit of the cumulative impulse response as \\(h \\rightarrow \\infty\\) is the long-run impulse response matrix\n\\[\n\\boldsymbol{C}=\\lim _{h \\rightarrow \\infty} \\operatorname{CIRF}(h)=\\sum_{\\ell=1}^{\\infty} \\Theta_{\\ell}=\\Theta(1)=\\boldsymbol{A}(1)^{-1} .\n\\]\nThis is the full (summed) effect of the innovation over all time.\nIt is useful to observe that when a VAR is estimated on differenced observations \\(\\Delta Y_{t}\\) then cumulative impulse response is\n\\[\n\\operatorname{CIRF}(h)=\\frac{\\partial}{\\partial e_{t}^{\\prime}} \\mathscr{P}_{t}\\left[\\sum_{\\ell=1}^{h} \\Delta Y_{t+\\ell}\\right]=\\frac{\\partial}{\\partial e_{t}^{\\prime}} \\mathscr{P}_{t}\\left[Y_{t+h}\\right]\n\\]\nwhich is the impulse response for the variable \\(Y_{t}\\) in levels. More generally, when a VAR is estimated with some variables in levels and some in differences then the cumulative impulse response for the second group will coincide with the impulse responses for the same variables measured in levels. It is typical to report cumulative impulse response functions for variables which enter a VAR in differences. In fact, in this context many authors will label the cumulative impulse response as “the impulse response”."
  },
  {
    "objectID": "chpt15-multiple-time-series.html#var1-model",
    "href": "chpt15-multiple-time-series.html#var1-model",
    "title": "15  Multivariate Time Series",
    "section": "15.6 VAR(1) Model",
    "text": "15.6 VAR(1) Model\nThe first-order vector autoregressive process, denoted VAR(1), is\n\\[\nY_{t}=a_{0}+A_{1} Y_{t-1}+e_{t}\n\\]\nwhere \\(e_{t}\\) is a strictly stationary and ergodic white noise process.\nWe are interested in conditions under which \\(Y_{t}\\) is a stationary process. Let \\(\\lambda_{i}(\\boldsymbol{A})\\) denote the \\(i^{\\text {th }}\\) eigenvalue of \\(\\boldsymbol{A}\\).\nTheorem 15.6 If \\(e_{t}\\) is strictly stationary, ergodic, \\(\\mathbb{E}\\left\\|e_{t}\\right\\|<\\infty\\), and \\(\\left|\\lambda_{i}\\left(A_{1}\\right)\\right|<1\\) for \\(i=1, \\ldots, m\\), then the \\(\\operatorname{VAR}(1)\\) process \\(Y_{t}\\) is strictly stationary and ergodic.\nThe proof is given in Section 15.31."
  },
  {
    "objectID": "chpt15-multiple-time-series.html#operatornamevarmathrmp-model",
    "href": "chpt15-multiple-time-series.html#operatornamevarmathrmp-model",
    "title": "15  Multivariate Time Series",
    "section": "15.7 \\(\\operatorname{VAR}(\\mathrm{p})\\) Model",
    "text": "15.7 \\(\\operatorname{VAR}(\\mathrm{p})\\) Model\nThe \\(\\mathbf{p}^{\\text {th }}\\)-order vector autoregressive process, denoted VAR(p), is\n\\[\nY_{t}=a_{0}+\\boldsymbol{A}_{1} Y_{t-1}+\\cdots+\\boldsymbol{A}_{p} Y_{t-p}+e_{t}\n\\]\nwhere \\(e_{t}\\) is a strictly stationary and ergodic white noise process.\nWe can write the model using the lag operator notation as\n\\[\n\\boldsymbol{A} \\text { (L) } Y_{t}=a_{0}+e_{t}\n\\]\nwhere\n\\[\n\\boldsymbol{A}(z)=\\boldsymbol{I}_{m}-\\boldsymbol{A}_{1} z-\\cdots-\\boldsymbol{A}_{p} z^{p} .\n\\]\nThe condition for stationarity of the system can be expressed as a restriction on the roots of the determinantal equation of the autoregressive polynomial. Recall, a \\(\\operatorname{root} r\\) of \\(\\operatorname{det}(\\boldsymbol{A}(z))\\) is a solution to \\(\\operatorname{det}(\\boldsymbol{A}(r))=0\\).\nTheorem \\(15.7\\) If all roots \\(r\\) of \\(\\operatorname{det}(\\boldsymbol{A}(z))\\) satisfy \\(|r|>1\\) then the \\(\\operatorname{VAR}(\\mathrm{p})\\) process \\(Y_{t}\\) is strictly stationary and ergodic.\nThe proof is structurally identical to that of Theorem \\(14.23\\) so is omitted."
  },
  {
    "objectID": "chpt15-multiple-time-series.html#regression-notation",
    "href": "chpt15-multiple-time-series.html#regression-notation",
    "title": "15  Multivariate Time Series",
    "section": "15.8 Regression Notation",
    "text": "15.8 Regression Notation\nDefine the \\((m p+1) \\times 1\\) vector\n\\[\nX_{t}=\\left(\\begin{array}{c}\n1 \\\\\nY_{t-1} \\\\\nY_{t-2} \\\\\n\\vdots \\\\\nY_{t-p}\n\\end{array}\\right)\n\\]\nand the \\(m \\times(m p+1)\\) matrix \\(\\boldsymbol{A}^{\\prime}=\\left(\\begin{array}{lllll}a_{0} & \\boldsymbol{A}_{1} & \\boldsymbol{A}_{2} & \\cdots & \\boldsymbol{A}_{p}\\end{array}\\right)\\). Then the VAR system of equations can be written as\n\\[\nY_{t}=\\boldsymbol{A}^{\\prime} X_{t}+e_{t} .\n\\]\nThis is a multivariate regression model. The error has covariance matrix\n\\[\n\\Sigma=\\mathbb{E}\\left[e_{t} e_{t}^{\\prime}\\right] .\n\\]\nWe can also write the coefficient matrix as \\(\\boldsymbol{A}=\\left(\\begin{array}{llll}a_{1} & a_{2} & \\cdots & a_{m}\\end{array}\\right)\\) where \\(a_{j}\\) is the vector of coefficients for the \\(j^{t h}\\) equation. Thus \\(Y_{j t}=a_{j}^{\\prime} X_{t}+e_{j t}\\).\nIn general, if \\(Y_{t}\\) is strictly stationary we can define the coefficient matrix \\(\\boldsymbol{A}\\) by linear projection.\n\\[\n\\boldsymbol{A}=\\left(\\mathbb{E}\\left[X_{t} X_{t}^{\\prime}\\right]\\right)^{-1} \\mathbb{E}\\left[X_{t} Y_{t}^{\\prime}\\right] .\n\\]\nThis holds whether or not \\(Y_{t}\\) is actually a \\(\\operatorname{VAR}(\\mathrm{p})\\) process. By the properties of projection errors\n\\[\n\\mathbb{E}\\left[X_{t} e_{t}^{\\prime}\\right]=0 .\n\\]\nThe projection coefficient matrix \\(\\boldsymbol{A}\\) is identified if \\(\\mathbb{E}\\left[X_{t} X_{t}^{\\prime}\\right]\\) is invertible.\nTheorem \\(15.8\\) If \\(Y_{t}\\) is strictly stationary and \\(0<\\Sigma<\\infty\\) for \\(\\Sigma\\) defined in (15.6), then \\(\\boldsymbol{Q}=\\mathbb{E}\\left[X_{t} X_{t}^{\\prime}\\right]>0\\) and the coefficient vector (14.46) is identified.\nThe proof is given in Section 15.31."
  },
  {
    "objectID": "chpt15-multiple-time-series.html#estimation",
    "href": "chpt15-multiple-time-series.html#estimation",
    "title": "15  Multivariate Time Series",
    "section": "15.9 Estimation",
    "text": "15.9 Estimation\nFrom Chapter 11 the systems estimator of a multivariate regression is least squares. The estimator can be written as\n\\[\n\\widehat{\\boldsymbol{A}}=\\left(\\sum_{t=1}^{n} X_{t} X_{t}^{\\prime}\\right)^{-1}\\left(\\sum_{t=1}^{n} X_{t} Y_{t}^{\\prime}\\right) .\n\\]\nAlternatively, the coefficient estimator for the \\(j^{t h}\\) equation is\n\\[\n\\widehat{a}_{j}=\\left(\\sum_{t=1}^{n} X_{t} X_{t}^{\\prime}\\right)^{-1}\\left(\\sum_{t=1}^{n} X_{t} Y_{j t}\\right) .\n\\]\nThe least squares residual vector is \\(\\widehat{e}_{t}=Y_{t}-\\widehat{A}^{\\prime} X_{t}\\). The estimator of the covariance matrix is\n\\[\n\\widehat{\\Sigma}=\\frac{1}{n} \\sum_{t=1}^{n} \\widehat{e}_{t} \\widehat{e}_{t}^{\\prime} .\n\\]\n(This may be adjusted for degrees-of-freedom if desired, but there is no established finite-sample justification for a specific adjustment.)\nIf \\(Y_{t}\\) is strictly stationary and ergodic with finite variances then we can apply the Ergodic Theorem (Theorem 14.9) to deduce that\n\\[\n\\frac{1}{n} \\sum_{t=1}^{n} X_{t} Y_{t}^{\\prime} \\underset{p}{\\longrightarrow} \\mathbb{E}\\left[X_{t} Y_{t}^{\\prime}\\right]\n\\]\nand\n\\[\n\\sum_{t=1}^{n} X_{t} X_{t}^{\\prime} \\underset{p}{\\longrightarrow} \\mathbb{E}\\left[X_{t} X_{t}^{\\prime}\\right] .\n\\]\nSince the latter is positive definite by Theorem \\(15.8\\) we conclude that \\(\\widehat{\\boldsymbol{A}}\\) is consistent for \\(\\boldsymbol{A}\\). Standard manipulations show that \\(\\widehat{\\Sigma}\\) is consistent as well.\nTheorem 15.9 If \\(Y_{t}\\) is strictly stationary, ergodic, and \\(0<\\Sigma<\\infty\\) then \\(\\widehat{A} \\underset{p}{\\rightarrow} A\\) and \\(\\widehat{\\Sigma} \\underset{p}{\\longrightarrow} \\Sigma\\) as \\(n \\rightarrow \\infty\\)\nVAR models can be estimated in Stata using the var command."
  },
  {
    "objectID": "chpt15-multiple-time-series.html#asymptotic-distribution",
    "href": "chpt15-multiple-time-series.html#asymptotic-distribution",
    "title": "15  Multivariate Time Series",
    "section": "15.10 Asymptotic Distribution",
    "text": "15.10 Asymptotic Distribution\nSet\n\\[\na=\\operatorname{vec}(\\boldsymbol{A})=\\left(\\begin{array}{c}\na_{1} \\\\\n\\vdots \\\\\na_{m}\n\\end{array}\\right), \\quad \\widehat{a}=\\operatorname{vec}(\\widehat{\\boldsymbol{A}})=\\left(\\begin{array}{c}\n\\widehat{a}_{1} \\\\\n\\vdots \\\\\n\\widehat{a}_{m}\n\\end{array}\\right) .\n\\]\nBy the same analysis as in Theorem \\(14.30\\) combined with Theorem \\(11.1\\) we obtain the following.\nTheorem 15.10 Suppose that \\(Y_{t}\\) follows the \\(\\operatorname{VAR}(\\mathrm{p})\\) model, all roots \\(r\\) of \\(\\operatorname{det}(\\boldsymbol{A}(z))\\) satisfy \\(|r|>1\\), \\(\\mathbb{E}\\left[e_{t} \\mid \\mathscr{F}_{t-1}\\right]=0, \\mathbb{E}\\left\\|e_{t}\\right\\|^{4}<\\infty\\), and \\(\\Sigma>0\\), then as \\(n \\rightarrow \\infty\\), \\(\\sqrt{n}(\\widehat{a}-a) \\underset{d}{\\longrightarrow} \\mathrm{N}(0, V)\\) where\n\\[\n\\begin{aligned}\n\\boldsymbol{V} &=\\overline{\\boldsymbol{Q}}^{-1} \\Omega \\overline{\\boldsymbol{Q}}^{-1} \\\\\n\\overline{\\boldsymbol{Q}} &=\\boldsymbol{I}_{m} \\otimes \\boldsymbol{Q} \\\\\n\\boldsymbol{Q} &=\\mathbb{E}\\left[X_{t} X_{t}^{\\prime}\\right] \\\\\n\\Omega &=\\mathbb{E}\\left[e_{t} e_{t}^{\\prime} \\otimes X_{t} X_{t}^{\\prime}\\right] .\n\\end{aligned}\n\\]\nNotice that the theorem uses the strong assumption that the innovation is a martingale difference sequence \\(\\mathbb{E}\\left[e_{t} \\mid \\mathscr{F}_{t-1}\\right]=0\\). This means that the \\(\\operatorname{VAR}(\\mathrm{p})\\) model is the correct conditional expectation for each variable. In words, these are the correct lags and there is no omitted nonlinearity.\nIf we further strengthen the MDS assumption to conditional homoskedasticity\n\\[\n\\mathbb{E}\\left[e_{t} e_{t}^{\\prime} \\mid \\mathscr{F}_{t-1}\\right]=\\Sigma\n\\]\nthen the asymptotic variance simplifies as\n\\[\n\\begin{aligned}\n&\\Omega=\\Sigma \\otimes \\boldsymbol{Q} \\\\\n&\\boldsymbol{V}=\\Sigma \\otimes \\boldsymbol{Q}^{-1} .\n\\end{aligned}\n\\]\nIn contrast, if the VAR(p) is an approximation then the MDS assumption is not appropriate. In this case the asymptotic distribution can be derived under mixing conditions.\nTheorem 15.11 Assume that \\(Y_{t}\\) is strictly stationary, ergodic, and for some \\(r>\\) \\(4, \\mathbb{E}\\left\\|Y_{t}\\right\\|^{r}<\\infty\\) and the mixing coefficients satisfy \\(\\sum_{\\ell=1}^{\\infty} \\alpha(\\ell)^{1-4 / r}<\\infty\\). Let \\(a\\) be the projection coefficient vector and \\(e_{t}\\) the projection error. Then as \\(n \\rightarrow \\infty\\), \\(\\sqrt{n}(\\widehat{a}-a) \\underset{d}{\\longrightarrow} \\mathrm{N}(0, V)\\) where\n\\[\n\\begin{aligned}\n&\\boldsymbol{V}=\\left(\\boldsymbol{I}_{m} \\otimes \\boldsymbol{Q}^{-1}\\right) \\Omega\\left(\\boldsymbol{I}_{m} \\otimes \\boldsymbol{Q}^{-1}\\right) \\\\\n&\\boldsymbol{Q}=\\mathbb{E}\\left[X_{t} X_{t}^{\\prime}\\right] \\\\\n&\\Omega=\\sum_{\\ell=-\\infty}^{\\infty} \\mathbb{E}\\left[e_{t-\\ell} e_{t}^{\\prime} \\otimes X_{t-\\ell} X_{t}^{\\prime}\\right] .\n\\end{aligned}\n\\]\nThis theorem does not require that the true process is a VAR. Instead, the coefficients are defined as those which produce the best (mean square) approximation, and the only requirements on the true process are general dependence conditions. The theorem shows that the coefficient estimators are asymptotically normal with a covariance matrix which takes a “long-run” sandwich form."
  },
  {
    "objectID": "chpt15-multiple-time-series.html#covariance-matrix-estimation",
    "href": "chpt15-multiple-time-series.html#covariance-matrix-estimation",
    "title": "15  Multivariate Time Series",
    "section": "15.11 Covariance Matrix Estimation",
    "text": "15.11 Covariance Matrix Estimation\nThe classic homoskedastic estimator of the covariance matrix for \\(\\widehat{a}\\) equals\n\\[\n\\widehat{\\boldsymbol{V}}_{\\widehat{a}}^{0}=\\widehat{\\Sigma} \\otimes\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} .\n\\]\nEstimators adjusted for degree-of-freedom can also be used though there is no established finite-sample justification. This variance estimator is appropriate under the assumption that the conditional expectation is correctly specified as a \\(\\operatorname{VAR}(\\mathrm{p})\\) and the innovations are conditionally homoskedastic.\nThe heteroskedasticity-robust estimator equals\n\\[\n\\widehat{\\boldsymbol{V}}_{\\widehat{a}}=\\left(\\boldsymbol{I}_{n} \\otimes\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1}\\right)\\left(\\sum_{t=1}^{n}\\left(\\widehat{e}_{t} \\widehat{e}_{t}^{\\prime} \\otimes X_{t} X_{t}^{\\prime}\\right)\\right)\\left(\\boldsymbol{I}_{n} \\otimes\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1}\\right) .\n\\]\nThis variance estimator is appropriate under the assumption that the conditional expectation is correctly specified as a \\(\\operatorname{VAR}(\\mathrm{p})\\) but does not require that the innovations are conditionally homoskedastic. The Newey-West estimator equals\n\\[\n\\begin{aligned}\n\\widehat{\\boldsymbol{V}}_{\\widehat{a}} &=\\left(\\boldsymbol{I}_{n} \\otimes\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1}\\right) \\widehat{\\Omega}_{M}\\left(\\boldsymbol{I}_{n} \\otimes\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1}\\right) \\\\\n\\widehat{\\Omega}_{M} &=\\sum_{\\ell=-M}^{M} w_{\\ell} \\sum_{1 \\leq t-\\ell \\leq n}\\left(\\widehat{e}_{t-\\ell} \\otimes X_{t-\\ell}\\right)\\left(\\widehat{e}_{t} \\otimes X_{t}^{\\prime}\\right) \\\\\nw_{\\ell} &=1-\\frac{|\\ell|}{M+1} .\n\\end{aligned}\n\\]\nThe number \\(M\\) is called the lag truncation number. An unweighted version sets \\(w_{\\ell}=1\\). The Newey-West estimator does not require that the \\(\\operatorname{VAR}(\\mathrm{p})\\) is correctly specified.\nTraditional textbooks have only used the homoskedastic variance estimation formula (15.9) and consequently existing software follows the same convention. For example, the var command in Stata displays only homoskedastic standard errors. Some researchers use the heteroskedasticity-robust estimator (15.10). The Newey-West estimator (15.11) is not commonly used for VAR models.\nAsymptotic approximations tend to be much less accurate under time series dependence than for independent observations. Therefore bootstrap methods are popular. In Section \\(14.46\\) we described several bootstrap methods for time series observations. While Section \\(14.46\\) focused on univariate time series, the extension to multivariate observations is straightforward."
  },
  {
    "objectID": "chpt15-multiple-time-series.html#selection-of-lag-length-in-an-var",
    "href": "chpt15-multiple-time-series.html#selection-of-lag-length-in-an-var",
    "title": "15  Multivariate Time Series",
    "section": "15.12 Selection of Lag Length in an VAR",
    "text": "15.12 Selection of Lag Length in an VAR\nFor a data-dependent rule to pick the lag length \\(p\\) it is recommended to minimize an information criterion. The formula for the AIC is\n\\[\n\\begin{aligned}\n\\operatorname{AIC}(p) &=n \\log \\operatorname{det} \\widehat{\\Sigma}(p)+2 K(p) \\\\\n\\widehat{\\Sigma}(p) &=\\frac{1}{n} \\sum_{t=1}^{n} \\widehat{e}_{t}(p) \\widehat{e}_{t}(p)^{\\prime} \\\\\nK(p) &=m(p m+1)\n\\end{aligned}\n\\]\nwhere \\(K(p)\\) is the number of parameters and \\(\\widehat{e}_{t}(p)\\) is the OLS residual vector from the model with \\(p\\) lags. The log determinant is the criterion from the multivariate normal likelihood.\nIn Stata the AIC for a set of estimated VAR models can be compared using the varsoc command. It should be noted, however, that the Stata routine actually displays \\(\\operatorname{AIC}(p) / n=\\log \\operatorname{det} \\widehat{\\Sigma}(p)+2 K(p) / n\\). This does not affect the ranking of the models but makes the differences between models appear misleadingly small."
  },
  {
    "objectID": "chpt15-multiple-time-series.html#illustration",
    "href": "chpt15-multiple-time-series.html#illustration",
    "title": "15  Multivariate Time Series",
    "section": "15.13 Illustration",
    "text": "15.13 Illustration\nWe estimate a three-variable system which is a simplified version of a model often used to study the impact of monetary policy. The three variables are quarterly from FRED-QD: real GDP growth rate \\(\\left(100 \\Delta \\log \\left(G D P_{t}\\right)\\right)\\), GDP inflation rate \\(\\left(100 \\Delta \\log \\left(P_{t}\\right)\\right)\\), and the Federal funds interest rate. VARs from lags 1 through 8 were estimated by least squares. The model with the smallest AIC is the VAR(6). The coefficient estimates and (homoskedastic) standard errors for the VAR(6) are reported in Table 15.1.\nExamining the coefficients in the table we can see that GDP displays a moderate degree of serial correlation and shows a large response to the federal funds rate, especially at lags 2 and 3. Inflation also displays serial correlation, shows minimal response to GDP, and also has meaningful response to the federal funds rate. The federal funds rate has the strongest serial correlation. Overall, it is difficult to read too much meaning into the coefficient estimates due to the complexity of the interactions. Because of this difficulty it is typical to focus on other representations of the coefficient estimates such as impulse responses which we discuss in the upcoming sections."
  },
  {
    "objectID": "chpt15-multiple-time-series.html#predictive-regressions",
    "href": "chpt15-multiple-time-series.html#predictive-regressions",
    "title": "15  Multivariate Time Series",
    "section": "15.14 Predictive Regressions",
    "text": "15.14 Predictive Regressions\nIn some contexts (including prediction) it is useful to consider models where the dependent variable is dated multiple periods ahead of the right-hand-side variables. These equations can be single equation or multivariate; we can consider both as special cases of a VAR (as a single equation model can be written as one equation taken from a VAR system). An \\(h\\)-step predictive VAR(p) takes the form\n\\[\nY_{t+h}=b_{0}+\\boldsymbol{B}_{1} Y_{t}+\\cdots+\\boldsymbol{B}_{p} Y_{t-p+1}+u_{t} .\n\\]\nThe integer \\(h \\geq 1\\) is the horizon. A one-step predictive VAR equals a standard VAR. The coefficients should be viewed as the best linear predictors of \\(Y_{t+h}\\) given \\(\\left(Y_{t}, \\ldots, Y_{t-p+1}\\right)\\).\nThere is an interesting relationship between a VAR model and the corresponding \\(h\\)-step predictive VAR model.\nTheorem 15.12 If \\(Y_{t}\\) is a \\(\\operatorname{VAR}(\\mathrm{p})\\) process then its \\(h\\)-step predictive regression is a predictive \\(\\operatorname{VAR}(\\mathrm{p})\\) with \\(u_{t}\\) a MA(h-1) process and \\(\\boldsymbol{B}_{1}=\\Theta_{h}=\\operatorname{IRF}(h)\\).\nThe proof of Theorem \\(15.12\\) is presented in Section 15.31.\nThere are several implications of this theorem. First, if \\(Y_{t}\\) is a \\(\\operatorname{VAR}(\\mathrm{p})\\) process then the correct number of lags for an \\(h\\)-step predictive regression is also \\(p\\) lags. Second, the error in a predictive regression is a MA process and is thus serially correlated. The linear dependence, however, is capped by the horizon. Third, the leading coefficient matrix corresponds to the \\(h^{\\text {th }}\\) moving average coefficient matrix which also equals the \\(h^{\\text {th }}\\) impulse response matrix.\nThe predictive regression (15.12) can be estimated by least squares. We can write the estimates as\n\\[\nY_{t+h}=\\widehat{b}_{0}+\\widehat{\\boldsymbol{B}}_{1} Y_{t}+\\cdots+\\widehat{\\boldsymbol{B}}_{p} Y_{t-p+1}+\\widehat{u}_{t} .\n\\]\nFor a distribution theory we need to apply Theorem \\(15.11\\) since the innovations \\(u_{t}\\) are a moving average and thus violate the MDS assumption. It follows as well that the covariance matrix for the estimators should be estimated by the Newey-West (15.11) estimator. There is a difference, however. Since \\(u_{t}\\) is known to be a MA(h-1) a reasonable choice is to set \\(M=h-1\\) and use the simple weights \\(w_{\\ell}=1\\). Indeed, this was the original suggestion by L. Hansen and Hodrick (1980).\nFor a distributional theory we can apply Theorem 15.11. Let \\(b\\) be the vector of coefficients in (15.12) and \\(\\widehat{b}\\) the corresponding least squares estimator. Let \\(X_{t}\\) be the vector of regressors in (15.12). Table 15.1: Vector Autoregression\n\n\n\n\\(G D P_{t-1}\\)\n\\(0.25\\)\n\\(0.01\\)\n\\(0.08\\)\n\n\n\n\n\n\\((0.07)\\)\n\\((0.02)\\)\n\\((0.02)\\)\n\n\n\\(G D P_{t-2}\\)\n\\(0.23\\)\n\\(-0.02\\)\n\\(0.04\\)\n\n\n\n\\((0.07)\\)\n\\((0.02)\\)\n\\((0.02)\\)\n\n\n\\(G D P_{t-3}\\)\n\\(0.00\\)\n\\(0.03\\)\n\\(0.01\\)\n\n\n\n\\((0.07)\\)\n\\((0.02)\\)\n\\((0.02)\\)\n\n\n\\(G D P_{t-4}\\)\n\\(0.14\\)\n\\(0.04\\)\n\\(-0.02\\)\n\n\n\n\\((0.07)\\)\n\\((0.02)\\)\n\\((0.02)\\)\n\n\n\\(G D P_{t-5}\\)\n\\(-0.02\\)\n\\(-0.03\\)\n\\(0.04\\)\n\n\n\n\\((0.07)\\)\n\\((0.02)\\)\n\\((0.02)\\)\n\n\n\\(G D P_{t-6}\\)\n\\(0.05\\)\n\\(-0.00\\)\n\\(-0.01\\)\n\n\n\n\\((0.06)\\)\n\\((0.02)\\)\n\\((0.02)\\)\n\n\n\\(I N F_{t-1}\\)\n\\(0.11\\)\n\\(0.57\\)\n\\(0.01\\)\n\n\n\n\\((0.20)\\)\n\\((0.07)\\)\n\\((0.05)\\)\n\n\n\\(I N F_{t-2}\\)\n\\(-0.17\\)\n\\(0.10\\)\n\\(0.17\\)\n\n\n\n\\((0.23)\\)\n\\((0.08)\\)\n\\((0.06)\\)\n\n\n\\(I N F_{t-3}\\)\n\\(0.01\\)\n\\(0.09\\)\n\\(-0.05\\)\n\n\n\n\\((0.23)\\)\n\\((0.08)\\)\n\\((0.06)\\)\n\n\n\\(I N F_{t-4}\\)\n\\(0.16\\)\n\\(0.14\\)\n\\(-0.05\\)\n\n\n\n\\((0.23)\\)\n\\((0.08)\\)\n\\((0.06)\\)\n\n\n\\(I N F_{t-5}\\)\n\\(0.12\\)\n\\(-0.05\\)\n\\(-0.05\\)\n\n\n\n\\((0.24)\\)\n\\((0.08)\\)\n\\((0.06)\\)\n\n\n\\(I N F_{t-6}\\)\n\\(-0.14\\)\n\\(0.10\\)\n\\(0.09\\)\n\n\n\n\\((0.21)\\)\n\\((0.07)\\)\n\\((0.05)\\)\n\n\n\\(F F_{t-1}\\)\n\\(0.13\\)\n\\(0.28\\)\n\\(1.14\\)\n\n\n\n\\((0.26)\\)\n\\((0.08)\\)\n\\((0.07)\\)\n\n\n\\(F F_{t-2}\\)\n\\(-1.50\\)\n\\(-0.27\\)\n\\(-0.53\\)\n\n\n\n\\((0.38)\\)\n\\((0.12)\\)\n\\((0.10)\\)\n\n\n\\(F F_{t-3}\\)\n\\(1.40\\)\n\\(0.12\\)\n\\(0.53\\)\n\n\n\n\\((0.40)\\)\n\\((0.13)\\)\n\\((0.10)\\)\n\n\n\\(F F_{t-4}\\)\n\\(-0.57\\)\n\\(-0.13\\)\n\\(-0.28\\)\n\n\n\n\\((0.41)\\)\n\\((0.13)\\)\n\\((0.11)\\)\n\n\n\n\\(0.01\\)\n\\(0.25\\)\n\\(0.28\\)\n\n\n\n\\((0.40)\\)\n\\((0.13)\\)\n\\((0.10)\\)\n\n\n\n\n\\(-0.27\\)\n\\(-0.24\\)\n\n\n\n\\((0.18)\\)\n\\((0.14)\\)\n\n\n\n\nTheorem 15.13 If \\(Y_{t}\\) is strictly stationary, ergodic, \\(\\Sigma>0\\), and for some \\(r>4\\), \\(\\mathbb{E}\\left\\|Y_{t}\\right\\|^{r}<\\infty\\) and the mixing coefficients satisfy \\(\\sum_{\\ell=1}^{\\infty} \\alpha(\\ell)^{1-4 / r}<\\infty\\), then as \\(n \\rightarrow\\) \\(\\infty, \\sqrt{n}(\\widehat{b}-b) \\underset{d}{\\longrightarrow} \\mathrm{N}(0, V)\\) where\n\\[\n\\begin{aligned}\n\\boldsymbol{V} &=\\left(\\boldsymbol{I}_{m} \\otimes \\boldsymbol{Q}^{-1}\\right) \\Omega\\left(\\boldsymbol{I}_{m} \\otimes \\boldsymbol{Q}^{-1}\\right) \\\\\n\\boldsymbol{Q} &=\\mathbb{E}\\left[X_{t} X_{t}^{\\prime}\\right] \\\\\n\\Omega &=\\sum_{\\ell=-\\infty}^{\\infty} \\mathbb{E}\\left[\\left(\\widehat{u}_{t-\\ell} \\otimes X_{t-\\ell}\\right)\\left(\\widehat{u}_{t}^{\\prime} \\otimes X_{t}^{\\prime}\\right)\\right] .\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "chpt15-multiple-time-series.html#impulse-response-estimation",
    "href": "chpt15-multiple-time-series.html#impulse-response-estimation",
    "title": "15  Multivariate Time Series",
    "section": "15.15 Impulse Response Estimation",
    "text": "15.15 Impulse Response Estimation\nReporting of impulse response estimates is one of the most common applications of vector autoregressive modeling. There are several methods to estimate the impulse response function. In this section we review the most common estimator based on the estimated VAR parameters.\nWithin a VAR(p) model the impulse responses are determined by the VAR coefficients. We can write this mapping as \\(\\Theta_{h}=g_{h}(\\boldsymbol{A})\\). The plug-in approach suggests the estimator \\(\\widehat{\\Theta}_{h}=g_{h}(\\widehat{\\boldsymbol{A}})\\) given the \\(\\operatorname{VAR}(\\mathrm{p})\\) coefficient estimator \\(\\widehat{A}\\). These are the impulse responses implied by the estimated VAR coefficients. While it is possible to explicitly write the function \\(g_{h}(\\boldsymbol{A})\\), a computationally simple approach is to use Theorem \\(15.5\\) which shows that the impulse response matrices can be written as a simple recursion in the VAR coefficients. Thus the impulse response estimator satisfies the recursion\n\\[\n\\widehat{\\Theta}_{h}=\\sum_{\\ell=1}^{\\min [h, p]} \\widehat{A}_{\\ell} \\widehat{\\Theta}_{h-\\ell} .\n\\]\nWe then set \\(\\widehat{\\operatorname{IRF}}(h)=\\widehat{\\Theta}_{h}\\).\nThis is the the most commonly used method for impulse response estimation and it is the method implemented in standard packages.\nSince \\(\\widehat{A}\\) is random so is \\(\\widehat{\\operatorname{IRF}}(h)\\) as it is a nonlinear function of \\(\\widehat{\\boldsymbol{A}}\\). Using the delta method, we deduce that the elements of \\(\\widehat{\\operatorname{IRF}}(h)\\) (the impulse responses) are asymptotically normally distributed. With some messy algebra explicit expressions for the asymptotic variances can be obtained. Sample versions can be used to calculate asymptotic standard errors. These can be used to form asymptotic confidence intervals for the impulse responses.\nThe asymptotic approximations, however, can be poor. As we discussed earlier the asymptotic approximations for the distribution of the coefficients \\(\\widehat{A}\\) can be poor due to the serial dependence in the observations. The asymptotic approximations for \\(\\widehat{\\operatorname{IRF}}(h)\\) can be significantly worse because the impulse responses are highly nonlinear functions of the coefficients. For example, in the simple AR(1) model with coefficient estimate \\(\\widehat{\\alpha}\\) the \\(h^{\\text {th }}\\) impulse response is \\(\\widehat{\\alpha}^{h}\\) which is highly nonlinear for even moderate horizons \\(h\\).\nConsequently, asymptotic approximations are less popular than bootstrap approximations. The most popular bootstrap approximation uses the recursive bootstrap (see Section 14.46) using the fitted VAR model and calculates confidence intervals for the impulse responses with the percentile method. An unfortunate feature of this choice is that the percentile bootstrap confidence interval is biased since the nonlinear impulse response estimates are biased and the percentile bootstrap accentuates bias. Some advantages of the estimation method as described is that it produces impulse response estimates which are directly related to the estimated \\(\\operatorname{VAR}(\\mathrm{p})\\) model and are internally consistent with one another. The method is also numerically stable. It is efficient when the true process is a true \\(\\operatorname{VAR}(\\mathrm{p})\\) with conditionally homoskedastic MDS innovations. When the true process is not a VAR(p) it can be thought of as a nonparametric estimator of the impulse response if \\(p\\) is large (or selected appropriately in a data-dependent fashion, such as by the AIC).\nA disadvantage of this estimator is that it is a highly nonlinear function of the VAR coefficient estimators. Therefore the distribution of the impulse response estimator is unlikely to be well approximated by the normal distribution. When the \\(\\operatorname{VAR}(\\mathrm{p})\\) is not the true process then it is possible that the nonlinear transformation accentuates the misspecification bias.\nImpulse response functions can be calculated and displayed in Stata using the irf command. The command irf create is used to calculate impulse response functions and confidence intervals. The default confidence intervals are asymptotic (delta method). Bootstrap (recursive method) standard errors can be substituted using the bs option. The command irf graph irf produces graphs of the impulse response function along with \\(95 %\\) asymptotic confidence intervals. The command irf graph cirf produces the cumulative impulse response function. It may be useful to know that the impulse response estimates are unscaled so represent the response due to a one-unit change in the impulse variable. A limitation of the Stata irf command is that there are limited options for standard error and confidence interval construction. The asymptotic standard errors are calculated using the homoskedastic formula not the correct heteroskedastic formula. The bootstrap confidence intervals are calculated using the normal approximation bootstrap confidence interval, the least reliable bootstrap confidence interval method. Better options such as the bias-corrected percentile confidence interval are not provided as options."
  },
  {
    "objectID": "chpt15-multiple-time-series.html#local-projection-estimator",
    "href": "chpt15-multiple-time-series.html#local-projection-estimator",
    "title": "15  Multivariate Time Series",
    "section": "15.16 Local Projection Estimator",
    "text": "15.16 Local Projection Estimator\nJordà (2005) observed that the impulse response can be estimated by a least squares predictive regression. The key is Theorem \\(15.12\\) which established that \\(\\Theta_{h}=\\boldsymbol{B}_{1}\\), the leading coefficient matrix in the \\(h\\)-step predictive regression.\nThe method is as follows. For each horizon \\(h\\) estimate a predictive regression (15.12) to obtain the leading coefficient matrix estimator \\(\\widehat{\\boldsymbol{B}}_{1}\\). The estimator is \\(\\widehat{\\operatorname{IRF}}(h)=\\widehat{\\boldsymbol{B}}_{1}\\) and is known as the local projection estimator.\nTheorem \\(15.13\\) shows that the local projection impulse response estimator is asymptotically normal. Newey-West methods must be used for calculation of asymptotic standard errors since the regression errors are serially correlated.\nJordà (2005) speculates that the local projection estimator will be less sensitive to misspecification since it is a straightforward linear estimator. This is intuitive but unclear. Theorem \\(15.12\\) relies on the assumption that \\(Y_{t}\\) is a \\(\\operatorname{VAR}(\\mathrm{p})\\) process, and fails otherwise. Thus if the true process is not a VAR(p) then the coefficient matrix \\(\\boldsymbol{B}_{1}\\) in (15.12) does not correspond to the desired impulse response matrix \\(\\Theta_{h}\\) and hence will be misspecified. The accuracy (in the sense of low bias) of both the conventional and the local projection estimator relies on \\(p\\) being sufficiently large that the \\(\\operatorname{VAR}(\\mathrm{p})\\) model is a good approximation to the true infinite-order regression (15.4). Without a formal theory it is difficult to know which estimator is more robust than the other.\nOne implementation challenge is the choice of \\(p\\). While the method allows for \\(p\\) to vary across horizon \\(h\\) there is no well-established method for selection of the VAR order for predictive regressions. (Standard selection criteria such as AIC are inappropriate under serially correlated errors just as conventional standard errors are inappropriate.) Therefore the seemingly natural choice is to use the same \\(p\\) for all horizons and base this choice on the one-step VAR model where AIC can be used for model selection.\nAn advantage of the local projection method is that it is a linear estimator of the impulse response and thus likely to have a better-behaved sampling distribution.\nA disadvantage is that the method relies on a regression (15.12) that has serially correlated errors. The latter are highly correlated at long horizons and this renders the estimator imprecise. Local projection estimators tend to be less smooth and more erratic than those produced by the conventional estimator reflecting a possible lack of precision."
  },
  {
    "objectID": "chpt15-multiple-time-series.html#regression-on-residuals",
    "href": "chpt15-multiple-time-series.html#regression-on-residuals",
    "title": "15  Multivariate Time Series",
    "section": "15.17 Regression on Residuals",
    "text": "15.17 Regression on Residuals\nIf the innovations \\(e_{t}\\) were observed it would be natural to directly estimate the coefficients of the multivariate Wold decomposition. We would pick a maximum horizon \\(h\\) and then estimate the equation\n\\[\nY_{t}=\\mu+\\Theta_{1} e_{t-1}+\\Theta_{2} e_{t-2}+\\cdots+\\Theta_{h} e_{t-h}+u_{t}\n\\]\nwhere\n\\[\nu_{t}=e_{t}+\\sum_{\\ell=h+1}^{\\infty} \\Theta_{\\ell} e_{t-\\ell} .\n\\]\nThe variables \\(\\left(e_{t-1}, \\ldots, e_{t-h}\\right)\\) are uncorrelated with \\(u_{t}\\) so the least squares estimator of the coefficients is consistent and asymptotically normal. Since \\(u_{t}\\) is serially correlated the Newey-West method should be used to calculate standard errors.\nIn practice the innovations \\(e_{t}\\) are not observed. If they are replaced by the residuals \\(\\widehat{e}_{t}\\) from an estimated \\(\\operatorname{VAR}(\\mathrm{p})\\) then we can estimate the coefficients by least squares applied to the equation\n\\[\nY_{t}=\\mu+\\Theta_{1} \\widehat{e}_{t-1}+\\Theta_{2} \\widehat{e}_{t-2}+\\cdots+\\Theta_{h} \\widehat{e}_{t-h}+\\widehat{u}_{t} .\n\\]\nThis idea originated with Durbin (1960).\nThis is a two-step estimator with generated regressors. (See Section 12.26.) The impulse response estimators are consistent and asymptotically normal but with a non-standard covariance matrix due to the two-step estimation. Conventional, robust, and Newey-West standard errors do not account for this without modification.\nChang and Sakata (2007) proposed a simplified version of the Durbin regression. Notice that for any horizon \\(h\\) we can rewrite the Wold decomposition as\n\\[\nY_{t+h}=\\mu+\\Theta_{h} e_{t}+v_{t+h}\n\\]\nwhere\n\\[\nv_{t}=\\sum_{\\ell=0}^{h-1} \\Theta_{\\ell} e_{t-\\ell}+\\sum_{\\ell=h+1}^{\\infty} \\Theta_{\\ell} e_{t-\\ell} .\n\\]\nThe regressor \\(e_{t}\\) is uncorrelated with \\(v_{t+h}\\). Thus \\(\\Theta_{h}\\) can be estimated by a regression of \\(Y_{t+h}\\) on \\(e_{t}\\). In practice we can replace \\(e_{t}\\) by the least squares residual \\(\\widehat{e}_{t}\\) from an estimated VAR(p) to estimate the regression\n\\[\nY_{t+h}=\\mu+\\Theta_{h} \\widehat{e}_{t}+\\widehat{v}_{t+h} .\n\\]\nSimilar to the Durbin regression the Chang-Sakata estimator is a two-step estimator with a generated regressor. However, as it takes the form studied in Section \\(12.27\\) it can be shown that the Chang-Sakata two-step estimator has the same asymptotic distribution as the idealized one-step estimator as if \\(e_{t}\\) were observed. Thus the standard errors do not need to be adjusted for generated regressors which is an advantage. The errors are serially correlated so Newey-West standard errors should be used. The variance of the error \\(v_{t+h}\\) is larger than the variance of the error \\(u_{t}\\) in the Durbin regression so the Chang-Sakata estimator may be less precise than the Durbin estimator.\nChang and Sakata (2007) also point out the following implication of the FWL theorem. The least squares slope estimator in (15.14) is algebraically identical \\({ }^{1}\\) to the slope estimator \\(\\widehat{\\boldsymbol{B}}_{1}\\) in a predictive regression with \\(p-1\\) lags. Thus the Chang-Sakata estimator is similar to a local projection estimator."
  },
  {
    "objectID": "chpt15-multiple-time-series.html#orthogonalized-shocks",
    "href": "chpt15-multiple-time-series.html#orthogonalized-shocks",
    "title": "15  Multivariate Time Series",
    "section": "15.18 Orthogonalized Shocks",
    "text": "15.18 Orthogonalized Shocks\nWe can use the impulse response function to examine how the innnovations impact the time-paths of the variables. A difficulty in interpretation, however, is that the elements of the innovation vector \\(e_{t}\\) are contemporeneously correlated. Thus \\(e_{j t}\\) and \\(e_{i t}\\) are (in general) not independent, so consequently it does not make sense to treat \\(e_{j t}\\) and \\(e_{i t}\\) as fundamental “shocks”. Another way of describing the problem is that it does not make sense, for example, to describe the impact of \\(e_{j t}\\) while “holding” \\(e_{i t}\\) constant.\nThe natural solution is to orthogonalize the innovations so that they are uncorrelated and then view the orthogonalized errors as the fundamental “shocks”. Recall that \\(e_{t}\\) is mean zero with covariance matrix \\(\\Sigma\\). We can factor \\(\\Sigma\\) into the product of an \\(m \\times m\\) matrix \\(\\boldsymbol{B}\\) with its transpose \\(\\Sigma=\\boldsymbol{B} \\boldsymbol{B}^{\\prime}\\). The matrix \\(\\boldsymbol{B}\\) is called a “square root” of \\(\\Sigma\\). (See Section A.13.) Define \\(\\varepsilon_{t}=\\boldsymbol{B}^{-1} e_{t}\\). The random vector \\(\\varepsilon_{t}\\) has mean zero and covariance matrix \\(\\boldsymbol{B}^{-1} \\Sigma \\boldsymbol{B}^{-1 \\prime}=\\boldsymbol{B}^{-1} \\boldsymbol{B} \\boldsymbol{B}^{\\prime} \\boldsymbol{B}^{-1 \\prime}=\\boldsymbol{I}_{m}\\). The elements \\(\\varepsilon_{t}=\\left(\\varepsilon_{1 t}, \\ldots, \\varepsilon_{m t}\\right)\\) are mutually uncorrelated. We can write the innovations as a function of the orthogonalized errors as\n\\[\ne_{t}=\\boldsymbol{B} \\varepsilon_{t} .\n\\]\nTo distinguish \\(\\varepsilon_{t}\\) from \\(e_{t}\\) we will typically call \\(\\varepsilon_{t}\\) the “orthogonalized shocks” or more simply as the “shocks” and continue to call \\(e_{t}\\) the “innovations”.\nWhen \\(m>1\\) there is not a unique square root matrix \\(\\boldsymbol{B}\\) so there is not a unique orthogonalization. The most common choice (and was originally advocated by Sims (1980)) is the Cholesky decomposition (see Section A.16). This sets \\(\\boldsymbol{B}\\) to be lower triangular, meaning that it takes the form\n\\[\n\\boldsymbol{B}=\\left[\\begin{array}{ccc}\nb_{11} & 0 & 0 \\\\\nb_{21} & b_{22} & 0 \\\\\nb_{31} & b_{32} & b_{33}\n\\end{array}\\right]\n\\]\nwith non-negative diagonal elements. We can write the Cholesky decomposition of a matrix \\(\\boldsymbol{A}\\) as \\(\\boldsymbol{C}=\\) \\(\\operatorname{chol}(\\boldsymbol{A})\\) which means that \\(\\boldsymbol{A}=\\boldsymbol{C} \\boldsymbol{C}^{\\prime}\\) with \\(\\boldsymbol{C}\\) lower triangular. We thus set\n\\[\n\\boldsymbol{B}=\\operatorname{chol}(\\Sigma)\n\\]\nEquivalently, the innovations are related to the orthogonalized shocks by the equations\n\\[\n\\begin{aligned}\n&e_{1 t}=b_{11} \\varepsilon_{1 t} \\\\\n&e_{2 t}=b_{21} \\varepsilon_{1 t}+b_{22} \\varepsilon_{2 t} \\\\\n&e_{3 t}=b_{31} \\varepsilon_{1 t}+b_{31} \\varepsilon_{2 t}+b_{33} \\varepsilon_{3 t} .\n\\end{aligned}\n\\]\nThis structure is recursive. The innovation \\(e_{1 t}\\) is a function only of the single shock \\(\\varepsilon_{1 t}\\). The innovation \\(e_{2 t}\\) is a function of the shocks \\(\\varepsilon_{1 t}\\) and \\(\\varepsilon_{2 t}\\), and the innovation \\(e_{3 t}\\) is a function of all three shocks. Another way of looking at the structure is that the first shock \\(\\varepsilon_{1 t}\\) affects all three innovationa, the second shock \\(\\varepsilon_{2 t}\\) affects \\(e_{2 t}\\) and \\(e_{3 t}\\), and the third shock \\(\\varepsilon_{3 t}\\) only affects \\(e_{3 t}\\).\n\\({ }^{1}\\) Technically, if the sample lengths are adjusted. A recursive structure is an exclusion restriction. The recursive structure excludes \\(\\varepsilon_{2 t}\\) and \\(\\varepsilon_{3 t}\\) contemporeneously affecting \\(e_{1 t}\\), and excludes \\(\\varepsilon_{3 t}\\) contemporeneously affecting \\(e_{2 t}\\).\nWhen using the Cholesky decomposition the recursive structure is determined by the ordering of the variables in the system. The order matters and is the key identifying assumption. We will return to this issue later.\nFinally, we mention that the system (15.15) is equivalent to the system\n\\[\nA e_{t}=\\varepsilon_{t}\n\\]\nwhere \\(\\boldsymbol{A}=\\boldsymbol{B}^{-1}\\) is lower triangular when \\(\\boldsymbol{B}\\) is lower triangular. The representation (15.15) is more convenient, however, for most of our purposes."
  },
  {
    "objectID": "chpt15-multiple-time-series.html#orthogonalized-impulse-response-function",
    "href": "chpt15-multiple-time-series.html#orthogonalized-impulse-response-function",
    "title": "15  Multivariate Time Series",
    "section": "15.19 Orthogonalized Impulse Response Function",
    "text": "15.19 Orthogonalized Impulse Response Function\nWe have defined the impulse response function as the change in the time \\(t\\) projection of the variables \\(Y_{t+h}\\) due to the innovation \\(e_{t}\\). As we discussed in the previous section, since the innovations are contemporeneously correlated it makes better sense to focus on changes due to the orthogonalized shocks \\(\\varepsilon_{t}\\). Consequently we define the orthgonalized impulse response function (OIRF) as\n\\[\n\\operatorname{OIRF}(h)=\\frac{\\partial}{\\partial \\varepsilon_{t}^{\\prime}} \\mathscr{P}_{t}\\left[Y_{t+h}\\right] .\n\\]\nWe can write the multivariate Wold representation as\n\\[\nY_{t}=\\mu+\\sum_{\\ell=0}^{\\infty} \\Theta_{\\ell} e_{t-\\ell}=\\mu+\\sum_{\\ell=0}^{\\infty} \\Theta_{\\ell} \\boldsymbol{B} \\varepsilon_{t-\\ell}\n\\]\nwhere \\(\\boldsymbol{B}\\) is from (15.16). We deduce that\n\\[\n\\operatorname{OIRF}(h)=\\Theta_{h} \\boldsymbol{B}=\\operatorname{IRF}(h) \\boldsymbol{B} .\n\\]\nThis is the non-orthogonalized impulse response matrix multiplied by the matrix square root \\(\\boldsymbol{B}\\).\nWrite the rows of the matrix \\(\\Theta_{h}\\) as\n\\[\n\\Theta_{h}=\\left[\\begin{array}{c}\n\\theta_{1 h}^{\\prime} \\\\\n\\theta_{m h}^{\\prime}\n\\end{array}\\right]\n\\]\nand the columns of the matrix \\(\\boldsymbol{B}\\) as \\(\\boldsymbol{B}=\\left[b_{1}, \\ldots, b_{m}\\right]\\). We can see that\n\\[\n\\operatorname{OIRF}_{i j}(h)=\\left[\\Theta_{h} \\boldsymbol{B}\\right]_{i j}=\\theta_{i h}^{\\prime} b_{j} .\n\\]\nThere are \\(m^{2}\\) such responses for each horizon \\(h\\).\nThe cumulative orthogonalized impulse response function (COIRF) is\n\\[\n\\operatorname{COIRF}(h)=\\sum_{\\ell=1}^{h} \\operatorname{OIRF}(\\ell)=\\sum_{\\ell=1}^{h} \\Theta_{\\ell} \\boldsymbol{B}\n\\]"
  },
  {
    "objectID": "chpt15-multiple-time-series.html#orthogonalized-impulse-response-estimation",
    "href": "chpt15-multiple-time-series.html#orthogonalized-impulse-response-estimation",
    "title": "15  Multivariate Time Series",
    "section": "15.20 Orthogonalized Impulse Response Estimation",
    "text": "15.20 Orthogonalized Impulse Response Estimation\nWe have discussed estimation of the moving average matrices \\(\\Theta_{\\ell}\\). We need an estimator of \\(\\boldsymbol{B}\\).\nWe first estimate the VAR(p) model by least squares. This gives us the coefficient matrices \\(\\widehat{A}\\) and the error covariance matrix \\(\\widehat{\\Sigma}\\). From the latter we apply the Cholesky decomposition \\(\\widehat{\\boldsymbol{B}}=\\operatorname{chol}(\\widehat{\\Sigma})\\) so that \\(\\widehat{\\Sigma}=\\widehat{\\boldsymbol{B}} \\widehat{\\boldsymbol{B}}^{\\prime}\\). (See Section A.16 for the algorithm.) The orthogonalized impulse response estimators are\n\\[\n\\widehat{\\operatorname{OIRF}}(h)=\\widehat{\\Theta}_{h} \\widehat{\\boldsymbol{B}}=\\widehat{\\theta}_{i h}^{\\prime} \\widehat{b}_{j} .\n\\]\nThe estimator \\(\\widehat{\\mathrm{OIRF}}(h)\\) is a nonlinear function of \\(\\widehat{\\boldsymbol{A}}\\) and \\(\\widehat{\\Sigma}\\). It is asymptotically normally distributed by the delta method. This allows for explicit calculation of asymptotic standard errors. These can be used to form asymptotic confidence intervals for the impulse responses.\nAs discussed earlier, the asymptotic approximations can be quite poor. Consequently bootstrap approximations are more widely used than asymptotic methods.\nOrthogonalized impulse response functions can be displayed in Stata using the irf command. The command irf graph oirf produces graphs of the orthogonalized impulse response function along with \\(95 %\\) asymptotic confidence intervals. The command irf graph coirf produces the cumulative orthogonalized impulse response function. It may also be useful to know that the OIRF are scaled for a one-standard deviation shock so the impulse response represents the response due to a one-standarddeviation change in the impulse variable. As discussed earlier, the Stata irf command has limited options for standard error and confidence interval construction. The asymptotic standard errors are calculated using the homoskedastic formula not the correct heteroskedastic formula. The bootstrap confidence intervals are calculated using the normal approximation bootstrap confidence interval."
  },
  {
    "objectID": "chpt15-multiple-time-series.html#illustration-1",
    "href": "chpt15-multiple-time-series.html#illustration-1",
    "title": "15  Multivariate Time Series",
    "section": "15.21 Illustration",
    "text": "15.21 Illustration\nTo illustrate we use the three-variable system from Section 15.13. We use the ordering (1) real GDP growth rate, (2) inflation rate, (3) Federal funds interest rate. We discuss the choice later when we discuss identification. We use the estimated VAR(6) and calculate the orthogonalized impulse response functions using the standard VAR estimator.\nIn Figure \\(15.1\\) we display the estimated orthogonalized impulse response of the GDP growth rate in response to a one standard deviation increase in the federal funds rate. Panel (a) shows the impulse response function and panel (b) the cumulative impulse response function. As we discussed earlier the interpretation of the impulse response and the cumulative impulse response depends on whether the variable enters the VAR in differences or in levels. In this case, GDP growth is the first difference of the natural logarithm. Thus panel (a) (the impulse response function) shows the effect of interest rates on the growth rate of GDP. Panel (b) (the cumulative impulse response) shows the effect on the log-level of GDP. The IRF shows that the GDP growth rate is negatively affected in the second quarter after an interest rate increase (a drop of about \\(0.2 %\\), non-annualized), and the negative effects continue for several quarters following. The CIRF shows the effect on the level of GDP measured as percentage changes. It shows that an interest rate increase causes GDP to fall for about 8 quarters, reducing GDP by about \\(0.6 %\\)."
  },
  {
    "objectID": "chpt15-multiple-time-series.html#forecast-error-decomposition",
    "href": "chpt15-multiple-time-series.html#forecast-error-decomposition",
    "title": "15  Multivariate Time Series",
    "section": "15.22 Forecast Error Decomposition",
    "text": "15.22 Forecast Error Decomposition\nAn alternative tool to investigate an estimated VAR is the forecast error decomposition which decomposes multi-step forecast error variances by the component shocks. The forecast error decomposition indicates which shocks contribute towards the fluctuations of each variable in the system.\n\n\nImpulse Response Function\n\n\n\nCumulative IRF\n\nFigure 15.1: Response of GDP Growth to Orthogonalized Fed Funds Shock\nIt is defined as follows. Take the moving average representation of the \\(i^{\\text {th }}\\) variable \\(Y_{i, t+h}\\) written as a function of the orthogonalized shocks\n\\[\nY_{i, t+h}=\\mu_{i}+\\sum_{\\ell=0}^{\\infty} \\theta_{i}(\\ell)^{\\prime} \\boldsymbol{B} \\varepsilon_{t+h-\\ell} .\n\\]\nThe best linear forecast of \\(Y_{t+h}\\) at time \\(t\\) is\n\\[\nY_{i, t+h \\mid t}=\\mu_{i}+\\sum_{\\ell=h}^{\\infty} \\theta_{i}(\\ell)^{\\prime} \\boldsymbol{B} \\varepsilon_{t+h-\\ell} .\n\\]\nThe \\(h\\)-step forecast error is the difference\n\\[\nY_{i, t+h}-Y_{i, t+h \\mid t}=\\sum_{\\ell=0}^{h-1} \\theta_{i}(\\ell)^{\\prime} \\boldsymbol{B} \\varepsilon_{t+h-\\ell} .\n\\]\nThe variance of this forecast error is\n\\[\n\\operatorname{var}\\left[Y_{i, t+h}-Y_{i, t+h \\mid t}\\right]=\\sum_{\\ell=0}^{h-1} \\operatorname{var}\\left[\\theta_{i}(\\ell)^{\\prime} \\boldsymbol{B} \\varepsilon_{t+h-\\ell}\\right]=\\sum_{\\ell=0}^{h-1} \\theta_{i}(\\ell)^{\\prime} \\boldsymbol{B} \\boldsymbol{B}^{\\prime} \\theta_{i}(\\ell) .\n\\]\nTo isolate the contribution of the \\(j^{t h}\\) shock, notice that\n\\[\ne_{t}=\\boldsymbol{B} \\varepsilon_{t}=b_{1} \\varepsilon_{1 t}+\\cdots+b_{m} \\varepsilon_{m t} .\n\\]\nThus the contribution of the \\(j^{t h}\\) shock is \\(b_{j} \\varepsilon_{j t}\\). Now imagine replacing \\(\\boldsymbol{B} \\varepsilon_{t}\\) in the variance calculation by the \\(j^{t h}\\) contribution \\(b_{j} \\varepsilon_{j t}\\). This is\n\\[\n\\operatorname{var}\\left[Y_{i t+h}-Y_{i, t+h \\mid t}\\right]=\\sum_{\\ell=0}^{h-1} \\operatorname{var}\\left[\\theta_{i}(\\ell)^{\\prime} b_{j} \\varepsilon_{j t+h-\\ell}\\right]=\\sum_{\\ell=0}^{h-1}\\left(\\theta_{i}(\\ell)^{\\prime} b_{j}\\right)^{2} .\n\\]\nExamining (15.18) and using \\(\\boldsymbol{B}=\\left[b_{1}, \\ldots, b_{m}\\right]\\) we can write (15.18) as\n\\[\n\\operatorname{var}\\left[Y_{i, t+h}-Y_{i, t+h \\mid t}\\right]=\\sum_{j=1}^{m} \\sum_{\\ell=0}^{h-1}\\left(\\theta_{i}(\\ell)^{\\prime} b_{j}\\right)^{2} .\n\\]\nThe forecast error decomposition is defined as the ratio of the \\(j^{t h}\\) contribution to the total which is the ratio of (15.19) to (15.20):\n\\[\n\\mathrm{FE}_{i j}(h)=\\frac{\\sum_{\\ell=0}^{h-1}\\left(\\theta_{i}(\\ell)^{\\prime} b_{j}\\right)^{2}}{\\sum_{j=1}^{m} \\sum_{\\ell=0}^{h-1}\\left(\\theta_{i}(\\ell)^{\\prime} b_{j}\\right)^{2}} .\n\\]\nThe \\(\\mathrm{FE}_{i j}(h)\\) lies in \\([0,1]\\) and varies across \\(h\\). Small values indicate that \\(\\varepsilon_{j t}\\) contributes only a small amount to the variance of \\(Y_{i t}\\). Large values indicate that \\(\\varepsilon_{j t}\\) contributes a major amount of the variance of \\(\\varepsilon_{i t}\\). version.\nA forecast error decomposition requires orthogonalized innovations. There is no non-orthogonalized\nThe forecast error decomposition can be calculated and displayed in Stata using the irf command. The command irf graph fevd produces graphs of the forecast error decomposition along with \\(95 %\\) asymptotic confidence intervals."
  },
  {
    "objectID": "chpt15-multiple-time-series.html#identification-of-recursive-vars",
    "href": "chpt15-multiple-time-series.html#identification-of-recursive-vars",
    "title": "15  Multivariate Time Series",
    "section": "15.23 Identification of Recursive VARs",
    "text": "15.23 Identification of Recursive VARs\nAs we have discussed, a common method to orthogonalize the VAR errors is the lower triangular Cholesky decomposition which implies a recursive structure. The ordering of the variables is critical this recursive structure. Unless the errors are uncorrelated different orderings will lead to different impulse response functions and forecast error decompositions. The ordering must be selected by the user; there is no data-dependent choice.\nIn order for impulse responses and forecast error decompositions to be interpreted causally the orthogonalization must be identified by the user based on a structural economic argument. The choice is similar to the exclusion restrictions necessary for specification of an instrumental variables regression. By ordering the variables recursively we are effectively imposing exclusion restrictions. Recall that in our empirical example we used the ordering: (1) real GDP growth rate, (2) inflation rate, (3) Federal funds interest rate. This means that in the equation for GDP we excluded the contemporeneous inflation rate and interest rate, and in the equation for inflation we excluded the contemporenous interest rate. These are exclusion restrictions. Are they justified?\nOne approach is to order first the variables which are believed to be contemporaneously affected by the fewest number of shocks. One way of thinking about it is that they are the variables which are “most sticky” within a period. The variables listed last are those which are believed to be contemporanously affected by the greatest number of shocks. These are the ones which are able to respond within a single period to the shocks or are most flexible. In our example we listed output first, prices second and interest rates last. This is consistent with the view that output is effectively pre-determined (within a period) and does not (within a period) respond to price and interest rate movements. Prices are allowed to respond within a period in response to output changes but not in response to interest rate changes. The latter could be justified if interest rate changes affect investment decisions but the latter take at least one period to implement. By listing the federal funds rate last the model allows monetary policy to respond within a period to contemporeneous information about output and prices.\nIn general, this line of reasoning suggests that production measures should be listed first, goods prices second, and financial prices last. This reasoning is more credible when the time periods are short, and less credible for longer time periods. Further justifications for possible recursive orderings can include: (1) information delays; (2) implementation delays; (3) institutions; (4) market structure; (5) homogeneity; (6) imposing estimates from other sources. In most cases such arguments can be made but will be viewed as debatable and restrictive. In any situation it is best to be explicit about your choice and reasoning.\nReturning to the empirical illustration it is fairly conventional to order the fed funds rate last. This allows the fed funds rate to respond to contemporeneous information about output and price growth and identifies the fed funds policy shock by the assumption that it does not have a contemporenous impact on the other variables. It is not clear, however, how to order the other two variables. For simplicity consider a traditional aggregate supply/aggregate demand model of the determination of output and the price level. If the aggregate supply curve is perfectly inelastic in the short run (one quarter) then output is effectively fixed (sticky) so changes in aggregate demand affect prices but not output. Changes in aggregate supply affect both output and prices. Thus we would want to order GDP first and inflation second. This choice would identify the GDP error as the aggregate supply shock. This is the ordering used in our example.\nIn contrast, suppose that the aggregate supply curve is perfectly elastic in the short run. Then prices are fixed and output is flexible. Changes in aggregate supply affect both price and output but changes in aggregate demand only affect output. In this case we would want to order inflation first and GDP second. This choice identifies the inflation error as the aggregate supply shock, the opposite case from the previous assumption!\nIf the choice between perfectly elastic and perfectly inelastic aggregate supply is not credible then the supply and demand shocks cannot be separately identified based on ordering alone. In this case the full set of impulse responses and error decompositions are not identified. However, a subset may be identified. In general, if the shocks can be ordered in groups then we can identify any shock for which a group has a single variable. In our example, consider the ordering (1) GDP and inflation; (2) federal funds rate. This means that the model assumes that GDP and inflation do not contemporeneously respond to interest rate movements but no other restrictions are imposed. In this case the fed funds policy shock is identified. This means that impulse responses of all three variables with respect to the policy shock are identified and similarly the forecast error composition of the effect of the fed funds shock on each variable is identified. These can be estimated by a VAR using the ordering (GDP, inflation, federal funds rate) as done in our example or using the ordering (inflation, GDP, federal funds rate). Both choices will lead to the same estimated impulse responses as described. The remaining impulse responses (responses to GDP and inflation shocks), however, will differ across these two orderings."
  },
  {
    "objectID": "chpt15-multiple-time-series.html#oil-price-shocks",
    "href": "chpt15-multiple-time-series.html#oil-price-shocks",
    "title": "15  Multivariate Time Series",
    "section": "15.24 Oil Price Shocks",
    "text": "15.24 Oil Price Shocks\nTo further illustrate the identification of impulse response functions by recursive structural assumptions we repeat here some of the analysis from Kilian (2009). His paper concerns the identification of the factors affecting crude oil prices, in particular separating supply and demand shocks. The goal is to determine how oil prices respond to economic shocks and how the responses differ by the type of shock.\nTo answer this question Kilian uses a three-variable VAR with monthly measures of global oil production, global economic activity, and the global price of crude oil for \\(1973 \\mathrm{~m} 2-2007 \\mathrm{~m} 12\\). He uses global variables since the price of crude oil is globally determined. One innovation in the paper is that Kilian develops a new index of global economic activity based on ocean freight rates. His motivation is that shipping rates are directly related to the global demand for industrial commodities. This data set is posted on the textbook webpage as Kilian2009.\nKilian argues that these three variables are determined by three economic shocks: oil supply, aggregate demand, and oil demand. He suggests that oil supply shocks should be thought of as disruptions in production, processing, or shipping. Aggregate demand is global economic activity. Kilian also argues that oil demand shocks are primarily due to the precautionary demand for oil driven by uncertainty about future oil supply shortfalls.\nTo identify the shocks Kilian makes the following exclusion restrictions. First, he assumes that the short-run (one month) supply of crude oil is inelastic with respect to price. Equivalently, oil production takes at least one month to respond to price changes. This restriction is believed to be plausible because of technological factors in crude oil production. It is costly to open new oil fields; and it is nearly impossible to cap an oil well once tapped. Second, Kilian assumes that in the short-run (one month) global real economic activity does not respond to changes in oil prices (due to shocks specific to the oil market), while economic activity is allowed to respond to oil production shocks. This assumption is viewed by Kilian as plausible due to the sluggishness in the response of economic activity to price changes. Crude oil prices, however, are allowed to respond simultaneously to all three shocks.\nKilian’s identification strategy is similar to that described in the previous section for the simple aggregate demand/aggregate supply model. The separation of supply and demand shocks is achieved by exclusion restrictions which imply short-run inelasticities. The plausibility of these assumptions rests in part on the monthly frequency of the data. While it is plausible that oil production and economic activity may not respond within one month to price shocks, it is much less plausible that there is no response for a full quarter. Kilian’s least convincing identifying assumption (in my opinion) is the assumption that economic activity does not respond simultaneously to oil price changes. While much economic activity is pre-planned and hence sluggish to respond, some economic activity (recreational driving, for example) may immediately respond to price changes.\nKilian estimates the three-variable VAR using 24 lags and calculates the orthogonalized impulse response functions using the ordering implied by these assumptions. He does not discuss the choice of 24 lags but presumably this is intended to allow for flexible dynamic responses. If the AIC is used for model selection, three lags would be selected. For the analysis reported here I used 4 lags. The results are qualitatively similar to those obtained using 24 lags. For ease of interpretation oil supply is entered negatively (multiplied by -1) so that all three shocks are scaled to increase oil prices. Two impulse response functions for the price of crude oil are displayed in Figure \\(15.2\\) for 1-24 months. Panel (a) displays the response of crude oil prices due to an oil supply shock; panel (b) displays the response due to an aggregate demand shock. Notice that both figures have been displayed using the same y-axis scalings so that the figures are comparable.\nWhat is noticeable about the figures is how differently crude oil prices respond to the two shocks. Panel (a) shows that oil prices are only minimally affected by oil production shocks. There is an estimated small short term increase in oil prices, but it is not statistically significant and it reverses within one year. In contrast, panel (b) shows that oil prices are significantly affected by aggregate demand shocks and the effect cumulatively increases over two years. Presumaby, this is because economic activity relies on crude oil and output growth is positively serially correlated.\nThe Kilian (2009) paper is an excellent example of how recursive orderings can be used to identify an orthogonalized VAR through a careful discussion of the causal system and the use of monthly observations."
  },
  {
    "objectID": "chpt15-multiple-time-series.html#structural-vars",
    "href": "chpt15-multiple-time-series.html#structural-vars",
    "title": "15  Multivariate Time Series",
    "section": "15.25 Structural VARs",
    "text": "15.25 Structural VARs\nRecursive models do not allow for simultaneity between the elements of \\(e_{t}\\) and thus the variables \\(Y_{t}\\) cannot be contemporeneously endogenous. This is highly restrictive and may not credibly describe many economic systems. There is a general preference in the economics community for structural vector autoregressive models (SVARs) which use alternative identification restrictions which do not rely\n\n\nSupply Shock\n\n\n\nAggregate Demand Schock\n\nFigure 15.2: Response of Oil Prices to Orthogonalized Shocks\nexclusively on recursiveness. Two popular categories of structural VAR models are those based on shortrun (contemporeneous) restrictions and those based on long-run (cumulative) restrictions. In this section we review SVARs based on short-run restrictions.\nWhen we introduced methods to orthogonalize the VAR errors we pointed out that we can represent the relationship between the errors and shocks using either the equation \\(e_{t}=\\boldsymbol{B} \\varepsilon_{t}\\) (15.15) or the equation \\(\\boldsymbol{A} e_{t}=\\varepsilon_{t}\\) (15.17). Equation (15.15) writes the errors as a function of the shocks. Equation (15.17) writes the errors as a simultaneous system. A broader class of models can be captured by the equation system\n\\[\n\\boldsymbol{A} e_{t}=\\boldsymbol{B} \\varepsilon_{t}\n\\]\nwhere (in the \\(3 \\times 3\\) case)\n\\[\n\\boldsymbol{A}=\\left[\\begin{array}{ccc}\n1 & a_{12} & a_{13} \\\\\na_{21} & 1 & a_{23} \\\\\na_{31} & a_{32} & 1\n\\end{array}\\right], \\quad \\boldsymbol{B}=\\left[\\begin{array}{lll}\nb_{11} & b_{12} & b_{13} \\\\\nb_{21} & b_{22} & b_{23} \\\\\nb_{31} & b_{32} & b_{33}\n\\end{array}\\right] .\n\\]\n(Note: This matrix \\(\\boldsymbol{A}\\) has nothing to do with the regression coefficient matrix \\(\\boldsymbol{A}\\). I apologize for the double use of \\(\\boldsymbol{A}\\), but I use the notation (15.21) to be consistent with the notation elsewhere in the literature.)\nWritten out,\n\\[\n\\begin{aligned}\n&e_{1 t}=-a_{12} e_{2 t}-a_{13} e_{3 t}+b_{11} \\varepsilon_{1 t}+b_{12} \\varepsilon_{2 t}+b_{13} \\varepsilon_{3 t} \\\\\n&e_{2 t}=-a_{21} e_{1 t}-a_{23} e_{3 t}+b_{21} \\varepsilon_{1 t}+b_{22} \\varepsilon_{2 t}+b_{23} \\varepsilon_{3 t} \\\\\n&e_{3 t}=-a_{31} e_{1 t}-a_{32} e_{2 t}+b_{31} \\varepsilon_{1 t}+b_{32} \\varepsilon_{2 t}+b_{33} \\varepsilon_{3 t} .\n\\end{aligned}\n\\]\nThe diagonal elements of the matrix \\(\\boldsymbol{A}\\) are set to 1 as normalizations. This normalization allows the shocks \\(\\varepsilon_{i t}\\) to have unit variance which is convenient for impulse response calculations.\nThe system as written is under-identified. In this three-equation example, the matrix \\(\\Sigma\\) provides only six moments, but the above system has 15 free parameters! To achieve identification we need nine restrictions. In most applications, it is common to start with the restriction that for each common non-diagonal element of \\(\\boldsymbol{A}\\) and \\(\\boldsymbol{B}\\) at most one can be non-zero. That is, for any pair \\(i \\neq j\\), either \\(b_{j i}=0\\) or \\(a_{j i}=0\\).\nWe will illustrate by using a simplified version of the model employed by Blanchard and Perotti (2002) who were interested in decomposing the effects of government spending and taxes on GDP. They proposed a three-variable system consisting of real government spending (net of transfers), real tax revenues (including transfer payments as negative taxes), and real GDP. All variables are measured in logs. They start with the restrictions \\(a_{21}=a_{12}=b_{31}=b_{32}=b_{13}=b_{23}=0\\), or\n\\[\n\\boldsymbol{A}=\\left[\\begin{array}{ccc}\n1 & 0 & a_{13} \\\\\n0 & 1 & a_{23} \\\\\na_{31} & a_{32} & 1\n\\end{array}\\right], \\quad \\boldsymbol{B}=\\left[\\begin{array}{ccc}\nb_{11} & b_{12} & 0 \\\\\nb_{21} & b_{22} & 0 \\\\\n0 & 0 & b_{33}\n\\end{array}\\right] .\n\\]\nThis is done so that that the relationship between the shocks \\(\\varepsilon_{1 t}\\) and \\(\\varepsilon_{2 t}\\) is treated as reduced-form but the coefficients in the \\(\\boldsymbol{A}\\) matrix can be interpreted as contemporeneous elasticities between the variables. For example, \\(a_{23}\\) is the within-quarter elasticity of tax revenue with respect to GDP, \\(a_{31}\\) is the within-quarter elasticity of GDP with respect to government spending, etc.\nWe just described six restrictions while nine are required for identification. Blanchard and Perotti (2002) made a strong case for two additional restrictions. First, the within-quarter elasticity of government spending with respect to GDP is zero, \\(a_{13}=0\\). This is because government fiscal policy does not (and cannot) respond to news about GDP within the same quarter. Since the authors defined government spending as net of transfer payments there is no “automatic stabilizer” component of spending. Second, the within-quarter elasticity of tax revenue with respect to GDP can be estimated from existing microeconometric studies. The authors survey the available literature and set \\(a_{23}=-2.08\\). To fully identify the model we need one final restriction. The authors argue that there is no clear case for any specific restriction, and so impose a recursive \\(\\boldsymbol{B}\\) matrix (setting \\(b_{12}=0\\) ) and experiment with the alternative \\(b_{21}=0\\), finding that the two specifications are near-equivalent since the two shocks are nearly uncorrelated. In summary the estimated model takes the form\n\\[\n\\boldsymbol{A}=\\left[\\begin{array}{ccc}\n1 & 0 & 0 \\\\\n0 & 1 & -2.08 \\\\\na_{31} & a_{32} & 1\n\\end{array}\\right], \\quad \\boldsymbol{B}=\\left[\\begin{array}{ccc}\nb_{11} & 0 & 0 \\\\\nb_{21} & b_{22} & 0 \\\\\n0 & 0 & b_{33}\n\\end{array}\\right] .\n\\]\nBlanchard and Perotti (2002) make use of both matrices \\(\\boldsymbol{A}\\) and \\(\\boldsymbol{B}\\). Other authors use either the simpler structure \\(\\boldsymbol{A} e_{t}=\\varepsilon_{t}\\) or \\(\\boldsymbol{e}_{t}=\\boldsymbol{B} \\varepsilon_{t}\\). In general, either of the two simpler structures are simpler to compute and interpret.\nTaking the variance of the variables on each side of (15.21) we find\n\\[\n\\boldsymbol{A} \\Sigma \\boldsymbol{A}^{\\prime}=\\boldsymbol{B} B^{\\prime} \\text {. }\n\\]\nThis is a system of quadratic equations in the free parameters. If the model is just identified it can be solved numerically to find the coefficients of \\(\\boldsymbol{A}\\) and \\(\\boldsymbol{B}\\) given \\(\\Sigma\\). Similarly, given the least squares error covariance matrix \\(\\widehat{\\Sigma}\\) we can numerically solve for the coefficients of \\(\\widehat{\\boldsymbol{A}}\\) and \\(\\widehat{\\boldsymbol{B}}\\).\nWhile most applications use just-identified models, if the model is over-identified (if there are fewer free parameters than estimated components of \\(\\Sigma\\) ) then the coefficients of \\(\\widehat{\\boldsymbol{A}}\\) and \\(\\widehat{\\boldsymbol{B}}\\) can be found using minimum distance. The implementation in Stata uses MLE (which simultaneously estimates the VAR coefficients). The latter is appropriate when the model is correctly specified (including normality) but otherwise an unclear choice.\nGiven the parameter estimates the structural impulse response function is\n\\[\n\\widehat{\\operatorname{SIRF}}(h)=\\widehat{\\Theta}(h) \\widehat{\\boldsymbol{A}}^{-1} \\widehat{\\boldsymbol{B}} .\n\\]\nThe structural forecast error decompositions are calculated as before with \\(b_{j}\\) replaced by the \\(j^{t h}\\) column of \\(\\widehat{\\boldsymbol{A}}^{-1} \\widehat{\\boldsymbol{B}}\\)\nThe structural impulse responses are nonlinear functions of the VAR coefficient and covariance matrix estimators so by the delta method are asymptotically normal. Thus asymptotic standard errors can be calculated (using numerical derivatives if convenient). As for orthogonalized impulse responses the asymptotic normal approximation is unlikely to be a good approximation so bootstrap methods are an attractive alternative.\nStructural VARs should be interpreted similarly to instrumental variable estimators. Their interpretation relies on valid exclusion restrictions which can only be justified by external information.\nWe replicate a simplified version of Blanchard-Perotti (2002). We use \\({ }^{2}\\) quarterly variables from FREDQD for 1959-2017: real GDP (gdpc1), real tax revenue (fgrecptx), and real government spending (gcec1), all in natural logarithms. Using the AIC for lag length selection we estimate VARs from one to eight lags and select a VAR(5). The model also includes a linear and quadratic function of time \\({ }^{3}\\). In Figure \\(15.3\\) we display the estimated structural impulse responses of GDP with respect to government spending (panel (a)) and tax shocks (panel (b)). The estimated impulse responses are similar to those reported by Blanchard-Perotti.\n\n\nSpending\n\n\n\nTaxes\n\nFigure 15.3: Response of GDP to Government Spending and Tax Shocks\nIn panel (a) we see that the effect of a \\(1 %\\) government spending shock on GDP is positive, small (around \\(0.2 %\\) ), but persistent, remaining stable at \\(0.2 %\\) for four years. In panel (b) we see that the effect of a \\(1 %\\) tax revenue shock is quite different. The effect on GDP is negative and persistent, and more substantial than the effect of a spending shock, reaching about \\(-0.5 %\\) at six quarters. Together, the impulse response estimates show that changes in government spending and tax revenue have meaningful economic impacts. Increased spending has a positive effect on GDP while increased taxes has a negative effect.\n\\({ }^{2}\\) These are similar to, but not the same as, the variables used by Blanchard and Perotti.\n\\({ }^{3}\\) The authors detrend their data using a quadratic function of time. By the FWL Theorem this is equivalent to including a quadratic in time in the regression. The Blanchard-Perotti (2002) paper is an excellent example of how credible exclusion restrictions can be used to identify a non-recursive structural system to help answer an important economic question. The within-quarter exogeneity of government spending is compelling and the use of external information to fix the elasticity of tax revenue with respect to GDP is clever.\nStructural vector autoregressions can be estimated in Stata using the svar command. Short-run restrictions of the form (15.21) can be imposed using the aeq and beq options. Structural impulse responses can be displayed using irf graph sirf and structural forecast error decompositions using irf graph sfevd. Unfortunately, Stata does not provide a convenient way to display cumulative structural impulse response functions. The same limitations for standard error and confidence interval construction in Stata hold for structural impulse responses as for non-structural impulse responses."
  },
  {
    "objectID": "chpt15-multiple-time-series.html#identification-of-structural-vars",
    "href": "chpt15-multiple-time-series.html#identification-of-structural-vars",
    "title": "15  Multivariate Time Series",
    "section": "15.26 Identification of Structural VARs",
    "text": "15.26 Identification of Structural VARs\nThe coefficient matrices \\(\\boldsymbol{A}\\) and \\(\\boldsymbol{B}\\) in (15.21) are identified if they can be uniquely solved from (15.23). This is a set of \\(m(m+1) / 2\\) unique equations so the total number of free coefficients in \\(\\boldsymbol{A}\\) and \\(\\boldsymbol{B}\\) cannot be larger than \\(m(m+1) / 2\\), e.g., 6 when \\(m=3\\). This is the order condition for identification. It is necessary, but not sufficient. It is easy to write down restrictions which satisfy the order condition but do not produce an identified system.\nIt is difficult to see if the system is identified simply by looking at the restrictions (except in the recursive case, which is relatively straightforward to identify). An intuitive way of verifying identification is to use our knowledge of instrumental variables. We can identify the equations sequentially, one at a time, or in blocks, using the metaphor of instrumental variables.\nThe general technique is as follows. Start by writing out the system imposing all restrictions and absorbing the diagonal elements of \\(\\boldsymbol{B}\\) into the shocks (so that they are still uncorrelated but have nonunit variances). For the Blanchard-Perotti (2002) example, this is\n\\[\n\\begin{aligned}\n&e_{1 t}=\\varepsilon_{1 t} \\\\\n&e_{2 t}=2.08 e_{3 t}+b_{21} \\varepsilon_{1 t}+\\varepsilon_{2 t} \\\\\n&e_{3 t}=-a_{31} e_{1 t}-a_{32} e_{2 t}+\\varepsilon_{3 t} .\n\\end{aligned}\n\\]\nTake the equations one at a time and ask if they can be estimated by instrumental variables using the excluded variables as instruments. Once an equation has been verified as identified then its shock is identified and can be used as an instrument since it is uncorrelated with the shocks in the other equations.\nIn this example take the equations as ordered. The first equation is identified as there are no coefficients to estimate. Thus \\(\\varepsilon_{1 t}\\) is identified. For the second equation there is one free parameter which can be estimated by least squares of \\(e_{2 t}-2.08 e_{3 t}\\) on \\(\\varepsilon_{1 t}\\), which is valid because \\(\\varepsilon_{1 t}\\) and \\(\\varepsilon_{2 t}\\) are uncorrelated. This identifies the second equation and the shock \\(\\varepsilon_{2 t}\\). The third equation has two free parameters and two endogenous regressors so we need two instruments. We can use the shocks \\(\\varepsilon_{1 t}\\) and \\(\\varepsilon_{2 t}\\) as they are uncorrelated with \\(\\varepsilon_{3 t}\\) and are correlated with the variables \\(e_{1 t}\\) and \\(e_{2 t}\\). Thus this equation is identified. We deduce that the system is identified.\nConsider another example based on Keating (1992). He estimated a four-variable system with prices, the fed funds rate, M2, and GDP. His model for the errors takes the form \\(\\boldsymbol{A} \\boldsymbol{e}_{t}=\\varepsilon_{t}\\). Written out explicitly:\n\\[\n\\begin{aligned}\ne_{P} &=\\varepsilon_{A S} \\\\\ne_{F F} &=a_{23} e_{M}+\\varepsilon_{M S} \\\\\ne_{M} &=a_{31}\\left(e_{P}+e_{G D P}\\right)+a_{32} e_{F F}+\\varepsilon_{M D} \\\\\ne_{G D P} &=a_{41} e_{P}+a_{42} e_{F F}+a_{43} e_{M}+\\varepsilon_{I S}\n\\end{aligned}\n\\]\nwhere the four shocks are “aggregate supply”, “money supply”, “money demand”, and “I-S”. This structure can be based on the following assumptions: An elastic short-run aggregate supply curve (prices do not respond within a quarter); a simple monetary supply policy (the fed funds rate only responds within quarter to the money supply); money demand only responds to nominal output (log price plus log real output) and fed funds rate within a quarter; and unrestricted I-S curve.\nTo analyze conditions for identification we start by checking the order condition. There are 10 coefficients in the system (including the four variances), which equals \\(m(m+1) / 2\\) because \\(m=4\\). Thus the order condition is exactly satisfied.\nWe check the equations for identification. We start with the first equation. It has no coefficients so is identified and thus so is \\(\\varepsilon_{A S}\\). The second equation has one coefficient. We can use \\(\\varepsilon_{A S}\\) as an instrument because it is uncorrelated with \\(\\varepsilon_{M S}\\). The relevance condition will hold if \\(\\varepsilon_{A S}\\) is correlated with \\(e_{M}\\). From the third equation we see that this will hold if \\(a_{31} \\neq 0\\). Given this assumption \\(a_{23}\\) and \\(\\varepsilon_{M S}\\) are identified. The third equation has two coefficients so we can use \\(\\left(\\varepsilon_{A S}, \\varepsilon_{M S}\\right)\\) as instruments because they are uncorrelated with \\(\\varepsilon_{M D} \\cdot \\varepsilon_{M S}\\) is correlated with \\(e_{F F}\\) and \\(\\varepsilon_{A S}\\) is correlated with \\(e_{P}\\). Thus the relevance condition is satisfied. The final equation has three coefficients so we use \\(\\left(\\varepsilon_{A S}, \\varepsilon_{M S}, \\varepsilon_{M D}\\right)\\) as instruments. They are uncorrelated with \\(\\varepsilon_{I S}\\) and correlated with the variables \\(\\left(e_{P}, e_{F F}, e_{M}\\right)\\) so this equation is identified.\nWe find that the system is identified if \\(a_{31} \\neq 0\\). This requires that money demand responds to nominal GDP which is a prediction from standard monetary economics. This condition seems reasonable. Regardless, the point of this exercise is to determine specific conditions for identification and articulate them in your analysis."
  },
  {
    "objectID": "chpt15-multiple-time-series.html#long-run-restrictions",
    "href": "chpt15-multiple-time-series.html#long-run-restrictions",
    "title": "15  Multivariate Time Series",
    "section": "15.27 Long-Run Restrictions",
    "text": "15.27 Long-Run Restrictions\nTo review, the algebraic identification problem for impulse response estimation is that we require a square root matrix \\(\\boldsymbol{B}=\\Sigma^{1 / 2}\\) yet the latter is not unique and the results are sensitive to the choice. The non-uniqueness arises because \\(\\boldsymbol{B}\\) has \\(m^{2}\\) elements while \\(\\Sigma\\) has \\(m(m+1) / 2\\) free elements. The recursive solution is to set \\(\\boldsymbol{B}\\) to equal the Cholesky decomposition of \\(\\Sigma\\), or equivalently to specify \\(\\boldsymbol{B}\\) as lower triangular. Structural VARs based on short-run (contemporeneous) restrictions generalize this idea by allowing general restrictions on \\(\\boldsymbol{B}\\) based on economic assumptions about contemporeneous causal relations and prior knowledge about \\(\\boldsymbol{B}\\). Identification requires \\(m(m-1) / 2\\) restrictions. Even more generally, a structural VAR can be constructed by imposing \\(m(m-1) / 2\\) restrictions due to any known structure or features of the impulse response functions.\nOne important class of such structural VARs are those based on long-run restrictions. Some economic hypotheses imply restrictions on long-run impulse responses. These can provide a compelling case for identification.\nAn influential example of a structural VAR based on a long-run restriction is Blanchard and Quah (1989). They were interested in decomposing the effects of demand and supply shocks on output. Their hypothesis is that demand shocks are long-run neutral, meaning that the long-run impact of a demand shock on output is zero. This implies that the long-run impulse response of output with respect to demand is zero. This can be used as an identifying restriction.\nThe long-run structural impulse response is the cumulative sum of all impulse responses\n\\[\n\\boldsymbol{C}=\\sum_{\\ell=1}^{\\infty} \\Theta_{\\ell} \\boldsymbol{B}=\\Theta(1) \\boldsymbol{B}=\\boldsymbol{A}(1)^{-1} \\boldsymbol{B} .\n\\]\nA long-run restriction is a restriction placed on the matrix \\(\\boldsymbol{C}\\). Since the sum \\(\\boldsymbol{A}\\) (1) is identified this provides identifying information on the matrix \\(\\boldsymbol{B}\\). Blanchard and Quah (1989) suggest a bivariate VAR for the first-differenced logarithm of real GDP and the unemployment rate. Blanchard-Quah assume that the structural shocks are aggregate supply and aggregate demand. They adopt the hypothesis that aggregate demand has no long-run impact on GDP. This means that the long-run impulse response matrix satisfies\n\\[\n\\boldsymbol{C}=\\left[\\begin{array}{cc}\nc_{11} & 0 \\\\\nc_{21} & c_{22}\n\\end{array}\\right] .\n\\]\nAnother way of thinking about this is that Blanchard-Quah label “aggregate supply” as the long-run component of GDP and label “aggregate demand” as the transitory component of GDP.\nThe relations \\(\\boldsymbol{C}=\\boldsymbol{A}(1)^{-1} \\boldsymbol{B}\\) and \\(\\boldsymbol{B} \\boldsymbol{B}^{\\prime}=\\Sigma\\) imply\n\\[\n\\boldsymbol{C} \\boldsymbol{C}^{\\prime}=\\boldsymbol{A}(1)^{-1} \\boldsymbol{B} \\boldsymbol{B}^{\\prime} \\boldsymbol{A}(1)^{-1 \\prime}=\\boldsymbol{A}(1)^{-1} \\Sigma \\boldsymbol{A}(1)^{-1 \\prime} .\n\\]\nThis is a set of \\(m^{2}\\) equations but because the matrices are positive semi-definite there are \\(m(m+1) / 2\\) independent equations. If the matrix \\(\\boldsymbol{C}\\) has \\(m(m+1) / 2\\) free coefficients then the system is identified. This requires \\(m(m-1) / 2\\) restrictions. In the Blanchard-Quah example, \\(m=2\\) so one restriction is sufficient for identification.\nIn many applications, including Blanchard-Quah, the matrix \\(C\\) is lower triangular which permits the following elegant solution. Examining (15.25) we see that \\(\\boldsymbol{C}\\) is a matrix square root of \\(\\boldsymbol{A}(1)^{-1} \\Sigma \\boldsymbol{A}(1)^{-1}\\), and because \\(\\boldsymbol{C}\\) is lower triangular it is the Cholesky decomposition. We deduce \\(\\boldsymbol{C}=\\operatorname{chol}\\left(\\boldsymbol{A}(1)^{-1} \\Sigma \\boldsymbol{A}(1)^{-1}\\right)\\).\nThe plug-in estimator for \\(\\boldsymbol{C}\\) is \\(\\widehat{\\boldsymbol{C}}=\\operatorname{chol}\\left(\\widehat{\\boldsymbol{A}}(1)^{-1} \\widehat{\\Sigma} \\widehat{\\boldsymbol{A}}(1)^{-1}\\right)\\) where \\(\\widehat{\\boldsymbol{A}}(1)=\\boldsymbol{I}_{m}-\\widehat{\\boldsymbol{A}}_{1}-\\cdots-\\widehat{\\boldsymbol{A}}_{p}\\). By construction the solution \\(\\widehat{C}\\) will be lower triangular and satisfy the desired restriction.\nMore generally if the restrictions on \\(\\boldsymbol{C}\\) do not take a lower triangular form then the estimator can be found by numerically solving the system of quadratic equations\n\\[\n\\widehat{\\boldsymbol{C}} \\widehat{\\boldsymbol{C}}^{\\prime}=\\widehat{\\boldsymbol{A}}(1)^{-1} \\widehat{\\Sigma} \\widehat{\\boldsymbol{A}}(1)^{-1 \\prime} .\n\\]\nIn either case the estimator is \\(\\widehat{\\boldsymbol{B}}=\\widehat{\\boldsymbol{A}}(1) \\widehat{\\boldsymbol{C}}\\) and the estimator of the structural impulse response is\n\\[\n\\widehat{\\operatorname{SIRF}}(h)=\\widehat{\\Theta}_{h} \\widehat{\\boldsymbol{B}}=\\widehat{\\Theta}_{h} \\widehat{\\boldsymbol{A}}(1) \\widehat{\\boldsymbol{C}} .\n\\]\nNotice that by construction the long-run impulse response is\n\\[\n\\sum_{\\ell=1}^{\\infty} \\widehat{\\operatorname{SIRF}}(h)=\\sum_{\\ell=1}^{\\infty} \\widehat{\\Theta}_{h} \\widehat{\\boldsymbol{A}}(1) \\widehat{\\boldsymbol{C}}=\\widehat{\\boldsymbol{A}}(1)^{-1} \\widehat{\\boldsymbol{A}}(1) \\widehat{\\boldsymbol{C}}=\\widehat{\\boldsymbol{C}}\n\\]\nso indeed \\(\\widehat{\\boldsymbol{C}}\\) is the estimated long-run impulse response and satisfies the desired restriction.\nLong-run structural vector autoregressions can be estimated in Stata using the svar command using the lreq option. Structural impulse responses can be displayed using irf graph sirf and structural forecast error decompositions using irf graph sfevd. This Stata option does not produce asymptotic standard errors when imposing long-run restrictions so for confidence intervals bootstrapping is recommended. The same limitations for such intervals constructed in Stata hold for structural impulse response functions as the other cases discussed.\nUnfortunately, a limitation of the Stata svar command is that it does not display cumulative structural impulse response functions. In order to display these one needs to cumulate the impulse response estimates. This can be done but then standard errors and confidence intervals are not available. This means that for serious applied work programming needs to be done outside of Stata."
  },
  {
    "objectID": "chpt15-multiple-time-series.html#blanchard-and-quah-1989-illustration",
    "href": "chpt15-multiple-time-series.html#blanchard-and-quah-1989-illustration",
    "title": "15  Multivariate Time Series",
    "section": "15.28 Blanchard and Quah (1989) Illustration",
    "text": "15.28 Blanchard and Quah (1989) Illustration\nAs we described in the previous section, Blanchard and Quah (1989) estimated a bivariate VAR in GDP growth and the unemployment rate assuming that the the structural shocks are aggregate supply and aggregate demand imposing that that the long-run response of GDP with respect to aggregate demand is zero. Their original application used U.S. data for 1950-1987. We revisit using FRED-QD (1959-2017). While Blanchard and Quah used a VAR(8) model the AIC selects a VAR(3). We use a VAR(4). To ease the interpretation of the impulse responses the unemployment rate is entered negatively (multiplied by -1) so that both series are pro-cyclical and positive shocks increase output. Blanchard and Quah used a careful detrending method; instead we including a linear time trend in the estimated VAR.\nThe fitted reduced form model coefficients satisfy\n\\[\n\\widehat{\\boldsymbol{A}}(1)=\\boldsymbol{I}_{m}-\\sum_{j=1}^{4} \\widehat{\\boldsymbol{A}}_{j}=\\left(\\begin{array}{cc}\n0.42 & 0.05 \\\\\n-0.15 & 0.04\n\\end{array}\\right)\n\\]\nand the residual covariance matrix is\n\\[\n\\widehat{\\Sigma}=\\left(\\begin{array}{ll}\n0.531 & 0.095 \\\\\n0.095 & 0.053\n\\end{array}\\right) .\n\\]\nWe calculate\n\\[\n\\begin{gathered}\n\\widehat{\\boldsymbol{C}}=\\operatorname{chol}\\left(\\widehat{\\boldsymbol{A}}(1)^{-1} \\widehat{\\Sigma} \\widehat{\\boldsymbol{A}}(1)^{-1 \\prime}\\right)=\\left(\\begin{array}{cc}\n1.00 & 0 \\\\\n4.75 & 5.42\n\\end{array}\\right) \\\\\n\\widehat{\\boldsymbol{B}}=\\widehat{\\boldsymbol{A}}(1) \\widehat{\\boldsymbol{C}}=\\left(\\begin{array}{cc}\n0.67 & 0.28 \\\\\n0.05 & 0.23\n\\end{array}\\right)\n\\end{gathered}\n\\]\nExamining \\(\\widehat{\\boldsymbol{B}}\\), the unemployment rate is contemporeneously mostly affected by the aggregate demand shock, while GDP growth is affected by both shocks.\nUsing this square root of \\(\\widehat{\\Sigma}\\) we construct the structural impulse response functions as a function of the two shocks (aggregate supply and aggregate demand). In Figure \\(15.4\\) we display the estimated structural impulse responses of the (negative) unemployment rate. Panel (a) displays the impulse response of the unemployment rate with respect to the aggregate supply shock. whiled panel (b) displays the impulse response of the unemployment rate with respect to the aggregate demand shock. Displayed are 95% normal approximation bootstrap intervals, calculated from 10,000 bootstrap replications. The estimated impulse responses have similar hump shapes with a peak around four quarters, and are similar to those found by Blanchard and Quah (1989).\nLet’s examine and contrast panels (a) and (b) of Figure 15.4. The response to a supply shock (panel (a)) takes several quarters to take effect, peaks around 5 quarters, and then decays. The response to a demand shock (panel (b)) is more immediate, peaks around 4 quarters, and then decays. Both are near zero by 6 years. The confidence intervals for the supply shock impulse responses are wider than those for the demand shocks indicating that the estimates of the impulse responses due to supply shocks are not precisely estimated.\nFigure \\(15.5\\) displays the estimated structural forecast error decompositions. Since there are only two errors we only display the percentage squared error due to the supply shock. In panel (a) we display the forecast error decomposition for GDP and in panel (b) the forecast error decomposition for the unemployment rate. We can see that about \\(80 %\\) of the fluctuations in GDP are attributed to the supply shock. For the unemployment rate the short-term fluctuations are mostly attributed to the demand shock but the long-run impact is about \\(40 %\\) due to the supply shock. The confidence intervals are very wide, however, indicating that these estimates are not precise.\n\n\nSupply Shock\n\n\n\nDemand Shock\n\nFigure 15.4: Response of Unemployment Rate\nIt is fascinating that the structural impulse response estimates shown here are nearly identical to those found by Blanchard and Quah (1989) despite the fact that we have used a considerably different sample period."
  },
  {
    "objectID": "chpt15-multiple-time-series.html#external-instruments",
    "href": "chpt15-multiple-time-series.html#external-instruments",
    "title": "15  Multivariate Time Series",
    "section": "15.29 External Instruments",
    "text": "15.29 External Instruments\nStructural VARs can also be identified and estimated using external instrumental variables. This method is also called Proxy SVARs. Consider the three-variable system for the innovations\n\\[\n\\begin{array}{rrr}\ne_{1 t}+a_{12} e_{2 t}+a_{13} e_{3 t} & =\\varepsilon_{1 t} \\\\\na_{21} e_{1 t}+e_{2 t} & = & \\varepsilon_{2 t}+b_{23} \\varepsilon_{3 t}=u_{2 t} \\\\\na_{31} e_{1 t}+\\quad e_{3 t} & = & b_{32} \\varepsilon_{2 t}+\\varepsilon_{3 t}=u_{3 t} .\n\\end{array}\n\\]\nIn this system we have used the normalization \\(b_{11}=b_{22}=b_{33}=1\\) rather than normalizing the variances of the shocks.\nSuppose we have an external instrumental variable \\(Z_{t}\\) which satisfies the properties\n\\[\n\\begin{aligned}\n&\\mathbb{E}\\left[Z_{t} \\varepsilon_{1 t}\\right] \\neq 0 \\\\\n&\\mathbb{E}\\left[Z_{t} \\varepsilon_{2 t}\\right]=0 \\\\\n&\\mathbb{E}\\left[Z_{t} \\varepsilon_{3 t}\\right]=0 .\n\\end{aligned}\n\\]\nEquation (15.29) is the relevance condition - that the instrument and the shock \\(\\varepsilon_{1 t}\\) are correlated. Equations (15.30)-(15.31) are the exogeneity condition - that the instrument is uncorrelated with the shocks \\(\\varepsilon_{2 t}\\) and \\(\\varepsilon_{3 t}\\). Identification rests on the validity of these assumptions.\nSuppose \\(e_{1 t}, e_{2 t}\\) and \\(e_{3 t}\\) were observed. Then the coefficient \\(a_{21}\\) in (15.27) can be estimated by instrumental variables regression of \\(e_{2 t}\\) on \\(e_{1 t}\\) using the instrumental variable \\(Z_{t}\\). This is valid because \\(Z_{t}\\)\n\n\nGDP\n\n\n\nUnemployment Rate\n\nFigure 15.5: Forecast Error Decomposition, Percent due to Supply Shock\nis uncorrelated with \\(u_{2 t}=\\varepsilon_{2 t}+b_{23} \\varepsilon_{3 t}\\) under the assumptions (15.30)-(15.31) yet is correlated with \\(e_{1 t}\\) under (15.29). Given this estimator we obtain a residual \\(\\widehat{u}_{2 t}\\). Similarly we can estimate \\(a_{31}\\) in (15.27) by instrumental variables regression of \\(e_{3 t}\\) on \\(e_{1 t}\\) using the instrumental variable \\(Z_{t}\\), obtaining a residual \\(\\widehat{u}_{3 t}\\). We can then estimate \\(a_{12}\\) and \\(a_{13}\\) in in (15.26) by instrumental variables regression of \\(e_{1 t}\\) on \\(\\left(e_{2 t}, e_{3 t}\\right)\\) using the instrumental variables \\(\\left(\\widehat{u}_{2 t}, \\widehat{u}_{3 t}\\right)\\). The latter are valid instruments because \\(\\mathbb{E}\\left[u_{2 t} \\varepsilon_{1 t}\\right]=0\\) and \\(\\mathbb{E}\\left[u_{3 t} \\varepsilon_{1 t}\\right]=0\\) as the structural errors are uncorrelated, and because \\(\\left(u_{2 t}, u_{3 t}\\right)\\) is correlated with \\(\\left(e_{2 t}, e_{3 t}\\right)\\) by construction. This regression also produces a residual \\(\\widehat{\\varepsilon}_{1 t}\\) which is an appropriate estimator for the shock \\(\\varepsilon_{1 t}\\).\nThis estimation method is not special for a three-variable system; it can be applied for any \\(m\\). The identified coefficients are those in the first equation (15.26), the structural shock \\(\\varepsilon_{1 t}\\), and the impacts \\(\\left(a_{21}\\right.\\) and \\(\\alpha_{31}\\) ) of this shock on the other variables. The other shocks \\(\\varepsilon_{2 t}\\) and \\(\\varepsilon_{3 t}\\) are not separately identified, and their correlation structure \\(\\left(b_{23}\\right.\\) and \\(\\left.b_{32}\\right)\\) is not identified. An exception arises when \\(m=2\\), in which case all coefficients and shocks are identified.\nWhile \\(e_{1 t}, e_{2 t}\\) and \\(e_{3 t}\\) are not observed we can replace their values by the residuals \\(\\widehat{e}_{1 t}, \\widehat{e}_{2 t}\\) and \\(\\widehat{e}_{3 t}\\) from the estimated \\(\\operatorname{VAR}(\\mathrm{p})\\) model. All of the coefficient estimates are then two-step estimators with generated regressors. This affects the asymptotic distribution so conventional asymptotic standard errors should not be used. Bootstrap confidence intervals are appropriate.\nThe structure (15.26)-(15.28) is convenient as four coefficients can be identified. Other structures can also be used. Consider the structure\n\\[\n\\begin{aligned}\n&e_{1 t}=\\varepsilon_{1 t}+b_{12} \\varepsilon_{2 t}+b_{23} \\varepsilon_{3 t} \\\\\n&e_{2 t}=b_{21} \\varepsilon_{1 t}+\\varepsilon_{2 t}+b_{23} \\varepsilon_{3 t} \\\\\n&e_{3 t}=b_{31} \\varepsilon_{1 t}+b_{32} \\varepsilon_{2 t}+\\varepsilon_{3 t}\n\\end{aligned}\n\\]\nIf the same procedure is applied we can identify the coefficients \\(b_{21}\\) and \\(b_{31}\\) and the shock \\(\\varepsilon_{1 t}\\) but no other coefficients or shocks. In this structure the coefficients \\(b_{12}\\) and \\(b_{23}\\) cannot be separately identified because the shocks \\(\\varepsilon_{2 t}\\) and \\(\\varepsilon_{3 t}\\) are not separately identified. For more details see Stock and Watson (2012) and Mertens and Ravn (2013)."
  },
  {
    "objectID": "chpt15-multiple-time-series.html#dynamic-factor-models",
    "href": "chpt15-multiple-time-series.html#dynamic-factor-models",
    "title": "15  Multivariate Time Series",
    "section": "15.30 Dynamic Factor Models",
    "text": "15.30 Dynamic Factor Models\nDynamic factor models are increasingly popular in applied time series, in particular for forecasting. For a recent detailed review of the methods see Stock and Watson (2016) and the references therein. For some of the foundational theory see Bai (2003) and Bai and \\(\\mathrm{Ng}(2002,2006)\\).\nIn Sections 11.13-11.16 we introduced the standard multi-factor model (11.23):\n\\[\nX_{t}=\\Lambda F_{t}+u_{t}\n\\]\nwhere \\(X_{t}\\) and \\(u_{t}\\) are \\(k \\times 1, \\Lambda\\) is \\(k \\times r\\) with \\(r<k\\), and \\(F_{t}\\) is \\(r \\times 1\\). The elements of \\(F_{t}\\) are called the common factors as they affect all elements of \\(X_{t}\\). The columns of \\(\\Lambda\\) are called the factor loadings. The variables \\(u_{t}\\) are called the idiosyncratic errors. It is often assumed that the elements of \\(X_{t}\\) have been transformed to be mean zero and have common variances.\nIn the time-series case it is natural to augment the model to allow for dynamic relationships. In particular we would like to allow \\(F_{t}\\) and \\(u_{t}\\) to be serially correlated. It is convenient to consider vector autoregressive models which can be written using lag operator notation as\n\\[\n\\begin{aligned}\n&\\boldsymbol{A}(\\mathrm{L}) F_{t}=v_{t} \\\\\n&\\boldsymbol{B}(\\mathrm{L}) u_{t}=e_{t}\n\\end{aligned}\n\\]\nwhere \\(\\boldsymbol{A}\\) (L) and \\(\\boldsymbol{B}\\) (L) are lag polynomials with \\(p\\) and \\(q\\) lags, respectively. Equations (15.32)-(15.33)(15.34) together make the standard dynamic factor model. To simplify the model and aid identification, further restrictions are often imposed, in particular that the lag polynomial \\(\\boldsymbol{B}(\\mathrm{L})\\) is diagonal.\nFurthermore we may wish to generalize (15.32) to allow \\(F_{t}\\) to impact \\(X_{t}\\) via a distributed lag relationship. This generalization can be written as\n\\[\nX_{t}=\\Lambda(\\mathrm{L}) F_{t}+u_{t}\n\\]\nwhere \\(\\Lambda(\\mathrm{L})\\) is an \\(\\ell^{t h}\\) order distributed lag of dimension \\(k \\times r\\). Equation (15.35), however, is not fundamentally different from (15.32). That is, if we define the stacked factor vector \\(\\bar{F}_{t}=\\left(F_{t}^{\\prime}, F_{t-1}^{\\prime}, \\ldots, F_{t-\\ell}^{\\prime}\\right)^{\\prime}\\) then (15.35) can be written in the form (15.32) with \\(\\bar{F}_{t}\\) replacing \\(F_{t}\\) and the matrix \\(\\Lambda\\) replaced by \\(\\left(\\Lambda_{1}, \\Lambda_{2}, \\ldots, \\Lambda_{\\ell}\\right)\\). Hence we will focus on the standard model (15.32)-(15.33)-(15.34).\nDefine the inverse lag operators \\(\\boldsymbol{D}(\\mathrm{L})=\\boldsymbol{A}(\\mathrm{L})^{-1}\\) and \\(\\boldsymbol{C}(\\mathrm{L})=\\boldsymbol{B}(\\mathrm{L})^{-1}\\). Then by applying \\(\\boldsymbol{C}(\\mathrm{L})\\) to (15.32) and \\(\\boldsymbol{D}(\\mathrm{L})\\) to (15.33) we obtain\n\\[\n\\begin{aligned}\n\\boldsymbol{C}(\\mathrm{L}) X_{t} &=\\boldsymbol{C} \\text { (L) } \\Lambda F_{t}+\\boldsymbol{C} \\text { (L) } u_{t} \\\\\n&=\\boldsymbol{C} \\text { (L) } \\Lambda \\boldsymbol{D}(\\mathrm{L}) v_{t}+e_{t} \\\\\n&=\\Lambda(\\mathrm{L}) v_{t}+e_{t}\n\\end{aligned}\n\\]\nwhere \\(\\Lambda(\\mathrm{L})=\\boldsymbol{C}\\) (L) \\(\\Lambda \\boldsymbol{D}(\\mathrm{L})\\). For simplicity treat this lag polynomial as if it has \\(\\ell\\) lags. Using the same stacking trick from the previous paragraph and defining \\(V_{t}=\\left(v_{t}^{\\prime}, v_{t-1}^{\\prime}, \\ldots, v_{t-\\ell}^{\\prime}\\right)^{\\prime}\\) we find that this model can be written as\n\\[\n\\boldsymbol{C} \\text { (L) } X_{t}=\\boldsymbol{H} V_{t}+e_{t}\n\\]\nfor some \\(k \\times r \\ell\\) matrix \\(\\boldsymbol{H}\\). This is known as the static form of the dynamic factor model. It shows that \\(X_{t}\\) can be written as a function of its own lags plus a linear function of the serially uncorrelated factors \\(V_{t}\\) and a serially uncorrelated error \\(e_{t}\\). The static form (15.36) is convenient as factor regression can be used for estimation. The model is identical to factor regression with additional regressors as described in Section 11.15. (The additional regressors are the lagged values of \\(X_{t}\\).) In that section it is described how to estimate the coefficients and factors by iterating between multivariate least squares and factor regression.\nTo estimate the explicit dynamic model (15.32)-(15.33)-(15.34) state-space methods are convenient. For details and references see Stock and Watson (2016).\nThe dynamic factor model (15.32)-(15.33)-(15.34) can be estimated in Stata using df actor."
  },
  {
    "objectID": "chpt15-multiple-time-series.html#technical-proofs",
    "href": "chpt15-multiple-time-series.html#technical-proofs",
    "title": "15  Multivariate Time Series",
    "section": "15.31 Technical Proofs*",
    "text": "15.31 Technical Proofs*\nProof of Theorem 15.6 Without loss of generality assume \\(a_{0}=0\\).\nBy the Jordan matrix decomposition (see Section A.13), \\(\\boldsymbol{A}=\\boldsymbol{P} \\boldsymbol{J} \\boldsymbol{P}^{-1}\\) where \\(\\boldsymbol{J}=\\operatorname{diag}\\left\\{\\boldsymbol{J}_{1}, \\ldots, \\boldsymbol{J}_{r}\\right\\}\\) is in Jordan normal form. The dimension of each Jordan block \\(\\boldsymbol{J}_{i}\\) is determined by the multiplicity of the eigenvalues \\(\\lambda_{i}\\) of \\(\\boldsymbol{A}\\). For unique eigenvalues \\(\\lambda_{i}, \\boldsymbol{J}_{i}=\\lambda_{i}\\). For eigenvalues \\(\\lambda_{i}\\) with double multiplicity the Jordan blocks take the form\n\\[\n\\boldsymbol{J}_{i}=\\left[\\begin{array}{cc}\n\\lambda_{i} & 1 \\\\\n0 & \\lambda_{i}\n\\end{array}\\right] .\n\\]\nFor eigenvalues with multiplicity \\(s>2\\) the Jordan blocks are \\(s \\times s\\) upper diagonal with the eigenvalue on the diagonal and 1 immediately above the diagonal (see A.7).\nDefine \\(X_{t}=\\boldsymbol{P}^{-1} Y_{t}\\) and \\(u_{t}=\\boldsymbol{P}^{-1} e_{t}\\), which satisfy \\(X_{t}=\\boldsymbol{J} X_{t-1}+u_{t}\\). Partition \\(X_{t}\\) and \\(u_{t}\\) conformably with \\(\\boldsymbol{J}\\). The \\(i^{\\text {th }}\\) set satisfy \\(X_{i t}=\\boldsymbol{J}_{i} X_{i, t-1}+u_{i t}\\). We now show that \\(X_{i t}\\) is strictly stationary and ergodic, from which we deduce that \\(Y_{t}=\\boldsymbol{P} X_{t}\\) is strictly stationary and ergodic.\nFor single dimension blocks \\(\\boldsymbol{J}_{i}=\\lambda_{i}\\), so \\(X_{i t}=\\lambda_{i} X_{i, t-1}+u_{i t}\\) which is an AR(1) model with coefficient \\(\\lambda_{i}\\) and innovation \\(u_{i t}\\). The assumptions imply \\(\\left|\\lambda_{i}\\right|<1\\) and \\(\\mathbb{E}\\left|u_{i t}\\right|<\\infty\\) so the conditions of Theorem \\(14.21\\) are satisfied, implying that \\(X_{i t}\\) is strictly stationary and ergodic.\nFor blocks with dimension two, by back-substitution we find \\(X_{i t}=\\sum_{\\ell=0}^{\\infty} J_{i}^{\\ell} u_{i, t-\\ell}\\). By direct calculation we find that\n\\[\n\\boldsymbol{J}_{i}^{\\ell}=\\left[\\begin{array}{cc}\n\\lambda_{i}^{\\ell} & \\ell \\lambda_{i}^{\\ell-1} \\\\\n0 & \\lambda_{i}^{\\ell}\n\\end{array}\\right] .\n\\]\nPartitioning \\(X_{i t}=\\left(X_{1 i t}, X_{2 i t}\\right)\\) and \\(u_{i t}=\\left(u_{1 i t}, u_{2 i t}\\right)\\) this means that\n\\[\n\\begin{aligned}\nX_{1 i t} &=\\sum_{\\ell=0}^{\\infty} \\lambda_{i}^{\\ell} u_{1 i, t-\\ell}+\\sum_{\\ell=0}^{\\infty} \\ell \\lambda_{i}^{\\ell} u_{2 i, t-\\ell} \\\\\nX_{2 i t} &=\\sum_{\\ell=0}^{\\infty} \\lambda_{i}^{\\ell} u_{2 i, t-\\ell}\n\\end{aligned}\n\\]\nThe series \\(\\sum_{\\ell=0}^{\\infty} \\lambda_{i}^{\\ell}\\) and \\(\\sum_{\\ell=0}^{\\infty} \\ell \\lambda_{i}^{\\ell}\\) are convergent by the ratio test (Theorem A.3 of Probability and Statistics for Economists) because \\(\\left|\\lambda_{i}\\right|<1\\). Thus the above sums satisfy the conditions of Theorem \\(14.6\\) so are strictly stationary and ergodic as required.\nBlocks with multiplicity \\(s>2\\) are handled by similar but more tedious calculations.\nProof of Theorem \\(15.8\\) The assumption that \\(\\Sigma>0\\) means that if we regress \\(Y_{1 t}\\) on \\(Y_{2 t}, \\ldots, Y_{p t}\\) and \\(Y_{t-1}, \\ldots, Y_{t-p}\\) that the error will have positive variance. If \\(\\boldsymbol{Q}\\) is singular then there is some \\(\\gamma\\) such that \\(\\gamma^{\\prime} \\boldsymbol{Q} \\gamma=0\\). As in the proof of Theorem \\(14.28\\) this means that the regression of \\(Y_{1 t}\\) on \\(Y_{2 t}, \\ldots, Y_{p t}, Y_{t-1}, \\ldots, Y_{t-p+1}\\) has a zero variance. This is a contradiction. We conclude that \\(\\boldsymbol{Q}\\) is not singular.\nProof of Theorem 15.12 The first part of the theorem is established by back-substitution. Since \\(Y_{t}\\) is a VAR(p) process,\n\\[\nY_{t+h}=a_{0}+\\boldsymbol{A}_{1} Y_{t+h-1}+\\boldsymbol{A}_{2} Y_{t+h-2}+\\cdots+\\boldsymbol{A}_{p} Y_{t+h-p}+e_{t} .\n\\]\nWe then substitute out the first lag. We find\n\\[\n\\begin{aligned}\nY_{t+h} &=a_{0}+\\boldsymbol{A}_{1}\\left(a_{0}+\\boldsymbol{A}_{1} Y_{t+h-2}+\\boldsymbol{A}_{2} Y_{t+h-3}+\\cdots+\\boldsymbol{A}_{p} Y_{t+h-p-1}+e_{t-1}\\right)+\\boldsymbol{A}_{2} Y_{t+h-2}+\\cdots+\\boldsymbol{A}_{p} Y_{t+h-p}+e_{t} \\\\\n&=a_{0}+\\boldsymbol{A}_{1} a_{0}+\\left(\\boldsymbol{A}_{1} \\boldsymbol{A}_{1}+\\boldsymbol{A}_{2}\\right) Y_{t+h-2}+\\left(\\boldsymbol{A}_{1} \\boldsymbol{A}_{2}+\\boldsymbol{A}_{3}\\right) Y_{t+h-3}+\\cdots+\\boldsymbol{A}_{p} \\boldsymbol{A}_{p} Y_{t+h-p-1}+\\boldsymbol{A}_{1} e_{t-1}+e_{t} .\n\\end{aligned}\n\\]\nWe continue making substitutions. With each substitution the error increases its MA order. After \\(h-1\\) substitutions the equation takes the form (15.12) with \\(u_{t}\\) an MA(h-1) process.\nTo recognize that \\(\\boldsymbol{B}_{1}=\\Theta_{h}\\), notice that the deduction that \\(u_{t}\\) is an MA(h-1) process means that we can equivalently write \\((15.12)\\) as\n\\[\nY_{t+h}=b_{0}+\\sum_{j=1}^{\\infty} \\boldsymbol{B}_{j} Y_{t+1-j}+u_{t}\n\\]\nwith \\(\\boldsymbol{B}_{j}=0\\) for \\(j>p\\). That is, the equation (15.12) includes all relevant lags. By the projection properties of regression coefficients this means that the coefficient \\(\\boldsymbol{B}_{1}\\) is invariant to replacing the regressor \\(Y_{t}\\) by the innovation from its regression on the other lags. This is the VAR(p) model itself which has innovation \\(e_{t}\\). We have deduced that the coefficient \\(\\boldsymbol{B}_{1}\\) is equivalent to that in the regression\n\\[\nY_{t+h}=b_{0}+\\boldsymbol{B}_{1} e_{t}+\\sum_{j=2}^{\\infty} \\boldsymbol{B}_{j} Y_{t+1-j}+u_{t} .\n\\]\nNotice that \\(e_{t}\\) is uncorrelated with the other regressors. Thus \\(\\boldsymbol{B}_{1}=\\frac{\\partial}{\\partial e_{t}^{t}} \\mathscr{P}_{t}\\left[Y_{t+h}\\right]=\\Theta_{h}\\) as claimed. This completes the proof."
  },
  {
    "objectID": "chpt15-multiple-time-series.html#exercises",
    "href": "chpt15-multiple-time-series.html#exercises",
    "title": "15  Multivariate Time Series",
    "section": "15.32 Exercises",
    "text": "15.32 Exercises\nExercise 15.1 Take the VAR(1) model \\(Y_{t}=\\boldsymbol{A} Y_{t-1}+e_{t}\\). Assume \\(e_{t}\\) is i.i.d. For each specified matrix \\(\\boldsymbol{A}\\) below, check if \\(Y_{t}\\) is strictly stationary. Use mathematical software to compute eigenvalues if needed.\\ (a) \\(\\boldsymbol{A}=\\left[\\begin{array}{ll}0.7 & 0.2 \\\\ 0.2 & 0.7\\end{array}\\right]\\)\\ (b) \\(\\boldsymbol{A}=\\left[\\begin{array}{ll}0.8 & 0.4 \\\\ 0.4 & 0.8\\end{array}\\right]\\)\\ (c) \\(\\boldsymbol{A}=\\left[\\begin{array}{cc}0.8 & 0.4 \\\\ -0.4 & 0.8\\end{array}\\right]\\)\nExercise 15.2 Take theVAR(2) model \\(Y_{t}=\\boldsymbol{A}_{1} Y_{t-1}+\\boldsymbol{A}_{2} Y_{t-2}+e_{t}\\) with \\(\\boldsymbol{A}_{1}=\\left[\\begin{array}{cc}0.3 & 0.2 \\\\ 0.2 & 0.3\\end{array}\\right]\\) and \\(\\boldsymbol{A}_{2}=\\left[\\begin{array}{cc}0.4 & -0.1 \\\\ -0.1 & 0.4\\end{array}\\right]\\). Assume \\(e_{t}\\) is i.i.d. Is \\(Y_{t}\\) strictly stationary? Use mathematical software if needed.\nExercise 15.3 Suppose \\(Y_{t}=\\boldsymbol{A} Y_{t-1}+u_{t}\\) and \\(u_{t}=\\boldsymbol{B} u_{t-1}+e_{t}\\). Show that \\(Y_{t}\\) is a VAR(2) and derive the coefficient matrices and equation error.\nExercise 15.4 Suppose \\(Y_{i t}, i=1, \\ldots, m\\), are independent AR(p) processes. Derive the form of their joint VAR representation.\nExercise 15.5 In the VAR(1) model \\(Y_{t}=\\boldsymbol{A}_{1} Y_{t-1}+e_{t}\\) find an explicit expression for the \\(h\\)-step moving average matrix \\(\\Theta_{h}\\) from (15.3). Exercise 15.6 In the VAR(2) model \\(Y_{t}=\\boldsymbol{A}_{1} Y_{t-1}+\\boldsymbol{A}_{2} Y_{t-2}+e_{t}\\) find explicit expressions for the moving average matrix \\(\\Theta_{h}\\) from (15.3) for \\(h=1, \\ldots 4\\).\nExercise 15.7 Derive a VAR(1) representation of a VAR(p) process analogously to equation (14.33) for autoregressions. Use this to derive an explicit formula for the \\(h\\)-step impulse response IRF(h) analogously to (14.42).\nExercise 15.8 Let \\(Y_{t}=\\left(Y_{1 t}, Y_{2 t}\\right)^{\\prime}\\) be \\(2 \\times 1\\) and consider a VAR(2) model. Suppose \\(Y_{2 t}\\) does not Grangercause \\(Y_{1 t}\\). What are the implications for the VAR coefficient matrices \\(\\boldsymbol{A}_{1}\\) and \\(\\boldsymbol{A}_{2}\\) ?\nExercise 15.9 Continuting the previous exercise, suppose that both \\(Y_{2 t}\\) does not Granger-cause \\(Y_{1 t}\\), and \\(Y_{1 t}\\) does not Granger-cause \\(Y_{2 t}\\). What are the implications for the VAR coefficient matrices \\(\\boldsymbol{A}_{1}\\) and \\(\\boldsymbol{A}_{2}\\) ?\nExercise 15.10 Suppose that you have 20 years of monthly observations on \\(m=8\\) variables. Your advisor recommends \\(p=12\\) lags to account for annual patterns. How many coefficients per equation will you be estimating? How many observations do you have? In this context does it make sense to you to estimate a VAR(12) with all eight variables?\nExercise 15.11 Let \\(\\widehat{e}_{t}\\) be the least squares residuals from an estimated VAR, \\(\\widehat{\\Sigma}\\) be the residual covariance matrix, and \\(\\widehat{\\boldsymbol{B}}=\\operatorname{chol}(\\widehat{\\Sigma})\\). Show that \\(\\widehat{\\boldsymbol{B}}\\) can be calculated by recursive least squares using the residuals.\nExercise 15.12 Cholesky factorization\n\nDerive the Cholesky decomposition of the covariance matrix \\(\\left[\\begin{array}{cc}\\sigma_{1}^{2} & \\rho \\sigma_{1} \\sigma_{2} \\\\ \\rho \\sigma_{1} \\sigma_{2} & \\sigma_{1}^{2}\\end{array}\\right]\\).\nWrite the answer for the correlation matrix (the special case \\(\\sigma_{1}^{2}=1\\) and \\(\\sigma_{2}^{2}=1\\) ).\nFind an upper triangular decomposition for the correlation matrix. That is, an upper-triangular matrix \\(R\\) which satisfies \\(R R^{\\prime}=\\left[\\begin{array}{ll}1 & \\rho \\\\ \\rho & 1\\end{array}\\right]\\).\nSuppose \\(\\Theta_{h}=\\left[\\begin{array}{ll}1 & 0 \\\\ 1 & 1\\end{array}\\right], \\sigma_{1}^{2}=1\\), and \\(\\sigma_{2}^{2}=1\\), and \\(\\rho=0.8\\). Find the orthogonalized impulse response OIRF(h) using the Cholesky decomposition.\nSuppose that the ordering of the variables is reversed. This is equivalent to using the upper triangular decomposition from part (c). Calculate the orthogonalized impulse response OIRF(h).\nCompare the two orthogonalized impulse responses.\n\nExercise 15.13 You read an empirical paper which estimates a VAR in a listed set of variables and displays estimated orthogonalized impulse response functions. No comment is made in the paper about the ordering or the identification of the system, and you have no reason to believe that the order used is “standard” in the literature. How should you interpret the estimated impulse response functions?\nExercise 15.14 Take the quarterly series gdpcl (real GDP), gdpctpi (GDP price deflator), and fedfunds (Fed funds interest rate) from FRED-QD. Transform the first two into growth rates as in Section 15.13. Estimate the same three-variable VAR(6) using the same ordering. The identification strategy discussed in Section \\(15.23\\) specifies the supply shock as the orthogonalized shock to the GDP equation. Calculate the impulse response function of GDP, the price level, and the Fed funds rate with respect to this supply shock. For the first two this will require calculating the cumulative impulse response function. (Explain why.) Comment on the estimated functions. Exercise 15.15 Take the Kilian2009 dataset which has the variables oil (oil production), output (global economic activity), and price (price of crude oil). Estimate an orthogonalized VAR(4) using the same ordering as in Kilian (2009) as described in Section 15.24. (As described in that section, multiply “oil” by \\(-1\\) so that all shocks increase prices.) Estimate the impulse response of output with respect to the three shocks. Comment on the estimated functions.\nExercise 15.16 Take the monthly series permit (building permits), houst (housing starts), and realln (real estate loans) from FRED-MD. The listed ordering is motivated by transaction timing. A developer is required to obtain a building permit before they start building a house (the latter is known as a “housing start”). A real estate loan is obtained when the house is purchased.\n\nTransform realln into growth rates (first difference of logs).\nSelect an appropriate lag order for the three-variable system by comparing the AIC of VARs of order 1 through 8.\nEstimate the VAR model and plot the impulse response functions of housing starts with respect to the three shocks.\nInterpret your findings.\n\nExercise 15.17 Take the quarterly series gpdicl (Real Gross Private Domestic Investment), gdpctpi (GDP price deflator), gdpcl (real GDP), and fedfunds (Fed funds interest rate) from FRED-QD. Transform the first three into logs, e.g. \\(g d p=100 \\log (g d p c 1)\\). Consider a structural VAR based on short-run restrictions. Use a structure of the form \\(\\boldsymbol{A} \\boldsymbol{e}_{t}=\\varepsilon_{t}\\). Impose the restrictions that the first three variables do not react to the fed funds rate, that investment does not respond to prices, and that prices do not respond to investment. Finally, impose that investment is short-run unit elastic with respect to GDP (in the equation for investment, the \\(\\boldsymbol{A}\\) coefficient on GDP is \\(-1\\) ).\n\nWrite down the matrix \\(\\boldsymbol{A}\\) similar to (15.22), imposing the identifying constraints as defined above.\nIs the model identified? Is there a condition for identification? Explain.\nIn this model are output and price simultaneous, or recursive as in the example described in Section \\(15.23\\) ?\nEstimate the structural VAR using 6 lags or a different number of your choosing (justify your choice) and include an exogenous time trend. Report your estimates of the \\(\\boldsymbol{A}\\) matrix. Can you interpret the coefficients?\nEstimate and report the following three impulse response functions:\n\n\nThe effect of the fed funds rate on GDP.\nThe effect of the GDP shock on GDP.\nThe effect of the GDP shock on prices.\n\nExercise 15.18 Take the Kilian2009 dataset which has the variables oil (oil production), output (global economic activity), and price (price of crude oil). Consider a structural VAR based on short-run restrictions. Use a structure of the form \\(A e_{t}=\\varepsilon_{t}\\). Impose the restrictions that oil production does not respond to output or oil prices, and that output does not respond to oil production. The last restriction can be motivated by the observation that supply disruptions take more than a month to reach the retail market so the effect on economic activity is similarly delayed by one month. (a) Write down the matrix \\(\\boldsymbol{A}\\) similar to (15.22) imposing the identifying constraints as defined above.\n\nIs the model identified? Is there a condition for identification? Explain.\nEstimate the structural VAR using 4 lags or a different number of your choosing (justify your choice). (As described in that section, multiply “oil” by \\(-1\\) so that all shocks increase prices.) Report your estimates of the \\(\\boldsymbol{A}\\) matrix. Can you interpret the coefficients?\nEstimate the impulse response of oil price with respect to the three shocks. Comment on the estimated functions.\n\nExercise 15.19 Take the quarterly series gdpc1 (real GDP), m1realx (real M1 money stock), and cpiaucsl (CPI) from FRED-QD. Create nominal M1 (multiply m1realx times cpiaucsl), and transform real GDP and nominal M1 to growth rates. The hypothesis of monetary neutrality is that the nominal money supply has no effect on real outcomes such as GDP. Strict monetary neutrality states that there is no short or long-term effect. Long-run neutrality states that there is no long-term effect.\n\nTo test strict neutrality use a Granger-causality test. Regress GDP growth on four lags of GDP growth and four lags of money growth. Test the hypothesis that the four money lags jointly have zero coeffficients. Use robust standard errors. Interpret the results.\nTo test long-run neutrality test if the sum of the four coefficients on money growth equals zero. Interpret the results.\nEstimate a structural VAR in real GDP growth and nominal money growth imposing the long-run neutrality of money. Explain your method.\nReport estimates of the impulse responses of the levels of GDP and nominal money to the two shocks. Interpret the results.\n\nExercise 15.20 Shapiro and Watson (1988) estimated a structural VAR imposing long-run constraints. Replicate a simplified version of their model. Take the quarterly series hoanbs (hours worked, nonfarm business sector), gdpcl (real GDP), and gdpctpi (GDP deflator) from FRED-QD. Transform the first two to growth rates and for the third (GDP deflator) take the second difference of the logarithm (differenced inflation). Shapiro and Watson estimated a structural model imposing the constraints that labor supply hours are long-run unaffected by output and inflation and GDP is long-run unaffected by demand shocks. This implies a recursive ordering in the variables for a long-run restriction.\n\nWrite down the matrix \\(\\boldsymbol{C}\\) as in (15.24) imposing the identifying constraints as defined above.\nIs the model identified?\nUse the AIC to select the number of lags for a VAR.\nEstimate the structural VAR. Report the estimated \\(\\boldsymbol{C}\\) matrix. Can you interpret the coefficients?\nEstimate the structural impulse responses of the level of GDP with respect to the three shocks. Interpret the results."
  },
  {
    "objectID": "chpt17-panel-data.html#introduction",
    "href": "chpt17-panel-data.html#introduction",
    "title": "16  Panel Data",
    "section": "16.1 Introduction",
    "text": "16.1 Introduction\nEconomists traditionally use the term panel data to refer to data structures consisting of observations on individuals for multiple time periods. Other fields such as statistics typically call this structure longitudinal data. The observed “individuals” can be, for example, people, households, workers, firms, schools, production plants, industries, regions, states, or countries. The distinguishing feature relative to cross-sectional data sets is the presence of multiple observations for each individual. More broadly, panel data methods can be applied to any context with cluster-type dependence.\nThere are several distinct advantages of panel data relative to cross-section data. One is the possibility of controlling for unobserved time-invariant endogeneity without the use of instrumental variables. A second is the possibility of allowing for broader forms of heterogeneity. A third is modeling dynamic relationships and effects.\nThere are two broad categories of panel data sets in economic applications: micro panels and macro panels. Micro panels are typically surveys or administrative records on individuals and are characterized by a large number of individuals (often in the 1000’s or higher) and a relatively small number of time periods (often 2 to 20 years). Macro panels are typically national or regional macroeconomic variables and are characterized by a moderate number of individuals (e.g. 7-20) and a moderate number of time periods (20-60 years).\nPanel data was once relatively esoteric in applied economic practice. Now, it is a dominant feature of applied research.\nA typical maintained assumption for micro panels (which we follow in this chapter) is that the individuals are mutually independent while the observations for a given individual are correlated across time periods. This means that the observations follow a clustered dependence structure. Because of this, current econometric practice is to use cluster-robust covariance matrix estimators when possible. Similar assumptions are often used for macro panels though the assumption of independence across individuals (e.g. countries) is much less compelling.\nThe application of panel data methods in econometrics started with the pioneering work of Mundlak (1961) and Balestra and Nerlove (1966).\nSeveral excellent monographs and textbooks have been written on panel econometrics, including Arellano (2003), Hsiao (2003), Wooldridge (2010), and Baltagi (2013). This chapter will summarize some of the main themes but for a more in-depth treatment see these references.\nOne challenge arising in panel data applications is that the computational methods can require meticulous attention to detail. It is therefore advised to use established packages for routine applications. For most panel data applications in economics Stata is the standard package."
  },
  {
    "objectID": "chpt17-panel-data.html#time-indexing-and-unbalanced-panels",
    "href": "chpt17-panel-data.html#time-indexing-and-unbalanced-panels",
    "title": "16  Panel Data",
    "section": "16.2 Time Indexing and Unbalanced Panels",
    "text": "16.2 Time Indexing and Unbalanced Panels\nIt is typical to index observations by both the individual \\(i\\) and the time period \\(t\\), thus \\(Y_{i t}\\) denotes a variable for individual \\(i\\) in period \\(t\\). We index individuals as \\(i=1, \\ldots, N\\) and time periods as \\(t=1, \\ldots T\\). Thus \\(N\\) is the number of individuals in the panel and \\(T\\) is the number of time series periods.\nPanel data sets can involve data at any time series frequency though the typical application involves annual data. The observations in a data set will be indexed by calendar time which for the case of annual observations is the year. For notational convenience it is customary to denote the time periods as \\(t=\\) \\(1, \\ldots, T\\), so that \\(t=1\\) is the first time period observed and \\(T\\) is the final time period.\nWhen observations are available on all individuals for the same time periods we say that the panel is balanced. In this case there are an equal number \\(T\\) of observations for each individual and the total number of observations is \\(n=N T\\).\nWhen different time periods are available for the individuals in the sample we say that the panel is unbalanced. This is the most common type of panel data set. It does not pose a problem for applications but does make the notation cumbersome and also complicates computer programming.\nTo illustrate, consider the data set Invest 1993 on the textbook webpage. This is a sample of 1962 U.S. firms extracted from Compustat, assembled by Bronwyn Hall, and used in the empirical work in Hall and Hall (1993). In Table 17.1 we display a set of variables from the data set for the first 13 observations. The first variable is the firm code number. The second variable is the year of the observation. These two variables are essential for any panel data analysis. In Table \\(17.1\\) you can see that the first firm (#32) is observed for the years 1970 through 1977. The second firm (#209) is observed for 1987 through 1991. You can see that the years vary considerably across the firms so this is an unbalanced panel.\nFor unbalanced panels the time index \\(t=1, \\ldots, T\\) denotes the full set of time periods. For example, in the data set Invest 1993 there are observations for the years 1960 through 1991, so the total number of time periods is \\(T=32\\). Each individual is observed for a subset of \\(T_{i}\\) periods. The set of time periods for individual \\(i\\) is denoted as \\(S_{i}\\) so that individual-specific sums (over time periods) are written as \\(\\sum_{t \\in S_{i}}\\).\nThe observed time periods for a given individual are typically contiguous (for example, in Table 17.1, firm #32 is observed for each year from 1970 through 1977) but in some cases are non-continguous (if, for example, 1973 was missing for firm #32). The total number of observations in the sample is \\(n=\\sum_{i=1}^{N} T_{i}\\).\nTable 17.1: Observations from Investment Data Set\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFirm Code Number\nYear\n\\(I_{i t}\\)\n\\(\\bar{I}_{i}\\)\n\\(\\dot{I}_{i t}\\)\n\\(Q_{i t}\\)\n\\(\\bar{Q}_{i}\\)\n\\(\\dot{Q}_{i t}\\)\n\\(\\widehat{e}_{i t}\\)\n\n\n\n\n32\n1970\n\\(0.122\\)\n\\(0.155\\)\n\\(-0.033\\)\n\\(1.17\\)\n\\(0.62\\)\n\\(0.55\\)\n.\n\n\n32\n1971\n\\(0.092\\)\n\\(0.155\\)\n\\(-0.063\\)\n\\(0.79\\)\n\\(0.62\\)\n\\(0.17\\)\n\\(-0.005\\)\n\n\n32\n1972\n\\(0.094\\)\n\\(0.155\\)\n\\(-0.061\\)\n\\(0.91\\)\n\\(0.62\\)\n\\(0.29\\)\n\\(-0.005\\)\n\n\n32\n1973\n\\(0.116\\)\n\\(0.155\\)\n\\(-0.039\\)\n\\(0.29\\)\n\\(0.62\\)\n\\(-0.33\\)\n\\(0.014\\)\n\n\n32\n1974\n\\(0.099\\)\n\\(0.155\\)\n\\(-0.057\\)\n\\(0.30\\)\n\\(0.62\\)\n\\(-0.32\\)\n\\(-0.002\\)\n\n\n32\n1975\n\\(0.187\\)\n\\(0.155\\)\n\\(0.032\\)\n\\(0.56\\)\n\\(0.62\\)\n\\(-0.06\\)\n\\(0.086\\)\n\n\n32\n1976\n\\(0.349\\)\n\\(0.155\\)\n\\(0.194\\)\n\\(0.38\\)\n\\(0.62\\)\n\\(-0.24\\)\n\\(0.248\\)\n\n\n32\n1977\n\\(0.182\\)\n\\(0.155\\)\n\\(0.027\\)\n\\(0.57\\)\n\\(0.62\\)\n\\(-0.05\\)\n\\(0.081\\)\n\n\n209\n1987\n\\(0.095\\)\n\\(0.071\\)\n\\(0.024\\)\n\\(9.06\\)\n\\(21.57\\)\n\\(-12.51\\)\n.\n\n\n209\n1988\n\\(0.044\\)\n\\(0.071\\)\n\\(-0.027\\)\n\\(16.90\\)\n\\(21.57\\)\n\\(-4.67\\)\n\\(-0.244\\)\n\n\n209\n1989\n\\(0.069\\)\n\\(0.071\\)\n\\(-0.002\\)\n\\(25.14\\)\n\\(21.57\\)\n\\(3.57\\)\n\\(-0.257\\)\n\n\n209\n1990\n\\(0.113\\)\n\\(0.071\\)\n\\(0.042\\)\n\\(25.60\\)\n\\(21.57\\)\n\\(4.03\\)\n\\(-0.226\\)\n\n\n209\n1991\n\\(0.034\\)\n\\(0.071\\)\n\\(-0.037\\)\n\\(31.14\\)\n\\(21.57\\)\n\\(9.57\\)\n\\(-0.283\\)"
  },
  {
    "objectID": "chpt17-panel-data.html#notation",
    "href": "chpt17-panel-data.html#notation",
    "title": "16  Panel Data",
    "section": "16.3 Notation",
    "text": "16.3 Notation\nThis chapter focuses on panel data regression models whose observations are pairs \\(\\left(Y_{i t}, X_{i t}\\right)\\) where \\(Y_{i t}\\) is the dependent variable and \\(X_{i t}\\) is a \\(k\\)-vector of regressors. These are the observations on individual \\(i\\) for time period \\(t\\).\nIt will be useful to cluster the observations at the level of the individual. We borrow the notation from Section \\(4.21\\) to write \\(\\boldsymbol{Y}_{i}\\) as the \\(T_{i} \\times 1\\) stacked observations on \\(Y_{i t}\\) for \\(t \\in S_{i}\\), stacked in chronological order. Similarly, we write \\(\\boldsymbol{X}_{i}\\) as the \\(T_{i} \\times k\\) matrix of stacked \\(X_{i t}^{\\prime}\\) for \\(t \\in S_{i}\\), stacked in chronological order.\nWe will also sometimes use matrix notation for the full sample. To do so, let \\(\\boldsymbol{Y}=\\left(\\boldsymbol{Y}_{1}^{\\prime}, \\ldots, \\boldsymbol{Y}_{N}^{\\prime}\\right)^{\\prime}\\) denote the \\(n \\times 1\\) vector of stacked \\(\\boldsymbol{Y}_{i}\\), and set \\(\\boldsymbol{X}=\\left(\\boldsymbol{X}_{1}^{\\prime}, \\ldots, \\boldsymbol{X}_{N}^{\\prime}\\right)^{\\prime}\\) similarly."
  },
  {
    "objectID": "chpt17-panel-data.html#pooled-regression",
    "href": "chpt17-panel-data.html#pooled-regression",
    "title": "16  Panel Data",
    "section": "16.4 Pooled Regression",
    "text": "16.4 Pooled Regression\nThe simplest model in panel regresion is pooled regresssion\n\\[\n\\begin{aligned}\nY_{i t} &=X_{i t}^{\\prime} \\beta+e_{i t} \\\\\n\\mathbb{E}\\left[X_{i t} e_{i t}\\right] &=0 .\n\\end{aligned}\n\\]\nwhere \\(\\beta\\) is a \\(k \\times 1\\) coefficient vector and \\(e_{i t}\\) is an error. The model can be written at the level of the individual as\n\\[\n\\begin{aligned}\n\\boldsymbol{Y}_{i} &=\\boldsymbol{X}_{i} \\beta+\\boldsymbol{e}_{i} \\\\\n\\mathbb{E}\\left[\\boldsymbol{X}_{i}^{\\prime} \\boldsymbol{e}_{i}\\right] &=0\n\\end{aligned}\n\\]\nwhere \\(\\boldsymbol{e}_{i}\\) is \\(T_{i} \\times 1\\). The equation for the full sample is \\(\\boldsymbol{Y}=\\boldsymbol{X} \\beta+\\boldsymbol{e}\\) where \\(\\boldsymbol{e}\\) is \\(n \\times 1\\).\nThe standard estimator of \\(\\beta\\) in the pooled regression model is least squares, which can be written as\n\\[\n\\begin{aligned}\n\\widehat{\\beta}_{\\text {pool }} &=\\left(\\sum_{i=1}^{N} \\sum_{t \\in S_{i}} X_{i t} X_{i t}^{\\prime}\\right)^{-1}\\left(\\sum_{i=1}^{N} \\sum_{t \\in S_{i}} X_{i t} Y_{i t}\\right) \\\\\n&=\\left(\\sum_{i=1}^{N} \\boldsymbol{X}_{i}^{\\prime} \\boldsymbol{X}_{i}\\right)^{-1}\\left(\\sum_{i=1}^{N} \\boldsymbol{X}_{i}^{\\prime} \\boldsymbol{Y}_{i}\\right) \\\\\n&=\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{Y}\\right) .\n\\end{aligned}\n\\]\nIn the context of panel data \\(\\widehat{\\beta}_{\\text {pool }}\\) is called the pooled regression estimator. The vector of residuals for the \\(i^{t h}\\) individual is \\(\\widehat{\\boldsymbol{e}}_{i}=\\boldsymbol{Y}_{i}-\\boldsymbol{X}_{i} \\widehat{\\beta}_{\\text {pool }}\\).\nThe pooled regression model is ideally suited for the context where the errors \\(e_{i t}\\) satisfy strict mean independence:\n\\[\n\\mathbb{E}\\left[e_{i t} \\mid \\boldsymbol{X}_{i}\\right]=0 .\n\\]\nThis occurs when the errors \\(e_{i t}\\) are mean independent of all regressors \\(X_{i j}\\) for all time periods \\(j=1, \\ldots, T\\). Strict mean independence is stronger than pairwise mean independence \\(\\mathbb{E}\\left[e_{i t} \\mid X_{i t}\\right]=0\\) as well as projection (17.1). Strict mean independence requires that neither lagged nor future values of \\(X_{i t}\\) help to forecast \\(e_{i t}\\). It excludes lagged dependent variables (such as \\(Y_{i t-1}\\) ) from \\(X_{i t}\\) (otherwise \\(e_{i t}\\) would be predictable given \\(X_{i t+1}\\) ). It also requires that \\(X_{i t}\\) is exogenous in the sense discussed in Chapter 12.\nWe now describe some statistical properties of \\(\\widehat{\\beta}_{\\text {pool }}\\) under (17.2). First, notice that by linearity and the cluster-level notation we can write the estimator as\n\\[\n\\widehat{\\beta}_{\\mathrm{pool}}=\\left(\\sum_{i=1}^{N} \\boldsymbol{X}_{i}^{\\prime} \\boldsymbol{X}_{i}\\right)^{-1}\\left(\\sum_{i=1}^{N} \\boldsymbol{X}_{i}^{\\prime}\\left(\\boldsymbol{X}_{i} \\beta+\\boldsymbol{e}_{i}\\right)\\right)=\\beta+\\left(\\sum_{i=1}^{N} \\boldsymbol{X}_{i}^{\\prime} \\boldsymbol{X}_{i}\\right)^{-1}\\left(\\sum_{i=1}^{N} \\boldsymbol{X}_{i}^{\\prime} \\boldsymbol{e}_{i}\\right) .\n\\]\nUsing (17.2)\n\\[\n\\mathbb{E}\\left[\\widehat{\\beta}_{\\text {pool }} \\mid \\boldsymbol{X}\\right]=\\beta+\\left(\\sum_{i=1}^{N} \\boldsymbol{X}_{i}^{\\prime} \\boldsymbol{X}_{i}\\right)^{-1}\\left(\\sum_{i=1}^{N} \\boldsymbol{X}_{i}^{\\prime} \\mathbb{E}\\left[\\boldsymbol{e}_{i} \\mid \\boldsymbol{X}_{i}\\right]\\right)=\\beta\n\\]\nso \\(\\widehat{\\beta}_{\\text {pool }}\\) is unbiased for \\(\\beta\\).\nUnder the additional assumption that the error \\(e_{i t}\\) is serially uncorrelated and homoskedastic the covariance estimator takes a classical form and the classical homoskedastic variance estimator can be used. If the error \\(e_{i t}\\) is heteroskedastic but serially uncorrelated then a heteroskedasticity-robust covariance matrix estimator can be used.\nIn general, however, we expect the errors \\(e_{i t}\\) to be correlated across time \\(t\\) for a given individual. This does not necessarily violate (17.2) but invalidates classical covariance matrix estimation. The conventional solution is to use a cluster-robust covariance matrix estimator which allows arbitrary withincluster dependence. Cluster-robust covariance matrix estimators for pooled regression equal\n\\[\n\\widehat{\\boldsymbol{V}}_{\\text {pool }}=\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1}\\left(\\sum_{i=1}^{N} \\boldsymbol{X}_{i}^{\\prime} \\widehat{\\boldsymbol{e}}_{i} \\widehat{\\boldsymbol{e}}_{i}^{\\prime} \\boldsymbol{X}_{i}\\right)\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} .\n\\]\nAs in (4.55) this can be multiplied by a degree-of-freedom adjustment. The adjustment used by the Stata regress command is\n\\[\n\\widehat{\\boldsymbol{V}}_{\\text {pool }}=\\left(\\frac{n-1}{n-k}\\right)\\left(\\frac{N}{N-1}\\right)\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1}\\left(\\sum_{i=1}^{N} \\boldsymbol{X}_{i}^{\\prime} \\widehat{\\boldsymbol{e}}_{i} \\widehat{\\boldsymbol{e}}_{i}^{\\prime} \\boldsymbol{X}_{i}\\right)\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1}\n\\]\nThe pooled regression estimator with cluster-robust standard errors can be obtained using the Stata command regress cluster(id) where id indicates the individual.\nWhen strict mean independence (17.2) fails the pooled least squares estimator \\(\\widehat{\\beta}_{\\text {pool }}\\) is not necessarily consistent for \\(\\beta\\). Since strict mean independence is a strong and undesirable restriction it is typically preferred to adopt one of the alternative estimators described in the following sections.\nTo illustrate the pooled regression estimator consider the data set Invest1993 described earlier. We consider a simple investment model\n\\[\nI_{i t}=\\beta_{1} Q_{i t-1}+\\beta_{2} D_{i t-1}+\\beta_{3} C F_{i t-1}+\\beta_{4} T_{i}+e_{i t}\n\\]\nwhere \\(I\\) is investment/assets, \\(Q\\) is market value/assets, \\(D\\) is long term debt/assets, \\(C F\\) is cash flow/assets, and \\(T\\) is a dummy variable indicating if the corporation’s stock is traded on the NYSE or AMEX. The regression also includes 19 dummy variables indicating an industry code. The \\(Q\\) theory of investment suggests that \\(\\beta_{1}>0\\) while \\(\\beta_{2}=\\beta_{3}=0\\). Theories of liquidity constraints suggest that \\(\\beta_{2}<0\\) and \\(\\beta_{3}>0\\). We will be using this example throughout this chapter. The values of \\(I\\) and \\(Q\\) for the first 13 observations are also displayed in Table 17.1.\nIn Table \\(17.2\\) we present the pooled regression estimates of (17.3) in the first column with clusterrobust standard errors."
  },
  {
    "objectID": "chpt17-panel-data.html#one-way-error-component-model",
    "href": "chpt17-panel-data.html#one-way-error-component-model",
    "title": "16  Panel Data",
    "section": "16.5 One-Way Error Component Model",
    "text": "16.5 One-Way Error Component Model\nOne approach to panel data regression is to model the correlation structure of the regression error \\(e_{i t}\\). The most common choice is an error-components structure. The simplest takes the form\n\\[\ne_{i t}=u_{i}+\\varepsilon_{i t}\n\\]\nTable 17.2: Estimates of Investment Equation\n\nCluster-robust standard errors in parenthesis.\nwhere \\(u_{i}\\) is an individual-specific effect and \\(\\varepsilon_{i t}\\) are idiosyncratic (i.i.d.) errors. This is known as a oneway error component model.\nIn vector notation we can write \\(\\boldsymbol{e}_{i}=\\mathbf{1}_{i} u_{i}+\\boldsymbol{\\varepsilon}_{i}\\) where \\(\\mathbf{1}_{i}\\) is a \\(T_{i} \\times 1\\) vector of 1’s.\nThe one-way error component regression model is\n\\[\nY_{i t}=X_{i t}^{\\prime} \\beta+u_{i}+\\varepsilon_{i t}\n\\]\nwritten at the level of the observation, or \\(\\boldsymbol{Y}_{i}=\\boldsymbol{X}_{i} \\beta+\\mathbf{1}_{i} u_{i}+\\boldsymbol{\\varepsilon}_{i}\\) written at the level of the individual.\nTo illustrate why an error-component structure such as (17.4) might be appropriate, examine Table 17.1. In the final column we have included the pooled regression residuals \\(\\widehat{e}_{i t}\\) for these observations. (There is no residual for the first year for each firm due to the lack of lagged regressors for this observation.) What is quite striking is that the residuals for the second firm (#209) are all negative, clustering around \\(-0.25\\). While informal, this suggests that it may be appropriate to model these errors using (17.4), expecting that firm #209 has a large negative value for its individual effect \\(u\\)."
  },
  {
    "objectID": "chpt17-panel-data.html#random-effects",
    "href": "chpt17-panel-data.html#random-effects",
    "title": "16  Panel Data",
    "section": "16.6 Random Effects",
    "text": "16.6 Random Effects\nThe random effects model assumes that the errors \\(u_{i}\\) and \\(\\varepsilon_{i t}\\) in (17.4) are conditionally mean zero, uncorrelated, and homoskedastic.\nAssumption 17.1 Random Effects. Model (17.4) holds with\n\\[\n\\begin{aligned}\n\\mathbb{E}\\left[\\varepsilon_{i t} \\mid \\boldsymbol{X}_{i}\\right] &=0 \\\\\n\\mathbb{E}\\left[\\varepsilon_{i t}^{2} \\mid \\boldsymbol{X}_{i}\\right] &=\\sigma_{\\varepsilon}^{2} \\\\\n\\mathbb{E}\\left[\\varepsilon_{i t} \\varepsilon_{j s} \\mid \\boldsymbol{X}_{i}\\right] &=0 \\\\\n\\mathbb{E}\\left[u_{i} \\mid \\boldsymbol{X}_{i}\\right] &=0 \\\\\n\\mathbb{E}\\left[u_{i}^{2} \\mid \\boldsymbol{X}_{i}\\right] &=\\sigma_{u}^{2} \\\\\n\\mathbb{E}\\left[u_{i} \\varepsilon_{i t} \\mid \\boldsymbol{X}_{i}\\right] &=0\n\\end{aligned}\n\\]\nwhere (17.7) holds for all \\(s \\neq t\\). Assumption \\(17.1\\) is known as a random effects specification. It implies that the vector of errors \\(\\boldsymbol{e}_{i}\\) for individual \\(i\\) has the covariance structure\n\\[\n\\begin{aligned}\n\\mathbb{E}\\left[\\boldsymbol{e}_{i} \\mid \\boldsymbol{X}_{i}\\right] &=0 \\\\\n\\mathbb{E}\\left[\\boldsymbol{e}_{i} \\boldsymbol{e}_{i}^{\\prime} \\mid \\boldsymbol{X}_{i}\\right] &=\\mathbf{1}_{i} \\mathbf{1}_{i}^{\\prime} \\sigma_{u}^{2}+\\boldsymbol{I}_{i} \\sigma_{\\varepsilon}^{2} \\\\\n&=\\left(\\begin{array}{cccc}\n\\sigma_{u}^{2}+\\sigma_{\\varepsilon}^{2} & \\sigma_{u}^{2} & \\cdots & \\sigma_{u}^{2} \\\\\n\\sigma_{u}^{2} & \\sigma_{u}^{2}+\\sigma_{\\varepsilon}^{2} & \\cdots & \\sigma_{u}^{2} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n\\sigma_{u}^{2} & \\sigma_{u}^{2} & \\cdots & \\sigma_{u}^{2}+\\sigma_{\\varepsilon}^{2}\n\\end{array}\\right) \\\\\n&=\\sigma_{\\varepsilon}^{2} \\Omega_{i},\n\\end{aligned}\n\\]\nsay, where \\(\\boldsymbol{I}_{i}\\) is an identity matrix of dimension \\(T_{i}\\). The matrix \\(\\Omega_{i}\\) depends on \\(i\\) since its dimension depends on the number of observed time periods \\(T_{i}\\).\nAssumptions 17.1.1 and 17.1.4 state that the idiosyncratic error \\(\\varepsilon_{i t}\\) and individual-specific error \\(u_{i}\\) are strictly mean independent so the combined error \\(e_{i t}\\) is strictly mean independent as well.\nThe random effects model is equivalent to an equi-correlation model. That is, suppose that the error \\(e_{i t}\\) satisfies\n\\[\n\\begin{aligned}\n\\mathbb{E}\\left[e_{i t} \\mid \\boldsymbol{X}_{i}\\right] &=0 \\\\\n\\mathbb{E}\\left[e_{i t}^{2} \\mid \\boldsymbol{X}_{i}\\right] &=\\sigma^{2}\n\\end{aligned}\n\\]\nand\n\\[\n\\mathbb{E}\\left[e_{i s} e_{i t} \\mid \\boldsymbol{X}_{i}\\right]=\\rho \\sigma^{2}\n\\]\nfor \\(s \\neq t\\). These conditions imply that \\(e_{i t}\\) can be written as (17.4) with the components satisfying Assumption \\(17.1\\) with \\(\\sigma_{u}^{2}=\\rho \\sigma^{2}\\) and \\(\\sigma_{\\varepsilon}^{2}=(1-\\rho) \\sigma^{2}\\). Thus random effects and equi-correlation are identical.\nThe random effects regression model is\n\\[\nY_{i t}=X_{i t}^{\\prime} \\beta+u_{i}+\\varepsilon_{i t}\n\\]\nor \\(\\boldsymbol{Y}_{i}=\\boldsymbol{X}_{i} \\beta+\\mathbf{1}_{i} u_{i}+\\boldsymbol{\\varepsilon}_{i}\\) where the errors satisfy Assumption 17.1.\nGiven the error structure the natural estimator for \\(\\beta\\) is GLS. Suppose \\(\\sigma_{u}^{2}\\) and \\(\\sigma_{\\varepsilon}^{2}\\) are known. The GLS estimator of \\(\\beta\\) is\n\\[\n\\widehat{\\beta}_{\\mathrm{gls}}=\\left(\\sum_{i=1}^{N} \\boldsymbol{X}_{i}^{\\prime} \\Omega_{i}^{-1} \\boldsymbol{X}_{i}\\right)^{-1}\\left(\\sum_{i=1}^{N} \\boldsymbol{X}_{i}^{\\prime} \\Omega_{i}^{-1} \\boldsymbol{Y}_{i}\\right) .\n\\]\nA feasible GLS estimator replaces the unknown \\(\\sigma_{u}^{2}\\) and \\(\\sigma_{\\varepsilon}^{2}\\) with estimators. See Section \\(17.15\\).\nWe now describe some statistical properties of the estimator under Assumption 17.1. By linearity\n\\[\n\\widehat{\\beta}_{\\mathrm{gls}}-\\beta=\\left(\\sum_{i=1}^{N} \\boldsymbol{X}_{i}^{\\prime} \\Omega_{i}^{-1} \\boldsymbol{X}_{i}\\right)^{-1}\\left(\\sum_{i=1}^{N} \\boldsymbol{X}_{i}^{\\prime} \\Omega_{i}^{-1} \\boldsymbol{e}_{i}\\right) .\n\\]\nThus\n\\[\n\\mathbb{E}\\left[\\widehat{\\beta}_{\\mathrm{gls}}-\\beta \\mid \\boldsymbol{X}\\right]=\\left(\\sum_{i=1}^{N} \\boldsymbol{X}_{i}^{\\prime} \\Omega_{i}^{-1} \\boldsymbol{X}_{i}\\right)^{-1}\\left(\\sum_{i=1}^{N} \\boldsymbol{X}_{i}^{\\prime} \\Omega_{i}^{-1} \\mathbb{E}\\left[\\boldsymbol{e}_{i} \\mid \\boldsymbol{X}_{i}\\right]\\right)=0 .\n\\]\nThus \\(\\widehat{\\beta}_{\\text {gls }}\\) is conditionally unbiased for \\(\\beta\\). The conditional variance of \\(\\widehat{\\beta}_{\\text {gls }}\\) is\n\\[\n\\boldsymbol{V}_{\\mathrm{gls}}=\\left(\\sum_{i=1}^{n} \\boldsymbol{X}_{i}^{\\prime} \\Omega_{i}^{-1} \\boldsymbol{X}_{i}\\right)^{-1} \\sigma_{\\varepsilon}^{2}\n\\]\nNow let’s compare \\(\\widehat{\\beta}_{\\text {gls }}\\) with the pooled estimator \\(\\widehat{\\beta}_{\\text {pool. }}\\). Under Assumption \\(17.1\\) the latter is also conditionally unbiased for \\(\\beta\\) and has conditional variance\n\\[\n\\boldsymbol{V}_{\\text {pool }}=\\left(\\sum_{i=1}^{n} \\boldsymbol{X}_{i}^{\\prime} \\boldsymbol{X}_{i}\\right)^{-1}\\left(\\sum_{i=1}^{n} \\boldsymbol{X}_{i}^{\\prime} \\Omega_{i} \\boldsymbol{X}_{i}\\right)^{-1}\\left(\\sum_{i=1}^{n} \\boldsymbol{X}_{i}^{\\prime} \\boldsymbol{X}_{i}\\right)^{-1} .\n\\]\nUsing the algebra of the Gauss-Markov Theorem we deduce that\n\\[\n\\boldsymbol{V}_{\\text {gls }} \\leq \\boldsymbol{V}_{\\text {pool }}\n\\]\nand thus the random effects estimator \\(\\widehat{\\beta}_{\\text {gls }}\\) is more efficient than the pooled estimator \\(\\widehat{\\beta}_{\\text {pool }}\\) under Assumption 17.1. (See Exercise 17.1.) The two variance matrices are identical when there is no individualspecific effect (when \\(\\sigma_{u}^{2}=0\\) ) for then \\(\\boldsymbol{V}_{\\text {gls }}=\\boldsymbol{V}_{\\text {pool }}=\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\sigma_{\\varepsilon}^{2}\\).\nUnder the assumption that the random effects model is a useful approximation but not literally true then we may consider a cluster-robust covariance matrix estimator such as\n\\[\n\\widehat{\\boldsymbol{V}}_{\\mathrm{gls}}=\\left(\\sum_{i=1}^{N} \\boldsymbol{X}_{i}^{\\prime} \\Omega_{i}^{-1} \\boldsymbol{X}_{i}\\right)^{-1}\\left(\\sum_{i=1}^{N} \\boldsymbol{X}_{i}^{\\prime} \\Omega_{i}^{-1} \\widehat{\\boldsymbol{e}}_{i} \\widehat{\\boldsymbol{e}}_{i}^{\\prime} \\Omega_{i}^{-1} \\boldsymbol{X}_{i}\\right)\\left(\\sum_{i=1}^{n} \\boldsymbol{X}_{i}^{\\prime} \\Omega_{i}^{-1} \\boldsymbol{X}_{i}\\right)^{-1}\n\\]\nwhere \\(\\widehat{\\boldsymbol{e}}_{i}=\\boldsymbol{Y}_{i}-\\boldsymbol{X}_{i} \\widehat{\\beta}_{\\mathrm{gls}}\\). This may be re-scaled by a degree of freedom adjustment if desired.\nThe random effects estimator \\(\\widehat{\\beta}_{\\text {gls }}\\) can be obtained using the Stata command xtreg. The default covariance matrix estimator is (17.11). For the cluster-robust covariance matrix estimator (17.14) use the command xtreg vce(robust). (The xtset command must be used first to declare the group identifier. For example, cusip is the group identifier in Table 17.1.)\nTo illustrate, in the second column of Table \\(17.2\\) we present the random effect regression estimates of the investment model (17.3) with cluster-robust standard errors (17.14). The point estimates are reasonably different from the pooled regression estimator. The coefficient on debt switches from positive to negative (the latter consistent with theories of liquidity constraints) and the coefficient on cash flow increases significantly in magnitude. These changes appear to be greater in magnitude than would be expected if Assumption \\(17.1\\) were correct. In the next section we consider a less restrictive specification."
  },
  {
    "objectID": "chpt17-panel-data.html#fixed-effect-model",
    "href": "chpt17-panel-data.html#fixed-effect-model",
    "title": "16  Panel Data",
    "section": "16.7 Fixed Effect Model",
    "text": "16.7 Fixed Effect Model\nConsider the one-way error component regression model\n\\[\nY_{i t}=X_{i t}^{\\prime} \\beta+u_{i}+\\varepsilon_{i t}\n\\]\nor\n\\[\n\\boldsymbol{Y}_{i}=\\boldsymbol{X}_{i} \\beta+\\mathbf{l}_{i} u_{i}+\\boldsymbol{\\varepsilon}_{i} .\n\\]\nIn many applications it is useful to interpret the individual-specific effect \\(u_{i}\\) as a time-invariant unobserved missing variable. For example, in a wage regression \\(u_{i}\\) may be the unobserved ability of individual \\(i\\). In the investment model (17.3) \\(u_{i}\\) may be a firm-specific productivity factor.\nWhen \\(u_{i}\\) is interpreted as an omitted variable it is natural to expect it to be correlated with the regressors \\(X_{i t}\\). This is especially the case when \\(X_{i t}\\) includes choice variables.\nTo illustrate, consider the entries in Table 17.1. The final column displays the pooled regression residuals \\(\\widehat{e}_{i t}\\) for the first 13 observations which we interpret as estimates of the error \\(e_{i t}=u_{i}+\\varepsilon_{i t}\\). As described before, what is particularly striking about the residuals is that they are all strongly negative for firm #209, clustering around \\(-0.25\\). We can interpret this as an estimate of \\(u_{i}\\) for this firm. Examining the values of the regressor \\(Q\\) for the two firms we can see that firm #209 has very large values (in all time periods) for \\(Q\\). (The average value \\(\\bar{Q}_{i}\\) for the two firms appears in the seventh column.) Thus it appears (though we are only looking at two observations) that \\(u_{i}\\) and \\(Q_{i t}\\) are correlated. It is not reasonable to infer too much from these limited observations, but the relevance is that such correlation violates strict mean independence.\nIn the econometrics literature if the stochastic structure of \\(u_{i}\\) is treated as unknown and possibly correlated with \\(X_{i t}\\) then \\(u_{i}\\) is called a fixed effect.\nCorrelation between \\(u_{i}\\) and \\(X_{i t}\\) will cause both pooled and random effect estimators to be biased. This is due to the classic problems of omitted variables bias and endogeneity. To see this in a generated example view Figure 17.1. This shows a scatter plot of three observations \\(\\left(Y_{i t}, X_{i t}\\right)\\) from three firms. The true model is \\(Y_{i t}=9-X_{i t}+u_{i}\\). (The true slope coefficient is \\(-1\\).) The variables \\(u_{i}\\) and \\(X_{i t}\\) are highly correlated so the fitted pooled regression line through the nine observations has a slope close to +1. (The random effects estimator is identical.) The apparent positive relationship between \\(Y\\) and \\(X\\) is driven entirely by the positive correlation between \\(X\\) and \\(u\\). Conditional on \\(u\\), however, the slope is \\(-1\\). Thus regression techniques which do not control for \\(u_{i}\\) will produce biased and inconsistent estimators.\n\nFigure 17.1: Scatter Plot and Pooled Regression Line\nThe presence of the unstructured individual effect \\(u_{i}\\) means that it is not possible to identify \\(\\beta\\) under a simple projection assumption such as \\(\\mathbb{E}\\left[X_{i t} \\varepsilon_{t}\\right]=0\\). It turns out that a sufficient condition for identification is the following. Definition 17.1 The regressor \\(X_{i t}\\) is strictly exogenous for the error \\(\\varepsilon_{i t}\\) if\n\\[\n\\mathbb{E}\\left[X_{i s} \\varepsilon_{i t}\\right]=0\n\\]\nfor all \\(s=1, \\ldots, T\\).\nStrict exogeneity is a strong projection condition, meaning that if \\(X_{i s}\\) for any \\(s \\neq t\\) is added to (17.15) it will have a zero coefficient. Strict exogeneity is a projection analog of strict mean independence\n\\[\n\\mathbb{E}\\left[\\varepsilon_{i t} \\mid \\boldsymbol{X}_{i}\\right]=0 .\n\\]\n(17.18) implies (17.17) but not conversely. While (17.17) is sufficient for identification and asymptotic theory we will also use the stronger condition (17.18) for finite sample analysis.\nWhile (17.17) and (17.18) are strong assumptions they are much weaker than (17.2) or Assumption 17.1, which require that the individual effect \\(u_{i}\\) is also strictly mean independent. In contrast, (17.17) and (17.18) make no assumptions about \\(u_{i}\\).\nStrict exogeneity (17.17) is typically inappropriate in dynamic models. In Section \\(17.41\\) we discuss estimation under the weaker assumption of predetermined regressors."
  },
  {
    "objectID": "chpt17-panel-data.html#within-transformation",
    "href": "chpt17-panel-data.html#within-transformation",
    "title": "16  Panel Data",
    "section": "16.8 Within Transformation",
    "text": "16.8 Within Transformation\nIn the previous section we showed that if \\(u_{i}\\) and \\(X_{i t}\\) are correlated then pooled and random-effects estimators will be biased and inconsistent. If we leave the relationship between \\(u_{i}\\) and \\(X_{i t}\\) fully unstructured then the only way to consistently estimate the coefficient \\(\\beta\\) is by an estimator which is invariant to \\(u_{i}\\). This can be achieved by transformations which eliminate \\(u_{i}\\).\nOne such transformation is the within transformation. In this section we describe this transformation in detail.\nDefine the mean of a variable for a given individual as\n\\[\n\\bar{Y}_{i}=\\frac{1}{T_{i}} \\sum_{t \\in S_{i}} Y_{i t} .\n\\]\nWe call this the individual-specific mean since it is the mean of a given individual. Contrarywise, some authors call this the time-average or time-mean since it is the average over the time periods.\nSubtracting the individual-specific mean from the variable we obtain the deviations\n\\[\n\\dot{Y}_{i t}=Y_{i t}-\\bar{Y}_{i} .\n\\]\nThis is known as the within transformation. We also refer to \\(\\dot{Y}_{i t}\\) as the demeaned values or deviations from individual means. Some authors refer to \\(\\dot{Y}_{i t}\\) as deviations from time means. What is important is that the demeaning has occured at the individual level.\nSome algebra may also be useful. We can write the individual-specific mean as \\(\\bar{Y}_{i}=\\left(\\mathbf{1}_{i}^{\\prime} \\mathbf{1}_{i}\\right)^{-1} \\mathbf{1}_{i}^{\\prime} \\boldsymbol{Y}_{i}\\). Stacking the observations for individual \\(i\\) we can write the within transformation using the notation\n\\[\n\\begin{aligned}\n\\dot{\\boldsymbol{Y}}_{i} &=\\boldsymbol{Y}_{i}-\\mathbf{1}_{i} \\bar{Y}_{i} \\\\\n&=\\boldsymbol{Y}_{i}-\\mathbf{1}_{i}\\left(\\mathbf{1}_{i}^{\\prime} \\mathbf{1}_{i}\\right)^{-1} \\mathbf{1}_{i}^{\\prime} \\boldsymbol{Y}_{i} \\\\\n&=\\boldsymbol{M}_{i} \\boldsymbol{Y}_{i}\n\\end{aligned}\n\\]\nwhere \\(\\boldsymbol{M}_{i}=\\boldsymbol{I}_{i}-\\mathbf{1}_{i}\\left(\\mathbf{1}_{i}^{\\prime} \\mathbf{1}_{i}\\right)^{-1} \\mathbf{1}_{i}^{\\prime}\\) is the individual-specific demeaning operator. Notice that \\(\\boldsymbol{M}_{i}\\) is an idempotent matrix.\nSimilarly for the regressors we define the individual-specific means and demeaned values:\n\\[\n\\begin{aligned}\n\\bar{X}_{i} &=\\frac{1}{T_{i}} \\sum_{t \\in S_{i}} X_{i t} \\\\\n\\dot{X}_{i t} &=X_{i t}-\\bar{X}_{i} \\\\\n\\dot{\\boldsymbol{X}}_{i} &=\\boldsymbol{M}_{i} \\boldsymbol{X}_{i} .\n\\end{aligned}\n\\]\nWe illustrate demeaning in Table 17.1. In the fourth and seventh columns we display the firm-specific means \\(\\bar{I}_{i}\\) and \\(\\bar{Q}_{i}\\) and in the fifth and eighth columns the demeaned values \\(\\dot{I}_{i t}\\) and \\(\\dot{Q}_{i t}\\).\nWe can also define the full-sample within operator. Define \\(\\boldsymbol{D}=\\operatorname{diag}\\left\\{\\mathbf{1}_{T_{1}}, \\ldots, \\mathbf{1}_{T_{N}}\\right\\}\\) and \\(\\boldsymbol{M}_{\\boldsymbol{D}}=\\boldsymbol{I}_{n}-\\) \\(\\boldsymbol{D}\\left(\\boldsymbol{D}^{\\prime} \\boldsymbol{D}\\right)^{-1} \\boldsymbol{D}^{\\prime}\\). Note that \\(\\boldsymbol{M}_{\\boldsymbol{D}}=\\operatorname{diag}\\left\\{\\boldsymbol{M}_{1}, \\ldots, \\boldsymbol{M}_{N}\\right\\}\\). Thus\n\\[\n\\boldsymbol{M}_{\\boldsymbol{D}} \\boldsymbol{Y}=\\dot{\\boldsymbol{Y}}=\\left(\\begin{array}{c}\n\\dot{\\boldsymbol{Y}}_{1} \\\\\n\\vdots \\\\\n\\dot{\\boldsymbol{Y}}_{N}\n\\end{array}\\right), \\quad \\boldsymbol{M}_{\\boldsymbol{D}} \\boldsymbol{X}=\\dot{\\boldsymbol{X}}=\\left(\\begin{array}{c}\n\\dot{\\boldsymbol{X}}_{1} \\\\\n\\vdots \\\\\n\\dot{\\boldsymbol{X}}_{N}\n\\end{array}\\right)\n\\]\nNow apply these operations to equation (17.15). Taking individual-specific averages we obtain\n\\[\n\\bar{Y}_{i}=\\bar{X}_{i}^{\\prime} \\beta+u_{i}+\\bar{\\varepsilon}_{i}\n\\]\nwhere \\(\\bar{\\varepsilon}_{i}=\\frac{1}{T_{i}} \\sum_{t \\in S_{i}} \\varepsilon_{i t}\\). Subtracting from (17.15) we obtain\n\\[\n\\dot{Y}_{i t}=\\dot{X}_{i t}^{\\prime} \\beta+\\dot{\\varepsilon}_{i t}\n\\]\nwhere \\(\\dot{\\varepsilon}_{i t}=\\varepsilon_{i t}-\\bar{\\varepsilon}_{i t}\\). The individual effect \\(u_{i}\\) has been eliminated! obtain\nWe can alternatively write this in vector notation. Applying the demeaning operator \\(\\boldsymbol{M}_{i}\\) to (17.16) we\n\\[\n\\dot{\\boldsymbol{Y}}_{i}=\\dot{\\boldsymbol{X}}_{i} \\beta+\\dot{\\boldsymbol{\\varepsilon}}_{i} .\n\\]\nThe individual-effect \\(u_{i}\\) is eliminated because \\(\\boldsymbol{M}_{i} \\mathbf{1}_{i}=0\\). Equation (17.22) is a vector version of (17.21).\nThe equation (17.21) is a linear equation in the transformed (demeaned) variables. As desired the individual effect \\(u_{i}\\) has been eliminated. Consequently estimators constructed from (17.21) (or equivalently (17.22)) will be invariant to the values of \\(u_{i}\\). This means that the the endogeneity bias described in the previous section will be eliminated.\nAnother consequence, however, is that all time-invariant regressors are also eliminated. That is, if the original model (17.15) had included any regressors \\(X_{i t}=X_{i}\\) which are constant over time for each individual then for these regressors the demeaned values are identically 0 . What this means is that if equation (17.21) is used to estimate \\(\\beta\\) it will be impossible to estimate (or identify) a coefficient on any regressor which is time invariant. This is not a consequence of the estimation method but rather a consequence of the model assumptions. In other words, if the individual effect \\(u_{i}\\) has no known structure then it is impossible to disentangle the effect of any time-invariant regressor \\(X_{i}\\). The two have observationally equivalent effects and cannot be separately identified.\nThe within transformation can greatly reduce the variance of the regressors. This can be seen in Table 17.1 where you can see that the variation between the elements of the transformed variables \\(\\dot{I}_{i t}\\) and \\(\\dot{Q}_{i t}\\) is less than that of the untransformed variables, as much of the variation is captured by the firm-specific means.\nIt is not typically needed to directly program the within transformation, but if it is desired the following Stata commands easily do so.\n\n\n\n\n\n\nStata Commands for Within Transformation\n\n\n\n\n\\(* \\quad \\quad \\mathrm{x}\\) is the original variable\n\n\n\\(* \\quad\\) id is the group identifier\n\n\n\\(* \\quad\\) xdot is the within-transformed variable\n\n\negen xmean \\(=\\) mean \\((\\mathrm{x})\\), by(id) gen xdot \\(=\\mathrm{x}-\\mathrm{xmean}\\)"
  },
  {
    "objectID": "chpt17-panel-data.html#fixed-effects-estimator",
    "href": "chpt17-panel-data.html#fixed-effects-estimator",
    "title": "16  Panel Data",
    "section": "16.9 Fixed Effects Estimator",
    "text": "16.9 Fixed Effects Estimator\nConsider least squares applied to the demeaned equation (17.21) or equivalently (17.22). This is\n\\[\n\\begin{aligned}\n\\widehat{\\beta}_{\\mathrm{fe}} &=\\left(\\sum_{i=1}^{N} \\sum_{t \\in S_{i}} \\dot{X}_{i t} \\dot{X}_{i t}^{\\prime}\\right)^{-1}\\left(\\sum_{i=1}^{N} \\sum_{t \\in S_{i}} \\dot{X}_{i t} \\dot{Y}_{i t}\\right) \\\\\n&=\\left(\\sum_{i=1}^{N} \\dot{\\boldsymbol{X}}_{i}^{\\prime} \\dot{\\boldsymbol{X}}_{i}\\right)^{-1}\\left(\\sum_{i=1}^{N} \\dot{\\boldsymbol{X}}_{i}^{\\prime} \\dot{\\boldsymbol{Y}}_{i}\\right) \\\\\n&=\\left(\\sum_{i=1}^{N} \\boldsymbol{X}_{i}^{\\prime} \\boldsymbol{M}_{i} \\boldsymbol{X}_{i}\\right)^{-1}\\left(\\sum_{i=1}^{N} \\boldsymbol{X}_{i}^{\\prime} \\boldsymbol{M}_{i} \\boldsymbol{Y}_{i}\\right)\n\\end{aligned}\n\\]\nThis is known as the fixed-effects or within estimator of \\(\\beta\\). It is called the fixed-effects estimator because it is appropriate for the fixed effects model (17.15). It is called the within estimator because it is based on the variation of the data within each individual.\nThe above definition implicitly assumes that the matrix \\(\\sum_{i=1}^{N} \\dot{\\boldsymbol{X}}_{i}^{\\prime} \\dot{\\boldsymbol{X}}_{i}\\) is full rank. This requires that all components of \\(X_{i t}\\) have time variation for at least some individuals in the sample.\nThe fixed effects residuals are\n\\[\n\\begin{aligned}\n\\widehat{\\varepsilon}_{i t} &=\\dot{Y}_{i t}-\\dot{X}_{i t}^{\\prime} \\widehat{\\beta}_{\\mathrm{fe}} \\\\\n\\widehat{\\boldsymbol{\\varepsilon}}_{i} &=\\dot{\\boldsymbol{Y}}_{i}-\\dot{\\boldsymbol{X}}_{i} \\widehat{\\beta}_{\\mathrm{fe}}\n\\end{aligned}\n\\]\nLet us describe some of the statistical properties of the estimator under strict mean independence (17.18). By linearity and the fact \\(\\boldsymbol{M}_{i} \\mathbf{1}_{i}=0\\), we can write\n\\[\n\\widehat{\\beta}_{\\mathrm{fe}}-\\beta=\\left(\\sum_{i=1}^{N} \\boldsymbol{X}_{i}^{\\prime} \\boldsymbol{M}_{i} \\boldsymbol{X}_{i}\\right)^{-1}\\left(\\sum_{i=1}^{N} \\boldsymbol{X}_{i}^{\\prime} \\boldsymbol{M}_{i} \\boldsymbol{\\varepsilon}_{i}\\right)\n\\]\nThen (17.18) implies\n\\[\n\\mathbb{E}\\left[\\widehat{\\beta}_{\\mathrm{fe}}-\\beta \\mid \\boldsymbol{X}\\right]=\\left(\\sum_{i=1}^{N} \\boldsymbol{X}_{i}^{\\prime} \\boldsymbol{M}_{i} \\boldsymbol{X}_{i}\\right)^{-1}\\left(\\sum_{i=1}^{N} \\boldsymbol{X}_{i}^{\\prime} \\boldsymbol{M}_{i} \\mathbb{E}\\left[\\boldsymbol{\\varepsilon}_{i} \\mid \\boldsymbol{X}_{i}\\right]\\right)=0\n\\]\nThus \\(\\widehat{\\beta}_{\\mathrm{fe}}\\) is unbiased for \\(\\beta\\) under (17.18).\nLet \\(\\Sigma_{i}=\\mathbb{E}\\left[\\boldsymbol{\\varepsilon}_{i} \\boldsymbol{\\varepsilon}_{i}^{\\prime} \\mid \\boldsymbol{X}_{i}\\right]\\) denote the \\(T_{i} \\times T_{i}\\) conditional covariance matrix of the idiosyncratic errors. The variance of \\(\\widehat{\\beta}_{\\mathrm{fe}}\\) is\n\\[\n\\boldsymbol{V}_{\\mathrm{fe}}=\\operatorname{var}\\left[\\widehat{\\beta}_{\\mathrm{fe}} \\mid \\boldsymbol{X}\\right]=\\left(\\sum_{i=1}^{N} \\dot{\\boldsymbol{X}}_{i}^{\\prime} \\dot{\\boldsymbol{X}}_{i}\\right)^{-1}\\left(\\sum_{i=1}^{N} \\dot{\\boldsymbol{X}}_{i}^{\\prime} \\Sigma_{i} \\dot{\\boldsymbol{X}}_{i}\\right)\\left(\\sum_{i=1}^{N} \\dot{\\boldsymbol{X}}_{i}^{\\prime} \\dot{\\boldsymbol{X}}_{i}\\right)^{-1}\n\\]\nThis expression simplifies when the idiosyncratic errors are homoskedastic and serially uncorrelated:\n\\[\n\\begin{aligned}\n\\mathbb{E}\\left[\\varepsilon_{i t}^{2} \\mid \\boldsymbol{X}_{i}\\right] &=\\sigma_{\\varepsilon}^{2} \\\\\n\\mathbb{E}\\left[\\varepsilon_{i j} \\varepsilon_{i t} \\mid \\boldsymbol{X}_{i}\\right] &=0\n\\end{aligned}\n\\]\nfor all \\(j \\neq t\\). In this case, \\(\\Sigma_{i}=\\boldsymbol{I}_{i} \\sigma_{\\varepsilon}^{2}\\) and (17.24) simplifies to\n\\[\n\\boldsymbol{V}_{\\mathrm{fe}}^{0}=\\sigma_{\\varepsilon}^{2}\\left(\\sum_{i=1}^{N} \\dot{\\boldsymbol{X}}_{i}^{\\prime} \\dot{\\boldsymbol{X}}_{i}\\right)^{-1} .\n\\]\nIt is instructive to compare the variances of the fixed-effects and pooled estimators under (17.25)(17.26) and the assumption that there is no individual-specific effect \\(u_{i}=0\\). In this case we see that\n\\[\n\\boldsymbol{V}_{\\mathrm{fe}}^{0}=\\sigma_{\\varepsilon}^{2}\\left(\\sum_{i=1}^{N} \\dot{\\boldsymbol{X}}_{i}^{\\prime} \\dot{\\boldsymbol{X}}_{i}\\right)^{-1} \\geq \\sigma_{\\varepsilon}^{2}\\left(\\sum_{i=1}^{N} \\boldsymbol{X}_{i}^{\\prime} \\boldsymbol{X}_{i}\\right)^{-1}=\\boldsymbol{V}_{\\text {pool }}\n\\]\nThe inequality holds since the demeaned variables \\(\\dot{\\boldsymbol{X}}_{i}\\) have reduced variation relative to the original observations \\(\\boldsymbol{X}_{i}\\). (See Exercise 17.28.) This shows the cost of using fixed effects relative to pooled estimation. The estimation variance increases due to reduced variation in the regressors. This reduction in efficiency is a necessary by-product of the robustness of the estimator to the individual effects \\(u_{i}\\)."
  },
  {
    "objectID": "chpt17-panel-data.html#differenced-estimator",
    "href": "chpt17-panel-data.html#differenced-estimator",
    "title": "16  Panel Data",
    "section": "16.10 Differenced Estimator",
    "text": "16.10 Differenced Estimator\nThe within transformation is not the only transformation which eliminates the individual-specific effect. Another important transformation which does the same is first-differencing.\nThe first-differencing transformation is \\(\\Delta Y_{i t}=Y_{i t}-Y_{i t-1}\\). This can be applied to all but the first observation (which is essentially lost). At the level of the individual this can be written as \\(\\Delta \\boldsymbol{Y}_{i}=\\boldsymbol{D}_{i} \\boldsymbol{Y}_{i}\\) where \\(\\boldsymbol{D}_{i}\\) is the \\(\\left(T_{i}-1\\right) \\times T_{i}\\) matrix differencing operator\n\\[\n\\boldsymbol{D}_{i}=\\left[\\begin{array}{cccccc}\n-1 & 1 & 0 & \\cdots & 0 & 0 \\\\\n0 & -1 & 1 & & 0 & 0 \\\\\n\\vdots & & & \\ddots & & \\vdots \\\\\n0 & 0 & 0 & \\cdots & -1 & 1\n\\end{array}\\right] .\n\\]\nApplying the transformation \\(\\Delta\\) to (17.15) or (17.16) we obtain \\(\\Delta Y_{i t}=\\Delta X_{i t}^{\\prime} \\beta+\\Delta \\varepsilon_{i t}\\) or\n\\[\n\\Delta \\boldsymbol{Y}_{i}=\\Delta \\boldsymbol{X}_{i} \\beta+\\Delta \\boldsymbol{\\varepsilon}_{i} .\n\\]\nWe can see that the individual effect \\(u_{i}\\) has been eliminated.\nLeast squares applied to the differenced equation (17.29) is\n\\[\n\\begin{aligned}\n\\widehat{\\beta}_{\\Delta} &=\\left(\\sum_{i=1}^{N} \\sum_{t \\geq 2} \\Delta X_{i t} \\Delta X_{i t}^{\\prime}\\right)^{-1}\\left(\\sum_{i=1}^{N} \\sum_{t \\geq 2} \\Delta X_{i t} \\Delta Y_{i t}\\right) \\\\\n&=\\left(\\sum_{i=1}^{N} \\Delta \\boldsymbol{X}_{i}^{\\prime} \\Delta \\boldsymbol{X}_{i}\\right)^{-1}\\left(\\sum_{i=1}^{N} \\Delta \\boldsymbol{X}_{i}^{\\prime} \\Delta \\boldsymbol{Y}_{i}\\right) \\\\\n&=\\left(\\sum_{i=1}^{N} \\boldsymbol{X}_{i}^{\\prime} \\boldsymbol{D}_{i}^{\\prime} \\boldsymbol{D}_{i} \\boldsymbol{X}_{i}\\right)^{-1}\\left(\\sum_{i=1}^{N} \\boldsymbol{X}_{i}^{\\prime} \\boldsymbol{D}_{i}^{\\prime} \\boldsymbol{D}_{i} \\boldsymbol{Y}_{i}\\right)\n\\end{aligned}\n\\]\n(17.30) is called the differenced estimator. For \\(T=2, \\widehat{\\beta}_{\\Delta}=\\widehat{\\beta}_{\\mathrm{fe}}\\) equals the fixed effects estimator. See Exercise 17.6. They differ, however, for \\(T>2\\).\nWhen the errors \\(\\varepsilon_{i t}\\) are serially uncorrelated and homoskedastic then the error \\(\\Delta \\boldsymbol{\\varepsilon}_{i}=\\boldsymbol{D}_{i} \\boldsymbol{\\varepsilon}_{i}\\) in (17.29) has covariance matrix \\(\\boldsymbol{H} \\sigma_{\\varepsilon}^{2}\\) where\n\\[\n\\boldsymbol{H}=\\boldsymbol{D}_{i} \\boldsymbol{D}_{i}^{\\prime}=\\left(\\begin{array}{cccc}\n2 & -1 & 0 & 0 \\\\\n-1 & 2 & \\ddots & 0 \\\\\n0 & \\ddots & \\ddots & -1 \\\\\n0 & 0 & -1 & 2\n\\end{array}\\right) .\n\\]\nWe can reduce estimation variance by using GLS. When the errors \\(\\varepsilon_{i t}\\) are i.i.d. (serially uncorrelated and homoskedastic), this is\n\\[\n\\begin{aligned}\n\\widetilde{\\beta}_{\\Delta} &=\\left(\\sum_{i=1}^{N} \\Delta \\boldsymbol{X}_{i}^{\\prime} \\boldsymbol{H}^{-1} \\Delta \\boldsymbol{X}_{i}\\right)^{-1}\\left(\\sum_{i=1}^{N} \\Delta \\boldsymbol{X}_{i}^{\\prime} \\boldsymbol{H}^{-1} \\Delta \\boldsymbol{Y}_{i}\\right) \\\\\n&=\\left(\\sum_{i=1}^{N} \\boldsymbol{X}_{i}^{\\prime} \\boldsymbol{D}_{i}^{\\prime}\\left(\\boldsymbol{D}_{i} \\boldsymbol{D}_{i}^{\\prime}\\right)^{-1} \\boldsymbol{D}_{i} \\boldsymbol{X}_{i}\\right)^{-1}\\left(\\sum_{i=1}^{N} \\boldsymbol{X}_{i}^{\\prime} \\boldsymbol{D}_{i}^{\\prime}\\left(\\boldsymbol{D}_{i} \\boldsymbol{D}_{i}^{\\prime}\\right)^{-1} \\boldsymbol{D}_{i} \\boldsymbol{Y}_{i}\\right) \\\\\n&=\\left(\\sum_{i=1}^{N} \\boldsymbol{X}_{i}^{\\prime} \\boldsymbol{M}_{i} \\boldsymbol{X}_{i}\\right)^{-1}\\left(\\sum_{i=1}^{N} \\boldsymbol{X}_{i}^{\\prime} \\boldsymbol{M}_{i} \\boldsymbol{Y}_{i}\\right)\n\\end{aligned}\n\\]\nwhere \\(\\boldsymbol{M}_{i}=\\boldsymbol{D}_{i}^{\\prime}\\left(\\boldsymbol{D}_{i} \\boldsymbol{D}_{i}^{\\prime}\\right)^{-1} \\boldsymbol{D}_{i}\\). Recall, the matrix \\(\\boldsymbol{D}_{i}\\) is \\(\\left(T_{i}-1\\right) \\times T_{i}\\) with rank \\(T_{i}-1\\) and is orthogonal to the vector of ones \\(\\mathbf{1}_{i}\\). This means \\(\\boldsymbol{M}_{i}\\) projects orthogonally to \\(\\mathbf{1}_{i}\\) and thus equals the within transformation matrix. Hence \\(\\widetilde{\\beta}_{\\Delta}=\\widehat{\\beta}_{\\mathrm{fe}}\\), the fixed effects estimator!\nWhat we have shown is that under i.i.d. errors, GLS applied to the first-differenced equation precisely equals the fixed effects estimator. Since the Gauss-Markov theorem shows that GLS has lower variance than least squares, this means that the fixed effects estimator is more efficient than first differencing under the assumption that \\(\\varepsilon_{i t}\\) is i.i.d.\nThis argument extends to any other transformation which eliminates the fixed effect. GLS applied after such a transformation is equal to the fixed effects estimator and is more efficient than least squares applied after the same transformation under i.i.d. errors. This shows that the fixed effects estimator is Gauss-Markov efficient in the class of estimators which eliminate the fixed effect, under these assumptions."
  },
  {
    "objectID": "chpt17-panel-data.html#dummy-variables-regression",
    "href": "chpt17-panel-data.html#dummy-variables-regression",
    "title": "16  Panel Data",
    "section": "16.11 Dummy Variables Regression",
    "text": "16.11 Dummy Variables Regression\nAn alternative way to estimate the fixed effects model is by least squares of \\(Y_{i t}\\) on \\(X_{i t}\\) and a full set of dummy variables, one for each individual in the sample. It turns out that this is algebraically equivalent to the within estimator.\nTo see this start with the error-component model without a regressor:\n\\[\nY_{i t}=u_{i}+\\varepsilon_{i t} .\n\\]\nConsider least squares estimation of the vector of fixed effects \\(u=\\left(u_{1}, \\ldots, u_{N}\\right)^{\\prime}\\). Since each fixed effect \\(u_{i}\\) is an individual-specific mean and the least squares estimate of the intercept is the sample mean it follows that the least squares estimate of \\(u_{i}\\) is \\(\\widehat{u}_{i}=\\bar{Y}_{i}\\). The least squares residual is then \\(\\widehat{\\varepsilon}_{i t}=Y_{i t}-\\bar{Y}_{i}=\\) \\(\\dot{Y}_{i t}\\), the within transformation. If you would prefer an algebraic argument, let \\(d_{i}\\) be a vector of \\(N\\) dummy variables where the \\(i^{t h}\\) element indicates the \\(i^{t h}\\) individual. Thus the \\(i^{t h}\\) element of \\(d_{i}\\) is 1 and the remaining elements are zero. Notice that \\(u_{i}=d_{i}^{\\prime} u\\) and (17.32) equals \\(Y_{i t}=d_{i}^{\\prime} u+\\varepsilon_{i t}\\). This is a regression with the regressors \\(d_{i}\\) and coefficients \\(u\\). We can also write this in vector notation at the level of the individual as \\(\\boldsymbol{Y}_{i}=\\mathbf{1}_{i} d_{i}^{\\prime} u+\\varepsilon_{i}\\) or using full matrix notation as \\(\\boldsymbol{Y}=\\boldsymbol{D} u+\\boldsymbol{\\varepsilon}\\) where \\(\\boldsymbol{D}=\\operatorname{diag}\\left\\{\\mathbf{1}_{T_{1}}, \\ldots, \\mathbf{1}_{T_{N}}\\right\\}\\).\nThe least squares estimate of \\(u\\) is\n\\[\n\\begin{aligned}\n\\widehat{\\boldsymbol{u}} &=\\left(\\boldsymbol{D}^{\\prime} \\boldsymbol{D}\\right)^{-1}\\left(\\boldsymbol{D}^{\\prime} \\boldsymbol{Y}\\right) \\\\\n&=\\operatorname{diag}\\left(\\mathbf{1}_{i}^{\\prime} \\mathbf{1}_{i}\\right)^{-1}\\left\\{\\mathbf{1}_{i}^{\\prime} \\boldsymbol{Y}_{i}\\right\\}_{i=1, \\ldots, n} \\\\\n&=\\left\\{\\left(\\mathbf{1}_{i}^{\\prime} \\mathbf{1}_{i}\\right)^{-1} \\mathbf{1}_{i}^{\\prime} \\boldsymbol{Y}_{i}\\right\\}_{i=1, \\ldots, n} \\\\\n&=\\left\\{\\bar{Y}_{i}\\right\\}_{i=1, \\ldots, n} .\n\\end{aligned}\n\\]\nThe least squares residuals are\n\\[\n\\widehat{\\boldsymbol{\\varepsilon}}=\\left(\\boldsymbol{I}_{n}-\\boldsymbol{D}\\left(\\boldsymbol{D}^{\\prime} \\boldsymbol{D}\\right)^{-1} \\boldsymbol{D}^{\\prime}\\right) \\boldsymbol{Y}=\\dot{\\boldsymbol{Y}}\n\\]\nas shown in (17.19). Thus the least squares residuals from the simple error-component model are the within transformed variables.\nNow consider the error-component model with regressors, which can be written as\n\\[\nY_{i t}=X_{i t}^{\\prime} \\beta+d_{i}^{\\prime} u+\\varepsilon_{i t}\n\\]\nsince \\(u_{i}=d_{i}^{\\prime} u\\) as discussed above. In matrix notation\n\\[\n\\boldsymbol{Y}=\\boldsymbol{X} \\beta+\\boldsymbol{D} u+\\boldsymbol{\\varepsilon} .\n\\]\nWe consider estimation of \\((\\beta, u)\\) by least squares and write the estimates as \\(\\boldsymbol{Y}=\\boldsymbol{X} \\widehat{\\beta}+\\boldsymbol{D} \\widehat{u}+\\widehat{\\boldsymbol{\\varepsilon}}\\). We call this the dummy variable estimator of the fixed effects model.\nBy the Frisch-Waugh-Lovell Theorem (Theorem 3.5) the dummy variable estimator \\(\\widehat{\\beta}\\) and residuals \\(\\widehat{\\boldsymbol{\\varepsilon}}\\) may be obtained by the least squares regression of the residuals from the regression of \\(\\boldsymbol{Y}\\) on \\(\\boldsymbol{D}\\) on the residuals from the regression of \\(\\boldsymbol{X}\\) on \\(\\boldsymbol{D}\\). We learned above that the residuals from the regression on \\(\\boldsymbol{D}\\) are the within transformations. Thus the dummy variable estimator \\(\\widehat{\\beta}\\) and residuals \\(\\widehat{\\boldsymbol{\\varepsilon}}\\) may be obtained from least squares regression of the within transformed \\(\\dot{Y}\\) on the within transformed \\(\\dot{X}\\). This is exactly the fixed effects estimator \\(\\widehat{\\beta}_{\\mathrm{fe}}\\). Thus the dummy variable and fixed effects estimators of \\(\\beta\\) are identical.\nThis is sufficiently important that we state this result as a theorem.\nTheorem 17.1 The fixed effects estimator of \\(\\beta\\) algebraically equals the dummy variable estimator of \\(\\beta\\). The two estimators have the same residuals.\nThis may be the most important practical application of the Frisch-Waugh-Lovell Theorem. It shows that we can estimate the coefficients either by applying the within transformation or by inclusion of dummy variables (one for each individual in the sample). This is important because in some cases one approach is more convenient than the other and it is important to know that the two methods are algebraically equivalent.\nWhen \\(N\\) is large it is advisable to use the within transformation rather than the dummy variable approach. This is because the latter requires considerably more computer memory. To see this consider the matrix \\(\\boldsymbol{D}\\) in (17.34) in the balanced case. It has \\(T N^{2}\\) elements which must be created and stored in memory. When \\(N\\) is large this can be excessive. For example, if \\(T=10\\) and \\(N=10,000\\), the matrix \\(\\boldsymbol{D}\\) has one billion elements! Whether or not a package can technically handle a matrix of this dimension depends on several particulars (system RAM, operating system, package version), but even if it can execute the calculation the computation time is slow. Hence for fixed effects estimation with large \\(N\\) it is recommended to use the within transformation rather than dummy variable regression.\nThe dummy variable formulation may add insight about how the fixed effects estimator achieves invariance to the fixed effects. Given the regression equation (17.34) we can write the least squares estimator of \\(\\beta\\) using the residual regression formula:\n\\[\n\\begin{aligned}\n\\widehat{\\beta}_{\\mathrm{fe}} &=\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{M}_{\\boldsymbol{D}} \\boldsymbol{X}\\right)^{-1}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{M}_{\\boldsymbol{D}} \\boldsymbol{Y}\\right) \\\\\n&=\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{M}_{\\boldsymbol{D}} \\boldsymbol{X}\\right)^{-1}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{M}_{\\boldsymbol{D}}(\\boldsymbol{X} \\beta+\\boldsymbol{D} u+\\boldsymbol{\\varepsilon})\\right) \\\\\n&=\\beta+\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{M}_{\\boldsymbol{D}} \\boldsymbol{X}\\right)^{-1}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{M}_{\\boldsymbol{D}} \\boldsymbol{\\varepsilon}\\right)\n\\end{aligned}\n\\]\nsince \\(\\boldsymbol{M}_{\\boldsymbol{D}} \\boldsymbol{D}=0\\). The expression (17.35) is free of the vector \\(u\\) and thus \\(\\widehat{\\beta}_{\\mathrm{fe}}\\) is invariant to \\(u\\). This is another demonstration that the fixed effects estimator is invariant to the actual values of the fixed effects, and thus its statistical properties do not rely on assumptions about \\(u_{i}\\)."
  },
  {
    "objectID": "chpt17-panel-data.html#fixed-effects-covariance-matrix-estimation",
    "href": "chpt17-panel-data.html#fixed-effects-covariance-matrix-estimation",
    "title": "16  Panel Data",
    "section": "16.12 Fixed Effects Covariance Matrix Estimation",
    "text": "16.12 Fixed Effects Covariance Matrix Estimation\nFirst consider estimation of the classical covariance matrix \\(\\boldsymbol{V}_{\\mathrm{fe}}^{0}\\) as defined in (17.27). This is\n\\[\n\\widehat{\\boldsymbol{V}}_{\\mathrm{fe}}^{0}=\\widehat{\\sigma}_{\\varepsilon}^{2}\\left(\\dot{\\boldsymbol{X}}^{\\prime} \\dot{\\boldsymbol{X}}\\right)^{-1}\n\\]\nwith\n\\[\n\\widehat{\\sigma}_{\\varepsilon}^{2}=\\frac{1}{n-N-k} \\sum_{i=1}^{n} \\sum_{t \\in S_{i}} \\widehat{\\varepsilon}_{i t}^{2}=\\frac{1}{n-N-k} \\sum_{i=1}^{n} \\widehat{\\boldsymbol{\\varepsilon}}_{i} \\widehat{\\boldsymbol{\\varepsilon}}_{i} .\n\\]\nThe \\(N+k\\) degree of freedom adjustment is motivated by the dummy variable representation. You can verify that \\(\\widehat{\\sigma}_{\\varepsilon}^{2}\\) is unbiased for \\(\\sigma_{\\varepsilon}^{2}\\) under assumptions (17.18), (17.25) and (17.26). See Exercise 17.8.\nNotice that the assumptions (17.18), (17.25), and (17.26) are identical to (17.5)-(17.7) of Assumption 17.1. The assumptions (17.8)-(17.10) are not needed. Thus the fixed effect model weakens the random effects model by eliminating the assumptions on \\(u_{i}\\) but retaining those on \\(\\varepsilon_{i t}\\).\nThe classical covariance matrix estimator (17.36) for the fixed effects estimator is valid when the errors \\(\\varepsilon_{i t}\\) are homoskedastic and serially uncorrelated but is invalid otherwise. A covariance matrix estimator which allows \\(\\varepsilon_{i t}\\) to be heteroskedastic and serially correlated across \\(t\\) is the cluster-robust covariance matrix estimator, clustered by individual\n\\[\n\\widehat{\\boldsymbol{V}}_{\\mathrm{fe}}^{\\text {cluster }}=\\left(\\dot{\\boldsymbol{X}}^{\\prime} \\dot{\\boldsymbol{X}}\\right)^{-1}\\left(\\sum_{i=1}^{N} \\dot{\\boldsymbol{X}}_{i}^{\\prime} \\widehat{\\boldsymbol{\\varepsilon}}_{i} \\widehat{\\boldsymbol{\\varepsilon}}_{i}^{\\prime} \\dot{\\boldsymbol{X}}_{i}\\right)\\left(\\dot{\\boldsymbol{X}}^{\\prime} \\dot{\\boldsymbol{X}}\\right)^{-1}\n\\]\nwhere \\(\\widehat{\\boldsymbol{\\varepsilon}}_{i}\\) as the fixed effects residuals as defined in (17.23). (17.38) was first proposed by Arellano (1987). As in (4.55) \\(\\widehat{V}_{\\text {fe }}^{\\text {cluster }}\\) can be multiplied by a degree-of-freedom adjustment. The adjustment recommended by the theory of C. Hansen (2007) is\n\\[\n\\widehat{\\boldsymbol{V}}_{\\mathrm{fe}}^{\\text {cluster }}=\\left(\\frac{N}{N-1}\\right)\\left(\\dot{\\boldsymbol{X}}^{\\prime} \\dot{\\boldsymbol{X}}\\right)^{-1}\\left(\\sum_{i=1}^{N} \\dot{\\boldsymbol{X}}_{i}^{\\prime} \\widehat{\\boldsymbol{\\varepsilon}}_{i} \\widehat{\\boldsymbol{\\varepsilon}}_{i}^{\\prime} \\dot{\\boldsymbol{X}}_{i}\\right)\\left(\\dot{\\boldsymbol{X}}^{\\prime} \\dot{\\boldsymbol{X}}\\right)^{-1}\n\\]\nand that corresponding to \\((4.55)\\) is\n\\[\n\\widehat{\\boldsymbol{V}}_{\\mathrm{fe}}^{\\text {cluster }}=\\left(\\frac{n-1}{n-N-k}\\right)\\left(\\frac{N}{N-1}\\right)\\left(\\dot{\\boldsymbol{X}}^{\\prime} \\dot{\\boldsymbol{X}}\\right)^{-1}\\left(\\sum_{i=1}^{N} \\dot{\\boldsymbol{X}}_{i}^{\\prime} \\widehat{\\boldsymbol{\\varepsilon}}_{i} \\widehat{\\boldsymbol{\\varepsilon}}_{i}^{\\prime} \\dot{\\boldsymbol{X}}_{i}\\right)\\left(\\dot{\\boldsymbol{X}}^{\\prime} \\dot{\\boldsymbol{X}}\\right)^{-1} \\text {. }\n\\]\nThese estimators are convenient because they are simple to apply and allow for unbalanced panels.\nIn typical micropanel applications \\(N\\) is very large and \\(k\\) is modest. Thus the adjustment in (17.39) is minor while that in (17.40) is approximately \\(\\bar{T} /(\\bar{T}-1)\\) where \\(\\bar{T}=n / N\\) is the average number of time periods per individual. When \\(\\bar{T}\\) is small this can be a very large adjustment. Hence the choice between (17.38), (17.39), and (17.40) can be substantial.\nTo understand if the degree of freedom adjustment in (17.40) is appropriate, consider the simplified setting where the residuals are constructed with the true \\(\\beta\\) but estimated fixed effects \\(u_{i}\\). This is a useful approximation since the number of estimated slope coefficients \\(\\beta\\) is small relative to the sample size \\(n\\). Then \\(\\widehat{\\boldsymbol{\\varepsilon}}_{i}=\\dot{\\boldsymbol{\\varepsilon}}_{i}=\\boldsymbol{M}_{i} \\boldsymbol{\\varepsilon}_{i}\\) so \\(\\dot{\\boldsymbol{X}}_{i}^{\\prime} \\widehat{\\boldsymbol{\\varepsilon}}_{i}=\\dot{\\boldsymbol{X}}_{i}^{\\prime} \\boldsymbol{\\varepsilon}_{i}\\) and (17.38) equals\n\\[\n\\widehat{\\boldsymbol{V}}_{\\mathrm{fe}}^{\\text {cluster }}=\\left(\\dot{\\boldsymbol{X}}^{\\prime} \\dot{\\boldsymbol{X}}\\right)^{-1}\\left(\\sum_{i=1}^{N} \\dot{\\boldsymbol{X}}_{i}^{\\prime} \\varepsilon_{i} \\varepsilon_{i}^{\\prime} \\dot{\\boldsymbol{X}}_{i}\\right)\\left(\\dot{\\boldsymbol{X}}^{\\prime} \\dot{\\boldsymbol{X}}\\right)^{-1}\n\\]\nwhich is the idealized estimator with the true errors rather than the residuals. Since \\(\\mathbb{E}\\left[\\varepsilon_{i} \\varepsilon_{i}^{\\prime} \\mid \\boldsymbol{X}_{i}\\right]=\\Sigma_{i}\\) it follows that \\(\\mathbb{E}\\left[\\widehat{\\boldsymbol{V}}_{\\mathrm{fe}}^{\\text {cluster }} \\mid \\boldsymbol{X}\\right]=\\boldsymbol{V}_{\\mathrm{fe}}\\) and \\(\\widehat{\\boldsymbol{V}}_{\\mathrm{fe}}^{\\text {cluster }}\\) is unbiased for \\(\\boldsymbol{V}_{\\mathrm{fe}}\\) ! Thus no degree of freedom adjustment is required. This is despite the fact that \\(N\\) fixed effects have been estimated. While this analysis concerns the idealized case where the residuals have been constructed with the true coefficients \\(\\beta\\) so does not translate into a direct recommendation for the feasible estimator, it still suggests that the strong ad hoc adjustment in (17.40) is unwarranted.\nThis (crude) analysis suggests that for the cluster robust covariance estimator for fixed effects regression the adjustment recommended by C. Hansen (17.39) is the most appropriate. It is typically well approximated by the unadjusted estimator (17.38). Based on current theory there is no justification for the ad hoc adjustment (17.40). The main argument for the latter is that it produces the largest standard errors and is thus the most conservative choice.\nIn current practice the estimators (17.38) and (17.40) are the most commonly used covariance matrix estimators for fixed effects estimation.\nIn Sections \\(17.22\\) and \\(17.23\\) we discuss covariance matrix estimation under heteroskedasticity but no serial correlation.\nTo illustrate, in Table \\(17.2\\) we present the fixed effect regression estimates of the investment model (17.3) in the third column with cluster-robust standard errors. The trading indicator \\(T_{i}\\) and the industry dummies cannot be included as they are time-invariant. The point estimates are similar to the random effects estimates, though the coefficients on debt and cash flow increase in magnitude."
  },
  {
    "objectID": "chpt17-panel-data.html#fixed-effects-estimation-in-stata",
    "href": "chpt17-panel-data.html#fixed-effects-estimation-in-stata",
    "title": "16  Panel Data",
    "section": "16.13 Fixed Effects Estimation in Stata",
    "text": "16.13 Fixed Effects Estimation in Stata\nThere are several methods to obtain the fixed effects estimator \\(\\widehat{\\beta}_{\\mathrm{fe}}\\) in Stata.\nThe first method is dummy variable regression. This can be obtained by the Stata regress command, for example reg y \\(\\mathrm{x}\\) , cluster(id) where id is the group (individual) identifier. In most cases, as discussed in Section 17.11, this is not recommended due to the excessive computer memory requirements and slow computation. If this command is done it may be useful to suppress display of the full list of coefficient estimates. To do so, type quietly reg y \\(x\\) , cluster(id) followed by estimates table, keep( \\(x_{-}\\)cons) be se. The second command will report the coefficient(s) on \\(x\\) only, not those on the index variable id. (Other statistics can be reported as well.) The second method is to manually create the within transformed variables as described in Section 17.8, and then use regress.\nThe third method is \\(x t r e g ~ f e\\) which is specifically written for panel data. This estimates the slope coefficients using the partialling-out approach. The default covariance matrix estimator is classical as defined in (17.36). The cluster-robust covariance matrix (17.38) can be obtained using the options vce(robust) or \\(r\\).\nThe fourth method is areg absorb (id). This command is an alternative implementation of partiallingout regression. The default covariance matrix estimator is the classical (17.36). The cluster-robust covariance matrix estimator (17.40) can be obtained using the cluster(id) option. The heteroskedasticityrobust covariance matrix is obtained when \\(\\mathrm{r}\\) or \\(\\mathrm{v} c e\\) (robust) is specified but this is not recommended unless \\(T_{i}\\) is large as will be discussed in Section \\(17.22\\).\nAn important difference between the Stata xtreg and areg commands is that they implement different cluster-robust covariance matrix estimators: (17.38) in the case of xtreg and (17.40) in the case of areg. As discussed in the previous section the adjustment used by areg is ad hoc and not well-justified but produces the largest and hence most conservative standard errors.\nAnother difference between the commands is how they report the equation \\(R^{2}\\). This difference can be huge and stems from the fact that they are estimating distinct population counter-parts. Full dummy variable regression and the areg command calculate \\(R^{2}\\) the same way: the squared correlation between \\(Y_{i t}\\) and the fitted regression with all predictors including the individual dummy variables. The \\(x t r e g ~ f e\\) command reports three values for \\(R^{2}\\) : within, between, and overall. The “within” \\(R^{2}\\) is identical to what is obtained from a second stage regression using the within transformed variables. (The second method described above.) The “overall” \\(R^{2}\\) is the squared correlation between \\(Y_{i t}\\) and the fitted regression excluding the individual effects.\nWhich \\(R^{2}\\) should be reported? The answer depends on the baseline model before regressors are added. If we view the baseline as an individual-specific mean, then the within calculation is appropriate. If the baseline is a single mean for all observations then the full regression (areg) calculation is appropriate. The latter (areg) calculation is typically much higher than the within calculation, as the fixed effects typically “explain” a large portion of the variance. In any event as there is not a single definition of \\(R^{2}\\) it is important to be explicit about the method if it is reported.\nIn current econometric practice both xtreg and areg are used, though areg appears to be the more popular choice. Since the latter typically produces a much higher value of \\(R^{2}\\), reported \\(R^{2}\\) values should be viewed skeptically unless their calculation method is documented by the author."
  },
  {
    "objectID": "chpt17-panel-data.html#between-estimator",
    "href": "chpt17-panel-data.html#between-estimator",
    "title": "16  Panel Data",
    "section": "16.14 Between Estimator",
    "text": "16.14 Between Estimator\nThe between estimator is calculated from the individual-mean equation (17.20)\n\\[\n\\bar{Y}_{i}=\\bar{X}_{i}^{\\prime} \\beta+u_{i}+\\bar{\\varepsilon}_{i} .\n\\]\nEstimation can be done at the level of individuals or at the level of observations. Least squares applied to (17.41) at the level of the \\(N\\) individuals is\n\\[\n\\widehat{\\beta}_{\\mathrm{be}}=\\left(\\sum_{i=1}^{N} \\bar{X}_{i} \\bar{X}_{i}^{\\prime}\\right)^{-1}\\left(\\sum_{i=1}^{N} \\bar{X}_{i} \\bar{Y}_{i}\\right) .\n\\]\nLeast squares applied to (17.41) at the level of observations is\n\\[\n\\widetilde{\\beta}_{\\mathrm{be}}=\\left(\\sum_{i=1}^{N} \\sum_{t \\in S_{i}} \\bar{X}_{i} \\bar{X}_{i}^{\\prime}\\right)^{-1}\\left(\\sum_{i=1}^{N} \\sum_{t \\in S_{i}} \\bar{X}_{i} \\bar{Y}_{i}\\right)=\\left(\\sum_{i=1}^{N} T_{i} \\bar{X}_{i} \\bar{X}_{i}^{\\prime}\\right)^{-1}\\left(\\sum_{i=1}^{N} T_{i} \\bar{X}_{i} \\bar{Y}_{i}\\right) .\n\\]\nIn balanced panels \\(\\widetilde{\\beta}_{\\mathrm{be}}=\\widehat{\\beta}_{\\text {be }}\\) but they differ on unbalanced panels. \\(\\widetilde{\\beta}_{\\mathrm{be}}\\) equals weighted least squares applied at the level of individuals with weight \\(T_{i}\\).\nUnder the random effects assumptions (Assumption 17.1) \\(\\widehat{\\beta}_{\\text {be }}\\) is unbiased for \\(\\beta\\) and has variance\n\\[\n\\boldsymbol{V}_{\\mathrm{be}}=\\operatorname{var}\\left[\\widehat{\\beta}_{\\mathrm{be}} \\mid \\boldsymbol{X}\\right]=\\left(\\sum_{i=1}^{N} \\bar{X}_{i} \\bar{X}_{i}^{\\prime}\\right)^{-1}\\left(\\sum_{i=1}^{N} \\bar{X}_{i} \\bar{X}_{i}^{\\prime} \\sigma_{i}^{2}\\right)\\left(\\sum_{i=1}^{N} \\bar{X}_{i} \\bar{X}_{i}^{\\prime}\\right)^{-1}\n\\]\nwhere\n\\[\n\\sigma_{i}^{2}=\\operatorname{var}\\left[u_{i}+\\bar{\\varepsilon}_{i}\\right]=\\sigma_{u}^{2}+\\frac{\\sigma_{\\varepsilon}^{2}}{T_{i}}\n\\]\nis the variance of the error in (17.41). When the panel is balanced the variance formula simplifies to\n\\[\n\\boldsymbol{V}_{\\mathrm{be}}=\\operatorname{var}\\left[\\widehat{\\beta}_{\\mathrm{be}} \\mid \\boldsymbol{X}\\right]=\\left(\\sum_{i=1}^{N} \\bar{X}_{i} \\bar{X}_{i}^{\\prime}\\right)^{-1}\\left(\\sigma_{u}^{2}+\\frac{\\sigma_{\\varepsilon}^{2}}{T}\\right) .\n\\]\nUnder the random effects assumption the between estimator \\(\\widehat{\\beta}_{\\text {be }}\\) is unbiased for \\(\\beta\\) but is less efficient than the random effects estimator \\(\\widehat{\\beta}_{\\text {gls }}\\). Consequently there seems little direct use for the between estimator in linear panel data applications.\nInstead, its primary application is to construct an estimate of \\(\\sigma_{u}^{2}\\). First, consider estimation of\n\\[\n\\sigma_{b}^{2}=\\frac{1}{N} \\sum_{i=1}^{N} \\sigma_{i}^{2}=\\sigma_{u}^{2}+\\frac{1}{N} \\sum_{i=1}^{N} \\frac{\\sigma_{\\varepsilon}^{2}}{T_{i}}=\\sigma_{u}^{2}+\\frac{\\sigma_{\\varepsilon}^{2}}{\\bar{T}}\n\\]\nwhere \\(\\bar{T}=N / \\sum_{i=1}^{N} T_{i}^{-1}\\) is the harmonic mean of \\(T_{i}\\). (In the case of a balanced panel \\(\\bar{T}=T\\).) A natural estimator of \\(\\sigma_{b}^{2}\\) is\n\\[\n\\widehat{\\sigma}_{b}^{2}=\\frac{1}{N-k} \\sum_{i=1}^{N} \\widehat{e}_{b i}^{2} .\n\\]\nwhere \\(\\widehat{e}_{b i}=\\bar{Y}_{i}-\\bar{X}_{i}^{\\prime} \\widehat{\\beta}_{\\text {be }}\\) are the between residuals. (Either \\(\\widehat{\\beta}_{\\text {be }}\\) or \\(\\widetilde{\\beta}_{\\text {be }}\\) can be used.)\nFrom the relation \\(\\sigma_{b}^{2}=\\sigma_{u}^{2}+\\sigma_{\\varepsilon}^{2} / \\bar{T}\\) and (17.42) we can deduce an estimator for \\(\\sigma_{u}^{2}\\). We have already described an estimator \\(\\widehat{\\sigma}_{\\varepsilon}^{2}\\) for \\(\\sigma_{\\varepsilon}^{2}\\) in (17.37) for the fixed effects model. Since the fixed effects model holds under weaker conditions than the random effects model, \\(\\widehat{\\sigma}_{\\varepsilon}^{2}\\) is valid for the latter as well. This suggests the following estimator for \\(\\sigma_{u}^{2}\\)\n\\[\n\\widehat{\\sigma}_{u}^{2}=\\widehat{\\sigma}_{b}^{2}-\\frac{\\widehat{\\sigma}_{\\varepsilon}^{2}}{\\bar{T}} .\n\\]\nTo summarize, the fixed effect estimator is used for \\(\\widehat{\\sigma}_{\\varepsilon}^{2}\\), the between estimator for \\(\\widehat{\\sigma}_{b}^{2}\\), and \\(\\widehat{\\sigma}_{u}^{2}\\) is constructed from the two.\nIt is possible for (17.43) to be negative. It is typical to use the constrained estimator\n\\[\n\\widehat{\\sigma}_{u}^{2}=\\max \\left[0, \\widehat{\\sigma}_{b}^{2}-\\frac{\\widehat{\\sigma}_{\\varepsilon}^{2}}{\\bar{T}}\\right] .\n\\]\n(17.44) is the most common estimator for \\(\\sigma_{u}^{2}\\) in the random effects model.\nThe between estimator \\(\\widehat{\\beta}_{\\text {be }}\\) can be obtained using the Stata command xtreg be. The estimator \\(\\widetilde{\\beta}_{\\text {be }}\\) can be obtained by xtreg be wls."
  },
  {
    "objectID": "chpt17-panel-data.html#feasible-gls",
    "href": "chpt17-panel-data.html#feasible-gls",
    "title": "16  Panel Data",
    "section": "16.15 Feasible GLS",
    "text": "16.15 Feasible GLS\nThe random effects estimator can be written as\n\\[\n\\widehat{\\beta}_{\\mathrm{re}}=\\left(\\sum_{i=1}^{N} \\boldsymbol{X}_{i}^{\\prime} \\Omega_{i}^{-1} \\boldsymbol{X}_{i}\\right)^{-1}\\left(\\sum_{i=1}^{N} \\boldsymbol{X}_{i}^{\\prime} \\Omega_{i}^{-1} \\boldsymbol{Y}_{i}\\right)=\\left(\\sum_{i=1}^{N} \\widetilde{\\boldsymbol{X}}_{i}^{\\prime} \\widetilde{\\boldsymbol{X}}_{i}\\right)^{-1}\\left(\\sum_{i=1}^{N} \\widetilde{\\boldsymbol{X}}_{i}^{\\prime} \\widetilde{\\boldsymbol{Y}}_{i}\\right)\n\\]\nwhere \\(\\widetilde{\\boldsymbol{X}}_{i}=\\Omega_{i}^{-1 / 2} \\boldsymbol{X}_{i}\\) and \\(\\widetilde{\\boldsymbol{Y}}_{i}=\\Omega_{i}^{-1 / 2} \\boldsymbol{Y}_{i}\\). It is instructive to study these transformations.\nDefine \\(\\boldsymbol{P}_{i}=\\mathbf{1}_{i}\\left(\\mathbf{1}_{i}^{\\prime} \\mathbf{1}_{i}\\right)^{-1} \\mathbf{1}_{i}^{\\prime}\\) so that \\(\\boldsymbol{M}_{i}=\\boldsymbol{I}_{i}-\\boldsymbol{P}_{i}\\). Thus while \\(\\boldsymbol{M}_{i}\\) is the within operator, \\(\\boldsymbol{P}_{i}\\) can be called the individual-mean operator since \\(\\boldsymbol{P}_{i} \\boldsymbol{Y}_{i}=\\mathbf{1}_{i} \\bar{Y}_{i}\\). We can write\n\\[\n\\Omega_{i}=\\boldsymbol{I}_{i}+\\mathbf{1}_{i} \\mathbf{1}_{i}^{\\prime} \\sigma_{u}^{2} / \\sigma_{\\varepsilon}^{2}=\\boldsymbol{I}_{i}+\\frac{T_{i} \\sigma_{u}^{2}}{\\sigma_{\\varepsilon}^{2}} \\boldsymbol{P}_{i}=\\boldsymbol{M}_{i}+\\rho_{i}^{-2} \\boldsymbol{P}_{i}\n\\]\nwhere\n\\[\n\\rho_{i}=\\frac{\\sigma_{\\varepsilon}}{\\sqrt{\\sigma_{\\varepsilon}^{2}+T_{i} \\sigma_{u}^{2}}} .\n\\]\nSince the matrices \\(\\boldsymbol{M}_{i}\\) and \\(\\boldsymbol{P}_{i}\\) are idempotent and orthogonal we find that \\(\\Omega_{i}^{-1}=\\boldsymbol{M}_{i}+\\rho_{i}^{2} \\boldsymbol{P}_{i}\\) and\n\\[\n\\Omega_{i}^{-1 / 2}=\\boldsymbol{M}_{i}+\\rho_{i} \\boldsymbol{P}_{i}=\\boldsymbol{I}_{i}-\\left(1-\\rho_{i}\\right) \\boldsymbol{P}_{i} .\n\\]\nTherefore the transformation used by the GLS estimator is\n\\[\n\\tilde{\\boldsymbol{Y}}_{i}=\\left(\\boldsymbol{I}_{i}-\\left(1-\\rho_{i}\\right) \\boldsymbol{P}_{i}\\right) \\boldsymbol{Y}_{i}=\\boldsymbol{Y}_{i}-\\left(1-\\rho_{i}\\right) \\mathbf{1}_{i} \\bar{Y}_{i}\n\\]\nwhich is a partial within transformation.\nThe transformation as written depends on \\(\\rho_{i}\\) which is unknown. It can be replaced by the estimator\n\\[\n\\widehat{\\rho}_{i}=\\frac{\\widehat{\\sigma}_{\\varepsilon}}{\\sqrt{\\widehat{\\sigma}_{\\varepsilon}^{2}+T_{i} \\widehat{\\sigma}_{u}^{2}}}\n\\]\nwhere the estimators \\(\\widehat{\\sigma}_{\\varepsilon}^{2}\\) and \\(\\widehat{\\sigma}_{u}^{2}\\) are given in (17.37) and (17.44). We obtain the feasible transformations\n\\[\n\\widetilde{\\boldsymbol{Y}}_{i}=\\boldsymbol{Y}_{i}-\\left(1-\\widehat{\\rho}_{i}\\right) \\mathbf{1}_{i} \\bar{Y}_{i}\n\\]\nand\n\\[\n\\widetilde{\\boldsymbol{X}}_{i}=\\boldsymbol{X}_{i}-\\left(1-\\widehat{\\rho}_{i}\\right) \\mathbf{1}_{i} \\bar{X}_{i}^{\\prime} .\n\\]\nThe feasible random effects estimator is (17.45) using (17.49) and (17.50).\nIn the previous section we noted that it is possible for \\(\\widehat{\\sigma}_{u}^{2}=0\\). In this case \\(\\widehat{\\rho}_{i}=1\\) and \\(\\widehat{\\beta}_{\\text {re }}=\\widehat{\\beta}_{\\text {pool }}\\).\nWhat this shows is the following. The random effects estimator (17.45) is least squares applied to the transformed variables \\(\\widetilde{\\boldsymbol{X}}_{i}\\) and \\(\\widetilde{\\boldsymbol{Y}}_{i}\\) defined in (17.50) and (17.49). When \\(\\widehat{\\rho}_{i}=0\\) these are the within transformations, so \\(\\widetilde{\\boldsymbol{X}}_{i}=\\dot{\\boldsymbol{X}}_{i}, \\widetilde{\\boldsymbol{Y}}_{i}=\\dot{\\boldsymbol{Y}}_{i}\\), and \\(\\widehat{\\beta}_{\\mathrm{re}}=\\widehat{\\beta}_{\\mathrm{fe}}\\) is the fixed effects estimator. When \\(\\widehat{\\rho}_{i}=1\\) the data are untransformed \\(\\widetilde{\\boldsymbol{X}}_{i}=\\boldsymbol{X}_{i}, \\widetilde{\\boldsymbol{Y}}_{i}=\\boldsymbol{Y}_{i}\\), and \\(\\widehat{\\beta}_{\\mathrm{re}}=\\widehat{\\beta}_{\\text {pool }}\\) is the pooled estimator. In general, \\(\\widetilde{\\boldsymbol{X}}_{i}\\) and \\(\\widetilde{\\boldsymbol{Y}}_{i}\\) can be viewed as partial within transformations.\nRecalling the definition \\(\\widehat{\\rho}_{i}=\\widehat{\\sigma}_{\\varepsilon} / \\sqrt{\\widehat{\\sigma}_{\\varepsilon}^{2}+T_{i} \\widehat{\\sigma}_{u}^{2}}\\) we see that when the idiosyncratic error variance \\(\\widehat{\\sigma}_{\\varepsilon}^{2}\\) is large relative to \\(T_{i} \\widehat{\\sigma}_{u}^{2}\\) then \\(\\widehat{\\rho}_{i} \\approx 1\\) and \\(\\widehat{\\beta}_{\\text {re }} \\approx \\widehat{\\beta}_{\\text {pool. }}\\). Thus when the variance estimates suggest that the individual effect is relatively small the random effect estimator simplifies to the pooled estimator. On the other hand when the individual effect error variance \\(\\widehat{\\sigma}_{u}^{2}\\) is large relative to \\(\\widehat{\\sigma}_{\\varepsilon}^{2}\\) then \\(\\widehat{\\rho}_{i} \\approx 0\\) and \\(\\widehat{\\beta}_{\\mathrm{re}} \\approx \\widehat{\\beta}_{\\mathrm{fe}}\\). Thus when the variance estimates suggest that the individual effect is relatively large the random effect estimator is close to the fixed effects estimator."
  },
  {
    "objectID": "chpt17-panel-data.html#intercept-in-fixed-effects-regression",
    "href": "chpt17-panel-data.html#intercept-in-fixed-effects-regression",
    "title": "16  Panel Data",
    "section": "16.16 Intercept in Fixed Effects Regression",
    "text": "16.16 Intercept in Fixed Effects Regression\nThe fixed effect estimator does not apply to any regressor which is time-invariant for all individuals. This includes an intercept. Yet some authors and packages (e.g. Amemiya (1971) and xtreg in Stata) report an intercept. To see how to construct an estimator of an intercept take the components regression equation adding an explicit intercept\n\\[\nY_{i t}=\\alpha+X_{i t}^{\\prime} \\beta+u_{i}+\\varepsilon_{i t} .\n\\]\nWe have already discussed estimation of \\(\\beta\\) by \\(\\widehat{\\beta}_{\\mathrm{fe}}\\). Replacing \\(\\beta\\) in this equation with \\(\\widehat{\\beta}_{\\mathrm{fe}}\\) and then estimating \\(\\alpha\\) by least squares, we obtain\n\\[\n\\widehat{\\alpha}_{\\mathrm{fe}}=\\bar{Y}-\\bar{X}^{\\prime} \\widehat{\\beta}_{\\mathrm{fe}}\n\\]\nwhere \\(\\bar{Y}\\) and \\(\\bar{X}\\) are averages from the full sample. This is the estimator reported by xtreg."
  },
  {
    "objectID": "chpt17-panel-data.html#estimation-of-fixed-effects",
    "href": "chpt17-panel-data.html#estimation-of-fixed-effects",
    "title": "16  Panel Data",
    "section": "16.17 Estimation of Fixed Effects",
    "text": "16.17 Estimation of Fixed Effects\nFor most applications researchers are interested in the coefficients \\(\\beta\\) not the fixed effects \\(u_{i}\\). But in some cases the fixed effects themselves are interesting. This arises when we want to measure the distribution of \\(u_{i}\\) to understand its heterogeneity. It also arises in the context of prediction. As discussed in Section \\(17.11\\) the fixed effects estimate \\(\\widehat{u}\\) is obtained by least squares applied to the regression (17.33). To find their solution, replace \\(\\beta\\) in (17.33) with the least squares minimizer \\(\\widehat{\\beta}_{\\mathrm{fe}}\\) and apply least squares. Since this is the individual-specific intercept the solution is\n\\[\n\\widehat{u}_{i}=\\frac{1}{T_{i}} \\sum_{t \\in S_{i}}\\left(Y_{i t}-X_{i t}^{\\prime} \\widehat{\\beta}_{\\mathrm{fe}}\\right)=\\bar{Y}_{i}-\\bar{X}_{i}^{\\prime} \\widehat{\\beta}_{\\mathrm{fe}} .\n\\]\nAlternatively, using (17.34) this is\n\\[\n\\begin{aligned}\n\\widehat{u} &=\\left(\\boldsymbol{D}^{\\prime} \\boldsymbol{D}\\right)^{-1} \\boldsymbol{D}^{\\prime}\\left(\\boldsymbol{Y}-\\boldsymbol{X} \\widehat{\\beta}_{\\mathrm{fe}}\\right) \\\\\n&=\\operatorname{diag}\\left\\{T_{i}^{-1}\\right\\} \\sum_{i=1}^{N} d_{i} \\mathbf{1}_{i}^{\\prime}\\left(\\boldsymbol{Y}_{i}-\\boldsymbol{X}_{i} \\widehat{\\beta}_{\\mathrm{fe}}\\right) \\\\\n&=\\sum_{i=1}^{N} d_{i}\\left(\\bar{Y}_{i}-\\bar{X}_{i}^{\\prime} \\widehat{\\beta}_{\\mathrm{fe}}\\right) \\\\\n&=\\left(\\widehat{u}_{1}, \\ldots, \\widehat{u}_{N}\\right)^{\\prime}\n\\end{aligned}\n\\]\nThus the least squares estimates of the fixed effects can be obtained from the individual-specific means and does not require a regression with \\(N+k\\) regressors.\nIf an intercept has been estimated (as discussed in the previous section) it should be subtracted from (17.51). In this case the estimated fixed effects are\n\\[\n\\widehat{u}_{i}=\\bar{Y}_{i}-\\bar{X}_{i}^{\\prime} \\widehat{\\beta}_{\\mathrm{fe}}-\\widehat{\\alpha}_{\\mathrm{fe}}\n\\]\nWith either estimator when the number of time series observations \\(T_{i}\\) is small \\(\\widehat{u}_{i}\\) will be an imprecise estimator of \\(u_{i}\\). Thus calculations based on \\(\\widehat{u}_{i}\\) should be interpreted cautiously.\nThe fixed effects (17.52) may be obtained in Stata after ivreg, fe using the predict u command or after areg using the predict d command."
  },
  {
    "objectID": "chpt17-panel-data.html#gmm-interpretation-of-fixed-effects",
    "href": "chpt17-panel-data.html#gmm-interpretation-of-fixed-effects",
    "title": "16  Panel Data",
    "section": "16.18 GMM Interpretation of Fixed Effects",
    "text": "16.18 GMM Interpretation of Fixed Effects\nWe can also interpret the fixed effects estimator through the generalized method of moments.\nTake the fixed effects model after applying the within transformation (17.21). We can view this as a system of \\(T\\) equations, one for each time period \\(t\\). This is a multivariate regression model. Using the notation of Chapter 11 define the \\(T \\times k T\\) regressor matrix\n\\[\n\\overline{\\boldsymbol{X}}_{i}=\\left(\\begin{array}{cccc}\n\\dot{X}_{i 1}^{\\prime} & 0 & \\cdots & 0 \\\\\n\\vdots & \\dot{X}_{i 2}^{\\prime} & & \\vdots \\\\\n0 & 0 & \\cdots & \\dot{X}_{i T}^{\\prime}\n\\end{array}\\right) .\n\\]\nIf we treat each time period as a separate equation we have the \\(k T\\) moment conditions\n\\[\n\\mathbb{E}\\left[\\overline{\\boldsymbol{X}}_{i}^{\\prime}\\left(\\dot{\\boldsymbol{Y}}_{i}-\\dot{\\boldsymbol{X}}_{i} \\beta\\right)\\right]=0 .\n\\]\nThis is an overidentified system of equations when \\(T \\geq 3\\) as there are \\(k\\) coefficients and \\(k T\\) moments. (However, the moments are collinear due to the within transformation. There are \\(k(T-1)\\) effective moments.) Interpreting this model in the context of multivariate regression, overidentification is achieved by the restriction that the coefficient vector \\(\\beta\\) is constant across time periods.\nThis model can be interpreted as a regression of \\(\\dot{\\boldsymbol{Y}}_{i}\\) on \\(\\dot{\\boldsymbol{X}}_{i}\\) using the instruments \\(\\overline{\\boldsymbol{X}}_{i}\\). The 2SLS estimator using matrix notation is\n\\[\n\\widehat{\\beta}=\\left(\\left(\\dot{\\boldsymbol{X}}^{\\prime} \\overline{\\boldsymbol{X}}\\right)\\left(\\overline{\\boldsymbol{X}}^{\\prime} \\overline{\\boldsymbol{X}}\\right)^{-1}\\left(\\overline{\\boldsymbol{X}}^{\\prime} \\dot{\\boldsymbol{X}}\\right)\\right)^{-1}\\left(\\left(\\dot{\\boldsymbol{X}}^{\\prime} \\overline{\\boldsymbol{X}}\\right)\\left(\\overline{\\boldsymbol{X}}^{\\prime} \\overline{\\boldsymbol{X}}\\right)^{-1}\\left(\\overline{\\boldsymbol{X}}^{\\prime} \\dot{\\boldsymbol{Y}}\\right)\\right)\n\\]\nNotice that\n\\[\n\\begin{aligned}\n& \\overline{\\boldsymbol{X}}^{\\prime} \\overline{\\boldsymbol{X}}=\\sum_{i=1}^{n}\\left(\\begin{array}{cccc}\\dot{X}_{i 1} & 0 & \\cdots & 0 \\\\\\vdots & \\dot{X}_{i 2} & & \\vdots \\\\0 & 0 & \\cdots & \\dot{X}_{i T}\\end{array}\\right)\\left(\\begin{array}{cccc}\\dot{X}_{i 1}^{\\prime} & 0 & \\cdots & 0 \\\\\\vdots & \\dot{X}_{i 2}^{\\prime} & & \\vdots \\\\0 & 0 & \\cdots & \\dot{X}_{i T}^{\\prime}\\end{array}\\right) \\\\\n& =\\left(\\begin{array}{cccc}\\sum_{i=1}^{n} \\dot{X}_{i 1} \\dot{X}_{i 1}^{\\prime} & 0 & \\cdots & 0 \\\\\\vdots & \\sum_{i=1}^{n} \\dot{X}_{i 2} \\dot{X}_{i 2}^{\\prime} & & \\vdots \\\\0 & 0 & \\cdots & \\sum_{i=1}^{n} \\dot{X}_{i T} \\dot{X}_{i T}^{\\prime}\\end{array}\\right) \\text {, } \\\\\n& \\overline{\\boldsymbol{X}}^{\\prime} \\dot{\\boldsymbol{X}}=\\left(\\begin{array}{c}\\sum_{i=1}^{n} \\dot{X}_{i 1} \\dot{X}_{i 1}^{\\prime} \\\\\\vdots \\\\\\sum_{i=1}^{n} \\dot{X}_{i T} \\dot{X}_{i T}^{\\prime}\\end{array}\\right) \\text {, }\n\\end{aligned}\n\\]\nand\n\\[\n\\overline{\\boldsymbol{X}}^{\\prime} \\dot{\\boldsymbol{Y}}=\\left(\\begin{array}{c}\n\\sum_{i=1}^{n} \\dot{X}_{i 1} \\dot{Y}_{i 1} \\\\\n\\vdots \\\\\n\\sum_{i=1}^{n} \\dot{X}_{i T} \\dot{Y}_{i T}\n\\end{array}\\right) \\text {. }\n\\]\nThus the 2SLS estimator simplifies to\n\\[\n\\begin{aligned}\n\\widehat{\\beta}_{2 \\mathrm{sls}} &=\\left(\\sum_{t=1}^{T}\\left(\\sum_{i=1}^{n} \\dot{X}_{i t} \\dot{X}_{i t}^{\\prime}\\right)\\left(\\sum_{i=1}^{n} \\dot{X}_{i t} \\dot{X}_{i t}^{\\prime}\\right)^{-1}\\left(\\sum_{i=1}^{n} \\dot{X}_{i t} \\dot{X}_{i t}^{\\prime}\\right)\\right)^{-1} \\\\\n& \\times\\left(\\sum_{t=1}^{T}\\left(\\sum_{i=1}^{n} \\dot{X}_{i t} \\dot{X}_{i t}^{\\prime}\\right)\\left(\\sum_{i=1}^{n} \\dot{X}_{i t} \\dot{X}_{i t}^{\\prime}\\right)^{-1}\\left(\\sum_{i=1}^{n} \\dot{X}_{i t} \\dot{Y}_{i t}\\right)\\right) \\\\\n&=\\left(\\sum_{t=1}^{T} \\sum_{i=1}^{n} \\dot{X}_{i t} \\dot{X}_{i t}^{\\prime}\\right)^{-1}\\left(\\sum_{t=1}^{T} \\sum_{i=1}^{n} \\dot{X}_{i t} \\dot{Y}_{i t}\\right) \\\\\n&=\\widehat{\\beta}_{\\mathrm{fe}}\n\\end{aligned}\n\\]\nthe fixed effects estimator!\nThis shows that if we treat each time period as a separate equation with its separate moment equation so that the system is over-identified, and then estimate by GMM using the 2SLS weight matrix, the resulting GMM estimator equals the simple fixed effects estimator. There is no change by adding the additional moment conditions.\nThe 2SLS estimator is the appropriate GMM estimator when the equation error is serially uncorrelated and homoskedastic. If we use a two-step efficient weight matrix which allows for heteroskedasticity and serial correlation the GMM estimator is\n\\[\n\\begin{aligned}\n\\widehat{\\beta}_{\\mathrm{gmm}} &=\\left(\\sum_{t=1}^{T}\\left(\\sum_{i=1}^{n} \\dot{X}_{i t} \\dot{X}_{i t}^{\\prime}\\right)\\left(\\sum_{i=1}^{n} \\dot{X}_{i t} \\dot{X}_{i t}^{\\prime} \\widehat{e}_{i t}^{2}\\right)^{-1}\\left(\\sum_{i=1}^{n} \\dot{X}_{i t} \\dot{X}_{i t}^{\\prime}\\right)\\right)^{-1} \\\\\n& \\times\\left(\\sum_{t=1}^{T}\\left(\\sum_{i=1}^{n} \\dot{X}_{i t} \\dot{X}_{i t}^{\\prime}\\right)\\left(\\sum_{i=1}^{n} \\dot{X}_{i t} \\dot{X}_{i t}^{\\prime} \\widehat{e}_{i t}^{2}\\right)^{-1}\\left(\\sum_{i=1}^{n} \\dot{X}_{i t} \\dot{Y}_{i t}\\right)\\right)\n\\end{aligned}\n\\]\nwhere \\(\\widehat{e}_{i t}\\) are the fixed effects residuals.\nNotationally, this GMM estimator has been written for a balanced panel. For an unbalanced panel the sums over \\(i\\) need to be replaced by sums over individuals observed during time period \\(t\\). Otherwise no changes need to be made."
  },
  {
    "objectID": "chpt17-panel-data.html#identification-in-the-fixed-effects-model",
    "href": "chpt17-panel-data.html#identification-in-the-fixed-effects-model",
    "title": "16  Panel Data",
    "section": "16.19 Identification in the Fixed Effects Model",
    "text": "16.19 Identification in the Fixed Effects Model\nThe identification of the slope coefficient \\(\\beta\\) in fixed effects regression is similar to that in conventional regression but somewhat more nuanced.\nIt is most useful to consider the within-transformed equation, which can be written as \\(\\dot{Y}_{i t}=\\dot{X}_{i t}^{\\prime} \\beta+\\dot{\\varepsilon}_{i t}\\) or \\(\\dot{\\boldsymbol{Y}}_{i}=\\dot{\\boldsymbol{X}}_{i} \\beta+\\dot{\\boldsymbol{\\varepsilon}}_{i}\\)\nFrom regression theory we know that the coefficient \\(\\beta\\) is the linear effect of \\(\\dot{X}_{i t}\\) on \\(\\dot{Y}_{i t}\\). The variable \\(\\dot{X}_{i t}\\) is the deviation of the regressor from its individual-specific mean and similarly for \\(\\dot{Y}_{i t}\\). Thus the fixed effects model does not identify the effect of the average level of \\(X_{i t}\\) on the average level of \\(Y_{i t}\\), but rather the effect of the deviations in \\(X_{i t}\\) on \\(Y_{i t}\\).\nIn any given sample the fixed effects estimator is only defined if \\(\\sum_{i=1}^{N} \\dot{\\boldsymbol{X}}_{i}^{\\prime} \\dot{\\boldsymbol{X}}_{i}\\) is full rank. The population analog (when individuals are i.i.d.) is\n\\[\n\\mathbb{E}\\left[\\dot{\\boldsymbol{X}}_{i}^{\\prime} \\dot{\\boldsymbol{X}}_{i}\\right]>0 .\n\\]\nEquation (17.54) is the identification condition for the fixed effects estimator. It requires that the regressor matrix is full-rank in expectation after application of the within transformation. The regressors cannot contain any variable which does not have time-variation at the individual level nor a set of regressors whose time-variation at the individual level is collinear."
  },
  {
    "objectID": "chpt17-panel-data.html#asymptotic-distribution-of-fixed-effects-estimator",
    "href": "chpt17-panel-data.html#asymptotic-distribution-of-fixed-effects-estimator",
    "title": "16  Panel Data",
    "section": "16.20 Asymptotic Distribution of Fixed Effects Estimator",
    "text": "16.20 Asymptotic Distribution of Fixed Effects Estimator\nIn this section we present an asymptotic distribution theory for the fixed effects estimator in balanced panels. Unbalanced panels are considered in the following section.\nWe use the following assumptions.\nAssumption $17.2\n\n\\(Y_{i t}=X_{i t}^{\\prime} \\beta+u_{i}+\\varepsilon_{i t}\\) for \\(i=1, \\ldots, N\\) and \\(t=1, \\ldots, T\\) with \\(T \\geq 2\\).\nThe variables \\(\\left(\\boldsymbol{\\varepsilon}_{i}, \\boldsymbol{X}_{i}\\right), i=1, \\ldots, N\\), are independent and identically distributed.\n\\(\\mathbb{E}\\left[X_{i s} \\varepsilon_{i t}\\right]=0\\) for all \\(s=1, \\ldots, T\\).\n\\(\\boldsymbol{Q}_{T}=\\mathbb{E}\\left[\\dot{\\boldsymbol{X}}_{i}^{\\prime} \\dot{\\boldsymbol{X}}_{i}\\right]>0\\).\n\\(\\mathbb{E}\\left[\\varepsilon_{i t}^{4}\\right]<\\infty\\).\n\\(\\mathbb{E}\\left\\|X_{i t}\\right\\|^{4}<\\infty\\)\n\nGiven Assumption \\(17.2\\) we can establish asymptotic normality for \\(\\widehat{\\beta}_{\\mathrm{fe}}\\).\nTheorem 17.2 Under Assumption 17.2, as \\(N \\rightarrow \\infty, \\sqrt{N}\\left(\\widehat{\\beta}_{\\mathrm{fe}}-\\beta\\right) \\underset{d}{\\longrightarrow} \\mathrm{N}\\left(0, \\boldsymbol{V}_{\\boldsymbol{\\beta}}\\right)\\) where \\(\\boldsymbol{V}_{\\beta}=\\boldsymbol{Q}_{T}^{-1} \\Omega_{T} \\boldsymbol{Q}_{T}^{-1}\\) and \\(\\Omega_{T}=\\mathbb{E}\\left[\\dot{\\boldsymbol{X}}_{i}^{\\prime} \\boldsymbol{\\varepsilon}_{i} \\boldsymbol{\\varepsilon}_{i}^{\\prime} \\dot{\\boldsymbol{X}}_{i}\\right]\\).\nThis asymptotic distribution is derived as the number of individuals \\(N\\) diverges to infinity while the time number of time periods \\(T\\) is held fixed. Therefore the normalization is \\(\\sqrt{N}\\) rather than \\(\\sqrt{n}\\) (though either could be used since \\(T\\) is fixed). This approximation is appropriate for the context of a large number of individuals. We could alternatively derive an approximation for the case where both \\(N\\) and \\(T\\) diverge to infinity but this would not be a stronger result. One way of thinking about this is that Theorem \\(17.2\\) does not require \\(T\\) to be large.\nTheorem \\(17.2\\) may appear standard given our arsenal of asymptotic theory but in a fundamental sense it is quite different from any other result we have introduced. Fixed effects regression is effectively estimating \\(N+k\\) coefficients - the \\(k\\) slope coefficients \\(\\beta\\) plus the \\(N\\) fixed effects \\(u\\) - and the theory specifies that \\(N \\rightarrow \\infty\\). Thus the number of estimated parameters is diverging to infinity at the same rate as sample size yet the the estimator obtains a conventional mean-zero sandwich-form asymptotic distribution. In this sense Theorem \\(17.2\\) is new and special.\nWe now discuss the assumptions.\nAssumption 17.2.2 states that the observations are independent across individuals \\(i\\). This is commonly used for panel data asymptotic theory. An important implied restriction is that it means that we exclude from the regressors any serially correlated aggregate time series variation. Assumption 17.2.3 imposes that \\(X_{i t}\\) is strictly exogeneous for \\(\\varepsilon_{i t}\\). This is stronger than simple projection but is weaker than strict mean independence (17.18). It does not impose any condition on the individual-specific effects \\(u_{i}\\).\nAssumption 17.2.4 is the identification condition discussed in the previous section.\nAssumptions 17.2.5 and 17.2.6 are needed for the central limit theorem.\nWe now prove Theorem 17.2. The assumptions imply that the variables \\(\\left(\\dot{\\boldsymbol{X}}_{i}, \\boldsymbol{\\varepsilon}_{i}\\right)\\) are i.i.d. across \\(i\\) and have finite fourth moments. Thus by the WLLN\n\\[\n\\frac{1}{N} \\sum_{i=1}^{N} \\dot{\\boldsymbol{X}}_{i}^{\\prime} \\dot{\\boldsymbol{X}}_{i} \\underset{p}{\\longrightarrow} \\mathbb{E}\\left[\\dot{\\boldsymbol{X}}_{i}^{\\prime} \\dot{\\boldsymbol{X}}_{i}\\right]=\\boldsymbol{Q}_{T} .\n\\]\nAssumption 17.2.3 implies\n\\[\n\\mathbb{E}\\left[\\dot{\\boldsymbol{X}}_{i}^{\\prime} \\boldsymbol{\\varepsilon}_{i}\\right]=\\sum_{t=1}^{T} \\mathbb{E}\\left[\\dot{X}_{i t} \\varepsilon_{i t}\\right]=\\sum_{t=1}^{T} \\mathbb{E}\\left[X_{i t} \\varepsilon_{i t}\\right]-\\sum_{t=1}^{T} \\sum_{j=1}^{T} \\mathbb{E}\\left[X_{i j} \\varepsilon_{i t}\\right]=0\n\\]\nso they are mean zero. Assumptions 17.2.5 and 17.2.6 imply that \\(\\dot{\\boldsymbol{X}}_{i}^{\\prime} \\boldsymbol{\\varepsilon}_{i}\\) has a finite covariance matrix \\(\\Omega_{T}\\). The assumptions for the CLT (Theorem 6.3) hold, thus\n\\[\n\\frac{1}{\\sqrt{N}} \\sum_{i=1}^{N} \\dot{\\boldsymbol{X}}_{i}^{\\prime} \\varepsilon_{i} \\underset{d}{\\longrightarrow} \\mathrm{N}\\left(0, \\Omega_{T}\\right)\n\\]\nTogether we find\n\\[\n\\sqrt{N}\\left(\\widehat{\\beta}_{\\mathrm{fe}}-\\beta\\right)=\\left(\\frac{1}{N} \\sum_{i=1}^{N} \\dot{\\boldsymbol{X}}_{i}^{\\prime} \\dot{\\boldsymbol{X}}_{i}\\right)^{-1}\\left(\\frac{1}{\\sqrt{N}} \\sum_{i=1}^{N} \\dot{\\boldsymbol{X}}_{i}^{\\prime} \\boldsymbol{\\varepsilon}_{i}\\right) \\underset{d}{\\longrightarrow} \\boldsymbol{Q}_{T}^{-1} \\mathrm{~N}\\left(0, \\Omega_{T}\\right)=\\mathrm{N}\\left(0, \\boldsymbol{V}_{\\beta}\\right)\n\\]\nas stated."
  },
  {
    "objectID": "chpt17-panel-data.html#asymptotic-distribution-for-unbalanced-panels",
    "href": "chpt17-panel-data.html#asymptotic-distribution-for-unbalanced-panels",
    "title": "16  Panel Data",
    "section": "16.21 Asymptotic Distribution for Unbalanced Panels",
    "text": "16.21 Asymptotic Distribution for Unbalanced Panels\nIn this section we extend the theory of the previous section to cover unbalanced panels under random selection. Our presentation is built on Section \\(17.1\\) of Wooldridge (2010).\nThink of an unbalanced panel as a shortened version of an idealized balanced panel where the shortening is due to “missing” observations due to random selection. Thus suppose that the underlying (potentially latent) variables are \\(\\boldsymbol{Y}_{i}=\\left(Y_{i 1}, \\ldots, Y_{i T}\\right)^{\\prime}\\) and \\(\\boldsymbol{X}_{i}=\\left(X_{i 1}, \\ldots, X_{i T}\\right)^{\\prime}\\). Let \\(\\boldsymbol{s}_{i}=\\left(s_{i 1}, \\ldots, s_{i T}\\right)^{\\prime}\\) be a vector of selection indicators, meaning that \\(s_{i t}=1\\) if the time period \\(t\\) is observed for individual \\(i\\) and \\(s_{i t}=0\\) otherwise. Then we can describe the estimators algebraically as follows.\nLet \\(\\boldsymbol{S}_{i}=\\operatorname{diag}\\left(\\boldsymbol{s}_{i}\\right)\\) and \\(\\boldsymbol{M}_{i}=\\boldsymbol{S}_{i}-\\boldsymbol{s}_{i}\\left(\\boldsymbol{s}_{i}^{\\prime} \\boldsymbol{s}_{i}\\right)^{-1} \\boldsymbol{s}_{i}^{\\prime}\\), which is idempotent. The within transformations can be written as \\(\\dot{\\boldsymbol{Y}}_{i}=\\boldsymbol{M}_{i} \\boldsymbol{Y}_{i}\\) and \\(\\dot{\\boldsymbol{X}}_{i}=\\boldsymbol{M}_{i} \\boldsymbol{X}_{i}\\). They have the property that if \\(s_{i t}=0\\) (so that time period \\(t\\) is missing) then the \\(t^{t h}\\) element of \\(\\dot{\\boldsymbol{Y}}_{i}\\) and the \\(t^{t h}\\) row of \\(\\dot{\\boldsymbol{X}}_{i}\\) are all zeros. The missing observations have been replaced by zeros. Consequently, they do not appear in matrix products and sums.\nThe fixed effects estimator of \\(\\beta\\) based on the observed sample is\n\\[\n\\widehat{\\beta}_{\\mathrm{fe}}=\\left(\\sum_{i=1}^{N} \\dot{\\boldsymbol{X}}_{i}^{\\prime} \\dot{\\boldsymbol{X}}_{i}\\right)^{-1}\\left(\\sum_{i=1}^{N} \\dot{\\boldsymbol{X}}_{i}^{\\prime} \\dot{\\boldsymbol{Y}}_{i}\\right) .\n\\]\nCentered and normalized,\n\\[\n\\sqrt{N}\\left(\\widehat{\\beta}_{\\mathrm{fe}}-\\beta\\right)=\\left(\\frac{1}{N} \\sum_{i=1}^{N} \\dot{\\boldsymbol{X}}_{i}^{\\prime} \\dot{\\boldsymbol{X}}_{i}\\right)^{-1}\\left(\\frac{1}{\\sqrt{N}} \\sum_{i=1}^{N} \\dot{\\boldsymbol{X}}_{i}^{\\prime} \\varepsilon_{i}\\right)\n\\]\nNotationally this appears to be identical to the case of a balanced panel but the difference is that the within operator \\(\\boldsymbol{M}_{i}\\) incorporates the sample selection induced by the unbalanced panel structure.\nTo derive a distribution theory for \\(\\widehat{\\beta}_{\\text {fe we }}\\) need to be explicit about the stochastic nature of \\(\\boldsymbol{s}_{i}\\). That is, why are some time periods observed and some not? We could take several approaches:\n\nWe could treat \\(s_{i}\\) as fixed (non-random). This is the easiest approach but the most unsatisfactory.\nWe could treat \\(s_{i}\\) as random but independent of \\(\\left(\\boldsymbol{Y}_{i}, \\boldsymbol{X}_{i}\\right)\\). This is known as “missing at random” and is a common assumption used to justify methods with missing observations. It is justified when the reason why observations are not observed is independent of the observations. This is appropriate, for example, in panel data sets where individuals enter and exit in “waves”. The statistical treatment is not substantially different from the case of fixed \\(s_{i}\\).\nWe could treat \\(\\left(\\boldsymbol{Y}_{i}, \\boldsymbol{X}_{i}, \\boldsymbol{s}_{i}\\right)\\) as jointly random but impose a condition sufficient for consistent estimation of \\(\\beta\\). This is the approach we take below. The condition turns out to be a form of mean independence. The advantage of this approach is that it is less restrictive than full independence. The disadvantage is that we must use a conditional mean restriction rather than uncorrelatedness to identify the coefficients.\n\nThe specific assumptions we impose are as follows.\nAssumption 17.3\n\n\\(Y_{i t}=X_{i t}^{\\prime} \\beta+u_{i}+\\varepsilon_{i t}\\) for \\(i=1, \\ldots, N\\) with \\(T_{i} \\geq 2\\).\nThe variables \\(\\left(\\boldsymbol{\\varepsilon}_{i}, \\boldsymbol{X}_{i}, \\boldsymbol{s}_{i}\\right), i=1, \\ldots, N\\), are independent and identically distributed.\n\\(\\mathbb{E}\\left[\\varepsilon_{i t} \\mid \\boldsymbol{X}_{i}, s_{i}\\right]=0\\).\n\\(\\boldsymbol{Q}_{T}=\\mathbb{E}\\left[\\dot{\\boldsymbol{X}}_{i}^{\\prime} \\dot{\\boldsymbol{X}}_{i}\\right]>0\\).\n\\(\\mathbb{E}\\left[\\varepsilon_{i t}^{4}\\right]<\\infty\\).\n\\(\\mathbb{E}\\left\\|X_{i t}\\right\\|^{4}<\\infty\\).\n\nThe primary difference with Assumption \\(17.2\\) is that we have strengthened strict exogeneity to strict mean independence. This imposes that the regression model is properly specified and that selection does not affect the mean of \\(\\varepsilon_{i t}\\). It is less restrictive than full independence since \\(\\boldsymbol{s}_{i}\\) can affect other moments of \\(\\varepsilon_{i t}\\) and more importantly does not restrict the joint dependence between \\(\\boldsymbol{s}_{i}\\) and \\(\\boldsymbol{X}_{i}\\).\nGiven the above development it is straightforward to establish asymptotic normality.\nTheorem 17.3 Under Assumption 17.3, as \\(N \\rightarrow \\infty, \\sqrt{N}\\left(\\widehat{\\beta}_{\\mathrm{fe}}-\\beta\\right) \\underset{d}{\\longrightarrow} \\mathrm{N}\\left(0, \\boldsymbol{V}_{\\beta}\\right)\\) where \\(\\boldsymbol{V}_{\\beta}=\\boldsymbol{Q}_{T}^{-1} \\Omega_{T} \\boldsymbol{Q}_{T}^{-1}\\) and \\(\\Omega_{T}=\\mathbb{E}\\left[\\dot{\\boldsymbol{X}}_{i}^{\\prime} \\boldsymbol{\\varepsilon}_{i} \\boldsymbol{\\varepsilon}_{i}^{\\prime} \\dot{\\boldsymbol{X}}_{i}\\right]\\). We now prove Theorem 17.3. The assumptions imply that the variables \\(\\left(\\dot{\\boldsymbol{X}}_{i}, \\boldsymbol{\\varepsilon}_{i}\\right)\\) are i.i.d. across \\(i\\) and have finite fourth moments. By the WLLN\n\\[\n\\frac{1}{N} \\sum_{i=1}^{N} \\dot{\\boldsymbol{X}}_{i}^{\\prime} \\dot{\\boldsymbol{X}}_{i} \\underset{p}{\\longrightarrow} \\mathbb{E}\\left[\\dot{\\boldsymbol{X}}_{i}^{\\prime} \\dot{\\boldsymbol{X}}_{i}\\right]=\\boldsymbol{Q}_{T} .\n\\]\nThe random vectors \\(\\dot{\\boldsymbol{X}}_{i}^{\\prime} \\boldsymbol{\\varepsilon}_{i}\\) are i.i.d. The matrix \\(\\dot{\\boldsymbol{X}}_{i}\\) is a function of \\(\\left(\\boldsymbol{X}_{i}, \\boldsymbol{s}_{i}\\right)\\) only. Assumption 17.3.3 and the law of iterated expectations implies\n\\[\n\\mathbb{E}\\left[\\dot{\\boldsymbol{X}}_{i}^{\\prime} \\boldsymbol{\\varepsilon}_{i}\\right]=\\mathbb{E}\\left[\\dot{\\boldsymbol{X}}_{i}^{\\prime} \\mathbb{E}\\left[\\boldsymbol{\\varepsilon}_{i} \\mid \\boldsymbol{X}_{i}, \\boldsymbol{s}_{i}\\right]\\right]=0 .\n\\]\nso that \\(\\dot{\\boldsymbol{X}}_{i}^{\\prime} \\varepsilon_{i}\\) is mean zero. Assumptions 17.3.5 and 17.3.6 and the fact that \\(\\boldsymbol{s}_{i}\\) is bounded implies that \\(\\dot{\\boldsymbol{X}}_{i}^{\\prime} \\varepsilon_{i}\\) has a finite covariance matrix, which is \\(\\Omega_{T}\\). The assumptions for the CLT hold, thus\n\\[\n\\frac{1}{\\sqrt{N}} \\sum_{i=1}^{N} \\dot{\\boldsymbol{X}}_{i}^{\\prime} \\varepsilon_{i} \\underset{d}{\\longrightarrow} \\mathrm{N}\\left(0, \\Omega_{T}\\right)\n\\]\nTogether we obtain the stated result."
  },
  {
    "objectID": "chpt17-panel-data.html#heteroskedasticity-robust-covariance-matrix-estimation",
    "href": "chpt17-panel-data.html#heteroskedasticity-robust-covariance-matrix-estimation",
    "title": "16  Panel Data",
    "section": "16.22 Heteroskedasticity-Robust Covariance Matrix Estimation",
    "text": "16.22 Heteroskedasticity-Robust Covariance Matrix Estimation\nWe have introduced two covariance matrix estimators for the fixed effects estimator. The classical estimator (17.36) is appropriate for the case where the idiosyncratic errors \\(\\varepsilon_{i t}\\) are homoskedastic and serially uncorrelated. The cluster-robust estimator (17.38) allows for heteroskedasticity and arbitrary serial correlation. In this and the following section we consider the intermediate case where \\(\\varepsilon_{i t}\\) is heteroskedastic but serially uncorrelated.\nAssume that (17.18) and (17.26) hold but not necessarily (17.25). Define the conditional variances\n\\[\n\\mathbb{E}\\left[\\varepsilon_{i t}^{2} \\mid \\boldsymbol{X}_{i}\\right]=\\sigma_{i t}^{2} .\n\\]\nThen \\(\\Sigma_{i}=\\mathbb{E}\\left[\\boldsymbol{\\varepsilon}_{i} \\boldsymbol{\\varepsilon}_{i}^{\\prime} \\mid \\boldsymbol{X}_{i}\\right]=\\operatorname{diag}\\left(\\sigma_{i t}^{2}\\right)\\). The covariance matrix (17.24) can be written as\n\\[\n\\boldsymbol{V}_{\\mathrm{fe}}=\\left(\\dot{\\boldsymbol{X}}^{\\prime} \\dot{\\boldsymbol{X}}\\right)^{-1}\\left(\\sum_{i=1}^{N} \\sum_{t \\in S_{i}} \\dot{X}_{i t} \\dot{X}_{i t}^{\\prime} \\sigma_{i t}^{2}\\right)\\left(\\dot{\\boldsymbol{X}}^{\\prime} \\dot{\\boldsymbol{X}}\\right)^{-1}\n\\]\nA natural estimator of \\(\\sigma_{i t}^{2}\\) is \\(\\widehat{\\varepsilon}_{i t}^{2}\\). Replacing \\(\\sigma_{i t}^{2}\\) with \\(\\widehat{\\varepsilon}_{i t}^{2}\\) in (17.56) and making a degree-of-freedom adjustment we obtain a White-type covariance matrix estimator\n\\[\n\\widehat{\\boldsymbol{V}}_{\\mathrm{fe}}=\\frac{n}{n-N-k}\\left(\\dot{\\boldsymbol{X}}^{\\prime} \\dot{\\boldsymbol{X}}\\right)^{-1}\\left(\\sum_{i=1}^{N} \\sum_{t \\in S_{i}} \\dot{X}_{i t} \\dot{X}_{i t}^{\\prime} \\widehat{\\varepsilon}_{i t}^{2}\\right)\\left(\\dot{\\boldsymbol{X}}^{\\prime} \\dot{\\boldsymbol{X}}\\right)^{-1} .\n\\]\nFollowing the insight of White (1980) it may seem appropriate to expect \\(\\widehat{\\boldsymbol{V}}_{\\text {fe }}\\) to be a reasonable estimator of \\(\\boldsymbol{V}_{\\text {fe. }}\\). Unfortunately this is not the case as discovered by Stock and Watson (2008). The problem is that \\(\\widehat{\\boldsymbol{V}}_{\\mathrm{fe}}\\) is a function of the individual-specific means \\(\\bar{\\varepsilon}_{i}\\) which are negligible only if the number of time series observations \\(T_{i}\\) are large.\nWe can see this by a simple bias calculation. Assume that the sample is balanced and that the residuals are constructed with the true \\(\\beta\\). Then\n\\[\n\\widehat{\\varepsilon}_{i t}=\\dot{\\varepsilon}_{i t}=\\varepsilon_{i t}-\\frac{1}{T} \\sum_{t=1}^{T} \\varepsilon_{i j} .\n\\]\nUsing (17.26) and (17.55)\n\\[\n\\mathbb{E}\\left[\\widehat{\\varepsilon}_{i t}^{2} \\mid \\boldsymbol{X}_{i}\\right]=\\left(\\frac{T-2}{T}\\right) \\sigma_{i t}^{2}+\\frac{\\bar{\\sigma}_{i}^{2}}{T}\n\\]\nwhere \\(\\bar{\\sigma}_{i}^{2}=T^{-1} \\sum_{t=1}^{T} \\sigma_{i t}^{2}\\). (See Exercise 17.10.) Using (17.57) and setting \\(k=0\\) we obtain\n\\[\n\\begin{aligned}\n\\mathbb{E}\\left[\\widehat{\\boldsymbol{V}}_{\\mathrm{fe}} \\mid \\boldsymbol{X}\\right] &=\\frac{T}{T-1}\\left(\\dot{\\boldsymbol{X}}^{\\prime} \\dot{\\boldsymbol{X}}\\right)^{-1}\\left(\\sum_{i=1}^{N} \\sum_{t \\in S_{i}} \\dot{X}_{i t} \\dot{X}_{i t}^{\\prime} \\mathbb{E}\\left[\\widehat{\\varepsilon}_{i t}^{2} \\mid \\boldsymbol{X}_{i}\\right]\\right)\\left(\\dot{\\boldsymbol{X}}^{\\prime} \\dot{\\boldsymbol{X}}\\right)^{-1} \\\\\n&=\\left(\\frac{T-2}{T-1}\\right) \\boldsymbol{V}_{\\mathrm{fe}}+\\frac{1}{T-1}\\left(\\dot{\\boldsymbol{X}}^{\\prime} \\dot{\\boldsymbol{X}}\\right)^{-1}\\left(\\sum_{i=1}^{N} \\dot{\\boldsymbol{X}}_{i}^{\\prime} \\dot{\\boldsymbol{X}}_{i} \\bar{\\sigma}_{i}^{2}\\right)\\left(\\dot{\\boldsymbol{X}}^{\\prime} \\dot{\\boldsymbol{X}}\\right)^{-1} .\n\\end{aligned}\n\\]\nThus \\(\\widehat{\\boldsymbol{V}}_{\\mathrm{fe}}\\) is biased of order \\(O\\left(T^{-1}\\right)\\). Unless \\(T \\rightarrow \\infty\\) this bias will persist as \\(N \\rightarrow \\infty . \\widehat{\\boldsymbol{V}}_{\\mathrm{fe}}\\) is unbiased in two contexts. The first is when the errors \\(\\varepsilon_{i t}\\) are homoskedastic. The second is when \\(T=2\\). (To show the latter requires some algebra so is omitted.)\nTo correct the bias for the case \\(T>2\\), Stock and Watson (2008) proposed the estimator\n\\[\n\\begin{aligned}\n\\widetilde{\\boldsymbol{V}}_{\\mathrm{fe}} &=\\left(\\frac{T-1}{T-2}\\right) \\widehat{\\boldsymbol{V}}_{\\mathrm{fe}}-\\frac{1}{T-1} \\widehat{\\boldsymbol{B}}_{\\mathrm{fe}} \\\\\n\\widehat{\\boldsymbol{B}}_{\\mathrm{fe}} &=\\left(\\dot{\\boldsymbol{X}}^{\\prime} \\dot{\\boldsymbol{X}}\\right)^{-1}\\left(\\sum_{i=1}^{N} \\dot{\\boldsymbol{X}}_{i}^{\\prime} \\dot{\\boldsymbol{X}}_{i} \\widehat{\\sigma}_{i}^{2}\\right)\\left(\\dot{\\boldsymbol{X}}^{\\prime} \\dot{\\boldsymbol{X}}\\right)^{-1} \\\\\n\\widehat{\\sigma}_{i}^{2} &=\\frac{1}{T-1} \\sum_{t=1}^{T} \\widehat{\\varepsilon}_{i t}^{2} .\n\\end{aligned}\n\\]\nYou can check that \\(\\mathbb{E}\\left[\\widehat{\\sigma}_{i}^{2} \\mid \\boldsymbol{X}_{i}\\right]=\\bar{\\sigma}_{i}^{2}\\) and \\(\\mathbb{E}\\left[\\widetilde{\\boldsymbol{V}}_{\\text {fe }} \\mid \\boldsymbol{X}_{i}\\right]=\\boldsymbol{V}_{\\text {fe }}\\) so \\(\\widetilde{\\boldsymbol{V}}_{\\text {fe }}\\) is unbiased for \\(\\boldsymbol{V}_{\\text {fe }}\\). (See Exercise 17.11.)\nStock and Watson (2008) show that \\(\\widetilde{\\boldsymbol{V}}_{\\text {fe }}\\) is consistent with \\(T\\) fixed and \\(N \\rightarrow \\infty\\). In simulations they show that \\(\\widetilde{\\boldsymbol{V}}_{\\mathrm{fe}}\\) has excellent performance.\nBecause of the Stock-Watson analysis Stata no longer calculates the heteroskedasticity-robust covariance matrix estimator \\(\\widehat{\\boldsymbol{V}}_{\\mathrm{fe}}\\) when the fixed effects estimator is calculated using the xtreg command. Instead, the cluster-robust estimator \\(\\widehat{\\boldsymbol{V}}_{\\mathrm{fe}}^{\\text {cluster }}\\) is reported when robust standard errors are requested. However, fixed effects is often implemented using the areg command which reports the biased estimator \\(\\widehat{\\boldsymbol{V}}_{\\mathrm{fe}}\\) if robust standard errors are requested. These leads to the practical recommendation that areg should be used with the cluster(id) option.\nAt present the corrected estimator (17.58) has not been programmed as a Stata option."
  },
  {
    "objectID": "chpt17-panel-data.html#heteroskedasticity-robust-estimation---unbalanced-case",
    "href": "chpt17-panel-data.html#heteroskedasticity-robust-estimation---unbalanced-case",
    "title": "16  Panel Data",
    "section": "16.23 Heteroskedasticity-Robust Estimation - Unbalanced Case",
    "text": "16.23 Heteroskedasticity-Robust Estimation - Unbalanced Case\nA limitation with the bias-corrected robust covariance matrix estimator of Stock and Watson (2008) is that it was only derived for balanced panels. In this section we generalize their estimator to cover unbalanced panels.\nThe estimator is\n\\[\n\\begin{aligned}\n&\\widetilde{\\boldsymbol{V}}_{\\mathrm{fe}}=\\left(\\dot{\\boldsymbol{X}}^{\\prime} \\dot{\\boldsymbol{X}}\\right)^{-1} \\widetilde{\\Omega}_{\\mathrm{fe}}\\left(\\dot{\\boldsymbol{X}}^{\\prime} \\dot{\\boldsymbol{X}}\\right)^{-1} \\\\\n&\\widetilde{\\Omega}_{\\mathrm{fe}}=\\sum_{i=1}^{N} \\sum_{t \\in S_{i}} \\dot{X}_{i t} \\dot{X}_{i t}^{\\prime}\\left[\\left(\\frac{T_{i} \\widehat{\\varepsilon}_{i t}^{2}-\\widehat{\\sigma}_{i}^{2}}{T_{i}-2}\\right) \\mathbb{1}\\left\\{T_{i}>2\\right\\}+\\left(\\frac{T_{i} \\widehat{\\varepsilon}_{i t}^{2}}{T_{i}-1}\\right) \\mathbb{1}\\left\\{T_{i}=2\\right\\}\\right]\n\\end{aligned}\n\\]\nwhere\n\\[\n\\widehat{\\sigma}_{i}^{2}=\\frac{1}{T_{i}-1} \\sum_{t \\in S_{i}} \\widehat{\\varepsilon}_{i t}^{2} .\n\\]\nTo justify this estimator, as in the previous section make the simplifying assumption that the residuals are constructed with the true \\(\\beta\\). We calculate that\n\\[\n\\begin{aligned}\n&\\mathbb{E}\\left[\\widehat{\\varepsilon}_{i t}^{2} \\mid \\boldsymbol{X}_{i}\\right]=\\left(\\frac{T_{i}-2}{T_{i}}\\right) \\sigma_{i t}^{2}+\\frac{\\bar{\\sigma}_{i}^{2}}{T_{i}} \\\\\n&\\mathbb{E}\\left[\\widehat{\\sigma}_{i}^{2} \\mid \\boldsymbol{X}_{i}\\right]=\\bar{\\sigma}_{i}^{2} .\n\\end{aligned}\n\\]\nYou can show that under these assumptions, \\(\\mathbb{E}\\left[\\widetilde{\\boldsymbol{V}}_{\\mathrm{fe}} \\mid \\boldsymbol{X}\\right]=\\boldsymbol{V}_{\\mathrm{fe}}\\) and thus \\(\\widetilde{\\boldsymbol{V}}_{\\mathrm{fe}}\\) is unbiased for \\(\\boldsymbol{V}_{\\mathrm{fe}}\\). (See Exercise 17.12.)\nIn balanced panels the estimator \\(\\widetilde{\\boldsymbol{V}}_{\\mathrm{fe}}\\) simplifies to the Stock-Watson estimator (with \\(k=0\\) )."
  },
  {
    "objectID": "chpt17-panel-data.html#hausman-test-for-random-vs-fixed-effects",
    "href": "chpt17-panel-data.html#hausman-test-for-random-vs-fixed-effects",
    "title": "16  Panel Data",
    "section": "16.24 Hausman Test for Random vs Fixed Effects",
    "text": "16.24 Hausman Test for Random vs Fixed Effects\nThe random effects model is a special case of the fixed effects model. Thus we can test the null hypothesis of random effects against the alternative of fixed effects. The Hausman test is typically used for this purpose. The statistic is a quadratic in the difference between the fixed effects and random effects estimators. The statistic is\n\\[\n\\begin{aligned}\nH &=\\left(\\widehat{\\beta}_{\\mathrm{fe}}-\\widehat{\\beta}_{\\mathrm{re}}\\right)^{\\prime} \\widehat{\\operatorname{var}}\\left[\\widehat{\\beta}_{\\mathrm{fe}}-\\widehat{\\beta}_{\\mathrm{re}}\\right]^{-1}\\left(\\widehat{\\beta}_{\\mathrm{fe}}-\\widehat{\\beta}_{\\mathrm{re}}\\right) \\\\\n&=\\left(\\widehat{\\beta}_{\\mathrm{fe}}-\\widehat{\\beta}_{\\mathrm{re}}\\right)^{\\prime}\\left(\\widehat{\\boldsymbol{V}}_{\\mathrm{fe}}-\\widehat{\\boldsymbol{V}}_{\\mathrm{re}}\\right)^{-1}\\left(\\widehat{\\beta}_{\\mathrm{fe}}-\\widehat{\\beta}_{\\mathrm{re}}\\right)\n\\end{aligned}\n\\]\nwhere both \\(\\widehat{\\boldsymbol{V}}_{\\mathrm{fe}}\\) and \\(\\widehat{\\boldsymbol{V}}_{\\mathrm{re}}\\) take the classical (non-robust) form.\nThe test can be implemented on a subset of the coefficients \\(\\beta\\). In particular this needs to be done if the regressors \\(X_{i t}\\) contain time-invariant elements so that the random effects estimator contains more coefficients than the fixed effects estimator. In this case the test should be implemented only on the coefficients on the time-varying regressors.\nAn asymptotic \\(100 \\alpha %\\) test rejects if \\(H\\) exceeds the \\(1-\\alpha^{t h}\\) quantile of the \\(\\chi_{k}^{2}\\) distribution where \\(k=\\) \\(\\operatorname{dim}(\\beta)\\). If the test rejects this is evidence that the individual effect \\(u_{i}\\) is correlated with the regressors so the random effects model is not appropriate. On the other hand if the test fails to reject this evidence says that the random effects hypothesis cannot be rejected.\nIt is tempting to use the Hausman test to select whether to use the fixed effects or random effects estimator. One could imagine using the random effects estimator if the Hausman test fails to reject the random effects hypothesis and using the fixed effects estimator otherwise. This is not, however, a wise approach. This procedure - selecting an estimator based on a test - is known as a pretest estimator and is biased. The bias arises because the result of the test is random and correlated with the estimators.\nInstead, the Hausman test can be used as a specification test. If you are planning to use the random effects estimator (and believe that the random effects assumptions are appropriate in your context) the Hausman test can be used to check this assumption and provide evidence to support your approach."
  },
  {
    "objectID": "chpt17-panel-data.html#random-effects-or-fixed-effects",
    "href": "chpt17-panel-data.html#random-effects-or-fixed-effects",
    "title": "16  Panel Data",
    "section": "16.25 Random Effects or Fixed Effects?",
    "text": "16.25 Random Effects or Fixed Effects?\nWe have presented the random effects and fixed effects estimators of the regression coefficients. Which should be used in practice? How should we view the difference?\nThe basic distinction is that the random effects estimator requires the individual error \\(u_{i}\\) to satisfy the conditional mean assumption (17.8). The fixed effects estimator does not require (17.8) and is robust to its violation. In particular, the individual effect \\(u_{i}\\) can be arbitrarily correlated with the regressors. On the other hand the random effects estimator is efficient under random effects (Assumption 17.1). Current econometric practice is to prefer robustness over efficiency. Consequently, current practice is (nearly uniformly) to use the fixed effects estimator for linear panel data models. Random effects estimators are only used in contexts where fixed effects estimation is unknown or challenging (which occurs in many nonlinear models).\nThe labels “random effects” and “fixed effects” are misleading. These are labels which arose in the early literature and we are stuck with these labels today. In a previous era regressors were viewed as “fixed”. Viewing the individual effect as an unobserved regressor leads to the label of the individual effect as “fixed”. Today, we rarely refer to regressors as “fixed” when dealing with observational data. We view all variables as random. Consequently describing \\(u_{i}\\) as “fixed” does not make much sense and it is hardly a contrast with the “random effect” label since under either assumption \\(u_{i}\\) is treated as random. Once again, the labels are unfortunate but the key difference is whether \\(u_{i}\\) is correlated with the regressors."
  },
  {
    "objectID": "chpt17-panel-data.html#time-trends",
    "href": "chpt17-panel-data.html#time-trends",
    "title": "16  Panel Data",
    "section": "16.26 Time Trends",
    "text": "16.26 Time Trends\nIn general we expect that economic agents will experience common shocks during the same time period. For example, business cycle fluctations, inflation, and interest rates affect all agents in the economy. Therefore it is often desirable to include time effects in a panel regression model.\nThe simplest specification is a linear time trend\n\\[\nY_{i t}=X_{i t}^{\\prime} \\beta+\\gamma t+u_{i}+\\varepsilon_{i t} .\n\\]\nFor a introduction to time trends see Section 14.42. More flexible specifications (such as a quadratic) can also be used. For estimation it is appropriate to include the time trend \\(t\\) as an element of the regressor vector \\(X_{i t}\\) and then apply fixed effects.\nIn some cases the time trends may be individual-specific. Series may be growing or declining at different rates. A linear time trend specification only extracts a common time trend. To allow for individualspecific time trends we need to include an interaction effect. This can be written as\n\\[\nY_{i t}=X_{i t}^{\\prime} \\beta+\\gamma_{i} t+u_{i}+\\varepsilon_{i t} .\n\\]\nIn a fixed effects specification the coefficients \\(\\left(\\gamma_{i}, u_{i}\\right)\\) are treated as possibly correlated with the regressors. To eliminate them from the model we treat them as unknown parameters and estimate all by least squares. By the FWL theorem the estimator for \\(\\beta\\) equals least squares of \\(\\dot{\\boldsymbol{Y}}\\) on \\(\\dot{\\boldsymbol{X}}\\) where their elements are the residuals from the least squares regressions on a linear time trend fit separately for each individual and variable."
  },
  {
    "objectID": "chpt17-panel-data.html#two-way-error-components",
    "href": "chpt17-panel-data.html#two-way-error-components",
    "title": "16  Panel Data",
    "section": "16.27 Two-Way Error Components",
    "text": "16.27 Two-Way Error Components\nIn the previous section we discussed inclusion of time trends and individual-specific time trends. The functional forms imposed by linear time trends are restrictive. There is no economic reason to expect the “trend” of a series to be linear. Business cycle “trends” are cyclic. This suggests that it is desirable to be more flexible than a linear (or polynomial) specifications. In this section we consider the most flexible specification where the trend is allowed to take any arbitrary shape but will require that it is common rather than individual-specific.\nThe model we consider is the two-way error component model\n\\[\nY_{i t}=X_{i t}^{\\prime} \\beta+v_{t}+u_{i}+\\varepsilon_{i t} .\n\\]\nIn this model \\(u_{i}\\) is an unobserved individual-specific effect, \\(v_{t}\\) is an unobserved time-specific effect, and \\(\\varepsilon_{i t}\\) is an idiosyncratic error.\nThe two-way model (17.63) can be handled either using random effects or fixed effects. In a random effects framework the errors \\(v_{t}\\) and \\(u_{i}\\) are modeled as in Assumption 17.1. When the panel is balanced the covariance matrix of the error vector \\(\\boldsymbol{e}=v \\otimes \\mathbf{1}_{N}+\\mathbf{1}_{T} \\otimes u+\\boldsymbol{\\varepsilon}\\) is\n\\[\n\\operatorname{var}[\\boldsymbol{e}]=\\Omega=\\left(\\boldsymbol{I}_{T} \\otimes \\mathbf{1}_{N} \\mathbf{1}_{N}^{\\prime}\\right) \\sigma_{v}^{2}+\\left(\\mathbf{1}_{T} \\mathbf{1}_{T}^{\\prime} \\otimes \\boldsymbol{I}_{N}\\right) \\sigma_{u}^{2}+\\boldsymbol{I}_{n} \\sigma_{\\varepsilon}^{2} .\n\\]\nWhen the panel is unbalanced a similar but cumbersome expression for (17.64) can be derived. This variance (17.64) can be used for GLS estimation of \\(\\beta\\).\nMore typically (17.63) is handled using fixed effects. The two-way within transformation subtracts both individual-specific means and time-specific means to eliminate both \\(v_{t}\\) and \\(u_{i}\\) from the two-way model (17.63). For a variable \\(Y_{i t}\\) we define the time-specific mean as follows. Let \\(S_{t}\\) be the set of individuals \\(i\\) for which the observation \\(t\\) is included in the sample and let \\(N_{t}\\) be the number of these individuals. Then the time-specific mean at time \\(t\\) is\n\\[\n\\widetilde{Y}_{t}=\\frac{1}{N_{t}} \\sum_{i \\in S_{t}} Y_{i t} .\n\\]\nThis is the average across all values of \\(Y_{i t}\\) observed at time \\(t\\).\nFor the case of balanced panels the two-way within transformation is\n\\[\n\\ddot{Y}_{i t}=Y_{i t}-\\bar{Y}_{i}-\\widetilde{Y}_{t}+\\bar{Y}\n\\]\nwhere \\(\\bar{Y}=n^{-1} \\sum_{i=1}^{N} \\sum_{t=1}^{T} Y_{i t}\\) is the full-sample mean. If \\(Y_{i t}\\) satisfies the two-way component model\n\\[\nY_{i t}=v_{t}+u_{i}+\\varepsilon_{i t}\n\\]\nthen \\(\\bar{Y}_{i}=\\bar{v}+u_{i}+\\bar{\\varepsilon}_{i}, \\widetilde{Y}_{t}=v_{t}+\\bar{u}+\\widetilde{\\varepsilon}_{t}\\) and \\(\\bar{Y}=\\bar{v}+\\bar{u}+\\bar{\\varepsilon}\\). Hence\n\\[\n\\begin{aligned}\n\\ddot{Y}_{i t} &=v_{t}+u_{i}+\\varepsilon_{i t}-\\left(\\bar{v}+u_{i}+\\bar{\\varepsilon}_{i}\\right)-\\left(v_{t}+\\bar{u}+\\widetilde{\\varepsilon}_{t}\\right)+\\bar{v}+\\bar{u}+\\bar{\\varepsilon} \\\\\n&=\\varepsilon_{i t}-\\bar{\\varepsilon}_{i}-\\widetilde{\\varepsilon}_{t}+\\bar{\\varepsilon}=\\ddot{\\varepsilon}_{i t}\n\\end{aligned}\n\\]\nso the individual and time effects are eliminated.\nThe two-way within transformation applied to (17.63) yields\n\\[\n\\ddot{Y}_{i t}=\\ddot{X}_{i t}^{\\prime} \\beta+\\ddot{\\varepsilon}_{i t}\n\\]\nwhich is invariant to both \\(v_{t}\\) and \\(u_{i}\\). The two-way within estimator is least squares applied to (17.66).\nFor the unbalanced case there are two computational approaches to implement the estimator. Both are based on the realization that the estimator is equivalent to including dummy variables for all time periods. Let \\(\\tau_{t}\\) be a set of \\(T\\) dummy variables where the \\(t^{t h}\\) indicates the \\(t^{t h}\\) time period. Thus the \\(t^{t h}\\) element of \\(\\tau_{t}\\) is 1 and the remaining elements are zero. Set \\(v=\\left(\\nu_{1}, \\ldots, \\nu_{T}\\right)^{\\prime}\\) as the vector of time fixed effects. Notice that \\(v_{t}=\\tau_{t}^{\\prime} \\nu\\). We can write the two-way model as\n\\[\nY_{i t}=X_{i t}^{\\prime} \\beta+\\tau_{t}^{\\prime} \\nu+u_{i}+\\varepsilon_{i t} .\n\\]\nThis is the dummy variable representation of the two-way error components model.\nModel (17.67) can be estimated by one-way fixed effects with regressors \\(X_{i t}\\) and \\(\\tau_{t}\\) and coefficient vectors \\(\\beta\\) and \\(\\nu\\). This can be implemented by standard one-way fixed effects methods including xtreg or areg in Stata. This produces estimates of the slopes \\(\\beta\\) as well as the time effects \\(\\nu\\). To achieve identification one time dummy variable is omitted from \\(\\tau_{t}\\) so the estimated time effects are all relative to this baseline time period. This is the most common method in practice to estimate a two-way fixed effects model. As the number of time periods is typically modest this is a computationally attractive approach.\nThe second computational approach is to eliminate the time effects by residual regression. This is done by the following steps. First, subtract individual-specific means for (17.67). This yields\n\\[\n\\dot{Y}_{i t}=\\dot{X}_{i t}^{\\prime} \\beta+\\dot{\\tau}_{t}^{\\prime} v+\\dot{\\varepsilon}_{i t} .\n\\]\nSecond, regress \\(\\dot{Y}_{i t}\\) on \\(\\dot{\\tau}_{t}\\) to obtain a residual \\(\\ddot{Y}_{i t}\\) and regress each element of \\(\\dot{X}_{i t}\\) on \\(\\dot{\\tau}_{t}\\) to obtain a residual \\(\\ddot{X}_{i t}\\). Third, regress \\(\\ddot{Y}_{i t}\\) on \\(\\ddot{X}_{i t}\\) to obtain the within estimator of \\(\\beta\\). These steps eliminate the fixed effects \\(v_{t}\\) so the estimator is invariant to their value. What is important about this two-step procedure is that the second step is not a within transformation across the time index but rather standard regression.\nIf the two-way within estimator is used then the regressors \\(X_{i t}\\) cannot include any time-invariant variables \\(X_{i}\\) or common time series variables \\(X_{t}\\). Both are eliminated by the two-way within transformation. Coefficients are only identified for regressors which have variation both across individuals and across time.\nIf desired, the relevance of the time effects can be tested by an exclusion test on the coefficients \\(\\nu\\). If the test rejects the hypothesis of zero coefficients then this indicates that the time effects are relevant in the regression model.\nThe fixed effects estimator of (17.63) is invariant to the values of \\(v_{t}\\) and \\(u_{i}\\), thus no assumptions need to be made concerning their stochastic properties.\nTo illustrate, the fourth column of Table \\(17.2\\) presents fixed effects estimates of the investment equation, augmented to included year dummy indicators, and is thus a two-way fixed effects model. In this example the coefficient estimates and standard errors are not greatly affected by the inclusion of the year dummy variables."
  },
  {
    "objectID": "chpt17-panel-data.html#instrumental-variables",
    "href": "chpt17-panel-data.html#instrumental-variables",
    "title": "16  Panel Data",
    "section": "16.28 Instrumental Variables",
    "text": "16.28 Instrumental Variables\nTake the fixed effects model\n\\[\nY_{i t}=X_{i t}^{\\prime} \\beta+u_{i}+\\varepsilon_{i t} .\n\\]\nWe say \\(X_{i t}\\) is exogenous for \\(\\varepsilon_{i t}\\) if \\(\\mathbb{E}\\left[X_{i t} \\varepsilon_{i t}\\right]=0\\), and we say \\(X_{i t}\\) is endogenous for \\(\\varepsilon_{i t}\\) if \\(\\mathbb{E}\\left[X_{i t} \\varepsilon_{i t}\\right] \\neq 0\\). In Chapter 12 we discussed several economic examples of endogeneity and the same issues apply in the panel data context. The primary difference is that in the fixed effects model we only need to be concerned if the regressors are correlated with the idiosyncratic error \\(\\varepsilon_{i t}\\), as correlation between \\(X_{i t}\\) and \\(u_{i}\\) is allowed.\nAs in Chapter 12 if the regressors are endogenous the fixed effects estimator will be biased and inconsistent for the structural coefficient \\(\\beta\\). The standard approach to handling endogeneity is to specify instrumental variables \\(Z_{i t}\\) which are both relevant (correlated with \\(X_{i t}\\) ) yet exogenous (uncorrelated with \\(\\varepsilon_{i t}\\) ).\nLet \\(Z_{i t}\\) be an \\(\\ell \\times 1\\) instrumental variable where \\(\\ell \\geq k\\). As in the cross-section case, \\(Z_{i t}\\) may contain both included exogenous variables (variables in \\(X_{i t}\\) that are exogenous) and excluded exogenous variables (variables not in \\(X_{i t}\\) ). Let \\(\\boldsymbol{Z}_{i}\\) be the stacked instruments by individual and \\(\\boldsymbol{Z}\\) be the stacked instruments for the full sample.\nThe dummy variable formulation of the fixed effects model is \\(Y_{i t}=X_{i t}^{\\prime} \\beta+d_{i}^{\\prime} u+\\varepsilon_{i t}\\) where \\(d_{i}\\) is an \\(N \\times 1\\) vector of dummy variables, one for each individual in the sample. The model in matrix notation for the full sample is\n\\[\n\\boldsymbol{Y}=\\boldsymbol{X} \\beta+\\boldsymbol{D} u+\\boldsymbol{\\varepsilon} .\n\\]\nTheorem \\(17.1\\) shows that the fixed effects estimator for \\(\\beta\\) can be calculated by least squares estimation of (17.69). Thus the dummies \\(\\boldsymbol{D}\\) should be viewed as included exogenous variables. Consider 2SLS estimation of \\(\\beta\\) using the instruments \\(\\boldsymbol{Z}\\) for \\(\\boldsymbol{X}\\). Since \\(\\boldsymbol{D}\\) is an included exogenous variable it should also be used as an instrument. Thus 2SLS estimation of the fixed effects model (17.68) is algebraically 2SLS of the regression (17.69) of \\(\\boldsymbol{Y}\\) on \\((\\boldsymbol{X}, \\boldsymbol{D})\\) using the pair \\((\\boldsymbol{Z}, \\boldsymbol{D})\\) as instruments.\nSince the dimension of \\(\\boldsymbol{D}\\) can be excessively large, as discussed in Section 17.11, it is advisable to use residual regression to compute the 2SLS estimator as we now describe.\nIn Section 12.12, we described several alternative representations for the 2SLS estimator. The fifth (equation (12.32)) shows that the 2SLS estimator for \\(\\beta\\) equals\n\\[\n\\widehat{\\beta}_{2 \\text { sls }}=\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{M}_{\\boldsymbol{D}} \\boldsymbol{Z}\\left(\\boldsymbol{Z}^{\\prime} \\boldsymbol{M}_{\\boldsymbol{D}} \\boldsymbol{Z}\\right)^{-1} \\boldsymbol{Z}^{\\prime} \\boldsymbol{M}_{\\boldsymbol{D}} \\boldsymbol{X}\\right)^{-1}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{M}_{\\boldsymbol{D}} \\boldsymbol{Z}\\left(\\boldsymbol{Z}^{\\prime} \\boldsymbol{M}_{\\boldsymbol{D}} \\boldsymbol{Z}\\right)^{-1} \\boldsymbol{Z}^{\\prime} \\boldsymbol{M}_{\\boldsymbol{D}} \\boldsymbol{Y}\\right)\n\\]\nwhere \\(\\boldsymbol{M}_{\\boldsymbol{D}}=\\boldsymbol{I}_{n}-\\boldsymbol{D}\\left(\\boldsymbol{D}^{\\prime} \\boldsymbol{D}\\right)^{-1} \\boldsymbol{D}^{\\prime}\\). The latter is the matrix within operator, thus \\(\\boldsymbol{M}_{\\boldsymbol{D}} \\boldsymbol{Y}=\\dot{\\boldsymbol{Y}}, \\boldsymbol{M}_{\\boldsymbol{D}} \\boldsymbol{X}=\\dot{\\boldsymbol{X}}\\), and \\(\\boldsymbol{M}_{\\boldsymbol{D}} \\boldsymbol{Z}=\\dot{Z}\\). It follows that the 2SLS estimator is\n\\[\n\\widehat{\\beta}_{2 \\text { sls }}=\\left(\\dot{\\boldsymbol{X}}^{\\prime} \\dot{Z}\\left(\\dot{Z}^{\\prime} \\dot{Z}\\right)^{-1} \\dot{Z}^{\\prime} \\dot{\\boldsymbol{X}}\\right)^{-1}\\left(\\dot{\\boldsymbol{X}}^{\\prime} \\dot{Z}\\left(\\dot{\\boldsymbol{Z}}^{\\prime} \\dot{Z}\\right)^{-1} \\dot{Z}^{\\prime} \\dot{\\boldsymbol{Y}}\\right) .\n\\]\nThis is convenient. It shows that the 2SLS estimator for the fixed effects model can be calculated by applying 2SLS to the within-transformed \\(Y_{i t}, X_{i t}\\), and \\(Z_{i t}\\). The 2SLS residuals are \\(\\widehat{\\boldsymbol{e}}=\\dot{\\boldsymbol{Y}}-\\dot{\\boldsymbol{X}} \\widehat{\\beta}_{2 s l s}\\).\nThis estimator can be obtained using the Stata command xtivreg fe. It can also be obtained using the Stata command ivregress after making the within transformations.\nThe presentation above focused for clarity on the one-way fixed effects model. There is no substantial change in the two-way fixed effects model\n\\[\nY_{i t}=X_{i t}^{\\prime} \\beta+u_{i}+v_{t}+\\varepsilon_{i t} .\n\\]\nThe easiest way to estimate the two-way model is to add \\(T-1\\) time-period dummies to the regression model and include these dummy variables as both regressors and instruments."
  },
  {
    "objectID": "chpt17-panel-data.html#identification-with-instrumental-variables",
    "href": "chpt17-panel-data.html#identification-with-instrumental-variables",
    "title": "16  Panel Data",
    "section": "16.29 Identification with Instrumental Variables",
    "text": "16.29 Identification with Instrumental Variables\nTo understand the identification of the structural slope coefficient \\(\\beta\\) in the fixed effects model it is necessary to examine the reduced form equation for the endogenous regressors \\(X_{i t}\\). This is\n\\[\nX_{i t}=\\Gamma Z_{i t}+W_{i}+\\zeta_{i t}\n\\]\nwhere \\(W_{i}\\) is a \\(k \\times 1\\) vector of fixed effects for the \\(k\\) regressors and \\(\\zeta_{i t}\\) is an idiosyncratic error.\nThe coefficient matrix \\(\\Gamma\\) is the linear effect of \\(Z_{i t}\\) on \\(X_{i t}\\) holding the fixed effects \\(W_{i}\\) constant. Thus \\(\\Gamma\\) has a similar interpretation as the coefficient \\(\\beta\\) in the fixed effects regression model. It is the effect of the variation in \\(Z_{i t}\\) about its individual-specific mean on \\(X_{i t}\\).\nThe 2SLS estimator is a function of the within transformed variables. Applying the within transformation to the reduced form we find \\(\\dot{X}_{i t}=\\Gamma \\dot{Z}_{i t}+\\dot{\\zeta}_{i t}\\). This shows that \\(\\Gamma\\) is the effect of the within-transformed instruments on the regressors. If there is no time-variation in the within-transformed instruments or there is no correlation between the instruments and the regressors after removing the individual-specific means then the coefficient \\(\\Gamma\\) will be either not identified or singular. In either case the coefficient \\(\\beta\\) will not be identified.\nThus for identification of the fixed effects instrumental variables model we need\n\\[\n\\mathbb{E}\\left[\\dot{Z}_{i}^{\\prime} \\dot{Z}_{i}\\right]>0\n\\]\nand\n\\[\n\\operatorname{rank}\\left(\\mathbb{E}\\left[\\dot{\\boldsymbol{Z}}_{i}^{\\prime} \\dot{\\boldsymbol{X}}_{i}\\right]\\right)=k .\n\\]\nCondition (17.70) is the same as the condition for identification in fixed effects regression - the instruments must have full variation after the within transformation. Condition (17.71) is analogous to the relevance condition for identification of instrumental variable regression in the cross-section context but applies to the within-transformed instruments and regressors.\nCondition (17.71) shows that to examine instrument validity in the context of fixed effects 2SLS it is important to estimate the reduced form equation using fixed effects (within) regression. Standard tests for instrument validity ( \\(F\\) tests on the excluded instruments) can be applied. However, since the correlation structure of the reduced form equation is in general unknown it is appropriate to use a cluster-robust covariance matrix, clustered at the level of the individual."
  },
  {
    "objectID": "chpt17-panel-data.html#asymptotic-distribution-of-fixed-effects-2sls-estimator",
    "href": "chpt17-panel-data.html#asymptotic-distribution-of-fixed-effects-2sls-estimator",
    "title": "16  Panel Data",
    "section": "16.30 Asymptotic Distribution of Fixed Effects 2SLS Estimator",
    "text": "16.30 Asymptotic Distribution of Fixed Effects 2SLS Estimator\nIn this section we present an asymptotic distribution theory for the fixed effects estimator. We provide a formal theory for the case of balanced panels and discuss an extension to the unbalanced case.\nWe use the following assumptions for balanced panels.\nAssumption $17.4\n\n\\(Y_{i t}=X_{i t}^{\\prime} \\beta+u_{i}+\\varepsilon_{i t}\\) for \\(i=1, \\ldots, N\\) and \\(t=1, \\ldots, T\\) with \\(T \\geq 2\\).\nThe variables \\(\\left(\\boldsymbol{\\varepsilon}_{i}, \\boldsymbol{X}_{i}, \\boldsymbol{Z}_{i}\\right), i=1, \\ldots, N\\), are independent and identically distributed.\n\\(\\mathbb{E}\\left[Z_{i s} \\varepsilon_{i t}\\right]=0\\) for all \\(s=1, \\ldots, T\\).\n\\(\\boldsymbol{Q}_{Z Z}=\\mathbb{E}\\left[\\dot{Z}_{i}^{\\prime} \\dot{Z}_{i}\\right]>0\\).\n\\(\\operatorname{rank}\\left(\\boldsymbol{Q}_{Z X}\\right)=k\\) where \\(\\boldsymbol{Q}_{Z X}=\\mathbb{E}\\left[\\dot{\\boldsymbol{Z}}_{i}^{\\prime} \\dot{\\boldsymbol{X}}_{i}\\right]\\).\n\\(\\mathbb{E}\\left[\\varepsilon_{i t}^{4}\\right]<\\infty\\).\n\\(\\mathbb{E}\\left\\|X_{i t}\\right\\|^{2}<\\infty\\).\n\\(\\mathbb{E}\\left\\|Z_{i t}\\right\\|^{4}<\\infty\\).\n\nGiven Assumption \\(17.4\\) we can establish asymptotic normality for \\(\\widehat{\\beta}_{2 s l s}\\).\nTheorem 17.4 Under Assumption 17.4, as \\(N \\rightarrow \\infty, \\sqrt{N}\\left(\\widehat{\\beta}_{2 s l s}-\\beta\\right) \\underset{d}{\\longrightarrow} \\mathrm{N}\\left(0, \\boldsymbol{V}_{\\beta}\\right)\\) where\n\\[\n\\begin{aligned}\n\\boldsymbol{V}_{\\beta} &=\\left(\\boldsymbol{Q}_{Z X}^{\\prime} \\Omega_{Z Z}^{-1} \\boldsymbol{Q}_{Z X}\\right)^{-1}\\left(\\boldsymbol{Q}_{Z X}^{\\prime} \\Omega_{Z Z}^{-1} \\Omega_{Z \\varepsilon} \\Omega_{Z Z}^{-1} \\boldsymbol{Q}_{Z X}\\right)\\left(\\boldsymbol{Q}_{Z X}^{\\prime} \\Omega_{Z Z}^{-1} \\boldsymbol{Q}_{Z X}\\right)^{-1} \\\\\n\\Omega_{Z \\varepsilon} &=\\mathbb{E}\\left[\\dot{\\boldsymbol{Z}}_{i}^{\\prime} \\boldsymbol{\\varepsilon}_{i} \\boldsymbol{\\varepsilon}_{i}^{\\prime} \\dot{\\boldsymbol{Z}}_{i}\\right] .\n\\end{aligned}\n\\]\nThe proof of the result is similar to Theorem \\(17.2\\) so is omitted. The key condition is Assumption 17.4.3, which states that the instruments are strictly exogenous for the idiosyncratic errors. The identification conditions are Assumptions 17.4.4 and 17.4.5, which were discussed in the previous section.\nThe theorem is stated for balanced panels. For unbalanced panels we can modify the theorem as in Theorem \\(17.3\\) by adding the selection indicators \\(\\boldsymbol{s}_{i}\\) and replacing Assumption \\(17.4 .3\\) with \\(\\mathbb{E}\\left[\\varepsilon_{i t} \\mid \\boldsymbol{Z}_{i}, \\boldsymbol{s}_{i}\\right]=\\) 0 , which states that the idiosyncratic errors are mean independent of the instruments and selection.\nIf the idiosyncratic errors \\(\\varepsilon_{i t}\\) are homoskedastic and serially uncorrelated then the covariance matrix simplifies to\n\\[\n\\boldsymbol{V}_{\\beta}=\\left(\\boldsymbol{Q}_{Z X}^{\\prime} \\Omega_{Z Z}^{-1} \\boldsymbol{Q}_{Z X}\\right)^{-1} \\sigma_{\\varepsilon}^{2} .\n\\]\nIn this case a classical homoskedastic covariance matrix estimator can be used. Otherwise a clusterrobust covariance matrix estimator can be used, and takes the form\n\\[\n\\begin{aligned}\n\\widehat{\\boldsymbol{V}}_{\\widehat{\\beta}} &=\\left(\\dot{\\boldsymbol{X}}^{\\prime} \\dot{\\boldsymbol{Z}}\\left(\\dot{\\boldsymbol{Z}}^{\\prime} \\dot{\\boldsymbol{Z}}\\right)^{-1} \\dot{\\boldsymbol{Z}}^{\\prime} \\dot{\\boldsymbol{X}}\\right)^{-1}\\left(\\dot{\\boldsymbol{X}}^{\\prime} \\dot{\\boldsymbol{Z}}\\right)\\left(\\dot{\\boldsymbol{Z}}^{\\prime} \\dot{\\boldsymbol{Z}}\\right)^{-1}\\left(\\sum_{i=1}^{N} \\dot{\\boldsymbol{Z}}_{i}^{\\prime} \\widehat{\\boldsymbol{\\varepsilon}}_{i} \\widehat{\\boldsymbol{\\varepsilon}}_{i}^{\\prime} \\dot{\\boldsymbol{Z}}_{i}\\right) \\\\\n& \\times\\left(\\dot{\\boldsymbol{Z}}^{\\prime} \\dot{\\boldsymbol{Z}}\\right)^{-1}\\left(\\dot{\\boldsymbol{Z}}^{\\prime} \\dot{\\boldsymbol{X}}\\right)\\left(\\dot{\\boldsymbol{X}}^{\\prime} \\dot{\\boldsymbol{Z}}\\left(\\dot{\\boldsymbol{Z}}^{\\prime} \\dot{\\boldsymbol{Z}}\\right)^{-1} \\dot{\\boldsymbol{Z}}^{\\prime} \\dot{\\boldsymbol{X}}\\right)^{-1}\n\\end{aligned}\n\\]\nAs for the case of fixed effects regression, the heteroskedasticity-robust covariance matrix estimator is not recommended due to bias when \\(T\\) is small, and a bias-corrected version has not been developed.\nThe Stata command xtivreg, fe by default reports the classical homoskedastic covariance matrix estimator. To obtain a cluster-robust covariance matrix use option vce (robust) orvce (cluster id)."
  },
  {
    "objectID": "chpt17-panel-data.html#linear-gmm",
    "href": "chpt17-panel-data.html#linear-gmm",
    "title": "16  Panel Data",
    "section": "16.31 Linear GMM",
    "text": "16.31 Linear GMM\nConsider the just-identified 2SLS estimator. It solves the equation \\(\\dot{\\boldsymbol{Z}}^{\\prime}(\\dot{\\boldsymbol{Y}}-\\dot{\\boldsymbol{X}} \\beta)=0\\). These are sample analogs of the population moment condition \\(\\mathbb{E}\\left[\\dot{\\boldsymbol{Z}}_{i}^{\\prime}\\left(\\dot{\\boldsymbol{Y}}_{i}-\\dot{\\boldsymbol{X}}_{i} \\beta\\right)\\right]=0\\). These population conditions hold at the true \\(\\beta\\) because \\(\\dot{\\boldsymbol{Z}}^{\\prime} u=\\boldsymbol{Z}^{\\prime} \\boldsymbol{M D} u=0\\) as \\(u\\) lies in the null space of \\(\\boldsymbol{D}\\), and \\(\\mathbb{E}\\left[\\dot{\\boldsymbol{Z}}_{i}^{\\prime} \\boldsymbol{\\varepsilon}\\right]=0\\) is implied by Assumption 17.4.3.\nThe population orthogonality conditions hold in the overidentified case as well. In this case an alternative to 2SLS is GMM. Let \\(\\widehat{\\boldsymbol{W}}\\) be an estimator of \\(\\boldsymbol{W}=\\mathbb{E}\\left[\\dot{\\boldsymbol{Z}}_{i}^{\\prime} \\varepsilon_{i} \\varepsilon_{i}^{\\prime} \\dot{Z}_{i}\\right]\\), for example\n\\[\n\\widehat{\\boldsymbol{W}}=\\frac{1}{N} \\sum_{i=1}^{N} \\dot{\\boldsymbol{Z}}_{i}^{\\prime} \\widehat{\\boldsymbol{\\varepsilon}}_{i} \\widehat{\\boldsymbol{\\varepsilon}}_{i}^{\\prime} \\dot{\\boldsymbol{Z}}_{i}\n\\]\nwhere \\(\\widehat{\\boldsymbol{\\varepsilon}}_{i}\\) are the 2SLS fixed effects residuals. The GMM fixed effects estimator is\n\\[\n\\widehat{\\beta}_{\\mathrm{gmm}}=\\left(\\dot{\\boldsymbol{X}}^{\\prime} \\dot{\\boldsymbol{Z}} \\widehat{\\boldsymbol{W}}^{-1} \\dot{\\boldsymbol{Z}}^{\\prime} \\dot{\\boldsymbol{X}}\\right)^{-1}\\left(\\dot{\\boldsymbol{X}}^{\\prime} \\dot{\\boldsymbol{Z}} \\widehat{\\boldsymbol{W}}^{-1} \\dot{\\boldsymbol{Z}}^{\\prime} \\dot{\\boldsymbol{Y}}\\right) .\n\\]\nThe estimator (17.73)-(17.72) does not have a Stata command but can be obtained by generating the within transformed variables \\(\\dot{\\boldsymbol{X}}, \\dot{Z}\\) and \\(\\dot{\\boldsymbol{Y}}\\), and then estimating by GMM a regression of \\(\\dot{\\boldsymbol{Y}}\\) on \\(\\dot{\\boldsymbol{X}}\\) using \\(\\dot{Z}\\) as instruments using a weight matrix clustered by individual."
  },
  {
    "objectID": "chpt17-panel-data.html#estimation-with-time-invariant-regressors",
    "href": "chpt17-panel-data.html#estimation-with-time-invariant-regressors",
    "title": "16  Panel Data",
    "section": "16.32 Estimation with Time-Invariant Regressors",
    "text": "16.32 Estimation with Time-Invariant Regressors\nOne of the disappointments with the fixed effects estimator is that it cannot estimate the effect of regressors which are time-invariant. They are not identified separately from the fixed effect and are eliminated by the within transformation. In contrast, the random effects estimator allows for time-invariant regressors but does so only by assuming strict exogeneity which is stronger than typically desired in economic applications.\nIt turns out that we can consider an intermediate case which maintains the fixed effects assumptions for the time-varying regressors but uses stronger assumptions on the time-invariant regressors. For our exposition we will denote the time-varying regressors by the \\(k \\times 1\\) vector \\(X_{i t}\\) and the time-invariant regressors by the \\(\\ell \\times 1\\) vector \\(Z_{i}\\).\nConsider the linear regression model\n\\[\nY_{i t}=X_{i t}^{\\prime} \\beta+Z_{i}^{\\prime} \\gamma+u_{i}+\\varepsilon_{i t} .\n\\]\nAt the level of the individual this can be written as\n\\[\n\\boldsymbol{Y}_{i}=\\boldsymbol{X}_{i} \\beta+\\boldsymbol{Z}_{i} \\gamma+\\boldsymbol{\\imath}_{i} u_{i}+\\boldsymbol{\\varepsilon}_{i}\n\\]\nwhere \\(Z_{i}=\\boldsymbol{\\imath}_{i} Z_{i}^{\\prime}\\). For the full sample in matrix notation we can write this as\n\\[\n\\boldsymbol{Y}=\\boldsymbol{X} \\beta+\\boldsymbol{Z} \\gamma+\\boldsymbol{u}+\\boldsymbol{\\varepsilon} .\n\\]\nWe maintain the assumption that the idiosyncratic errors \\(\\varepsilon_{i t}\\) are uncorrelated with both \\(X_{i t}\\) and \\(Z_{i}\\) at all time horizons:\n\\[\n\\begin{aligned}\n\\mathbb{E}\\left[X_{i s} \\varepsilon_{i t}\\right] &=0 \\\\\n\\mathbb{E}\\left[Z_{i} \\varepsilon_{i t}\\right] &=0 .\n\\end{aligned}\n\\]\nIn this section we consider the case where \\(Z_{i}\\) is uncorrelated with the individual-level error \\(u_{i}\\), thus\n\\[\n\\mathbb{E}\\left[Z_{i} u_{i}\\right]=0,\n\\]\nbut the correlation of \\(X_{i t}\\) and \\(u_{i}\\) is left unrestricted. In this context we say that \\(Z_{i}\\) is exogenous with respect to the fixed effect \\(u_{i}\\) while \\(X_{i t}\\) is endogenous with respect to \\(u_{i}\\). Note that this is a different type of endogeneity than considered in the sections on instrumental variables: there endogeneity meant correlation with the idiosyncratic error \\(\\varepsilon_{i t}\\). Here endogeneity means correlation with the fixed effect \\(u_{i}\\).\nWe consider estimation of (17.74) by instrumental variables and thus need instruments which are uncorrelated with the error \\(u_{i}+\\varepsilon_{i t}\\). The time-invariant regressors \\(Z_{i}\\) satisfy this condition due to (17.76) and (17.77), thus\n\\[\n\\mathbb{E}\\left[\\boldsymbol{Z}_{i}^{\\prime}\\left(\\boldsymbol{Y}_{i}-\\boldsymbol{X}_{i} \\beta-\\boldsymbol{Z}_{i} \\gamma\\right)\\right]=0 .\n\\]\nWhile the time-varying regressors \\(X_{i t}\\) are correlated with \\(u_{i}\\) the within transformed variables \\(\\dot{X}_{i t}\\) are uncorrelated with \\(u_{i}+\\varepsilon_{i t}\\) under (17.75), thus\n\\[\n\\mathbb{E}\\left[\\dot{\\boldsymbol{X}}_{i}^{\\prime}\\left(\\boldsymbol{Y}_{i}-\\boldsymbol{X}_{i} \\beta-\\boldsymbol{Z}_{i} \\gamma\\right)\\right]=0 .\n\\]\nTherefore we can estimate \\((\\beta, \\gamma)\\) by instrumental variable regression using the instrument set \\((\\dot{\\boldsymbol{X}}, \\boldsymbol{Z})\\). Specifically, regression of \\(\\boldsymbol{Y}\\) on \\(\\boldsymbol{X}\\) and \\(\\boldsymbol{Z}\\) treating \\(\\boldsymbol{X}\\) as endogenous, \\(\\boldsymbol{Z}\\) as exogenous, and using the instrument \\(\\dot{\\boldsymbol{X}}\\). Write this estimator as \\((\\widehat{\\beta}, \\widehat{\\gamma})\\). This can be implemented using the Stata ivregress command after constructing the within transformed \\(\\dot{\\boldsymbol{X}}\\).\nThis instrumental variables estimator is algebraically equal to a simple two-step estimator. The first step \\(\\widehat{\\beta}=\\widehat{\\beta}_{\\text {fe }}\\) is the fixed effects estimator. The second step sets \\(\\widehat{\\gamma}=\\left(\\boldsymbol{Z}^{\\prime} \\boldsymbol{Z}\\right)^{-1}\\left(\\boldsymbol{Z}^{\\prime} \\widehat{\\boldsymbol{u}}\\right)\\), the least squares coefficient from the regression of the estimated fixed effect \\(\\widehat{u}_{i}\\) on \\(Z_{i}\\). To see this equivalence observe that the instrumental variables estimator estimator solves the sample moment equations\n\\[\n\\begin{aligned}\n&\\dot{\\boldsymbol{X}}^{\\prime}(\\boldsymbol{Y}-\\boldsymbol{X} \\beta-\\boldsymbol{Z} \\gamma)=0 \\\\\n&\\boldsymbol{Z}^{\\prime}(\\boldsymbol{Y}-\\boldsymbol{X} \\beta-\\boldsymbol{Z} \\gamma)=0 .\n\\end{aligned}\n\\]\nNotice that \\(\\dot{\\boldsymbol{X}}_{i}^{\\prime} \\boldsymbol{Z}_{i}=\\dot{\\boldsymbol{X}}_{i}^{\\prime} \\boldsymbol{l}_{i} Z_{i}^{\\prime}=0\\) so \\(\\dot{\\boldsymbol{X}}^{\\prime} \\boldsymbol{Z}=0\\). Thus (17.78) is the same as \\(\\dot{\\boldsymbol{X}}^{\\prime}(\\boldsymbol{Y}-\\boldsymbol{X} \\beta)=0\\) whose solution is \\(\\widehat{\\beta}_{\\mathrm{fe}}\\). Plugging this into the left-side of (17.79) we obtain\n\\[\n\\boldsymbol{Z}^{\\prime}\\left(\\boldsymbol{Y}-\\boldsymbol{X} \\widehat{\\beta}_{\\mathrm{fe}}-\\boldsymbol{Z} \\gamma\\right)=\\boldsymbol{Z}^{\\prime}\\left(\\overline{\\boldsymbol{Y}}-\\overline{\\boldsymbol{X}} \\widehat{\\beta}_{\\mathrm{fe}}-\\boldsymbol{Z} \\gamma\\right)=\\boldsymbol{Z}^{\\prime}(\\widehat{\\boldsymbol{u}}-\\boldsymbol{Z} \\gamma)\n\\]\nwhere \\(\\overline{\\boldsymbol{Y}}\\) and \\(\\overline{\\boldsymbol{X}}\\) are the stacked individual means \\(\\boldsymbol{\\imath}_{i} \\bar{Y}_{i}\\) and \\(\\boldsymbol{\\imath}_{i} \\bar{X}_{i}^{\\prime}\\). Set equal to 0 and solving we obtain the least squares estimator \\(\\widehat{\\gamma}=\\left(\\boldsymbol{Z}^{\\prime} \\boldsymbol{Z}\\right)^{-1}\\left(\\boldsymbol{Z}^{\\prime} \\widehat{\\boldsymbol{u}}\\right)\\) as claimed. This equivalence was first observed by Hausman and Taylor (1981).\nFor standard error calculation it is recommended to estimate \\((\\beta, \\gamma)\\) jointly by instrumental variable regression and use a cluster-robust covariance matrix clustered at the individual level. Classical and heteroskedasticity-robust estimators are misspecified due to the individual-specific effect \\(u_{i}\\).\nThe estimator \\((\\widehat{\\beta}, \\widehat{\\gamma})\\) is a special case of the Hausman-Taylor estimator described in the next section. (For an unknown reason the above estimator cannot be estimated using Stata’s xthtaylor command.)"
  },
  {
    "objectID": "chpt17-panel-data.html#hausman-taylor-model",
    "href": "chpt17-panel-data.html#hausman-taylor-model",
    "title": "16  Panel Data",
    "section": "16.33 Hausman-Taylor Model",
    "text": "16.33 Hausman-Taylor Model\nHausman and Taylor (1981) consider a generalization of the previous model. Their model is\n\\[\nY_{i t}=X_{1 i t}^{\\prime} \\beta_{1}+X_{2 i t}^{\\prime} \\beta_{2}+Z_{1 i}^{\\prime} \\gamma_{1}+Z_{2 i}^{\\prime} \\gamma_{2}+u_{i}+\\varepsilon_{i t}\n\\]\nwhere \\(X_{1 i t}\\) and \\(X_{2 i t}\\) are time-varying and \\(Z_{1 i}\\) and \\(Z_{2 i}\\) are time-invariant. Let the dimensions of \\(X_{1 i t}\\), \\(X_{2 i t}, Z_{1 i}\\), and \\(Z_{2 i}\\) be \\(k_{1}, k_{2}, \\ell_{1}\\), and \\(\\ell_{2}\\), respectively.\nWrite the model in matrix notation as\n\\[\n\\boldsymbol{Y}=\\boldsymbol{X}_{1} \\beta_{1}+\\boldsymbol{X}_{2} \\beta_{2}+\\boldsymbol{Z}_{1} \\gamma_{1}+\\boldsymbol{Z}_{2} \\gamma_{2}+\\boldsymbol{u}+\\boldsymbol{\\varepsilon} .\n\\]\nLet \\(\\overline{\\boldsymbol{X}}_{1}\\) and \\(\\overline{\\boldsymbol{X}}_{2}\\) denote conformable matrices of individual-specific means and let \\(\\dot{\\boldsymbol{X}}_{1}=\\boldsymbol{X}_{1}-\\overline{\\boldsymbol{X}}_{1}\\) and \\(\\dot{\\boldsymbol{X}}_{2}=\\boldsymbol{X}_{2}-\\overline{\\boldsymbol{X}}_{2}\\) denote the within-transformed variables.\nThe Hausman-Taylor model assumes that all regressors are uncorrelated with the idiosyncratic error \\(\\varepsilon_{i t}\\) at all time horizons and that \\(X_{1 i t}\\) and \\(Z_{1 i}\\) are exogenous with respect to the fixed effect \\(u_{i}\\) so that\n\\[\n\\begin{aligned}\n\\mathbb{E}\\left[X_{1 i t} u_{i}\\right] &=0 \\\\\n\\mathbb{E}\\left[Z_{1 i} u_{i}\\right] &=0 .\n\\end{aligned}\n\\]\nThe regressors \\(X_{2 i t}\\) and \\(Z_{2 i}\\), however, are allowed to be correlated with \\(u_{i}\\).\nSet \\(\\boldsymbol{X}=\\left(\\boldsymbol{X}_{1}, \\boldsymbol{X}_{2}, \\boldsymbol{Z}_{1}, \\boldsymbol{Z}_{2}\\right)\\) and \\(\\beta=\\left(\\beta_{1}, \\beta_{2}, \\gamma_{1}, \\gamma_{2}\\right)\\). The assumptions imply the following population moment conditions\n\\[\n\\begin{aligned}\n&\\mathbb{E}\\left[\\dot{\\boldsymbol{X}}_{1}^{\\prime}(\\boldsymbol{Y}-\\boldsymbol{X} \\beta)\\right]=0 \\\\\n&\\mathbb{E}\\left[\\dot{\\boldsymbol{X}}_{2}^{\\prime}(\\boldsymbol{Y}-\\boldsymbol{X} \\beta)\\right]=0 \\\\\n&\\mathbb{E}\\left[\\overline{\\boldsymbol{X}}_{1}^{\\prime}(\\boldsymbol{Y}-\\boldsymbol{X} \\beta)\\right]=0 \\\\\n&\\mathbb{E}\\left[\\boldsymbol{Z}_{1}^{\\prime}(\\boldsymbol{Y}-\\boldsymbol{X} \\beta)\\right]=0 .\n\\end{aligned}\n\\]\nThere are \\(2 k_{1}+k_{2}+\\ell_{1}\\) moment conditions and \\(k_{1}+k_{2}+\\ell_{1}+\\ell_{2}\\) coefficients. Identification requires \\(k_{1} \\geq \\ell_{2}\\) : that there are at least as many exogenous time-varying regressors as endogenous time-invariant regressors. (This includes the model of the previous section where \\(k_{1}=\\ell_{2}=0\\).) Given the moment conditions the coefficients \\(\\beta=\\left(\\beta_{1}, \\beta_{2}, \\gamma_{1}, \\gamma_{2}\\right)\\) can be estimated by 2SLS regression of (17.80) using the instruments \\(\\boldsymbol{Z}=\\left(\\dot{\\boldsymbol{X}}_{1}, \\dot{\\boldsymbol{X}}_{2}, \\overline{\\boldsymbol{X}}_{1}, \\boldsymbol{Z}_{1}\\right)\\) or equivalently \\(\\boldsymbol{Z}=\\left(\\boldsymbol{X}_{1}, \\dot{\\boldsymbol{X}}_{2}, \\overline{\\boldsymbol{X}}_{1}, \\boldsymbol{Z}_{1}\\right)\\). This is 2SLS regression treating \\(\\boldsymbol{X}_{1}\\) and \\(Z_{1}\\) as exogenous and \\(\\boldsymbol{X}_{2}\\) and \\(\\boldsymbol{Z}_{2}\\) as endogenous using the excluded instruments \\(\\dot{\\boldsymbol{X}}_{2}\\) and \\(\\overline{\\boldsymbol{X}}_{1}\\)\nIt is recommended to use cluster-robust covariance matrix estimation clustered at the individual level. Neither conventional nor heteroskedasticity-robust covariance matrix estimators should be used as they are misspecified due to the individual-specific effect \\(u_{i}\\).\nWhen the model is just-identified the estimators simplify as follows. \\(\\widehat{\\beta}_{1}\\) and \\(\\widehat{\\beta}_{2}\\) are the fixed effects estimator. \\(\\widehat{\\gamma}_{1}\\) and \\(\\widehat{\\gamma}_{2}\\) equal the 2 SLS estimator from a regression of \\(\\widehat{u}_{i}\\) on \\(Z_{1 i}\\) and \\(Z_{2 i}\\) using \\(\\bar{X}_{1 i}\\) as an instrument for \\(Z_{2 i}\\). (See Exercise 17.14.)\nWhen the model is over-identified the equation can also be estimated by GMM with a cluster-robust weight matrix using the same equations and instruments.\nThis estimator with cluster-robust standard errors can be calculated using the Stata ivregress cluster(id) command after constructing the transformed variables \\(\\dot{\\boldsymbol{X}}_{2}\\) and \\(\\overline{\\boldsymbol{X}}_{1}\\).\nThe 2SLS estimator described above corresponds with the Hausman and Taylor (1981) estimator in the just-identified case with a balanced panel.\nHausman and Taylor derived their estimator under the stronger assumption that the errors \\(\\varepsilon_{i t}\\) and \\(u_{i}\\) are strictly mean independent and homoskedastic and consequently proposed a GLS-type estimator which is more efficient when these assumptions are correct. Define \\(\\Omega=\\operatorname{diag}\\left(\\Omega_{i}\\right)\\) where \\(\\Omega_{i}=\\boldsymbol{I}_{i}+\\) \\(\\mathbf{1}_{i} \\mathbf{1}_{i}^{\\prime} \\sigma_{u}^{2} / \\sigma_{\\varepsilon}^{2}\\) and \\(\\sigma_{\\varepsilon}^{2}\\) and \\(\\sigma_{u}^{2}\\) are the variances of the error components \\(\\varepsilon_{i t}\\) and \\(u_{i}\\). Define as well the transformed variables \\(\\widetilde{\\boldsymbol{Y}}=\\Omega^{-1 / 2} \\boldsymbol{Y}, \\widetilde{\\boldsymbol{X}}=\\Omega^{-1 / 2} \\boldsymbol{X}\\) and \\(\\widetilde{\\boldsymbol{Z}}=\\Omega^{-1 / 2} \\boldsymbol{Z}\\). The Hausman-Taylor estimator is\n\\[\n\\begin{aligned}\n\\widehat{\\beta}_{\\mathrm{ht}} &=\\left(\\boldsymbol{X}^{\\prime} \\Omega^{-1} \\boldsymbol{Z}\\left(\\boldsymbol{Z}^{\\prime} \\Omega^{-1} \\boldsymbol{Z}\\right)^{-1} \\boldsymbol{Z}^{\\prime} \\Omega^{-1} \\boldsymbol{X}\\right)^{-1}\\left(\\boldsymbol{X}^{\\prime} \\Omega^{-1} \\boldsymbol{Z}\\left(\\boldsymbol{Z}^{\\prime} \\Omega^{-1} \\boldsymbol{Z}\\right)^{-1} \\boldsymbol{Z}^{\\prime} \\Omega^{-1} \\boldsymbol{Y}\\right) \\\\\n&=\\left(\\widetilde{\\boldsymbol{X}}^{\\prime} \\widetilde{\\boldsymbol{Z}}\\left(\\widetilde{\\boldsymbol{Z}}^{\\prime} \\widetilde{\\boldsymbol{Z}}\\right)^{-1} \\widetilde{\\boldsymbol{Z}}^{\\prime} \\widetilde{\\boldsymbol{X}}\\right)^{-1}\\left(\\widetilde{\\boldsymbol{X}}^{\\prime} \\widetilde{\\boldsymbol{Z}}\\left(\\widetilde{\\boldsymbol{Z}}^{\\prime} \\widetilde{\\boldsymbol{Z}}\\right)^{-1} \\widetilde{\\boldsymbol{Z}}^{\\prime} \\widetilde{\\boldsymbol{Y}}\\right) .\n\\end{aligned}\n\\]\nRecall from (17.47) that \\(\\Omega_{i}^{-1 / 2}=\\boldsymbol{M}_{i}+\\rho_{i} \\boldsymbol{P}_{i}\\) where \\(\\rho_{i}\\) is defined in (17.46). Thus\n\\[\n\\begin{aligned}\n\\widetilde{Y}_{i} &=Y_{i}-\\left(1-\\rho_{i}\\right) \\bar{Y}_{i} \\\\\n\\widetilde{X}_{1 i} &=X_{1 i}-\\left(1-\\rho_{i}\\right) \\bar{X}_{1 i} \\\\\n\\widetilde{X}_{2 i} &=X_{2 i}-\\left(1-\\rho_{i}\\right) \\bar{X}_{2 i} \\\\\n\\widetilde{Z}_{1 i} &=\\rho_{i} Z_{1 i} \\\\\n\\widetilde{Z}_{2 i} &=\\rho_{i} Z_{2 i} \\\\\n\\widetilde{\\dot{X}}_{1 i} &=\\dot{X}_{1 i} \\\\\n\\widetilde{\\dot{X}}_{2 i} &=\\dot{X}_{2 i} .\n\\end{aligned}\n\\]\nIt follows that the Hausman-Taylor estimator can be calculated by 2SLS regression of \\(\\widetilde{\\boldsymbol{Y}}_{i}\\) on \\(\\left(\\widetilde{\\boldsymbol{X}}_{1 i}, \\widetilde{\\boldsymbol{X}}_{2 i}, \\rho_{i} \\boldsymbol{Z}_{1 i}, \\rho_{i} \\boldsymbol{Z}_{2 i}\\right)\\) using the instruments \\(\\left(\\dot{\\boldsymbol{X}}_{1 i}, \\dot{\\boldsymbol{X}}_{2 i}, \\rho_{i} \\overline{\\boldsymbol{X}}_{1 i}, \\rho_{i} \\boldsymbol{Z}_{2 i}\\right)\\)\nWhen the panel is balanced the coefficients \\(\\rho_{i}\\) all equal and scale out from the instruments. Thus the estimator can be calculated by 2SLS regression of \\(\\widetilde{\\boldsymbol{Y}}_{i}\\) on \\(\\left(\\widetilde{\\boldsymbol{X}}_{1 i}, \\widetilde{\\boldsymbol{X}}_{2 i}, Z_{1 i}, \\boldsymbol{Z}_{2 i}\\right)\\) using the instruments \\(\\left(\\dot{\\boldsymbol{X}}_{1 i}, \\dot{\\boldsymbol{X}}_{2 i}, \\overline{\\boldsymbol{X}}_{1 i}, \\boldsymbol{Z}_{2 i}\\right)\\)\nIn practice \\(\\rho_{i}\\) is unknown. It can be estimated as in (17.48) with the modification that the error variance is estimated from the untransformed 2SLS regression. Under the homoskedasticity assumptions used by Hausman and Taylor the estimator \\(\\widehat{\\beta}_{\\text {ht }}\\) has a classical asymptotic covariance matrix. When these assumptions are relaxed the covariance matrix can be estimated using cluster-robust methods. The Hausman-Taylor estimator with cluster-robust standard errors can be implemented in Stata by the command xthtaylor vce(robust). This Stata command, for an unknown reason, requires that there is at least one exogenous time-invariant variable \\(\\left(\\ell_{1} \\geq 1\\right)\\) and at least one exogenous time-varying variable \\(\\left(k_{1} \\geq 1\\right)\\), even when the model is identified. Otherwise, the estimator can be implemented using the instrumental variable method described above.\nThe Hausman-Taylor estimator was refined by Amemiya and MaCurdy (1986) and Breusch, Mizon and Schmidt (1989) who proposed more efficient versions using additional instruments which are valid under stronger orthogonality conditions. The observation that in the unbalanced case the instruments should be weighted by \\(\\rho_{i}\\) was made by Gardner (1998).\nIn the over-identified case it is unclear if it is preferred to use the simpler 2SLS estimator \\(\\widehat{\\beta}_{2 s l s}\\) or the GLS-type Hausman-Taylor estimator \\(\\widehat{\\beta}_{\\mathrm{ht}}\\). The advantages of \\(\\widehat{\\beta}_{\\mathrm{ht}}\\) are that it is asymptotically efficient under their stated homoskedasticity and serial correlation conditions and that there is an available program in Stata. The advantage of \\(\\widehat{\\beta}_{2 \\text { sls }}\\) is that it is much simpler to program (if doing so yourself), may have better finite sample properties (because it avoids variance-component estimation), and is the natural estimator from the the modern GMM viewpoint.\nTo illustrate, the final column of Table \\(17.2\\) contains Hausman-Taylor estimates of the investment model treating \\(Q_{i t-1}, D_{i t-1}\\), and \\(T_{i}\\) as endogenous for \\(u_{i}\\) and \\(C F_{i t-1}\\) and the industry dummies as exogenous. Relative to the fixed effects models this allows estimation of the coefficients on the trading indicator \\(T_{i}\\). The most interesting change relative to the previous estimates is that the coefficient on the trading indicator \\(T_{i}\\) doubles in magnitude relative to the random effects estimate. This is consistent with the hypothesis that \\(T_{i}\\) is correlated with the fixed effect and hence the random effects estimate is biased."
  },
  {
    "objectID": "chpt17-panel-data.html#jackknife-covariance-matrix-estimation",
    "href": "chpt17-panel-data.html#jackknife-covariance-matrix-estimation",
    "title": "16  Panel Data",
    "section": "16.34 Jackknife Covariance Matrix Estimation",
    "text": "16.34 Jackknife Covariance Matrix Estimation\nAs an alternative to asymptotic inference the delete-cluster jackknife can be used for covariance matrix calculation. In the context of fixed effects estimation the delete-cluster estimators take the form\n\\[\n\\widehat{\\beta}_{(-i)}=\\left(\\sum_{j \\neq i} \\dot{\\boldsymbol{X}}_{j}^{\\prime} \\dot{\\boldsymbol{X}}_{j}\\right)^{-1}\\left(\\sum_{j \\neq i} \\dot{\\boldsymbol{X}}_{j}^{\\prime} \\dot{\\boldsymbol{Y}}_{j}\\right)=\\widehat{\\beta}_{\\mathrm{fe}}-\\left(\\sum_{i=1}^{N} \\dot{\\boldsymbol{X}}_{i}^{\\prime} \\dot{\\boldsymbol{X}}_{i}\\right)^{-1} \\dot{\\boldsymbol{X}}_{i}^{\\prime} \\widetilde{\\boldsymbol{e}}_{i}\n\\]\nwhere\n\\[\n\\begin{aligned}\n&\\widetilde{\\boldsymbol{e}}_{i}=\\left(\\boldsymbol{I}_{i}-\\dot{\\boldsymbol{X}}_{i}\\left(\\dot{\\boldsymbol{X}}_{i}^{\\prime} \\dot{\\boldsymbol{X}}_{i}\\right)^{-1} \\dot{\\boldsymbol{X}}_{i}^{\\prime}\\right)^{-1} \\widehat{\\boldsymbol{e}}_{i} \\\\\n&\\widehat{\\boldsymbol{e}}_{i}=\\dot{\\boldsymbol{Y}}_{i}-\\dot{\\boldsymbol{X}}_{i} \\widehat{\\beta}_{\\mathrm{fe}}\n\\end{aligned}\n\\]\nThe delete-cluster jackknife estimator of the variance of \\(\\widehat{\\beta}_{\\mathrm{fe}}\\) is\n\\[\n\\begin{aligned}\n\\widehat{\\boldsymbol{V}}_{\\widehat{\\beta}}^{\\text {jack }} &=\\frac{N-1}{N} \\sum_{i=1}^{N}\\left(\\widehat{\\beta}_{(-i)}-\\bar{\\beta}\\right)\\left(\\widehat{\\beta}_{(-i)}-\\bar{\\beta}\\right)^{\\prime} \\\\\n\\bar{\\beta} &=\\frac{1}{N} \\sum_{i=1}^{N} \\widehat{\\beta}_{(-i)} .\n\\end{aligned}\n\\]\nThe delete-cluster jackknife estimator \\(\\widehat{\\boldsymbol{V}}_{\\widehat{\\beta}}^{\\text {jack }}\\) is similar to the cluster-robust covariance matrix estimator.\nFor parameters which are functions \\(\\widehat{\\theta}_{\\mathrm{fe}}=r\\left(\\widehat{\\beta}_{\\mathrm{fe}}\\right)\\) of the fixed effects estimator the delete-cluster jack- knife estimator of the variance of \\(\\widehat{\\theta}_{\\mathrm{fe}}\\) is\n\\[\n\\begin{aligned}\n\\widehat{\\boldsymbol{V}}_{\\widehat{\\theta}}^{\\mathrm{jack}} &=\\frac{N-1}{N} \\sum_{i=1}^{N}\\left(\\widehat{\\theta}_{(-i)}-\\bar{\\theta}\\right)\\left(\\widehat{\\theta}_{(-i)}-\\bar{\\theta}\\right)^{\\prime} \\\\\n\\widehat{\\theta}_{(-i)} &=r\\left(\\widehat{\\beta}_{(-i)}\\right) \\\\\n\\bar{\\theta} &=\\frac{1}{N} \\sum_{i=1}^{N} \\widehat{\\theta}_{(-i)} .\n\\end{aligned}\n\\]\nThe estimator \\(\\widehat{\\boldsymbol{V}}_{\\widehat{\\theta}}^{\\text {jack }}\\) is similar to the delta-method cluster-robust covariance matrix estimator for \\(\\widehat{\\theta}\\).\nAs in the context of i.i.d. samples one advantage of the jackknife covariance matrix estimator is that it does not require the user to make a technical calculation of the asymptotic distribution. A downside is an increase in computation cost as \\(N\\) separate regressions are effectively estimated. This can be particularly costly in micro panels which have a large number \\(N\\) of individuals.\nIn Stata jackknife standard errors for fixed effects estimators are obtained by using either \\(x\\) treg \\(f e\\) vce(jackknife) or areg absorb(id) cluster(id) vce(jackknife) where id is the cluster variable. For the fixed effects 2SLS estimator usextivreg fe vce(jackknife)."
  },
  {
    "objectID": "chpt17-panel-data.html#panel-bootstrap",
    "href": "chpt17-panel-data.html#panel-bootstrap",
    "title": "16  Panel Data",
    "section": "16.35 Panel Bootstrap",
    "text": "16.35 Panel Bootstrap\nBootstrap methods can also be applied to panel data by a straightforward application of the pairs cluster bootstrap which samples entire individuals rather than single observations. In the context of panel data we call this the panel nonparametric bootstrap.\nThe panel nonparametric bootstrap samples \\(N\\) individual histories \\(\\left(\\boldsymbol{Y}_{i}, \\boldsymbol{X}_{i}\\right)\\) to create the bootstrap sample. Fixed effects (or any other estimation method) is applied to the bootstrap sample to obtain the coefficient estimates. By repeating \\(B\\) times, bootstrap standard errors for coefficients estimates, or functions of the coefficient estimates, can be calculated. Percentile-type and percentile-t confidence intervals can be calculated. The \\(\\mathrm{BC}_{a}\\) interval requires an estimator of the acceleration coefficient \\(a\\) which is a scaled jackknife estimate of the third moment of the estimator. In panel data the delete-cluster jackknife should be used for estimation of \\(a\\).\nIn Stata, to obtain bootstrap standard errors and confidence intervals use either xtreg, vce(bootstrap, reps (#)) or areg, absorb(id) cluster(id) vce(bootstrap, reps(#)) where id is the cluster variable and # is the number of bootstrap replications. For the fixed effects 2SLS estimator use xtivreg, fe vce(bootstrap, reps(#))."
  },
  {
    "objectID": "chpt17-panel-data.html#dynamic-panel-models",
    "href": "chpt17-panel-data.html#dynamic-panel-models",
    "title": "16  Panel Data",
    "section": "16.36 Dynamic Panel Models",
    "text": "16.36 Dynamic Panel Models\nThe models so far considered in this chapter have been static with no dynamic relationships. In many economic contexts it is natural to expect that behavior and decisions are dynamic, explicitly depending on past behavior. In our investment equation, for example, economic models predict that a firm’s investment in any given year will depend on investment decisions from previous years. These considerations lead us to consider explicitly dynamic models.\nThe workhorse dynamic model in a panel framework is the \\(p^{t h}\\)-order autoregression with regressors and a one-way error component structure. This is\n\\[\nY_{i t}=\\alpha_{1} Y_{i, t-1}+\\cdots+\\alpha_{p} Y_{i, t-p}+X_{i t}^{\\prime} \\beta+u_{i}+\\varepsilon_{i t} .\n\\]\nwhere \\(\\alpha_{j}\\) are the autoregressive coefficients, \\(X_{i t}\\) is a \\(k\\) vector of regressors, \\(u_{i}\\) is an individual effect, and \\(\\varepsilon_{i t}\\) is an idiosyncratic error. It is conventional to assume that the errors \\(u_{i}\\) and \\(\\varepsilon_{i t}\\) are mutually independent and the \\(\\varepsilon_{i t}\\) are serially uncorrelated and mean zero. For the present we will assume that the regressors \\(X_{i t}\\) are strictly exogenous (17.17). In Section \\(17.41\\) we discuss predetermined regressors.\nFor many illustrations we will focus on the AR(1) model\n\\[\nY_{i t}=\\alpha Y_{i, t-1}+u_{i}+\\varepsilon_{i t}\n\\]\nThe dynamics should be interpreted individual-by-individual. The coefficient \\(\\alpha\\) in (17.82) equals the first-order autocorrelation. When \\(\\alpha=0\\) the series is serially uncorrelated (conditional on \\(u_{i}\\) ). \\(\\alpha>0\\) means \\(Y_{i t}\\) is positively serially correlated. \\(\\alpha<0\\) means \\(Y_{i t}\\) is negatively serially correlated. An autoregressive unit root holds when \\(\\alpha=1\\), which means that \\(Y_{i t}\\) follows a random walk with possible drift. Since \\(u_{i}\\) is constant for a given individual it should be treated as an individual-specific intercept. The idiosyncratic error \\(\\varepsilon_{i t}\\) plays the role of the error in a standard time series autoregression.\nIf \\(|\\alpha|<1\\) the model (17.82) is stationary. By standard autoregressive backwards recursion we calculate that\n\\[\nY_{i t}=\\sum_{j=0}^{\\infty} \\alpha^{j}\\left(u_{i}+\\varepsilon_{i t}\\right)=(1-\\alpha)^{-1} u_{i}+\\sum_{j=0}^{\\infty} \\alpha^{j} \\varepsilon_{i, t-j}\n\\]\nThus conditional on \\(u_{i}\\) the mean and variance of \\(Y_{i t}\\) are \\((1-\\alpha)^{-1} u_{i}\\) and \\(\\left(1-\\alpha^{2}\\right)^{-1} \\sigma_{\\varepsilon}^{2}\\), respectively. The \\(k^{t h}\\) autocorrelation (conditional on \\(u_{i}\\) ) is \\(\\alpha^{k}\\). Notice that the effect of cross-section variation in \\(u_{i}\\) is to shift the mean but not the variance or serial correlation. This implies that if we view time series plots of \\(Y_{i t}\\) against time for a set of individuals \\(i\\), the series \\(Y_{i t}\\) will appear to have different means but have similar variances and serial correlation.\nAs with the case with time series data, serial correlation (large \\(\\alpha\\) ) can proxy for other factors such as time trends. Thus in applications it will often be useful to include time effects to eliminate spurious serial correlation."
  },
  {
    "objectID": "chpt17-panel-data.html#the-bias-of-fixed-effects-estimation",
    "href": "chpt17-panel-data.html#the-bias-of-fixed-effects-estimation",
    "title": "16  Panel Data",
    "section": "16.37 The Bias of Fixed Effects Estimation",
    "text": "16.37 The Bias of Fixed Effects Estimation\nTo estimate the panel autoregression (17.81) it may appear natural to use the fixed effects (within) estimator. Indeed, the within transformation eliminates the individual effect \\(u_{i}\\). The trouble is that the within operator induces correlation between the AR(1) lag and the error. The result is that the within estimator is inconsistent for the coefficients when \\(T\\) is fixed. A thorough explanation appears in Nickell (1981). We describe the basic problem in this section focusing on the AR(1) model (17.82).\nApplying the within operator to (17.82) we obtain\n\\[\n\\dot{Y}_{i t}=\\alpha \\dot{Y}_{i t-1}+\\dot{\\varepsilon}_{i t}\n\\]\nfor \\(t \\geq 2\\). As expected the individual effect is eliminated. The difficulty is that \\(\\mathbb{E}\\left[\\dot{Y}_{i t-1} \\dot{\\varepsilon}_{i t}\\right] \\neq 0\\) because both \\(\\dot{Y}_{i t-1}\\) and \\(\\dot{\\varepsilon}_{i t}\\) are functions of the entire time series.\nTo see this clearly in a simple example, suppose we have a balanced panel with \\(T=3\\). There are two observed pairs \\(\\left(Y_{i t}, Y_{i t-1}\\right)\\) per individual so the within estimator equals the differenced estimator. Applying the differencing operator to (17.82) for \\(t=3\\) we find\n\\[\n\\Delta Y_{i 3}=\\alpha \\Delta Y_{i 2}+\\Delta \\varepsilon_{i 3} .\n\\]\nBecause of the lagged dependent variable and differencing there is effectively one observation per individual. Notice that the individual effect has been eliminated.\nThe fixed effects estimator of \\(\\alpha\\) is equal to the least squares estimator applied to (17.84), which is\n\\[\n\\widehat{\\alpha}_{\\mathrm{fe}}=\\left(\\sum_{i=1}^{N} \\Delta Y_{i 2}^{2}\\right)^{-1}\\left(\\sum_{i=1}^{N} \\Delta Y_{i 2} \\Delta Y_{i 3}\\right)=\\alpha+\\left(\\sum_{i=1}^{N} \\Delta Y_{i 2}^{2}\\right)^{-1}\\left(\\sum_{i=1}^{N} \\Delta Y_{i 2} \\Delta \\varepsilon_{i 3}\\right) .\n\\]\nThe differenced regressor and error are negatively correlated. Indeed\n\\[\n\\begin{aligned}\n\\mathbb{E}\\left[\\Delta Y_{i 2} \\Delta \\varepsilon_{i 3}\\right] &=\\mathbb{E}\\left[\\left(Y_{i 2}-Y_{i 1}\\right)\\left(\\varepsilon_{i 3}-\\varepsilon_{i 2}\\right)\\right] \\\\\n&=\\mathbb{E}\\left[Y_{i 2} \\varepsilon_{i 3}\\right]-\\mathbb{E}\\left[Y_{i 1} \\varepsilon_{i 3}\\right]-\\mathbb{E}\\left[Y_{i 2} \\varepsilon_{i 2}\\right]+\\mathbb{E}\\left[Y_{i 1} \\varepsilon_{i 2}\\right] \\\\\n&=0-0-\\sigma_{\\varepsilon}^{2}+0 \\\\\n&=-\\sigma_{\\varepsilon}^{2}\n\\end{aligned}\n\\]\nUsing the variance formula for \\(\\operatorname{AR}(1)\\) models (assuming \\(|\\alpha|<1)\\) we calculate that \\(\\mathbb{E}\\left[\\left(\\Delta Y_{i 2}\\right)^{2}\\right]=2 \\sigma_{\\varepsilon}^{2} /(1+\\) \\(\\alpha\\) ). It follows that the probability limit of the fixed effects estimator \\(\\widehat{\\alpha}_{\\mathrm{fe}}\\) of \\(\\alpha\\) in (17.84) is\n\\[\n\\operatorname{plim}_{N \\rightarrow \\infty}\\left(\\widehat{\\alpha}_{\\mathrm{fe}}-\\alpha\\right)=\\frac{\\mathbb{E}\\left[\\Delta Y_{i 2} \\Delta \\varepsilon_{i 3}\\right]}{\\mathbb{E}\\left[\\left(\\Delta Y_{i 2}\\right)^{2}\\right]}=-\\frac{1+\\alpha}{2} .\n\\]\nIt is typical to call (17.85) the “bias” of \\(\\widehat{\\alpha}_{\\text {fe }}\\), though it is technically a probability limit.\nThe bias found in (17.85) is large. For \\(\\alpha=0\\) the bias is \\(-1 / 2\\) and increases towards 1 as \\(\\alpha \\rightarrow 1\\). Thus for any \\(\\alpha<1\\) the probability limit of \\(\\widehat{\\alpha}_{\\mathrm{fe}}\\) is negative! This is extreme bias.\nNow take the case \\(T>3\\). From Nickell’s (1981) expressions and some algebra, we can calculate that the probability limit of the fixed effects estimator for \\(|\\alpha|<1\\) is\n\\[\n\\operatorname{plim}_{N \\rightarrow \\infty}\\left(\\widehat{\\alpha}_{\\mathrm{fe}}-\\alpha\\right)=\\frac{1+\\alpha}{\\frac{2 \\alpha}{1-\\alpha}-\\frac{T-1}{1-\\alpha^{T-1}}} .\n\\]\nIt follows that the bias is of order \\(O(1 / T)\\).\nIt is often asserted that it is okay to use fixed effects if \\(T\\) is sufficiently large, e.g. \\(T \\geq 30\\). However, from (17.86) we can calculate that for \\(T=30\\) the bias of the fixed effects estimator is \\(-0.056\\) when \\(\\alpha=0.5\\) and the bias is \\(-0.15\\) when \\(\\alpha=0.9\\). For \\(T=60\\) and \\(\\alpha=0.9\\) the bias is \\(-0.05\\). These magnitudes are unacceptably large. This includes the longer time series encountered in macro panels. Thus the Nickell bias problem applies to both micro and macro panel applications.\nThe conclusion from this analysis is that the fixed effects estimator should not be used for models with lagged dependent variables even if the time series dimension \\(T\\) is large."
  },
  {
    "objectID": "chpt17-panel-data.html#anderson-hsiao-estimator",
    "href": "chpt17-panel-data.html#anderson-hsiao-estimator",
    "title": "16  Panel Data",
    "section": "16.38 Anderson-Hsiao Estimator",
    "text": "16.38 Anderson-Hsiao Estimator\nAnderson and Hsiao (1982) made an important breakthrough by showing that a simple instrumental variables estimator is consistent for the parameters of (17.81).\nThe method first eliminates the individual effect \\(u_{i}\\) by first-differencing (17.81) for \\(t \\geq p+1\\)\n\\[\n\\Delta Y_{i t}=\\alpha_{1} \\Delta Y_{i, t-1}+\\alpha_{2} \\Delta Y_{i, t-2}+\\cdots+\\alpha_{p} \\Delta Y_{i, t-p}+\\Delta X_{i t}^{\\prime} \\beta+\\Delta \\varepsilon_{i t} .\n\\]\nThis eliminates the individual effect \\(u_{i}\\). The challenge is that first-differencing induces correlation between \\(\\Delta Y_{i t-1}\\) and \\(\\Delta \\varepsilon_{i t}\\) :\n\\[\n\\mathbb{E}\\left[\\Delta Y_{i, t-1} \\Delta \\varepsilon_{i t}\\right]=\\mathbb{E}\\left[\\left(Y_{i, t-1}-Y_{i, t-2}\\right)\\left(\\varepsilon_{i t}-\\varepsilon_{i t-1}\\right)\\right]=-\\sigma_{\\varepsilon}^{2} .\n\\]\nThe other regressors are not correlated with \\(\\Delta \\varepsilon_{i t}\\). For \\(s>1, \\mathbb{E}\\left[\\Delta Y_{i t-s} \\Delta \\varepsilon_{i t}\\right]=0\\), and when \\(X_{i t}\\) is strictly exogenous \\(\\mathbb{E}\\left[\\Delta X_{i t} \\Delta \\varepsilon_{i t}\\right]=0\\).\nThe correlation between \\(\\Delta Y_{i t-1}\\) and \\(\\Delta \\varepsilon_{i t}\\) is endogeneity. One solution to endogeneity is to use an instrument. Anderson-Hsiao pointed out that \\(Y_{i t-2}\\) is a valid instrument because it is correlated with \\(\\Delta Y_{i, t-1}\\) yet uncorrelated with \\(\\Delta \\varepsilon_{i t}\\).\n\\[\n\\mathbb{E}\\left[Y_{i, t-2} \\Delta \\varepsilon_{i t}\\right]=\\mathbb{E}\\left[Y_{i, t-2} \\varepsilon_{i t}\\right]-\\mathbb{E}\\left[Y_{i, t-2} \\varepsilon_{i t-1}\\right]=0 .\n\\]\nThe Anderson-Hsiao estimator is IV using \\(Y_{i, t-2}\\) as an instrument for \\(\\Delta Y_{i, t-1}\\). Equivalently, this is IV using the instruments \\(\\left(Y_{i, t-2}, \\ldots, Y_{i, t-p-1}\\right)\\) for \\(\\left(\\Delta Y_{i, t-1}, \\ldots, \\Delta Y_{i, t-p}\\right)\\). The estimator requires \\(T \\geq p+2\\).\nTo show that this estimator is consistent, for simplicity assume we have a balanced panel with \\(T=3\\), \\(p=1\\), and no regressors. In this case the Anderson-Hsiao IV estimator is\n\\[\n\\widehat{\\alpha}_{\\mathrm{iv}}=\\left(\\sum_{i=1}^{N} Y_{i 1} \\Delta Y_{i 2}\\right)^{-1}\\left(\\sum_{i=1}^{N} Y_{i 1} \\Delta Y_{i 3}\\right)=\\alpha+\\left(\\sum_{i=1}^{N} Y_{i 1} \\Delta Y_{i 2}\\right)^{-1}\\left(\\sum_{i=1}^{N} Y_{i 1} \\Delta \\varepsilon_{i 3}\\right) .\n\\]\nUnder the assumption that \\(\\varepsilon_{i t}\\) is serially uncorrelated, (17.88) shows that \\(\\mathbb{E}\\left[Y_{i 1} \\Delta \\varepsilon_{i 3}\\right]=0\\). In general, \\(\\mathbb{E}\\left[Y_{i 1} \\Delta Y_{i 2}\\right] \\neq 0\\). As \\(N \\rightarrow \\infty\\)\n\\[\n\\widehat{\\alpha}_{\\mathrm{iv}} \\underset{p}{\\longrightarrow} \\alpha-\\frac{\\mathbb{E}\\left[Y_{i 1} \\Delta \\varepsilon_{i 3}\\right]}{\\mathbb{E}\\left[Y_{i 1} \\Delta Y_{i 2}\\right]}=\\alpha .\n\\]\nThus the IV estimator is consistent for \\(\\alpha\\).\nThe Anderson-Hsiao IV estimator relies on two critical assumptions. First, the validity of the instrument (uncorrelatedness with the equation error) relies on the assumption that the dynamics are correctly specified so that \\(\\varepsilon_{i t}\\) is serially uncorrelated. For example, many applications use an AR(1). If instead the true model is an \\(\\operatorname{AR}(2)\\) then \\(Y_{i t-2}\\) is not a valid instrument and the IV estimates will be biased. Second, the relevance of the instrument (correlatedness with the endogenous regressor) requires \\(\\mathbb{E}\\left[Y_{i 1} \\Delta Y_{i 2}\\right] \\neq 0\\). This turns out to be problematic and is explored further in Section 17.40. These considerations suggest that the validity and accuracy of the estimator are likely to be sensitive to these unknown features."
  },
  {
    "objectID": "chpt17-panel-data.html#arellano-bond-estimator",
    "href": "chpt17-panel-data.html#arellano-bond-estimator",
    "title": "16  Panel Data",
    "section": "16.39 Arellano-Bond Estimator",
    "text": "16.39 Arellano-Bond Estimator\nThe orthogonality condition (17.88) is one of many implied by the dynamic panel model. Indeed, all lags \\(Y_{i t-2}, Y_{i t-3}, \\ldots\\) are valid instruments. If \\(T>p+2\\) these can be used to potentially improve estimation efficiency. This was first pointed out by Holtz-Eakin, Newey, and Rosen (1988) and further developed by Arellano and Bond (1991).\nUsing these extra instruments has a complication that there are a different number of instruments for each time period. The solution is to view the model as a system of \\(T\\) equations as in Section 17.18.\nIt will be useful to first write the model in vector notation. Stack the differenced regressors \\(\\left(\\Delta Y_{i, t-1}, \\ldots\\right.\\), \\(\\Delta Y_{i, t-p}, \\Delta X_{i t}^{\\prime}\\) ) into a matrix \\(\\Delta \\boldsymbol{X}_{i}\\) and the coefficients into a vector \\(\\theta\\). We can write (17.87) as \\(\\Delta \\boldsymbol{Y}_{i}=\\) \\(\\Delta \\boldsymbol{X}_{i} \\theta+\\Delta \\boldsymbol{\\varepsilon}_{i}\\). Stacking all \\(N\\) individuals this can be written as \\(\\Delta \\boldsymbol{Y}=\\Delta \\boldsymbol{X} \\theta+\\Delta \\boldsymbol{\\varepsilon}\\).\nFor period \\(t=p+2\\) we have \\(p+k\\) valid instruments \\(\\left[Y_{i 1} \\ldots, Y_{i p}, \\Delta X_{i, p+2}\\right]\\). For period \\(t=p+3\\) there are \\(p+1+k\\) valid instruments \\(\\left[Y_{i 1} \\ldots, Y_{i p+1}, \\Delta X_{i, p+3}\\right]\\). For period \\(t=p+4\\) there are \\(p+2+k\\) instruments. In general, for any \\(t \\geq p+2\\) there are \\(t-2\\) instruments \\(\\left[Y_{i 1}, \\ldots, Y_{i, t-2}, \\Delta X_{i t}\\right]\\). Similarly to (17.53) we can define the instrument matrix for individual \\(i\\) as\n\\[\n\\boldsymbol{Z}_{i}=\\left[\\begin{array}{ccc}\n{\\left[Y_{i 1}, \\ldots, Y_{i p}, \\Delta X_{i, p+2}^{\\prime}\\right]} & 0 & 0 \\\\\n0 & {\\left[Y_{i 1}, \\ldots, Y_{i, p+1}, \\Delta X_{i, p+3}^{\\prime}\\right]} & \\\\\n0 & \\ddots & 0 \\\\\n& 0 & {\\left[Y_{i 1}, Y_{i 2}, \\ldots, Y_{i, T-2}, \\Delta X_{i, T}^{\\prime}\\right]}\n\\end{array}\\right] .\n\\]\nThis is \\((T-p-1) \\times \\ell\\) where \\(\\ell=k(T-p-1)+((T-2)(T-1)-(p-2)(p-1)) / 2\\). This instrument matrix consists of all lagged values \\(Y_{i, t-2}, Y_{i, t-3}, \\ldots\\) which are available in the data set plus the differenced strictly exogenous regressors.\nThe \\(\\ell\\) moment conditions are\n\\[\n\\mathbb{E}\\left[\\boldsymbol{Z}_{i}^{\\prime}\\left(\\Delta \\boldsymbol{Y}_{i}-\\Delta \\boldsymbol{X}_{i} \\alpha\\right)\\right]=0\n\\]\nIf \\(T>p+2\\) then \\(\\ell>p\\) and the model is overidentified. Define the \\(\\ell \\times \\ell\\) covariance matrix for the moment conditions\n\\[\n\\Omega=\\mathbb{E}\\left[\\boldsymbol{Z}_{i}^{\\prime} \\Delta \\boldsymbol{\\varepsilon}_{i} \\Delta \\boldsymbol{\\varepsilon}_{i}^{\\prime} \\boldsymbol{Z}_{i}\\right] .\n\\]\nLet \\(\\boldsymbol{Z}\\) denote \\(\\boldsymbol{Z}_{i}\\) stacked into a \\((T-p-1) N \\times \\ell\\) matrix. The efficient GMM estimator of \\(\\alpha\\) is\n\\[\n\\widehat{\\alpha}_{\\mathrm{gmm}}=\\left(\\Delta \\boldsymbol{X}^{\\prime} \\boldsymbol{Z} \\Omega^{-1} \\boldsymbol{Z}^{\\prime} \\Delta \\boldsymbol{X}\\right)^{-1}\\left(\\Delta \\boldsymbol{X}^{\\prime} \\boldsymbol{Z} \\Omega^{-1} \\boldsymbol{Z}^{\\prime} \\Delta \\boldsymbol{Y}\\right) .\n\\]\nIf the errors \\(\\varepsilon_{i t}\\) are conditionally homoskedastic then\n\\[\n\\Omega=\\mathbb{E}\\left[\\boldsymbol{Z}_{i}^{\\prime} \\boldsymbol{H} \\boldsymbol{Z}_{i}\\right] \\sigma_{\\varepsilon}^{2}\n\\]\nwhere \\(\\boldsymbol{H}\\) is given in (17.31). In this case set\n\\[\n\\widehat{\\Omega}_{1}=\\sum_{i=1}^{N} \\boldsymbol{Z}_{i}^{\\prime} \\boldsymbol{H} \\boldsymbol{Z}_{i}\n\\]\nas a (scaled) estimate of \\(\\Omega\\). Under these assumptions an asymptotically efficient GMM estimator is\n\\[\n\\widehat{\\alpha}_{1}=\\left(\\Delta \\boldsymbol{X}^{\\prime} \\boldsymbol{Z} \\widehat{\\Omega}_{1}^{-1} \\boldsymbol{Z}^{\\prime} \\Delta \\boldsymbol{X}\\right)^{-1}\\left(\\Delta \\boldsymbol{X}^{\\prime} \\boldsymbol{Z} \\widehat{\\Omega}_{1}^{-1} \\boldsymbol{Z}^{\\prime} \\Delta \\boldsymbol{Y}\\right) .\n\\]\nEstimator (17.91) is known as the one-step Arellano-Bond GMM estimator.\nUnder the assumption that the error \\(\\varepsilon_{i t}\\) is homoskedastic and serially uncorrelated, a classical covariance matrix estimator for \\(\\widehat{\\alpha}_{1}\\) is\n\\[\n\\widehat{\\boldsymbol{V}}_{1}^{0}=\\left(\\Delta \\boldsymbol{X}^{\\prime} \\boldsymbol{Z} \\widehat{\\Omega}_{1}^{-1} \\boldsymbol{Z}^{\\prime} \\Delta \\boldsymbol{X}\\right)^{-1} \\widehat{\\sigma}_{\\varepsilon}^{2}\n\\]\nwhere \\(\\widehat{\\sigma}_{\\varepsilon}^{2}\\) is the sample variance of the one-step residuals \\(\\widehat{\\boldsymbol{\\varepsilon}}_{i}=\\Delta \\boldsymbol{Y}_{i}-\\Delta \\boldsymbol{X}_{i} \\widehat{\\alpha}\\). A covariance matrix estimator which is robust to violation of these assumptions is\n\\[\n\\widehat{\\boldsymbol{V}}_{1}=\\left(\\Delta \\boldsymbol{X}^{\\prime} \\boldsymbol{Z} \\widehat{\\Omega}_{1}^{-1} \\boldsymbol{Z}^{\\prime} \\Delta \\boldsymbol{X}\\right)^{-1}\\left(\\Delta \\boldsymbol{X}^{\\prime} \\boldsymbol{Z} \\widehat{\\Omega}_{1}^{-1} \\boldsymbol{Z}^{\\prime} \\widehat{\\Omega}_{2} \\boldsymbol{Z} \\widehat{\\Omega}_{1}^{-1} \\boldsymbol{Z}^{\\prime} \\Delta \\boldsymbol{X}\\right)\\left(\\Delta \\boldsymbol{X}^{\\prime} \\boldsymbol{Z} \\widehat{\\Omega}_{1}^{-1} \\boldsymbol{Z}^{\\prime} \\Delta \\boldsymbol{X}\\right)^{-1}\n\\]\nwhere\n\\[\n\\widehat{\\Omega}_{2}=\\sum_{i=1}^{N} \\boldsymbol{Z}_{i}^{\\prime} \\widehat{\\varepsilon}_{i} \\widehat{\\boldsymbol{\\varepsilon}}_{i}^{\\prime} \\boldsymbol{Z}_{i}\n\\]\nis a (scaled) cluster-robust estimator of \\(\\Omega\\) using the one-step residuals.\nAn asymptotically efficient two-step GMM estimator which allows heterskedasticity is\n\\[\n\\widehat{\\alpha}_{2}=\\left(\\Delta \\boldsymbol{X}^{\\prime} \\boldsymbol{Z} \\widehat{\\Omega}_{2}^{-1} \\boldsymbol{Z}^{\\prime} \\Delta \\boldsymbol{X}\\right)^{-1}\\left(\\Delta \\boldsymbol{X}^{\\prime} \\boldsymbol{Z} \\widehat{\\Omega}_{2}^{-1} \\boldsymbol{Z}^{\\prime} \\Delta \\boldsymbol{Y}\\right) .\n\\]\nEstimator (17.94) is known as the two-step Arellano-Bond GMM estimator. An appropriate robust covariance matrix estimator for \\(\\widehat{\\alpha}_{2}\\) is\n\\[\n\\widehat{\\boldsymbol{V}}_{2}=\\left(\\Delta \\boldsymbol{X}^{\\prime} \\boldsymbol{Z} \\widehat{\\Omega}_{2}^{-1} \\boldsymbol{Z}^{\\prime} \\Delta \\boldsymbol{X}\\right)^{-1}\\left(\\Delta \\boldsymbol{X}^{\\prime} \\boldsymbol{Z} \\widehat{\\Omega}_{2}^{-1} \\boldsymbol{Z}^{\\prime} \\widehat{\\Omega}_{3} \\boldsymbol{Z} \\widehat{\\Omega}_{2}^{-1} \\boldsymbol{Z}^{\\prime} \\Delta \\boldsymbol{X}\\right)\\left(\\Delta \\boldsymbol{X}^{\\prime} \\boldsymbol{Z} \\widehat{\\Omega}_{2}^{-1} \\boldsymbol{Z}^{\\prime} \\Delta \\boldsymbol{X}\\right)^{-1}\n\\]\nwhere\n\\[\n\\widehat{\\Omega}_{3}=\\sum_{i=1}^{N} \\boldsymbol{Z}_{i}^{\\prime} \\widehat{\\varepsilon}_{i} \\widehat{\\boldsymbol{\\varepsilon}}_{i}^{\\prime} \\boldsymbol{Z}_{i}\n\\]\nis a (scaled) cluster-robust estimator of \\(\\Omega\\) using the two-step residuals \\(\\widehat{\\boldsymbol{\\varepsilon}}_{i}=\\Delta \\boldsymbol{Y}_{i}-\\Delta \\boldsymbol{X}_{i} \\widehat{\\alpha}_{2}\\). Asymptotically, \\(\\widehat{\\boldsymbol{V}}_{2}\\) is equivalent to\n\\[\n\\widetilde{\\boldsymbol{V}}_{2}=\\left(\\Delta \\boldsymbol{X}^{\\prime} \\boldsymbol{Z} \\widehat{\\Omega}_{2}^{-1} \\boldsymbol{Z}^{\\prime} \\Delta \\boldsymbol{X}\\right)^{-1} .\n\\]\nThe GMM estimator can be iterated until convergence to produce an iterated GMM estimator.\nThe advantage of the Arellano-Bond estimator over the Anderson-Hsiao estimator is that when \\(T>\\) \\(p+2\\) the additional (overidentified) moment conditions reduce the asymptotic variance of the estimator and stabilize its performance. The disadvantage is that when \\(T\\) is large using the full set of lags as instruments may cause a “many weak instruments” problem. The advised compromise is to limit the number of lags used as instruments.\nThe advantage of the one-step Arellano-Bond estimator is that the weight matrix \\(\\widehat{\\Omega}_{1}\\) does not depend on residuals and is therefore less random than the two-step weight matrix \\(\\widehat{\\Omega}_{2}\\). This can result in better performance by the one-step estimator in small to moderate samples especially when the errors are approximately homoskedastic. The advantage of the two-step estimator is that it achieves asymptotic efficiency allowing for heteroskedasticity and is thus expected to perform better in large samples with non-homoskedastic errors.\nTo summarize, the Arellano-Bond estimator applies GMM to the first-differenced equation (17.87) using a set of available lags \\(Y_{i, t-2}, Y_{i, t-3}, \\ldots\\) as instruments for \\(\\Delta Y_{i, t-1}, \\ldots, \\Delta Y_{i, t-p}\\).\nThe Arellano-Bond estimator may be obtained in Stata using either the xtabond or \\(x t d p d\\) command. The default setting is the one-step estimator (17.91) and non-robust standard errors (17.92). For the twostep estimator and robust standard errors use the twostep vce (robust) options. Reported standard errors in Stata are based on Windmeijer’s (2005) finite-sample correction to the asymptotic estimator (17.96). The robust covariance matrix (17.95) nor the iterated GMM estimator are implemented."
  },
  {
    "objectID": "chpt17-panel-data.html#weak-instruments",
    "href": "chpt17-panel-data.html#weak-instruments",
    "title": "16  Panel Data",
    "section": "16.40 Weak Instruments",
    "text": "16.40 Weak Instruments\nBlundell and Bond (1998) pointed out that the Anderson-Hsiao and Arellano-Bond estimators suffer from weak instruments. This can be seen easiest in the AR(1) model with the Anderson-Hsiao estimator which uses \\(Y_{i, t-2}\\) as an instrument for \\(\\Delta Y_{i, t-1}\\). The reduced form equation for \\(\\Delta Y_{i t-1}\\) is\n\\[\n\\Delta Y_{i, t-1}=Y_{i, t-2} \\gamma+v_{i t} .\n\\]\nThe reduced form coefficient \\(\\gamma\\) is defined by projection. Using \\(\\Delta Y_{i, t-1}=(\\alpha-1) Y_{i, t-2}+u_{i}+\\varepsilon_{i, t-1}\\) and \\(\\mathbb{E}\\left[Y_{i t-2} \\varepsilon_{i, t-1}\\right]=0\\) we calculate that\n\\[\n\\gamma=\\frac{\\mathbb{E}\\left[Y_{i, t-2} \\Delta Y_{i, t-1}\\right]}{\\mathbb{E}\\left[Y_{i t-2}^{2}\\right]}=(\\alpha-1)+\\frac{\\mathbb{E}\\left[Y_{i, t-2} u_{i}\\right]}{\\mathbb{E}\\left[Y_{i, t-2}^{2}\\right]} .\n\\]\nAssuming stationarity so that (17.83) holds,\n\\[\n\\mathbb{E}\\left[Y_{i, t-2} u_{i}\\right]=\\mathbb{E}\\left[\\left(\\frac{u_{i}}{1-\\alpha}+\\sum_{j=0}^{\\infty} \\alpha^{j} \\varepsilon_{i, t-2-j}\\right) u_{i}\\right]=\\frac{\\sigma_{u}^{2}}{1-\\alpha}\n\\]\nand\n\\[\n\\mathbb{E}\\left[Y_{i, t-2}^{2}\\right]=\\mathbb{E}\\left[\\left(\\frac{u_{i}}{1-\\alpha}+\\sum_{j=0}^{\\infty} \\alpha^{j} \\varepsilon_{i t-2-j}\\right)^{2}\\right]=\\frac{\\sigma_{u}^{2}}{(1-\\alpha)^{2}}+\\frac{\\sigma_{\\varepsilon}^{2}}{\\left(1-\\alpha^{2}\\right)}\n\\]\nwhere \\(\\sigma_{u}^{2}=\\mathbb{E}\\left[u_{i}^{2}\\right]\\) and \\(\\sigma_{\\varepsilon}^{2}=\\mathbb{E}\\left[\\varepsilon_{i t}^{2}\\right]\\). Using these expressions and a fair amount of algebra, Blundell and Bond (1998) found that the reduced form coefficient equals\n\\[\n\\gamma=(\\alpha-1)\\left(\\frac{k}{k+\\sigma_{u}^{2} / \\sigma_{\\varepsilon}^{2}}\\right)\n\\]\nwhere \\(k=(1-\\alpha) /(1+\\alpha)\\). The Anderson-Hsiao instrument \\(Y_{i, t-2}\\) is weak if \\(\\gamma\\) is close to zero. From (17.97) we see that \\(\\gamma=0\\) when either \\(\\alpha=1\\) (a unit root) or \\(\\sigma_{u}^{2} / \\sigma_{\\varepsilon}^{2}=\\infty\\) (the idiosyncratic effect is small relative to the individualspecific effect). In either case the coefficient \\(\\alpha\\) is not identified. We know from our earlier study of the weak instruments problem (Section 12.36) that when \\(\\gamma\\) is close to zero then \\(\\alpha\\) is weakly identified and the estimators will perform poorly. This means that when the autoregressive coefficient \\(\\alpha\\) is large or the individual-specific effect dominates the idiosyncratic effect these estimators will be weakly identified, have poor performance, and conventional inference methods will be misleading. Since the value of \\(\\alpha\\) and the relative variances are unknown a priori this means that we should generically treat this class of estimators as weakly identified.\nAn alternative estimator which has improved performance is discussed in Section 17.42."
  },
  {
    "objectID": "chpt17-panel-data.html#dynamic-panels-with-predetermined-regressors",
    "href": "chpt17-panel-data.html#dynamic-panels-with-predetermined-regressors",
    "title": "16  Panel Data",
    "section": "16.41 Dynamic Panels with Predetermined Regressors",
    "text": "16.41 Dynamic Panels with Predetermined Regressors\nThe assumption that regressors are strictly exogenous is restrictive. A less restrictive assumption is that the regressors are predetermined. Dynamic panel methods can be modified to handle predetermined regressors by using their lags as instruments.\nDefinition 17.2 The regressor \\(X_{i t}\\) is predetermined for the error \\(\\varepsilon_{i t}\\) if\n\\[\n\\mathbb{E}\\left[X_{i, t-s} \\varepsilon_{i t}\\right]=0\n\\]\nfor all \\(s \\geq 0\\).\nThe difference between strictly exogenous and predetermined regressors is that for the former (17.98) holds for all \\(s\\) not just \\(s \\geq 0\\). One way of interpreting a regression model with predetermined regressors is that the model is a projection on the complete past history of the regressors.\nUnder (17.98), leads of \\(X_{i t}\\) can be correlated with \\(\\varepsilon_{i t}\\), that is \\(\\mathbb{E}\\left[X_{i t+s} \\varepsilon_{i t}\\right] \\neq 0\\) for \\(s \\geq 1\\), or equivalently \\(X_{i t}\\) can be correlated with lags of \\(\\varepsilon_{i j}\\), that is \\(\\mathbb{E}\\left[X_{i t} \\varepsilon_{i t-s}\\right] \\neq 0\\) for \\(s \\geq 1\\). This means that \\(X_{i t}\\) can respond dynamically to past values of \\(Y_{i t}\\), as in, for example, an unrestricted vector autoregression.\nConsider the differenced equation (17.87)\n\\[\n\\Delta Y_{i t}=\\alpha_{1} \\Delta Y_{i, t-1}+\\alpha_{2} \\Delta Y_{i, t-2}+\\cdots+\\alpha_{p} \\Delta Y_{i, t-p}+\\Delta X_{i t}^{\\prime} \\beta+\\Delta \\varepsilon_{i t} .\n\\]\nWhen the regressors are predetermined but not strictly exogenous, \\(X_{i t}\\) and \\(\\varepsilon_{i t}\\) are uncorrelated but \\(\\Delta X_{i t}\\) and \\(\\Delta \\varepsilon_{i t}\\) are correlated. To see this,\n\\[\n\\begin{aligned}\n\\mathbb{E}\\left[\\Delta X_{i t} \\Delta \\varepsilon_{i t}\\right] &=\\mathbb{E}\\left[X_{i t} \\varepsilon_{i t}\\right]-\\mathbb{E}\\left[X_{i, t-1} \\varepsilon_{i t}\\right]-\\mathbb{E}\\left[X_{i t} \\varepsilon_{i, t-1}\\right]+\\mathbb{E}\\left[X_{i, t-1} \\varepsilon_{i, t-1}\\right] \\\\\n&=-\\mathbb{E}\\left[X_{i t} \\varepsilon_{i, t-1}\\right] \\neq 0 .\n\\end{aligned}\n\\]\nThis means that if we treat \\(\\Delta X_{i t}\\) as exogenous the coefficient estimates will be biased.\nTo solve the correlation problem we can use instruments for \\(\\Delta X_{i t}\\). A valid instrument is \\(X_{i, t-1}\\) because it is generally correlated with \\(\\Delta X_{i t}\\) yet uncorrelated with \\(\\Delta \\varepsilon_{i t}\\). Indeed, for any \\(s \\geq 1\\)\n\\[\n\\mathbb{E}\\left[X_{i, t-s} \\Delta \\varepsilon_{i t}\\right]=\\mathbb{E}\\left[X_{i, t-s} \\varepsilon_{i t}\\right]-\\mathbb{E}\\left[X_{i, t-s} \\varepsilon_{i, t-1}\\right]=0 .\n\\]\nConsequently, Arellano and Bond (1991) recommend the instrument set \\(\\left(X_{i 1}, X_{i 2}, \\ldots, X_{i t-1}\\right)\\). When the number of time periods is large it is advised to limit the number of instrument lags to avoid the many weak instruments problem. Algebraically, GMM estimation is the same as the estimators described in Section 17.39, except that the instrument matrix (17.89) is modified to\n\nTo understand how the model is identified we examine the reduced form equation for the regressor. For \\(t=p+2\\) and using the first lag as an instrument the reduced form is\n\\[\n\\Delta X_{i t}=\\gamma_{1} Y_{i, t-2}+\\Gamma_{2} X_{i, t-1}+\\zeta_{i t} .\n\\]\nThe model is identified if \\(\\Gamma_{2}\\) is full rank. This is valid (in general) when \\(X_{i t}\\) is stationary. Identification fails, however, when \\(X_{i t}\\) has a unit root. This indicates that the model will be weakly identified when the predetermined regressors are highly persistent.\nThe method generalizes to handle multiple lags of the predetermined regressors. To see this, write the model explicitly as\n\\[\nY_{i t}=\\alpha_{1} Y_{i, t-1}+\\cdots+\\alpha_{p} Y_{i, t-p}+X_{i t}^{\\prime} \\beta_{1}+\\cdots+X_{i, t-q}^{\\prime} \\beta_{q}+u_{i}+\\varepsilon_{i t} .\n\\]\nIn first differences the model is\n\\[\n\\Delta Y_{i t}=\\alpha_{1} \\Delta Y_{i, t-1}+\\cdots+\\alpha_{p} \\Delta Y_{i, t-p}+\\Delta X_{i t}^{\\prime} \\beta_{1}+\\cdots+\\Delta X_{i, t-q}^{\\prime} \\beta_{q}+\\Delta \\varepsilon_{i t} .\n\\]\nA sufficient set of instruments for the regressors are \\(\\left(X_{i t-1}, \\Delta X_{i, t-1}, \\ldots, \\Delta X_{i, t-q}\\right)\\) or equivalently \\(\\left(X_{i, t-1}, X_{i, t-2}, \\ldots, X_{i, t-q-1}\\right)\\).\nIn many cases it is more reasonable to assume that \\(X_{i t-1}\\) is predetermined but not \\(X_{i t}\\), because \\(X_{i t}\\) and \\(\\varepsilon_{i t}\\) may be endogenous. This, for example, is the standard assumption in vector autoregressions. In this case the estimation method is modified to use the instruments \\(\\left(X_{i, t-2}, X_{i, t-3}, \\ldots, X_{i, t-q-1}\\right)\\). While this weakens the exogeneity assumption it also weakens the instrument set as now the reduced form uses the second lag \\(X_{i, t-2}\\) to predict \\(\\Delta X_{i t}\\).\nThe advantage obtained by treating a regressor as predetermined (rather than strictly exogenous) is that it is a substantial relaxation of the dynamic assumptions. Otherwise the parameter estimates will be inconsistent due to endogeneity.\nThe major disadvantage of treating a regressor as predetermined is that it substantially reduces the strength of identification especially when the predetermined regressors are highly persistent.\nIn Stata the xtabond command by default treats independent regressors as strictly exogenous. To treat the regressors as predetermined use the option pre. By default all regressor lags are used as instruments, but the number can be limited if specified."
  },
  {
    "objectID": "chpt17-panel-data.html#blundell-bond-estimator",
    "href": "chpt17-panel-data.html#blundell-bond-estimator",
    "title": "16  Panel Data",
    "section": "16.42 Blundell-Bond Estimator",
    "text": "16.42 Blundell-Bond Estimator\nArellano and Bover (1995) and Blundell and Bond (1998) introduced a set of orthogonality conditions which reduce the weak instrument problem discussed in the Section \\(17.40\\) and improve performance in finite samples.\nConsider the levels AR(1) model with no regressors (17.82). Recall, least squares (pooled) regression is inconsistent because the regressor \\(Y_{i, t-1}\\) is correlated with the error \\(u_{i}\\). This raises the question: Is there an instrument \\(Z_{i t}\\) which solves this problem in the sense that \\(Z_{i t}\\) is correlated with \\(Y_{i, t-1}\\) yet uncorrelated with \\(u_{i t}+\\varepsilon_{i t}\\) ? Blundell-Bond propose the instrument \\(\\Delta Y_{i, t-1}\\). Clearly, \\(\\Delta Y_{i, t-1}\\) and \\(Y_{i, t-1}\\) are correlated so \\(\\Delta Y_{i, t-1}\\) satisfies the relevance condition. Also, \\(\\Delta Y_{i, t-1}\\) is uncorrelated with the idiosyncratic error \\(\\varepsilon_{i t}\\) when the latter is serially uncorrelated. Thus the key to the Blundell-Bond instrument is whether or not\n\\[\n\\mathbb{E}\\left[\\Delta Y_{i t-1} u_{i}\\right]=0 .\n\\]\nBlundell and Bond (1998) show that a sufficient condition for (17.100) is\n\\[\n\\mathbb{E}\\left[\\left(Y_{i 1}-\\frac{u_{i}}{1-\\alpha}\\right) u_{i}\\right]=0 .\n\\]\nRecall that \\(u_{i} /(1-\\alpha)\\) is the conditional mean of \\(Y_{i t}\\) under stationarity. Condition (17.101) states that the deviation of the initial condition \\(Y_{i 1}\\) from this conditional mean is uncorrelated with the individual effect \\(u_{i}\\). Condition (17.101) is implied by stationarity but is somewhat weaker.\nTo see that (17.101) implies (17.100), by applying recursion to (17.87) we find that\n\\[\n\\Delta Y_{i, t-1}=\\alpha^{t-3} \\Delta Y_{i 2}+\\sum_{j=0}^{t-3} \\alpha^{j} \\Delta \\varepsilon_{i, t-1-j} .\n\\]\nAlso,\n\\[\n\\Delta Y_{i 2}=(\\alpha-1) Y_{i 1}+u_{i}+\\varepsilon_{i 2}=(\\alpha-1)\\left(Y_{i 1}-\\frac{u_{i}}{1-\\alpha}\\right)+\\varepsilon_{i 2} .\n\\]\nHence\n\\[\n\\begin{aligned}\n\\mathbb{E}\\left[\\Delta Y_{i, t-1} u_{i}\\right] &=\\mathbb{E}\\left[\\left(\\alpha^{t-3}(\\alpha-1)\\left(Y_{i 1}-\\frac{u_{i}}{1-\\alpha}\\right)+\\alpha^{t-3} \\varepsilon_{i 2}+\\sum_{j=0}^{t-3} \\alpha^{j} \\Delta \\varepsilon_{i, t-1-j}\\right) u_{i}\\right] \\\\\n&=\\alpha^{t-3}(\\alpha-1) \\mathbb{E}\\left[\\left(Y_{i 1}-\\frac{u_{i}}{1-\\alpha}\\right) u_{i}\\right] \\\\\n&=0\n\\end{aligned}\n\\]\nunder (17.101), as claimed.\nNow consider the full model (17.81) with predetermined regressors. Consider the assumption that the regressors have constant correlation with the individual effect\n\\[\n\\mathbb{E}\\left[X_{i t} u_{i}\\right]=\\mathbb{E}\\left[X_{i s} u_{i}\\right]\n\\]\nfor all \\(s\\). This implies\n\\[\n\\mathbb{E}\\left[\\Delta X_{i t} u_{i}\\right]=0\n\\]\nwhich means that the differenced predetermined regressors \\(\\Delta X_{i t}\\) can also be used as instruments for the level equation.\nUsing (17.100) and (17.102) Blundell and Bond propose the following moment conditions for GMM estimation\n\\[\n\\begin{gathered}\n\\mathbb{E}\\left[\\Delta Y_{i, t-1}\\left(Y_{i t}-\\alpha_{1} Y_{i, t-1}-\\cdots-\\alpha_{p} Y_{i, t-p}-X_{i t}^{\\prime} \\beta\\right)\\right]=0 \\\\\n\\mathbb{E}\\left[\\Delta X_{i, t}\\left(Y_{i t}-\\alpha_{1} Y_{i, t-1}-\\cdots-\\alpha_{p} Y_{i, t-p}-X_{i t}^{\\prime} \\beta\\right)\\right]=0\n\\end{gathered}\n\\]\nfor \\(t=p+2, \\ldots, T\\). Notice that these are for the levels (undifferenced) equation while the Arellano-Bond (17.90) moments are for the differenced equation (17.87). We can write (17.103)-(17.104) in vector notation if we set \\(\\boldsymbol{Z}_{2 i}=\\operatorname{diag}\\left(\\Delta Y_{i 2}, \\ldots, \\Delta Y_{i T-1}, \\Delta X_{i 3}, \\ldots, \\Delta X_{i T}\\right)\\). Then (17.103)-(17.104) equals\n\\[\n\\mathbb{E}\\left[\\boldsymbol{Z}_{2 i}\\left(\\boldsymbol{Y}_{i}-\\boldsymbol{X}_{i} \\theta\\right)\\right]=0 .\n\\]\nBlundell and Bond proposed combining the \\(\\ell\\) Arellano-Bond moments with the levels moments. This can be done by stacking the moment conditions (17.90) and (17.105). Recall from Section \\(17.39\\) the variables \\(\\Delta \\boldsymbol{Y}_{i}, \\Delta \\boldsymbol{X}_{i}\\), and \\(\\boldsymbol{Z}_{i}\\). Define the stacked variables \\(\\overline{\\boldsymbol{Y}}_{i}=\\left(\\Delta \\boldsymbol{Y}_{i}^{\\prime}, \\boldsymbol{Y}_{i}^{\\prime}\\right)^{\\prime}, \\overline{\\boldsymbol{X}}_{i}=\\left(\\Delta \\boldsymbol{X}_{i}^{\\prime}, \\boldsymbol{X}_{i}^{\\prime}\\right)^{\\prime}\\) and \\(\\overline{\\boldsymbol{Z}}_{i}=\\) \\(\\operatorname{diag}\\left(\\boldsymbol{Z}_{i}, \\boldsymbol{Z}_{2 i}\\right)\\). The stacked moment conditions are\n\\[\n\\mathbb{E}\\left[\\overline{\\boldsymbol{Z}}_{i}\\left(\\overline{\\boldsymbol{Y}}_{i}-\\overline{\\boldsymbol{X}}_{i} \\theta\\right)\\right]=0 .\n\\]\nThe Blundell-Bond estimator is found by applying GMM to this equation. They call this a systems GMM estimator. Let \\(\\overline{\\boldsymbol{Y}}, \\overline{\\boldsymbol{X}}\\), and \\(\\overline{\\boldsymbol{Z}}\\) denote \\(\\overline{\\boldsymbol{Y}}_{i}, \\overline{\\boldsymbol{X}}_{i}\\), and \\(\\overline{\\boldsymbol{Z}}_{i}\\) stacked into matrices. Define \\(\\overline{\\boldsymbol{H}}=\\operatorname{diag}\\left(\\boldsymbol{H}, \\boldsymbol{I}_{T-2}\\right)\\) where \\(\\boldsymbol{H}\\) is from (17.31) and set\n\\[\n\\widehat{\\Omega}_{1}=\\sum_{i=1}^{N} \\overline{\\boldsymbol{Z}}_{i}^{\\prime} \\overline{\\boldsymbol{Z Z}}_{i} .\n\\]\nThe Blundell-Bond one-step GMM estimator is\n\\[\n\\widehat{\\theta}_{1}=\\left(\\overline{\\boldsymbol{X}}^{\\prime} \\overline{\\boldsymbol{Z}} \\widehat{\\Omega}_{1}^{-1} \\overline{\\boldsymbol{Z}}^{\\prime} \\overline{\\boldsymbol{X}}\\right)^{-1}\\left(\\overline{\\boldsymbol{X}}^{\\prime} \\overline{\\boldsymbol{Z}} \\widehat{\\Omega}_{1}^{-1} \\overline{\\boldsymbol{Z}}^{\\prime} \\overline{\\boldsymbol{Y}}\\right) .\n\\]\nThe systems residuals are \\(\\widehat{\\boldsymbol{\\varepsilon}}_{i}=\\overline{\\boldsymbol{Y}}_{i}-\\overline{\\boldsymbol{X}}_{i} \\widehat{\\theta}_{1}\\). A robust covariance matrix estimator is\n\\[\n\\widehat{\\boldsymbol{V}}_{1}=\\left(\\overline{\\boldsymbol{X}}^{\\prime} \\overline{\\boldsymbol{Z}} \\widehat{\\Omega}_{1}^{-1} \\overline{\\boldsymbol{Z}}^{\\prime} \\overline{\\boldsymbol{X}}\\right)^{-1}\\left(\\overline{\\boldsymbol{X}}^{\\prime} \\overline{\\boldsymbol{Z}} \\widehat{\\Omega}_{1}^{-1} \\overline{\\boldsymbol{Z}}^{\\prime} \\widehat{\\Omega}_{2} \\overline{\\boldsymbol{Z}} \\widehat{\\Omega}_{1}^{-1} \\overline{\\boldsymbol{Z}}^{\\prime} \\overline{\\boldsymbol{X}}\\right)\\left(\\overline{\\boldsymbol{X}}^{\\prime} \\overline{\\boldsymbol{Z}} \\widehat{\\Omega}_{1}^{-1} \\overline{\\boldsymbol{Z}}^{\\prime} \\overline{\\boldsymbol{X}}\\right)^{-1}\n\\]\nwhere\n\\[\n\\widehat{\\Omega}_{2}=\\sum_{i=1}^{N} \\overline{\\boldsymbol{Z}}_{i}^{\\prime} \\widehat{\\boldsymbol{\\varepsilon}}_{i} \\widehat{\\boldsymbol{\\varepsilon}}_{i}^{\\prime} \\overline{\\boldsymbol{Z}}_{i} .\n\\]\nThe Blundell-Bond two-step GMM estimator is\n\\[\n\\widehat{\\theta}_{2}=\\left(\\overline{\\boldsymbol{X}}^{\\prime} \\overline{\\boldsymbol{Z}} \\widehat{\\Omega}_{2}^{-1} \\overline{\\boldsymbol{Z}}^{\\prime} \\overline{\\boldsymbol{X}}\\right)^{-1}\\left(\\overline{\\boldsymbol{X}}^{\\prime} \\overline{\\boldsymbol{Z}}^{-1} \\overline{\\boldsymbol{Z}}^{\\prime} \\overline{\\boldsymbol{Y}}\\right) .\n\\]\nThe two-step systems residuals are \\(\\widehat{\\boldsymbol{\\varepsilon}}_{i}=\\overline{\\boldsymbol{Y}}_{i}-\\overline{\\boldsymbol{X}}_{i} \\widehat{\\theta}_{2}\\). A robust covariance matrix estimator is\n\\[\n\\widehat{\\boldsymbol{V}}_{2}=\\left(\\overline{\\boldsymbol{X}}^{\\prime} \\overline{\\boldsymbol{Z}} \\widehat{\\Omega}_{2}^{-1} \\overline{\\boldsymbol{Z}}^{\\prime} \\overline{\\boldsymbol{X}}\\right)^{-1}\\left(\\overline{\\boldsymbol{X}}^{\\prime} \\overline{\\boldsymbol{Z}} \\widehat{\\Omega}_{2}^{-1} \\overline{\\boldsymbol{Z}}^{\\prime} \\widehat{\\Omega}_{3} \\overline{\\boldsymbol{Z}} \\widehat{\\Omega}_{2}^{-1} \\overline{\\boldsymbol{Z}}^{\\prime} \\overline{\\boldsymbol{X}}\\right)\\left(\\overline{\\boldsymbol{X}}^{\\prime} \\overline{\\boldsymbol{Z}}_{2}^{-1} \\overline{\\boldsymbol{Z}}^{\\prime} \\overline{\\boldsymbol{X}}\\right)^{-1}\n\\]\nwhere\n\\[\n\\widehat{\\Omega}_{3}=\\sum_{i=1}^{N} \\overline{\\boldsymbol{Z}}_{i}^{\\prime} \\widehat{\\boldsymbol{\\varepsilon}}_{i} \\widehat{\\boldsymbol{\\varepsilon}}_{i}^{\\prime} \\overline{\\boldsymbol{Z}}_{i} .\n\\]\nAsymptotically, \\(\\widehat{\\boldsymbol{V}}_{2}\\) is equivalent to\n\\[\n\\widetilde{\\boldsymbol{V}}_{2}=\\left(\\overline{\\boldsymbol{X}}^{\\prime} \\overline{\\boldsymbol{Z}} \\widehat{\\Omega}_{2}^{-1} \\overline{\\boldsymbol{Z}}^{\\prime} \\overline{\\boldsymbol{X}}\\right)^{-1} .\n\\]\nThe GMM estimator can be iterated until convergence to produce an iterated GMM estimator.\nSimulation experiments reported in Blundell and Bond (1998) indicate that their systems GMM estimator performs substantially better than the Arellano-Bond estimator, especially when \\(\\alpha\\) is close to one or the variance ratio \\(\\sigma_{u}^{2} / \\sigma_{\\varepsilon}^{2}\\) is large. The explanation is that the orthogonality condition (17.103) does not suffer the weak instrument problem in these cases.\nThe advantage of the Blundell-Bond estimator is that the added orthogonality condition (17.103) greatly improves performance relative to the Arellano-Bond estimator when the latter is weakly identified. A disadvantage of the Blundell-Bond estimator is that their orthogonality condition is justified by a stationarity condition (17.101) and violation of the latter may induce estimation bias.\nThe advantages and disadvantages of the one-step versus two-step Blundell-Bond estimators are the same as described for the Arellano-Bond estimator as described in Section 17.39. Also as described there when \\(T\\) is large it may be desired to limit the number of lags to use as instruments in order to avoid the many weak instruments problem.\nThe Blundell-Bond estimator may be obtained in Stata using either the xtdpdsys or xtdpd command. The default setting is the one-step estimator (17.106) and non-robust standard errors. For the two-step estimator and robust standard errors use the twostep vce (robust) options. Stata standard errors are Windmeijer’s (2005) finite-sample correction to the asymptotic estimate (17.110). The robust covariance matrix estimator (17.109) nor the iterated GMM estimator are implemented."
  },
  {
    "objectID": "chpt17-panel-data.html#forward-orthogonal-transformation",
    "href": "chpt17-panel-data.html#forward-orthogonal-transformation",
    "title": "16  Panel Data",
    "section": "16.43 Forward Orthogonal Transformation",
    "text": "16.43 Forward Orthogonal Transformation\nArellano and Bover (1995) proposed an alternative transformation which eliminates the individualspecific effect and may have advantages in dynamic panel models. The forward orthogonal transformation is\n\\[\nY_{i t}^{*}=c_{i t}\\left(Y_{i t}-\\frac{1}{T_{i}-t}\\left(Y_{i, t+1}+\\cdots+Y_{i T_{i}}\\right)\\right)\n\\]\nwhere \\(c_{i t}^{2}=\\left(T_{i}-t\\right) /\\left(T_{i}-t+1\\right)\\). This can be applied to all but the final observation (which is lost). Essentially, \\(Y_{i t}^{*}\\) subtracts from \\(Y_{i t}\\) the average of the remaining values and then rescales so that the variance is constant under the assumption of homoskedastic errors. The transformation (17.111) was originally proposed for time-series observations by Hayashi and Sims (1983).\nAt the level of the individual this can be written as \\(\\boldsymbol{Y}_{i}^{*}=\\boldsymbol{A}_{i} \\boldsymbol{Y}_{i}\\) where \\(\\boldsymbol{A}_{i}\\) is the \\(\\left(T_{i}-1\\right) \\times T_{i}\\) orthogonal deviation operator\n\\[\n\\boldsymbol{A}_{i}=\\operatorname{diag}\\left(\\sqrt{\\frac{T_{i}-1}{T_{i}}}, \\ldots, \\sqrt{\\frac{1}{2}}\\right)\\left[\\begin{array}{ccccccc}\n1 & -\\frac{1}{T_{i}-1} & -\\frac{1}{T_{i}-1} & \\cdots & -\\frac{1}{T_{i}-1} & -\\frac{1}{T_{i}-1} & -\\frac{1}{T_{i}-1} \\\\\n0 & 1 & -\\frac{1}{T_{i}-2} & \\cdots & -\\frac{1}{T_{i}-2} & -\\frac{1}{T_{i}-2} & -\\frac{1}{T_{i}-2} \\\\\n\\vdots & \\vdots & \\vdots & & \\vdots & \\vdots & \\vdots \\\\\n0 & 0 & 0 & \\cdots & 1 & -\\frac{1}{2} & -\\frac{1}{2} \\\\\n0 & 0 & 0 & \\cdots & 0 & -1 & 1\n\\end{array}\\right] .\n\\]\nImportant properties of the matrix \\(\\boldsymbol{A}_{i}\\) are that \\(\\boldsymbol{A}_{i} \\mathbf{1}_{i}=0\\) (so it eliminates individual effects), \\(\\boldsymbol{A}_{i}^{\\prime} \\boldsymbol{A}_{i}=\\boldsymbol{M}_{i}\\), and \\(\\boldsymbol{A}_{i} \\boldsymbol{A}_{i}^{\\prime}=\\boldsymbol{I}_{T_{i}-1}\\). These can be verified by direct multiplication.\nApplying the transformation \\(\\boldsymbol{A}_{i}\\) to (17.81) we obtain\n\\[\nY_{i t}^{*}=\\alpha_{1} Y_{i, t-1}^{*}+\\cdots+\\alpha_{p} Y_{i, t-p}^{*}+X_{i t}^{* \\prime} \\beta+\\varepsilon_{i t}^{*} .\n\\]\nfor \\(t=p+1, \\ldots, T-1\\). This is equivalent to first differencing (17.87) when \\(T=3\\) but differs for \\(T>3\\).\nWhat is special about the transformed equation (17.112) is that under the assumption that \\(\\varepsilon_{i t}\\) are serially uncorrelated and homoskedastic the error vector \\(\\boldsymbol{\\varepsilon}_{i}^{*}\\) has variance \\(\\sigma_{\\varepsilon}^{2} \\boldsymbol{A}_{i} \\boldsymbol{A}_{i}^{\\prime}=\\sigma_{\\varepsilon}^{2} \\boldsymbol{I}_{T_{i}-1}\\). This means that \\(\\varepsilon_{i}^{*}\\) has the same covariance structure as \\(\\varepsilon_{i}\\). Thus the orthogonal transformation operator eliminates the fixed effect while preserving the covariance structure. This is in contrast to (17.87) which has serially correlated errors \\(\\Delta \\varepsilon_{i t}\\).\nThe transformed error \\(\\varepsilon_{i t}^{*}\\) is a function of \\(\\varepsilon_{i t}, \\varepsilon_{i t+1}, \\ldots, \\varepsilon_{i T}\\). Thus valid instruments are \\(Y_{i t-1}, Y_{i t-2}, \\ldots\\). Using the instrument matrix \\(Z_{i}\\) from (17.89) in the case of strictly exogenous regressors or (17.99) with predetermined regressors the \\(\\ell\\) moment conditions can be written using matrix notation as\n\\[\n\\mathbb{E}\\left[\\boldsymbol{Z}_{i}^{\\prime}\\left(\\boldsymbol{Y}_{i}^{*}-\\boldsymbol{X}_{i}^{*} \\theta\\right)\\right]=0 .\n\\]\nDefine the \\(\\ell \\times \\ell\\) covariance matrix\n\\[\n\\Omega=\\mathbb{E}\\left[Z_{i}^{\\prime} \\varepsilon_{i}^{*} \\varepsilon_{i}^{* \\prime} Z_{i}\\right]\n\\]\nIf the errors \\(\\varepsilon_{i t}\\) are conditionally homoskedastic then \\(\\Omega=\\mathbb{E}\\left[\\boldsymbol{Z}_{i}^{\\prime} \\boldsymbol{Z}_{i}\\right] \\sigma_{\\varepsilon}^{2}\\). Thus an asymptotically efficient GMM estimator is 2SLS applied to the orthogonalized equation using \\(Z_{i}\\) as an instrument. In matrix notation,\n\\[\n\\widehat{\\theta}_{1}=\\left(\\boldsymbol{X}^{* \\prime} \\boldsymbol{Z}\\left(\\boldsymbol{Z}^{\\prime} \\boldsymbol{Z}\\right)^{-1} \\boldsymbol{Z}^{\\prime} \\boldsymbol{X}^{*}\\right)^{-1} \\boldsymbol{Y}^{*} \\text {. }\n\\]\nThis is the one-step GMM estimator.\nGiven the residuals \\(\\widehat{\\boldsymbol{\\varepsilon}}_{i}=\\boldsymbol{Y}_{i}^{*}-\\boldsymbol{X}_{i}^{*} \\widehat{\\theta}_{1}\\) the two-step GMM estimator which is robust to heteroskedasticity and arbitrary serial correlation is\n\\[\n\\widehat{\\theta}_{2}=\\left(\\boldsymbol{X}^{* \\prime} \\boldsymbol{Z} \\widehat{\\Omega}_{2}^{-1} \\boldsymbol{Z}^{\\prime} \\boldsymbol{X}^{*}\\right)^{-1}\\left(\\boldsymbol{X}^{* \\prime} \\boldsymbol{Z} \\widehat{\\Omega}_{2}^{-1} \\boldsymbol{Z}^{\\prime} \\boldsymbol{Y}^{*}\\right)\n\\]\nwhere\n\\[\n\\widehat{\\Omega}_{2}=\\sum_{i=1}^{N} \\boldsymbol{Z}_{i}^{\\prime} \\widehat{\\boldsymbol{\\varepsilon}}_{i} \\widehat{\\boldsymbol{\\varepsilon}}_{i}^{\\prime} \\boldsymbol{Z}_{i} .\n\\]\nStandard errors for \\(\\widehat{\\theta}_{1}\\) and \\(\\widehat{\\theta}_{2}\\) can be obtained using cluster-robust methods.\nForward orthogonalization may have advantages over first differencing. First, the equation errors in (17.112) have a scalar covariance structure under i.i.d. idiosyncratic errors which is expected to improve estimation precision. It also implies that the one-step estimator is 2SLS rather than GMM. Second, while there has not been a formal analysis of the weak instrument properties of the estimators after forward orthogonalization it appears that if \\(T>p+2\\) the method is less affected by weak instruments than first differencing. The disadvantages of forward orthogonalization are that it treats early observations asymmetrically from late observations, it is less thoroughly studied than first differencing, and is not available with several popular estimation methods.\nThe Stata command xtdpd includes forward orthogonalization as an option but not when levels (Blundell-Bond) instruments are included or if there are gaps in the data. An alternative is the downloadable Stata package \\(x\\) tabond2."
  },
  {
    "objectID": "chpt17-panel-data.html#empirical-illustration",
    "href": "chpt17-panel-data.html#empirical-illustration",
    "title": "16  Panel Data",
    "section": "16.44 Empirical Illustration",
    "text": "16.44 Empirical Illustration\nWe illustrate the dynamic panel methods with the investment model (17.3). Estimates from two models are presented in Table 17.3. Both are estimated by Blundell-Bond two-step GMM with lags 2 through 6 as instruments, a cluster-robust weight matrix, and clustered standard errors.\nThe first column presents estimates of an AR(2) model. The estimates show that the series has a moderate amount of positive serial correlation but appears to be well modeled as an AR(1) as the AR(2) coefficient is close to zero. This pattern of serial correlation is consistent with the presence of investment projects which span two years.\nThe second column presents estimates of the dynamic version of the investment regression (17.3) excluding the trading indicator. Two lags are included of the dependent variable and each regressor. The regressors are treated as predetermined in contrast to the fixed effects regressions which treated the regressors as strictly exogenous. The regressors are not contemporaneous with the dependent variable but lagged one and two periods. This is done so that they are valid predetermined variables. Contemporaneous variables are likely endogenous so should not be treated as predetermined.\nThe estimates in the second column of Table \\(17.3\\) complement the earlier results. The evidence shows that investment has a moderate degree of serial dependence, is positively related to the first lag of Q, and is negatively related to lagged debt. Investment appears to be positively related to change in cash flow, rather than the level. Thus an increase in cash flow in year \\(t-1\\) leads to investment in year \\(t\\). Table 17.3: Estimates of Dynamic Investment Equation\n\n\n\n\nAR(2)\nAR(2) with Regressors\n\n\n\n\n\\(I_{i t-1}\\)\n\\(0.3191\\)\n\\(0.2519\\)\n\n\n\n\\((0.0172)\\)\n\\((0.0220)\\)\n\n\n\\(I_{i t-2}\\)\n\\(0.0309\\)\n\\(0.0137\\)\n\n\n\n\\((0.0112)\\)\n\\((0.0125)\\)\n\n\n\\(Q_{i t-1}\\)\n\n\\(0.0018\\)\n\n\n\n\n\\((0.0007)\\)\n\n\n\\(Q_{i t-2}\\)\n\n\\(-0.0000\\)\n\n\n\n\n\\((0.0003)\\)\n\n\n\\(D_{i t-1}\\)\n\n\\(-0.0154\\)\n\n\n\n\n\\((0.0058)\\)\n\n\n\\(D_{i t-2}\\)\n\n\\(-0.0043\\)\n\n\n\n\n\\((0.0054)\\)\n\n\n\\(C F_{i t-1}\\)\n\n\\(0.0400\\)\n\n\n\n\n\\((0.0091)\\)\n\n\n\\(C F_{i t-2}\\)\n\n\\(-0.0290\\)\n\n\n\n\n\\((0.0051)\\)\n\n\n\nTwo-step GMM estimates. Cluster-robust standard errors in parenthesis.\nAll regressions include time effects. GMM instruments include lags 2 through 6."
  },
  {
    "objectID": "chpt17-panel-data.html#exercises",
    "href": "chpt17-panel-data.html#exercises",
    "title": "16  Panel Data",
    "section": "16.45 Exercises",
    "text": "16.45 Exercises"
  },
  {
    "objectID": "chpt17-panel-data.html#exercise-17.1",
    "href": "chpt17-panel-data.html#exercise-17.1",
    "title": "16  Panel Data",
    "section": "16.46 Exercise 17.1",
    "text": "16.46 Exercise 17.1\n\nShow (17.11) and (17.12).\nShow (17.13).\n\nExercise 17.2 Is \\(\\mathbb{E}\\left[\\varepsilon_{i t} \\mid X_{i t}\\right]=0\\) sufficient for \\(\\widehat{\\beta}_{\\mathrm{fe}}\\) to be unbiased for \\(\\beta\\) ? Explain why or why not.\nExercise 17.3 Show that \\(\\operatorname{var}\\left[\\dot{X}_{i t}\\right] \\leq \\operatorname{var}\\left[X_{i t}\\right]\\)\nExercise 17.4 Show (17.24).\nExercise 17.5 Show (17.28).\nExercise 17.6 Show that when \\(T=2\\) the differenced estimator equals the fixed effects estimator.\nExercise 17.7 In Section \\(17.14\\) it is described how to estimate the individual-effect variance \\(\\sigma_{u}^{2}\\) using the between residuals. Develop an alternative estimator of \\(\\sigma_{u}^{2}\\) only using the fixed effects error variance \\(\\widehat{\\sigma}_{\\varepsilon}^{2}\\) and the levels error variance \\(\\widehat{\\sigma}_{e}^{2}=n^{-1} \\sum_{i=1}^{N} \\sum_{t \\in S_{i}} \\widehat{e}_{i t}^{2}\\) where \\(\\widehat{e}_{i t}=Y_{i t}-X_{i t}^{\\prime} \\widehat{\\beta}_{\\text {fe }}\\) are computed from the levels variables.\nExercise 17.8 Verify that \\(\\widehat{\\sigma}_{\\varepsilon}^{2}\\) defined in (17.37) is unbiased for \\(\\sigma_{\\varepsilon}^{2}\\) under (17.18), (17.25) and (17.26). Exercise 17.9 Develop a version of Theorem \\(17.2\\) for the differenced estimator \\(\\widehat{\\beta}_{\\Delta}\\). Can you weaken Assumption 17.2.3? State an appropriate version which is sufficient for asymptotic normality.\nExercise 17.10 Show (17.57)."
  },
  {
    "objectID": "chpt17-panel-data.html#exercise-17.11",
    "href": "chpt17-panel-data.html#exercise-17.11",
    "title": "16  Panel Data",
    "section": "16.47 Exercise \\(17.11\\)",
    "text": "16.47 Exercise \\(17.11\\)\n\nFor \\(\\widehat{\\sigma}_{i}^{2}\\) defined in (17.59) show \\(\\mathbb{E}\\left[\\widehat{\\sigma}_{i}^{2} \\mid \\boldsymbol{X}_{i}\\right]=\\bar{\\sigma}_{i}^{2}\\)\nFor \\(\\widetilde{\\boldsymbol{V}}_{\\mathrm{fe}}\\) defined in (17.58) show \\(\\mathbb{E}\\left[\\widetilde{\\boldsymbol{V}}_{\\mathrm{fe}} \\mid \\boldsymbol{X}\\right]=\\boldsymbol{V}_{\\mathrm{fe}}\\)."
  },
  {
    "objectID": "chpt17-panel-data.html#exercise-17.12",
    "href": "chpt17-panel-data.html#exercise-17.12",
    "title": "16  Panel Data",
    "section": "16.48 Exercise \\(17.12\\)",
    "text": "16.48 Exercise \\(17.12\\)\n\nShow (17.61).\\\nShow (17.62).\\\nFor \\(\\widetilde{\\boldsymbol{V}}_{\\mathrm{fe}}\\) defined in (17.60) show \\(\\mathbb{E}\\left[\\widetilde{\\boldsymbol{V}}_{\\mathrm{fe}} \\mid \\boldsymbol{X}\\right]=\\boldsymbol{V}_{\\mathrm{fe}}\\).\n\nExercise 17.13 Take the fixed effects model \\(Y_{i t}=X_{i t} \\beta_{1}+X_{i t}^{2} \\beta_{2}+u_{i}+\\varepsilon_{i t}\\). A researcher estimates the model by first obtaining the within transformed \\(\\dot{Y}_{i t}\\) and \\(\\dot{X}_{i t}\\) and then regressing \\(\\dot{Y}_{i t}\\) on \\(\\dot{X}_{i t}\\) and \\(\\dot{X}_{i t}^{2}\\). Is the correct estimation method? If not, describe the correct fixed effects estimator.\nExercise 17.14 In Section \\(17.33\\) verify that in the just-identified case the 2SLS estimator \\(\\widehat{\\beta}_{2 \\text { sls }}\\) simplifies as claimed: \\(\\widehat{\\beta}_{1}\\) and \\(\\widehat{\\beta}_{2}\\) are the fixed effects estimator. \\(\\widehat{\\gamma}_{1}\\) and \\(\\widehat{\\gamma}_{2}\\) equal the 2SLS estimator from a regression of \\(\\widehat{\\boldsymbol{u}}\\) on \\(Z_{1}\\) and \\(Z_{2}\\) using \\(\\bar{X}_{1}\\) as an instrument for \\(Z_{2}\\).\nExercise 17.15 In this exercise you will replicate and extend the empirical work reported in Arellano and Bond (1991) and Blundell and Bond (1998). Arellano-Bond gathered a dataset of 1031 observations from an unbalanced panel of 140 U.K. companies for 1976-1984 and is in the datafile AB1991 on the textbook webpage. The variables we will be using are log employment \\((N)\\), log real wages \\((W)\\), and log capital \\((K)\\). See the description file for definitions.\n\nEstimate the panel AR(1) \\(K_{i t}=\\alpha K_{i t-1}+u_{i}+v_{t}+\\varepsilon_{i t}\\) using Arellano-Bond one-step GMM with clustered standard errors. Note that the model includes year fixed effects.\nRe-estimate using Blundell-Bond one-step GMM with clustered standard errors.\nExplain the difference in the estimates.\n\nExercise 17.16 This exercise uses the same dataset as the previous question. Blundell and Bond (1998) estimated a dynamic panel regression of log employment \\(N\\) on log real wages \\(W\\) and log capital \\(K\\). The following specification \\({ }^{1}\\) used the Arellano-Bond one-step estimator, treating \\(W_{i, t-1}\\) and \\(K_{i, t-1}\\) as predetermined.\n\nThis equation also included year dummies and the standard errors are clustered.\n\\({ }^{1}\\) Blundell and Bond (1998), Table 4, column 3. (a) Estimate (17.114) using the Arellano-Bond one-step estimator treating \\(W_{i t}\\) and \\(K_{i t}\\) as strictly exogenous.\n\nEstimate (17.114) treating \\(W_{i, t-1}\\) and \\(K_{i, t-1}\\) as predetermind to verify the results in (17.114). What is the difference between the estimates treating the regressors as strictly exogenous versus predetermined?\nEstimate the equation using the Blundell-Bond one-step systems GMM estimator.\nInterpret the coefficient estimates viewing (17.114) as a firm-level labor demand equation.\nDescribe the impact on the standard errors of the Blundell-Bond estimates in part (c) if you forget to use clustering. (You do not have to list all the standard errors, but describe the magnitude of the impact.)\n\nExercise 17.17 Use the datafile Invest 1993 on the textbook webpage. You will be estimating the panel AR(1) \\(D_{i t}=\\alpha D_{i, t-1}+u_{i}+\\varepsilon_{i t}\\) for \\(D=\\) debt/ assets (this is debta in the datafile). See the description file for definitions.\n\nEstimate the model using Arellano-Bond twostep GMM with clustered standard errors.\nRe-estimate using Blundell-Bond twostep GMM.\nExperiment with your results, trying twostep versus onestep, AR(1) versus AR(2), number of lags used as instruments, and classical versus robust standard errors. What makes the most difference for the coefficient estimates? For the standard errors?\n\nExercise 17.18 Use the datafile Invest1993 on the textbook webpage. You will be estimating the model\n\\[\nD_{i t}=\\alpha D_{i, t-1}+\\beta_{1} I_{i, t-1}+\\beta_{2} Q_{i, t-1}+\\beta_{3} C F_{i, t-1}+u_{i}+\\varepsilon_{i t} .\n\\]\nThe variables are debta, inva, vala, and \\(c f a\\) in the datafile. See the description file for definitions.\n\nEstimate the above regression using Arellano-Bond two-step GMM with clustered standard errors treating all regressors as predetermined.\nRe-estimate using Blundell-Bond two-step GMM treating all regressors as predetermined.\nExperiment with your results, trying two-step versus one-step, number of lags used as instruments, and classical versus robust standard errors. What makes the most difference for the coefficient estimates? For the standard errors?"
  },
  {
    "objectID": "chpt18-did.html#introduction",
    "href": "chpt18-did.html#introduction",
    "title": "17  Difference in Differences",
    "section": "17.1 Introduction",
    "text": "17.1 Introduction\nOne of the most popular ways to estimate the effect of a policy change is the method of difference in differences, often called “diff in diffs”. Estimation is typically a two-way panel data regression with a policy indicator as a regressor. Clustered variance estimation is generally recommended for inference.\nIn order to intrepret a difference in difference estimate as a policy effect there are three key conditions. First, that the estimated regression is the correct conditional expectation. In particular, this requires that all trends and interactions are properly included. Second, that the policy is exogenous it satisfies conditional independence. Third, there are no other relevant unincluded factors coincident with the policy change. If these assumptions are satisfied the difference in difference estimand is a valid causal effect."
  },
  {
    "objectID": "chpt18-did.html#minimum-wage-in-new-jersey",
    "href": "chpt18-did.html#minimum-wage-in-new-jersey",
    "title": "17  Difference in Differences",
    "section": "17.2 Minimum Wage in New Jersey",
    "text": "17.2 Minimum Wage in New Jersey\nThe most well-known application of the difference in difference methodology is Card and Krueger (1994) who investigated the impact of New Jersey’s 1992 increase of the minimum hourly wage from \\(\\$ 4.25\\) to \\(\\$ 5.05\\). Classical economics teaches that an increase in the minimum wage will lead to decreases in employment and increases in prices. To investigate the magnitude of this impact the authors surveyed a panel of 331 fast food restaurants in New Jersey during the period 2/15/1992-3/4/1992 (before the enactment of the minimum wage increase) and then again during the period 11/5/1992-12/31/1992 (after the enactment). Fast food restaurants were selected for investigation as they are a major employer of minimum wage employees. Before the change about \\(30 %\\) of the sampled workers were paid the minimum wage of \\(\\$ 4.25\\).\nTable 18.1: Average Employment at Fast Food Restaurants\n\n\n\n\nNew Jersey\nPennsylvania\nDifference\n\n\n\n\nBefore Increase\n\\(20.43\\)\n\\(23.38\\)\n\\(2.95\\)\n\n\nAfter Increase\n\\(20.90\\)\n\\(21.10\\)\n\\(0.20\\)\n\n\nDifference\n\\(0.47\\)\n\\(-2.28\\)\n\\(\\mathbf{2 . 7 5}\\)\n\n\n\nThe data file CK1994 is extracted from the original Card-Krueger data set and is posted on the textbook webpage. Table \\(18.1\\) (first column) displays the mean number \\({ }^{1}\\) of full-time equivalent employees \\({ }^{2}\\) at New Jersey fast food restaurants before and after the minimum wage increase. Before the increase the average number of employees was 20.4. After the increase the average number of employees was 20.9. Contrary to the predictions of conventional theory employment slightly increased (by \\(0.5\\) employees per restaurant) rather than decreased.\nThis estimate - the change in employment - could be called a difference estimator. It is the change in employment coincident with the change in policy. A difficulty in interpretation is that all employment change is attributed to the policy. It does not provide direct evidence of the counterfactual - what would have happened if the minimum wage had not been increased.\nA difference in difference estimator improves on a difference estimator by comparing the change in the treatment sample with a comparable change in a control sample.\nCard and Krueger selected eastern Pennsylvania for their control sample. The minimum wage was constant at \\(\\$ 4.25\\) an hour in the state of Pennsylvania during 1992. At the beginning of the year starting wages at fast food restaurants in the two states were similar. The two areas (New Jersey and eastern Pennsylvania) share further similarities. Any trends or economic shocks which affect one state are likely to affect both. Therefore Card and Krueger argued that it is appropriate to treat eastern Pennsylvania as a control. This means that in the absence of a minimum wage increase they expected the same changes in employment to occur in both New Jersey and eastern Pennsylvania.\nCard and Krueger surveyed a panel of 79 fast food restaurants in eastern Pennsylvania simultaneously while surveying the New Jersey restaurants. The average number of full-time equivalent employees is displayed in the second column of Table 18.1. Before the policy change the average number of employees was 23.4. After the policy change the average number was 21.1. Thus in Pennsylvania average employment decreased by \\(2.3\\) employees per restaurant.\nTreating Pennsylvania as a control means comparing the change in New Jersey (0.5) with that in Pennsylvnia \\((-2.3)\\). The difference ( \\(2.75\\) employees per restaurant) is the difference-in-difference estimate of the impact of the minimum wage increase. In complete contradiction to conventional economic theory the estimate indicates an increase in employment rather than a decrease. This surprising estimate has been widely discussed among economists \\({ }^{3}\\) and the popular press.\nIt is constructive to re-write the estimates from Table \\(18.1\\) in regression format. Let \\(Y_{i t}\\) denote employment at restaurant \\(i\\) surveyed at time \\(t\\). Let State \\(_{i}\\) be a dummy variable indicating the state, with State \\(_{i}=1\\) for New Jersey and State \\(_{i}=0\\) for Pennsylvania. Let Time \\({ }_{t}\\) be a dummy variable indicating the time period, with Time \\(e_{t}=0\\) for the period before the policy change and Time \\(e_{t}=1\\) for the period after the policy change. Let \\(D_{i t}\\) denote a treatment dummy, with \\(D_{i t}=1\\) if the minimum wage equals \\(\\$ 5.05\\) and \\(D_{i t}=0\\) if the minimum wage equals \\(\\$ 4.25\\). In this application it equals the interaction dummy \\(D_{i t}=\\) State \\(_{i}\\) Time \\(_{t}\\).\nTable \\(18.1\\) is a saturated regression in the two dummy variables and can therefore be written as the regression equation\n\\[\nY_{i t}=\\beta_{0}+\\beta_{1} \\text { State }_{i}+\\beta_{2} \\text { Time }_{t}+\\theta D_{i t}+\\varepsilon_{i t} .\n\\]\nIndeed the coefficients can be written in terms of Table \\(18.1\\) by the following correspondence:\n\\({ }^{1}\\) Our calculations drop restaurants if they were missing the number of full-type equivalent employees in either survey.\n\\({ }^{2}\\) Following Card and Krueger full-time equivalent employees is defined as the sum of the number of full-time employees, managers, and assistant managers, plus one-half of the number of part-time employees.\n\\({ }^{3}\\) Most economists do not take the estimate literally - they do not believe that increasing the minimum wage will cause employment increases. Instead it has been interpreted as evidence that small changes in the minimum wage may have only minor impacts on employment levels.\n\n\n\n\n\n\n\n\n\n\nNew Jersey\nPennsylvania\nDifference\n\n\n\n\nBefore Increase\n\\(\\beta_{0}+\\beta_{1}\\)\n\\(\\beta_{0}\\)\n\\(\\beta_{1}\\)\n\n\nAfter Increase\n\\(\\beta_{0}+\\beta_{1}+\\beta_{2}+\\theta\\)\n\\(\\beta_{0}+\\beta_{2}\\)\n\\(\\beta_{1}+\\theta\\)\n\n\nDifference\n\\(\\beta_{2}+\\theta\\)\n\\(\\beta_{2}\\)\n\\(\\theta\\)\n\n\n\nWe see that the coefficients in the regression (18.1) correspond to interpretable difference and difference in difference estimands. \\(\\beta_{1}\\) is the difference estimand of the effect of “New Jersey vs. Pennsylvania” in the period before the policy change. \\(\\beta_{2}\\) is the difference estimand of the time effect in the control state. \\(\\theta\\) is the difference in difference estimand - the change in New Jersey relative to the change in Pennsylvania.\nOur estimate of the regression (18.1) is\n\\[\n\\begin{aligned}\n& Y_{i t}=23.4-2.9 \\text { State }_{i}-2.3 \\text { Time }_{t}+2.75 D_{i t}+\\varepsilon_{i t} . \\\\\n& \\text { (1.4) (1.5) (1.2) (1.34) }\n\\end{aligned}\n\\]\nThe standard errors are clustered by restaurant. As expected the coefficient \\(\\widehat{\\theta}\\) on the treatment dummy precisely equals the difference in difference estimate from Table 18.1. The coefficient estimates can be interpreted as described. The pre-change difference between New Jersey and Pennsylvania is \\(-2.9\\) and the time effect is \\(-2.3\\). The difference in difference effect is \\(2.75\\). The t-statistic to test the hypothesis of zero effect is just above 2 with an asymptotic \\(p\\)-value of \\(0.04\\).\nSince the observations are divided into the groups State \\(_{i}=0\\) and State \\(_{i}=1\\) and Time \\(_{t}\\) is equivalent to a time index this regression is identical to a two-way fixed effects regression of \\(Y_{i t}\\) on \\(D_{i t}\\) with state and time fixed effects. Furthermore, because the regressor \\(D_{i t}\\) does not vary across individuals within the state this fixed effects regression is unchanged if restaurant-level fixed effects are included instead of state fixed effects. (Restaurant fixed effects are orthogonal to any variable demeaned at the state level. See Exercise 18.1.) Thus the above regression is identical to the two-way fixed effects regression\n\\[\nY_{i t}=\\theta D_{i t}+u_{i}+v_{t}+\\varepsilon_{i t}\n\\]\nwhere \\(u_{i}\\) is a restaurant fixed effect and \\(v_{t}\\) is a time fixed effect. The simplest method to implement this is by a one-way fixed effects regression with time dummies. The estimates are\n\\[\nY_{i t}=\\underset{(1.34)}{2.75} D_{i t}-\\begin{gathered}\n2.3 \\\\\n(1.2)\n\\end{gathered} \\text { Time }_{t}+u_{i}+\\varepsilon_{i t}\n\\]\nwhich are identical to the previous regression.\nEquation (18.3) is the basic difference-in-difference model. It is a two-way fixed effects regression of the response \\(Y_{i t}\\) on a binary policy \\(D_{i t}\\). The coefficient \\(\\theta\\) corresponds to the double difference in sample means and can be interpreted as the policy impact (also called the treatment effect) of \\(D\\) on \\(Y\\). (We discuss identification in the next section.) Our presentation (and the Card-Krueger example) focuses on the basic case of two aggregate units (states) and two time periods. The regression formulation (18.3) is convenient as it can be easily generalized to allow for multiple states and time periods. Doing so provides more convincing evidence of an identified policy effect. The equation (18.3) can also be generalized by changing the trend specification and by using a continuous treatment variable.\nAnother common generalization is to augment the regression with controls \\(X_{i t}\\). This model is\n\\[\nY_{i t}=\\theta D_{i t}+X_{i t}^{\\prime} \\beta+u_{i}+v_{t}+\\varepsilon_{i t}\n\\]\nMany empirical studies report estimates both of the basic model and regressions with controls. For example we could augment the Card-Krueger regression to include the variable hoursopen, the number of hours a day the restaurant is open. A restaurant with longer hours will tend to have more employees.\n\\[\nY_{i t}=\\underset{(1.31)}{2.84} D_{i t}-\\underset{(1.2)}{2.2} \\text { Time }_{t}+\\underset{(0.4)}{1.2} \\text { hoursopen }_{i t}+u_{i}+\\varepsilon_{i t} .\n\\]\nThe estimated effect is that a restaurant employs an additional \\(1.2\\) employees for each hour open and this effect is statistically significant. The estimated treatment effect is not meaningfully changed."
  },
  {
    "objectID": "chpt18-did.html#identification",
    "href": "chpt18-did.html#identification",
    "title": "17  Difference in Differences",
    "section": "17.3 Identification",
    "text": "17.3 Identification\nConsider the difference-in-difference equation (18.5) for \\(i=1, \\ldots, N\\) and \\(t=1, \\ldots, T\\). We are interested in conditions under which the coefficient \\(\\theta\\) is the causal impact of the treatment \\(D_{i t}\\) on the outcome \\(Y_{i t}\\). The answer can be found by applying Theorem \\(2.12\\) from Section \\(2.30\\).\nIn Section \\(2.30\\) we introduced the potential outcomes framework which writes the outcome as a function of the treatment, controls, and unobservables. The outcome (e.g. employment at a restaurant) is written as \\(Y=h(D, X, e)\\) where \\(D\\) is treatment (minimum wage policy), \\(X\\) are controls, and \\(e\\) is a vector of unobserved factors. Model (18.5) specifies that \\(h(D, X, e)\\) is separable and linear in its arguments and that the unobservables consist of individual-specific, time-specific, and idiosyncratic effects.\nWe now present sufficient conditions under which the coefficient \\(\\theta\\) can be interpreted as a causal effect. Recall the two-way within transformation (17.65) and set \\(\\ddot{Z}_{i t}=\\left(\\ddot{D}_{i t}, \\ddot{X}_{i t}^{\\prime}\\right)^{\\prime}\\).\nTheorem 18.1 Suppose the following conditions hold:\n\n\\(Y_{i t}=\\theta D_{i t}+X_{i t}^{\\prime} \\beta+u_{i}+v_{t}+\\varepsilon_{i t}\\).\n\\(\\mathbb{E}\\left[\\ddot{Z}_{i t} \\ddot{Z}_{i t}^{\\prime}\\right]>0\\).\n\\(\\mathbb{E}\\left[X_{i t} \\varepsilon_{i s}\\right]=0\\) for all \\(t\\) and \\(s\\).\nConditional on \\(X_{i 1}, X_{i 2}, \\ldots, X_{i T}\\) the random variables \\(D_{i t}\\) and \\(\\varepsilon_{i s}\\) are statistically independent for all \\(t\\) and \\(s\\).\n\nThen the coefficient \\(\\theta\\) in (18.5) equals the average causal effect for \\(D\\) on \\(Y\\) conditional on \\(X\\).\nCondition 1 states that the outcome equals the specified linear regression model which is additively separable in the observables, individual effect, and time effect.\nCondition 2 states that the two-way within transformed regressors have a non-singular design matrix. This requires that all elements of \\(D_{i t}\\) and \\(X_{i t}\\) vary across time and individuals.\nCondition 3 is the standard exogeneity asumption for regressors in a fixed-effects model.\nCondition 4 states that the treatment variable is conditionally independent of the idiosyncratic error. This is the conditional independence assumption for fixed effects regression.\nTo show Theorem 18.1 apply the two-way within transformation (17.65) to (18.5). We obtain\n\\[\n\\ddot{Y}_{i t}=\\theta \\ddot{D}_{i t}+\\ddot{X}_{i t}^{\\prime} \\beta+\\ddot{\\varepsilon}_{i t} .\n\\]\nUnder Condition 2 the projection coefficients \\((\\theta, \\beta)\\) are uniquely defined and under Conditions 3 and 4 they equal the linear regression coefficients. Thus \\(\\theta\\) is the regression derivative with respect to \\(D\\). Condition 4 implies that conditional on \\(\\ddot{X}_{i t}\\) the random variables \\(\\ddot{D}_{i t}\\) and \\(\\ddot{\\varepsilon}_{i s}\\) are statistically independent. Theorem \\(2.12\\) shows that the regression derivative \\(\\theta\\) equals the average causal effect as stated.\nThe assumption that \\(D\\) and \\(\\varepsilon\\) are independent is the fundamental exogeneity assumption. To intrepret \\(\\theta\\) as a treatment effect it is important that \\(D\\) is defined as the treatment and not simply as an interaction (time and state) dummy. This is subtle. Examine equation (18.5) recalling that \\(D\\) is defined as the treatment (an increase in the minimum wage). In this equation the error \\(\\varepsilon_{i t}\\) contains all variables and effects not included in the regression. Thus if there are other changes in New Jersey which are coincident with the minimum wage increase the assumption that \\(D\\) and \\(\\varepsilon\\) are independent means that those coincident changes are independent of \\(\\varepsilon\\) and thus do not affect employment. This is a strong assumption. Once again, Condition 4 states that all other effects which are coincident with the minimum wage increase have no effect on employment. Without this assumption it would not be possible to claim that the diff-in-diff regression identifies the causal effect of the treatment.\nFurthermore, independence of \\(D_{i t}\\) and \\(\\varepsilon_{i s}\\) means that neither can be affected by the other. This means that the policy (treatment) was not enacted in response to knowledge about the response variable in either period and it means that the outcome (employment) did not change in the first period in anticipation of the upcoming policy change.\nIt is difficult to know if the exogeneity of \\(D\\) is a reasonable assumption. It is similar to instrument exogeneity in instrumental variable regression. Its validity hinges on a well-articulated structural argument. An empirical investigation based on a difference-in-difference specification needs to make an explicit case for exogeneity of \\(D\\) similar to that for IV regression.\nIn the case of the Card-Krueger application the authors argue that the policy was exogeneous because it was adopted two years before taking effect. At the time of the passage of the legislation the economy was in an expansion but by the time of adoption the economy has slipped into recession. This suggests that it is credible to assume that the policy decision in 1990 was not affected by employment levels in 1992. Furthermore, concern about the impact of the increased minimum wage during a recession led to a serious discussion about reversing the policy, meaning that there was uncertainty about whether or not the policy would actually be enacted at the time of the first survey. It thus seems credible that employment decisions at that time were not determined in anticipation of the upcoming minimum wage increase.\nThe authors do not discuss, however, whether or not there were other coincident events in the New Jersey or Pennsylvania economies during 1992 which could have affected employment differentially in the two states. It seems plausible that there could have been many such coincident events. This seems to be the greatest weakness in their identification argument.\nIdentification (the conditions for Theorem 18.1) also requires that the regression model is correctly specified. This means that the true model is linear in the specified variables and all interactions are included. Since the basic \\(2 \\times 2\\) specification is a saturated dummy variable model it is necessarily a conditional expectation and thus correctly specified. This is not the case in applications with more than two states or time periods and thus model specification needs to be carefully considered in such cases."
  },
  {
    "objectID": "chpt18-did.html#multiple-units",
    "href": "chpt18-did.html#multiple-units",
    "title": "17  Difference in Differences",
    "section": "17.4 Multiple Units",
    "text": "17.4 Multiple Units\nThe basic difference-in-difference model has two aggregate units (e.g. states) and two time periods. Additional information can be obtained if there are multiple units or multiple time periods. In this section we focus on the case of multiple units. There can be multiple treatment units, multiple control units, or both. In this section we suppose that the number of periods is \\(T=2\\). Let \\(N_{1} \\geq 1\\) be the number of untreated (control) units, and \\(N_{2} \\geq 1\\) be the number of treated units, with \\(N=N_{1}+N_{2}\\).\nThe basic regression model\n\\[\nY_{i t}=\\theta D_{i t}+u_{i}+v_{t}+\\varepsilon_{i t}\n\\]\nimposes two strong restrictions. First, that all units are equally affected by time as \\(v_{t}\\) is common across \\(i\\). Second, that the treatment effect \\(\\theta\\) is common across all treated units.\nThe Card-Krueger data set only contains observations from two states but the authors did record additional variables including the region of the state. They divided New Jersey into three regions (North, Central, and South) and eastern Pennsylvania into two regions ( 1 for northeast Philadelphia suburbs and 2 for the remainder).\nTable \\(18.2\\) displays the mean number of full-time equivalent employees by region, before and after the minimum wage increase. We observe that two of the three New Jersey regions had nearly identical increases in employment and all three changes are small. We can also observe that both of the Pennsylvania regions had employment decreases though with different magnitudes.\nWe can test the assumption of equal treatment effect \\(\\theta\\) by a regression exclusion test. This can be done by adding interaction dummies to the regression and testing for the exclusion of the interactions. As there are three treated regions in New Jersey we include two of the three New Jersey region dummies interacted with the time index. In general we would include \\(N_{2}-1\\) such interactions. These coefficients measure the treatment effect difference across regions. Testing that these two coefficients are zero we obtain a p-value of \\(0.60\\) which is far from significant. Thus we accept the hypothesis that the treatment effect \\(\\theta\\) is common across the New Jersey regions.\nIn contrast, when the treatment effect \\(\\theta\\) varies we call this a heterogeneous treatment effect. It is not a violation of the treatment effect framework but it can be considerably more complicated to analyze. (A model which incorrectly imposes a homogeneous treatment effect is misspecified and produces inconsistent estimates.)\nA more serious problem arises if the control effect is heterogeneous. The control effect is the change in the control group. Table \\(18.2\\) breaks down the estimated control effect across the two Pennsylvania regions. While both estimates are negative they are somewhat different from one another. If the effects are distinct there is not a homogeneous control effect. We can test the assumption of equal control effects by a regression exclusion test. As there are two Pennsylvania regions we include the interaction of one of the Pennsylvania regions with the time index. (In general we would include \\(N_{1}-1\\) interactions.) This coefficient measures the difference in the control effect across the regions. We test that this coefficient is zero obtaining a t-statistic of \\(1.2\\) and a p-value of \\(0.23\\). It is not statistically significant, meaning that we cannot reject the hypothesis that the control effect is homogeneous.\nIn contrast, if the control effect were heterogeneous then the difference-in-difference estimation strategy is misspecified. The method relies on the ability to identify a credible control sample. Therefore if a test for equal control effects rejects the hypothesis of homogeneous control effects this should be taken as evidence against interpretation of the difference-in-difference parameter as a treatment effect.\nTable 18.2: Average Employment at Fast Food Restaurants\n\n\n\n\nSouth NJ\nCentral NJ\nNorth NJ\nPA 1\nPA 2\n\n\n\n\nBefore Increase\n\\(16.6\\)\n\\(22.0\\)\n\\(22.0\\)\n\\(24.8\\)\n\\(22.2\\)\n\n\nAfter Increase\n\\(17.3\\)\n\\(21.4\\)\n\\(22.7\\)\n\\(21.0\\)\n\\(21.2\\)\n\n\nDifference\n\\(0.7\\)\n\\(-0.6\\)\n\\(0.7\\)\n\\(-3.8\\)\n\\(-1.0\\)"
  },
  {
    "objectID": "chpt18-did.html#do-police-reduce-crime",
    "href": "chpt18-did.html#do-police-reduce-crime",
    "title": "17  Difference in Differences",
    "section": "17.5 Do Police Reduce Crime?",
    "text": "17.5 Do Police Reduce Crime?\nDiTella and Schargrodsky (2004) use a difference-in-difference approach to study the question of whether the street presence of police officers reduces car theft. Rational crime models predict that the the presence of an observable police force will reduce crime rates (at least locally) due to deterrence. The causal effect is difficult to measure, however, as police forces are not allocated exogenously, but rather are allocated in anticipation of need. A difference-in-difference estimator requires an exogenous event which changes police allocations. The innovation in DiTella-Schargrodsky was to use the police response to a terrorist attack as exogenous variation.\nIn July 1994 there was a horrific terrorist attack on the main Jewish center in Buenos Aires, Argentina. Within two weeks the federal government provided police protection to all Jewish and Muslim buildings in the country. DiTella and Schargrodsky (2004) hypothesized that their presence, while allocated to deter a terror or reprisal attack, would also deter other street crimes such as automobile theft locally to the deployed police. The authors collected detailed information on car thefts in selected neighborhoods of Buenos Aires for April-December 1994, resulting in a panel for 876 city blocks. They hypothesized that the terrorist attack and the government’s response were exogenous to auto thievery and is thus a valid treatment. They postulated that the deterrence effect would be strongest for any city block which contained a Jewish institution (and thus police protection). Potential car thiefs would be deterred from a burglary due to the threat of being caught. The deterrence effect was expected to weaken as the distance from the protected sites increased. The authors therefore proposed a difference-in-difference estimator based on the average number of car thefts per block, before and after the terrorist attack, and between city blocks with and without a Jewish institution. Their sample has 37 blocks with Jewish institutions (the treatment sample) and 839 blocks without an institution (the control sample).\nThe data file DS2004 is a slightly revised version of the author’s AER replication file and is posted on the textbook webpage.\nTable 18.3: Number of Car Thefts by City Block\n\n\n\n\nSame Block\nNot on Same Block\nDifference\n\n\n\n\nApril-June\n\\(0.112\\)\n\\(0.095\\)\n\\(-0.017\\)\n\n\nAugust-December\n\\(0.035\\)\n\\(0.105\\)\n\\(0.070\\)\n\n\nDifference\n\\(-0.077\\)\n\\(0.010\\)\n\\(-\\mathbf{0 . 0 8 7}\\)\n\n\n\nTable \\(18.3\\) displays the average number of car thefts per block, separately for the months before the July attack and the months after the July attack, and separately for city blocks which have a Jewish institution (and therefore received police protection starting in late July) and for other city blocks. We can see that the average number of car thefts dramatically decreased in the protected city blocks, from \\(0.112\\) per month to 0.035, while the average number in non-protected blocks was near-constant, rising from \\(0.095\\) to \\(0.105\\). Taking the difference in difference we find that the effect of police presence decreased car thefts by \\(0.087\\), which is about \\(78 %\\).\nA general way to estimate a diff-in-diff model is a regression of the form (18.3) where \\(Y_{i t}\\) is the number of car thefts on block \\(i\\) during month \\(t\\), and \\(u_{i}\\) and \\(v_{t}\\) are block and month fixed effects. This regression \\(^{4}\\) yields the same estimate of \\(0.087\\) because the panel is balanced and there are no control variables.\nThe model (18.3) makes the strong assumption that the treatment effect is constant across the five treated months. We investigate this assumption in Table \\(18.4\\) which breaks down the car thefts by month. For the control sample the number of car thefts is near constant across the months. For seven of the eight\n\\({ }^{4}\\) We omit the observations for July as the car theft data is only for the first half of the month. Table 18.4: Number of Car Thefts by City Block\n\n\n\nPre-Attack\nApril\nSame Block\nNot on Same Block\nDifference\n\n\n\n\n\nMay\n\\(0.112\\)\n\\(0.110\\)\n\\(-0.012\\)\n\n\n\nJune\n\\(0.088\\)\n\\(0.100\\)\n\\(0.012\\)\n\n\nPost-Attack\nAugust\n\\(0.128\\)\n\\(0.076\\)\n\\(-0.052\\)\n\n\n\nSeptember\n\\(0.014\\)\n\\(0.111\\)\n\\(0.064\\)\n\n\n\nOctober\n\\(0.061\\)\n\\(0.099\\)\n\\(0.085\\)\n\n\n\nNovember\n\\(0.027\\)\n\\(0.108\\)\n\\(0.047\\)\n\n\n\nDecember\n\\(0.027\\)\n\\(0.100\\)\n\\(0.073\\)\n\n\n\n\n\n\\(0.106\\)\n\\(0.079\\)\n\n\n\nmonths the average number per block ranges from \\(0.10\\) to \\(0.11\\), with only one month (June) a bit lower at \\(0.08\\). In the treatment sample the average number of thefts per block in the three months before the terrorist atack are similar to the averages in the control sample. But in the five months following the attack the number of car thefts is uniformly reduced. The averages range from \\(0.014\\) to \\(0.061\\). In each month after the attack the control sample has lower thefts with averages ranging from \\(0.047\\) to \\(0.085\\). Given the small sample size (37) of the treatment sample this is strikingly uniform evidence.\nWe can formally test the homogeneity of the treatment effect by including four dummy variables for the interactions of four post-attack months with the treatment sample and then testing the exclusion of these variables. The \\(\\mathrm{p}\\)-value for this test is \\(0.81\\), exceedingly far from significant. Thus there is no reason in the data to be suspicious of the homogeneity assumption.\nThe goal was to estimate the causal effect of police presence as a deterrence for crime. Let us evaluate the case for identification. It seems reasonable to treat the terrorist attack as exogenous. The government response also appears exogenous. Neither is reasonably related to the auto theft rate. We also observe that the evidence in Tables \\(18.3\\) and \\(18.4\\) indicates that theft rates were similar in the pre-attack treatment and control samples. Thus the additional police protection seems credibly provided for the purpose of attack prevention rather than as an excuse for crime prevention. The general homogeneity of the theft rate across months, once allowing for the treatment effect, gives credibility to the claim that the police response was a causal effect. The terror attack itself did not reduce car theft rates as there seems to be no measurable effect outside of the treatment sample. Finally, while the paper does not explicitly address whether or not there was any other coincident event in July 1994 which may have effected these specific city blocks it is difficult to conceive of an alternative explanation for such a large effect. Our conclusion is that this is a strong identification argument. Police presence greatly reduces the incidence of car theft.\nThe authors asserted the inference that police presence deters crime more broadly. This is a tenuous extension as the paper does not provide direct evidence of this claim. While it may seem reasonable we should be cautious about making generalizations without supporting evidence.\nOverall, DiTella and Schargrodsky (2004) is an excellent example of a well-articulated and credibly identified difference-in-difference estimate of an important policy effect."
  },
  {
    "objectID": "chpt18-did.html#trend-specification",
    "href": "chpt18-did.html#trend-specification",
    "title": "17  Difference in Differences",
    "section": "17.6 Trend Specification",
    "text": "17.6 Trend Specification\nSome applications (including the two introduced earlier in this chapter) apply to a short period of time such as one year in which case we may not expect the variables to be trended. Other applications cover many years or decades in which case the variables are likely to be trended. These trend can reflect long-term growth, business cycle effects, changing tastes, or many other features. If trends are incor- rectly specified then the model will be misspecified and the estimated policy effect will be inconsistent due to omitted variable bias. Consider the difference-in-difference equation (18.5). This model imposes the strong assumption that the trends in \\(Y_{i t}\\) are entirely explained by the included controls \\(X_{i t}\\) and the common unobserved time component \\(v_{t}\\). This can be quite restrictive. It is reasonable to expect that trends may differ across units and are not fully captured by observed controls.\nOne way to think about this is in terms of overidentification. For simplicity suppose there are no controls and the panel is balanced. Then there are \\(N T\\) observations. The two-way model with a policy effect has \\(N+T\\) coefficients. Unless \\(N=T=2\\) this model is overidentified. In addition to considering heterogeneous treatment effects it is reasonable to consider heterogeneous trends.\nOne generalization is to include interactions of a linear trend with a control variable. This model is\n\\[\nY_{i t}=\\theta D_{i t}+X_{i t}^{\\prime} \\beta+Z_{i}^{\\prime} \\delta t+u_{i}+v_{t}+\\varepsilon_{i t} .\n\\]\nIt specifies that the trend in \\(Y_{i t}\\) differs across units depending on the controls \\(Z_{i}\\).\nA broader generalization is to include unit-specific linear time trends. This model is\n\\[\nY_{i t}=\\theta D_{i t}+X_{i t}^{\\prime} \\beta+u_{i}+v_{t}+t w_{i}+\\varepsilon_{i t} .\n\\]\nIn this model \\(w_{i}\\) is a time trend fixed effect which varies across units. If there are no controls this model has \\(2 N+T\\) coefficients and is identified as long as \\(T \\geq 4\\).\nEstimation of model (18.6) can be done one of three ways. If \\(N\\) is small (for example, applications with state-level data) the regression can be estimated using the explicit dummy variable approach. Let \\(d_{i}\\) and \\(S_{t}\\) be dummy variables indicating the \\(i^{t h}\\) unit and \\(t^{t h}\\) time period. Set \\(d_{i t}=d_{i} t\\), the interaction of the individual dummy with the time trend. The equation is estimated by regression of \\(Y_{i t}\\) on \\(D_{i t}, X_{i t}\\), \\(d_{i}, S_{t}\\), and \\(d_{i t}\\). Equivalently, one can apply one-way fixed effects with regressors \\(D_{i t}, X_{i t}, S_{t}\\), and \\(d_{i t}\\).\nWhen \\(N\\) is large a computationally more efficient approach is to use residual regression. For each unit \\(i\\), estimate a time trend model for each variable \\(Y_{i t}, D_{i t}, X_{i t}\\) and \\(S_{t}\\). That is, for each \\(i\\) estimate\n\\[\nY_{i t}=\\widehat{\\alpha}_{0}+\\widehat{\\alpha}_{1} t+\\dot{Y}_{i t} .\n\\]\nThis is a generalized within transformation. The residuals \\(\\dot{Y}_{i t}\\) are used in place of the original observations. Regress \\(\\dot{Y}_{i t}\\) on \\(\\dot{D}_{i t}, \\dot{X}_{i t}\\), and \\(\\dot{S}_{t}\\) to obtain the estimates of (18.6).\nThe relevance of the trend fixed effects \\(\\nu_{t}\\) can be assessed by a significance test. Specifically, the hypothesis that the coefficients on the period dummies are zero can be tested using a standard exclusion test. Similarly, trend interaction terms can be tested for significance using standard exclusion tests. If the tests are statistically significant this indicates that their inclusion is relevant for correct specification. Unfortunately, the unit-specific linear time trends cannot be tested for significance when the covariance matrix is clustered at the unit level. This is similar to the problem of testing the significance of a dummy variable with a single observation. The unit-specific time trends can only be tested for significance if the covariance matrix is clustered at a finer level. Otherwise the covariance matrix estimate is singular and biased downwards. Naïve tests will over-state significance.\nOur discussion for simplicity has focused on the case of balanced panels. The methods equally apply to unbalanced panels, using standard panel data estimation."
  },
  {
    "objectID": "chpt18-did.html#do-blue-laws-affect-liquor-sales",
    "href": "chpt18-did.html#do-blue-laws-affect-liquor-sales",
    "title": "17  Difference in Differences",
    "section": "17.7 Do Blue Laws Affect Liquor Sales?",
    "text": "17.7 Do Blue Laws Affect Liquor Sales?\nHistorically many U.S. states prohibited or limited the sale of alcoholic beverages on Sundays. These laws are known as “blue laws”. In recent years these laws have been relaxed. Have these changes led to increased consumption of alcoholic beverages? Bernheim, Meer and Novarro (2016) investigated this question using a detailed panel on alcohol consumption and sales hours. It is possible that observed changes coincident with changes in the law might reflect underlying trends. The fact that different states changed their laws during different years allows for a difference-in-difference methodology to identify the treatment effect.\nThe paper focuses on distilled liquor sales though wine and beer sales are also included in their data. An abridged version of their data set BMN2016 is posted on the textbook webpage. Liquor is measured in per capita gallons of pure ethanol equivalent. The data are state-level for 47 U.S. states for the years 1970-2007, unbalanced.\nThe authors carefully gathered information on the allowable hours that alcohol can be sold on a Sunday. They make a distinction between off-premise sales (liquor stores, supermarkets) where consumption is off-premise, and on-premise sales (restaurants, bars) where consumption is on-premise. Let \\(Y_{i t}\\) denote the natural logarithm of per-capita liquor sales in state \\(i\\) in year \\(t\\). A simplified version of their basic model is\nOnHours and OffHours are the number of allowable Sunday on-premises and off-premises sale hours. UR is the state unemploment rate. OnOutFlows (OffOutFlows) is the weighted number of on(off)-premises sale hours less than neighbor states. These are added to adjust for possible cross-border transactions. The model includes both state and year fixed effects. The standard errors are clustered by state.\nThe estimates indicate that increased on-premise sale hours lead to a small increase in liquor sales. This is consistent with alcohol being a complementary good in social (restaurant and bar) settings. The small and insignificant coefficient on OffHours indicates that increased off-premise sale hours does not lead to an increase in liquor sales. This is consistent with rational consumers who adjust their purchases to known hours. The negative effect of the unemployment rate means that liquor sales are pro-cyclical.\nThe authors were concerned whether their dynamic and trend specifications were correctly specified so tried some alternative specifications and interactions. To understand the trend issue we plot in Figure \\(18.1\\) the time-series path of the log of per-capita liquor sales for three states: California, Iowa, and New York. You can see that all three exhibit a downward trend from 1970 until about 1995 and then an increasing trend. The trend components of the three series, however, are not identical. This suggests that it may be incorrect to treat the trends as common across states.\nIf we augment the basic model to include state-specific linear trends the estimates are as follows.\n\\[\n\\begin{aligned}\n& +0.005 \\text { OnOutFlows }{ }_{i t}-0.005 \\text { OffOutFlows }{ }_{i t}+t w_{i}+u_{i}+v_{t}+\\varepsilon_{i t} \\text {. } \\\\\n& (0.005) \\quad(0.005)\n\\end{aligned}\n\\]\nThe estimated coefficient for OnHours drops to zero and becomes insignificant. The other estimates do not change meaningfully. The authors only discuss this regression in a footnote stating that adding statespecific trends “demands a great deal from the data and leaves too little variation to identify the effects of interest.” This is an unfortunate claim as actually the standard errors have decreased, not increased,\n\n\n\n\nFigure 18.1: Liquor Sales by State\nindicating that the effects are better identified. The trouble is that OnHours and OffHours are trended and the trends vary by state. This means that these variables are correlated with the state-trend interaction. Omitting the trend interaction induced omitted variable bias. That explains why the coefficient estimates change when the trend specification changes.\nBernheim, Meer and Novarro (2016) is an excellent example of meticulous empirical work with careful attention to detail and isolating a treatment strategy. It is also a good example of how attention to trend specification can affect results."
  },
  {
    "objectID": "chpt18-did.html#check-your-code-does-abortion-impact-crime",
    "href": "chpt18-did.html#check-your-code-does-abortion-impact-crime",
    "title": "17  Difference in Differences",
    "section": "17.8 Check Your Code: Does Abortion Impact Crime?",
    "text": "17.8 Check Your Code: Does Abortion Impact Crime?\nIn a highly-discussed paper, Donohue and Levitt (2001) used a difference-in-difference approach to develop an unusual theory. Crime rates fell dramatically throughout the United States in the 1990s. Donohue and Levitt postulated that one contributing explanation was the landmark 1973 legalization of abortion. The latter might affect the crime rate through two potential channels. First, it reduced the cohort size of young males. Second, it reduced the cohort size of young males at risk for criminal behavior. This suggests the substantial increase in abortions in the early 1970s will translate into a substantial reduction in crime 20 years later.\nAs you might imagine this paper was controversial on several dimensions. The paper was also meticulous in its empirical analysis, investigating the potential links using a variety of tools and differing levels of granularity. The most detailed-oriented regressions were presented at the very end of the paper where the authors exploited differences across age groups. These regressions took the form\n\\[\n\\log \\left(\\text { Arrests }_{i t b}\\right)=\\beta \\text { Abortion }_{i b}+u_{i}+\\lambda_{t b}+\\theta_{i t}+\\varepsilon_{i t b}\n\\]\nwhere \\(i, t\\), and \\(b\\) index state, year, and birth cohort. Arrests is the raw number of arrests for a given crime and Abortion is the ratio of abortions per live births. The regression includes state fixed effects, cohortyear interactions, and state-year interactions. By including all these interaction effects the regression is estimating a triple-difference, and is identifying the abortion impact on within-state cross-cohort variation, which is a much stronger identification argument than a simple cross-state diff-in-diff regression. Donohue and Levitt reported an estimate of \\(\\beta\\) equalling \\(-0.028\\) with a small standard error. Based on these estimates Donohue and Levitt suggest that legalizing abortion reduced crime by about 15-25%.\nUnfortunately, their estimates contained an error. In an attempt to replicate Donohue-Levitt’s work Foote and Goetz (2008) discovered that Donohue-Levitt’s computer code inadvertently omitted the stateyear interactions \\(\\theta_{i t}\\). This was an important omission as without \\(\\theta_{i t}\\) the estimates are based on a mix of cross-state and cross-cohort variation rather than just cross-cohort variation as claimed. Foote and Goetz re-estimated the regression and found an estimate of \\(\\beta\\) equalling \\(-0.010\\). While still statistically different from zero, the reduction in magnitude substantially decreased the estimated impact. Foote and Gootz include more extensive empirical analysis as well.\nRegardless of the errors and political ramifications the Donohue-Levitt paper is a very clever and creative use of the difference-in-difference method. It is unfortunate that this creative work was somewhat overshadowed by a debate over computer code.\nI believe there are two important messages from this episode. First, include the appropriate controls! In the Donohue-Levitt regression they were correct to advocate for the regression which includes stateyear interactions as this allows the most precise measurement of the desired causal impact. Second, check your code! Computation errors are pervasive in applied economic work. It is very easy to make errors; it is very difficult to clean them out of lengthy code. Errors in most papers are ignored as the details receive minor attention. Important and influential papers, however, are scrutinized. If you ever are so blessed as to write a paper which receives significant attention you will find it most embarrassing if a coding error is found after publication. The solution is to be pro-active and vigilant."
  },
  {
    "objectID": "chpt18-did.html#inference",
    "href": "chpt18-did.html#inference",
    "title": "17  Difference in Differences",
    "section": "17.9 Inference",
    "text": "17.9 Inference\nMany difference-in-difference applications use highly aggregate (e.g. state level) data because they are investigating the impact of policy changes which occur at an aggregate level. It has become customary in the recent literature to use clustering methods to calculate standard errors with clustering applied at a high level of aggregation.\nTo understand the motivation for this choice it is useful to review the traditional argument for clustered variance estimation. Suppose that the error \\(e_{i g}\\) for individual \\(i\\) in group \\(g\\) is independent of the regressors, has variance \\(\\sigma^{2}\\), and has correlation \\(\\rho\\) across individuals within the group. If the number of individuals in each group is \\(N\\) then the exact variance of the least squares estimator (recall equation \\((4.53))\\) is\n\\[\n\\boldsymbol{V}_{\\widehat{\\beta}}=\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\sigma^{2}(1+\\rho(N-1))\n\\]\nas originally derived by Moulton (1990). This inflates the “usual” variance by the factor \\((1+\\rho(N-1))\\). Even if \\(\\rho\\) is very small, if \\(N\\) is huge then this inflation factor can be large as well.\nThe clustered variance estimator imposes no structure on the conditional variances and correlations within each group. It allows for arbitrary relationships. The advantage is that the resulting variance estimators are robust to a broad range of correlation structures. The disadvantage is that the estimators can be much less precise. Effectively, clustered variance estimators should be viewed as constructed from the number of groups. If you are using U.S. states as your groups (as is commonly seen in applications) then the number of groups is (at most) 51. This means that you are estimating the covariance matrix using 51 observations regardless of the number of “observations” in the sample. One implication is that if you are estimating more than 51 coefficients the sample covariance matrix estimator will not be full rank which can invalidate potentially relevant inference methods.\nThe case for clustered standard errors was made convincingly in an influential paper by Bertrand, Duflo, and Mullainathan (2004). These authors demonstrated their point by taking the well-known CPS dataset and then adding randomly generated regressors. They found that if non-clustered variance estimators were used then standard errors would be much too small and a researcher would inappropriately conclude that the randomly generated “variable” has a significant effect in a regression. The false rejections could be eliminated by using clustered standard errors, clustered at the state level. Based on the recommendations from this paper, researchers in economics now routinely cluster at the state level.\nThere are limitations, however. Take the Card-Krueger (1994) example introduced earlier. Their sample had only two states (New Jersey and Pennsylvania). If the standard errors are clustered at the state level then there are only two effective observations available for standard error calculation, which is much too few. For this application clustering at the state level is impossible. One implication might be that this casts doubts on applications involving just a handful of states. If we cannot rule out clustered dependence structures, and cannot use clustering methods due to the small number of states, then it may be inappropriate to trust the reported standard errors.\nAnother challenge arises when treatment \\(\\left(D_{i t}=1\\right)\\) applies to only a small number of units. The most extreme case is where there is only one treated unit. This could arise, for example, when you are interested in measuring the effect of a policy which only one state has adopted. This situation is particularly treacherous and is algebraically identical to the problem of robust covariance matrix estimation with sparse dummy variables. (See Section 4.16.) As we learned from that analysis, in the extreme case of a single treated unit the robust covariance matrix estimator is singular and highly biased towards zero. The problem is because the variance of the sub-group is estimated from a single observation.\nThe same analysis applies to cluster-variance estimators. If there is a single treated unit then the standard clustered covariance matrix estimator will be singular. If you calculate a standard error for the sub-group mean it will be algebraically zero despite being the most imprecisely estimated coefficient. The treatment effect will have a non-zero reported standard error but it will be incorrect and highly biased towards zero. For a more detailed analysis and recommendations for inference see Conley and Taber (2011)."
  },
  {
    "objectID": "chpt18-did.html#exercises",
    "href": "chpt18-did.html#exercises",
    "title": "17  Difference in Differences",
    "section": "17.10 Exercises",
    "text": "17.10 Exercises\nExercise 18.1 In the text it was claimed that in a balanced sample individual-level fixed effects are orthogonal to any variable demeaned at the state level.\n\nShow this claim.\nDoes this claim hold in unbalanced samples?\nExplain why this claim implies that the regressions\n\n\\[\nY_{i t}=\\beta_{0}+\\beta_{1} \\text { State }_{i}+\\beta_{2} \\text { Time }_{t}+\\theta D_{i t}+\\varepsilon_{i t}\n\\]\nand\n\\[\nY_{i t}=\\theta D_{i t}+u_{i}+\\delta_{t}+\\varepsilon_{i t}\n\\]\nyield identical estimates of \\(\\theta\\).\nExercise 18.2 In regression (18.1) with \\(T=2\\) and \\(N=2\\) suppose the time variable is omitted. Thus the estimating equation is\n\\[\nY_{i t}=\\beta_{0}+\\beta_{1} \\text { State }_{i}+\\theta D_{i t}+\\varepsilon_{i t} .\n\\]\nwhere \\(D_{i t}=\\) State \\(_{i}\\) Time \\(_{t}\\) is the treatment indicator.\n\nFind an algebraic expression for the least squares estimator \\(\\widehat{\\theta}\\).\nShow that \\(\\hat{\\theta}\\) is a function only of the treated sub-sample and is not a function of the untreated sub-sample.\nIs \\(\\hat{\\theta}\\) a difference-in-difference estimator?\nUnder which assumptions might \\(\\widehat{\\theta}\\) be an appropriate estimator of the treatment effect?\n\nExercise 18.3 Take the basic difference-in-difference model\n\\[\nY_{i t}=\\theta D_{i t}+u_{i}+\\delta_{t}+\\varepsilon_{i t} .\n\\]\nInstead of assuming that \\(D_{i t}\\) and \\(\\varepsilon_{i t}\\) are independent, assume we have an instrumental variable \\(Z_{i t}\\) which is independent of \\(\\varepsilon_{i t}\\) but is correlated with \\(D_{i t}\\). Describe how to estimate \\(\\theta\\).\nHint: Review Section 17.28.\nExercise 18.4 For the specification tests of Section 18.4 explain why the regression test for homogeneous treatment effects includes only \\(N_{2}-1\\) interaction dummy variables rather than all \\(N_{2}\\) interaction dummies. Also explain why the regression test for equal control effects includes only \\(N_{1}-1\\) interaction dummy variables rather than all \\(N_{1}\\) interaction dummies.\nExercise 18.5 An economist is interested in the impact of Wisconsin’s 2011”Act 10” legislation on wages. (For background, Act 10 reduced the power of labor unions.) She computes the following statistics \\({ }^{5}\\) for average wage rates in Wisconsin and the neighboring state of Minnesota for the decades before and after Act 10 was enacted.\n\n\n\n\nYears\nAverage Wage\n\n\n\n\nWisconsin\n\\(2001-2010\\)\n\\(15.23\\)\n\n\nWisconsin\n\\(2010-2020\\)\n\\(16.72\\)\n\n\nMinnesota\n\\(2001-2010\\)\n\\(16.42\\)\n\n\nMinnesota\n\\(2010-2020\\)\n\\(18.10\\)\n\n\n\n\nBased on this information, what is her point estimate of the impact of Act 10 on average wages?\nThe numbers in the above table were calculated as county-level averages. (The economist was given the average wage in each county. She calculated the average for the state by taking the average across the counties.) Now suppose that she estimates the following linear regression, treating individual counties as observations.\n\n\\[\n\\text { wage }=\\alpha+\\beta \\text { Act } 10+\\gamma \\text { Wisconsin }+\\delta \\text { Post } 2010+e\n\\]\n\\({ }^{5}\\) This numbers are completely fictitious. The three regressors are dummy variables for “Act 10 in effect in the state”, “county is in Wisconsin”, and “time period is 2011-2020.”\nWhat value of \\(\\widehat{\\beta}\\) does she find?\n\nWhat value of \\(\\widehat{\\gamma}\\) does she find?\n\nExercise 18.6 Use the datafile CK1994 on the textbook webpage. Classical economics teaches that increasing the minimum wage will increase product prices. You can therefore use the Card-Krueger diffin-diff methodology to estimate the effect of the 1992 New Jersey minimum wage increase on product prices. The data file contains the variables priceentree, pricefry and pricesoda. Create the variable price as the sum of these three, indicating the cost of a typical meal.\n\nSome values of price are missing. Delete these observations. This will produce an unbalanced panel as price may be missing for only one of the two surveys. Balance the panel by deleting the paired observation. This can be accomplished in Stata by the commands:\n\n\ndrop if price \\(==\\).\nbys store: gen nperiods \\(=\\) [_N \\(]\\)\nkeep if nperiods \\(==2\\)\n\n\nCreate an analog of Table \\(18.1\\) but with the price of a meal rather than the number of employees. Interpret the results.\nEstimate an analog of regression (18.2) with price as the dependent variable.\nEstimate an analog of regression (18.4) with state fixed effects and price as the dependent variable.\nEstimate an analog of regression (18.4) with restaurant fixed effects and price as the dependent variable.\nAre the results of these regressions the same?\nCreate an analog of Table \\(18.2\\) for the price of a meal. Interpret the results.\nTest for homogeneous treatment effects across regions.\nTest for equal control effects across regions.\n\nExercise 18.7 Use the datafile DS2004 on the textbook webpage. The authors argued that an exogenous police presence would deter automobile theft. The evidence presented in the chapter showed that car theft was reduced for city blocks which received police protection. Does this deterrence effect extend beyond the same block? The dataset has the dummy variable oneblock which indicates if the city block is one block away from a protected institution.\n\nCalculate an analog of Table \\(18.3\\) which shows the difference between city blocks which are one block away from a protected institution and those which are more than one block away from a protected institution.\nEstimate a regression with block and month fixed effects which includes two treatment variables: for city blocks which are on the same block as a protected institution, and for city blocks which are one block away, both interacted with a post-July dummy. Exclude observations for July. (c) Comment on your findings. Does the deterrence effect extend beyond the same city block?\n\nExercise 18.8 Use the datafile BMN2016 on the textbook webpage. The authors report results for liquor sales. The data file contains the same information for beer and wine sales. For either beer or wine sales, estimate diff-in-diff models similar to (18.7) and (18.8) and interpret your results. Some relevant variables are \\(i d\\) (state identification), year, unempw (unemployment rate). For beer the relevant variables are logbeer (log of beer sales), beeronsun (number of hours of allowed on-premise sales), beeroffsun (number of hours of allowed off-premise sales), beerOnOutflows, beerOffOutflows. For wine the variables have similar names."
  },
  {
    "objectID": "chpt19-nonparameter.html#introduction",
    "href": "chpt19-nonparameter.html#introduction",
    "title": "18  Nonparametric Regression",
    "section": "18.1 Introduction",
    "text": "18.1 Introduction\nWe now turn to nonparametric estimation of the conditional expectation function (CEF)\n\\[\n\\mathbb{E}[Y \\mid X=x]=m(x) .\n\\]\nUnless an economic model restricts the form of \\(m(x)\\) to a parametric function, \\(m(x)\\) can take any nonlinear shape and is therefore nonparametric. In this chapter we discuss nonparametric kernel smoothing estimators of \\(m(x)\\). These are related to the nonparametric density estimators of Chapter 17 of Probability and Statistics for Economists. In Chapter 20 of this textbook we explore estimation by series methods.\nThere are many excellent monographs written on nonparametric regression estimation, including Härdle (1990), Fan and Gijbels (1996), Pagan and Ullah (1999), and Li and Racine (2007).\nTo get started, suppose that there is a single real-valued regressor \\(X\\). We consider the case of vectorvalued regressors later. The nonparametric regression model is\n\\[\n\\begin{aligned}\nY &=m(X)+e \\\\\n\\mathbb{E}[e \\mid X] &=0 \\\\\n\\mathbb{E}\\left[e^{2} \\mid X\\right] &=\\sigma^{2}(X) .\n\\end{aligned}\n\\]\nWe assume that we have \\(n\\) observations for the pair \\((Y, X)\\). The goal is to estimate \\(m(x)\\) either at a single point \\(x\\) or at a set of points. For most of our theory we focus on estimation at a single point \\(x\\) which is in the interior of the support of \\(X\\).\nIn addition to the conventional regression assumptions we assume that both \\(m(x)\\) and \\(f(x)\\) (the marginal density of \\(X\\) ) are continuous in \\(x\\). For our theoretical treatment we assume that the observations are i.i.d. The methods extend to dependent observations but the theory is more advanced. See Fan and Yao (2003). We discuss clustered observations in Section 19.20."
  },
  {
    "objectID": "chpt19-nonparameter.html#binned-means-estimator",
    "href": "chpt19-nonparameter.html#binned-means-estimator",
    "title": "18  Nonparametric Regression",
    "section": "18.2 Binned Means Estimator",
    "text": "18.2 Binned Means Estimator\nFor clarity, fix the point \\(x\\) and consider estimation of \\(m(x)\\). This is the expectation of \\(Y\\) for random pairs \\((Y, X)\\) such that \\(X=x\\). If the distribution of \\(X\\) were discrete then we could estimate \\(m(x)\\) by taking the average of the sub-sample of observations \\(Y_{i}\\) for which \\(X_{i}=x\\). But when \\(X\\) is continuous then the probability is zero that \\(X\\) exactly equals \\(x\\). So there is no sub-sample of observations with \\(X=x\\) and this estimation idea is infeasible. However, if \\(m(x)\\) is continuous then it should be possible to get a good approximation by taking the average of the observations for which \\(X_{i}\\) is close to \\(x\\), perhaps for the observations for which \\(\\left|X_{i}-x\\right| \\leq h\\) for some small \\(h>0\\). As for the case of density estimation we call \\(h\\) a bandwidth. This binned means estimator can be written as\n\\[\n\\widehat{m}(x)=\\frac{\\sum_{i=1}^{n} \\mathbb{1}\\left\\{\\left|X_{i}-x\\right| \\leq h\\right\\} Y_{i}}{\\sum_{i=1}^{n} \\mathbb{1}\\left\\{\\left|X_{i}-x\\right| \\leq h\\right\\}} .\n\\]\nThis is an step function estimator of the regression function \\(m(x)\\).\n\n\nNadaraya-Watson\n\n\n\nLocal Linear\n\nFigure 19.1: Nadaraya-Watson and Local Linear Regression\nTo visualize, Figure 19.1(a) displays a scatter plot of 100 random pairs \\(\\left(Y_{i}, X_{i}\\right)\\) generated by simulation. The observations are displayed as the open circles. The estimator (19.1) of \\(m(x)\\) at \\(x=1\\) with \\(h=1\\) is the average of the \\(Y_{i}\\) for the observations such that \\(X_{i}\\) falls in the interval [ \\(\\left.0 \\leq X_{i} \\leq 2\\right]\\). This estimator is \\(\\widehat{m}(1)\\) and is shown on Figure 19.1(a) by the first solid square. We repeat the calculation (19.1) for \\(x=3\\), 5,7 , and 9, which is equivalent to partitioning the support of \\(X\\) into the bins \\([0,2]\\), [2,4], \\([4,6],[6,8]\\), and \\([8,10]\\). These bins are shown in Figure 19.1(a) by the vertical dotted lines and the estimates \\((19.1)\\) by the five solid squares.\nThe binned estimator \\(\\widehat{m}(x)\\) is the step function which is constant within each bin and equals the binned mean. In Figure 19.1(a) it is displayed by the horizontal dashed lines which pass through the solid squares. This estimate roughly tracks the central tendency of the scatter of the observations \\(\\left(Y_{i}, X_{i}\\right)\\). However, the huge jumps at the edges of the partitions are disconcerting, counter-intuitive, and clearly an artifact of the discrete binning.\nIf we take another look at the estimation formula (19.1) there is no reason why we need to evaluate (19.1) only on a course grid. We can evaluate \\(\\widehat{m}(x)\\) for any set of values of \\(x\\). In particular, we can evaluate (19.1) on a fine grid of values of \\(x\\) and thereby obtain a smoother estimate of the CEF. This estimator is displayed in Figure 19.1(a) with the solid line. We call this estimator “Rolling Binned Means”. This is a generalization of the binned estimator and by construction passes through the solid squares. It turns out that this is a special case of the Nadaraya-Watson estimator considered in the next section. This estimator, while less abrupt than the Binned Means estimator, is still quite jagged."
  },
  {
    "objectID": "chpt19-nonparameter.html#kernel-regression",
    "href": "chpt19-nonparameter.html#kernel-regression",
    "title": "18  Nonparametric Regression",
    "section": "18.3 Kernel Regression",
    "text": "18.3 Kernel Regression\nOne deficiency with the estimator (19.1) is that it is a step function in \\(x\\) even when evaluated on a fine grid. That is why its plot in Figure \\(19.1\\) is jagged. The source of the discontinuity is that the weights are discontinuous indicator functions. If instead the weights are continuous functions then \\(\\widehat{m}(x)\\) will also be continuous in \\(x\\). Appropriate weight functions are called kernel functions.\nDefinition 19.1 A (second-order) kernel function \\(K(u)\\) satisfies\n\n\\(0 \\leq K(u) \\leq \\bar{K}<\\infty\\)\n\\(K(u)=K(-u)\\),\n\\(\\int_{-\\infty}^{\\infty} K(u) d u=1\\),\n\\(\\int_{-\\infty}^{\\infty}|u|^{r} K(u) d u<\\infty\\) for all positive integers \\(r\\).\n\nEssentially, a kernel function is a bounded probability density function which is symmetric about zero. Assumption 19.1.4 is not essential for most results but is a convenient simplification and does not exclude any kernel function used in standard empirical practice. Some of the mathematical expressions are simplified if we restrict attention to kernels whose variance is normalized to unity.\nDefinition 19.2 A normalized kernel function satisfies \\(\\int_{-\\infty}^{\\infty} u^{2} K(u) d u=1\\).\nThere are a large number of functions which satisfy Definition 19.1, and many are programmed as options in statistical packages. We list the most important in Table \\(19.1\\) below: the Rectangular, Gaussian, Epanechnikov, Triangular, and Biweight kernels. In practice it is unnecessary to consider kernels beyond these five. For nonparametric regression we recommend either the Gaussian or Epanechnikov kernel, and either will give similar results. In Table \\(19.1\\) we express the kernels in normalized form.\nFor more discussion on kernel functions see Chapter 17 of Probability and Statistics for Economists. A generalization of (19.1) is obtained by replacing the indicator function with a kernel function:\n\\[\n\\widehat{m}_{\\mathrm{nw}}(x)=\\frac{\\sum_{i=1}^{n} K\\left(\\frac{X_{i}-x}{h}\\right) Y_{i}}{\\sum_{i=1}^{n} K\\left(\\frac{X_{i}-x}{h}\\right)} .\n\\]\nThe estimator (19.2) is known as the Nadaraya-Watson estimator, the kernel regression estimator, or the local constant estimator, and was introduced independently by Nadaraya (1964) and Watson (1964).\nThe rolling binned means estimator (19.1) is the Nadarya-Watson estimator with the rectangular kernel. The Nadaraya-Watson estimator (19.2) can be used with any standard kernel and is typically estimated using the Gaussian or Epanechnikov kernel. In general we recommend the Gaussian kernel because it produces an estimator \\(\\widehat{m}_{\\mathrm{nw}}(x)\\) which possesses derivatives of all orders.\nThe bandwidth \\(h\\) plays a similar role in kernel regression as in kernel density estimation. Namely, larger values of \\(h\\) will result in estimates \\(\\widehat{m}_{\\mathrm{nw}}(x)\\) which are smoother in \\(x\\), and smaller values of \\(h\\) will result in estimates which are more erratic. It might be helpful to consider the two extreme cases \\(h \\rightarrow 0\\) Table 19.1: Common Normalized Second-Order Kernels\n\nand \\(h \\rightarrow \\infty\\). As \\(h \\rightarrow 0\\) we can see that \\(\\widehat{m}_{\\mathrm{nw}}\\left(X_{i}\\right) \\rightarrow Y_{i}\\) (if the values of \\(X_{i}\\) are unique), so that \\(\\widehat{m}_{\\mathrm{nw}}(x)\\) is simply the scatter of \\(Y_{i}\\) on \\(X_{i}\\). In contrast, as \\(h \\rightarrow \\infty\\) then \\(\\widehat{m}_{\\mathrm{nw}}(x) \\rightarrow \\bar{Y}\\), the sample mean. For intermediate values of \\(h, \\widehat{m}_{\\mathrm{nw}}(x)\\) will smooth between these two extreme cases.\nThe estimator (19.2) using the Gaussian kernel and \\(h=1 / \\sqrt{3}\\) is also displayed in Figure 19.1(a) with the long dashes. As you can see, this estimator appears to be much smoother than the binned estimator but tracks exactly the same path. The bandwidth \\(h=1 / \\sqrt{3}\\) for the Gaussian kernel is equivalent to the bandwidth \\(h=1\\) for the binned estimator because the latter is a kernel estimator using the rectangular kernel scaled to have a standard deviation of \\(1 / 3\\)."
  },
  {
    "objectID": "chpt19-nonparameter.html#local-linear-estimator",
    "href": "chpt19-nonparameter.html#local-linear-estimator",
    "title": "18  Nonparametric Regression",
    "section": "18.4 Local Linear Estimator",
    "text": "18.4 Local Linear Estimator\nThe Nadaraya-Watson (NW) estimator is often called a local constant estimator as it locally (about \\(x\\) ) approximates \\(m(x)\\) as a constant function. One way to see this is to observe that \\(\\widehat{m}_{\\mathrm{nw}}(x)\\) solves the minimization problem\n\\[\n\\widehat{m}_{\\mathrm{nw}}(x)=\\underset{m}{\\operatorname{argmin}} \\sum_{i=1}^{n} K\\left(\\frac{X_{i}-x}{h}\\right)\\left(Y_{i}-m\\right)^{2} .\n\\]\nThis is a weighted regression of \\(Y\\) on an intercept only.\nThis means that the NW estimator is making the local approximation \\(m(X) \\simeq m(x)\\) for \\(X \\simeq x\\), which means it is making the approximation\n\\[\nY=m(X)+e \\simeq m(x)+e .\n\\]\nThe NW estimator is a local estimator of this approximate model using weighted least squares.\nThis interpretation suggests that we can construct alternative nonparametric estimators of \\(m(x)\\) by alternative local approximations. Many such local approximations are possible. A popular choice is the Local Linear (LL) approximation. Instead of the approximation \\(m(X) \\simeq m(x)\\), LL uses the linear approximation \\(m(X) \\simeq m(x)+m^{\\prime}(x)(X-x)\\). Thus\n\\[\nY=m(X)+e \\simeq m(x)+m^{\\prime}(x)(X-x)+e .\n\\]\nThe LL estimator then applies weighted least squares similarly as in NW estimation.\nOne way to represent the LL estimator is as the solution to the minimization problem\n\\[\n\\left\\{\\widehat{m}_{\\mathrm{LL}}(x), \\widehat{m}_{\\mathrm{LL}}^{\\prime}(x)\\right\\}=\\underset{\\alpha, \\beta}{\\operatorname{argmin}} \\sum_{i=1}^{n} K\\left(\\frac{X_{i}-x}{h}\\right)\\left(Y_{i}-\\alpha-\\beta\\left(X_{i}-x\\right)\\right)^{2} .\n\\]\nAnother is to write the approximating model as\n\\[\nY \\simeq Z(X, x)^{\\prime} \\beta(x)+e\n\\]\nwhere \\(\\beta(x)=\\left(m(x), m^{\\prime}(x)\\right)^{\\prime}\\) and\n\\[\nZ(X, x)=\\left(\\begin{array}{c}\n1 \\\\\nX-x\n\\end{array}\\right) .\n\\]\nThis is a linear regression with regressor vector \\(Z_{i}(x)=Z\\left(X_{i}, x\\right)\\) and coefficient vector \\(\\beta(x)\\). Applying weighted least squares with the kernel weights we obtain the LL estimator\n\\[\n\\begin{aligned}\n\\widehat{\\beta}_{\\mathrm{LL}}(x) &=\\left(\\sum_{i=1}^{n} K\\left(\\frac{X_{i}-x}{h}\\right) Z_{i}(x) Z_{i}(x)^{\\prime}\\right)^{-1} \\sum_{i=1}^{n} K\\left(\\frac{X_{i}-x}{h}\\right) Z_{i}(x) Y_{i} \\\\\n&=\\left(\\boldsymbol{Z}^{\\prime} \\boldsymbol{K} \\boldsymbol{Z}\\right)^{-1} \\boldsymbol{Z}^{\\prime} \\boldsymbol{K} \\boldsymbol{Y}\n\\end{aligned}\n\\]\nwhere \\(\\boldsymbol{K}=\\operatorname{diag}\\left\\{K\\left(\\left(X_{1}-x\\right) / h\\right), \\ldots, K\\left(\\left(X_{n}-x\\right) / h\\right)\\right\\}, \\boldsymbol{Z}\\) is the stacked \\(Z_{i}(x)^{\\prime}\\), and \\(\\boldsymbol{Y}\\) is the stacked \\(Y_{i}\\). This expression generalizes the Nadaraya-Watson estimator as the latter is obtained by setting \\(Z_{i}(x)=1\\) or constraining \\(\\beta=0\\). Notice that the matrices \\(\\boldsymbol{Z}\\) and \\(\\boldsymbol{K}\\) depend on \\(x\\) and \\(h\\).\nThe local linear estimator was first suggested by Stone (1977) and came into prominence through the work of Fan (1992, 1993).\nTo visualize, Figure 19.1(b) displays the scatter plot of the same 100 observations from panel (a) divided into the same five bins. A linear regression is fit to the observations in each bin. These five fitted regression lines are displayed by the short dashed lines. This “binned regression estimator” produces a flexible appromation for the CEF but has large jumps at the edges of the partitions. The midpoints of each of these five regression lines are displayed by the solid squares and could be viewed as the target estimate for the binned regression estimator. A rolling version of the binned regression estimator moves these estimation windows continuously across the support of \\(X\\) and is displayed by the solid line. This corresponds to the local linear estimator with a rectangular kernel and a bandwidth of \\(h=1 / \\sqrt{3}\\). By construction this line passes through the solid squares. To obtain a smoother estimator we replace the rectangular with the Gaussian kernel (using the same bandwidth \\(h=1 / \\sqrt{3}\\) ). We display these estimates with the long dashes. This has the same shape as the rectangular kernel estimate (rolling binned regression) but is visually much smoother. We label this the “Local Linear” estimator because it is the standard implementation. One interesting feature is that as \\(h \\rightarrow \\infty\\) the LL estimator approaches the full-sample least squares estimator \\(\\widehat{m}_{\\mathrm{LL}}(x) \\rightarrow \\widehat{\\alpha}+\\widehat{\\beta} x\\). That is because as \\(h \\rightarrow \\infty\\) all observations receive equal weight. In this sense the LL estimator is a flexible generalization of the linear OLS estimator.\nAnother useful property of the LL estimator is that it simultaneously provides estimates of the regression function \\(m(x)\\) and its slope \\(m^{\\prime}(x)\\) at \\(x\\)."
  },
  {
    "objectID": "chpt19-nonparameter.html#local-polynomial-estimator",
    "href": "chpt19-nonparameter.html#local-polynomial-estimator",
    "title": "18  Nonparametric Regression",
    "section": "18.5 Local Polynomial Estimator",
    "text": "18.5 Local Polynomial Estimator\nThe NW and LL estimators are both special cases of the local polynomial estimator. The idea is to approximate the regression function \\(m(x)\\) by a polynomial of fixed degree \\(p\\), and then estimate locally using kernel weights.\nThe approximating model is a \\(p^{\\text {th }}\\) order Taylor series approximation\n\\[\n\\begin{aligned}\nY &=m(X)+e \\\\\n& \\simeq m(x)+m^{\\prime}(x)(X-x)+\\cdots+m^{(p)}(x) \\frac{(X-x)^{p}}{p !}+e \\\\\n&=Z(X, x)^{\\prime} \\beta(x)+e_{i}\n\\end{aligned}\n\\]\nwhere\n\\[\nZ(X, x)=\\left(\\begin{array}{c}\n1 \\\\\nX-x \\\\\n\\vdots \\\\\n\\frac{(X-x)^{p}}{p !}\n\\end{array}\\right) \\quad \\beta(x)=\\left(\\begin{array}{c}\nm(x) \\\\\nm^{\\prime}(x) \\\\\n\\vdots \\\\\nm^{(p)}(x)\n\\end{array}\\right)\n\\]\nThe estimator is\n\\[\n\\begin{aligned}\n\\widehat{\\beta}_{\\mathrm{LP}}(x) &=\\left(\\sum_{i=1}^{n} K\\left(\\frac{X_{i}-x}{h}\\right) Z_{i}(x) Z_{i}(x)^{\\prime}\\right)^{-1}\\left(\\sum_{i=1}^{n} K\\left(\\frac{Y_{i}-x}{h}\\right) Z_{i}(x) Y_{i}\\right) \\\\\n&=\\left(\\boldsymbol{Z}^{\\prime} \\boldsymbol{K} \\boldsymbol{Z}\\right)^{-1} \\boldsymbol{Z}^{\\prime} \\boldsymbol{K} \\boldsymbol{Y}\n\\end{aligned}\n\\]\nwhere \\(Z_{i}(x)=Z\\left(X_{i}, x\\right)\\) Notice that this expression includes the Nadaraya-Watson and local linear estimators as special cases with \\(p=0\\) and \\(p=1\\), respectively.\nThere is a trade-off between the polynomial order \\(p\\) and the local smoothing bandwidth \\(h\\). By increasing \\(p\\) we improve the model approximation and thereby can use a larger bandwidth \\(h\\). On the other hand, increasing \\(p\\) increases estimation variance."
  },
  {
    "objectID": "chpt19-nonparameter.html#asymptotic-bias",
    "href": "chpt19-nonparameter.html#asymptotic-bias",
    "title": "18  Nonparametric Regression",
    "section": "18.6 Asymptotic Bias",
    "text": "18.6 Asymptotic Bias\nSince \\(\\mathbb{E}[Y \\mid X=x]=m(x)\\), the conditional expectation of the Nadaraya-Watson estimator is\n\\[\n\\mathbb{E}\\left[\\widehat{m}_{\\mathrm{nw}}(x) \\mid \\boldsymbol{X}\\right]=\\frac{\\sum_{i=1}^{n} K\\left(\\frac{X_{i}-x}{h}\\right) \\mathbb{E}\\left[Y_{i} \\mid X_{i}\\right]}{\\sum_{i=1}^{n} K\\left(\\frac{X_{i}-x}{h}\\right)}=\\frac{\\sum_{i=1}^{n} K\\left(\\frac{X_{i}-x}{h}\\right) m\\left(X_{i}\\right)}{\\sum_{i=1}^{n} K\\left(\\frac{X_{i}-x}{h}\\right)} .\n\\]\nWe can simplify this expression as \\(n \\rightarrow \\infty\\).\nThe following regularity conditions will be maintained through the chapter. Let \\(f(x)\\) denote the marginal density of \\(X\\) and let \\(\\sigma^{2}(x)=\\mathbb{E}\\left[e^{2} \\mid X=x\\right]\\) denote the conditional variance of \\(e=Y-m(X)\\).\nAssumption $19.1\n\n\\(h \\rightarrow 0\\).\n\\(n h \\rightarrow \\infty\\).\n\\(m(x), f(x)\\), and \\(\\sigma^{2}(x)\\) are continuous in some neighborhood \\(\\mathscr{N}\\) of \\(x\\).\n\\(f(x)>0\\).\n\nThese conditions are similar to those used for the asymptotic theory for kernel density estimation. The assumptions \\(h \\rightarrow 0\\) and \\(n h \\rightarrow \\infty\\) means that the bandwidth gets small yet the number of observations in the estimation window diverges to infinity. Assumption 19.1.3 are minimal smoothness conditions on the conditional expectation \\(m(x)\\), marginal density \\(f(x)\\), and conditional variance \\(\\sigma^{2}(x)\\). Assumption 19.1.4 specifies that the marginal density is non-zero. This is required because we are estimating the conditional expectation at \\(x\\), so there needs to be a non-trivial number of observations for \\(X_{i}\\) near \\(x\\).\nTheorem 19.1 Suppose Assumption \\(19.1\\) holds and \\(m^{\\prime \\prime}(x)\\) and \\(f^{\\prime}(x)\\) are continuous in \\(\\mathscr{N}\\). Then as \\(n n \\rightarrow \\infty\\) with \\(h \\rightarrow 0\\)\n\n\\(\\mathbb{E}\\left[\\widehat{m}_{\\mathrm{nw}}(x) \\mid \\boldsymbol{X}\\right]=m(x)+h^{2} B_{\\mathrm{nw}}(x)+o_{p}\\left(h^{2}\\right)+O_{p}\\left(\\sqrt{\\frac{h}{n}}\\right)\\)\n\nwhere\n\\[\nB_{\\mathrm{nw}}(x)=\\frac{1}{2} m^{\\prime \\prime}(x)+f(x)^{-1} f^{\\prime}(x) m^{\\prime}(x) .\n\\]\n 1. \\(\\mathbb{E}\\left[\\widehat{m}_{\\mathrm{LL}}(x) \\mid \\boldsymbol{X}\\right]=m(x)+h^{2} B_{\\mathrm{LL}}(x)+o_{p}\\left(h^{2}\\right)+O_{p}\\left(\\sqrt{\\frac{h}{n}}\\right)\\)\nwhere\n\\[\nB_{\\mathrm{LL}}(x)=\\frac{1}{2} m^{\\prime \\prime}(x) .\n\\]\nThe proof for the Nadaraya-Watson estimator is presented in Section 19.26. For a proof for the local linear estimator see Fan and Gijbels (1996).\nWe call the terms \\(h^{2} B_{\\mathrm{nw}}(x)\\) and \\(h^{2} B_{\\mathrm{LL}}(x)\\) the asymptotic bias of the estimators.\nTheorem 19.1 shows that the asymptotic bias of the Nadaraya-Watson and local linear estimators is proportional to the squared bandwidth \\(h^{2}\\) (the degree of smoothing) and to the functions \\(B_{\\mathrm{nw}}(x)\\) and \\(B_{\\mathrm{LL}}(x)\\). The asymptotic bias of the local linear estimator depends on the curvature (second derivative) of the CEF function \\(m(x)\\) similarly to the asymptotic bias of the kernel density estimator in Theorem \\(17.1\\) of Probability and Statistics for Economists. When \\(m^{\\prime \\prime}(x)<0\\) then \\(\\hat{m}_{\\mathrm{LL}}(x)\\) is downwards biased. When \\(m^{\\prime \\prime}(x)>0\\) then \\(\\widehat{m}_{\\mathrm{LL}}(x)\\) is upwards biased. Local averaging smooths \\(m(x)\\), inducing bias, and this bias is increasing in the level of curvature of \\(m(x)\\). This is called smoothing bias.\nThe asymptotic bias of the Nadaraya-Watson estimator adds a second term which depends on the first derivatives of \\(m(x)\\) and \\(f(x)\\). This is because the Nadaraya-Watson estimator is a local average. If the density is upward sloped at \\(x\\) (if \\(f^{\\prime}(x)>0\\) ) then there are (on average) more observations to the right of \\(x\\) than to the left so a local average will be biased if \\(m(x)\\) has a non-zero slope. In contrast the bias of the local linear estimator does not depend on the local slope \\(m^{\\prime}(x)\\) because it locally fits a linear regression. The fact that the bias of the local linear estimator has fewer terms than the bias of the Nadaraya-Watson estimator (and is invariant to the slope \\(m^{\\prime}(x)\\) ) justifies the claim that the local linear estimator has generically reduced bias relative to Nadaraya-Watson.\nWe illustrate asymptotic smoothing bias in Figure 19.2. The solid line is the true CEF for the data displayed in Figure 19.1. The dashed lines are the asymptotic approximations to the expectation \\(m(x)+\\) \\(h^{2} B(x)\\) for bandwidths \\(h=1 / 2, h=1\\), and \\(h=3 / 2\\). (The asymptotic biases of the NW and LL estimators are the same because \\(X\\) has a uniform distribution.) You can see that there is minimal bias for the smallest bandwidth but considerable bias for the largest. The dashed lines are smoothed versions of the CEF, attenuating the peaks and valleys.\nSmoothing bias is a natural by-product of nonparametric estimation of nonlinear functions. It can only be reduced by using a small bandwidth. As we see in the following section this will result in high estimation variance.\n\nFigure 19.2: Smoothing Bias"
  },
  {
    "objectID": "chpt19-nonparameter.html#asymptotic-variance",
    "href": "chpt19-nonparameter.html#asymptotic-variance",
    "title": "18  Nonparametric Regression",
    "section": "18.7 Asymptotic Variance",
    "text": "18.7 Asymptotic Variance\nFrom (19.3) we deduce that\n\\[\n\\widehat{m}_{\\mathrm{nw}}(x)-\\mathbb{E}\\left[\\widehat{m}_{\\mathrm{nw}}(x) \\mid \\boldsymbol{X}\\right]=\\frac{\\sum_{i=1}^{n} K\\left(\\frac{X_{i}-x}{h}\\right) e_{i}}{\\sum_{i=1}^{n} K\\left(\\frac{X_{i}-x}{h}\\right)} .\n\\]\nSince the denominator is a function only of \\(X_{i}\\) and the numerator is linear in \\(e_{i}\\) we can calculate that the finite sample variance of \\(\\widehat{m}_{\\mathrm{nw}}(x)\\) is\n\\[\n\\operatorname{var}\\left[\\widehat{m}_{\\mathrm{nw}}(x) \\mid \\boldsymbol{X}\\right]=\\frac{\\sum_{i=1}^{n} K\\left(\\frac{X_{i}-x}{h}\\right)^{2} \\sigma^{2}\\left(X_{i}\\right)}{\\left(\\sum_{i=1}^{n} K\\left(\\frac{X_{i}-x}{h}\\right)\\right)^{2}} .\n\\]\nWe can simplify this expression as \\(n \\rightarrow \\infty\\). Let \\(\\sigma^{2}(x)=\\mathbb{E}\\left[e^{2} \\mid X=x\\right]\\) denote the conditional variance of \\(e=Y-m(X)\\).\nTheorem 19.2 Under Assumption 19.1,\n\n\\(\\operatorname{var}\\left[\\widehat{m}_{\\mathrm{nw}}(x) \\mid \\boldsymbol{X}\\right]=\\frac{R_{K} \\sigma^{2}(x)}{f(x) n h}+o_{p}\\left(\\frac{1}{n h}\\right)\\).\n\\(\\operatorname{var}\\left[\\hat{m}_{\\mathrm{LL}}(x) \\mid \\boldsymbol{X}\\right]=\\frac{R_{K} \\sigma^{2}(x)}{f(x) n h}+o_{p}\\left(\\frac{1}{n h}\\right)\\).\n\nIn these expressions\n\\[\nR_{K}=\\int_{-\\infty}^{\\infty} K(u)^{2} d u\n\\]\nis the roughness of the kernel \\(K(u)\\).\nThe proof for the Nadaraya-Watson estimator is presented in Section 19.26. For the local linear estimator see Fan and Gijbels (1996).\nWe call the leading terms in Theorem \\(19.2\\) the asymptotic variance of the estimators. Theorem \\(19.2\\) shows that the asymptotic variance of the two estimators are identical. The asymptotic variance is proportional to the roughness \\(R_{K}\\) of the kernel \\(K(u)\\) and to the conditional variance \\(\\sigma^{2}(x)\\) of the regression error. It is inversely proportional to the effective number of observations \\(n h\\) and to the marginal density \\(f(x)\\). This expression reflects the fact that the estimators are local estimators. The precision of \\(\\widehat{m}(x)\\) is low for regions where \\(e\\) has a large conditional variance and/or \\(X\\) has a low density (where there are relatively few observations)."
  },
  {
    "objectID": "chpt19-nonparameter.html#aimse",
    "href": "chpt19-nonparameter.html#aimse",
    "title": "18  Nonparametric Regression",
    "section": "18.8 AIMSE",
    "text": "18.8 AIMSE\nWe define the asymptotic MSE (AMSE) of an estimator \\(\\widehat{m}(x)\\) as the sum of its squared asymptotic bias and asymptotic variance. Using Theorems \\(19.1\\) and \\(19.2\\) for the Nadaraya-Watson and local linear estimators, we obtain\n\\[\n\\operatorname{AMSE}(x) \\stackrel{\\text { def }}{=} h^{4} B(x)^{2}+\\frac{R_{K} \\sigma^{2}(x)}{n h f(x)}\n\\]\nwhere \\(B(x)=B_{\\mathrm{nw}}(x)\\) for the Nadaraya-Watson estimator and \\(B(x)=B_{\\mathrm{LL}}(x)\\) for the local linear estimator. This is the asymptotic MSE for the estimator \\(\\widehat{m}(x)\\) for a single point \\(x\\).\nA global measure of fit can be obtained by integrating AMSE \\((x)\\). It is standard to weight the AMSE by \\(f(x) w(x)\\) for some integrable weight function \\(w(x)\\). This is called the asymptotic integrated MSE (AIMSE). Let \\(S\\) be the support of \\(X\\) (the region where \\(f(x)>0\\) ).\n\\[\n\\operatorname{AIMSE} \\stackrel{\\text { def }}{=} \\int_{S} \\operatorname{AMSE}(x) f(x) w(x) d x=\\int_{S}\\left(h^{4} B(x)^{2}+\\frac{R_{K} \\sigma^{2}(x)}{n h f(x)}\\right) f(x) w(x) d x=h^{4} \\bar{B}+\\frac{R_{K}}{n h} \\bar{\\sigma}^{2}\n\\]\nwhere\n\\[\n\\begin{aligned}\n\\bar{B} &=\\int_{S} B(x)^{2} f(x) w(x) d x \\\\\n\\bar{\\sigma}^{2} &=\\int_{S} \\sigma^{2}(x) w(x) d x .\n\\end{aligned}\n\\]\nThe weight function \\(w(x)\\) can be omitted if \\(S\\) is bounded. Otherwise, a common choice is \\(w(x)=\\) \\(\\mathbb{1}\\left\\{\\xi_{1} \\leq x \\leq \\xi_{2}\\right\\}\\). An integrable weight function is needed when \\(X\\) has unbounded support to ensure that \\(\\bar{\\sigma}^{2}<\\infty\\)\nThe form of the AIMSE is similar to that for kernel density estimation (Theorem \\(17.3\\) of Probability and Statistics for Economists). It has two terms (squared bias and variance). The first is increasing in the bandwidth \\(h\\) and the second is decreasing in \\(h\\). Thus the choice of \\(h\\) affects AIMSE with a trade-off between these two components. Similarly to density estimation we can calculate the bandwidth which minimizes the AIMSE. (See Exercise 19.2.) The solution is given in the following theorem.\nTheorem 19.3 The bandwidth which minimizes the AIMSE (19.5) is\n\\[\nh_{0}=\\left(\\frac{R_{K} \\bar{\\sigma}^{2}}{4 \\bar{B}}\\right)^{1 / 5} n^{-1 / 5} .\n\\]\nWith \\(h \\sim n^{-1 / 5}\\) then AIMSE \\([\\widehat{m}(x)]=O\\left(n^{-4 / 5}\\right)\\).\nThis result characterizes the AIMSE-optimal bandwidth. This bandwidth satisfies the rate \\(h=\\mathrm{cn}^{-1 / 5}\\) which is the same rate as for kernel density estimation. The optimal constant \\(c\\) depends on the kernel \\(K(x)\\), the weighted average squared bias \\(\\bar{B}\\), and the weighted average variance \\(\\bar{\\sigma}^{2}\\). The constant \\(c\\) is different, however, from that for density estimation.\nInserting (19.6) into (19.5) plus some algebra we find that the AIMSE using the optimal bandwidth is\n\\[\n\\operatorname{AIMSE}_{0} \\simeq 1.65\\left(R_{K}^{4} \\bar{B} \\bar{\\sigma}^{8}\\right)^{1 / 5} n^{-4 / 5} .\n\\]\nThis depends on the kernel \\(K(u)\\) only through the constant \\(R_{K}\\). Since the Epanechnikov kernel has the smallest value \\({ }^{1}\\) of \\(R_{K}\\) it is also the kernel which produces the smallest AIMSE. This is true for both the NW and LL estimators.\n\\({ }^{1}\\) See Theorem \\(17.4\\) of Probability and Statistics for Economists. Theorem 19.4 The AIMSE (19.5) of the Nadaraya-Watson and Local Linear regression estimators is minimized by the Epanechnikov kernel.\nThe efficiency loss by using the other standard kernels, however, is small. The relative efficiency \\({ }^{2}\\) of estimation using the another kernel is \\(\\left(R_{K} / R_{K} \\text { (Epanechnikov) }\\right)^{2 / 5}\\). Using the values of \\(R_{K}\\) from Table \\(19.1\\) we calculate that the efficiency loss from using the Triangle, Gaussian, and Rectangular kernels are \\(1 %, 2 %\\), and \\(3 %\\), respectively, which are minimal. Since the Gaussian kernel produces the smoothest estimates, which is important for estimation of marginal effects, our overall recommendation is the Gaussian kernel."
  },
  {
    "objectID": "chpt19-nonparameter.html#reference-bandwidth",
    "href": "chpt19-nonparameter.html#reference-bandwidth",
    "title": "18  Nonparametric Regression",
    "section": "18.9 Reference Bandwidth",
    "text": "18.9 Reference Bandwidth\nThe NW, LL and LP estimators depend on a bandwidth and without an empirical rule for selection of \\(h\\) the methods are incomplete. It is useful to have a reference bandwith which mimics the optimal bandwidth in a simplified setting and provides a baseline for further investigations.\nTheorem \\(19.3\\) and a little re-writing reveals that the optimal bandwidth equals\n\\[\nh_{0}=\\left(\\frac{R_{K}}{4}\\right)^{1 / 5}\\left(\\frac{\\bar{\\sigma}^{2}}{n \\bar{B}}\\right)^{1 / 5} \\simeq 0.58\\left(\\frac{\\bar{\\sigma}^{2}}{n \\bar{B}}\\right)^{1 / 5}\n\\]\nwhere the approximation holds for all single-peaked kernels by similar calculations \\({ }^{3}\\) as in Section \\(17.9\\) of Probability and Statistics for Economists.\nA reference approach can be used to develop a rule-of-thumb for regression estimation. In particular, Fan and Gijbels (1996, Section 4.2) develop what they call the ROT (rule of thumb) bandwidth for the local linear estimator. We now describe their derivation.\nFirst, set \\(w(x)=\\mathbb{1}\\left\\{\\xi_{1} \\leq x \\leq \\xi_{2}\\right\\}\\). Second, form a pilot or preliminary estimator of the regression function \\(m(x)\\) using a \\(q^{t h}\\)-order polynomial regression\n\\[\nm(x)=\\beta_{0}+\\beta_{1} x+\\beta_{2} x^{2}+\\cdots+\\beta_{q} x^{q}\n\\]\nfor \\(q \\geq 2\\). (Fan and Gijbels (1996) suggest \\(q=4\\) but this is not essential.) By least squares we obtain the coefficient estimates \\(\\widehat{\\beta}_{0}, \\ldots, \\widehat{\\beta}_{\\underline{q}}\\) and implied second derivative \\(\\widehat{m}^{\\prime \\prime}(x)=2 \\widehat{\\beta}_{2}+6 \\widehat{\\beta}_{3} x+12 \\widehat{\\beta}_{4} x^{2}+\\cdots+q(q-\\) 1) \\(\\widehat{\\beta}_{q} x^{q-2}\\). Third, notice that \\(\\frac{q}{B}\\) can be written as an expectation\n\\[\n\\bar{B}=\\mathbb{E}\\left[B(X)^{2} w(X)\\right]=\\mathbb{E}\\left[\\left(\\frac{1}{2} m^{\\prime \\prime}(X)\\right)^{2} \\mathbb{1}\\left\\{\\xi_{1} \\leq X \\leq \\xi_{2}\\right\\}\\right] .\n\\]\nA moment estimator is\n\\[\n\\widehat{B}=\\frac{1}{n} \\sum_{i=1}^{n}\\left(\\frac{1}{2} \\widehat{m}^{\\prime \\prime}\\left(X_{i}\\right)\\right)^{2} \\mathbb{1}\\left\\{\\xi_{1} \\leq X_{i} \\leq \\xi_{2}\\right\\} .\n\\]\nFourth, assume that the regression error is homoskedastic \\(\\mathbb{E}\\left[e^{2} \\mid X\\right]=\\sigma^{2}\\) so that \\(\\bar{\\sigma}^{2}=\\sigma^{2}\\left(\\xi_{2}-\\xi_{1}\\right)\\). Estimate \\(\\sigma^{2}\\) by the error variance estimate \\(\\widehat{\\sigma}^{2}\\) from the preliminary regression. Plugging these into (19.7) we obtain the reference bandwidth\n\\[\nh_{\\mathrm{rot}}=0.58\\left(\\frac{\\widehat{\\sigma}^{2}\\left(\\xi_{2}-\\xi_{1}\\right)}{n \\widehat{B}}\\right)^{1 / 5} .\n\\]\n\\({ }^{2}\\) Measured by root AIMSE.\n\\({ }^{3}\\) The constant \\(\\left(R_{K} / 4\\right)^{1 / 5}\\) is bounded between \\(0.58\\) and \\(0.59\\). Fan and Gijbels (1996) call this the Rule-of-Thumb (ROT) bandwidth.\nFan and Gijbels developed similar rules for higher-order odd local polynomial estimators but not for the local constant (Nadaraya-Watson) estimator. However, we can derive a ROT for the NW as well by using a reference model for the marginal density \\(f(x)\\). A convenient choice is the uniform density under which \\(f^{\\prime}(x)=0\\) and the optimal bandwidths for NW and LL coincide. This motivates using (19.9) as a ROT bandwidth for both the LL and NW estimators.\nAs we mentioned above, Fan and Gijbels suggest using a \\(4^{t h}\\)-order polynomial for the pilot estimator but this specific choice is not essential. In applications it may be prudent to assess sensitivity of the ROT bandwith to the choice of \\(q\\) and to examine the estimated pilot regression for precision of the estimated higher-order polynomial terms.\nWe now comment on the choice of the weight region \\(\\left[\\xi_{1}, \\xi_{2}\\right]\\). When \\(X\\) has bounded support then \\(\\left[\\xi_{1}, \\xi_{2}\\right]\\) can be set equal to this support. Otherwise, \\(\\left[\\xi_{1}, \\xi_{2}\\right]\\) can be set equal to the region of interest for \\(\\widehat{m}(x)\\), or the endpoints can be set to equal fixed quantiles (e.g. \\(0.05\\) and \\(0.95\\) ) of the distribution of \\(X\\).\nTo illustrate, take the data shown in Figure 19.1. If we fit a \\(4^{\\text {th }}\\) order polynomial we find \\(\\widehat{m}(x)=\\) \\(.49+.70 x-.28 x^{2}-.033 x^{3}-.0012 x^{4}\\) which implies \\(\\widehat{m}^{\\prime \\prime}(x)=-.56-.20 x-.014 x^{2}\\). Setting \\(\\left[\\xi_{1}, \\xi_{2}\\right]=[0,10]\\) from the support of \\(X\\) we find \\(\\widehat{B}=0.00889\\). The residuals from the polynomial regression have variance \\(\\widehat{\\sigma}^{2}=0.0687\\). Plugging these into (19.9) we find \\(h_{\\mathrm{rot}}=0.551\\) which is similar that used in Figure 19.1."
  },
  {
    "objectID": "chpt19-nonparameter.html#estimation-at-a-boundary",
    "href": "chpt19-nonparameter.html#estimation-at-a-boundary",
    "title": "18  Nonparametric Regression",
    "section": "18.10 Estimation at a Boundary",
    "text": "18.10 Estimation at a Boundary\nOne advantage of the local linear over the Nadaraya-Watson estimator is that the LL has better performance at the boundary of the support of \\(X\\). The NW estimator has excessive smoothing bias near the boundaries. In many contexts in econometrics the boundaries are of great interest. In such cases it is strongly recommended to use the local linear estimator (or a local polynomial estimator with \\(p \\geq 1\\) ).\nTo understand the problem it may be helpful to examine Figure 19.3. This shows a scatter plot of 100 observations generated as \\(X \\sim U[0,10]\\) and \\(Y \\sim \\mathrm{N}(X, 1)\\) so that \\(m(x)=x\\). Suppose we are interested in the CEF \\(m(0)\\) at the lower boundary \\(x=0\\). The Nadaraya-Watson estimator equals a weighted average of the \\(Y\\) observations for small values of \\(|X|\\). Since \\(X \\geq 0\\), these are all observations for which \\(m(X) \\geq m(0)\\), and therefore \\(\\hat{m}_{\\mathrm{nw}}(0)\\) is biased upwards. Symmetrically, the Nadaraya-Watson estimator at the upper boundary \\(x=10\\) is a weighted average of observations for which \\(m(X) \\leq m(10)\\) and therefore \\(\\widehat{m}_{\\mathrm{nw}}(10)\\) is biased downwards.\nIn contrast, the local linear estimators \\(\\widehat{m}_{\\mathrm{LL}}(0)\\) and \\(\\widehat{m}_{\\mathrm{LL}}(10)\\) are unbiased in this example because \\(m(x)\\) is linear in \\(x\\). The local linear estimator fits a linear regression line. Since the expectation is correctly specified there is no estimation bias.\nThe exact bias \\({ }^{4}\\) of the NW estimator is shown in Figure \\(19.3\\) by the dashed lines. The long dashes is the expectation \\(\\mathbb{E}\\left[\\widehat{m}_{\\mathrm{nw}}(x)\\right]\\) for \\(h=1\\) and the short dashes is the expectation \\(\\mathbb{E}\\left[\\widehat{m}_{\\mathrm{nw}}(x)\\right]\\) for \\(h=2\\). We can see that the bias is substantial. For \\(h=2\\) the bias is visible for all values of \\(x\\). For the smaller bandwidth \\(h=1\\) the bias is minimal for \\(x\\) in the central range of the support, but is still quite substantial for \\(x\\) near the boundaries.\nTo calculate the asymptotic smoothing bias we can revisit the proof of Theorem 19.1.1 which calculated the asymptotic bias at interior points. Equation (19.29) calculates the bias of the numerator of the estimator expressed as an integral over the marginal density. Evaluated at a lower boundary the density is positive only for \\(u \\geq 0\\) so the integral is over the positive region \\([0, \\infty)\\). This applies as well to equation (19.31) and the equations which follow. In this case the leading term of this expansion is the first term (19.32) which is proportional to \\(h\\) rather than \\(h^{2}\\). Completing the calculations we find the following. Define \\(m(x+)=\\lim _{z \\downarrow x} m(z)\\) and \\(m(x-)=\\lim _{z \\uparrow x} m(z)\\).\n\\({ }^{4}\\) Calculated by simulation from 10,000 simulation replications.\n\nFigure 19.3: Boundary Bias\nTheorem 19.5 Suppose Assumption \\(19.1\\) holds. Set \\(\\mu_{K}=2 \\int_{0}^{\\infty} K(u) d u\\). Let the support of \\(X\\) be \\(S=[\\underline{x}, \\bar{x}]\\).\nIf \\(m^{\\prime \\prime}(\\underline{x}+), \\sigma^{2}(\\underline{x}+)\\) and \\(f^{\\prime}(\\underline{x}+)\\) exist, and \\(f(\\underline{x}+)>0\\) then\n\\[\n\\mathbb{E}\\left[\\hat{m}_{\\mathrm{nw}}(\\underline{x}) \\mid \\boldsymbol{X}\\right]=m(\\underline{x})+h m^{\\prime}(\\underline{x}) \\mu_{K}+o_{p}(h)+O_{p}\\left(\\sqrt{\\frac{h}{n}}\\right) .\n\\]\nIf \\(m^{\\prime \\prime}(\\bar{x}-), \\sigma^{2}(\\bar{x}-)\\) and \\(f^{\\prime}(\\bar{x}-)\\) exist, and \\(f(\\bar{x}-)>0\\) then\n\\[\n\\mathbb{E}\\left[\\widehat{m}_{\\mathrm{nw}}(\\bar{x}) \\mid \\boldsymbol{X}\\right]=m(\\bar{x})-h m^{\\prime}(\\bar{x}) \\mu_{K}+o_{p}(h)+O_{p}\\left(\\sqrt{\\frac{h}{n}}\\right) .\n\\]\nTheorem \\(19.5\\) shows that the asymptotic bias of the NW estimator at the boundary is \\(O(h)\\) and depends on the slope of \\(m(x)\\) at the boundary. When the slope is positive the NW estimator is upward biased at the lower boundary and downward biased at the upper boundary. The standard interpretation of Theorem \\(19.5\\) is that the NW estimator has high bias near boundary points.\nSimilarly we can evaluate the performance of the LL estimator. We summarize the results without derivation (as they are more technically challenging) and instead refer interested readers to Cheng, Fan and Marron (1997) and Imbens and Kalyahnaraman (2012).\nDefine the kernel moments \\(v_{j}=\\int_{0}^{\\infty} u^{j} K(u) d u, \\pi_{j}=\\int_{0}^{\\infty} u^{j} K(u)^{2} d u\\), and projected kernel\n\\[\nK^{*}(u)=\\left[\\begin{array}{ll}\n1 & 0\n\\end{array}\\right]\\left[\\begin{array}{ll}\nv_{0} & v_{1} \\\\\nv_{1} & v_{2}\n\\end{array}\\right]^{-1}\\left[\\begin{array}{c}\n1 \\\\\nu\n\\end{array}\\right] K(u)=\\frac{v_{2}-v_{1} u}{v_{0} v_{2}-v_{1}^{2}} K(u) .\n\\]\nDefine its second moment\n\\[\n\\sigma_{K^{*}}^{2}=\\int_{0}^{\\infty} u^{2} K^{*}(u) d u=\\frac{v_{2}^{2}-v_{1} v_{3}}{v_{0} v_{2}-v_{1}^{2}}\n\\]\nand roughness\n\\[\nR_{K}^{*}=\\int_{0}^{\\infty} K^{*}(u)^{2} d u=\\frac{v_{2}^{2} \\pi_{0}-2 v_{1} v_{2} \\pi_{1}+v_{1}^{2} \\pi_{2}}{\\left(v_{0} v_{2}-v_{1}^{2}\\right)^{2}}\n\\]\nTheorem 19.6 Under the assumptions of Theorem 19.5, at a boundary point \\(\\underline{x}\\)\n\n\\(\\mathbb{E}\\left[\\hat{m}_{\\mathrm{LL}}(\\underline{x}) \\mid \\boldsymbol{X}\\right]=m(\\underline{x})+\\frac{h^{2} m^{\\prime \\prime}(\\underline{x}) \\sigma_{K^{*}}^{2}}{2}+o_{p}\\left(h^{2}\\right)+O_{p}\\left(\\sqrt{\\frac{h}{n}}\\right)\\)\n\\(\\operatorname{var}\\left[\\widehat{m}_{\\mathrm{LL}}(\\underline{x}) \\mid \\boldsymbol{X}\\right]=\\frac{R_{K}^{*} \\sigma^{2}(\\underline{x})}{f(\\underline{x}) n h}+o_{p}\\left(\\frac{1}{n h}\\right)\\)\n\nTheorem 19.6 shows that the asymptotic bias of the LL estimator at a boundary is \\(O\\left(h^{2}\\right.\\) ), the same as at interior points and is invariant to the slope of \\(m(x)\\). The theorem also shows that the asymptotic variance has the same rate as at interior points.\nTaking Theorems 19.1, 19.2, 19.5, and \\(19.6\\) together we conclude that the local linear estimator has superior asymptotic properties relative to the NW estimator. At interior points the two estimators have the same asymptotic variance. The bias of the LL estimator is invariant to the slope of \\(m(x)\\) and its asymptotic bias only depends on the second derivative while the bias of the NW estimator depends on both the first and second derivatives. At boundary points the asymptotic bias of the NW estimator is \\(O(h)\\) which is of higher order than the \\(O\\left(h^{2}\\right)\\) bias of the LL estimator. For these reasons we recommend the local linear estimator over the Nadaraya-Watson estimator. A similar argument can be made to recommend the local cubic estimator, but this is not widely used.\nThe asymptotic bias and variance of the LL estimator at the boundary is slightly different than in the interior. The difference is that the bias and variance depend on the moments of the kernel-like function \\(K^{*}(u)\\) rather than the original kernel \\(K(u)\\).\nAn interesting question is to find the optimal kernel function for boundary estimation. By the same calculations as for Theorem \\(19.4\\) we find that the optimal kernel \\(K^{*}(u)\\) minimizes the roughness \\(R_{K}^{*}\\) given the second moment \\(\\sigma_{K^{*}}^{2}\\) and as argued for Theorem \\(19.4\\) this is achieved when \\(K^{*}(u)\\) equals a quadratic function in \\(u\\). Since \\(K^{*}(u)\\) is the product of \\(K(u)\\) and a linear function this means that \\(K(u)\\) must be linear in \\(|u|\\), implying that the optimal kernel \\(K(u)\\) is the Triangular kernel. See Cheng, Fan, and Marron (1997). Calculations similar to those following Theorem \\(19.4\\) show that efficiency loss \\({ }^{5}\\) of estimation using the Epanechnikov, Gaussian, and Rectangular kernels are 1%, 1%, and 3%, respectively.\n\\({ }^{5}\\) Measured by root AIMSE."
  },
  {
    "objectID": "chpt19-nonparameter.html#nonparametric-residuals-and-prediction-errors",
    "href": "chpt19-nonparameter.html#nonparametric-residuals-and-prediction-errors",
    "title": "18  Nonparametric Regression",
    "section": "18.11 Nonparametric Residuals and Prediction Errors",
    "text": "18.11 Nonparametric Residuals and Prediction Errors\nGiven any nonparametric regression estimator \\(\\widehat{m}(x)\\) the fitted regression at \\(x=X_{i}\\) is \\(\\widehat{m}\\left(X_{i}\\right)\\) and the fitted residual is \\(\\widehat{e}_{i}=Y_{i}-\\widehat{m}\\left(X_{i}\\right)\\). As a general rule, but especially when the bandwidth \\(h\\) is small, it is hard to view \\(\\widehat{e}_{i}\\) as a good measure of the fit of the regression. For the NW and LL estimators, as \\(h \\rightarrow 0\\) then \\(\\widehat{m}\\left(X_{i}\\right) \\rightarrow Y_{i}\\) and therefore \\(\\widehat{e}_{i} \\rightarrow 0\\). This is clear overfitting as the true error \\(e_{i}\\) is not zero. In general, because \\(\\widehat{m}\\left(X_{i}\\right)\\) is a local average which includes \\(Y_{i}\\), the fitted value will be necessarily close to \\(Y_{i}\\) and the residual \\(\\widehat{e}_{i}\\) small, and the degree of this overfitting increases as \\(h\\) decreases.\nA standard solution is to measure the fit of the regression at \\(x=X_{i}\\) by re-estimating the model excluding the \\(i^{t h}\\) observation. Let \\(\\widetilde{m}_{-i}(x)\\) be the leave-one-out nonparametric estimator computed without observation \\(i\\). For example, for Nadaraya-Watson regression, this is\n\\[\n\\widetilde{Y}_{i}=\\widetilde{m}_{-i}(x)=\\frac{\\sum_{j \\neq i} K\\left(\\frac{X_{j}-x}{h}\\right) Y_{j}}{\\sum_{j \\neq i} K\\left(\\frac{X_{j}-x}{h}\\right)} .\n\\]\nNotationally, the “-i” subscript is used to indicate that the \\(i^{t h}\\) observation is omitted.\nThe leave-one-out predicted value for \\(Y_{i}\\) at \\(x=X_{i}\\) is \\(\\widetilde{Y}_{i}=\\widetilde{m}_{-i}\\left(X_{i}\\right)\\) and the leave-one-out prediction error is\n\\[\n\\widetilde{e}_{i}=Y_{i}-\\widetilde{Y}_{i} .\n\\]\nSince \\(\\widetilde{Y}_{i}\\) is not a function of \\(Y_{i}\\) there is no tendency for \\(\\widetilde{Y}_{i}\\) to overfit for small \\(h\\). Consequently, \\(\\widetilde{e}_{i}\\) is a good measure of the fit of the estimated nonparametric regression.\nWhen possible the leave-one-out prediction errors should be used instead of the residuals \\(\\widehat{e}_{i}\\)."
  },
  {
    "objectID": "chpt19-nonparameter.html#cross-validation-bandwidth-selection",
    "href": "chpt19-nonparameter.html#cross-validation-bandwidth-selection",
    "title": "18  Nonparametric Regression",
    "section": "18.12 Cross-Validation Bandwidth Selection",
    "text": "18.12 Cross-Validation Bandwidth Selection\nThe most popular method in applied statistics to select bandwidths is cross-validation. The general idea is to estimate the model fit based on leave-one-out estimation. Here we describe the method as typically applied for regression estimation. The method applies to NW, LL, and LP estimation, as well as other nonparametric estimators.\nTo be explicit about the dependence of the estimator on the bandwidth let us write an estimator of \\(m(x)\\) with a given bandwidth \\(h\\) as \\(\\widehat{m}(x, h)\\).\nIdeally, we would like to select \\(h\\) to minimize the integrated mean-squared error (IMSE) of \\(\\widehat{m}(x, h)\\) as a estimator of \\(m(x)\\) :\n\\[\n\\operatorname{IMSE}_{n}(h)=\\int_{S} \\mathbb{E}\\left[(\\widehat{m}(x, h)-m(x))^{2}\\right] f(x) w(x) d x\n\\]\nwhere \\(f(x)\\) is the marginal density of \\(X\\) and \\(w(x)\\) is an integrable weight function. The weight \\(w(x)\\) is the same as used in (19.5) and can be omitted when \\(X\\) has bounded support.\nThe difference \\(\\widehat{m}(x, h)-m(x)\\) at \\(x=X_{i}\\) can be estimated by the leave-one-out prediction errors \\((19.10)\\)\n\\[\n\\widetilde{e}_{i}(h)=Y_{i}-\\widetilde{m}_{-i}\\left(X_{i}, h\\right)\n\\]\nwhere we are being explicit about the dependence on the bandwidth \\(h\\). A reasonable estimator of IMSE \\({ }_{n}(h)\\) is the weighted average mean squared prediction errors\n\\[\n\\mathrm{CV}(h)=\\frac{1}{n} \\sum_{i=1}^{n} \\widetilde{e}_{i}(h)^{2} w\\left(X_{i}\\right) .\n\\]\nThis function of \\(h\\) is known as the cross-validation criterion. Once again, if \\(X\\) has bounded support then the weights \\(w\\left(X_{i}\\right)\\) can be omitted and this is typically done in practice.\nIt turns out that the cross-validation criterion is an unbiased estimator of the IMSE plus a constant for a sample with \\(n-1\\) observations.\nTheorem $19.7\n\\[\n\\mathbb{E}[\\mathrm{CV}(h)]=\\bar{\\sigma}^{2}+\\operatorname{IMSE}_{n-1}(h)\n\\]\nwhere \\(\\bar{\\sigma}^{2}=\\mathbb{E}\\left[e^{2} w(X)\\right]\\)\nThe proof of Theorem \\(19.7\\) is presented in Section 19.26.\nSince \\(\\bar{\\sigma}^{2}\\) is a constant independent of the bandwidth \\(h, \\mathbb{E}[\\mathrm{CV}(h)]\\) is a shifted version of \\(\\operatorname{IMSE}_{n-1}(h)\\). In particular, the \\(h\\) which minimizes \\(\\mathbb{E}[\\mathrm{CV}(h)]\\) and \\(\\operatorname{IMSE}_{n-1}(h)\\) are identical. When \\(n\\) is large the bandwidth which minimizes \\(\\operatorname{IMSE}_{n-1}(h)\\) and \\(\\operatorname{IMSE}_{n}(h)\\) are nearly identical so \\(\\mathrm{CV}(h)\\) is essentially unbiased as an estimator of \\(\\operatorname{IMSE}_{n}(h)+\\bar{\\sigma}^{2}\\). This considerations lead to the recommendation to select \\(h\\) as the value which minimizes \\(\\mathrm{CV}(h)\\).\nThe cross-validation bandwidth \\(h_{\\mathrm{cv}}\\) is the value which minimizes \\(\\mathrm{CV}(h)\\)\n\\[\nh_{\\mathrm{cv}}=\\underset{h \\geq h_{\\ell}}{\\operatorname{argmin}} \\mathrm{CV}(h)\n\\]\nfor some \\(h_{\\ell}>0\\). The restriction \\(h \\geq h_{\\ell}\\) can be imposed so that \\(\\mathrm{CV}(h)\\) is not evaluated over unreasonably small bandwidths.\nThere is not an explicit solution to the minimization problem (19.13), so it must be solved numerically. One method is grid search. Create a grid of values for \\(h\\), e.g. [ \\(\\left.h_{1}, h_{2}, \\ldots, h_{J}\\right]\\), evaluate \\(C V\\left(h_{j}\\right)\\) for \\(j=1, \\ldots, J\\), and set\n\\[\nh_{\\mathrm{cv}}=\\underset{h \\in\\left[h_{1}, h_{2}, \\ldots, h_{J}\\right]}{\\operatorname{argmin}} \\mathrm{CV}(h) .\n\\]\nEvaluation using a coarse grid is typically sufficient for practical application. Plots of CV( \\(h)\\) against \\(h\\) are a useful diagnostic tool to verify that the minimum of \\(\\mathrm{CV}(h)\\) has been obtained. A computationally more efficient method for obtaining the solution (19.13) is Golden-Section Search. See Section \\(12.4\\) of Probability and Statistics for Economists.\nIt is possible for the solution (19.13) to be unbounded, that is, \\(\\mathrm{CV}(h)\\) is decreasing for large \\(h\\) so that \\(h_{\\mathrm{cv}}=\\infty\\). This is okay. It simply means that the regression estimator simplifies to its full-sample version. For Nadaraya-Watson estimator this is \\(\\hat{m}_{\\mathrm{nw}}(x)=\\bar{Y}\\). For the local linear estimator this is \\(\\hat{m}_{\\mathrm{LL}}(x)=\\widehat{\\alpha}+\\widehat{\\beta} x\\).\nFor NW and LL estimation, the criterion (19.11) requires leave-one-out estimation of the conditional mean at each observation \\(X_{i}\\). This is different from calculation of the estimator \\(\\widehat{m}(x)\\) as the latter is typically done at a set of fixed values of \\(x\\) for purposes of display.\nTo illustrate, Figure 19.4(a) displays the cross-validation criteria \\(\\mathrm{CV}(h)\\) for the Nadaraya-Watson and Local Linear estimators using the data from Figure 19.1, both using the Gaussian kernel. The CV functions are computed on a grid on \\(\\left[h_{\\mathrm{rot}} / 3,3 h_{\\mathrm{rot}}\\right]\\) with 200 gridpoints. The CV-minimizing bandwidths are \\(h_{\\mathrm{nw}}=0.830\\) for the Nadaraya-Watson estimator and \\(h_{\\mathrm{LL}}=0.764\\) for the local linear estimator. These are somewhat higher than the rule of thumb \\(h_{\\mathrm{rot}}=0.551\\) value calculated earlier. Figure 19.4(a) shows the minimizing bandwidths by the arrows.\nThe CV criterion can also be used to select between different nonparametric estimators. The CVselected estimator is the one with the lowest minimized CV criterion. For example, in Figure 19.4(a), you can see that the LL estimator has a minimized CV criterion of \\(0.0699\\) which is lower than the minimum\n\n\nCross-Validation Criterion\n\n\n\nNonparametric Estimates\n\nFigure 19.4: Bandwidth Selection\n\\(0.0703\\) obtained by the NW estimator. Since the LL estimator achieves a lower value of the CV criterion, LL is the CV-selected estimator. The difference, however, is small, indicating that the two estimators achieve similar IMSE.\nFigure 19.4(b) displays the local linear estimates \\(\\widehat{m}(x)\\) using the ROT and CV bandwidths along with the true conditional mean \\(m(x)\\). The estimators track the true function quite well, and the difference between the bandwidths is relatively minor in this application."
  },
  {
    "objectID": "chpt19-nonparameter.html#asymptotic-distribution",
    "href": "chpt19-nonparameter.html#asymptotic-distribution",
    "title": "18  Nonparametric Regression",
    "section": "18.13 Asymptotic Distribution",
    "text": "18.13 Asymptotic Distribution\nWe first provide a consistency result.\nTheorem 19.8 Under Assumption 19.1, \\(\\hat{m}_{\\mathrm{nw}}(x) \\underset{p}{\\rightarrow} m(x)\\) and \\(\\hat{m}_{\\mathrm{LL}}(x) \\underset{p}{\\longrightarrow}\\) \\(m(x)\\)\nA proof for the Nadaraya-Watson estimator is presented in Section 19.26. For the local linear estimator see Fan and Gijbels (1996).\nTheorem \\(19.8\\) shows that the estimators are consistent for \\(m(x)\\) under mild continuity assumptions. In particular, no smoothness conditions on \\(m(x)\\) are required beyond continuity.\nWe next present an asymptotic distribution result. The following shows that the kernel regression estimators are asymptotically normal with a nonparametric rate of convergence, a non-trivial asymptotic bias, and a non-degenerate asymptotic variance. Theorem 19.9 Suppose Assumption \\(19.1\\) holds. Assume in addition that \\(m^{\\prime \\prime}(x)\\) and \\(f^{\\prime}(x)\\) are continuous in \\(\\mathscr{N}\\), that for some \\(r>2\\) and \\(x \\in \\mathscr{N}\\),\n\\[\n\\mathbb{E}\\left[|e|^{r} \\mid X=x\\right] \\leq \\bar{\\sigma}<\\infty,\n\\]\nand\n\\[\nn h^{5}=O(1) .\n\\]\nThen\n\\[\n\\sqrt{n h}\\left(\\widehat{m}_{\\mathrm{nw}}(x)-m(x)-h^{2} B_{\\mathrm{nw}}(x)\\right) \\underset{d}{\\longrightarrow} \\mathrm{N}\\left(0, \\frac{R_{K} \\sigma^{2}(x)}{f(x)}\\right) .\n\\]\nSimilarly,\n\\[\n\\sqrt{n h}\\left(\\widehat{m}_{\\mathrm{LL}}(x)-m(x)-h^{2} B_{\\mathrm{LL}}(x)\\right) \\underset{d}{\\longrightarrow} \\mathrm{N}\\left(0, \\frac{R_{K} \\sigma^{2}(x)}{f(x)}\\right) .\n\\]\nA proof for the Nadaraya-Watson estimator appears in Section 19.26. For the local linear estimator see Fan and Gijbels (1996).\nRelative to Theorem 19.8, Theorem \\(19.9\\) requires stronger smoothness conditions on the conditional mean and marginal density. There are also two technical regularity conditions. The first is a conditional moment bound (19.14) (which is used to verify the Lindeberg condition for the CLT) and the second is the bandwidth bound \\(n h^{5}=O(1)\\). The latter means that the bandwidth must decline to zero at least at the rate \\(n^{-1 / 5}\\) and is used \\({ }^{6}\\) to ensure that higher-order bias terms do not enter the asymptotic distribution (19.16).\nThere are several interesting features about the asymptotic distribution which are noticeably different than for parametric estimators. First, the estimators converge at the rate \\(\\sqrt{n h}\\) not \\(\\sqrt{n}\\). Since \\(h \\rightarrow 0\\), \\(\\sqrt{n h}\\) diverges slower than \\(\\sqrt{n}\\), thus the nonparametric estimators converge more slowly than a parametric estimator. Second, the asymptotic distribution contains a non-negligible bias term \\(h^{2} B(x)\\). Third, the distribution (19.16) is identical in form to that for the kernel density estimator (Theorem \\(17.7\\) of Probability and Statistics for Economists).\nThe fact that the estimators converge at the rate \\(\\sqrt{n h}\\) has led to the interpretation of \\(n h\\) as the “effective sample size”. This is because the number of observations being used to construct \\(\\widehat{m}(x)\\) is proportional to \\(n h\\), not \\(n\\) as for a parametric estimator.\nIt is helpful to understand that the nonparametric estimator has a reduced convergence rate relative to parametric asymptotic theory because the object being estimated - \\(m(x)-\\) is nonparametric. This is harder than estimating a finite dimensional parameter, and thus comes at a cost.\nUnlike parametric estimation the asymptotic distribution of the nonparametric estimator includes a term representing the bias of the estimator. The asymptotic distribution (19.16) shows the form of this bias. It is proportional to the squared bandwidth \\(h^{2}\\) (the degree of smoothing) and to the function \\(B_{\\mathrm{nw}}(x)\\) or \\(B_{\\mathrm{LL}}(x)\\) which depends on the slope and curvature of the CEF \\(m(x)\\). Interestingly, when \\(m(x)\\) is constant then \\(B_{\\mathrm{nw}}(x)=B_{\\mathrm{LL}}(x)=0\\) and the kernel estimator has no asymptotic bias. The bias is essentially increasing in the curvature of the CEF function \\(m(x)\\). This is because the local averaging smooths \\(m(x)\\), and the smoothing induces more bias when \\(m(x)\\) is curved. Since the bias terms are multiplied by \\(h^{2}\\)\n\\({ }^{6}\\) This could be weakened if stronger smoothness conditions are assumed. For example, if \\(m^{(4)}(x)\\) and \\(f^{(3)}(x)\\) are continuous then (19.15) can be weakened to \\(n h^{9}=O(1)\\), which means that the bandwidth must decline to zero at least at the rate \\(n^{-1 / 9}\\). which tends to zero it might be thought that the bias terms are asymptotically negligible and can be omitted, but this is mistaken because they are within the parentheses which are mutiplied by the factor \\(\\sqrt{n h}\\). The bias terms can only be omitted if \\(\\sqrt{n h} h^{2} \\rightarrow 0\\), which is known as an undersmoothing condition and is discussed in the next section.\nThe asymptotic variance of \\(\\widehat{m}(x)\\) is inversely proportional to the marginal density \\(f(x)\\). This means that \\(\\widehat{m}(x)\\) has relatively low precision for regions where \\(X\\) has a low density. This makes sense because these are regions where there are relatively few observations. An implication is that the nonparametric estimator \\(\\widehat{m}(x)\\) will be relatively inaccurate in the tails of the distribution of \\(X\\)."
  },
  {
    "objectID": "chpt19-nonparameter.html#undersmoothing",
    "href": "chpt19-nonparameter.html#undersmoothing",
    "title": "18  Nonparametric Regression",
    "section": "18.14 Undersmoothing",
    "text": "18.14 Undersmoothing\nThe bias term in the asymptotic distribution of the kernel density estimator can be technically eliminated if the bandwidth is selected to converge to zero faster than the optimal rate \\(n^{-1 / 5}\\), thus \\(h=\\) \\(o\\left(n^{-1 / 5}\\right)\\). This is called an under-smoothing bandwidth. By using a small bandwidth the bias is reduced and the variance is increased. Thus the random component dominates the bias component (asymptotically). The following is the technical statement.\nTheorem 19.10 Under the conditions of Theorem 19.9, and \\(n h^{5}=o(1)\\),\n\\[\n\\begin{aligned}\n&\\sqrt{n h}\\left(\\widehat{m}_{\\mathrm{nw}}(x)-m(x)\\right) \\underset{d}{\\longrightarrow} \\mathrm{N}\\left(0, \\frac{R_{K} \\sigma^{2}(x)}{f(x)}\\right) \\\\\n&\\sqrt{n h}\\left(\\widehat{m}_{\\mathrm{LL}}(x)-m(x)\\right) \\underset{d}{\\longrightarrow} \\mathrm{N}\\left(0, \\frac{R_{K} \\sigma^{2}(x)}{f(x)}\\right) .\n\\end{aligned}\n\\]\nTheorem \\(19.10\\) has the advantage of no bias term. Consequently this theorem is popular with some authors. There are also several disadvantages. First, the assumption of an undersmoothing bandwidth does not really eliminate the bias, it simply assumes it away. Thus in any finite sample there is always bias. Second, it is not clear how to set a bandwidth so that it is undersmoothing. Third, a undersmoothing bandwidth implies that the estimator has increased variance and is inefficient. Finally, the theory is simply misleading as a characterization of the distribution of the estimator."
  },
  {
    "objectID": "chpt19-nonparameter.html#conditional-variance-estimation",
    "href": "chpt19-nonparameter.html#conditional-variance-estimation",
    "title": "18  Nonparametric Regression",
    "section": "18.15 Conditional Variance Estimation",
    "text": "18.15 Conditional Variance Estimation\nThe conditional variance is\n\\[\n\\sigma^{2}(x)=\\operatorname{var}[Y \\mid X=x]=\\mathbb{E}\\left[e^{2} \\mid X=x\\right] .\n\\]\nThere are a number of contexts where it is desirable to estimate \\(\\sigma^{2}(x)\\) including prediction intervals and confidence intervals for the estimated CEF. In general the conditional variance function is nonparametric as economic models rarely specify the form of \\(\\sigma^{2}(x)\\). Thus estimation of \\(\\sigma^{2}(x)\\) is typically done nonparametrically. Since \\(\\sigma^{2}(x)\\) is the CEF of \\(e^{2}\\) given \\(X\\) it can be estimated by nonparametric regression. For example, the ideal NW estimator (if \\(e\\) were observed) is\n\\[\n\\bar{\\sigma}^{2}(x)=\\frac{\\sum_{i=1}^{n} K\\left(\\frac{X_{i}-x}{h}\\right) e_{i}^{2}}{\\sum_{i=1}^{n} K\\left(\\frac{X_{i}-x}{h}\\right)} .\n\\]\nSince the errors \\(e\\) are not observed, we need to replace them with an estimator. A simple choice are the residuals \\(\\widehat{e}_{i}=Y_{i}-\\widehat{m}\\left(X_{i}\\right)\\). A better choice are the leave-one-out prediction errors \\(\\widetilde{e}_{i}=Y_{i}-\\widehat{m}_{-i}\\left(X_{i}\\right)\\). The latter are recommended for variance estimation as they are not subject to overfitting. With this substitution the NW estimator of the conditional variance is\n\\[\n\\widehat{\\sigma}^{2}(x)=\\frac{\\sum_{i=1}^{n} K\\left(\\frac{X_{i}-x}{h}\\right) \\widetilde{e}_{i}^{2}}{\\sum_{i=1}^{n} K\\left(\\frac{X_{i}-x}{h}\\right)} .\n\\]\nThis estimator depends on a bandwidth \\(h\\) but there is no reason for this bandwidth to be the same as that used to estimate the CEF. The ROT or cross-validation using \\(\\widetilde{e}_{i}^{2}\\) as the dependent variable can be used to select the bandwidth for estimation of \\(\\widehat{\\sigma}^{2}(x)\\) separately from the choice for estimation of \\(\\widehat{m}(x)\\).\nThere is one subtle difference between CEF and conditional variance estimation. The conditional variance is inherently non-negative \\(\\sigma^{2}(x) \\geq 0\\) and it is desirable for the estimator to satisfy this property. The NW estimator (19.17) is necessarily non-negative because it is a smoothed average of the nonnegative squared residuals. The LL estimator, however, is not guaranteed to be non-negative for all \\(x\\). Furthermore, the NW estimator has as a special case the homoskedastic estimator \\(\\widehat{\\sigma}^{2}(x)=\\widehat{\\sigma}^{2}\\) (full sample variance) which may be a relevant selection. For these reasons, the NW estimator may be preferred for conditional variance estimation.\nFan and Yao (1998) derive the asymptotic distribution of the estimator (19.17). They obtain the surprising result that the asymptotic distribution of the two-step estimator \\(\\widehat{\\sigma}^{2}(x)\\) is identical to that of the one-step idealized estimator \\(\\bar{\\sigma}^{2}(x)\\)."
  },
  {
    "objectID": "chpt19-nonparameter.html#variance-estimation-and-standard-errors",
    "href": "chpt19-nonparameter.html#variance-estimation-and-standard-errors",
    "title": "18  Nonparametric Regression",
    "section": "18.16 Variance Estimation and Standard Errors",
    "text": "18.16 Variance Estimation and Standard Errors\nIt is relatively straightforward to calculate the exact conditional variance of the Nadaraya-Watson, local linear, or local polynomial estimator. The estimators can be written as\n\\[\n\\widehat{\\beta}(x)=\\left(Z^{\\prime} \\boldsymbol{K} \\boldsymbol{Z}\\right)^{-1}\\left(\\boldsymbol{Z}^{\\prime} \\boldsymbol{K} \\boldsymbol{Y}\\right)=\\left(\\boldsymbol{Z}^{\\prime} \\boldsymbol{K} \\boldsymbol{Z}\\right)^{-1}\\left(\\boldsymbol{Z}^{\\prime} \\boldsymbol{K} \\boldsymbol{m}\\right)+\\left(\\boldsymbol{Z}^{\\prime} \\boldsymbol{K} \\boldsymbol{Z}\\right)^{-1}\\left(\\boldsymbol{Z}^{\\prime} \\boldsymbol{K} \\boldsymbol{e}\\right)\n\\]\nwhere \\(m\\) is the \\(n \\times 1\\) vector of means \\(m\\left(X_{i}\\right)\\). The first component is a function only of the regressors and the second is linear in the error \\(\\boldsymbol{e}\\). Thus conditionally on the regressors \\(\\boldsymbol{X}\\),\n\\[\n\\boldsymbol{V}_{\\widehat{\\beta}}(x)=\\operatorname{var}[\\widehat{\\beta} \\mid \\boldsymbol{X}]=\\left(\\boldsymbol{Z}^{\\prime} \\boldsymbol{K} \\boldsymbol{Z}\\right)^{-1}\\left(\\boldsymbol{Z}^{\\prime} \\boldsymbol{K} \\boldsymbol{D} \\boldsymbol{K} \\boldsymbol{Z}\\right)\\left(\\boldsymbol{Z}^{\\prime} \\boldsymbol{K} \\boldsymbol{Z}\\right)^{-1}\n\\]\nwhere \\(\\boldsymbol{D}=\\operatorname{diag}\\left(\\sigma^{2}\\left(X_{1}\\right), \\ldots \\sigma^{2}\\left(X_{n}\\right)\\right)\\)\nA White-type estimator can be formed by replacing \\(\\sigma^{2}\\left(X_{i}\\right)\\) with the squared residuals \\(\\widehat{e}_{i}^{2}\\) or prediction errors \\(\\widetilde{e}_{i}^{2}\\)\n\\[\n\\widehat{\\boldsymbol{V}}_{\\widehat{\\beta}}(x)=\\left(\\boldsymbol{Z}^{\\prime} \\boldsymbol{K} \\boldsymbol{Z}\\right)^{-1}\\left(\\sum_{i=1}^{n} K\\left(\\frac{X_{i}-x}{h}\\right)^{2} Z_{i}(x) Z_{i}(x)^{\\prime} \\widetilde{e}_{i}^{2}\\right)\\left(\\boldsymbol{Z}^{\\prime} \\boldsymbol{K} \\boldsymbol{Z}\\right)^{-1} .\n\\]\nAlternatively, \\(\\sigma^{2}\\left(X_{i}\\right)\\) could be replaced with an estimator such as (19.17) evaluated at \\(\\widehat{\\sigma}^{2}\\left(X_{i}\\right)\\) or \\(\\widehat{\\sigma}^{2}(x)\\).\nA simple option is the asymptotic formula\n\\[\n\\widehat{V}_{\\widehat{m}(x)}=\\frac{R_{K} \\widehat{\\sigma}^{2}(x)}{n h \\widehat{f}(x)}\n\\]\nwith \\(\\widehat{\\sigma}^{2}(x)\\) from (19.17) and \\(\\widehat{f}(x)\\) a density estimator such as\n\\[\n\\widehat{f}(x)=\\frac{1}{n b} \\sum_{i=1}^{n} K\\left(\\frac{X_{i}-x}{b}\\right)\n\\]\nwhere \\(b\\) is a bandwidth. (See Chapter 17 of Probability and Statistics for Economists.)\nIn general we recommend (19.18) calculated with prediction errors as this is the closest analog of the finite sample covariance matrix.\nFor local linear and local polynomial estimators the estimator \\(\\widehat{V}_{\\widehat{m}(x)}\\) is the first diagonal element of the matrix \\(\\widehat{\\boldsymbol{V}}_{\\widehat{\\beta}}(x)\\). For any of the variance estimators a standard error for \\(\\widehat{m}(x)\\) is the square root of \\(\\widehat{V}_{\\widehat{m}(x)}\\)."
  },
  {
    "objectID": "chpt19-nonparameter.html#confidence-bands",
    "href": "chpt19-nonparameter.html#confidence-bands",
    "title": "18  Nonparametric Regression",
    "section": "18.17 Confidence Bands",
    "text": "18.17 Confidence Bands\nWe can construct asymptotic confidence intervals. An 95% interval for \\(m(x)\\) is\n\\[\n\\widehat{m}(x) \\pm 1.96 \\sqrt{\\widehat{V}_{\\widehat{m}(x)}} .\n\\]\nThis confidence interval can be plotted along with \\(\\widehat{m}(x)\\) to assess precision.\nIt should be noted, however, that this confidence interval has two unusual properties. First, it is pointwise in \\(x\\), meaning that it is designed to have coverage probability at each \\(x\\) not uniformly across \\(x\\). Thus they are typically called pointwise confidence intervals.\nSecond, because it does not account for the bias it is not an asymptotically valid confidence interval for \\(m(x)\\). Rather, it is an asymptotically valid confidence interval for the pseudo-true (smoothed) value, e.g. \\(m(x)+h^{2} B(x)\\). One way of thinking about this is that the confidence intervals account for the variance of the estimator but not its bias. A technical trick which solves this problem is to assume an undersmoothing bandwidth. In this case the above confidence intervals are technically asymptotically valid. This is only a technical trick as it does not really eliminate the bias only assumes it away. The plain fact is that once we honestly acknowledge that the true CEF is nonparametric it then follows that any finite sample estimator will have finite sample bias and this bias will be inherently unknown and thus difficult to incorporate into confidence intervals.\nDespite these unusual properties we can still use the interval (19.20) to display uncertainty and as a check on the precision of the estimates."
  },
  {
    "objectID": "chpt19-nonparameter.html#the-local-nature-of-kernel-regression",
    "href": "chpt19-nonparameter.html#the-local-nature-of-kernel-regression",
    "title": "18  Nonparametric Regression",
    "section": "18.18 The Local Nature of Kernel Regression",
    "text": "18.18 The Local Nature of Kernel Regression\nThe kernel regression estimators (Nadaraya-Watson, Local Linear, and Local Polynomial) are all essentially local estimators in that given \\(h\\) the estimator \\(\\widehat{m}(x)\\) is a function only of the sub-sample for which \\(X\\) is close to \\(x\\). The other observations do not directly affect the estimator. This is reflected in the distribution theory as well. Theorem \\(19.8\\) shows that \\(\\widehat{m}(x)\\) is consistent for \\(m(x)\\) if the latter is continuous at \\(x\\). Theorem \\(19.9\\) shows that the asymptotic distribution of \\(\\widehat{m}(x)\\) depends only on the functions \\(m(x)\\), \\(f(x)\\) and \\(\\sigma^{2}(x)\\) at the point \\(x\\). The distribution does not depend on the global behavior of \\(m(x)\\). Global features do affect the estimator \\(\\widehat{m}(x)\\), however, through the bandwidth \\(h\\). The bandwidth selection methods described here are global in nature as they attempt to minimize AIMSE. Local bandwidths (designed to minimize the AMSE at a single point \\(x\\) ) can alternatively be employed but these are less commonly used, in part because such bandwidth estimators have high imprecision. Picking local bandwidths adds extra noise.\nFurthermore, selected bandwidths may be meaningfully large so that the estimation window may be a large portion of the sample. In this case estimation is neither local nor fully global."
  },
  {
    "objectID": "chpt19-nonparameter.html#application-to-wage-regression",
    "href": "chpt19-nonparameter.html#application-to-wage-regression",
    "title": "18  Nonparametric Regression",
    "section": "18.19 Application to Wage Regression",
    "text": "18.19 Application to Wage Regression\nWe illustrate the methods with an application to the the CPS data set. We are interested in the nonparametric regression of \\(\\log\\) (wage) on experience. To illustrate we take the subsample of Black men with 12 years of education (high school graduates). This sample has 762 observations.\nWe first need to decide on the region of interest (range of experience) for which we will calculate the regression estimator. We select the range \\([0,40]\\) because most observations (90%) have experience levels below 40 years.\nTo avoid boundary bias we use the local linear estimator.\nWe next calculate the Fan-Gijbels rule-of-thumb bandwidth (19.9) and find \\(h_{\\text {rot }}=5.14\\). We then calculate the cross-validation criterion using the rule-of-thumb as a baseline. The CV criterion is displayed in Figure 19.5(a). The minimizer is \\(h_{\\mathrm{cv}}=4.32\\) which is somewhat smaller than the ROT bandwidth.\nWe calculate the local linear estimator using both bandwidths and display the estimates in Figure 19.5(b). The regression functions are increasing for experience levels up to 20 years and then become flat. While the functions are roughly concave they are noticably different than a traditional quadratic specification. Comparing the estimates, the smaller CV-selected bandwidth produces a regression estimate which is a bit too wavy while the ROT bandwidth produces a regression estimate which is much smoother yet captures the same essential features. Based on this inspection we select the estimate based on the ROT bandwidth (the solid line in panel (b)).\nWe next consider estimation of the conditional variance function. We calculate the ROT bandwidth for a regression using the squared prediction errors and find \\(h_{\\mathrm{rot}}=6.77\\) which is larger than the bandwidth used for conditional mean estimation. We next calculate the cross-validation functions for conditional variance estimation (regression of squared prediction errors on experience) using both NW and LL regression. The CV functions are displayed in Figure 19.6(a). The CV plots are quite interesting. For the LL estimator the CV function has a local minimum around \\(h=5\\) but the global minimizer is unbounded. The CV function for the NW estimator is globally decreasing with an unbounded minimizer. The NW also achieves a considerably lower CV value than the LL estimator. This means that the CV-selected variance estimator is the NW estimator with \\(h=\\infty\\), which is the simple full-sample estimator \\(\\widehat{\\sigma}^{2}\\) calculated with the prediction errors.\nWe next compute standard errors for the regression function estimates using formula (19.18). In Figure 19.6(b) we display the estimated regression (the same as Figure \\(19.5\\) using the ROT bandwidth) along with \\(95 %\\) asymptotic confidence bands computed as in (19.20). By displaying the confidence bands we can see that there is considerable imprecision in the estimator for low experience levels. We can still see that the estimates and confidence bands show that the experience profile is increasing up to about 20 years of experience and then flattens above 20 years. The estimates imply that for this population (Black men who are high school graduates) the average wage rises for the first 20 years of work experience (from 18 to 38 years of age) and then flattens with no further increases in average wages for the next 20 years of work experience (from 38 to 58 years of age).\n\n\nCross-Validation Criterion\n\n\n\nLocal Linear Regression\n\nFigure 19.5: Log Wage Regression on Experience"
  },
  {
    "objectID": "chpt19-nonparameter.html#clustered-observations",
    "href": "chpt19-nonparameter.html#clustered-observations",
    "title": "18  Nonparametric Regression",
    "section": "18.20 Clustered Observations",
    "text": "18.20 Clustered Observations\nClustered observations are \\(\\left(Y_{i g}, X_{i g}\\right)\\) for individuals \\(i=1, \\ldots, n_{g}\\) in cluster \\(g=1, \\ldots, G\\). The model is\n\\[\n\\begin{aligned}\nY_{i g} &=m\\left(X_{i g}\\right)+e_{i g} \\\\\n\\mathbb{E}\\left[e_{i g} \\mid \\boldsymbol{X}_{g}\\right] &=0\n\\end{aligned}\n\\]\nwhere \\(\\boldsymbol{X}_{g}\\) is the stacked \\(X_{i g}\\). The assumption is that the clusters are mutually independent. Dependence within each cluster is unstructured.\nWrite\n\\[\nZ_{i g}(x)=\\left(\\begin{array}{c}\n1 \\\\\nX_{i g}-x\n\\end{array}\\right) .\n\\]\nStack \\(Y_{i g}, e_{i g}\\) and \\(Z_{i g}(x)\\) into cluster-level variables \\(\\boldsymbol{Y}_{g}, \\boldsymbol{e}_{g}\\) and \\(Z_{g}(x)\\). Let \\(\\boldsymbol{K}_{g}(x)=\\operatorname{diag}\\left\\{K\\left(\\frac{X_{i g}-x}{h}\\right)\\right\\}\\). The local linear estimator can be written as\n\\[\n\\begin{aligned}\n\\widehat{\\beta}(x) &=\\left(\\sum_{g=1}^{G} \\sum_{i=1}^{n_{g}} K\\left(\\frac{X_{i g}-x}{h}\\right) Z_{i g}(x) Z_{i g}(x)^{\\prime}\\right)^{-1}\\left(\\sum_{g=1}^{G} \\sum_{i=1}^{n_{g}} K\\left(\\frac{X_{i g}-x}{h}\\right) Z_{i g}(x) Y_{i g}\\right) \\\\\n&=\\left(\\sum_{g=1}^{G} \\boldsymbol{Z}_{g}(x)^{\\prime} \\boldsymbol{K}_{g}(x) \\boldsymbol{Z}_{g}(x)\\right)^{-1}\\left(\\sum_{g=1}^{G} \\boldsymbol{Z}_{g}(x)^{\\prime} \\boldsymbol{K}_{g}(x) \\boldsymbol{Y}_{g}\\right) .\n\\end{aligned}\n\\]\nThe local linear estimator \\(\\widehat{m}(x)=\\widehat{\\beta}_{1}(x)\\) is the intercept in (19.21).\nThe natural method to obtain prediction errors is by delete-cluster regression. The delete-cluster estimator of \\(\\beta\\) is\n\\[\n\\widetilde{\\beta}_{(-g)}(x)=\\left(\\sum_{j \\neq g} \\boldsymbol{Z}_{j}(x)^{\\prime} \\boldsymbol{K}_{j}(x) \\boldsymbol{Z}_{j}(x)\\right)^{-1}\\left(\\sum_{j \\neq g} \\boldsymbol{Z}_{j}(x)^{\\prime} \\boldsymbol{K}_{j}(x) \\boldsymbol{Y}_{j}\\right) .\n\\]\n\n\nCross-Validation for Conditional Variance\n\n\n\nRegression with Confidence Bands\n\nFigure 19.6: Confidence Band Construction\nThe delete-cluster estimator of \\(m(x)\\) is the intercept \\(\\widetilde{m}_{1}(x)=\\widetilde{\\beta}_{1(-g)}(x)\\) from (19.22). The delete-cluster prediction error for observation \\(i g\\) is\n\\[\n\\widetilde{e}_{i g}=Y_{i g}-\\widetilde{\\beta}_{1(-g)}\\left(X_{i g}\\right) .\n\\]\nLet \\(\\widetilde{\\boldsymbol{e}}_{g}\\) be the stacked \\(\\widetilde{e}_{i g}\\) for cluster \\(g\\).\nThe variance of (19.21), conditional on the regressors \\(\\boldsymbol{X}\\), is\n\\[\n\\boldsymbol{V}_{\\widehat{\\beta}}(x)=\\left(\\sum_{g=1}^{G} \\boldsymbol{Z}_{g}(x)^{\\prime} \\boldsymbol{K}_{g}(x) \\boldsymbol{Z}_{g}(x)\\right)^{-1}\\left(\\sum_{g=1}^{G} \\boldsymbol{Z}_{g}(x)^{\\prime} \\boldsymbol{K}_{g}(x) \\boldsymbol{S}_{g}(x) \\boldsymbol{K}_{g}(x) \\boldsymbol{Z}_{g}(x)\\right)\\left(\\sum_{g=1}^{G} \\boldsymbol{Z}_{g}(x)^{\\prime} \\boldsymbol{K}_{g}(x) \\boldsymbol{Z}_{g}(x)\\right)^{-1}\n\\]\nwhere \\(\\boldsymbol{S}_{g}=\\mathbb{E}\\left[\\boldsymbol{e}_{g} \\boldsymbol{e}_{g}^{\\prime} \\mid \\boldsymbol{X}_{g}\\right]\\). The covariance matrix (19.24) can be estimated by replacing \\(\\boldsymbol{S}_{g}\\) with an estimator of \\(\\boldsymbol{e}_{g} \\boldsymbol{e}_{g}^{\\prime}\\). Based on analogy with regression estimation we suggest the delete-cluster prediction errors \\(\\widetilde{\\boldsymbol{e}}_{g}\\) as they are not subject to over-fitting. This covariance matrix estimator using this choice is\n\\[\n\\widehat{\\boldsymbol{V}}_{\\widehat{\\beta}}(x)=\\left(\\sum_{g=1}^{G} \\boldsymbol{Z}_{g}(x)^{\\prime} \\boldsymbol{K}_{g}(x) \\boldsymbol{Z}_{g}(x)\\right)^{-1}\\left(\\sum_{g=1}^{G} Z_{g}(x) \\boldsymbol{K}_{g}(x) \\widetilde{\\boldsymbol{e}}_{g} \\widetilde{\\boldsymbol{e}}_{g}^{\\prime} \\boldsymbol{K}_{g}(x) \\boldsymbol{Z}_{g}(x)\\right)\\left(\\sum_{g=1}^{G} \\boldsymbol{Z}_{g}(x) \\boldsymbol{K}_{g}(x) \\boldsymbol{Z}_{g}(x)\\right)^{-1} .\n\\]\nThe standard error for \\(\\widehat{m}(x)\\) is the square root of the first diagonal element of \\(\\widehat{\\boldsymbol{V}}_{\\widehat{\\beta}}(x)\\).\nThere is no current theory on how to select the bandwidth \\(h\\) for nonparametric regression using clustered observations. The Fan-Ghybels ROT bandwidth \\(h_{\\text {rot }}\\) is designed for independent observations so is likely to be a crude choice in the case of clustered observations. Standard cross-validation has similar limitations. A practical alternative is to select the bandwidth \\(h\\) to minimize a delete-cluster crossvaliation criterion. While there is no formal theory to justify this choice, it seems like a reasonable option. The delete-cluster CV criterion is\n\\[\n\\mathrm{CV}(h)=\\frac{1}{n} \\sum_{g=1}^{G} \\sum_{i=1}^{n_{g}} \\widetilde{e}_{i g}^{2}\n\\]\nwhere \\(\\widetilde{e}_{i g}\\) are the delete-cluster prediction errors (19.23). The delete-cluster CV bandwidth is the value which minimizes this function:\n\\[\nh_{\\mathrm{cV}}=\\underset{h \\geq h_{\\ell}}{\\operatorname{argmin}} \\mathrm{CV}(h) .\n\\]\nAs for the case of conventional cross-validation, it may be valuable to plot \\(\\mathrm{CV}(h)\\) against \\(h\\) to verify that the minimum has been obtained and to assess sensitivity."
  },
  {
    "objectID": "chpt19-nonparameter.html#application-to-testscores",
    "href": "chpt19-nonparameter.html#application-to-testscores",
    "title": "18  Nonparametric Regression",
    "section": "18.21 Application to Testscores",
    "text": "18.21 Application to Testscores\nWe illustrate kernel regression with clustered observations by using the Duflo, Dupas, and Kremer (2011) investigation of the effect of student tracking on testscores. Recall that the core question was effect of the dummy variable tracking on the continuous variable testscore. A set of controls were included including a continuous variable percentile which recorded the student’s initial test score (as a percentile). We investigate the authors’ specification of this control using local linear regression.\nWe took the subsample of 1487 girls who experienced tracking and estimated the regression of testscores on percentile. For this application we used unstandardized \\({ }^{7}\\) test scores which range from 0 to about 40 . We used local linear regression with a Gaussian kernel.\nFirst consider bandwidth selection. The Fan-Ghybels ROT and conventional cross-validation bandwidths are \\(h_{\\mathrm{rot}}=6.7\\) and \\(h_{\\mathrm{cv}}=12.3\\). We then calculated the clustered cross-validation criterion which has minimizer \\(h_{\\mathrm{cv}}=6.2\\). To understand the differences we plot the standard and clustered cross-validation functions in Figure 19.7(a). In order to plot on the same graph we normalize each by subtracting their minimized value (so each is minimized at zero). What we can see from Figure 19.7(a) is that while the conventional CV criterion is sharply minimized at \\(h=12.3\\), the clustered CV criterion is essentially flat between 5 and 11. This means that the clustered CV criterion has difficulty discriminating between these bandwidth choices.\nUsing the bandwidth selected by clustered cross-validation, we calculate the local linear estimator \\(\\hat{m}_{\\mathrm{LL}}(x)\\) of the regression function. The estimate is plotted in Figure 19.7(b). We calculate the deletecluster prediction errors \\(\\widetilde{\\boldsymbol{e}}_{g}\\) and use these to calculate the standard errors for the local linear estimator \\(\\widehat{m}_{\\mathrm{LL}}(x)\\) using formula (19.25). (These standard errors are roughly twice as large as those calculated using the non-clustered formula.) We use the standard errors to calculate \\(95 %\\) asymptotic pointwise confidence bands as in (19.20). These are plotted in Figure 19.7(b) along with the point estimate. Also plotted for comparison is an estimated linear regression line. The local linear estimator is similar to the global linear regression for initial percentiles below \\(80 %\\). But for initial percentiles above \\(80 %\\) the two lines diverge. The confidence bands suggest that these differences are statistically meaningful. Students with initial testscores at the top of the initial distribution have higher final testscores on average than predicted by a linear specification."
  },
  {
    "objectID": "chpt19-nonparameter.html#multiple-regressors",
    "href": "chpt19-nonparameter.html#multiple-regressors",
    "title": "18  Nonparametric Regression",
    "section": "18.22 Multiple Regressors",
    "text": "18.22 Multiple Regressors\nOur analysis has focused on the case of real-valued \\(X\\) for simplicity, but the methods of kernel regression extend to the multiple regressor case at the cost of a reduced rate of convergence. In this section we\n\\({ }^{7}\\) In Section 4.21, following Duflo, Dupas and Kremer (2011) the dependent variable was standardized testscores (normalized to have mean zero and variance one).\n\n\nCross-Validation Criterion\n\n\n\nLocal Linear Regression\n\nFigure 19.7: Testscore as a Function of Initial Percentile\nconsider the case of estimation of the conditional expectation function \\(\\mathbb{E}[Y \\mid X=x]=m(x)\\) where\n\\[\nX=\\left(\\begin{array}{c}\nX_{1} \\\\\n\\vdots \\\\\nX_{d}\n\\end{array}\\right) \\in \\mathbb{R}^{d} .\n\\]\nFor any evaluation point \\(x\\) and observation \\(i\\) define the kernel weights\n\\[\nK_{i}(x)=K\\left(\\frac{X_{1 i}-x_{1}}{h_{1}}\\right) K\\left(\\frac{X_{2 i}-x_{2}}{h_{2}}\\right) \\cdots K\\left(\\frac{X_{d i}-x_{d}}{h_{d}}\\right),\n\\]\na \\(d\\)-fold product kernel. The kernel weights \\(K_{i}(x)\\) assess if the regressor vector \\(X_{i}\\) is close to the evaluation point \\(x\\) in the Euclidean space \\(\\mathbb{R}^{d}\\).\nThese weights depend on a set of \\(d\\) bandwidths, \\(h_{j}\\), one for each regressor. Given these weights, the Nadaraya-Watson estimator takes the form\n\\[\n\\widehat{m}(x)=\\frac{\\sum_{i=1}^{n} K_{i}(x) Y_{i}}{\\sum_{i=1}^{n} K_{i}(x)} .\n\\]\nFor the local-linear estimator, define\n\\[\nZ_{i}(x)=\\left(\\begin{array}{c}\n1 \\\\\nX_{i}-x\n\\end{array}\\right)\n\\]\nand then the local-linear estimator can be written as \\(\\widehat{m}(x)=\\widehat{\\alpha}(x)\\) where\n\\[\n\\begin{aligned}\n\\left(\\begin{array}{c}\n\\widehat{\\alpha}(x) \\\\\n\\widehat{\\beta}(x)\n\\end{array}\\right) &=\\left(\\sum_{i=1}^{n} K_{i}(x) Z_{i}(x) Z_{i}(x)^{\\prime}\\right)^{-1} \\sum_{i=1}^{n} K_{i}(x) Z_{i}(x) Y_{i} \\\\\n&=\\left(\\boldsymbol{Z}^{\\prime} \\boldsymbol{K} \\boldsymbol{Z}\\right)^{-1} \\boldsymbol{Z}^{\\prime} \\boldsymbol{K} \\boldsymbol{Y}\n\\end{aligned}\n\\]\nwhere \\(K=\\operatorname{diag}\\left\\{K_{1}(x), \\ldots, K_{n}(x)\\right\\}\\).\nIn multiple regressor kernel regression cross-validation remains a recommended method for bandwidth selection. The leave-one-out residuals \\(\\widetilde{e}_{i}\\) and cross-validation criterion \\(\\mathrm{CV}\\left(h_{1}, \\ldots, h_{d}\\right)\\) are defined identically as in the single regressor case. The only difference is that now the CV criterion is a function over the \\(d\\) bandwidths \\(h_{1}, \\ldots, h_{d}\\). This means that numerical minimization needs to be done more efficiently than by a simple grid search.\nThe asymptotic distribution of the estimators in the multiple regressor case is an extension of the single regressor case. Let \\(f(x)\\) denote the marginal density of \\(X, \\sigma^{2}(x)=\\mathbb{E}\\left[e^{2} \\mid X=x\\right]\\) denote the conditional variance of \\(e=Y-m(X)\\), and set \\(|h|=h_{1} h_{2} \\cdots h_{d}\\).\nProposition 19.1 Let \\(\\hat{m}(x)\\) denote either the Nadarya-Watson or Local Linear estimator of \\(m(x)\\). As \\(n \\rightarrow \\infty\\) and \\(h_{j} \\rightarrow 0\\) such that \\(n|h| \\rightarrow \\infty\\),\n\\[\n\\sqrt{n|h|}\\left(\\widehat{m}(x)-m(x)-\\sum_{j=1}^{d} h_{j}^{2} B_{j}(x)\\right) \\underset{d}{\\longrightarrow} \\mathrm{N}\\left(0, \\frac{R_{K}^{d} \\sigma^{2}(x)}{f(x)}\\right) .\n\\]\nFor the Nadaraya-Watson estimator\n\\[\nB_{j}(x)=\\frac{1}{2} \\frac{\\partial^{2}}{\\partial x_{j}^{2}} m(x)+f(x)^{-1} \\frac{\\partial}{\\partial x_{j}} f(x) \\frac{\\partial}{\\partial x_{j}} m(x)\n\\]\nand for the Local Linear estimator\n\\[\nB_{j}(x)=\\frac{1}{2} \\frac{\\partial^{2}}{\\partial x_{j}^{2}} m(x) .\n\\]\nWe do not provide regularity conditions or a formal proof but instead refer interested readers to Fan and Gijbels (1996)."
  },
  {
    "objectID": "chpt19-nonparameter.html#curse-of-dimensionality",
    "href": "chpt19-nonparameter.html#curse-of-dimensionality",
    "title": "18  Nonparametric Regression",
    "section": "18.23 Curse of Dimensionality",
    "text": "18.23 Curse of Dimensionality\nThe term “curse of dimensionality” is used to describe the phenomenon that the convergence rate of nonparametric estimators slows as the dimension increases.\nWhen \\(X\\) is vector-valued we define the AIMSE as the integral of the squared bias plus variance, integrating with respect to \\(f(x) w(x)\\) where \\(w(x)\\) is an integrable weight function. For notational simplicity consider the case that there is a single common bandwidth \\(h\\). In this case the AIMSE of \\(\\widehat{m}(x)\\) equals\n\\[\n\\text { AIMSE }=h^{4} \\int_{S}\\left(\\sum_{j=1}^{d} B_{j}(x)\\right)^{2} f(x) w(x) d x+\\frac{R_{K}^{d}}{n h^{d}} \\int_{S} \\sigma^{2}(x) w(x) d x\n\\]\nWe see that the squared bias is of order \\(h^{4}\\), the same as in the single regressor case. The variance, however, is of larger order \\(\\left(n h^{d}\\right)^{-1}\\).\nIf pick the bandwith to minimizing the AIMSE we find that it equals \\(h=\\mathrm{cn}^{-1 /(4+d)}\\) for some constant c. This generalizes the formula for the one-dimensional case. The rate \\(n^{-1 /(4+d)}\\) is slower than the \\(n^{-1 / 5}\\) rate. This effectively means that with multiple regressors a larger bandwidth is required. When the bandwidth is set as \\(h=c n^{-1 /(4+d)}\\) then the AIMSE is of order \\(O\\left(n^{-4 /(4+d)}\\right)\\). This is a slower rate of convergence than in the one-dimensional case.\nTheorem 19.11 For vector-valued \\(X\\) the bandwidth which minimizes the AIMSE is of order \\(h \\sim n^{-1 /(4+d)}\\). With \\(h \\sim n^{-1 /(4+d)}\\) then AIMSE \\(=O\\left(n^{-4 /(4+d)}\\right)\\).\nSee Exercise 19.6.\nWe see that the optimal AIMSE rate \\(O\\left(n^{-4 /(4+d)}\\right)\\) depends on the dimension \\(d\\). As \\(d\\) increases this rate slows. Thus the precision of kernel regression estimators worsens with multiple regressors. The reason is the estimator \\(\\widehat{m}(x)\\) is a local average of \\(Y\\) for observations such that \\(X\\) is close to \\(x\\), and when there are multiple regressors the number of such observations is inherently smaller.\nThis phenomenon - that the rate of convergence of nonparametric estimation decreases as the dimension increases - is called the curse of dimensionality. It is common across most nonparametric estimation problems and is not specific to kernel regression.\nThe curse of dimensionality has led to the practical rule that most applications of nonparametric regression have a single regressor. Some have two regressors; on occassion, three. More is uncommon."
  },
  {
    "objectID": "chpt19-nonparameter.html#partially-linear-regression",
    "href": "chpt19-nonparameter.html#partially-linear-regression",
    "title": "18  Nonparametric Regression",
    "section": "18.24 Partially Linear Regression",
    "text": "18.24 Partially Linear Regression\nTo handle discrete regressors and/or reduce the dimensionality we can separate the regression function into a nonparametric and a parametric part. Let the regressors be partitioned as \\((X, Z)\\) where \\(X\\) and \\(Z\\) are \\(d\\) - and \\(k\\)-dimensional, respectively. A partially linear regression model is\n\\[\n\\begin{aligned}\nY &=m(X)+Z^{\\prime} \\beta+e \\\\\n\\mathbb{E}[e \\mid X, Z] &=0 .\n\\end{aligned}\n\\]\nThis model combines two elements. One, it specifies that the CEF is separable between \\(X\\) and \\(Z\\) (there are no nonparametric interactions). Two, it specifies that the CEF is linear in the regressors \\(Z\\). These are assumptions which may be true or may be false. In practice it is best to think of the assumptions as approximations.\nWhen some regressors are discrete (as is common in econometric applications) they belong in \\(Z\\). The regressors \\(X\\) must be continuously distributed. In typical applications \\(X\\) is either scalar or twodimensional. This may not be a restriction in practice as many econometric applications only have a small number of continuously distributed regressors.\nThe seminal contribution for estimation of (19.26) is Robinson (1988) who proposed a nonparmametric version of residual regression. His key insight was to see that the nonparametric component can be eliminated by transformation. Take the expectation of equation (19.26) conditional on \\(X\\). This is\n\\[\n\\mathbb{E}[Y \\mid X]=m(X)+\\mathbb{E}[Z \\mid X]^{\\prime} \\beta .\n\\]\nSubtract this from (19.26), obtaining\n\\[\nY-\\mathbb{E}[Y \\mid X]=(Z-\\mathbb{E}[Z \\mid X])^{\\prime} \\beta+e .\n\\]\nThe model is now a linear regression of the nonparametric regression error \\(Y-\\mathbb{E}[Y \\mid X]\\) on the vector of nonparametric regression errors \\(Z-\\mathbb{E}[Z \\mid X]\\).\nRobinson’s estimator replaces the infeasible regression errors by nonparametric counterparts. The result is a three-step estimator. 1. Using nonparametric regression (NW or LL), regress \\(Y_{i}\\) on \\(X_{i}, Z_{1 i}\\) on \\(X_{i}, Z_{2 i}\\) on \\(X_{i}, \\ldots\\), and \\(Z_{k i}\\) on \\(X_{i}\\), obtaining the fitted values \\(\\widehat{g}_{0 i}, \\widehat{g}_{1 i}, \\ldots\\), and \\(\\widehat{g}_{k i}\\).\n 1. Regress \\(Y_{i}-\\widehat{g}_{0 i}\\) on \\(Z_{1 i}-\\widehat{g}_{1 i}, \\ldots, Z_{k i}-\\widehat{g}_{k i}\\) to obtain the coefficient estimate \\(\\widehat{\\beta}\\) and standard errors.\n\nUse nonparametric regression to regress \\(Y_{i}-Z_{i}^{\\prime} \\widehat{\\beta}\\) on \\(X_{i}\\) to obtain the nonparametric estimator \\(\\widehat{m}(x)\\) and confidence intervals.\n\nThe resulting estimators and standard errors have conventional asymptotic distributions under specific assumptions on the bandwidths. A full proof is provided by Robinson (1988). Andrews (2004) provides a more general treatment with insight to the general structure of semiparametric estimators.\nThe most difficult challenge is to show that the asymptotic distribution \\(\\widehat{\\beta}\\) is unaffected by the first step estimation. Briefly, these are the steps of the argument. First, the first-step error \\(Z-\\mathbb{E}[Z \\mid X]\\) has zero covariance with the regression error \\(e\\). Second, the asymptotic distribution will be unaffected by the firststep estimation if replacing (in this covariance) the expectation \\(\\mathbb{E}[Z \\mid X]\\) with its first-step nonparametric estimator induces an error of order \\(o_{p}\\left(n^{-1 / 2}\\right)\\). Third, because the covariance is a product, this holds when the first-step estimator has a convergence rate of \\(o_{p}\\left(n^{-1 / 4}\\right)\\). Fourth, this holds under Theorem \\(19.11\\) if \\(h \\sim n^{-1 /(4+d)}\\) and \\(d<4\\).\nThe reason why the third step estimator has a conventional asymptotic distribution is a bit simpler to explain. The estimator \\(\\widehat{\\beta}\\) converges at a conventional \\(O_{p}\\left(n^{-1 / 2}\\right)\\) rate. The nonparametric estimator \\(\\widehat{m}(x)\\) converges at a rate slower than \\(O_{p}\\left(n^{-1 / 2}\\right)\\). Thus the sampling error for \\(\\widehat{\\beta}\\) is of lower order and does not affect the first-order asymptotic distribution of \\(\\widehat{m}(x)\\).\nOnce again, the theory is advanced so the above two paragraphs should not be taken as an explanation. The good news is that the estimation method is straightforward."
  },
  {
    "objectID": "chpt19-nonparameter.html#computation",
    "href": "chpt19-nonparameter.html#computation",
    "title": "18  Nonparametric Regression",
    "section": "18.25 Computation",
    "text": "18.25 Computation\nStata has two commands which implement kernel regression: lpoly and npregress. 1poly implements local polynomial estimation for any \\(p\\), including Nadaraya-Watson (the default) and local linear estimation, and selects the bandwidth using the Fan-Gijbels ROT method. It uses the Epanechnikov kernel by default but the Gaussian can be selected as an option. The l poly command automatically displays the estimated CEF along with 95% confidence bands with standard errors computed using (19.18).\nThe Stata command npregress estimates local linear (the default) or Nadaraya-Watson regression. By default it selects the bandwidth by cross-validation. It uses the Epanechnikov kernel by default but the Gaussian can be selected as an option. Confidence intervals may be calculated using the percentile bootstrap. A display of the estimated CEF and 95% confidence bands at specific points (computed using the percentile bootstrap) may be obtained with the postestimation command margins.\nThere are several R packages which implement kernel regression. One flexible choice is npreg available in the np package. Its default method is Nadaraya-Watson estimation using a Gaussian kernel with bandwidth selected by cross-validation. There are options which allow local linear and local polynomial estimation, alternative kernels, and alternative bandwidth selection methods."
  },
  {
    "objectID": "chpt19-nonparameter.html#technical-proofs",
    "href": "chpt19-nonparameter.html#technical-proofs",
    "title": "18  Nonparametric Regression",
    "section": "18.26 Technical Proofs*",
    "text": "18.26 Technical Proofs*\nFor all technical proofs we make the simplifying assumption that the kernel function \\(K(u)\\) has bounded support, thus \\(K(u)=0\\) for \\(|u|>a\\). The results extend to the Gaussian kernel but with addition technical arguments. Proof of Theorem 19.1.1. Equation (19.3) shows that\n\\[\n\\mathbb{E}\\left[\\widehat{m}_{\\mathrm{nW}}(x) \\mid \\boldsymbol{X}\\right]=m(x)+\\frac{\\widehat{b}(x)}{\\widehat{f}(x)}\n\\]\nwhere \\(\\widehat{f}(x)\\) is the kernel density estimator (19.19) of \\(f(x)\\) with \\(b=h\\) and\n\\[\n\\widehat{b}(x)=\\frac{1}{n h} \\sum_{i=1}^{n} K\\left(\\frac{X_{i}-x}{h}\\right)\\left(m\\left(x_{i}\\right)-m(x)\\right) .\n\\]\nTheorem \\(17.6\\) of Probability and Statistics for Economists established that \\(\\widehat{f}(x) \\underset{p}{\\longrightarrow} f(x)\\). The proof is completed by showing that \\(\\widehat{b}(x)=h^{2} f(x) B_{\\mathrm{nw}}(x)+o_{p}\\left(h^{2}+1 / \\sqrt{n h}\\right)\\).\nSince \\(\\widehat{b}(x)\\) is a sample average it has the expectation\n\\[\n\\begin{aligned}\n\\mathbb{E}[\\widehat{b}(x)] &=\\frac{1}{h} \\mathbb{E}\\left[K\\left(\\frac{X-x}{h}\\right)(m(X)-m(x))\\right] \\\\\n&=\\int_{-\\infty}^{\\infty} \\frac{1}{h} K\\left(\\frac{v-x}{h}\\right)(m(v)-m(x)) f(v) d v \\\\\n&=\\int_{-\\infty}^{\\infty} K(u)(m(x+h u)-m(x)) f(x+h u) d u .\n\\end{aligned}\n\\]\nThe second equality writes the expectation as an integral with respect to the density of \\(X\\). The third uses the change-of-variables \\(v=x+h u\\). We next use the two Taylor series expansions\n\\[\n\\begin{gathered}\nm(x+h u)-m(x)=m^{\\prime}(x) h u+\\frac{1}{2} m^{\\prime \\prime}(x) h^{2} u^{2}+o\\left(h^{2}\\right) \\\\\nf(x+h u)=f(x)+f^{\\prime}(x) h u+o(h)\n\\end{gathered}\n\\]\nInserted into (19.29) we find that (19.29) equals\n\\[\n\\begin{aligned}\n&\\int_{-\\infty}^{\\infty} K(u)\\left(m^{\\prime}(x) h u+\\frac{1}{2} m^{\\prime \\prime}(x) h^{2} u^{2}+o\\left(h^{2}\\right)\\right)\\left(f(x)+f^{\\prime}(x) h u+o(h)\\right) d u \\\\\n&=h\\left(\\int_{-\\infty}^{\\infty} u K(u) d u\\right) m^{\\prime}(x)(f(x)+o(h)) \\\\\n&+h^{2}\\left(\\int_{-\\infty}^{\\infty} u^{2} K(u) d u\\right)\\left(\\frac{1}{2} m^{\\prime \\prime}(x) f(x)+m^{\\prime}(x) f^{\\prime}(x)\\right) \\\\\n&+h^{3}\\left(\\int_{-\\infty}^{\\infty} u^{3} K(u) d u\\right) \\frac{1}{2} m^{\\prime \\prime}(x) f^{\\prime}(x)+o\\left(h^{2}\\right) \\\\\n&=h^{2}\\left(\\frac{1}{2} m^{\\prime \\prime}(x) f(x)+m^{\\prime}(x) f^{\\prime}(x)\\right)+o\\left(h^{2}\\right) \\\\\n&=h^{2} B_{\\mathrm{nw}}(x) f(x)+o\\left(h^{2}\\right) .\n\\end{aligned}\n\\]\nThe second equality uses the fact that the kernel \\(K(x)\\) integrates to one, its odd moments are zero, and the kernel variance is one. We have shown that \\(\\mathbb{E}[\\widehat{b}(x)]=B_{\\mathrm{nw}}(x) f(x) h^{2}+o\\left(h^{2}\\right)\\).\nNow consider the variance of \\(\\widehat{b}(x)\\). Since \\(\\widehat{b}(x)\\) is a sample average of independent components and the variance is smaller than the second moment\n\\[\n\\begin{aligned}\n\\operatorname{var}[\\widehat{b}(x)] &=\\frac{1}{n h^{2}} \\operatorname{var}\\left[K\\left(\\frac{X-x}{h}\\right)(m(X)-m(x))\\right] \\\\\n& \\leq \\frac{1}{n h^{2}} \\mathbb{E}\\left[K\\left(\\frac{X-x}{h}\\right)^{2}(m(X)-m(x))^{2}\\right] \\\\\n&=\\frac{1}{n h} \\int_{-\\infty}^{\\infty} K(u)^{2}(m(x+h u)-m(x))^{2} f(x+h u) d u \\\\\n&=\\frac{1}{n h} \\int_{-\\infty}^{\\infty} u^{2} K(u)^{2} d u\\left(m^{\\prime}(x)\\right)^{2} f(x)\\left(h^{2}+o(1)\\right) \\\\\n& \\leq \\frac{h}{n} \\bar{K}\\left(m^{\\prime}(x)\\right)^{2} f(x)+o\\left(\\frac{h}{n}\\right)\n\\end{aligned}\n\\]\nThe second equality writes the expectation as an integral. The third uses (19.30). The final inequality uses \\(K(u) \\leq \\bar{K}\\) from Definition 19.1.1 and the fact that the kernel variance is one. This shows that\n\\[\n\\operatorname{var}[\\widehat{b}(x)] \\leq O\\left(\\frac{h}{n}\\right) .\n\\]\nTogether we conclude that\n\\[\n\\widehat{b}(x)=h^{2} f(x) B_{\\mathrm{nw}}(x)+o\\left(h^{2}\\right)+O_{p}\\left(\\sqrt{\\frac{h}{n}}\\right)\n\\]\nand\n\\[\n\\frac{\\widehat{b}(x)}{\\widehat{f}(x)}=h^{2} B_{\\mathrm{nw}}(x)+o_{p}\\left(h^{2}\\right)+O_{p}\\left(\\sqrt{\\frac{h}{n}}\\right) .\n\\]\nTogether with (19.27) this implies Theorem 19.1.1.\nProof of Theorem 19.2.1. Equation (19.4) states that\n\\[\nn h \\operatorname{var}\\left[\\widehat{m}_{\\mathrm{nw}}(x) \\mid \\boldsymbol{X}\\right]=\\frac{\\widehat{v}(x)}{\\widehat{f}(x)^{2}}\n\\]\nwhere\n\\[\n\\widehat{v}(x)=\\frac{1}{n h} \\sum_{i=1}^{n} K\\left(\\frac{X_{i}-x}{h}\\right)^{2} \\sigma^{2}\\left(X_{i}\\right)\n\\]\nand \\(\\widehat{f}(x)\\) is the estimator (19.19) of \\(f(x)\\). Theorem \\(17.6\\) of Probability and Statistics for Economists established \\(\\widehat{f}(x) \\underset{p}{\\longrightarrow} f(x)\\). The proof is completed by showing \\(\\widehat{v}(x) \\underset{p}{\\rightarrow} R_{K} \\sigma^{2}(x) f(x)\\).\nFirst, writing the expectation as an integral with respect to \\(f(x)\\), making the change-of-variables \\(v=x+h u\\), and appealing to the continuity of \\(\\sigma^{2}(x)\\) and \\(f(x)\\) at \\(x\\),\n\\[\n\\begin{aligned}\n\\mathbb{E}[\\widehat{v}(x)] &=\\int_{-\\infty}^{\\infty} \\frac{1}{h} K\\left(\\frac{v-x}{h}\\right)^{2} \\sigma^{2}(v) f(v) d v \\\\\n&=\\int_{-\\infty}^{\\infty} K(u)^{2} \\sigma^{2}(x+h u) f(x+h u) d u \\\\\n&=\\int_{-\\infty}^{\\infty} K(u)^{2} \\sigma^{2}(x) f(x)+o(1) \\\\\n&=R_{K} \\sigma^{2}(x) f(x) .\n\\end{aligned}\n\\]\nSecond, because \\(\\widehat{v}(x)\\) is an average of independent random variables and the variance is smaller than the second moment\n\\[\n\\begin{aligned}\nn h \\operatorname{var}[\\widehat{v}(x)] &=\\frac{1}{h} \\operatorname{var}\\left[K\\left(\\frac{X-x}{h}\\right)^{2} \\sigma^{2}(X)\\right] \\\\\n& \\leq \\frac{1}{h} \\int_{-\\infty}^{\\infty} K\\left(\\frac{v-x}{h}\\right)^{4} \\sigma^{4}(v) f(v) d v \\\\\n&=\\int_{-\\infty}^{\\infty} K(u)^{4} \\sigma^{4}(x+h u) f(x+h u) d u \\\\\n& \\leq \\bar{K}^{2} R_{k} \\sigma^{4}(x) f(x)+o(1)\n\\end{aligned}\n\\]\nso \\(\\operatorname{var}[\\widehat{v}(x)] \\rightarrow 0\\).\nWe deduce from Markov’s inequality that \\(\\widehat{v}(x) \\underset{p}{\\longrightarrow} R_{K} \\sigma^{2}(x) f(x)\\), completing the proof.\nProof of Theorem 19.7. Observe that \\(m\\left(X_{i}\\right)-\\tilde{m}_{-i}\\left(X_{i}, h\\right)\\) is a function only of \\(\\left(X_{1}, \\ldots, X_{n}\\right)\\) and \\(\\left(e_{1}, \\ldots, e_{n}\\right)\\) excluding \\(e_{i}\\), and is thus uncorrelated with \\(e_{i}\\). Since \\(\\widetilde{e}_{i}(h)=m\\left(X_{i}\\right)-\\widetilde{m}_{-i}\\left(X_{i}, h\\right)+e_{i}\\), then\n\\[\n\\begin{aligned}\n\\mathbb{E}[\\mathrm{CV}(h)] &=\\mathbb{E}\\left(\\widetilde{e}_{i}(h)^{2} w\\left(X_{i}\\right)\\right) \\\\\n&=\\mathbb{E}\\left[e_{i}^{2} w\\left(X_{i}\\right)\\right]+\\mathbb{E}\\left[\\left(\\widetilde{m}_{-i}\\left(X_{i}, h\\right)-m\\left(X_{i}\\right)\\right)^{2} w\\left(X_{i}\\right)\\right] \\\\\n&+2 \\mathbb{E}\\left[\\left(\\widetilde{m}_{-i}\\left(X_{i}, h\\right)-m\\left(X_{i}\\right)\\right) w\\left(X_{i}\\right) e_{i}\\right] \\\\\n&=\\bar{\\sigma}^{2}+\\mathbb{E}\\left[\\left(\\widetilde{m}_{-i}\\left(X_{i}, h\\right)-m\\left(X_{i}\\right)\\right)^{2} w\\left(X_{i}\\right)\\right] .\n\\end{aligned}\n\\]\nThe second term is an expectation over the random variables \\(X_{i}\\) and \\(\\widetilde{m}_{-i}(x, h)\\), which are independent as the second is not a function of the \\(i^{t h}\\) observation. Thus taking the conditional expectation given the sample excluding the \\(i^{t h}\\) observation, this is the expectation over \\(X_{i}\\) only, which is the integral with respect to its density\n\\[\n\\mathbb{E}_{-i}\\left[\\left(\\widetilde{m}_{-i}\\left(X_{i}, h\\right)-m\\left(X_{i}\\right)\\right)^{2} w\\left(X_{i}\\right)\\right]=\\int\\left(\\widetilde{m}_{-i}(x, h)-m(x)\\right)^{2} f(x) w(x) d x .\n\\]\nTaking the unconditional expecation yields\n\\[\n\\begin{aligned}\n\\mathbb{E}\\left[\\left(\\widetilde{m}_{-i}\\left(X_{i}, h\\right)-m\\left(X_{i}\\right)\\right)^{2} w\\left(X_{i}\\right)\\right] &=\\mathbb{E}\\left[\\int\\left(\\widetilde{m}_{-i}(x, h)-m(x)\\right)^{2} f(x) w(x) d x\\right] \\\\\n&=\\operatorname{IMSE}_{n-1}(h)\n\\end{aligned}\n\\]\nwhere this is the IMSE of a sample of size \\(n-1\\) as the estimator \\(\\widetilde{m}_{-i}\\) uses \\(n-1\\) observations. Combined with (19.35) we obtain (19.12), as desired.\nProof of Theorem 19.8. We can write the Nadaraya-Watson estimator as\n\\[\n\\widehat{m}_{\\mathrm{nw}}(x)=m(x)+\\frac{\\widehat{b}(x)}{\\widehat{f}(x)}+\\frac{\\widehat{g}(x)}{\\widehat{f}(x)}\n\\]\nwhere \\(\\widehat{f}(x)\\) is the estimator (19.19), \\(\\widehat{b}(x)\\) is defined in (19.28), and\n\\[\n\\widehat{g}(x)=\\frac{1}{n h} \\sum_{i=1}^{n} K\\left(\\frac{X_{i}-x}{h}\\right) e_{i} .\n\\]\nSince \\(\\widehat{f}(x) \\underset{p}{\\longrightarrow} f(x)>0\\) by Theorem \\(17.6\\) of Probability and Statistics for Economists, the proof is completed by showing \\(\\widehat{b}(x) \\underset{p}{\\longrightarrow} 0\\) and \\(\\widehat{g}(x) \\underset{p}{\\longrightarrow} 0\\). Take \\(\\widehat{b}(x)\\). From (19.29) and the continuity of \\(m(x)\\) and \\(f(x)\\)\n\\[\n\\mathbb{E}[\\widehat{b}(x)]=\\int_{-\\infty}^{\\infty} K(u)(m(x+h u)-m(x)) f(x+h u) d u=o(1)\n\\]\nas \\(h \\rightarrow \\infty\\). From (19.33),\n\\[\nn h \\operatorname{var}[\\widehat{b}(x)] \\leq \\int_{-\\infty}^{\\infty} K(u)^{2}(m(x+h u)-m(x))^{2} f(x+h u) d u=o(1)\n\\]\nas \\(h \\rightarrow \\infty\\). Thus \\(\\operatorname{var}[\\widehat{b}(x)] \\longrightarrow 0\\). By Markov’s inequality we conclude \\(\\widehat{b}(x) \\stackrel{p}{\\longrightarrow} 0\\).\nTake \\(\\widehat{g}(x)\\). Since \\(\\widehat{g}(x)\\) is linear in \\(e_{i}\\) and \\(\\mathbb{E}[e \\mid X]=0\\), we find \\(\\mathbb{E}[\\widehat{g}(x)]=0\\). Since \\(\\widehat{g}(x)\\) is an average of independent random variables, the variance is smaller than the second moment, and the definition \\(\\sigma^{2}(X)=\\mathbb{E}\\left[e^{2} \\mid X\\right]\\)\n\\[\n\\begin{aligned}\nn h \\operatorname{var}[\\widehat{g}(x)] &=\\frac{1}{h} \\operatorname{var}\\left[K\\left(\\frac{X-x}{h}\\right) e\\right] \\\\\n& \\leq \\frac{1}{h} \\mathbb{E}\\left[K\\left(\\frac{X-x}{h}\\right)^{2} e^{2}\\right] \\\\\n&=\\frac{1}{h} \\mathbb{E}\\left[K\\left(\\frac{X-x}{h}\\right)^{2} \\sigma^{2}(X)\\right] \\\\\n&=\\int_{-\\infty}^{\\infty} K(u)^{2} \\sigma^{2}(x+h u) f(x+h u) d u \\\\\n&=R_{K} \\sigma^{2}(x) f(x)+o(1)\n\\end{aligned}\n\\]\nbecause \\(\\sigma^{2}(x)\\) and \\(f(x)\\) are continuous in \\(x\\). Thus \\(\\operatorname{var}[\\widehat{g}(x)] \\longrightarrow 0\\). By Markov’s inequality we conclude \\(\\widehat{g}(x) \\underset{p}{\\longrightarrow} 0\\), completing the proof.\nProof of Theorem 19.9. From (19.36), Theorem \\(17.6\\) of Probability and Statistics for Economists, and (19.34) we have\n\\[\n\\begin{aligned}\n\\sqrt{n h}\\left(\\widehat{m}_{\\mathrm{nw}}(x)-m(x)-h^{2} B_{\\mathrm{nw}}(x)\\right) &=\\sqrt{n h}\\left(\\frac{\\widehat{g}(x)}{\\widehat{f}(x)}\\right)+\\sqrt{n h}\\left(\\frac{\\widehat{b}(x)}{\\widehat{f}(x)}-h^{2} B_{\\mathrm{nw}}(x)\\right) \\\\\n&=\\sqrt{n h}\\left(\\frac{\\widehat{g}(x)}{f(x)}\\right)\\left(1+o_{p}(1)\\right)+\\sqrt{n h}\\left(o_{p}\\left(h^{2}\\right)+O_{p}\\left(\\sqrt{\\frac{h}{n}}\\right)\\right) \\\\\n&=\\sqrt{n h}\\left(\\frac{\\widehat{g}(x)}{f(x)}\\right)\\left(1+o_{p}(1)\\right)+\\left(o_{p}\\left(\\sqrt{n h^{5}}\\right)+O_{p}(h)\\right) \\\\\n&=\\sqrt{n h}\\left(\\frac{\\widehat{g}(x)}{f(x)}\\right)+o_{p}(1)\n\\end{aligned}\n\\]\nwhere the final equality holds because \\(\\sqrt{n h} \\widehat{g}(x)=O_{p}(1)\\) by (19.38) and the assumption \\(n h^{5}=O(1)\\). The proof is completed by showing \\(\\sqrt{n h} \\widehat{g}(x) \\underset{d}{\\longrightarrow} \\mathrm{N}\\left(0, R_{K} \\sigma^{2}(x) f(x)\\right)\\).\nDefine \\(Y_{n i}=h^{-1 / 2} K\\left(\\frac{X_{i}-x}{h}\\right) e_{i}\\) which are independent and mean zero. We can write \\(\\sqrt{n h} \\widehat{g}(x)=\\sqrt{n} \\bar{Y}\\) as a standardized sample average. We verify the conditions for the Lindeberg CLT (Theorem 6.4). In the notation of Theorem 6.4, set \\(\\bar{\\sigma}_{n}^{2}=\\operatorname{var}[\\sqrt{n} \\bar{Y}] \\rightarrow R_{K} f(x) \\sigma^{2}(x)\\) as \\(h \\rightarrow 0\\). The CLT holds if we can verify the Lindeberg condition.\nThis is an advanced calculation and will not interest most readers. It is provided for those interested in a complete derivation. Fix \\(\\epsilon>0\\) and \\(\\delta>0\\). Since \\(K(u)\\) is bounded we can write \\(K(u) \\leq \\bar{K}\\). Let \\(n h\\) be sufficiently large so that\n\\[\n\\left(\\frac{\\epsilon n h}{\\bar{K}^{2}}\\right)^{(r-2) / 2} \\geq \\frac{\\bar{\\sigma}}{\\delta} .\n\\]\nThe conditional moment bound (19.14) implies that for \\(x \\in \\mathscr{N}\\),\n\\[\n\\begin{aligned}\n\\mathbb{E}\\left[e^{2} \\mathbb{1}\\left\\{e^{2}>\\frac{\\epsilon n h}{\\bar{K}^{2}}\\right\\} \\mid X=x\\right] &=\\mathbb{E}\\left[\\frac{|e|^{r}}{|e|^{r-2}} \\mathbb{1}\\left\\{e^{2}>\\frac{\\epsilon n h}{\\bar{K}^{2}}\\right\\} \\mid X=x\\right] \\\\\n& \\leq \\mathbb{E}\\left[\\frac{|e|^{r}}{\\left(\\epsilon n h / \\bar{K}^{2}\\right)^{(r-2) / 2}} \\mid X=x\\right] \\\\\n& \\leq \\delta .\n\\end{aligned}\n\\]\nSince \\(Y_{n i}^{2} \\leq h^{-1} \\bar{K}^{2} e_{i}^{2}\\) we find\n\\[\n\\begin{aligned}\n\\mathbb{E}\\left[Y_{n i}^{2} \\mathbb{1}\\left\\{Y_{n i}^{2}>\\epsilon n\\right\\}\\right] & \\leq \\frac{1}{h} \\mathbb{E}\\left[K\\left(\\frac{X-x}{h}\\right)^{2} e^{2} \\mathbb{1}\\left\\{e^{2}>\\frac{\\epsilon n h}{\\bar{K}^{2}}\\right\\}\\right] \\\\\n&=\\frac{1}{h} \\mathbb{E}\\left[K\\left(\\frac{X-x}{h}\\right)^{2} \\mathbb{E}\\left(e^{2} \\mathbb{1}\\left\\{e^{2}>\\frac{\\epsilon n h}{\\bar{K}^{2}}\\right\\} \\mid X\\right)\\right] \\\\\n&=\\int_{-\\infty}^{\\infty} K(u)^{2} \\mathbb{E}\\left[e^{2} \\mathbb{1}\\left\\{e^{2}>\\epsilon n h / \\bar{K}^{2}\\right\\} \\mid X=x+h u\\right] f(x+h u) d u \\\\\n& \\leq \\delta \\int_{-\\infty}^{\\infty} K(u)^{2} f(x+h u) d u \\\\\n&=\\delta R_{K} f(x)+o(1) \\\\\n&=o(1)\n\\end{aligned}\n\\]\nbecause \\(\\delta\\) is arbitrary. This is the Lindeberg condition (6.2). The Lindeberg CLT (Theorem 6.4) shows that\n\\[\n\\sqrt{n h} \\widehat{g}(x)=\\sqrt{n} \\bar{Y} \\underset{d}{\\longrightarrow} \\mathrm{N}\\left(0, R_{K} \\sigma^{2}(x) f(x)\\right)\n\\]\nThis completes the proof."
  },
  {
    "objectID": "chpt19-nonparameter.html#exercises",
    "href": "chpt19-nonparameter.html#exercises",
    "title": "18  Nonparametric Regression",
    "section": "18.27 Exercises",
    "text": "18.27 Exercises\nExercise 19.1 For kernel regression suppose you rescale \\(Y\\), for example replace \\(Y\\) with \\(100 Y\\). How should the bandwidth \\(h\\) change? To answer this, first address how the functions \\(m(x)\\) and \\(\\sigma^{2}(x)\\) change under rescaling, and then calculate how \\(\\bar{B}\\) and \\(\\bar{\\sigma}^{2}\\) change. Deduce how the optimal \\(h_{0}\\) changes due to rescaling \\(Y\\). Does your answer make intuitive sense?\nExercise 19.2 Show that (19.6) minimizes the AIMSE (19.5).\nExercise 19.3 Describe in words how the bias of the local linear estimator changes over regions of convexity and concavity in \\(m(x)\\). Does this make intuitive sense?\nExercise 19.4 Suppose the true regression function is linear \\(m(x)=\\alpha+\\beta x\\) and we estimate the function using the Nadaraya-Watson estimator. Calculate the bias function \\(B(x)\\). Suppose \\(\\beta>0\\). For which regions is \\(B(x)>0\\) and for which regions is \\(B(x)<0\\) ? Now suppose that \\(\\beta<0\\) and re-answer the question. Can you intuitively explain why the NW estimator is positively and negatively biased for these regions? Exercise 19.5 Suppose \\(m(x)=\\alpha\\) is a constant function. Find the AIMSE-optimal bandwith (19.6) for NW estimation? Explain.\nExercise 19.6 Prove Theorem 19.11: Show that when \\(d \\geq 1\\) the AIMSE optimal bandwidth takes the form \\(h_{0}=c n^{-1 /(4+d)}\\) and AIMSE is \\(O\\left(n^{-4 /(4+d)}\\right)\\).\nExercise 19.7 Take the DDK2011 dataset and the subsample of boys who experienced tracking. As in Section \\(19.21\\) use the Local Linear estimator to estimate the regression of testscores on percentile but now with the subsample of boys. Plot with \\(95 %\\) confidence intervals. Comment on the similarities and differences with the estimate for the subsample of girls.\nExercise 19.8 Take the cps09mar dataset and the subsample of individuals with education=20 (professional degree or doctorate), with experience between 0 and 40 years.\n\nUse Nadaraya-Watson to estimate the regression of log(wage) on experience, separately for men and women. Plot with \\(95 %\\) confidence intervals. Comment on how the estimated wage profiles vary with experience. In particular, do you think the evidence suggests that expected wages fall for experience levels above 20 for this education group?\nRepeat using the Local Linear estimator. How do the estimates and confidence intervals change?\n\nExercise 19.9 Take the Invest1993 dataset and the subsample of observations with \\(Q \\leq 5\\). (In the dataset \\(Q\\) is the variable vala.)\n\nUse Nadaraya-Watson to estimate the regression of \\(I\\) on \\(Q\\). (In the dataset \\(I\\) is the variable inva.) Plot with \\(95 %\\) confidence intervals.\nRepeat using the Local Linear estimator.\nIs there evidence to suggest that the regression function is nonlinear?\n\nExercise 19.10 The RR2010 dataset is from Reinhart and Rogoff (2010). It contains observations on annual U.S. GDP growth rates, inflation rates, and the debt/gdp ratio for the long time span 1791-2009. The paper made the strong claim that gdp growth slows as debt/gdp increases and in particular that this relationship is nonlinear with debt negatively affecting growth for debt ratios exceeding \\(90 %\\). Their full dataset includes 44 countries. Our extract only includes the United States.\n\nUse Nadaraya-Watson to estimate the regression of gdp growth on the debt ratio. Plot with 95% confidence intervals.\nRepeat using the Local Linear estimator.\nDo you see evidence of nonlinearity and/or a change in the relationship at \\(90 %\\) ?\nNow estimate a regression of gdp growth on the inflation rate. Comment on what you find.\n\nExercise 19.11 We will consider a nonlinear AR(1) model for gdp growth rates\n\\[\n\\begin{aligned}\n&Y_{t}=m\\left(Y_{t-1}\\right)+e_{t} \\\\\n&Y_{t}=100\\left(\\left(\\frac{G D P_{t}}{G D P_{t-1}}\\right)^{4}-1\\right)\n\\end{aligned}\n\\]\n\nCreate GDP growth rates \\(Y_{t}\\). Extract the level of real U.S. GDP ( \\(\\left.g d p c 1\\right)\\) from FRED-QD and make the above transformation to growth rates.\nUse Nadaraya-Watson to estimate \\(m(x)\\). Plot with 95% confidence intervals.\nRepeat using the Local Linear estimator.\nDo you see evidence of nonlinearity?"
  },
  {
    "objectID": "chpt20-series-reg.html#introduction",
    "href": "chpt20-series-reg.html#introduction",
    "title": "19  Series Regression",
    "section": "19.1 Introduction",
    "text": "19.1 Introduction\nChapter 19 studied nonparametric regression by kernel smoothing methods. In this chapter we study an alternative class of nonparametric methods known as series regression.\nThe basic model is identical to that examined in Chapter 19. We assume that there are random variables \\((Y, X)\\) such that \\(\\mathbb{E}\\left[Y^{2}\\right]<\\infty\\) and satisfy the regression model\n\\[\n\\begin{aligned}\nY &=m(X)+e \\\\\n\\mathbb{E}[e \\mid X] &=0 \\\\\n\\mathbb{E}\\left[e^{2} \\mid X\\right] &=\\sigma^{2}(X) .\n\\end{aligned}\n\\]\nThe goal is to estimate the CEF \\(m(x)\\). We start with the simple setting where \\(X\\) is scalar and consider more general cases later.\nA series regression model is a sequence \\(K=1,2, \\ldots\\), of approximating models \\(m_{K}(x)\\) with \\(K\\) parameters. In this chapter we exclusively focus on linear series models, and in particular polynomials and splines. This is because these are simple, convenient, and cover most applications of series methods in applied economics. Other series models include trigonometric polynomials, wavelets, orthogonal wavelets, B-splines, and neural networks. For a detailed review see Chen (2007).\nLinear series regression models take the form\n\\[\nY=X_{K}^{\\prime} \\beta_{K}+e_{K}\n\\]\nwhere \\(X_{K}=X_{K}(X)\\) is a vector of regressors obtained by making transformations of \\(X\\) and \\(\\beta_{K}\\) is a coefficient vector. There are multiple possible definitions of the coefficient \\(\\beta_{K}\\). We define \\({ }^{1}\\) it by projection\n\\[\n\\beta_{K}=\\mathbb{E}\\left[X_{K} X_{K}^{\\prime}\\right]^{-1} \\mathbb{E}\\left[X_{K} Y\\right]=\\mathbb{E}\\left[X_{K} X_{K}^{\\prime}\\right]^{-1} \\mathbb{E}\\left[X_{K} m(X)\\right] .\n\\]\nThe series regression error \\(e_{K}\\) is defined by (20.2) and (20.3), is distinct from the regression error \\(e\\) in (20.1), and is indexed by \\(K\\) because it depends on the regressors \\(X_{K}\\). The series approximation to \\(m(x)\\) is\n\\[\nm_{K}(x)=X_{K}(x)^{\\prime} \\beta_{K} .\n\\]\n\\({ }^{1}\\) An alternative is to define \\(\\beta_{K}\\) as the best uniform approximation as in (20.8). It is not critical so long as we are careful to be consistent with our notation. The coefficient is typically \\({ }^{2}\\) estimated by least squares\n\\[\n\\widehat{\\beta}_{K}=\\left(\\sum_{i=1}^{n} X_{K i} X_{K i}^{\\prime}\\right)^{-1}\\left(\\sum_{i=1}^{n} X_{K i} Y_{i}\\right)=\\left(\\boldsymbol{X}_{K}^{\\prime} \\boldsymbol{X}_{K}\\right)^{-1}\\left(\\boldsymbol{X}_{K}^{\\prime} \\boldsymbol{Y}\\right)\n\\]\nThe estimator for \\(m(x)\\) is\n\\[\n\\widehat{m}_{K}(x)=X_{K}(x)^{\\prime} \\widehat{\\beta}_{K} .\n\\]\nThe difference between specific models arises due to the different choices of transformations \\(X_{K}(x)\\).\nThe theoretical issues we will explore in this chapter are: (1) Approximation properties of polynomials and splines; (2) Consistent estimation of \\(m(x)\\); (3) Asymptotic normal approximations; (4) Selection of \\(K\\); (5) Extensions.\nFor a textbook treatment of series regression see Li and Racine (2007). For an advanced treatment see Chen (2007). Two seminal contributions are Andrews (1991a) and Newey (1997). Two recent important papers are Belloni, Chernozhukov, Chetverikov, and Kato (2015) and Chen and Christensen (2015)."
  },
  {
    "objectID": "chpt20-series-reg.html#polynomial-regression",
    "href": "chpt20-series-reg.html#polynomial-regression",
    "title": "19  Series Regression",
    "section": "19.2 Polynomial Regression",
    "text": "19.2 Polynomial Regression\nThe prototypical series regression model for \\(m(x)\\) is a \\(p^{t h}\\) order polynomial\n\\[\nm_{K}(x)=\\beta_{0}+\\beta_{1} x+\\beta_{2} x^{2}+\\cdots+\\beta_{p} x^{p} .\n\\]\nWe can write it in vector notation as (20.4) where\n\\[\nX_{K}(x)=\\left(\\begin{array}{c}\n1 \\\\\nx \\\\\n\\vdots \\\\\nx^{p}\n\\end{array}\\right) .\n\\]\nThe number of parameters is \\(K=p+1\\). Notice that we index \\(X_{K}(x)\\) and \\(\\beta_{K}\\) by \\(K\\) as their dimensions and values vary with \\(K\\).\nThe implied polynomial regression model for the random pair \\((Y, X)\\) is \\((20.2)\\) with\n\\[\nX_{K}=X_{K}(X)=\\left(\\begin{array}{c}\n1 \\\\\nX \\\\\n\\vdots \\\\\nX^{p}\n\\end{array}\\right)\n\\]\nThe degree of flexibility of a polynomial regression is controlled by the polynomial order \\(p\\). A larger \\(p\\) yields a more flexible model while a smaller \\(p\\) typically results in a estimator with a smaller variance.\nIn general, a linear series regression model takes the form\n\\[\nm_{K}(x)=\\beta_{1} \\tau_{1}(x)+\\beta_{2} \\tau_{2}(x)+\\cdots+\\beta_{K} \\tau_{K}(x)\n\\]\nwhere the functions \\(\\tau_{j}(x)\\) are called the basis transformations. The polynomial regression model uses the power basis \\(\\tau_{j}(x)=x^{j-1}\\). The model \\(m_{K}(x)\\) is called a series regression because it is obtained by sequentially adding the series of variables \\(\\tau_{j}(x)\\).\n\\({ }^{2}\\) Penalized estimators have also been recommended. We do not review these methods here."
  },
  {
    "objectID": "chpt20-series-reg.html#illustrating-polynomial-regression",
    "href": "chpt20-series-reg.html#illustrating-polynomial-regression",
    "title": "19  Series Regression",
    "section": "19.3 Illustrating Polynomial Regression",
    "text": "19.3 Illustrating Polynomial Regression\nConsider the cps09mar dataset and a regression of \\(\\log (\\) wage \\()\\) on experience for women with a college education (education=16), separately for white women and Black women. The classical Mincer model uses a quadratic in experience. Given the large sample sizes (4682 for white women and 517 for Black women) we can consider higher order polynomials. In Figure \\(20.1\\) we plot least squares estimates of the CEFs using polynomials of order 2, 4, 8, and 12.\nExamine panel (a) which shows the estimates for the sub-sample of white women. The quadratic specification appears mis-specified with a shape noticably different from the other estimates. The difference between the polynomials of order 4,8 , and 12 is relatively minor, especially for experience levels below 20.\nNow examine panel (b) which shows the estimates for the sub-sample of Black women. This panel is quite different from panel (a). The estimates are erratic and increasingly so as the polynomial order increases. Assuming we are expecting a concave (or nearly concave) experience profile the only estimate which satisfies this is the quadratic.\nWhy the difference between panels (a) and (b)? The most likely explanation is the different sample sizes. The sub-sample of Black women has much fewer observations so the CEF is much less precisely estimated, giving rise to the erratic plots. This suggests (informally) that it may be preferred to use a smaller polynomial order \\(p\\) in the second sub-sample, or equivalently to use a larger \\(p\\) when the sample size \\(n\\) is larger. The idea that model complexity - the number of coefficients \\(K\\) - should vary with sample size \\(n\\) is an important feature of series regression.\nThe erratic nature of the estimated polynomial regressions in Figure 20.1(b) is a common feature of higher-order estimated polynomial regressions. Better results can sometimes be obtained by a spline regression which is described in Section 20.5.\n\n\nWhite Women\n\n\n\nBlack Women\n\nFigure 20.1: Polynomial Estimates of Experience Profile"
  },
  {
    "objectID": "chpt20-series-reg.html#orthogonal-polynomials",
    "href": "chpt20-series-reg.html#orthogonal-polynomials",
    "title": "19  Series Regression",
    "section": "19.4 Orthogonal Polynomials",
    "text": "19.4 Orthogonal Polynomials\nStandard implementation of the least squares estimator (20.5) of a polynomial regression may return a computational error message when \\(p\\) is large. (See Section 3.24.) This is because the moments of \\(X^{j}\\) can be highly heterogeneous across \\(j\\) and because the variables \\(X^{j}\\) can be highly correlated. These two factors imply in practice that the matrix \\(\\boldsymbol{X}_{K}^{\\prime} \\boldsymbol{X}_{K}\\) can be ill-conditioned (the ratio of the largest to smallest eigenvalue can be quite large) and some packages will return error messages rather than compute \\(\\widehat{\\beta}_{K}\\).\nIn most cases the condition of \\(\\boldsymbol{X}_{K}^{\\prime} \\boldsymbol{X}_{K}\\) can be dramatically improved by rescaling the observations. As discussed in Section \\(3.24\\) a simple method for non-negative regressors is to rescale each by its sample mean, e.g. replace \\(X_{i}^{j}\\) with \\(X_{i}^{j} /\\left(n^{-1} \\sum_{i=1}^{n} X_{i}^{j}\\right)\\). Even better conditioning can often be obtained by rescaling \\(X_{i}\\) to lie in \\([-1,1]\\) before applying powers. In most applications one of these methods will be sufficient for a well-conditioned regression.\nA computationally more robust implementation can be obtained by using orthogonal polynomials. These are linear combinations of the polynomial basis functions and produce identical regression estimators (20.6). The goal of orthogonal polynomials is to produce regressors which are either orthogonal or close to orthogonal and have similar variances so that \\(\\boldsymbol{X}_{K}^{\\prime} \\boldsymbol{X}_{K}\\) is close to diagonal with similar diagonal elements. These orthogonalized regressors \\(X_{K}^{*}=\\boldsymbol{A}_{K} X_{K}\\) can be written as linear combinations of the original variables \\(X_{K}\\). If the regressors are orthogonalized then the regression estimator (20.6) is modified by replacing \\(X_{K}(x)\\) with \\(X_{K}^{*}(x)=\\boldsymbol{A}_{K} X_{K}(x)\\).\nOne approach is to use sample orthogonalization. This is done by a sequence of regressions of \\(X_{i}^{j}\\) on the previously orthogonalized variables and then rescaling. This will result in perfectly orthogonalized variables. This is what is implemented in many statistical packages under the label “orthogonal polynomials”, for example, the function poly in R. If this is done then the least squares coefficients have no meaning outside this specific sample and it is not convenient for calculation of \\(\\widehat{m}_{K}(x)\\) for values of \\(x\\) other than sample values. This is the approach used for the examples presented in the previous section.\nAnother approach is to use an algebraic orthogonal polynomial. This is a polynomial which is orthogonal with respect to a known weight function \\(w(x)\\). Specifically, it is a sequence \\(p_{j}(x), j=0,1,2, \\ldots\\), with the property that \\(\\int p_{j}(x) p_{\\ell}(x) w(x) d x=0\\) for \\(j \\neq \\ell\\). This means that if \\(w(x)=f(x)\\), the marginal density of \\(X\\), then the basis transformations \\(p_{j}(X)\\) will be mutually orthogonal (in expectation). Since we do now know the density of \\(X\\) this is not feasible in practice, but if \\(w(x)\\) is close to the density of \\(X\\) then we can expect that the basis transformations will be close to mutually orthogonal. To implement an algebraic orthogonal polynomial you first should rescale your \\(X\\) variable so that it satisfies the support for the weight function \\(w(x)\\).\nThe following three choices are most relevant for economic applications.\nLegendre Polynomial. These are orthogonal with respect to the uniform density on \\([-1,1]\\). (So should be applied to regressors scaled to have support in \\([-1,1]\\).)\n\\[\np_{j}(x)=\\frac{1}{2^{j}} \\sum_{\\ell=0}^{j}\\left(\\begin{array}{l}\nj \\\\\n\\ell\n\\end{array}\\right)^{2}(x-1)^{j-\\ell}(x+1)^{\\ell} .\n\\]\nFor example, the first four are \\(p_{0}(x)=1, p_{1}(x)=x, p_{2}(x)=\\left(3 x^{2}-1\\right) / 2\\), and \\(p_{3}(x)=\\left(5 x^{3}-3 x\\right) / 2\\). The best computational method is the recurrence relationship\n\\[\np_{j+1}(x)=\\frac{(2 j+1) x p_{j}(x)-j p_{j-1}(x)}{j+1} .\n\\]\nLaguerre Polynomial. These are orthogonal with respect to the exponential density \\(e^{-x}\\) on \\([0, \\infty)\\). (So should be applied to non-negative regressors scaled if possible to have approximately unit mean and/or variance.)\n\\[\np_{j}(x)=\\sum_{\\ell=0}^{j}\\left(\\begin{array}{l}\nj \\\\\n\\ell\n\\end{array}\\right) \\frac{(-x)^{\\ell}}{\\ell !} .\n\\]\nFor example, the first four are \\(p_{0}(x)=1, p_{1}(x)=1-x, p_{2}(x)=\\left(x^{2}-4 x+2\\right) / 2\\), and \\(p_{3}(x)=\\left(-x^{3}+9 x^{2}-18 x+6\\right) / 6\\). The best computational method is the recurrence relationship\n\\[\np_{j+1}(x)=\\frac{(2 j+1-x) p_{j}(x)-j p_{j-1}(x)}{j+1} .\n\\]\nHermite Polynomial. These are orthogonal with respect to the standard normal density on \\((-\\infty, \\infty)\\). (So should be applied to regressors scaled to have mean zero and variance one.)\n\\[\np_{j}(x)=j ! \\sum_{\\ell=0}^{\\lfloor j / 2\\rfloor} \\frac{(-1 / 2)^{\\ell} x^{\\ell-2 j}}{\\ell !(j-2 \\ell !)} .\n\\]\nFor example, the first four are \\(p_{0}(x)=1, p_{1}(x)=x, p_{2}(x)=x^{2}-1\\), and \\(p_{3}(x)=x^{3}-3 x\\). The best computational method is the recurrence relationship\n\\[\np_{j+1}(x)=x p_{j}(x)-j p_{j-1}(x) .\n\\]\nThe R package orthopolynom provides a convenient set of commands to compute many orthogonal polynomials including the above."
  },
  {
    "objectID": "chpt20-series-reg.html#splines",
    "href": "chpt20-series-reg.html#splines",
    "title": "19  Series Regression",
    "section": "19.5 Splines",
    "text": "19.5 Splines\nA spline is a piecewise polynomial. Typically the order of the polynomial is pre-selected to be linear, quadratic, or cubic. The flexibility of the model is determined by the number of polynomial segments. The join points between the segments are called knots.\nTo impose smoothness and parsimony it is common to constrain the spline function to have continuous derivatives up to the order of the spline. Thus a linear spline is constrained to be continuous, a quadratic spline is constrained to have a continuous first derivative, and a cubic spline is constrained to have continuous first and second derivatives.\nA simple way to construct a regression spline is as follows. A linear spline with one knot \\(\\tau\\) is\n\\[\nm_{K}(x)=\\beta_{0}+\\beta_{1} x+\\beta_{2}(x-\\tau) \\mathbb{1}\\{x \\geq \\tau\\} .\n\\]\nTo see that this is a linear spline, observe that for \\(x \\leq \\tau\\) the function \\(m_{K}(x)=\\beta_{0}+\\beta_{1} x\\) is linear with slope \\(\\beta_{1}\\); for \\(x \\geq \\tau\\) the function \\(m_{K}(x)\\) is linear with slope \\(\\beta_{1}+\\beta_{2}\\); and the function is continuous at \\(x=\\tau\\). Note that \\(\\beta_{2}\\) is the change in the slope at \\(\\tau\\). A linear spline with two knots \\(\\tau_{1}<\\tau_{2}\\) is\n\\[\nm_{K}(x)=\\beta_{0}+\\beta_{1} x+\\beta_{2}\\left(x-\\tau_{1}\\right) \\mathbb{1}\\left\\{x \\geq \\tau_{2}\\right\\}+\\beta_{3}\\left(x-\\tau_{2}\\right) \\mathbb{1}\\left\\{x \\geq \\tau_{2}\\right\\} .\n\\]\nA quadratic spline with one knot is\n\\[\nm_{K}(x)=\\beta_{0}+\\beta_{1} x+\\beta_{2} x^{2}+\\beta_{3}(x-\\tau)^{2} \\mathbb{1}\\{x \\geq \\tau\\} .\n\\]\nTo see that this is a quadratic spline, observe that for \\(x \\leq \\tau\\) the function is the quadratic \\(\\beta_{0}+\\beta_{1} x+\\beta_{2} x^{2}\\) with second derivative \\(m_{K}^{\\prime \\prime}(\\tau)=2 \\beta_{2}\\); for \\(x \\geq \\tau\\) the second derivative is \\(m_{K}^{\\prime \\prime}(\\tau)=2\\left(\\beta_{2}+\\beta_{3}\\right)\\); so \\(2 \\beta_{3}\\) is the change in the second derivative at \\(\\tau\\). The first derivative at \\(x=\\tau\\) is the continuous function \\(m_{K}^{\\prime}(\\tau)=\\) \\(\\beta_{1}+2 \\beta_{2} \\tau\\).\nIn general, a \\(p^{t h}\\)-order spline with \\(N\\) knots \\(\\tau_{1}<\\tau_{2}<\\cdots<\\tau_{N}\\) is\n\\[\nm_{K}(x)=\\sum_{j=0}^{p} \\beta_{j} x^{j}+\\sum_{k=1}^{N} \\beta_{p+k}\\left(x-\\tau_{k}\\right)^{p} \\mathbb{1}\\left\\{x \\geq \\tau_{k}\\right\\}\n\\]\nwhich has \\(K=N+p+1\\) coefficients.\nThe implied spline regression model for the random pair \\((Y, X)\\) is \\((20.2)\\) where\n\\[\nX_{K}=X_{K}(X)=\\left(\\begin{array}{c}\n1 \\\\\nX \\\\\n\\vdots \\\\\nX^{p} \\\\\n\\left(X-\\tau_{1}\\right)^{p} \\mathbb{1}\\left\\{X \\geq \\tau_{1}\\right\\} \\\\\n\\vdots \\\\\n\\left(X-\\tau_{N}\\right)^{p} \\mathbb{1}\\left\\{X \\geq \\tau_{N}\\right\\}\n\\end{array}\\right) .\n\\]\nIn practice a spline will depend critically on the choice of the knots \\(\\tau_{k}\\). When \\(X\\) is bounded with an approximately uniform distribution it is common to space the knots evenly so all segments have the same length. When the distribution of \\(X\\) is not uniform an alternative is to set the knots at the quantiles \\(j /(N+1)\\) so that the probability mass is equalized across segments. A third alternative is to set the knots at the points where \\(m(x)\\) has the greatest change in curvature (see Schumaker (2007), Chapter 7). In all cases the set of knots \\(\\tau_{j}\\) can change with \\(K\\). Therefore a spline is a special case of an approximation of the form\n\\[\nm_{K}(x)=\\beta_{1} \\tau_{1 K}(x)+\\beta_{2} \\tau_{2 K}(x)+\\cdots+\\beta_{K} \\tau_{K K}(x)\n\\]\nwhere the basis transformations \\(\\tau_{j K}(x)\\) depend on both \\(j\\) and \\(K\\). Many authors call such approximations a sieve rather than a series because the basis transformations change with \\(K\\). This distinction is not critical to our treatment so for simplicity we refer to splines as series regression models."
  },
  {
    "objectID": "chpt20-series-reg.html#illustrating-spline-regression",
    "href": "chpt20-series-reg.html#illustrating-spline-regression",
    "title": "19  Series Regression",
    "section": "19.6 Illustrating Spline Regression",
    "text": "19.6 Illustrating Spline Regression\nIn Section \\(20.3\\) we illustrated regressions of log(wage) on experience for white and Black women with a college education. Now we consider a similar regression for Black men with a college education, a sub-sample with 394 observations.\nWe use a quadratic spline with four knots at experience levels of \\(10,20,30\\), and 40 . This is a regression model with seven coefficients. The estimated regression function is displayed in Figure \\(20.2(\\mathrm{a})\\). An estimated \\(6^{\\text {th }}\\) order polynomial regression is also displayed for comparison (a \\(6^{\\text {th }}\\) order polynomial is an appropriate comparison because it also has seven coefficients).\nWhile the spline is a quadratic over each segment, what you can see is that the first two segments (experience levels between 0-10 and 10-20 years) are essentially linear. Most of the curvature occurs in the third and fourth segments (20-30 and 30-40 years) where the estimated regression function peaks and twists into a negative slope. The estimated regression function is smooth.\nA quadratic or cubic spline is useful when it is desired to impose smoothness as in Figure 20.2(a). In contrast, a linear spline is useful when it is desired to allow for sharp changes in slope.\nTo illustrate we consider the data set CHJ2004 which is a sample of 8684 urban Phillipino households from Cox, B. E. Hansen, and Jimenez (2004). This paper studied the crowding-out impact of a\n\n\nExperience Profile\n\n\n\nEffect of Income on Transfers\n\nFigure 20.2: Spline Regression Estimates\nfamily’s income on non-governmental (e.g., extended family) income transfers \\({ }^{3}\\). A model of altruistic transfers predicts that extended families will make gifts (transfers) when the recipient family’s income is sufficiently low, but will not make transfers if the recipient family’s income exceeds a threshold. A pure altruistic model predicts that the regression of transfers received on family income should have a slope of \\(-1\\) up to this threshold and be flat above this threshold. We estimated this regression (including the same controls as the authors \\({ }^{4}\\) ) using a linear spline with knots at 10000, 20000, 50000, 100000, and 150000 pesos. These knots were selected to give flexibility for low income levels where there are more observations. This model has a total of 22 coefficients.\nThe estimated regression function (as a function of household income) is displayed in Figure \\(20.2\\) (b). For the first two segments (incomes levels below 20000 pesos) the regression function is negatively sloped as predicted with a slope about \\(-0.7\\) from 0 to 10000 pesos, and \\(-0.3\\) from 10000 to 20000 pesos. The estimated regression function is effectively flat for income levels above 20000 pesos. This shape is consistent with the pure altruism model. A linear spline model is particularly well suited for this application as it allows for discontinuous changes in slope.\nLinear spline models with a single knot have been recently popularized by Card, Lee, Pei, and Weber (2015) with the label regression kink design."
  },
  {
    "objectID": "chpt20-series-reg.html#the-globallocal-nature-of-series-regression",
    "href": "chpt20-series-reg.html#the-globallocal-nature-of-series-regression",
    "title": "19  Series Regression",
    "section": "19.7 The Global/Local Nature of Series Regression",
    "text": "19.7 The Global/Local Nature of Series Regression\nRecall from Section \\(19.18\\) that we described kernel regression as inherently local in nature. The Nadaraya-Watson, Local Linear, and Local Polynomial estimators of the CEF \\(m(x)\\) are weighted averages of \\(Y_{i}\\) for observations for which \\(X_{i}\\) is close to \\(x\\).\n\\({ }^{3}\\) Defined as the sum of transfers received domestically, from abroad, and in-kind, less gifts.\n\\({ }^{4}\\) The controls are: age of household head, education (5 dummy categories), married, female, married female, number of children (3 dummies), size of household, employment status (2 dummies). In contrast, series regression is typically described as global in nature. The estimator \\(\\widehat{m}_{K}(x)=X_{K}(x)^{\\prime} \\widehat{\\beta}_{K}\\) is a function of the entire sample. The coefficients of a fitted polynomial (or spline) are affected by the global shape of the function \\(m(x)\\) and thus affect the estimator \\(\\widehat{m}_{K}(x)\\) at any point \\(x\\).\nWhile this description has some merit it is not a complete description. As we now show, series regression estimators share the local smoothing property of kernel regression. As the number of series terms \\(K\\) increase a series estimator \\(\\widehat{m}_{K}(x)=X_{K}(x)^{\\prime} \\widehat{\\beta}_{K}\\) also becomes a local weighted average estimator.\nTo see this, observe that we can write the estimator as\n\\[\n\\begin{aligned}\n\\widehat{m}_{K}(x) &=X_{K}(x)^{\\prime}\\left(\\boldsymbol{X}_{K}^{\\prime} \\boldsymbol{X}_{K}\\right)^{-1}\\left(\\boldsymbol{X}_{K}^{\\prime} \\boldsymbol{Y}\\right) \\\\\n&=\\frac{1}{n} \\sum_{i=1}^{n} X_{K}(x)^{\\prime} \\widehat{\\boldsymbol{Q}}_{K}^{-1} X_{K}\\left(X_{i}\\right) Y_{i} \\\\\n&=\\frac{1}{n} \\sum_{i=1}^{n} \\widehat{w}_{K}\\left(x, X_{i}\\right) Y_{i}\n\\end{aligned}\n\\]\nwhere \\(\\widehat{\\boldsymbol{Q}}_{K}=n^{-1} \\boldsymbol{X}_{K}^{\\prime} \\boldsymbol{X}_{K}\\) and \\(\\widehat{w}_{K}(x, u)=x_{K}(x)^{\\prime} \\widehat{\\boldsymbol{Q}}_{K}^{-1} x_{K}(u)\\). Thus \\(\\widehat{m}_{K}(x)\\) is a weighted average of \\(Y_{i}\\) using the weights \\(\\widehat{w}_{K}\\left(x, X_{i}\\right)\\). The weight function \\(\\widehat{w}_{K}\\left(x, X_{i}\\right)\\) appears to be maximized at \\(X_{i}=x\\), so \\(\\widehat{m}(x)\\) puts more weight on observations for which \\(X_{i}\\) is close to \\(x\\), similarly to kernel regression.\n\n\n\\(x=0.5\\)\n\n\n\n\\(x=0.25\\)\n\nFigure 20.3: Kernel Representation of Polynomial Weight Function\nTo see this more precisely, observe that because \\(\\widehat{\\boldsymbol{Q}}_{K}\\) will be close in large samples to \\(\\boldsymbol{Q}_{K}=\\mathbb{E}\\left[X_{K} X_{K}^{\\prime}\\right]\\), \\(\\widehat{w}_{K}(x, u)\\) will be close to the deterministic weight function\n\\[\nw_{K}(x, u)=X_{K}(x)^{\\prime} \\boldsymbol{Q}_{K}^{-1} X_{K}(u) .\n\\]\nTake the case \\(X \\sim U[0,1]\\). In Figure \\(20.3\\) we plot the weight function \\(w_{K}(x, u)\\) as a funtion of \\(u\\) for \\(x=\\) \\(0.5\\) (panel (a)) and \\(x=0.25\\) (panel (b)) for \\(p=4,8,12\\) in panel (a) and \\(p=4\\), 12 in panel (b). First, examine panel (a). Here you can see that the weight function \\(w(x, u)\\) is symmetric in \\(u\\) about \\(x\\). For \\(p=4\\) the weight function appears similar to a quadratic in \\(u\\), and as \\(p\\) increases the weight function concentrates its main weight around \\(x\\). However, the weight function is not non-negative. It is quite similar in shape to what are known as higher-order (or bias-reducing) kernels, which were not reviewed in the previous chapter but are part of the kernel estimation toolkit. Second, examine panel (b). Again the weight function is maximized at \\(x\\), but now it is asymmetric in \\(u\\) about the point \\(x\\). Still, the general features from panel (a) carry over to panel (b). Namely, as \\(p\\) increases the polynomial estimator puts most weight on observations for which \\(X\\) is close to \\(x\\) (just as for kernel regression), but is different from conventional kernel regression in that the weight function is not non-negative. Qualitatively similar plots are obtained for spline regression.\nThere is little formal theory (of which I am aware) which makes a formal link between series regression and kernel regression so the comments presented here are illustrative \\({ }^{5}\\). However, the point is that statements of the form “Series regession is a global method; Kernel regression is a local method” may not be complete. Both are global in nature when \\(h\\) is large (kernels) or \\(K\\) is small (series), and are local in nature when \\(h\\) is small (kernels) or \\(K\\) is large (series)."
  },
  {
    "objectID": "chpt20-series-reg.html#stone-weierstrass-and-jackson-approximation-theory",
    "href": "chpt20-series-reg.html#stone-weierstrass-and-jackson-approximation-theory",
    "title": "19  Series Regression",
    "section": "19.8 Stone-Weierstrass and Jackson Approximation Theory",
    "text": "19.8 Stone-Weierstrass and Jackson Approximation Theory\nA good series approximation \\(m_{K}(x)\\) has the property that it gets close to the true CEF \\(m(x)\\) as the complexity \\(K\\) increases. Formal statements can be derived from the mathematical theory of the approximation of functions.\nAn elegant and famous theorem is the Stone-Weierstrass Theorem (Weierstrass, 1885, Stone, 1948) which states that any continuous function can be uniformly well approximated by a polynomial of sufficiently high order. Specifically, the theorem states that if \\(m(x)\\) is continuous on a compact set \\(S\\) then for any \\(\\epsilon>0\\) there is some \\(K\\) sufficiently large such that\n\\[\n\\inf _{\\beta} \\sup _{x \\in S}\\left|m(x)-X_{K}(x)^{\\prime} \\beta\\right| \\leq \\epsilon .\n\\]\nThus the true unknown \\(m(x)\\) can be arbitrarily well approximated by selecting a suitable polynomial.\nJackson (1912) strengthened this result to give convergence rates which depend on the smoothness of \\(m(x)\\). The basic result has been extended to spline functions. The following notation will be useful. Define the \\(\\beta\\) which minimizes the left-side of (20.7) as\n\\[\n\\beta_{K}^{*}=\\underset{\\beta}{\\operatorname{argmin}} \\sup _{x \\in S}\\left|m(x)-X_{K}(x)^{\\prime} \\beta\\right|,\n\\]\ndefine the approximation error\n\\[\nr_{K}^{*}(x)=m(x)-X_{K}(x)^{\\prime} \\beta_{K}^{*},\n\\]\nand define the minimized value of (20.7)\n\\[\n\\delta_{K}^{*} \\stackrel{\\text { def }}{=} \\inf _{\\beta} \\sup _{x \\in S}\\left|m(x)-X_{K}(x)^{\\prime} \\beta\\right|=\\sup _{x \\in S}\\left|m(x)-X_{K}(x)^{\\prime} \\beta_{K}^{*}\\right|=\\sup _{x \\in S}\\left|r_{K}^{*}(x)\\right| .\n\\]\n\\({ }^{5}\\) Similar connections are made in the appendix of Chen, Liao, and Sun (2012). Theorem 20.1 If for some \\(\\alpha \\geq 0, m^{(\\alpha)}(x)\\) is uniformly continuous on a compact set \\(S\\) and \\(X_{K}(x)\\) is either a polynomial basis or a spline basis (with uniform knot spacing) of order \\(s \\geq \\alpha\\), then as \\(K \\rightarrow \\infty\\)\n\\[\n\\delta_{K}^{*} \\leq o\\left(K^{-\\alpha}\\right) .\n\\]\nFurthermore, if \\(m^{(2)}(x)\\) is uniformly continuous on \\(S\\) and \\(X_{K}(x)\\) is a linear spline basis, then \\(\\delta_{K}^{*} \\leq O\\left(K^{-2}\\right)\\).\nFor a proof for the polynomial case see Theorem \\(4.3\\) of Lorentz (1986) or Theorem \\(3.12\\) of Schumaker (2007) plus his equations (2.119) and (2.121). For the spline case see Theorem \\(6.27\\) of Schumaker (2007) plus his equations (2.119) and (2.121). For the linear spline case see Theorem \\(6.15\\) of Schumaker, equation (6.28).\nTheorem \\(20.1\\) is more useful than the classic Stone-Weierstrass Theorem as it gives an approximation rate which depends on the smoothness order \\(\\alpha\\). The rate \\(o\\left(K^{-\\alpha}\\right)\\) in (20.11) means that the approximation error (20.10) decreases as \\(K\\) increases and decreases at a faster rate when \\(\\alpha\\) is large. The standard interpretation is that when \\(m(x)\\) is smoother it is possible to approximate it with fewer terms.\nIt will turn out that for our distribution theory it is sufficient to consider the case that \\(m^{(2)}(x)\\) is uniformly continuous. For this case Theorem \\(20.1\\) shows that polynomials and quadratic/cubic splines achieve the rate \\(o\\left(K^{-2}\\right)\\) and linear splines achieve the rate \\(O\\left(K^{-2}\\right)\\). For most of of our results the latter bound will be sufficient.\nMore generally, Theorem \\(20.1\\) makes a distinction between polynomials and splines as polynomials achieve the rate \\(o\\left(K^{-\\alpha}\\right)\\) adaptively (without input from the user) while splines achieve the rate \\(o\\left(K^{-\\alpha}\\right)\\) only if the spline order \\(s\\) is appropriately chosen. This is an advantage for polynomials. However, as emphasized by Schumaker (2007), splines simultaneously approximate the derivatives \\(m^{(q)}(x)\\) for \\(q<\\) \\(\\alpha\\). Thus, for example, a quadratic spline simultaneously approximates the function \\(m(x)\\) and its first derivative \\(m^{\\prime}(x)\\). There is no comparable result for polynomials. This is an advantage for quadratic and cubic splines. Since economists are often more interested in marginal effects (derivatives) than in levels this may be a good reason to prefer splines over polynomials.\nTheorem \\(20.1\\) is a bound on the best uniform approximation error. The coefficient \\(\\beta_{K}^{*}\\) which minimizes (20.11) is not, however, the projection coefficient \\(\\beta_{K}\\) as defined in (20.3). Thus Theorem \\(20.1\\) does not directly inform us concerning the approximation error obtained by series regression. It turns out, however, that the projection error can be easily deduced from (20.11).\nDefinition 20.1 The projection approximation error is\n\\[\nr_{K}(x)=m(x)-X_{K}(x)^{\\prime} \\beta_{K}\n\\]\nwhere the coefficient \\(\\beta_{K}\\) is the projection coefficient (20.3). The realized projection approximation error is \\(r_{K}=r_{K}(X)\\). The expected squared projection error is\n\\[\n\\delta_{K}^{2}=\\mathbb{E}\\left[r_{K}^{2}\\right] .\n\\]\nThe projection approximation error is similar to (20.9) but evaluated using the projection coefficient rather than the minimizing coefficient \\(\\beta_{K}^{*}\\) (20.8). Assuming that \\(X\\) has compact support \\(S\\) the expected squared projection error satisfies\n\\[\n\\begin{aligned}\n\\delta_{K} &=\\left(\\int_{S}\\left(m(x)-X_{K}(x)^{\\prime} \\beta_{K}\\right)^{2} d F(x)\\right)^{1 / 2} \\\\\n& \\leq\\left(\\int_{S}\\left(m(x)-X_{K}(x)^{\\prime} \\beta_{K}^{*}\\right)^{2} d F(x)\\right)^{1 / 2} \\\\\n& \\leq\\left(\\int_{S} \\delta_{K}^{* 2} d F(x)\\right)^{1 / 2} \\\\\n&=\\delta_{K}^{*} .\n\\end{aligned}\n\\]\nThe first inequality holds because the projection coefficient \\(\\beta_{K}\\) minimizes the expected squared projection error (see Section 2.25). The second inequality is the definition of \\(\\delta_{K}^{*}\\). Combined with Theorem \\(20.1\\) we have established the following result.\nTheorem \\(20.2\\) If \\(X\\) has compact support \\(S\\), for some \\(\\alpha \\geq 0 m^{(\\alpha)}(x)\\) is uniformly continuous on \\(S\\), and \\(X_{K}(x)\\) is either a polynomial basis or a spline basis of order \\(s \\geq \\alpha\\), then as \\(K \\rightarrow \\infty\\)\n\\[\n\\delta_{K} \\leq \\delta_{K}^{*} \\leq o\\left(K^{-\\alpha}\\right) .\n\\]\nFurthermore, if \\(m^{(2)}(x)\\) is uniformly continuous on \\(S\\) and \\(X_{K}(x)\\) is a linear spline basis, then \\(\\delta_{K} \\leq O\\left(K^{-2}\\right)\\).\nThe available theory of the approximation of functions goes beyond the results described here. For example, there is a theory of weighted polynomial approximation (Mhaskar, 1996) which provides an analog of Theorem \\(20.2\\) for the unbounded real line when \\(X\\) has a density with exponential tails."
  },
  {
    "objectID": "chpt20-series-reg.html#regressor-bounds",
    "href": "chpt20-series-reg.html#regressor-bounds",
    "title": "19  Series Regression",
    "section": "19.9 Regressor Bounds",
    "text": "19.9 Regressor Bounds\nThe approximation result in Theorem \\(20.2\\) assumes that the regressors \\(X\\) have bounded support \\(S\\). This is conventional in series regression theory as it greatly simplifies the analysis. Bounded support implies that the regressor function \\(X_{K}(x)\\) is bounded. Define\n\\[\n\\begin{gathered}\n\\zeta_{K}(x)=\\left(X_{K}(x)^{\\prime} \\boldsymbol{Q}_{K}^{-1} X_{K}(x)\\right)^{1 / 2} \\\\\n\\zeta_{K}=\\sup _{x} \\zeta_{K}(x)\n\\end{gathered}\n\\]\nwhere \\(\\boldsymbol{Q}_{K}=\\mathbb{E}\\left[X_{K} X_{K}^{\\prime}\\right]\\) is the population design matrix given the regressors \\(X_{K}\\). This implies that for all realizations of \\(X_{K}\\)\n\\[\n\\left(X_{K}^{\\prime} \\boldsymbol{Q}_{K}^{-1} X_{K}\\right)^{1 / 2} \\leq \\zeta_{K} .\n\\]\nThe constant \\(\\zeta_{K}(x)\\) is the normalized length of the regressor vector \\(X_{K}(x)\\). The constant \\(\\zeta_{K}\\) is the maximum normalized length. Their values are determined by the basis function transformations and the distribution of \\(X\\). They are invariant to rescaling \\(X_{K}\\) or linear rotations.\nFor polynomials and splines we have explicit expressions for the rate at which \\(\\zeta_{K}\\) grows with \\(K\\). Theorem 20.3 If \\(X\\) has compact support \\(S\\) with a strictly positive density \\(f(x)\\) on \\(S\\) then\n\n\\(\\zeta_{K} \\leq O(K)\\) for polynomials\n\\(\\zeta_{K} \\leq O\\left(K^{1 / 2}\\right)\\) for splines.\n\nFor a proof of Theorem \\(20.3\\) see Newey (1997, Theorem 4).\nFurthermore, when \\(X\\) is uniformly distributed then we can explicitly calculate for polynomials that \\(\\zeta_{K}=K\\), so the polynomial bound \\(\\zeta_{K} \\leq O(K)\\) cannot be improved.\nTo illustrate, we plot in Figure \\(20.4\\) the values \\(\\zeta_{K}(x)\\) for the case \\(X \\sim U[0,1]\\). We plot \\(\\zeta_{K}(x)\\) for a polynomial of degree \\(p=9\\) and a quadratic spline with \\(N=7\\) knots (both satisfy \\(K=10\\) ). You can see that the values of \\(\\zeta_{K}(x)\\) are close to 3 for both basis transformations and most values of \\(x\\), but \\(\\zeta_{K}(x)\\) increases sharply for \\(x\\) near the boundary. The maximum values are \\(\\zeta_{K}=10\\) for the polynomial and \\(\\zeta_{K}=7.4\\) for the quadratic spline. While Theorem \\(20.3\\) shows the two have different rates for large \\(K\\), we see for moderate \\(K\\) that the differences are relatively minor.\n\nFigure 20.4: Normalized Regressor Length"
  },
  {
    "objectID": "chpt20-series-reg.html#matrix-convergence",
    "href": "chpt20-series-reg.html#matrix-convergence",
    "title": "19  Series Regression",
    "section": "19.10 Matrix Convergence",
    "text": "19.10 Matrix Convergence\nOne of the challenges which arise when developing a theory for the least squares estimator is how to describe the large-sample behavior of the sample design matrix\n\\[\n\\widehat{\\boldsymbol{Q}}_{K}=\\frac{1}{n} \\sum_{i=1}^{n} X_{K i} X_{K i}^{\\prime}\n\\]\nas \\(K \\rightarrow \\infty\\). The difficulty is that its dimension changes with \\(K\\) so we cannot apply a standard WLLN.\nIt turns out to be convenient if we first rotate the regressor vector so that the elements are orthogonal in expectation. Thus we define the standardized regressors and design matrix as\n\\[\n\\begin{aligned}\n\\widetilde{X}_{K i} &=\\boldsymbol{Q}_{K}^{-1 / 2} X_{K i} \\\\\n\\widetilde{\\boldsymbol{Q}}_{K} &=\\frac{1}{n} \\sum_{i=1}^{n} \\widetilde{X}_{K i} \\widetilde{X}_{K i}^{\\prime} .\n\\end{aligned}\n\\]\nNote that \\(\\mathbb{E}\\left[\\widetilde{X}_{K} \\widetilde{X}_{K}^{\\prime}\\right]=\\boldsymbol{I}_{K}\\). The standardized regressors are not used in practice; they are introduced only to simplify the theoretical derivations.\nOur convergence theory will require the following fundamental rate bound on the number of coefficients \\(K\\).\nAssumption $20.1\n\n\\(\\lambda_{\\min }\\left(\\boldsymbol{Q}_{K}\\right) \\geq \\underline{\\lambda}>0\\)\n\\(\\zeta_{K}^{2} \\log (K) / n \\rightarrow 0\\) as \\(n, K \\rightarrow \\infty\\)\n\nAssumption 20.1.1 ensures that the transformation (20.18) is well defined \\({ }^{6}\\). Assumption 20.1.2 states that the squared maximum regressor length \\(\\zeta_{K}^{2}\\) grows slower than \\(n\\). Since \\(\\zeta_{K}\\) increases with \\(K\\) this is a bound on the rate at which \\(K\\) can increase with \\(n\\). By Theorem \\(20.2\\) the rate in Assumption \\(20.1 .2\\) holds for polynomials if \\(K^{2} \\log (K) / n \\rightarrow 0\\) and for splines if \\(K \\log (K) / n \\rightarrow 0\\). In either case, this means that the number of coefficients \\(K\\) is growing at a rate slower than \\(n\\).\nWe are now in a position to describe a convergence result for the standardized design matrix. The following is Lemma \\(6.2\\) of Belloni, Chernozhukov, Chetverikov, and Kato (2015).\nTheorem \\(20.4\\) If Assumption \\(20.1\\) holds then\n\\[\n\\left\\|\\widetilde{\\boldsymbol{Q}}_{K}-\\boldsymbol{I}_{K}\\right\\| \\stackrel{p}{\\longrightarrow} 0 .\n\\]\nA proof of Theorem \\(20.4\\) using a stronger condition than Assumption \\(20.1\\) can be found in Section 20.31. The norm in (20.19) is the spectral norm\n\\[\n\\|\\boldsymbol{A}\\|=\\left(\\lambda_{\\max }\\left(\\boldsymbol{A}^{\\prime} \\boldsymbol{A}\\right)\\right)^{1 / 2}\n\\]\n\\({ }^{6}\\) Technically, what is required is that \\(\\lambda_{\\min }\\left(\\boldsymbol{B}_{K} \\boldsymbol{Q}_{K} \\boldsymbol{B}_{K}^{\\prime}\\right) \\geq \\underline{\\lambda}>0\\) for some \\(K \\times K\\) sequence of matrices \\(\\boldsymbol{B}_{K}\\), or equivalently that Assumption 20.1.1 holds after replacing \\(X_{K}\\) with \\(\\boldsymbol{B}_{K} X_{K}\\). where \\(\\lambda_{\\max }(\\boldsymbol{B})\\) denotes the largest eigenvalue of the matrix \\(\\boldsymbol{B}\\). For a full description see Section A.23.\nFor the least squares estimator what is particularly important is the inverse of the sample design matrix. Fortunately we can easily deduce consistency of its inverse from (20.19) when the regressors have been orthogonalized as described.\nTheorem 20.5 If Assumption \\(20.1\\) holds then\n\\[\n\\left\\|\\widetilde{\\boldsymbol{Q}}_{K}^{-1}-\\boldsymbol{I}_{K}\\right\\| \\stackrel{p}{\\longrightarrow} 0\n\\]\nand\n\\[\n\\lambda_{\\max }\\left(\\widetilde{\\boldsymbol{Q}}_{K}^{-1}\\right)=1 / \\lambda_{\\min }\\left(\\widetilde{\\boldsymbol{Q}}_{K}\\right) \\stackrel{p}{\\longrightarrow} 1 .\n\\]\nThe proof of Theorem \\(20.5\\) can be found in Section \\(20.31\\)."
  },
  {
    "objectID": "chpt20-series-reg.html#consistent-estimation",
    "href": "chpt20-series-reg.html#consistent-estimation",
    "title": "19  Series Regression",
    "section": "19.11 Consistent Estimation",
    "text": "19.11 Consistent Estimation\nIn this section we give conditions for consistent estimation of \\(m(x)\\) by the series estimator \\(\\widehat{m}_{K}(x)=\\) \\(X_{K}(x)^{\\prime} \\widehat{\\beta}_{K}\\).\nWe know from standard regression theory that for any fixed \\(K, \\widehat{\\beta}_{K} \\stackrel{p}{\\rightarrow} \\beta_{K}\\) and thus \\(\\widehat{m}_{K}(x)=X_{K}(x)^{\\prime} \\widehat{\\beta}_{K} \\stackrel{p}{\\rightarrow}\\) \\(X_{K}(x)^{\\prime} \\beta_{K}\\) as \\(n \\rightarrow \\infty\\). Furthermore, from the Stone-Weierstrass Theorem we know that \\(X_{K}(x)^{\\prime} \\beta_{K} \\rightarrow m(x)\\) as \\(K \\rightarrow \\infty\\). It therefore seems reasonable to expect that \\(\\hat{m}_{K}(x) \\stackrel{p}{\\longrightarrow} m(x)\\) as both \\(n \\rightarrow \\infty\\) and \\(K \\rightarrow \\infty\\) together. Making this argument rigorous, however, is technically challenging, in part because the dimensions of \\(\\widehat{\\beta}_{K}\\) and its components are changing with \\(K\\).\nSince \\(\\widehat{m}_{K}(x)\\) and \\(m(x)\\) are functions, convergence should be defined with respect to an appropriate metric. For kernel regression we focused on pointwise convergence (for each value of \\(x\\) separately) as that is the simplest to analyze. For series regression it turns out to be simplest to describe convergence with respect to integrated squared error (ISE). We define the latter as\n\\[\n\\operatorname{ISE}(K)=\\int\\left(\\widehat{m}_{K}(x)-m(x)\\right)^{2} d F(x)\n\\]\nwhere \\(F\\) is the marginal distribution of \\(X\\). ISE \\((K)\\) is the average squared distance between \\(\\widehat{m}_{K}(x)\\) and \\(m(x)\\), weighted by the marginal distribution of \\(X\\). The ISE is random, depends on both sample size \\(n\\) and model complexity \\(K\\), and its distribution is determined by the joint distribution of \\((Y, X)\\). We can establish the following.\nTheorem 20.6 Under Assumption \\(20.1\\) and \\(\\delta_{K}=o(1)\\), then as \\(n, K \\rightarrow \\infty\\),\n\\[\n\\operatorname{ISE}(K)=o_{p}(1) .\n\\]\nThe proof of Theorem \\(20.6\\) can be found in Section \\(20.31\\).\nTheorem \\(20.6\\) shows that the series estimator \\(\\hat{m}_{K}(x)\\) is consistent in the ISE norm under mild conditions. The assumption \\(\\delta_{K}=o(1)\\) holds for polynomials and splines if \\(K \\rightarrow \\infty\\) and \\(m(x)\\) is uniformly continuous. This result is analogous to Theorem \\(19.8\\) which showed that kernel regression estimator is consistent if \\(m(x)\\) is continuous."
  },
  {
    "objectID": "chpt20-series-reg.html#convergence-rate",
    "href": "chpt20-series-reg.html#convergence-rate",
    "title": "19  Series Regression",
    "section": "19.12 Convergence Rate",
    "text": "19.12 Convergence Rate\nWe now give a rate of convergence.\nTheorem 20.7 Under Assumption \\(20.1\\) and \\(\\sigma^{2}(x) \\leq \\bar{\\sigma}^{2}<\\infty\\), then as \\(n, K \\rightarrow \\infty\\),\n\\[\n\\operatorname{ISE}(K) \\leq O_{p}\\left(\\delta_{K}^{2}+\\frac{K}{n}\\right)\n\\]\nwhere \\(\\delta_{K}^{2}\\) is the expected squared prediction error (20.13). Furthermore, if \\(m^{\\prime \\prime}(x)\\) is uniformly continuous then for polynomial or spline basis functions\n\\[\n\\operatorname{ISE}(K) \\leq O_{p}\\left(K^{-4}+\\frac{K}{n}\\right) .\n\\]\nThe proof of Theorem \\(20.7\\) can be found in Section 20.31. It is based on Newey (1997).\nThe bound (20.25) is particularly useful as it gives an explicit rate in terms of \\(K\\) and \\(n\\). The result shows that the integrated squared error is bounded in probability by two terms. The first \\(K^{-4}\\) is the squared bias. The second \\(K / n\\) is the estimation variance. This is analogous to the AIMSE for kernel regression (19.5). We can see that increasing the number of series terms \\(K\\) affects the integrated squared error by decreasing the bias but increasing the variance. The fact that the estimation variance is of order \\(K / n\\) can be intuitively explained by the fact that the regression model is estimating \\(K\\) coefficients.\nFor polynomials and quadratic splines the bound (20.25) can be written as \\(o_{p}\\left(K^{-4}\\right)+O_{p}(K / n)\\).\nWe are interested in the sequence \\(K\\) which minimizes the trade-off in (20.25). By examining the firstorder condition we find that the sequence which minimizes this bound is \\(K \\sim n^{1 / 5}\\). With this choice we obtain the optimal integrated squared error \\(\\operatorname{ISE}(K) \\leq O_{p}\\left(n^{-4 / 5}\\right)\\). This is the same convergence rate as obtained by kernel regression under similar assumptions.\nIt is interesting to contrast the optimal rate \\(K \\sim n^{1 / 5}\\) for series regression with \\(h \\sim n^{-1 / 5}\\) for kernel regression. Essentially, one can view \\(K^{-1}\\) in series regression as a “bandwidth” similar to kernel regression, or one can view \\(1 / h\\) in kernel regression as the effective number of coefficients.\nThe rate \\(K \\sim n^{1 / 5}\\) means that the optimal \\(K\\) increases very slowly with the sample size. For example, doubling your sample size implies a \\(15 %\\) increase in the optimal number of coefficients \\(K\\). To obtain a doubling in the optimal number of coefficients you need to multiply the sample size by 32.\nTo illustrate, Figure \\(20.5\\) displays the ISE rate bounds \\(K^{-4}+K / n\\) as a function of \\(K\\) for \\(n=10,30,150\\). The filled circles mark the ISE-minimizing \\(K\\), which are \\(K=2\\), 3, and 4 for the three functions. Notice that the ISE functions are steeply downward sloping for small \\(K\\) and nearly flat for large \\(K\\) (when \\(n\\) is large). This is because the bias term \\(K^{-4}\\) dominates for small values of \\(K\\) while the variance term \\(K / n\\) dominates for large values of \\(K\\) and the latter flattens as \\(n\\) increases."
  },
  {
    "objectID": "chpt20-series-reg.html#asymptotic-normality",
    "href": "chpt20-series-reg.html#asymptotic-normality",
    "title": "19  Series Regression",
    "section": "19.13 Asymptotic Normality",
    "text": "19.13 Asymptotic Normality\nTake a parameter \\(\\theta=a(m)\\) which is a real-valued linear function of the regression function. This includes the regression function \\(m(x)\\) at a given point \\(x\\), derivatives of \\(m(x)\\), and integrals over \\(m(x)\\). Given \\(\\widehat{m}_{K}(x)=X_{K}(x)^{\\prime} \\widehat{\\beta}_{K}\\) as an estimator for \\(m(x)\\), the estimator for \\(\\theta\\) is \\(\\widehat{\\theta}_{K}=a\\left(\\widehat{m}_{K}\\right)=a_{K}^{\\prime} \\widehat{\\beta}_{K}\\) for some \\(K \\times 1\\) vector of constants \\(a_{K} \\neq 0\\). (The relationship \\(a\\left(\\widehat{m}_{K}\\right)=a_{K}^{\\prime} \\widehat{\\beta}_{K}\\) follows because \\(a\\) is linear in \\(m\\) and \\(\\widehat{m}_{K}\\) is linear in \\(\\widehat{\\beta}_{K}\\).)\n\nFigure 20.5: Integrated Squared Error\nIf \\(K\\) were fixed as \\(n \\rightarrow \\infty\\) then by standard asymptotic theory we would expect \\(\\widehat{\\theta}_{K}\\) to be asymptotically normal with variance \\(V_{K}=a_{K}^{\\prime} \\boldsymbol{Q}_{K}^{-1} \\Omega_{K} \\boldsymbol{Q}_{K}^{-1} a_{K}\\) where \\(\\Omega_{K}=\\mathbb{E}\\left[X_{K} X_{K}^{\\prime} e^{2}\\right]\\). The standard justification, however, is not valid in the nonparametric case. This is in part because \\(V_{K}\\) may diverge as \\(K \\rightarrow \\infty\\), and in part due to the finite sample bias due to the approximation error. Therefore a new theory is required. Interestingly, it turns out that in the nonparametric case \\(\\widehat{\\theta}_{K}\\) is still asymptotically normal and \\(V_{K}\\) is still the appropriate variance for \\(\\widehat{\\theta}_{K}\\). The proof is different than the parametric case as the dimensions of the matrices are increasing with \\(K\\) and we need to be attentive to the estimator’s bias due to the series approximation.\n\nAssumption 20.2.1 is conditional square integrability. It implies that the conditional variance \\(\\mathbb{E}\\left[e^{2} \\mid X\\right]\\) is bounded. It is used to verify the Lindeberg condition for the CLT. Assumption 20.2.2 states that the conditional variance is nowhere degenerate. Thus there is no \\(X\\) for which \\(Y\\) is perfectly predictable. This is a technical condition used to bound \\(V_{K}\\) from below.\nAssumption 20.2.3 states that approximation error \\(\\delta_{K}\\) declines faster than the maximal regressor length \\(\\zeta_{K}\\). For polynomials a sufficient condition for this assumption is that \\(m^{\\prime \\prime}(x)\\) is uniformly continuous. For splines a sufficient condition is that \\(m^{\\prime}(x)\\) is uniformly continuous.\nTheorem 20.8 Under Assumption 20.2, as \\(n \\rightarrow \\infty\\),\n\\[\n\\frac{\\sqrt{n}\\left(\\widehat{\\theta}_{K}-\\theta+a\\left(r_{K}\\right)\\right)}{V_{K}^{1 / 2}} \\underset{d}{\\longrightarrow} \\mathrm{N}(0,1) .\n\\]\nThe proof of Theorem \\(20.8\\) can be found in Section \\(20.31\\).\nTheorem \\(20.8\\) shows that the estimator \\(\\widehat{\\theta}_{K}\\) is approximately normal with bias \\(-a\\left(r_{K}\\right)\\) and variance \\(V_{K} / n\\). The variance is the same as in the parametric case. The asymptotic bias is similar to that found in kernel regression.\nOne useful message from Theorem \\(20.8\\) is that the classical variance formula \\(V_{K}\\) for \\(\\widehat{\\theta}_{K}\\) applies to series regression. This justifies conventional estimators for \\(V_{K}\\) as will be discussed in Section \\(20.18\\).\nTheorem \\(20.8\\) shows that the estimator \\(\\widehat{\\theta}_{K}\\) has a bias \\(a\\left(r_{K}\\right)\\). What is this? It is the same transformation of the function \\(r_{K}(x)\\) as \\(\\theta=a(m)\\) is of the regression function \\(m(x)\\). For example, if \\(\\theta=m(x)\\) is the regression at a fixed point \\(x\\) then \\(a\\left(r_{K}\\right)=r_{K}(x)\\), the approximation error at the same point. If \\(\\theta=m^{\\prime}(x)\\) is the regression derivative then \\(a\\left(r_{K}\\right)=r_{K}^{\\prime}(x)\\) is the derivative of the approximation error.\nThis means that the bias in the estimator \\(\\widehat{\\theta}_{K}\\) for \\(\\theta\\) shown in Theorem \\(20.8\\) is simply the approximation error transformed by the functional of interest. If we are estimating the regression function then the bias is the error in approximating the regression function; if we are estimating the regression derivative then the bias is the error in the derivative in the approximation error for the regression function."
  },
  {
    "objectID": "chpt20-series-reg.html#regression-estimation",
    "href": "chpt20-series-reg.html#regression-estimation",
    "title": "19  Series Regression",
    "section": "19.14 Regression Estimation",
    "text": "19.14 Regression Estimation\nA special yet important example of a linear estimator is the regression function at a fixed point \\(x\\). In the notation of the previous section, \\(a(m)=m(x)\\) and \\(a_{K}=X_{K}(x)\\). The series estimator of \\(m(x)\\) is \\(\\widehat{\\theta}_{K}=\\widehat{m}_{K}(x)=X_{K}(x)^{\\prime} \\widehat{\\beta}_{K}\\). As this is a key problem of interest we restate the asymptotic result of Theorem \\(20.8\\) for this estimator.\nTheorem 20.9 Under Assumption 20.2, as \\(n \\rightarrow \\infty\\),\n\\[\n\\frac{\\sqrt{n}\\left(\\hat{m}_{K}(x)-m(x)+r_{K}(x)\\right)}{V_{K}^{1 / 2}(x)} \\underset{d}{\\longrightarrow} \\mathrm{N}(0,1)\n\\]\nwhere \\(V_{K}(x)=X_{K}(x)^{\\prime} \\boldsymbol{Q}_{K}^{-1} \\Omega_{K} \\boldsymbol{Q}_{K}^{-1} X_{K}(x)\\).\nThere are several important features about the asymptotic distribution (20.27).\nFirst, as mentioned in the previous section it shows that the classical variance formula \\(V_{K}(x)\\) applies for the series estimator \\(\\widehat{m}_{K}(x)\\). Second, (20.27) shows that the estimator has the asymptotic bias \\(r_{K}(x)\\). This is due to the fact that the finite order series is an approximation to the unknown regression function \\(m(x)\\) and this results in finite sample bias.\nThere is another fascinating connection between the asymptotic variance of Theorem \\(20.9\\) and the regression lengths \\(\\zeta_{K}(x)\\) of (20.15). Under conditional homoskedasticity we have the simplification \\(V_{K}(x)=\\sigma^{2} \\zeta_{K}(x)^{2}\\). Thus the asymptotic variance of the regression estimator is proportional to the squared regression lengths. From Figure \\(20.4\\) we learned that the regression length \\(\\zeta_{K}(x)\\) is much higher at the edge of the support of the regressors, especially for polynomials. This means that the precision of the series regression estimator is considerably degraded at the edge of the support."
  },
  {
    "objectID": "chpt20-series-reg.html#undersmoothing",
    "href": "chpt20-series-reg.html#undersmoothing",
    "title": "19  Series Regression",
    "section": "19.15 Undersmoothing",
    "text": "19.15 Undersmoothing\nAn unpleasant aspect about Theorem \\(20.9\\) is the bias term. An interesting trick is that this bias term can be made asymptotically negligible if we assume that \\(K\\) increases with \\(n\\) at a sufficiently fast rate.\nTheorem 20.10 Under Assumption 20.2, if in addition \\(n \\delta_{K}^{* 2} \\rightarrow 0\\) then\n\\[\n\\frac{\\sqrt{n}\\left(\\widehat{m}_{K}(x)-m(x)\\right)}{V_{K}^{1 / 2}(x)} \\underset{d}{\\longrightarrow} \\mathrm{N}(0,1) \\text {. }\n\\]\nThe condition \\(n \\delta_{K}^{* 2} \\rightarrow 0\\) implies that the squared bias converges faster than the estimation variance so the former is asymptotically negligible. If \\(m^{\\prime \\prime}(x)\\) is uniformly continuous then a sufficient condition for polynomials and quadratic splines is \\(K \\sim n^{1 / 4}\\). For linear splines a sufficient condition is for \\(K\\) to diverge faster than \\(K^{1 / 4}\\). The rate \\(K \\sim n^{1 / 4}\\) is somewhat faster than the ISE-optimal rate \\(K \\sim n^{1 / 5}\\).\nThe assumption \\(n \\delta_{K}^{* 2} \\rightarrow 0\\) is often stated by authors as an innocuous technical condition. This is misleading as it is a technical trick and should be discussed explicitly. The reason why the assumption eliminates the bias from (20.28) is that the assumption forces the estimation variance to dominate the squared bias so that the latter can be ignored. This means that the estimator itself is inefficient.\nBecause \\(n \\delta_{K}^{* 2} \\rightarrow 0\\) means that \\(K\\) is larger than optimal we say that \\(\\widehat{m}_{K}(x)\\) is undersmoothed relative to the optimal series estimator.\nMany authors like to focus their asymptotic theory on the assumptions in Theorem \\(20.10\\) as the distribution (20.28) appears cleaner. However, it is a poor use of asymptotic theory. There are three problems with the assumption \\(n \\delta_{K}^{* 2} \\rightarrow 0\\) and the approximation (20.28). First, the estimator \\(\\widehat{m}_{K}(x)\\) is inefficient. Second, while the assumption \\(n \\delta_{K}^{* 2} \\rightarrow 0\\) makes the bias of lower order than the variance it only makes the bias of slightly lower order, meaning that the accuracy of the asymptotic approximation is poor. Effectively, the estimator is still biased in finite samples. Third, \\(n \\delta_{K}^{* 2} \\rightarrow 0\\) is an assumption not a rule for empirical practice. It is unclear what the statement “Assume \\(n \\delta_{K}^{* 2} \\rightarrow 0\\)” means in a practical application. From this viewpoint the difference between (20.26) and (20.28) is in the assumptions not in the actual reality nor in the actual empirical practice. Eliminating a nuisance (the asymptotic bias) through an assumption is a trick not a substantive use of theory. My strong view is that the result (20.26) is more informative than (20.28). It shows that the asymptotic distribution is normal but has a non-trivial finite sample bias."
  },
  {
    "objectID": "chpt20-series-reg.html#residuals-and-regression-fit",
    "href": "chpt20-series-reg.html#residuals-and-regression-fit",
    "title": "19  Series Regression",
    "section": "19.16 Residuals and Regression Fit",
    "text": "19.16 Residuals and Regression Fit\nThe fitted regression at \\(x=X_{i}\\) is \\(\\widehat{m}_{K}\\left(X_{i}\\right)=X_{K i}^{\\prime} \\widehat{\\beta}_{K}\\) and the fitted residual is \\(\\widehat{e}_{K i}=Y_{i}-\\widehat{m}_{K}\\left(X_{i}\\right)\\). The leave-one-out prediction errors are\n\\[\n\\widetilde{e}_{K i}=Y_{i}-\\widehat{m}_{K,-i}\\left(X_{i}\\right)=Y_{i}-X_{K i}^{\\prime} \\widehat{\\beta}_{K,-i}\n\\]\nwhere \\(\\widehat{\\beta}_{K,-i}\\) is the least squares coefficient with the \\(i^{t h}\\) observation omitted. Using (3.44) we have the simple computational formula\n\\[\n\\widetilde{e}_{K i}=\\widehat{e}_{K i}\\left(1-X_{K i}^{\\prime}\\left(\\boldsymbol{X}_{K}^{\\prime} \\boldsymbol{X}_{K}\\right)^{-1} X_{K i}\\right)^{-1} .\n\\]\nAs for kernel regression the prediction errors \\(\\widetilde{e}_{K i}\\) are better estimators of the errors than the fitted residuals \\(\\widehat{e}_{K i}\\) as the former do not have the tendency to over-fit when the number of series terms is large."
  },
  {
    "objectID": "chpt20-series-reg.html#cross-validation-model-selection",
    "href": "chpt20-series-reg.html#cross-validation-model-selection",
    "title": "19  Series Regression",
    "section": "19.17 Cross-Validation Model Selection",
    "text": "19.17 Cross-Validation Model Selection\nA common method for selection of the number of series terms \\(K\\) is cross-validation. The crossvalidation criterion is the \\(\\operatorname{sum}^{7}\\) of squared prediction errors\n\\[\n\\operatorname{CV}(K)=\\sum_{i=1}^{n} \\widetilde{e}_{K i}^{2}=\\sum_{i=1}^{n} \\widehat{e}_{K i}^{2}\\left(1-X_{K i}^{\\prime}\\left(\\boldsymbol{X}_{K}^{\\prime} \\boldsymbol{X}_{K}\\right)^{-1} X_{K i}\\right)^{-2} .\n\\]\nThe CV-selected value of \\(K\\) is the integer which minimizes \\(\\mathrm{CV}(K)\\).\nAs shown in Theorem \\(19.7 \\mathrm{CV}(K)\\) is an approximately unbiased estimator of the integrated meansquared error (IMSE), which is the expected integrated squared error (ISE). The proof of the result is the same for all nonparametric estimators (series as well as kernels) so does not need to be repeated here. Therefore, finding the \\(K\\) which produces the smallest value of \\(\\mathrm{CV}(K)\\) is a good indicator that the estimator \\(\\widehat{m}_{K}(x)\\) has small IMSE.\nFor practical implementation we first designate a set of models (sets of basis transformations and number of variables \\(K\\) ) over which to search. (For example, polynomials of order 1 through \\(K_{\\max }\\) for some pre-selected \\(K_{\\max }\\).) For each, there is a set of regressors \\(X_{K}\\) which are obtained by transformations of the original variables \\(X\\). For each set we estimate the regression by least squares, calculate the leaveone-out prediction errors, and the CV criterion. Since the errors are a linear operation this is a simple calculation. The CV-selected \\(K\\) is the integer which produces the smallest value of \\(\\mathrm{CV}(K)\\). \\(\\operatorname{Plots}\\) of \\(\\mathrm{CV}(K)\\) against \\(K\\) can aid assessment and interpretation. Since the model order \\(K\\) is an integer the CV criterion for series regression is a discrete function, unlike the case of kernel regression.\nIf it is desired to produce an estimator \\(\\widehat{m}_{K}(x)\\) with reduced bias it may be preferred to select a value of \\(K\\) slightly higher than that selected by CV alone.\nTo illustrate, in Figure \\(20.6\\) we plot the cross-validation functions for the polynomial regression estimates from Figure 20.1. The lowest point marks the polynomial order which minimizes the crossvalidation function. In panel (a) we plot the CV function for the sub-sample of white women. Here we see that the CV-selected order is \\(p=3\\), a cubic polynomial. In panel (b) we plot the CV function for the sub-sample of Black women, and find that the CV-selected order is \\(p=2\\), a quadratic. As expected from visual examination of Figure 20.1, the selected model is more parsimonious for panel (b), most likely because it has a substantially smaller sample size. What may be surprising is that even for panel (a), which has a large sample and smooth estimates, the CV-selected model is still relatively parsimonious.\nA user who desires a reduced bias estimator might increase the polynomial orders to \\(p=4\\) or even \\(p=5\\) for the subsample of white women and to \\(p=3\\) or \\(p=4\\) for the subsample of Black women. Both CV functions are relatively similar across these values.\n\\({ }^{7}\\) Some authors define \\(\\mathrm{CV}(K)\\) as the average rather than the sum.\n\n\nWhite Women\n\n\n\nBlack Women\n\nFigure 20.6: Cross-Validation Functions for Polynomial Estimates of Experience Profile"
  },
  {
    "objectID": "chpt20-series-reg.html#variance-and-standard-error-estimation",
    "href": "chpt20-series-reg.html#variance-and-standard-error-estimation",
    "title": "19  Series Regression",
    "section": "19.18 Variance and Standard Error Estimation",
    "text": "19.18 Variance and Standard Error Estimation\nThe exact conditional variance of the least squares estimator \\(\\widehat{\\beta}_{K}\\) under independent sampling is\n\\[\n\\boldsymbol{V}_{\\widehat{\\beta}}=\\left(\\boldsymbol{X}_{K}^{\\prime} \\boldsymbol{X}_{K}\\right)^{-1}\\left(\\sum_{i=1}^{n} X_{K i} X_{K i}^{\\prime} \\sigma^{2}\\left(X_{i}\\right)\\right)\\left(\\boldsymbol{X}_{K}^{\\prime} \\boldsymbol{X}_{K}\\right)^{-1} .\n\\]\nThe exact conditional variance for the conditional mean estimator \\(\\widehat{m}_{K}(x)=X_{K}(x)^{\\prime} \\widehat{\\beta}_{K}\\) is\n\\[\nV_{K}(x)=X_{K}(x)^{\\prime}\\left(\\boldsymbol{X}_{K}^{\\prime} \\boldsymbol{X}_{K}\\right)^{-1}\\left(\\sum_{i=1}^{n} X_{K i} X_{K i}^{\\prime} \\sigma^{2}\\left(X_{i}\\right)\\right)\\left(\\boldsymbol{X}_{K}^{\\prime} \\boldsymbol{X}_{K}\\right)^{-1} X_{K}(x) .\n\\]\nUsing the notation of Section \\(20.7\\) this equals\n\\[\n\\frac{1}{n^{2}} \\sum_{i=1}^{n} \\widehat{w}_{K}\\left(x, X_{i}\\right)^{2} \\sigma^{2}\\left(X_{i}\\right) .\n\\]\nIn the case of conditional homoskedasticity the latter simplifies to\n\\[\n\\frac{1}{n} \\widehat{w}_{K}(x, x) \\sigma^{2} \\simeq \\frac{1}{n} \\zeta_{K}(x)^{2} \\sigma^{2} .\n\\]\nwhere \\(\\zeta_{K}(x)\\) is the normalized regressor length defined in (20.15). Under conditional heteroskedasticty, large samples, and \\(K\\) large (so that \\(\\widehat{w}_{K}\\left(x, X_{i}\\right)\\) is a local kernel) it approximately equals\n\\[\n\\frac{1}{n} w_{K}(x, x) \\sigma^{2}(x)=\\frac{1}{n} \\zeta_{K}(x)^{2} \\sigma^{2}(x) .\n\\]\nIn either case we find that the variance is approximately\n\\[\nV_{K}(x) \\simeq \\frac{1}{n} \\zeta_{K}(x)^{2} \\sigma^{2}(x) .\n\\]\nThis shows that the variance of the series regression estimator is a scale of \\(\\zeta_{K}(x)^{2}\\) and the conditional variance. From the plot of \\(\\zeta_{K}(x)\\) shown in Figure \\(20.4\\) we can deduce that the series regression estimator will be relatively imprecise at the boundary of the support of \\(X\\).\nThe estimator of (20.31) recommended by Andrews (1991a) is the HC3 estimator\n\\[\n\\widehat{\\boldsymbol{V}}_{\\widehat{\\beta}}=\\left(\\boldsymbol{X}_{K}^{\\prime} \\boldsymbol{X}_{K}\\right)^{-1}\\left(\\sum_{i=1}^{n} X_{K i} X_{K i}^{\\prime} \\widetilde{e}_{K i}^{2}\\right)\\left(\\boldsymbol{X}_{K}^{\\prime} \\boldsymbol{X}_{K}\\right)^{-1}\n\\]\nwhere \\(\\widetilde{e}_{K i}\\) is the leave-one-out prediction error (20.29). Alternatives include the HC1 or HC2 estimators.\nGiven (20.32) a variance estimator for \\(\\widehat{m}_{K}(x)=X_{K}(x)^{\\prime} \\widehat{\\beta}_{K}\\) is\n\\[\n\\widehat{V}_{K}(x)=X_{K}(x)^{\\prime}\\left(\\boldsymbol{X}_{K}^{\\prime} \\boldsymbol{X}_{K}\\right)^{-1}\\left(\\sum_{i=1}^{n} X_{K i} X_{K i}^{\\prime} \\widetilde{e}_{K i}^{2}\\right)\\left(\\boldsymbol{X}_{K}^{\\prime} \\boldsymbol{X}_{K}\\right)^{-1} X_{K}(x) .\n\\]\nA standard error for \\(\\widehat{m}(x)\\) is the square root of \\(\\widehat{V}_{K}(x)\\)."
  },
  {
    "objectID": "chpt20-series-reg.html#clustered-observations",
    "href": "chpt20-series-reg.html#clustered-observations",
    "title": "19  Series Regression",
    "section": "19.19 Clustered Observations",
    "text": "19.19 Clustered Observations\nClustered observations are \\(\\left(Y_{i g}, X_{i g}\\right)\\) for individuals \\(i=1, \\ldots, n_{g}\\) in cluster \\(g=1, \\ldots, G\\). The model is\n\\[\n\\begin{aligned}\nY_{i g} &=m\\left(X_{i g}\\right)+e_{i g} \\\\\n\\mathbb{E}\\left[e_{i g} \\mid \\boldsymbol{X}_{g}\\right] &=0\n\\end{aligned}\n\\]\nwhere \\(\\boldsymbol{X}_{g}\\) is the stacked \\(X_{i g}\\). Stack \\(Y_{i g}\\) and \\(e_{i g}\\) into cluster-level variables \\(\\boldsymbol{Y}_{g}\\) and \\(\\boldsymbol{e}_{g}\\).\nThe series regression model using cluster-level notation is \\(\\boldsymbol{Y}_{g}=\\boldsymbol{X}_{g} \\beta_{K}+\\boldsymbol{e}_{K g}\\). We can write the series estimator as\n\\[\n\\widehat{\\beta}_{K}=\\left(\\sum_{g=1}^{G} \\boldsymbol{X}_{g}^{\\prime} \\boldsymbol{X}_{g}\\right)^{-1}\\left(\\sum_{g=1}^{G} \\boldsymbol{X}_{g}^{\\prime} \\boldsymbol{Y}_{g}\\right) .\n\\]\nThe cluster-level residual vector is \\(\\widehat{\\boldsymbol{e}}_{g}=\\boldsymbol{Y}_{g}-\\boldsymbol{X}_{g} \\widehat{\\beta}_{K}\\).\nAs for parametric regression with clustered observations the standard assumption is that the clusters are mutually independent but dependence within each cluster is unstructured. We therefore use the same variance formulae as used for parametric regression. The standard estimator is\n\\[\n\\widehat{\\boldsymbol{V}}_{\\widehat{\\beta}}^{\\mathrm{CR1}}=\\left(\\frac{G}{G-1}\\right)\\left(\\boldsymbol{X}_{K}^{\\prime} \\boldsymbol{X}_{K}\\right)^{-1}\\left(\\sum_{g=1}^{G} \\boldsymbol{X}_{g}^{\\prime} \\widehat{\\boldsymbol{e}}_{g} \\widehat{\\boldsymbol{e}}_{g}^{\\prime} \\boldsymbol{X}_{g}\\right)\\left(\\boldsymbol{X}_{K}^{\\prime} \\boldsymbol{X}_{K}\\right)^{-1} .\n\\]\nAn alternative is to use the delete-cluster prediction error \\(\\widetilde{\\boldsymbol{e}}_{g}=\\boldsymbol{Y}_{g}-\\boldsymbol{X}_{g} \\widetilde{\\beta}_{K,-g}\\) where\n\\[\n\\widetilde{\\beta}_{K,-g}=\\left(\\sum_{j \\neq g} \\boldsymbol{X}_{j}^{\\prime} \\boldsymbol{X}_{j}\\right)^{-1}\\left(\\sum_{j \\neq g} \\boldsymbol{X}_{j}^{\\prime} \\boldsymbol{Y}_{j}\\right)\n\\]\nleading to the estimator\n\\[\n\\widehat{\\boldsymbol{V}}_{\\widehat{\\beta}}^{\\mathrm{CR} 3}=\\left(\\boldsymbol{X}_{K}^{\\prime} \\boldsymbol{X}_{K}\\right)^{-1}\\left(\\sum_{g=1}^{G} \\boldsymbol{X}_{g}^{\\prime} \\widetilde{\\boldsymbol{e}}_{g} \\widetilde{\\boldsymbol{e}}_{g}^{\\prime} \\boldsymbol{X}_{g}\\right)\\left(\\boldsymbol{X}_{K}^{\\prime} \\boldsymbol{X}_{K}\\right)^{-1} .\n\\]\nThere is no current theory on how to select the number of series terms \\(K\\) for clustered observations. A reasonable choice is to minimize the delete-cluster cross-validation criterion \\(\\mathrm{CV}(K)=\\sum_{g=1}^{G} \\widetilde{\\boldsymbol{e}}_{g}^{\\prime} \\widetilde{\\boldsymbol{e}}_{g}\\)."
  },
  {
    "objectID": "chpt20-series-reg.html#confidence-bands",
    "href": "chpt20-series-reg.html#confidence-bands",
    "title": "19  Series Regression",
    "section": "19.20 Confidence Bands",
    "text": "19.20 Confidence Bands\nWhen displaying nonparametric estimators such as \\(\\widehat{m}_{K}(x)\\) it is customary to display confidence intervals. An asymptotic pointwise \\(95 %\\) confidence interval for \\(m(x)\\) is \\(\\widehat{m}_{K}(x) \\pm 1.96 \\widehat{V}_{K}^{1 / 2}(x)\\). These confidence intervals can be plotted along with \\(\\widehat{m}_{K}(x)\\).\nTo illustrate, Figure \\(20.7\\) plots polynomial estimates of the regression of \\(\\log (\\) wage \\()\\) on experience using the selected estimates from Figure 20.1, plus 95% confidence bands. Panel (a) plots the estimate for the subsample of white women using \\(p=5\\). Panel (b) plots the estimate for the subsample of Black women using \\(p=3\\). The standard errors are calculated using the formula (20.33). You can see that the confidence bands widen at the boundaries. The confidence bands are tight for the larger subsample of white women, and significantly wider for the smaller subsample of Black women. Regardless, both plots indicate that the average wage rises for experience levels up to about 20 years and then flattens for experience levels above 20 years.\n\n\nWhite Women\n\n\n\nBlack Women\n\nFigure 20.7: Polynomial Estimates with 95% Confidence Bands\nThere are two deficiencies with these confidence bands. First, they do not take into account the bias \\(r_{K}(x)\\) of the series estimator. Consequently, we should interpret the confidence bounds as valid for the pseudo-true regression (the best finite \\(K\\) approximation) rather than the true regression function \\(m(x)\\). Second, the above confidence intervals are based on a pointwise (in \\(x\\) ) asymptotic distribution theory. Consequently we should interpret their coverage as having pointwise validity and be cautious about interpreting global shapes from the confidence bands."
  },
  {
    "objectID": "chpt20-series-reg.html#uniform-approximations",
    "href": "chpt20-series-reg.html#uniform-approximations",
    "title": "19  Series Regression",
    "section": "19.21 Uniform Approximations",
    "text": "19.21 Uniform Approximations\nSince \\(\\widehat{m}_{K}(x)\\) is a function it is desirable to have a distribution theory which applies to the entire function, not just the estimator at a point. This can be used, for example, to construct confidence bands with uniform (in \\(x\\) ) coverage properties. For those familiar with empirical process theory, it might be hoped that the stochastic process\n\\[\n\\eta_{K}(x)=\\frac{\\sqrt{n}\\left(\\widehat{m}_{K}(x)-m(x)\\right)}{V_{K}^{1 / 2}(x)}\n\\]\nmight converge to a stochastic (Gaussian) process, but this is not the case. Effectively, the process \\(\\eta_{K}(x)\\) is not stochastically equicontinuous so conventional empirical process theory does not apply.\nTo develop a uniform theory, Belloni, Chernozhukov, Chetverikov, and Kato (2015) have introduced what are known as strong approximations. Their method shows that \\(\\eta_{K}(x)\\) is equal in distribution to a sequence of Gaussian processes plus a negligible error. Their theory (Theorem 4.4) takes the following form. Under stronger conditions than Assumption \\(20.2\\)\n\\[\n\\eta_{K}(x)={ }_{d} \\frac{X_{K}(x)^{\\prime}\\left(\\boldsymbol{Q}_{K}^{-1} \\Omega_{K} \\boldsymbol{Q}_{K}^{-1}\\right)^{1 / 2}}{V_{K}^{1 / 2}(x)} G_{K}+o_{p}(1)\n\\]\nuniformly in \\(x\\), where ” \\(=d\\) ” means “equality in distribution” and \\(G_{K} \\sim \\mathrm{N}\\left(0, \\boldsymbol{I}_{K}\\right)\\).\nThis shows the distributional result in Theorem \\(20.10\\) can be interpreted as holding uniformly in \\(x\\). It can also be used to develop confidence bands (different from those from the previous section) with asymptotic uniform coverage."
  },
  {
    "objectID": "chpt20-series-reg.html#partially-linear-model",
    "href": "chpt20-series-reg.html#partially-linear-model",
    "title": "19  Series Regression",
    "section": "19.22 Partially Linear Model",
    "text": "19.22 Partially Linear Model\nA common use of a series regression is to allow \\(m(x)\\) to be nonparametric with respect to one variable yet linear in the other variables. This allows flexibility in a particular variable of interest. A partially linear model with vector-valued regressor \\(X_{1}\\) and real-valued continuous \\(X_{2}\\) takes the form\n\\[\nm\\left(x_{1}, x_{2}\\right)=x_{1}^{\\prime} \\beta_{1}+m_{2}\\left(x_{2}\\right) .\n\\]\nThis model is common when \\(X_{1}\\) are discrete (e.g. binary) and \\(X_{2}\\) is continuously distributed.\nSeries methods are convenient for partially linear models as we can replace the unknown function \\(m_{2}\\left(x_{2}\\right)\\) with a series expansion to obtain\n\\[\nm(X) \\simeq m_{K}(X)=X_{1}^{\\prime} \\beta_{1}+X_{2 K}\\left(X_{2}\\right)^{\\prime} \\beta_{2 K}=X_{K}^{\\prime} \\beta_{K}\n\\]\nwhere \\(X_{2 K}=X_{2 K}\\left(x_{2}\\right)\\) are basis transformations of \\(x_{2}\\) (typically polynomials or splines). After transformation the regressors are \\(X_{K}=\\left(X_{1}^{\\prime}, X_{2 K}^{\\prime}\\right)\\) with coefficients \\(\\beta_{K}=\\left(\\beta_{1}^{\\prime}, \\beta_{2 K}^{\\prime}\\right)^{\\prime}\\)."
  },
  {
    "objectID": "chpt20-series-reg.html#panel-fixed-effects",
    "href": "chpt20-series-reg.html#panel-fixed-effects",
    "title": "19  Series Regression",
    "section": "19.23 Panel Fixed Effects",
    "text": "19.23 Panel Fixed Effects\nThe one-way error components nonparametric regression model is\n\\[\nY_{i t}=m\\left(X_{i t}\\right)+u_{i}+\\varepsilon_{i t}\n\\]\nfor \\(i=1, \\ldots, N\\) and \\(t=1, \\ldots, T\\). It is standard to treat the individual effect \\(u_{i}\\) as a fixed effect. This model can be interpreted as a special case of the partially linear model from the previous section though the dimension of \\(u_{i}\\) is increasing with \\(N\\).\nA series estimator approximates the function \\(m(x)\\) with \\(m_{K}(x)=X_{K}(x)^{\\prime} \\beta_{K}\\) as in (20.4). This leads to the series regression model \\(Y_{i t}=X_{K i t}^{\\prime} \\beta_{K}+u_{i}+\\varepsilon_{K i t}\\) where \\(X_{K i t}=X_{K}\\left(X_{i t}\\right)\\).\nThe fixed effects estimator is the same as in linear panel data regression. First, the within transformation is applied to \\(Y_{i t}\\) and to the elements of the basis transformations \\(X_{K i t}\\). These are \\(\\dot{Y}_{i t}=Y_{i t}-\\bar{Y}_{i}\\) and \\(\\dot{X}_{K i t}=X_{K i t}-\\bar{X}_{K i t}\\). The transformed regression equation is \\(\\dot{Y}_{i t}=\\dot{X}_{K i t}^{\\prime} \\beta_{K}+\\dot{\\varepsilon}_{K i t}\\). What is important about the within transformation for the regressors is that it is applied to the transformed variables \\(\\dot{X}_{K i t}\\) not the original regressor \\(X_{i t}\\). For example, in a polynomial regression the within transformation is applied to the powers \\(X_{i t}^{j}\\). It is inappropriate to apply the within transformation to \\(X_{i t}\\) and then construct the basis transformations.\nThe coefficient is estimated by least squares on the within transformed variables\n\\[\n\\widehat{\\beta}_{K}=\\left(\\sum_{i=1}^{n} \\sum_{t=1}^{T} \\dot{X}_{K i t} \\dot{X}_{K i t}^{\\prime}\\right)^{-1}\\left(\\sum_{i=1}^{n} \\sum_{t=1}^{T} \\dot{X}_{K i t} \\dot{Y}_{i t}\\right) .\n\\]\nVariance estimators should be calculated using the clustered variance formulas, clustered at the level of the individual \\(i\\), as described in Section 20.19.\nFor selection of the number of series terms \\(K\\) there is no current theory. A reasonable method is to use delete-cluster cross-validation as described in Section \\(20.19\\)."
  },
  {
    "objectID": "chpt20-series-reg.html#multiple-regressors",
    "href": "chpt20-series-reg.html#multiple-regressors",
    "title": "19  Series Regression",
    "section": "19.24 Multiple Regressors",
    "text": "19.24 Multiple Regressors\nSuppose \\(X \\in \\mathbb{R}^{d}\\) is vector-valued and continuously distributed. A multivariate series approximation can be obtained as follows. Construct a set of basis transformations for each variable separately. Take their tensor cross-products. Use these as regressors. For example, a \\(p^{t h}\\)-order polynomial is\n\\[\nm_{K}(x)=\\beta_{0}+\\sum_{j_{1}=1}^{p} \\cdots \\sum_{j_{d}=1}^{p} x_{1}^{j_{1}} \\cdots x_{d}^{j_{d}} \\beta_{j_{1}, \\ldots, j_{d} K}\n\\]\nThis includes all powers and cross-products. The coefficient vector has dimension \\(K=1+p^{d}\\).\nThe inclusion of cross-products greatly increases the number of coefficients relative to the univariate case. Consequently series applications with multiple regressors typically require large sample sizes."
  },
  {
    "objectID": "chpt20-series-reg.html#additively-separable-models",
    "href": "chpt20-series-reg.html#additively-separable-models",
    "title": "19  Series Regression",
    "section": "19.25 Additively Separable Models",
    "text": "19.25 Additively Separable Models\nAs discussed in the previous section, when \\(X \\in \\mathbb{R}^{d}\\) a full series expansion requires a large number of coefficients, which means that estimation precision will be low unless the sample size is quite large. A common simplification is to treat the regression function \\(m(x)\\) as additively separable in the individual regressors. This means that\n\\[\nm(x)=m_{1}\\left(x_{1}\\right)+m_{2}\\left(x_{2}\\right)+\\cdots+m_{d}\\left(x_{d}\\right) .\n\\]\nWe then apply series expansions (polynomials or splines) separately for each component \\(m_{j}\\left(x_{j}\\right)\\). Essentially, this is the same as the expansions discussed in the previous section but omitting the interaction terms.\nThe advantage of additive separability is the reduction in dimensionality. While an unconstrained \\(p^{t h}\\) order polynomial has \\(1+p^{d}\\) coefficients, an additively separable polynomial model has only \\(1+d p\\) coefficients. This is a major reduction.\nThe disadvantage of additive separability is that the interaction effects have been eliminated. This is a substantive restriction on \\(m(x)\\).\nThe decision to impose additive separability can be based on an economic model which suggests the absence of interaction effects, or can be a model selection decision similar to the selection of the number of series terms."
  },
  {
    "objectID": "chpt20-series-reg.html#nonparametric-instrumental-variables-regression",
    "href": "chpt20-series-reg.html#nonparametric-instrumental-variables-regression",
    "title": "19  Series Regression",
    "section": "19.26 Nonparametric Instrumental Variables Regression",
    "text": "19.26 Nonparametric Instrumental Variables Regression\nThe basic nonparametric instrumental variables (NPIV) model takes the form\n\\[\n\\begin{aligned}\nY &=m(X)+e \\\\\n\\mathbb{E}[e \\mid Z] &=0\n\\end{aligned}\n\\]\nwhere \\(Y, X\\), and \\(Z\\) are real valued. Here, \\(Z\\) is an instrumental variable and \\(X\\) an endogenous regressor.\nIn recent years there have been many papers in the econometrics literature examining the NPIV model, exploring identification, estimation, and inference. Many of these papers are mathematically advanced. Two important and accessible contributions are Newey and Powell (2003) and Horowitz (2011). Here we describe some of the primary results.\nA series estimator approximates the function \\(m(x)\\) with \\(m_{K}(x)=X_{K}(x)^{\\prime} \\beta_{K}\\) as in (20.4). This leads to the series structural equation\n\\[\nY=X_{K}^{\\prime} \\beta_{K}+e_{K}\n\\]\nwhere \\(X_{K}=X_{K}(X)\\). For example, if a polynomial basis is used then \\(X_{K}=\\left(1, X, \\ldots, X^{K-1}\\right)\\).\nSince \\(X\\) is endogenous so is the entire vector \\(X_{K}\\). Thus we need at least \\(K\\) instrumental varibles. It is useful to consider the reduced form equation for \\(X\\). A nonparametric specification is\n\\[\n\\begin{aligned}\nX &=g(Z)+u \\\\\n\\mathbb{E}[u \\mid Z] &=0 .\n\\end{aligned}\n\\]\nWe can appropriate \\(g(z)\\) by the series expansion\n\\[\ng(z) \\simeq g_{L}(z)=Z_{L}(z)^{\\prime} \\gamma_{L}\n\\]\nwhere \\(Z_{L}(z)\\) is an \\(L \\times 1\\) vector of basis transformations and \\(\\gamma_{L}\\) is an \\(L \\times 1\\) coefficient vector. For example, if a polynomial basis is used then \\(Z_{L}(z)=\\left(1, z, \\ldots, z^{L-1}\\right)\\). Most of the literature for simplicity focuses on the case \\(L=K\\), but this is not essential to the method.\nIf \\(L \\geq K\\) we can then use \\(Z_{L}=Z_{L}(Z)\\) as instruments for \\(X_{K}\\). The 2 SLS estimator \\(\\widehat{\\beta}_{K, L}\\) of \\(\\beta_{K}\\) is\n\\[\n\\widehat{\\beta}_{K, L}=\\left(\\boldsymbol{X}_{K}^{\\prime} \\boldsymbol{Z}_{L}\\left(\\boldsymbol{Z}_{L}^{\\prime} \\boldsymbol{Z}_{L}\\right)^{-1} \\boldsymbol{Z}_{L}^{\\prime} \\boldsymbol{X}_{K}\\right)^{-1}\\left(\\boldsymbol{X}_{K}^{\\prime} \\boldsymbol{Z}_{L}\\left(\\boldsymbol{Z}_{L}^{\\prime} \\boldsymbol{Z}_{L}\\right)^{-1} \\boldsymbol{Z}_{L}^{\\prime} \\boldsymbol{Y}\\right) .\n\\]\nThe estimator of \\(m(x)\\) is \\(\\hat{m}_{K}(x)=X_{K}(x)^{\\prime} \\widehat{\\beta}_{K, L}\\). If \\(L>K\\) the linear GMM estimator can be similarly defined.\nOne way to think about the choice of instruments is to realize that we are actually estimating reduced form equations for each element of \\(X_{K}\\). The reduced form system is\n\\[\n\\begin{aligned}\nX_{K} &=\\Gamma_{K}^{\\prime} Z_{L}+u_{K} \\\\\n\\Gamma_{K} &=\\mathbb{E}\\left[Z_{L} Z_{L}^{\\prime}\\right]^{-1} \\mathbb{E}\\left[Z_{L} X_{K}^{\\prime}\\right] .\n\\end{aligned}\n\\]\nFor example, suppose we use a polynomial basis with \\(K=L=3\\). Then the reduced form system (ignoring intercepts) is\n\\[\n\\left[\\begin{array}{c}\nX \\\\\nX^{2} \\\\\nX^{3}\n\\end{array}\\right]=\\left[\\begin{array}{lll}\n\\Gamma_{11} & \\Gamma_{21} & \\Gamma_{31} \\\\\n\\Gamma_{12} & \\Gamma_{22} & \\Gamma_{32} \\\\\n\\Gamma_{13} & \\Gamma_{13} & \\Gamma_{23}\n\\end{array}\\right]\\left[\\begin{array}{c}\nZ \\\\\nZ^{2} \\\\\nZ^{3}\n\\end{array}\\right]+\\left[\\begin{array}{l}\nu_{1} \\\\\nu_{2} \\\\\nu_{3}\n\\end{array}\\right] .\n\\]\nThis is modeling the conditional mean of \\(X, X^{2}\\), and \\(X^{3}\\) as linear functions of \\(Z, Z^{2}\\), and \\(Z^{3}\\).\nTo understand if the coefficient \\(\\beta_{K}\\) is identified it is useful to consider the simple reduced form equation \\(X=\\gamma_{0}+\\gamma_{1} Z+u\\). Assume that \\(\\gamma_{1} \\neq 0\\) so that the equation is strongly identified and assume for simplicity that \\(u\\) is independent of \\(Z\\) with mean zero and variance \\(\\sigma_{u}^{2}\\). The identification properties of the reduced form are invariant to rescaling and recentering \\(X\\) and \\(Z\\) so without loss of generality we can set \\(\\gamma_{0}=0\\) and \\(\\gamma_{1}=1\\). Then we can calculate that the coefficient matrix in (20.36) is\n\\[\n\\left[\\begin{array}{lll}\n\\Gamma_{11} & \\Gamma_{21} & \\Gamma_{31} \\\\\n\\Gamma_{12} & \\Gamma_{22} & \\Gamma_{32} \\\\\n\\Gamma_{13} & \\Gamma_{13} & \\Gamma_{23}\n\\end{array}\\right]=\\left[\\begin{array}{ccc}\n1 & 0 & 0 \\\\\n0 & 1 & 0 \\\\\n3 \\sigma_{u}^{2} & 0 & 1\n\\end{array}\\right] .\n\\]\nNotice that this is lower triangular and full rank. It turns out that this property holds for any values of \\(K=L\\) so the coefficient matrix in (20.36) is full rank for any choice of \\(K=L\\). This means that identification of the coefficient \\(\\beta_{K}\\) is strong if the reduced form equation for \\(X\\) is strong. Thus to check the identification condition for \\(\\beta_{K}\\) it is sufficient to check the reduced form equation for \\(X\\). A critically important caveat, however, as discussed in the following section, is that identification of \\(\\beta_{K}\\) does not mean that the structural function \\(m(x)\\) is identified.\nA simple method for pointwise inference is to use conventional methods to estimate \\(V_{K, L}=\\operatorname{var}\\left[\\widehat{\\beta}_{K, L}\\right]\\) and then estimate \\(\\operatorname{var}\\left[\\hat{m}_{K}(x)\\right]\\) by \\(X_{K}(x)^{\\prime} \\widehat{V}_{K, L} X_{K}(x)\\) as in series regression. Bootstrap methods are typically advocated to achieve better coverage. See Horowitz (2011) for details. For state-of-the-art inference methods see Chen and Pouzo (2015) and Chen and Christensen (2018)."
  },
  {
    "objectID": "chpt20-series-reg.html#npiv-identification",
    "href": "chpt20-series-reg.html#npiv-identification",
    "title": "19  Series Regression",
    "section": "19.27 NPIV Identification",
    "text": "19.27 NPIV Identification\nIn the previous section we discussed identication of the pseudo-true coefficient \\(\\beta_{K}\\). In this section we discuss identification of the structural function \\(m(x)\\). This is considerably more challenging.\nTo understand how the function \\(m(x)\\) is determined, apply the expectation operator \\(\\mathbb{E}[\\cdot \\mid Z=z]\\) to (20.34). We find\n\\[\n\\mathbb{E}[Y \\mid Z=z]=\\mathbb{E}[m(X) \\mid Z=z]\n\\]\nwith the remainder equal to zero because \\(\\mathbb{E}[e \\mid Z]=0\\). We can write this equation as\n\\[\n\\mu(z)=\\int m(x) f(x \\mid z) d x\n\\]\nwhere \\(\\mu(z)=\\mathbb{E}[Y \\mid Z=z]\\) is the CEF of \\(Y\\) given \\(Z=z\\) and \\(f(x \\mid z)\\) is the conditional density of \\(X\\) given \\(Z\\). These two functions are identified \\({ }^{8}\\) from the joint distribution of \\((Y, X, Z)\\). This means that the unknown function \\(m(x)\\) is the solution to the integral equation (20.37). Conceptually, you can imagine estimating \\(\\mu(z)\\) and \\(f(x \\mid z)\\) using standard techniques and then finding the solution \\(m(x)\\). In essence, this is how \\(m(x)\\) is defined and is the nonparametric analog of the classical relationship between the structural and reduced forms.\nUnfortunately the solution \\(m(x)\\) may not be unique even in situations where a linear IV model is strongly identified. It is related to what is known as the ill-posed inverse problem. The latter means that the solution \\(m(x)\\) is not necessarily a continuous function of \\(\\mu(z)\\). Identification requires restricting the class of allowable functions \\(f(x \\mid z)\\). This is analogous to the linear IV model where identification requires restrictions on the reduced form equations. Specifying and understanding the needed restrictions is more subtle than in the linear case.\nThe function \\(m(x)\\) is identified if it is the unique solution to (20.37). Equivalently, \\(m(x)\\) is not identified if we can replace \\(m(x)\\) in (20.37) with \\(m(x)+\\delta(x)\\) for some non-trivial function \\(\\delta(x)\\) yet the solution does not change. The latter occurs when\n\\[\n\\int \\delta(x) f(x \\mid z) d x=0\n\\]\n\\({ }^{8}\\) Technically, if \\(\\mathbb{E}|Y|<\\infty\\), the joint density of \\((Z, X)\\) exists, and the marginal density of \\(Z\\) is positive. for all \\(z\\). Equivalently, \\(m(x)\\) is identified if (and only if) (20.38) holds only for the trivial function \\(\\delta(x)=0\\).\nNewey and Powell (2003) defined this fundamental condition as completeness.\nProposition 20.1 Completeness. \\(m(x)\\) is identified if (and only if) the completeness condition holds: (20.38) for all \\(z\\) implies \\(\\delta(x)=0\\).\nCompleteness is a property of the reduced form conditional density \\(f(x \\mid z)\\). It is unaffected by the structural equation \\(m(x)\\). This is analogous to the linear IV model where identification is a property of the reduced form equations, not a property of the structural equation.\nAs we stated above, completeness may not be satisfied even if the reduced form relationship is strong. This may be easiest to see by a constructed example \\({ }^{9}\\). Suppose that the reduced form is \\(X=Z+u\\), \\(\\operatorname{var}[Z]=1, u\\) is independent of \\(Z\\), and \\(u\\) is distributed \\(U[-1,1]\\). This reduced form equation has \\(R^{2}=\\) \\(0.75\\) so is strong. The reduced form conditional density is \\(f(x \\mid z)=1 / 2\\) on \\([-1+z, 1+z]\\). Consider \\(\\delta(x)=\\sin (x / \\pi)\\). We calculate that\n\\[\n\\int \\delta(x) f(x \\mid z) d x=\\int_{-1+z}^{1+z} \\sin (x / \\pi) d x=0\n\\]\nfor every \\(z\\), because \\(\\sin (x / \\pi)\\) is periodic on intervals of length 2 and integrates to zero over \\([-1,1]\\). This means that equation (20.37) holds \\({ }^{10}\\) for \\(m(x)+\\sin (x / \\pi)\\). Thus \\(m(x)\\) is not identified. This is despite the fact that the reduced form equation is strong.\nWhile identification fails for some conditional distributions, it does not fail for all. Andrews (2017) provides classes of distributions which satisfy the completeness condition and shows that these distribution classes are quite general.\nWhat does this mean in practice? If completeness fails then the structural equation is not identified and cannot be consistently estimated. Furthermore, by analogy with the weak instruments literature, we expect that if the conditional distribution is close to incomplete then the structural equation will be poorly identified and our estimators will be imprecise. Since whether or not the conditional distribution is complete is unknown (and more difficult to assess than in the linear model) this is troubling for empirical research. Effectively, in any given application we do not know whether or not the structural function \\(m(x)\\) is identified.\nA partial answer is provided by Freyberger (2017). He shows that the joint hypothesis of incompleteness and small asymptotic bias can be tested. By applying the test proposed in Freyberger (2017) a user can obtain evidence that their NPIV estimator is well-behaved in the sense of having low bias. Unlike Stock and Yogo (2005), however, Freyberger’s result does not address inference."
  },
  {
    "objectID": "chpt20-series-reg.html#npiv-convergence-rate",
    "href": "chpt20-series-reg.html#npiv-convergence-rate",
    "title": "19  Series Regression",
    "section": "19.28 NPIV Convergence Rate",
    "text": "19.28 NPIV Convergence Rate\nAs described in Horowitz (2011), the convergence rate of \\(\\widehat{m}_{K}(x)\\) for \\(m(x)\\) is\n\\[\n\\left|\\widehat{m}_{K}(x)-m(x)\\right|=O_{p}\\left(K^{-s}+K^{r}\\left(\\frac{K}{n}\\right)^{1 / 2}\\right)\n\\]\n\\({ }^{9}\\) This example was suggested by Joachim Freyberger.\n\\({ }^{10}\\) In fact, (20.38) holds for \\(m(x)+\\delta(x)\\) for any function \\(\\delta(x)\\) which is periodic on intervals of length 2 and integrates to zero on \\([-1,1]\\) where \\(s\\) is the smoothness \\({ }^{11}\\) of \\(m(x)\\) and \\(r\\) is the smoothness of the joint density \\(f_{X Z}(x, z)\\) of \\((X, Z)\\). The first term \\(K^{-s}\\) is the bias due to the approximation of \\(m(x)\\) by \\(m_{K}(x)\\) and takes the same form as for series regression. The second term \\(K^{r}(K / n)^{1 / 2}\\) is the standard deviation of \\(\\widehat{m}_{K}(x)\\). The component \\((K / n)^{1 / 2}\\) is the same as for series regression. The extra component \\(K^{r}\\) is due to the ill-posed inverse problem (see the previous section).\nFrom the rate (20.39) we can calculate that the optimal number of series terms is \\(K \\sim n^{1 /(2 r+2 s+1)}\\). Given this rate the best possible convergence rate in (20.39) is \\(O_{p}\\left(n^{-s /(2 r+2 s+1)}\\right)\\). For \\(r>0\\) these rates are slower than for series regression. If we consider the case \\(s=2\\) these rates are \\(K \\sim n^{1 /(2 r+5)}\\) and \\(O_{p}\\left(n^{-2 /(2 r+5)}\\right)\\), which are slower than the \\(K \\sim n^{1 / 5}\\) and \\(O_{p}\\left(n^{-2 / 5}\\right)\\) rates obtained by series regression.\nA very unusual aspect of the rate (20.39) is that smoothness of \\(f_{X Z}(x, z)\\) adversely affects the convergence rate. Larger \\(r\\) means a slower rate of convergence. The limiting case as \\(r \\rightarrow \\infty\\) (for example, joint normality of \\(X\\) and \\(Z\\) ) results in a logarithmic convergence rate. This seems very strange. The reason is that when the density \\(f_{X Z}(x, z)\\) is very smooth the data contain little information about the function \\(m(x)\\). This is not intuitive and requires a deeper mathematical treatment.\nA practical implication of the convergence rate (20.39) is that the number of series terms \\(K\\) should be much smaller than for regression estimation. Estimation variance increases quickly as \\(K\\) increases. Therefore \\(K\\) should not be taken to be too large. In practice, however, it is unclear how to select the series order \\(K\\) as standard cross-validation methods do not apply."
  },
  {
    "objectID": "chpt20-series-reg.html#nonparametric-vs-parametric-identification",
    "href": "chpt20-series-reg.html#nonparametric-vs-parametric-identification",
    "title": "19  Series Regression",
    "section": "19.29 Nonparametric vs Parametric Identification",
    "text": "19.29 Nonparametric vs Parametric Identification\nOne of the insights from the nonparametric identification literature is that it is important to understand which features of a model are nonparametrically identified, meaning which are identified without functional form assumptions, and which are only identified based on functional form assumptions. Since functional form assumptions are dubious in most economic applications the strong implication is that researchers should strive to work only with models which are nonparametrically identified.\nEven if a model is determined to be nonparametrically identified a researcher may estimate a linear (or another simple parametric) model. This is valid because it can be viewed as an approximation to the nonparametric structure. If, however, the model is identified only under a parametric assumption, then it cannot be viewed as an approximation and it is unclear how to interpret the model more broadly.\nFor example, in the regression model \\(Y=m(X)+e\\) with \\(\\mathbb{E}[e \\mid X]=0\\) the CEF is nonparametrically identified by Theorem 2.14. This means that researchers who estimate linear regressions (or other lowdimensional regressions) can interpret their estimated model as an approximation to the underlying CEF.\nAs another example, in the NPIV model where \\(\\mathbb{E}[e \\mid Z]=0\\) the structural function \\(m(x)\\) is identified under the completeness condition. This means that researchers who estimate linear 2SLS regressions can interpret their estimated model as an approximation to \\(m(x)\\) (subject to the caveat that it is difficult to know if completeness holds).\nBut the analysis can also point out simple yet subtle mistakes. Take the simple IV model with one exogenous regressor \\(X_{1}\\) and one endogenous regressor \\(X_{2}\\)\n\\[\n\\begin{aligned}\nY &=\\beta_{0}+\\beta_{1} X_{1}+\\beta_{2} X_{2}+e \\\\\n\\mathbb{E}\\left[e \\mid X_{1}\\right] &=0\n\\end{aligned}\n\\]\nwith no additional instruments. Suppose that an enterprising researcher suggests using the instrument \\(X_{1}^{2}\\) for \\(X_{2}\\), using the reasoning that the assumptions imply that \\(\\mathbb{E}\\left[X_{1}^{2} e\\right]=0\\) so \\(X_{1}^{2}\\) is a valid instrument.\n\\({ }^{11}\\) The number of bounded derivatives. The trouble is that the basic model is not nonparametrically identified. If we write (20.40) as a partially linear nonparametric IV problem\n\\[\n\\begin{aligned}\nY &=m\\left(X_{1}\\right)+\\beta_{2} X_{2}+e \\\\\n\\mathbb{E}\\left[e \\mid X_{1}\\right] &=0\n\\end{aligned}\n\\]\nthen we can see that this model is not identified. We need a valid excluded instrument \\(Z\\). Since (20.41) is not identified, then (20.40) cannot be viewed as a valid approximation. The apparent identification of (20.40) critically rests on the unknown truth of the linearity in (20.40).\nThe point of this example is that (20.40) should never be estimated by 2 SLS using the instrument \\(X_{1}^{2}\\) for \\(X_{2}\\), fundamentally because the nonparametric model (20.41) is not identified.\nAnother way to describe the mistake is to observe that \\(X_{1}^{2}\\) is a valid instrument in (20.40) only if it is a valid exclusion restriction from the structural equation (20.40). Viewed in the context of (20.41) we can see that this is a functional form restriction. As stated above, identification based on functional form restrictions alone is highly undesirable because functional form assumptions are dubious."
  },
  {
    "objectID": "chpt20-series-reg.html#example-angrist-and-lavy-1999",
    "href": "chpt20-series-reg.html#example-angrist-and-lavy-1999",
    "title": "19  Series Regression",
    "section": "19.30 Example: Angrist and Lavy (1999)",
    "text": "19.30 Example: Angrist and Lavy (1999)\nTo illustrate nonparametric instrumental variables in practice we follow Horowitz (2011) by extending the empirical work reported in Angrist and Lavy (1999). Their paper is concerned with measuring the causal effect of the number of students in an elementary school classroom on academic achievement. They address this using a sample of 4067 Israeli \\(4^{t h}\\) and \\(5^{t h}\\) grade classrooms. The dependent variable is the classroom average score on an achievement test. Here we consider the reading score avgverb. The explanatory variables are the number of students in the classroom (classize), the number of students in the grade at the school (enrollment), and a school-level index of students’ socioeconomic status that the authors call percent disadvantaged. The variables enrollment and disadvantaged are treated as exogenous but classize is treated as endogenous because wealthier schools may be able to offer smaller class sizes.\nThe authors suggest the following instrumental variable for classsize. Israeli regulations specify that class sizes must be capped at 40. This means that classize should be perfectly predictable from enrollment. If the regulation is followed a school with up to 40 students will have one classroom in the grade and schools with 41-80 students will have two classrooms. The precise prediction is that classsize equals\n\\[\np=\\frac{\\text { enrollment }}{1+\\lfloor 1-\\text { enrollment } / 40\\rfloor}\n\\]\nwhere \\(\\lfloor a\\rfloor\\) is the integer part of \\(a\\). Angrist and Lavy use \\(p\\) as an instrumental variable for classize.\nThey estimate several specifications. We focus on equation (6) from their Table VII which specifies avgverb as a linear function of classize, disadvantaged, enrollment, grade4, and the interaction of classize and disadvantaged, where grade4 is a dummy indicator for \\(4^{t h}\\) grade classrooms. The equation is estimated by instrumental variables, using \\(p\\) and \\(p \\times\\) disadvantaged as instruments. The observations are treated as clustered at the level of the school. Their estimates show a negative and statistically significant impact of classsize on reading test scores.\nWe are interested in a nonparametric version of their equation. To keep the specification reasonably parsimonious yet flexible we use the following equation.\n\\[\n\\begin{aligned}\n\\text { avgverb } &=\\beta_{1}\\left(\\frac{\\text { classize }}{40}\\right)+\\beta_{2}\\left(\\frac{\\text { classize }}{40}\\right)^{2}+\\beta_{3}\\left(\\frac{\\text { classize }}{40}\\right)^{3} \\\\\n&+\\beta_{4}\\left(\\frac{\\text { disadvantaged }}{14}\\right)+\\beta_{5}\\left(\\frac{\\text { disadvantaged }}{14}\\right)^{2}+\\beta_{6}\\left(\\frac{\\text { disadvantaged }}{14}\\right)^{3} \\\\\n&+\\beta_{7}\\left(\\frac{\\text { classize }}{40}\\right)\\left(\\frac{\\text { disadvantaged }}{14}\\right)+\\beta_{8} \\text { enrollment }+\\beta_{9} \\text { grade } 4+\\beta_{10}+e .\n\\end{aligned}\n\\]\nThis is a cubic equation in classize and disadvantaged, with a single interaction term, and linear in enrollment and grade4. The cubic in disadvantaged was selected by a delete-cluster cross-validation regression without classize. The cubic in classize was selected to allow for a minimal degree of nonparametric flexibility without overparameterization. The variables classize and disadvantaged were scaled by 40 and 14 , respectively, so that the regression is well conditioned. The scaling for classize was selected so that the variable essentially falls in \\([0,1]\\) and the scaling for disadvantaged was selected so that its mean is 1.\nTable 20.1: Nonparametric Instrumental Variable Regression for Reading Test Score\n\n\n\nclassize/40\n\\(34.2\\)\n\n\n\n\n\n\\((33.4)\\)\n\n\nclassize/40) \\(^{2}\\)\n\\(-61.2\\)\n\n\n\n\\((53.0)\\)\n\n\n(classize/40) \\(^{3}\\)\n\\(29.0\\)\n\n\n\n\\((26.8)\\)\n\n\ndisadvantaged/14\n\\(-12.4\\)\n\n\n\n\\((1.7)\\)\n\n\n(disadvantaged/14) \\(^{2}\\)\n\\(3.33\\)\n\n\n\n\\((0.54)\\)\n\n\n(disadvantaged/14) \\(^{3}\\)\n\\(-0.377\\)\n\n\n\n\\((0.078)\\)\n\n\n(classize/40)(disadvantaged/14)\n\\(0.81\\)\n\n\n\n\\((1.77)\\)\n\n\nenrollment\n\\(0.015\\)\n\n\n\n\\((0.007)\\)\n\n\ngrade 4\n\\(-1.96\\)\n\n\n\n\\((0.16)\\)\n\n\nIntercept\n\\(77.0\\)\n\n\n\n\\((6.9)\\)\n\n\n\nThe equation is estimated by 2 SLS using \\((p / 40),(p / 40)^{2},(p / 40)^{3}\\) and \\((p / 40) \\times(\\) disadvantaged/14) as instruments for the four variables involving classize. The parameter estimates are reported in Table 20.1. The standard errors are clustered at the level of the school. Most of the individual coefficients do not have interpretable meaning, except the positive coefficient on enrollment shows that larger schools achieve slightly higher testscores, and the negative coefficient on grade4 shows that \\(4^{\\text {th }}\\) grade students have somewhat lower testscores than \\(5^{\\text {th }}\\) grade students.\nTo obtain a better interpretation of the results we display the estimated regression functions in Figure 20.8. Panel (a) displays the estimated effect of classize on reading test scores. Panel (b) displays the estimated effect of disadvantaged. In both figures the other variables are set at their sample means \\({ }^{12}\\).\n\\({ }^{12}\\) If they are set at other values it does not change the qualitative nature of the plots.\n\n\nEffect of Classize\n\n\n\nEffect of Percent Disadvantaged\n\nFigure 20.8: Nonparametric Instrumental Variables Estimates of the Effect of Classize and Disadvantaged on Reading Test Scores\nIn panel (a) we can see that increasing class size decreases the average test score. This is consistent with the results from the linear model estimated by Angrist and Lavy (1999). The estimated effect is remarkably close to linear.\nIn panel (b) we can see that increasing the percentage of disadvantaged students greatly decreases the average test score. This effect is substantially greater in magnitude than the effect of classsize. The effect also appears to be nonlinear. The effect is precisely estimated with tight pointwise confidence bands.\nWe can also use the estimated model for hypothesis testing. The question addressed by Angrist and Lavy was whether or not classsize has an effect on test scores. Within the nonparametric model estimated here this hypothesis holds under the linear restriction \\(\\mathbb{H}_{0}: \\beta_{1}=\\beta_{2}=\\beta_{3}=\\beta_{7}=0\\). Examining the individual coefficient estimates and standard errors it is unclear if this is a significant effect as none of these four coefficient estimates is statistically different from zero. This hypothesis is better tested by a Wald test (using cluster-robust variance estimates). This statistic is \\(12.7\\) which has an asymptotic \\(\\mathrm{p}\\)-value of \\(0.013\\). This suppports the hypothesis that class size has a negative effect on student performance.\nWe can also use the model to quantify the impact of class size on test scores. Consider the impact of increasing a class from 20 to 40 students. In the above model the predicted impact on test scores is\n\\[\n\\theta=\\frac{1}{2} \\beta_{1}+\\frac{3}{4} \\beta_{2}+\\frac{7}{8} \\beta_{3}+\\frac{1}{2} \\beta_{4} .\n\\]\nThis is a linear function of the coefficients. The point estimate is \\(\\widehat{\\theta}=-2.96\\) with a standard error of \\(1.21\\). (The point estimate is identical to the difference between the endpoints of the estimated function shown in panel (a).) This is a small but substantive impact."
  },
  {
    "objectID": "chpt20-series-reg.html#technical-proofs",
    "href": "chpt20-series-reg.html#technical-proofs",
    "title": "19  Series Regression",
    "section": "19.31 Technical Proofs*",
    "text": "19.31 Technical Proofs*\nProof of Theorem 20.4. We provide a proof under the stronger assumption \\(\\zeta_{K}^{2} K / n \\rightarrow 0\\). (The proof presented by Belloni, Chernozhukov, Chetverikov, and Kato (2015) requires a more advanced treatment.) Let \\(\\|\\boldsymbol{A}\\|_{F}\\) denote the Frobenius norm (see Section A.23), and write the \\(j^{t h}\\) element of \\(\\widetilde{X}_{K i}\\) as \\(\\widetilde{X}_{j K i}\\). Using (A.18),\n\\[\n\\left\\|\\widetilde{\\boldsymbol{Q}}_{K}-\\boldsymbol{I}_{K}\\right\\|^{2} \\leq\\left\\|\\widetilde{\\boldsymbol{Q}}_{K}-\\boldsymbol{I}_{K}\\right\\|_{F}^{2}=\\sum_{j=1}^{K} \\sum_{\\ell=1}^{K}\\left(\\frac{1}{n} \\sum_{i=1}^{n}\\left(\\widetilde{X}_{j K i} \\widetilde{X}_{\\ell K i}-\\mathbb{E}\\left[\\widetilde{X}_{j K i} \\widetilde{X}_{\\ell K i}\\right]\\right)\\right)^{2} .\n\\]\nThen\n\\[\n\\begin{aligned}\n\\mathbb{E}\\left[\\left\\|\\widetilde{\\boldsymbol{Q}}_{K}-\\boldsymbol{I}_{K}\\right\\|^{2}\\right] & \\leq \\sum_{j=1}^{K} \\sum_{\\ell=1}^{K} \\operatorname{var}\\left[\\frac{1}{n} \\sum_{i=1}^{n} \\widetilde{X}_{j K i} \\widetilde{X}_{\\ell K i}\\right] \\\\\n&=\\frac{1}{n} \\sum_{j=1}^{K} \\sum_{\\ell=1}^{K} \\operatorname{var}\\left[\\widetilde{X}_{j K i} \\widetilde{X}_{\\ell K i}\\right] \\\\\n& \\leq \\frac{1}{n} \\mathbb{E}\\left[\\sum_{j=1}^{K} \\widetilde{X}_{j K i}^{2} \\sum_{\\ell=1}^{K} \\widetilde{X}_{\\ell K i}^{2}\\right] \\\\\n&=\\frac{1}{n} \\mathbb{E}\\left[\\left(\\widetilde{X}_{K i}^{\\prime} \\widetilde{X}_{K i}\\right)^{2}\\right] \\\\\n& \\leq \\frac{\\zeta_{K}^{2}}{n} \\mathbb{E}\\left[\\widetilde{X}_{K i}^{\\prime} \\widetilde{X}_{K i}\\right]=\\frac{\\zeta_{K}^{2} K}{n} \\rightarrow 0\n\\end{aligned}\n\\]\nwhere final lines use (20.17), \\(\\mathbb{E}\\left[\\widetilde{X}_{K i}^{\\prime} \\widetilde{X}_{K i}\\right]=K\\), and \\(\\zeta_{K}^{2} K / n \\rightarrow 0\\). Markov’s inequality implies (20.19).\nProof of Theorem 20.5. By the spectral decomposition we can write \\(\\widetilde{\\boldsymbol{Q}}_{K}=\\boldsymbol{H}^{\\prime} \\Lambda \\boldsymbol{H}\\) where \\(\\boldsymbol{H}^{\\prime} \\boldsymbol{H}=\\boldsymbol{I}_{K}\\) and \\(\\Lambda=\\operatorname{diag}\\left(\\lambda_{1}, \\ldots, \\lambda_{K}\\right)\\) are the eigenvalues. Then\n\\[\n\\left\\|\\widetilde{\\boldsymbol{Q}}_{K}-\\boldsymbol{I}_{K}\\right\\|=\\left\\|\\boldsymbol{H}^{\\prime}\\left(\\Lambda-\\boldsymbol{I}_{K}\\right) \\boldsymbol{H}\\right\\|=\\left\\|\\Lambda-\\boldsymbol{I}_{K}\\right\\|=\\max _{j \\leq K}\\left|\\lambda_{j}-1\\right| \\underset{p}{\\longrightarrow} 0\n\\]\nby Theorem 20.4. This implies \\(\\min _{j \\leq K}\\left|\\lambda_{j}\\right| \\underset{p}{\\longrightarrow} 1\\) which is (20.21). Similarly\n\\[\n\\begin{aligned}\n\\left\\|\\widetilde{\\boldsymbol{Q}}_{K}^{-1}-\\boldsymbol{I}_{K}\\right\\| &=\\left\\|\\boldsymbol{H}^{\\prime}\\left(\\Lambda^{-1}-\\boldsymbol{I}_{K}\\right) \\boldsymbol{H}\\right\\| \\\\\n&=\\left\\|\\Lambda^{-1}-\\boldsymbol{I}_{K}\\right\\| \\\\\n&=\\max _{j \\leq K}\\left|\\lambda_{j}^{-1}-1\\right| \\\\\n& \\leq \\frac{\\max _{j \\leq K}\\left|1-\\lambda_{j}\\right|}{\\min _{j \\leq K}\\left|\\lambda_{j}\\right|} \\underset{p}{\\longrightarrow} 0 .\n\\end{aligned}\n\\]\nProof of Theorem 20.6. Using (20.12) we can write\n\\[\n\\widehat{m}_{K}(x)-m(x)=X_{K}(x)^{\\prime}\\left(\\widehat{\\beta}_{K}-\\beta_{K}\\right)-r_{K}(x) .\n\\]\nSince \\(e_{K}=r_{K}+e\\) is a projection error it satisfies \\(\\mathbb{E}\\left[X_{K} e_{K}\\right]=0\\). Since \\(e\\) is a regression error it satisfies \\(\\mathbb{E}\\left[X_{K} e\\right]=0\\). We deduce \\(\\mathbb{E}\\left[X_{K} r_{K}\\right]=0\\). Hence \\(\\int X_{K}(x) r_{K}(x) f(x) d x=\\mathbb{E}\\left[X_{K} r_{K}\\right]=0\\). Also observe that \\(\\int X_{K}(x) X_{K}(x)^{\\prime} d F(x)=\\boldsymbol{Q}_{K}\\) and \\(\\int r_{K}(x)^{2} d F(x)=\\mathbb{E}\\left[r_{K}^{2}\\right]=\\delta_{K}^{2}\\). Then\n\\[\n\\begin{aligned}\n\\operatorname{ISE}(K) &=\\int\\left(X_{K}(x)^{\\prime}\\left(\\widehat{\\beta}_{K}-\\beta_{K}\\right)-r_{K}(x)\\right)^{2} d F(x) \\\\\n&=\\left(\\widehat{\\beta}_{K}-\\beta_{K}\\right)^{\\prime}\\left(\\int X_{K}(x) X_{K}(x)^{\\prime} d F(x)\\right)\\left(\\widehat{\\beta}_{K}-\\beta_{K}\\right) \\\\\n&-2\\left(\\widehat{\\beta}_{K}-\\beta_{K}\\right)^{\\prime}\\left(\\int X_{K}(x) r_{K}(x) d F(x)\\right)+\\int r_{K}(x)^{2} d F(x) \\\\\n&=\\left(\\widehat{\\beta}_{K}-\\beta_{K}\\right)^{\\prime} \\boldsymbol{Q}_{K}\\left(\\widehat{\\beta}_{K}-\\beta_{K}\\right)+\\delta_{K}^{2}\n\\end{aligned}\n\\]\nWe calculate that\n\\[\n\\begin{aligned}\n\\left(\\widehat{\\beta}_{K}-\\beta_{K}\\right)^{\\prime} \\boldsymbol{Q}_{K}\\left(\\widehat{\\beta}_{K}-\\beta_{K}\\right) &=\\left(\\boldsymbol{e}_{K}^{\\prime} \\boldsymbol{X}_{K}\\right)\\left(\\boldsymbol{X}_{K}^{\\prime} \\boldsymbol{X}_{K}\\right)^{-1} \\boldsymbol{Q}_{K}\\left(\\boldsymbol{X}_{K}^{\\prime} \\boldsymbol{X}_{K}\\right)^{-1}\\left(\\boldsymbol{X}_{K}^{\\prime} \\boldsymbol{e}_{K}\\right) \\\\\n&=\\left(\\boldsymbol{e}_{K}^{\\prime} \\widetilde{\\boldsymbol{X}}_{K}\\right)\\left(\\widetilde{\\boldsymbol{X}}_{K}^{\\prime} \\widetilde{\\boldsymbol{X}}_{K}\\right)^{-1}\\left(\\widetilde{\\boldsymbol{X}}_{K}^{\\prime} \\widetilde{\\boldsymbol{X}}_{K}\\right)^{-1}\\left(\\widetilde{\\boldsymbol{X}}_{K}^{\\prime} \\boldsymbol{e}_{K}\\right) \\\\\n&=n^{-2}\\left(\\boldsymbol{e}_{K}^{\\prime} \\widetilde{\\boldsymbol{X}}_{K}\\right) \\widetilde{\\boldsymbol{Q}}_{K}^{-1} \\widetilde{\\boldsymbol{Q}}_{K}^{-1}\\left(\\widetilde{\\boldsymbol{X}}_{K}^{\\prime} \\boldsymbol{e}_{K}\\right) \\\\\n& \\leq\\left(\\lambda_{\\max }\\left(\\widetilde{\\boldsymbol{Q}}_{K}^{-1}\\right)\\right)^{2}\\left(n^{-2} \\boldsymbol{e}_{K}^{\\prime} \\widetilde{\\boldsymbol{X}}_{K} \\widetilde{\\boldsymbol{X}}_{K}^{\\prime} \\boldsymbol{e}_{K}\\right) \\\\\n& \\leq O_{p}(1)\\left(n^{-2} \\boldsymbol{e}_{K}^{\\prime} \\boldsymbol{X}_{K} \\boldsymbol{Q}_{K}^{-1} \\boldsymbol{X}_{K}^{\\prime} \\boldsymbol{e}_{K}\\right)\n\\end{aligned}\n\\]\nwhere \\(\\widetilde{\\boldsymbol{X}}_{K}\\) and \\(\\widetilde{\\boldsymbol{Q}}_{K}\\) are the orthogonalized regressors as defined in (20.18). The first inequality is the Quadratic Inequality (B.18), the second is (20.21).\nUsing the fact that \\(X_{K} e_{K}\\) are mean zero and uncorrelated, (20.17), \\(\\mathbb{E}\\left[e_{K}^{2}\\right] \\leq \\mathbb{E}\\left[Y^{2}\\right]<\\infty\\), and Assumption 20.1.2,\n\\[\n\\begin{aligned}\n\\mathbb{E}\\left[n^{-2} \\boldsymbol{e}_{K}^{\\prime} \\boldsymbol{X}_{K} \\boldsymbol{Q}_{K}^{-1} \\boldsymbol{X}_{K}^{\\prime} \\boldsymbol{e}_{K}\\right] &=n^{-1} \\mathbb{E}\\left[X_{K}^{\\prime} \\boldsymbol{Q}_{K}^{-1} X_{K} e_{K}^{2}\\right] \\\\\n& \\leq \\frac{\\zeta_{K}^{2}}{n} \\mathbb{E}\\left[e_{K}^{2}\\right] \\leq o(1) .\n\\end{aligned}\n\\]\nThis shows that (20.45) is \\(o_{p}\\) (1). Combined with (20.44) we find \\(\\operatorname{ISE}(K)=o_{p}(1)\\) as claimed.\nProof of Theorem 20.7. The assumption \\(\\sigma^{2}(x) \\leq \\bar{\\sigma}^{2}\\) implies that\n\\[\n\\mathbb{E}\\left[e_{K}^{2} \\mid X\\right]=\\mathbb{E}\\left[\\left(r_{K}+e\\right)^{2} \\mid X\\right]=r_{K}^{2}+\\sigma^{2}(X) \\leq r_{K}^{2}+\\bar{\\sigma}^{2} .\n\\]\nThus \\((20.46)\\) is bounded by\n\\[\n\\begin{aligned}\nn^{-1} \\mathbb{E}\\left[X_{K}^{\\prime} \\boldsymbol{Q}_{K}^{-1} X_{K} r_{K}^{2}\\right]+n^{-1} \\mathbb{E}\\left[X_{K}^{\\prime} \\boldsymbol{Q}_{K}^{-1} X_{K}\\right] \\bar{\\sigma}^{2} & \\leq \\frac{\\zeta_{K}^{2}}{n} \\mathbb{E}\\left[r_{K}^{2}\\right]+n^{-1} \\mathbb{E}\\left[\\operatorname{tr}\\left(\\boldsymbol{Q}_{K}^{-1} X_{K} X_{K}^{\\prime}\\right)\\right] \\bar{\\sigma}^{2} \\\\\n&=\\frac{\\zeta_{K}^{2}}{n} \\delta_{K}^{2}+n^{-1} \\operatorname{tr}\\left(\\boldsymbol{I}_{K}\\right) \\bar{\\sigma}^{2} \\\\\n& \\leq o\\left(\\delta_{K}^{2}\\right)+\\frac{K}{n} \\bar{\\sigma}^{2}\n\\end{aligned}\n\\]\nwhere the inequality is Assumption 20.1.2. This implies (20.45) is \\(o_{p}\\left(\\delta_{K}^{2}\\right)+O_{p}(K / n)\\). Combined with (20.44) we find \\(\\operatorname{ISE}(K)=O_{p}\\left(\\delta_{K}^{2}+K / n\\right)\\) as claimed.\nProof of Theorem 20.8. Using (20.12) and linearity\n\\[\n\\theta=a(m)=a\\left(Z_{K}(x)^{\\prime} \\beta_{K}\\right)+a\\left(r_{K}\\right)=a_{K}^{\\prime} \\beta_{K}+a\\left(r_{K}\\right) .\n\\]\nThus\n\\[\n\\begin{aligned}\n\\sqrt{\\frac{n}{V_{K}}}\\left(\\widehat{\\theta}_{K}-\\theta+a\\left(r_{K}\\right)\\right) &=\\sqrt{\\frac{n}{V_{K}}} a_{K}^{\\prime}\\left(\\widehat{\\beta}_{K}-\\beta_{K}\\right) \\\\\n&=\\sqrt{\\frac{1}{n V_{K}}} a_{K}^{\\prime} \\widehat{\\boldsymbol{Q}}_{K}^{-1} \\boldsymbol{X}_{K}^{\\prime} \\boldsymbol{e}_{K} \\\\\n&=\\frac{1}{\\sqrt{n V_{K}}} a_{K}^{\\prime} \\boldsymbol{Q}_{K}^{-1} \\boldsymbol{X}_{K}^{\\prime} \\boldsymbol{e} \\\\\n&+\\frac{1}{\\sqrt{n V_{K}}} a_{K}^{\\prime}\\left(\\widehat{\\boldsymbol{Q}}_{K}^{-1}-\\boldsymbol{Q}_{K}^{-1}\\right) \\boldsymbol{X}_{K}^{\\prime} \\boldsymbol{e} \\\\\n&+\\frac{1}{\\sqrt{n V_{K}}} a_{K}^{\\prime} \\widehat{\\boldsymbol{Q}}_{K}^{-1} \\boldsymbol{X}_{K}^{\\prime} \\boldsymbol{r}_{K}\n\\end{aligned}\n\\]\nwhere we have used \\(\\boldsymbol{e}_{K}=\\boldsymbol{e}+\\boldsymbol{r}_{K}\\). We take the terms in (20.47)-(20.49) separately. We show that (20.47) is asymptotically normal and (20.48)-(20.49) are asymptotically negligible.\nFirst, take (20.47). We can write\n\\[\n\\frac{1}{\\sqrt{n V_{K}}} a_{K}^{\\prime} \\boldsymbol{Q}_{K}^{-1} \\boldsymbol{X}_{K}^{\\prime} \\boldsymbol{e}=\\frac{1}{\\sqrt{n}} \\sum_{i=1}^{n} \\frac{1}{\\sqrt{V_{K}}} a_{K}^{\\prime} \\boldsymbol{Q}_{K}^{-1} X_{K i} e_{i} .\n\\]\nObserve that \\(a_{K}^{\\prime} \\boldsymbol{Q}_{K}^{-1} X_{K i} e_{i} / \\sqrt{V_{K}}\\) are independent across \\(i\\), mean zero, and have variance 1 . We will apply Theorem 6.4, for which it is sufficient to verify Lindeberg’s condition: For all \\(\\epsilon>0\\)\n\\[\n\\mathbb{E}\\left[\\frac{\\left(a_{K}^{\\prime} \\boldsymbol{Q}_{K}^{-1} X_{K} e\\right)^{2}}{V_{K}} \\mathbb{1}\\left\\{\\frac{\\left(a_{K}^{\\prime} \\boldsymbol{Q}_{K}^{-1} X_{K} e\\right)^{2}}{V_{K}} \\geq n \\epsilon\\right\\}\\right] \\rightarrow 0 .\n\\]\nPick \\(\\eta>0\\). Set \\(B\\) sufficiently large so that \\(\\mathbb{E}\\left[e^{2} \\mathbb{1}\\left\\{e^{2}>B\\right\\} \\mid X\\right] \\leq \\underline{\\sigma}^{2} \\eta\\) which is feasible by Assumption 20.2.1. Pick \\(n\\) sufficiently large so that \\(\\zeta_{K}^{2} / n \\leq \\epsilon \\underline{\\sigma}^{2} / B\\), which is feasible under Assumption 20.1.2.\nBy Assumption 20.2.2\n\\[\n\\begin{aligned}\nV_{K} &=\\mathbb{E}\\left[\\left(a_{K}^{\\prime} \\boldsymbol{Q}_{K}^{-1} X_{K}\\right)^{2} e^{2}\\right] \\\\\n&=\\mathbb{E}\\left[\\left(a_{K}^{\\prime} \\boldsymbol{Q}_{K}^{-1} X_{K}\\right)^{2} \\sigma\\left(X^{2}\\right)\\right] \\\\\n& \\geq \\mathbb{E}\\left[\\left(a_{K}^{\\prime} \\boldsymbol{Q}_{K}^{-1} X_{K}\\right)^{2} \\underline{\\sigma}^{2}\\right] \\\\\n&=a_{K}^{\\prime} \\boldsymbol{Q}_{K}^{-1} \\mathbb{E}\\left[X_{K} X_{K}^{\\prime}\\right] \\boldsymbol{Q}_{K}^{-1} a_{K} \\underline{\\sigma}^{2} \\\\\n&=a_{K}^{\\prime} \\boldsymbol{Q}_{K}^{-1} a_{K} \\underline{\\sigma}^{2} .\n\\end{aligned}\n\\]\nThen by the Schwarz Inequality, (20.17), (20.52), and \\(\\zeta_{K}^{2} / n \\leq \\epsilon \\underline{\\sigma}^{2} / B\\)\n\\[\n\\frac{\\left(a_{K}^{\\prime} \\boldsymbol{Q}_{K}^{-1} X_{K}\\right)^{2}}{V_{K}} \\leq \\frac{\\left(a_{K}^{\\prime} \\boldsymbol{Q}_{K}^{-1} a_{K}\\right)\\left(X_{K}^{\\prime} \\boldsymbol{Q}_{K}^{-1} X_{K}\\right)}{V_{K}} \\leq \\frac{\\zeta_{K}^{2}}{\\underline{\\sigma}^{2}} \\leq \\frac{\\epsilon}{B} n .\n\\]\nThen the left-side of (20.51) is smaller than\n\\[\n\\begin{aligned}\n\\mathbb{E}\\left[\\frac{\\left(a_{K}^{\\prime} \\boldsymbol{Q}_{K}^{-1} X_{K}\\right)^{2}}{V_{K}} e^{2} \\mathbb{1}\\left\\{e^{2} \\geq B\\right\\}\\right] &=\\mathbb{E}\\left[\\frac{\\left(a_{K}^{\\prime} \\boldsymbol{Q}_{K}^{-1} X_{K}\\right)^{2}}{V_{K}} \\mathbb{E}\\left[e^{2} \\mathbb{1}\\left\\{e^{2} \\geq B\\right\\} \\mid X\\right]\\right] \\\\\n& \\leq \\mathbb{E}\\left[\\frac{\\left(a_{K}^{\\prime} \\boldsymbol{Q}_{K}^{-1} X_{K}\\right)^{2}}{V_{K}}\\right] \\underline{\\sigma}^{2} \\eta \\\\\n& \\leq \\frac{a_{K}^{\\prime} \\boldsymbol{Q}_{K}^{-1} a_{K}}{V_{K}} \\underline{\\sigma}^{2} \\eta \\leq \\eta\n\\end{aligned}\n\\]\nthe final inequality by (20.52). Since \\(\\eta\\) is arbitrary this verifies (20.51) and we conclude\n\\[\n\\frac{1}{\\sqrt{n V_{K}}} a_{K}^{\\prime} \\boldsymbol{Q}_{K}^{-1} \\boldsymbol{X}_{K}^{\\prime} \\boldsymbol{e} \\underset{d}{\\longrightarrow} \\mathrm{N}(0,1) \\text {. }\n\\]\nSecond, take (20.48). Assumption \\(20.2\\) implies \\(\\mathbb{E}\\left[e^{2} \\mid X\\right] \\leq \\bar{\\sigma}^{2}<\\infty\\). Since \\(\\mathbb{E}[\\boldsymbol{e} \\mid \\boldsymbol{X}]=0\\), applying \\(\\mathbb{E}\\left[e^{2} \\mid X\\right] \\leq \\bar{\\sigma}^{2}\\), the Schwarz and Norm Inequalities, (20.52), and Theorems \\(20.4\\) and \\(20.5\\),\n\\[\n\\begin{aligned}\n&\\mathbb{E}\\left[\\left(\\frac{1}{\\sqrt{n V_{K}}} a_{K}^{\\prime}\\left(\\widehat{\\boldsymbol{Q}}_{K}^{-1}-\\boldsymbol{Q}_{K}^{-1}\\right) \\boldsymbol{X}_{K}^{\\prime} \\boldsymbol{e}\\right)^{2} \\mid \\boldsymbol{X}\\right] \\\\\n&=\\frac{1}{n V_{K}} a_{K}^{\\prime}\\left(\\widehat{\\boldsymbol{Q}}_{K}^{-1}-\\boldsymbol{Q}_{K}^{-1}\\right) \\boldsymbol{X}_{K}^{\\prime} \\mathbb{E}\\left[\\boldsymbol{e} \\boldsymbol{e}^{\\prime} \\mid \\boldsymbol{X}\\right] \\boldsymbol{X}_{K}\\left(\\widehat{\\boldsymbol{Q}}_{K}^{-1}-\\boldsymbol{Q}_{K}^{-1}\\right) a_{K} \\\\\n&\\leq \\frac{\\bar{\\sigma}^{2}}{V_{K}} a_{K}^{\\prime}\\left(\\widehat{\\boldsymbol{Q}}_{K}^{-1}-\\boldsymbol{Q}_{K}^{-1}\\right) \\widehat{\\boldsymbol{Q}}_{K}\\left(\\widehat{\\boldsymbol{Q}}_{K}^{-1}-\\boldsymbol{Q}_{K}^{-1}\\right) a_{K} \\\\\n&\\leq \\frac{\\bar{\\sigma}^{2} a_{K}^{\\prime} \\boldsymbol{Q}_{K}^{-1} a_{K}}{V_{K}}\\left\\|\\left(\\widehat{\\boldsymbol{Q}}_{K}^{-1}-\\boldsymbol{Q}_{K}^{-1}\\right) \\widehat{\\boldsymbol{Q}}_{K}\\left(\\widehat{\\boldsymbol{Q}}_{K}^{-1}-\\boldsymbol{Q}_{K}^{-1}\\right)\\right\\| \\\\\n&=\\frac{\\bar{\\sigma}^{2} a_{K}^{\\prime} \\boldsymbol{Q}_{K}^{-1} a_{K}}{V_{K}}\\left\\|\\left(\\boldsymbol{I}_{K}-\\widetilde{\\boldsymbol{Q}}_{K}\\right)\\left(\\widetilde{\\boldsymbol{Q}}_{K}^{-1}-\\boldsymbol{I}_{K}\\right)\\right\\| \\\\\n&\\leq \\frac{\\bar{\\sigma}^{2}}{\\underline{\\sigma}^{2}}\\left\\|\\boldsymbol{I}_{K}-\\widetilde{\\boldsymbol{Q}}_{K}\\right\\|\\left\\|\\widetilde{\\boldsymbol{Q}}_{K}^{-1}-\\boldsymbol{I}_{K}\\right\\| \\\\\n&\\leq \\frac{\\bar{\\sigma}^{2}}{\\underline{\\sigma}^{2}} o_{p}(1) .\n\\end{aligned}\n\\]\nThis establishes that (20.48) is \\(o_{p}(1)\\).\nThird, take (20.49). By the Cauchy-Schwarz inequality, the Quadratic Inequality, (20.52), and (20.21),\n\\[\n\\begin{aligned}\n\\left(\\frac{1}{\\sqrt{n v_{K}}} a_{K}^{\\prime} \\widehat{\\boldsymbol{Q}}_{K}^{-1} \\boldsymbol{X}_{K}^{\\prime} \\boldsymbol{r}_{K}\\right)^{2} & \\leq \\frac{a_{K}^{\\prime} \\boldsymbol{Q}_{K}^{-1} a_{K}}{n v_{K}} \\boldsymbol{r}_{K}^{\\prime} \\boldsymbol{X}_{K} \\widehat{\\boldsymbol{Q}}_{K}^{-1} \\boldsymbol{Q}_{K} \\widehat{\\boldsymbol{Q}}_{K}^{-1} \\boldsymbol{X}_{K}^{\\prime} \\boldsymbol{r}_{K} \\\\\n& \\leq \\frac{1}{\\underline{\\sigma}^{2}}\\left(\\lambda_{\\max } \\widetilde{\\boldsymbol{Q}}_{K}^{-1}\\right)^{2} \\frac{1}{n} \\boldsymbol{r}_{K}^{\\prime} \\boldsymbol{X}_{K} \\boldsymbol{Q}_{K}^{-1} \\boldsymbol{X}_{K}^{\\prime} \\boldsymbol{r}_{K} \\\\\n& \\leq O_{p}(1) \\frac{1}{n} \\boldsymbol{r}_{K}^{\\prime} \\boldsymbol{X}_{K} \\boldsymbol{Q}_{K}^{-1} \\boldsymbol{X}_{K}^{\\prime} \\boldsymbol{r}_{K} .\n\\end{aligned}\n\\]\nObserve that because the observations are independent, \\(\\mathbb{E}\\left[X_{K} r_{K}\\right]=0, X_{K i}^{\\prime} \\boldsymbol{Q}_{K}^{-1} X_{K i} \\leq \\zeta_{K}^{2}\\), and \\(\\mathbb{E}\\left[r_{K}^{2}\\right]=\\delta_{K}^{2}\\),\n\\[\n\\begin{aligned}\n\\mathbb{E}\\left[\\frac{1}{n} \\boldsymbol{r}_{K}^{\\prime} \\boldsymbol{X}_{K} \\boldsymbol{Q}_{K}^{-1} \\boldsymbol{X}_{K}^{\\prime} \\boldsymbol{r}_{K}\\right] &=\\mathbb{E}\\left[\\frac{1}{n} \\sum_{i=1}^{n} r_{K i} X_{K i}^{\\prime} \\boldsymbol{Q}_{K}^{-1} \\sum_{i j=1}^{n} X_{K j} r_{K j}\\right] \\\\\n&=\\mathbb{E}\\left[X_{K}^{\\prime} \\boldsymbol{Q}_{K}^{-1} X_{K} r_{K}^{2}\\right] \\\\\n& \\leq \\zeta_{K}^{2} \\mathbb{E}\\left[r_{K}^{2}\\right]=\\zeta_{K}^{2} \\delta_{K}^{2}=o(1)\n\\end{aligned}\n\\]\nunder Assumption 20.2.3. Thus \\(\\frac{1}{n} \\boldsymbol{r}_{K}^{\\prime} \\boldsymbol{X}_{K} \\boldsymbol{Q}_{K}^{-1} \\boldsymbol{X}_{K}^{\\prime} \\boldsymbol{r}_{K}=o_{p}(1)\\), (20.54) is \\(o_{p}(1)\\), and (20.49) is \\(o_{p}(1)\\).\nTogether, we have shown that\n\\[\n\\sqrt{\\frac{n}{V_{K}}}\\left(\\widehat{\\theta}_{K}-\\theta_{K}+a\\left(r_{K}\\right)\\right) \\underset{d}{\\longrightarrow} \\mathrm{N}(0,1)\n\\]\nas claimed. Proof of Theorem 20.10. It is sufficient to show that\n\\[\n\\frac{\\sqrt{n}}{V_{K}^{1 / 2}(x)} r_{K}(x)=o(1) \\text {. }\n\\]\nNotice that by Assumption \\(20.2 .2\\)\n\\[\n\\begin{aligned}\nV_{K}(x) &=X_{K}(x)^{\\prime} \\boldsymbol{Q}_{K}^{-1} \\boldsymbol{\\Omega}_{K} \\boldsymbol{Q}_{K}^{-1} X_{K}(x) \\\\\n&=\\mathbb{E}\\left[\\left(X_{K}(x)^{\\prime} \\boldsymbol{Q}_{K}^{-1} X_{K}\\right)^{2} e^{2}\\right] \\\\\n&=\\mathbb{E}\\left[\\left(X_{K}(x)^{\\prime} \\boldsymbol{Q}_{K}^{-1} X_{K}\\right)^{2} \\sigma^{2}(X)\\right] \\\\\n& \\geq \\mathbb{E}\\left[\\left(X_{K}(x)^{\\prime} \\boldsymbol{Q}_{K}^{-1} X_{K}\\right)^{2}\\right] \\underline{\\sigma}^{2} \\\\\n&=X_{K}(x)^{\\prime} \\boldsymbol{Q}_{K}^{-1} \\mathbb{E}\\left[X_{K} X_{K}^{\\prime}\\right] \\boldsymbol{Q}_{K}^{-1} X_{K}(x) \\underline{\\sigma}^{2} \\\\\n&=X_{K}(x)^{\\prime} \\boldsymbol{Q}_{K}^{-1} X_{K}(x) \\underline{\\sigma}^{2} \\\\\n&=\\zeta_{K}(x)^{2} \\underline{\\sigma}^{2} .\n\\end{aligned}\n\\]\nUsing the definitions for \\(\\beta_{K}^{*}, r_{K}^{*}(x)\\), and \\(\\delta_{K}^{*}\\) from Section 20.8, note that\n\\[\nr_{K}(x)=m(x)-X_{K}^{\\prime}(x) \\beta_{K}=r_{K}^{*}(x)+X_{K}^{\\prime}(x)\\left(\\beta_{K}^{*}-\\beta_{K}\\right) .\n\\]\nBy the Triangle Inequality, the definition (20.10), the Schwarz Inequality, and definition (20.15)\n\\[\n\\begin{aligned}\n\\left|r_{K}(x)\\right| & \\leq\\left|r_{K}^{*}(x)\\right|+\\left|X_{K}^{\\prime}(x)\\left(\\beta_{K}^{*}-\\beta_{K}\\right)\\right| \\\\\n& \\leq \\delta_{K}^{*}+\\left|X_{K}^{\\prime}(x) \\boldsymbol{Q}_{K}^{-1} X_{K}^{\\prime}(x)\\right|^{1 / 2}\\left|\\left(\\beta_{K}^{*}-\\beta_{K}\\right)^{\\prime} \\boldsymbol{Q}_{K}\\left(\\beta_{K}^{*}-\\beta_{K}\\right)\\right|^{1 / 2} \\\\\n&=\\delta_{K}^{*}+\\zeta_{K}(x)\\left|\\left(\\beta_{K}^{*}-\\beta_{K}\\right)^{\\prime} \\boldsymbol{Q}_{K}\\left(\\beta_{K}^{*}-\\beta_{K}\\right)\\right|^{1 / 2} .\n\\end{aligned}\n\\]\nThe coefficients satisfy the relationship\n\\[\n\\beta_{K}=\\mathbb{E}\\left[X_{K} X_{K}^{\\prime}\\right]^{-1} \\mathbb{E}\\left[X_{K} m(X)\\right]=\\beta_{K}^{*}+\\mathbb{E}\\left[X_{K} X_{K}^{\\prime}\\right]^{-1} \\mathbb{E}\\left[X_{K} r_{K}^{*}\\right] .\n\\]\nThus\n\\[\n\\left(\\beta_{K}^{*}-\\beta_{K}\\right)^{\\prime} \\boldsymbol{Q}_{K}\\left(\\beta_{K}^{*}-\\beta_{K}\\right)=\\mathbb{E}\\left[r_{K}^{*} X_{K}^{\\prime}\\right] \\mathbb{E}\\left[X_{K} X_{K}^{\\prime}\\right]^{-1} \\mathbb{E}\\left[X_{K} r_{K}^{*}\\right] \\leq \\mathbb{E}\\left[r_{K}^{2 *}\\right] \\leq \\delta_{K}^{* 2} .\n\\]\nThe first inequality is because \\(\\mathbb{E}\\left[r_{K}^{*} X_{K}^{\\prime}\\right] \\mathbb{E}\\left[X_{K} X_{K}^{\\prime}\\right]^{-1} \\mathbb{E}\\left[X_{K} r_{K}^{*}\\right]\\) is a projection. The second inequality follows from the definition (20.10). We deduce that\n\\[\n\\left|r_{K}(x)\\right| \\leq\\left(1+\\zeta_{K}(x)\\right) \\delta_{K}^{*} \\leq 2 \\zeta_{K}(x) \\delta_{K}^{*} .\n\\]\nEquations (20.56), (20.57), and \\(n \\delta_{K}^{* 2}=o(1)\\) together imply that\n\\[\n\\frac{n}{V_{K}(x)} r_{K}^{2}(x) \\leq \\frac{4}{\\underline{\\sigma}^{2}} n \\delta_{K}^{* 2}=o(1)\n\\]\nwhich is (20.55), as required."
  },
  {
    "objectID": "chpt20-series-reg.html#exercises",
    "href": "chpt20-series-reg.html#exercises",
    "title": "19  Series Regression",
    "section": "19.32 Exercises",
    "text": "19.32 Exercises\nExercise 20.1 Take the estimated model\n\\[\nY=-1+2 X+5(X-1) \\mathbb{1}\\{X \\geq 1\\}-3(X-2) \\mathbb{1}\\{X \\geq 2\\}+e .\n\\]\nWhat is the estimated marginal effect of \\(X\\) on \\(Y\\) for \\(X=3\\) ?\nExercise 20.2 Take the linear spline with three knots\n\\[\nm_{K}(x)=\\beta_{0}+\\beta_{1} x+\\beta_{2}\\left(x-\\tau_{1}\\right) \\mathbb{1}\\left\\{x \\geq \\tau_{1}\\right\\}+\\beta_{3}\\left(x-\\tau_{2}\\right) \\mathbb{1}\\left\\{x \\geq \\tau_{2}\\right\\}+\\beta_{4}\\left(x-\\tau_{3}\\right) \\mathbb{1}\\left\\{x \\geq \\tau_{3}\\right\\} .\n\\]\nFind the inequality restrictions on the coefficients \\(\\beta_{j}\\) so that \\(m_{K}(x)\\) is non-decreasing.\nExercise 20.3 Take the linear spline from the previous question. Find the inequality restrictions on the coefficients \\(\\beta_{j}\\) so that \\(m_{K}(x)\\) is concave.\nExercise 20.4 Take the quadratic spline with three knots\n\\[\nm_{K}(x)=\\beta_{0}+\\beta_{1} x+\\beta_{2} x^{3}+\\beta_{3}\\left(x-\\tau_{1}\\right)^{2} \\mathbb{1}\\left\\{x \\geq \\tau_{1}\\right\\}+\\beta_{4}\\left(x-\\tau_{2}\\right)^{2} \\mathbb{1}\\left\\{x \\geq \\tau_{2}\\right\\}+\\beta_{5}\\left(x-\\tau_{3}\\right)^{2} \\mathbb{1}\\left\\{x \\geq \\tau_{3}\\right\\} .\n\\]\nFind the inequality restrictions on the coefficients \\(\\beta_{j}\\) so that \\(m_{K}(x)\\) is concave.\nExercise 20.5 Consider spline estimation with one knot \\(\\tau\\). Explain why the knot \\(\\tau\\) must be within the sample support of \\(X\\). [Explain what happens if you estimate the regression with the knot placed outside the support of \\(X]\\)\nExercise 20.6 You estimate the polynomial regression model:\n\\[\n\\widehat{m}_{K}(x)=\\widehat{\\beta}_{0}+\\widehat{\\beta}_{1} x+\\widehat{\\beta}_{2} x^{2}+\\cdots+\\widehat{\\beta}_{p} x^{p} .\n\\]\nYou are interested in the regression derivative \\(m^{\\prime}(x)\\) at \\(x\\).\n\nWrite out the estimator \\(\\widehat{m}_{K}^{\\prime}(x)\\) of \\(m^{\\prime}(x)\\).\nIs \\(\\widehat{m}_{K}^{\\prime}(x)\\) is a linear function of the coefficient estimates?\nUse Theorem \\(20.8\\) to obtain the asymptotic distribution of \\(\\widehat{m}_{K}^{\\prime}(x)\\).\nShow how to construct standard errors and confidence intervals for \\(\\widehat{m}_{K}^{\\prime}(x)\\).\n\nExercise 20.7 Does rescaling \\(Y\\) or \\(X\\) (multiplying by a constant) affect the \\(\\mathrm{CV}(K)\\) function? The \\(K\\) which minimizes it?\nExercise 20.8 Take the NPIV approximating equation (20.35) and error \\(e_{K}\\).\n\nDoes it satisfy \\(\\mathbb{E}\\left[e_{K} \\mid Z\\right]=0\\) ?\nIf \\(L=K\\) can you define \\(\\beta_{K}\\) so that \\(\\mathbb{E}\\left[Z_{K} e_{K}\\right]=0\\) ?\nIf \\(L>K\\) does \\(\\mathbb{E}\\left[Z_{K} e_{K}\\right]=0\\) ?\n\nExercise 20.9 Take the cps09mar dataset (full sample). (a) Estimate a \\(6^{\\text {th }}\\) order polynomial regression of \\(\\log (\\) wage \\()\\) on experience. To reduce the ill-conditioned problem first rescale experience to lie in the interval \\([0,1]\\) before estimating the regression.\n\nPlot the estimated regression function along with 95% pointwise confidence intervals.\nInterpret the findings. How do you interpret the estimated function for experience levels above 65 ?\n\nExercise 20.10 Continuing the previous exercise, compute the cross-validation function (or alternatively the AIC) for polynomial orders 1 through 8.\n\nWhich order minimizes the function?\nPlot the estimated regression function along with \\(95 %\\) pointwise confidence intervals.\n\nExercise 20.11 Take the cps09mar dataset (full sample).\n\nEstimate a \\(6^{\\text {th }}\\) order polynomial regression of \\(\\log (\\) wage \\()\\) on education. To reduce the ill-conditioned problem first rescale education to lie in the interval \\([0,1]\\).\nPlot the estimated regression function along with \\(95 %\\) pointwise confidence intervals.\n\nExercise 20.12 Continuing the previous exercise, compute the cross-validation function (or alternatively the AIC) for polynomial orders 1 through 8.\n\nWhich order minimizes the function?\nPlot the estimated regression function along with \\(95 %\\) pointwise confidence intervals.\n\nExercise 20.13 Take the \\(\\mathrm{cps} 09 \\mathrm{mar}\\) dataset (full sample).\n\nEstimate quadratic spline regressions of \\(\\log (\\) wage \\()\\) on experience. Estimate four models: (1) no knots (a quadratic); (2) one knot at 20 years; (3) two knots at 20 and 40; (4) four knots at 10, 20, 30, \\(\\& 40\\). Plot the four estimates. Intrepret your findings.\nCompare the four splines models using either cross-validation or AIC. Which is the preferred specification?\nFor your selected specification plot the estimated regression function along with 95% pointwise confidence intervals. Intrepret your findings.\nIf you also estimated a polynomial specification do you prefer the polynomial or the quadratic spline estimates?\n\nExercise 20.14 Take the cps09mar dataset (full sample).\n\nEstimate quadratic spline regressions of \\(\\log (\\) wage \\()\\) on education. Estimate four models: (1) no knots (a quadratic); (2) one knot at 10 years; (3) three knots at 5,10 , and 15 ; (4) four knots at 4,8 , 12, & 16. Plot the four estimates. Intrepret your findings.\nCompare the four splines models using either cross-validation or AIC. Which is the preferred specification?\nFor your selected specification plot the estimated regression function along with 95% pointwise confidence intervals. Intrepret your findings. (d) If you also estimated a polynomial specification do you prefer the polynomial or the quadratic spline estimates?\n\nExercise 20.15 The RR2010 dataset is from Reinhart and Rogoff (2010). It contains observations on annual U.S. GDP growth rates, inflation rates, and the debt/gdp ratio for the long time span 1791-2009. The paper made the strong claim that GDP growth slows as debt/gdp increases, and in particular that this relationship is nonlinear with debt negatively affecting growth for debt ratios exceeding \\(90 %\\). Their full dataset includes 44 countries, our extract only includes the United States. Let \\(Y_{t}\\) denote GDP growth and let \\(D_{t}\\) denote debt/gdp. We will estimate the partially linear specification\n\\[\nY_{t}=\\alpha Y_{t-1}+m\\left(D_{t-1}\\right)+e_{t}\n\\]\nusing a linear spline for \\(m(D)\\).\n\nEstimate (1) linear model; (2) linear spline with one knot at \\(D_{t-1}=60\\); (3) linear spline with two knots at 40 and 80 . Plot the three estimates.\nFor the model with one knot plot with \\(95 %\\) confidence intervals.\nCompare the three splines models using either cross-validation or AIC. Which is the preferred specification?\nInterpret the findings.\n\nExercise 20.16 Take the DDK2011 dataset (full sample). Use a quadratic spline to estimate the regression of testscore on percentile.\n\nEstimate five models: (1) no knots (a quadratic); (2) one knot at 50; (3) two knots at 33 and 66; (4) three knots at 25,50 & 75 ; (5) knots at 20, 40, 60, & 80. Plot the five estimates. Intrepret your findings.\nSelect a model. Consider using leave-cluster-one CV.\nFor your selected specification plot the estimated regression function along with 95% pointwise confidence intervals. [Use cluster-robust standard errors.] Intrepret your findings.\n\nExercise 20.17 The CH J2004 dataset is from Cox, Hansen and Jimenez (2004). As described in Section \\(20.6\\) it contains a sample of 8684 urban Phillipino households. This paper studied the crowding-out impact of a family’s income on non-governmental transfers. Estimate an analog of Figure 20.2(b) using polynomial regression. Regress transfers on a high-order polynomial in income, and possibly a set of regression controls. Ideally, select the polynomial order by cross-validation. You will need to rescale the variable income before taking polynomial powers. Plot the estimated function along with \\(95 %\\) pointwise confidence intervals. Comment on the similarities and differences with Figure 20.2(b). For the regression controls consider the following options: (a) Include no additional controls; (b) Follow the original paper and Figure 20.2(b) by including the variables 12-26 listed in the data description file; (c) Make a different selection, possibly based on cross-validation.\nExercise 20.18 The AL1999 dataset is from Angrist and Lavy (1999). It contains 4067 observations on classroom test scores and explanatory variables including those described in Section 20.30. In Section \\(20.30\\) we report a nonparametric instrumental variables regression of reading test scores (avgverb) on classize, disadvantaged, enrollment, and a dummy for grade=4, using the Angrist-Levy variable (20.42) as an instrument. Repeat the analysis but instead of reading test scores use math test scores (avgmath) as the dependent variable. Comment on the similarities and differences with the results for reading test scores."
  },
  {
    "objectID": "chpt21-rdd.html#introduction",
    "href": "chpt21-rdd.html#introduction",
    "title": "20  Regression Discontinuity",
    "section": "20.1 Introduction",
    "text": "20.1 Introduction\nOne of the core goals in applied econometrics is estimation of treatment effects. A major barrier is that in observational data treatment is rarely exogenous. Techniques discussed so far in this textbook to deal with potential endogeneity include instrumental variables, fixed effects, and difference in differences. Another important method arises in the context of the regression discontinuity design. This is a rather special situation (not at the control of the econometrician) where treatment is determined by a threshold crossing rule. For example: (1) Do political incumbants have an advantage in elections? An incumbant is the winner of the previous election, which means their vote share exceeded a threshold. (2) What is the effect of college attendence? College students are admitted based on an admission exam, which means their exam score exceeded a specific threshold. In these contexts the treatment (incumbancy, college attendence) can be viewed as randomly assigned for individuals near the cut-off. (In the examples, for candidates who had vote shares near the winning threshold and for students who had admission exam scores near the cut-off threshold.) This setting is called the Regression Discontinuity Design (RDD). When it applies there are simple techniques for estimation of the causal effect of treatment.\nThe first use of regression discontinuity is attributed to Thistlethwaite and Campbell (1960). It was popularized in economics by Black (1999), Ludwig and Miller (2007), and Lee (2008). Important reviews include Imbens and Leimieux (2008), Lee and Leimieux (2010), and Cattaneo, Idrobo, and Titiunik (2020, \\(2021)\\)\nThe core model is sharp regression discontinuity where treatment is a discontinuous deterministic rule of an observable. Most applications, however, concern fuzzy regression discontinuity where the probability of treatment is discontinuous in an observable. We start by reviewing sharp regression discontinuity and then cover fuzzy regression discontinuity."
  },
  {
    "objectID": "chpt21-rdd.html#sharp-regression-discontinuity",
    "href": "chpt21-rdd.html#sharp-regression-discontinuity",
    "title": "20  Regression Discontinuity",
    "section": "20.2 Sharp Regression Discontinuity",
    "text": "20.2 Sharp Regression Discontinuity\nTake the potential outcomes framework. An individual is untreated if \\(D=0\\) and is treated if \\(D=1\\). The individual has outcome \\(Y_{0}\\) if untreated and \\(Y_{1}\\) if treated. The treatment effect for an individual is \\(\\theta=Y_{1}-Y_{0}\\), which is random. An observable covariate is \\(X\\). The conditional Average Treatment Effect (ATE) for the subpopulation with \\(X=x\\) is \\(\\theta(x)=\\mathbb{E}[\\theta \\mid X=x]\\).\nThe sharp regression discontinuity design occurs when treatment is determined by a threshold function of \\(X\\), e.g. \\(D=\\mathbb{1}\\{X \\geq c\\}\\). In most applications the threshold \\(c\\) is determined by policy or rule. The covariate \\(X\\) which determines treatment is typically called the running variable. The threshold \\(c\\) is often called the “cut-off”.\nIt may be helpful to discuss a specific example. Ludwig and Miller (2007) used a sharp regression discontinuity design to evaluate a U.S. federal anti-poverty program called Head Start. Head Start was established in 1965 to provide preschool, health, and other social services to poor children age three to five and their families. Head Start funding was awarded to local municipalities through a competitive grant application. Due to a worry that poor regions may not apply at the same rate as well-funded regions, during the spring of 1965 the federal government provided grant-writing assistance to the 300 poorest counties in the United States. The 300 counties were selected based on the poverty rate as measured by the 1960 U.S. census.\nAs Ludwig and Miller document, the result was a surge in applications from the assisted counties with a resulting surge in program funding. \\(80 %\\) of the 300 treated counties received Head Start support while only \\(43 %\\) of the remaining counties received support. Thus it seems reasonable to conclude that these counties received a substantial exogenous increase in funding.\nLudwig and Miller were interested to see if this increase in Head Start funding led to measurable changes in outcomes. Their paper examined both mortality and education. We will focus exclusively on mortality. Specifically, they were interested in the impact on mortality for children in the age range 5-9, for deaths they coded as “Head Start Related” (for example, tuberculosis) meaning that a goal of the Head Start program was to reduce these events. They were also interested in the long-term effects of this intervention so focused on mortality rates in the 1973-1983 period which is eight to eighteen years after the grant-writing intervention. A subset of their data (assembled by Cattaneo, Titiunik, and VazquezBare (2017)) is posted on the textbook website as LM2007.\nTo summarize, the question addressed by Ludwig and Miller was whether grant-writing assistance in 1965 to the 300 U.S. counties selected on a poverty index had a measurable effect on childhood mortality eight to eighteen years later in the same counties, relative to counties which did not receive the grantwriting assistance.\nIn this application the unit of measurement is a U.S. county. The outcome variable \\(Y\\) is the county mortality rate in 1973-1983. The running variable \\(X\\) is the county poverty rate (percentage of the population below the poverty line) in 1960. The cut-off \\(c\\) is 59.1984. (The later is simply due to the fact that there were 300 counties with poverty rates equal or above this cut-off.)"
  },
  {
    "objectID": "chpt21-rdd.html#identification",
    "href": "chpt21-rdd.html#identification",
    "title": "20  Regression Discontinuity",
    "section": "20.3 Identification",
    "text": "20.3 Identification\nIn this section we present the core identification theorem for the regression discontinuity model. Recall that \\(\\theta\\) is the random individual treatment effect and \\(\\theta(x)=\\mathbb{E}[\\theta \\mid X=x]\\) is the conditional ATE. Set \\(\\bar{\\theta}=\\theta(c)\\), the conditional ATE for the subpopulation at the cut-off. This is the subpopulation affected at the margin by the decision to set the cut-off at \\(c\\). The core identification theorem states that \\(\\bar{\\theta}\\) is identified by the regression discontinuity design under mild assumptions.\nLet \\(m(x)=\\mathbb{E}[Y \\mid X=x], m_{0}(x)=\\mathbb{E}\\left[Y_{0} \\mid X=x\\right]\\), and \\(m_{1}(x)=\\mathbb{E}\\left[Y_{1} \\mid X=x\\right]\\). Note that \\(\\theta(x)=m_{1}(x)-\\) \\(m_{0}(x)\\). Set \\(m(x+)=\\lim _{z \\downarrow x} m(z)\\) and \\(m(x-)=\\lim _{z \\uparrow x} m(z)\\).\nThe following is the core identification theorem for the regression discontinuity design. It is due to Hahn, Todd, and Van der Klaauw (2001).\nTheorem 21.1 Assume that treatment is assigned as \\(D=\\mathbb{1}\\{X \\geq c\\}\\). Suppose that \\(m_{0}(x)\\) and \\(m_{1}(x)\\) are continuous at \\(x=c\\). Then \\(\\bar{\\theta}=m(c+)-m(c-)\\). The conditions for Theorem \\(21.1\\) are minimal. The continuity of \\(m_{0}(x)\\) and \\(m_{1}(x)\\) means that the conditional expectation of the untreated and treated outcome are continuously affected by the running variable. Take the Head Start example. \\(m_{0}(x)\\) is the average mortality rate given the poverty rate for counties which received no grant-writing assistance. \\(m_{1}(x)\\) is the average mortality rate for counties which received grant-writing assistance. There is no reason to expect a discontinuity in either function.\nThe intuition for the theorem can be seen in Figure 21.1(a). The two continuous functions plotted are the CEFs \\(m_{0}(x)\\) and \\(m_{1}(x)\\). The vertical distance between these functions is the conditional ATE function \\(\\theta(x)\\). Since the treatment rule assigns all counties with \\(X \\geq c\\) to treatment and all counties with \\(X<c\\) to non-treatment the CEF of the observed outcome \\(m(x)\\) is the solid line, which equals \\(m_{0}(x)\\) for \\(x<c\\) and \\(m_{1}(x)\\) for \\(x \\geq 0\\). The discontinuity in \\(m(x)\\) at \\(x=c\\) equals the RDD treatment effect \\(\\bar{\\theta}\\).\nThe plot in Figure 21.1 (a) was designed to mimic what we might expect in the Head Start application. We have plotted both \\(m_{0}(x)\\) and \\(m_{1}(x)\\) as increasing functions of \\(x\\), meaning that the mortality rate is increasing in the poverty rate. We also have plotted the functions so that \\(m_{1}(x)\\) lies below \\(m_{0}(x)\\) as we expect that grant-writing assistance should reduce mortality.\nWe know from regression theory that the CEF \\(m(x)\\) is generically identified. Thus so is the RDD treatment effect \\(\\bar{\\theta}=m(c+)-m(c-)\\). This is the key take-away from the identification theorem. The regression discontinuity design identifies the conditional ATE at the treatment cut-off. In the Head Start example this is the ATE for a county with a poverty rate of \\(59.1984 %\\). Use of \\(\\bar{\\theta}\\) to infer the ATE for other counties is extrapolation. As displayed in Figure 21.1(a) all that is identified is the solid line, the dashed lines are not identified. Thus a limitation of the RDD approach is that it estimates a narrowly-defined treatment effect.\nIdentification of the RDD treatment effect is intertwined with nonparametric treatment of the functions \\(m_{0}(x)\\) and \\(m_{1}(x)\\). If parametric (e.g. linear) forms are imposed, then the best-fitting approximations for \\(x<c\\) and \\(x \\geq c\\) will generically have a discontinuity even if the true CEF is continuous. Thus a nonparametric treatment is essential to preclude falsely labeling nonlinearity as a discontinuity.\nA formal proof of Theorem \\(21.1\\) is simple. We can write the observed outcome as \\(Y=Y_{0} \\mathbb{1}\\{X<c\\}+\\) \\(Y_{1} \\mathbb{1}\\{X \\geq c\\}\\). Taking expectations conditional on \\(X=x\\) we find\n\\[\nm(x)=m_{0}(x) \\mathbb{1}\\{x<c\\}+m_{1}(x) \\mathbb{1}\\{x \\geq c\\} .\n\\]\nSince \\(m_{0}(x)\\) and \\(m_{1}(x)\\) are continuous at \\(x=c\\), we deduce \\(m(c+)=m_{1}(c)\\) and \\(m(c-)=m_{0}(c)\\). Thus \\(m(c+)-m(c-)=m_{1}(c)-m_{0}(c)=\\theta(c)\\), as claimed."
  },
  {
    "objectID": "chpt21-rdd.html#estimation",
    "href": "chpt21-rdd.html#estimation",
    "title": "20  Regression Discontinuity",
    "section": "20.4 Estimation",
    "text": "20.4 Estimation\nOur goal is estimation of the conditional ATE \\(\\bar{\\theta}\\) given observations \\(\\left\\{Y_{i}, X_{i}\\right\\}\\) and known cut-off \\(c\\). The conditional ATE can be calculated from the CEF \\(m(x)\\). Estimation of the CEF nonparametrically allowing for a discontinuity is the same as separately estimating the CEF for the untreated observations \\(X_{i}<c\\) and the treated observations \\(X_{i} \\geq c\\). The estimator for \\(\\bar{\\theta}\\) is the difference between the adjoining estimated endpoints.\nThe previous two chapters have studied nonparametric kernel and series regression. One of the findings is that for boundary estimation the preferred method is local linear (LL) regression (Section 19.4). In contrast, the Nadaraya-Watson estimator is biased at a boundary point (see Section 19.10), and series estimators have high variance at the boundary (see Section \\(20.14\\) and Gelman and Imbens (2019)). Consequently, local linear estimation is preferred and is the most widely used technique \\({ }^{1}\\) for regression discontinuity designs.\n\\({ }^{1}\\) Some authors use polynomials in addition to local linear estimation as an appeal to “robustness”. This should be discouraged as argued in Gelman and Imbens (2019).\n\n\nSharp Regression Discontinuity\n\n\n\nEffect of Head Start on Childhood Mortality\n\nFigure 21.1: Sharp Regression Discontinuity Design\nTo describe the estimator set\n\\[\nZ_{i}(x)=\\left(\\begin{array}{c}\n1 \\\\\nX_{i}-x\n\\end{array}\\right) .\n\\]\nLet \\(K(u)\\) be a kernel function and \\(h\\) a bandwidth. The LL coefficient estimator for \\(x<c\\) is\n\\[\n\\widehat{\\beta}_{0}(x)=\\left(\\sum_{i=1}^{n} K\\left(\\frac{X_{i}-x}{h}\\right) Z_{i}(x) Z_{i}(x)^{\\prime} \\mathbb{1}\\left\\{X_{i}<c\\right\\}\\right)^{-1}\\left(\\sum_{i=1}^{n} K\\left(\\frac{X_{i}-x}{h}\\right) Z_{i}(x) Y_{i} \\mathbb{1}\\left\\{X_{i}<c\\right\\}\\right)\n\\]\nand for \\(x \\geq c\\) is\n\\[\n\\widehat{\\beta}_{1}(x)=\\left(\\sum_{i=1}^{n} K\\left(\\frac{X_{i}-x}{h}\\right) Z_{i}(x) Z_{i}(x)^{\\prime} \\mathbb{1}\\left\\{X_{i} \\geq c\\right\\}\\right)^{-1}\\left(\\sum_{i=1}^{n} K\\left(\\frac{X_{i}-x}{h}\\right) Z_{i}(x) Y_{i} \\mathbb{1}\\left\\{X_{i} \\geq c\\right\\}\\right) .\n\\]\nThe estimator of the CEF is the first element of the coefficient vectors\n\\[\n\\widehat{m}(x)=\\left[\\widehat{\\beta}_{0}(x)\\right]_{1} \\mathbb{1}\\{x<c\\}+\\left[\\widehat{\\beta}_{1}(x)\\right]_{1} \\mathbb{1}\\{x \\geq c\\} .\n\\]\nThe estimator of \\(\\bar{\\theta}\\) is the difference at \\(x=c\\)\n\\[\n\\widehat{\\theta}=\\left[\\widehat{\\beta}_{1}(c)\\right]_{1}-\\left[\\widehat{\\beta}_{0}(c)\\right]_{1}=\\widehat{m}(c+)-\\widehat{m}(c-) .\n\\]\nFor efficient estimation at boundary points the Triangular kernel is recommended. However, the Epanechnikov and Gaussian have similar efficiencies (see Section 19.10). Some authors have made a case for the Rectangular kernel as this permits standard regression software to be used. There is an efficiency loss (3% in root AMSE) in return for this convenience.\nThe CEF estimate \\(\\widehat{m}(x)\\) should be plotted to give a visual inspection of the regression function and discontinuity. Many authors plot the CEF only over the support near \\(x=c\\) to emphasize the local nature of the estimation. Confidence bands should be calculated and plotted as described in Section 19.17. These are calculated separately for the non-treatment and treatment subsamples but otherwise are identical to those described in Section 19.17.\nTo illustrate, Figure 21.1(b) displays our estimates of the Ludwig-Miller (2007) Head Start RDD model for childhood mortality due to HS-related causes. We use a normalized \\({ }^{2}\\) Triangular kernel and a bandwidth of \\(h=8\\). This bandwidth choice is described in Section 21.6. The x-axis is the 1960 poverty rate. The cut-off is \\(59.1984 %\\). Counties below the cut-off did not receive grant-writing assistance, counties above the cut-off received assistance. The mortality rate is on the y-axis (deaths per 100,000). The estimates show that the mortality rate is increasing in the poverty rate (nearly linear) with a substantial downward discontinuity at the \\(59.1984 %\\) cut-off. The discontinuity is about \\(1.5\\) deaths per 100,000 . The confidence bands indicate that the estimated CEFs have a fair amount of uncertainty at the boundaries. The CEF in the treated sample appears nonlinear and the confidence bands are very wide.\nThere is a custom in the applied economics literature to display Figure 21.1(b) somewhat differently. Rather than displaying confidence intervals along with the local linear estimates many applied economists display binned means. The binned means are displayed by squares or triangles and are meant to indicate a raw estimate of the nonparametric shape of the CEF. This custom is a poor choice, a bad habit, and should be avoided. There are two problems with this practice. First, the use of symbols creates the visual impression of a scatter plot of raw data, when in fact what is displayed are binned means. The latter is a nonparametric histogram-shaped estimator, and should be displayed as a histogram rather than as a scatter plot. Second, binned means are not really raw data, but are instead a different (and inaccurate) nonparametric estimator. Binned means is the same as the Nadaraya-Watson estimator using a Rectangular kernel and only evalutated at a grid of points rather than continuously. Local linear estimation is superior to the Nadaraya-Watson, any kernel is superior to the Rectangular, and there is no reason to evaluate only on an arbitrary grid. These plots are not “best practice”; rather, they are a bad habit which arose from undisciplined applied practice. The best practice is to plot the best possible nonparametric estimator and to plot confidence intervals to convey uncertainty."
  },
  {
    "objectID": "chpt21-rdd.html#inference",
    "href": "chpt21-rdd.html#inference",
    "title": "20  Regression Discontinuity",
    "section": "20.5 Inference",
    "text": "20.5 Inference\nAs described in Theorems \\(19.6\\) and 19.9, the LL estimator \\(\\widehat{m}(x)\\) is asymptotically normal under standard regularity conditions. This extends to the RDD estimator \\(\\widehat{\\theta}\\). It has asymptotic bias\n\\[\n\\operatorname{bias}[\\widehat{\\theta}]=\\frac{h^{2} \\sigma_{K^{*}}^{2}}{2}\\left(m^{\\prime \\prime}(c+)-m^{\\prime \\prime}(c-)\\right)\n\\]\nand variance\n\\[\n\\operatorname{var}[\\widehat{\\theta}]=\\frac{R_{K}^{*}}{n h}\\left(\\frac{\\sigma^{2}(c+)}{f(c+)}+\\frac{\\sigma^{2}(c-)}{f(c-)}\\right) .\n\\]\nThe asymptotic variance can be estimated by the sum of the asymptotic variance estimators of the two boundary regression estimators as described in Section 19.16. Let \\(\\widetilde{e}_{i}\\) be the leave-one-out prediction error and set\n\\[\n\\begin{gathered}\nZ_{i}=\\left(\\begin{array}{c}\n1 \\\\\nX_{i}-c\n\\end{array}\\right) \\\\\nK_{i}=K\\left(\\frac{X_{i}-c}{h}\\right) .\n\\end{gathered}\n\\]\n\\({ }^{2}\\) Normalized to have unit variance. Some software implements the Triangular kernel scaled to have support on [-1,1]. The results are identical if the bandwidth is multiplied by \\(\\sqrt{6}\\). For example, my estimates using \\(h=8\\) and a normalized Triangular kernel are the same as estimates using a \\([-1,1]\\) Triangular kernel with a bandwidth of \\(h=19.6\\). The covariance matrix estimators are\n\\[\n\\begin{aligned}\n\\widehat{\\boldsymbol{V}}_{0} &=\\left(\\sum_{i=1}^{n} K_{i} Z_{i} Z_{i}^{\\prime} \\mathbb{1}\\left\\{X_{i}<c\\right\\}\\right)^{-1}\\left(\\sum_{i=1}^{n} K_{i}^{2} Z_{i} Z_{i}^{\\prime} \\widetilde{e}_{i}^{2} \\mathbb{1}\\left\\{X_{i}<c\\right\\}\\right)\\left(\\sum_{i=1}^{n} K_{i} Z_{i} Z_{i}^{\\prime} \\mathbb{1}\\left\\{X_{i}<c\\right\\}\\right)^{-1} \\\\\n\\widehat{\\boldsymbol{V}}_{1} &=\\left(\\sum_{i=1}^{n} K_{i} Z_{i} Z_{i}^{\\prime} \\mathbb{1}\\left\\{X_{i} \\geq c\\right\\}\\right)^{-1}\\left(\\sum_{i=1}^{n} K_{i}^{2} Z_{i} Z_{i}^{\\prime} \\widetilde{e}_{i}^{2} \\mathbb{1}\\left\\{X_{i} \\geq c\\right\\}\\right)\\left(\\sum_{i=1}^{n} K_{i} Z_{i} Z_{i}^{\\prime} \\mathbb{1}\\left\\{X_{i} \\geq c\\right\\}\\right)^{-1} .\n\\end{aligned}\n\\]\nThe asymptotic variance estimator for \\(\\widehat{\\theta}\\) is the sum of the first diagonal element from these two covariance matrix estimators, \\(\\left[\\widehat{\\boldsymbol{V}}_{0}\\right]_{11}+\\left[\\widehat{\\boldsymbol{V}}_{0}\\right]_{11}\\). The standard error for \\(\\hat{\\theta}\\) is the square root of the variance estimator.\nInferential statements about the treatment effect \\(\\bar{\\theta}\\) are affected by bias just as in any nonparametric estimation context. In general the degree of bias is uncertain. There are two recommendations which may help to reduce the finite sample bias. First, use a common bandwidth for estimation of the LL regression on each sub-sample. When \\(m(x)\\) has a continuous second derivative at \\(x=c\\) this will result in a zero first-order asymptotic bias. Second, use a bandwidth which is smaller than the AMSE-optimal bandwidth. This reduces the bias at the cost of increased variance and standard errors. Overall this leads to more honest inference statements.\nTable 21.1: RDD Estimates of the Effect of Head Start Assistance on Childhood Mortality\n\nTo illustrate, Table \\(21.1\\) presents the RDD estimate of the Head Start treatment effect (the effect of grant-writing assistance on a county with poverty rate at the policy cut-off). This equals the vertical distance between the estimated CEFs from Figure 21.1(b). The point estimate is \\(-1.51\\) with a standard error of \\(0.71\\). The t-statistic for a test of no effect has a p-value of \\(3 %\\), consistent with statistical significance at conventional levels. The estimated policy impact is large. It states that federal grant-writing assistance, and the resulting surge in spending on the Head Start program, led to a long-term decrease in targeted mortality by about \\(1.5\\) children per 100,000. Given that the estimated untreated mortality rate is \\(3.3\\) children per 100,000 at the cut-off this is a near \\(50 %\\) decrease in the mortality rate."
  },
  {
    "objectID": "chpt21-rdd.html#bandwidth-selection",
    "href": "chpt21-rdd.html#bandwidth-selection",
    "title": "20  Regression Discontinuity",
    "section": "20.6 Bandwidth Selection",
    "text": "20.6 Bandwidth Selection\nIn nonparametric estimation the most critical choice is the bandwidth. This is especially important in RDD estimation as there is not broad agreement on the best bandwidth selection method. It therefore is prudent to calculate several data-based bandwidth rules before estimation. I will describe two simple approaches based on the global fit of the RDD estimator.\nOur first suggestion is the Rule-of-Thumb (ROT) bandwidth (19.9) of Fan and Gijbels (1996) modified to allow for a discontinuity at \\(x=c\\). The method requires a reference model. A modest extension of FanGijbels’ approach is a \\(q^{t h}\\) order polynomial plus a level shift discontinuity. This model is\n\\[\nm(x)=\\beta_{0}+\\beta_{1} x+\\beta_{2} x^{2}+\\cdots+\\beta_{q} x^{q}+\\beta_{q+1} D\n\\]\nwhere \\(D=\\mathbb{1}\\{x \\geq c\\}\\). Estimate this model by least squares, obtain coefficient estimates and the variance estimate \\(\\widehat{\\sigma}^{2}\\). From the coefficient estimates calculate the estimated second derivative\n\\[\n\\widehat{m}^{\\prime \\prime}(x)=2 \\widehat{\\beta}_{2}+6 \\widehat{\\beta}_{3} x+12 \\widehat{\\beta}_{4} x^{2}+\\cdots+q(q-1) \\widehat{\\beta}_{q} x^{q-2} .\n\\]\nThe constant \\(\\bar{B}\\) in (19.9) is estimated by\n\\[\n\\widehat{B}=\\frac{1}{n} \\sum_{i=1}^{n}\\left(\\frac{1}{2} \\widehat{m}^{\\prime \\prime}\\left(X_{i}\\right)\\right)^{2} \\mathbb{1}\\left\\{\\xi_{1} \\leq X_{i} \\leq \\xi_{2}\\right\\}\n\\]\nwhere \\(\\left[\\xi_{1}, \\xi_{2}\\right]\\) is the region of evaluation (and can be set to equal to the support of \\(X\\) when the latter is bounded). The reference bandwidth (19.9) is then\n\\[\nh_{\\mathrm{rot}}=0.58\\left(\\frac{\\widehat{\\sigma}^{2}\\left(\\xi_{2}-\\xi_{1}\\right)}{\\widehat{B}}\\right)^{1 / 5} n^{-1 / 5} .\n\\]\nFan-Gijbels recommend \\(q=4\\) but other choices can be used for the polynomial order. The ROT bandwidth (21.3) is appropriate for any normalized (variance one) kernel. For the unnormalized rectangular kernel \\(K(u)=1 / 2\\) for \\(|u| \\leq 1\\) replace the constant \\(0.58\\) with \\(1.00\\). For the unnormalized Triangular kernel \\(K(u)=1-|u|\\) for \\(|u| \\leq 1\\) replace the constant \\(0.58\\) with \\(1.42\\).\nAnother useful method is cross-validation. CV for the RDD estimator is essentially the same as for any other nonparametric estimator. For each bandwidth the leave-one-out residuals are calculated and their sum of squares recorded. The bandwidth which minimizes this criterion is the CV-selected choice. Plots of the CV criterion as a function of \\(h\\) can aid in determinining the sensitivity of the fit with respect to the bandwidth.\nThese two proposals aim to produce a bandwidth \\(h\\) with global accuracy. An alternative is a bandwidth selection rule which aims at accuracy at or near the cut-off. The advantage of the global approach is that it is a simpler estimation problem and thus more accurate and less variable. Bandwidth estimation is a hard problem. Noise in estimation of the bandwidth will translate into estimation noise for the RDD estimate. On the other hand, methods which aim at accuracy at the cut-off are targeted at the object of interest. This is a challenging estimation issue so I will not review it further. For specific proposals see Imbens and Kalyanaraman (2012), Arai and Ichimura (2018), and Cattaneo, Idrobo, Titiunik (2020).\nA compromise is calculate the CV criteria with the region of evaluation \\(\\left[\\xi_{1}, \\xi_{2}\\right]\\) a subset of the full support of \\(X\\) centered close to the cut-off. Several of the early review papers recommended this approach. The challenge with this approach is that the CV criteria is a noisy estimator and by restricting the region of evaluation we are increasing its estimation variance. This increases noise.\nIn applications I recommend that you start by calculating the Fan-Gijbels ROT bandwidth for several values of polynomial order \\(q\\). When comparing the results pay attention to the precision of the coefficients in the polynomial regression. If the high-order powers are imprecisely estimated the bandwidth estimates may be noisy as well. Second, find the bandwidth which minimizes the cross-validation criterion. Plot the CV criterion. If it is relatively flat this informs you that it is difficult to rank bandwidths. Combine the above information to select an AMSE-minimizing bandwidth. Then reduce this bandwidth somewhat (perhaps 25%) to reduce estimation bias.\nSome robustness checking (estimation with alternative bandwidths) is prudent, but narrowly so. A rather odd implication of the robustness craze is to desire results which do not change with bandwidths. Contrariwise, if the true regression function is nonlinear then results will change with bandwidths. What you should expect is that as you reduce the bandwidth the estimated function will reveal a combination of shape and noise accompanied by wider confidence bands. As you increase the bandwidth the estimates will straighten out and the confidence bands will narrow. The narrowness means that the estimates have reduced variance but this comes at the cost of increased (and uncertain) bias. We illustrate using the Ludwig-Miller (2007) Head Start application. We calculated the modified FanGijbels ROT using \\(q=2,3\\), and 4, obtaining bandwidths of \\(h_{\\mathrm{rot}}(q=2)=24.6, h_{\\mathrm{rot}}(q=3)=11.0\\), and \\(h_{\\text {rot }}(q=4)=5.2\\). These results are sensitive to the choice of polynomial. Examining these polynomial regressions we see that the third and fourth coefficient estimates have large standard errors so are noisy. We next evalulated the cross-validation criterion on the region [1,30] (not shown). We found that the CV criterion is monotonically decreasing with \\(h\\), though quite flat for \\(h \\geq 20\\). Essentially the CV criterion recommends an infinite bandwidth which means using all observations equally weighted. Since we want a bandwidth which is smaller than AMSE-optimal, we lean towards smaller bandwidths and take a rough average of the ROT bandwidths with \\(q=3\\) and \\(q=4\\) to obtain \\(h=8\\). This is the bandwidth used in the empirical results shown in this chapter.\nLarger bandwidths result in flatter (more linear) estimated conditional mean functions and a smaller estimated Head Start effect. Smaller bandwidths result in more curvature in the estimated conditional mean functions, in particular for the section above the cut-off."
  },
  {
    "objectID": "chpt21-rdd.html#rdd-with-covariates",
    "href": "chpt21-rdd.html#rdd-with-covariates",
    "title": "20  Regression Discontinuity",
    "section": "20.7 RDD with Covariates",
    "text": "20.7 RDD with Covariates\nA powerful implication of Theorem \\(21.1\\) is that covariates are not necessary to identify the conditional ATE. This implies that augmenting the regression model to include covariates is not necessary for estimation and inference. The precision of estimation, however, will be affected. Inclusion of relevant covariates can reduce the equation error. It is therefore prudent to consider the addition of relevant covariates when available.\nDenote the variables as \\((Y, X, Z)\\) where \\(Z\\) is a vector of covariates. Again consider the potential outcomes framework where \\(Y_{0}\\) and \\(Y_{1}\\) are the outcome with and without treatment. Assume that the CEFs take the partially linear form\n\\[\n\\begin{aligned}\n&\\mathbb{E}\\left[Y_{0} \\mid X=x, Z=z\\right]=m_{0}(x)+\\beta^{\\prime} z \\\\\n&\\mathbb{E}\\left[Y_{1} \\mid X=x, Z=z\\right]=m_{1}(x)+\\beta^{\\prime} z .\n\\end{aligned}\n\\]\nFor simplicity we assume that the linear coefficients are the same in the two equations. This is not essential but simplifies the estimation strategy. It follows that the CEF for \\(Y\\) equals\n\\[\nm(x, z)=m_{0}(x) \\mathbb{1}\\{x, c\\}+m_{1}(x) \\mathbb{1}\\{x \\geq c\\}+\\beta^{\\prime} z .\n\\]\nA minor extension of Theorem \\(21.1\\) shows that the conditional ATE is \\(\\bar{\\theta}=m(c+, z)-m(c-, z)\\).\nDifferent authors have suggested different methods for estimation of the RDD with covariates model. The preferred method is the estimator of Robinson (1988). See Section 19.24. (It is preferred because Robinson demonstrated that it is semiparametrically efficient while the other suggestions have no efficiency justification.) The estimation method is as follows.\n\nUse the RDD local linear estimator to regress \\(Y_{i}\\) on \\(X_{i}\\) to obtain the first-step fitted values \\(\\widehat{m}_{i}=\\) \\(\\widehat{m}\\left(X_{i}\\right)\\)\nUsing LL regression, regress \\(Z_{i 1}\\) on \\(X_{i}, Z_{i 2}\\) on \\(X_{i}, \\ldots\\), and \\(Z_{i k}\\) on \\(X_{i}\\), obtaining the fitted values for the covariates, say \\(\\widehat{g}_{1 i}, \\ldots, \\widehat{g}_{k i}\\).\nRegress \\(Y_{i}-\\widehat{m}_{i}\\) on \\(Z_{1 i}-\\widehat{g}_{1 i}, \\ldots, Z_{k i}-\\widehat{g}_{k i}\\) to obtain the coefficient estimate \\(\\widehat{\\beta}\\) and standard errors.\nConstruct the residual \\(\\widehat{e}_{i}=Y_{i}-Z_{i}^{\\prime} \\widehat{\\beta}\\). 5. Use the RDD local linear estimator to regress \\(\\widehat{e}_{i}\\) on \\(X_{i}\\) to obtain the nonparametric estimator \\(\\widehat{m}(x)\\), conditional ATE \\(\\widehat{\\theta}\\), and associated standard errors.\n\nAs shown by Robinson (1988) and discussed in Section 19.24, the above estimator is semiparametrically efficient, the conventional asymptotic theory valid, and conventional inference is valid. Thus the estimators can be used to assess the conditional ATE.\nAs mentioned above, inclusion of covariates does not alter the conditional ATE parameter \\(\\bar{\\theta}\\) under correct specification. Inclusion of covariates can, however, affect the conditional mean function \\(m(x)\\) at points \\(x\\) away from the discontinuity. Covariates will also affect the precision of the estimator and standard errors.\nTo illustrate, we augment the Ludwig-Miller Head Start estimates with two covariates: the countylevel Black population percentage, and the county-level urban population percentage. These variables can be viewed as proxies for income. We estimate the model using the Robinson estimator. The estimated nonlinear function \\(m(x)\\) is displayed in Figure 21.2(a), the coefficient estimates in Table 21.1.\nComparing Figure 21.2(a) with Figure 21.1(b) it appears that the estimated conditional ATE (the treatment effect of the policy) is about the same but the shape of \\(m(x)\\) is different. With the covariates included \\(m(x)\\) is considerably flatter. Examining Table \\(21.1\\) we can see that the estimated treatment effect is nearly the same as in the baseline model without covariates. We also see that the coefficient on the Black percentage is positive and that on the urban percentage is negative, consistent with the view that these are serving as proxies for income.\n\n\nRDD with Covariates\n\n\n\nHistogram of Poverty Rate\n\nFigure 21.2: RDD Diagnostics"
  },
  {
    "objectID": "chpt21-rdd.html#a-simple-rdd-estimator",
    "href": "chpt21-rdd.html#a-simple-rdd-estimator",
    "title": "20  Regression Discontinuity",
    "section": "20.8 A Simple RDD Estimator",
    "text": "20.8 A Simple RDD Estimator\nA simple RDD estimator can be implement by a standard regression using conventional software. It is equivalent to a LL estimator with an unnormalized Rectangular bandwidth. Estimate the regression\n\\[\nY=\\beta_{0}+\\beta_{1} X+\\beta_{3}(X-c) D+\\theta D+e\n\\]\nfor the subsample of observations such that \\(|X-c| \\leq h\\). The coefficient estimate \\(\\widehat{\\theta}\\) is the estimated conditional ATE and inference can proceed conventionally using regression standard errors. The most important choice is the bandwidth. The ROT choice is (21.3) with \\(1.00\\) replacing the constant \\(0.58\\).\nTo illustrate, take the Head Start sample. For the normalized Triangular kernel we had used a bandwidth of \\(h=8\\). This is consistent with a bandwidth of \\(h=8 \\sqrt{3} \\simeq 13.8\\) for the unnormalized Rectangular kernel. We took the subsample of 482 with poverty rates in the interval \\(59.1984 \\pm 13.8=[45.4,72.0]\\) and estimated equation (21.4) by least squares. The estimates are\n\\[\n\\widehat{Y}=\\begin{array}{cc}\n-3.11+ \\\\\n(9.13) & 0.11 \\\\\n(0.17)\n\\end{array} \\quad X+\\underset{(0.23)}{0.18}(X-59.2) D-\\underset{(1.06)}{2.20} D .\n\\]\nThe point estimate \\(-2.2\\) of the conditional ATE is larger than those reported in Table \\(21.1\\) but within sampling variation. The standard error for the effect is also larger, consistent with our expectation that the rectangular kernel estimator is less accurate."
  },
  {
    "objectID": "chpt21-rdd.html#density-discontinuity-test",
    "href": "chpt21-rdd.html#density-discontinuity-test",
    "title": "20  Regression Discontinuity",
    "section": "20.9 Density Discontinuity Test",
    "text": "20.9 Density Discontinuity Test\nThe core identification theorem assumes that the CEFs \\(m_{0}(x)\\) and \\(m_{1}(x)\\) are continuous at the cutoff. These assumptions may be violated if the running variable is manipulated by individuals seeking or avoiding treatment. Manipulation to obtain treatment is likely to lead to bunching of the running variable just above or below the cut-off. If there is no manipulation we expect the density of \\(X\\) to be continuous at \\(x=c\\), but if there is manipulation we expect that there might be a discontinuity in the density of \\(X\\) at \\(x=c\\).\nA reasonable specification check is to assess if the density \\(f(x)\\) of \\(X\\) is continuous at \\(x=c\\). Some care needs to be exercised in implementation, however, as conventional density estimators smooth over discontinuities and conventional density estimators are biased at boundary points (similarly to the bias of the Nadaraya-Watson estimator at boundary points).\nA simple visual check is the histogram of the running variable with narrow bins, carefully constructed so that no bin spans the cut-off. If the histogram bins display no evidence of bunching at one side of the cut-off this is consistent with the hypothesis that the density is continuous at the cut-off; on the other hand if there is a noticable spike on either side this is inconsistent with the hypothesis of correct specification.\nIn the Head Start example it is not credible that the running variable was manipulated by the individual counties because it was constructed from the 1960 census by a federal agency in 1965 . Never-the-less we can examine the evidence. In Figure 21.2(b) we display a histogram of frequency counts for the running variable (county poverty rate), with bins of width 2, constructed so that one of the bin endpoints falls exactly at the cut-off (the solid line). The histogram appears to be continuously decreasing throughout its support. In particular there is no visual evidence of bunching around the cut-off.\nMcCrary (2008) implements a formal test for continuity of the density at the cut-off. I only give a brief summary here; see his paper for details. The first step is a fine histogram estimator, similar to Figure 21.2(b) but with more narrow bin widths. The second step is to apply the RDD local linear estimator treating the histogram heights as the outcome variable and the bin midpoints at the running variable. This is a local linear density estimator and is not subject to the boundary bias problems of the conventional kernel density estimator. The RDD conditional ATE is the difference in the density at the cut-off. McCrary derives the asymptotic distribution of the estimator of the density difference and proposes an appropriate t-statistic for testing the hypothesis of a continuous density. If the statistic is large this is evidence against the assumption of no manipulation, suggesting that the RDD design is not appropriate."
  },
  {
    "objectID": "chpt21-rdd.html#fuzzy-regression-discontinuity",
    "href": "chpt21-rdd.html#fuzzy-regression-discontinuity",
    "title": "20  Regression Discontinuity",
    "section": "20.10 Fuzzy Regression Discontinuity",
    "text": "20.10 Fuzzy Regression Discontinuity\nThe sharp regression discontinuity requires that the cut-off perfectly separates treatment from nontreatment. An alternative context is where this separation is imperfect but the conditional probability of treatment is discontinuous at the cut-off. This is called fuzzy regression discontinuity (FRD).\nAgain consider the potential outcomes framework, where \\(Y_{0}\\) and \\(Y_{1}\\) are the outcomes without treatment and with treatment, \\(\\theta=Y_{1}-Y_{0}\\) is the treatment effect, \\(X\\) is the running variable, the conditional average treatment effect at the cutoff is \\(\\bar{\\theta}=\\mathbb{E}[\\theta \\mid X=c]\\), and \\(D=1\\) indicates treatment. Define the conditional probability of treatment\n\\[\np(x)=\\mathbb{P}[D=1 \\mid X=x] .\n\\]\nand the left and right limits at the cut-off \\(p(c+)\\) and \\(p(c-)\\). The FRD applies when \\(p(c+) \\neq p(c-)\\).\nThis siutation is illustrated in Figure 21.3(a). This displays the conditional probability of treatment as a function of the running variable \\(X\\) with a discontinuity at \\(X=c\\).\n\n\nConditional Treatment Probability\n\n\n\nFuzzy Regression Discontinuity\n\nFigure 21.3: Fuzzy Regression Discontinuity Design\nThe following is the core identification theorem for the regression discontinuity design. It is due to Hahn, Todd, and Van der Klaauw (2001).\nTheorem 21.2 Suppose that \\(m_{0}(x)\\) and \\(m_{1}(x)\\) are continuous at \\(x=c, p(x)\\) is discontinuous at \\(x=c\\), and \\(D\\) is independent of \\(\\theta\\) for \\(X\\) near \\(c\\). Then\n\\[\n\\bar{\\theta}=\\frac{m(c+)-m(c-)}{p(c+)-p(c-)} .\n\\]\nTheorem \\(21.2\\) is a more substantial identification result than Theorem \\(21.1\\) as it is inherently surprising. It states that the conditional ATE is identified by the ratio of the discontinuities in the CEF and conditional probability functions under the stated assumptions. This broadens the scope for potential application of the regression discontinuity framework beyond the sharp RDD.\nIn addition to the discontinuity of \\(p(x)\\), the key additional assumption relative to Theorem \\(21.1\\) is that treatment \\(D\\) is independent of the treatment effect \\(\\theta\\) at \\(X=x\\). This is a strong assumption. It means that treatment assignment is randomly assigned for individuals with \\(X\\) near \\(c\\). This does not allow, for example, for individuals to select into treatment, for then individuals with high treatment effects \\(\\theta\\) are more likely to seek treatment than individuals with low treatment effects \\(\\theta\\). Hahn, Todd, and Van der Klaauw (2001) use the somewhat stronger assumption that treatment effect \\(\\theta\\) is constant across individuals.\nA display of the outcomes is given in Figure 21.3(b). The two dashed lines are the mean potential outcomes \\(m_{0}(x)\\) and \\(m_{1}(x)\\). The realized CEF \\(m(x)\\) is the probability weighted average of these two functions using the probability function displayed in panel (a). Since the probability function is discontinuous at \\(x=c\\) the CEF \\(m(x)\\) also is discontinuous at \\(x=c\\). The discontinuity, however, is not the full conditional ATE \\(\\bar{\\theta}\\). The important contribution of Theorem \\(21.2\\) is that the conditional ATE equals the ratio of the discontinuities in panels (b) and (a).\nTo prove the Theorem, first observe that the observed outcome is\n\\[\n\\begin{aligned}\nY &=Y_{0} \\mathbb{1}\\{D=0\\}+Y_{1} \\mathbb{1}\\{D=1\\} \\\\\n&=Y_{0}+\\theta \\mathbb{1}\\{D=1\\} .\n\\end{aligned}\n\\]\nTaking expectations conditional on \\(X=x\\) for \\(x\\) near \\(c\\) we obtain\n\\[\n\\begin{aligned}\nm(x) &=m_{0}(x)+\\mathbb{E}[\\theta \\mathbb{1}\\{D=1\\} \\mid X=x] \\\\\n&=m_{0}(x)+\\theta(x) p(x)\n\\end{aligned}\n\\]\nwhere the second equality uses the assumption that \\(\\theta\\) and \\(D\\) are independent for \\(X\\) near \\(c\\). The left and right limits at \\(c\\) are\n\\[\n\\begin{aligned}\n&m(c+)=m_{0}(c)+\\bar{\\theta} p(c+) \\\\\n&m(c-)=m_{0}(c)+\\bar{\\theta} p(c-) .\n\\end{aligned}\n\\]\nTaking the difference and re-arranging we establish the theorem."
  },
  {
    "objectID": "chpt21-rdd.html#estimation-of-frd",
    "href": "chpt21-rdd.html#estimation-of-frd",
    "title": "20  Regression Discontinuity",
    "section": "20.11 Estimation of FRD",
    "text": "20.11 Estimation of FRD\nAs displayed in (21.2) the LL estimator of the discontinuity \\(m(c+)-m(c-)\\) is obtained by local linear regression of \\(Y\\) on \\(X\\) on the two sides of the cut-off, leading to\n\\[\n\\widehat{m}(c+)-\\widehat{m}(c-)=\\left[\\widehat{\\beta}_{1}(c)\\right]_{1}-\\left[\\widehat{\\beta}_{0}(c)\\right]_{1} .\n\\]\nSimilarly, a LL estimator \\(\\hat{p}(c+)-\\widehat{p}(c-)\\) of the discontinuity \\(p(c+)-p(c-)\\) can obtained by local linear regression of \\(Y\\) on \\(D\\) on the two sides of the cut-off. Dividing we obtain the estimator of the conditional ATE\n\\[\n\\widehat{\\theta}=\\frac{\\widehat{m}(c+)-\\widehat{m}(c-)}{\\widehat{p}(c+)-\\widehat{p}(c-)} .\n\\]\nThis generalizes the sharp RDD estimator, for in that case \\(p(c+)-p(c-)=1\\).\nThis estimator bears a striking resemblance to the Wald expression (12.27) for the structural coefficient and estimator (12.28) in an IV regression with a binary instrument. In fact, \\(\\widehat{\\theta}\\) can be thought of as a locally weighted IV estimator of a regression of \\(Y\\) on \\(X\\) with instrument \\(D\\). However, the easiest way to implement estimation is using the expression for \\(\\widehat{\\theta}\\) above.\nThe estimator (21.7) requires four LL regressions. It is unclear if common bandwidths should be used for the numerator and denominator or if different bandwidths is a better choice. Bandwidth selection is critically important. In addition to assessing the fit of the regression of \\(Y\\) on \\(X\\), it is important to check the fit of the regression of \\(D\\) on \\(X\\) for the estimator \\(\\hat{p}(x)\\). The latter is the reduced form of the IV model. Identification rests on its precision.\nThe identification of the FRD conditional ATE depends on the magnitude of the discontinuity in the conditional probability \\(p(x)\\) at \\(x=c\\). A small discontinuity will lead to a weak instruments problem.\nStandard errors can be calculated similar to IV regression. Let \\(s(\\widehat{\\theta})\\) be a standard error \\(\\widehat{m}(c+)-\\) \\(\\widehat{m}(c-)\\). Then a standard error for \\(\\widehat{\\theta}\\) is \\(s(\\widehat{\\theta}) /|\\widehat{p}(c+)-\\widehat{p}(c-)|\\).\nIn FRD applications it is recommended to plot the estimated functions \\(\\widehat{m}(x)\\) and \\(\\widehat{p}(x)\\) along with confidence bands to assess precision. You are looking for evidence that the discontinuity in \\(p(x)\\) is real and meaningful so that the conditional ATE \\(\\theta\\) is identified. A discontinuity in \\(m(x)\\) is an indicator whether or not the conditional ATE is non-zero. If there is no discontinuity in \\(m(x)\\) then \\(\\theta=0\\). The estimate of the conditional ATE is the ratio of these two estimated discontinuities."
  },
  {
    "objectID": "chpt21-rdd.html#exercises",
    "href": "chpt21-rdd.html#exercises",
    "title": "20  Regression Discontinuity",
    "section": "20.12 Exercises",
    "text": "20.12 Exercises\nExercise 21.1 We have described the RDD when treatment occurs for \\(D=\\mathbb{1}\\{X \\geq c\\}\\). Suppose instead that treatment occurs for \\(D=\\mathbb{1}\\{X \\leq c\\}\\). Describe the differences (if any) involved in estimating the conditional ATE \\(\\bar{\\theta}\\).\nExercise 21.2 Suppose treatment occurs for \\(D=\\mathbb{1}\\left\\{c_{1} \\leq X \\leq c_{2}\\right\\}\\) where both \\(c_{1}\\) and \\(c_{2}\\) are in the interior of the support of \\(X\\). What treatment effects are identified?\nExercise 21.3 Show that (21.1) is obtained by taking the conditional expectation as described.\nExercise 21.4 Explain why equation (21.4) estimated on the subsample for which \\(|X-c| \\leq h\\) is identical to a local linear regression with a Rectangular bandwidth.\nExercise 21.5 Use the datafile LM2007 on the textbook webpage. Replicate the regresssion (21.5) using the subsample with poverty rates in the interval \\(59.1984 \\pm 13.8\\) (as described in the text). Repeat with intervals of \\(59.1984 \\pm 7\\) and \\(59.1984 \\pm 20\\). Report your estimates of the conditional ATE and standard error. The dependent variable is mort_age59_related_postHS. (The running variable is povrate60.)\nExercise 21.6 Use the datafile LM2007 on the textbook webpage. Replicate the baseline RDD estimate as reported in Table 21.1. This uses a normalized Triangular kernel with a bandwidth of \\(h=8\\). (If you use an unnormalized Triangular kernel (as used, for example, in Stata) this corresponds to a bandwidth of \\(h=19.6\\) ). Repeat with a bandwidth of \\(h=4\\) and \\(h=12\\) (or \\(h=9.8\\) and \\(h=29.4\\) if an unnormalized Triangular kernel is used). Report your estimates of the conditional ATE and standard error.\nExercise 21.7 Use the datafile LM2007 on the textbook webpage. Ludwig and Miller (2007) shows that similar RDD estimates for other forms of mortality do not display similar discontinuities. Perform a similar check. Estimate the conditional ATE using the dependent variable mort_age59_injury_postHS (mortality due to injuries in the 5-9 age group). Exercise 21.8 Do a similar estimation as in the previous exercise, but using the dependent variable mort_age25plus_related_postHS (mortality due to HS-related causes in the \\(25+\\) age group).\nExercise 21.9 Do a similar estimation as in the previous exercise, but using the dependent variable mort_age59_related_preHS (mortality due to HS-related causes in the 5-9 age group during 1959-1964, before the Head Start program was started)."
  },
  {
    "objectID": "chpt22-m-est.html#introduction",
    "href": "chpt22-m-est.html#introduction",
    "title": "21  M-Estimators",
    "section": "21.1 Introduction",
    "text": "21.1 Introduction\nSo far in this textbook we have primarily focused on estimators which have explicit algebraic expressions. However, many econometric estimators need to be calculated by numerical methods. These estimators are collectively described as nonlinear. Many fall in a broad class known as m-estimators. In this part of the textbook we describe a number of m-estimators in wide use in econometrics. They have a common structure which allows for a unified treatment of estimation and inference.\nAn m-estimator is defined as a minimizer of a sample average\n\\[\n\\begin{gathered}\n\\widehat{\\theta}=\\underset{\\theta \\in \\Theta}{\\operatorname{argmin}} S_{n}(\\theta) \\\\\nS_{n}(\\theta)=\\frac{1}{n} \\sum_{i=1}^{n} \\rho\\left(Y_{i}, X_{i}, \\theta\\right)\n\\end{gathered}\n\\]\nwhere \\(\\rho(Y, X, \\theta)\\) is some function of \\((Y, X)\\) and a parameter \\(\\theta \\in \\Theta\\). The function \\(S_{n}(\\theta)\\) is called the criterion function or objective function. For notational simplicity set \\(\\rho_{i}(\\theta)=\\rho\\left(Y_{i}, X_{i}, \\theta\\right)\\).\nThis includes maximum likelihood when \\(\\rho_{i}(\\theta)\\) is the negative log-density function. “m-estimators” are a broader class; the prefix “m” stands for “maximum likelihood-type”.\nThe issues we focus on in this chaper are: (1) identification; (2) estimation; (3) consistency; (4) asymptotic distribution; and (5) covariance matrix estimation."
  },
  {
    "objectID": "chpt22-m-est.html#examples",
    "href": "chpt22-m-est.html#examples",
    "title": "21  M-Estimators",
    "section": "21.2 Examples",
    "text": "21.2 Examples\nThere are many m-estimators in common econometric usage. Some examples include the following.\n\nOrdinary Least Squares: \\(\\rho_{i}(\\theta)=\\left(Y_{i}-X_{i}^{\\prime} \\theta\\right)^{2}\\).\nNonlinear Least Squares: \\(\\rho_{i}(\\theta)=\\left(Y_{i}-m\\left(X_{i}, \\theta\\right)\\right)^{2}\\) (Chapter 23).\nLeast Absolute Deviations: \\(\\rho_{i}(\\theta)=\\left|Y_{i}-X_{i}^{\\prime} \\theta\\right|\\) (Chapter 24).\nQuantile Regression: \\(\\rho_{i}(\\theta)=\\left(Y_{i}-X_{i}^{\\prime} \\theta\\right)\\left(\\tau-\\mathbb{1}\\left\\{\\left(Y_{i}-X_{i}^{\\prime} \\theta\\right)<0\\right\\}\\right)\\) (Chapter 24).\nMaximum Likelihood: \\(\\rho_{i}(\\theta)=-\\log f\\left(Y_{i} \\mid X_{i}, \\theta\\right)\\). The final category - Maximum Likelihood Estimation - includes many estimators as special cases. This includes many standard estimators of limited-dependent-variable models (Chapters 25-27). To illustrate, the probit model for a binary dependent variable is\n\n\\[\n\\mathbb{P}[Y=1 \\mid X]=\\Phi\\left(X^{\\prime} \\theta\\right)\n\\]\nwhere \\(\\Phi(u)\\) is the normal cumulative distribution function. We will study probit estimation in detail in Chapter 25. The negative log-density function is\n\\[\n\\rho_{i}(\\theta)=-Y_{i} \\log \\left(\\Phi\\left(X_{i}^{\\prime} \\theta\\right)\\right)-\\left(1-Y_{i}\\right) \\log \\left(1-\\Phi\\left(X_{i}^{\\prime} \\theta\\right)\\right) .\n\\]\nNot all nonlinear estimators are m-estimators. Examples include method of moments, GMM, and minimum distance."
  },
  {
    "objectID": "chpt22-m-est.html#identification-and-estimation",
    "href": "chpt22-m-est.html#identification-and-estimation",
    "title": "21  M-Estimators",
    "section": "21.3 Identification and Estimation",
    "text": "21.3 Identification and Estimation\nA parameter vector \\(\\theta\\) is identified if it is uniquely determined by the probability distribution of the observations. This is a property of the probability distribution, not of the estimator.\nHowever, when discussing a specific estimator it is common to describe identification in terms of the criterion function. Assume \\(\\mathbb{E}|\\rho(Y, X, \\theta)|<\\infty\\). Define\n\\[\nS(\\theta)=\\mathbb{E}\\left[S_{n}(\\theta)\\right]=\\mathbb{E}[\\rho(Y, X, \\theta)]\n\\]\nand its population minimizer\n\\[\n\\theta_{0}=\\underset{\\theta \\in \\Theta}{\\operatorname{argmin}} S(\\theta) .\n\\]\nWe say that \\(\\theta\\) is identified (or point identified) by \\(S(\\theta)\\) if the minimizer \\(\\theta_{0}\\) is unique.\nIn nonlinear models it is difficult to provide general conditions under which a parameter is identified. Identification needs to be examined on a model-by-model basis.\nAn m-estimator \\(\\widehat{\\theta}\\) by definition minimizes \\(S_{n}(\\theta)\\). When there is no explicit algebraic expression for the solution the minimization is done numerically. Such numerical methods are reviewed in Chapter 12 of Probability and Statistics for Economists.\nWe illustrate using the probit model of the previous section. We use the CPS dataset for \\(Y\\) equal to an indicator that the individual is married \\({ }^{1}\\), and set the regressors equal to years of education, age, and age squared. We obtain the following estimates\n\nStandard error calculation will be discussed in Section 22.8. In this application we see that the probability of marriage is increasing in years of education and is an increasing yet concave function of age."
  },
  {
    "objectID": "chpt22-m-est.html#consistency",
    "href": "chpt22-m-est.html#consistency",
    "title": "21  M-Estimators",
    "section": "21.4 Consistency",
    "text": "21.4 Consistency\nIt seems reasonable to expect that if a parameter is identified then we should be able to estimate the parameter consistently. For linear estimators we demonstrated consistency by applying the WLLN to the\n\\({ }^{1}\\) We define married \\(=1\\) if marital equals 1,2 , or 3. explicit algebraic expressions for the estimators. This is not possible for nonlinear estimators because they do not have explicit algebraic expressions.\nInstead, what is available to us is that an m-estimator minimizes the criterion function \\(S_{n}(\\theta)\\) which is itself a sample average. For any given \\(\\theta\\) the WLLN shows that \\(S_{n}(\\theta) \\underset{p}{\\longrightarrow} S(\\theta)\\). It is intuitive that the minimizer of \\(S_{n}(\\theta)\\) (the m-estimator \\(\\widehat{\\theta}\\) ) will converge in probability to the minimizer of \\(S(\\theta)\\) (the parameter \\(\\theta_{0}\\) ). However, the WLLN by itself is not sufficient to make this extension.\n\n\nNon-Uniform Convergence\n\n\n\nUniform Convergence\n\nFigure 22.1: Non-Uniform vs. Uniform Convergence\nTo see the problem examine Figure 22.1(a). This displays a sequence of functions \\(S_{n}(\\theta)\\) (the dashed lines) for three values of \\(n\\). What is illustrated is that for each \\(\\theta\\) the function \\(S_{n}(\\theta)\\) converges towards the limit function \\(S(\\theta)\\). However for each \\(n\\) the function \\(S_{n}(\\theta)\\) has a severe dip in the right-hand region. The result is that the sample minimizer \\(\\widehat{\\theta}_{n}\\) converges to the right-limit of the parameter space. In contrast, the minimizer \\(\\theta_{0}\\) of the limit criterion \\(S(\\theta)\\) is in the interior of the parameter space. What we observe is that \\(S_{n}(\\theta)\\) converges to \\(S(\\theta)\\) for each \\(\\theta\\) but the minimizer \\(\\widehat{\\theta}_{n}\\) does not converge to \\(\\theta_{0}\\).\nA sufficient condition to exclude this pathological behavior is uniform convergence- uniformity over the parameter space \\(\\Theta\\). As we show in Theorem 22.1, uniform convergence in probability of \\(S_{n}(\\theta)\\) to \\(S(\\theta)\\) is sufficient to establish that the m-estimator \\(\\widehat{\\theta}\\) is consistent for \\(\\theta_{0}\\).\nDefinition 22.1 \\(S_{n}(\\theta)\\) converges in probability to \\(S(\\theta)\\) uniformly over \\(\\theta \\in \\Theta\\) if\n\\[\n\\sup _{\\theta \\in \\Theta}\\left|S_{n}(\\theta)-S(\\theta)\\right| \\underset{p}{\\longrightarrow} 0\n\\]\nas \\(n \\rightarrow \\infty\\)\nUniform convergence excludes erratic wiggles in \\(S_{n}(\\theta)\\) uniformly across \\(\\theta\\) and \\(n\\) (e.g., what occurs in Figure 22.1(a)). The idea is illustrated in Figure 22.1(b). The heavy solid line is the function \\(S(\\theta)\\). The dashed lines are \\(S(\\theta)+\\varepsilon\\) and \\(S(\\theta)-\\varepsilon\\). The thin solid line is the sample criterion \\(S_{n}(\\theta)\\). The figure illustrates a situation where the sample criterion satisifes \\(\\sup _{\\theta \\in \\Theta}\\left|S_{n}(\\theta)-S(\\theta)\\right|<\\varepsilon\\). The sample criterion as displayed weaves up and down but stays within \\(\\varepsilon\\) of \\(S(\\theta)\\). Uniform convergence holds if the event shown in Figure 22.1(b) holds with high probability for \\(n\\) sufficiently large, for any arbitrarily small \\(\\varepsilon\\).\nTheorem \\(22.1 \\hat{\\theta} \\underset{p}{\\longrightarrow} \\theta_{0}\\) as \\(n \\rightarrow \\infty\\) if\n\n\\(S_{n}(\\theta)\\) converges in probability to \\(S(\\theta)\\) uniformly over \\(\\theta \\in \\Theta\\).\n\\(\\theta_{0}\\) uniquely minimizes \\(S(\\theta)\\) in the sense that for all \\(\\epsilon>0\\),\n\n\\[\n\\inf _{\\theta:\\left\\|\\theta-\\theta_{0}\\right\\| \\geq \\epsilon} S(\\theta)>S\\left(\\theta_{0}\\right) .\n\\]\nTheorem \\(22.1\\) shows that an m-estimator is consistent for its population parameter. There are only two conditions. First, the criterion function converges uniformly in probability to its expected value, and second, the minimizer \\(\\theta_{0}\\) is unique. The assumption excludes the possibility that \\(\\lim _{j} S\\left(\\theta_{j}\\right)=S\\left(\\theta_{0}\\right.\\) ) for some sequence \\(\\theta_{j} \\in \\Theta\\) not converging to \\(\\theta_{0}\\).\nThe proof of Theorem \\(22.1\\) is provided in Section 22.9."
  },
  {
    "objectID": "chpt22-m-est.html#uniform-law-of-large-numbers",
    "href": "chpt22-m-est.html#uniform-law-of-large-numbers",
    "title": "21  M-Estimators",
    "section": "21.5 Uniform Law of Large Numbers",
    "text": "21.5 Uniform Law of Large Numbers\nThe uniform convergence of Definition \\(22.1\\) is a high-level assumption. In this section we provide lower level sufficient conditions.\nTheorem 22.2 Uniform Law of Large Numbers (ULLN) Assume\n\n\\(\\left(Y_{i}, X_{i}\\right)\\) are i.i.d.\n\\(\\rho(Y, X, \\theta)\\) is continuous in \\(\\theta \\in \\Theta\\) with probability one.\n\\(|\\rho(Y, X, \\theta)| \\leq G(Y, X)\\) where \\(\\mathbb{E}[G(Y, X)]<\\infty\\).\n\\(\\Theta\\) is compact.\n\nThen \\(\\sup _{\\theta \\in \\Theta}\\left|S_{n}(\\theta)-S(\\theta)\\right| \\underset{p}{\\longrightarrow} 0\\)\nTheorem \\(22.2\\) is established in Theorem \\(18.2\\) of Probability and Statistics for Economists.\nAssumption 2 holds if \\(\\rho(y, x, \\theta)\\) is continuous in \\(\\theta\\), or if the discontinuities occur at points of zero probability. This allows for most relevant applications in econometrics. Theorem \\(18.2\\) of Probability and Statistics for Economists also provides conditions based on finite bracketing or covering numbers which allow for more generality. Assumption 3 is a slight strengthening of the finite-expectation condition \\(\\mathbb{E}[\\rho(Y, X, \\theta)]<\\infty\\). The function \\(G(Y, X)\\) is called an envelope. The ULLN extends to time series and clustered samples. See B. E. Hansen and S. Lee (2019) for clustered samples.\nCombining Theorems \\(22.1\\) and \\(22.2\\) we obtain a set of conditions for consistent estimation.\nTheorem \\(22.3 \\hat{\\theta} \\underset{p}{\\longrightarrow} \\theta_{0}\\) as \\(n \\rightarrow \\infty\\) if\n\n\\(\\left(Y_{i}, X_{i}\\right)\\) are i.i.d.\n\\(\\rho(Y, X, \\theta)\\) is continuous in \\(\\theta \\in \\Theta\\) with probability one.\n\\(|\\rho(Y, X, \\theta)| \\leq G(Y, X)\\) where \\(\\mathbb{E}[G(Y, X)]<\\infty\\).\n\\(\\Theta\\) is compact.\n\\(\\theta_{0}\\) uniquely minimizes \\(S(\\theta)\\)."
  },
  {
    "objectID": "chpt22-m-est.html#asymptotic-distribution",
    "href": "chpt22-m-est.html#asymptotic-distribution",
    "title": "21  M-Estimators",
    "section": "21.6 Asymptotic Distribution",
    "text": "21.6 Asymptotic Distribution\nWe now establish an asymptotic distribution theory. We start by an informal demonstration, present a general result under high-level conditions, and then discuss the assumptions and conditions. Define\n\\[\n\\begin{aligned}\n\\psi(Y, X, \\theta) &=\\frac{\\partial}{\\partial \\theta} \\rho(Y, X, \\theta) \\\\\n\\bar{\\psi}_{n}(\\theta) &=\\frac{\\partial}{\\partial \\theta} S_{n}(\\theta) \\\\\n\\psi(\\theta) &=\\frac{\\partial}{\\partial \\theta} S(\\theta) .\n\\end{aligned}\n\\]\nAlso define \\(\\psi_{i}(\\theta)=\\psi\\left(Y_{i}, X_{i}, \\theta\\right)\\) and \\(\\psi_{i}=\\psi_{i}\\left(\\theta_{0}\\right)\\).\nSince the m-estimator \\(\\widehat{\\theta}\\) minimizes \\(S_{n}(\\theta)\\) it satisfies \\({ }^{2}\\) the first-order condition \\(0=\\bar{\\psi}_{n}(\\widehat{\\theta})\\). Expand the right-hand side as a first order Taylor expansion about \\(\\theta_{0}\\). This is valid when \\(\\widehat{\\theta}\\) is in a neighborhood of \\(\\theta_{0}\\), which holds for \\(n\\) sufficiently large by Theorem 22.1. This yields\n\\[\n0=\\bar{\\psi}_{n}(\\widehat{\\theta}) \\simeq \\bar{\\psi}_{n}\\left(\\theta_{0}\\right)+\\frac{\\partial^{2}}{\\partial \\theta \\partial \\theta^{\\prime}} S_{n}\\left(\\theta_{0}\\right)\\left(\\widehat{\\theta}-\\theta_{0}\\right) .\n\\]\nRewriting, we obtain\n\\[\n\\sqrt{n}\\left(\\widehat{\\theta}-\\theta_{0}\\right) \\simeq-\\left(\\frac{\\partial^{2}}{\\partial \\theta \\partial \\theta^{\\prime}} S_{n}\\left(\\theta_{0}\\right)\\right)^{-1}\\left(\\sqrt{n} \\bar{\\psi}_{n}\\left(\\theta_{0}\\right)\\right) .\n\\]\nConsider the two components. First, by the WLLN\n\\[\n\\frac{\\partial^{2}}{\\partial \\theta \\partial \\theta^{\\prime}} S_{n}\\left(\\theta_{0}\\right)=\\frac{1}{n} \\sum_{i=1}^{n} \\frac{\\partial^{2}}{\\partial \\theta \\partial \\theta^{\\prime}} \\rho\\left(Y_{i}, X_{i}, \\theta_{0}\\right) \\underset{p}{\\longrightarrow} \\mathbb{E}\\left[\\frac{\\partial^{2}}{\\partial \\theta \\partial \\theta^{\\prime}} \\rho_{i}\\left(Y, X, \\theta_{0}\\right)\\right] \\stackrel{\\text { def }}{=} \\boldsymbol{Q}\n\\]\n\\({ }^{2}\\) If \\(\\widehat{\\theta}\\) is an interior solution. Since \\(\\widehat{\\theta}\\) is consistent this occurs with probability approaching one if \\(\\theta_{0}\\) is in the interior of the parameter space \\(\\Theta\\). Second,\n\\[\n\\sqrt{n} \\bar{\\psi}_{n}\\left(\\theta_{0}\\right)=\\frac{1}{\\sqrt{n}} \\sum_{i=1}^{n} \\psi_{i} .\n\\]\nSince \\(\\theta_{0}\\) minimizes \\(S(\\theta)=\\mathbb{E}\\left[\\rho_{i}(\\theta)\\right]\\) it satisfies the first-order condition\n\\[\n0=\\psi\\left(\\theta_{0}\\right)=\\mathbb{E}\\left[\\psi\\left(Y, X, \\theta_{0}\\right)\\right] .\n\\]\nThus the summands in (22.2) are mean zero. Applying a CLT this sum converges in distribution to \\(\\mathrm{N}(0, \\Omega)\\) where \\(\\Omega=\\mathbb{E}\\left[\\psi_{i} \\psi_{i}^{\\prime}\\right]\\). We deduce that\n\\[\n\\sqrt{n}\\left(\\widehat{\\theta}-\\theta_{0}\\right) \\underset{d}{\\longrightarrow} \\boldsymbol{Q}^{-1} \\mathrm{~N}(0, \\Omega)=\\mathrm{N}\\left(0, \\boldsymbol{Q}^{-1} \\Omega \\boldsymbol{Q}^{-1}\\right) .\n\\]\nThe technical hurdle to make this derivation rigorous is justifying the Taylor expansion (22.1). This can be done through smoothness of the second derivative of \\(\\rho_{i}\\left(\\theta_{0}\\right)\\). An alternative (more advanced) argument based on empirical process theory uses weaker assumptions. Set\n\\[\n\\begin{aligned}\n\\boldsymbol{Q}(\\theta) &=\\frac{\\partial^{2}}{\\partial \\theta \\partial \\theta^{\\prime}} S(\\theta) \\\\\n\\boldsymbol{Q} &=\\boldsymbol{Q}\\left(\\theta_{0}\\right)\n\\end{aligned}\n\\]\nLet \\(\\mathscr{N}\\) be some neighborhood of \\(\\theta_{0}\\).\nTheorem 22.4 Assume the conditions of Theorem \\(22.1\\) hold, plus\n\n\\(\\mathbb{E}\\left\\|\\psi\\left(Y, X, \\theta_{0}\\right)\\right\\|^{2}<\\infty\\)\n\\(Q>0\\).\n\\(\\boldsymbol{Q}(\\theta)\\) is continuous in \\(\\theta \\in \\mathscr{N}\\).\nFor all \\(\\theta_{1}, \\theta_{2} \\in \\mathcal{N},\\left\\|\\psi\\left(Y, X, \\theta_{1}\\right)-\\psi\\left(Y, X, \\theta_{2}\\right)\\right\\| \\leq B(Y, X)\\left\\|\\theta_{1}-\\theta_{2}\\right\\|\\) where \\(\\mathbb{E}\\left[B(Y, X)^{2}\\right]<\\infty\\)\n\\(\\theta_{0}\\) is in the interior of \\(\\Theta\\).\n\nThen as \\(n \\rightarrow \\infty, \\sqrt{n}\\left(\\widehat{\\theta}-\\theta_{0}\\right) \\underset{d}{\\longrightarrow} \\mathrm{N}(0, \\boldsymbol{V})\\) where \\(\\boldsymbol{V}=\\boldsymbol{Q}^{-1} \\Omega \\boldsymbol{Q}^{-1}\\).\nThe proof of Theorem \\(22.4\\) is presented in Section \\(22.9\\).\nIn some cases the asymptotic covariance matrix simplifies. The leading case is correctly specified maximum likelihood estimation, where \\(\\boldsymbol{Q}=\\Omega\\) so \\(\\boldsymbol{V}=\\boldsymbol{Q}^{-1}=\\Omega^{-1}\\).\nAssumption 1 states that the scores \\(\\psi\\left(Y, X, \\theta_{0}\\right)\\) have a finite second moment. This is necessary in order to apply the CLT. Assumption 2 is a full-rank condition and is related to identification. A sufficient condition for Assumption 3 is that the scores \\(\\psi(Y, X, \\theta)\\) are continuously differentiable but this is not necessary. Assumption 3 is broader, allowing for discontinuous \\(\\psi(Y, X, \\theta)\\), so long as its expectation is continuous and differentiable. Assumption 4 states that \\(\\psi(Y, X, \\theta)\\) is Lipschitz-continuous for \\(\\theta\\) near \\(\\theta_{0}\\). Assumption 5 is required in order to justify the application of the mean-value expansion."
  },
  {
    "objectID": "chpt22-m-est.html#asymptotic-distribution-under-broader-conditions",
    "href": "chpt22-m-est.html#asymptotic-distribution-under-broader-conditions",
    "title": "21  M-Estimators",
    "section": "21.7 Asymptotic Distribution Under Broader Conditions*",
    "text": "21.7 Asymptotic Distribution Under Broader Conditions*\nAssumption 4 in Theorem \\(22.4\\) requires that \\(\\psi(Y, X, \\theta)\\) is Lipschitz-continuous. While this holds in most applications, it is violated in some important applications including quantile regression. In such cases we can appeal to alternative regularity conditions. These are more flexible, but less intuitive.\nThe following result is a simple generalization of Lipschitz-continuity.\nTheorem 22.5 The results of Theorem \\(22.4\\) hold if Assumption 4 is replaced with the following condition: For all \\(\\delta>0\\) and all \\(\\theta_{1} \\in \\mathcal{N}\\),\n\\[\n\\left(\\mathbb{E}\\left[\\sup _{\\left\\|\\theta-\\theta_{1}\\right\\|<\\delta}\\left\\|\\psi(Y, X, \\theta)-\\psi\\left(Y, X, \\theta_{1}\\right)\\right\\|^{2}\\right]\\right)^{1 / 2} \\leq C \\delta^{\\psi}\n\\]\nfor some \\(C<\\infty\\) and \\(0<\\psi<\\infty\\).\nSee Theorem \\(18.5\\) of Probability and Statistics for Economists or Theorem 5 of Andrews (1994).\nThe bound (22.4) holds for many examples with discontinuous \\(\\psi(Y, X, \\theta)\\) when the discontinuities occur with zero probability.\nWe next present a set of flexible results.\nTheorem 22.6 The results of Theorem \\(22.4\\) hold if Assumption 4 is replaced with the following. First, for \\(\\theta \\in \\mathcal{N},\\|\\psi(Y, X, \\theta)\\| \\leq G(Y, X)\\) with \\(\\mathbb{E}\\left[G(Y, X)^{2}\\right]<\\) \\(\\infty\\). Second, one of the following holds.\n\n\\(\\psi(y, x, \\theta)\\) is Lipschitz-continuous.\n\\(\\psi(y, x, \\theta)=h\\left(\\theta^{\\prime} \\psi(x)\\right)\\) where \\(h(u)\\) has finite total variation.\n\\(\\psi(y, x, \\theta)\\) is a combination of functions of the form in parts 1 and 2 obtained by addition, multiplication, minimum, maximum, and composition.\n\\(\\psi(y, x, \\theta)\\) is a Vapnik-Červonenkis (VC) class.\n\nSee Theorem 18.6 of Probability and Statistics for Economists or Theorems 2 and 3 of Andrews (1994).\nThe function \\(h\\) in part 2 allows for discontinuous functions, including the indicator and sign functions. Part 3 shows that combinations of smooth (Lipschitz) functions and discontinuous functions satisfying the condition of part 2 are allowed. This covers many relevant applications, including quantile regression. Part 4 states a general condition, that \\(\\psi(y, x, \\theta)\\) is a VC class. As we will not be using this property in this textbook we will not discuss this further, but refer the interested reader to any textbook on empirical processes.\nTheorems \\(22.5\\) and \\(22.6\\) provide alternative conditions on \\(\\psi(y, x, \\theta)\\) (other than Lipschitz-continuity) which can be used to establish asymptotic normality of an m-estimator."
  },
  {
    "objectID": "chpt22-m-est.html#covariance-matrix-estimation",
    "href": "chpt22-m-est.html#covariance-matrix-estimation",
    "title": "21  M-Estimators",
    "section": "21.8 Covariance Matrix Estimation",
    "text": "21.8 Covariance Matrix Estimation\nThe standard estimator for \\(\\boldsymbol{V}\\) takes the sandwich form. We estimate \\(\\Omega\\) by\n\\[\n\\widehat{\\Omega}=\\frac{1}{n} \\sum_{i=1}^{n} \\widehat{\\psi}_{i} \\widehat{\\psi}_{i}^{\\prime}\n\\]\nwhere \\(\\widehat{\\psi}_{i}=\\frac{\\partial}{\\partial \\theta} \\rho_{i}(\\widehat{\\theta})\\). When \\(\\rho_{i}(\\theta)\\) is twice differentiable an estimator of \\(\\boldsymbol{Q}\\) is\n\\[\n\\widehat{\\boldsymbol{Q}}=\\frac{1}{n} \\sum_{i=1}^{n} \\frac{\\partial^{2}}{\\partial \\theta \\partial \\theta^{\\prime}} \\rho_{i}(\\widehat{\\theta}) .\n\\]\nWhen \\(\\rho_{i}(\\theta)\\) is not second differentiable then estimators of \\(\\boldsymbol{Q}\\) are constructed on a case-by-case basis.\nGiven \\(\\widehat{\\Omega}\\) and \\(\\widehat{\\boldsymbol{Q}}\\) an estimator for \\(\\boldsymbol{V}\\) is\n\\[\n\\widehat{\\boldsymbol{V}}=\\widehat{\\boldsymbol{Q}}^{-1} \\widehat{\\Omega} \\widehat{\\boldsymbol{Q}}^{-1} .\n\\]\nIt is possible to adjust \\(\\widehat{\\boldsymbol{V}}\\) by multiplying by a degree-of-freedom scaling such as \\(n /(n-k)\\) where \\(k=\\) \\(\\operatorname{dim}(\\theta)\\). There is no formal guidance.\nFor maximum likelihood estimators the standard covariance matrix estimator is \\(\\widehat{\\boldsymbol{V}}=\\widehat{\\boldsymbol{Q}}^{-1}\\). This choice is not robust to misspecification. Therefore it is recommended to use the robust version (22.5), for example by using the “, \\(r\\)” option in Stata. This is unfortunately not uniformly done in practice.\nFor clustered and time-series observations the estimator \\(\\widehat{\\boldsymbol{Q}}\\) is unaltered but the estimator \\(\\widehat{\\Omega}\\) changes. For clustered samples it is\n\\[\n\\widehat{\\Omega}=\\frac{1}{n} \\sum_{g=1}^{G}\\left(\\sum_{\\ell=1}^{n_{g}} \\widehat{\\psi}_{\\ell g}\\right)\\left(\\sum_{\\ell=1}^{n_{g}} \\widehat{\\psi} \\widehat{\\psi}_{\\ell g}\\right)^{\\prime} .\n\\]\nFor time-series data the estimator \\(\\widehat{\\Omega}\\) is unaltered if the scores \\(\\psi_{i}\\) are serially uncorrelated (which occurs when a model is dynamically correctly specified). Otherwise a Newey-West covariance matrix estimator can be used and equals\n\\[\n\\widehat{\\Omega}=\\sum_{\\ell=-M}^{M}\\left(1-\\frac{|\\ell|}{M+1}\\right) \\frac{1}{n} \\sum_{1 \\leq t-\\ell \\leq n} \\widehat{\\psi}_{t-\\ell} \\widehat{\\psi}_{t}^{\\prime} .\n\\]\nStandard errors for the parameter estimates are formed by taking the square roots of the diagonal elements of \\(n^{-1} \\widehat{\\boldsymbol{V}}\\)."
  },
  {
    "objectID": "chpt22-m-est.html#technical-proofs",
    "href": "chpt22-m-est.html#technical-proofs",
    "title": "21  M-Estimators",
    "section": "21.9 Technical Proofs*",
    "text": "21.9 Technical Proofs*\nProof of Theorem 22.1 The proof proceeds in two steps. First, we show that \\(S(\\widehat{\\theta}) \\underset{p}{\\longrightarrow} S(\\theta)\\). Second we show that this implies \\(\\widehat{\\theta} \\underset{p}{\\longrightarrow} \\theta\\).\nSince \\(\\theta_{0}\\) minimizes \\(S(\\theta), S\\left(\\theta_{0}\\right) \\leq S(\\widehat{\\theta})\\). Hence\n\\[\n\\begin{aligned}\n0 & \\leq S(\\widehat{\\theta})-S\\left(\\theta_{0}\\right) \\\\\n&=S(\\widehat{\\theta})-S_{n}(\\widehat{\\theta})+S_{n}\\left(\\theta_{0}\\right)-S\\left(\\theta_{0}\\right)+S_{n}(\\widehat{\\theta})-S_{n}\\left(\\theta_{0}\\right) \\\\\n& \\leq 2 \\sup _{\\theta \\in \\Theta}\\left\\|S_{n}(\\theta)-S(\\theta)\\right\\| \\underset{p}{\\longrightarrow} .\n\\end{aligned}\n\\]\nThe second inequality uses the fact that \\(\\hat{\\theta}\\) minimizes \\(S_{n}(\\theta)\\) so \\(S_{n}(\\widehat{\\theta}) \\leq S_{n}\\left(\\theta_{0}\\right)\\) and replaces the other two pairwise comparisons by the supremum. The final convergence is the assumed uniform convergence in probability.\n\nFigure 22.2: Consistency of M-Estimator\nThe preceeding argument is illustrated in Figure 22.2. The figure displays the expected criterion \\(S(\\theta)\\) with the solid line, and the sample criterion \\(S_{n}(\\theta)\\) is displayed with the dashed line. The distances between the two functions at the true value \\(\\theta_{0}\\) and the estimator \\(\\hat{\\theta}\\) are marked by the two dash-dotted lines. The sum of these two lengths is greater than the vertical distance between \\(S(\\widehat{\\theta})\\) and \\(S\\left(\\theta_{0}\\right)\\) because the latter distance equals the sum of the two dash-dotted lines plus the vertical height of the thick section of the dashed line (between \\(S_{n}\\left(\\theta_{0}\\right)\\) and \\(S_{n}(\\widehat{\\theta})\\) ) which is positive because \\(S_{n}(\\widehat{\\theta}) \\leq S_{n}\\left(\\theta_{0}\\right)\\). The lengths of the dotted lines converge to zero under the assumption of uniform convergence. Hence \\(S(\\widehat{\\theta})\\) converges to \\(S\\left(\\theta_{0}\\right)\\). This completes the first step.\nIn the second step of the proof we show \\(\\widehat{\\theta} \\underset{p}{\\rightarrow} \\theta\\). Fix \\(\\epsilon>0\\). The unique minimum assumption implies there is a \\(\\delta>0\\) such that \\(\\left\\|\\theta_{0}-\\theta\\right\\|>\\epsilon\\) implies \\(S(\\theta)-S\\left(\\theta_{0}\\right) \\geq \\delta\\). This means that \\(\\left\\|\\theta_{0}-\\widehat{\\theta}\\right\\|>\\epsilon\\) implies \\(S(\\widehat{\\theta})-S\\left(\\theta_{0}\\right) \\geq \\delta\\). Hence\n\\[\n\\mathbb{P}\\left[\\left\\|\\theta_{0}-\\widehat{\\theta}\\right\\|>\\epsilon\\right] \\leq \\mathbb{P}\\left[S(\\widehat{\\theta})-S\\left(\\theta_{0}\\right) \\geq \\delta\\right] .\n\\]\nThe right-hand-side converges to zero because \\(S(\\widehat{\\theta}) \\underset{p}{\\rightarrow} S(\\theta)\\). Thus the left-hand-side converges to zero as well. Since \\(\\epsilon\\) is arbitrary this implies that \\(\\hat{\\theta} \\underset{p}{\\rightarrow} \\theta\\) as stated.\nTo illustrate, again examine Figure 22.2. We see \\(S(\\widehat{\\theta})\\) marked on the graph of \\(S(\\theta)\\). Since \\(S(\\widehat{\\theta})\\) converges to \\(S\\left(\\theta_{0}\\right)\\) this means that \\(S(\\hat{\\theta})\\) slides down the graph of \\(S(\\theta)\\) towards the minimum. The only way for \\(\\widehat{\\theta}\\) to not converge to \\(\\theta_{0}\\) would be if the function \\(S(\\theta)\\) were flat at the minimum. This is excluded by the assumption of a unique minimum. Proof of Theorem 22.4 Expanding the population first-order condition \\(0=\\psi\\left(\\theta_{0}\\right)\\) around \\(\\theta=\\widehat{\\theta}\\) using the mean value theorem we find\n\\[\n0=\\psi(\\widehat{\\theta})+\\boldsymbol{Q}\\left(\\theta_{n}^{*}\\right)\\left(\\theta_{0}-\\widehat{\\theta}\\right)\n\\]\nwhere \\(\\theta_{n}^{*}\\) is intermediate \\({ }^{3}\\) between \\(\\theta_{0}\\) and \\(\\widehat{\\theta}\\). Solving, we find\n\\[\n\\sqrt{n}\\left(\\widehat{\\theta}-\\theta_{0}\\right)=\\boldsymbol{Q}\\left(\\theta_{n}^{*}\\right)^{-1} \\sqrt{n} \\psi(\\widehat{\\theta}) .\n\\]\nThe assumption that \\(\\psi(\\theta)\\) is continuously differentiable means that \\(\\boldsymbol{Q}(\\theta)\\) is continuous in \\(\\mathscr{N}\\). Since \\(\\theta_{n}^{*}\\) is intermediate between \\(\\theta_{0}\\) and \\(\\widehat{\\theta}\\) and the latter converges in probability to \\(\\theta_{0}\\), it follows that \\(\\theta_{n}^{*}\\) converges in probability to \\(\\theta_{0}\\) as well. Thus by the continuous mapping theorem \\(\\boldsymbol{Q}\\left(\\theta_{n}^{*}\\right) \\underset{p}{\\longrightarrow} \\boldsymbol{Q}\\left(\\theta_{0}\\right)=\\boldsymbol{Q}\\).\nWe next examine the asymptotic distribution of \\(\\sqrt{n} \\psi(\\widehat{\\theta})\\). Define\n\\[\nv_{n}(\\theta)=\\sqrt{n}\\left(\\bar{\\psi}_{n}(\\theta)-\\psi(\\theta)\\right) .\n\\]\nAn implication of the sample first-order condition \\(\\psi_{n}(\\widehat{\\theta})=0\\) is\n\\[\n\\sqrt{n} \\psi(\\widehat{\\theta})=\\sqrt{n}\\left(\\psi(\\widehat{\\theta})-\\psi_{n}(\\widehat{\\theta})\\right)=-v_{n}(\\widehat{\\theta})=-v_{n}\\left(\\theta_{0}\\right)+r_{n}\n\\]\nwhere \\(r_{n}=v_{n}\\left(\\theta_{0}\\right)-v_{n}(\\widehat{\\theta})\\)\nSince \\(\\psi_{i}\\) is mean zero (see (22.3)) and has a finite covariance matrix \\(\\Omega\\) by assumption it satisfies the multivariate central limit theorem. Thus\n\\[\n\\sqrt{n} \\psi_{n}(\\theta)=\\frac{1}{\\sqrt{n}} \\sum_{i=1}^{n} \\psi_{i} \\underset{d}{\\longrightarrow} \\mathrm{N}(0, \\Omega) .\n\\]\nThe final step is to show that \\(r_{n}=o_{p}\\) (1). Pick any \\(\\eta>0\\) and \\(\\epsilon>0\\). As shown by Theorem \\(18.5\\) of Probability and Statistics for Economists, Assumption 4 implies that \\(v_{n}(\\theta)\\) is asymptotically equicontinuous, which means that (see Definition \\(18.7\\) in Probability and Statistics for Economists) given \\(\\epsilon\\) and \\(\\eta\\) there is a \\(\\delta>0\\) such that\nTheorem \\(22.1\\) implies that \\(\\widehat{\\theta} \\underset{p}{\\rightarrow} \\theta_{0}\\) or\n\\[\n\\limsup _{n \\rightarrow \\infty} \\mathbb{P}\\left[\\sup _{\\left\\|\\theta-\\theta_{0}\\right\\| \\leq \\delta}\\left\\|v_{n}\\left(\\theta_{0}\\right)-v_{n}(\\theta)\\right\\|>\\eta\\right] \\leq \\epsilon .\n\\]\n\\[\n\\limsup _{n \\rightarrow \\infty} \\mathbb{P}\\left[\\left\\|\\widehat{\\theta}-\\theta_{0}\\right\\|>\\delta\\right] \\leq \\epsilon .\n\\]\nWe calculate that\n\\[\n\\begin{aligned}\n\\limsup _{n \\rightarrow \\infty} \\mathbb{P}\\left[r_{n}>\\eta\\right] & \\leq \\limsup _{n \\rightarrow \\infty} \\mathbb{P}\\left[\\left\\|v_{n}\\left(\\theta_{0}\\right)-v_{n}(\\widehat{\\theta})\\right\\|>\\eta,\\left\\|\\widehat{\\theta}-\\theta_{0}\\right\\| \\leq \\delta\\right]+\\limsup _{n \\rightarrow \\infty} \\mathbb{P}\\left[\\left\\|\\widehat{\\theta}-\\theta_{0}\\right\\|>\\delta\\right] \\\\\n& \\leq \\limsup _{n \\rightarrow \\infty} \\mathbb{P}\\left[\\sup _{\\left\\|\\theta-\\theta_{0}\\right\\| \\leq \\delta}\\left\\|v_{n}\\left(\\theta_{0}\\right)-v_{n}(\\theta)\\right\\|>\\eta\\right]+\\epsilon \\leq 2 \\epsilon .\n\\end{aligned}\n\\]\nThe second inequality is (22.7) and the final inequality is (22.6). Since \\(\\eta\\) and \\(\\epsilon\\) are arbitrary we deduce that \\(r_{n}=o_{p}(1)\\). We conclude that\n\\[\n\\sqrt{n} \\psi(\\widehat{\\theta})=-v_{n}\\left(\\theta_{0}\\right)+r_{n} \\underset{d}{\\longrightarrow} \\mathrm{N}(0, \\Omega) .\n\\]\nTogether, we have shown that\n\\[\n\\sqrt{n}\\left(\\widehat{\\theta}-\\theta_{0}\\right)=\\boldsymbol{Q}\\left(\\theta_{n}^{*}\\right)^{-1} \\sqrt{n} \\psi(\\widehat{\\theta}) \\underset{d}{\\longrightarrow} \\boldsymbol{Q}^{-1} \\mathrm{~N}(0, \\Omega) \\sim \\mathrm{N}\\left(0, \\boldsymbol{Q}^{-1} \\Omega \\boldsymbol{Q}^{-1}\\right)\n\\]\nas claimed.\n\\({ }^{3}\\) Technically, since \\(\\psi(\\widehat{\\theta})\\) is a vector, the expansion is done separately for each element of the vector so the intermediate value varies by the rows of \\(\\boldsymbol{Q}\\left(\\theta_{n}^{*}\\right)\\). This doesn’t affect the conclusion."
  },
  {
    "objectID": "chpt22-m-est.html#exercises",
    "href": "chpt22-m-est.html#exercises",
    "title": "21  M-Estimators",
    "section": "21.10 Exercises",
    "text": "21.10 Exercises\nExercise 22.1 Take the model \\(Y=X^{\\prime} \\theta+e\\) where \\(e\\) is independent of \\(X\\) and has known density function \\(f(e)\\) which is continuously differentiable.\n\nShow that the conditional density of \\(Y\\) given \\(X=x\\) is \\(f\\left(y-x^{\\prime} \\theta\\right)\\).\nFind the functions \\(\\rho(Y, X, \\theta)\\) and \\(\\psi(Y, X, \\theta)\\).\nCalculate the asymptotic covariance matrix.\n\nExercise 22.2 Take the model \\(Y=X^{\\prime} \\theta+e\\). Consider the m-estimator of \\(\\theta\\) with \\(\\rho(Y, X, \\theta)=g\\left(Y-X^{\\prime} \\theta\\right)\\) where \\(g(u)\\) is a known function.\n\nFind the functions \\(\\rho(Y, X, \\theta)\\) and \\(\\psi(Y, X, \\theta)\\).\nCalculate the asymptotic covariance matrix.\n\nExercise 22.3 For the estimator described in Exercise \\(22.2\\) set \\(g(u)=\\frac{1}{4} u^{4}\\).\n\nSketch \\(g(u)\\). Is \\(g(u)\\) continuous? Differentiable? Second differentiable?\nFind the functions \\(\\rho(Y, X, \\theta)\\) and \\(\\psi(Y, X, \\theta)\\).\nCalculate the asymptotic covariance matrix.\n\nExercise 22.4 For the estimator described in Exercise \\(22.2\\) set \\(g(u)=1-\\cos (u)\\).\n\nSketch \\(g(u)\\). Is \\(g(u)\\) continuous? Differentiable? Second differentiable?\nFind the functions \\(\\rho(Y, X, \\theta)\\) and \\(\\psi(Y, X, \\theta)\\).\nCalculate the asymptotic covariance matrix."
  },
  {
    "objectID": "chpt23-nonliear-ls.html#introduction",
    "href": "chpt23-nonliear-ls.html#introduction",
    "title": "22  Nonlinear Least Squares",
    "section": "22.1 Introduction",
    "text": "22.1 Introduction\nA nonlinear regression model is a parametric regression function \\(m(x, \\theta)=\\mathbb{E}[Y \\mid X=x]\\) which is nonlinear in the parameters \\(\\theta \\in \\Theta\\). We write the model as\n\\[\n\\begin{aligned}\nY &=m(X, \\theta)+e \\\\\n\\mathbb{E}[e \\mid X] &=0 .\n\\end{aligned}\n\\]\nIn nonlinear regression the ordinary least squares estimator does not apply. Instead the parameters are typically estimated by nonlinear least squares (NLLS). NLLS is an m-estimator which requires numerical optimization.\nWe illustrate nonlinear regression with three examples.\nOur first example is the Box-Cox regression model. The Box-Cox transformation (Box and Cox, 1964) for a strictly positive variable \\(x>0\\) is\n\\[\nx^{(\\lambda)}= \\begin{cases}\\frac{x^{\\lambda}-1}{\\lambda}, & \\text { if } \\lambda \\neq 0 \\\\ \\log (x), & \\text { if } \\lambda=0 .\\end{cases}\n\\]\nThe Box-Cox transformation continuously nests linear \\((\\lambda=1)\\) and logarithmic \\((\\lambda=0)\\) functions. Figure 23.1(a) displays the Box-Cox transformation (23.1) over \\(x \\in(0,2]\\) for \\(\\lambda=2,1,0,0.5,0\\), and \\(-1\\). The parameter \\(\\lambda\\) controls the curvature of the function.\nThe Box-Cox regression model is\n\\[\nY=\\beta_{0}+\\beta_{1} X^{(\\lambda)}+e\n\\]\nwhich has parameters \\(\\theta=\\left(\\beta_{0}, \\beta_{1}, \\lambda\\right)\\). The regression function is linear in \\(\\left(\\beta_{0}, \\beta_{1}\\right)\\) but nonlinear in \\(\\lambda\\).\nTo illustrate we revisit the reduced form regression (12.87) of risk on \\(\\log\\) (mortality) from Acemoglu, Johnson, and Robinson (2001). A reasonable question is why the authors specified the equation as a regression on \\(\\log\\) (mortality) rather than on mortality. The Box-Cox regression model allows both as special cases, and equals\n\\[\n\\text { risk }=\\beta_{0}+\\beta_{1} \\text { mortality }{ }^{(\\lambda)}+e .\n\\]\nOur second example is a Constant Elasticity of Substitution (CES) production function, which was introduced by Arrow, Chenery, Minhas, and Solow (1961) as a generalization of the popular Cobb-Douglass production function. The CES function for two inputs is\n\\[\nY=\\left\\{\\begin{array}{cc}\nA\\left(\\alpha X_{1}^{\\rho}+(1-\\alpha) X_{2}^{\\rho}\\right)^{v / \\rho}, & \\text { if } \\rho \\neq 0 \\\\\nA\\left(X_{1}^{\\alpha} X_{2}^{(1-\\alpha)}\\right)^{v}, & \\text { if } \\rho=0 .\n\\end{array}\\right.\n\\]\nwhere \\(A\\) is heterogeneous (random) productivity, \\(v>0, \\alpha \\in(0,1)\\), and \\(\\rho \\in(-\\infty, 1]\\). The coefficient \\(v\\) is the elasticity of scale. The coefficient \\(\\alpha\\) is the share parameter. The coefficient \\(\\rho\\) is a re-writing \\({ }^{1}\\) of the elasticity of substitution \\(\\sigma\\) between the inputs and satisfies \\(\\sigma=1 /(1-\\rho)\\). The elasticity satisfies \\(\\sigma>1\\) if \\(\\rho>0\\), and \\(\\sigma<1\\) if \\(\\rho<0\\). At \\(\\rho=0\\) we obtain the unit elastic Cobb-Douglas function. Setting \\(\\rho=1\\) and \\(v=1\\) we obtain a linear production function. Taking the limit \\(\\rho \\rightarrow-\\infty\\) we obtain the Leontief production function.\nSet \\(\\log A=\\beta+e\\). The framework implies the regression model\n\\[\n\\log Y=\\beta+\\frac{v}{\\rho} \\log \\left(\\alpha X_{1}^{\\rho}+(1-\\alpha) X_{2}^{\\rho}\\right)+e\n\\]\nwith parameters \\(\\theta=(\\rho, v, \\alpha, \\beta)\\).\nWe illustrate CES production function estimation with a modification of Papageorgiou, Saam, and Schulte (2017). These authors estimate a CES production function for electricity production where \\(X_{1}\\) is generation capacity using “clean” technology and \\(X_{2}\\) is generation capacity using “dirty” technology. They estimate the model using a panel of 26 countries for the years 1995 to 2009 . Their goal was to measure the elasticity of substitution between clean and dirty electrical generation. The data file PPS2017 is an extract of the authors’ dataset.\nOur third example is the regression kink model. This is essentially a piecewise continuous linear spline where the knot is treated as a free parameter. The model used in our application is the nonlinear AR(1) model\n\\[\nY_{t}=\\beta_{1}\\left(X_{t-1}-c\\right)_{-}+\\beta_{2}\\left(X_{t-1}-c\\right)_{+}+\\beta_{3} Y_{t-1}+\\beta_{4}+e_{t}\n\\]\nwhere \\((a)_{-}\\)and \\((a)_{+}\\)are the negative-part and positive-part functions, \\(c\\) is the kink point, and the slopes are \\(\\beta_{1}\\) and \\(\\beta_{2}\\) on the two sides of the kink. The parameters are \\(\\theta=\\left(\\beta_{1}, \\beta_{2}, \\beta_{3}, \\beta_{4}, c\\right)\\). The regression function is linear in \\(\\left(\\beta_{1}, \\beta_{2}, \\beta_{3}, \\beta_{4}\\right)\\) and nonlinear in \\(c\\).\nWe illustrate the regression kink model with an application from B. E. Hansen (2017) which is a formalization of Reinhart and Rogoff (2010). The data are a time-series of annual observations on U.S. real GDP growth \\(Y_{t}\\) and the ratio of federal debt to GDP \\(X_{t}\\) for the years 1791-2009. Reinhart-Rogoff were interested in the hypothesis that the growth rate of GDP slows when the level of debt exceeds a threshold. To illustrate, Figure 23.1 (b) displays the regression kink function. The kink \\(c=44\\) is marked by the square. You can see that the function is upward sloped for \\(X<c\\) and downward sloped for \\(X>c\\)."
  },
  {
    "objectID": "chpt23-nonliear-ls.html#identification",
    "href": "chpt23-nonliear-ls.html#identification",
    "title": "22  Nonlinear Least Squares",
    "section": "22.2 Identification",
    "text": "22.2 Identification\nThe regression model \\(m(x, \\theta)\\) is correctly specified if there exists a parameter value \\(\\theta_{0}\\) such that \\(m\\left(x, \\theta_{0}\\right)=\\mathbb{E}[Y \\mid X=x]\\). The parameter is point identified if \\(\\theta_{0}\\) is unique. In correctly-specified nonlinear regression models the parameter is point identified if there is a unique true parameter.\nAssume \\(\\mathbb{E}\\left[Y^{2}\\right]<\\infty\\). Since the conditional expectation is the best mean-squared predictor it follows that the true parameter \\(\\theta_{0}\\) satisfies the optimization expression\n\\[\n\\theta_{0}=\\underset{\\theta \\in \\Theta}{\\operatorname{argmin}} S(\\theta)\n\\]\n\\({ }^{1}\\) It is tempting to write the model as a function of the elasticity of substitution \\(\\sigma\\) rather than its transformation \\(\\rho\\). However this is unadvised as it renders the regression function more nonlinear and difficult to optimize.\n\n\nBox-Cox Transformation\n\n\n\nRegression Kink Model\n\nFigure 23.1: Nonlinear Regression Models\nwhere\n\\[\nS(\\theta)=\\mathbb{E}\\left[(Y-m(X, \\theta))^{2}\\right]\n\\]\nis the expected squared error. This expresses the parameter as a function of the distribution of \\((Y, X)\\).\nThe regression model is mis-specified if there is no \\(\\theta\\) such that \\(m(x, \\theta)=\\mathbb{E}[Y \\mid X=x]\\). In this case we define the pseudo-true value \\(\\theta_{0}\\) as the best-fitting parameter (23.5). It is difficult to give general conditions under which the solution is unique. Hence identification of the pseudo-true value under mis-specification is typically assumed rather than deduced."
  },
  {
    "objectID": "chpt23-nonliear-ls.html#estimation",
    "href": "chpt23-nonliear-ls.html#estimation",
    "title": "22  Nonlinear Least Squares",
    "section": "22.3 Estimation",
    "text": "22.3 Estimation\nThe analog estimator of the expected squared error \\(S(\\theta)\\) is the sample average of squared errors\n\\[\nS_{n}(\\theta)=\\frac{1}{n} \\sum_{i=1}^{n}\\left(Y_{i}-m\\left(X_{i}, \\theta\\right)\\right)^{2} .\n\\]\nSince \\(\\theta_{0}\\) minimizes \\(S(\\theta)\\) its analog estimator minimizes \\(S_{n}(\\theta)\\)\n\\[\n\\widehat{\\theta}_{\\mathrm{nlls}}=\\underset{\\theta \\in \\Theta}{\\operatorname{argmin}} S_{n}(\\theta) .\n\\]\nThis is called the Nonlinear Least Squares (NLLS) estimator. It includes OLS as the special case when \\(m\\left(X_{i}, \\theta\\right)\\) is linear in \\(\\theta\\). It is an m-estimator with \\(\\rho_{i}(\\theta)=\\left(Y_{i}-m\\left(X_{i}, \\theta\\right)\\right)^{2}\\).\nAs \\(S_{n}(\\theta)\\) is a nonlinear function of \\(\\theta\\) in general there is no explicit algebraic expression for the solution \\(\\widehat{\\theta}_{\\text {nlls. }}\\) Instead it is found by numerical minimization. Chapter 12 of Probability and Statistics for Economists provides an overview. The NLLS residuals are \\(\\widehat{e}_{i}=Y_{i}-m\\left(X_{i}, \\widehat{\\theta}_{\\text {nlls }}\\right)\\).\nIn some cases, including our first and third examples in Section 23.1, the model \\(m(x, \\theta)\\) is linear in most of the parameters. In these cases a computational shortcut is to use nested minimization (also known as concentration or profiling). Take Example 1 (Box-Cox Regression). Given the Box-Cox parameter \\(\\lambda\\) the regression is linear. The coefficients \\(\\left(\\beta_{0}, \\beta_{1}\\right)\\) can be estimated by least squares, obtaining the residuals and sample concentrated average of squared errors \\(S_{n}^{*}(\\lambda)\\). The latter can be minimized using one-dimensional methods. The minimizer \\(\\hat{\\lambda}\\) is the NLLS estimator of \\(\\lambda\\). Given \\(\\hat{\\lambda}_{\\text {nlls }}\\), the NLLS coefficient estimators \\(\\left(\\widehat{\\beta}_{0}, \\widehat{\\beta}_{1}\\right)\\) are found by OLS regression of \\(Y_{i}\\) on a constant and \\(X_{i}^{(\\widehat{\\lambda})}\\).\n\n\nBox-Cox Regression\n\n\n\nCES Production Function\n\nFigure 23.2: Average of Squared Errors Functions\nWe illustrate with two of our examples.\nFigure 23.2(a) displays the concentrated average of squared errors \\(S_{n}^{*}(\\lambda)\\) for the Box-Cox regression model applied to (23.2), displayed as a function of the Box-Cox parameter \\(\\lambda\\). You can see that \\(S_{n}^{*}(\\lambda)\\) is neither quadratic nor globally convex, but has a well-defined minimum at \\(\\widehat{\\lambda}=-0.77\\). This is a parameter value which produces a regression model considerably more curved than the logarithm specification used by Acemoglou et. al.\nFigure 23.2(b) displays the average of squared errors for the CES production function application, displayed as a function of \\((\\rho, \\alpha)\\) with the other parameters set at the minimizer. You can see that the minimum is obtained at \\((\\widehat{\\rho}, \\widehat{\\alpha})=(.36, .39)\\). We have displayed the function \\(S_{n}(\\rho, \\alpha)\\) by its contour surfaces. A quadratic function has elliptical contour surfaces. You can see that the function appears to be close to quadratic near the minimum but becomes increasingly non-quadratic away from the minimum.\nThe parameter estimates and standard errors for the three models are presented in Table 23.1. Standard error calculation will be discussed in Section 23.5. The standard errors for the Box-Cox and Regression Kink models were calculated using the heteroskedasticity-robust formula, and those for the CES production function were calculated by the cluster-robust formula, clustering by country.\nTake the Box-Cox regression. The estimate \\(\\hat{\\lambda}=-0.77\\) shows that the estimated relationship between risk and mortality has stronger curvature than the logarithm function, and the estimate \\(\\widehat{\\beta}_{1}=-17\\) is negative as predicted. The large standard error for \\(\\widehat{\\beta}_{1}\\), however, indicates that the slope coefficient is not precisely estimated.\nTake the CES production function. The estimate \\(\\widehat{\\rho}=0.36\\) is positive, indicating that clean and dirty technologies are substitutes. The implied elasticity of substitution \\(\\sigma=1 /(1-\\rho)\\) is \\(\\widehat{\\sigma}=1.57\\). The estimated Table 23.1: NLLS Estimates of Example Models\n\n\n\n\n\\(\\lambda\\)\n\\(-0.77\\)\n\\(0.28\\)\n\n\n\n\nCES Production Function\n\\(\\rho\\)\n\\(0.36\\)\n\\(0.29\\)\n\n\n\n\\(v\\)\n\\(1.05\\)\n\\(0.03\\)\n\n\n\n\\(\\alpha\\)\n\\(0.39\\)\n\\(0.06\\)\n\n\n\n\\(\\beta\\)\n\\(1.66\\)\n\\(0.31\\)\n\n\n\n\\(\\sigma\\)\n\\(1.57\\)\n\\(0.46\\)\n\n\nRegression Kink Regression\n\\(\\beta_{1}\\)\n\\(0.033\\)\n\\(0.026\\)\n\n\n\n\\(\\beta_{2}\\)\n\\(-0.067\\)\n\\(0.046\\)\n\n\n\n\\(\\beta_{3}\\)\n\\(0.28\\)\n\\(0.09\\)\n\n\n\n\\(\\beta_{4}\\)\n\\(3.78\\)\n\\(0.68\\)\n\n\n\n\\(c\\)\n\\(43.9\\)\n\\(11.8\\)\n\n\n\nelasticity of scale \\(\\widehat{v}=1.05\\) is slightly above one, consistent with increasing returns to scale. The share parameter for clean technology \\(\\widehat{\\alpha}=0.39\\) is somewhat less than one-half, indicating that dirty technology is the dominating input.\nTake the regression kink function. The estimated slope of GDP growth for low debt levels \\(\\widehat{\\beta}_{1}=0.03\\) is positive, and the estimated slope for high debt levels \\(\\widehat{\\beta}_{2}=-0.07\\) is negative. This is consistent with the Reinhart-Rogoff hypothesis that high debt levels lead to a slowdown in economic growth. The estimated kink point is \\(\\widehat{c}=44 %\\) which is considerably lower than the postulated \\(90 %\\) kink point suggested by Reinhart-Rogoff based on their informal analysis.\nInterpreting conventional t-ratios and p-values in nonlinear models should be done thoughtfully. This is a context where the annoying empirical custom of appending asterisks to all “significant” coefficient estimates is particularly inappropriate. Take, for example, the CES estimates in Table 23.1. The “t-ratio” for \\(v\\) is for the test of the hypothesis that \\(v=0\\), which is a meaningless hypothesis. Similarly the t-ratio for \\(\\alpha\\) is for an uninteresting hypothesis. It does not make sense to append asterisks to these estimates and describe them as “significant” as there is no reason to take 0 as an interesting value for the parameter. Similarly in the Box-Cox regression there is no reason to take \\(\\lambda=0\\) as an important hypothesis. In the Regression Kink model the hypothesis \\(c=0\\) is generally meaningless and could easily lie outside the parameter space."
  },
  {
    "objectID": "chpt23-nonliear-ls.html#asymptotic-distribution",
    "href": "chpt23-nonliear-ls.html#asymptotic-distribution",
    "title": "22  Nonlinear Least Squares",
    "section": "22.4 Asymptotic Distribution",
    "text": "22.4 Asymptotic Distribution\nWe first consider the consistency of the NLLS estimator. We appeal to Theorems \\(22.3\\) and \\(22.4\\) for m-estimators.\nAssumption $23.1\n\n\\(\\left(Y_{i}, X_{i}\\right)\\) are i.i.d.\n\\(m(X, \\theta)\\) is continuous in \\(\\theta \\in \\Theta\\) with probability one.\n\\(\\mathbb{E}\\left[Y^{2}\\right]<\\infty\\).\n\\(|m(X, \\theta)| \\leq m(X)\\) with \\(\\mathbb{E}\\left[m(X)^{2}\\right]<\\infty\\).\n\\(\\Theta\\) is compact.\nFor all \\(\\theta \\neq \\theta_{0}, S(\\theta)>S\\left(\\theta_{0}\\right)\\).\n\nAssumptions 1-4 are fairly standard. Assumption 5 is not essential but simplifies the proof. Assumption 6 is critical. It states that the minimizer \\(\\theta_{0}\\) is unique.\nTheorem 23.1 Consistency of NLLS Estimator If Assumption \\(23.1\\) holds then \\(\\widehat{\\theta} \\underset{p}{\\longrightarrow} \\theta_{0}\\) as \\(n \\rightarrow \\infty\\).\nWe next discuss the asymptotic distribution for differentiable models. We first present the main result, then discuss the assumptions. Set \\(m_{\\theta}(x, \\theta)=\\frac{\\partial}{\\partial \\theta} m(x, \\theta), m_{\\theta \\theta}(x, \\theta)=\\frac{\\partial^{2}}{\\partial \\theta \\partial \\theta^{\\prime}} m(x, \\theta)\\), and \\(m_{\\theta i}=\\) \\(m_{\\theta}\\left(X_{i}, \\theta_{0}\\right)\\). Define \\(\\boldsymbol{Q}=\\mathbb{E}\\left[m_{\\theta i} m_{\\theta i}^{\\prime}\\right]\\) and \\(\\Omega=\\mathbb{E}\\left[m_{\\theta i} m_{\\theta i}^{\\prime} e_{i}^{2}\\right]\\)\nAssumption \\(23.2\\) For some neighborhood \\(\\mathscr{N}\\) of \\(\\theta_{0}\\),\n\n\\(\\mathbb{E}[e \\mid X]=0\\).\n\\(\\mathbb{E}\\left[Y^{4}\\right]<\\infty\\).\n\\(m(x, \\theta)\\) and \\(m_{\\theta}(X, \\theta)\\) are differentiable in \\(\\theta \\in \\mathcal{N}\\).\n\\(\\mathbb{E}|m(X, \\theta)|^{4}<\\infty, \\mathbb{E}\\left\\|m_{\\theta}(X, \\theta)\\right\\|^{4}<\\infty\\), and \\(\\mathbb{E}\\left\\|m_{\\theta \\theta}(X, \\theta)\\right\\|^{4}<\\infty\\) for \\(\\theta \\in \\mathcal{N}\\).\n\\(\\boldsymbol{Q}=\\mathbb{E}\\left[m_{\\theta i} m_{\\theta i}^{\\prime}\\right]>0\\).\n\\(\\theta_{0}\\) is in the interior of \\(\\Theta\\).\n\nAssumption 1 imposes that the model is correctly specified. If we relax this assumption the asymptotic distribution is still normal but the covariance matrix changes. Assumption 2 is a moment bound needed for asymptotic normality. Assumption 3 states that the regression function is second-order differentiable. This can be relaxed but with a complication of the conditions and derivation. Assumption 4 states moment bounds on the regression function and its derivatives. Assumption 5 states that the “linearized regressor” \\(m_{\\theta i}\\) has a full rank population design matrix. If this assumption fails then \\(m_{\\theta i}\\) will be multicollinear. Assumption 6 requires that the parameters are not on the boundary of the parameter space. This is important as otherwise the sampling distribution will be asymmetric.\nTheorem $23.2 Asymptotic Normality of NLLS Estimator If Assumptions \\(23.1\\) and \\(23.2\\) hold then \\(\\sqrt{n}\\left(\\widehat{\\theta}-\\theta_{0}\\right) \\underset{d}{\\longrightarrow} \\mathrm{N}(0, \\boldsymbol{V})\\) as \\(n \\rightarrow \\infty\\), where \\(V=Q^{-1} \\Omega Q^{-1}\\)\nTheorem \\(23.2\\) shows that under general conditions the NLLS estimator has an asymptotic distribution with similar structure to that of the OLS estimator. The estimator converges at a conventional rate to a normal distribution with a sandwich-form covariance matrix. Furthermore, the asymptotic variance is identical to that in a hypothetical OLS regression with the linearized regressor \\(m_{\\theta i}\\). Thus, asymptotically, the distribution of NLLS is identical to a linear regression.\nThe asymptotic distribution simplifies under conditional homoskedasticity. If \\(\\mathbb{E}\\left[e^{2} \\mid X\\right]=\\sigma^{2}\\) then the asymptotic variance is \\(\\boldsymbol{V}=\\sigma^{2} \\boldsymbol{Q}^{-1}\\)."
  },
  {
    "objectID": "chpt23-nonliear-ls.html#covariance-matrix-estimation",
    "href": "chpt23-nonliear-ls.html#covariance-matrix-estimation",
    "title": "22  Nonlinear Least Squares",
    "section": "22.5 Covariance Matrix Estimation",
    "text": "22.5 Covariance Matrix Estimation\nThe asymptotic covariance matrix \\(\\boldsymbol{V}\\) is estimated similarly to linear regression with the adjustment that we use an estimate of the linearized regressor \\(m_{\\theta i}\\). This estimate is\n\\[\n\\widehat{m}_{\\theta i}=m_{\\theta}\\left(X_{i}, \\widehat{\\theta}\\right)=\\frac{\\partial}{\\partial \\theta} m\\left(X_{i}, \\widehat{\\theta}\\right) .\n\\]\nIt is best if the derivative is calculated algebraically but a numerical derivative (a discrete derivative) can substitute.\nTake, for example, the Box-Cox regression model for which \\(m\\left(x, \\beta_{0}, \\beta_{1}, \\lambda\\right)=\\beta_{0}+\\beta_{1} x^{(\\lambda)}\\). We calculate that for \\(\\lambda \\neq 0\\)\n\\[\nm_{\\theta}\\left(x, \\beta_{0}, \\beta_{1}, \\lambda\\right)=\\left(\\begin{array}{c}\n\\frac{\\partial}{\\partial \\beta_{0}}\\left(\\beta_{0}+\\beta_{1} x^{(\\lambda)}\\right) \\\\\n\\frac{\\partial}{\\partial \\beta_{1}}\\left(\\beta_{0}+\\beta_{1} x^{(\\lambda)}\\right) \\\\\n\\frac{\\partial}{\\partial \\beta_{\\lambda}}\\left(\\beta_{0}+\\beta_{1} x_{i}^{(\\lambda)}\\right)\n\\end{array}\\right)=\\left(\\begin{array}{c}\n1 \\\\\nx^{(\\lambda)} \\\\\n\\frac{x^{\\lambda} \\log (x)-x^{(\\lambda)}}{\\lambda}\n\\end{array}\\right) .\n\\]\nFor \\(\\lambda=0\\) the third entry is \\(\\log ^{2}(x) / 2\\). The estimate is obtained by replacing \\(\\lambda\\) with the estimator \\(\\hat{\\lambda}\\). Hence for \\(\\widehat{\\lambda} \\neq 0\\)\n\\[\n\\widehat{m}_{\\theta i}=\\left(\\begin{array}{c}\n1 \\\\\nx^{(\\widehat{\\lambda})} \\\\\n\\frac{1-x^{\\hat{\\lambda}}+\\lambda x^{\\hat{\\lambda}} \\log (x)}{\\hat{\\lambda}^{2}}\n\\end{array}\\right) .\n\\]\nThe covariance matrix components are estimated as\n\\[\n\\begin{aligned}\n\\widehat{\\boldsymbol{Q}} &=\\frac{1}{n} \\sum_{i=1}^{n} \\widehat{m}_{\\theta i} \\widehat{m}_{\\theta i}^{\\prime} \\\\\n\\widehat{\\Omega}=\\frac{1}{n} \\sum_{i=1}^{n} \\widehat{m}_{\\theta i} \\widehat{m}_{\\theta i}^{\\prime} \\widehat{e}_{i}^{2} \\\\\n\\widehat{\\boldsymbol{V}} &=\\widehat{\\boldsymbol{Q}}^{-1} \\widehat{\\Omega} \\widehat{\\boldsymbol{Q}}^{-1}\n\\end{aligned}\n\\]\nwhere \\(\\widehat{e}_{i}=Y_{i}-m\\left(X_{i}, \\widehat{\\theta}\\right)\\) are the NLLS residuals. Standard errors are calculated conventionally as the square roots of the diagonal elements of \\(n^{-1} \\widehat{\\boldsymbol{V}}\\).\nIf the error is homoskedastic the covariance matrix can be estimated using the formula\n\\[\n\\begin{aligned}\n\\widehat{\\boldsymbol{V}}^{0} &=\\widehat{\\boldsymbol{Q}}^{-1} \\widehat{\\sigma}^{2} \\\\\n\\widehat{\\sigma}^{2} &=\\frac{1}{n} \\sum_{i=1}^{n} \\widehat{e}_{i}^{2} .\n\\end{aligned}\n\\]\nIf the observations satisfy cluster dependence then a standard cluster variance estimator can be used, again treating the linearized regressor estimate \\(\\widehat{m}_{\\theta i}\\) as the effective regressor.\nTo illustrate, standard errors for our three estimated models are displayed in Table 23.1. The standard errors for the first and third models were calculated using the formula (23.6). The standard errors for the CES model were clustered by country.\nIn small samples the standard errors for NLLS may not be reliable. An alternative is to use bootstrap methods for inference. The nonparametric bootstrap draws with replacement from the observation pairs \\(\\left(Y_{i}, X_{i}\\right)\\) to create bootstrap samples, to which NLLS is applied to obtain bootstrap parameter estimates \\(\\widehat{\\theta}^{*}\\). From \\(\\widehat{\\theta}^{*}\\) we can calculate bootstrap standard errors and/or bootstrap confidence intervals, for example by the bias-corrected percentile method."
  },
  {
    "objectID": "chpt23-nonliear-ls.html#panel-data",
    "href": "chpt23-nonliear-ls.html#panel-data",
    "title": "22  Nonlinear Least Squares",
    "section": "22.6 Panel Data",
    "text": "22.6 Panel Data\nConsider the nonlinear regression model with an additive individual effect\n\\[\n\\begin{aligned}\nY_{i t} &=m\\left(X_{i t}, \\theta\\right)+u_{i}+\\varepsilon_{i t} \\\\\n\\mathbb{E}\\left[\\varepsilon_{i t} \\mid X_{i t}\\right] &=0 .\n\\end{aligned}\n\\]\nTo eliminate the individual effect we can apply the within or first-differencing transformations. Applying the within transformation we obtain\n\\[\n\\dot{Y}_{i t}=\\dot{m}\\left(X_{i t}, \\theta\\right)+\\dot{\\varepsilon}_{i t}\n\\]\nwhere\n\\[\n\\dot{m}\\left(X_{i t}, \\theta\\right)=m\\left(X_{i t}, \\theta\\right)-\\frac{1}{T_{i}} \\sum_{t \\in S_{i}} m\\left(X_{i t}, \\theta\\right)\n\\]\nusing the panel data notation. Thus \\(\\dot{m}\\left(X_{i t}, \\theta\\right)\\) is the within transformation applied to \\(m\\left(X_{i t}, \\theta\\right)\\). It is not \\(m\\left(\\dot{X}_{i t}, \\theta\\right)\\). Equation (23.7) is a nonlinear panel model. The coefficient can be estimated by NLLS. The estimator is appropriate when \\(X_{i t}\\) is strictly exogenous, as \\(\\dot{m}\\left(X_{i t}, \\theta\\right)\\) is a function of \\(X_{i s}\\) for all time periods.\nAn alternative is to apply the first-difference transformation. Thus yields\n\\[\n\\Delta Y_{i t}=\\Delta m\\left(X_{i t}, \\theta\\right)+\\Delta \\varepsilon_{i t}\n\\]\nwhere \\(\\Delta m\\left(X_{i t}, \\theta\\right)=m\\left(X_{i t}, \\theta\\right)-m\\left(X_{i, t-1}, \\theta\\right)\\). Equation (23.8) can be estimated by NLLS. Again this requires that \\(X_{i t}\\) is strictly exogenous for consistent estimation.\nIf the regressors \\(X_{i t}\\) contains a lagged dependent variable \\(Y_{i, t-1}\\) then NLLS is not an appropriate estimator. GMM can be applied to (23.8) similar to linear dynamic panel regression models."
  },
  {
    "objectID": "chpt23-nonliear-ls.html#threshold-models",
    "href": "chpt23-nonliear-ls.html#threshold-models",
    "title": "22  Nonlinear Least Squares",
    "section": "22.7 Threshold Models",
    "text": "22.7 Threshold Models\nAn extreme example of nonlinear regression is the class of threshold regression models. These are discontinuous regression models where the kink points are treated as free parameters. They have been used succesfully in economics to model threshold effects and tipping points. They are also the core tool for the modern machine learning methods of regression trees and random forests. In this section we provide a review.\nA threshold regression model takes the form\n\\[\n\\begin{aligned}\nY &=\\beta_{1}^{\\prime} X_{1}+\\beta_{2}^{\\prime} X_{2} \\mathbb{1}\\{Q \\geq \\gamma\\}+e \\\\\n\\mathbb{E}[e \\mid X] &=0\n\\end{aligned}\n\\]\nwhere \\(X_{1}\\) and \\(X_{2}\\) are \\(k_{1} \\times 1\\) and \\(k_{2} \\times 1\\), respectively, and \\(Q\\) is scalar. The variable \\(Q\\) is called the threshold variable and \\(\\gamma\\) is called the threshold.\nTypically, both \\(X_{1}\\) and \\(X_{2}\\) contain an intercept, and \\(X_{2}\\) and \\(Q\\) are subsets of \\(X_{1}\\). In the latter case \\(\\beta_{2}\\) is the change in the slope at the threshold. The threshold variable \\(Q\\) should be either continuously distributed or ordinal.\nIn a full threshold specification \\(X_{1}=X_{2}=X\\). In this case all coefficients switch at the threshold. This regression can alternatively be written as\n\\[\nY=\\left\\{\\begin{array}{cc}\n\\theta_{1}^{\\prime} X+e, & Q<\\gamma \\\\\n\\theta_{2}^{\\prime} X+e, & Q \\geq \\gamma\n\\end{array}\\right.\n\\]\nwhere \\(\\theta_{1}=\\beta_{1}\\) and \\(\\theta_{2}=\\beta_{1}+\\beta_{2}\\).\nA simple yet full threshold model arises when there is only a single regressor \\(X\\). The regression can be written as\n\\[\nY=\\alpha_{1}+\\beta_{1} X+\\alpha_{2} \\mathbb{1}\\{X \\geq \\gamma\\}+\\beta_{2} X \\mathbb{1}\\{X \\geq \\gamma\\}+e .\n\\]\nThis resembles a Regression Kink model, but is more general as it allows for a discontinuity at \\(X=\\gamma\\). The Regression Kink model imposes the restriction \\(\\alpha+\\beta \\gamma=0\\).\nA threshold model is most suitable for a context where an economic model predicts a discontinuity in the CEF. It can also be used as a flexible approximation for a context where it is believed the CEF has a sharp nonlinearity with respect to one variable, or has sharp interaction effects. The Regression Kink model, for example, does not allow for kink interaction effects.\nThe threshold model is critically dependent on the choice of threshold variable \\(Q\\). This variable controls the ability of the regression model to display nonlinearity. In principle this can be generalized by incorporating multiple thresholds in potentially different variables but this generalization is limited by sample size and information.\nThe threshold model is linear in the coefficients \\(\\beta=\\left(\\beta_{1}, \\beta_{2}\\right)\\) and nonlinear in \\(\\gamma\\). The parameter \\(\\gamma\\) is of critical importance as it determines the model’s nonlinearity - the sample split.\nMany empirical applications estimate threshold models using informal ad hoc methods. What you may see is a splitting of the sample into “subgroups” based on regressor characteristics. When the latter split is based on a continuous regressor the split point is exactly a threshold parameter. When you see such tables it is prudent to be skeptical. How was this threshold parameter selected? Based on intuition? Or based on data exploration? If the former do you expect the results to be informative? If the latter should you trust the reported tests?\nTo illustrate threshold regression we review an influential paper by Card, Mas and Rothstein (2008). They were interested in the process of racial segregation in U.S. cities. A common hypothesis concerning the behavior of white Americans is that they are only comfortable living in a neighborhood if it has a small percentage of minority residents. A simple model of this behavior (explored in their paper) predicts that this preference leads to an unstable mixed-race equilibrium in the fraction of minorities. They call this equilibrium the tipping point. If the minority fraction exceeds this tipping point the outcome will change discontinuously. The economic mechanism is that if minorities move into a neighborhood at a roughly continuous rate, when the tipping point is reached there will be a surge in exits by white residents who elect to move due to their discomfort. This predicts a threshold regression with a discontinuity at the tipping point. The data file CMR2008 is an abridged version of the authors’ dataset.\n\n\nEstimation Criterion\n\n\n\nThreshold Regression Estimates\n\nFigure 23.3: Threshold Regression - Card-Mas-Rothstein (2008) Model\nThe authors use a specification similar to the following\n\\[\n\\begin{aligned}\n\\Delta W_{c i t} &=\\delta_{0} \\mathbb{1}\\left\\{M_{c i t-1} \\geq \\gamma\\right\\}+\\delta_{1}\\left(M_{c i t-1}-\\gamma\\right) \\mathbb{1}\\left\\{M_{c i t-1} \\geq \\gamma\\right\\} \\\\\n&+\\beta_{1} M_{c i t-1}+\\beta_{2} M_{c i t-1}^{2}+\\theta^{\\prime} X_{c i t-1}+\\alpha+u_{c}+e_{c i t}\n\\end{aligned}\n\\]\nwhere \\(c\\) is the city (MSA),\\(i\\) is a census tract within the city, \\(t\\) is the time period (decade), \\(\\Delta W_{c i t}\\) is the white population percentage change in the tract over the decade, \\(M_{c i t}\\) is the fraction of minorties in the tract, \\(u_{c}\\) is a fixed effect for the city, and \\(X_{c i t}\\) are tract-level regression controls. The sample is based on Census data which is collected at ten-year intervals. They estimate models for three decades; we focus on 1970-1980. Thus \\(\\Delta W_{c i t}\\) is the change in white population over the period 1970-1980 and the remaining variables are for 1970 . The controls used in the regression are the unemployment rate, the log mean family income, housing vacancy rate, renter share, fraction of homes in single-unit buildings, and fraction of workers who commute by public transport. This model has \\(n=35,656\\) observations and \\(N=104\\) cities. This specification allows the relationship between \\(\\Delta W\\) and \\(M\\) to be nonlinear (a quadratic) with a discontinuous shift in the intercept and slope at the threshold. The authors’ major prediction is that \\(\\delta_{0}\\) should be large and negative. The threshold parameter \\(\\gamma\\) is the minority fraction which triggers discontinuous white outward migration.\n\\({ }^{2}\\) Metropolitan Statistical Area (MSA). The authors use the 104 MSAs with at least 100 census tracts. As the threshold regression model is an explicit nonlinear regression, the appropriate estimation method is NLLS. Since the model is linear in all coefficients except for \\(\\gamma\\), the best computational technique is concentrated least squares. For each \\(\\gamma\\) the model is linear and the coefficients can be estimated by least squares. This produces a concentrated average of squared errors \\(S_{n}^{*}(\\gamma)\\) which can be minimized to find the NLLS estimator \\(\\widehat{\\gamma}\\). To illustrate, the concentrated least squares criterion for the Card-Mas-Rothstein dataset \\({ }^{3}\\) is displayed in Figure 23.3(a). As you can see, the criterion \\(S_{n}^{*}(\\gamma)\\) is highly non-smooth. This is typical in threshold applications. Consequently, the criterion needs to be minimized by grid search. The criterion is a step function with a step at each observation. A full search would calculate \\(S_{n}^{*}(\\gamma)\\) for \\(\\gamma\\) equalling each value of \\(M_{c i t-1}\\) in the sample. A simplification (which we employ) is to calculate the criterion at a smaller number of gridpoints. In our illustration we use 100 gridpoints equally-spaced between the \\(0.1\\) and \\(0.9\\) quantiles \\({ }^{4}\\) of \\(M_{c i t-1}\\). (These quantiles are the boundaries of the displayed graph.) What you can see is that the criterion is generally lower for values of \\(\\gamma\\) between \\(0.05\\) and \\(0.25\\), and especially lower for values of \\(\\gamma\\) near \\(0.2\\). The minimum is obtained at \\(\\widehat{\\gamma}=0.198\\). This is the NLLS estimator. In the context of the application this means that the point estimate of the tipping point is \\(20 %\\), which means that when the neighborhood minority fraction exceeds \\(20 %\\) white households discontinuously change their behavior. The remaining NLLS estimates are obtained by least squares regression (23.9) setting \\(\\gamma=\\widehat{\\gamma}\\).\nOur estimates are reported in Table 23.2. Following Card, Mas, and Rothstein (2008) the standard errors are clustered \\({ }^{5}\\) by city (MSA). Examining Table \\(23.2\\) we can see that the estimates suggest that neighborhood declines in the white population were increasing in the minority fraction, with a sharp and accelerating decline above the tipping point of \\(20 %\\). The estimated discontinuity is \\(-11.6 %\\). This is nearly identical to the estimate obtained by Card, Mas and Rothstein (2008) using an ad hoc estimation method.\nThe white population was also decreasing in response to the unemployment rate, the renter share, and the use of public transportation, but increasing in response to the vacancy rate. Another interesting observation is that despite the fact that the sample has a very large \\((35,656)\\) number of observations the standard errors for the parameter estimates are rather large indicating considerable imprecision. This is mostly due to the clustered covariance matrix calculation as there are only \\(N=104\\) clusters.\nThe asymptotic theory of threshold regression is non-standard. Chan (1993) showed that under correct specification the threshold estimator \\(\\widehat{\\gamma}\\) converges in probability to \\(\\gamma\\) at the fast rate \\(O_{p}\\left(n^{-1}\\right)\\) and that the other parameter estimators have conventional asymptotic distributions, justifying the standard errors as reported in Table 23.2. He also showed that the threshold estimator \\(\\widehat{\\gamma}\\) has a non-standard asymptotic distribution which cannot be used for confidence interval construction.\nB. E. Hansen (2000) derived the asymptotic distribution of \\(\\widehat{\\gamma}\\) and associated test statistics under a “small threshold effect” asymptotic framework for a continuous threshold variable \\(Q\\). This distribution theory permits simple construction of an asymptotic confidence interval for \\(\\gamma\\). In brief, he shows that under correct specification, independent observations, and homoskedasticity, the F statistic for testing\n\\({ }^{3}\\) Using the 1970-1980 sample and model (23.9).\n\\({ }^{4}\\) It is important that the search be constrained to values of \\(\\gamma\\) which lie well within the support of the threshold variable. Otherwise the regression may be infeasible. The required degree of trimming (away from the boundaries of the support) depends on the individual application.\n\\({ }^{5}\\) It is not clear to me whether clustering is appropriate in this application. One motivation for clustering is inclusion of fixed effects as this induces correlation across observations within a cluster. However in this case the typical number of observations per cluster is several hundred so this correlation is near zero. Another motivation for clustering is that the regression error \\(e_{c i t}\\) (the unobserved factors for changes in white population) is correlated across tracts within a city. While it may be expected that attitudes towards minorities among whites may be correlated within a city, it seems less clear that we should expect unconditional correlation in population changes. Table 23.2: Threshold Estimates: Card-Mas-Rothstein (2008) Model\n\n\n\nVariable\nEstimate\nStandard Error\n\n\n\n\nIntercept Change\n\\(-11.6\\)\n\\(3.7\\)\n\n\nSlope Change\n\\(-74.1\\)\n\\(42.6\\)\n\n\nMinority Fraction\n\\(-54.4\\)\n\\(28.8\\)\n\n\nMinority Fraction \\({ }^{2}\\)\n\\(142.3\\)\n\\(23.9\\)\n\n\nUnemployment Rate\n\\(-81.1\\)\n\\(38.8\\)\n\n\n\\(\\log (\\) Mean Family Income)\n\\(3.4\\)\n\\(3.6\\)\n\n\nHousing Vacancy Rate\n\\(324.9\\)\n\\(40.2\\)\n\n\nRenter Share\n\\(-62.7\\)\n\\(13.6\\)\n\n\nFraction Single-Unit\n\\(-4.8\\)\n\\(9.5\\)\n\n\nFraction Public Transport\n\\(-91.6\\)\n\\(24.5\\)\n\n\nIntercept\n\\(14.8\\)\nna\n\n\nMSA Fixed Effects\nyes\n\n\n\nThreshold\n\\(0.198\\)\n\n\n\n\\(99 %\\) Confidence Interval\n\\([0.198,0.209]\\)\n\n\n\n\\(N=\\) Number of MSAs\n104\n\n\n\n\\(n=\\) Number of observations\n35,656\n\n\n\n\nthe hypothesis \\(\\mathbb{H}_{0}: \\gamma=\\gamma_{0}\\) has the asymptotic distribution\n\\[\n\\frac{n\\left(S_{n}^{*}\\left(\\gamma_{0}\\right)-S_{n}^{*}(\\widehat{\\gamma})\\right)}{S_{n}^{*}(\\widehat{\\gamma})} \\underset{d}{\\longrightarrow} \\xi\n\\]\nwhere \\(\\mathbb{P}[\\xi \\leq x]=(1-\\exp (-x / 2))^{2}\\). The \\(1-\\alpha\\) quantile of \\(\\xi\\) can be found by solving \\(\\left(1-\\exp \\left(-c_{1-\\alpha} / 2\\right)\\right)^{2}=\\) \\(1-\\alpha\\), and equals \\(c_{1-\\alpha}=-2 \\log (1-\\sqrt{1-\\alpha})\\). For example, \\(c_{.95}=7.35\\) and \\(c_{.99}=10.6\\).\nBased on test inversion a valid \\(1-\\alpha\\) asymptotic confidence interval for \\(\\gamma\\) is the set of \\(F\\) statistics which are less than \\(c_{1-\\alpha}\\) and equals\n\\[\nC_{1-\\alpha}=\\left\\{\\gamma: \\frac{n\\left(S_{n}^{*}(\\gamma)-S_{n}^{*}(\\widehat{\\gamma})\\right)}{S_{n}^{*}(\\widehat{\\gamma})} \\leq c_{1-\\alpha}\\right\\}=\\left\\{\\gamma: S_{n}^{*}(\\gamma) \\leq S_{n}^{*}(\\widehat{\\gamma})\\left(1+\\frac{c_{1-\\alpha}}{n}\\right)\\right\\}\n\\]\nThis is constructed numerically by grid search. In our example \\(C_{0.99}=[0.198,0.209]\\). This is a narrow confidence interval. However, this interval does not take into account clustered dependence. Based on Hansen’s theory we can expect that under cluster dependence the asymptotic distribution \\(\\xi\\) needs to be re-scaled. This will result in replacing \\(1+c_{1-\\alpha} / n\\) in the above formula with \\(1+\\rho c_{1-\\alpha} / n\\) for some adjustment factor \\(\\rho\\). This will widen the confidence interval. Based on the shape of Figure \\(23.3(\\mathrm{a})\\) the adjusted confidence interval may not be too wide. However this is a conjecture as the theory has not been worked out so we cannot estimate the adjustment factor \\(\\rho\\).\nEmpirical practice and simulation results suggest that threshold estimates tend to be quite imprecise unless a moderately large sample (e.g., \\(n \\geq 500\\) ) is used. The threshold parameter is identified by observations close to the threshold, not by observations far from the threshold. This requires large samples to ensure that there are a sufficient number of observations near the threshold in order to be able to pin down its location\nGiven the coefficient estimates the regression function can be plotted along with confidence intervals calculated conventionally. In Figure 23.3(b) we plot the estimated regression function with \\(95 %\\) asymptotic confidence intervals calculated based on the covariance matrix for the estimates \\(\\left(\\widehat{\\beta}_{1}, \\widehat{\\beta}_{2}, \\widehat{\\delta}_{1}, \\widehat{\\delta}_{2}\\right)\\). The estimate \\(\\widehat{\\theta}\\) does not contribute if the regression function is evaluated at mean values. We ignore estimation of the intercept \\(\\widehat{\\alpha}\\) as its variance is not identified under clustering dependence and we are primarily interest in the magnitude of relative comparisons. What we see in Figure 23.3(b) is that the regression function is generally downward sloped, indicating that the change in the white population is generally decreasing as the minority fraction increases, as expected. The tipping effect is visually strong. When the fraction minority crosses the tipping point there are sharp decreases in both the level and the slope of the regression function. The level of the estimated regression function also indicates that the expected change in the white population switches from positive to negative at the tipping point, consistent with the segregation hypothesis. It is instructive to observe that the confidence bands are quite wide despite the large sample. This is largely due to the decision to use a clustered covariance matrix estimator. Consequently there is considerable uncertainty in the location of the regression function. The confidence bands are widest at the estimated tipping point.\nThe empirical results presented in this section are distinct from, yet similar to, those reported in Card, Mas, and Rothstein (2008). This is an influential paper as it used the rigor of an economic model to give insight about segregation behavior, and used a rich detailed dataset to investigate the strong tipping point prediction."
  },
  {
    "objectID": "chpt23-nonliear-ls.html#testing-for-nonlinear-components",
    "href": "chpt23-nonliear-ls.html#testing-for-nonlinear-components",
    "title": "22  Nonlinear Least Squares",
    "section": "22.8 Testing for Nonlinear Components",
    "text": "22.8 Testing for Nonlinear Components\nIdentification can be tricky in nonlinear regression models. Suppose that\n\\[\nm(X, \\theta)=X^{\\prime} \\beta+X(\\gamma)^{\\prime} \\delta\n\\]\nwhere \\(X(\\gamma)\\) is a function of \\(X\\) and an unknown parameter \\(\\gamma\\). Examples for \\(X(\\gamma)\\) include the Box-Cox transformation and \\(X \\mathbb{1}\\{X>\\gamma\\}\\). The latter arises in the Regression Kink and threshold regression models.\nThe model is linear when \\(\\delta=0\\). This is often a useful hypothesis (sub-model) to consider. For example, in the Card-Mas-Rothstein (2008) application this is the hypothesis of no tipping point which is the key issue explored in their paper.\nIn this section we consider tests of the hypothesis \\(\\mathbb{H}_{0}: \\delta=0\\). Under \\(\\mathbb{H}_{0}\\) the model is \\(Y=X^{\\prime} \\beta+e\\) and both \\(\\delta\\) and \\(\\gamma\\) have dropped out. This means that under \\(\\mathbb{H}_{0}\\) the parameter \\(\\gamma\\) is not identified. This renders standard distribution theory invalid. When the truth is \\(\\delta=0\\) the NLLS estimator of \\((\\beta, \\delta, \\gamma)\\) is not asymptotically normally distributed. Classical tests excessively over-reject \\(\\mathbb{H}_{0}\\) if applied with conventional critical values.\nAs an example consider the threshold regression (23.9). The hypothesis of no tipping point corresponds to the joint hypothesis \\(\\delta_{0}=0\\) and \\(\\delta_{1}=0\\). Under this hypothesis the parameter \\(\\gamma\\) is not identified.\nTo test the hypothesis a standard test is to reject for large values of the F statistic\n\\[\n\\mathrm{F}=\\frac{n\\left(\\widetilde{S}_{n}-S_{n}^{*}(\\widehat{\\gamma})\\right)}{S_{n}^{*}(\\widehat{\\gamma})}\n\\]\nwhere \\(\\widetilde{S}_{n}=n^{-1} \\sum_{i=1}^{n}\\left(Y_{i}-X_{i}^{\\prime} \\widehat{\\beta}\\right)^{2}\\) and \\(\\widehat{\\beta}\\) is the least squares coefficient from the regression of \\(Y\\) on \\(X\\). This is the difference between the error variance estimators based on estimates calculated under the null \\(\\left(\\widetilde{S}_{n}\\right)\\) and alternative \\(\\left(S_{n}^{*}(\\widehat{\\gamma})\\right)\\).\nThe F statistic can be written as\n\\[\n\\mathrm{F}=\\max _{\\gamma} \\mathrm{F}_{n}(\\gamma)=\\mathrm{F}_{n}(\\widehat{\\gamma})\n\\]\nwhere\n\\[\n\\mathrm{F}_{n}(\\gamma)=\\frac{n\\left(\\widetilde{S}_{n}-S_{n}^{*}(\\gamma)\\right)}{S_{n}^{*}(\\gamma)}\n\\]\nThe statistic \\(\\mathrm{F}_{n}(\\gamma)\\) is the classical \\(\\mathrm{F}\\) statistic for a test of \\(\\mathbb{H}_{0}: \\delta=0\\) when \\(\\gamma\\) is known. We can see from this representation that \\(\\mathrm{F}\\) is non-standard as it is the maximum over a potentially large number of statistics \\(\\mathrm{F}_{n}(\\gamma)\\)\n\nFigure 23.4: Test for Threshold Regression in CMR Model\nTo illustrate, Figure \\(23.4\\) plots the test statistic \\(\\mathrm{F}_{n}(\\gamma)\\) as a function of \\(\\gamma\\). You can see that the function is erratic, similar to the concentrated criterion \\(S_{n}^{*}(\\gamma)\\). This is sensible, because \\(\\mathrm{F}_{n}(\\gamma)\\) is an affine function of the inverse of \\(S_{n}^{*}(\\gamma)\\). The statistic is maximized at \\(\\widehat{\\gamma}\\) because of this duality. The maximum value is \\(\\mathrm{F}=\\mathrm{F}_{n}(\\hat{\\gamma})\\). In this application we find \\(\\mathrm{F}=62.4\\). This is extremely high by conventional standards.\nThe asymptotic theory of the test has been worked out by Andrews and Ploberger (1994) and B. E. Hansen (1996). In particular, Hansen shows the validity of the multiplier bootstrap for calculation of p-values for independent observations. The method is as follows.\n\nOn the observations \\(\\left(Y_{i}, X_{i}\\right)\\) calculate the \\(\\mathrm{F}\\) test statistic for \\(\\mathbb{H}_{0}\\) against \\(\\mathbb{H}_{1}\\) (or any other standard statistic such as a Wald or likelihood ratio).\nFor \\(b=1, \\ldots, B\\) :\n\n\nGenerate \\(n\\) random variables \\(\\xi_{i}^{*}\\) with mean zero and variance 1 (standard choices are normal and Rademacher).\nSet \\(Y_{i}^{*}=\\widehat{e}_{i} \\xi_{i}^{*}\\) where \\(\\widehat{e}_{i}\\) are the NLLS residuals.\n\\(\\operatorname{On}\\left(Y_{i}^{*}, X_{i}\\right)\\) calculate the \\(\\mathrm{F}\\) statistic \\(\\mathrm{F}_{b}^{*}\\) for \\(\\mathbb{H}_{0}\\) against \\(\\mathbb{H}_{1}\\). 3. The multiplier bootstrap p-value is \\(p_{n}^{*}=\\frac{1}{B} \\sum_{b=1}^{B} \\mathbb{1}\\left\\{\\mathrm{F}_{b}^{*}>\\mathrm{F}\\right\\}\\).\n\n 1. If \\(p_{n}^{*}<\\alpha\\) the test is significant at level \\(\\alpha\\).\n\nCritical values can be calcualted as empirical quantiles of the bootstrap statistics \\(\\mathrm{F}_{b}^{*}\\).\n\nIn step \\(2 \\mathrm{~b}\\) you can alternatively set \\(Y_{i}^{*}=\\widehat{\\beta}^{\\prime} Z_{i}+\\widehat{e}_{i} \\xi_{i}^{*}\\). Tests on \\(\\delta\\) are invariant to the bootstrap value of \\(\\delta\\). What is important is that the bootstrap data satisfy the null hypothesis.\nFor clustered samples we need to make a minor modification. Write the regression by cluster as\n\\[\n\\boldsymbol{Y}_{g}=\\boldsymbol{X}_{g} \\beta+\\boldsymbol{X}_{g}(\\gamma) \\delta+\\boldsymbol{e}_{g} .\n\\]\nThe bootstrap method is modified by altering steps \\(2 \\mathrm{a}\\) and \\(2 \\mathrm{~b}\\) above. Let \\(N\\) denote the number of clusters. The modified algorithm uses the following steps.\n\n\nGenerate \\(N\\) random variables \\(\\xi_{g}^{*}\\) with mean zero and variance 1 .\n\n\n\n\\(\\operatorname{Set} \\boldsymbol{Y}_{g}^{*}=\\widehat{\\boldsymbol{e}}_{g} \\xi_{g}^{*}\\)\n\nTo illustrate we apply this test to the threshold regression (23.9) estimated with the Card-Mas-Rothstein (2008) data. We use \\(B=10,000\\) bootstrap replications. Applying the first algorithm (suitable for independent observations) the bootstrap p-value is \\(0 %\\). The \\(99 %\\) critical value is \\(16.7\\), so the observed value of \\(\\mathrm{F}=62.4\\) far exceeds this threshold. Applying the second algorithm (suitable under cluster dependence) the bootstrap p-value is \\(3.1 %\\). The \\(95 %\\) critical value is \\(56.6\\) and the \\(99 %\\) is \\(75.3\\). Thus the observed value of \\(F=62.4\\) is “significant” at the \\(5 %\\) but not the \\(1 %\\) level. For a sample of size \\(n=35,656\\) this is surprisingly mild significance. These critical values are indicated on Figure \\(23.4\\) by the dashed lines. The F statistic process breaks the \\(90 %\\) and \\(95 %\\) critical values but not the \\(99 %\\). Thus despite the visually strong evidence of a tipping effect from the previous section the statistical evidence of this effect is strong but not overwhelming."
  },
  {
    "objectID": "chpt23-nonliear-ls.html#computation",
    "href": "chpt23-nonliear-ls.html#computation",
    "title": "22  Nonlinear Least Squares",
    "section": "22.9 Computation",
    "text": "22.9 Computation\nStata has a built-in command nl for NLLS estimation. You need to specify the nonlinear equation and give starting values for the numerical search. It is prudent to try several starting values because the algorithm is not guaranteed to converge to the global minimum.\nEstimation of NLLS in R or MATLAB requires a bit more programming but is straightforward. You write a function which calculates the average squared error \\(S_{n}(\\theta)\\) (or concentrated average squared error) as a function of the parameters. You then call a numerical optimizer to minimize this function. For example, in R for vector-valued parameters the standard optimizer is optim. For scalar parameters use optimize."
  },
  {
    "objectID": "chpt23-nonliear-ls.html#technical-proofs",
    "href": "chpt23-nonliear-ls.html#technical-proofs",
    "title": "22  Nonlinear Least Squares",
    "section": "22.10 Technical Proofs*",
    "text": "22.10 Technical Proofs*\nProof of Theorem 23.1. We appeal to Theorem \\(22.3\\) which holds under five conditions. Conditions 1,2 , 4, and 5 are satisfied directly by Assumption 23.1, parts 1, 2, 5, and 6. To verify condition 3, observe that by the \\(c_{r}\\) inequality (B.5) and \\(|m(X, \\theta)| \\leq m(X)\\)\n\\[\n(Y-m(X, \\theta))^{2} \\leq 2 Y^{2}+2 m(X)^{2} .\n\\]\nThe right side has finite expectation under Assumptions 23.1, parts 3 and 4 . We conclude that \\(\\widehat{\\theta} \\underset{p}{\\longrightarrow} \\theta_{0}\\) as stated.\nProof of Theorem 23.2. We appeal to Theorem \\(22.4\\) which holds under five conditions (in addition to consistency, which was established in Theorem 23.1). It is convenient to rescale the criterion so that \\(\\rho_{i}(\\theta)=\\frac{1}{2}\\left(Y_{i}-m\\left(X_{i}, \\theta\\right)\\right)^{2}\\). Then \\(\\psi_{i}=-m_{\\theta i} e_{i}\\).\nTo show condition 1, by the Cauchy-Schwarz inequality (B.32) and Assumption 23.2.2 and 23.2.4\n\\[\n\\mathbb{E}\\left\\|\\psi_{i}\\right\\|^{2}=\\mathbb{E}\\left\\|m_{\\theta i} e_{i}\\right\\|^{2} \\leq\\left(\\mathbb{E}\\left\\|m_{\\theta i}\\right\\|^{4} \\mathbb{E}\\left[e_{i}^{4}\\right]\\right)^{1 / 2}<\\infty .\n\\]\nWe next show condition 3. Using Assumption 23.2.1, we calculate that\n\\[\nS(\\theta)=\\mathbb{E}\\left[\\rho_{i}(\\theta)\\right]=\\frac{1}{2} \\mathbb{E}\\left[e^{2}\\right]+\\frac{1}{2} \\mathbb{E}\\left[\\left(m\\left(X, \\theta_{0}\\right)-m(X, \\theta)\\right)^{2}\\right] .\n\\]\nThus\n\\[\n\\psi(\\theta)=\\frac{\\partial}{\\partial \\theta} S(\\theta)=-\\mathbb{E}\\left[m_{\\theta}(X, \\theta)\\left(m\\left(X, \\theta_{0}\\right)-m(X, \\theta)\\right)\\right]\n\\]\nwith derivative\n\\[\n\\begin{aligned}\n\\boldsymbol{Q}(\\theta) &=-\\frac{\\partial}{\\partial \\theta^{\\prime}} \\mathbb{E}\\left[m_{\\theta}(X, \\theta)\\left(m\\left(X, \\theta_{0}\\right)-m(X, \\theta)\\right)\\right] \\\\\n&=\\mathbb{E}\\left[m_{\\theta}(X, \\theta) m_{\\theta}(X, \\theta)^{\\prime}\\right]-\\mathbb{E}\\left[m_{\\theta \\theta}\\left(X, \\theta_{0}\\right)\\left(m\\left(X, \\theta_{0}\\right)-m(X, \\theta)\\right)\\right] .\n\\end{aligned}\n\\]\nThis exists and is continuous for \\(\\theta \\in \\mathcal{N}\\) under Assumption 23.2.4.\nEvaluating (23.10) at \\(\\theta_{0}\\) we obtain\n\\[\n\\boldsymbol{Q}=\\boldsymbol{Q}\\left(\\theta_{0}\\right)=\\mathbb{E}\\left[m_{\\theta i} m_{\\theta i}^{\\prime}\\right]>0\n\\]\nunder Assumption 23.2.5. This verifies condition 2.\nCondition 4 holds if \\(\\psi(Y, X, \\theta)=m_{\\theta}(X, \\theta)(Y-m(X, \\theta))\\) is Lipschitz-continuous in \\(\\theta \\in \\mathscr{N}\\). This holds because both \\(m_{\\theta}(X, \\theta)\\) and \\(m(X, \\theta)\\) are differentiable in the compact set \\(\\theta \\in \\mathscr{N}\\), and bounded fourth moments (Assumptions \\(23.2 .2\\) and 23.2.4) implies that the Lipschitz bound for \\(\\psi(Y, X, \\theta)\\) has a finite second moment.\nCondition 5 is implied by Assumption 23.2.6.\nTogether, the five conditions of Theorem \\(22.4\\) are satisfied and the stated result follows."
  },
  {
    "objectID": "chpt23-nonliear-ls.html#exercises",
    "href": "chpt23-nonliear-ls.html#exercises",
    "title": "22  Nonlinear Least Squares",
    "section": "22.11 Exercises",
    "text": "22.11 Exercises\nExercise 23.1 Take the model \\(Y=\\exp (\\theta)+e\\) with \\(\\mathbb{E}[e]=0\\).\n\nIs the CEF linear or nonlinear in \\(\\theta\\) ? Is this a nonlinear regression model?\nIs there a way to estimate the model using linear methods? If so, explain how to obtain an estimator \\(\\widehat{\\theta}\\) for \\(\\theta\\).\nIs your answer in part (b) the same as the NLLS estimator, or different?\n\nExercise 23.2 Take the model \\(Y^{(\\lambda)}=\\beta_{0}+\\beta_{1} X+e\\) with \\(\\mathbb{E}[e \\mid X]=0\\) where \\(Y^{(\\lambda)}\\) is the Box-Cox transformation of \\(Y\\). (a) Is this a nonlinear regression model in the parameters \\(\\left(\\lambda, \\beta_{0}, \\beta_{1}\\right)\\) ? (Careful, this is tricky.)\nExercise 23.3 Take the model \\(Y=\\frac{\\beta_{1}}{\\beta_{2}+\\beta_{3} X}+e\\) with \\(\\mathbb{E}[e \\mid X]=0\\).\n\nAre the parameters \\(\\left(\\beta_{1}, \\beta_{2}, \\beta_{3}\\right)\\) identified?\nIf not, what parameters are identified? How would you estimate the model?\n\nExercise 23.4 Take the model \\(Y=\\beta_{1} \\exp \\left(\\beta_{2} X\\right)+e\\) with \\(\\mathbb{E}[e \\mid X]=0\\).\n\nAre the parameters \\(\\left(\\beta_{1}, \\beta_{2}\\right)\\) identified?\nFind an expression to calculate the covariance matrix of the NLLS estimatiors \\(\\left(\\widehat{\\beta}_{1}, \\widehat{\\beta}_{2}\\right)\\).\n\nExercise 23.5 Take the model \\(Y=m(X, \\theta)+e\\) with \\(e \\mid X \\sim \\mathrm{N}\\left(0, \\sigma^{2}\\right)\\). Find the MLE for \\(\\theta\\) and \\(\\sigma^{2}\\).\nExercise 23.6 Take the model \\(Y=\\exp \\left(X^{\\prime} \\theta\\right)+e\\) with \\(\\mathbb{E}[Z e]=0\\), where \\(X\\) is \\(k \\times 1\\) and \\(Z\\) is \\(\\ell \\times 1\\).\n\nWhat relationship between \\(\\ell\\) and \\(k\\) is necessary for identification of \\(\\theta\\) ?\nDescribe how to estimate \\(\\theta\\) by GMM.\nDescribe an estimator of the asymptotic covariance matrix.\n\nExercise 23.7 Suppose that \\(Y=m(X, \\theta)+e\\) with \\(\\mathbb{E}[e \\mid X]=0, \\widehat{\\theta}\\) is the NLLS estimator, and \\(\\widehat{\\boldsymbol{V}}\\) the estimator of \\(\\operatorname{var}[\\widehat{\\theta}]\\). You are interested in the CEF \\(\\mathbb{E}[Y \\mid X=x]=m(x)\\) at some \\(x\\). Find an asymptotic \\(95 %\\) confidence interval for \\(m(x)\\).\nExercise 23.8 The file PSS2017 contains a subset of the data from Papageorgiou, Saam, and Schulte (2017). For a robustness check they re-estimated their CES production function using approximated capital stocks rather than capacities as their input measures. Estimate the model (23.3) using this alternative measure. The variables for \\(Y, X_{1}\\), and \\(X_{2}\\) are EG_total, EC_c_alt, and EC_d_alt, respectively. Compare the estimates with those reported in Table 23.1.\nExercise 23.9 The file RR2010 contains the U.S. observations from the Reinhart and Rogoff (2010). The data set has observations on real GDP growth, debt/GDP, and inflation rates. Estimate the model (23.4) setting \\(Y\\) as the inflation rate and \\(X\\) as the debt ratio.\nExercise 23.10 In Exercise 9.26, you estimated a cost function on a cross-section of electric companies. Consider the nonlinear specification\n\\[\n\\log T C=\\beta_{1}+\\beta_{2} \\log Q+\\beta_{3}(\\log P L+\\log P K+\\log P F)+\\beta_{4} \\frac{\\log Q}{1+\\exp (-(\\log Q-\\gamma))}+e .\n\\]\nThis model is called a smooth threshold model. For values of \\(\\log Q\\) much below \\(\\gamma\\), the variable \\(\\log Q\\) has a regression slope of \\(\\beta_{2}\\). For values much above \\(\\beta_{7}\\), the regression slope is \\(\\beta_{2}+\\beta_{4}\\). The model imposes a smooth transition between these regimes.\n\nThe model works best when \\(\\gamma\\) is selected so that several values (in this example, at least 10 to 15) of \\(\\log Q_{i}\\) are both below and above \\(\\gamma\\). Examine the data and pick an appropriate range for \\(\\gamma\\).\nEstimate the model by NLLS using a global numerical search over \\(\\left(\\beta_{1}, \\beta_{2}, \\beta_{3}, \\beta_{4}, \\gamma\\right)\\).\nEstimate the model by NLLS using a concentrated numerical search over \\(\\gamma\\). Do you obtain the same results?\nCalculate standard errors for all the parameters estimates \\(\\left(\\beta_{1}, \\beta_{2}, \\beta_{3}, \\beta_{4}, \\gamma\\right)\\)."
  },
  {
    "objectID": "chpt24-quantile-reg.html#introduction",
    "href": "chpt24-quantile-reg.html#introduction",
    "title": "23  Quantile Regression",
    "section": "23.1 Introduction",
    "text": "23.1 Introduction\nThis chapter introduces median regression (least absolute deviations) and quantile regression. An excellent monograph on the subject is Koenker (2005).\nA conventional goal in econometrics is estimation of impact of a variable \\(X\\) on another variable \\(Y\\). We have discussed projections and conditional expectations but these are not the only measures of impact. Alternative measures include the conditional median and conditional quantile. We will focus on the case of continuously-distributed \\(Y\\) where quantiles are uniquely defined."
  },
  {
    "objectID": "chpt24-quantile-reg.html#median-regression",
    "href": "chpt24-quantile-reg.html#median-regression",
    "title": "23  Quantile Regression",
    "section": "23.2 Median Regression",
    "text": "23.2 Median Regression\nRecall that the median of \\(Y\\) is the value \\(m=\\operatorname{med}[Y]\\) such that \\(\\mathbb{P}[Y \\leq m]=\\mathbb{P}[Y \\geq m]=0.5\\). The median can be thought of the “typical realization”. For example, the median wage \\(\\$ 19.23\\) in the CPS dataset can be interpreted as the wage of a “typical wage-earner”. One-half of wage earners have wages less than \\(\\$ 19\\) and one-half have wages greater than \\(\\$ 19\\).\nWhen a distribution is symmetric then the median equals the mean but when the distribution is asymmetric they differ.\nThroughout this textbook we have primarily focused on conditional relationships. For example, the conditional expectation is the expected value within a sub-population. Similarly we define the conditional median as the median of a sub-population.\nDefinition 24.1 The conditional median of \\(Y\\) given \\(X=x\\) is the value \\(m(x)=\\) med \\([Y \\mid X=x]\\) such that \\(\\mathbb{P}[Y \\leq m(x) \\mid X=x]=0.5\\).\nFor example, in the CPS sample the median wage for men is \\(\\$ 21.15\\) and the median wage for women is \\(\\$ 16.83\\). These are the wages of a “typical” man and woman.\nWe can write the relationship between \\(Y\\) and \\(X\\) as the median regression model:\n\\[\n\\begin{aligned}\nY &=m(X)+e \\\\\n\\operatorname{med}[e \\mid X] &=0 .\n\\end{aligned}\n\\]\nAs stated this is simply a definitional framework. \\(m(X)\\) is the conditional median given the random variable \\(X\\). The error \\(e\\) is the deviation of \\(Y\\) from its conditional median and by definition has a conditional median of zero.\nWe call \\(m(x)\\) the median regression function. In general it can take any shape. However, for practical convenience we focus on models which are linear in parameters \\(m(x)=x^{\\prime} \\beta\\). (This is not fundamentally restrictive as it allows series approximations.) This gives rise to the linear median regression model:\n\\[\n\\begin{aligned}\nY &=X^{\\prime} \\beta+e \\\\\n\\operatorname{med}[e \\mid X] &=0 .\n\\end{aligned}\n\\]\nEquivalently, the model states that \\(\\operatorname{med}[Y \\mid X]=X^{\\prime} \\beta\\). As in the case of regression the true median regression function is not necessarily linear, so the assumption of linearity is a meaningful assumption. The model resembles the linear regression model but is different. The coefficients \\(\\beta\\) in the median and mean regression models are not necessarily equal to one another.\nTo estimate \\(\\beta\\) it is useful to characterize \\(\\beta\\) as a function of the distribution. Recall that the least squares estimator is derived from the foundational property that the expectation minimizes the expected squared loss, that is, \\(\\mu=\\operatorname{argmin}_{\\theta} \\mathbb{E}\\left[(Y-\\theta)^{2}\\right]\\). We now present analogous properties of the median.\nDefine the sign function\n\\[\n\\frac{d}{d x}|x|=\\operatorname{sgn}(x)=\\left\\{\\begin{array}{cc}\n\\mathbb{1}\\{x>0\\}-\\mathbb{1}\\{x<0\\}, & x \\neq 0 \\\\\n0 & x=0 .\n\\end{array}\\right.\n\\]\nTheorem 24.1 Assume \\(Y\\) is continuously distributed. Then the median \\(m\\) satisfies\n\\[\n\\mathbb{E}[\\operatorname{sgn}(Y-m)]=0 .\n\\]\nIf in addition \\(\\mathbb{E}|Y|<\\infty\\) it satisfies\n\\[\nm=\\underset{\\theta}{\\operatorname{argmin}} \\mathbb{E}|Y-\\theta| .\n\\]\nIf the conditional distribution \\(F(y \\mid x)\\) of \\(Y\\) given \\(X=x\\) is continuous in \\(y\\) the conditional median error \\(e=Y-m(X)\\) satisfies\n\\[\n\\mathbb{E}[\\operatorname{sgn}(e) \\mid X]=0 .\n\\]\nIf in addition \\(\\mathbb{E}|Y|<\\infty\\) the conditional median satisfies\n\\[\nm(x)=\\underset{\\theta}{\\operatorname{argmin}} \\mathbb{E}[|Y-\\theta| \\mid X=x] .\n\\]\nIf \\((Y, X)\\) satisfy the linear median regression model (24.1) and \\(E|Y|<\\infty\\) then the coefficient \\(\\beta\\) satisfies\n\\[\n\\beta=\\underset{b}{\\operatorname{argmin}} \\mathbb{E}\\left|Y-X^{\\prime} b\\right| .\n\\]\nThe proof is in Section \\(24.16\\). Expression (24.6) is foundational. It shows that the median regression coefficient \\(\\beta\\) minimizes the expected absolute difference between \\(Y\\) and the predicted value \\(X^{\\prime} \\beta\\). This is foundational as it expresses the coefficient as a function of the probability distribution. This result is a direct analog of the property that the mean regression coefficient minimizes the expected squared loss. The difference between the two is the loss function - the measure of the magnitude of a prediction error. To visualize, Figure 24.1 (a) displays the two loss functions. Comparing the two, squared loss puts small penalty on small errors yet large penalty on large errors. Both are symmetric and so treat positive and negative errors identically.\n\n\nQuadratic and Absolute Loss Functions\n\n\n\nLAD Criterion with \\(n=7\\)\n\nFigure 24.1: LAD Criterion\nIn applications the linear assumption \\(X^{\\prime} \\beta\\) is unlikely to be valid except in a saturated dummy variable regression. Thus in practice we should view a linear model as a useful approximation rather than a literal truth. To allow the model to be an approximation we define the coefficient \\(\\beta\\) as the best linear median predictor\n\\[\n\\beta \\stackrel{\\text { def }}{=} \\underset{b}{\\operatorname{argmin}} \\mathbb{E}\\left|Y-X^{\\prime} b\\right| .\n\\]\nThis equals the true conditional median coefficient when the conditional median is linear, but is defined for general distributions satisfying \\(E|Y|<\\infty\\). The first order condition for minimization implies that\n\\[\n\\mathbb{E}[X \\operatorname{sgn}(e)]=0 .\n\\]\nThe facts that (24.4) holds for median regression and (24.8) for the best linear median predictor are analogs to the relationships \\(\\mathbb{E}[e \\mid X]=0\\) and \\(\\mathbb{E}[X e]=0\\) in the conditional expectation and linear projection models."
  },
  {
    "objectID": "chpt24-quantile-reg.html#least-absolute-deviations",
    "href": "chpt24-quantile-reg.html#least-absolute-deviations",
    "title": "23  Quantile Regression",
    "section": "23.3 Least Absolute Deviations",
    "text": "23.3 Least Absolute Deviations\nTheorem \\(24.1\\) shows that in the linear median regression model the median regression coefficient minimizes \\(M(\\beta)=\\mathbb{E}\\left|Y-X^{\\prime} \\beta\\right|\\), the expected absolute error. The sample estimator of this function is the average of absolute errors\n\\[\nM_{n}(\\beta)=\\frac{1}{n} \\sum_{i=1}^{n}\\left|Y_{i}-X_{i}^{\\prime} \\beta\\right| .\n\\]\nThis is similar to the classical average of squared errors function but instead is the average of absolute errors. By not squaring the errors, \\(M_{n}(\\beta)\\) puts less penalty on large errors relative to the average of squared errors function. \\(M_{n}(\\beta)\\)\nSince \\(\\beta\\) minimizes \\(M(\\beta)\\) which is estimated by \\(M_{n}(\\beta)\\) the m-estimator for \\(\\beta\\) is the minimizer of\n\\[\n\\widehat{\\beta}=\\underset{\\beta}{\\operatorname{argmin}} M_{n}(\\beta) .\n\\]\nThis is called the Least Absolute Deviations (LAD) estimator of \\(\\beta\\) as it minimizes the sum of absolute “deviations” of \\(Y_{i}\\) from the fitted value \\(X_{i}^{\\prime} \\beta\\). The function \\(\\widehat{m}(x)=x^{\\prime} \\widehat{\\beta}\\) is the median regression estimator. The LAD estimator \\(\\widehat{\\beta}\\) does not has a closed form solution so must be found by numerical minimization.\nThe LAD residuals are \\(\\widehat{e}_{i}=Y_{i}-X_{i}^{\\prime} \\widehat{\\beta}\\). They approximately satisfy the property\n\\[\n\\frac{1}{n} \\sum_{i=1}^{n} X_{i} \\operatorname{sgn}\\left(\\widehat{e}_{i}\\right) \\simeq 0 .\n\\]\nThe approximation holds exactly if \\(\\widehat{e}_{i} \\neq 0\\) for all \\(i\\) which can occur when \\(Y\\) is continuously distributed. This is the sample version of (24.8).\nThe criterion \\(M_{n}(\\beta)\\) is globally continuous and convex. Its surface resembles the surface of an inverted cut gemstone, as it is covered by a network of flat facets. The facets are joined at the \\(n\\) lines where \\(\\operatorname{sgn}\\left(Y_{i}-X_{i}^{\\prime} \\beta\\right)=0\\). To illustrate, Figure 24.1(b) displays the LAD criterion \\(M_{n}(\\beta)\\) for seven observations \\({ }^{1}\\) with a single regressor and no intercept. The LAD estimator is the minimizer. As the sample size is small the criterion \\(M_{n}(\\beta)\\) is visually facetted. In large samples the facets diminish in size and the criterion approaches a smooth function.\nSince the criterion is faceted the minimum may be a set. Furthermore, because the criterion has discontinuous derivatives classical minimization methods fail. The minimizer can be defined by a set of linear constraints so linear programming methods are appropriate. Fortunately for applications good estimation algorithms are available and simple to use.\nIn Stata, LAD is implemented by qreg. In R, LAD is implemented by rq in the quantreg package."
  },
  {
    "objectID": "chpt24-quantile-reg.html#quantile-regression",
    "href": "chpt24-quantile-reg.html#quantile-regression",
    "title": "23  Quantile Regression",
    "section": "23.4 Quantile Regression",
    "text": "23.4 Quantile Regression\nThe mean and median are measures of the central tendency of a distribution. A measure of the spread of the distribution is its quantiles. Recall that for \\(\\tau \\in[0,1]\\) the \\(\\tau^{t h}\\) quantile \\(q_{\\tau}\\) of \\(Y\\) is defined as the value such that \\(\\mathbb{P}\\left[Y \\leq q_{\\tau}\\right]=\\tau\\). The median is the special case \\(\\tau=0.5\\). It will be convenient to define the quantile operator \\(\\mathbb{Q}_{\\tau}[Y]\\) as the solution to the equation\n\\[\n\\mathbb{P}\\left[Y \\leq \\mathbb{Q}_{\\tau}[Y]\\right]=\\tau .\n\\]\nAs an example, take the distribution of wages from the CPS dataset. The median wage is \\(\\$ 21.14\\). This tells us the “typical” wage rate but not the range of typical values. The \\(0.2\\) quantile is \\(\\$ 11.65\\) and the \\(0.8\\) quantile is \\(\\$ 31.25\\). This shows us that \\(20 %\\) of wage earners had wages of \\(\\$ 11.65\\) or below and \\(20 %\\) had wages of \\(\\$ 31.25\\) and above.\nWe are also interested in the quantiles of conditional distributions. Continuing the above example, consider the distribution of wages among men and women. The \\(0.2,0.5\\), and \\(0.8\\) quantiles are displayed in Table 24.1. We see that the differences between men’s and women’s wages are increasing by quantile.\n\\({ }^{1}\\) These are seven of the twenty observations from Table \\(3.1\\). Table 24.1: Quantiles of Wage Distribution\n\n\n\n\n\\(q_{.2}\\)\n\\(q_{.5}\\)\n\\(q_{.8}\\)\n\n\n\n\nAll\n\\(\\$ 11.65\\)\n\\(\\$ 19.23\\)\n\\(\\$ 31.25\\)\n\n\nMen\n\\(\\$ 12.82\\)\n\\(\\$ 21.14\\)\n\\(\\$ 35.90\\)\n\n\nWomen\n\\(\\$ 10.58\\)\n\\(\\$ 16.83\\)\n\\(\\$ 26.44\\)\n\n\n\nDefinition 24.2 The conditional quantile of \\(Y\\) given \\(X=x\\) is the value \\(q_{\\tau}(x)\\) such that \\(\\mathbb{P}\\left[Y \\leq q_{\\tau}(x) \\mid X=x\\right]=\\tau\\).\nGiven this notation we define the conditional quantile operators \\(\\mathbb{Q}_{\\tau}[Y \\mid X=x]\\) and \\(\\mathbb{Q}_{\\tau}[Y \\mid X]\\). The function \\(q_{\\tau}(x)\\) is also called the quantile regression function.\nThe conditional quantile function \\(q_{\\tau}(x)\\) can take any shape with respect to \\(x\\). It is monotonically increasing in \\(\\tau\\), thus if \\(\\tau_{1}<\\tau_{2}\\) then \\(q_{\\tau_{1}}(x) \\leq q_{\\tau_{2}}(x)\\) for all \\(x\\).\n\n\nWage Quantile Regression\n\n\n\nLog Wage Quantile Regression\n\nFigure 24.2: Quantile Regressions\nTo illustrate we display in Figure 24.2(a) the conditional quantile function of U.S. wages \\({ }^{2}\\) as a function of education, for \\(\\tau=0.1,0.3,0.5,0.7\\), and \\(0.9\\). The five lines plotted are the quantile regression functions \\(q_{\\tau}(x)\\) with wage on the \\(y\\)-axis and education on the \\(x\\)-axis. For each level of education the conditional quantiles \\(q_{\\tau}(x)\\) are strictly ranked in \\(\\tau\\), though for low levels of education they are close to one another. The five quantile regression functions are (generally) increasing in education, though not monotonically. The quantile regression functions also spread out as education increases; thus the gap between the quantiles increases with education. These quantile regression functions provide a summary of the conditional distribution of wages given education.\n\\({ }^{2}\\) Calculated using the full cps \\(90 \\operatorname{mar}\\) dataset. A useful feature of quantile regression is that it is equivariant to monotone transformations. If \\(Y_{2}=\\) \\(\\phi\\left(Y_{1}\\right)\\) where \\(\\phi(y)\\) is nondecreasing then \\(\\mathbb{Q}_{\\tau}\\left[Y_{2} \\mid X=x\\right]=\\phi\\left(\\mathbb{Q}_{\\tau}\\left[Y_{1} \\mid X=x\\right]\\right)\\). Alternatively, if \\(q_{\\tau}^{1}(x)\\) and \\(q_{\\tau}^{2}(x)\\) are the quantile functions of \\(Y_{1}\\) and \\(Y_{2}\\) then \\(q_{\\tau}^{2}(x)=\\phi\\left(q_{\\tau}^{1}(x)\\right)\\). For example, the quantile regression of log wages on education is the logarithm of the quantile regression of wages on eduction. This is displayed in Figure 24.2(b). Interestingly, the quantile regression functions of log wages are roughly parallel with one another and are roughly linear in education for levels above 12 years.\nWe define the quantile regression model analogously to the median regression model:\n\\[\n\\begin{aligned}\nY &=q_{\\tau}(X)+e \\\\\n\\mathbb{Q}_{\\tau}[e \\mid X] &=0 .\n\\end{aligned}\n\\]\nAn important feature of the quantile regression model is that the error \\(e\\) is not centered at zero. Instead it is centered so that its \\(\\tau^{t h}\\) quantile is zero. This is a normalization but it points out that the meaning of the intercept changes when we move from mean regression to quantile regression and as we move between quantiles. The linear quantile regression model is\n\\[\n\\begin{aligned}\nY &=X^{\\prime} \\beta_{\\tau}+e \\\\\n\\mathbb{Q}_{\\tau}[e \\mid X] &=0 .\n\\end{aligned}\n\\]\nRecall that the mean minimizes the squared error loss and the median minimizes the absolute error loss. There is an analog for the quantile. Define the tilted absolute loss function:\n\\[\n\\begin{aligned}\n\\rho_{\\tau}(x) &=\\left\\{\\begin{array}{cc}\n-x(1-\\tau) & x<0 \\\\\nx \\tau & x \\geq 0\n\\end{array}\\right.\\\\\n&=x(\\tau-\\mathbb{1}\\{x<0\\}) .\n\\end{aligned}\n\\]\nFor \\(\\tau=0.5\\) this is the scaled absolute loss \\(\\frac{1}{2}|x|\\). For \\(\\tau<0.5\\) the function is tilted to the right. For \\(\\tau>0\\) it is tilted to the left. To visualize, Figure \\(24.3\\) displays the functions \\(\\rho_{\\tau}(x)\\) for \\(\\tau=0.5\\) and \\(\\tau=0.2\\). The latter function is a tilted version of the former. The function \\(\\rho_{\\tau}(x)\\) has come to be known as the check function because it resembles a check mark \\((\\checkmark)\\).\nLet \\(\\psi_{\\tau}(x)=\\frac{d}{d x} \\rho_{\\tau}(x)=\\tau-\\mathbb{1}\\{x<0\\}\\) for \\(x \\neq 0\\). We now describe some properties of the quantile regression function.\n\nFigure 24.3: Quantile Loss Function\nTheorem 24.2 Assume \\(Y\\) is continuously distributed. Then the quantile \\(q_{\\tau}\\) satisfies\n\\[\n\\mathbb{E}\\left[\\psi_{\\tau}\\left(Y-q_{\\tau}\\right)\\right]=0 .\n\\]\nIf in addition \\(\\mathbb{E}|Y|<\\infty\\) it satisfies\n\\[\nq_{\\tau}=\\underset{\\theta}{\\operatorname{argmin}} \\mathbb{E}\\left[\\rho_{\\tau}(Y-\\theta)\\right] .\n\\]\nIf the conditional distribution \\(F(y \\mid x)\\) of \\(Y\\) given \\(X=x\\) is continuous in \\(y\\) the conditional quantile error \\(e=Y-q_{\\tau}(X)\\) satisfies\n\\[\n\\mathbb{E}\\left[\\psi_{\\tau}(e) \\mid X\\right]=0 .\n\\]\nIf in addition \\(\\mathbb{E}|Y|<\\infty\\) the conditional quantile function satisfies\n\\[\nq_{\\tau}(x)=\\underset{\\theta}{\\operatorname{argmin}} \\mathbb{E}\\left[\\rho_{\\tau}(Y-\\theta) \\mid X=x\\right] .\n\\]\nIf \\((Y, X)\\) satisfy the linear quantile regression model (24.9) and \\(\\mathbb{E}|Y|<\\infty\\) then the coefficient \\(\\beta\\) satisfies\n\\[\n\\beta=\\underset{b}{\\operatorname{argmin}} \\mathbb{E}\\left[\\rho_{\\tau}\\left(Y-X^{\\prime} b\\right)\\right] .\n\\]\nThe proof is in Section \\(24.16\\).\nExpression (24.15) shows that the quantile regression coefficient \\(\\beta\\) minimizes the expected check function distance between \\(Y\\) and the predicted value \\(X^{\\prime} \\beta\\). This connects quantile regression with median and mean regression.\nAs for mean and median regression we should think of the linear model \\(X^{\\prime} \\beta\\) as an approximation. In general we therefore define the coefficient \\(\\beta\\) as the best linear quantile predictor\n\\[\n\\beta_{\\tau} \\stackrel{\\text { def }}{=} \\underset{b}{\\operatorname{argmin}} \\mathbb{E}\\left[\\rho_{\\tau}\\left(Y-X^{\\prime} b\\right)\\right] .\n\\]\nThis equals the true conditional quantile coefficient when true function is linear. The first order condition for minimization implies that\n\\[\n\\mathbb{E}\\left[X \\psi_{\\tau}(e)\\right]=0 .\n\\]\nUnlike the best linear predictor we do not have an explicit expression for \\(\\beta_{\\tau}\\). However from its definition we can see that \\(\\beta_{\\tau}\\) will produce an approximation \\(x^{\\prime} \\beta_{\\tau}\\) to the true conditional quantile function \\(q_{\\tau}(x)\\) with the approximation weighted by the probability distribution of \\(X\\)."
  },
  {
    "objectID": "chpt24-quantile-reg.html#example-quantile-shapes",
    "href": "chpt24-quantile-reg.html#example-quantile-shapes",
    "title": "23  Quantile Regression",
    "section": "23.5 Example Quantile Shapes",
    "text": "23.5 Example Quantile Shapes\n\n\nLinear\n\n\n\nParallel\n\nFigure 24.4: Quantile Shapes"
  },
  {
    "objectID": "chpt24-quantile-reg.html#linear-quantile-functions",
    "href": "chpt24-quantile-reg.html#linear-quantile-functions",
    "title": "23  Quantile Regression",
    "section": "23.6 Linear Quantile Functions",
    "text": "23.6 Linear Quantile Functions\nThe linear quantile regression model implies that the the quantile functions \\(q_{\\tau}(x)\\) are linear in \\(x\\). An example is shown in Figure 24.4(a). Here we plot linear quantile regression functions for \\(\\tau=0.1,0.3,0.5\\), 0.7, and 0.9. In this example the slopes are positive and increasing with \\(\\tau\\).\nLinear quantile regressions are convenient as they are simple to estimate and report. Sometimes linearity can be induced by judicious choice of variable transformation. Compare the quantile regressions in Figure 24.2(a) and Figure 24.2(b). The quantile regression functions for the level of wages appear to be concave; in contrast the quantile regression functions for log wages are close to linear for education above 12 years."
  },
  {
    "objectID": "chpt24-quantile-reg.html#parallel-quantile-functions",
    "href": "chpt24-quantile-reg.html#parallel-quantile-functions",
    "title": "23  Quantile Regression",
    "section": "23.7 Parallel Quantile Functions",
    "text": "23.7 Parallel Quantile Functions\nConsider the model \\(Y=m(X)+e\\) with \\(e\\) independent of \\(X\\). Let \\(z_{\\tau}\\) be the \\(\\tau^{t h}\\) quantile of \\(e\\). In this case the conditional quantile function for \\(Y\\) is \\(q_{\\tau}(x)=m(x)+z_{\\tau}\\). This implies that the functions \\(q_{\\tau_{1}}(x)\\) and \\(q_{\\tau_{2}}(x)\\) are parallel so all of the quantile regression functions are mutually parallel.\nAn example is shown in Figure 24.4(b). Here we plot a set of quantile regression functions which are mutually parallel.\nIn this context - when \\(e\\) is independent of \\(X\\) and/or the quantile regression functions are parallel there is little gained by quantile regression analysis relative to mean regression or median regression. The models have the same slope coefficients and only differ by their intercepts. Furthermore, a regression with \\(e\\) independent of \\(X\\) is a homoskedastic regression. Thus parallel quantile functions is indicative of conditional homoskedasticity.\nOnce again examine the quantile regression functions for log wages displayed in Figure 24.2(b). These functions are visually close to parallel shifts of one another. Thus it appears that the log(wage) regression is close to a homoskedastic regression and slope coefficients should be relatively robust to estimation by least squares, LAD, or quantile regression. This is a strong motivation for applying the logarithmic transformation for a wage regression."
  },
  {
    "objectID": "chpt24-quantile-reg.html#coefficient-heterogeneity",
    "href": "chpt24-quantile-reg.html#coefficient-heterogeneity",
    "title": "23  Quantile Regression",
    "section": "23.8 Coefficient Heterogeneity",
    "text": "23.8 Coefficient Heterogeneity\nConsider the process \\(Y=\\eta^{\\prime} X\\) where \\(\\eta \\sim \\mathrm{N}(\\beta, \\Sigma)\\) is independent of \\(X\\). We described this earlier as a random coefficient model, as the coefficients \\(\\eta\\) are specific to the individual. In this setting the conditional distribution of \\(Y\\) given \\(X=x\\) is \\(\\mathrm{N}\\left(x^{\\prime} \\beta, x^{\\prime} \\Sigma x\\right)\\) so the conditional quantile functions are \\(q_{\\tau}(x)=\\) \\(x^{\\prime} \\beta+z_{\\tau} \\sqrt{x^{\\prime} \\Sigma x}\\) where \\(z_{\\tau}\\) is the \\(\\tau^{t h}\\) quantile of \\(\\mathrm{N}(0,1)\\). These quantile functions are parabolic."
  },
  {
    "objectID": "chpt24-quantile-reg.html#estimation",
    "href": "chpt24-quantile-reg.html#estimation",
    "title": "23  Quantile Regression",
    "section": "23.9 Estimation",
    "text": "23.9 Estimation\nTheorem \\(24.2\\) shows that in the linear quantile regression model the coefficient \\(\\beta_{\\tau}\\) minimizes \\(M(\\beta ; \\tau)=\\) \\(\\mathbb{E}\\left[\\rho_{\\tau}\\left(Y-X^{\\prime} \\beta\\right)\\right]\\), the expected check function loss. The estimator of this function is the sample average\n\\[\nM_{n}(\\beta ; \\tau)=\\frac{1}{n} \\sum_{i=1}^{n} \\rho_{\\tau}\\left(Y_{i}-X_{i}^{\\prime} \\beta\\right) .\n\\]\nSince \\(\\beta_{\\tau}\\) minimizes \\(M(\\beta ; \\tau)\\) which is estimated by \\(M_{n}(\\beta ; \\tau)\\) the m-estimator for \\(\\beta_{\\tau}\\) is the minimizer of \\(M_{n}(\\beta ; \\tau)\\) :\n\\[\n\\widehat{\\beta}_{\\tau}=\\underset{\\beta}{\\operatorname{argmin}} M_{n}(\\beta ; \\tau) .\n\\]\nThis is called the Quantile Regression estimator of \\(\\beta_{\\tau}\\). The coefficient \\(\\widehat{\\beta}_{\\tau}\\) does not have a closed form solution so must be found by numerical minimization. The minimization techniques are identical to those used for median regression; hence typical software packages treat the two together.\nThe quantile regression residuals \\(\\widehat{e}_{i}(\\tau)=Y_{i}-X_{i}^{\\prime} \\widehat{\\beta}_{\\tau}\\) satisfy the approximate property\n\\[\n\\frac{1}{n} \\sum_{i=1}^{n} X_{i} \\psi_{\\tau}\\left(\\widehat{e}_{i}(\\tau)\\right) \\simeq 0 .\n\\]\nAs for LAD, (24.17) holds exactly if \\(\\widehat{e}_{i}(\\tau) \\neq 0\\) for all \\(i\\), which occurs with high probability if \\(Y\\) is continuously distributed.\nIn Stata, quantile regression is implemented by qreg. In R, quantile regression is implemented by \\(\\mathrm{rq}\\) in the quantreg package."
  },
  {
    "objectID": "chpt24-quantile-reg.html#asymptotic-distribution",
    "href": "chpt24-quantile-reg.html#asymptotic-distribution",
    "title": "23  Quantile Regression",
    "section": "23.10 Asymptotic Distribution",
    "text": "23.10 Asymptotic Distribution\nWe first provide conditions for consistent estimation. Let \\(\\beta_{\\tau}\\) be defined in (24.16), \\(e=Y-X^{\\prime} \\beta_{\\tau}\\), and \\(f_{\\tau}(e \\mid x)\\) denote the conditional density of \\(e\\) given \\(X=x\\).\nTheorem 24.3 Consistency of Quantile Regression Estimator Assume that \\(\\left(Y_{i}, X_{i}\\right)\\) are i.i.d., \\(\\mathbb{E}|Y|<\\infty, \\mathbb{E}\\left[\\|X\\|^{2}\\right]<\\infty, f_{\\tau}(e \\mid x)\\) exists and satisfies \\(f_{\\tau}(e \\mid x) \\leq D<\\infty\\), and the parameter space for \\(\\beta\\) is compact. For any \\(\\tau \\in(0,1)\\) such that\n\\[\n\\boldsymbol{Q}_{\\tau} \\stackrel{\\text { def }}{=} \\mathbb{E}\\left[X X^{\\prime} f_{\\tau}(0 \\mid X)\\right]>0\n\\]\nthen \\(\\widehat{\\beta}_{\\tau} \\underset{p}{\\rightarrow} \\beta_{\\tau}\\) as \\(n \\rightarrow \\infty\\)\nThe proof is provided in Section \\(24.16\\).\nTheorem \\(24.3\\) shows that the quantile regression estimator is consistent for the best linear quantile predictor coefficient under broad assumptions.\nA technical condition is (24.18) which is used to establish uniqueness of the coefficient \\(\\beta_{\\tau}\\). One sufficient condition for (24.18) occurs when the conditional density \\(f_{\\tau}(e \\mid x)\\) doesn’t depend on \\(x\\) at \\(e=0\\), thus \\(f_{\\tau}(0 \\mid x)=f_{\\tau}(e)\\) and\n\\[\n\\boldsymbol{Q}_{\\tau}=\\mathbb{E}\\left[X X^{\\prime}\\right] f_{\\tau}(0) .\n\\]\nIn this context, (24.18) holds if \\(\\mathbb{E}\\left[X X^{\\prime}\\right]>0\\) and \\(f_{\\tau}(0)>0\\). The assumption that \\(f_{\\tau}(e \\mid x)\\) doesn’t depend on \\(x\\) at \\(e=0\\) (we call this quantile independence) is a traditional assumption in the early median regression/quantile regression literature, but does not make sense outside the narrow context where \\(e\\) is independent of \\(X\\). Thus we should avoid (24.19) whenever possible, and if not view it as a convenient simplification rather than a literal truth. The assumption that \\(f_{\\tau}(0)>0\\) means that there are a non-trivial set of observations for which the error \\(e\\) is near zero, or equivalently for which \\(Y\\) is close to \\(X^{\\prime} \\beta_{\\tau}\\). These are the observations which provide the decisive information to pin down \\(\\beta_{\\tau}\\).\nA weaker way to obtain a sufficient condition for (24.18) is to assume that for some bounded set \\(\\mathscr{X}\\) in the support of \\(X\\), that (a) \\(\\mathbb{E}\\left[X X^{\\prime} \\mid X \\in \\mathscr{X}\\right]>0\\) and (b) \\(f_{\\tau}(0 \\mid x) \\geq c>0\\) for \\(x \\in \\mathscr{X}\\). This is the same as stating that if we truncate the regressor \\(X\\) to a bounded set that the design matrix is full rank and the conditional density of the error at zero is bounded away from zero. These conditions are rather abstract but mild.\nWe now provide the asymptotic distribution.\nTheorem 24.4 Asymptotic Distribution of Quantile Regression Estimator In addition to the assumptions of Theorem 24.3, assume that \\(f_{\\tau}(e \\mid x)\\) is continuous in \\(e\\), and \\(\\beta_{\\tau}\\) is in the interior of the parameter space. Then as \\(n \\rightarrow \\infty\\)\n\\[\n\\sqrt{n}\\left(\\widehat{\\beta}_{\\tau}-\\beta_{\\tau}\\right) \\underset{d}{\\longrightarrow} \\mathrm{N}\\left(0, \\boldsymbol{V}_{\\tau}\\right)\n\\]\nwhere \\(\\boldsymbol{V}_{\\tau}=\\boldsymbol{Q}_{\\tau}^{-1} \\Omega_{\\tau} \\boldsymbol{Q}_{\\tau}^{-1}\\) and \\(\\Omega_{\\tau}=\\mathbb{E}\\left[X X^{\\prime} \\psi_{\\tau}^{2}\\right]\\) for \\(\\psi_{\\tau}=\\tau-\\mathbb{1}\\left\\{Y<X^{\\prime} \\beta_{\\tau}\\right\\}\\). The proof is provided in Section \\(24.16\\).\nTheorem \\(24.4\\) shows that the quantile regression estimator is asymptotically normal with a sandwich asymptotic covariance matrix. Asymptotic normality does not rely on correct model specification, and therefore applies broadly for practical applications where linear models are approximations rather than literal truths. The proof of the asymptotic distribution relies on the theory for general m-estimators (Theorem 22.4). Theorem \\(24.4\\) includes the least absolute deviations estimator as the special case \\(\\tau=0.5\\).\nThe asymptotic covariance matrix in Theorem \\(24.4\\) simplifies under correct specification. If \\(\\mathbb{Q}_{\\tau}[Y \\mid X]=\\) \\(X^{\\prime} \\beta_{\\tau}\\) then \\(\\mathbb{E}\\left[\\psi_{\\tau}^{2} \\mid X\\right]=\\tau(1-\\tau)\\). It follows that \\(\\Omega_{\\tau}=\\tau(1-\\tau) \\boldsymbol{Q}\\) where \\(\\boldsymbol{Q}=\\mathbb{E}\\left[X X^{\\prime}\\right]\\).\nCombined with (24.19) we have three levels of asymptotic covariance matrices.\n\nGeneral: \\(\\boldsymbol{V}_{\\tau}=\\boldsymbol{Q}_{\\tau}^{-1} \\Omega_{\\tau} \\boldsymbol{Q}_{\\tau}^{-1}\\)\nCorrect Specification: \\(\\boldsymbol{V}_{\\tau}^{c}=\\tau(1-\\tau) \\boldsymbol{Q}_{\\tau}^{-1} \\boldsymbol{Q} \\boldsymbol{Q}_{\\tau}^{-1}\\)\nQuantile Independence: \\(\\boldsymbol{V}_{\\tau}^{0}=\\frac{\\tau(1-\\tau)}{f_{\\tau}(0)^{2}} \\boldsymbol{Q}^{-1}\\)\n\nThe quantile independence case \\(\\boldsymbol{V}_{\\tau}^{0}\\) is similar to the homoskedastic least squares covariance matrix. While \\(\\boldsymbol{V}_{\\tau}\\) is the generally appropriate covariance matrix formula, the simplified formula \\(\\boldsymbol{V}_{\\tau}^{0}\\) is easier to interpret to obtain intuition about the precision of the quantile regression estimator. Similarly to the least squares estimator the covariance matrix is a scale multiple of \\(\\left(\\mathbb{E}\\left[X X^{\\prime}\\right]\\right)^{-1}\\). Thus it inherits the related properties of the least-squares estimator: \\(\\widehat{\\beta}_{\\tau}\\) is more efficient when \\(X\\) has greater variance and is less collinear. The covariance matrix \\(\\boldsymbol{V}_{\\tau}^{0}\\) is inversely proportional to \\(f_{\\tau}(0)^{2}\\). Thus \\(\\widehat{\\beta}_{\\tau}\\) is more efficient when the density is high at 0 which means that there are many observations near the \\(\\tau^{t h}\\) quantile of the conditional distribution. If there are few observations near the \\(\\tau^{\\text {th }}\\) quantile then \\(f_{\\tau}(0)\\) will be small and \\(\\boldsymbol{V}_{\\tau}^{0}\\) large. We can also express this relationship in terms of the standard deviation \\(\\sigma\\) of \\(e\\). Let \\(u=e / \\sigma\\) be the error scaled to have a unit variance, which has density \\(g_{\\tau}(x)=\\sigma f_{\\tau}(\\sigma u)\\). Then \\(\\boldsymbol{V}_{\\tau}^{0}=\\frac{\\tau(1-\\tau)}{g_{\\tau}(0)^{2}} \\sigma^{2}\\left(\\mathbb{E}\\left[X X^{\\prime}\\right]\\right)^{-1}\\), which is a scale of the homoskedastic least squares covariance matrix."
  },
  {
    "objectID": "chpt24-quantile-reg.html#covariance-matrix-estimation",
    "href": "chpt24-quantile-reg.html#covariance-matrix-estimation",
    "title": "23  Quantile Regression",
    "section": "23.11 Covariance Matrix Estimation",
    "text": "23.11 Covariance Matrix Estimation\nThere are multiple methods to estimate the asymptotic covariance matrix \\(\\boldsymbol{V}_{\\tau}\\). The easiest is based on the quantile independence assumption, leading to\n\\[\n\\begin{aligned}\n\\widehat{\\boldsymbol{V}}_{\\tau}^{0} &=\\tau(1-\\tau) \\widehat{f}_{\\tau}(0)^{-2} \\widehat{\\boldsymbol{Q}}^{-1} \\\\\n\\widehat{\\boldsymbol{Q}} &=\\frac{1}{n} \\sum_{i=1}^{n} X_{i} X_{i}^{\\prime} .\n\\end{aligned}\n\\]\nwhere \\(\\widehat{f}_{\\tau}(0)^{-2}\\) is a nonparametric estimator of \\(f_{\\tau}(0)^{-2}\\). For the latter there are several proposed methods. One uses a difference in the distribution function of \\(Y\\). A second uses a nonparametric estimator of \\(f_{\\tau}(0)\\).\nAn estimator of \\(\\boldsymbol{V}_{\\tau}^{c}\\) assuming correct specification is\n\\[\n\\widehat{\\boldsymbol{V}}_{\\tau}^{c}=\\tau(1-\\tau) \\widehat{\\boldsymbol{Q}}_{\\tau}^{-1} \\widehat{\\boldsymbol{Q}} \\widehat{\\boldsymbol{Q}}_{\\tau}^{-1}\n\\]\nwhere \\(\\widehat{\\boldsymbol{Q}}_{\\tau}\\) is a nonparametric estimator of \\(\\boldsymbol{Q}_{\\tau}\\). A feasible choice given a bandwidth \\(h\\) is\n\\[\n\\widehat{\\boldsymbol{Q}}_{\\tau}=\\frac{1}{2 n h} \\sum_{i=1}^{n} X_{i} X_{i}^{\\prime} \\mathbb{1}\\left\\{\\left|\\widehat{e}_{i}\\right|<h\\right\\} .\n\\]\nAn estimator of \\(\\boldsymbol{V}_{\\tau}\\) allowing misspecification is\n\\[\n\\begin{aligned}\n\\widehat{\\boldsymbol{V}}_{\\tau} &=\\widehat{\\boldsymbol{Q}}_{\\tau}^{-1} \\widehat{\\Omega}_{\\tau} \\widehat{\\boldsymbol{Q}}_{\\tau}^{-1} \\\\\n\\widehat{\\Omega}_{\\tau} &=\\frac{1}{h} \\sum_{i=1}^{n} X_{i} X_{i}^{\\prime} \\widehat{\\psi}_{i \\tau}^{2} \\\\\n\\widehat{\\psi}_{i \\tau} &=\\tau-\\mathbb{1}\\left\\{Y_{i}<X_{i}^{\\prime} \\widehat{\\beta}_{\\tau}\\right\\} .\n\\end{aligned}\n\\]\nOf the three covariance matrix methods introduced above \\(\\left(\\widehat{\\boldsymbol{V}}_{\\tau}^{0}, \\widehat{\\boldsymbol{V}}_{\\tau}^{c}\\right.\\), and \\(\\left.\\widehat{\\boldsymbol{V}}_{\\tau}\\right)\\) the classical estimator \\(\\widehat{\\boldsymbol{V}}_{\\tau}^{0}\\) should be avoided for the same reasons why we avoid classical homoskedastic covariance matrix estimators for least squares estimation. Of the two robust estimators the better choice is \\(\\widehat{\\boldsymbol{V}}_{\\tau}\\) (because it does not require correct specification) but unfortunately it is not programmed in standard packages. This means that in practice the estimator \\(\\widehat{V}_{\\tau}^{c}\\) is recommended.\nThe most common method for estimation of quantile regression covariance matrices, standard errors, and confidence intervals is the bootstrap. The conventional nonparametric bootstrap is appropriate for the general model allowing for misspecification, and the bootstrap variance is an estimator for \\(\\widehat{\\boldsymbol{V}}_{\\tau}\\). As we have learned in our study of bootstrap methods, it is generally advised to use a large number \\(B\\) of bootstrap replications (at least 1000 , with 10,000 preferred). This is somewhat computationally costly in large samples but this should not be a barrier to implementation as the full bootstrap calculation only needs to be done for the final calculation. Also, as we have learned, for confidence intervals percentile-based intervals are greatly preferred over the normal-based intervals (which use bootstrap standard errors multiplied by normal quantiles). I recommend the BC percentile intervals. This requires changing the default settings in common programs such as Stata.\nIn Stata, quantile regression is implemented using qreg. The default standard errors are \\(\\widehat{\\boldsymbol{V}}_{\\tau}^{0}\\). Use vce(robust) for \\(\\widehat{\\boldsymbol{V}}_{\\tau}^{c}\\). The covariance matrix estimator \\(\\widehat{\\boldsymbol{V}}_{\\tau}\\) is not implemented. For bootstrap standard errors and confidence intervals use bootstrap, reps (#): qreg y x. The bootstrap command followed by estat bootstrap produces BC percentile confidence intervals.\nIn R, quantile regression is implemented by the function rq in the quantreg package. The default standard errors are \\(\\widehat{\\boldsymbol{V}}_{\\tau}^{c}\\). The covariance matrix estimator \\(\\widehat{\\boldsymbol{V}}_{\\tau}\\) is not implemented. For bootstrap standard errors one method is to use the option se=“boot” with the summary command. At present, the quantreg package does not include bootstrap percentile confidence intervals."
  },
  {
    "objectID": "chpt24-quantile-reg.html#clustered-dependence",
    "href": "chpt24-quantile-reg.html#clustered-dependence",
    "title": "23  Quantile Regression",
    "section": "23.12 Clustered Dependence",
    "text": "23.12 Clustered Dependence\nUnder clustered dependence the asymptotic covariance matrix changes. In the formula \\(\\boldsymbol{V}_{\\tau}=\\boldsymbol{Q}_{\\tau}^{-1} \\Omega_{\\tau} \\boldsymbol{Q}_{\\tau}^{-1}\\) the matrix \\(\\boldsymbol{Q}_{\\tau}\\) is unaltered but \\(\\Omega_{\\tau}\\) changes to\n\\[\n\\Omega_{\\tau}^{\\text {cluster }}=\\lim _{n \\rightarrow \\infty} \\frac{1}{n} \\sum_{g=1}^{G} \\mathbb{E}\\left[\\left(\\sum_{\\ell=1}^{n_{g}} X_{\\ell g} \\psi_{\\ell g \\tau}\\right)\\left(\\sum_{\\ell=1}^{n_{g}} X_{\\ell g} \\psi_{\\ell g \\tau}\\right)^{\\prime}\\right] .\n\\]\nThis can be estimated as\n\\[\n\\widehat{\\Omega}_{\\tau}^{\\text {cluster }}=\\frac{1}{n} \\sum_{g=1}^{G}\\left[\\left(\\sum_{\\ell=1}^{n_{g}} X_{\\ell g} \\widehat{\\psi}_{\\ell g \\tau}\\right)\\left(\\sum_{\\ell=1}^{n_{g}} X_{\\ell g} \\widehat{\\psi}_{\\ell g \\tau}\\right)^{\\prime}\\right] .\n\\]\nThis leads to the cluster-robust asymptotic covariance matrix estimator \\(\\widehat{\\boldsymbol{V}}_{\\tau}^{\\text {cluster }}=\\widehat{\\boldsymbol{Q}}_{\\tau}^{-1} \\widehat{\\Omega}_{\\tau}^{\\text {cluster }} \\widehat{\\boldsymbol{Q}}_{\\tau}^{-1}\\).\nThe cluster-robust estimator \\(\\widehat{\\boldsymbol{V}}_{\\tau}^{\\text {cluster }}\\) is not implemented in Stata nor in the R quantreg package. Instead, the clustered bootstrap (sampling clusters with replacement) is recommended. In Stata, the clustered bootstrap can be accomplished by: bootstrap, reps(#) cluster(id): qreg y x, followed by estat bootstrap.\nIn \\(\\mathrm{R}\\), the clustered bootstrap is included as an option in the quantreg package for calculation of standard errors.\nWe illustrate the application of clustered quantile regression using the Duflo, Dupas, and Kremer (2011) school tracking application. (See Section 4.21.) Recall, the question was whether or not tracking (separating students into classrooms based on an initial test) influenced average end-of-year scores. We repeat the analysis using quantile regression. Parameter estimates and bootstrap standard errors (calculated by clustered bootstrap using 10,000 replications, clustered by school) are reported in Table \\(24.2\\).\nThe results are mixed. The point estimates suggest that there is a stronger effect of tracking at higher quantiles than lower quantiles. This is consistent with the premise that tracking affects students heterogeneously, has no negative effects, and has the greatest impact on the upper end. The standard errors and confidence intervals, however, are also larger for the higher quantiles, such that the quantile regression coefficients at high quantiles are imprecisely estimated. Using the \\(t\\) test, two of the five slope coefficients are (borderline) statistically significant at the 5% level and one at the \\(10 %\\) level. In apparant contradiction, all five of the \\(95 %\\) BC percentile intervals include 0. Overall the evidence that tracking affects student performance is weak.\nTable 24.2: Quantile Regressions of Student Testscores on Tracking\n\n\n\n\n\n\n\n\n\n\n\n\n\\(\\tau=0.1\\)\n\\(\\tau=0.3\\)\n\\(\\tau=0.5\\)\n\\(\\tau=0.7\\)\n\\(\\tau=0.9\\)\n\n\n\n\ntracking\n\\(0.069\\)\n\\(0.136\\)\n\\(0.125\\)\n\\(0.185\\)\n\\(0.151\\)\n\n\nbootstrap standard error\n\\((0.045)\\)\n\\((0.069)\\)\n\\((0.074)\\)\n\\((0.127)\\)\n\\((0.126)\\)\n\n\n95% confidence interval\n\\([-0.02, .15]\\)\n\\([-0.01, .27]\\)\n\\([-0.01, .28]\\)\n\\([-0.06, .44]\\)\n\\([-0.11, .40]\\)"
  },
  {
    "objectID": "chpt24-quantile-reg.html#quantile-crossings",
    "href": "chpt24-quantile-reg.html#quantile-crossings",
    "title": "23  Quantile Regression",
    "section": "23.13 Quantile Crossings",
    "text": "23.13 Quantile Crossings\nA property of the quantile regression functions \\(q_{\\tau}(x)\\) is that they are monotonically increasing in \\(\\tau\\). This means that quantile functions for different quantiles, e.g. \\(q_{\\tau_{1}}(x)\\) and \\(q_{\\tau_{2}}(x)\\) for \\(\\tau_{1} \\neq \\tau_{2}\\), cannot cross each other. However a property of linear functions \\(x^{\\prime} \\beta\\) with differing slopes is that they will necessarily cross if the support for \\(X\\) is sufficiently large. This is a potential problem in applications as practical uses of estimated quantile functions may require monotonicity in \\(\\tau\\) (for example if they are to be inverted to obtain a conditional distribution function).\nThis is only a problem in practical applications if estimated quantile functions actually cross. If they do not this issue can be ignored. However when estimated quantile regression functions cross one another it can be prudent to address the issue.\nTo illustrate examine Figure 24.5(a). This shows estimated linear quantile regressions of wage on education in the full cps09mar data set. These are linear projection approximations to the plots in Figure 24.2(a). Since the actual quantile regression functions are convex the estimated linear models cross one another at low education levels. This is the quantile regression crossing phenomenon.\nWhen quantile regressions cross one another there are several possible remedies.\nFirst, you could re-specify the model. In the example of Figure 24.5(a) the problem arises in part because the true quantile regression functions are convex and poorly approximated by linear functions. In this example we know that an improved approximation is obtained through a logarithmic transformation for wages. After a log transformation the quantile regression functions are much better approximated by linearity. Indeed, such estimates (obtained by quantile regression of log wages on education, and then\n\n\nLinear Model\n\n\n\nLogarithmic Model\n\nFigure 24.5: Quantile Crossings\napplying the exponential transformation to return to the original units) are displayed in Figure \\(24.5\\) (b). These functions are smooth approximations and are strictly monotonic in \\(\\tau\\). Problem solved.\nWhile the logarithmic/exponential transformation works well for a wage regression, it is not a generic solution. If the underlying quantile regressions are non-linear in \\(X\\), an improved approximation (and possible elimination of the quantile crossing) may be obtained by a nonlinear or simple series approximation. A visual examination of Figure 24.2(a) suggests that the functions may be piecewise linear with a kink at 11 years of education. This suggests a linear spline with a single knot at \\(x=11\\). The estimates from fitting this model (not displayed) are strictly monotonic in \\(\\tau\\). Problem solved.\nA second approach is to reassess the empirical task. Examining Figure 24.5(a) we see that the crossing phenomenon occurs at very low levels of education (4 years) for which there are very few observations. This may not be viewed as an empirical interesting region. A solution is to truncate the data to eliminate observations with low education levels.\nA third approach is to constrain the estimated functions to satisfy monotonicity. Examine Figure 24.5(a). The five regression functions are increasing with increasing slopes and the support for \\(X\\) is \\([0,20]\\) so it is necessary and sufficient to constrain the five intercepts to be monotonically ranked. This can be imposed on this example by sequentially imposing cross-equation equality constraints. The R function rq has an option to impose parameter contraints. This approach may be feasible if the quantile crossing problem is mild.\nA final approach is rearrangement. For each \\(x\\) take the five estimated quantile regression functions as displayed in Figure 24.5(a) and rearrange the estimates so that they satisfy the monotonicity requirement. This does not alter the coefficient estimates, only the estimated quantile regressions. This approach is flexible and works in general contexts without the need for model re-specification. For details see Chernozhukov, Fernandez-Val, and Galichon (2010). The R package quantreg includes the option rearrange to implement their procedure.\nOf these four approaches, my recommendation is to start with a careful and thoughtful re-specification of the model."
  },
  {
    "objectID": "chpt24-quantile-reg.html#quantile-causal-effects",
    "href": "chpt24-quantile-reg.html#quantile-causal-effects",
    "title": "23  Quantile Regression",
    "section": "23.14 Quantile Causal Effects",
    "text": "23.14 Quantile Causal Effects\nOne question which frequently arises in the study of quantile regression is “Can we interpret the quantile regression causally?” We can partially answer this question in the treatment response framework by providing conditions under which the quantile regression derivatives equal quantile treatment effects.\nRecall that the treatment response model is \\(Y=h(D, X, U)\\) where \\(Y\\) is the outcome, \\(D\\) is the treatment variable, \\(X\\) are controls, and \\(U\\) is an unobserved structural random error. For simplicity take the case that \\(D\\) is binary. For concreteness let \\(Y\\) be wage, \\(D\\) college attendence, and \\(U\\) unobserved ability.\nIn this framework, the causal effect of \\(D\\) on \\(Y\\) is\n\\[\nC(X, U)=h(1, X, U)-h(0, X, U) .\n\\]\nIn general this is heterogeneous. While the average causal effect is the expectation of this random variable, the quantile treatment effect is its \\(\\tau^{t h}\\) conditional quantile\n\\[\nQ_{\\tau}(x)=\\mathbb{Q}_{\\tau}[C(X, U) \\mid X=x] .\n\\]\nIn Section \\(2.30\\) we presented an example of a population of Jennifers and Georges who had differential wage effects from college attendence. In this example the unobserved effect \\(U\\) is a person’s type (Jennifer or George). The quantile treatment effect \\(Q_{\\tau}\\) traces out the distribution of the causal effect of college attendence and is therefore more informative than the average treatment effect alone.\nFrom observational data we can estimate the quantile regression function\n\\[\nq_{\\tau}(d, x)=\\mathbb{Q}_{\\tau}[Y \\mid D=d, X=x]=\\mathbb{Q}_{\\tau}[h(D, X, U) \\mid D=d, X=x]\n\\]\nand its implied effect of \\(D\\) on \\(Y\\) :\n\\[\nD_{\\tau}(x)=q_{\\tau}(1, x)-q_{\\tau}(0, x) .\n\\]\nThe question is: Under what condition does \\(D_{\\tau}=Q_{\\tau}\\) ? That is, when does quantile regression measure the causal effect of \\(D\\) on \\(Y\\) ?\nAssumption 24.1 Conditions for Quantile Causal Effect\n\nThe error \\(U\\) is real valued.\nThe causal effect \\(C(x, u)\\) is monotonically increasing in \\(u\\).\nThe treatment response \\(h(D, X, u)\\) is monotonically increasing in \\(u\\).\nConditional on \\(X\\) the random variables \\(D\\) and \\(U\\) are independent.\n\nAssumption 24.1.1 excludes multi-dimensional unobserved heterogeneity. Assumptions \\(24.1 .2\\) and 24.1.3 are known as monotonicity conditions. A single monotonicity assumption is not restrictive (it is similar to a normalization) but the two conditions together are a substantive restriction. Take, for example, the case of the impact of college attendence on wages. Assumption 24.1.2 requires that the wage gain from attending college is increasing in latent ability \\(U\\) (given \\(X\\) ). Assumption \\(24.1 .3\\) further requires that wages are increasing in latent ability \\(U\\) whether or not an individual attends college. In our Jennifer and George example these assumptions require that Jennifer receives a higher wage than George if they both are high school graduates, if they are both college graduates, and that Jennifer’s gain from attending college exceeds George’s gain. These conditions were satisfied in the example of Section \\(2.30\\) but with a tweak we can change the model so that one of the monotonicity conditions is violated.\nAssumption 24.1.4 is the traditional conditional independence assumption. This assumption is critical for the causal effect interpretation. The idea is that by conditioning on a sufficiently rich set of variables \\(X\\) any endogeneity between \\(D\\) and \\(U\\) has been eliminated.\nTheorem 24.5 Quantile Causal Effect If Assumption \\(24.1\\) holds then \\(D_{\\tau}(x)=\\) \\(Q_{\\tau}(x)\\), the quantile regression derivative equals the quantile treatment effect.\nThe proof is in Section 24.16.\nTheorem \\(24.5\\) provides conditions under which quantile regression is a causal model. Under the conditional independence and monotonicity assumptions the quantile regression coefficients are the marginal causal effects of the treatment variable \\(D\\) upon the distribution of \\(Y\\). The coefficients are not the marginal causal effects for specific individuals, rather they are the causal effect for the distribution. Theorem \\(24.5\\) shows that under suitable assumptions we can learn more than just the average treatment effect - we can learn the distribution of treatment effects."
  },
  {
    "objectID": "chpt24-quantile-reg.html#random-coefficient-representation",
    "href": "chpt24-quantile-reg.html#random-coefficient-representation",
    "title": "23  Quantile Regression",
    "section": "23.15 Random Coefficient Representation",
    "text": "23.15 Random Coefficient Representation\nFor some theoretical purposes it is convenient to write the quantile regression model using a random coefficient representation. This also provides an alternative interpretation of the coefficients.\nRecall that when \\(Y\\) has a continuous and invertible distribution function \\(F(y)\\) the probability integral transformation is \\(U=F(Y) \\sim U[0,1]\\). As the inverse of the distribution function is the quantile function, this implies that we can write \\(Y=q_{U}\\), the quantile function evaluated at the random variable \\(U\\). The intuition is that \\(U\\) is the “relative rank” of \\(Y\\).\nSimilarly when the conditional distribution \\(F(y \\mid x)\\) of \\(Y\\) given \\(X\\) is invertible, the probability integral transformation is \\(U=F(Y \\mid X) \\sim U[0,1]\\) which is independent of \\(X\\). Here, \\(U\\) is the relative rank of \\(Y\\) within the conditional distribution. Inverting, we obtain \\(Y=q_{U}(X)\\). There is no additional error term \\(e\\) as the randomness is captured by \\(U\\). The equation \\(Y=q_{U}(X)\\) is a representation of the conditional distribution of \\(Y\\) given \\(X\\), not a structural model. However it does imply a mechanism by which we can generate \\(Y\\). First, draw \\(U \\sim U[0,1]\\). Second, draw \\(X\\) from its marginal distibution. Third, set \\(Y=q_{U}(X)\\).\nIf we interpret \\(Y=q_{U}(X)\\) as a structural model (that is, take \\(U\\) as a structural unobservable variable, not merely a derivation based on the probability integral transformation) then we can view \\(U\\) as an individual’s latent relative rank which is invariant to \\(X\\). Each person is identified with a specific \\(U=\\tau\\). In this framework the quantile slope (the derivative of the quantile regression) is the quantile causal effect of \\(X\\) on \\(Y\\). This representation satisfies the conditions of Theorem \\(24.5\\) because \\(U\\) is independent of \\(X\\).\nIn the linear quantile regression model \\(\\mathbb{Q}_{\\tau}[Y \\mid X]=X^{\\prime} \\beta_{\\tau}\\), the random coefficient \\({ }^{3}\\) representation is \\(Y=X^{\\prime} \\beta_{U}\\)\n\\({ }^{3}\\) The coefficients depends on \\(U\\) so are random, but the model is different from the random coefficient model where each individual’s coefficient is a random vector."
  },
  {
    "objectID": "chpt24-quantile-reg.html#nonparametric-quantile-regression",
    "href": "chpt24-quantile-reg.html#nonparametric-quantile-regression",
    "title": "23  Quantile Regression",
    "section": "23.16 Nonparametric Quantile Regression",
    "text": "23.16 Nonparametric Quantile Regression\nAs emphasized in Section 24.10, quantile regression functions are undoubtedly nonlinear with unknown functional form and hence nonparametric. Quantile regression functions may be estimated using standard nonparametric methods. This is a potentially large subject. For brevity we briefly discuss series methods which have the advantage that they are easily implemented with conventional software.\nThe nonparametric quantile regression model is\n\\[\n\\begin{aligned}\nY &=q_{\\tau}(X)+e \\\\\n\\mathbb{Q}_{\\tau}[e \\mid X] &=0 .\n\\end{aligned}\n\\]\nThe function \\(q_{\\tau}(x)\\) can be approximated by a series regression as described in Chapter 20. For example, a polynomial approximation is\n\\[\n\\begin{aligned}\nY &=\\beta_{0}+\\beta_{1} X+\\beta_{2} X^{2}+\\cdots+\\beta_{K} X^{K}+e_{K} \\\\\n\\mathbb{Q}_{\\tau}\\left[e_{K} \\mid X\\right] & \\simeq 0 .\n\\end{aligned}\n\\]\nA spline approximation is defined similarly.\nFor any \\(K\\) the coefficients and regression function \\(q_{\\tau}(x)\\) can be estimated by quantile regression. As in series regression the model order \\(K\\) should be selected to trade off flexibility (bias reduction) and parsimony (variance reduction). Asymptotic theory requires that \\(K \\rightarrow \\infty\\) as \\(n \\rightarrow \\infty\\) but at a slower rate.\nAn important practical question is how to select \\(K\\) in a given application. Unfortunately, standard information criterion (such as the AIC) do not apply for quantile regression and it is unclear if crossvalidation is an appropriate model selection technique. Undoubtedly these questions are an important topic for future study.\nTo illustrate we revisit the nonparametric polynomial estimates of the experience profile for collegeeducated women earlier displayed in Figure 20.1. We estimate \\({ }^{4} \\log\\) wage quantile regressions on a \\(5^{\\text {th }}\\) order polynomial in experience and display the estimates in Figure 24.6. There are two notable features. First, the \\(\\tau=0.1\\) quantile function peaks at a low level of experience (about 10 years) and then declines substantially with experience. This is likely an indicator of the wage-path of women on the low end of the pay scale. Second, even though this is in a logarithmic scale the gaps betwen the quantile functions substantially widen with experience. This means that heterogeneity in wages increases more than proportionately as experience increases."
  },
  {
    "objectID": "chpt24-quantile-reg.html#panel-data",
    "href": "chpt24-quantile-reg.html#panel-data",
    "title": "23  Quantile Regression",
    "section": "23.17 Panel Data",
    "text": "23.17 Panel Data\nGiven a panel data structure \\(\\left\\{Y_{i t}, X_{i t}\\right\\}\\) it is natural to consider a panel data quantile regression estimator. A linear model with an individual effect \\(\\alpha_{i \\tau}\\) is\n\\[\n\\mathbb{Q}_{\\tau}\\left[Y_{i t} \\mid X_{i t}, \\alpha_{i}\\right]=X_{i t}^{\\prime} \\beta_{\\tau}+\\alpha_{i \\tau} .\n\\]\nIt seems natural to consider estimation by one of our standard methods: (1) Remove the individual effect by the within transformation; (2) Remove the invidual effect by first differencing; (3) Estimate a full quantile regression model using the dummy variable representation. However, all of these methods fail. The reason why methods (1) and (2) fail are the same: The quantile operator \\(\\mathbb{Q}_{\\tau}\\) is not a linear operator. The within transformation of \\(\\mathbb{Q}_{\\tau}\\left[Y_{i t} \\mid X_{i t}, \\alpha_{i \\tau}\\right]\\) does not equal \\(\\mathbb{Q}_{\\tau}\\left[\\dot{Y}_{i t} \\mid X_{i t}, \\alpha_{i \\tau}\\right]\\), and similarly \\(\\Delta \\mathbb{Q}_{\\tau}\\left[Y_{i t} \\mid X_{i t}, \\alpha_{i \\tau}\\right] \\neq \\mathbb{Q}_{\\tau}\\left[\\Delta Y_{i t} \\mid X_{i t}, \\alpha_{i \\tau}\\right]\\). The reason why (3) fails is the incidental parameters problem. A\n\\({ }^{4}\\) The sample is the \\(n=5199\\) observations of women with a college degree (16 years of education).\n\nFigure 24.6: Log Wage Quantile Regressions\ndummy variable model has the number of parameters proportional to sample size and in this context nonlinear estimators (including quantile regression) are inconsistent.\nThere have been several proposals to deal with this issue but none are particularly satisfactory. We present here a method due to Canay (2011) which has the advantage of simplicity and wide applicability. The substantive assumption is that the individual effect is common across quantiles: \\(\\alpha_{i \\tau}=\\alpha_{i}\\). Thus \\(\\alpha_{i}\\) shifts the quantile regressions up and down uniformly. This is a sensible assumption when \\(\\alpha_{i}\\) represents omitted time-invariant variables with coefficients which do not vary across quantiles.\nGiven this assumption we can write the quantile regression model as\n\\[\nY_{i t}=X_{i t}^{\\prime} \\beta(\\tau)+\\alpha_{i}+e_{i t} .\n\\]\nWe can also use the random coefficient representation of Section \\(24.12\\) to write\n\\[\nY_{i t}=X_{i t}^{\\prime} \\beta\\left(U_{i \\tau}\\right)+\\alpha_{i}\n\\]\nwhere \\(U_{i \\tau} \\sim U[0,1]\\) is independent of \\(\\left(X_{i t}, \\alpha_{i}\\right)\\). Taking conditional expectations we obtain the model\n\\[\nY_{i t}=X_{i t}^{\\prime} \\theta+\\alpha_{i}+u_{i t}\n\\]\nwhere \\(\\theta=\\mathbb{E}\\left[\\beta\\left(U_{i \\tau}\\right)\\right]\\) and \\(u_{i t}\\) is conditionally mean zero. The coefficient \\(\\theta\\) is a weighted average of the quantile regression coefficients \\(\\beta(\\tau)\\).\nCanay’s estimator takes the following steps. 1. Estimate \\(\\alpha_{i}\\) by fixed effects \\(\\widehat{\\alpha}_{i}\\) as in (17.51). [Estimate \\(\\theta\\) by the within estimator \\(\\widehat{\\theta}\\) and \\(\\alpha_{i}\\) by taking averages of \\(Y_{i t}-X_{i t}^{\\prime} \\widehat{\\theta}\\) for each individual.]\n 1. Estimate \\(\\beta(\\tau)\\) by quantile regression of \\(Y_{i t}-\\widehat{\\alpha}_{i}\\) on \\(X_{i t}\\).\nThe key to Canay’s estimator is that the assumption that the fixed effect \\(\\alpha_{i}\\) does not vary across the quantiles \\(\\tau\\), which means that the fixed effects can be estimated by conventional fixed effects. Once eliminated we can apply conventional quantile regression. The primary disadvantage of this approach is that the assumption that \\(\\alpha_{i}\\) does not vary across quantiles is restrictive. In general the topic of panel quantile regression is a potentially important topic for further econometric research."
  },
  {
    "objectID": "chpt24-quantile-reg.html#quantile-regression-1",
    "href": "chpt24-quantile-reg.html#quantile-regression-1",
    "title": "23  Quantile Regression",
    "section": "23.18 Quantile Regression",
    "text": "23.18 Quantile Regression\nAs we studied in Chapter 12, in many structural economic models some regressors are potentially endogenous, meaning jointly dependent with the regression error. This situation equally arises in quantile regression models. A standard method to handle endogenous regressors is instrumental variables regression which relies on a set of instruments \\(Z\\) which satisfy an uncorrelatedness or independence condition. Similar methods can be applied in quantile regression though the techniques are computationally more difficult, the theory less well developed, and applications limited.\nThe model is\n\\[\n\\begin{aligned}\nY &=X^{\\prime} \\beta_{\\tau}+e \\\\\n\\mathbb{Q}_{\\tau}[e \\mid Z] &=0\n\\end{aligned}\n\\]\nwhere \\(X\\) and \\(\\beta_{\\tau}\\) are \\(k \\times 1, Z\\) is \\(\\ell \\times 1\\), amd \\(\\ell \\geq k\\). The difference with the conventional quantile regression model is that the second equation is conditional on \\(Z\\) rather than \\(X\\).\nThe assumption on the error implies that \\(\\mathbb{E}\\left[\\psi_{\\tau}(e) \\mid Z\\right]=0\\). This holds by the same derivation as for the quantile regression model. This is a conditional moment equation. It implies the unconditional moment equation \\({ }^{5} \\mathbb{E}\\left[Z \\psi_{\\tau}(e)\\right]=0\\). Written as a function of the observations and parameters\n\\[\n\\mathbb{E}\\left[Z \\psi_{\\tau}\\left(Y-X^{\\prime} \\beta_{\\tau}\\right)\\right]=0 .\n\\]\nThis is a set of \\(\\ell\\) moment equations for \\(k\\) parameters. A suitable estimation method is GMM. A computational challenge is that the moment condition functions are discontinuous in \\(\\beta_{\\tau}\\) so conventional minimization techniques fail.\nThe method of IV quantile regression was articulated by Chernozhukov and C. Hansen (2005), which should be consulted for further details."
  },
  {
    "objectID": "chpt24-quantile-reg.html#technical-proofs",
    "href": "chpt24-quantile-reg.html#technical-proofs",
    "title": "23  Quantile Regression",
    "section": "23.19 Technical Proofs*",
    "text": "23.19 Technical Proofs*\nProof of Theorem 24.1: Since \\(\\mathbb{P}[Y=m]=0\\),\n\\[\n\\mathbb{E}[\\operatorname{sgn}(Y-m)]=\\mathbb{E}[\\mathbb{1}\\{Y>m\\}]-\\mathbb{E}[\\mathbb{1}\\{Y<m\\}]=\\mathbb{P}[Y>m]-\\mathbb{P}[Y<m]=\\frac{1}{2}-\\frac{1}{2}=0\n\\]\nwhich is (24.2).\n\\({ }^{5}\\) In fact, the assumptions imply \\(\\mathbb{E}\\left[\\phi(Z) \\psi_{\\tau}(e)\\right]=0\\) for any function \\(\\phi\\). We assume that the desired instruments have been selected and are incorporated in the vector \\(Z\\) as denoted. Exchanging integration and differentiation\n\\[\n\\frac{d}{d \\theta} \\mathbb{E}|Y-\\theta|=\\mathbb{E}\\left[\\frac{d}{d \\theta}|Y-\\theta|\\right]=\\mathbb{E}[\\operatorname{sgn}(Y-\\theta)]=0,\n\\]\nthe final equality at \\(\\theta=m\\) by (24.2). This is the first order condition for an optimum. Since \\(\\mathbb{E}[\\operatorname{sgn}(Y-\\theta)]=\\) \\(1-2 \\mathbb{P}[Y<\\theta]\\) is globally decreasing in \\(\\theta\\), the second order condition shows that \\(m\\) is the unique minimizer. This is (24.3).\n(24.4) and (24.5) follow by similar arguments using the conditional distribution. (24.6) follows from (24.5) under the assumption that \\(\\operatorname{med}[Y \\mid X]=X^{\\prime} \\beta\\).\nProof of Theorem 24.2: Since \\(\\mathbb{P}\\left[Y=q_{\\tau}\\right]=0\\),\n\\[\n\\mathbb{E}\\left[\\psi_{\\tau}\\left(Y-q_{\\tau}\\right)\\right]=\\tau-\\mathbb{P}\\left[Y<q_{\\tau}\\right]=0\n\\]\nwhich is (24.11).\nExchanging integration and differentiation\n\\[\n\\frac{d}{d \\theta} \\mathbb{E}\\left[\\rho_{\\tau}(Y-\\theta)\\right]=\\mathbb{E}\\left[\\psi_{\\tau}(Y-\\theta)\\right]=0,\n\\]\nthe final equality at \\(\\theta=q_{\\tau}\\) by (24.11). This is the first order condition for an optimum. Since \\(\\mathbb{E}\\left[\\psi_{\\tau}(Y-\\theta)\\right]=\\) \\(\\tau-\\mathbb{P}[Y<\\theta]\\) is globally decreasing in \\(\\theta\\), the second order condition shows that \\(q_{\\tau}\\) is the unique minimizer. This is (24.12).\n(24.13) and (24.14) follow by similar arguments using the conditional distribution. (24.15) follows from (24.14) under the assumption that \\(\\mathbb{Q}_{\\tau}[Y \\mid X]=X^{\\prime} \\beta\\).\nProof of Theorem 24.3: The quantile regression estimator is an m-estimator, so we appeal to Theorem 22.3, which holds under five conditions. Conditions 1 and 4 are satisfied by assumption, and condition 2 holds because \\(\\rho_{\\tau}\\left(Y-X^{\\prime} \\beta\\right)\\) is continuous in \\(\\beta\\) as \\(\\rho_{\\tau}(u)\\) is a continuous function. For condition 3, observe that \\(\\left|\\rho_{\\tau}\\left(Y-X^{\\prime} \\beta\\right)\\right| \\leq|Y|+\\bar{\\beta}\\|X\\|\\) where \\(\\bar{\\beta}=\\sup _{\\beta \\in B}\\|\\beta\\|\\). The right side has finite expectation under the assumptions.\nFor condition 5 we need to show that \\(\\beta_{\\tau}\\) uniquely minimizes \\(M(\\beta ; \\tau)\\). It is a minimizer by (24.16). It is unique because \\(M(\\beta ; \\tau)\\) is a convex function and\n\\[\n\\frac{\\partial^{2}}{\\partial \\beta \\partial \\beta^{\\prime}} M\\left(\\beta_{\\tau} ; \\tau\\right)=\\mathbb{E}\\left[X X^{\\prime} f_{\\tau}(0 \\mid X)\\right]>0 .\n\\]\nThe inequality holds by assumption; we now establish the equality.\nExchanging integration and differentiation, using \\(\\psi_{\\tau}(x)=\\frac{d}{d x} \\rho_{\\tau}(x)=\\tau-\\mathbb{1}\\{x<0\\}\\), the law of iterated expectations, and the conditional distribution function \\(F_{\\tau}(u \\mid x)=\\mathbb{E}[\\mathbb{1}\\{e<u\\} \\mid X]\\)\n\\[\n\\begin{aligned}\n\\frac{\\partial}{\\partial \\beta} M(\\beta ; \\tau) &=-\\mathbb{E}\\left[X \\psi_{\\tau}\\left(Y-X^{\\prime} \\beta\\right)\\right] \\\\\n&=-\\tau \\mathbb{E}[X]+\\mathbb{E}\\left[X \\mathbb{E}\\left[\\mathbb{1}\\left\\{Y<X^{\\prime}\\left(\\beta-\\beta_{\\tau}\\right)\\right\\} \\mid X\\right]\\right] \\\\\n&=-\\tau \\mathbb{E}[X]+\\mathbb{E}\\left[X F_{\\tau}\\left(X^{\\prime}\\left(\\beta-\\beta_{\\tau}\\right) \\mid X\\right)\\right] .\n\\end{aligned}\n\\]\nHence\n\\[\n\\frac{\\partial^{2}}{\\partial \\beta \\partial \\beta^{\\prime}} M(\\beta ; \\tau)=\\frac{\\partial}{\\partial \\beta^{\\prime}} \\mathbb{E}\\left[X F_{\\tau}\\left(X^{\\prime}\\left(\\beta-\\beta_{\\tau}\\right) \\mid X\\right)\\right]=\\mathbb{E}\\left[X X^{\\prime} f_{\\tau}\\left(X^{\\prime}\\left(\\beta-\\beta_{\\tau}\\right) \\mid X\\right)\\right]\n\\]\nThe right-hand-side of (24.22) is bounded below \\(\\mathbb{E}\\left[X X^{\\prime}\\right] D\\) which has finite elements under the assumptions. (24.22) is also positive semi-definite for all \\(\\beta\\) so \\(M(\\beta ; \\tau)\\) is globally convex. Evaluated at \\(\\beta_{\\tau}\\), (24.22) equals (24.20). This shows that \\(M(\\beta ; \\tau)\\) is strictly convex at the minimum \\(\\beta_{\\tau}\\). Thus the latter is the unique minimizer.\nTogether we have established the five conditions of Theorem \\(22.3\\) as needed.\nProof of Theorem 24.4: Since \\(\\widehat{\\beta}_{\\tau}\\) is an m-estimator with a discontinuous score we verify the conditions of Theorem 22.6, which holds under conditions \\(1,2,3\\), and 5 of Theorem 22.4, \\(\\left\\|X \\psi_{\\tau}\\left(Y-X^{\\prime} \\beta\\right)\\right\\| \\leq G(Y, X)\\) with \\(\\mathbb{E}\\left[G(Y, X)^{2}\\right]<\\infty\\), plus one of the four listed categories.\nIt is useful to observe that because \\(\\psi_{\\tau}(u) \\leq 1\\),\n\\[\n\\left\\|X \\psi_{\\tau}\\left(Y-X^{\\prime} \\beta\\right)\\right\\| \\leq\\|X\\| .\n\\]\nWe verify conditions \\(1,2,3\\), and 5 of Theorem 22.4. Condition 1 holds because (24.23) implies \\(\\mathbb{E}\\left[\\|X\\|^{2} \\psi_{\\tau}^{2}\\right] \\leq \\mathbb{E}\\|X\\|^{2}<\\infty\\). Condition 2 holds by (24.18). Equation (24.22) shows that \\(\\frac{\\partial^{2}}{\\partial \\beta \\partial \\beta^{\\prime}} M(\\beta ; \\tau)\\) is continuous under the assumption that \\(f_{\\tau}(e \\mid x)\\) is continuous in \\(e\\), implying condition 3. Condition 5 holds by assumption.\nThe upper bound (24.23) satisfies \\(\\mathbb{E}\\|X\\|^{2}<\\infty\\), as needed. It remains to verify one of the four listed categories of Theorem 22.6. Observe that \\(\\psi_{\\tau}(u)\\) is a function of bounded variation, so \\(\\psi_{\\tau}\\left(Y-X^{\\prime} \\beta\\right)\\) is in the second category. The score \\(X \\psi_{\\tau}\\left(Y-X^{\\prime} \\beta\\right)\\) is the product of the Lipschitz-continuous function \\(X\\) and \\(\\psi_{\\tau}\\left(Y-X^{\\prime} \\beta\\right)\\), and thus falls in the third category. This shows that Theorem \\(22.6\\) can be applied.\nWe have verified the conditions for Theorem \\(22.6\\) so asymptotic normality follows. For the covariance matrix we calculate that\n\\[\n\\mathbb{E}\\left[\\left(X \\psi_{\\tau}\\right)\\left(X \\psi_{\\tau}\\right)^{\\prime}\\right]=\\mathbb{E}\\left[X X^{\\prime} \\psi_{\\tau}^{2}\\right]=\\Omega_{\\tau} .\n\\]\nProof of Theorem 24.5: By the definition of the quantile treatment effect, monotonicity of causal effect (Assumption 24.1.2), definition of the causal effect, monotonicity of the treatment response (Assumption 24.1.3), and the definition of the quantile regression function, we find that\n\\[\n\\begin{aligned}\nQ_{\\tau}(x) &=\\mathbb{Q}_{\\tau}[C(X, U) \\mid X=x] \\\\\n&=C\\left(x, \\mathbb{Q}_{\\tau}[U \\mid X=x]\\right) \\\\\n&=h\\left(1, x, \\mathbb{Q}_{\\tau}[U \\mid X=x]\\right)-h\\left(0, x, \\mathbb{Q}_{\\tau}[U \\mid X=x]\\right) \\\\\n&=\\mathbb{Q}_{\\tau}[h(1, X, U) \\mid X=x]-\\mathbb{Q}_{\\tau}[h(0, X, U) \\mid X=x] \\\\\n&=q_{\\tau}(1, x)-q_{\\tau}(0, x) \\\\\n&=D_{\\tau}(x)\n\\end{aligned}\n\\]\nas claimed."
  },
  {
    "objectID": "chpt24-quantile-reg.html#exercises",
    "href": "chpt24-quantile-reg.html#exercises",
    "title": "23  Quantile Regression",
    "section": "23.20 Exercises",
    "text": "23.20 Exercises\nExercise 24.1 Prove (24.4) in Theorem 24.1.\nExercise 24.2 Prove (24.5) in Theorem 24.1.\nExercise 24.3 Define \\(\\psi(x)=\\tau-\\mathbb{1}\\{x<0\\}\\). Let \\(\\theta\\) satisfy \\(\\mathbb{E}[\\psi(Y-\\theta)]=0\\). Is \\(\\theta\\) a quantile of the distribution of \\(Y\\) ? Exercise 24.4 Take the model \\(Y=X^{\\prime} \\beta+e\\) where the distribution of \\(e\\) given \\(X\\) is symmetric about zero.\n\nFind \\(\\mathbb{E}[Y \\mid X]\\) and \\(\\operatorname{med}[Y \\mid X]\\).\nDo OLS and LAD estimate the same coefficient \\(\\beta\\) or different coefficients?\nUnder which circumstances would you prefer LAD over OLS? Under which circumstances would you prefer OLS over LAD? Explain.\n\nExercise 24.5 You are interested in estimating the equation \\(Y=X^{\\prime} \\beta+e\\). You believe the regressors are exogenous, but you are uncertain about the properties of the error. You estimate the equation both by least absolute deviations (LAD) and OLS. A colleague suggests that you should prefer the OLS estimate, because it produces a higher \\(R^{2}\\) than the LAD estimate. Is your colleague correct?\nExercise 24.6 Prove (24.13) in Theorem 24.2.\nExercise 24.7 Prove (24.14) in Theorem 24.2.\nExercise \\(24.8\\) Suppose \\(X\\) is binary. Show that \\(\\mathbb{Q}_{\\tau}[Y \\mid X]\\) is linear in \\(X\\).\nExercise 24.9 Suppose \\(X_{1}\\) and \\(X_{2}\\) are binary. Find \\(\\mathbb{Q}_{\\tau}\\left[Y \\mid X_{1}, X_{2}\\right]\\).\nExercise 24.10 Show (24.19).\nExercise 24.11 Show under correct specification that \\(\\Omega_{\\tau}=\\mathbb{E}\\left[X X^{\\prime} \\psi_{\\tau}^{2}\\right]\\) satisfies the simplification \\(\\Omega_{\\tau}=\\) \\(\\tau(1-\\tau) \\boldsymbol{Q}\\)\nExercise 24.12 Take the treatment response setting of Theorem 24.5. Suppose \\(h\\left(0, X_{2}, U\\right)=0\\), meaning that the response variable \\(Y\\) is zero whenever there is no treatment. Show that Assumption \\(24.1 .3\\) is not necessary for Theorem \\(24.5\\).\nExercise 24.13 Using the cps09mar dataset take the sample of Hispanic men with education 11 years or higher. Estimate linear quantile regression functions for log wages on education. Interpret your findings.\nExercise 24.14 Using the cps09mar dataset take the sample of Hispanic women with education 11 years or higher. Estimate linear quantile regression functions for log wages on education. Interpret.\nExercise 24.15 Take the Duflo, Dupas, and Kremer (2011) dataset DDK2011 and the subsample of students for which tracking=1. Estimate linear quantile regressions of totalscore on percentile (the latter is the student’s test score before the school year). Calculate standard errors by clustered bootstrap. Do the coefficients change meaningfully by quantile? How do you interpret these results?\nExercise 24.16 Using the cps09mar dataset estimate similarly to Figure \\(24.6\\) the quantile regressions for log wages on a \\(5^{\\text {th }}\\) - order polynomial in experience for college-educated Black women. Repeat for college-educated white women. Interpret your findings."
  },
  {
    "objectID": "chpt26-multiple-choice.html#introduction",
    "href": "chpt26-multiple-choice.html#introduction",
    "title": "24  Multiple Choice",
    "section": "24.1 Introduction",
    "text": "24.1 Introduction\nThis chapter surveys multinomial models. This includes multinomial response, multinomial logit, conditional logit, nested logit, mixed logit, multinomial probit, ordered response, count data, and the BLP demand model.\nFor more detailed treatments see Maddala (1983), Cameron and Trivedi (1998), Cameron and Trivedi (2005), Train (2009), and Wooldridge (2010)."
  },
  {
    "objectID": "chpt26-multiple-choice.html#multinomial-response",
    "href": "chpt26-multiple-choice.html#multinomial-response",
    "title": "24  Multiple Choice",
    "section": "24.2 Multinomial Response",
    "text": "24.2 Multinomial Response\nA multinomial random variable \\(Y\\) takes values in a finite set, typically written as \\(Y \\in\\{1,2, \\ldots, J\\}\\). The elements of the set are often called alternatives. In most applications the alternatives are categorical (car, bicycle, airplane, train) and unordered. When there are no regressors the model is fully described by the \\(J\\) probabilities \\(P_{j}=\\mathbb{P}[Y=j]\\).\nWe typically describe the pair \\((Y, X)\\) as multinomial response when \\(Y\\) is multinomial and \\(X \\in \\mathbb{R}^{k}\\) are regressors. The conditional distribution of \\(Y\\) given \\(X\\) is summarized by the response probability\n\\[\nP_{j}(x)=\\mathbb{P}[Y=j \\mid X=x] .\n\\]\nThe response probabilities are nonparametrically identified and can be arbitrary functions of \\(x\\).\nWe illustrate by extending the marriage status example of the previous chapter. The CPS variable marital records seven categories. We partition these into four alternatives: “married”1 , “divorced”, “separated”, and “never married”. Let \\(X\\) be age. \\(P_{j}(x)\\) for \\(j=1, \\ldots, 4\\) is the probability of each marriage status as a function of age. For our illustration we take the population of college-educated women.\nSince the response probabilities \\(P_{j}(x)\\) are nonparametrically identified a simple estimation method is binary response separately for each category. We plot in Figure 26.1 (a) logit estimates using a quadratic spline in age and a single knot at age 40 . The estimates show that the probability of “never married” decreases monotonically with age, that for “married” increases until around 38 and then decreases slowly, the probability of “divorced” increases monotonically with age, and the probability of “separated” is low for all age groups.\nA defect of the estimates of Figure 26.1(a) is that the sum of the four estimated probabilities (displayed as “Total”) does not equal one. This shows that separate estimation of the response probabilities neglects system information. For the remainder of this chapter the estimators discussed do not have this defect.\n\\({ }^{1}\\) marital \\(=1,2,3,4\\), which includes widowed.\n\n\nBinary Response Estimates\n\n\n\nMultinomial Logit\n\nFigure 26.1: Probability of Marital Status Given Age for College Educated Women\nMultinomial response is typically motivated and derived from a model of latent utility. The utility of alternative \\(j\\) is assumed to equal\n\\[\nU_{j}^{*}=X^{\\prime} \\beta_{j}+\\varepsilon_{j}\n\\]\nwhere \\(\\beta_{j}\\) are coefficients and \\(\\varepsilon_{j}\\) is an alternative-specific error. The coefficients \\(\\beta_{j}\\) describe how the variable \\(X\\) affects an individual’s utility of alternative \\(j\\). The error \\(\\varepsilon_{j}\\) is individual-specific and contains unobserved factors affecting an individual’s utility. In the marriage status example (where \\(X\\) is age) the coefficients \\(\\beta_{j}\\) describe how the utility of each marriage status varies with age, while the error \\(\\varepsilon_{j}\\) contains the individual factors which are not captured by age.\nIn the latent utility model an individual is assumed to select the alternative with the highest utility \\(U_{j}^{*}\\). Thus \\(Y=j\\) if \\(U_{j}^{*} \\geq U_{\\ell}^{*}\\) for all \\(\\ell\\). In model (26.1) this choice is unaltered if we add \\(X^{\\prime} \\gamma\\) to each utility. This means that the coefficients \\(\\beta_{j}\\) are not separately identified, at best the differences between alternatives \\(\\beta_{j}-\\beta_{\\ell}\\) are identified. Identification is achieved by imposing a normalization; the standard choice is to set \\(\\beta_{j}=0\\) for a base alternative \\(j\\), often taken as the last category \\(J\\). Reported coefficients \\(\\beta_{j}\\) should be interpreted as differences relative to the base alternative.\nThe choice is also unchanged if each utility (26.1) is multiplied by positive constant. This means that the scale of the coefficients \\(\\beta_{j}\\) is not identified. To achieve identification it is typical to fix the scale of the errors \\(\\varepsilon_{j}\\). Consequently the scale of the coefficients \\(\\beta_{j}\\) has no interpretive meaning.\nTwo classical multinomial response models are logit and probit. We introduce multinomial logit in the next section and multinomial probit in Section \\(26.8\\)."
  },
  {
    "objectID": "chpt26-multiple-choice.html#multinomial-logit",
    "href": "chpt26-multiple-choice.html#multinomial-logit",
    "title": "24  Multiple Choice",
    "section": "24.3 Multinomial Logit",
    "text": "24.3 Multinomial Logit\nThe simple multinomial logit model is\n\\[\nP_{j}(x)=\\frac{\\exp \\left(x^{\\prime} \\beta_{j}\\right)}{\\sum_{\\ell=1}^{J} \\exp \\left(x^{\\prime} \\beta_{\\ell}\\right)} .\n\\]\nThe model includes binary logit \\((J=2)\\) as a special case. We call (26.2) the simple multinomial logit to distinguish it from the conditional logit model of the next section.\nThe multinomial logit arises from the latent utility model (26.1) for the following error distributions.\nDefinition 26.1 The Type I Extreme Value distribution function is\n\\[\nF(\\varepsilon)=\\exp (-\\exp (-\\varepsilon)) .\n\\]\nDefinition 26.2 The Generalized Extreme Value (GEV) joint distribution is\n\\[\nF\\left(\\varepsilon_{1}, \\varepsilon_{2}, \\ldots, \\varepsilon_{J}\\right)=\\exp \\left(-\\left[\\sum_{j=1}^{J} \\exp \\left(-\\frac{\\varepsilon_{j}}{\\tau}\\right)\\right]^{\\tau}\\right)\n\\]\nfor \\(0<\\tau \\leq 1\\).\nFor \\(J=1\\) the GEV distribution (26.3) equals the Type I extreme value. For \\(J>1\\) and \\(\\tau=1\\) the GEV distribution equals the product of independent Type I extreme value distributions. For \\(J>1\\) and \\(\\tau<1\\) GEV random variables are dependent with correlation equal to \\(1-\\tau^{2}\\) (see Kotz and Nadarajah (2000)). The parameter \\(\\tau\\) is known as the dissimilarity parameter. The distribution (26.3) is a special case of the “GEV distribution” introduced by McFadden (1981). Furthermore, there is heterogeneity among authors regarding the choice of notation and labeling. The notation used above is consistent with the Stata manual. In contrast, McFadden \\((1978,1981)\\) used \\(1-\\sigma\\) in place of \\(\\tau\\) and called \\(\\sigma\\) the similarity parameter. Cameron and Trivedi (2005) used \\(\\rho\\) instead of \\(\\tau\\) and called \\(\\rho\\) the scale parameter.\nThe following result is due to McFadden \\((1978,1981)\\).\nTheorem 26.1 Assume the utility of alternative \\(j\\) is \\(U_{j}^{*}=X^{\\prime} \\beta_{j}+\\varepsilon_{j}\\) and the error vector \\(\\left(\\varepsilon_{1}, \\ldots, \\varepsilon_{j}\\right)\\) has GEV distribution (26.3). Then the response probabilities equal\n\\[\nP_{j}(X)=\\frac{\\exp \\left(X^{\\prime} \\beta_{j} / \\tau\\right)}{\\sum_{\\ell=1}^{J} \\exp \\left(X^{\\prime} \\beta_{\\ell} / \\tau\\right)} .\n\\]\nThe proof is in Section 26.13. The response probabilities in Theorem \\(26.1\\) are multinomial logit (26.2) with coefficients \\(\\beta_{j}^{*}=\\beta_{j} / \\tau\\). The dissimilarity parameter \\(\\tau\\) only affects the scale of the coefficients, which is not identified. Thus GEV errors imply a multinomial logit model and \\(\\tau\\) is not identified.\nAs discussed above, when \\(\\tau=1\\) the GEV distribution (26.3) specializes to i.i.d. Type I extreme value. Thus a special case of Theorem \\(26.1\\) is the following: If the errors \\(\\varepsilon_{j}\\) are i.i.d. Type I extreme value then the response probabilities are multinomial logit (26.2) with coefficients \\(\\beta_{j}\\). This is the most commonly-used and commonly-stated implication of Theorem \\(26.1\\).\nIn contemporary choice modelling a commonly-used assumption is that utility is extreme value distributed. This is done so that Theorem \\(26.1\\) can be invoked to deduce that the choice probabilities are multinomial logit. A reasonable deduction is that this assumption is made for algebraic convenience, not because anyone believes that utility is actually extreme valued distributed.\nThe likelihood function given a random sample \\(\\left\\{Y_{i}, X_{i}\\right\\}\\) is straightforward to construct. Write the response probabilities \\(P_{j}(X \\mid \\beta)\\) as functions of the parameter vector \\(\\beta=\\left(\\beta_{1}, \\ldots, \\beta_{J}\\right)\\). The probability mass function for \\(Y\\) is\n\\[\n\\pi(Y \\mid X, \\beta)=\\prod_{j=1}^{J} P_{j}(X \\mid \\beta)^{\\mathbb{1}\\{Y=j\\}} .\n\\]\nThe log-likelihood function is\n\\[\n\\ell_{n}(\\beta)=\\sum_{i=1}^{n} \\sum_{j=1}^{J} \\mathbb{1}\\left\\{Y_{i}=j\\right\\} \\log P_{j}\\left(X_{i} \\mid \\beta\\right)\n\\]\nThe maximum likelihood estimator (MLE) is:\n\\[\n\\widehat{\\beta}=\\underset{\\beta}{\\operatorname{argmax}} \\ell_{n}(\\beta) .\n\\]\nThere is no algebraic solution so \\(\\widehat{\\beta}\\) needs to be found numerically. The log-likelihood function is globally concave so maximization is numerically straightforward.\nTo illustrate, we estimate the marriage status example of the previous section using multinomial logit and display the estimated response probabilities in Figure 26.1(b). The estimates are similar to the binary choice estimates in panel (a) but by construction sum to one.\nThe coefficients of a multinomial choice model can be difficult to interpret. Therefore in applications it may be useful to examine and report marginal effects. We can calculate \\({ }^{2}\\) that the marginal effects are\n\\[\n\\delta_{j}(x)=\\frac{\\partial}{\\partial x} P_{j}(x)=P_{j}(x)\\left(\\beta_{j}-\\sum_{\\ell=1}^{J} \\beta_{\\ell} P_{\\ell}(x)\\right) .\n\\]\nThis is estimated by\n\\[\n\\widehat{\\delta}_{j}(x)=\\widehat{P}_{j}(x)\\left(\\widehat{\\beta}_{j}-\\sum_{\\ell=1}^{J} \\widehat{\\beta}_{\\ell} \\widehat{P}_{\\ell}(x)\\right) .\n\\]\nThe average marginal effect \\(\\operatorname{AME}_{j}=\\mathbb{E}\\left[\\delta_{j}(X)\\right]\\) can be estimated by\n\\[\n\\widehat{\\mathrm{AME}}_{j}=\\frac{1}{n} \\sum_{i=1}^{n} \\widehat{\\delta}_{j}\\left(X_{i}\\right) .\n\\]\nIn Stata, multinomial logit can be implemented using the mlogit command. Probabilities can be calculated by predict and average marginal effects by margins, dydx. In R, multinomial logit can be implemented using the mlogit command.\n\\({ }^{2}\\) See Exercise 26.3."
  },
  {
    "objectID": "chpt26-multiple-choice.html#conditional-logit",
    "href": "chpt26-multiple-choice.html#conditional-logit",
    "title": "24  Multiple Choice",
    "section": "24.4 Conditional Logit",
    "text": "24.4 Conditional Logit\nIn the simple multinomial logit model of the previous section the regressors \\(X\\) (e.g., age) are specific to the individual but not the alternative (they do not have a \\(j\\) subscript). In most applications, however, there are regressors which vary across alternatives. A typical example is the price or cost of an alternative. In a latent utility model it is reasonable to assume that these alternative-specific regressors only affect an individual’s utility if that specific alternative is selected. A choice model which allows for regressors which differ across alternatives was developed by McFadden in the 1970s, which he called the Conditional Logit model.\nAn example will help illustrate the setting. Suppose you (a student) need to select a mode of travel from your apartment to the university. Travel alternatives may include: walk, bicycle, bus, train, or car. Which will you select? Your choice will undoubtedly depend on a number of factors, and of particular importance is the \\(\\operatorname{cost}^{3}\\) of each alternative. We can model this by specifying that the utility \\(Y_{j}^{*}\\) (26.1) of alternative \\(j\\) is a function of its cost \\(X_{j}\\).\nAs a concrete example consider the dataset Koppelman on the textbook webpage. This is an abridged version of the dataset ModeCanada distributed with the R package mlogit, and used in the papers Forinash and Koppelman (1993), Koppelman and Wen (2000), and Wen and Koppelman (2001). The data are responses to a survey \\({ }^{4}\\) of Canadian business travelers concerning their actual travel choices in the Toronto-Montreal corridor. Each observation \\((n=2779)\\) is a specific individual making a specific trip. Four travel alternatives were considered: train, air, bus, and car. Available regressors include the cost of each alternative, the in-vehicle travel time (intime) of each alternative, household income, and an indicator if one of the trip endpoints is an urban center.\nThe conditional logit model posits that the utility of alternative \\(j\\) is a function of regressors \\(X_{j}\\) which vary across alternative \\(j\\) :\n\\[\nU_{j}^{*}=X_{j}^{\\prime} \\gamma+\\varepsilon_{j} .\n\\]\nHere, \\(\\gamma\\) are coefficients and \\(\\varepsilon_{j}\\) is an alternative-specific error. Notice that in contrast to (26.1) that \\(X_{j}\\) varies across \\(j\\) while the coefficients \\(\\gamma\\) are common. For example, in the Koppelman data set the variables cost and intime are recorded for each individual/alternative pair. (For example, the first observation in the sample is a traveler who could have selected train travel for \\(\\$ 58.25\\) and a travel time of 215 minutes, air travel for \\(\\$ 142.80\\) and 56 minutes, bus travel for \\(\\$ 27.52\\) and 301 minutes, or car travel for \\(\\$ 71.63\\) and 262 minutes. This traveler selected to travel by air.)\nTo understand the difference between the multinomial logit and the conditional logit models, (26.1) describes how the utility of a specific alternative (e.g. married or divorced) is affected by a variable such as age. This requires a separate coefficient for each alternative to have an impact. In contrast, (26.6) describes how the utility of an alternative (e.g. train or car) is affected by factors such as cost and time. These variables have common meanings across alternatives so the restriction that the coefficients are common appears reasonable.\nMore generally the conditional logit model allows some regressors \\(X_{j}\\) to vary across alternatives while other regressors \\(W\\) do not vary across \\(j\\). This model is\n\\[\nU_{j}^{*}=W^{\\prime} \\beta_{j}+X_{j}^{\\prime} \\gamma+\\varepsilon_{j} .\n\\]\nFor example, in the Koppelman dataset the variables cost and intime are components of \\(X_{j}\\) while the variables income and urban are components of \\(W\\).\nIn model (26.7) the coefficients \\(\\gamma\\) and coefficient differences \\(\\beta_{j}-\\beta_{\\ell}\\) are identified up to scale. Identification is achieved by normalizing the scale of \\(\\varepsilon_{j}\\) and setting \\(\\beta_{J}=0\\) for a base alternative \\(J\\).\n\\({ }^{3}\\) Cost can be multi-dimensional, for example including monetary cost and travel time.\n\\({ }^{4}\\) The survey was conducted by the Canadian national rail carrier to assess the demand for high-speed rail. The conditional logit model is (26.6) or (26.7) plus the assumption that the errors \\(\\varepsilon_{j}\\) are distributed i.i.d. Type I extreme value \\({ }^{5}\\). From Theorem \\(26.1\\) we deduce that the probability response functions equal\n\\[\nP_{j}(w, x)=\\frac{\\exp \\left(w^{\\prime} \\beta_{j}+x_{j}^{\\prime} \\gamma\\right)}{\\sum_{\\ell=1}^{J} \\exp \\left(w^{\\prime} \\beta_{\\ell}+x_{\\ell}^{\\prime} \\gamma\\right)} .\n\\]\nThis is multinomial logit but with regressors and coefficients \\(W^{\\prime} \\beta_{j}+X_{j}^{\\prime} \\gamma\\).\nLet \\(\\theta=\\left(\\beta_{1}, \\ldots \\beta_{J}, \\gamma\\right)\\). Given the observations \\(\\left\\{Y_{i}, W_{i}, X_{i}\\right\\}\\) where \\(X_{i}=\\left\\{X_{1 i}, \\ldots, X_{J i}\\right\\}\\), the log-likelihood function is\n\\[\n\\ell_{n}(\\theta)=\\sum_{i=1}^{n} \\sum_{j=1}^{J} \\mathbb{1}\\left\\{Y_{i}=j\\right\\} \\log P_{j}\\left(W_{i}, X_{i} \\mid \\theta\\right) .\n\\]\nThe maximum likelihood estimator (MLE) \\(\\widehat{\\theta}\\) maximizes \\(\\ell_{n}(\\theta)\\). There is no algebraic solution so \\(\\widehat{\\theta}\\) needs to be found numerically.\nUsing the Koppelman dataset we estimate a conditional logit model. Estimates are reported in Table 26.1. Included as regressors are cost, intime, income, and urban. The base alternative is travel by train. The first two coefficient estimates are negative, meaning that the probability of selecting any mode of transport is decreasing in the monetary and time cost of this mode of travel. The income and urban variables are not alternative-specific so have coefficients which vary by alternative. The urban coefficient for air is positive and that for car is negative, indicating that the probability of air travel is increased relative to train travel if an endpoint is urban, and conversely for car travel. The income coefficient is positive for air travel and negative for bus travel, indicating that transportation choice is affected by a traveler’s income in the expected way.\nAs discussed previously, coefficient estimates can be difficult to interpret. It may be useful to calculate transformations such as average marginal effects. The average marginal effects with respect to the input \\(W\\) are estimated as in (26.5) with \\(\\widehat{P}_{\\ell}\\left(X_{i}\\right)\\) replaced by \\(\\widehat{P}_{\\ell}\\left(W_{i}, X_{i}\\right)\\). For the inputs \\(X_{j}\\) we calculate \\({ }^{6}\\) that\n\\[\n\\delta_{j j}(w, x)=\\frac{\\partial}{\\partial x_{j}} P_{j}(w, x)=\\gamma P_{j}(w, x)\\left(1-P_{j}(w, x)\\right)\n\\]\nand for \\(j \\neq \\ell\\)\n\\[\n\\delta_{j \\ell}(w, x)=\\frac{\\partial}{\\partial x_{\\ell}} P_{j}(w, x)=-\\gamma P_{j}(w, x) P_{\\ell}(w, x) .\n\\]\nNote that these are double indexed ( \\(j\\) and \\(\\ell\\) ). For example for \\(X=\\operatorname{cost}, j=\\) train and \\(\\ell=\\) air, \\(\\delta_{j \\ell}\\) is the marginal effect of a change in the cost of air travel on the probability of train travel. In the conditional logit model, calculation (26.10) implies the symmetric response \\(\\delta_{j \\ell}(w, x)=\\delta_{\\ell j}(w, x)\\). This means that the marginal effect of (for example) air cost on train travel equals the marginal effect of train cost on air travel \\(^{7}\\). The average marginal effects \\(\\mathrm{AME}_{j \\ell}=\\mathbb{E}\\left[\\delta_{j \\ell}(W, X)\\right]\\) can be estimated by the analogous sample averages as in (26.5). One useful implication of (26.9) and (26.10) is that the components of AME \\({ }_{j j}\\) have the same signs as the components of \\(\\gamma\\) and the components of \\(\\mathrm{AME}_{j \\ell}\\) have the opposite signs. Thus, for example, if the coefficient \\(\\gamma\\) on a cost variable is negative then the own-price effect is negative and the cross-price effects are positive.\nTo illustrate, we report a set of estimated AME of cost and time factors on the probability of train travel in Table 26.2. We focus on train travel because the demand for high-speed rail was the focus of the\n\\({ }^{5}\\) The model is unaltered if the errors are jointly GEV with dissimilarity parameter \\(\\tau\\). However, \\(\\tau\\) is not identified so without loss of generality it is assumed that \\(\\tau=1\\).\n\\({ }^{6}\\) See Exercise \\(26.5\\).\n\\({ }^{7}\\) This symmetry breaks down if nonlinear transformations are included in the model. Table 26.1: Multinomial Models for Transportation Choice\n\noriginal study. We calculate and report the AME of the monetary cost and travel time of train, air, and car travel. To convert the AME into approximate elasticities (which may be easier to interpret), divide each AME by the probability of train travel (0.17) and multiply by the sample mean of the factor, reported in the first column. You can calculate that the estimated approximate elasticity of train travel with respect to train cost is \\(-0.9\\), with respect to train travel time is \\(-2.5\\), with respect to air cost is \\(1.0\\), with respect to air travel time is \\(0.25\\), with respect to car cost is \\(0.6\\), and with respect to car travel time is \\(1.5\\). These estimates indicate that train travel is sensitive to its travel time, is sensitive with respect to its monetary cost and that of airfare, and is sensitive to the travel time of car travel. We can use the estimated AME to calculate the rough effects of cost and travel time changes. For example, suppose high-speed rail reduces train travel time by \\(33 %\\) - an average reduction of 75 minutes - while price is unchanged. The estimates imply this will increase train travel probability by \\(0.14\\), that is, from \\(17 %\\) to \\(31 %\\), which is close to a doubling of usage.\nIn many cases it is natural to expect that the coefficients \\(\\gamma\\) will vary across individuals. We discuss models with random \\(\\gamma\\) in Section 26.7. A simpler specification is to allow \\(\\gamma\\) to vary with the individual Table 26.2: AME of Cost and Time on Train Travel\n\n\n\n\n\n\n\n\n\n\n\nEffect of\nMean\nCond. Logit\nMixed Logit\nSimple Multi. Probit\nMulti. Probit\n\n\n\n\nTrain Cost ($)\n{56\n\\(-0.27\\)\n\\(-0.28\\)\n\\(-0.32\\)\n\\(-0.08\\)\n\n\n\n\n\\((0.04)\\)\n\\((0.05)\\)\n\\((0.04)\\)\n\\((0.03)\\)\n\n\nTrain Time (min.)\n{224\n\\(-0.19\\)\n\\(-0.20\\)\n\\(-0.19\\)\n\\(-0.09\\)\n\n\n\n\n\\((0.01)\\)\n\\((0.01)\\)\n\\((0.01)\\)\n\\((0.01)\\)\n\n\nAir Cost ($)\n{153\n\\(0.11\\)\n\\(0.11\\)\n\\(0.13\\)\n\\(0.05\\)\n\n\n\n{\n\\((0.02)\\)\n\\((0.02)\\)\n\\((0.02)\\)\n\n\n\nAir Time (min.)\n{54\n\\(0.08\\)\n\\(0.08\\)\n\\(0.08\\)\n\\(0.06\\)\n\n\n\n\n\\((0.01)\\)\n\\((0.01)\\)\n\\((0.01)\\)\n\\((0.01)\\)\n\n\nCar Cost ($)\n{65\n\\(0.16\\)\n\\(0.17\\)\n\\(0.18\\)\n\\(0.02\\)\n\n\n\n\n\\((0.01)\\)\n\\((0.03)\\)\n\\((0.02)\\)\n\\((0.01)\\)\n\n\nCar Time (min.)\n{232\n\\(0.11\\)\n\\(0.12\\)\n\\(0.11\\)\n\\(0.02\\)\n\n\n\n\n\\((0.01)\\)\n\\((0.01)\\)\n\\((0.01)\\)\n\\((0.01)\\)\n\n\n\nNote: For ease of reading, the reported AME estimates have been multiplied by 100.\ncharacteristic \\(W\\). For example in the transportation application the opportunity cost of travel time is likely related to an individual’s wage which can be proxied by household income. We can write this as \\(\\gamma=\\gamma_{1}+\\gamma_{2} X\\). Substituted into (26.7) we obtain the model\n\\[\nU_{j}^{*}=W \\beta_{j}+X_{j} \\gamma_{1}+X_{j} W \\gamma_{2}+\\varepsilon_{j}\n\\]\nwhere for simplicity we assume \\(W\\) and \\(X_{j}\\) are scalar. This can be written in form (26.7) by redefining \\(X_{j}\\) as \\(\\left(X_{j}, X_{j} W\\right)\\) and the same estimation methods apply. In our application this model yields a negative estimate for \\(\\gamma_{2}\\), indicating that the cost of travel time is indeed increasing in income.\nIn Stata, model (26.7) can be estimated using cmclogit. Probabilities can be calculated by predict, and marginal effects by margins. In R, use mlogit."
  },
  {
    "objectID": "chpt26-multiple-choice.html#independence-of-irrelevant-alternatives",
    "href": "chpt26-multiple-choice.html#independence-of-irrelevant-alternatives",
    "title": "24  Multiple Choice",
    "section": "24.5 Independence of Irrelevant Alternatives",
    "text": "24.5 Independence of Irrelevant Alternatives\nThe multinomial logit model has an undesirable restriction. For fixed parameters and regressors the ratio of the probability of two alternatives is\n\\[\n\\frac{P_{j}(W, X \\mid \\theta)}{P_{\\ell}(W, X \\mid \\theta)}=\\frac{\\exp \\left(W^{\\prime} \\beta_{j}+X_{j}^{\\prime} \\gamma\\right)}{\\exp \\left(W^{\\prime} \\beta_{\\ell}+X_{\\ell}^{\\prime} \\gamma\\right)} .\n\\]\nThis odds ratio is a function only of the inputs \\(X_{j}\\) and \\(X_{\\ell}\\), does not depend on any of the inputs specific to the other alternatives, and is unaltered by the presence of other alternatives. This property is called independence of irrelevant alternatives (IIA), meaning that the choice between option \\(j\\) and \\(\\ell\\) is independent of the other alternatives and hence the latter are irrelevant to the bivariate choice. This property is strongly tied to the multinomial logit model as the latter was derived axiomatically by Luce (1959) from an IIA assumption.\nTo understand why IIA may be problematic it is helpful to think through specific examples. Take the transportation choice problem of the previous section. The IIA condition means that the ratio of the probability of selecting train to that of selecting car is unaffected by the price of an airplane ticket. This may make sense if individuals view the set of choices as similarly substitutable, but does not make sense if train and air are close substitutes. In this latter setting a low airplane ticket may make it highly unlikely that an individual will select train travel while unaffecting their likelihood of selecting car travel.\nA famous example of this problem is the following setting. Suppose the alternatives are car and bus and suppose that the probability of the alternatives is split \\(50 %-50 %\\). Now suppose that we can split the bus alternative into “red bus” and “blue bus” so there are a total of three alternatives. Suppose the blue bus and red bus are close equivalents: they have similar schedules, convenience, and cost. In this context most individuals would be near indifferent between the blue and red bus so these alternatives would receive similar probabilities. It would thus seem reasonable to expect that the probabilities of these three choices would be close to \\(50 %-25 %-25 %\\). The IIA condition, however, implies that the ratio of the first two probabilities must remain 1, so this implies that the probabilities of the three choices would be 33%-33%-33%. We deduce that the multinomial logit model implies that adding “red bus” to the choice list results in the reduction of car usage from \\(50 %\\) to \\(33 %\\). This doesn’t make sense; it is an unreasonable implication. This example is known as the “red bus/blue bus puzzle”.\nThe source of the problem is that the IIA structure and multinomial logit model exclude differentiated substitutability among the alternatives. This may be appropriate when the alternatives (e.g. bus, train, and car) are clearly differentiated and have reasonably similar degrees of substitutability. It is not appropriate when a subset of alternatives (e.g. red bus and blue bus) are close substitutes.\nPart of the problem is due to the restrictive correlation pattern imposed on the errors by the generalized extreme value distribution. To allow for cases such as red bus/blue bus we require a more flexible correlation structure which allows subsets of alternatives to have differential correlations."
  },
  {
    "objectID": "chpt26-multiple-choice.html#nested-logit",
    "href": "chpt26-multiple-choice.html#nested-logit",
    "title": "24  Multiple Choice",
    "section": "24.6 Nested Logit",
    "text": "24.6 Nested Logit\nThe nested logit model circumvents the IIA problem described in the previous section by separating the alternatives into groups. Alternatives within groups are allowed to be correlated but are assumed uncorrelated across groups.\nThe model posits that there are \\(J\\) groups each with \\(K_{j}\\) alternatives. We use \\(j\\) to denote the group, \\(k\\) to denote the alternative within a group, and ” \\(j k\\) ” to denote a specific alternative. Let \\(W\\) denote individualspecific regressors and \\(X_{j k}\\) denote regressors which vary by alternative. The utility of the \\(j k^{t h}\\) alternative is a function of the regressors plus an error:\n\\[\nU_{j k}^{*}=W^{\\prime} \\beta_{j k}+X_{j k}^{\\prime} \\gamma+\\varepsilon_{j k} .\n\\]\nThe model assumes that the individual selects the alternative \\(j k\\) with the highest utility \\(U_{j k}^{*}\\).\nMcFadden’s Nested Logit model assumes that the errors have the following GEV joint distribution\n\\[\nF\\left(\\varepsilon_{11}, \\ldots, \\varepsilon_{J K_{J}}\\right)=\\exp \\left(-\\sum_{j=1}^{J}\\left[\\sum_{k=1}^{K_{j}} \\exp \\left(-\\frac{\\varepsilon_{j k}}{\\tau_{j}}\\right)\\right]^{\\tau_{j}}\\right) .\n\\]\nThis is a generalization of the GEV distribution (26.3). The distribution (26.13) is the product of \\(J\\) GEV distributions (26.3) each with dissimilarity parameter \\(\\tau_{j}\\), which means that the errors within each group are GEV distributed with dissimilarity parameter \\(\\tau_{j}\\). Across groups the errors are independent. When \\(\\tau_{j}=1\\) for all \\(j\\) the errors are mutually independent and the joint model equals conditional logit. When \\(\\tau_{j}<1\\) for some \\(j\\) the errors within group \\(j\\) are correlated but not with the other errors. If a group has a single alternative its dissimilarity parameter is not identified so should be set to one.\nThe nested logit model (26.12)-(26.13) is structurally identical to the conditional logit model except that the error distribution is (26.13) instead of (26.3). The coefficients \\(\\beta_{j k}\\) and \\(\\gamma\\) have the same interpretation as in the conditional logit model. As written, (26.12) allows the coefficients \\(\\beta_{j k}\\) to vary across alternatives \\(j k\\) while the coefficients \\(\\gamma\\) are common across \\(j\\) and \\(k\\). Other specifications are possible. For example, the model can be altered to allow the coefficients \\(\\beta_{j}\\) and/or \\(\\gamma_{j}\\) to vary across groups but not alternatives. The degree of variability is a modeling choice with a flexibility/parsimony trade-off. It is also possible (but less common in practice) to have variables \\(W_{j}\\) which vary by group but not by alternative. These can be included in the model with common coefficients.\nThe partition of alternatives into groups is a modeling decision. Alternatives with a high degree of substitutability should be placed in the same group. Alternatives with a low degree of substitutability should be placed in different groups.\nTo illustrate, consider a consumer choice of an automobile purchase. For simplicity suppose there are four choices: Honda Civic, Ford Fusion, Honda CR-V, and Ford Escape. The first two are compact cars and the last two are sports utility vehicles (SUVs). Consequently it is reasonable to think of the first two as substitutes and the last two as substitutes. We display this nesting as a tree diagram as in Figure 26.2. This shows the division of the decision “Car” into “Compact” and “Sports Utility Vehicle” and the further division by model.\n\nFigure 26.2: Nested Choice\nOnly the differences between the coefficients \\(\\beta_{j k}\\) are identified. Identification is achieved by setting one alternative \\(j k\\) as the base alternative. If the coefficients \\(\\beta_{j}\\) are constrained to vary by by group then identification is achieved by setting a base group. The scale of the coefficients is not identified separately from the scaling of the errors implicit in the GEV distribution (26.13).\nSome authors interpret model (26.12) as a nested sequential choice. An individual first selects a group and second selects the best option within the group. For example, in the car choice example you could imagine first deciding on the style of car (compact or SUV) and then deciding on the specific car within each category (e.g. Civic vs. Fusion or CR-V vs. Escape). The sequential choice interpretation may help structure the groupings. However, sequential choice should be used cautiously as it is not technically correct. The correct interpretation is degree of substitutability not the timing of decisions.\nIf the coefficients \\(\\beta_{j}\\) on \\(W\\) are constrained to only vary across groups (this, for example, is the default in Stata) then the effect \\(W^{\\prime} \\beta_{j}\\) in (26.12) shifts the utilities of all alternatives within a group, and thus does not affect the choice of an alternative within a group. In this case the variable \\(W\\) can be described as “affecting the choice of group”.\nWe now describe the nested logit response probabilities.\nTheorem 26.2 Assume the utility of alternative \\(j k\\) is \\(U_{j k}^{*}=\\mu_{j k}+\\varepsilon_{j k}\\) and the error vector has distribution function (26.13). Then the response probabilities equal \\(P_{j k}=P_{k \\mid j} P_{j}\\) where\n\\[\nP_{k \\mid j}=\\frac{\\exp \\left(\\mu_{j k} / \\tau_{j}\\right)}{\\sum_{m=1}^{K_{j}} \\exp \\left(\\mu_{j m} / \\tau_{j}\\right)}\n\\]\nand\n\\[\nP_{j}=\\frac{\\left(\\sum_{m=1}^{K_{j}} \\exp \\left(\\mu_{j m} / \\tau_{j}\\right)\\right)^{\\tau_{j}}}{\\sum_{\\ell=1}^{J}\\left(\\sum_{m=1}^{K_{\\ell}} \\exp \\left(\\mu_{\\ell m} / \\tau_{\\ell}\\right)\\right)^{\\tau_{\\ell}}} .\n\\]\nTheorem \\(26.2\\) shows that the response probabilities equal the product of two terms: \\(P_{k \\mid j}\\) and \\(P_{j}\\). The first, \\(P_{k \\mid j}\\), is the conditional probability of alternative \\(k\\) given the group \\(j\\) and takes the standard conditional logit form. The second, \\(P_{j}\\), is the probability of group \\(j\\).\nLet \\(\\theta\\) be the parameters. The log-likelihood function is\n\\[\n\\ell_{n}(\\theta)=\\sum_{i=1}^{n} \\sum_{j=1}^{J} \\sum_{k=1}^{K_{j}} \\mathbb{1}\\left\\{Y_{i}=j k\\right\\}\\left(\\log P_{k \\mid j}\\left(W_{i}, X_{i} \\mid \\theta\\right)+\\log P_{j}\\left(W_{i}, X_{i} \\mid \\theta\\right)\\right) .\n\\]\nThe MLE \\(\\widehat{\\theta}\\) maximizes \\(\\ell_{n}(\\theta)\\). There is no algebraic solution so \\(\\widehat{\\theta}\\) needs to be found numerically.\nBecause the probability structure of a nested logit model is more complicated than the conditional logit model it may be difficult to interpret the coefficient estimates. Marginal effects can (in principle) be calculated but these are complicated functions of the coefficients.\nTo illustrate, we estimate a nested logit model of transportation choice using the Koppelman dataset. To facilitate comparisons we estimate the same specification as for conditional logit. The difference is that we use the GEV distribution (26.13) with the groupings \\(\\{\\) car, air \\(\\}\\) and \\(\\{\\) train, bus \\(\\}\\). This adds two dissimilarity parameters. The results are reported in the second column of Table \\(26.1\\).\nThe dissimilarity parameter estimate for \\(\\{\\) car, air \\(\\}\\) is \\(0.24\\) which is small. It implies a correlation of \\(0.94\\) between the car and air utility shocks. This suggests that the conditional logit model - which assumes the utility errors are independent - is misspecified. The dissimilarity parameter estimate for {train, bus} is on the boundary \\({ }^{8} 1.00\\) so has no standard error.\nNested logit modeling is limited by the necessity of selecting the groupings. Typically there is not a unique obvious structure; consequently any proposed grouping is subject to misspecification.\nIn this section we described the nested logit model with one nested layer. The model extends to multiple nesting layers. The difference is that the joint distribution (26.13) is modified to allow higher levels of interactions with additional dissimilarity parameters. An applied example is Goldberg (1995) who used a five-level nested logit model to estimate the demand for automobilies. The levels used in her analysis were (1) Buy/Not Buy; (2) New/Used; (3) Car Class; (4) Foreign/Domestic; and (5) Car Model.\nIn Stata, nested logit models can be estimated by nlogit."
  },
  {
    "objectID": "chpt26-multiple-choice.html#mixed-logit",
    "href": "chpt26-multiple-choice.html#mixed-logit",
    "title": "24  Multiple Choice",
    "section": "24.7 Mixed Logit",
    "text": "24.7 Mixed Logit\nA generalization of the conditional logit model which allows the coefficients \\(\\gamma\\) on the alternativevarying regressors to be random across individuals is known as mixed logit. The model is also known as conditional mixed logit and random parameters logit.\nRecall that the conditional logit model is \\(U_{j}^{*}=W^{\\prime} \\beta_{j}+X_{j}^{\\prime} \\gamma+\\varepsilon_{j}\\) with \\(\\varepsilon_{j}\\) i.i.d. extreme value. Now replace \\(\\gamma\\) with an individual-specific random variable \\(\\eta\\) with distribution \\(F(\\eta \\mid \\alpha)\\) and parameters \\(\\alpha\\). This model is\n\\[\n\\begin{aligned}\nU_{j}^{*} &=W^{\\prime} \\beta_{j}+X_{j}^{\\prime} \\eta+\\varepsilon_{j} \\\\\n\\eta & \\sim F(\\eta \\mid \\alpha) .\n\\end{aligned}\n\\]\nFor example, in our transportation choice application the variables \\(X_{j}\\) are the cost and travel time of each alternative. The above model allows the effect of cost and time on utility to be heterogeneous across individuals.\nThe most common distributional assumption for \\(\\eta\\) is \\(\\mathrm{N}(\\gamma, D)\\) with diagonal covariance matrix \\(D\\). Other common specifications include \\(\\mathrm{N}(\\gamma, \\Sigma)\\) with unconstrained covariance matrix \\(\\Sigma\\), and log-normallydistributed \\(\\eta\\) to enforce \\(\\eta \\geq 0\\). (A constraint \\(\\eta \\leq 0\\) can be imposed by first multiplying the relevant regressor \\(X_{j}\\) by -1.) It is also common to partition \\(X_{j}\\) so that some variables have random coefficients and others have fixed coefficients. The reason why these constraints may be desirable is parsimony and simpler computation.\nUnder the normality specifications \\(\\eta \\sim \\mathrm{N}(\\gamma, D)\\) and \\(\\eta \\sim \\mathrm{N}(\\gamma, \\Sigma)\\) the mean \\(\\gamma\\) equals the average random coefficient in the population and has a similar interpretation to the coefficient \\(\\gamma\\) in the conditional logit model. The variances in \\(D\\) or \\(\\Sigma\\) control the dispersion of the distribution of \\(\\eta\\) in the population. Smaller variances mean that \\(\\eta\\) is mildly dispersed; larger variances mean high dispersion and heterogeneity.\nA useful feature of the mixed logit model is that the random coefficients induce correlation among the alternatives. To see this, write \\(\\gamma=\\mathbb{E}[\\eta]\\) and \\(V_{j}=X_{j}^{\\prime}(\\eta-\\gamma)+\\varepsilon_{j}\\). Then the model can be written as\n\\[\nY_{j}^{*}=W^{\\prime} \\beta_{j}+X_{j}^{\\prime} \\gamma+V_{j}\n\\]\nwhich is the conventional random utility framework but with errors \\(V_{j}\\) instead of \\(\\varepsilon_{j}\\). An important difference is that these errors are conditionally heteroskedastic and correlated across alternatives:\n\\[\n\\mathbb{E}\\left[V_{j} V_{\\ell} \\mid X_{j}, X_{\\ell}\\right]=X_{j}^{\\prime} \\operatorname{var}[\\eta] X_{\\ell} .\n\\]\n\\({ }^{8}\\) The uncontrained maximizer exceeds one which violates the parameter space so the the model is effectively estimated constraining this dissimilarity parameter to equal one. This non-zero correlation means that the IIA property is partially broken, giving the mixed logit model more flexibility than the conditional logit model to capture choice behavior.\nConditional on \\(\\eta\\) the response probabilities follow from (26.8)\n\\[\nP_{j}(w, x \\mid \\eta)=\\frac{\\exp \\left(w^{\\prime} \\beta_{j}+x_{j}^{\\prime} \\eta\\right)}{\\sum_{\\ell=1}^{J} \\exp \\left(w^{\\prime} \\beta_{\\ell}+x_{\\ell}^{\\prime} \\eta\\right)} .\n\\]\nThe unconditional response probabilities are found by integration.\n\\[\nP_{j}(w, x)=\\int P_{j}(w, x \\mid \\eta) d F(\\eta \\mid \\alpha) .\n\\]\nThe log-likelihood function is\n\\[\n\\ell_{n}(\\theta)=\\sum_{i=1}^{n} \\sum_{j=1}^{J} \\mathbb{1}\\left\\{Y_{i}=j\\right\\} \\log P_{j}\\left(W_{i}, X_{i} \\mid \\theta\\right)\n\\]\nwhere \\(\\theta\\) is the list of all parameters including \\(\\eta\\).\nThe integral in (26.14) is not available in closed form. A standard numerical implementation \\({ }^{9}\\) is Monte Carlo integration (estimation by simulation). This technique works as follows.Let \\(\\left\\{\\eta_{1}, \\ldots, \\eta_{G}\\right\\}\\) be a set of i.i.d. pseudo-random draws from \\(F(\\eta \\mid \\alpha)\\). The simulation estimator of (26.14) is\n\\[\n\\widetilde{P}_{j}(w, x)=\\frac{1}{G} \\sum_{g=1}^{G} P_{j}\\left(w, x \\mid \\eta_{g}\\right)\n\\]\nAs \\(G\\) increases this converges in probability to (26.14). Monte Carlo integration is computationally more efficient than numerical integration when the dimension of \\(\\eta\\) is three or larger, but is considerably more computationally intensive than non-random conditional logit.\nTo illustrate, we estimate a mixed logit model for the transportation application treating the coefficient on travel time as a normal random variable. The coefficient estimates are reported in Table \\(26.1\\) with estimated marginal effects in Table \\(26.2\\). The results are similar to the conditional logit model. The coefficient on travel time has a mean \\(-0.014\\) which is nearly identical to the conditional logit estimate and a standard deviation of \\(0.005\\) which is about one-third of the value of the mean. This suggests that the coefficient is mildly heterogenous among travelers. An interpretation of this random coefficient is that travelers have heterogeneous costs associated with travel time.\nIn Stata, mixed logit can be estimated by cmmixlogit."
  },
  {
    "objectID": "chpt26-multiple-choice.html#simple-multinomial-probit",
    "href": "chpt26-multiple-choice.html#simple-multinomial-probit",
    "title": "24  Multiple Choice",
    "section": "24.8 Simple Multinomial Probit",
    "text": "24.8 Simple Multinomial Probit\nThe simple multinomial probit and simple conditional multinomial probit models combine the latent utility model\n\\[\nU_{j}^{*}=W^{\\prime} \\beta_{j}+\\varepsilon_{j}\n\\]\nor\n\\[\nU_{j}^{*}=W^{\\prime} \\beta_{j}+X_{j}^{\\prime} \\gamma+\\varepsilon_{j}\n\\]\nwith the assumption that \\(\\varepsilon_{j}\\) is i.i.d. \\(\\mathrm{N}(0,1)\\). These are identical to the simple multinomial logit model of Section \\(26.3\\) and the conditional logit model of Section \\(26.4\\) except that the error distribution is normal instead of extreme value.\n\\({ }^{9}\\) If the random coefficient \\(\\eta\\) is scalar a computationally more efficient method is integration by quadrature. Simple multinomial probit does not precisely satisfy IIA but its properties are similar to IIA. The model assumes that the errors are independent and thus does not allow two alternatives, e.g. “red bus” and “blue bus”, to be close substitutes. This means that in practice the simple multinomial probit will produce results which are similar to simple multinomial logit.\nIdentification is identical to multinomial logit. The coefficients \\(\\beta_{j}\\) and \\(\\gamma\\) are only identified up to scale and the coefficients \\(\\beta_{j}\\) are only identified relative to a base alternative.\nThe response probability \\(P_{j}(W, X)\\) is not available in closed form. However, it can be expressed as a one-dimensional integral, as we now show.\nTheorem 26.3 In the simple multinomial probit and simple conditional multinomial probit models the response probabilities equal\n\\[\nP_{j}(W, X)=\\int_{-\\infty}^{\\infty} \\prod_{\\ell \\neq j} \\Phi\\left(W^{\\prime}\\left(\\beta_{j}-\\beta_{\\ell}\\right)+\\left(X_{j}-X_{\\ell}\\right)^{\\prime} \\gamma+v\\right) \\phi(v) d v\n\\]\nwhere \\(\\Phi(\\nu)\\) and \\(\\phi(\\nu)\\) are the normal distribution and density functions.\nThe proof is presented in Section 26.13. Theorem \\(26.3\\) shows that the response probability is a onedimensional normal integral over the \\(J-1\\)-fold product of normal distribution functions. This integral (26.18) is straightforward to numerically evaluate by quadrature methods.\nLet \\(\\theta=\\left(\\beta_{1}, \\ldots \\beta_{J}, \\gamma\\right)\\) denote the parameters. Given the sample \\(\\left\\{Y_{i}, W_{i}, X_{i}\\right\\}\\) the log-likelihood is\n\\[\n\\ell_{n}(\\theta)=\\sum_{i=1}^{n} \\sum_{j=1}^{J} \\mathbb{1}\\left\\{Y_{i}=j\\right\\} \\log P_{j}\\left(W_{i}, X_{i} \\mid \\theta\\right) .\n\\]\nThe maximum likelihood estimator (MLE) \\(\\widehat{\\theta}\\) maximizes \\(\\ell_{n}(\\theta)\\).\nTo illustrate, we estimate a simple conditional multinomial probit model for transportation choice using the same specification as before. The results are reported in the fourth column of Table 26.1. We report average marginal effects in Table \\(26.2\\). We see that the estimated AME are very close to those of the conditional logit model.\nIn Stata, simple multivariate probit can be estimated by mprobit. The response probabilities and loglikelihood are calculated by applying quadrature to the integral (26.18). Simple conditional multinomial probit can be estimated by cmmprobit. The latter uses the method of simulated maximum likelihood (discussed in the next section) even though numerical calculation could be implemented efficiently using the one-dimensional integral (26.18)."
  },
  {
    "objectID": "chpt26-multiple-choice.html#general-multinomial-probit",
    "href": "chpt26-multiple-choice.html#general-multinomial-probit",
    "title": "24  Multiple Choice",
    "section": "24.9 General Multinomial Probit",
    "text": "24.9 General Multinomial Probit\nA model which avoids the correlation constraints of multinomial and nested logit is general multinomial probit, which is (26.17) with the error vector \\(\\varepsilon \\sim \\mathrm{N}(0, \\Sigma)\\) and unconstrained \\(\\Sigma\\).\nIdentification of the coefficients is the same as multinomial logit. The coefficients \\(\\beta_{j}\\) and \\(\\gamma\\) are only identified up to scale, and the coefficients \\(\\beta_{j}\\) are only identified relative to a base alternative \\(J\\).\nIdentification of the covariance matrix \\(\\Sigma\\) requires more attention. It turns out to be useful to rewrite the model in terms of differenced utility, where differences are taken with respect to the base alternative \\(J\\). The differenced utilities are\n\\[\nU_{j}^{*}-U_{J}^{*}=W^{\\prime}\\left(\\beta_{j}-\\beta_{J}\\right)+\\left(X_{j}-X_{J}\\right)^{\\prime} \\gamma+\\varepsilon_{j J}\n\\]\nwhere \\(\\varepsilon_{j J}=\\varepsilon_{j}-\\varepsilon_{J}\\). Let \\(\\Sigma_{J}\\) be the covariance matrix of \\(\\varepsilon_{j J}\\) for \\(j=1, \\ldots, J-1\\). For example, suppose that the errors \\(\\varepsilon_{j}\\) are i.i.d. \\(N(0,1)\\). In this case \\(\\Sigma_{J}\\) equals\n\\[\n\\Sigma_{J}=\\left[\\begin{array}{cccc}\n2 & 1 & \\cdots & 1 \\\\\n1 & 2 & \\cdots & 1 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n1 & 1 & \\cdots & 2\n\\end{array}\\right] .\n\\]\nThe scale of (26.19) is not identified so \\(\\Sigma_{J}\\) is normalizing by fixing one diagonal element of \\(\\Sigma_{J}\\). In Stata, for example, cmmprobit normalizes the variance of one element - the “scale alternative” - to 2 , in order to match the case (26.20). Consequently, \\(\\Sigma_{J}\\) has \\((J-1) J / 2-1\\) free covariance parameters.\nMultinomial probit with a general covariance matrix \\(\\Sigma_{J}\\) is more flexible than conditional logit and nested logit. This flexibility allows general multinomial probit to escape the IIA restrictions.\nThe response probabilities do not have a closed-form expressions but can be written as \\(J-1\\) dimensional integrals. Numerical evaluation of integrals in dimensions three and greater is computationally prohibitive. A feasible alternative is numerical simulation. The idea, roughly, is to simulate a large number of random draws from the model and count the frequency which satisfy the desired inequality. This gives a simulation estimate of the response probability. Brute force implementation of this idea can be inefficient, so clever tricks have been introduced to produce computationally efficient estimates. The standard implementation was developed in a series of papers by Geweke, Hajivassiliou, and Keane, and is known as the GHK simulator. See Train (2009) for a description and references. The GHK simulator provides a feasible method to estimate the likelihood function and is known as simulated maximum likelihood. While feasible, simulated maximum likelihood is computationally intensive so optimizating the likelihood to find the MLE is computationally slow. Furthermore the likelihood is not concave in the parameters so convergence can be difficult to obtain in some applications. Consequently it may be prudent to use simpler methods such as conditional and nested logit for exploratory analysis and multinomial probit for final-stage estimation.\nTo illustrate, we estimate the general multinomial probit model for the transportation application. We set the base alternative to train and the scale alternative to air. The coefficient estimates are reported in Table \\(26.1\\) and marginal effects in Table \\(26.2\\). We see that the estimated marginal effects with respect to cost and travel time are considerably smaller than in the conditional logit model. This indicates greatly reduced price elasticity \\((-0.3)\\) and travel time elasticity \\((-1.1)\\). Suppose (as we considered in Section 26.4) that high-speed rail reduces train travel time by \\(33 %\\). The multinomial probit estimates imply that this increases train travel from \\(17 %\\) to \\(24 %\\) - about a \\(40 %\\) increase. This is substantial but one-half of the increase estimated by conditional logit.\nA multinomial probit model with four alternatives has five covariance parameters. The estimates for the transportation application are reported in the following \\(3 \\times 3\\) table. The diagonal elements are the variance estimates, the off-diagonal elements are the correlation estimates. One interesting finding is that the estimated correlation between air and car travel is \\(0.99\\), which is similar to the estimate from the nested logit model. In both frameworks the estimates indicate a high correlation between air and car travel, implying that specifications with independent errors are misspecified.\n\nIn Stata, multivariate probit can be estimated by cmmprobit. It uses GHK simulated maximum likelihood as described above."
  },
  {
    "objectID": "chpt26-multiple-choice.html#ordered-response",
    "href": "chpt26-multiple-choice.html#ordered-response",
    "title": "24  Multiple Choice",
    "section": "24.10 Ordered Response",
    "text": "24.10 Ordered Response\nA multinomial \\(Y\\) is ordered if the alternatives have ordinal (ordered) interpretation. For example, a student may be asked to “rate your [econometrics] professor” with possible responses: poor, fair, average, good, or excellent, coded as \\(\\{1,2,3,4,5\\}\\). These responses are categorical but are also ordinally related. We could use standard multinomial methods (e.g. multinomial logit or probit) but this ignores the ordinal structure and is therefore inefficient.\nThe standard approach to ordered response is based on the latent variable framework\n\\[\n\\begin{aligned}\nU^{*} &=X^{\\prime} \\beta+\\varepsilon \\\\\n\\varepsilon & \\sim G\n\\end{aligned}\n\\]\nwhere \\(X\\) does not include an intercept. The model specifies that the response \\(Y\\) is determined by \\(U^{*}\\) crossing a series of ordered thresholds \\(\\alpha_{1}<\\alpha_{2}<\\cdots<\\alpha_{J-1}\\). Thus\n\\[\n\\begin{array}{ccc}\nY=1 & \\text { if } & U^{*} \\leq \\alpha_{1} \\\\\nY=2 & \\text { if } & \\alpha_{1}<U^{*} \\leq \\alpha_{2} \\\\\n\\vdots & \\vdots & \\vdots \\\\\nY=J-1 & \\text { if } & \\alpha_{J-2}<U^{*} \\leq \\alpha_{J-1} \\\\\nY=J & \\text { if } & \\alpha_{J-1}<U^{*} .\n\\end{array}\n\\]\nWriting \\(\\alpha_{0}=-\\infty\\) and \\(\\alpha_{J}=\\infty\\) we can write these \\(J\\) equations more compactly as \\(Y=j\\) if \\(\\alpha_{j-1}<U^{*} \\leq \\alpha_{j}\\). When \\(J=2\\) this model specializes to binary choice.\nThe standard interpretation is that \\(U^{*}\\) is a latent continuous response and \\(Y\\) is a discretized version. Consider again the example of “rate your professor”. In the model, \\(U^{*}\\) is a student’s true assessment. The response \\(Y\\) is a discretized version. The threshold crossing model postulates that responses are increasing in the latent variable and are determined by the thresholds.\nIn the standard ordered response framework the distribution \\(G(x)\\) of the error \\(\\varepsilon\\) is assumed known; in practice either the normal or logistic distribution is used. When \\(\\varepsilon\\) is normal the model is called ordered probit. When \\(\\varepsilon\\) is logistic the model is called ordered logit. The coefficients and thresholds are only identified up to scale; the standard normalization is to fix the scale of the distribution of \\(\\varepsilon\\).\nThe response probabilities are\n\\[\n\\begin{aligned}\nP_{j}(x) &=\\mathbb{P}[Y=j \\mid X=x] \\\\\n&=\\mathbb{P}\\left[\\alpha_{j-1}<U^{*} \\leq \\alpha_{j} \\mid X=x\\right] \\\\\n&=\\mathbb{P}\\left[\\alpha_{j-1}-X^{\\prime} \\beta<\\varepsilon \\leq \\alpha_{j}-X^{\\prime} \\beta \\mid X=x\\right] \\\\\n&=G\\left(\\alpha_{j}-x^{\\prime} \\beta\\right)-G\\left(\\alpha_{j-1}-x^{\\prime} \\beta\\right) .\n\\end{aligned}\n\\]\nIt may be easier to interpret the cumulative response probabilities\n\\[\n\\mathbb{P}[Y \\leq j \\mid X=x]=G\\left(\\alpha_{j}-x^{\\prime} \\beta\\right) .\n\\]\nThe marginal effects are\n\\[\n\\frac{\\partial}{\\partial x} P_{j}(x)=\\beta\\left(g\\left(\\alpha_{j-1}-x^{\\prime} \\beta\\right)-g\\left(\\alpha_{j}-x^{\\prime} \\beta\\right)\\right)\n\\]\nand marginal cumulative effects are\n\\[\n\\frac{\\partial}{\\partial x} \\mathbb{P}[Y \\leq j \\mid X=x]=-\\beta g\\left(\\alpha_{j}-x^{\\prime} \\beta\\right) .\n\\]\nTo illustrate, Figure \\(26.3\\) displays how the response probabilities are determined. The figure plots the distribution function of latent utility \\(U^{*}\\) with four thresholds \\(\\alpha_{1}, \\alpha_{2}, \\alpha_{3}\\) and \\(\\alpha_{4}\\) displayed on the xaxis. The response \\(Y\\) is determined by \\(U^{*}\\) crossing each threshold. Each threshold is mapped to a point on the \\(y\\)-axis. The probability of each outcome is marked on the y-axis as the difference between each probability crossing.\n\nFigure 26.3: Ordered Choice\nThe parameters are \\(\\theta=\\left(\\beta, \\alpha_{1}, \\ldots \\alpha_{J-1}\\right)\\). Given the sample \\(\\left\\{Y_{i}, X_{i}\\right\\}\\) the log-likelihood is\n\\[\n\\ell_{n}(\\theta)=\\sum_{i=1}^{n} \\sum_{j=1}^{J} \\mathbb{1}\\left\\{Y_{i}=j\\right\\} \\log P_{j}\\left(X_{i} \\mid \\theta\\right) .\n\\]\nThe maximum likelihood estimator (MLE) \\(\\widehat{\\theta}\\) maximizes \\(\\ell_{n}(\\theta)\\).\nIn Stata, ordered probit and logit can be estimated by oprobit and ologit."
  },
  {
    "objectID": "chpt26-multiple-choice.html#count-data",
    "href": "chpt26-multiple-choice.html#count-data",
    "title": "24  Multiple Choice",
    "section": "24.11 Count Data",
    "text": "24.11 Count Data\nCount data refers to situations where the dependent variable is the number of “events” recorded as positive integers \\(Y \\in\\{0,1,2, \\ldots\\}\\). Examples include the number of doctor visits, the number of accidents, the number of patent registrations, the number of absences, or the number of bank failures. Count data models are typically employed in contexts where the counts are small integers.\nA count data model specifies the response probabilities \\(P_{j}(x)=\\mathbb{P}[Y=j \\mid x]\\) for \\(j=0,1,2, \\ldots\\), with the property \\(\\sum_{j=0}^{\\infty} P_{j}(x)=1\\). The baseline model is Poisson regression. This model specifies that \\(Y\\) is conditionally Poisson distributed with a Poisson parameter \\(\\lambda\\) written as an exponential link of a linear function of the regressors. The exponential link is used to ensure that the Poisson parameter is strictly positive. This model is\n\\[\n\\begin{aligned}\nP_{j}(x) &=\\frac{\\exp (-\\lambda(x)) \\lambda(x)^{j}}{j !} \\\\\n\\lambda(x) &=\\exp \\left(x^{\\prime} \\beta\\right) .\n\\end{aligned}\n\\]\nThe Poisson distribution has the property that its mean and variance equal the Poisson parameter \\(\\lambda\\). Thus\n\\[\n\\begin{aligned}\n\\mathbb{E}[Y \\mid X] &=\\exp \\left(X^{\\prime} \\beta\\right) \\\\\n\\operatorname{var}[Y \\mid X] &=\\exp \\left(X^{\\prime} \\beta\\right) .\n\\end{aligned}\n\\]\nThe first equation shows that the conditional expectation (e.g., the regression function) equals \\(\\exp \\left(X^{\\prime} \\beta\\right)\\). This is why the model is called Poisson regression.\nThe log-likelihood function is\n\\[\n\\ell_{n}(\\beta)=\\sum_{i=1}^{n} \\log P_{Y_{i}}\\left(X_{i} \\mid \\beta\\right)=\\sum_{i=1}^{n}\\left(-\\exp \\left(X_{i}^{\\prime} \\beta\\right)+Y_{i} X_{i}^{\\prime} \\beta-\\log \\left(Y_{i} !\\right)\\right)\n\\]\nThe MLE \\(\\widehat{\\beta}\\) is the value \\(\\beta\\) which maximizes \\(\\ell_{n}(\\beta)\\). Its first and second derivatives are\n\\[\n\\begin{aligned}\n\\frac{\\partial}{\\partial \\beta} \\ell_{n}(\\beta) &=\\sum_{i=1}^{n} X_{i}\\left(Y_{i}-\\exp \\left(X_{i}^{\\prime} \\beta\\right)\\right) \\\\\n\\frac{\\partial^{2}}{\\partial \\beta \\partial \\beta^{\\prime}} \\ell_{n}(\\beta) &=-\\sum_{i=1}^{n} X_{i} X_{i}^{\\prime} \\exp \\left(X_{i}^{\\prime} \\beta\\right)\n\\end{aligned}\n\\]\nSince the second derivative is globally negative definite the log-likelihood function is globally concave. Hence numerical optimization to find the MLE is computationally straightforward.\nIn general there is no reason to expect the Poisson model to be correctly specified. Hence we should view the parameter \\(\\beta\\) as the best-fitting pseudo-true value. From the first-order condition for maximization we find that this value satisfies\n\\[\n\\mathbb{E}\\left[X\\left(Y-\\exp \\left(X^{\\prime} \\beta\\right)\\right)\\right]=0 .\n\\]\nThis holds under the conditional expectation assumption \\(\\mathbb{E}[Y \\mid X]=\\exp \\left(X^{\\prime} \\beta\\right)\\). If the latter is correctly specified, Poisson regression correctly identifies the coeffiicent \\(\\beta\\), the MLE is consistent for this value, and the estimated response probabilities are consistent for the true response probabilities.\nTo explore this concept further, suppose the true CEF is nonparametric. Since it is non-negative we can write it using an exponential link \\({ }^{10}\\) as \\(\\mathbb{E}[Y \\mid X]=\\exp (m(x))\\). The function \\(m(x)\\) is nonparametrically identified and can be approximated by a series \\(x_{K}^{\\prime} \\beta_{K}\\). Thus \\(\\mathbb{E}[Y \\mid X] \\simeq \\exp \\left(X_{K}^{\\prime} \\beta_{K}\\right)\\). What this shows is that if Poisson regression is implemented using a flexible set of regressors (as in series regression) the model will approximate the true CEF and hence will consistently estimate the true response probabilities. This is a broad justification for Poisson regression in count data applications if suitable attention is paid to the functional form for the included regressors.\nSince the model is an approximation, however, the conventional covariance matrix estimator will be inconsistent. Consequently it is advised to use the robust formula for covariance matrix and standard error estimation.\n\\({ }^{10} \\mathrm{Or}\\), equivalently, \\(m(x)=\\log (\\mathbb{E}[Y \\mid X])\\). For a greater degree of flexibility the Poisson model can be generalized. One approach, similar to mixed logit, is to treat the parameters as random variables, thereby obtaining a mixed probit model. One particular mixed model of importance is the negative binomial model which can be obtained as a mixed model as follows. Specify the Poisson parameter as \\(\\lambda(X)=V \\exp \\left(X^{\\prime} \\beta\\right)\\) where \\(V\\) is a random variable with a Gamma distribution. This is equivalent to treating the regression intercept as random with a logGamma distribution. Integrating out \\(V\\), the resulting conditional distribution for \\(Y\\) is Negative Binomial. The Negative Binomial is a popular model for count data regression and has the advantage that the CEF and variance are separately varying.\nFor more detail see the excellent monograph on count data models by Cameron and Trivedi (1998).\nIn Stata, Poisson and Negative Binomial regression can be estimated by poisson and nbreg. Generalizations to allow truncation, fixed effects, and random effects are also available."
  },
  {
    "objectID": "chpt26-multiple-choice.html#blp-demand-model",
    "href": "chpt26-multiple-choice.html#blp-demand-model",
    "title": "24  Multiple Choice",
    "section": "24.12 BLP Demand Model",
    "text": "24.12 BLP Demand Model\nA major development in the 1990s was the extension of conditional logit to models of aggregate market demand. Many of the ideas were developed in the seminal papers of Berry (1994) and Berry, Levinsohn, and Pakes (1995). For a review see Ackerberg, Benkard, Berry, and Pakes (2007). This model widely known as the BLP model - has become popular in applied industrial organization. To discuss implementation we use as examples the applications in Berry, Levinsohn, and Pakes (1995) and Nevo (2001).\nThe context is market-level observations. A “market” is typically a time period matched with a location. For example, a market in Berry, Levinsohn, and Pakes (1995) is the United States for one calendar year. A market in Nevo (2001) is one of 65 U.S. cities for one quarter of a year. An observation contains a set of \\(J\\) goods. In Berry, Levinsohn, and Pakes (1995) the goods are 997 distinct automobile models. In Nevo (2001) the goods are 25 ready-to-eat breakfast cereals. Observations typically include the price and sale quantities of each good, a set of characteristics of each good, and possibly information on demographic characteristics of the market population.\nThe model is derived from a conditional logit specification of individual behavior. The standard assumption is that each individual in the market purchases one of the \\(J\\) goods or makes no purchase (the latter is called the outside alternative). This requires taking a stand on the number of individuals in the market. For example, in Berry, Levinsohn, and Pakes (1995) the number of individuals is the entire U.S. population. Their assumption is that each individual makes at most one automobile purchase during each calendar year. In Nevo (2001) the population is the number of individuals in each city. He assumes that each individual purchases a one-quarter (91-day) supply of one brand of breakfast cereal, or purchases no breakfast cereal (the outside alternative). By explicitly including the outside option as a choice these authors model aggregate demand. Alternatively, they could have excluded the outside option and examined choice among the \\(J\\) goods. This would have modelled market shares (percentages of total purchases) but not aggregate demand. The trade-off is the need to take a stand on the number of individuals in the market.\nThe model is that each individual purchases one of a set of \\(J\\) goods indexed \\(j=1, \\ldots, J\\) or an unobserved outside good. The utility from good \\(j\\) takes a mixed logit form:\n\\[\nU_{j}^{*}=X_{j}^{\\prime} \\eta+\\xi_{j}+\\varepsilon_{j}\n\\]\nwhere \\(X_{j}\\) includes the price and characteristics of good \\(j\\). The coefficient \\(\\eta\\) is random (specific to an individual) as in the mixed logit model. The variables \\(\\xi_{j}\\) and \\(\\varepsilon_{j}\\) are unobserved errors. \\(\\xi_{j}\\) is market-level and \\(\\varepsilon_{j}\\) is specific to the individual. The market error \\(\\xi_{j}\\) may contain unobserved product characteristics so is likely correlated with product price. Identification requires a vector of instruments \\(Z_{j}\\) which satisfy\n\\[\n\\mathbb{E}\\left[Z_{j} \\xi_{j}\\right]=0 .\n\\]\nBerry, Levinsohn, and Pakes (1995) recommend as instruments the non-price characteristics in \\(X_{j}\\), the sum of characteristics of goods sold by the same firm, and the sum of characteristics of goods sold by other firms. Nevo (2001) also includes the prices of goods in other markets, which is valid if demand shocks are uncorrelated across markets. There is considerable attention in the literature given to the choice and construction of instruments.\nWrite \\(\\gamma=\\mathbb{E}[\\eta], V=\\eta-\\gamma\\), and assume that \\(V\\) has distribution \\(F(V \\mid \\alpha)\\) with parameters \\(\\alpha\\) (typically \\(\\mathrm{N}(0, \\Sigma))\\). Set\n\\[\n\\delta_{j}=X_{j}^{\\prime} \\gamma+\\xi_{j} .\n\\]\nSince the model is mixed logit, (26.14) shows that the response probabilities given \\(\\delta=\\left(\\delta_{1}, \\ldots, \\delta_{J}\\right)\\) are\n\\[\nP_{j}(\\delta, \\alpha)=\\int \\frac{\\exp \\left(\\delta_{j}+X_{j}^{\\prime} V\\right)}{\\sum_{\\ell=1}^{J} \\exp \\left(\\delta_{\\ell}+X_{\\ell}^{\\prime} V\\right)} d F(V \\mid \\alpha) d V .\n\\]\nAs discussed in Section \\(26.7\\) the integral in (26.14) is typically evaluated by numerical simulation. Let \\(\\left\\{V_{1}, \\ldots, V_{G}\\right\\}\\) be i.i.d. pseudo-random draws from \\(F(V \\mid \\alpha)\\). The simulation estimator is\n\\[\n\\widetilde{P}_{j}(\\delta, \\alpha)=\\frac{1}{G} \\sum_{g=1}^{G} \\frac{\\exp \\left(\\delta_{j}+X_{j}^{\\prime} V_{g}\\right)}{\\sum_{\\ell=1}^{J} \\exp \\left(\\delta_{\\ell}+X_{\\ell}^{\\prime} V_{g}\\right)} .\n\\]\nIn each market we observe the quantity purchased \\(Q_{j}\\) of each good and we are assumed to know the number of individuals \\(M\\). The market share of good \\(j\\) is defined as \\(S_{j}=Q_{j} / M\\) which is a direct estimate of the probability \\(P_{j}\\). If the number of individuals \\(M\\) is large then \\(S_{j}\\) approximately equals \\(P_{j}\\) by the WLLN. The BLP approach assumes that \\(M\\) is large enough that we can treat these two as equal. This implies the set of \\(J\\) equalities\n\\[\nS_{j}=\\widetilde{P}_{j}(\\delta, \\alpha)\n\\]\nwhere \\(S=\\left(S_{1}, \\ldots, S_{J}\\right)\\). The left side of (26.25) is the observed market share of good \\(j\\) (that is, the ratio of sales to individuals in the market). The right side is the estimated probability that the good is selected given the market attributes and parameters. As there are \\(J\\) elements in each of \\(\\delta\\) and \\(S\\) (and \\(\\widetilde{P}_{j}(\\delta, \\alpha)\\) is monotonically increasing in each element of \\(\\delta\\) ) there is a one-to-one and invertible mapping between \\(\\delta\\) and \\(S\\). Thus given the market shares \\(S\\) and parameters \\(\\alpha\\) we can numerically calculate the elements \\(\\delta\\) which solve the \\(J\\) equations (26.25). Berry, Levinsohn, and Pakes (1995) show that the solution can be obtained by iterating on\n\\[\n\\delta_{j}^{i}=\\delta_{j}^{i-1}+\\log S_{j}-\\log \\widetilde{P}_{j}\\left(\\delta^{i-1}, \\alpha\\right) .\n\\]\nThe solution is an implicit set of \\(J\\) equations \\(\\delta_{j}=\\delta_{j}(S, \\alpha)\\).\nWe combine \\(\\delta_{j}=\\delta_{j}(S, \\alpha)\\) with (26.23) to obtain the regression-like expression \\(\\delta_{j}(S, \\alpha)=X_{j}^{\\prime} \\gamma+\\xi_{j}\\). Combined with (26.22) we obtain the moment equations\n\\[\n\\mathbb{E}\\left[Z_{j}\\left(\\delta_{j}(S, \\alpha)-X_{j}^{\\prime} \\gamma\\right)\\right]=0\n\\]\nfor \\(j=1, \\ldots, J\\). Estimation is by nonlinear GMM. The observations are markets indexed \\(t=1, \\ldots, T\\), including quantities \\(Q_{j t}\\), prices and characteristics \\(X_{j t}\\), and instruments \\(Z_{j t}\\). Market shares are \\(S_{j t}=Q_{j t} / M_{t}\\), where \\(M_{t}\\) is the number of individuals in the market. Let \\(S_{t}=\\left(S_{1 t}, \\ldots, S_{J t}\\right)\\). The moment equation is\n\\[\n\\bar{g}(\\gamma, \\alpha)=\\frac{1}{T J} \\sum_{t=1}^{T} \\sum_{j=1}^{J} Z_{j t}\\left(\\delta_{j t}\\left(S_{t}, \\alpha\\right)-X_{j t}^{\\prime} \\gamma\\right) .\n\\]\nThe GMM estimator \\((\\widehat{\\gamma}, \\widehat{\\alpha})\\) minimizes the criterion \\(\\bar{g}(\\gamma, \\alpha)^{\\prime} \\boldsymbol{W} \\bar{g}(\\gamma, \\alpha)\\) for a weight matrix \\(\\boldsymbol{W}\\).\nWe mentioned earlier that observations may include demographic information. This can be incorporated as follows. We can add individual characteristics (e.g. income) to the utility model (26.21) as interactions with the product characteristics \\(X_{j}\\). Since individual characteristics are unobserved they can be treated as random but with a known distribution (taken from the known market-level demographic data). For example, Berry, Levinsohn, and Pakes (1995) treat individual income as log-normally distributed. These random variables are then treated jointly with the random coefficients with no effective change in the estimation method.\nAn asymptotic theory developed by Berry, Linton, and Pakes (2004) shows that this GMM estimator is consistent and asymptotically normal as \\(J \\rightarrow \\infty\\) under certain assumptions. This means that the estimator can be applied in contexts with small \\(T\\) and large \\(J\\), as well as in contexts with large \\(T\\).\nTo estimate a BLP model in Stata there is an add-on command blp. In R there is a package BLPestimatoR. In Python there is a package PyBLP."
  },
  {
    "objectID": "chpt26-multiple-choice.html#technical-proofs",
    "href": "chpt26-multiple-choice.html#technical-proofs",
    "title": "24  Multiple Choice",
    "section": "24.13 Technical Proofs*",
    "text": "24.13 Technical Proofs*\nProof of Theorem 26.1: Define \\(\\mu_{j \\ell}=X^{\\prime}\\left(\\beta_{j}-\\beta_{\\ell}\\right)\\). It will useful to observe that\n\\[\nP_{j}(X)=\\frac{\\exp \\left(X^{\\prime} \\beta_{j} / \\tau\\right)}{\\sum_{\\ell=1}^{J} \\exp \\left(X^{\\prime} \\beta_{\\ell} / \\tau\\right)}=\\left(\\sum_{\\ell=1}^{J} \\exp \\left(-\\frac{\\mu_{j \\ell}}{\\tau}\\right)\\right)^{-1} .\n\\]\nDefine\n\\[\n\\begin{aligned}\nF_{j}\\left(\\varepsilon_{1}, \\ldots, \\varepsilon_{J}\\right) &=\\frac{\\partial}{\\partial \\varepsilon_{j}} F\\left(\\varepsilon_{1}, \\ldots, \\varepsilon_{J}\\right) \\\\\n&=\\exp \\left(-\\left[\\sum_{\\ell=1}^{J} \\exp \\left(-\\frac{\\varepsilon_{\\ell}}{\\tau}\\right)\\right]^{\\tau}\\right)\\left[\\sum_{\\ell=1}^{J} \\exp \\left(-\\frac{\\varepsilon_{\\ell}}{\\tau}\\right)\\right]^{\\tau-1} \\exp \\left(-\\frac{\\varepsilon_{j}}{\\tau}\\right)\n\\end{aligned}\n\\]\nThe event \\(Y=j\\) occurs if \\(U_{j}^{*} \\geq U_{\\ell}^{*}\\) for all \\(\\ell\\), which occurs when \\(\\varepsilon_{\\ell} \\leq \\varepsilon_{j}+\\mu_{j \\ell}\\). The probability \\(\\mathbb{P}[Y=j]\\) is the integral of the joint density \\(f\\left(\\varepsilon_{1}, \\ldots, \\varepsilon_{J}\\right)\\) over the region \\(\\varepsilon_{\\ell} \\leq \\varepsilon_{j}+\\mu_{j \\ell}\\). This is\n\\[\n\\mathbb{P}[Y=j]=\\mathbb{P}\\left[\\varepsilon_{\\ell} \\leq \\varepsilon_{j}+\\mu_{j \\ell}, \\text { all } \\ell\\right]=\\int_{-\\infty}^{\\infty}\\left[\\int_{-\\infty}^{\\varepsilon_{j}+\\mu_{j 1}} \\cdots \\int_{-\\infty}^{\\varepsilon_{J}+\\mu_{j J}} f\\left(\\varepsilon_{1}, \\ldots, \\varepsilon_{J}\\right) d \\varepsilon_{1} d \\varepsilon_{2} \\cdots d \\varepsilon_{J}\\right] d \\varepsilon_{j}\n\\]\nwhere the outer integral is over \\(\\varepsilon_{j}\\). The \\(J-1\\) inner set of integrals equals \\(F_{j}\\left(\\varepsilon_{j}+\\mu_{j 1}, \\ldots, \\varepsilon_{j}+\\mu_{j J}\\right)\\). Thus\n\\[\n\\mathbb{P}[Y=j]=\\int_{-\\infty}^{\\infty} F_{j}\\left(\\varepsilon_{j}+\\mu_{j 1}, \\ldots, \\varepsilon_{j}+\\mu_{j J}\\right) d \\varepsilon_{j} .\n\\]\nNext, we substute the above expression for \\(F_{j}\\) and collect terms to find that (26.27) equals\n\\[\n\\begin{aligned}\n&\\int_{-\\infty}^{\\infty} \\exp \\left(-\\left[\\sum_{\\ell=1}^{J} \\exp \\left(-\\frac{\\varepsilon_{\\ell}+\\mu_{j \\ell}}{\\tau}\\right)\\right]^{\\tau}\\right)\\left[\\sum_{\\ell=1}^{J} \\exp \\left(-\\frac{\\varepsilon_{\\ell}+\\mu_{j \\ell}}{\\tau}\\right)\\right]^{\\tau-1} \\exp \\left(-\\frac{\\varepsilon_{j}}{\\tau}\\right) d \\varepsilon_{j} \\\\\n&=\\int_{-\\infty}^{\\infty} \\exp \\left(-\\exp \\left(-\\varepsilon_{j}\\right) P_{j}(X)^{-\\tau}\\right) P_{j}(X)^{1-\\tau} \\exp \\left(-\\frac{\\varepsilon_{j}}{\\tau}\\right)^{\\tau-1} \\exp \\left(-\\frac{\\varepsilon_{j}}{\\tau}\\right) d \\varepsilon_{j} \\\\\n&=\\int_{-\\infty}^{\\infty} \\exp \\left(-\\exp \\left(-\\varepsilon_{j}-\\log P_{j}(X)^{\\tau}\\right)\\right) P_{j}(X)^{1-\\tau} \\exp \\left(-\\varepsilon_{j}\\right) d \\varepsilon_{j} \\\\\n&=P_{j}(X)^{1-\\tau} \\int_{-\\infty}^{\\infty} \\exp \\left(-\\exp \\left(-\\varepsilon_{j}-\\log P_{j}(X)^{\\tau}\\right)\\right) \\exp \\left(-\\varepsilon_{j}\\right) d \\varepsilon_{j} \\\\\n&=P_{j}(X) \\int_{-\\infty}^{\\infty} \\exp (-\\exp (-u)) \\exp (-u) d u \\\\\n&=P_{j}(X)\n\\end{aligned}\n\\]\nThe second-to-last equality makes the change of variables \\(u=\\varepsilon_{j}+\\log P_{j}(X)^{\\tau}\\). The final uses the fact that \\(\\exp (-\\exp (-u)) \\exp (-u)\\) is the Type I extreme value density which integrates to one. This shows \\(\\mathbb{P}[Y=j]=P_{j}(X)\\), as claimed.\nProof of Theorem 26.2: The proof method is similar to that of Theorem 26.1. The joint distribution of the errors is\n\\[\nF\\left(\\varepsilon_{11}, \\ldots, \\varepsilon_{J K_{J}}\\right)=\\exp \\left(-\\sum_{\\ell=1}^{J}\\left[\\sum_{m=1}^{K_{\\ell}} \\exp \\left(-\\frac{\\varepsilon_{\\ell m}}{\\tau_{\\ell}}\\right)\\right]^{\\tau_{\\ell}}\\right) .\n\\]\nThe derivative with respect to \\(\\varepsilon_{j k}\\) is\n\\[\n\\begin{aligned}\nF_{j k}\\left(\\varepsilon_{11}, \\ldots, \\varepsilon_{J K_{J}}\\right) &=\\frac{\\partial}{\\partial \\varepsilon_{j k}} F\\left(\\varepsilon_{11}, \\ldots, \\varepsilon_{J K_{J}}\\right) \\\\\n&=\\exp \\left(-\\sum_{\\ell=1}^{J}\\left[\\sum_{m=1}^{K_{\\ell}} \\exp \\left(-\\frac{\\varepsilon_{\\ell m}}{\\tau_{\\ell}}\\right)\\right]^{\\tau_{\\ell}}\\right)\\left[\\sum_{m=1}^{K_{j}} \\exp \\left(-\\frac{\\varepsilon_{j m}}{\\tau_{j}}\\right)\\right]^{\\tau_{j}-1} \\exp \\left(-\\frac{\\varepsilon_{j k}}{\\tau_{j}}\\right) .\n\\end{aligned}\n\\]\nThe event \\(Y_{j k}=1\\) occurs if \\(U_{j k}^{*} \\geq U_{\\ell m}^{*}\\) for all \\(\\ell\\) and \\(m\\), which occurs when \\(\\varepsilon_{\\ell m} \\leq \\varepsilon_{j k}+\\mu_{j k}-\\mu_{l m}\\). Setting \\(I_{j}=\\sum_{m=1}^{K_{j}} \\exp \\left(\\mu_{j m} / \\tau_{j}\\right)\\) and \\(I=\\sum_{\\ell=1}^{J} I_{\\ell}^{\\tau_{\\ell}}\\) we find that\n\\[\n\\begin{aligned}\n\\mathbb{P}\\left[Y_{j k}=1\\right] &=\\int_{-\\infty}^{\\infty} F_{j k}\\left(v+\\mu_{j k}-\\mu_{11}, \\ldots, v+\\mu_{j k}-\\mu_{J K_{J}}\\right) d v \\\\\n&=\\int_{-\\infty}^{\\infty} \\exp \\left(-\\sum_{\\ell=1}^{J}\\left[\\sum_{m=1}^{K_{\\ell}} \\exp \\left(-\\frac{v+\\mu_{j k}-\\mu_{\\ell m}}{\\tau_{\\ell}}\\right)\\right]^{\\tau_{\\ell}}\\right)\\left[\\sum_{m=1}^{K_{j}} \\exp \\left(-\\frac{v+\\mu_{j k}-\\mu_{j m}}{\\tau_{j}}\\right)\\right]^{\\tau_{j}-1} \\exp \\left(-\\frac{v}{\\tau_{j}}\\right) d v \\\\\n&=I_{j}^{\\tau_{j}-1}\\left(\\exp \\left(-\\mu_{j k}\\right)\\right)^{\\frac{\\tau_{j}-1}{\\tau_{j}}} \\int_{-\\infty}^{\\infty} \\exp \\left(-\\exp \\left(-v-\\mu_{j k}\\right) \\sum_{\\ell=1}^{J} I_{\\ell}^{\\tau_{\\ell}}\\right) \\exp (-v) d v \\\\\n&=\\frac{\\exp \\left(\\mu_{j k} / \\tau_{j}\\right) I_{j}^{\\tau_{j}-1}}{I} \\int_{-\\infty}^{\\infty} \\exp \\left(-\\exp \\left(-v-\\mu_{j k}+\\log I\\right)\\right) \\exp \\left(-v-\\mu_{j k}+\\log I\\right) d v \\\\\n&=\\frac{\\exp \\left(\\mu_{j k} / \\tau_{j}\\right) I_{j}^{\\tau_{j}-1}}{I}=P_{k \\mid j} P_{j}\n\\end{aligned}\n\\]\nas claimed. Proof of Theorem 26.3: We follow the proof of Theorem \\(26.1\\) through (26.27), where in this case \\(\\mu_{j \\ell}=\\) \\(X^{\\prime}\\left(\\beta_{j}-\\beta_{\\ell}\\right)+\\left(Z_{j}-Z_{\\ell}\\right)^{\\prime} \\gamma\\) and\n\\[\nF_{j}\\left(\\varepsilon_{1}, \\ldots, \\varepsilon_{J}\\right)=\\frac{\\partial}{\\partial \\varepsilon_{j}} F\\left(\\varepsilon_{1}, \\ldots, \\varepsilon_{J}\\right)=\\prod_{\\ell \\neq j} \\Phi\\left(\\mu_{j \\ell}+\\varepsilon_{j}\\right) \\phi\\left(\\varepsilon_{j}\\right)\n\\]\nThus\n\\[\n\\mathbb{P}[Y=j]=\\int_{-\\infty}^{\\infty} \\prod_{\\ell \\neq j} \\Phi\\left(\\mu_{j \\ell}+v\\right) \\phi(v) d v\n\\]\nas claimed."
  },
  {
    "objectID": "chpt26-multiple-choice.html#exercises",
    "href": "chpt26-multiple-choice.html#exercises",
    "title": "24  Multiple Choice",
    "section": "24.14 Exercises",
    "text": "24.14 Exercises\nExercise 26.1 For the multinomial logit model (26.2) show that \\(0 \\leq P_{j}(x) \\leq 1\\) and \\(\\sum_{j=1}^{J} P_{j}(x)=1\\).\nExercise 26.2 Show that \\(P_{j}(x)\\) in the multinomial logit model (26.2) only depends on the coefficient differences \\(\\beta_{j}-\\beta_{J}\\).\nExercise 26.3 For the multinomial logit model (26.2) show that the marginal effects equal (26.4).\nExercise 26.4 Show that (26.8) holds for the conditional logit model.\nExercise 26.5 For the conditional logit model (26.8) show that the marginal effects are (26.9) and (26.10).\nExercise 26.6 Show that \\(P_{j}(w, x)\\) in the conditional logit model (26.8) only depends on the coefficient differences \\(\\beta_{j}-\\beta_{J}\\) and variable differences \\(x_{j}-x_{J}\\).\nExercise 26.7 In the conditional logit model find an estimator for \\(\\mathrm{AME}_{j j}\\).\nExercise 26.8 Show (26.11).\nExercise 26.9 In the conditional logit model with no alternative-invariant regressors \\(W\\) show that (26.11) implies \\(P_{j}(x) / P_{\\ell}(x)=\\exp \\left(\\left(x_{j}-x_{\\ell}\\right)^{\\prime} \\gamma\\right)\\).\nExercise 26.10 Take the nested logit model. If \\(k\\) and \\(\\ell\\) are alternatives in the same group \\(j\\), show that the ratio \\(P_{j k} / P_{j \\ell}\\) is independent of variables in the other groups. What does this mean?\nExercise 26.11 Take the nested logit model. For groups \\(j\\) and \\(\\ell\\), show that the ratio \\(P_{j} / P_{\\ell}\\) is independent of variables in the other groups. What does this mean?\nExercise 26.12 Use the cps09mar dataset and the subset of men. Estimate a multinomial logit model for marriage status similar to Figure \\(26.1\\) as a function of age. How do your findings compare with those for women?\nExercise 26.13 Use the cps09mar dataset and the subset of women with ages up to 35. Estimate a multinomial logit model for marriage status as linear functions of age and education. Interpret your results.\nExercise 26.14 Use the cps09mar dataset and the subset of women. Estimate a nested logit model for marriage status as a function of age. Describe how you decide on the grouping of alternatives. Exercise 26.15 Use the Koppelman dataset. Estimate conditional logit models similar to those reported in Table \\(26.1\\) but with the following modifications. For each case report the estimated coefficients and standard errors for the cost and time variables, the log-likelihood, and describe how the results change.\n\nReplicate the results of Table \\(26.1\\) for conditional logit with the same variables. Note: the regressors used in Table \\(26.1\\) are cost, intime, income, and urban.\nAdd the variable outtime, which is out-of-vehicle time.\nReplace intime with time=intime+outtime.\nReplace cost and intime with \\(\\log (\\cos t)\\) and \\(\\log (\\) intime \\()\\).\n\nExercise 26.16 Use the Koppelman dataset. Estimate a nested logit model similar to those reported in Table \\(26.1\\) but with the following modifications. For each case report the estimated coefficients and standard errors for the cost and time variables, the log-likelihood, and describe how the results change.\n\nReplicate the results of Table \\(26.1\\) for nested logit with the same variables. Note: You will need to constrain the dissimilarity parameter for \\(\\{\\) train, bus \\(\\}\\).\nReplace cost and intime with \\(\\log (\\cos t)\\) and \\(\\log (\\) intime \\()\\).\nUse the groupings \\(\\{\\) car \\(\\}\\) and \\(\\{\\) train, bus, air \\(\\}\\). Why (or why not) might this nesting make sense?\nUse the groupings {air} and {train, bus, car}.Why (or why not) might this nesting make sense?\n\nExercise 26.17 Use the Koppelman dataset. Estimate a mixed logit model similar to that reported in Table \\(26.1\\) but with the following modifications. For each case report the estimated coefficients and standard errors for the cost and time variables, the log-likelihood, and describe how the results change.\n\nReplicate the results of Table \\(26.1\\) for mixed logit with the same variables.\nReplace intime with time=intime+outtime .\nTreat the coefficient on intime as the negative of a lognormal random variable. (Replace intime with nintime =-intime and treat the coefficient as lognormally distributed.) How do you compare the results of the estimated models?\n\nExercise 26.18 Use the Koppelman dataset. Estimate a general multinomial probit model similar to that reported in Table \\(26.1\\) but with the following modifications. For each case report the estimated coefficients and standard errors for the cost and time variables, the log-likelihood, and describe how the results change.\n\nReplicate the results of Table \\(26.1\\) for multinomial probit with the same variables.\nReplace cost and intime with \\(\\log (\\) cost \\()\\) and \\(\\log (\\) intime \\()\\)."
  },
  {
    "objectID": "chpt27-censor-selection.html#introduction",
    "href": "chpt27-censor-selection.html#introduction",
    "title": "25  Censoring and Selection",
    "section": "25.1 Introduction",
    "text": "25.1 Introduction\nCensored regression occurs when the dependent variable is constrained, resulting in a pile-up of observations on a boundary. Selection occurs when sampling is endogenous. Under either censoring or selection, conventional (e.g. least squares) estimators are biased for the population parameters of the uncensored/unselected distributions. Methods have been developed to circumvent this bias, including the Tobit, CLAD, and sample selection estimators.\nFor more detail see Maddala (1983), Amemiya (1985), Gourieroux (2000), Cameron and Trivedi (2005), and Wooldridge (2010)."
  },
  {
    "objectID": "chpt27-censor-selection.html#censoring",
    "href": "chpt27-censor-selection.html#censoring",
    "title": "25  Censoring and Selection",
    "section": "25.2 Censoring",
    "text": "25.2 Censoring\nIt is common in economic applications for a dependent variable to have a mixed discrete/continuous distribution, where the discrete component is on the boundary of support. Most commonly this boundary occurs at 0. For example, Figure 27.1(a) displays the density of tabroad (transfers from abroad) from the data file \\(\\mathrm{CH} \\mathrm{J} 2004\\). This variable is the amount \\({ }^{1}\\) of remittances received by a Philippino household from a foreign source. For \\(80 %\\) of households this variable equals 0 . The associated mass point is displayed by the bar at zero. For \\(20 %\\) of households tabroad is positive and continuously distributed with a thick right tail. The associated density is displayed by the line graph.\nGiven such observations it is unclear how to proceed with a regression analysis. Should we use the full sample including the 0’s? Should we use only the sub-sample excluding the 0’s? Or should we do something else?\nTo answer these questions it is useful to have a statistical model. A classical framework is censored regression, which posits that the observed variable is a censored version of a latent continuouslydistributed variable. Without loss of generality we focus on the case of censoring from below at zero.\nThe censored regression model was proposed by Tobin (1958) to explain household consumption of durable goods. Tobin observed that in survey data, durable good consumption is zero for a positive fraction of households. He proposed treating the observations as censored realizations from a continuous\n\\({ }^{1}\\) In thousands of Philippino pesos.\n\n\nTransfers from Abroad\n\n\n\nCensoring Process\n\nFigure 27.1: Censored Densities\ndistribution. His model is\n\\[\n\\begin{aligned}\n&Y^{*}=X^{\\prime} \\beta+e \\\\\n&e \\mid X \\sim \\mathrm{N}\\left(0, \\sigma^{2}\\right) \\\\\n&Y=\\max \\left(Y^{*}, 0\\right) .\n\\end{aligned}\n\\]\nThis model is known as Tobit regression or censored regression. It is also known as the Type 1 Tobit model. The variable \\(Y^{*}\\) is latent (unobserved). The observed variable \\(Y\\) is censored from below at zero. This means that positive values are uncensored and negative values are transformed to 0 . This censoring model replicates the observed phenomenon of a pile-up of observations at 0 .\nThe Tobit model can be justified by a latent choice framework where an individual’s optimal (unconstrained) continuously distributed choice is \\(Y^{*}\\). Feasible choices, however, are constrained to satisfy \\(Y \\geq 0\\). (For example, negative purchases are not allowed.) Consequently the realized value \\(Y\\) is a censored version of \\(Y^{*}\\). To justify this interpretation of the model we need to envisage a context where desired choices include negative values. This may be a strained interpretation for consumption purchases, but may be reasonable when negative values make economic sense.\nThe censoring process is depicted in Figure 27.1(b). The latent variable \\(Y^{*}\\) has a normal density centered at \\(X^{\\prime} \\beta\\). The portion for \\(Y^{*}>0\\) is maintained while the portion for \\(Y^{*}<0\\) is transformed to a point mass at zero. The location of the density and the degree of censoring are controlled by the conditional mean \\(X^{\\prime} \\beta\\). As \\(X^{\\prime} \\beta\\) moves to the right the amount of censoring is decreased. As \\(X^{\\prime} \\beta\\) moves to the left the amount of censoring is increased.\nA common “remedy” to the censoring problem is deletion of the censored observations. This creates a truncated distribution which is defined by the following transformation\n\\[\nY^{\\#}=\\left\\{\\begin{array}{cc}\nY & \\text { if } Y>0 \\\\\n\\text { missing } & \\text { if } Y=0 .\n\\end{array}\\right.\n\\]\nIn Figure 27.1(a) and Figure 27.1(b) the truncated distribution is the continuous portion above 0 with the mass point at 0 omitted.\nThe censoring and truncation processes are depicted in Figure 27.2(a) which plots 100 random \\(^{2}\\) draws \\(\\left(Y^{*}, X\\right)\\). The uncensored variables are marked by the open circles and squares. The open squares are the realizations for which \\(Y^{*}>0\\) and the open circles are the realizations for which \\(Y^{*}<0\\). The censored distribution replaces the negative values of \\(Y^{*}\\) with 0 , and thus replaces the open with the filled circles. The censored distribution thus consists of the open squares and filled circles. The truncated distribution is obtained by deleting the censored observations so consists of just the open squares.\n\n\nCensored Regression Functions\n\n\n\nCensoring Probability\n\nFigure 27.2: Properties of Censored Distributions\nTo summarize: we distinguish between three distributions and variables: uncensored \\(\\left(Y^{*}\\right)\\), censored \\((Y)\\), and truncated \\(\\left(Y^{\\#}\\right)\\).\nThe censored regression model (27.1) makes several strong assumptions: (1) linearity of the conditional mean; (2) independence of the error; (3) normal distribution. The linearity assumption is not critical as we can interpret \\(X^{\\prime} \\beta\\) as a series expansion or similar flexible approximation. The independence assumption, however, is quite important as its violation (e.g. heteroskedasticity) changes the properties of the censoring process. The normality assumption is also quite important, yet difficult to justify from first principles."
  },
  {
    "objectID": "chpt27-censor-selection.html#censored-regression-functions",
    "href": "chpt27-censor-selection.html#censored-regression-functions",
    "title": "25  Censoring and Selection",
    "section": "25.3 Censored Regression Functions",
    "text": "25.3 Censored Regression Functions\nWe can calculate some properties of the conditional distribution of the censored random variable. The conditional probability of censoring is\n\\[\n\\mathbb{P}\\left[Y^{*}<0 \\mid X\\right]=\\mathbb{P}\\left[e<-X^{\\prime} \\beta \\mid X\\right]=\\Phi\\left(-\\frac{X^{\\prime} \\beta}{\\sigma}\\right) .\n\\]\n\\({ }^{2} X \\sim U[-3,3]\\) and \\(Y^{*} \\mid X \\sim \\mathrm{N}(1+X, 1)\\) We illustrate in Figure 27.2(b). This plots the censoring probability as a function of \\(X\\) for the example from Figure \\(27.2(\\mathrm{a})\\). The censoring probability is \\(98 %\\) for \\(X=-3,50 %\\) for \\(X=-1\\) and \\(2 %\\) for \\(X=1\\).\nThe conditional mean of the uncensored, censored, and truncated distributions are\n\\[\n\\begin{aligned}\nm^{*}(X) &=\\mathbb{E}\\left[Y^{*} \\mid X\\right]=X^{\\prime} \\beta, \\\\\nm(X) &=\\mathbb{E}[Y \\mid X]=X^{\\prime} \\beta \\Phi\\left(\\frac{X^{\\prime} \\beta}{\\sigma}\\right)+\\sigma \\phi\\left(\\frac{X^{\\prime} \\beta}{\\sigma}\\right) \\\\\nm^{\\#}(X) &=\\mathbb{E}\\left[Y^{\\#} \\mid X\\right]=X^{\\prime} \\beta+\\sigma \\lambda\\left(\\frac{X^{\\prime} \\beta}{\\sigma}\\right) .\n\\end{aligned}\n\\]\nThe function \\(\\lambda(x)=\\phi(x) / \\Phi(x)\\) in (27.3) is called the inverse Mills ratio. To obtain (27.2) and (27.3) see Theorems 5.8.4 and 5.8.6 of Probability and Statistics for Economists and Exercise 27.1.\nSince \\(Y^{*} \\leq Y \\leq Y^{\\#}\\) it follows that\n\\[\nm^{*}(x) \\leq m(x) \\leq m^{\\#}(x)\n\\]\nwith strict inequality if the censoring probability is positive. This shows that the conditional means of the truncated and censored distributions are biased for the uncensored conditional mean.\nWe illustrate in Figure 27.2(a). The uncensored mean \\(m^{*}(x)\\) is marked by the straight line, the censored mean \\(m(x)\\) is marked with the dashed line, and the truncated mean \\(m^{\\#}(x)\\) is marked with the long dashes. The functions are strictly ranked with the truncated mean exhibiting the highest bias."
  },
  {
    "objectID": "chpt27-censor-selection.html#the-bias-of-least-squares-estimation",
    "href": "chpt27-censor-selection.html#the-bias-of-least-squares-estimation",
    "title": "25  Censoring and Selection",
    "section": "25.4 The Bias of Least Squares Estimation",
    "text": "25.4 The Bias of Least Squares Estimation\nIf the observations \\((Y, X)\\) are generated by the censored model (27.1) then least squares estimation using either the full sample including the censored observations or the truncated sample excluding the censored observations will be biased. Indeed, an estimator which is consistent for the CEF (such as a series estimator) will estimate the censored mean \\(m(x)\\) or truncated mean \\(m^{\\#}(x)\\) in the censored and truncated samples, respectively, not the latent CEF \\(m^{*}(x)\\).\nIt is also interesting to consider the properties of the best linear predictor of \\(Y\\) on \\(X\\), which is the estimand of the least squares estimator. In general, this depends on the marginal distribution of the regressors. However, when the regressors are normally distributed it takes a simple form as discovered by Greene (1981). Write the model with an explicit intercept as \\(Y^{*}=\\alpha+X^{\\prime} \\beta+e\\) and assume \\(X \\sim \\mathrm{N}(0, \\Sigma)\\). Greene showed that the best linear predictor slope coefficient is\n\\[\n\\beta_{\\mathrm{BLP}}=\\beta(1-\\pi)\n\\]\nwhere \\(\\pi=\\mathbb{P}[Y=0]\\) is the censoring probability. We derive (27.4) at the end of this section.\nGreene’s formula (27.4) shows that the least squares slope coefficients are shrunk towards zero proportionately with the censoring percentage. While Greene’s formula is special to normal regressors it gives a baseline estimate of the bias due to censoring. The censoring proportion \\(\\pi\\) is easily estimated from the sample (e.g. \\(\\pi=0.80\\) in our transfers example) allowing a quick calculation of the expected bias due to censoring. This can be used as a rule of thumb. If the expected bias is sufficiently small (e.g. less than 5%) the resulting expected estimation bias (e.g. 5%) may be acceptable, leading to conventional least squares estimation using the full sample without an explicit treatment of censoring. However, if the censoring proportion \\(\\pi\\) is sufficiently high (e.g. 10%) then estimation methods which correct for censoring bias may be desired.\nWe close this section by deriving (27.4). The calculation is simplified by a trick suggested by Goldberger (1981). Notice that \\(Y^{*} \\sim \\mathrm{N}\\left(\\alpha, \\sigma_{Y}^{2}\\right)\\) with \\(\\sigma_{Y}^{2}=\\sigma^{2}+\\beta^{\\prime} \\Sigma \\beta\\). Using the moments of the truncated normal distribution (Probability and Statistics for Economists, Theorems 5.7.6 and 5.7.8) and setting \\(\\lambda=\\lambda\\left(\\alpha / \\sigma_{Y}\\right)\\) we can calculate that\n\\[\n\\begin{aligned}\n\\mathbb{E}\\left[\\left(Y^{*}-\\alpha\\right) Y^{*} \\mid Y^{*}>0\\right] &=\\operatorname{var}\\left[Y^{*} \\mid Y^{*}>0\\right]+\\left(\\mathbb{E}\\left[Y^{*} \\mid Y^{*}>0\\right]-\\alpha\\right) \\mathbb{E}\\left[Y^{*} \\mid Y^{*}>0\\right] \\\\\n&=\\sigma_{Y}^{2}\\left(1-\\frac{\\alpha}{\\sigma_{Y}} \\lambda-\\lambda^{2}\\right)+\\sigma_{Y} \\lambda\\left(\\alpha+\\sigma_{Y} \\lambda\\right)=\\sigma_{Y}^{2} .\n\\end{aligned}\n\\]\nThe projection of \\(X\\) on \\(Y^{*}\\) is \\(X=\\mathbb{E}\\left[X Y^{*}\\right] \\sigma_{Y}^{-2}\\left(Y^{*}-\\alpha\\right)+u\\) where \\(u\\) is independent of \\(Y^{*}\\). This implies\n\\[\n\\begin{aligned}\n\\mathbb{E}\\left[X Y^{*} \\mid Y^{*}>0\\right] &=\\mathbb{E}\\left[\\left(\\mathbb{E}\\left[X Y^{*}\\right] \\sigma_{Y}^{-2}\\left(Y^{*}-\\alpha\\right)+u\\right) Y^{*} \\mid Y^{*}>0\\right] \\\\\n&=\\mathbb{E}\\left[X Y^{*}\\right] \\sigma_{Y}^{-2} \\mathbb{E}\\left[\\left(Y^{*}-\\alpha\\right) Y^{*} \\mid Y^{*}>0\\right] \\\\\n&=\\mathbb{E}\\left[X Y^{*}\\right]\n\\end{aligned}\n\\]\nHence\n\\[\n\\begin{aligned}\n\\beta_{\\mathrm{BLP}} &=\\mathbb{E}\\left[X X^{\\prime}\\right]^{-1} \\mathbb{E}[X Y] \\\\\n&=\\mathbb{E}\\left[X X^{\\prime}\\right]^{-1} \\mathbb{E}\\left[X Y^{*} \\mid Y^{*}>0\\right](1-\\pi) \\\\\n&=\\mathbb{E}\\left[X X^{\\prime}\\right]^{-1} \\mathbb{E}\\left[X Y^{*}\\right](1-\\pi) \\\\\n&=\\beta(1-\\pi)\n\\end{aligned}\n\\]\nwhich is (27.4) as claimed."
  },
  {
    "objectID": "chpt27-censor-selection.html#tobit-estimator",
    "href": "chpt27-censor-selection.html#tobit-estimator",
    "title": "25  Censoring and Selection",
    "section": "25.5 Tobit Estimator",
    "text": "25.5 Tobit Estimator\nTobin (1958) proposed estimation of the censored regression model (27.1) by maximum likelihood.\nThe censored variable \\(Y\\) has a conditional distribution function which is a mixture of continuous and discrete components:\n\\[\nF(y \\mid x)=\\left\\{\\begin{array}{cc}\n0, & y<0 \\\\\n\\Phi\\left(\\frac{y-x^{\\prime} \\beta}{\\sigma}\\right), & y \\geq 0\n\\end{array}\\right.\n\\]\nThe associated density \\({ }^{3}\\) function is\n\\[\nf(y \\mid x)=\\Phi\\left(-\\frac{x^{\\prime} \\beta}{\\sigma}\\right)^{\\mathbb{1}\\{y=0\\}}\\left[\\sigma^{-1} \\phi\\left(\\frac{y-x^{\\prime} \\beta}{\\sigma}\\right)\\right]^{\\mathbb{1}\\{y>0\\}} .\n\\]\nThe first component is the probability of censoring and the second component is the normal regression density.\nThe log-likelihood is the sum of the log density functions evaluated at the observations:\n\\[\n\\begin{aligned}\n\\ell_{n}\\left(\\beta, \\sigma^{2}\\right) &=\\sum_{i=1}^{n} \\log f\\left(Y_{i} \\mid X_{i}\\right) \\\\\n&=\\sum_{i=1}^{n}\\left(\\mathbb{1}\\left\\{Y_{i}=0\\right\\} \\log f\\left(Y_{i} \\mid X_{i}\\right)+\\mathbb{1}\\left\\{Y_{i}>0\\right\\} \\log \\left[\\sigma^{-1} \\phi\\left(\\frac{Y_{i}-X_{i}^{\\prime} \\beta}{\\sigma}\\right)\\right]\\right) \\\\\n&=\\sum_{Y_{i}=0} \\log \\Phi\\left(-\\frac{X_{i}^{\\prime} \\beta}{\\sigma}\\right)-\\frac{1}{2} \\sum_{Y_{i}>0}\\left(\\log \\left(2 \\pi \\sigma^{2}\\right)+\\frac{1}{\\sigma^{2}}\\left(Y_{i}-X_{i}^{\\prime} \\beta\\right)^{2}\\right)\n\\end{aligned}\n\\]\n\\({ }^{3}\\) Since the distribution function is discontinuous at \\(y=0\\) the density is technically the derivative with respect to a mixed continuous/discrete measure. The first component is the same as in a probit model, and the second component is the same as for the normal regression model.\nThe \\(\\operatorname{MLE}\\left(\\widehat{\\beta}, \\widehat{\\sigma}^{2}\\right)\\) are the values which maximize the log-likelihood \\(\\ell_{n}\\left(\\beta, \\sigma^{2}\\right)\\). This estimator was nicknamed “Tobit” by Goldberger because of its connection with the probit estimator. Amemiya (1973) established its asymptotic normality.\nComputation is improved, as shown by Olsen (1978), if we transform the parameters to \\(\\gamma=\\beta / \\sigma\\) and \\(\\nu=1 / \\sigma\\). Then the reparameterized log-likelihood equals\n\\[\n\\ell_{n}(\\gamma, v)=\\sum_{Y_{i}=0} \\log \\Phi\\left(-X_{i}^{\\prime} \\gamma\\right)+\\sum_{Y_{i}>0} \\log (v / \\sqrt{2 \\pi})+\\left(-\\frac{1}{2}\\right) \\sum_{Y_{i}>0}\\left(Y_{i} v-X_{i}^{\\prime} \\gamma\\right)^{2} .\n\\]\nThis is the sum of three terms, each of which is globally concave in \\((\\gamma, v)\\) (as we now discuss), so \\(\\ell_{n}(\\gamma, v)\\) is globally concave in \\((\\gamma, v)\\) ensuring global convergence of Newton-based optimizers. Indeed, the third term in (27.5) is the negative of a quadratic in \\((\\gamma, \\nu)\\), so is concave. The second term in (27.5) is logarithmic in \\(v\\), which is concave. The first term in (27.5) is a function only of \\(\\gamma\\) and has second derivative\n\\[\n\\frac{\\partial^{2}}{\\partial \\gamma \\partial \\gamma^{\\prime}} \\sum_{Y_{i}=0} \\log \\Phi\\left(-X_{i}^{\\prime} \\gamma\\right)=\\sum_{Y_{i}=0} X_{i} X_{i}^{\\prime} \\lambda^{\\prime}\\left(-X_{i}^{\\prime} \\gamma\\right)\n\\]\nwhich is negative definite because the Mills ratio satisfies \\(\\lambda^{\\prime}(u)<0\\) (see Theorem 5.7.7 in Probability and Statistics for Economists). Hence the first term in (27.5) is concave.\nIn Stata, Tobit regression can be estimated with the tobit command. In R there are several options including the tobit command in the AER package.\nJames Tobin\\ James Tobin (1918-2002) of the United States was one of the leading macroe-\\ conomists of the mid-twentieth century and winner of the 1981 Nobel Memorial\\ Prize in Economic Sciences. His 1958 paper introduced censored regression and\\ its MLE, typically called the Tobit estimator. As a fascinating coincidence, the\\ name “Tobit” also arises in the 1951 novel The Caine Mutiny, set on a U.S. Navy\\ destroyer during World War II. At one point in the novel the author describes\\ a crew member named “Tobit” who had “a mind like a sponge” because of his\\ strong intellect. It turns out the author (Herman Wouk) and James Tobin served\\ on the same Navy destroyer during WWII. Go figure!"
  },
  {
    "objectID": "chpt27-censor-selection.html#identification-in-tobit-regression",
    "href": "chpt27-censor-selection.html#identification-in-tobit-regression",
    "title": "25  Censoring and Selection",
    "section": "25.6 Identification in Tobit Regression",
    "text": "25.6 Identification in Tobit Regression\nThe Tobit model (27.1) makes several strong assumptions. Which are critical? To investigate this question consider the nonparametric censored regression framework\n\\[\n\\begin{aligned}\nY^{*} &=m(X)+e \\\\\n\\mathbb{E}[e] &=0 \\\\\nY &=\\max \\left(Y^{*}, 0\\right)\n\\end{aligned}\n\\]\nwhere \\(e \\sim F\\) independent of \\(X\\), and the regression function \\(m(x)\\) and distribution function \\(F(e)\\) are unknown. What is identified?\nSuppose that the random variable \\(m(X)\\) has unbounded support on the real line (as occurs when \\(m(X)=X^{\\prime} \\beta\\) and \\(X\\) has an unbounded distribution such as the normal). Then we can find a set \\(\\mathscr{X} \\subset \\mathbb{R}^{k}\\) such that for \\(x \\in \\mathscr{X}, \\mathbb{P}[Y=0 \\mid X=x]=F(-m(x)) \\simeq 0\\). We can then imagine taking the subsample of observations for which \\(X \\in \\mathscr{X}\\). The function \\(m(x)\\) is identified for \\(x \\in \\mathscr{X}\\), permitting the identification of the distribution \\(F(e)\\). As the censoring probability \\(\\mathbb{P}[Y=0 \\mid X=x]=F(-m(x))\\) is globally identified the function \\(m(x)\\) is globally identified as well. This discussion shows that so long as we maintain the assumption that \\(X\\) and \\(e\\) are independent, the regression function \\(m(x)\\) and distribution function \\(F(e)\\) are nonparametrically identified when the CEF \\(m(X)\\) has full support. These two assumptions, however, are essential as we now discuss.\nSuppose the full support condition fails in the sense that the regression function is bounded \\(m(X) \\leq\\) \\(\\bar{m}\\) at a value such that \\(\\mathbb{P}[Y=0 \\mid X=x]=F(-\\bar{m})>0\\). In this case the error distribution \\(F(e)\\) is not identified for \\(e \\leq-\\bar{m}\\). This means that the distribution function can take any shape for \\(e \\leq-\\bar{m}\\) so long as it is weakly increasing. This implies that the expectation \\(\\mathbb{E}[e]\\) is not identified so the location of \\(m(x)\\) (the intercept of the regression) is not identified.\nThe second important assumption is that \\(e\\) is independent of \\(X\\). This assumption has been relaxed by Powell \\((1984,1986)\\) in the conditional quantile framework. The model is\n\\[\n\\begin{aligned}\nY^{*} &=q_{\\tau}(X)+e_{\\tau} \\\\\n\\mathbb{Q}_{\\tau}\\left[e_{\\tau} \\mid X\\right] &=0 \\\\\nY &=\\max \\left(Y^{*}, 0\\right)\n\\end{aligned}\n\\]\nfor some \\(\\tau \\in(0,1)\\). This model defines \\(q_{\\tau}(x)\\) as the \\(\\tau^{t h}\\) conditional quantile function. Since quantiles are equivariant to monotone transformations we have the relationship\n\\[\n\\mathbb{Q}_{\\tau}[Y \\mid X=x]=\\max \\left(q_{\\tau}(x), 0\\right) .\n\\]\nThus the conditional quantile function of \\(Y\\) is the censored quantile function of \\(Y^{*}\\). The function \\(\\mathbb{Q}_{\\tau}[Y \\mid X=x]\\) is identified from the joint distribution of \\((Y, X)\\). Consequently the function \\(q_{\\tau}(x)\\) is identified for any \\(x\\) such that \\(q_{\\tau}(x)>0\\). This is an important conceptual breakthrough. Powell’s result shows that identification of \\(q_{\\tau}(x)\\) does not require the error to be independent of \\(X\\) nor have a known distribution. The key insight is that quantiles, not means, are nonparametrically identified from a censored distribution.\nA limitation with Powell’s result is that the function \\(q_{\\tau}(x)\\) is only identifed on sub-populations for which censoring does not exceed \\(\\tau %\\).\nTo illustrate, Figure 27.3(a) displays the conditional quantile functions \\(q_{\\tau}(x)\\) for \\(\\tau=0.3,0.5,0.7\\), and \\(0.9\\) for the conditional distribution \\(Y^{*} \\mid X \\sim N\\left(\\sqrt{x}-\\frac{3}{2}, 2+x\\right)\\). The portions above zero (which are identified from the censored distribution) are plotted with solid lines. The portions below zero (which are not identified from the censored distribution) are plotted with dashed lines. We can see that in this example the quantile function \\(q_{.9}(x)\\) is identified for all values of \\(x\\), the quantile function \\(q_{.3}(x)\\) is not identified for any values of \\(x\\), and the quantile functions \\(q_{.7}(x)\\) and \\(q_{.5}(x)\\) are identified for a subset of values of \\(x\\). The explanation is that for any fixed value of \\(X=x\\) we only observe the censored distribution \\(Y\\) and so only observe the quantiles above the censoring point. There is no nonparametric information about the distribution of \\(Y^{*}\\) below the censoring point.\n\n\n\\(Y^{*} \\mid X \\sim \\mathrm{N}\\left(\\sqrt{X}-\\frac{3}{2}, 2+X\\right)\\)\n\n\n\nEffect of Income on Transfers\n\nFigure 27.3: Censored Regression Quantiles"
  },
  {
    "objectID": "chpt27-censor-selection.html#clad-and-cqr-estimators",
    "href": "chpt27-censor-selection.html#clad-and-cqr-estimators",
    "title": "25  Censoring and Selection",
    "section": "25.7 CLAD and CQR Estimators",
    "text": "25.7 CLAD and CQR Estimators\nPowell \\((1984,1986)\\) applied the quantile identification strategy described in the previous section to develop straightforward censored regression estimators.\nThe model in Powell (1984) is censored median regression:\n\\[\n\\begin{aligned}\nY^{*} &=X^{\\prime} \\beta+e \\\\\n\\operatorname{med}[e \\mid X] &=0 \\\\\nY &=\\max \\left(Y^{*}, 0\\right) .\n\\end{aligned}\n\\]\nIn this model \\(Y^{*}\\) is latent with \\(\\operatorname{med}\\left[Y^{*} \\mid X\\right]=X^{\\prime} \\beta\\) and \\(Y\\) is censored at zero. As described in the previous section the equivariance property of the median implies that the conditional median of \\(Y\\) equals\n\\[\n\\operatorname{med}[Y \\mid X]=\\max \\left(X^{\\prime} \\beta, 0\\right) .\n\\]\nThis is a parametric but nonlinear median regression model for \\(Y\\).\nThe appropriate estimator for median regression is least absolute deviations (LAD). The censored least absolute deviations (CLAD) criterion is\n\\[\nM_{n}(\\beta)=\\frac{1}{n} \\sum_{i=1}^{n}\\left|Y_{i}-\\max \\left(X_{i}^{\\prime} \\beta, 0\\right)\\right| .\n\\]\nThe CLAD estimator minimizes \\(M_{n}(\\beta)\\)\n\\[\n\\widehat{\\beta}_{\\mathrm{CLAD}}=\\underset{\\beta}{\\operatorname{argmin}} M_{n}(\\beta) .\n\\]\nThe CLAD criterion \\(M_{n}(\\beta)\\) has similar properties as LAD criterion, namely that it is continuous, faceted, and has discontinuous first derivatives. An important difference, however, is that \\(M_{n}(\\beta)\\) is not globally convex, so minimization algorithms may converge to a local rather than a global minimum. Powell (1986) extended CLAD to censored quantile regression (CQR). The model is\n\\[\n\\begin{aligned}\nY^{*} &=X^{\\prime} \\beta+e \\\\\n\\mathbb{Q}_{\\tau}[e \\mid X] &=0 \\\\\nY &=\\max \\left(Y^{*}, 0\\right)\n\\end{aligned}\n\\]\nfor \\(\\tau \\in(0,1)\\). The equivariance property implies that the conditional quantile function for \\(Y\\) is\n\\[\n\\mathbb{Q}_{\\tau}[Y \\mid X]=\\max \\left(X^{\\prime} \\beta, 0\\right) .\n\\]\nThe CQR criterion is\n\\[\nM_{n}(\\beta ; \\tau)=\\frac{1}{n} \\sum_{i=1}^{n} \\rho_{\\tau}\\left(Y_{i}-\\max \\left(X_{i}^{\\prime} \\beta, 0\\right)\\right)\n\\]\nwhere \\(\\rho_{\\tau}(u)\\) is the check function (24.10). The CQR estimator minimizes this criterion\n\\[\n\\widehat{\\beta}_{\\mathrm{CQR}}(\\tau)=\\underset{\\beta}{\\operatorname{argmin}} M_{n}(\\beta ; \\tau) .\n\\]\nAs for CLAD, the criterion is not globally concave so numerical minimization is not guarenteed to converge to the global minimum.\nPowell \\((1984,1986)\\) shows that the CLAD and CQR estimators are asymptotically normal by similar arguments as for quantile regression. An important technical difference with quantile regression is that the CLAD and CQR estimators require stronger conditions for identification. As we discussed in the previous section the quantile function \\(X^{\\prime} \\beta\\) is only identified for regions where it is positive. This means that we require a positive fraction of the population to satisfy \\(X^{\\prime} \\beta>0\\). Furthermore, the relevant design matrix (24.18) is defined on this sub-population, and must be full rank for conventional inference. Essentially, there must be sufficient variation in the regressors over the region of the sample space where there is no censoring.\nCLAD can be estimated in Stata with the add-on package clad. In R, CLAD and CQR can be estimated with the crq command in the package quantreg."
  },
  {
    "objectID": "chpt27-censor-selection.html#illustrating-censored-regression",
    "href": "chpt27-censor-selection.html#illustrating-censored-regression",
    "title": "25  Censoring and Selection",
    "section": "25.8 Illustrating Censored Regression",
    "text": "25.8 Illustrating Censored Regression\nTo illustrate the methods we revisit of the applications reported in Section 20.6, where we used a linear spline to estimate the impact of income on non-governmental transfers for a sample of 8684 Phillipino households. The least squares estimates indicated a sharp discontinuity in the conditional mean around 20,000 pesos. The dependent variable is the sum of transfers received domestically, from abroad, and in-kind, less gifts. Each of these four sub-variables is non-negative. If we apply the model to any of these sub-variables there is substantial censoring. To illustrate, we set the dependent variable to equal the sum of transfers received domestically, from abroad, and in-kind, for which the censoring proportion is \\(18 %\\). This proportion is sufficiently high that we should expect significant censoring bias if censoring is ignored.\nWe estimate the same model as reported in Section \\(20.6\\) and displayed in Figure 20.2(b), which is a linear spline in income with 5 knots and 15 additional control regressors. We estimated the equation using four methods: (a) least squares; (b) Tobit regression; (c) LAD; (d) CLAD. We display the estimated regression as a function of income (with remaining regressors set at sample means) in Figure 27.3(b).\nThe basic insight - that the regression has a slope close to \\(-1\\) for low income levels and is flat for high income levels with a sharp discontinuity at an income level of 20,000 pesos - is remarkably robust across the four estimates. What is noticably different, however, is the level of the regression function. The least squares estimate is several thousand pesos above the others. The fact that the LAD and CLAD estimates have a meaningfully different level should not be surprising. The dependent variable is highly skewed, so the mean and median are quite different (the unconditional mean and median are 7700 and 1200 , respectively). This implies a level shift of the regression function. This does not explain, however, why the Tobit estimate also is substantially shifted down. Instead, this can be explained by censoring bias. Since the regression function is negatively sloped the censoring probability is increasing in income, so the bias of the least squares estimator is positive and increasing in the income level. The LAD and CLAD estimates are quite similar even though the LAD estimates do not account for censoring. Overall, the CLAD estimates are the preferred choice because they are robust to both censoring and non-normality."
  },
  {
    "objectID": "chpt27-censor-selection.html#sample-selection-bias",
    "href": "chpt27-censor-selection.html#sample-selection-bias",
    "title": "25  Censoring and Selection",
    "section": "25.9 Sample Selection Bias",
    "text": "25.9 Sample Selection Bias\nWhile econometric models typically assume random sampling, actual observations are typically gathered non-randomly. This can induce estimation bias if selection (presence in the sample) is endogeneous. The following are examples of potential sample selection.\n\nWage regression. Wages are only observed for individuals who have wage income, which means that the individual is a member of the labor force and has a wage-paying job. The decision to work may be endogenously related to the person’s observed and unobserved characteristics.\nProgram evaluation. The goal is to measure the impact of a program such as workforce training through a pilot program. Endogenous selection arises when individuals volunteer to participate (rather than being randomly assigned). Individuals who volunteer for a training program may have abilities which are correlated with outcomes.\nSurveys. While a survey may be randomly distributed the act of completing the survey is nonrandom. Most surveys have low response rates. Endogenous selection arises when the decision to complete and return the survey is correlated with the survey responses.\nRatings. We are routinely asked to rate products, services, and experiences. Most people do not respond to the request. Endogenous selection arises when the decision to rate the product is correlated with the response.\n\nTo understand the effect of sample selection it is useful to view sampling as a two-stage process. In the first stage the random variables \\((Y, X)\\) are drawn. In the second stage the pair is either selected into the sample \\((S=1)\\) or unobserved \\((S=0)\\). The sample then consists of the pairs \\((Y, X)\\) for which \\(S=1\\). Suppose that the variables satisfy the latent regression model \\(Y=X^{\\prime} \\beta+e\\) with \\(\\mathbb{E}[e \\mid X]=0\\). Then the CEF in the observed (selected) sample is\n\\[\n\\mathbb{E}[Y \\mid X, S=1]=X^{\\prime} \\beta+\\mathbb{E}[e \\mid X, S=1] .\n\\]\nSelection bias occurs when the second term is non-zero. To understand this further suppose that selection can be modelled as \\(S=\\mathbb{1}\\left\\{X^{\\prime} \\gamma+u>0\\right\\}\\) for some error \\(u\\). This is consistent with a latent utility framework where \\(X^{\\prime} \\gamma+u\\) is the latent utility of participation. Given this framework we can write the CEF of \\(Y\\) in the selected sample as\n\\[\n\\mathbb{E}[Y \\mid X, S=1]=X^{\\prime} \\beta+\\mathbb{E}\\left[e \\mid u>-X^{\\prime} \\gamma\\right] .\n\\]\nLet \\(e=\\rho u+\\varepsilon\\) be the projection of \\(e\\) on \\(u\\). Suppose that the errors are independent of \\(X\\), and \\(u\\) and \\(\\varepsilon\\) are mutually independent. Then the above expression equals\n\\[\n\\mathbb{E}[Y \\mid X, S=1]=X^{\\prime} \\beta+\\rho \\mathbb{E}\\left[u \\mid u>-X^{\\prime} \\gamma\\right]=X^{\\prime} \\beta+\\rho g\\left(X^{\\prime} \\gamma\\right)\n\\]\nfor some function \\(g(u)\\). When \\(u \\sim \\mathrm{N}(0,1), g(u)=\\phi(u) / \\Phi(u)=\\lambda(u)\\) (see Exercise 27.7) so the expression equals\n\\[\n\\mathbb{E}[Y \\mid X, S=1]=X^{\\prime} \\beta+\\rho \\lambda\\left(X^{\\prime} \\gamma\\right) .\n\\]\nThis is the same as (27.3) in the special case \\(\\rho=\\sigma\\) and \\(\\gamma=\\beta / \\sigma\\). This, as shown in Figure 27.2(a), deviates from the latent \\(\\mathrm{CEF} X^{\\prime} \\beta\\)\nOne way to interpret this effect is that the regression function (27.6) contains two components: \\(X^{\\prime} \\beta\\) and \\(\\rho \\lambda\\left(X^{\\prime} \\gamma\\right)\\). A linear regression on \\(X\\) omits the second term and thus inherits omitted variables bias as \\(X\\) and \\(\\lambda\\left(X^{\\prime} \\gamma\\right)\\) are correlated. The extent of omitted variables bias depends on the magnitude of \\(\\rho\\) which is the coefficient from the projection of \\(e\\) on \\(u\\). When the errors \\(e\\) and \\(u\\) are independent (when selection is exogenous) then \\(\\rho=0\\) and (27.6) simplifies to \\(X^{\\prime} \\beta\\) and there is no omitted term. Thus sample selection bias arises if (and only if) selection is correlated with the equation error.\nFurthermore, the omitted selection term \\(\\lambda\\left(X^{\\prime} \\gamma\\right)\\) only impacts estimated marginal effects if the slope coefficients \\(\\gamma\\) are non-zero. In contrast suppose that \\(X^{\\prime} \\gamma=\\gamma_{0}\\), a constant. Then (27.6) equals \\(\\mathbb{E}[Y \\mid X, S=1]=\\) \\(X^{\\prime} \\beta+\\rho \\lambda\\left(\\gamma_{0}\\right)\\) so the impact of selection is an intercept shift. If our focus is on marginal effects sample selection bias only arises when the selection equation has non-trivial dependence on the regressors \\(X\\).\nIn Figure 27.2(a) we saw that censoring attenuates (flattens) the regression function. While the selection CEF (27.6) takes a similar form it is broader and can have a different impact. In contrast to the censoring case, selection can both steepen as well as flatten the regression function. In general it is difficult to predict the effect of selection on regression functions.\nAs we have shown, endogenous selection changes the conditional expectation. If samples are generated by endogenous selection then estimation will be biased for the parameters of interest. Without information on the selection process there is little that can be done to “correct” the bias other than to be aware of its presence. In the next section we discuss one approach which corrects for sample selection bias when we have information on the selection process."
  },
  {
    "objectID": "chpt27-censor-selection.html#heckmans-model",
    "href": "chpt27-censor-selection.html#heckmans-model",
    "title": "25  Censoring and Selection",
    "section": "25.10 Heckman’s Model",
    "text": "25.10 Heckman’s Model\nHeckman (1979) showed that sample selection bias can be corrected if we have a sample which includes the non-selected observations. Suppose that the observations \\(\\left\\{Y_{i}, X_{i}, Z_{i}\\right\\}\\) are a random sample where \\(Y\\) is a selected variable (such as wage, which is only observed if a person has wage income). Heckman’s approach is to build a joint model of the full sample (not just the selected sample) and use this to estimate the model parameters.\nHeckman’s model is\n\\[\n\\begin{aligned}\nY^{*} &=X^{\\prime} \\beta+e \\\\\nS^{*} &=Z^{\\prime} \\gamma+u \\\\\nS &=\\mathbb{1}\\left\\{S^{*}>0\\right\\} \\\\\nY &=\\left\\{\\begin{array}{cc}\nY^{*} & \\text { if } S=1 \\\\\n\\text { missing } & \\text { if } S=0\n\\end{array}\\right.\n\\end{aligned}\n\\]\nwith\n\\[\n\\left(\\begin{array}{c}\ne \\\\\nu\n\\end{array}\\right) \\sim \\mathrm{N}\\left(0,\\left(\\begin{array}{cc}\n\\sigma^{2} & \\sigma_{21} \\\\\n\\sigma_{21} & 1\n\\end{array}\\right)\\right)\n\\]\nThe model specifies that the latent variables \\(Y^{*}\\) and \\(S^{*}\\) are linear in regressors \\(X\\) and \\(Z\\) with structural errors \\(e\\) and \\(u\\). The variable \\(S\\) indicates selection and follows a probit equation. The variable \\(Y\\) equals the latent variable \\(Y^{*}\\) if selected \\((S=1)\\) and otherwise is missing. The model specifies that the errors are jointly normal with covariance \\(\\sigma_{21}\\). The variance of \\(u\\) is not identified so is normalized to equal 1 .\nIn Heckman’s classic example, \\(Y^{*}\\) is the wage (or log(wage)) an individual would receive if they were employed, \\(S\\) is employment status, and \\(Y\\) is observed wage. The coefficients \\(\\beta\\) are those of the wage regression; the coefficients \\(\\gamma\\) are those which determine employment status. The error \\(e\\) is unobserved ability and other unobserved factors which determine an individual’s wages; the error \\(u\\) is the unobserved factors which determine employment status; and the two are likely to be correlated.\nBased on the same calculations as discussed in the previous section, the CEF of \\(Y\\) in the selected sample is\n\\[\n\\mathbb{E}[Y \\mid X, Z, S=1]=X^{\\prime} \\beta+\\sigma_{21} \\lambda\\left(Z^{\\prime} \\gamma\\right)\n\\]\nwhere \\(\\lambda(x)\\) is the inverse Mills ratio.\nHeckman proposed a two-step estimator of the coefficients. The insight is that the coefficient \\(\\gamma\\) is identified by the probit regression of \\(S\\) on \\(Z\\). Given \\(\\gamma\\) the coefficients \\(\\beta\\) and \\(\\sigma_{21}\\) are identified by least squares regression of \\(Y\\) on \\(\\left(X, \\lambda\\left(Z^{\\prime} \\gamma\\right)\\right)\\) using the selected sample. The steps are as follows.\n\nConstruct (if necessary) the binary variable \\(S\\) from the observed series \\(Y\\).\nEstimate the coefficient \\(\\widehat{\\gamma}\\) by probit regression of \\(S\\) on \\(Z\\).\nConstruct the variables \\(\\widehat{\\lambda}_{i}=\\lambda\\left(Z_{i}^{\\prime} \\widehat{\\gamma}\\right)\\).\nEstimate the coefficients \\(\\left(\\widehat{\\beta}, \\widehat{\\sigma}_{21}\\right)\\) by least-squares regression of \\(Y_{i}\\) on \\(\\left(X_{i}, \\widehat{\\lambda}_{i}\\right)\\) using the sub-sample with \\(S_{i}=1\\).\n\nHeckman showed that the estimator \\(\\widehat{\\beta}\\) is consistent and asymptotically normal. The variable \\(\\widehat{\\lambda}_{i}\\) is a generated regressor (see Section 12.26) which affects covariance matrix estimation. The method is sometimes called “Heckit” as it is an analog of probit, logit, and Tobit regression.\nAs a by-product we obtain an estimator of the covariance \\(\\sigma_{21}\\). This parameter indicates the magnitude of sample selection endogeneity. If selection is exogenous then \\(\\sigma_{21}=0\\). The null hypothesis of exogenous selection can be tested by examining the t-statistic for \\(\\widehat{\\sigma}_{21}\\).\nAn alternative to two-step estimation is joint maximum likelihood. The joint density of \\(S\\) and \\(Y\\) is\n\\[\nf(s, y \\mid x, z)=\\mathbb{P}[S=0 \\mid x, z]^{1-s} f(y, S=1 \\mid x, z)^{s} \\text {. }\n\\]\nThe selection probability is \\(\\mathbb{P}[S=0 \\mid x, z]=1-\\Phi\\left(z^{\\prime} \\gamma\\right)\\). The conditional density component is\n\\[\n\\begin{aligned}\nf(y, S=1, \\mid x, z) &=\\int_{0}^{\\infty} f\\left(y, s^{*} \\mid x, z\\right) d s^{*} \\\\\n&=\\int_{0}^{\\infty} f\\left(s^{*} \\mid y, x, z\\right) f(y \\mid x, z) d s^{*} \\\\\n&=\\left(1-F\\left(s^{*} \\mid y, x, z\\right)\\right) f(y \\mid x, z) .\n\\end{aligned}\n\\]\nThe first equality holds because \\(S=1\\) is the same as \\(S^{*}>0\\). The second factors the joint density into the product of the conditional of \\(S^{*}\\) given \\(Y\\) and the marginal of \\(Y\\). The marginal density of \\(Y\\) is \\(\\sigma^{-1} \\phi\\left(\\left(y-x^{\\prime} \\beta\\right) / \\sigma\\right)\\). The conditional distribution of \\(S^{*}\\) given \\(Y\\) is \\(\\mathrm{N}\\left(Z^{\\prime} \\gamma+\\frac{\\sigma_{21}}{\\sigma^{2}}\\left(Y-X^{\\prime} \\beta\\right), 1-\\frac{\\sigma_{21}}{\\sigma^{2}}\\right)\\). Making these substitutions we obtain the joint mixed density\n\\[\nf(s, y \\mid x, z)=\\left(1-\\Phi\\left(z^{\\prime} \\gamma\\right)\\right)^{1-s}\\left[\\Phi\\left(\\frac{z^{\\prime} \\gamma+\\frac{\\sigma_{21}}{\\sigma^{2}}\\left(y-x^{\\prime} \\beta\\right)}{\\sqrt{1-\\frac{\\sigma_{21}}{\\sigma^{2}}}}\\right) \\frac{1}{\\sigma} \\phi\\left(\\frac{y-x^{\\prime} \\beta}{\\sigma}\\right)\\right]^{s} .\n\\]\nEvaluated at the observations we obtain the log-likelihood function\n\\[\n\\ell_{n}\\left(\\beta, \\gamma, \\sigma^{2}, \\sigma_{21}\\right)=\\sum_{S_{i}=0} \\log \\left(1-\\Phi\\left(Z_{i}^{\\prime} \\gamma\\right)\\right)+\\sum_{S_{i}=1}\\left[\\log \\Phi\\left(\\frac{Z_{i}^{\\prime} \\gamma+\\frac{\\sigma_{21}}{\\sigma^{2}}\\left(Y_{i}-X_{i}^{\\prime} \\beta\\right)}{\\sqrt{1-\\frac{\\sigma_{21}}{\\sigma^{2}}}}\\right)-\\frac{1}{2} \\log \\left(2 \\pi \\sigma^{2}\\right)-\\frac{1}{2 \\sigma^{2}}\\left(Y_{i}-X_{i}^{\\prime} \\beta\\right)^{2}\\right]\n\\]\nThe maximum likelihood estimator \\(\\left(\\widehat{\\beta}, \\widehat{\\gamma}, \\widehat{\\sigma}^{2}, \\widehat{\\sigma}_{21}\\right)\\) maximizes the log-likelihood.\nThe MLE is the preferred estimation method for final reporting. It can be computationally demanding in some applications, however, so the two-step estimator can be useful for preliminary analysis.\nIn Stata the two-step estimator and joint MLE can be obtained with the heckman command."
  },
  {
    "objectID": "chpt27-censor-selection.html#nonparametric-selection",
    "href": "chpt27-censor-selection.html#nonparametric-selection",
    "title": "25  Censoring and Selection",
    "section": "25.11 Nonparametric Selection",
    "text": "25.11 Nonparametric Selection\nA nonparametric selection model is\n\\[\n\\begin{aligned}\nY^{*} &=m(X)+e \\\\\nS^{*} &=g(Z)+u \\\\\nS &=\\mathbb{1}\\left\\{S^{*}>0\\right\\} \\\\\nY &=\\left\\{\\begin{array}{cc}\nY^{*} & \\text { if } S=1 \\\\\n\\text { missing } & \\text { if } S=0\n\\end{array}\\right.\n\\end{aligned}\n\\]\nwhere the distribution of \\((e, u)\\) is unknown. For simplicity we assume that \\((e, u)\\) are independent of \\((X, Z)\\).\nSelection occurs if \\(u>-g(Z)\\). This is unaffected by monotonically increasing transformations. Therefore the distribution of \\(u\\) is not separately identified from the function \\(g(Z)\\). Consequently we can normalize the distribution of \\(u\\) to a convenient form. Here we use the normal distribution: \\(u \\sim \\Phi(x)\\).\nSince the functions \\(m(X)\\) and \\(g(Z)\\) are nonparametric we can use series methods to approximate them by linear models of the form \\(m(X)=X^{\\prime} \\beta\\) and \\(g(Z)=Z^{\\prime} \\gamma\\) after suitable variable transformation. We will use this latter notation to link the models to estimation methods.\nThe conditional probability of selection is\n\\[\np(Z)=\\mathbb{P}[S=1 \\mid Z]=\\mathbb{P}\\left[u>-Z^{\\prime} \\gamma \\mid Z\\right]=\\Phi\\left(Z^{\\prime} \\gamma\\right) .\n\\]\nThe probability \\(p(Z)\\) is known as the propensity score; it is nonparametrically identified from the joint distribution of \\((S, Z)\\), so the function \\(g(Z)=Z^{\\prime} \\gamma\\) is identified. The coefficient \\(\\gamma\\) and propensity score can be estimated by binary choice methods, for example by a series probit regression.\nThe CEF of \\(Y\\) given selection is\n\\[\n\\mathbb{E}[Y \\mid X, Z, S=1]=X^{\\prime} \\beta+h_{1}\\left(Z^{\\prime} \\gamma\\right)\n\\]\nwhere \\(h_{1}(x)=\\mathbb{E}[e \\mid u>-x]\\). In general \\(h_{1}(x)\\) can take a range of possible shapes. When \\((e, u)\\) are jointly normal with covariance \\(\\sigma_{21}\\) then \\(h_{1}(x)=\\sigma_{21} \\lambda(x)\\) where \\(\\lambda(x)=\\phi(x) / \\Phi(x)\\) is the inverse Mills ratio. There are two alternative representations of the CEF which are potentially useful. Since \\(g(Z)=\\Phi^{-1}(p(Z))\\) we have the representation\n\\[\n\\left.\\mathbb{E}[Y \\mid X, Z, S=1]=X^{\\prime} \\beta+h_{2}(p(Z))\\right)\n\\]\nwhere \\(h_{2}(x)=h_{1}\\left(\\Phi^{-1}(x)\\right)\\). Also, because \\(\\lambda(x)\\) is invertible we have the representation\n\\[\n\\mathbb{E}[Y \\mid X, Z, S=1]=X^{\\prime} \\beta+h_{3}\\left(\\lambda\\left(Z^{\\prime} \\gamma\\right)\\right)\n\\]\nwhere \\(h_{3}(x)=h_{1}\\left(\\lambda^{-1}(x)\\right)\\).\nThe three equations (27.8)-(27.10) suggest three two-step approaches to nonparametric estimation which we now describe. Each is based on a first-step binary choice estimator \\(\\widehat{\\gamma}\\) of \\(\\gamma\\).\nEquation (27.8) suggests a regression of \\(Y\\) on \\(X\\) and a series expansion in \\(Z^{\\prime} \\widehat{\\gamma}\\), for example a loworder polynomial in \\(Z^{\\prime} \\widehat{\\gamma}\\)\nEquation (27.9) suggests a regression of \\(Y\\) on \\(X\\) and a series expansion in the propensity score \\(\\widehat{p}=\\) \\(\\Phi\\left(Z^{\\prime} \\widehat{\\gamma}\\right)\\), for example a low-order polynomial in \\(\\hat{p}\\).\nEquation (27.10) suggests a regression of \\(Y\\) on \\(X\\) and a series expansion in \\(\\hat{\\lambda}=\\lambda\\left(Z^{\\prime} \\hat{\\gamma}\\right)\\), for example a low-order polynomial in \\(\\hat{\\lambda}\\).\nThe advantage of expansions based on (27.10) is that it will be first-order accurate in the leading case of the normal distribution. This means that for distributions close to the normal, series expansions will be accurate even with a small number of terms. The advantage of expansions based on (27.9) is interpretability: The regression is expressed as a function of the propensity score.\nDas, Newey, and Vella (2003) provide a detailed asymptotic theory for this class of estimators focusing on those based on (27.9). They provide conditions under which the models are identified, the estimators consistent, and asymptotically normally distributed.\nThese nonparametric selection estimators are two-step estimators with generated regressors (see Section 12.26). Therefore conventional covariance matrix estimators and standard errors are inconsistent. Asymptotically valid covariance matrix estimators can be constructed using GMM. An alternative is to use bootstrap methods. The latter should be implemented as an explicit two-step estimator so that the first-step estimation is treated by the bootstrap distribution.\nA standard recommendation is that the regressors \\(Z\\) in the selection equation should include at least one relevant variable which is a valid exclusion from the regressors \\(X\\) in the main equation. The reason is that otherwise the series expansions for \\(m(x)\\) and \\(h\\left(Z^{\\prime} \\gamma\\right)\\) can be highly collinear and not separately identified. This insight applies to the parametric case as well. One difficulty is that in applications it may be challenging to identify variables which affect selection \\(S^{*}\\) but not the outcome \\(Y^{*}\\)."
  },
  {
    "objectID": "chpt27-censor-selection.html#panel-data",
    "href": "chpt27-censor-selection.html#panel-data",
    "title": "25  Censoring and Selection",
    "section": "25.12 Panel Data",
    "text": "25.12 Panel Data\nA panel censored regression (panel Tobit) equation is\n\\[\n\\begin{aligned}\n&Y_{i t}^{*}=X_{i t}^{\\prime} \\beta+u_{i}+e_{i t} \\\\\n&Y_{i t}=\\max \\left(Y_{i t}^{*}, 0\\right) .\n\\end{aligned}\n\\]\nThe individual effect \\(u_{i}\\) can be treated as a random effect (uncorrelated with the errors) or a fixed effect (unstructured correlation).\nA random effects estimator can be derived under the assumption of joint normality of the errors. This is implemented in the Stata command xttobit. The advantage is that the procedure is simple to implement. The disadvantages are those typically associated with random effects estimators.\nA fixed effects estimator was developed by Honoré (1992). His key insight is the following, which we illustrate assuming \\(T=2\\). If the errors \\(\\left(e_{i 1}, e_{i 2}\\right)\\) are independent of \\(\\left(X_{i 1}, X_{i 2}, u_{i}\\right)\\) then the distribution of \\(\\left(Y_{i 1}^{*}, Y_{i 2}^{*}\\right)\\) conditional on \\(\\left(X_{i 1}, X_{i 2}\\right)\\) is symmetric about the 45 degree line through the point \\(\\left(\\Delta X^{\\prime} \\beta, 0\\right)\\) in \\(\\left(Y_{1}, Y_{2}\\right)\\) space. This distribution does not depend on the fixed effect \\(u_{i}\\). From this symmetry and the censoring rules Honoré derived moment conditions which identify the coefficients \\(\\beta\\) and allow estimation by GMM. Honoré (1992) provides a complete asymptotic distribution theory. Honoré has provided a Stata command Pantob which implements his estimator and is available on his website. honore/stata/. A panel sample selection model is\n\\[\n\\begin{aligned}\nY_{i t}^{*} &=X_{i t}^{\\prime} \\beta+u_{i}+e_{i t} \\\\\nS_{i t}^{*} &=Z_{i t}^{\\prime} \\gamma+\\eta_{i}+v_{i t} \\\\\nS_{i t} &=\\mathbb{1}\\left\\{S_{i t}^{*}>0\\right\\} \\\\\nY_{i t} &=\\left\\{\\begin{array}{cc}\nY_{i t}^{*} & \\text { if } S_{i t}=1 \\\\\n\\text { missing } & \\text { if } S_{i t}=0\n\\end{array}\\right.\n\\end{aligned}\n\\]\nA method to estimated this model is presented in Kyriazidou (1997). Again for exposition we focus on the \\(T=2\\) case. Her estimator is motivated by the observation that \\(\\beta\\) could be consistently estimated by least squares applied to the sub-sample where \\(S_{i 1}=S_{i 2}=1\\) (both observations are selected) and \\(Z_{i 1}^{\\prime} \\gamma=Z_{i 2}^{\\prime} \\gamma\\) (both observations have same probability of selection). The parameter \\(\\gamma\\) is identified up to scale by the selection equation so can be estimated as \\(\\widehat{\\gamma}\\) by the methods described in Section \\(25.13\\) (e.g. Chamberlain (1980, 1984)). Given \\(\\hat{\\gamma}\\) we estimate \\(\\beta\\) by kernel-weighted least squares on the sub-sample with \\(S_{i 1}=S_{i 2}=1\\), with kernel weights depending on \\(\\left(Z_{i 1}-Z_{i 2}\\right)^{\\prime} \\widehat{\\gamma}\\). Kyriazidou (1997) provides a complete distribution theory."
  },
  {
    "objectID": "chpt27-censor-selection.html#exercises",
    "href": "chpt27-censor-selection.html#exercises",
    "title": "25  Censoring and Selection",
    "section": "25.13 Exercises",
    "text": "25.13 Exercises\nExercise 27.1 Derive (27.2) and (27.3). Hint: Use Theorems \\(5.7\\) and \\(5.8\\) of Probability and Statistics for Economists.\nExercise 27.2 Take the model\n\\[\n\\begin{aligned}\nY^{*} &=X^{\\prime} \\beta+e \\\\\ne & \\sim \\mathrm{N}\\left(0, \\sigma^{2}\\right) \\\\\nY &=\\left\\{\\begin{array}{cc}\nY^{*} & \\text { if } Y^{*} \\leq \\tau \\\\\n\\text { missing } & \\text { if } Y^{*}>\\tau\n\\end{array} .\\right.\n\\end{aligned}\n\\]\nIn this model, we say that \\(Y\\) is capped from above. Suppose you regress \\(Y\\) on \\(X\\). Is OLS consistent for \\(\\beta\\) ? Describe the nature of the effect of the mis-measured observation on the OLS estimator.\nExercise 27.3 Take the model\n\\[\n\\begin{aligned}\nY &=X^{\\prime} \\beta+e \\\\\ne & \\sim \\mathrm{N}\\left(0, \\sigma^{2}\\right) .\n\\end{aligned}\n\\]\nLet \\(\\widehat{\\beta}\\) denote the OLS estimator for \\(\\beta\\) based on an available sample.\n\nSuppose that an observation is in the sample only if \\(X_{1}>0\\) where \\(X_{1}\\) is an element of \\(X\\). Is \\(\\widehat{\\beta}\\) consistent for \\(\\beta\\) ? Obtain an expression for its probability limit.\nSuppose that an observation is in the sample only if \\(Y>0\\). Is \\(\\widehat{\\beta}\\) consistent for \\(\\widehat{\\beta}\\) ? Obtain an expression for its probability limit.\n\nExercise 27.4 For the censored conditional mean (27.2) propose a NLLS estimator of \\((\\beta, \\sigma)\\).\nExercise 27.5 For the truncated conditional mean (27.3) propose a NLLS estimator of \\((\\beta, \\sigma)\\). Exercise 27.6 A latent variable \\(Y^{*}\\) is generated by\n\\[\n\\begin{gathered}\nY^{*}=\\beta_{0}+X \\beta_{1}+e \\\\\ne \\mid X \\sim \\mathrm{N}\\left(0, \\sigma^{2}(X)\\right) \\\\\n\\sigma^{2}(X)=\\gamma_{0}+X^{2} \\gamma_{1} \\\\\nY=\\max \\left(Y^{*}, 0\\right) .\n\\end{gathered}\n\\]\nwhere \\(X\\) is scalar. Assume \\(\\gamma_{0}>0\\) and \\(\\gamma_{1}>0\\). The parameters are \\(\\beta, \\gamma_{0}, \\gamma_{1}\\). Find the log-likelihood function for the conditional distribution of \\(Y\\) given \\(X\\).\nExercise 27.7 Take the model\n\\[\n\\begin{aligned}\nS &=\\mathbb{1}\\left\\{X^{\\prime} \\gamma+u>0\\right\\} \\\\\nY &=\\left\\{\\begin{array}{cc}\nX^{\\prime} \\beta+e & \\text { if } S=1 \\\\\n\\text { missing } & \\text { if } S=0\n\\end{array}\\right.\\\\\n\\left(\\begin{array}{c}\ne \\\\\nu\n\\end{array}\\right) & \\sim \\mathrm{N}\\left(0,\\left(\\begin{array}{cc}\n\\sigma^{2} & \\sigma_{21} \\\\\n\\sigma_{21} & 1\n\\end{array}\\right)\\right)\n\\end{aligned}\n\\]\nShow \\(\\mathbb{E}[Y \\mid X, S=1]=X^{\\prime} \\beta+\\sigma_{21} \\lambda\\left(X^{\\prime} \\gamma\\right)\\).\nExercise 27.8 Show (27.7).\nExercise 27.9 Take the \\(\\mathrm{CH} \\mathrm{J} 2004\\) dataset. The variables tinkind and income are household transfers received in-kind and household income, respectively. Divide both variables by 1000 to standardize. Create the regressor Dincome \\(=(\\) income \\(-1) \\times \\mathbb{1}\\{\\) income \\(>1\\}\\).\n\nEstimate a linear regression of tinkind on income and Dincome. Interpret the results.\nCalculate the percentage of censored observations (the percentage for which tinkind= 0 . Do you expect censoring bias to be a problem in this example?\nSuppose you try and fix the problem by omitting the censored observations. Estimate the regression on the subsample of observations for which tinkind \\(>0\\).\nEstimate a Tobit regression of of tinkind on income and Dincome.\nEstimate the same regression using CLAD.\nInterpret and explain the differences between your results in (a)-(e).\n\nExercise 27.10 Take the cps09mar dataset and the subsample of individuals with at least 12 years of education. Create wage \\(=\\) earnings/(hours \\(\\times\\) weeks \\()\\) and lwage \\(=\\log (\\) wage \\()\\).\n\nEstimate a linear regression of lwage on education and education \\({ }^{\\wedge}\\). Interpret the results.\nSuppose the wage data had been capped about \\(\\$ 30\\) /hour. Create a variable cwage which is lwage capped at 3.4. Estimate a linear regression of cwage on education and education \\(\\wedge\\) 2. How would you interpret these results if you were unaware that the dependent variable was capped?\nSuppose you try and fix the problem by omitting the capped observations. Estimate the regression on the subsample of observations for which cwage is less than 3.4. (d) Estimate a Tobit regression of cwage on education and education^{}2 with upper censoring at \\(3.4\\).\nEstimate the same regression using CLAD. You may need to impose an upper censoring of 3.3.\nInterpret and explain the differences between your results in (a)-(e).\n\nExercise 27.11 Take the DDK2011 dataset. Create a variable testscore which is totalscore standardized to have mean zero and variance one. The variable tracking is a dummy indicating that the students were tracked (separated by initial test score). The varible percentile is the student’s percentile in the initial distribution. For the following regressions cluster by school.\n\nEstimate a linear regression of testscore on tracking, percentile, and percentile^{}2. Interpret the results.\nSuppose the scores were censored from below. Create a variable ctest which is testscore censored at 0 . Estimate a linear regression of ctest on tracking, percentile, and percentile \\({ }^{\\wedge}\\). How would you interpret these results if you were unaware that the dependent variable was censored?\nSuppose you try and fix the problem by omitting the censored observations. Estimate the regression on the subsample of observations for which ctest is positive.\nInterpret and explain the differences between your results in (a), (b), and (c)."
  },
  {
    "objectID": "chpt28-model-selection.html#introduction",
    "href": "chpt28-model-selection.html#introduction",
    "title": "26  Model Selection, Stein Shrinkage, and Model Averaging",
    "section": "26.1 Introduction",
    "text": "26.1 Introduction\nThe chapter reviews model selection, James-Stein shrinkage, and model averaging.\nModel selection is a tool for selecting one model (or estimator) out of a set of models. Different model selection methods are distinguished by the criteria used to rank and compare models.\nModel averaging is a generalization of model selection. Models and estimators are averaged using data-dependent weights.\nJames-Stein shrinkage modifies classical estimators by shrinking towards a reasonable target. Shrinking reduces mean squared error.\nTwo excellent monographs on model selection and averaging are Burnham and Anderson (1998) and Claeskens and Hjort (2008). James-Stein shrinkage theory is thoroughly covered in Lehmann and Casella (1998). See also Wasserman (2006) and Efron (2010)."
  },
  {
    "objectID": "chpt28-model-selection.html#model-selection",
    "href": "chpt28-model-selection.html#model-selection",
    "title": "26  Model Selection, Stein Shrinkage, and Model Averaging",
    "section": "26.2 Model Selection",
    "text": "26.2 Model Selection\nIn the course of an applied project an economist will routinely estimate multiple models. Indeed, most applied papers include tables displaying the results from different specifications. The question arises: Which model is best? Which should be used in practice? How can we select the best choice? This is the question of model selection.\nTake, for example, a wage regression. Suppose we want a model which includes education, experience, region, and marital status. How should we proceed? Should we estimate a simple linear model plus a quadratic in experience? Should education enter linearly, a simple spline as in Figure 2.6(a), or with separate dummies for each education level? Should marital status enter as a simple dummy (married or not) or allowing for all recorded categories? Should interactions be included? Which? How many? Taken together we need to select the specific regressors to include in the regression model.\nModel “selection” may be mis-named. It would be more appropriate to call the issue “estimator selection”. When we examine a table containing the results from multiple regressions we are comparing multiple estimates of the same regression. One estimator may include fewer variables than another; that is a restricted estimator. One may be estimated by least squares and another by 2SLS. Another could be nonparametric. The underlying model is the same; the difference is the estimator. Regardless, the literature has adopted the term “model selection” and we will adhere to this convention. To gain some basic understanding it may be helpful to start with a stylzed example. Suppose that we have a \\(K \\times 1\\) estimator \\(\\widehat{\\theta}\\) which has expectation \\(\\theta\\) and known covariance matrix \\(\\boldsymbol{V}\\). An alternative feasible estimator is \\(\\widetilde{\\theta}=0\\). The latter may seem like a silly estimator but it captures the feature that model selection typically concerns exclusion restrictions. In this context we can compare the accuracy of the two estimators by their weighted mean-squared error (WMSE). For a given weight matrix \\(\\boldsymbol{W}\\) define\n\\[\n\\text { wmse }[\\widehat{\\theta}]=\\operatorname{tr}\\left(\\mathbb{E}\\left[(\\widehat{\\theta}-\\theta)(\\widehat{\\theta}-\\theta)^{\\prime}\\right] \\boldsymbol{W}\\right)=\\mathbb{E}\\left[(\\widehat{\\theta}-\\theta)^{\\prime} \\boldsymbol{W}(\\widehat{\\theta}-\\theta)\\right] \\text {. }\n\\]\nThe calculations simplify by setting \\(\\boldsymbol{W}=\\boldsymbol{V}^{-1}\\) which we do for our remaining calculations.\nFor our two estimators we calculate that\n\\[\n\\begin{aligned}\n\\text { wmse }[\\hat{\\theta}] &=K \\\\\n\\text { wmse }[\\widetilde{\\theta}] &=\\theta^{\\prime} \\boldsymbol{V}^{-1} \\theta \\stackrel{\\text { def }}{=} \\lambda .\n\\end{aligned}\n\\]\n(See Exercise 28.1) The WMSE of \\(\\widehat{\\theta}\\) is smaller if \\(K<\\lambda\\) and the WMSE of \\(\\widetilde{\\theta}\\) is smaller if \\(K>\\lambda\\). One insight from this simple analysis is that we should prefer smaller (simpler) models when potentially omitted variables have small coefficients relative to estimation variance, and should prefer larger (more complicated) models when these variables have large coefficients relative to estimation variance. Another insight is that this choice is infeasible because \\(\\lambda\\) is unknown.\nThe comparison between (28.1) and (28.2) is a basic bias-variance trade-off. The estimator \\(\\widehat{\\theta}\\) is unbiased but has a variance contribution of \\(K\\). The estimator \\(\\widetilde{\\theta}\\) has zero variance but has a squared bias contribution \\(\\lambda\\). The WMSE combines these two components.\nSelection based on WMSE suggests that we should ideally select the estimator \\(\\widehat{\\theta}\\) if \\(K<\\lambda\\) and select \\(\\tilde{\\theta}\\) if \\(K>\\lambda\\). A feasible implementation replaces \\(\\lambda\\) with an estimator. A plug-in estimator is \\(\\hat{\\lambda}=\\widehat{\\theta}^{\\prime} \\boldsymbol{V}^{-1} \\widehat{\\theta}=W\\), the Wald statistic for the test of \\(\\theta=0\\). However, the estimator \\(\\widehat{\\lambda}\\) has expectation\n\\[\n\\mathbb{E}[\\widehat{\\lambda}]=\\mathbb{E}\\left[\\widehat{\\theta}^{\\prime} \\boldsymbol{V}^{-1} \\hat{\\theta}\\right]=\\theta^{\\prime} \\boldsymbol{V}^{-1 \\prime} \\theta+\\mathbb{E}\\left[(\\widehat{\\theta}-\\theta)^{\\prime} \\boldsymbol{V}^{-1}(\\widehat{\\theta}-\\theta)\\right]=\\lambda+K\n\\]\nso is biased. An unbiased estimator is \\(\\tilde{\\lambda}=\\widehat{\\lambda}-K\\). Notice that \\(\\tilde{\\lambda}>K\\) is the same as \\(W>2 K\\). This leads to the model-selection rule: Use \\(\\widehat{\\theta}\\) if \\(W>2 K\\) and use \\(\\widetilde{\\theta}\\) otherwise.\nThis is an overly-simplistic setting but highlights the fundamental ingredients of criterion-based model selection. Comparing the MSE of different estimators typically involves a trade-off between the bias and variance with more complicated models exhibiting less bias but increased estimation variance. The actual trade-off is unknown because the bias depends on the unknown true parameters. The bias, however, can be estimated, giving rise to empirical estimates of the MSE and empirical model selection rules.\nA large number of model selection criteria have been proposed. We list here those most frequently used in applied econometrics.\nWe first list selection criteria for the linear regression model \\(Y=X^{\\prime} \\beta+e\\) with \\(\\sigma^{2}=\\mathbb{E}\\left[e^{2}\\right]\\) and a \\(k \\times 1\\) coefficient vector \\(\\beta\\). Let \\(\\widehat{\\beta}\\) be the least squares estimator, \\(\\widehat{e}_{i}\\) the least squares residual, and \\(\\widehat{\\sigma}^{2}=n^{-1} \\sum_{i=1}^{n} \\widehat{e}_{i}^{2}\\) the variance estimator. The number of estimated parameters \\(\\left(\\beta\\right.\\) and \\(\\left.\\sigma^{2}\\right)\\) is \\(K=k+1\\)."
  },
  {
    "objectID": "chpt28-model-selection.html#bayesian-information-criterion",
    "href": "chpt28-model-selection.html#bayesian-information-criterion",
    "title": "26  Model Selection, Stein Shrinkage, and Model Averaging",
    "section": "26.3 Bayesian Information Criterion",
    "text": "26.3 Bayesian Information Criterion\n\\[\n\\mathrm{BIC}=n+n \\log \\left(2 \\pi \\widehat{\\sigma}^{2}\\right)+K \\log (n) .\n\\]"
  },
  {
    "objectID": "chpt28-model-selection.html#akaike-information-criterion",
    "href": "chpt28-model-selection.html#akaike-information-criterion",
    "title": "26  Model Selection, Stein Shrinkage, and Model Averaging",
    "section": "26.4 Akaike Information Criterion",
    "text": "26.4 Akaike Information Criterion\n\\[\n\\mathrm{AIC}=n+n \\log \\left(2 \\pi \\widehat{\\sigma}^{2}\\right)+2 K .\n\\]"
  },
  {
    "objectID": "chpt28-model-selection.html#cross-validation",
    "href": "chpt28-model-selection.html#cross-validation",
    "title": "26  Model Selection, Stein Shrinkage, and Model Averaging",
    "section": "26.5 Cross-Validation",
    "text": "26.5 Cross-Validation\n\\[\n\\mathrm{CV}=\\sum_{i=1}^{n} \\widetilde{e}_{i}^{2}\n\\]\nwhere \\(\\widetilde{e}_{i}\\) are the least squares leave-one-out prediction errors.\nWe next list two commonly-used selection criteria for likelihood-based estimation. Let \\(f(y, \\theta)\\) be a parametric density with a \\(K \\times 1\\) parameter \\(\\theta\\). The likelihood \\(L_{n}(\\theta)=\\prod_{i=1}^{n} f\\left(Y_{i}, \\theta\\right)\\) is the density evaluated at the observations. The maximum likelihood estimator \\(\\widehat{\\theta} \\operatorname{maximizes} \\ell_{n}(\\theta)=\\log L_{n}(\\theta)\\)."
  },
  {
    "objectID": "chpt28-model-selection.html#bayesian-information-criterion-1",
    "href": "chpt28-model-selection.html#bayesian-information-criterion-1",
    "title": "26  Model Selection, Stein Shrinkage, and Model Averaging",
    "section": "26.6 Bayesian Information Criterion",
    "text": "26.6 Bayesian Information Criterion\n\\[\n\\mathrm{BIC}=-2 \\ell_{n}(\\widehat{\\theta})+K \\log (n) .\n\\]"
  },
  {
    "objectID": "chpt28-model-selection.html#akaike-information-criterion-1",
    "href": "chpt28-model-selection.html#akaike-information-criterion-1",
    "title": "26  Model Selection, Stein Shrinkage, and Model Averaging",
    "section": "26.7 Akaike Information Criterion",
    "text": "26.7 Akaike Information Criterion\n\\[\n\\mathrm{AIC}=-2 \\ell_{n}(\\widehat{\\theta})+2 K .\n\\]\nIn the following sections we derive and discuss these and other model selection criteria."
  },
  {
    "objectID": "chpt28-model-selection.html#bayesian-information-criterion-2",
    "href": "chpt28-model-selection.html#bayesian-information-criterion-2",
    "title": "26  Model Selection, Stein Shrinkage, and Model Averaging",
    "section": "26.8 Bayesian Information Criterion",
    "text": "26.8 Bayesian Information Criterion\nThe Bayesian Information Criterion (BIC), also known as the Schwarz Criterion, was introduced by Schwarz (1978). It is appropriate for parametric models estimated by maximum likelihood and is used to select the model with the highest approximate probability of being the true model.\nLet \\(\\pi(\\theta)\\) be the prior density for \\(\\theta\\). The joint density of \\(Y\\) and \\(\\theta\\) is \\(f(y, \\theta) \\pi(\\theta)\\). The marginal density of \\(Y\\) is\n\\[\np(y)=\\int f(y, \\theta) \\pi(\\theta) d \\theta\n\\]\nThe marginal density \\(p(Y)\\) evaluated at the observations is known as the marginal likelihood.\nSchwarz (1978) established the following approximation.\nTheorem 28.1 Schwarz. If the model \\(f(y, \\theta)\\) satisfies standard regularity conditions and the prior \\(\\pi(\\theta)\\) is diffuse then\n\\[\n-2 \\log p(Y)=-2 \\ell_{n}(\\widehat{\\theta})+K \\log (n)+O(1)\n\\]\nwhere the \\(O(1)\\) term is bounded as \\(n \\rightarrow \\infty\\).\nA heuristic proof for normal linear regression is given in Section 28.32. A “diffuse” prior is one which distributes weight uniformly over the parameter space.\nSchwarz’s theorem shows that the marginal likelihood approximately equals the maximized likelihood multiplied by an adjustment depending on the number of estimated parameters and the sample size. The approximation (28.6) is commonly called the Bayesian Information Criterion or BIC. The BIC is a penalized \\(\\log\\) likelihood. The term \\(K \\log (n)\\) can be interpreted as an over-parameterization penalty. The multiplication of the log likelihood by \\(-2\\) is traditional as it puts the criterion into the same units as a log-likelihood statistic. In the context of normal linear regression we have calculated in (5.6) that\n\\[\n\\ell_{n}(\\widehat{\\theta})=-\\frac{n}{2}(\\log (2 \\pi)+1)-\\frac{n}{2} \\log \\left(\\widehat{\\sigma}^{2}\\right)\n\\]\nwhere \\(\\widehat{\\sigma}^{2}\\) is the residual variance estimate. Hence BIC equals (28.3) with \\(K=k+1\\).\nSince \\(n \\log (2 \\pi)+n\\) does not vary across models this term is often omitted. It is better, however, to define the BIC as described above so that different parametric families are comparable. It is also useful to know that some authors define the BIC by dividing the above expression by \\(n\\) (e.g. \\(\\mathrm{BIC}=\\log \\left(2 \\pi \\widehat{\\sigma}^{2}\\right)+\\) \\(K \\log (n) / n)\\) which does not change the rankings between models. However, this is an unwise choice because it alters the scaling, making it difficult to compare the degree of difference between models.\nNow suppose that we have two models \\(\\mathscr{M}_{1}\\) and \\(\\mathscr{M}_{2}\\) which have marginal likelihoods \\(p_{1}(Y)\\) and \\(p_{2}(Y)\\). Assume that both models have equal prior probability. Bayes Theorem states that the probability that a model is true given the data is proportional to its marginal likelihood. Specifically\n\\[\n\\begin{aligned}\n&\\mathbb{P}\\left[\\mathscr{M}_{1} \\mid Y\\right]=\\frac{p_{1}(Y)}{p_{1}(Y)+p_{2}(Y)} \\\\\n&\\mathbb{P}\\left[\\mathscr{M}_{2} \\mid Y\\right]=\\frac{p_{2}(Y)}{p_{1}(Y)+p_{2}(Y)} .\n\\end{aligned}\n\\]\nBayes selection picks the model with highest probability. Thus if \\(p_{1}(Y)>p_{2}(Y)\\) we select \\(\\mathscr{M}_{1}\\). If \\(p_{1}(Y)<p_{2}(Y)\\) we select \\(\\mathscr{M}_{2}\\).\nFinding the model with highest marginal likelihood is the same as finding the model with lowest value of \\(-2 \\log p(Y)\\). Theorem \\(28.1\\) shows that the latter approximately equals the BIC. BIC selection picks the model with the lowest \\({ }^{1}\\) value of BIC. Thus BIC selection is approximate Bayes selection.\nThe above discussion concerned two models but applies to any number of models. BIC selection picks the model with the smallest BIC. For implementation you simply estimate each model, calculate its BIC, and compare. model.\nThe BIC may be obtained in Stata by using the command estimates stats after an estimated"
  },
  {
    "objectID": "chpt28-model-selection.html#akaike-information-criterion-for-regression",
    "href": "chpt28-model-selection.html#akaike-information-criterion-for-regression",
    "title": "26  Model Selection, Stein Shrinkage, and Model Averaging",
    "section": "26.9 Akaike Information Criterion for Regression",
    "text": "26.9 Akaike Information Criterion for Regression\nThe Akaike Information Criterion (AIC) was introduced by Akaike (1973). It is used to select the model whose estimated density is closest to the true density. It is designed for parametric models estimated by maximum likelihood.\nLet \\(\\widehat{f}(y)\\) be an estimator of the unknown true density \\(g(y)\\) of the observation vector \\(Y=\\left(Y_{1}, \\ldots, Y_{n}\\right)\\). For example, the normal linear regression estimate of \\(g(y)\\) is \\(\\widehat{f}(y)=\\prod_{i=1}^{n} \\phi_{\\widehat{\\sigma}}\\left(Y_{i}-X_{i}^{\\prime} \\widehat{\\beta}\\right)\\).\nTo measure the distance between the two densities \\(g\\) and \\(\\widehat{f}\\) Akaike used the Kullback-Leibler information criterion (KLIC)\n\\[\n\\operatorname{KLIC}(g, f)=\\int g(y) \\log \\left(\\frac{g(y)}{f(y)}\\right) d y .\n\\]\nNotice that \\(\\operatorname{KLIC}(g, f)=0\\) when \\(f(y)=g(y)\\). By Jensen’s inequality,\n\\[\n\\operatorname{KLIC}(g, f)=-\\int g(y) \\log \\left(\\frac{f(y)}{g(y)}\\right) d y \\geq-\\log \\int f(y) d y=0 .\n\\]\nThus \\(\\operatorname{KLIC}(g, f)\\) is a non-negative measure of the deviation of \\(f\\) from \\(g\\), with small values indicating a smaller deviation.\n\\({ }^{1}\\) When the BIC is negative this means taking the most negative value. The KLIC distance between the true and estimated densities is\n\\[\n\\begin{aligned}\n\\operatorname{KLIC}(g, \\widehat{f}) &=\\int g(y) \\log \\left(\\frac{g(y)}{\\widehat{f}(y)}\\right) d y \\\\\n&=\\int g(y) \\log (g(y)) d y-\\int g(y) \\log (\\widehat{f}(y)) d y .\n\\end{aligned}\n\\]\nThis is random as it depends on the estimator \\(\\widehat{f}\\). Akaike proposed the expected KLIC distance\n\\[\n\\mathbb{E}[\\operatorname{KLIC}(g, \\widehat{f})]=\\int g(y) \\log (g(y)) d y-\\mathbb{E}\\left[\\int g(y) \\log (\\widehat{f}(y)) d y\\right] .\n\\]\nThe first term in (28.8) does not depend on the model. So minimization of expected KLIC distance is minimization of the second term. Multiplied by 2 (similarly to the BIC) this is\n\\[\nT=-2 \\mathbb{E}\\left[\\int g(y) \\log (\\widehat{f}(y)) d y\\right] .\n\\]\nThe expectation is over the random estimator \\(\\widehat{f}\\).\nAn alternative interpretation is to notice that the integral in (28.9) is an expectation over \\(Y\\) with respect to the true data density \\(g(y)\\). Thus we can write (28.9) as\n\\[\nT=-2 \\mathbb{E}[\\log (\\widehat{f}(\\widetilde{Y}))]\n\\]\nwhere \\(\\tilde{Y}\\) is an independent copy of \\(Y\\). The key to understand this expression is that both the estimator \\(\\widehat{f}\\) and the evaluation points \\(\\widetilde{Y}\\) are random and independent. \\(T\\) is the expected log-likelihood fit using the estimated model \\(\\widehat{f}\\) of an out-of-sample realization \\(\\widetilde{Y}\\). Thus \\(T\\) can be interpreted as an expected predictive log likelihood. Models with low values of \\(T\\) have good fit based on the out-of-sample loglikelihood.\nTo gain further understanding we consider the simple case of the normal linear regression model with \\(K\\) regressors. The log density of the model for the observations is\n\\[\n\\log f(Y, \\boldsymbol{X}, \\theta)=-\\frac{n}{2} \\log \\left(2 \\pi \\sigma^{2}\\right)-\\frac{1}{2 \\sigma^{2}} \\sum_{i=1}^{n}\\left(Y_{i}-X_{i}^{\\prime} \\beta\\right)^{2} .\n\\]\nThe expected value at the true parameter values is \\(-\\frac{n}{2} \\log \\left(2 \\pi \\sigma^{2}\\right)-\\frac{n}{2}\\). This means that the idealized value of \\(T\\) is \\(T_{0}=n \\log \\left(2 \\pi \\sigma^{2}\\right)+n\\). This would be the value obtained if there were no estimation error.\nTo simplify the calculations, we add the assumption that the variance \\(\\sigma^{2}\\) is known.\nTheorem 28.2 Suppose \\(\\widehat{f}(y)\\) is an estimated normal linear regression model with \\(K\\) regressors and a known variance \\(\\sigma^{2}\\). Suppose that the true density \\(g(y)\\) is a conditionally homoskedastic regression with variance \\(\\sigma^{2}\\). Then\n\\[\n\\begin{aligned}\nT &=n \\log \\left(2 \\pi \\sigma^{2}\\right)+n+K=T_{0}+K \\\\\n\\mathbb{E}\\left[-2 \\ell_{n}(\\widehat{\\theta})\\right] &=n \\log \\left(2 \\pi \\sigma^{2}\\right)+n-K=T_{0}-K .\n\\end{aligned}\n\\]\nThe proof is given in Section \\(28.32\\). These expressions are interesting. Expression (28.12) shows that the expected KLIC distance \\(T\\) equals the idealized value \\(T_{0}\\) plus \\(K\\). The latter is the cost of parameter estimation, measured in terms of expected KLIC distance. By estimating parameters (rather than using the true values) the expected KLIC distance increases by \\(K\\).\nExpression (28.13) shows the converse story. It shows that the sample log-likelihood function is smaller than the idealized value \\(T_{0}\\) by \\(K\\). This is the cost of in-sample over-fitting. The sample loglikelihood is an in-sample measure of fit and therefore understates the population log-likelihood. The two expressions together show that the expected sample log-likelihood is smaller than the target value \\(T\\) by \\(2 K\\). This is the combined cost of over-fitting and parameter estimation.\nCombining these expressions we can suggest an unbiased estimator for \\(T\\). In the normal regression model we use (28.4). Since \\(n \\log (2 \\pi)+n\\) does not vary across models it are often omitted. Thus for linear regression it is common to use the definition \\(\\mathrm{AIC}=n \\log \\left(\\widehat{\\sigma}^{2}\\right)+2 K\\).\nInterestingly the AIC takes a similar form to the BIC. Both the AIC and BIC are penalized log likelihoods, and both penalties are proportional to the number of estimated parameters \\(K\\). The difference is that the AIC penalty is \\(2 K\\) while the BIC penalty is \\(K \\log (n)\\). Since \\(2<\\log (n)\\) if \\(n \\geq 8\\) the BIC uses a stronger parameterization penalty.\nSelecting a model by the AIC is equivalent to calculating the AIC for each model and selecting the model with the lowest \\({ }^{2}\\) value.\nTheorem 28.3 Under the assumptions of Theorem 28.2, \\(\\mathbb{E}[\\mathrm{AIC}]=T\\). AIC is thus an unbiased estimator of \\(T\\).\nOne of the interesting features of these results are that they are exact - there is no approximation and they do not require that the true error is normally distributed. The critical assumption is conditional homoskedasticity. If homoskedasticity fails then the AIC loses its validity.\nThe AIC may be obtained in Stata by using the command estimates stats after an estimated model."
  },
  {
    "objectID": "chpt28-model-selection.html#akaike-information-criterion-for-likelihood",
    "href": "chpt28-model-selection.html#akaike-information-criterion-for-likelihood",
    "title": "26  Model Selection, Stein Shrinkage, and Model Averaging",
    "section": "26.10 Akaike Information Criterion for Likelihood",
    "text": "26.10 Akaike Information Criterion for Likelihood\nFor the general likelihood context Akaike proposed the criterion (28.7). Here, \\(\\widehat{\\theta}\\) is the maximum likelihood estimator, \\(\\ell_{n}(\\widehat{\\theta})\\) is the maximized log-likelihood function, and \\(K\\) is the number of estimated parameters. This specializes to (28.4) for the case of a normal linear regression model.\nAs for regression, AIC selection is performed by estimating a set of models, calculating AIC for each, and selecting the model with the smallest AIC.\nThe advantages of the AIC are that it is simple to calculate, easy to implement, and straightforward to interpret. It is intuitive as it is a simple penalized likelihood.\nThe disadvantage is that its simplicity may be deceptive. The proof shows that the criterion is based on a quadratic approximation to the log likelihood and an asymptotic chi-square approximation to the classical Wald statistic. When these conditions fail then the AIC may not be accurate. For example, if the model is an approximate (quasi) likelihood rather than a true likelihood then the failure of the information matrix equality implies that the classical Wald statistic is not asymptotically chi-square. In this case the accuracy of AIC fails. Another problem is that many nonlinear models have parameter regions where parametric identification fails. In these models the quadratic approximation to the log\n\\({ }^{2}\\) When the AIC is negative this means taking the most negative value. likelihood function fails to hold uniformly in the parameter space so the accuracy of the AIC fails. These qualifications point to challenges in interpretation of the AIC in nonlinear models.\nThe following is an analog of Theorem 28.3.\nTheorem 28.4 Under standard regularity conditions for maximum likelihood estimation, plus the assumption that certain statistics (identified in the proof) are uniformly integrable, \\(\\mathbb{E}[\\mathrm{AIC}]=T+O\\left(n^{1 / 2}\\right)\\). AIC is thus an approximately unbiased estimator of \\(T\\)\nA sketch of the proof is given in Section \\(28.32\\).\nThis result shows that the AIC is, in general, a reasonable estimator of the KLIC fit of an estimated parametric model. The theorem holds broadly for maximum likelihood estimation and thus the AIC can be used in a wide variety of contexts."
  },
  {
    "objectID": "chpt28-model-selection.html#mallows-criterion",
    "href": "chpt28-model-selection.html#mallows-criterion",
    "title": "26  Model Selection, Stein Shrinkage, and Model Averaging",
    "section": "26.11 Mallows Criterion",
    "text": "26.11 Mallows Criterion\nThe Mallows Criterion was proposed by Mallows (1973) and is often called the \\(C_{p}\\) criterion. It is appropriate for linear estimators of homoskedastic regression models.\nTake the homoskedastic regression framework\n\\[\n\\begin{aligned}\nY &=m+e \\\\\nm &=m(X) \\\\\n\\mathbb{E}[e \\mid X] &=0 \\\\\n\\mathbb{E}\\left[e^{2} \\mid X\\right] &=\\sigma^{2} .\n\\end{aligned}\n\\]\nWrite the first equation in vector notation for the \\(n\\) observations as \\(\\boldsymbol{Y}=\\boldsymbol{m}+\\boldsymbol{e}\\). Let \\(\\widehat{\\boldsymbol{m}}=\\boldsymbol{A} \\boldsymbol{Y}\\) be a linear estimator of \\(\\boldsymbol{m}\\), meaning that \\(\\boldsymbol{A}\\) is some \\(n \\times n\\) function of the regressor matrix \\(\\boldsymbol{X}\\) only. The residuals are \\(\\widehat{\\boldsymbol{e}}=\\boldsymbol{Y}-\\widehat{\\boldsymbol{m}}\\). The class of linear estimators includes least squares, weighted least squares, kernel regression, local linear regression, and series regression. For example, the least squares estimator using a regressor matrix \\(\\boldsymbol{Z}\\) is the case \\(\\boldsymbol{A}=\\boldsymbol{Z}\\left(\\boldsymbol{Z}^{\\prime} \\boldsymbol{Z}\\right)^{-1} \\boldsymbol{Z}^{\\prime}\\).\nMallows (1973) proposed the criterion\n\\[\nC_{p}=\\widehat{\\boldsymbol{e}}^{\\prime} \\widehat{\\boldsymbol{e}}+2 \\widetilde{\\sigma}^{2} \\operatorname{tr}(\\boldsymbol{A})\n\\]\nwhere \\(\\widetilde{\\sigma}^{2}\\) is a preliminary estimator of \\(\\sigma^{2}\\) (typically based on fitting a large model). In the case of least squares regression with \\(K\\) coefficients this simplifies to\n\\[\nC_{p}=n \\widehat{\\sigma}^{2}+2 K \\widetilde{\\sigma}^{2} .\n\\]\nThe Mallows crierion can be used similarly to the AIC. A set of regression models are estimated and the criterion \\(C_{p}\\) calculated for each. The model with the smallest value of \\(C_{p}\\) is the Mallows-selected model.\nMallows designed the criterion \\(C_{p}\\) as an unbiased estimator of the following measure of fit\n\\[\nR=\\mathbb{E}\\left[\\sum_{i=1}^{n}\\left(\\widehat{m}_{i}-m_{i}\\right)^{2}\\right] .\n\\]\nThis is the expected squared difference between the estimated and true regressions evaluated at the observations.\nAn alternative motivation for \\(R\\) is in terms of prediction accuracy. Consider an independent set of observations \\(\\widetilde{Y}_{i}, i=1, \\ldots, n\\), which have the same regressors \\(X_{i}\\) as those in sample. Consider prediction of \\(\\widetilde{Y}_{i}\\) given \\(X_{i}\\) and the fitted regression. The least squares predictor is \\(\\widehat{m}_{i}\\). The sum of expected squared prediction errors is\n\\[\n\\text { MSFE }=\\sum_{i=1}^{n} \\mathbb{E}\\left[\\left(\\widetilde{Y}_{i}-\\widehat{m}_{i}\\right)^{2}\\right] .\n\\]\nThe best possible (infeasible) value of this quantity is\n\\[\n\\operatorname{MSFE}_{0}=\\sum_{i=1}^{n} \\mathbb{E}\\left[\\left(\\widetilde{Y}_{i}-m_{i}\\right)^{2}\\right] .\n\\]\nThe difference is the prediction accuracy of the estimator:\n\\[\n\\begin{aligned}\n\\operatorname{MSFE}^{-\\operatorname{MSFE}_{0}} &=\\sum_{i=1}^{n} \\mathbb{E}\\left[\\left(\\widetilde{Y}_{i}-\\widehat{m}_{i}\\right)^{2}\\right]-\\sum_{i=1}^{n} \\mathbb{E}\\left[\\left(\\widetilde{Y}_{i}-m_{i}\\right)^{2}\\right] \\\\\n&=\\mathbb{E}\\left[\\sum_{i=1}^{n}\\left(\\widehat{m}_{i}-m_{i}\\right)^{2}\\right] \\\\\n&=R\n\\end{aligned}\n\\]\nwhich equals Mallows’ measure of fit. Thus \\(R\\) is a measure of prediction accuracy.\nWe stated that the Mallows criterion is an unbiased estimator of \\(R\\). More accurately, the adjusted criterion \\(C_{p}^{*}=C_{p}-\\boldsymbol{e}^{\\prime} \\boldsymbol{e}\\) is unbiased for \\(R\\). When comparing models \\(C_{p}\\) and \\(C_{p}^{*}\\) are equivalent so this substitution has no consequence for model selection.\nTheorem 28.5 If \\(\\widehat{\\boldsymbol{m}}=\\boldsymbol{A} \\boldsymbol{Y}\\) is a linear estimator, the regression error is conditionally mean zero and homoskedastic, and \\(\\widetilde{\\sigma}^{2}\\) is unbiased for \\(\\sigma^{2}\\), then\n\\[\n\\mathbb{E}\\left[C_{p}^{*}\\right]=R\n\\]\nso the adjusted Mallows criterion \\(C_{p}^{*}\\) is an unbiased estimator of \\(R\\).\nThe proof is given in Section 28.32."
  },
  {
    "objectID": "chpt28-model-selection.html#hold-out-criterion",
    "href": "chpt28-model-selection.html#hold-out-criterion",
    "title": "26  Model Selection, Stein Shrinkage, and Model Averaging",
    "section": "26.12 Hold-Out Criterion",
    "text": "26.12 Hold-Out Criterion\nDividing the sample into two parts, one for estimation and the second for evaluation, creates a simple device for model evaluation and selection. This procedure is often labelled hold-out evaluation. In the recent machine learning literature the data division is typically described as a training sample and a test sample.\nThe sample is typically divided randomly so that the estimation (training) sample has \\(N\\) observations and the evaluation (test) sample has \\(P\\) observations, where \\(N+P=n\\). There is no universal rule for the choice of \\(N \\& P\\), but \\(N=P=n / 2\\) is a standard choice. For more complicated procedures, such as the evaluation of model selection methods, it is desirable to make a tripartite division of the sample into (1) training, (2) model selection, and (3) final estimation and assessment. This can be particularly useful when it is desired to obtain a parameter estimator whose distribution is not distorted by the model selection process. Such divisions are most suited for a context of an extremely large sample.\nTake the standard case of a bipartite division where \\(1 \\leq i \\leq N\\) is the estimation sample and \\(N+1 \\leq\\) \\(i \\leq N+P\\) is the evaluation sample. On the estimation sample we construct the parameter estimates, for example the least squares coefficients\n\\[\n\\widetilde{\\beta}_{N}=\\left(\\sum_{i=1}^{N} X_{i} X_{i}^{\\prime}\\right)^{-1}\\left(\\sum_{i=1}^{N} X_{i} Y_{i}\\right)\n\\]\nCombining this coefficient with the evaluation sample we calculate the prediction errors \\(\\widetilde{e}_{N, i}=Y_{i}-X_{i}^{\\prime} \\widetilde{\\beta}_{N}\\) for \\(i \\geq N+1\\).\nIn Section \\(4.12\\) we defined the mean squared forecast error (MSFE) based on a estimation sample of size \\(N\\) as the expectation of the squared out-of-sample prediction error \\(\\operatorname{MSFE}_{N}=\\mathbb{E}\\left[\\widetilde{e}_{N, i}^{2}\\right]\\). The hold-out estimator of the MSFE is the average of the squared prediction errors\n\\[\n\\widetilde{\\sigma}_{N, P}^{2}=\\frac{1}{P} \\sum_{i=N+1}^{N+P} \\widetilde{e}_{N, i}^{2} .\n\\]\nWe can see that \\(\\widetilde{\\sigma}_{N, P}^{2}\\) is unbiased for \\(\\mathrm{MSFE}_{N}\\).\nWhen \\(N=P\\) we can improve estimation of the MSFE by flipping the procedure. Exchanging the roles of estimation and evaluation samples we obtain a second MSFE estimator, say \\(\\widetilde{\\omega}_{N, P}^{2}\\). The global estimator is their average \\(\\widetilde{\\sigma}_{N, P}^{* 2}=\\left(\\widetilde{\\sigma}_{N, P}^{2}+\\widetilde{\\omega}_{N, P}^{2}\\right) / 2\\). This estimator also has expectation MSFE \\({ }_{N}\\) but has reduced variance.\nThe estimated MSFE \\(\\widetilde{\\sigma}_{N, P}^{* 2}\\) can be used for model selection. The quantity \\(\\widetilde{\\sigma}_{N, P}^{* 2}\\) is calculated for a set of proposed models. The selected model is the one with the smallest value of \\(\\widetilde{\\sigma}_{N, P}^{* 2}\\). The method is intuitive, general, and flexible, and does not rely on technical assumptions.\nThe hold-out method has two disadvantages. First, if our goal is estimation using the full sample, our desired estimate is \\(\\mathrm{MSFE}_{n}\\), not \\(\\operatorname{MSFE}_{N}\\). Hold-out estimation provides an estimator of the MSFE based on estimation using a substantially reduced sample size, and is thus biased for the MSFE based on estimation using the full sample. Second, the estimator \\(\\widetilde{\\sigma}_{N, P}^{* 2}\\) is sensitive to the random sorting of the observations into the estimation and evaluation samples. This affects model selection. Results can depend on the initial sample sorting and are therefore partially arbitrary."
  },
  {
    "objectID": "chpt28-model-selection.html#cross-validation-criterion",
    "href": "chpt28-model-selection.html#cross-validation-criterion",
    "title": "26  Model Selection, Stein Shrinkage, and Model Averaging",
    "section": "26.13 Cross-Validation Criterion",
    "text": "26.13 Cross-Validation Criterion\nIn applied statistics and machine learning the default method for model selection and tuning parameter selection is cross-validation. We have introduced some of the concepts throughout the textbook, and review and unify the concepts at this point. Cross-validation is closely related to the hold-out criterion introduced in the previous section.\nIn Section \\(3.20\\) we defined the leave-one-out estimator as that obtained by applying an estimation formula to the sample omitting the \\(i^{t h}\\) observation. This is identical to the hold-out problem as described previously, where the estimation sample is \\(N=n-1\\) and the evaluation sample is \\(P=1\\). The estimator obtained omitting observation \\(i\\) is written as \\(\\widehat{\\beta}_{(-i)}\\). The prediction error is \\(\\widetilde{e}_{i}=Y_{i}-X_{i}^{\\prime} \\widehat{\\beta}_{(-i)}\\). The out-of-sample mean squared error “estimate” is \\(\\widetilde{e}_{i}^{2}\\). This is repeated \\(n\\) times, once for each observation \\(i\\), and the MSFE estimate is the average of the \\(n\\) squared prediction errors\n\\[\n\\mathrm{CV}=\\frac{1}{n} \\sum_{i=1}^{n} \\widetilde{e}_{i}^{2} .\n\\]\nThe estimator CV is called the cross-validation (CV) criterion. It is a natural generalization of the hold-out criterion and eliminates the two disadvantages described in the previous section. First, the CV criterion is an unbiased estimator of MSFE \\({ }_{n-1}\\), which is essentially the same as MSFE . Thus CV \\(_{n}\\). is essentially unbiased for model selection. Second, the CV criterion does not depend on a random sorting of the observations. As there is no random component the criterion takes the same value in any implementation.\nIn least squares estimation the CV criterion has a simple computational implementation. Theorem 3.7 shows that the leave-one-out least squares estimator (3.42) equals\n\\[\n\\widehat{\\beta}_{(-i)}=\\widehat{\\beta}-\\frac{1}{\\left(1-h_{i i}\\right)}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} X_{i} \\widehat{e}_{i}\n\\]\nwhere \\(\\widehat{e}_{i}\\) are the least squares residuals and \\(h_{i i}\\) are the leverage values. The prediction error thus equals\n\\[\n\\widetilde{e}_{i}=Y_{i}-X_{i}^{\\prime} \\widehat{\\beta}_{(-i)}=\\left(1-h_{i i}\\right)^{-1} \\widehat{e}_{i}\n\\]\nwhere the second equality is from Theorem 3.7. Consequently the CV criterion is\n\\[\n\\mathrm{CV}=\\frac{1}{n} \\sum_{i=1}^{n} \\widetilde{e}_{i}^{2}=\\frac{1}{n} \\sum_{i=1}^{n}\\left(1-h_{i i}\\right)^{-2} \\widehat{e}_{i}^{2} .\n\\]\nRecall as well that in our study of nonparametric regression (Section 19.12) we defined the crossvalidation criterion for kernel regression as the weighted average of the squared prediction errors\n\\[\n\\mathrm{CV}=\\frac{1}{n} \\sum_{i=1}^{n} \\tilde{e}_{i}^{2} w\\left(X_{i}\\right) .\n\\]\nTheorem \\(19.7\\) showed that \\(\\mathrm{CV}\\) is approximately unbiased for the integrated mean squared error (IMSE), which is a standard measure of accuracy for nonparametric regression. These results show that CV is an unbiased estimator for both the MSFE and IMSE, showing a close connection between these measures of accuracy.\nIn Section \\(20.17\\) and equation (20.30) we defined the CV criterion for series regression as in (28.5). Selecting variables for series regression is identical to model selection. The results as described above show that the CV criterion is an estimator for the MSFE and IMSE of the regression model and is therefore a good candidate for assessing model accuracy. The validity of the CV criterion is much broader than the AIC as the theorems for CV do not require conditional homoskedasticity. This is not an artifact of the proof method; cross-validation is inherently more robust than AIC or BIC.\nImplementation of CV model selection is the same as for the other criteria. A set of regression models are estimated. For each the CV criterion is calculated. The model with the smallest value of CV is the CVselected model.\nThe CV method is also much broader in concept and potential application. It applies to any estimation method so long as a “leave one out” error can be calculated. It can also be applied to other loss functions beyond squared error loss. For example, a cross-validation estimate of absolute loss is\n\\[\n\\mathrm{CV}=\\frac{1}{n} \\sum_{i=1}^{n}\\left|\\widetilde{e}_{i}\\right| .\n\\]\nComputationally and conceptually it is straightforward to select models by minimizing such criterion. However, the properties of applying CV to general criterion is not known.\nStata does not have a standard command to calculate the CV criterion for regression models."
  },
  {
    "objectID": "chpt28-model-selection.html#k-fold-cross-validation",
    "href": "chpt28-model-selection.html#k-fold-cross-validation",
    "title": "26  Model Selection, Stein Shrinkage, and Model Averaging",
    "section": "26.14 K-Fold Cross-Validation",
    "text": "26.14 K-Fold Cross-Validation\nThere are two deficiencies with the CV criterion which can be alleviated by the closely related K-fold cross-validation criterion. The first deficiency is that CV calculation can be computationally costly when sample sizes are very large or the estimation method is other than least squares. For estimators other than least squares it may be necessary to calculate \\(n\\) separate estimations. This can be computationally prohibitive in some contexts. A second deficiency is that the CV criterion, viewed as an estimator of \\(\\operatorname{MSFE}_{n}\\), has a high variance. The source is that the leave-one-out estimators \\(\\widehat{\\beta}_{(-i)}\\) have minimal variation across \\(i\\) and are therefore highly correlated.\nAn alternative is is to split the sample into \\(K\\) groups (or “folds”) and treat each group as a hold-out sample. This effectively reduces the number of estimations from \\(n\\) to \\(K\\). (This \\(K\\) is not the number of estimated coefficients. I apologize for the possible confusion in notation but this is the standard label.) A common choice is \\(K=10\\), leading to what is known as \\(\\mathbf{1 0}\\)-fold cross-validation.\nThe method works by the following steps. This description is for estimation of a regression model \\(Y=g(X, \\theta)+e\\) with estimator \\(\\widehat{\\theta}\\)\n\nRandomly sort the observations.\nSplit the observations into folds \\(k=1, \\ldots, K\\) of (roughly) equal size \\(n_{k} \\simeq n / K\\). Let \\(I_{k}\\) denote the observations in fold \\(k\\)\nFor \\(k=1, . ., K\\)\n\n\nExclude fold \\(I_{k}\\) from the dataset. This produces a sample with \\(n-n_{k}\\) observations.\nCalculate the estimator \\(\\widehat{\\theta}_{(-k)}\\) on this sample.\nCalculate the prediction errors \\(\\widetilde{e}_{i}=Y_{i}-g\\left(X_{i}, \\widehat{\\theta}_{(-k)}\\right)\\) for \\(i \\in I_{k}\\).\nCalculate \\(\\mathrm{CV}_{k}=n_{k}^{-1} \\sum_{i \\in I_{k}} \\widetilde{e}_{i}^{2}\\)\n\n 1. Calculate \\(\\mathrm{CV}=K^{-1} \\sum_{k=1}^{K} \\mathrm{CV}_{k}\\).\nIf \\(K=n\\) the method is identical to leave-one-out cross validation.\nA useful feature of \\(K\\)-fold CV is that we can calculate an approximate standard error. It is based on the approximation \\(\\operatorname{var}[\\mathrm{CV}] \\simeq K^{-1} \\operatorname{var}\\left[\\mathrm{CV}_{k}\\right]\\) which is based on the idea that \\(\\mathrm{CV}_{k}\\) are approximately uncorrelated acros folds. This leads to the standard error\n\\[\ns(\\mathrm{CV})=\\sqrt{\\frac{1}{K(K-1)} \\sum_{k=1}^{K}\\left(\\mathrm{CV}_{k}-\\mathrm{CV}\\right)^{2}} .\n\\]\nThis is similar to a clustered variance formula, where the folds are treated as clusters. The standard error \\(s\\) (CV) can be reported to assess the precision of CV as an estimate of the MSFE.\nOne disadvantage of K-fold cross-validation is that CV can be sensitive to the initial random sorting of the observations, leading to partially arbitrary results. This problem can be reduced by a technique called repeated CV, which repeats the K-fold CV algorithm \\(M\\) times (each time with a different random sorting), leading to \\(M\\) values of \\(C\\). These are averaged to produce the repeated CV value. As \\(M\\) increases, the randomness due to sorting is eliminated. An associated standard error can be obtained by taking the square root of the average squared standard errors.\nCV model selection is typically implemented by selecting the model with the smallest value of CV. An alternative implementation is known as the one standard error (1se) rule and selects the most parsimonious model whose value of CV is within one standard error of the minimum CV. The (informal) idea is that models whose value of \\(\\mathrm{CV}\\) is within one standard error of one another are not statistically distinguishable, and all else held equal we should lean towards parsimony. The 1se rule is the default in the popular cv.glmnet R function. The lse rule is an oversmoothing choice, meaning that it leans towards higher bias and reduced variance. In contrast, for inference many econometricians recommend undersmoothing bandwidths, which means selecting a less parsimonious model than the CV minimizing choice."
  },
  {
    "objectID": "chpt28-model-selection.html#many-selection-criteria-are-similar",
    "href": "chpt28-model-selection.html#many-selection-criteria-are-similar",
    "title": "26  Model Selection, Stein Shrinkage, and Model Averaging",
    "section": "26.15 Many Selection Criteria are Similar",
    "text": "26.15 Many Selection Criteria are Similar\nFor the linear regression model many selection criteria have been introduced. However, many of these alternative criteria are quite similar to one another. In this section we review some of these connections. The following discussion is for the standard regression model \\(Y=X^{\\prime} \\beta+e\\) with \\(n\\) observations, \\(K\\) estimated coefficients, and least squares variance estimator \\(\\widehat{\\sigma}_{K}^{2}\\).\nShibata (1980) proposed the criteria\n\\[\n\\text { Shibata }=\\widehat{\\sigma}_{K}^{2}\\left(1+\\frac{2 K}{n}\\right)\n\\]\nas an estimator of the MSFE. Recalling the Mallows criterion for regression (28.15) we see that Shibata = \\(C_{p} / n\\) if we replace the preliminary estimator \\(\\widetilde{\\sigma}^{2}\\) with \\(\\widehat{\\sigma}_{K}^{2}\\). Thus the two are quite similar in practice.\nTaking logarithms and using the approximation \\(\\log (1+x) \\simeq x\\) for small \\(x\\)\n\\[\nn \\log (\\text { Shibata })=n \\log \\left(\\widehat{\\sigma}_{K}^{2}\\right)+n \\log \\left(1+\\frac{2 K}{n}\\right) \\simeq n \\log \\left(\\widehat{\\sigma}_{K}^{2}\\right)+2 K=\\text { AIC. }\n\\]\nThus minimization of Shibata’s criterion and AIC are similar.\nAkaike (1969) proposed the Final Prediction Error Criteria\n\\[\n\\mathrm{FPE}=\\widehat{\\sigma}_{K}^{2}\\left(\\frac{1+K / n}{1-K / n}\\right) .\n\\]\nUsing the expansions \\((1-x)^{-1} \\simeq 1+x\\) and \\((1+x)^{2} \\simeq 1+2 x\\) we see that \\(\\mathrm{FPE} \\simeq\\) Shibata.\nCraven and Wahba (1979) proposed Generalized Cross Validation\n\\[\n\\mathrm{GCV}=\\frac{n \\widehat{\\sigma}_{K}^{2}}{(n-K)^{2}} .\n\\]\nBy the expansion \\((1-x)^{-2} \\simeq 1+2 x\\) we find that\n\\[\nn \\mathrm{GCV}=\\frac{\\widehat{\\sigma}_{K}^{2}}{(1-K / n)^{2}} \\simeq \\widehat{\\sigma}_{K}^{2}\\left(1+\\frac{2 K}{n}\\right)=\\text { Shibata. }\n\\]\nThe above calculations show that the WMSE, AIC, Shibata, FPE, GCV, and Mallows criterion are all close approximations to one another when \\(K / n\\) is small. Differences arise in finite samples for large \\(K\\). However, the above analysis shows that there is no fundamental difference between these criteria. They are all estimating the same target. This is in contrast to BIC which uses a different parameterization penalty and is asymptotically distinct. Interestingly there also is a connection between \\(\\mathrm{CV}\\) and the above criteria. Again using the expansion \\((1-x)^{-2} \\simeq 1+2 x\\) we find that\n\\[\n\\begin{aligned}\n\\mathrm{CV} &=\\sum_{i=1}^{n}\\left(1-h_{i i}\\right)^{-2} \\widehat{e}_{i}^{2} \\\\\n& \\simeq \\sum_{i=1}^{n} \\widehat{e}_{i}^{2}+\\sum_{i=1}^{n} 2 h_{i i} \\widehat{e}_{i}^{2} \\\\\n&=n \\widehat{\\sigma}_{K}^{2}+2 \\sum_{i=1}^{n} X_{i}^{\\prime}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} X_{i} \\widehat{e}_{i}^{2} \\\\\n&=n \\widehat{\\sigma}_{K}^{2}+2 \\operatorname{tr}\\left(\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1}\\left(\\sum_{i=1}^{n} X_{i} X_{i}^{\\prime} \\widehat{e}_{i}^{2}\\right)\\right) \\\\\n& \\simeq n \\widehat{\\sigma}_{K}^{2}+2 \\operatorname{tr}\\left(\\left(\\mathbb{E}\\left[X X^{\\prime}\\right]\\right)^{-1}\\left(\\mathbb{E}\\left[X X^{\\prime} e^{2}\\right]\\right)\\right) \\\\\n&=n \\widehat{\\sigma}_{K}^{2}+2 K \\sigma^{2} \\\\\n& \\simeq S h i b a t a .\n\\end{aligned}\n\\]\nThe third-to-last line holds asymptotically by the WLLN. The following equality holds under conditional homoskedasiticity. The final approximation replaces \\(\\sigma^{2}\\) by the estimator \\(\\widehat{\\sigma}_{K}^{2}\\). This calculation shows that under the assumption of conditional homoskedasticity the CV criterion is similar to the other criteria. It differs under heteroskedasticity, however, which is one of its primary advantages."
  },
  {
    "objectID": "chpt28-model-selection.html#relation-with-likelihood-ratio-testing",
    "href": "chpt28-model-selection.html#relation-with-likelihood-ratio-testing",
    "title": "26  Model Selection, Stein Shrinkage, and Model Averaging",
    "section": "26.16 Relation with Likelihood Ratio Testing",
    "text": "26.16 Relation with Likelihood Ratio Testing\nSince the AIC and BIC are penalized log-likelihoods, AIC and BIC selection are related to likelihood ratio testing. Suppose we have two nested models \\(\\mathscr{M}_{1}\\) and \\(\\mathscr{M}_{2}\\) with log-likelihoods \\(\\ell_{1 n}\\left(\\widehat{\\theta}_{1}\\right)\\) and \\(\\ell_{2 n}\\left(\\widehat{\\theta}_{2}\\right)\\) and \\(K_{1}<K_{2}\\) estimated parameters. AIC selects \\(\\mathscr{M}_{1}\\) if \\(\\operatorname{AIC}\\left(K_{1}\\right)<\\operatorname{AIC}\\left(K_{2}\\right)\\) which occurs when\n\\[\n\\left.-2 \\ell_{1 n}\\left(\\widehat{\\theta}_{1}\\right)+2 K_{1}<-2 \\ell_{2 n}\\left(\\widehat{\\theta}_{2}\\right)\\right)+2 K_{2}\n\\]\nor\n\\[\n\\mathrm{LR}=2\\left(\\ell_{2 n}\\left(\\widehat{\\theta}_{2}\\right)-\\ell_{1 n}\\left(\\widehat{\\theta}_{1}\\right)\\right)<2 r\n\\]\nwhere \\(r=K_{2}-K_{1}\\). Thus AIC selection is similar to selection by likelihood ratio testing with a different critical value. Rather than using a critical value from the chi-square distribution the “critical value” is \\(2 r\\). This is not to say that AIC selection is testing (it is not). But rather that there is a similar structure in the decision.\nThere are two useful practical implications. One is that when test statistics are reported in their \\(F\\) form (which divide by the difference in coefficients \\(r\\) ) then the AIC “critical value” is 2 . The AIC selects the restricted (smaller) model if \\(F<2\\). It selects the unrestricted (larger) model if \\(F>2\\).\nAnother useful implication is in the case of considering a single coefficient (when \\(r=1\\) ). AIC selects the coefficient (the larger model) if \\(\\mathrm{LR}>2\\). In contrast a \\(5 %\\) significance test “selects” the larger model (rejects the smaller) if LR \\(>3.84\\). Thus AIC is more generous in terms of selecting larger models. An equivalent way of seeing this is that AIC selects the coefficient if the t-ratio exceeds \\(1.41\\) while the \\(5 %\\) significance test selects if the t-ratio exceeds \\(1.96\\).\nSimilar comments apply to BIC selection though the effective critical values are different. For comparing models with coefficients \\(K_{1}<K_{2}\\) the BIC selects \\(\\mathscr{M}_{1}\\) if \\(\\mathrm{LR}<\\log (n) r\\). The “critical value” for an \\(F\\) statistic is \\(\\log (n)\\). Hence BIC selection becomes stricter as sample sizes increase."
  },
  {
    "objectID": "chpt28-model-selection.html#consistent-selection",
    "href": "chpt28-model-selection.html#consistent-selection",
    "title": "26  Model Selection, Stein Shrinkage, and Model Averaging",
    "section": "26.17 Consistent Selection",
    "text": "26.17 Consistent Selection\nAn important property of a model selection procedure is whether it selects a true model in large samples. We call such a procedure consistent.\nTo discuss this further we need to thoughtfully define what is a “true” model. The answer depends on the type of model.\nWhen a model is a parametric density or distribution \\(f(y, \\theta)\\) with \\(\\theta \\in \\Theta\\) (as in likelihood estimation) then the model is true if there is some \\(\\theta_{0} \\in \\Theta\\) such that \\(f\\left(y, \\theta_{0}\\right)\\) equals the true density or distribution. Notice that it is important in this context both that the function class \\(f(y, \\theta)\\) and parameter space \\(\\Theta\\) are appropriately defined.\nIn a semiparametric conditional moment condition model which states \\(\\mathbb{E}[g(Y, X, \\theta) \\mid X]=0\\) with \\(\\theta \\in \\Theta\\) then the model is true if there is some \\(\\theta_{0} \\in \\Theta\\) such that \\(\\mathbb{E}\\left[g\\left(Y, X, \\theta_{0}\\right) \\mid X\\right]=0\\). This includes the regression model \\(Y=m(X, \\theta)+e\\) with \\(\\mathbb{E}[e \\mid X]=0\\) where the model is true if there is some \\(\\theta_{0} \\in \\Theta\\) such that \\(m\\left(X, \\theta_{0}\\right)=\\mathbb{E}[Y \\mid X]\\). It also includes the homoskedastic regression model which adds the requirement that \\(\\mathbb{E}\\left[e^{2} \\mid X\\right]=\\sigma^{2}\\) is a constant.\nIn a semiparametric unconditional moment condition model \\(\\mathbb{E}[g(Y, X, \\theta)]=0\\) then the model is true if there is some \\(\\theta_{0} \\in \\Theta\\) such that \\(\\mathbb{E}\\left[g\\left(Y, X, \\theta_{0}\\right)\\right]=0\\). A subtle issue here is that when the model is just identified and \\(\\Theta\\) is unrestricted then this condition typically holds and so the model is typically true. This includes least squares regression interpreted as a projection and just-identified instrumental variables regression.\nIn a nonparametric model such as \\(Y \\sim f \\in \\mathscr{F}\\) where \\(\\mathscr{F}\\) is some function class (such as second-order differentiable densities) then the model is true if the true density is a member of the function class \\(\\mathscr{F}\\).\nA complication arises that there may be multiple true models. This cannot occur when models are strictly non-nested (meaning that there is no common element in both model classes) but strictly nonnested models are rare. Most models have non-trivial intersections. For example, the linear regression models \\(Y=\\alpha+X_{1}^{\\prime} \\beta_{1}+e\\) and \\(Y=\\alpha+X_{2}^{\\prime} \\beta_{2}+e\\) with \\(X_{1}\\) and \\(X_{2}\\) containing no common elements may appear non-nested but they intersect when \\(\\beta_{1}=0\\) and \\(\\beta_{2}=0\\). As another example consider the linear model \\(Y=\\alpha+X^{\\prime} \\beta+e\\) and \\(\\log\\)-linear model \\(\\log (Y)=\\alpha+X^{\\prime} \\beta+e\\). If we add the assumption that \\(e \\sim \\mathrm{N}\\left(0, \\sigma^{2}\\right)\\) then the models are non-intersecting. But if we relax normality and instead use the conditional mean assumption \\(\\mathbb{E}[e \\mid X]=0\\) then the models are intersecting when \\(\\beta_{1}=0\\) and \\(\\beta_{2}=0\\).\nThe most common type of intersecting models are nested. In regression this occurs when the two models are \\(Y=X_{1}^{\\prime} \\beta_{1}+e\\) and \\(Y=X_{1}^{\\prime} \\beta_{1}+X_{2}^{\\prime} \\beta_{2}+e\\). If \\(\\beta_{2} \\neq 0\\) then only the second model is true. But if \\(\\beta_{2}=0\\) then both are true models.\nIn general, given a set of models \\(\\overline{\\mathscr{M}}=\\left\\{\\mathscr{M}_{1}, \\ldots, \\mathscr{M}_{M}\\right\\}\\) a subset \\(\\overline{\\mathscr{M}}^{*}\\) are true models (as described above) while the remainder are not true models.\nA model selection rule \\(\\widehat{M}\\) selects one model from the set \\(\\bar{M}\\). We say a method is consistent if it asymptotically selects a true model.\nDefinition 28.1 A model selection rule is model selection consistent if \\(\\mathbb{P}\\left[\\widehat{M} \\in \\bar{M}^{*}\\right] \\rightarrow 1\\) as \\(n \\rightarrow \\infty\\)\nThis states that the model selection rule selects a true model with probability tending to 1 as the sample size diverges.\nA broad class of model selection methods satisfy this definition of consistency. To see this consider the class of information criteria\n\\[\n\\mathrm{IC}=-2 \\ell_{n}(\\widehat{\\theta})+c(n, K) .\n\\]\nThis includes AIC \\((c=2 K), \\mathrm{BIC}(c=K \\log (n))\\), and testing-based selection ( \\(c\\) equals a fixed quantile of the \\(\\chi_{K}^{2}\\) distribution).\nTheorem 28.6 Under standard regularity conditions for maximum likelihood estimation, selection based on IC is model selection consistent if \\(c(n, K)=o(n)\\) as \\(n \\rightarrow \\infty\\).\nThe proof is given in Section \\(28.32\\).\nThis result covers AIC, BIC and testing-based selection. Thus all are model selection consistent.\nA major limitation with this result is that the definition of model selection consistency is weak. A model may be true but over-parameterized. To understand the distinction consider the models \\(Y=\\) \\(X_{1}^{\\prime} \\beta_{1}+e\\) and \\(Y=X_{1}^{\\prime} \\beta_{1}+X_{2}^{\\prime} \\beta_{2}+e\\). If \\(\\beta_{2}=0\\) then both \\(\\mathscr{M}_{1}\\) and \\(\\mathscr{M}_{2}\\) are true, but \\(\\mathscr{M}_{1}\\) is the preferred model as it is more parsimonious. When two nested models are both true models it is conventional to think of the more parsimonious model as the correct model. In this context we do not describe the larger model as an incorrect model but rather as over-parameterized. If a selection rule asymptotically selects an over-parameterized model we say that it “over-selects”.\nDefinition 28.2 A model selection rule asymptotically over-selects if there are models \\(\\mathscr{M}_{1} \\subset \\mathscr{M}_{2}\\) such that \\(\\liminf _{n \\rightarrow \\infty} \\mathbb{P}\\left[\\widehat{\\mathscr{M}}=\\mathscr{M}_{2} \\mid \\mathscr{M}_{1}\\right]>0\\).\nThe definition states that over-selection occurs when two models are nested and the smaller model is true (so both models are true models but the smaller model is more parsimonious) if the larger model is asymptotically selected with positive probability.\nTheorem 28.7 Under standard regularity conditions for maximum likelihood estimation, selection based on IC asymptotically over-selects if \\(c(n, K)=O(1)\\) as \\(n \\rightarrow \\infty\\).\nThe proof is given in Section \\(28.32\\).\nThis result includes both AIC and testing-based selection. Thus these procedures over-select. For example, if the models are \\(Y=X_{1}^{\\prime} \\beta_{1}+e\\) and \\(Y=X_{1}^{\\prime} \\beta_{1}+X_{2}^{\\prime} \\beta_{2}+e\\) and \\(\\beta_{2}=0\\) holds, then these procedures select the over-parameterized regression with positive probability.\nFollowing this line of reasoning, it is useful to draw a distinction between true and parsimonious models. We define the set of parsimonious models \\(\\bar{M}^{0} \\subset \\overline{\\mathscr{M}}^{*}\\) as the set of true models with the fewest number of parameters. When the models in \\(\\bar{M}^{*}\\) are nested then \\(\\overline{\\mathscr{M}}^{0}\\) will be a singleton. In the regression example with \\(\\beta_{2}=0\\) then \\(\\mathscr{M}_{1}\\) is the unique parsimonious model among \\(\\left\\{\\mathscr{M}_{1}, \\mathscr{M}_{2}\\right\\}\\). We introduce a stronger consistency definition for procedures which asymptotically select parsimonious models. Definition 28.3 A model selection rule is consistent for parsimonious models if \\(\\mathbb{P}\\left[\\widehat{\\mathscr{M}} \\in \\overline{\\mathscr{M}}^{0}\\right] \\rightarrow 1\\) as \\(n \\rightarrow \\infty\\)\nOf the methods we have reviewed, only BIC selection is consistent for parsimonious models, as we now show.\nTheorem 28.8 Under standard regularity conditions for maximum likelihood estimation, selection based on IC is consistent for parsimonious models if for all \\(K_{2}>K_{1}\\)\n\\[\nc\\left(n, K_{2}\\right)-c\\left(n, K_{1}\\right) \\rightarrow \\infty\n\\]\nas \\(n \\rightarrow \\infty\\), yet \\(c(n, K)=o(n)\\) as \\(n \\rightarrow \\infty\\).\nThe proof is given in Section 28.32.\nThe condition includes BIC because \\(c\\left(n, K_{2}\\right)-c\\left(n, K_{1}\\right)=\\left(K_{2}-K_{1}\\right) \\log (n) \\rightarrow \\infty\\) if \\(K_{2}>K_{1}\\).\nSome economists have interpreted Theorem \\(28.8\\) as indicating that BIC selection is preferred over the other methods. This is an incorrect deduction. In the next section we show that the other selection procedures are asymptotically optimal in terms of model fit and in terms of out-of-sample forecasting. Thus consistent model selection is only one of several desirable statistical properties."
  },
  {
    "objectID": "chpt28-model-selection.html#asymptotic-selection-optimality",
    "href": "chpt28-model-selection.html#asymptotic-selection-optimality",
    "title": "26  Model Selection, Stein Shrinkage, and Model Averaging",
    "section": "26.18 Asymptotic Selection Optimality",
    "text": "26.18 Asymptotic Selection Optimality\nRegressor selection by the AIC/Shibata/Mallows/CV class turns out to be asymptotically optimal with respect to out-of-sample prediction under quite broad conditions. This may appear to conflict with the results of the previous section but it does not as there is a critical difference between the goals of consistent model selection and accurate prediction.\nOur analysis will be in the homoskedastic regression model conditioning on the regressor matrix \\(\\boldsymbol{X}\\). We write the regression model as\n\\[\n\\begin{aligned}\nY &=m+e \\\\\nm &=\\sum_{j=1}^{\\infty} X_{j} \\beta_{j} \\\\\n\\mathbb{E}[e \\mid X] &=0 \\\\\n\\mathbb{E}\\left[e^{2} \\mid X\\right] &=\\sigma^{2}\n\\end{aligned}\n\\]\nwhere \\(X=\\left(X_{1}, X_{2}, \\ldots\\right)\\). We can also write the regression equation in matrix notation as \\(\\boldsymbol{Y}=\\boldsymbol{m}+\\boldsymbol{e}\\).\nThe \\(K^{t h}\\) regression model uses the first \\(K\\) regressors \\(X_{K}=\\left(X_{1}, X_{2}, \\ldots, X_{K}\\right)\\). The least squares estimates in matrix notation are\n\\[\n\\boldsymbol{Y}=\\boldsymbol{X}_{K} \\widehat{\\beta}_{K}+\\widehat{\\boldsymbol{e}}_{K} .\n\\]\nAs in Section \\(28.6\\) define the fitted values \\(\\widehat{\\boldsymbol{m}}=\\boldsymbol{X}_{K} \\widehat{\\beta}_{K}\\) and regression fit (sum of expected squared prediction errors) as\n\\[\nR_{n}(K)=\\mathbb{E}\\left[(\\widehat{\\boldsymbol{m}}-\\boldsymbol{m})^{\\prime}(\\widehat{\\boldsymbol{m}}-\\boldsymbol{m}) \\mid \\boldsymbol{X}\\right]\n\\]\nthough now we index \\(R\\) by sample size \\(n\\) and model \\(K\\).\nIn any sample there is an optimal model \\(K\\) which minimizes \\(R_{n}(K)\\) :\n\\[\nK_{n}^{\\mathrm{opt}}=\\underset{K}{\\operatorname{argmin}} R_{n}(K) .\n\\]\nModel \\(K_{n}^{\\text {opt }}\\) obtains the minimized value of \\(R_{n}(K)\\)\n\\[\nR_{n}^{\\mathrm{opt}}=R_{n}\\left(K_{n}^{\\mathrm{opt}}\\right)=\\min _{K} R_{n}(K) .\n\\]\nNow consider model selection using the Mallow’s criterion for regression models\n\\[\nC_{p}(K)=\\widehat{\\boldsymbol{e}}_{K}^{\\prime} \\widehat{\\boldsymbol{e}}_{K}+2 \\sigma^{2} K\n\\]\nwhere we explicitly index by \\(K\\), and for simplicity we assume the error variance \\(\\sigma^{2}\\) is known. (The results are unchanged if it is replaced by a consistent estimator.) Let the selected model be\n\\[\n\\widehat{K}_{n}=\\underset{K}{\\operatorname{argmin}} C_{p}(K) .\n\\]\nPrediction accuracy using the Mallows-selected model is \\(R_{n}\\left(\\widehat{K}_{n}\\right)\\). We say that a selection procedure is asymptotically optimal if the prediction accuracy is asymptotically equivalent with the infeasible optimum. This can be written as\n\\[\n\\frac{R_{n}\\left(\\widehat{K}_{n}\\right)}{R_{n}^{\\mathrm{opt}}} \\underset{p}{\\longrightarrow} 1 .\n\\]\nWe consider convergence in (28.18) in terms of the risk ratio because \\(R_{n}^{\\text {opt }}\\) diverges as the sample size increases.\nLi (1987) established the asymptotic optimality (28.18). His result depends on the following conditions.\n\n\nThe observations \\(\\left(Y_{i}, X_{\\boldsymbol{i}}\\right), i=1, \\ldots, n\\), are independent and identically distributed.\n\\(\\mathbb{E}[e \\mid X]=0\\).\n\\(\\mathbb{E}\\left[e^{2} \\mid X\\right]=\\sigma^{2}\\).\n\\(\\mathbb{E}\\left[|e|^{4 r} \\mid X\\right] \\leq B<\\infty\\) for some \\(r>1\\).\n\\(R_{n}^{\\mathrm{opt}} \\rightarrow \\infty\\) as \\(n \\rightarrow \\infty\\)\nThe estimated models are nested.\n\nAssumptions 28.1.2 and 28.1.3 state that the true model is a conditionally homoskedastic regression. Assumption 28.1.4 is a technical condition, that a conditional moment of the error is uniformly bounded. Assumption 28.1.5 is subtle. It effectively states that there is no correctly specified finite-dimensional model. To see this, suppose that there is a \\(K_{0}\\) such that the model is correctly specified, meaning that \\(m_{i}=\\sum_{j=1}^{K_{0}} X_{j i} \\beta_{j}\\). In this case we can show that for \\(K \\geq K_{0}, R_{n}(K)=R_{n}\\left(K_{0}\\right)\\) does not change with \\(n\\), violating Assumption 28.1.5. Assumption 28.1.6 is a technical condition that restricts the number of estimated models. This assumption can be generalized to allow non-nested models, but in this case an alternative restriction on the number of estimated models is needed.\nTheorem 28.9 Assumption \\(28.1\\) implies (28.18). Thus Mallows selection is asymptotically equivalent to using the infeasible optimal model.\nThe proof is given in Section 28.32.\nTheorem \\(28.9\\) states that Mallows selection in a conditional homoskedastic regression is asymptotically optimal. The key assumptions are homoskedasticity and that all finite-dimensional models are misspecified (incomplete), meaning that there are always omitted variables. The latter means that regardless of the sample size there is always a trade-off between omitted variables bias and estimation variance. The theorem as stated is specific for Mallows selection but extends to AIC, Shibata, GCV, FPE, and CV with some additional technical considerations. The primary message is that the selection methods discussed in the previous section asymptotically select a sequence of models which are best-fitting in the sense of minimizing the prediction error.\nUsing a similar argument, Andrews (1991c) showed that selection by cross-validation satisfies the same asymptotic optimality condition without requiring conditional homoskedasticity. The treatment is a bit more technical so we do not review it here. This indicates an important advantage for crossvalidation selection over the other methods."
  },
  {
    "objectID": "chpt28-model-selection.html#focused-information-criterion",
    "href": "chpt28-model-selection.html#focused-information-criterion",
    "title": "26  Model Selection, Stein Shrinkage, and Model Averaging",
    "section": "26.19 Focused Information Criterion",
    "text": "26.19 Focused Information Criterion\nClaeskens and Hjort (2003) introduced the Focused Information Criterion (FIC) as an estimator of the MSE of a scalar parameter. The criterion is appropriate in correctly-specified likelihood models when one of the estimated models nests the other models. Let \\(f(y, \\theta)\\) be a parametric model density with a \\(K \\times 1\\) parameter \\(\\theta\\).\nThe class of models (sub-models) allowed are those defined by a set of differentiable restrictions \\(r(\\theta)=0\\). Let \\(\\widetilde{\\theta}\\) be the restricted MLE which maximizes the likelihood subject to \\(r(\\theta)=0\\).\nA key feature of the FIC is that it focuses on a real-valued parameter \\(\\mu=g(\\theta)\\) where \\(g\\) is some differentiable function. Claeskens and Hjort call \\(\\mu\\) the target parameter. The choice of \\(\\mu\\) is made by the researcher and is a critical choice. In most applications \\(\\mu\\) is the key coefficient in the application (for example, the returns to schooling in a wage regression). The unrestricted MLE for \\(\\mu\\) is \\(\\widehat{\\mu}=g(\\widehat{\\theta})\\), the restricted MLE is \\(\\widetilde{\\mu}=g(\\widetilde{\\theta})\\).\nEstimation accuracy is measured by the MSE of the estimator of the target parameter, which is the squared bias plus the variance:\n\\[\n\\operatorname{mse}[\\widetilde{\\mu}]=\\mathbb{E}\\left[(\\widetilde{\\mu}-\\mu)^{2}\\right]=(\\mathbb{E}[\\widetilde{\\mu}]-\\mu)^{2}+\\operatorname{var}[\\widetilde{\\mu}] .\n\\]\nIt turns out to be convenient to normalize the MSE by that of the unrestricted estimator. We define this as the Focus\n\\[\n\\mathrm{F}=\\operatorname{mse}[\\widetilde{\\mu}]-\\operatorname{mse}[\\widehat{\\mu}] .\n\\]\nThe Claeskens-Hjort FIC is an estimator of F. Specifically,\n\\[\n\\mathrm{FIC}=(\\widetilde{\\mu}-\\widehat{\\mu})^{2}-2 \\widehat{\\boldsymbol{G}}^{\\prime} \\widehat{\\boldsymbol{V}}_{\\widehat{\\theta}} \\widehat{\\boldsymbol{R}}\\left(\\widehat{\\boldsymbol{R}}^{\\prime} \\widehat{\\boldsymbol{V}}_{\\widehat{\\theta}} \\widehat{\\boldsymbol{R}}^{-1} \\widehat{\\boldsymbol{R}}^{\\prime} \\widehat{\\boldsymbol{V}}_{\\widehat{\\theta}} \\widehat{\\boldsymbol{G}}\\right.\n\\]\nwhere \\(\\widehat{\\boldsymbol{V}}_{\\widehat{\\theta}}, \\widehat{\\boldsymbol{G}}\\) and \\(\\widehat{\\boldsymbol{R}}\\) are estimators of \\(\\operatorname{var}[\\widehat{\\theta}], \\boldsymbol{G}=\\frac{\\partial}{\\partial \\theta^{\\prime}} g(\\theta)\\) and \\(\\boldsymbol{R}=\\frac{\\partial}{\\partial \\theta^{\\prime}} r(\\theta)\\).\nIn a least squares regression \\(\\boldsymbol{Y}=\\boldsymbol{X} \\beta+\\boldsymbol{e}\\) with a linear restriction \\(\\boldsymbol{R}^{\\prime} \\beta=0\\) and linear parameter of interest \\(\\mu=\\boldsymbol{G}^{\\prime} \\beta\\) the FIC equals\n\\[\n\\begin{aligned}\n\\mathrm{FIC} &=\\left(\\boldsymbol{G}^{\\prime} \\boldsymbol{R}\\left(\\boldsymbol{R}^{\\prime}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\boldsymbol{R}\\right)^{-1} \\boldsymbol{R}^{\\prime}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\widehat{\\beta}\\right)^{2} \\\\\n&-2 \\widehat{\\sigma}^{2} \\boldsymbol{G}^{\\prime}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\boldsymbol{R}\\left(\\boldsymbol{R}^{\\prime}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\boldsymbol{R}\\right)^{-1} \\boldsymbol{R}^{\\prime}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\boldsymbol{G} .\n\\end{aligned}\n\\]\nThe FIC is used similarly to AIC. The FIC is calculated for each sub-model of interest and the model with the lowest value of FIC is selected.\nThe advantage of the FIC is that it is specifically targeted to minimize the MSE of the target parameter. The FIC is therefore appropriate when the goal is to estimate a specific target parameter. A disadvantage is that it does not necessarily produce a model with good estimates of the other parameters. For example, in a linear regression \\(Y=X_{1} \\beta_{1}+X_{2} \\beta_{2}+e\\), if \\(X_{1}\\) and \\(X_{2}\\) are uncorrelated and the focus parameter is \\(\\beta_{1}\\) then the FIC will tend to select the sub-model without \\(X_{2}\\), and thus the selected model will produce a highly biased estimate of \\(\\beta_{2}\\). Consequently when using the FIC it is dubious if attention should be paid to estimates other than those of \\(\\mu\\).\nComputationally it may be convenient to implement the FIC using an alternative formulation. Define the adjusted focus\n\\[\n\\mathrm{F}^{*}=n(\\mathrm{~F}+2 \\operatorname{mse}[\\widehat{\\mu}])=n(\\operatorname{mse}[\\widetilde{\\mu}]+\\operatorname{mse}[\\widehat{\\mu}]) .\n\\]\nThis adds the same quantity to all models and therefore does not alter the minimizing model. Multiplication by \\(n\\) puts the FIC in units which are easier for reporting. The estimate of the adjusted focus is an adjusted FIC and can be written as\n\\[\n\\begin{aligned}\n\\text { FIC }^{*} &=n(\\widetilde{\\mu}-\\widehat{\\mu})^{2}+2 n \\widehat{\\boldsymbol{V}}_{\\widetilde{\\mu}} \\\\\n&=n(\\widetilde{\\mu}-\\widehat{\\mu})^{2}+2 n s(\\widetilde{\\mu})^{2}\n\\end{aligned}\n\\]\nwhere\n\\[\n\\widehat{\\boldsymbol{V}}_{\\widetilde{\\mu}}=\\widehat{\\boldsymbol{G}}^{\\prime}\\left(\\boldsymbol{I}_{k}-\\widehat{\\boldsymbol{V}}_{\\widehat{\\theta}} \\widehat{\\boldsymbol{R}}\\left(\\widehat{\\boldsymbol{R}}^{\\prime} \\widehat{\\boldsymbol{V}}_{\\widehat{\\theta}} \\widehat{\\boldsymbol{R}}\\right)^{-1} \\widehat{\\boldsymbol{R}}^{\\prime} \\widehat{\\boldsymbol{V}}_{\\widehat{\\theta}}\\right) \\widehat{\\boldsymbol{G}}\n\\]\nis an estimator of \\(\\operatorname{var}[\\widetilde{\\mu}]\\) and \\(s(\\widetilde{\\mu})=\\widehat{V}_{\\widetilde{\\mu}}^{1 / 2}\\) is a standard error for \\(\\widetilde{\\mu}\\).\nThis means that \\(\\mathrm{FIC}^{*}\\) can be easily calculated using conventional software without additional programming. The estimator \\(\\widehat{\\mu}\\) can be calculated from the full model (the long regression) and the estimator \\(\\widetilde{\\mu}\\) and its standard error \\(s(\\widetilde{\\mu})\\) from the restricted model (the short regression). The formula (28.20) can then be applied to obtain FIC \\(^{*}\\).\nThe formula (28.19) also provides an intuitive understanding of the FIC. When we minimize FIC* we are minimizing the variance of the estimator of the target parameter \\(\\left(\\widehat{\\boldsymbol{V}}_{\\widetilde{\\mu}}\\right)\\) while not altering the estimate \\(\\widetilde{\\mu}\\) too much from the unrestricted estimate \\(\\widehat{\\mu}\\).\nWhen selecting from amongst just two models, the FIC selects the restricted model if \\((\\widetilde{\\mu}-\\widehat{\\mu})^{2}+2 \\widehat{\\boldsymbol{V}}_{\\widetilde{\\mu}}<\\) 0 which is the same as \\((\\widetilde{\\mu}-\\widehat{\\mu})^{2} / \\widehat{V}_{\\widetilde{\\mu}}<2\\). The statistic to the left of the inequality is the squared t-statistic in the restricted model for testing the hypothesis that \\(\\mu\\) equals the unrestricted estimator \\(\\widehat{\\mu}\\) but ignoring the estimation error in the latter. Thus a simple implementation (when just comparing two models) is to estimate the long and short regressions, take the difference in the two estimates of the coefficient of interest, and compute a t-ratio using the standard error from the short (restricted) regression. If this t-ratio exceeds \\(1.4\\) the FIC selects the long regression estimate. If the t-ratio is smaller than \\(1.4\\) the FIC selects the short regression estimate. Claeskens and Hjort motivate the FIC using a local misspecification asymptotic framework. We use a simpler heuristic motivation. First take the unrestricted MLE. Under standard conditions \\(\\widehat{\\mu}\\) has asymptotic variance \\(\\boldsymbol{G}^{\\prime} \\boldsymbol{V}_{\\theta} \\boldsymbol{G}\\) where \\(\\boldsymbol{V}_{\\theta}=\\mathscr{I}^{-1}\\). As the estimator is asymptotically unbiased it follows that\n\\[\n\\operatorname{mse}[\\widehat{\\mu}] \\simeq \\operatorname{var}[\\widehat{\\mu}] \\simeq n^{-1} \\boldsymbol{G}^{\\prime} \\boldsymbol{V}_{\\theta} \\boldsymbol{G} .\n\\]\nSecond take the restricted MLE. Under standard conditions \\(\\widetilde{\\mu}\\) has asymptotic variance\n\\[\n\\boldsymbol{G}^{\\prime}\\left(\\boldsymbol{V}_{\\theta}-\\boldsymbol{V}_{\\theta} \\boldsymbol{R}\\left(\\boldsymbol{R}^{\\prime} \\boldsymbol{V}_{\\theta} \\boldsymbol{R}\\right)^{-1} \\boldsymbol{R} \\boldsymbol{V}_{\\theta}\\right) \\boldsymbol{G} .\n\\]\n\\(\\widetilde{\\mu}\\) also has a probability limit, say \\(\\mu_{R}\\), which (generally) differs from \\(\\mu\\). Together we find that\n\\[\n\\operatorname{mse}[\\widetilde{\\mu}] \\simeq B+n^{-1} \\boldsymbol{G}^{\\prime}\\left(\\boldsymbol{V}_{\\theta}-\\boldsymbol{V}_{\\theta} \\boldsymbol{R}\\left(\\boldsymbol{R}^{\\prime} \\boldsymbol{V}_{\\theta} \\boldsymbol{R}\\right)^{-1} \\boldsymbol{R} \\boldsymbol{V}_{\\theta}\\right) \\boldsymbol{G}\n\\]\nwhere \\(B=\\left(\\mu-\\mu_{R}\\right)^{2}\\). Subtracting, we find that the Focus is\n\\[\n\\mathrm{F} \\simeq B-n^{-1} \\boldsymbol{G}^{\\prime} \\boldsymbol{V}_{\\theta} \\boldsymbol{R}\\left(\\boldsymbol{R}^{\\prime} \\boldsymbol{V}_{\\theta} \\boldsymbol{R}\\right)^{-1} \\boldsymbol{R} \\boldsymbol{V}_{\\theta} \\boldsymbol{G} .\n\\]\nThe plug-in estimator \\(\\widehat{B}=(\\widehat{\\mu}-\\widetilde{\\mu})^{2}\\) of \\(B\\) is biased because\n\\[\n\\begin{aligned}\n\\mathbb{E}[\\widehat{B}] &=(\\mathbb{E}[\\widehat{\\mu}-\\widetilde{\\mu}])^{2}+\\operatorname{var}[\\widehat{\\mu}-\\widetilde{\\mu}] \\\\\n& \\simeq B+\\operatorname{var}[\\widehat{\\mu}]-\\operatorname{var}[\\widetilde{\\mu}] \\\\\n& \\simeq B+n^{-1} \\boldsymbol{G}^{\\prime} \\boldsymbol{V}_{\\theta} \\boldsymbol{R}\\left(\\boldsymbol{R}^{\\prime} \\boldsymbol{V}_{\\theta} \\boldsymbol{R}\\right)^{-1} \\boldsymbol{R}_{\\theta} \\boldsymbol{G} .\n\\end{aligned}\n\\]\nIt follows that an approximately unbiased estimator for \\(F\\) is\n\\[\n\\widehat{B}-2 n^{-1} \\boldsymbol{G}^{\\prime} \\boldsymbol{V}_{\\theta} \\boldsymbol{R}\\left(\\boldsymbol{R}^{\\prime} \\boldsymbol{V}_{\\theta} \\boldsymbol{R}\\right)^{-1} \\boldsymbol{R} \\boldsymbol{V}_{\\theta} \\boldsymbol{G} .\n\\]\nThe FIC is obtained by replacing the unknown \\(\\boldsymbol{G}, \\boldsymbol{R}\\), and \\(n^{-1} \\boldsymbol{V}_{\\theta}\\) by estimates."
  },
  {
    "objectID": "chpt28-model-selection.html#best-subset-and-stepwise-regression",
    "href": "chpt28-model-selection.html#best-subset-and-stepwise-regression",
    "title": "26  Model Selection, Stein Shrinkage, and Model Averaging",
    "section": "26.20 Best Subset and Stepwise Regression",
    "text": "26.20 Best Subset and Stepwise Regression\nSuppose that we have a set of potential regressors \\(\\left\\{X_{1}, \\ldots, X_{K}\\right\\}\\) and we want to select a subset of the regressors to use in a regression. Let \\(S_{m}\\) denote a subset of the regressors, and let \\(m=1, \\ldots, M\\) denote the set of potential subsets. Given a model selection criterion (e.g. AIC, Mallows, or CV) the best subset model is the one which minimizes the criterion across the \\(M\\) models. This is implemented by estimating the \\(M\\) models and comparing the model selection criteria.\nIf \\(K\\) is small it is computationally feasible to compare all subset models. However, when \\(K\\) is large this may not be feasible. This is because the number of potential subsets is \\(M=2^{K}\\) which increases quickly with \\(K\\). For example, \\(K=10\\) implies \\(M=1024, K=20\\) implies \\(M \\geq 1,000,000\\), and \\(K=40\\) implies \\(M\\) exceeds one trillion. It simply does not make sense to estimate all subset regressions in such cases.\nIf the goal is to find the set of regressors which produces the smallest selection criterion it seems likely that we should be able to find an approximating set of regressors at much reduced computation cost. Some specific algorithms to implement this goal are as called stepwise, stagewise, and least angle regression. None of these procedures actually achieve the goal of minimizing any specific selection criterion; rather they are viewed as useful computational approximations. There is also some potential confusion as different authors seem to use the same terms for somewhat different implementations. We use the terms here as described in Hastie, Tibshirani, and Friedman (2008).\nIn the following descriptions we use \\(\\operatorname{SSE}(m)\\) to refer to the sum of squared residuals from a fitted model and \\(C(m)\\) to refer to the selection criterion used for model comparison (AIC is most typically used)."
  },
  {
    "objectID": "chpt28-model-selection.html#backward-stepwise-regression",
    "href": "chpt28-model-selection.html#backward-stepwise-regression",
    "title": "26  Model Selection, Stein Shrinkage, and Model Averaging",
    "section": "26.21 Backward Stepwise Regression",
    "text": "26.21 Backward Stepwise Regression\n\nStart with all regressors \\(\\left\\{X_{1}, \\ldots, X_{K}\\right\\}\\) included in the “active set”.\nFor \\(m=0, \\ldots, K-1\\)\n\n\nEstimate the regression of \\(Y\\) on the active set.\nIdentify the regressor whose omission will have the smallest impact on \\(C(m)\\).\nPut this regressor in slot \\(K-m\\) and delete from the active set.\nCalculate \\(C(m)\\) and store in slot \\(K-m\\).\n\n 1. The model with the smallest value of \\(C(m)\\) is the selected model.\nBackward stepwise regression requires \\(K<n\\) so that regression with all variables is feasible. It produces an ordering of the regressors from “most relevant” to “least relevant”. A simplified version is to exit the loop when \\(C(m)\\) increases. (This may not yield the same result as completing the loop.) For the case of AIC selection, step (b) can be implemented by calculating the classical (homoskedastic) t-ratio for each active regressor and find the regressor with the smallest absolute t-ratio. (See Exercise 28.3.)"
  },
  {
    "objectID": "chpt28-model-selection.html#forward-stepwise-regression",
    "href": "chpt28-model-selection.html#forward-stepwise-regression",
    "title": "26  Model Selection, Stein Shrinkage, and Model Averaging",
    "section": "26.22 Forward Stepwise Regression",
    "text": "26.22 Forward Stepwise Regression\n\nStart with the null set \\(\\{\\varnothing\\}\\) as the “active set” and all regressors \\(\\left\\{X_{1}, \\ldots, X_{K}\\right\\}\\) as the “inactive set”.\nFor \\(m=1, \\ldots, \\min (n-1, K)\\)\n\n\nEstimate the regression of \\(Y\\) on the active set.\nIdentify the regressor in the inactive set whose inclusion will have the largest impact on \\(C(m)\\).\nPut this regressor in slot \\(m\\) and move it from the inactive to the active set.\nCalculate \\(C(m)\\) and store in slot \\(m\\).\n\n 1. The model with the smallest value of \\(C(m)\\) is the selected model.\nA simplified version is to exit the loop when \\(C(m)\\) increases. (This may not yield the same answer as completing the loop.) For the case of AIC selection step (b) can be implemented by finding the regressor in the inactive set with the largest absolute correlation with the residual from step (a). (See Exercise 28.4.)\nThere are combined algorithms which check both forward and backward movements at each step. The algorithms can also be implemented with the regressors organized in groups (so that all elements are either included or excluded at each step). There are also old-fashioned versions which use significance testing rather than selection criterion (this is generally not advised).\nStepwise regression based on old-fashioned significance testing can be implemented in Stata using the stepwise command. If attention is confined to models which include regressors one-at-a-time, AIC selection can be implemented by setting the significance level equal to \\(p=0.32\\). Thus the command stepwise, \\(\\operatorname{pr}\\) (.32) implements backward stepwise regression with the AIC criterion, and stepwise, pe (.32) implements forward stepwise regression with the AIC criterion.\nStepwise regression can be implemented in R using the lars command."
  },
  {
    "objectID": "chpt28-model-selection.html#the-mse-of-model-selection-estimators",
    "href": "chpt28-model-selection.html#the-mse-of-model-selection-estimators",
    "title": "26  Model Selection, Stein Shrinkage, and Model Averaging",
    "section": "26.23 The MSE of Model Selection Estimators",
    "text": "26.23 The MSE of Model Selection Estimators\nModel selection can lead to estimators with poor sampling performance. In this section we show that the mean squared error of estimation is not necessarily improved, and can be considerably worsened, by model selection.\nTo keep things simple consider an estimator with an exact normal distribution and known covariance matrix. Normalizing the latter to the identity we consider the setting\n\\[\n\\widehat{\\theta} \\sim \\mathrm{N}\\left(\\theta, I_{K}\\right)\n\\]\nand the class of model selection estimators\n\\[\n\\widehat{\\theta}_{\\mathrm{pms}}=\\left\\{\\begin{array}{lll}\n\\widehat{\\theta} & \\text { if } & \\widehat{\\theta}^{\\prime} \\widehat{\\theta}>c \\\\\n0 & \\text { if } & \\widehat{\\theta}^{\\prime} \\hat{\\theta} \\leq c\n\\end{array}\\right.\n\\]\nfor some \\(c\\). AIC sets \\(c=2 K\\), BIC sets \\(c=K \\log (n)\\), and \\(5 %\\) significance testing sets \\(c\\) to equal the \\(95 %\\) quantile of the \\(\\chi_{K}^{2}\\) distribution. It is common to call \\(\\widehat{\\theta}_{\\mathrm{pms}}\\) a post-model-selection (PMS) estimator\nWe can explicitly calculate the MSE of \\(\\widehat{\\theta}_{\\mathrm{pms}}\\).\nTheorem 28.10 If \\(\\widehat{\\theta} \\sim \\mathrm{N}\\left(\\theta, \\boldsymbol{I}_{K}\\right)\\) then\n\\[\n\\operatorname{mse}\\left[\\widehat{\\theta}_{\\mathrm{pms}}\\right]=K+(2 \\lambda-K) F_{K+2}(c, \\lambda)-\\lambda F_{K+4}(c, \\lambda)\n\\]\nwhere \\(F_{r}(x, \\lambda)\\) is the non-central chi-square distribution function with \\(r\\) degrees of freedom and non-centrality parameter \\(\\lambda=\\theta^{\\prime} \\theta\\).\nThe proof is given in Section \\(28.32\\).\nThe MSE is determined only by \\(K, \\lambda\\), and \\(c . \\lambda=\\theta^{\\prime} \\theta\\) turns out to be an important parameter for the MSE. As the squared Euclidean length, it indexes the magnitude of the coefficient \\(\\theta\\).\nWe can see the following limiting cases. If \\(\\lambda=0\\) then mse \\(\\left[\\widehat{\\theta}_{\\mathrm{pms}}\\right]=K\\left(1-F_{K+2}(c, 0)\\right)\\). As \\(\\lambda \\rightarrow \\infty\\) then mse \\(\\left[\\widehat{\\theta}_{\\mathrm{pms}}\\right] \\rightarrow K\\). The unrestricted estimator obtains if \\(c=0\\), in which case mse \\(\\left[\\widehat{\\theta}_{\\mathrm{pms}}\\right]=K\\). As \\(c \\rightarrow \\infty\\), mse \\(\\left[\\widehat{\\theta}_{\\mathrm{pms}}\\right] \\rightarrow \\lambda\\). The latter fact implies that the PMS estimator based on the BIC has MSE \\(\\rightarrow \\infty\\) as \\(n \\rightarrow \\infty\\).\nUsing Theorem \\(28.10\\) we can numerically calculate the MSE. In Figure 28.1(a) and (b) we plot the MSE of a set of estimators for a range of values of \\(\\sqrt{\\lambda}\\). Panel (a) is for \\(K=1\\), panel (b) is for \\(K=5\\). Note that the MSE of the unselected estimator \\(\\widehat{\\theta}\\) is invariant to \\(\\lambda\\), so its MSE plot is a flat line at \\(K\\). The other estimators plotted are AIC selection ( \\(c=2 K\\) ), 5% significance testing selection (chi-square critical value), and BIC selection \\((c=K \\log (n))\\) for \\(n=200\\) and \\(n=1000\\).\nIn the plots you can see that the PMS estimators have lower MSE than the unselected estimator roughly for \\(\\lambda<K\\) but higher MSE for \\(\\lambda>K\\). The AIC estimator has MSE which is least distorted from the unselected estimator, reaching a peak of about \\(1.5\\) for \\(K=1\\). The BIC estimators, however, have very large MSE for larger values of \\(\\lambda\\), and the distortion is growing as \\(n\\) increases. The MSE of the selection estimators increases with \\(\\lambda\\) until it reaches a peak and then slowly decreases and asymptotes back to \\(K\\). Furthermore, the MSE of BIC is unbounded as \\(n\\) diverges. Thus for very large sample sizes the MSE of a BIC-selected estimator can be a very large multiple of the MSE of the unselected estimator. The plots show that if \\(\\lambda\\) is small there are advantages to model selection as MSE can be greatly reduced. However if \\(\\lambda\\) is large then MSE can be greatly increased if BIC is used, and moderately increased if AIC is used. A sensible reading of the plots leads to the practical recommendation to not use the BIC for model selection, and use the AIC with care.\n\n\nMSE, \\(K=1\\)\n\n\n\nMSE, \\(K=5\\)\n\nFigure 28.1: MSE of Post-Model-Selection Estimators\nThe numerical calculations show that MSE is reduced by selection when \\(\\lambda\\) is small but increased when \\(\\lambda\\) is moderately large. What does this mean in practice? \\(\\lambda\\) is small when \\(\\theta\\) is small which means the compared models are similar in terms of estimation accuracy. In these contexts model selection can be valuable as it helps select smaller models to improve precision. However when \\(\\lambda\\) is moderately large (which means that \\(\\theta\\) is moderately large) the smaller model has meaningful omitted variable bias, yet the selection criteria have difficulty detecting which model to use. The conservative BIC selection procedure tends to select the smaller model and thus incurs greater bias resulting in high MSE. These considerations suggest that it is better to use the AIC when selecting among models with similar estimation precision. Unfortunately it is impossible to known a priori the appropriate models.\nThe results of this section may appear to contradict Theorem \\(28.8\\) which showed that the BIC is consistent for parsimonious models as for all \\(\\lambda>0\\) in the plots the correct parsimonious model is the larger model. Yet BIC is not selecting this model with sufficient frequency to produce a low MSE. There is no contradiction. The consistency of the BIC appears in the lower portion of the plots where the MSE of the BIC estimator is very small, and approaching zero as \\(\\lambda \\rightarrow 0\\). The fact that the MSE of the AIC estimator somewhat exceeds that of the BIC in this region is due to the over-selection property of the AIC."
  },
  {
    "objectID": "chpt28-model-selection.html#inference-after-model-selection",
    "href": "chpt28-model-selection.html#inference-after-model-selection",
    "title": "26  Model Selection, Stein Shrinkage, and Model Averaging",
    "section": "26.24 Inference After Model Selection",
    "text": "26.24 Inference After Model Selection\nEconomists are typically interested in inferential questions such as hypothesis tests and confidence intervals. If an econometric model has been selected by a procedure such as AIC or CV what are the properties of statistical tests applied to the selected model?\nTo be concrete, consider the regression model \\(Y=X_{1} \\beta_{1}+X_{2} \\beta_{2}+e\\) and selection of the variable \\(X_{2}\\). That is, we compare \\(Y=X_{1} \\beta_{1}+e\\) with \\(Y=X_{1} \\beta_{1}+X_{2} \\beta_{2}+e\\). It is not too deep a realization that in this context it is inappropriate to conduct conventional inference for \\(\\beta_{2}\\) in the selected model. If we select the smaller model there is no estimate of \\(\\beta_{2}\\). If we select the larger it is because the \\(\\mathrm{t}\\)-ratio for \\(\\beta_{2}\\) exceeds the critical value. The distribution of the t-ratio, conditional on exceeding a critical value, is not conventionally distributed and there seems little point to push this issue further.\nThe more interesting and subtle question is the impact on inference concerning \\(\\beta_{1}\\). This indeed is a context of typical interest. An economist is interested in the impact of \\(X_{1}\\) on \\(Y\\) given a set of controls \\(X_{2}\\). It is common to select across these controls to find a suitable empirical model. Once this has been obtained we want to make inferential statements about \\(\\beta_{1}\\). Has selection over the controls impacted inference?\nWe illustrate the issue numerically. Suppose that \\(\\left(X_{1}, X_{2}\\right)\\) are jointly normal with unit variances and correlation \\(\\rho, e\\) is independent and standard normal, and \\(n=30\\). We estimate the long regression of \\(Y\\) on \\(\\left(X_{1}, X_{2}\\right)\\) and the short regression of \\(Y\\) on \\(X_{1}\\) alone. We construct the t-statistic \\({ }^{3}\\) for \\(\\beta_{2}=0\\) in the long regression and select the long regression if the t-statistic is significant at the \\(5 %\\) level and select the short regression if the \\(\\mathrm{t}\\)-statistic is not significant. We construct the standard \\(95 %\\) confidence interval \\({ }^{4}\\) for \\(\\beta_{1}\\) in the selected regression. These confidence intervals will have exact \\(95 %\\) coverage when there is no selection and the estimated model is correct, so deviations from \\(95 %\\) are due to model selection and misspecification. We calculate the actual coverage probability by simulation using one million replications, varying \\({ }^{5} \\beta_{2}\\) and \\(\\rho\\).\n\nFigure 28.2: Coverage Probability of Post-Model Selection\nWe display in Figure \\(28.2\\) the coverage probabilities as a function of \\(\\beta_{2}\\) for several values of \\(\\rho\\). If the\n\\({ }^{3}\\) Using the homoskedastic variance formula and assuming the error variance is known. This is done to focus on the selection issue rather than covariance matrix estimation.\n\\({ }^{4}\\) Using the homoskedastic variance formula and assuming the correct error variance is known.\n\\({ }^{5}\\) The coverage probability is invariant to \\(\\beta_{1}\\). regressors are uncorrelated \\((\\rho=0)\\) then the actual coverage probability equals the nominal level of \\(0.95\\). This is because the t-statistic for \\(\\beta_{2}\\) is independent of those for \\(\\beta_{1}\\) in this normal regression model and the coefficients on \\(X_{1}\\) in the short and long regression are identical.\nThis invariance breaks down for \\(\\rho \\neq 0\\). As \\(\\rho\\) increases the coverage probability of the confidence intervals fall below the nominal level. The distortion is strongly affected by the value of \\(\\beta_{2}\\). For \\(\\beta_{2}=0\\) the distortion is mild. The reason is that when \\(\\beta_{2}=0\\) the selection t-statistic selects the short regression with high probability (95%) which leads to approximately valid inference. Also, as \\(\\beta_{2} \\rightarrow \\infty\\) the coverage probability converges to the nominal level. The reason is that for large \\(\\beta_{2}\\) the selection t-statistic selects the long regression with high probability, again leading to approximately valid inference. The distortion is large, however, for intermediate values of \\(\\beta_{2}\\). For \\(\\rho=0.5\\) the coverage probability falls to \\(88 %\\), and for \\(\\rho=0.8\\) the probability is low as \\(62 %\\). The reason is that for intermediate values of \\(\\beta_{2}\\) the selection \\(\\mathrm{t}\\)-statistic selects both models with meaningful probability, and this selection decision is correlated with the t-statistics for \\(\\beta_{1}\\). The degree of under-coverage is enormous and greatly troubling.\nThe message from this display is that inference after model selection is problematic. Conventional inference procedures do not have conventional distributions and the distortions are potentially unbounded."
  },
  {
    "objectID": "chpt28-model-selection.html#empirical-illustration",
    "href": "chpt28-model-selection.html#empirical-illustration",
    "title": "26  Model Selection, Stein Shrinkage, and Model Averaging",
    "section": "26.25 Empirical Illustration",
    "text": "26.25 Empirical Illustration\nWe illustrate the model selection methods with an application. Take the CPS dataset and the subsample of Asian women which has \\(n=1149\\) observations. Consider a log wage regression with primary interest on the return to experience measured as the percentage difference between expected wages between 0 and 30 years of experience. We consider and compare nine least squares regressions. All include an indicator for married and three indicators for the region. The estimated models range in complexity concerning the impact of education and experience.\nTable 28.1: Estimates of Return to Experience among Asian Women\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nModel 1\nModel 2\nModel 3\nModel 4\nModel 5\nModel 6\nModel 7\nModel 8\nModel 9\n\n\n\n\nReturn\n\\(13 %\\)\n\\(22 %\\)\n\\(20 %\\)\n\\(29 %\\)\n\\(40 %\\)\n\\(37 %\\)\n\\(33 %\\)\n\\(47 %\\)\n\\(45 %\\)\n\n\ns.e.\n7\n8\n7\n11\n11\n11\n17\n18\n17\n\n\nBIC\n956\n\\(\\mathbf{9 0 7}\\)\n924\n964\n913\n931\n977\n925\n943\n\n\nAIC\n915\n861\n858\n914\n858\n\\(\\mathbf{8 5 5}\\)\n916\n860\n857\n\n\nCV\n405\n387\n386\n405\n385\n\\(\\mathbf{3 8 5}\\)\n406\n387\n386\n\n\nFIC\n86\n48\n53\n58\n\\(\\mathbf{3 2}\\)\n34\n86\n71\n68\n\n\nEducation\nCollege\nSpline\nDummy\nCollege\nSpline\nDummy\nCollege\nSpline\nDummy\n\n\nExperience\n2\n2\n2\n4\n4\n4\n6\n6\n6\n\n\n\nTerms for experience:\n\nModels 1-3 include include experience and its square.\nModels 4-6 include powers of experience up to power 4 .\nModels 7-9 include powers of experience up to power 6 .\n\nTerms for education: - Models 1, 4, and 7 include a single dummy variable college indicating that years of education is 16 or higher.\n\nModels 2, 5, and 8 include a linear spline in education with a single knot at education=9.\nModels 3, 6, and 9 include six dummy variables, for education equalling 12, 13, 14, 16, 18, and 20.\n\nTable \\(28.1\\) reports key estimates from the nine models. Reported are the estimate of the return to experience as a percentage wage difference, its standard error (HC1), the BIC, AIC, CV, and FIC*, the latter treating the return to experience as the focus. What we can see is that the estimates vary meaningfully, ranging from \\(13 %\\) to \\(47 %\\). Some of the estimates also have moderately large standard errors. (In most models the return to experience is “statistically significant”, but by large standard errors we mean that it is difficult to pin down the precise value of the return to experience.) We can also see that the most important factors impacting the magnitude of the point estimate is going beyond the quadratic specification for experience, and going beyond the simplest specification for education. Another item to notice is that the standard errors are most affected by the number of experience terms.\nThe BIC picks a parsimonious model with the linear spline in education and a quadratic in experience. The AIC and CV select a less parsimonious model with the full dummy specification for education and a \\(4^{\\text {th }}\\) order polynomial in experience. The FIC selects an intermediate model, with a linear spline in education and a \\(4^{t h}\\) order polynomial in experience.\nWhen selecting a model using information criteria it is useful to examine several criteria. In applications, decisions should be made by a combination of judgment as well as the formal criteria. In this case the cross-validation criterion selects model 6 which has the estimate \\(37 %\\), but near-similar values of the CV criterion are obtained by models 3 and 9 which have the estimates \\(20 %\\) and \\(45 %\\). The FIC, which focuses on this specific coefficient, selects model 5 which has the point estimate \\(40 %\\) which is similar to the CV-selected model. Overall based on this evidence the CV-selected model and its point estimate of \\(37 %\\) seems an appropriate choice. However, the uncertainty reflected by the flatness of the CV criterion suggests that uncertainty remains in the choice of specification."
  },
  {
    "objectID": "chpt28-model-selection.html#shrinkage-methods",
    "href": "chpt28-model-selection.html#shrinkage-methods",
    "title": "26  Model Selection, Stein Shrinkage, and Model Averaging",
    "section": "26.26 Shrinkage Methods",
    "text": "26.26 Shrinkage Methods\nShrinkage methods are a broad class of estimators which reduce variance by moving an estimator \\(\\hat{\\theta}\\) towards a pre-selected point such as the zero vector. In high dimensions the reduction in variance more than compensates for the increase in bias resulting in improved efficiency when measured by mean squared error. This and the next few sections review material presented in Chapter 15 of Probability and Statistics for Economists.\nThe simplest shrinkage estimator takes the form \\(\\widetilde{\\theta}=(1-w) \\widehat{\\theta}\\) for some shrinkage weight \\(w \\in[0,1]\\). Setting \\(w=0\\) we obtain \\(\\widetilde{\\theta}=\\widehat{\\theta}\\) (no shrinkage) and setting \\(w=1\\) we obtain \\(\\widetilde{\\theta}=0\\) (full shrinkage). It is straightforward to calculate the MSE of this estimator. Assume \\(\\widehat{\\theta} \\sim(\\theta, V)\\). Then \\(\\widetilde{\\theta}\\) has bias\n\\[\n\\operatorname{bias}[\\widetilde{\\theta}]=\\mathbb{E}[\\widetilde{\\theta}]-\\theta=-w \\theta,\n\\]\nvariance\n\\[\n\\operatorname{var}[\\widetilde{\\theta}]=(1-w)^{2} \\boldsymbol{V},\n\\]\nand weighted mean squared error (using the weight matrix \\(\\boldsymbol{W}=\\boldsymbol{V}^{-1}\\) )\n\\[\n\\text { wmse }[\\widetilde{\\theta}]=K(1-w)^{2}+w^{2} \\lambda\n\\]\nwhere \\(\\lambda=\\theta^{\\prime} \\boldsymbol{V}^{-1} \\theta\\). Theorem 28.11 If \\(\\widehat{\\theta} \\sim(\\theta, V)\\) and \\(\\widetilde{\\theta}=(1-w) \\widehat{\\theta}\\) then\n\nwmse \\([\\widetilde{\\theta}]<\\) wmse \\([\\hat{\\theta}]\\) if \\(0<w<2 K /(K+\\lambda)\\).\nwmse \\([\\widetilde{\\theta}]\\) is minimized by the shrinkage weight \\(w_{0}=K /(K+\\lambda)\\).\nThe minimized WMSE is wmse \\([\\widetilde{\\theta}]=K \\lambda /(K+\\lambda)\\).\n\nFor the proof see Exercise \\(28.6\\).\nPart 1 of the theorem shows that the shrinkage estimator has reduced WMSE for a range of values of the shrinkage weight \\(w\\). Part 2 of the theorem shows that the WMSE-minimizing shrinkage weight is a simple function of \\(K\\) and \\(\\lambda\\). The latter is a measure of the magnitude of \\(\\theta\\) relative to the estimation variance. When \\(\\lambda\\) is large (the coefficients are large) then the optimal shrinkage weight \\(w_{0}\\) is small; when \\(\\lambda\\) is small (the coefficients are small) then the optimal shrinkage weight \\(w_{0}\\) is large. Part 3 calculates the associated optimal WMSE. This can be substantially less than the WMSE of the original estimator \\(\\widehat{\\theta}\\). For example, if \\(\\lambda=K\\) then wmse \\([\\widetilde{\\theta}]=K / 2\\), one-half the WMSE of the original estimator.\nTo construct the optimal shrinkage weight we need the unknown \\(\\lambda\\). An unbiased estimator is \\(\\hat{\\lambda}=\\) \\(\\widehat{\\theta}^{\\prime} \\boldsymbol{V}^{-1} \\widehat{\\theta}-K\\) (see Exercise 28.7) implying the shrinkage weight\n\\[\n\\widehat{w}=\\frac{K}{\\widehat{\\theta}^{\\prime} \\boldsymbol{V}^{-1} \\widehat{\\theta}} .\n\\]\nReplacing \\(K\\) with a free parameter \\(c\\) (which we call the shrinkage coefficient) we obtain\n\\[\n\\widetilde{\\theta}=\\left(1-\\frac{c}{\\widehat{\\theta}^{\\prime} \\boldsymbol{V}^{-1} \\widehat{\\theta}}\\right) \\widehat{\\theta} .\n\\]\nThis is often called a Stein-Rule estimator.\nThis estimator has many appealing properties. It can be viewed as a smoothed selection estimator. The quantity \\(\\widehat{\\theta}^{\\prime} \\boldsymbol{V}^{-1} \\widehat{\\theta}\\) is a Wald statistic for the hypothesis \\(\\mathbb{H}_{0}: \\theta=0\\). Thus when this Wald statistic is large (when the evidence suggests the hypothesis of a zero coefficient is false) the shrinkage estimator is close to the original estimator \\(\\widehat{\\theta}\\). However when this Wald statistic is small (when the evidence is consistent with the hypothesis of a zero coefficient) then the shrinkage estimator moves the original estimator towards zero."
  },
  {
    "objectID": "chpt28-model-selection.html#james-stein-shrinkage-estimator",
    "href": "chpt28-model-selection.html#james-stein-shrinkage-estimator",
    "title": "26  Model Selection, Stein Shrinkage, and Model Averaging",
    "section": "26.27 James-Stein Shrinkage Estimator",
    "text": "26.27 James-Stein Shrinkage Estimator\nJames and Stein (1961) made the following discovery.\nTheorem 28.12 Assume that \\(\\widehat{\\theta} \\sim \\mathrm{N}(\\theta, V), \\widetilde{\\theta}\\) is defined in (28.25), and \\(K>2\\).\n\nIf \\(0<c<2(K-2)\\) then wmse \\([\\widetilde{\\theta}]<\\) wmse \\([\\widehat{\\theta}]\\).\nThe WMSE is minimized by setting \\(c=K-2\\) and equals\n\n\\[\n\\text { wmse }[\\widetilde{\\theta}]=K-(K-2)^{2} \\mathbb{E}\\left[Q_{K}^{-1}\\right]\n\\]\nwhere \\(Q_{K} \\sim \\chi_{K}^{2}(\\lambda)\\). See Theorem \\(15.3\\) of Probability and Statistics for Economists.\nThis result stunned the world of statistics. Part 1 shows that the shrinkage estimator has strictly smaller WMSE for all values of the parameters and thus dominates the original estimator. The latter is the MLE so this result shows that the MLE is dominated and thus inadmissible. This is a stunning result because it had previously been assumed that it would be impossible to find an estimator which dominates the MLE.\nTheorem \\(28.12\\) critically depends on the condition \\(K>2\\). This means that shrinkage achieves uniform improvements only in dimensions three or larger.\nThe minimizing choice for the shrinkage coefficient \\(c=K-2\\) leads to what is commonly known as the James-Stein estimator\n\\[\n\\widetilde{\\theta}=\\left(1-\\frac{K-2}{\\widehat{\\theta}^{\\prime} \\boldsymbol{V}^{-1} \\widehat{\\theta}}\\right) \\widehat{\\theta} .\n\\]\nIn practice \\(\\boldsymbol{V}\\) is unknown so we substitute an estimator \\(\\widehat{\\boldsymbol{V}}\\). This leads to\n\\[\n\\widetilde{\\theta}_{\\mathrm{JS}}=\\left(1-\\frac{K-2}{\\widehat{\\theta}^{\\prime} \\widehat{\\boldsymbol{V}}^{-1} \\widehat{\\theta}}\\right) \\widehat{\\theta}\n\\]\nwhich is fully feasible as it does not depend on unknowns or tuning parameters. The substitution of \\(\\widehat{\\boldsymbol{V}}\\) for \\(\\boldsymbol{V}\\) can be justified by finite sample or asymptotic arguments."
  },
  {
    "objectID": "chpt28-model-selection.html#interpretation-of-the-stein-effect",
    "href": "chpt28-model-selection.html#interpretation-of-the-stein-effect",
    "title": "26  Model Selection, Stein Shrinkage, and Model Averaging",
    "section": "26.28 Interpretation of the Stein Effect",
    "text": "26.28 Interpretation of the Stein Effect\nThe James-Stein Theorem appears to conflict with classical statistical theory. The original estimator \\(\\widehat{\\theta}\\) is the maximum likelihood estimator. It is unbiased. It is minimum variance unbiased. It is CramerRao efficient. How can it be that the James-Stein shrinkage estimator achieves uniformly smaller mean squared error?\nPart of the answer is that classical theory has caveats. The Cramer-Rao Theorem, for example, restricts attention to unbiased estimators and thus precludes consideration of shrinkage estimators. The James-Stein estimator has reduced MSE, but is not Cramer-Rao efficient because it is biased. Therefore the James-Stein Theorem does not conflict with the Cramer-Rao Theorem. Rather, they are complementary results. On the one hand, the Cramer-Rao Theorem describes the best possible variance when unbiasedness is an important property for estimation. On the other hand, the James-Stein Theorem shows that if unbiasedness is not a critical property but instead MSE is important, then there are better estimators than the MLE.\nThe James-Stein Theorem may also appear to conflict with our results from Section \\(28.16\\) which showed that selection estimators do not achieve uniform MSE improvements over the MLE. This may appear to be a conflict because the James-Stein estimator has a similar form to a selection estimator. The difference is that selection estimators are hard threshold rules - they are discontinuous functions of the data - while the James-Stein estimator is a soft threshold rule - it is a continuous function of the data. Hard thresholding tends to result in high variance; soft thresholding tends to result in low variance. The James-Stein estimator is able to achieve reduced variance because it is a soft threshold function.\nThe MSE improvements achieved by the James-Stein estimator are greatest when \\(\\lambda\\) is small. This occurs when the parameters \\(\\theta\\) are small in magnitude relative to the estimation variance \\(\\boldsymbol{V}\\). This means that the user needs to choose the centering point wisely."
  },
  {
    "objectID": "chpt28-model-selection.html#positive-part-estimator",
    "href": "chpt28-model-selection.html#positive-part-estimator",
    "title": "26  Model Selection, Stein Shrinkage, and Model Averaging",
    "section": "26.29 Positive Part Estimator",
    "text": "26.29 Positive Part Estimator\nThe simple James-Stein estimator has the odd property that it can “over-shrink”. When \\(\\widehat{\\theta}^{\\prime} \\boldsymbol{V}^{-1} \\widehat{\\theta}<K-2\\) then \\(\\widetilde{\\theta}\\) has the opposite sign from \\(\\widehat{\\theta}\\). This does not make sense and suggests that further improvements can be made. The standard solution is to use “positive-part” trimming by bounding the shrinkage weight (28.24) between zero and one. This estimator can be written as\n\\[\n\\begin{aligned}\n\\widetilde{\\theta}^{+} &=\\left\\{\\begin{array}{cc}\n\\widetilde{\\theta}, & \\widehat{\\theta}^{\\prime} \\boldsymbol{V}^{-1} \\widehat{\\theta} \\geq K-2 \\\\\n0, & \\widehat{\\theta}^{\\prime} \\boldsymbol{V}^{-1} \\widehat{\\theta}<K-2\n\\end{array}\\right.\\\\\n&=\\left(1-\\frac{K-2}{\\hat{\\theta}^{\\prime} \\boldsymbol{V}^{-1} \\widehat{\\theta}}\\right)_{+} \\widehat{\\theta}\n\\end{aligned}\n\\]\nwhere \\((a)_{+}=\\max [a, 0]\\) is the “positive-part” function. Alternatively, it can be written as\n\\[\n\\widetilde{\\theta}^{+}=\\widehat{\\theta}-\\left(\\frac{K-2}{\\widehat{\\theta}^{\\prime} \\boldsymbol{V}^{-1} \\widehat{\\theta}}\\right)_{1} \\widehat{\\theta}\n\\]\nwhere \\((a)_{1}=\\min [a, 1]\\)\nThe positive part estimator simultaneously performs “selection” as well as “shrinkage”. If \\(\\widehat{\\theta}^{\\prime} \\boldsymbol{V}^{-1} \\widehat{\\theta}\\) is sufficiently small, \\(\\widetilde{\\theta}^{+}\\)“selects” 0 . When \\(\\widehat{\\theta}^{\\prime} \\boldsymbol{V}^{-1} \\widehat{\\theta}\\) is of moderate size, \\(\\widetilde{\\theta}^{+}\\)shrinks \\(\\widehat{\\theta}\\) towards zero. When \\(\\widehat{\\theta}^{\\prime} \\boldsymbol{V}^{-1} \\widehat{\\theta}\\) is very large, \\(\\widetilde{\\theta}^{+}\\)is close to the original estimator \\(\\widehat{\\theta}\\).\nConsistent with our intuition the positive part estimator has uniformly lower WMSE than the unadjusted James-Stein estimator.\nTheorem 28.13 Under the assumptions of Theorem \\(28.12\\)\n\\[\n\\operatorname{wmse}\\left[\\widetilde{\\theta}^{+}\\right]<\\operatorname{wmse}[\\tilde{\\theta}] .\n\\]\nFor a proof see Theorem \\(15.6\\) of Probability and Statistics for Economists. Theorem \\(15.7\\) of Probability and Statistics for Economists provides an explicit numerical evaluation of the MSE for the positive-part estimator.\nIn Figure \\(28.3\\) we plot wmse \\(\\left[\\widetilde{\\theta}^{+}\\right] / K\\) as a function of \\(\\lambda / K\\) for \\(K=4,6,12\\), and 48 . The plots are uniformly below 1 (the normalized WMSE of the MLE) and substantially so for small and moderate values of \\(\\lambda\\). The WMSE functions fall as \\(K\\) increases, demonstrating that the MSE reductions are more substantial when \\(K\\) is large.\nIn summary, the positive-part transformation is an important improvement over the unadjusted James-Stein estimator. It is more reasonable and reduces the mean squared error. The broader message is that imposing boundary conditions on shrinkage weights can improve estimation efficiency."
  },
  {
    "objectID": "chpt28-model-selection.html#shrinkage-towards-restrictions",
    "href": "chpt28-model-selection.html#shrinkage-towards-restrictions",
    "title": "26  Model Selection, Stein Shrinkage, and Model Averaging",
    "section": "26.30 Shrinkage Towards Restrictions",
    "text": "26.30 Shrinkage Towards Restrictions\nThe classical James-Stein estimator does not have direct use in applications because it is rare that we wish to shrink an entire parameter vector towards a specific point. Rather, it is more common to shrink a parameter vector towards a set of restrictions. Here are a few examples:\n\nShrink a long regression towards a short regression.\n\n\nFigure 28.3: WMSE of James-Stein Estimator\n 1. Shrink a regression towards an intercept-only model.\n\nShrink the regression coefficients towards a set of restrictions.\nShrink a set of estimates (or coefficients) towards their common mean.\nShrink a set of estimates (or coefficients) towards a parametric model.\nShrink a nonparametric series model towards a parametric model.\n\nThe way to think generally about these applications it that the researcher wants to allow for generality with the large model but believes that the smaller model may be a useful approximation. A shrinkage estimator allows the data to smoothly select between these two options depending on the strength of information for the two specifications.\nLet \\(\\widehat{\\theta} \\sim \\mathrm{N}(\\theta, \\boldsymbol{V})\\) be the original estimator, for example a set of regression coefficient estimates. The normality assumption is used for the exact theory but can be justified based on an asymtotic approximation as well. The researcher considers a set of \\(q>2\\) linear restrictions which can be written as \\(\\boldsymbol{R}^{\\prime} \\theta=\\boldsymbol{r}\\) where \\(\\boldsymbol{R}\\) is \\(K \\times q\\) and \\(\\boldsymbol{r}\\) is \\(q \\times 1\\). A minimum distance estimator for \\(\\theta\\) is\n\\[\n\\widehat{\\theta}_{\\boldsymbol{R}}=\\widehat{\\theta}-\\boldsymbol{V} \\boldsymbol{R}\\left(\\boldsymbol{R}^{\\prime} \\boldsymbol{V} \\boldsymbol{R}\\right)^{-1}\\left(\\boldsymbol{R}^{\\prime} \\widehat{\\theta}-\\boldsymbol{r}\\right) .\n\\]\nThe James-Stein estimator with positive-part trimming is\n\\[\n\\widetilde{\\theta}^{+}=\\widehat{\\theta}-\\left(\\frac{q-2}{\\left(\\widehat{\\theta}-\\widehat{\\theta}_{\\boldsymbol{R}}\\right)^{\\prime} \\boldsymbol{V}^{-1}\\left(\\widehat{\\theta}-\\widehat{\\theta}_{\\boldsymbol{R}}\\right)}\\right)_{1}\\left(\\widehat{\\theta}-\\widehat{\\theta}_{\\boldsymbol{R}}\\right) .\n\\]\nThe function \\((a)_{1}=\\min [a, 1]\\) bounds the shrinkage weight below one.\nTheorem 28.14 Under the assumptions of Theorem 28.12, if \\(q>2\\) then\n\\[\n\\operatorname{wmse}\\left[\\widetilde{\\theta}^{+}\\right]<\\operatorname{wmse}[\\widetilde{\\theta}] .\n\\]\nThe shrinkage estimator achieves uniformly smaller MSE if the number of restrictions is three or greater. The number of restrictions \\(q\\) plays the same role as the number of parameters \\(K\\) in the classical James-Stein estimator. Shrinkage achieves greater gains when there are more restrictions \\(q\\), and achieves greater gains when the restrictions are close to being satisfied in the population. If the imposed restrictions are far from satisfied then the shrinkage estimator will have similar performance as the original estimator. It is therefore important to select the restrictions carefully.\nIn practice the covariance matrix \\(\\boldsymbol{V}\\) is unknown so it is replaced by an estimator \\(\\widehat{\\boldsymbol{V}}\\). Thus the feasible version of the estimators equal\n\\[\n\\widehat{\\theta}_{\\boldsymbol{R}}=\\widehat{\\theta}-\\widehat{\\boldsymbol{V}} \\boldsymbol{R}\\left(\\boldsymbol{R}^{\\prime} \\widehat{\\boldsymbol{V}} \\boldsymbol{R}\\right)^{-1}\\left(\\boldsymbol{R}^{\\prime} \\widehat{\\theta}-\\boldsymbol{r}\\right)\n\\]\nand\n\\[\n\\widetilde{\\theta}^{+}=\\widehat{\\theta}-\\left(\\frac{q-2}{J}\\right)_{1}\\left(\\widehat{\\theta}-\\widehat{\\theta}_{\\boldsymbol{R}}\\right)\n\\]\nwhere\n\\[\nJ=\\left(\\widehat{\\theta}-\\widehat{\\theta}_{\\boldsymbol{R}}\\right)^{\\prime} \\widehat{\\boldsymbol{V}}^{-1}\\left(\\widehat{\\theta}-\\widehat{\\theta}_{\\boldsymbol{R}}\\right) .\n\\]\nIt is insightful to notice that \\(J\\) is the minimum distance statistic for the test of the hypothesis \\(\\mathbb{H}_{0}\\) : \\(\\boldsymbol{R}^{\\prime} \\theta=\\boldsymbol{r}\\) against \\(\\mathbb{H}_{1}: \\boldsymbol{R}^{\\prime} \\theta \\neq \\boldsymbol{r}\\). Thus the degree of shrinkage is a smoothed version of the standard test of the restrictions. When \\(J\\) is large (so the evidence indicates that the restrictions are false) the shrinkage estimator is close to the unrestricted estimator \\(\\widehat{\\theta}\\). When \\(J\\) is small (so the evidence indicates that the restrictions could be correct) the shrinkage estimator equals the restricted estimator \\(\\widehat{\\theta}_{\\boldsymbol{R}}\\). For intermediate values of \\(J\\) the shrinkage estimator shrinks \\(\\widehat{\\theta}\\) towards \\(\\widehat{\\theta}_{\\boldsymbol{R}}\\).\nWe can substitute for \\(J\\) any similar asymptotically chi-square statistic, including the Wald, Likelihood Ratio, and Score statistics. We can also use the F statistic (which is commonly produced by statistical software) if we multiply by \\(q\\). These substitutions do not produce the same exact finite sample distribution but are asymptotically equivalent.\nIn linear regression we have some very convenient simplifications available. In general, \\(\\widehat{\\boldsymbol{V}}\\) can be a heteroskedastic-robust or cluster-robust covariance matrix estimator. However, if the dimension \\(K\\) of the unrestricted estimator is quite large or has sparse dummy variables then these covariance matrix estimators are ill-behaved and it may be better to use a classical covariance matrix estimator to perform the shrinkage. If this is done then \\(\\widehat{\\boldsymbol{V}}=\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} s^{2}, \\widehat{\\theta}_{\\boldsymbol{R}}\\) is the constrained least squares estimator (in most applications the least squares estimator of the short regression) and \\(J\\) is a conventional (homoskedastic) Wald statistic for a test of the restrictions. We can write the latter in F statistic form\n\\[\nJ=\\frac{n\\left(\\widehat{\\sigma}_{R}^{2}-\\widehat{\\sigma}^{2}\\right)}{s^{2}}\n\\]\nwhere \\(\\widehat{\\sigma}_{R}^{2}\\) and \\(\\widehat{\\sigma}^{2}\\) are the least squares error variance estimators from the restricted and unrestricted models. The shrinkage weight \\(\\left((q-2) / J_{1}\\right.\\) can be easily calculated from standard regression output."
  },
  {
    "objectID": "chpt28-model-selection.html#group-james-stein",
    "href": "chpt28-model-selection.html#group-james-stein",
    "title": "26  Model Selection, Stein Shrinkage, and Model Averaging",
    "section": "26.31 Group James-Stein",
    "text": "26.31 Group James-Stein\nThe James-Stein estimator can be applied to groups (blocks) of parameters. Suppose we have the parameter vector \\(\\theta=\\left(\\theta_{1}, \\theta_{2}, \\ldots, \\theta_{G}\\right)\\) partitioned into \\(G\\) groups each of dimension \\(K_{g} \\geq 3\\). We have a standard estimator \\(\\widehat{\\theta}=\\left(\\widehat{\\theta}_{1}, \\widehat{\\theta}_{2}, \\ldots, \\widehat{\\theta}_{G}\\right)\\) (for example, least squares regression or MLE) with covariance matrix \\(\\boldsymbol{V}\\). The group James-Stein estimator is\n\\[\n\\begin{aligned}\n\\widetilde{\\theta} &=\\left(\\widetilde{\\theta}_{1}, \\widetilde{\\theta}_{2}, \\ldots, \\widetilde{\\theta}_{G}\\right) \\\\\n\\widetilde{\\theta}_{g} &=\\widehat{\\theta}_{g}\\left(1-\\frac{K_{g}-2}{\\hat{\\theta}_{g}^{\\prime} \\boldsymbol{V}_{g}^{-1} \\widehat{\\theta}_{g}}\\right)_{+}\n\\end{aligned}\n\\]\nwhere \\(\\boldsymbol{V}_{g}\\) is the \\(g^{t h}\\) diagonal block of \\(\\boldsymbol{V}\\). A feasible version of the estimator replaces \\(\\boldsymbol{V}\\) with \\(\\widehat{\\boldsymbol{V}}\\) and \\(\\boldsymbol{V}_{g}\\) with \\(\\widehat{V}_{g}\\).\nThe group James-Stein estimator separately shrinks each block of coefficients. The advantage relative to the classical James-Stein estimator is that this allows the shrinkage weight to vary across blocks. Some parameter blocks can use a large amount of shrinkage while others a minimal amount. Since the positive-part trimming is used the estimator simultaneously performs shrinkage and selection. Blocks with small effects will be shrunk to zero and eliminated. The disadvantage of the estimator is that the benefits of shrinkage may be reduced because the shrinkage dimension is reduced. The trade-off between these factors will depend on how heterogeneous the optimal shrinkage weight varies across the parameters.\nThe groups should be selected based on two criteria. First, they should be selected so that the groups separate variables by expected amount of shrinkage. Thus coefficients which are expected to be “large” relative to their estimation variance should be grouped together and coefficients which are expected to be “small” should be grouped together. This will allow the estimated shrinkage weights to vary according to the group. For example, a researcher may expect high-order coefficients in a polynomial regression to be small relative to their estimation variance. Hence it is appropriate to group the polynomial variables into “low order” and “high order”. Second, the groups should be selected so that the researcher’s loss (utility) is separable across groups of coefficients. This is because the optimality theory (given below) relies on the assumption that the loss is separable. To understand the implications of these recommendations consider a wage regression. Our interpretation of the education and experience coefficients are separable if we use them for separate purposes, such as for estimation of the return to education and the return to experience. In this case it is appropriate to separate the education and experience coefficients into different groups.\nFor an optimality theory we define weighted MSE with respect to the block-diagonal weight matrix \\(\\boldsymbol{W}=\\operatorname{diag}\\left(\\boldsymbol{V}_{1}^{-1}, \\ldots, \\boldsymbol{V}_{G}^{-1}\\right)\\)\nTheorem 28.15 Under the assumptions of Theorem 28.12, if WMSE is defined with respect to \\(\\boldsymbol{W}=\\operatorname{diag}\\left(\\boldsymbol{V}_{1}^{-1}, \\ldots, \\boldsymbol{V}_{G}^{-1}\\right)\\) and \\(K_{g}>2\\) for all \\(g=1, \\ldots, G\\) then\n\\[\n\\operatorname{wmse}[\\widetilde{\\theta}]<\\operatorname{wmse}[\\widehat{\\theta}] \\text {. }\n\\]\nThe proof is a simple extension of the classical James-Stein theory. The block diagonal structure of \\(W\\) means that the WMSE is the sum of the WMSE of each group. The classical James-Stein theory can be applied to each group finding that the WMSE is reduced by shrinkage group-by-group. Thus the total WMSE is reduced by shrinkage."
  },
  {
    "objectID": "chpt28-model-selection.html#empirical-illustrations",
    "href": "chpt28-model-selection.html#empirical-illustrations",
    "title": "26  Model Selection, Stein Shrinkage, and Model Averaging",
    "section": "26.32 Empirical Illustrations",
    "text": "26.32 Empirical Illustrations\nWe illustrate James-Stein shrinkage with three empirical applications.\nThe first application is to the sample used in Section 28.18, the CPS dataset with the subsample of Asian women ( \\(n=1149\\) ) focusing on the return to experience profile. We consider shrinkage of Model 9 ( \\(6^{t h}\\) order polynomial in experience) towards Model 3 ( \\(2^{\\text {nd }}\\) order polynomial in experience). The difference in the number of estimated coefficients is 4 . We set \\(\\widehat{\\boldsymbol{V}}\\) to equal the \\(\\mathrm{HCl}\\) covariance matrix estimator. The empirically-determined shrinkage weight is \\(0.46\\), meaning that the Stein Rule estimator is approximately an equal weighted average of the estimates from the two models. The estimated experience profiles are displayed in Figure 28.4(a).\n\n\nExperience Profile\n\n\n\nFirm Effects\n\nFigure 28.4: Shrinkage Illustrations\nThe two least squares estimates are visually distinct. The \\(6^{t h}\\) order polynomial (Model 9) shows a steep return to experience for the first 10 years, then a wobbly experience profile up to 40 years, and declining above that. It also shows a dip around 25 years. The quadratic specification misses some of these features. The James-Stein estimator is essentially an average of the two profiles. It retains most features of the quartic specification, except that it smooths out the unappealing 25-year dip.\nThe second application is to the Invest1993 data set used in Chapter 17. This is a panel data set of annual observations on investment decisions by corporations. We focus on the firm-specific effects. These are of interest when studying firm heterogeneity and are important for firm-specific forecasting. Accurate estimation of firm effects is challenging when the number of time series observations per firm is small.\nTo keep the analysis focused we restrict attention to firms which are traded on either the NYSE or AMEX and to the last ten years of the sample (1982-1991). Since the regressors are lagged this means that there are at most nine time-series observations per firm. The sample has a total of \\(N=786\\) firms and \\(n=5692\\) observations for estimation. Our baseline model is the two-way fixed effects linear regression as reported in the fourth column of Table 17.2. Our restricted model replaces the firm fixed effects with 19 industry-specific dummy variables. This is similar to the first column of Table \\(17.2\\) except that the trading dummy is omitted and time dummies are added. The Stein Rule estimator thus shrinks the fixed effects model towards the industry effects model. The latter will do well if most of the fixed effects are explained by industry rather than firm-specific variation.\nDue to the large number of estimated coefficients in the unrestricted model we use the homoskedastic weight matrix as a simplification. This allows the calculation of the shrinkage weight using the simple formula (28.28) for the statistic \\(J\\). The heteroskedastic covariance matrix is not appropriate and the cluster-robust covariance matrix will not be reliable due to the sparse dummy specification.\nThe empirically-determined shrinkage weight is \\(0.35\\) which means that the Stein Rule estimator puts about \\(1 / 3\\) weight on the industry-effect specification and \\(2 / 3\\) weight on the firm-specific specification.\nTo report our results we focus on the distribution of the firm-specific effects. For the fixed effects model these are the estimated fixed effects. For the industry-effect model these are the estimated industry dummy coefficients (for each firm). For the Stein Rule estimates they are a weighted average of the two. We estimate \\({ }^{6}\\) the densities of the estimated firm-specific effects from the fixed-effects and Stein Rule estimators, and plot them in Figure 28.4(b).\nYou can see that the fixed-effects estimate of the firm-specific density is more dispersed while the Stein estimator is sharper and more peaked indicating that the fixed effects estimator attributes more variation in firm-specific factors than the Stein estimator. The Stein estimator pulls the fixed effects towards their common mean, adjusting for the randomness due to their estimation. Our expectation is that the Stein estimates, if used for an application such as firm-specific forecasting, will be more accurate because they will have reduced variance relative to the fixed effects estimates.\nThe third application uses the CPS dataset with the subsample of Black men ( \\(n=2413)\\) focusing on the return to education across U.S. regions (Northeast, Midwest, South, West). Suppose you are asked to flexibly estimate the return to education for Black men allowing for the return to education to vary across the regions. Given the model selection information from Section \\(28.18\\) a natural baseline is model 6 augmented to allow for greater variation across regions. A flexible specification interacts the six education dummy variables with the four regional dummies (omitting the intercept), which adds 18 coefficients and allows the return to education to vary without restriction in each region.\nThe least squares estimate of the return to education by region is displayed in Figure 28.5(a). For simplicity we combine the omitted education group (less than 12 years education) as “11 years”. The estimates appear noisy due to the small samples. One feature which we can see is that the four lines track one another for years of education between 12 and 18. That is, they are roughly linear in years of education with the same slope but different intercepts.\nTo improve the precision of the estimates we shrink the four profiles towards Model 6 . This means that we are shrinking the profiles not towards each other but towards the model with the same effect of education but regional-specific intercepts. Again we use the HCl covariance matrix estimate. The number of restrictions is 18 . The empirically-determined shrinkage weight is \\(0.49\\) which means that the Stein Rule estimator puts equal weight on the two models.\nThe Stein Rule estimates are displayed in Figure 28.5(b). The estimates are less noisy than panel (a) and it is easier to see the patterns. The four lines track each other and are approximately linear over 1218. For 20 years of education the four lines disperse which seems likely due to small samples. In panel (b) it is easier to see the patterns across regions. It appears that the northeast region has the highest wages (conditional on education) while the west region has the lowest wages. This ranking is constant for nearly all levels of education.\nWhile the Stein Rule estimates shrink the nonparametric estimates towards the common-educationfactor specification it does not impose the latter specification. The Stein Rule estimator has the ability to\n\\({ }^{6}\\) The two densities are estimated with a common bandwidth to aid comparison. The bandwidth was selected to compromise between those selected for the two samples.\n\n\nLeast Squares Estimates\n\n\n\nStein Rule Estimates\n\nFigure 28.5: Stein Rule Estimation of Education Profiles Across Regions\nput near zero weight on the common-factor model. The fact that the estimates put \\(1 / 2\\) weight on both models is the choice selected by the Stein Rule and is data-driven.\nThe message from these three applications is that the James-Stein shrinkage approach can be constructively used to reduce estimation variance in economic applications. These applications illustrate common forms of potential applications: Shrinkage of a flexible specification towards a simpler specification; Shrinkage of heterogeneous estimates towards homogeneous estimates; Shrinkage of fixed effects towards group dummy estimates. These three applications also employed moderately large sample sizes ( \\(n=1149,2413\\), and 5692 ) yet found shrinkage weights near \\(50 %\\). This shows that the benefits of Stein shrinkage are not confined to “small” samples but rather can be constructive used in moderately large samples with complicated structures."
  },
  {
    "objectID": "chpt28-model-selection.html#model-averaging",
    "href": "chpt28-model-selection.html#model-averaging",
    "title": "26  Model Selection, Stein Shrinkage, and Model Averaging",
    "section": "26.33 Model Averaging",
    "text": "26.33 Model Averaging\nRecall that the problem of model selection is how to select a single model from a general set of models. The James-Stein shrinkage estimator smooths between two nested models by taking a weighted average of two estimators. More generally we can take an average of an arbitrary number of estimators. These estimators are known as model averaging estimators. The key issue for estimation is how to select the averaging weights.\nSuppose we have a set of \\(M\\) models \\(\\overline{\\mathscr{M}}=\\left\\{\\mathcal{M}_{1}, \\ldots, \\mathcal{M}_{M}\\right\\}\\). For each model there is an estimator \\(\\widehat{\\theta}_{m}\\) of the parameter \\(\\theta\\). The natural way to think about multiple models, parameters, and estimators is the same as for model selection. All models are subsets of a general superset (overlapping) model which contains all submodels as special cases.\nCorresponding to the set of models we introduce a set of weights \\(w=\\left\\{w_{1}, \\ldots, w_{M}\\right\\}\\). It is common to restrict the weights to be non-negative and sum to one. The set of such weights is called the \\(\\mathbb{R}^{M}\\) probability simplex. Definition 28.4 Probability Simplex. The set \\(\\mathscr{S} \\subset \\mathbb{R}^{M}\\) of vectors such that \\(\\sum_{m=1}^{M} w_{m}=1\\) and \\(w_{i} \\geq 1\\) for \\(i=1, \\ldots, M\\).\nThe probability simplex in \\(\\mathbb{R}^{2}\\) and \\(\\mathbb{R}^{3}\\) is shown in the two panels of Figure \\(28.6\\). The simplex in \\(\\mathbb{R}^{2}\\) (panel (a)) is the line between the vertices \\((1,0)\\) and \\((0,1)\\). An example element is the point \\((.7, .3)\\) indicated by the point \\(w\\). This is the weight vector which puts weight \\(0.7\\) on model 1 and weight \\(0.3\\) on model 2 . The vertice \\((1,0)\\) is the weight vector which puts all weight on model 1, corresponding to model selection, and similarly the vertice \\((0,1)\\) is the weight vector which puts all weight on model 2 .\nThe simplex in \\(\\mathbb{R}^{3}\\) (panel (b)) is the equilateral triangle formed between \\((1,0,0),(0,1,0)\\), and \\((0,0,1)\\). An example element is the point \\((.1, .5, .4)\\) indicated by the pont \\(w\\). The edges are weight vectors which are averages between two of the three models. For example the bottom edge are weight vectors which divide the weight between models 1 and 2, placing no weight on model 3. The vertices are weight vectors which put all weight on one of the three models and correspond to model selection.\\\n\nFigure 28.6: Probability Simplex in \\(\\mathbb{R}^{2}\\) and \\(\\mathbb{R}^{3}\\)\nSince the weights on the probability simplex sum to one, an alternative representation is to eliminate one weight by substitution. Thus we can set \\(w_{M}=1-\\sum_{m=1}^{M-1} w_{m}\\) and define the set of vectors \\(w=\\left\\{w_{1}, \\ldots, w_{M-1}\\right\\}\\) which lie in the \\(\\mathbb{R}^{M-1}\\) unit simplex, which is the region bracketed by the probability simplex and the origin.\nGiven a weight vector we define the averaging estimator\n\\[\n\\widehat{\\theta}(w)=\\sum_{m=1}^{M} w_{m} \\widehat{\\theta}_{m} .\n\\]\nSelection estimators emerge as the special case where the weight vector \\(w\\) is a unit vector, e.g. the vertices in Figure 28.6.\nIt is not absolutely necessary to restrict the weight vector of an averaging estimator to lie in the probability simplex \\(\\mathscr{S}\\), but in most cases it is a sensible restriction which improves performance. The unadjusted James-Stein estimator, for example, is an averaging estimator which does not enforce nonnegativity of the weights. The positive-part version, however, imposes non-negativity and achieves reduced MSE as a result.\nIn Section \\(28.19\\) and Theorem \\(28.11\\) we explored the MSE of a simple shrinkage estimator which shrinks an unrestricted estimator towards the zero vector. This is the same as a model averaging estimator where one of the two estimators is the zero vector. In Theorem \\(28.11\\) we showed that the MSE of the optimal shrinkage (model averaging) estimator is less than the unrestricted estimator. This result extends to the case of averaging between an arbitrary number of estimators. The MSE of the optimal averaging estimator is less than the MSE of the estimator of the full model in any given sample.\nThe optimal averaging weights, however, are unknown. A number of methods have been proposed for selection of the averaging weights.\nOne simple method is equal weighting. This is achieved by setting \\(w_{m}=1 / M\\) and results in the estimator\n\\[\n\\widehat{\\theta}^{*}=\\frac{1}{M} \\sum_{m=1}^{M} \\widehat{\\theta}_{m} .\n\\]\nThe advantages of equal weighting are that it is simple, easy to motivate, and no randomness is introduced by estimation of the weights. The variance of the equal weighting estimator can be calculated because the weights are fixed. Another important advantage is that the estimator can be constructed in contexts where it is unknown how to construct empirical-based weights, for example when averaging models from completely different probability families. The disadvantages of equal weighting are that the method can be sensitive to the set of models considered, there is no guarantee that the estimator will perform better than the unrestricted estimator, and sample information is inefficiently used. In practice, equal weighting is best used in contexts where the set of models have been pre-screened so that all are considered “reasonable” models. From the standpoint of econometric methodology equal weighting is not a proper statistical method as it is an incomplete methodology.\nDespite these concerns equal weighting can be constructively employed when summarizing information for a non-technical audience. The relevant context is when you have a small number of reasonable but distinct estimates typically made using different assumptions. The distinct estimates are presented to illustrate the range of possible results and the average taken to represent the “consensus” or “recommended” estimate.\nAs mentioned above, a number of methods have been proposed for selection of the averaging weights. In the following sections we outline four popular methods: Smoothed BIC, Smoothed AIC, Mallows averaging, and Jackknife averaging."
  },
  {
    "objectID": "chpt28-model-selection.html#smoothed-bic-and-aic",
    "href": "chpt28-model-selection.html#smoothed-bic-and-aic",
    "title": "26  Model Selection, Stein Shrinkage, and Model Averaging",
    "section": "26.34 Smoothed BIC and AIC",
    "text": "26.34 Smoothed BIC and AIC\nRecall that Schwarz’s Theorem \\(28.1\\) states that for a probability model \\(f(y, \\theta)\\) and a diffuse prior the marginal likelihood \\(p(Y)\\) satisfies\n\\[\n-2 \\log p(Y) \\simeq-2 \\ell_{n}(\\widehat{\\theta})+K \\log (n)=\\mathrm{BIC} .\n\\]\nThis has been been interpreted to mean that the model with the highest value of the right-hand-side approximately has the highest marginal likelihood and is thus the model with the highest probability of being the true model.\nThere is another interpretation of Schwarz’s result. The marginal likelihood is approximately proportional to the probability that the model is true, conditional on the data. Schwarz’s Theorem implies that this is approximately\n\\[\np(Y) \\simeq \\exp (-\\mathrm{BIC} / 2)\n\\]\nwhich is a simple exponential transformation of the BIC. Weighting by posterior probability can be achieved by setting model weights proportional to this transformation. These are known as BIC weights and produce the smoothed BIC estimator.\nTo describe the method completely, we have a set of models \\(\\overline{\\mathscr{M}}=\\left\\{\\mathscr{M}_{1}, \\ldots, \\mathscr{M}_{M}\\right\\}\\). Each model \\(f_{m}\\left(y, \\theta_{m}\\right)\\) depends on a \\(K_{m} \\times 1\\) parameter vector \\(\\theta_{m}\\) which is estimated by the maximum likelihood. The maximized likelihood is \\(L_{m}\\left(\\widehat{\\theta}_{m}\\right)=f_{m}\\left(Y, \\widehat{\\theta}_{m}\\right)\\). The BIC for model \\(m\\) is \\(\\operatorname{BIC}_{m}=-2 \\log L_{m}\\left(\\widehat{\\theta}_{m}\\right)+K_{m} \\log (n)\\).\nThe \\(\\mathrm{BIC}\\) weights are\n\\[\nw_{m}=\\frac{\\exp \\left(-\\mathrm{BIC}_{m} / 2\\right)}{\\sum_{j=1}^{M} \\exp \\left(-\\mathrm{BIC}_{j} / 2\\right)} .\n\\]\nSome properties of the BIC weights are as follows. They are non-negative so all models receive positive weight. Some models can receive weight arbitrarily close to zero and in practice many estimated models may receive BIC weight that is essentially zero. The model which is selected by BIC receives the greatest weight and models which have BIC values close to the minimum receive weights closest to the largest weight. Models whose \\(\\mathrm{BIC}\\) is not close to the minimum receive weight near zero.\nThe Smoothed BIC (SBIC) estimator is\n\\[\n\\widehat{\\theta}_{\\text {sbic }}=\\sum_{m=1}^{M} w_{m} \\widehat{\\theta}_{m} .\n\\]\nThe SBIC estimator is a smoother function of the data than BIC selection as there are no discontinuous jumps across models.\nAn advantage of the smoothed BIC weights and estimator is that it can be used to combine models from different probability families. As for the BIC it is important that all models are estimated on the same sample. It is also important that the full formula is used for the BIC (no omission of constants) when combining models from different probability families.\nComputationally it is better to implement smoothed BIC with what are called “BIC differences” rather than the actual values of the BIC, as the formula as written can produce numerical overflow problems. The difficulty is due to the exponentiation in the formula. This problem can be eliminated as follows. Let\n\\[\n\\mathrm{BIC}^{*}=\\min _{1 \\leq m \\leq M} \\mathrm{BIC}_{m}\n\\]\ndenote the lowest BIC among the models and define the BIC differences\n\\[\n\\Delta \\mathrm{BIC}_{m}=\\mathrm{BIC}_{m}-\\mathrm{BIC}^{*} .\n\\]\nThen\n\\[\n\\begin{aligned}\nw_{m} &=\\frac{\\exp \\left(-\\mathrm{BIC}_{m} / 2\\right)}{\\sum_{j=1}^{M} \\exp \\left(-\\mathrm{BIC}_{j} / 2\\right)} \\\\\n&=\\frac{\\exp \\left(-\\mathrm{BIC}_{m} / 2\\right) \\exp \\left(\\mathrm{BIC}^{*} / 2\\right)}{\\sum_{j=1}^{M} \\exp \\left(-\\mathrm{BIC}_{j} / 2\\right) \\exp \\left(\\mathrm{BIC}^{*} / 2\\right)} \\\\\n&=\\frac{\\exp \\left(-\\Delta \\mathrm{BIC}_{m} / 2\\right)}{\\sum_{j=1}^{M} \\exp \\left(-\\Delta \\mathrm{BIC}_{j} / 2\\right)} .\n\\end{aligned}\n\\]\nThus the weights are algebraically identically whether computed on \\(\\mathrm{BIC}_{m}\\) or \\(\\Delta \\mathrm{BIC}_{m}\\). Since \\(\\Delta \\mathrm{BIC}_{m}\\) are of smaller magnitude than \\(\\mathrm{BIC}_{m}\\) overflow problems are less likely to occur.\nBecause of the properties of the exponential, if \\(\\Delta \\mathrm{BIC}_{m} \\geq 10\\) then \\(w_{m} \\leq 0.01\\). Thus smoothed BIC typically concentrates weight on models whose BIC values are close to the minimum. This means that in practice smoothed BIC puts effective non-zero weight on a small number of models. Burnham and Anderson (1998) follow a suggestion they credit to Akaike that if we make the same transformation to the AIC as to the BIC to obtain the smoothed BIC weights we obtain frequentist approximate probabilities for the models. Specifically they propose the AIC weights\n\\[\nw_{m}=\\frac{\\exp \\left(-\\mathrm{AIC}_{m} / 2\\right)}{\\sum_{j=1}^{M} \\exp \\left(-\\mathrm{AIC}_{j} / 2\\right)} .\n\\]\nThey do not provide a strong theoretical justification for this specific choice of transformation but it seems natural given the smoothed BIC formula and works well in simulations.\nThe algebraic properties of the AIC weights are similar to those of the BIC weights. All models receive positive weight though some receive weight which is arbitrarily close to zero. The model with the smallest AIC receives the greatest AIC weight, and models with similar AIC values receive similar AIC weights.\nComputationally the AIC weights should be computed using AIC differences. Define\n\\[\n\\begin{aligned}\n\\mathrm{AIC}^{*} &=\\min _{1 \\leq m \\leq M} \\mathrm{AIC}_{m} \\\\\n\\Delta \\mathrm{AIC}_{m} &=\\mathrm{AIC}_{m}-\\mathrm{AIC}^{*} .\n\\end{aligned}\n\\]\nThe AIC weights algebraically equal\n\\[\nw_{m}=\\frac{\\exp \\left(-\\Delta \\mathrm{AIC}_{m} \\mathrm{AIC}_{m} / 2\\right)}{\\sum_{j=1}^{M} \\exp \\left(-\\Delta \\mathrm{AIC}_{j} / 2\\right)} .\n\\]\nAs for the BIC weights \\(w_{m} \\leq 0.01\\) if \\(\\Delta \\mathrm{AIC}_{m} \\geq 10\\) so the AIC weights will concentrated on models whose AIC values are close to the minimum. However, in practice it is common that the AIC criterion is less concentrated than the BIC criterion as the AIC puts a smaller penalty on large penalizations. The AIC weights tend to be more spread out across models than the corresponding BIC weights.\nThe Smoothed AIC (SAIC) estimator is\n\\[\n\\widehat{\\theta}_{\\text {saic }}=\\sum_{m=1}^{M} w_{m} \\widehat{\\theta}_{m} .\n\\]\nThe SAIC estimator is a smoother function of the data than AIC selection.\nRecall that both AIC selection and BIC selection are model selection consistent in the sense that as the sample size gets large the probability that the selected model is a true model is arbtrarily close to one. Furthermore, BIC is consistent for parsimonious models and AIC asymptotically over-selects.\nThese properties extend to SBIC and SAIC. In large samples SAIC and SBIC weights will concentrate exclusively on true models; the weight on incorrect models will asymptotically approach zero. However, SAIC will asymptotically spread weight across both parsimonious true models and overparameterized true models, while SBIC asymptotically concentrates weight only on parsimonious true models.\nAn interesting property of the smoothed estimators is the possibility of asymptotically spreading weight across equal-fitting parsimonious models. Suppose we have two non-nested models with the same number of parameters and the same KLIC value so they are equal approximations. In large samples both SBIC and SAIC will be weighted averages of the two estimators rather than simply selecting one of the two."
  },
  {
    "objectID": "chpt28-model-selection.html#mallows-model-averaging",
    "href": "chpt28-model-selection.html#mallows-model-averaging",
    "title": "26  Model Selection, Stein Shrinkage, and Model Averaging",
    "section": "26.35 Mallows Model Averaging",
    "text": "26.35 Mallows Model Averaging\nIn linear regression the Mallows criterion (28.14) applies directly to the model averaging estimator (28.29). The homoskedastic regression model is\n\\[\n\\begin{aligned}\nY &=m+e \\\\\nm &=m(X) \\\\\n\\mathbb{E}[e \\mid X] &=0 \\\\\n\\mathbb{E}\\left[e^{2} \\mid X\\right] &=\\sigma^{2} .\n\\end{aligned}\n\\]\nSuppose that there are \\(M\\) models for \\(m(X)\\), each which takes the form \\(\\beta_{m}^{\\prime} X_{m}\\) for some \\(K_{m} \\times 1\\) regression vector \\(X_{m}\\). The \\(m^{t h}\\) model estimator of the coefficient is \\(\\widehat{\\beta}_{m}=\\left(\\boldsymbol{X}_{m}^{\\prime} \\boldsymbol{X}_{m}\\right)^{-1} \\boldsymbol{X}_{m}^{\\prime} \\boldsymbol{Y}\\), and the estimator of the vector \\(\\boldsymbol{m}\\) is \\(\\widehat{\\boldsymbol{m}}_{m}=\\boldsymbol{P}_{m} \\boldsymbol{Y}\\) where \\(\\boldsymbol{P}_{m}=\\boldsymbol{X}_{m}\\left(\\boldsymbol{X}_{m}^{\\prime} \\boldsymbol{X}_{m}\\right)^{-1} \\boldsymbol{X}_{m}^{\\prime}\\). The corresponding residual vector is \\(\\widehat{\\boldsymbol{e}}_{m}=\\) \\(\\left(\\boldsymbol{I}_{n}-\\boldsymbol{P}_{m}\\right) \\boldsymbol{Y}\\).\nThe model averaging estimator for fixed weights is\n\\[\n\\widehat{\\boldsymbol{m}}_{m}(w)=\\sum_{m=1}^{M} w_{m} \\boldsymbol{P}_{m} \\boldsymbol{Y}=\\boldsymbol{P}(w) \\boldsymbol{Y}\n\\]\nwhere\n\\[\n\\boldsymbol{P}(w)=\\sum_{m=1}^{M} w_{m} \\boldsymbol{P}_{m} .\n\\]\nThe model averaging residual is\n\\[\n\\widehat{\\boldsymbol{e}}(w)=\\left(\\boldsymbol{I}_{n}-\\boldsymbol{P}(w)\\right) \\boldsymbol{Y}=\\sum_{m=1}^{M} w_{m}\\left(\\boldsymbol{I}_{n}-\\boldsymbol{P}_{m}\\right) \\boldsymbol{Y} .\n\\]\nThe estimator \\(\\widehat{\\boldsymbol{m}}_{m}(w)\\) is linear in \\(\\boldsymbol{Y}\\) so the Mallows criterion can be applied. It equals\n\\[\n\\begin{aligned}\nC(w) &=\\widehat{\\boldsymbol{e}}(w)^{\\prime} \\widehat{\\boldsymbol{e}}(w)+2 \\widetilde{\\sigma}^{2} \\operatorname{tr}(\\boldsymbol{P}(w)) \\\\\n&=\\widehat{\\boldsymbol{e}}(w)^{\\prime} \\widehat{\\boldsymbol{e}}(w)+2 \\widetilde{\\sigma}^{2} \\sum_{m=1}^{M} w_{m} K_{m}\n\\end{aligned}\n\\]\nwhere \\(\\widetilde{\\sigma}^{2}\\) is a preliminary \\({ }^{7}\\) estimator of \\(\\sigma^{2}\\).\nIn the case of model selection the Mallows penalty is proportional to the number of estimated coefficients. In the model averaging case the Mallows penalty is the average number of estimated coefficients.\nThe Mallows-selected weight vector is that which minimizes the Mallows criterion. It equals\n\\[\n\\widehat{w}_{\\mathrm{mma}}=\\underset{w \\in \\mathscr{S}}{\\operatorname{argmin}} C(w) .\n\\]\nComputationally it is useful to observe that \\(C(w)\\) is a quadratric function in \\(w\\). Indeed, by defining the \\(n \\times M\\) matrix \\(\\widehat{\\boldsymbol{E}}=\\left[\\widehat{\\boldsymbol{e}}_{1}, \\ldots, \\widehat{\\boldsymbol{e}}_{M}\\right]\\) of residual vectors and the \\(M \\times 1\\) vector \\(\\boldsymbol{K}=\\left[K_{1}, \\ldots, K_{M}\\right]\\) the criterion is\n\\[\nC(w)=w^{\\prime} \\widehat{\\boldsymbol{E}}^{\\prime} \\widehat{\\boldsymbol{E}} w+2 \\widetilde{\\sigma}^{2} \\boldsymbol{K}^{\\prime} w .\n\\]\nThe probability simplex \\(\\mathscr{S}\\) is defined by one equality and \\(2 M\\) inequality constraints. The minimization problem (28.30) falls in the category of quadratic programming which means optimization of a\n\\({ }^{7}\\) It is typical to use the bias-corrected least squares variance estimator from the largest model. quadratic subject to linear equality and inequality constraints. This is a well-studied area of numerical optimization and numerical solutions are widely available. In R use the command solve.QP in the package quadprog. In MATLAB use the command quadprog.\nFigure \\(28.7\\) illustrates the Mallows weight computation problem. Displayed is the probability simplex \\(\\mathscr{S}\\) in \\(\\mathbb{R}^{3}\\). The axes are the weight vectors. The ellipses are the contours of the unconstrained sum of squared errors as a function of the weight vectors projected onto the constrained set \\(\\sum_{m=1}^{M} w_{m}=1\\). This is the extension of the probability simplex as a two-dimensional plane in \\(\\mathbb{R}^{3}\\). The midpoint of the contours is the minimizing weight vector allowing for weights outside \\([0,1]\\). The point where the lowest contour ellipse hits the probability simplex is the solution (28.30), the Mallows selected weight vector. In the left panel is displayed an example where the solution is the vertex \\((0,1,0)\\) so the selected weight vector puts all weight on model 2. In the right panel is displayed an example where the solution lies on the edge between \\((1,0,0)\\) and \\((0,0,1)\\), meaning that the selected weight vector averages models 1 and 3 but puts no weight on model 2. Since the contour sets are ellipses and the constraint set is a simplex, solution points tend to be on edges and vertices meaning that some models receive zero weight. In fact, where there are a large number of models a generic feature of the solution is that most models receive zero weight; the selected weight vector puts positive weight on a small subset of the eligible models.\\\n\nFigure 28.7: Mallows Weight Selection\nOnce the weights \\(\\widehat{w}\\) are obtained the model averaging estimator of the coefficients are found by averaging the model estimates \\(\\widehat{\\beta}_{m}\\) using the weights.\nIn the special case of two nested models the Mallows criterion can be written as\n\\[\n\\begin{aligned}\nC(w) &=(w, 1-w)\\left(\\begin{array}{cc}\n\\widehat{\\boldsymbol{e}}_{1}^{\\prime} \\widehat{\\boldsymbol{e}}_{1} & \\widehat{\\boldsymbol{e}}_{1}^{\\prime} \\widehat{\\boldsymbol{e}}_{2} \\\\\n\\widehat{\\boldsymbol{e}}_{2}^{\\prime} \\widehat{\\boldsymbol{e}}_{1} & \\widehat{\\boldsymbol{e}}_{2}^{\\prime} \\widehat{\\boldsymbol{e}}_{2}\n\\end{array}\\right)\\left(\\begin{array}{c}\nw \\\\\n1-w\n\\end{array}\\right)+2 \\widetilde{\\sigma}^{2}\\left(w K_{1}+(1-w) K_{2}\\right) \\\\\n&=(w, 1-w)\\left(\\begin{array}{ll}\n\\widehat{\\boldsymbol{e}}_{1}^{\\prime} \\widehat{\\boldsymbol{e}}_{1} & \\widehat{\\boldsymbol{e}}_{2}^{\\prime} \\widehat{\\boldsymbol{e}}_{2} \\\\\n\\widehat{\\boldsymbol{e}}_{2}^{\\prime} \\widehat{\\boldsymbol{e}}_{2} & \\widehat{\\boldsymbol{e}}_{2}^{\\prime} \\widehat{\\boldsymbol{e}}_{2}\n\\end{array}\\right)\\left(\\begin{array}{c}\n1-w \\\\\nw\n\\end{array}\\right)+2 \\widetilde{\\sigma}^{2}\\left(w K_{1}+(1-w) K_{2}\\right) \\\\\n&=w^{2}\\left(\\widehat{\\boldsymbol{e}}_{1}^{\\prime} \\widehat{\\boldsymbol{e}}_{1}-\\widehat{\\boldsymbol{e}}_{2}^{\\prime} \\widehat{\\boldsymbol{e}}_{2}\\right)+\\widehat{\\boldsymbol{e}}_{2}^{\\prime} \\widehat{\\boldsymbol{e}}_{2}-2 \\widetilde{\\sigma}^{2}\\left(K_{2}-K_{1}\\right) w+2 \\widetilde{\\sigma}^{2}\n\\end{aligned}\n\\]\nwhere we assume \\(K_{1}<K_{2}\\) so that \\(\\widehat{\\boldsymbol{e}}_{1}^{\\prime} \\widehat{\\boldsymbol{e}}_{2}=\\boldsymbol{Y}^{\\prime}\\left(\\boldsymbol{I}_{n}-\\boldsymbol{P}_{1}\\right)\\left(\\boldsymbol{I}_{n}-\\boldsymbol{P}_{2}\\right) \\boldsymbol{Y}=\\boldsymbol{Y}^{\\prime}\\left(\\boldsymbol{I}_{n}-\\boldsymbol{P}_{2}\\right) \\boldsymbol{Y}=\\widehat{\\boldsymbol{e}}_{2}^{\\prime} \\widehat{\\boldsymbol{e}}_{2}\\). The minimizer of this criterion is\n\\[\n\\widehat{w}=\\left(\\frac{\\widetilde{\\sigma}^{2}\\left(K_{2}-K_{1}\\right)}{\\widehat{\\boldsymbol{e}}_{1}^{\\prime} \\widehat{\\boldsymbol{e}}_{1}-\\widehat{\\boldsymbol{e}}_{2}^{\\prime} \\widehat{\\boldsymbol{e}}_{2}}\\right)_{1} .\n\\]\nThis is the same as the Stein Rule weight (28.27) with a slightly different shrinkage constant. Thus the Mallows averaging estimator for \\(M=2\\) is a Stein Rule estimator. Hence for \\(M>2\\) the Mallows averaging estimator is a generalization of the James-Stein estimator to multiple models.\nBased on the latter observation, B. E. Hansen (2014) shows that the MMA estimator has lower WMSE than the unrestricted least squares estimator when the models are nested linear regressions, the errors are homoskedastic, and the models are separated by 4 coefficients or greater. The latter condition is analogous to the conditions for improvements in the Stein Rule theory.\nB. E. Hansen (2007) showed that the MMA estimator asymptotically achieves the same MSE as the infeasible optimal best weighted average using the theory of Li (1987) under similar conditions. This shows that using model selection tools to select the averaging weights is asymptotically optimal for regression fitting and point forecasting."
  },
  {
    "objectID": "chpt28-model-selection.html#jackknife-cv-model-averaging",
    "href": "chpt28-model-selection.html#jackknife-cv-model-averaging",
    "title": "26  Model Selection, Stein Shrinkage, and Model Averaging",
    "section": "26.36 Jackknife (CV) Model Averaging",
    "text": "26.36 Jackknife (CV) Model Averaging\nA disadvantage of Mallows selection is that the criterion is valid only when the errors are conditionally homoskedastic. In constrast, selection by cross-validation does not require homoskedasticity. Therefore it seems sensible to use cross-validation rather than Mallows to select the weight vectors. It turns out that this is a simple extension with excellent finite sample performance. In the Machine Learning literature this method is called stacking.\nA fitted averaging regression (with fixed weights) can be written as\n\\[\nY_{i}=\\sum_{m=1}^{M} w_{m} X_{m i}^{\\prime} \\widehat{\\beta}_{m}+\\widehat{e}_{i}(w)\n\\]\nwhere \\(\\widehat{\\beta}_{m}\\) are the least squares coefficient estimates from Model \\(m\\). The corresponding leave-one-out equation is\n\\[\nY_{i}=\\sum_{m=1}^{M} w_{m} X_{m i}^{\\prime} \\widehat{\\beta}_{m,(-i)}+\\widetilde{e}_{i}(w)\n\\]\nwhere \\(\\widehat{\\beta}_{m,(-i)}\\) are the least squares coefficient estimates from Model \\(m\\) when observation \\(i\\) is deleted. The leave-one-out prediction errors satisfy the simple relationship\n\\[\n\\widetilde{e}_{i}(w)=\\sum_{m=1}^{M} w_{m} \\widetilde{e}_{m i}\n\\]\nwhere \\(\\widetilde{e}_{m i}\\) are the leave-one-out prediction errors for model \\(m\\). In matrix notation \\(\\widetilde{\\boldsymbol{e}}(w)=\\widetilde{\\boldsymbol{E}} w\\) where \\(\\widetilde{\\boldsymbol{E}}\\) is the \\(n \\times M\\) matrix of leave-one-out prediction errors.\nThis means that the jackknife estimate of variance (or equivalently the cross-validation criterion) equals\n\\[\n\\mathrm{CV}(w)=w^{\\prime} \\widetilde{\\boldsymbol{E}}^{\\prime} \\widetilde{\\boldsymbol{E}} w\n\\]\nwhich is a quadratic function of the weight vector. The cross-validation choice for weight vector is the minimizer\n\\[\n\\widehat{w}_{\\mathrm{jma}}=\\underset{w \\in \\mathscr{S}}{\\operatorname{argmin}} \\mathrm{CV}(w) .\n\\]\nGiven the weights the coefficient estimates (and any other parameter of interest) are found by taking weighted averages of the model estimates using the weight vector \\(\\widehat{w}_{\\text {jma. }}\\). B. E. Hansen and Racine (2012) call this the Jackknife Model Averaging (JMA) estimator.\nThe algebraic properties of the solution are similar to Mallows. Since (28.31) minimizes a quadratic function subject to a simplex constraint, solutions tend to be on edges and vertices which means that many (or most) models receive zero weight. Hence JMA weight selection simultaneously performs selection and shrinkage. The solution is found numerically by quadratic programming which is computationally simple and fast even when the number of models \\(M\\) is large.\nB. E. Hansen and Racine (2012) showed that the JMA estimator is asymptotically equivalent to the infeasible optimal weighted average across least squares estimates based on a regression fit criteria. Their results hold under quite mild conditions including conditional heteroskedasticity. This result is similar to Andrews (1991c) generalization of Li (1987)’s result for model selection.\nThe implication of this theory is that JMA weight selection is computationally simple and has excellent sampling performance."
  },
  {
    "objectID": "chpt28-model-selection.html#granger-ramanathan-averaging",
    "href": "chpt28-model-selection.html#granger-ramanathan-averaging",
    "title": "26  Model Selection, Stein Shrinkage, and Model Averaging",
    "section": "26.37 Granger-Ramanathan Averaging",
    "text": "26.37 Granger-Ramanathan Averaging\nA method similar to JMA based on hold-out samples was proposed for forecast combination by Granger and Ramanathan (1984), and has emerged as a popular method in the modern machine learning literature.\nRandomly split the sample into two parts: an estimation an an evaluation sample. Using the estimation sample, estimate the \\(M\\) regression models, obtaining the coefficients \\(\\widehat{\\beta}_{m}\\). Using these coefficients and the evaluation sample construct the fitted values \\(\\widetilde{Y}_{m i}=X_{m i}^{\\prime} \\widehat{\\beta}_{m}\\) for the \\(M\\) models. Then estimate the model weights by a least squares regression of \\(Y_{i}\\) on \\(\\widetilde{Y}_{m i}\\) and no intercept using the evaluation sample. This regression is\n\\[\nY_{i}=\\sum_{m=1}^{M} \\widehat{w}_{m} \\widetilde{Y}_{m i}+\\widehat{e}_{i} .\n\\]\nThe least squares coefficients \\(\\widehat{w}_{m}\\) are the Granger-Ramanathan weights.\nBased on an informal argument Granger and Ramanathan (1984) recommended an unconstrained least squares regression to obtain the weights but this is not advised as this produces extremely erratic empirical weights, especially when \\(M\\) is large. Instead, it is recommended to use constrained regression, imposing the constraints \\(\\widehat{w}_{m} \\geq 0\\) and \\(\\sum_{m=1}^{M} \\widehat{w}_{m}=1\\). To impose the non-negativity constraints it is best to use quadratic programming.\nThis Granger-Ramanathan approach is best suited for applications with a very large sample size where the efficiency loss from the hold-out sample split is not a concern."
  },
  {
    "objectID": "chpt28-model-selection.html#empirical-illustration-1",
    "href": "chpt28-model-selection.html#empirical-illustration-1",
    "title": "26  Model Selection, Stein Shrinkage, and Model Averaging",
    "section": "26.38 Empirical Illustration",
    "text": "26.38 Empirical Illustration\nWe illustrate the model averaging methods with the empirical application from Section 28.18, which reported wage regression estimates for the CPS sub-sample of Asian women focusing on the return to experience between 0 and 30 years.\nTable \\(28.2\\) reports the model averaging weights obtained using the methods of SBIC, SAIC, Mallows model averaging (MMA), and jackknife model averaging (JMA). Also reported in the final column is the weighted average estimate of the return to experience as a percentage.\nThe results show that the methods put weight on somewhat different models. The SBIC puts nearly all weight on model 2 . The SAIC puts nearly \\(1 / 2\\) of the weight on model 6 with most of the remainder split between models 5 and 9. MMA puts nearly \\(1 / 2\\) of the weight on model \\(9,30 %\\) on 5 , and \\(9 %\\) on model 1. JMA is similar to MMA but more emphasis on parsimony, with \\(1 / 2\\) of the weight on model 5 , \\(17 %\\) on model \\(9,17 %\\) on model 1 , and \\(8 %\\) on model 3 . One of the interesting things about the MMA/JMA methods is that they can split weight between quite different models, e.g. models 1 and 9.\nThe averaging estimators from the non-BIC methods are similar to one another but SBIC produces a much smaller estimate than the other methods.\nTable 28.2: Model Averaging Weights and Estimates of Return to Experience among Asian Women\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nModel 1\nModel 2\nModel 3\nModel 4\nModel 5\nModel 6\nModel 7\nModel 8\nModel 9\nReturn\n\n\n\n\nSBIC\n\\(.02\\)\n\\(.96\\)\n\\(.00\\)\n\\(.00\\)\n\\(.04\\)\n\\(.00\\)\n\\(.00\\)\n\\(.00\\)\n\\(.00\\)\n\\(22 %\\)\n\n\nSAIC\n\\(.00\\)\n\\(.02\\)\n\\(.10\\)\n\\(.00\\)\n\\(.15\\)\n\\(.44\\)\n\\(.00\\)\n\\(.06\\)\n\\(.22\\)\n\\(38 %\\)\n\n\nMMA\n\\(.09\\)\n\\(.02\\)\n\\(.02\\)\n\\(.00\\)\n\\(.30\\)\n\\(.00\\)\n\\(.00\\)\n\\(.00\\)\n\\(.57\\)\n\\(39 %\\)\n\n\nJMA\n\\(.17\\)\n\\(.00\\)\n\\(.08\\)\n\\(.00\\)\n\\(.57\\)\n\\(.01\\)\n\\(.00\\)\n\\(.00\\)\n\\(.17\\)\n\\(34 %\\)"
  },
  {
    "objectID": "chpt28-model-selection.html#technical-proofs",
    "href": "chpt28-model-selection.html#technical-proofs",
    "title": "26  Model Selection, Stein Shrinkage, and Model Averaging",
    "section": "26.39 Technical Proofs*",
    "text": "26.39 Technical Proofs*\nProof of Theorem 28.1 We establish the theorem under the simplifying assumptions of the normal linear regression model with a \\(K \\times 1\\) coefficient vector \\(\\beta\\) and known variance \\(\\sigma^{2}\\). The likelihood function is\n\\[\nL_{n}(\\beta)=\\left(2 \\pi \\sigma^{2}\\right)^{-n / 2} \\exp \\left(-\\frac{1}{2 \\sigma^{2}} \\sum_{i=1}^{n}\\left(Y_{i}-X_{i}^{\\prime} \\beta\\right)^{2}\\right) .\n\\]\nEvaluated at the MLE \\(\\widehat{\\beta}\\) this equals\n\\[\nL_{n}(\\widehat{\\beta})=\\left(2 \\pi \\sigma^{2}\\right)^{-n / 2} \\exp \\left(-\\frac{\\sum_{i=1}^{n} \\widehat{e}_{i}^{2}}{2 \\sigma^{2}}\\right) .\n\\]\nUsing (8.21) we can write\n\\[\n\\begin{aligned}\nL_{n}(\\beta) &=\\left(2 \\pi \\sigma^{2}\\right)^{-n / 2} \\exp \\left(-\\frac{1}{2 \\sigma^{2}}\\left(\\sum_{i=1}^{n} \\widehat{e}_{i}^{2}+(\\widehat{\\beta}-\\beta)^{\\prime} \\boldsymbol{X}^{\\prime} \\boldsymbol{X}(\\widehat{\\beta}-\\beta)\\right)\\right) \\\\\n&=L_{n}(\\widehat{\\beta}) \\exp \\left(-\\frac{1}{2 \\sigma^{2}}(\\widehat{\\beta}-\\beta)^{\\prime} \\boldsymbol{X}^{\\prime} \\boldsymbol{X}(\\widehat{\\beta}-\\beta)\\right) .\n\\end{aligned}\n\\]\nFor a diffuse prior \\(\\pi(\\beta)=C\\) the marginal likelihood is\n\\[\n\\begin{aligned}\np(Y) &=L_{n}(\\widehat{\\beta}) \\int \\exp \\left(-\\frac{1}{2 \\sigma^{2}}(\\widehat{\\beta}-\\beta)^{\\prime} \\boldsymbol{X}^{\\prime} \\boldsymbol{X}(\\widehat{\\beta}-\\beta)\\right) C d \\beta \\\\\n&=L_{n}(\\widehat{\\beta}) n^{-K / 2}\\left(2 \\pi \\sigma^{2}\\right)^{K / 2} \\operatorname{det}\\left(\\frac{1}{n} \\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1 / 2} C\n\\end{aligned}\n\\]\nwhere the final equality is the multivariate normal integral. Rewriting and taking logs\n\\[\n\\begin{aligned}\n-2 \\log p(Y) &=-2 \\log L_{n}(\\widehat{\\beta})+K \\log n-K \\log \\left(2 \\pi \\sigma^{2}\\right)+\\log \\operatorname{det}\\left(\\frac{1}{n} \\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)+\\log C \\\\\n&=-2 \\ell_{n}(\\widehat{\\beta})+K \\log n+O(1) .\n\\end{aligned}\n\\]\nThis is the theorem.\nProof of Theorem 28.2 From (28.11)\n\\[\n\\begin{aligned}\n\\int g(y) \\log f(y, \\widehat{\\theta}) d y &=-\\frac{n}{2} \\log \\left(2 \\pi \\sigma^{2}\\right)-\\frac{1}{2 \\sigma^{2}} \\sum_{i=1}^{n} \\int\\left(y-X_{i}^{\\prime} \\widehat{\\beta}\\right)^{2} g\\left(y \\mid X_{i}\\right) d y \\\\\n&=-\\frac{n}{2} \\log \\left(2 \\pi \\sigma^{2}\\right)-\\frac{1}{2 \\sigma^{2}} \\sum_{i=1}^{n}\\left(\\sigma^{2}+(\\widehat{\\beta}-\\beta)^{\\prime} X_{i} X_{i}^{\\prime}(\\widehat{\\beta}-\\beta)\\right) \\\\\n&=-\\frac{n}{2} \\log \\left(2 \\pi \\sigma^{2}\\right)-\\frac{n}{2}-\\frac{1}{2 \\sigma^{2}} \\boldsymbol{e}^{\\prime} \\boldsymbol{P} \\boldsymbol{e} .\n\\end{aligned}\n\\]\nThus\n\\[\nT=-2 \\mathbb{E}\\left[\\int g(y) \\log \\widehat{f}(y) d y\\right]=n \\log \\left(2 \\pi \\sigma^{2}\\right)+n+\\frac{1}{\\sigma^{2}} \\mathbb{E}[\\boldsymbol{e} \\boldsymbol{P} \\boldsymbol{e}]=n \\log \\left(2 \\pi \\sigma^{2}\\right)+n+K .\n\\]\nThis is (28.12). The final equality holds under the assumption of conditional homoskedasticity.\nEvaluating (28.11) at \\(\\widehat{\\beta}\\) we obtain the log likelihood\n\\[\n-2 \\ell_{n}(\\widehat{\\beta})=n \\log \\left(2 \\pi \\sigma^{2}\\right)+\\frac{1}{\\sigma^{2}} \\sum_{i=1}^{n} \\widehat{e}_{i}^{2}=n \\log \\left(2 \\pi \\sigma^{2}\\right)+\\frac{1}{\\sigma^{2}} \\boldsymbol{e}^{\\prime} \\boldsymbol{M} \\boldsymbol{e} .\n\\]\nThis has expectation\n\\[\n-\\mathbb{E}\\left[2 \\ell_{n}(\\widehat{\\beta})\\right]=n \\log \\left(2 \\pi \\sigma^{2}\\right)+\\frac{1}{\\sigma^{2}} \\mathbb{E}\\left[\\boldsymbol{e}^{\\prime} \\boldsymbol{M e}\\right]=n \\log \\left(2 \\pi \\sigma^{2}\\right)+n-K .\n\\]\nThis is (28.13). The final equality holds under conditional homoskedasticity.\nProof of Theorem 28.4 The proof uses Taylor expansions similar to those used for the asymptotic distribution theory of the MLE in nonlinear models. We avoid technical details so this is not a full proof.\nWrite the model density as \\(f(y, \\theta)\\) and the estimated model as \\(\\widehat{f}(y)=f(y, \\widehat{\\theta})\\). Recall from (28.10) that we can write the target \\(T\\) as\n\\[\nT=-2 \\mathbb{E}[\\log f(\\widetilde{Y}, \\widehat{\\theta})]\n\\]\nwhere \\(\\widetilde{Y}\\) is an independent copy of \\(Y\\). Let \\(\\widetilde{\\theta}\\) be the MLE calculated on the sample \\(\\widetilde{Y} . \\widetilde{\\theta}\\) is an independent copy of \\(\\widehat{\\theta}\\). By symmetry we can write \\(T\\) as\n\\[\nT=-2 \\mathbb{E}[\\log f(Y, \\widetilde{\\theta})] .\n\\]\nDefine the Hessian \\(H=-\\frac{\\partial}{\\partial \\theta \\partial \\theta^{\\prime}} \\mathbb{E}[\\log f(Y, \\theta)]>0\\). Now take a second-order Taylor series expansion of the \\(\\log\\) likelihood \\(\\log f(Y, \\widetilde{\\theta})\\) about \\(\\widehat{\\theta}\\). This is\n\\[\n\\begin{aligned}\n\\log f(Y, \\widetilde{\\theta}) &=\\log f(Y, \\widehat{\\theta})+\\frac{\\partial}{\\partial \\theta^{\\prime}} \\log f(Y, \\widehat{\\theta})(\\widetilde{\\theta}-\\widehat{\\theta})-\\frac{1}{2}(\\widetilde{\\theta}-\\widehat{\\theta})^{\\prime} H(\\widetilde{\\theta}-\\widehat{\\theta})+O_{p}\\left(n^{-1 / 2}\\right) \\\\\n&=\\log f(Y, \\widehat{\\theta})-\\frac{n}{2}(\\widetilde{\\theta}-\\widehat{\\theta})^{\\prime} H(\\widetilde{\\theta}-\\widehat{\\theta})+O_{p}\\left(n^{-1 / 2}\\right) .\n\\end{aligned}\n\\]\nThe second equality holds because of the first-order condition for the MLE \\(\\widehat{\\theta}\\). If the \\(O_{p}\\left(n^{-1 / 2}\\right)\\) term in (28.34) is uniformly integrable (28.33) and (28.34) imply that\n\\[\n\\begin{aligned}\nT &=-\\mathbb{E}[2 \\log f(Y, \\widehat{\\theta})]+\\mathbb{E}\\left[n(\\widetilde{\\theta}-\\widehat{\\theta})^{\\prime} H(\\widetilde{\\theta}-\\widehat{\\theta})\\right]+O\\left(n^{-1 / 2}\\right) \\\\\n&=-\\mathbb{E}[2 \\log L(\\widehat{\\theta})]+\\mathbb{E}\\left[n(\\widetilde{\\theta}-\\theta)^{\\prime} H(\\widetilde{\\theta}-\\theta)\\right]+\\mathbb{E}\\left[n(\\widehat{\\theta}-\\theta)^{\\prime} H(\\widehat{\\theta}-\\theta)\\right] \\\\\n&+2 \\mathbb{E}\\left[n(\\widetilde{\\theta}-\\theta)^{\\prime} H(\\widehat{\\theta}-\\theta)\\right]+O\\left(n^{-1 / 2}\\right) \\\\\n&=-\\mathbb{E}\\left[2 \\ell_{n}(\\widehat{\\theta})\\right]+\\mathbb{E}\\left[\\chi_{K}^{2}\\right]+\\mathbb{E}\\left[\\widetilde{\\chi}_{K}^{2}\\right]+O\\left(n^{-1 / 2}\\right) \\\\\n&=-\\mathbb{E}\\left[2 \\ell_{n}(\\widehat{\\theta})\\right]+2 K+O\\left(n^{-1 / 2}\\right)\n\\end{aligned}\n\\]\nwhere \\(\\chi_{K}^{2}\\) and \\(\\widetilde{\\chi}_{K}^{2}\\) are chi-square random variables with \\(K\\) degrees of freedom. The second-to-last equality holds if\n\\[\nn(\\widehat{\\theta}-\\theta)^{\\prime} H(\\widehat{\\theta}-\\theta) \\underset{d}{\\longrightarrow} \\chi_{K}^{2}\n\\]\nand the Wald statistic on the left-side of (28.35) is uniformly integrable. The asymptotic convergence (28.35) holds for the MLE under standard regularity conditions (including correct specification).\nProof of Theorem 28.5 Using matrix notation we can write \\(\\widehat{\\boldsymbol{m}}-\\boldsymbol{m}=-\\left(\\boldsymbol{I}_{n}-\\boldsymbol{A}\\right) \\boldsymbol{m}+\\boldsymbol{A} \\boldsymbol{e}\\). We can then write the fit as\n\\[\n\\begin{aligned}\nR &=\\mathbb{E}\\left[(\\widehat{\\boldsymbol{m}}-\\boldsymbol{m})^{\\prime}(\\widehat{\\boldsymbol{m}}-\\boldsymbol{m}) \\mid \\boldsymbol{X}\\right] \\\\\n&=\\mathbb{E}\\left[\\boldsymbol{m}^{\\prime}\\left(\\boldsymbol{I}_{n}-\\boldsymbol{A}^{\\prime}\\right)\\left(\\boldsymbol{I}_{n}-\\boldsymbol{A}\\right) \\boldsymbol{m}-2 \\boldsymbol{m}^{\\prime}\\left(\\boldsymbol{I}_{n}-\\boldsymbol{A}^{\\prime}\\right) \\boldsymbol{A} \\boldsymbol{e}+\\boldsymbol{e}^{\\prime} \\boldsymbol{A}^{\\prime} \\boldsymbol{A} \\boldsymbol{e} \\mid \\boldsymbol{X}\\right] \\\\\n&=\\boldsymbol{m}^{\\prime}\\left(\\boldsymbol{I}_{n}-\\boldsymbol{A}^{\\prime}\\right)\\left(\\boldsymbol{I}_{n}-\\boldsymbol{A}\\right) \\boldsymbol{m}+\\sigma^{2} \\operatorname{tr}\\left(\\boldsymbol{A}^{\\prime} \\boldsymbol{A}\\right) .\n\\end{aligned}\n\\]\nNotice that this calculation relies on the assumption of conditional homoskedasticity.\nNow consider the Mallows criterion. We find that\n\\[\n\\begin{aligned}\nC_{p}^{*} &=\\widehat{\\boldsymbol{e}}^{\\prime} \\widehat{\\boldsymbol{e}}+2 \\widetilde{\\sigma}^{2} \\operatorname{tr}(\\boldsymbol{A})-\\boldsymbol{e}^{\\prime} \\boldsymbol{e} \\\\\n&=(\\boldsymbol{m}+\\boldsymbol{e})^{\\prime}\\left(\\boldsymbol{I}_{n}-\\boldsymbol{A}^{\\prime}\\right)\\left(\\boldsymbol{I}_{n}-\\boldsymbol{A}\\right)(\\boldsymbol{m}+\\boldsymbol{e})+2 \\widetilde{\\sigma}^{2} \\operatorname{tr}(\\boldsymbol{A})-\\boldsymbol{e}^{\\prime} \\boldsymbol{e} \\\\\n&=\\boldsymbol{m}^{\\prime}\\left(\\boldsymbol{I}_{n}-\\boldsymbol{A}^{\\prime}\\right)\\left(\\boldsymbol{I}_{n}-\\boldsymbol{A}\\right) \\boldsymbol{m}+2 \\boldsymbol{m}^{\\prime}\\left(\\boldsymbol{I}_{n}-\\boldsymbol{A}^{\\prime}\\right)\\left(\\boldsymbol{I}_{n}-\\boldsymbol{A}\\right) \\boldsymbol{e}+\\boldsymbol{e}^{\\prime} \\boldsymbol{A}^{\\prime} \\boldsymbol{A} \\boldsymbol{e}-2 \\boldsymbol{e}^{\\prime} \\boldsymbol{A} \\boldsymbol{e}+2 \\widetilde{\\sigma}^{2} \\operatorname{tr}(\\boldsymbol{A}) .\n\\end{aligned}\n\\]\nTaking expectations and using the assumptions of conditional homoskedasticity and \\(\\mathbb{E}\\left[\\widetilde{\\sigma}^{2} \\mid \\boldsymbol{X}\\right]=\\sigma^{2}\\)\n\\[\n\\mathbb{E}\\left[C_{p}^{*} \\mid \\boldsymbol{X}\\right]=\\boldsymbol{m}^{\\prime}\\left(\\boldsymbol{I}_{n}-\\boldsymbol{A}^{\\prime}\\right)\\left(\\boldsymbol{I}_{n}-\\boldsymbol{A}\\right) \\boldsymbol{m}+\\sigma^{2} \\operatorname{tr}\\left(\\boldsymbol{A}^{\\prime} \\boldsymbol{A}\\right)=R\n\\]\nThis is the result as stated.\nProof of Theorem 28.6 Take any two models \\(\\mathscr{M}_{1}\\) and \\(\\mathscr{M}_{2}\\) where \\(\\mathcal{M}_{1} \\notin \\overline{\\mathscr{M}}^{*}\\) and \\(\\mathscr{M}_{2} \\in \\overline{\\mathscr{M}}^{*}\\). Let their information criteria be written as\n\\[\n\\begin{aligned}\n&\\mathrm{IC}_{1}=-2 \\ell_{1}\\left(\\widehat{\\theta}_{1}\\right)+c\\left(n, K_{1}\\right) \\\\\n&\\mathrm{IC}_{2}=-2 \\ell_{2}\\left(\\widehat{\\theta}_{2}\\right)+c\\left(n, K_{2}\\right) .\n\\end{aligned}\n\\]\nModel \\(\\mathscr{M}_{1}\\) is selected over \\(\\mathscr{M}_{2}\\) if\n\\[\n\\mathrm{LR}<c\\left(n, K_{2}\\right)-c\\left(n, K_{1}\\right)\n\\]\nwhere \\(\\mathrm{LR}=2\\left(\\ell_{2}\\left(\\widehat{\\theta}_{2}\\right)-\\ell\\left(\\widehat{\\theta}_{1}\\right)\\right)\\) is the likelihood ratio statistic for testing \\(\\mathcal{M}_{1}\\) against \\(\\mathcal{M}_{2}\\). Since we have assumed that \\(\\mathscr{M}_{1}\\) is not a true model while \\(\\mathscr{M}_{2}\\) is true, then LR diverges to \\(+\\infty\\) at rate \\(n\\). This means that for any \\(\\alpha>0, n^{-1+\\alpha} \\mathrm{LR} \\underset{p}{\\rightarrow}+\\infty\\). Furthermore, the assumptions imply \\(n^{-1+\\alpha}\\left(c\\left(n, K_{1}\\right)-c\\left(n, K_{2}\\right)\\right) \\longrightarrow 0\\). Fix \\(\\epsilon>0\\). There is an \\(n\\) sufficiently large such that \\(n^{-1+\\alpha}\\left(c\\left(n, K_{1}\\right)-c\\left(n, K_{2}\\right)\\right)<\\epsilon\\). Thus\n\\[\n\\begin{aligned}\n\\mathbb{P}\\left[\\widehat{M}=\\mathscr{M}_{1}\\right] & \\leq \\mathbb{P}\\left[n^{-1+\\alpha} \\operatorname{LR}<n^{-1+\\alpha}\\left(c\\left(n, K_{2}\\right)-c\\left(n, K_{1}\\right)\\right)\\right] \\\\\n& \\leq \\mathbb{P}[\\mathrm{LR}<\\epsilon] \\rightarrow 0 .\n\\end{aligned}\n\\]\nSince this holds for any \\(\\mathscr{M}_{1} \\notin \\overline{\\mathscr{M}}^{*}\\) we deduce that the selected model is in \\(\\overline{\\mathscr{M}}^{*}\\) with probability approaching one. This means that the selection criterion is model selection consistent as claimed.\nProof of Theorem 28.7 Take the setting as described in the proof of Theorem \\(28.6\\) but now assume \\(\\mathscr{M}_{1} \\subset\\) \\(\\mathcal{M}_{2}\\) and \\(\\mathscr{M}_{1}, \\mathscr{M}_{2} \\in \\overline{\\mathcal{M}}^{*}\\). The likelihood ratio statistic satisfies LR \\(\\underset{d}{\\longrightarrow} \\chi_{r}^{2}\\) where \\(r=K_{2}-K_{1}\\). Let\n\\[\nB=\\limsup _{n \\rightarrow \\infty}\\left(c\\left(n, K_{1}\\right)-c\\left(n, K_{2}\\right)\\right)<\\infty .\n\\]\nLetting \\(F_{r}(u)\\) denote the \\(\\chi_{r}^{2}\\) distribution function\n\\[\n\\begin{aligned}\n\\mathbb{P}\\left[\\widehat{\\mathscr{M}}=\\mathscr{M}_{2}\\right] &=\\mathbb{P}\\left[\\operatorname{LR}>\\left(c\\left(n, K_{2}\\right)-c\\left(n, K_{1}\\right)\\right)\\right] \\\\\n& \\geq \\mathbb{P}[\\operatorname{LR}>B] \\\\\n& \\rightarrow \\mathbb{P}\\left[\\chi_{r}^{2}>B\\right]=1-F_{r}(B)>0\n\\end{aligned}\n\\]\nbecause \\(\\chi_{r}^{2}\\) has support over the positive real line and \\(B<\\infty\\). This shows that the selection criterion asymptotically over-selects with positive probability.\nProof of Theorem \\(28.8\\) Since \\(c(n, K)=o(n)\\) the procedure is model selection consistent. Take two models \\(\\mathscr{M}_{1}, \\mathscr{M}_{2} \\in \\overline{\\mathscr{M}}^{*}\\) with \\(K_{1}<K_{2}\\). Since both models are true then LR \\(=O_{p}(1)\\). Fix \\(\\epsilon>0\\). There is a \\(B<\\infty\\) such that \\(\\mathrm{LR} \\leq B\\) with probability exceeding \\(1-\\epsilon\\). By (28.16) there is an \\(n\\) sufficiently large such that \\(c\\left(n, K_{2}\\right)-c\\left(n, K_{1}\\right)>B\\). Thus\n\\[\n\\mathbb{P}\\left[\\widehat{\\mathscr{M}}=\\mathscr{M}_{2}\\right] \\leq \\mathbb{P}\\left[\\mathrm{LR}>\\left(c\\left(n, K_{2}\\right)-c\\left(n, K_{1}\\right)\\right)\\right] \\leq \\mathbb{P}[\\mathrm{LR}>B] \\leq \\epsilon .\n\\]\nSince \\(\\epsilon\\) is arbitrary \\(\\mathbb{P}\\left[\\widehat{\\mathscr{M}}=\\mathscr{M}_{2}\\right] \\longrightarrow 0\\) as claimed.\nProof of Theorem 28.9 First, we examine \\(R_{n}(K)\\). Write the predicted values in matrix notation as \\(\\widehat{\\boldsymbol{m}}_{K}=\\) \\(\\boldsymbol{X}_{K} \\widehat{\\beta}_{K}=\\boldsymbol{P}_{K} \\boldsymbol{Y}\\) where \\(\\boldsymbol{P}_{K}=\\boldsymbol{X}_{K}\\left(\\boldsymbol{X}_{K}^{\\prime} \\boldsymbol{X}_{K}\\right)^{-1} \\boldsymbol{X}_{K}^{\\prime}\\). It is useful to observe that \\(\\boldsymbol{m}-\\widehat{\\boldsymbol{m}}_{K}=\\boldsymbol{M}_{K} \\boldsymbol{m}-\\boldsymbol{P}_{K} \\boldsymbol{e}\\) where \\(\\boldsymbol{M}_{K}=\\boldsymbol{I}_{K}-\\boldsymbol{P}_{K}\\). We find that the prediction risk equals\n\\[\n\\begin{aligned}\nR_{n}(K) &=\\mathbb{E}\\left[\\left(\\boldsymbol{m}-\\widehat{\\boldsymbol{m}}_{K}\\right)^{\\prime}\\left(\\boldsymbol{m}-\\widehat{\\boldsymbol{m}}_{K}\\right) \\mid \\boldsymbol{X}\\right] \\\\\n&=\\mathbb{E}\\left[\\left(\\boldsymbol{M}_{K} \\boldsymbol{m}-\\boldsymbol{P}_{K} \\boldsymbol{e}\\right)^{\\prime}\\left(\\boldsymbol{M}_{K} \\boldsymbol{m}-\\boldsymbol{P}_{K} \\boldsymbol{e}\\right) \\mid \\boldsymbol{X}\\right] \\\\\n&=\\boldsymbol{m}^{\\prime} \\boldsymbol{M}_{K} \\boldsymbol{m}+\\mathbb{E}\\left[\\boldsymbol{e}^{\\prime} \\boldsymbol{P}_{K} \\boldsymbol{e} \\mid \\boldsymbol{X}\\right] \\\\\n&=\\boldsymbol{m}^{\\prime} \\boldsymbol{M}_{K} \\boldsymbol{m}+\\sigma^{2} K .\n\\end{aligned}\n\\]\nThe choice of regressors affects \\(R_{n}(K)\\) through the two terms in the final line. The first term \\(\\boldsymbol{m}^{\\prime} \\boldsymbol{M}_{K} \\boldsymbol{m}\\) is the squared bias due to omitted variables. As \\(K\\) increases this term decreases reflecting reduced omitted variables bias. The second term \\(\\sigma^{2} K\\) is estimation variance. It is increasing in the number of regressors. Increasing the number of regressors affects the quality of out-of-sample prediction by reducing the bias but increasing the variance. We next examine the adjusted Mallows criterion. We find that\n\\[\n\\begin{aligned}\nC_{n}^{*}(K) &=\\widehat{\\boldsymbol{e}}_{K}^{\\prime} \\widehat{\\boldsymbol{e}}_{K}+2 \\sigma^{2} K-\\boldsymbol{e}^{\\prime} \\boldsymbol{e} \\\\\n&=(\\boldsymbol{m}+\\boldsymbol{e})^{\\prime} \\boldsymbol{M}_{K}(\\boldsymbol{m}+\\boldsymbol{e})+2 \\sigma^{2} K-\\boldsymbol{e}^{\\prime} \\boldsymbol{e} \\\\\n&=\\boldsymbol{m}^{\\prime} \\boldsymbol{M}_{K} \\boldsymbol{m}+2 \\boldsymbol{m}^{\\prime} \\boldsymbol{M}_{K} \\boldsymbol{e}-\\boldsymbol{e}^{\\prime} \\boldsymbol{P}_{K} \\boldsymbol{e}+2 \\sigma^{2} K .\n\\end{aligned}\n\\]\nThe next step is to show that\n\\[\n\\sup _{K}\\left|\\frac{C_{n}^{*}(K)-R_{n}(K)}{R_{n}(K)}\\right| \\underset{p}{\\longrightarrow} 0\n\\]\nas \\(n \\rightarrow \\infty\\). To establish (28.36), observe that\n\\[\nC_{n}^{*}(K)-R_{n}(K)=2 \\boldsymbol{m}^{\\prime} \\boldsymbol{M}_{K} \\boldsymbol{e}-\\boldsymbol{e}^{\\prime} \\boldsymbol{P}_{K} \\boldsymbol{e}+\\sigma^{2} K .\n\\]\nPick \\(\\epsilon>0\\) and some sequence \\(B_{n} \\rightarrow \\infty\\) such that \\(B_{n} /\\left(R_{n}^{\\text {opt }}\\right)^{r} \\rightarrow 0\\). (This is feasible by Assumption 28.1.5.) By Boole’s inequality (B.24), Whittle’s inequality (B.48), the facts that \\(\\boldsymbol{m}^{\\prime} \\boldsymbol{M}_{K} \\boldsymbol{m} \\leq R_{n}(K)\\) and \\(R_{n}(K) \\geq \\sigma^{2} K\\), \\(B_{n} /\\left(R_{n}^{\\text {opt }}\\right)^{r} \\rightarrow 0\\), and \\(\\sum_{K=1}^{\\infty} K^{-r}<\\infty\\)\n\\[\n\\begin{aligned}\n& \\mathbb{P}\\left[\\sup _{K}\\left|\\frac{\\boldsymbol{m}^{\\prime} \\boldsymbol{M}_{K} \\boldsymbol{e}}{R_{n}(K)}\\right|>\\epsilon \\mid \\boldsymbol{X}\\right] \\leq \\sum_{K=1}^{\\infty} \\mathbb{P}\\left[\\left|\\frac{\\boldsymbol{m}^{\\prime} \\boldsymbol{M}_{K} \\boldsymbol{e}}{R_{n}(K)}\\right|>\\epsilon \\mid \\boldsymbol{X}\\right] \\\\\n& \\leq \\frac{C_{1} r}{\\epsilon^{2 r}} \\sum_{K=1}^{\\infty} \\frac{\\left|\\boldsymbol{m}^{\\prime} \\boldsymbol{M}_{K} \\boldsymbol{m}\\right|^{r}}{R_{n}(K)^{2 r}} \\\\\n& \\leq \\frac{C_{1 r}}{\\epsilon^{2 r}} \\sum_{K=1}^{\\infty} \\frac{1}{R_{n}(K)^{r}} \\\\\n& =\\frac{C_{1} r}{\\epsilon^{2 r}} \\sum_{K=1}^{B_{n}} \\frac{1}{R_{n}(K)^{r}}+\\frac{C_{1 r}}{\\epsilon^{2 r}} \\sum_{K=B_{n}+1}^{\\infty} \\frac{1}{R_{n}(K)^{r}} \\\\\n& \\leq \\frac{C_{1 r}}{\\epsilon^{2 r}} \\frac{B_{n}}{\\left(R_{n}^{\\mathrm{opt}}\\right)^{r}}+\\frac{C_{1 r}}{\\epsilon^{2 r} \\sigma^{2 r}} \\sum_{K=B_{n}+1}^{\\infty} \\frac{1}{K^{r}} \\\\\n& \\rightarrow 0 \\text {. }\n\\end{aligned}\n\\]\nBy a similar argument but using Whittle’s inequality (B.49), \\(\\operatorname{tr}\\left(\\boldsymbol{P}_{K} \\boldsymbol{P}_{K}\\right)=\\operatorname{tr}\\left(\\boldsymbol{P}_{K}\\right)=K\\), and \\(K \\leq \\sigma^{-2} R_{n}(K)\\)\n\\[\n\\begin{aligned}\n\\mathbb{P}\\left[\\sup _{K}\\left|\\frac{\\boldsymbol{e}^{\\prime} \\boldsymbol{P}_{K} \\boldsymbol{e}-\\sigma^{2} K}{R_{n}(K)}\\right|>\\epsilon \\mid \\boldsymbol{X}\\right] & \\leq \\sum_{K=1}^{\\infty} \\mathbb{P}\\left[\\left|\\frac{\\boldsymbol{e}^{\\prime} \\boldsymbol{P}_{K} \\boldsymbol{e}-\\mathbb{E}\\left(\\boldsymbol{e}^{\\prime} \\boldsymbol{P}_{K} \\boldsymbol{e}\\right)}{R_{n}(K)}\\right|>\\epsilon \\mid \\boldsymbol{X}\\right] \\\\\n& \\leq \\frac{C_{2 r}}{\\epsilon^{2 r}} \\sum_{K=1}^{\\infty} \\frac{\\operatorname{tr}\\left(\\boldsymbol{P}_{K} \\boldsymbol{P}_{K}\\right)^{r}}{R_{n}(K)^{2 r}} \\\\\n&=\\frac{C_{2 r}}{\\epsilon^{2 r}} \\sum_{K=1}^{\\infty} \\frac{K^{r}}{R_{n}(K)^{2 r}} \\\\\n& \\leq \\frac{C_{1 r}}{\\epsilon^{2 r} \\sigma^{2 r}} \\sum_{K=1}^{\\infty} \\frac{1}{R_{n}(K)^{r}} \\\\\n& \\rightarrow 0 .\n\\end{aligned}\n\\]\nTogether these imply (28.36).\nFinally we show that (28.36) implies (28.18). The argument is similar to the standard consistency proof for nonlinear estimators. (28.36) states that \\(C_{n}^{*}(K)\\) converges uniformly in probability to \\(R_{n}(K)\\). This implies that the minimizer of \\(C_{n}^{*}(K)\\) converges in probability to that of \\(R_{n}(K)\\). Formally, because \\(K_{n}^{\\mathrm{opt}}\\) minimizes \\(R_{n}(K)\\)\n\\[\n\\begin{aligned}\n0 & \\leq \\frac{R_{n}\\left(\\widehat{K}_{n}\\right)-R_{n}\\left(K_{n}^{\\mathrm{opt}}\\right)}{R_{n}\\left(\\widehat{K}_{n}\\right)} \\\\\n&=\\frac{C_{n}^{*}\\left(\\widehat{K}_{n}\\right)-R_{n}\\left(K_{n}^{\\mathrm{opt}}\\right)}{R_{n}\\left(\\widehat{K}_{n}\\right)}-\\frac{C_{n}^{*}\\left(\\widehat{K}_{n}\\right)-R_{n}\\left(\\widehat{K}_{n}\\right)}{-R_{n}\\left(\\widehat{K}_{n}\\right)} \\\\\n& \\leq \\frac{C_{n}^{*}\\left(\\widehat{K}_{n}\\right)-R_{n}\\left(K_{n}^{\\mathrm{opt}}\\right)}{R_{n}\\left(\\widehat{K}_{n}\\right)}+o_{p}(1) \\\\\n& \\leq \\frac{C_{n}^{*}\\left(K_{n}^{\\mathrm{opt}}\\right)-R_{n}\\left(K_{n}^{\\mathrm{opt}}\\right)}{R_{n}\\left(K_{n}^{\\mathrm{opt}}\\right)}+o_{p}(1) \\\\\n& \\leq o_{p}(1) .\n\\end{aligned}\n\\]\nThe second inequality is (28.36). The following uses the facts that \\(\\widehat{K}_{n}\\) minimizes \\(C_{n}^{*}(K)\\) and \\(K_{n}^{\\text {opt }}\\) minimizes \\(R_{n}(K)\\). The final is (28.36). This is (28.18).\nBefore providing the proof of Theorem \\(28.10\\) we present two technical results related to the noncentral chi-square density function with degree of freedom \\(K\\) and non-centrality parameter \\(\\lambda\\) which equals\n\\[\nf_{K}(x, \\lambda)=\\sum_{i=0}^{\\infty} \\frac{e^{-\\lambda / 2}}{i !}\\left(\\frac{\\lambda}{2}\\right)^{i} f_{K+2 i}(x)\n\\]\nwhere \\(f_{r}(x)=\\frac{x^{r / 2-1} e^{-x / 2}}{\\left.2^{r / 2} \\Gamma(r / 2)\\right)}\\) is the \\(\\chi_{K}^{2}\\) density function.\nTheorem 28.16 The non-central chi-square density function (28.37) obeys the recursive relationship \\(f_{K}(x, \\lambda)=\\frac{K}{x} f_{K+2}(x, \\lambda)+\\frac{\\lambda}{x} f_{K+4}(x, \\lambda)\\).\nThe proof of Theorem \\(28.16\\) is a straightforward manipulation of the non-central chi-square density function (28.37).\nThe second technical result is from Bock (1975, Theorems A&B).\nTheorem 28.17 If \\(X \\sim \\mathrm{N}\\left(\\theta, \\boldsymbol{I}_{K}\\right)\\) then for any function \\(h(u)\\)\n\\[\n\\begin{aligned}\n\\mathbb{E}\\left[X h\\left(X^{\\prime} X\\right)\\right] &=\\theta \\mathbb{E}\\left[h\\left(Q_{K+2}\\right)\\right] \\\\\n\\mathbb{E}\\left[X^{\\prime} X h\\left(X^{\\prime} X\\right)\\right] &=K \\mathbb{E}\\left[h\\left(Q_{K+2}\\right)\\right]+\\lambda \\mathbb{E}\\left[h\\left(Q_{K+4}\\right)\\right]\n\\end{aligned}\n\\]\nwhere \\(\\lambda=\\theta^{\\prime} \\theta\\) and \\(Q_{r} \\sim \\chi_{r}^{2}(\\lambda)\\), a non-central chi-square random variable with \\(r\\) degrees of freedom and non-centrality parameter \\(\\lambda\\).\nProof of Theorem 28.17 To show (28.38) we first show that for \\(Z \\sim \\mathrm{N}(\\mu, 1)\\) then for any function \\(g(u)\\)\n\\[\n\\mathbb{E}\\left[Z g\\left(Z^{2}\\right)\\right]=\\mu \\mathbb{E}\\left[g\\left(Q_{3}\\right)\\right] .\n\\]\nAssume \\(\\mu>0\\). Using the change-of-variables \\(y=x^{2}\\)\n\\[\n\\begin{aligned}\n\\mathbb{E}\\left[Z g\\left(Z^{2}\\right)\\right] &=\\int_{-\\infty}^{\\infty} \\frac{x}{\\sqrt{2 \\pi}} g\\left(x^{2}\\right) \\exp \\left(-\\frac{1}{2}(x-\\mu)^{2}\\right) d x \\\\\n&=\\int_{0}^{\\infty} \\frac{y}{2 \\sqrt{2 \\pi}} e^{-\\left(y+\\mu^{2}\\right) / 2}\\left(e^{\\sqrt{y} \\mu}-e^{-\\sqrt{y} \\mu}\\right) g(y) d y .\n\\end{aligned}\n\\]\nBy expansion and Legendre’s duplication formula\n\\[\ne^{x}-e^{-x}=2 \\sum_{i=0}^{\\infty} \\frac{x^{1+2 i}}{(1+2 i) !}=\\sqrt{\\pi} x \\sum_{i=0}^{\\infty} \\frac{\\left(x^{2} / 2\\right)^{i}}{2^{i} i ! \\Gamma(i+3 / 2)} .\n\\]\nThen (28.41) equals\n\\[\n\\mu \\int_{0}^{\\infty} y e^{-\\left(y+\\mu^{2}\\right) / 2} \\sum_{i=0}^{\\infty} \\frac{\\left(\\mu^{2} / 2\\right)^{i} y^{i+1 / 2}}{2^{3 / 2+i} i ! \\Gamma(i+3 / 2)} g(y) d y=\\mu \\int_{0}^{\\infty} y f_{3}\\left(y, \\mu^{2}\\right) g(y) d y=\\mu \\mathbb{E}\\left[g\\left(Q_{3}\\right)\\right]\n\\]\nwhere \\(f_{3}(y, \\lambda)\\) is the non-central chi-square density (28.37) with 3 degrees of freedom. This is (28.40).\nTake the \\(j^{t h}\\) row of (28.38). Write \\(X^{\\prime} X=X_{j}^{2}+J\\), where \\(X_{j} \\sim \\mathrm{N}\\left(\\theta_{j}, 1\\right)\\) and \\(J \\sim \\chi_{K-1}^{2}\\left(\\lambda-\\theta_{j}^{2}\\right)\\) are independent. Setting \\(g(u)=h(u+J)\\) and using (28.41)\n\\[\n\\begin{aligned}\n\\mathbb{E}\\left[X_{j} h\\left(X^{\\prime} X\\right)\\right] &=\\mathbb{E}\\left[X_{j} h\\left(X_{j}^{2}+J\\right)\\right] \\\\\n&=\\mathbb{E}\\left[\\mathbb{E}\\left[X_{j} g\\left(X_{j}^{2}\\right) \\mid J\\right]\\right] \\\\\n&=\\mathbb{E}\\left[\\theta_{j} \\mathbb{E}\\left[g\\left(Q_{3}\\right) \\mid J\\right]\\right] \\\\\n&=\\theta_{j} \\mathbb{E}\\left[h\\left(Q_{3}+J\\right)\\right] \\\\\n&=\\theta_{j} \\mathbb{E}\\left[h\\left(Q_{K+2}\\right)\\right]\n\\end{aligned}\n\\]\nwhich is (28.38). The final equality uses the fact that \\(Q_{3}+J \\sim Q_{K+2}\\).\nObserve that \\(X^{\\prime} X\\) has density \\(f_{K}(x, \\lambda)\\). Using Theorem \\(28.16\\)\n\\[\n\\begin{aligned}\n\\mathbb{E}\\left[X^{\\prime} X\\left(X^{\\prime} X\\right)\\right] &=\\int_{0}^{\\infty} x h(x) f_{K}(x, \\lambda) d x \\\\\n&=K \\int_{0}^{\\infty} h(x) f_{K+2}(x, \\lambda) d x+\\lambda \\int_{0}^{\\infty} h(x) f_{K+4}(x, \\lambda) d x \\\\\n&=K \\mathbb{E}\\left[h\\left(Q_{K+2}\\right)\\right]+\\lambda \\mathbb{E}\\left[h\\left(Q_{K+4}\\right)\\right]\n\\end{aligned}\n\\]\nwhich is (28.39).\nProof of Theorem 28.10 By the quadratic structure we can calculate that\n\\[\n\\begin{aligned}\n\\operatorname{MSE}\\left[\\widehat{\\theta}^{*}\\right] &=\\mathbb{E}\\left[\\left(\\widehat{\\theta}-\\theta-\\widehat{\\theta} \\mathbb{1}\\left\\{\\widehat{\\theta}^{\\prime} \\hat{\\theta} \\leq c\\right\\}\\right)^{\\prime}\\left(\\widehat{\\theta}-\\theta-\\widehat{\\theta} \\mathbb{1}\\left\\{\\widehat{\\theta}^{\\prime} \\widehat{\\theta} \\leq c\\right\\}\\right)\\right] \\\\\n&=\\mathbb{E}\\left[(\\widehat{\\theta}-\\theta)^{\\prime}(\\widehat{\\theta}-\\theta)\\right]-\\mathbb{E}\\left[\\widehat{\\theta}^{\\prime} \\widehat{\\theta} \\mathbb{1}\\left\\{\\widehat{\\theta}^{\\prime} \\widehat{\\theta} \\leq c\\right\\}\\right]+2 \\mathbb{E}\\left[\\theta^{\\prime} \\widehat{\\theta} \\mathbb{1}\\left\\{\\widehat{\\theta}^{\\prime} \\widehat{\\theta} \\leq c\\right\\}\\right] \\\\\n&=K-K \\mathbb{E}\\left[\\mathbb{1}\\left\\{Q_{K+2} \\leq c\\right\\}\\right]-\\lambda \\mathbb{E}\\left[\\mathbb{1}\\left\\{Q_{K+4} \\leq c\\right\\}\\right]+2 \\lambda \\mathbb{E}\\left[\\mathbb{1}\\left\\{Q_{K+2} \\leq c\\right\\}\\right] \\\\\n&=K+(2 \\lambda-K) F_{K+2}(c, \\lambda)-\\lambda F_{K+4}(c, \\lambda) .\n\\end{aligned}\n\\]\nThe third equality uses the two results from Theorem 28.17, setting \\(h(u)=\\mathbb{1}\\{u \\leq c\\}\\)."
  },
  {
    "objectID": "chpt28-model-selection.html#exercises",
    "href": "chpt28-model-selection.html#exercises",
    "title": "26  Model Selection, Stein Shrinkage, and Model Averaging",
    "section": "26.40 Exercises",
    "text": "26.40 Exercises\nExercise 28.1 Verify equations (28.1)-(28.2).\nExercise 28.2 Find the Mallows criterion for the weighted least squares estimator of a linear regression \\(Y_{i}=X_{i}^{\\prime} \\beta+e_{i}\\) with weights \\(\\omega_{i}\\) (assume conditional homoskedasticity).\nExercise 28.3 Backward Stepwise Regression. Verify the claim that for the case of AIC selection, step (b) of the algorithm can be implemented by calculating the classical (homoskedastic) t-ratio for each active regressor and find the regressor with the smallest absolute t-ratio.\nHint: Use the relationship between likelihood ratio and F statistics and the equality between \\(\\mathrm{F}\\) and Wald statistics to show that for tests on one coefficient the smallest change in the AIC is identical to identifying the smallest squared t statistic.\nExercise 28.4 Forward Stepwise Regression. Verify the claim that for the case of AIC selection, step (b) of the algorithm can be implemented by identifying the regressor in the inactive set with the greatest absolute correlation with the residual from step (a).\nHint: This is challenging. First show that the goal is to find the regressor which will most decrease SSE \\(=\\widehat{\\boldsymbol{e}}^{\\prime} \\widehat{\\boldsymbol{e}}=\\|\\widehat{\\boldsymbol{e}}\\|^{2}\\). Use a geometric argument to show that the regressor most parallel to \\(\\widehat{\\boldsymbol{e}}\\) will most decreases \\(\\|\\widehat{\\boldsymbol{e}}\\|\\). Show that this regressor has the greatest absolute correlation with \\(\\widehat{\\boldsymbol{e}}\\).\nExercise 28.5 An economist estimates several models and reports a single selected specification, stating that “the other specifications had insignificant coefficients”. How should we interpret the reported parameter estimates and t-ratios?\nExercise 28.6 Verify Theorem 28.11, including (28.21), (28.22), and (28.23).\nExercise 28.7 Under the assumptions of Theorem 28.11, show that \\(\\hat{\\lambda}=\\widehat{\\theta}^{\\prime} \\boldsymbol{V}^{-1} \\widehat{\\theta}-K\\) is an unbiased estimator of \\(\\lambda=\\theta^{\\prime} \\boldsymbol{V}^{-1} \\theta\\).\nExercise 28.8 Prove Theorem \\(28.14\\) for the simpler case of the unadjusted (not positive part) Stein estimator \\(\\widetilde{\\theta}, \\boldsymbol{V}=\\boldsymbol{I}_{K}\\) and \\(r=0\\).\nExtra challenge: Show under these assumptions that\n\\[\n\\begin{aligned}\n\\operatorname{wmse}[\\tilde{\\theta}] &=K-(q-2)^{2} J_{q}\\left(\\lambda_{R}\\right) \\\\\n\\lambda_{\\boldsymbol{R}} &=\\theta^{\\prime} \\boldsymbol{R}\\left(\\boldsymbol{R}^{\\prime} \\boldsymbol{R}\\right)^{-1} \\boldsymbol{R}^{\\prime} \\theta .\n\\end{aligned}\n\\]\nExercise 28.9 Suppose you have two unbiased estimators \\(\\widehat{\\theta}_{1}\\) and \\(\\widehat{\\theta}_{2}\\) of a parameter vector \\(\\widehat{\\theta}\\) with covariance matrices \\(\\boldsymbol{V}_{1}\\) and \\(\\boldsymbol{V}_{2}\\). Take the goal of minimizing the unweighted mean squared error, e.g. \\(\\operatorname{tr} \\boldsymbol{V}_{1}\\) for \\(\\widehat{\\theta}_{1}\\). Assume that \\(\\widehat{\\theta}_{1}\\) and \\(\\widehat{\\theta}_{2}\\) are uncorrelated.\n\nShow that the optimal weighted average estimator equals\n\n\\[\n\\frac{\\frac{1}{\\operatorname{tr} \\boldsymbol{V}_{1}} \\widehat{\\theta}_{1}+\\frac{1}{\\operatorname{tr} \\boldsymbol{V}_{2}} \\widehat{\\theta}_{2}}{\\frac{1}{\\operatorname{tr} \\boldsymbol{V}_{1}}+\\frac{1}{\\operatorname{tr} \\boldsymbol{V}_{2}}} .\n\\]\n\nGeneralize to the case of \\(M\\) unbiased uncorrelated estimators.\nInterpret the formulae. Exercise 28.10 You estimate \\(M\\) linear regressions \\(Y=X_{m}^{\\prime} \\beta_{m}+e_{m}\\) by least squares. Let \\(\\widehat{Y}_{m i}=X_{m i}^{\\prime} \\widehat{\\beta}_{m}\\) be the fitted values.\nShow that the Mallows averaging criterion is the same as\n\n\\[\n\\sum_{i=1}^{n}\\left(Y_{i}-w_{1} \\widehat{Y}_{1 i}-w_{2} \\widehat{Y}_{2 i}-\\cdots-w_{M} \\widehat{Y}_{M i}\\right)^{2}+2 \\sigma^{2} \\sum_{m=1}^{M} w_{m} k_{m}\n\\]\n\nAssume the models are nested with \\(M\\) the largest model. If the previous criterion were minimized over \\(w\\) in the probability simplex but the penalty was omitted, what would be the solution? (What would be the minimizing weight vector?)\n\nExercise 28.11 You estimate \\(M\\) linear regressions \\(Y=X_{m}^{\\prime} \\beta_{m}+e_{m}\\) by least squares. Let \\(\\widetilde{Y}_{m i}=X_{m i}^{\\prime} \\widehat{\\beta}_{m(-i)}\\) be the predicted values from the leave-one-out regressions. Show that the JMA criterion equals\n\\[\n\\sum_{i=1}^{n}\\left(Y_{i}-w_{1} \\widetilde{Y}_{1 i}-w_{2} \\widetilde{Y}_{2 i}-\\cdots-w_{M} \\widetilde{Y}_{M i}\\right)^{2}\n\\]\nExercise 28.12 Using the cps09mar dataset perform an analysis similar to that presented in Section \\(28.18\\) but instead use the sub-sample of Hispanic women. This sample has 3003 observations. Which models are selected by BIC, AIC, CV and FIC? (The precise information criteria you examine may be limited depending on your software.) How do you interpret the results? Which model/estimate would you select as your preferred choice?"
  },
  {
    "objectID": "chpt29-ML.html#introduction",
    "href": "chpt29-ML.html#introduction",
    "title": "27  Machine Learning",
    "section": "27.1 Introduction",
    "text": "27.1 Introduction\nThis chapter reviews machine learning methods for econometrics. This is a large and growing topic so our treatment is selective. This chapter briefly covers ridge regression, Lasso, elastic net, regression trees, bagging, random forests, ensembling, Lasso IV, double-selection/post-regularization, and double/debiased machine learning.\nA classic reference is Hastie, Tibshirani, and Friedman (2008). Introductory textbooks include James, Witten, Hastie, and Tibshirani (2013) and Efron and Hastie (2017). For a theoretical treatment see Bühlmann and van der Geer (2011). For reviews of machine learning in econometrics see Belloni, Chernozhukov and Hansen (2014a), Mullainathan and Spiess (2017), Athey and Imbens (2019), and Belloni, Chernozhukov, Chetverikov, Hansen, and Kato (2021)."
  },
  {
    "objectID": "chpt29-ML.html#big-data-high-dimensionality-and-machine-learning",
    "href": "chpt29-ML.html#big-data-high-dimensionality-and-machine-learning",
    "title": "27  Machine Learning",
    "section": "27.2 Big Data, High Dimensionality, and Machine Learning",
    "text": "27.2 Big Data, High Dimensionality, and Machine Learning\nThree inter-related concepts are “big data”, “high dimensionality”, and “machine learning”.\nBig data is typically used to describe datasets which are unusually large and/or complex relative to traditional applications. The definition of “large” varies across discipline and time, but typically refers to datasets with millions of observations. These datasets can arise in economics from household census data, government administrative records, and supermarket scanner data. Some challenges associated with big data are storage, transmission, and computation.\nHigh Dimensional is typically used to describe datasets with an unusually large number of variables. Again the definition of “large” varies across applications, but typically refers to hundreds or thousands of variables. In the theoretical literature “high dimensionality” is used specifically for the context where \\(p>n\\), meaning that the number of variables \\(p\\) greatly exceeds the number of observations \\(n\\).\nMachine Learning is typically used to describe a set of algorithmic approaches to statistical learning. The methods are primarily focused on point prediction in settings with unknown structure. Machine learning methods generally allow for large sample sizes, large number of variables, and unknown structural form. The early literature was algorithmic with no associated statistical theory. This was followed by a statistical literature examining the properties of machine learning methods, mostly providing convergence rates under sparsity assumptions. Only recently has the literature expanded to include inference.\nMachine learning embraces a large and diverse set of tools for a variety of settings, including supervised learning (prediction rules for \\(Y\\) given high-dimensional \\(X\\) ), unsupervised learning (uncovering structure amongst high-dimensional \\(X\\) ), and classification (discrete choice analysis with highdimensional predictors). In this chapter we focus on supervised learning as it is a natural extension of linear regression.\nMachine learning arose from the computer science literature and thereby adopted a distinct set of labels to describe familar concepts. For example, it speaks of “training” rather than “estimation” and “features” rather than “regressors”. In this chapter, however, we will use standard econometric language and terminology.\nFor econometrics, machine learning can be thought of as “highly nonparametric”. Suppose we are interested in estimating the conditional mean \\(m(X)=\\mathbb{E}[Y \\mid X]\\) when the shape of \\(m(x)\\) is unknown. A nonparametric analysis typically assumes that \\(X\\) is low-dimensional. In contrast, a machine learning analysis may allow for hundreds or even thousands of regressors in \\(X\\), and does not require prior information about which regressors are most relevant.\nConnections between nonparametric estimation, model selection, and machine learning methods arise in tuning parameter selection by cross-validation and evaluation by out-of-sample prediction accuracy. These issues are taken seriously in machine learning applications; frequently with multiple levels of hold-out samples."
  },
  {
    "objectID": "chpt29-ML.html#high-dimensional-regression",
    "href": "chpt29-ML.html#high-dimensional-regression",
    "title": "27  Machine Learning",
    "section": "27.3 High Dimensional Regression",
    "text": "27.3 High Dimensional Regression\nWe are familiar with the linear regression model \\(Y=X^{\\prime} \\beta+e\\) where \\(X\\) and \\(\\beta\\) are \\(p \\times 1\\) vectors \\({ }^{1}\\). In conventional regression models we are accustomed to thinking of the number of variables \\(p\\) as small relative to the sample size \\(n\\). Traditional parametric asymptotic theory assumes that \\(p\\) is fixed as \\(n \\rightarrow \\infty\\) which is typically interpreted as implying that \\(p\\) is much smaller than \\(n\\). Nonparametric regression theory assumes that \\(p \\rightarrow \\infty\\) but at a much slower rate than \\(n\\). This is interpreted as \\(p\\) being moderately large but still much smaller than \\(n\\). High-dimensional regression is used to describe the context where \\(p\\) is very large, including the case where \\(p\\) is larger than \\(n\\). It even includes the case where \\(p\\) is exponentially larger than \\(n\\).\nIt may seem shocking to contemplate an application with more regressors than observations. But the situation arises in a number of contexts. First, in our discussion of series regression (Chapter 20) we described how a regression function can be approximated by an infinite series expansion in basis transformations of the underlying regressors. Expressed as a linear model this implies a regression model with an infinite number of regressors. Practical models (as discussed in that chapter) use a moderate number of regressors in estimated regressions because this provides a balance between bias and variance. This latter models, however, are not the true conditional mean (which has an infinite number of regressors) but rather a low-dimensional best linear approximation. Second, many economic applications involve a large number of binary, discrete, and categorical variables. A saturated regression model converts all discrete and categorical variables into binary variables and includes all interactions. Such manipulations can result in thousands of regressors. For example, ten binary variables fully interacted yields 1024 regressors. Twenty binary variables fully interacted yields over one million regressors. Third, many contemporary “big” datasets contain thousands of potential regressors. Many of the variables may be low-information but it is difficult to know a priori which are relevant and which irrelevant.\nWhen \\(p>n\\) the least squares estimator \\(\\widehat{\\beta}_{\\text {ols }}\\) is not uniquely defined because \\(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\) has deficient rank. Furthermore, for \\(p<n\\) but “large” the matrix \\(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\) can be near-singular or ill-conditioned so the least squares estimator can be numerically unstable and high variance. Consequently we turn to estimation methods other than least squares. In this chapter we discuss several alternative estimation methods, including ridge regression, Lasso, elastic net, regression trees, and random forests.\n\\({ }^{1}\\) In most of this textbook we have denoted the dimension of \\(X\\) as \\(k\\). In this chapter we will instead denote the dimension of \\(X\\) as \\(p\\) as this is the custom in the machine learning literature."
  },
  {
    "objectID": "chpt29-ML.html#p-norms",
    "href": "chpt29-ML.html#p-norms",
    "title": "27  Machine Learning",
    "section": "27.4 p-norms",
    "text": "27.4 p-norms\nFor discussion of ridge and Lasso regression we will be making extensive use of the 1-norm and 2norm, so it is useful to review the definition of the general p-norm. For a vector \\(a=\\left(a_{1}, \\ldots, a_{k}\\right)^{\\prime}\\) the p-norm \\((p \\geq 1)\\) is\n\\[\n\\|a\\|_{p}=\\left(\\sum_{j=1}^{k}\\left|a_{j}\\right|^{p}\\right)^{1 / p} .\n\\]\nImportant special cases include the 1-norm\n\\[\n\\|a\\|_{1}=\\sum_{j=1}^{k}\\left|a_{j}\\right|\n\\]\nthe 2-norm\n\\[\n\\|a\\|_{2}=\\left(\\sum_{j=1}^{k} a_{j}^{2}\\right)^{1 / 2},\n\\]\nand the sup-norm\n\\[\n\\|a\\|_{\\infty}=\\max _{1 \\leq j \\leq k}\\left|a_{j}\\right| .\n\\]\nWe also define the “0-norm”\n\\[\n\\|a\\|_{0}=\\sum_{j=1}^{k} \\mathbb{1}\\left\\{a_{j} \\neq 0\\right\\},\n\\]\nthe number of non-zero elements. This is only heuristically labeled as a “norm”.\nThe p-norm satisfies the following additivity property. If \\(a=\\left(a_{0}, a_{1}\\right)\\) then\n\\[\n\\|a\\|_{p}^{p}=\\left\\|a_{0}\\right\\|_{p}^{p}+\\left\\|a_{1}\\right\\|_{p}^{p} .\n\\]\nThe following inequalities are useful. The Hölder inequality for \\(1 / p+1 / q=1\\) is\n\\[\n\\left|a^{\\prime} b\\right| \\leq\\|a\\|_{p}\\|b\\|_{q} .\n\\]\nThe case \\(p=1\\) and \\(q=\\infty\\) is\n\\[\n\\left|a^{\\prime} b\\right| \\leq\\|a\\|_{1}\\|b\\|_{\\infty} .\n\\]\nThe Minkowski inequality for \\(p \\geq 1\\) is\n\\[\n\\|a+b\\|_{p} \\leq\\|a\\|_{p}+\\|b\\|_{p} .\n\\]\nThe p-norms for \\(p \\geq 1\\) satisfy norm monotonicity. In particular\n\\[\n\\|a\\|_{1} \\geq\\|a\\|_{2} \\geq\\|a\\|_{\\infty} .\n\\]\nApplying Hölder’s (29.1) we also have the inequality\n\\[\n\\|a\\|_{1}=\\sum_{j=1}^{k}\\left|a_{j}\\right| \\mathbb{1}\\left\\{a_{j} \\neq 0\\right\\} \\leq\\|a\\|_{2}\\|a\\|_{0}^{1 / 2} .\n\\]"
  },
  {
    "objectID": "chpt29-ML.html#ridge-regression",
    "href": "chpt29-ML.html#ridge-regression",
    "title": "27  Machine Learning",
    "section": "27.5 Ridge Regression",
    "text": "27.5 Ridge Regression\nRidge regression is a shrinkage-type estimator with similar but distinct properties from the JamesStein estimator (see Section 28.20). There are two competing motivations for ridge regression. The traditional motivation is to reduce the degree of collinearity among the regressors. The modern motivation (though in mathematics it pre-dates the “traditional” motivation) is regularization of high-dimensional and ill-posed inverse problems. We discuss both in turn.\nAs discussed in the previous section, when \\(p\\) is large the least squares coefficient estimate can be numerically unreliable due to an ill-conditioned \\(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\). As a numerical improvement, Hoerl and Kennard (1970) proposed the ridge regression estimator\n\\[\n\\widehat{\\beta}_{\\text {ridge }}=\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}+\\lambda \\boldsymbol{I}_{p}\\right)^{-1} \\boldsymbol{X}^{\\prime} \\boldsymbol{Y}\n\\]\nwhere \\(\\lambda>0\\) is called the ridge parameter. This estimator has the property that it is well-defined and does not suffer from multicollinearity or ill-conditioning. This even holds if \\(p>n\\) ! That is, the ridge regression estimator is well-defined even when the number of regressors exceeds the sample size.\nThe ridge parameter \\(\\lambda\\) controls the extent of shrinkage, and can be viewed as a tuning parameter. We discuss how to select \\(\\lambda\\) below.\nTo see how \\(\\lambda>0\\) ensures that the inverse problem is solved, use the spectral decomposition to write \\(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}=\\boldsymbol{H}^{\\prime} \\boldsymbol{D} \\boldsymbol{H}\\) where \\(\\boldsymbol{H}\\) is orthonormal and \\(\\boldsymbol{D}=\\operatorname{diag}\\left\\{r_{1}, \\ldots, r_{p}\\right\\}\\) is a diagonal matrix with the eigenvalues \\(r_{j}\\) of \\(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\) on the diagonal. Set \\(\\Lambda=\\lambda \\boldsymbol{I}_{p}\\). We can write\n\\[\n\\boldsymbol{X}^{\\prime} \\boldsymbol{X}+\\lambda \\boldsymbol{I}_{p}=\\boldsymbol{H}^{\\prime} \\boldsymbol{D} \\boldsymbol{H}+\\lambda \\boldsymbol{H}^{\\prime} \\boldsymbol{H}=\\boldsymbol{H}^{\\prime}(\\boldsymbol{D}+\\Lambda) \\boldsymbol{H}\n\\]\nwhich has strictly positive eigenvalues \\(r_{j}+\\lambda>0\\). Thus all eigenvalues are bounded away from zero so \\(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}+\\lambda \\boldsymbol{I}_{p}\\) is full rank and well-conditioned.\nThe second motivation is based on penalization. When \\(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\) is ill-conditioned its inverse is ill-posed. Techniques to deal with ill-posed estimators are called regularization and date back to Tikhonov (1943). A leading method is penalization. Consider the sum of squared errors penalized by the squared 2-norm of the coefficient vector\n\\[\n\\operatorname{SSE}_{2}(\\beta, \\lambda)=(\\boldsymbol{Y}-\\boldsymbol{X} \\beta)^{\\prime}(\\boldsymbol{Y}-\\boldsymbol{X} \\beta)+\\lambda \\beta^{\\prime} \\beta=\\|\\boldsymbol{Y}-\\boldsymbol{X} \\beta\\|_{2}^{2}+\\lambda\\|\\beta\\|_{2}^{2} .\n\\]\nThe minimizer of \\(\\operatorname{SSE}_{2}(\\beta, \\lambda)\\) is a regularized least squares estimator.\nThe first order condition for minimization of \\(\\operatorname{SSE}_{2}(\\beta, \\lambda)\\) over \\(\\beta\\) is\n\\[\n-2 \\boldsymbol{X}^{\\prime}(\\boldsymbol{Y}-\\boldsymbol{X} \\beta)+2 \\lambda \\beta=0 .\n\\]\nThe solution is \\(\\widehat{\\beta}_{\\text {ridge }}\\). Thus the regularized (penalized) least squares estimator equals ridge regression. This shows that the ridge regression estimator minimizes the sum of squared errors subject to a penalty on the squared 2-norm of the regression coefficient. Penalizing large coefficient vectors keeps the latter from being too large and erratic. Hence one interpretation of \\(\\lambda\\) is as a penalty on the magnitude of the coefficient vector.\nMinimization subject to a penalty has a dual representation as constrained minimization. The latter is\n\\[\n\\min _{\\beta^{\\prime} \\beta \\leq \\tau}(\\boldsymbol{Y}-\\boldsymbol{X} \\beta)^{\\prime}(\\boldsymbol{Y}-\\boldsymbol{X} \\beta)\n\\]\nfor some \\(\\tau>0\\). To see the connection, the Lagrangian for the constrained problem is\n\\[\n\\min _{\\beta}(\\boldsymbol{Y}-\\boldsymbol{X} \\beta)^{\\prime}(\\boldsymbol{Y}-\\boldsymbol{X} \\beta)+\\lambda\\left(\\beta^{\\prime} \\beta-\\tau\\right)\n\\]\nwhere \\(\\lambda\\) is a Lagrange multiplier. The first order condition is (29.5) which is the same as the penalization problem. This shows that they have the same solution.\nThe practical difference between the penalization and constraint problems is that in the first you specify the ridge parameter \\(\\lambda\\) while in the second you specify the constraint parameter \\(\\tau\\). They are connected because the values of \\(\\lambda\\) and \\(\\tau\\) satisfy the relationship\n\\[\n\\boldsymbol{Y}^{\\prime} \\boldsymbol{X}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}+\\lambda \\boldsymbol{I}_{p}\\right)^{-1}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}+\\lambda \\boldsymbol{I}_{p}\\right)^{-1} \\boldsymbol{X}^{\\prime} \\boldsymbol{Y}=\\tau .\n\\]\nTo find \\(\\lambda\\) given \\(\\tau\\) it is sufficient to (numerically) solve this equation.\nFigure 29.1: Ridge Regression Dual Minimization Solution\nTo visualize the constraint problem see Figure \\(29.1\\) which plots an example in \\(\\mathbb{R}^{2}\\). The constraint set \\(\\beta^{\\prime} \\beta \\leq \\tau\\) is displayed as the ball about the origin and the contour sets of the sum of squared errors are displayed as ellipses. The least squares estimator is the center of the ellipses, while the ridge regression estimator is the point on the circle where the contour is tangent. This shrinks the least squares coefficient towards the zero vector. Unlike the Stein estimator, however, it does not shrink along the line segment connecting least squares with the origin, rather it shrinks along a trajectory determined by the degree of correlation between the variables. This trajectory is displayed with the dashed lines, marked as “Ridge path”. This is the sequence of ridge regression coefficients obtained as \\(\\lambda\\) is varied from 0 to \\(\\infty\\). When \\(\\lambda=0\\) the ridge estimator equals least squares. For small \\(\\lambda\\) the ridge estimator moves slightly towards the origin by sliding along the ridge of the contour set. As \\(\\lambda\\) increases the ridge estimator takes a more direct path towards the origin. This is unlike the Stein estimator which shrinks the least squares estimator towards the origin along the connecting line segment. It is straightforward to generalize ridge regression to allow different penalties on different groups of regressors. Take the model\n\\[\nY=X_{1}^{\\prime} \\beta_{1}+\\cdots+X_{G}^{\\prime} \\beta_{G}+e\n\\]\nand minimize the SSE subject to the penalty\n\\[\n\\lambda_{1} \\beta_{1}^{\\prime} \\beta_{1}+\\cdots+\\lambda_{G} \\beta_{G}^{\\prime} \\beta_{G} .\n\\]\nThe solution is\n\\[\n\\widehat{\\beta}_{\\text {ridge }}=\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}+\\Lambda\\right)^{-1} \\boldsymbol{X}^{\\prime} \\boldsymbol{Y}\n\\]\nwhere\n\\[\n\\Lambda=\\operatorname{diag}\\left\\{\\lambda_{1} \\boldsymbol{I}_{p_{1}}, \\ldots, \\lambda_{G} \\boldsymbol{I}_{p_{G}}\\right\\}\n\\]\nThis allows some coefficients to be penalized more (or less) than other coefficients. This added flexibility comes at the cost of selecting the ridge parameters \\(\\lambda=\\left(\\lambda_{1}, \\ldots, \\lambda_{G}\\right)\\). One important special case is \\(\\lambda_{1}=0\\), thus one group of coefficients are not penalized. With \\(G=2\\) this partitions the coefficients into two groups: penalized and non-penalized.\nThe most popular method to select the ridge parameter \\(\\lambda\\) is cross validation. The leave-one-out ridge regression estimator, prediction errors, and CV criterion are\n\\[\n\\begin{aligned}\n\\widehat{\\beta}_{-i}(\\lambda) &=\\left(\\sum_{j \\neq i} X_{j} X_{j}^{\\prime}+\\Lambda\\right)^{-1}\\left(\\sum_{j \\neq i} X_{j} Y_{i}\\right) \\\\\n\\widetilde{e}_{i}(\\lambda) &=Y_{i}-X_{i}^{\\prime} \\widehat{\\beta}_{-i}(\\lambda) \\\\\n\\mathrm{CV}(\\lambda) &=\\sum_{i=1}^{n} \\widetilde{e}_{i}(\\lambda)^{2} .\n\\end{aligned}\n\\]\nThe CV-selected ridge parameter \\(\\lambda_{\\mathrm{cv}}\\) minimizes \\(\\mathrm{CV}(\\lambda)\\). The cross-validation ridge estimator is calculated using \\(\\lambda_{\\mathrm{cv}}\\).\nIn practice it may be tricky to minimize \\(\\mathrm{CV}(\\lambda)\\). The minimum may occur at \\(\\lambda=0\\) (ridge equals least squares), at \\(\\lambda=\\infty\\) (full shrinkage), or there may be multiple local minima. The scale of the minimizing \\(\\lambda\\) depends on the scaling of the regressors and in particular the singular values of \\(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\). It can be important to explore CV \\((\\lambda)\\) for very small values of \\(\\lambda\\).\nAs for least squares there is a simple formula to calculate the CV criterion for ridge regression which greatly speeds computation.\nTheorem 29.1 The leave-one-out ridge regression prediction errors are\n\\[\n\\widetilde{e}_{i}(\\lambda)=\\left(1-X_{i}^{\\prime}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}+\\Lambda\\right)^{-1} X_{i}\\right)^{-1} \\widehat{e}_{i}(\\lambda)\n\\]\nwhere \\(\\widehat{e}_{i}(\\lambda)=Y_{i}-X_{i}^{\\prime} \\widehat{\\beta}_{\\text {ridge }}(\\lambda)\\) are the ridge regression residuals.\nFor a proof see Exercise 29.1.\nAn alternative method for selection of \\(\\lambda\\) is to minimize the Mallows criterion which equals\n\\[\nC(\\lambda)=\\sum_{i=1}^{n} \\widehat{e}_{i}(\\lambda)^{2}+2 \\widehat{\\sigma}^{2} \\operatorname{tr}\\left(\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}+\\Lambda\\right)^{-1}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)\\right)\n\\]\nwhere \\(\\widehat{\\sigma}^{2}\\) is the variance estimator from least squares estimation. For a derivation of (29.7) see Exercise 29.2. The Mallows-selected ridge parameter \\(\\lambda_{\\mathrm{m}}\\) minimizes \\(C(\\lambda)\\). The Mallows-selected ridge estimator is calculated using \\(\\lambda_{\\mathrm{m}}\\). \\(\\mathrm{Li}\\) (1986) showed that in the normal regression model the ridge estimator with the Mallows-selected ridge parameter is asymptotically equivalent to the infeasible best ridge parameter in terms of regression fit. I am unaware of a similar optimality result for cross-validated-selected ridge estimation.\nAn important caveat is that the ridge regression estimator is not invariant to rescaling the regressors nor other linear transformations. Therefore it is common to apply ridge regression after applying standardizing transformations to the regressors.\nRidge regression can be implemented in \\(\\mathrm{R}\\) with the glmnet command. In Stata, ridge regression is available in the downloadable package lassopack."
  },
  {
    "objectID": "chpt29-ML.html#statistical-properties-of-ridge-regression",
    "href": "chpt29-ML.html#statistical-properties-of-ridge-regression",
    "title": "27  Machine Learning",
    "section": "27.6 Statistical Properties of Ridge Regression",
    "text": "27.6 Statistical Properties of Ridge Regression\nUnder the assumptions of the linear regression model it is straightforward to calculate the exact bias and variance of the ridge regression estimator. Take the linear regression model\n\\[\n\\begin{aligned}\nY &=X^{\\prime} \\beta+e \\\\\n\\mathbb{E}[e \\mid X] &=0 .\n\\end{aligned}\n\\]\nThe bias of the ridge estimator with fixed \\(\\lambda\\) is\n\\[\n\\operatorname{bias}\\left[\\widehat{\\beta}_{\\text {ridge }} \\mid \\boldsymbol{X}\\right]=-\\lambda\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}+\\lambda \\boldsymbol{I}_{p}\\right)^{-1} \\beta \\text {. }\n\\]\nUnder random sampling its covariance matrix is\n\\[\n\\operatorname{var}\\left[\\widehat{\\beta}_{\\text {ridge }} \\mid \\boldsymbol{X}\\right]=\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}+\\lambda \\boldsymbol{I}_{p}\\right)^{-1}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{D} \\boldsymbol{X}\\right)\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}+\\lambda \\boldsymbol{I}_{p}\\right)^{-1}\n\\]\nwhere \\(\\boldsymbol{D}=\\operatorname{diag}\\left\\{\\sigma^{2}\\left(X_{1}\\right), \\ldots, \\sigma^{2}\\left(X_{n}\\right)\\right\\}\\) and \\(\\sigma^{2}(X)=\\mathbb{E}\\left[e^{2} \\mid X\\right]\\). For a derivation of (29.8) and (29.9) see Exercise 29.3. Under cluster or serial dependence the central component modifies in the standard way.\nWe can measure estimation efficiency by the mean squared error (MSE) matrix\n\\[\n\\operatorname{mse}[\\widehat{\\beta} \\mid \\boldsymbol{X}]=\\mathbb{E}\\left[(\\widehat{\\beta}-\\beta)(\\widehat{\\beta}-\\beta)^{\\prime} \\mid \\boldsymbol{X}\\right] .\n\\]\nDefine \\(\\underline{\\sigma}^{2}=\\min _{x \\in \\mathscr{X}} \\sigma^{2}(x)\\) where \\(\\mathscr{X}\\) is the support of \\(X\\).\nTheorem 29.2 In the linear regression model, if \\(0<\\lambda<2 \\underline{\\sigma}^{2} / \\beta^{\\prime} \\beta\\),\n\\[\n\\operatorname{mse}\\left[\\widehat{\\beta}_{\\text {ridge }} \\mid \\boldsymbol{X}\\right]<\\operatorname{mse}\\left[\\widehat{\\beta}_{\\text {ols }} \\mid \\boldsymbol{X}\\right] .\n\\]\nFor a proof see Section \\(29.23\\).\nTheorem \\(29.2\\) shows that the ridge estimator dominates the least squares estimator, if \\(\\lambda\\) satisfies a specific range of values. This holds regardless of the dimension of \\(\\beta\\). Since the upper bound \\(2 \\underline{\\sigma}^{2} / \\beta^{\\prime} \\beta\\) is unknown, however, it is unclear if feasible ridge regression dominates least squares. The upper bound does not give practical guidance for selection of \\(\\lambda\\). Given (29.9) it is straightforward to construct estimators of \\(V_{\\widehat{\\beta}}=\\operatorname{var}\\left[\\widehat{\\beta}_{\\text {ridge }} \\mid \\boldsymbol{X}\\right]\\). I suggest the HC3 analog\n\\[\n\\widetilde{V}_{\\widehat{\\beta}}=\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}+\\lambda \\boldsymbol{I}_{p}\\right)^{-1}\\left(\\sum_{i=1}^{n} X_{i} X_{i}^{\\prime} \\widetilde{e}_{i}(\\lambda)^{2}\\right)\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}+\\lambda \\boldsymbol{I}_{p}\\right)^{-1}\n\\]\nwhere \\(\\widetilde{e}_{i}(\\lambda)\\) are the ridge regression prediction errors (29.6). Alternatively, the ridge regression residuals \\(\\widehat{e}_{i}(\\lambda)\\) can be used but it is unclear how to make an appropriate degree-of-freedom correction. Under clustering or serial dependence the central component of \\(\\widetilde{V}_{\\widehat{\\beta}}\\) can be modified as usual. If the regressors are highly sparse (as in a sparse dummy variable regression) it may be prudent to use the homoskedastic estimator\n\\[\n\\widetilde{V}_{\\widehat{\\beta}}^{0}=\\widetilde{\\sigma}^{2}(\\lambda)\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}+\\lambda \\boldsymbol{I}_{p}\\right)^{-1}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}+\\lambda \\boldsymbol{I}_{p}\\right)^{-1}\n\\]\nwith \\(\\widetilde{\\sigma}^{2}(\\lambda)=n^{-1} \\sum_{i=1}^{n} \\widetilde{e}_{i}(\\lambda)^{2}\\).\nGiven that the ridge estimator is explicitly biased there are natural concerns about how to interpret standard errors calculated from these covariance matrix estimators. Confidence intervals calculated the usual way will have deficient coverage due to the bias. One answer is to interpret the ridge estimator \\(\\widehat{\\beta}_{\\text {ridge }}\\) and its standard errors similarly to those obtained in nonparametric regression. The estimators and confidence intervals are valid for the pseudo-true projections, e.g. \\(\\beta^{*}=\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}+\\lambda \\boldsymbol{I}_{p}\\right)^{-1} \\boldsymbol{X}^{\\prime} \\boldsymbol{X} \\beta\\), not the coefficients \\(\\beta\\) themselves. This is the same interpretation as we use for the projection model and for nonparametric regression. For asymptotically accurate inference on the true coefficients \\(\\beta\\) the ridge parameter \\(\\lambda\\) could be selected to satisfy \\(\\lambda=o(\\sqrt{n})\\) analogously to an undersmoothing bandwidth in nonparametric regression."
  },
  {
    "objectID": "chpt29-ML.html#illustrating-ridge-regression",
    "href": "chpt29-ML.html#illustrating-ridge-regression",
    "title": "27  Machine Learning",
    "section": "27.7 Illustrating Ridge Regression",
    "text": "27.7 Illustrating Ridge Regression\n\nCross-Validation Function\n\n\nEstimates of Return to Experience\n\nFigure 29.2: Least Squares and Ridge Regression Estimates of the Return to Experience To illustrate ridge regression we use the CPS dataset with the sample of Asian men with a college education (16 years of education or more) to estimate the experience profile. We consider a fifth-order polynomial in experience for the conditional mean of log wages. We start by standardizing the regressors. We first center experience at its mean, create powers up to order five, and then standardized each to have mean zero and variance one. We estimate the polynomial regression by least squares and by ridge regression, the latter shrinking the five coefficients on experience but not the intercept.\nWe calculate the ridge parameter by cross-validation. The cross-validation function is displayed in Figure 29.2(a) over the interval [0,60]. Since we have standardized the regressors to have zero mean and unit variance the ridge parameter is scaled comparably with sample size, which in this application is \\(n=875\\). The cross-validation function is uniquely minimized at \\(\\lambda=19\\). I use this value of \\(\\lambda\\) for the following ridge regression estimation.\nFigure 29.2(b) displays the estimated experience profiles. Least squares is displayed by dashes and ridge regression by the solid line. The ridge regression estimate is smoother and more compelling. The grey shaded region are \\(95 %\\) normal confidence bands centered at the ridge regression estimate, calculated using the HC3 covariance matrix estimator (29.10)."
  },
  {
    "objectID": "chpt29-ML.html#lasso",
    "href": "chpt29-ML.html#lasso",
    "title": "27  Machine Learning",
    "section": "27.8 Lasso",
    "text": "27.8 Lasso\nIn the previous section we learned that ridge regression minimizes the sum of squared errors plus a 2-norm penalty on the coefficient vector. Model selection (e.g. Mallows) minimizes the sum of squared errors plus the 0-norm penalty (the number of non-zero coefficients). An intermediate case uses the 1-norm penalty. This was proposed by Tibshirani (1996) and is known as the Lasso (for Least Absolute Shrinkage and Selection Operator). The least squares criterion with a 1-norm penalty is\n\\[\n\\operatorname{SSE}_{1}(\\beta, \\lambda)=(\\boldsymbol{Y}-\\boldsymbol{X} \\beta)^{\\prime}(\\boldsymbol{Y}-\\boldsymbol{X} \\beta)+\\lambda \\sum_{j=1}^{p}\\left|\\beta_{j}\\right|=\\|\\boldsymbol{Y}-\\boldsymbol{X} \\beta\\|_{2}^{2}+\\lambda\\|\\beta\\|_{1} .\n\\]\nThe Lasso estimator is its minimizer\n\\[\n\\widehat{\\beta}_{\\text {Lasso }}=\\underset{\\beta}{\\operatorname{argmin}} \\operatorname{SSE}_{1}(\\beta, \\lambda) .\n\\]\nExcept for special cases the solution must be found numerically. Fortunately, computational algorithms are surprisingly simple and fast. An important property is that when \\(\\lambda>0\\) the Lasso estimator is welldefined even if \\(p>n\\).\nThe Lasso minimization problem has the dual constrained minimization problem\n\\[\n\\widehat{\\beta}_{\\text {Lasso }}=\\underset{\\|\\beta\\|_{1} \\leq \\tau}{\\operatorname{argmin}} \\operatorname{SSE}_{1}(\\beta) .\n\\]\nTo see that the two problems are the same observe that the constrained minimization problem has the Lagrangian\n\\[\n\\min _{\\beta}(\\boldsymbol{Y}-\\boldsymbol{X} \\beta)^{\\prime}(\\boldsymbol{Y}-\\boldsymbol{X} \\beta)+\\lambda\\left(\\sum_{j=1}^{p}\\left|\\beta_{j}\\right|-\\tau\\right)\n\\]\nwhich has first order conditions\n\\[\n-2 \\boldsymbol{X}_{j}^{\\prime}(\\boldsymbol{Y}-\\boldsymbol{X} \\beta)+\\lambda \\operatorname{sgn}\\left(\\beta_{j}\\right)=0 .\n\\]\nThis is the same as those for minimization of the penalized criterion. Thus the solutions are identical.\nFigure 29.3: Lasso Dual Minimization Solution\nThe constraint set \\(\\left\\{\\|\\beta\\|_{1} \\leq \\tau\\right\\}\\) for the dual problem is a cross-polytope resembling a multi-faceted diamond. The minimization problem in \\(\\mathbb{R}^{2}\\) is illustrated in Figure 29.3. The sum of squared error contour sets are the ellipses with the least squares solution at the center. The constraint set is the shaded polytope. The Lasso estimator is the intersection point between the constraint set and the largest ellipse drawn. In this example it hits a vertex of the constraint set and so the constrained estimator sets \\(\\widehat{\\beta}_{1}=0\\). This is a typical outcome in Lasso estimation. Since we are minimizing a quadratic subject to a polytope, solutions tend to be at vertices. This eliminates a subset of the coefficients.\nThe Lasso path is drawn with the dashed line. This is the sequence of solutions obtained as the constraint set is varied. The solution path has the property that it is a straight line from the least squares estimator to the \\(y\\)-axis (in this example), at which point \\(\\beta_{2}\\) is set to zero, and then the solution path follows the \\(y\\)-axis to the origin. In general, the solution path is linear on segments until a coefficient hits zero, at which point that coefficient is eliminated. In this particular example the solution path shows \\(\\beta_{2}\\) increasing while \\(\\beta_{1}\\) decreases. Thus while Lasso is a shrinkage estimator it does not shrink individual coefficients monotonically.\nIt is instructive to compare Figures \\(29.1\\) and \\(29.3\\) which have the same sum of squares contours. The ridge estimator is generically an interior solution with no individual coefficient set to zero, while the Lasso estimator typically sets some coefficients equal to zero. However both estimators follow similar solution paths, following the ridge of the SSE criterion rather than taking a direct path towards the origin.\nOne case where we can explicitly calculate the Lasso estimates is when the regressors are orthogonal, e.g., \\(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}=\\boldsymbol{I}_{p}\\). Then the first order condition for minimization simplifies to\n\\[\n-2\\left(\\widehat{\\beta}_{\\text {ols }, j}-\\widehat{\\beta}_{\\text {Lasso }, j}\\right)+\\lambda \\operatorname{sgn}\\left(\\widehat{\\beta}_{\\text {Lasso }, j}\\right)=0\n\\]\nwhich has the explicit solution\n\\[\n\\widehat{\\beta}_{\\text {Lasso }, j}=\\left\\{\\begin{array}{cc}\n\\widehat{\\beta}_{\\mathrm{ols}, j}-\\lambda / 2 & \\widehat{\\beta}_{\\mathrm{ols}, j}>\\lambda / 2 \\\\\n0 & \\left|\\widehat{\\beta}_{\\mathrm{ols}, j}\\right| \\leq \\lambda / 2 \\\\\n\\widehat{\\beta}_{\\mathrm{ols}, j}+\\lambda / 2 & \\widehat{\\beta}_{\\mathrm{ols}, j}<-\\lambda / 2\n\\end{array}\\right.\n\\]\nThis shows that theLasso estimate is a continuous transformation of the least squares estimate. For small values of the least squares estimate the Lasso estimate is set to zero. For all other values the Lasso estimate moves the least squares estimate towards zero by \\(\\lambda / 2\\).\n\nSelection and Ridge\n\n\nLasso\n\nFigure 29.4: Transformations of Least Squares Estimates by Selection, Ridge, and Lasso\nIt is constructive to contrast this behavior with ridge regression and selection estimation. When \\(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}=\\boldsymbol{I}_{k}\\) the ridge estimator equals \\(\\widehat{\\beta}_{\\text {ridge }}=(1+\\lambda)^{-1} \\widehat{\\beta}_{\\text {ols }}\\) so shrinks the coefficients towards zero by a common multiple. A selection estimator (for simplicity consider selection based on a homoskedastic \\(\\mathrm{t}\\)-test with \\(\\widehat{\\sigma}^{2}=1\\) and critical value \\(c\\) ) equals \\(\\widehat{\\beta}_{\\text {select }}=\\mathbb{1}\\left\\{\\left|\\widehat{\\beta}_{\\text {ols }, j}\\right|>c\\right\\} \\widehat{\\beta}_{\\text {ols }, j}\\). Thus the Lasso, ridge, and selection estimates are all transformations of the least squares coefficient estimate. We illustrate these transformations in Figure 29.4. Panel (a) displays the selection and ridge transformations, and panel (b) displays the Lasso transformation.\nThe Lasso and ridge estimators are continuous functions while the selection estimator is a discontinuous function. The Lasso and selection estimators are thresholding functions, meaning that the function equals zero for a region about the origin. Thresholding estimators are selection estimators because they equal zero when the least squares estimator is sufficiently small. The Lasso function is a “soft thresholding” rule as it is a continuous function with bounded first derivative. The selection estimator is a “hard thresholding” rule as it is discontinuous. Hard thresholding rules tend to have high variance due to the discontinuous transformation. Consequently, we expect the Lasso to have reduced variance relative to selection estimators, permitting overall lower MSE.\nAs for ridge regression, Lasso is not invariant to the scaling of the regressors. If you rescale a regressor then the penalty has a different meaning. Consequently, it is important to scale the regressors appropriately before applying Lasso. It is conventional to scale all the variables to have mean zero and unit variance.\nLasso is also not invariant to rotations of the regressors. For example, Lasso on \\(\\left(\\boldsymbol{X}_{1}, \\boldsymbol{X}_{2}\\right)\\) is not the same as Lasso on \\(\\left(\\boldsymbol{X}_{1}-\\boldsymbol{X}_{2}, \\boldsymbol{X}_{2}\\right)\\) despite having identical least squares solutions. This is troubling as typically there is no default specification.\nApplications of Lasso estimation in economics are growing. Belloni, Chernozhukov and Hansen (2014) illustrate the method using three application: (1) the effect of eminent domain on housing prices in an instrumental variables framework, (2) a re-examination of the effect of abortion on crime using the framework of Donohue and Levitt (2001), (3) a re-examination of the the effect of democracy on growth using the framework of Acemoglu, Johnson, and Robinson (2001). Mullainathan and Spiess (2017) illustrate machine learning using a prediction model for housing prices using characteristics. Oster (2018) uses household scanner data to measure the effect of a diabetes diagnosis on food purchases."
  },
  {
    "objectID": "chpt29-ML.html#lasso-penalty-selection",
    "href": "chpt29-ML.html#lasso-penalty-selection",
    "title": "27  Machine Learning",
    "section": "27.9 Lasso Penalty Selection",
    "text": "27.9 Lasso Penalty Selection\nCritically important for Lasso estimation is the penalty \\(\\lambda\\). For \\(\\lambda\\) close to zero the estimates are close to least squares. As \\(\\lambda\\) increases the number of selected variables falls. Picking \\(\\lambda\\) induces a trade-off between complexity and parsimony.\nIt is common in the statistics literature to see coefficients plotted as a function of \\(\\lambda\\). This can be used to visualize the trade-off between parsimony and variable inclusion. It does not, however, provide a statistical rule for selection.\nThe most common selection method is minimization of K-fold cross validation (see Section 28.9). Leave-one-out CV is not typically used as it is computationally expensive. Many programs set the default number of folds as \\(K=10\\), though some authors use \\(K=5\\), while others recommend \\(K=20\\).\nK-fold cross validation is an estimator of out-of-sample mean squared forecast error. Therefore penalty selection by minimization of the K-fold criterion is aimed to select models with good forecast accuracy, but not necessarily for other purposes such as accurate inference.\nConventionally, the value of \\(\\lambda\\) selected by CV is the value which minimizes the CV criterion. Another popular choice is called the “1se” rule, which is the \\(\\lambda\\) which yields the most parsimonious model for \\(\\lambda\\) values within one standard error of the minimum. The idea is to select a model similar but more parsimonious than the CV-minimizing choice.\nK-fold cross validation is implemented by first randomly dividing the observations into \\(K\\) groups. Consequently the \\(\\mathrm{CV}\\) criterion is sensitive to the random sorting. It is therefore prudent to set the random number seed for replicability and to assess sensitivity across initial seeds. In general, selecting a larger value for \\(K\\) reduces this sensitivity.\nAsymptotic consistency of CV selection for Lasso estimation has been demonstrated by Chetverikov, Liao, and Chernozhukov (2021)."
  },
  {
    "objectID": "chpt29-ML.html#lasso-computation",
    "href": "chpt29-ML.html#lasso-computation",
    "title": "27  Machine Learning",
    "section": "27.10 Lasso Computation",
    "text": "27.10 Lasso Computation\nThe constraint representation of Lasso is minimization of a quadratic subject to linear inequality constraints. This can be implemented by standard quadratic programming which is computationally simple. For evaluation of the cross-validation function, however, it is useful to compute the entire Lasso path. For this a computationally appropriate method is the modified LARS algorithm. (LARS stands for least angle regression.)\nThe LARS algorithm produces a path of coefficients starting at the origin and ending at least squares. The sequence corresponds to the sequence of constraints \\(\\tau\\) which can be calculated by the absolute sum of the coefficients, but neither these values (nor \\(\\lambda\\) ) are used by the algorithm. The steps are as follows.\n\nStart with all coefficients equal to zero.\nFind \\(X_{j}\\) most correlated with \\(Y\\).\nIncrease \\(\\beta_{j}\\) in the direction of correlation.\n\n\nCompute residuals along the way.\nStop when some other \\(X_{\\ell}\\) has the same correlation with the residual as \\(X_{j}\\).\nIf a non-zero coefficient hits zero, drop from the active set of variables and recompute the joint least squares direction.\n\n 1. Increase \\(\\left(\\beta_{j}, \\beta_{\\ell}\\right)\\) in their joint least squares direction until some other \\(X_{m}\\) has the same correlation with the residual.\n\nRepeat until all predictors are in model.\n\nThis algorithm produces the Lasso path. The equality between the two is not immediately apparent. The demonstration is tedious so is not shown here.\nThe most popular computational implementation for Lasso is the R glmnet command in the glmnet package. Penalty selection by K-fold cross validation is implmented by the \\(\\mathrm{cv}\\).glmnet command. The latter by default reports the penalty selected by the “1se” rule, and reports the minimizing \\(\\lambda\\) as lambda .min. The default number of folds is \\(K=10\\).\nIn Stata, Lasso is available with the command lasso. By default it selects the penalty by minimizing the K-fold cross validation criterion with \\(K=10\\) folds. Many options are available, including constraining the estimator to penalize only a subset of the coefficients. An alternative downloadable package with many options is lassopack."
  },
  {
    "objectID": "chpt29-ML.html#asymptotic-theory-for-the-lasso",
    "href": "chpt29-ML.html#asymptotic-theory-for-the-lasso",
    "title": "27  Machine Learning",
    "section": "27.11 Asymptotic Theory for the Lasso",
    "text": "27.11 Asymptotic Theory for the Lasso\nThe current distribution theory of Lasso estimation is challenging and mostly focused on convergence rates. The results are derived under sparsity or approximate sparsity conditions, the former restricting the number of non-zero coefficients, and the second restricting how a sparse model can approximate a general parameterization. In this section we provide a basic convergence rate for the Lasso estimator \\(\\widehat{\\beta}_{\\text {Lasso }}\\) under a mild moment bound on the errors and a sparsity assumption on the coefficients.\nThe model is the high-dimensional projection framework:\n\\[\n\\begin{aligned}\nY &=X^{\\prime} \\beta+e \\\\\n\\mathbb{E}[X e] &=0\n\\end{aligned}\n\\]\nwhere \\(X\\) is \\(p \\times 1\\) with \\(p>>n\\). The true coefficient vector \\(\\beta\\) is assumed to be sparse in the sense that only a subset of the elements of \\(\\beta\\) are non-zero. For some \\(\\lambda\\) let \\(\\widehat{\\beta}_{\\text {Lasso }}\\) be the Lasso estimator which minimizes \\(\\operatorname{SSE}_{1}(\\beta, \\lambda)\\). Define the scaled design matrix \\(\\boldsymbol{Q}_{n}=n^{-1} \\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\) and the regression fit\n\\[\n\\left(\\widehat{\\beta}_{\\text {Lasso }}-\\beta\\right)^{\\prime} \\boldsymbol{Q}_{n}\\left(\\widehat{\\beta}_{\\text {Lasso }}-\\beta\\right)=\\frac{1}{n} \\sum_{i=1}^{n}\\left(X_{i}^{\\prime}\\left(\\widehat{\\beta}_{\\text {Lasso }}-\\beta\\right)\\right)^{2} .\n\\]\nWe provide a bound on the regression fit (29.12), the 1-norm fit \\(\\|\\widehat{\\beta}-\\beta\\|_{1}\\) and the 2-norm fit \\(\\|\\widehat{\\beta}-\\beta\\|_{2}\\).\nThe regression fit (29.12) is similar to measures of fit we have used before, including the integrated squared error (20.22) in series regression and the regression fit \\(R_{n}(K)\\) (equation (28.17)) for evaluation of model selection optimality.\nWhen \\(p>n\\) the matrix \\(\\boldsymbol{Q}_{n}\\) is singular. The theory, however, requires that it not be “too singular”. What is required is non-singularity of all sub-matrices of \\(\\boldsymbol{Q}_{n}\\) corresponding to the non-zero coefficients and not “too many” of the zero coefficients. The specific requirement is rather technical. Partition \\(\\beta=\\left(\\beta_{0}, \\beta_{1}\\right)\\) where the elements of \\(\\beta_{0}\\) are all 0 and the elements of \\(\\beta_{1}\\) are non-zero. (This partition is a theoretical device and unknown to the econometrician.) Let \\(b=\\left(b_{0}, b_{1}\\right) \\in \\mathbb{R}^{p}\\) be partitioned conformably. Define the cone \\(B=\\left\\{b \\in \\mathbb{R}^{p}:\\left\\|b_{0}\\right\\|_{1} \\leq 3\\left\\|b_{1}\\right\\|_{1}\\right\\}\\). This is the set of vectors \\(b\\) such that the sub-vector \\(b_{0}\\) is not “too large” relative to the sub-vector \\(b_{1}\\).\nAssumption 29.1 Restricted Eigenvalue Condition (REC) With probability approaching 1 as \\(n \\rightarrow \\infty\\)\n\\[\n\\min _{b \\in B} \\frac{b^{\\prime} \\boldsymbol{Q}_{n} b}{b^{\\prime} b} \\geq c^{2}>0 .\n\\]\nTo gain some understanding of what the REC means, notice that if the minimum (29.13) is taken without restriction over \\(\\mathbb{R}^{p}\\) it equals the smallest eigenvalue of \\(\\boldsymbol{Q}_{n}\\). Thus when \\(p<n\\) a sufficient condition for the REC is \\(\\lambda_{\\min }\\left(\\boldsymbol{Q}_{n}\\right) \\geq c^{2}>0\\). Instead, the minimum in (29.13) is calculated only over the cone B. In this sense this calculation is similar to a “restricted eigenvalue” which is the source of its name. The REC takes a variety of forms in the theoretical literautre; Assumption \\(29.1\\) is not the weakest but is the most intuitive. Assumption \\(29.1\\) has been shown to hold under primitive conditions on \\(X\\), including normality and boundedness. See Section 3 of Bickel, Ritov, and Tsybakov (2009) and Section \\(3.1\\) of Belloni, Chen, Chernozhukov, and Hansen (2012).\nWe provide a rate for the Lasso estimator under the assumption of normal errors. Theorem 29.3 Suppose model (29.11) holds with \\(p>1\\) and Assumption \\(29.1\\) holds. Assume that each regressor has been standardized so that \\(n^{-1} \\boldsymbol{X}_{j}^{\\prime} \\boldsymbol{X}_{j}=1\\) before applying the Lasso. Suppose \\(e \\mid X \\sim \\mathrm{N}\\left(0, \\sigma^{2}(X)\\right)\\) where \\(\\sigma^{2}(x) \\leq \\bar{\\sigma}^{2}<\\infty\\). For some \\(C\\) sufficiently large set\n\\[\n\\lambda=C \\sqrt{n \\log p}\n\\]\nThen there is \\(D<\\infty\\) such that with probability arbitrarily close to 1 ,\n\\[\n\\begin{gathered}\n\\left(\\widehat{\\beta}_{\\text {Lasso }}-\\beta\\right)^{\\prime} \\boldsymbol{Q}_{n}\\left(\\widehat{\\beta}_{\\text {Lasso }}-\\beta\\right) \\leq D\\|\\beta\\|_{0} \\frac{\\log p}{n}, \\\\\n\\left\\|\\widehat{\\beta}_{\\text {Lasso }}-\\beta\\right\\|_{1} \\leq D\\|\\beta\\|_{0} \\sqrt{\\frac{\\log p}{n}},\n\\end{gathered}\n\\]\nand\n\\[\n\\left\\|\\widehat{\\beta}_{\\text {Lasso }}-\\beta\\right\\|_{2} \\leq D\\|\\beta\\|_{0}^{1 / 2} \\sqrt{\\frac{\\log p}{n}} .\n\\]\nFor a proof see Section \\(29.23\\).\nTheorem \\(29.3\\) presents three convergence rates for the Lasso coefficient estimator \\(\\widehat{\\beta}_{\\text {Lasso }}\\), for the regression fit (29.12), covering the 1-norm, and the 2-norm. These rates depend on the number of non-zero coefficients \\(\\|\\beta\\|_{0}\\), the number of variables \\(p\\), and the sample size \\(n\\). Suppose that \\(\\|\\beta\\|_{0}\\) is fixed. Then the bounds (29.15)-(29.17) are \\(o(1)\\) if \\(\\log p=o(n)\\). This shows that Lasso estimation is consistent even for an exponentially large number of variables. The rates, however, allow the number of non-zero coefficients \\(\\|\\beta\\|_{0}\\) to increase with \\(n\\) at the cost of slowing the allowable rate of increase of \\(p\\).\nWe stated earlier in this section that we had assumed that the coefficient vector \\(\\beta\\) is sparse, meaning that only a subset of the elements of \\(\\beta\\) are non-zero. This appears in the theory through the 0 -norm \\(\\|\\beta\\|_{0}\\), the number of non-zero coefficients. If all elements of \\(\\beta\\) are non-zero then \\(\\|\\beta\\|_{0}=p\\) and the bound (29.15) is \\(O(p \\log p / n)\\), which is similar to bound (20.24) for series regression obtained in Theorem 20.7. Instead, the assumption of sparsity enables the Lasso estimator to achieve a much improved rate of convergence.\nThe key to establishing Theorem \\(29.3\\) is a maximal inequality applied to \\(\\frac{1}{n} \\boldsymbol{X}^{\\prime} \\boldsymbol{e}\\). Our proof uses the Gaussian Tail inequality (B.39) which requires the assumption of normality. This follows the analysis of Bickel, Ritov, and Tsybakov (2009), though these authors impose homoskedastic errors, which as shown in Theorem \\(29.3\\) can be replaced by the assumption of bounded heteroskedasticity. Other papers in the statistics literature (see the monograph of Bühlmann and van de Geer (2011)) use instead the assumption of sub-Gaussian tails, which is weaker than normality.\nA theory which allows for non-normal heteroskedastic errors has been developed by Belloni, Chen, Chernozhukov, and Hansen (2012). These authors examine an alternative Lasso estimator which adds regresssor-specific weights to the penalty function, with weights equalling the square roots of \\(n^{-1} \\sum X_{j i}^{2} \\widehat{e}_{i}^{2}\\). They use a maximal inequality based on self-normalization and obtain rates similar to (29.15)-(29.17), though with considerably more complicated regularity conditions. While their specific conditions may not be the weakest possible, their theory shows that the assumption of Gaussian or sub-Gaussian errors is not essential to the convergence rates (29.15)-(29.17). I expect that future research will further clarify the needed conditions. An important limitation of results such as Theorem \\(29.3\\) is the sparsity assumption. It is untestable and counter-intuitive. Researchers in this field frequently use the phrase “imposing sparsity” as if it is something under the control of the theorist - but sparsity is a property of the true coefficients, not a choice of the researcher. Fortunately there are alternatives to the sparsity assumption as we discuss in the following section."
  },
  {
    "objectID": "chpt29-ML.html#approximate-sparsity",
    "href": "chpt29-ML.html#approximate-sparsity",
    "title": "27  Machine Learning",
    "section": "27.12 Approximate Sparsity",
    "text": "27.12 Approximate Sparsity\nThe theory of the previous section used the strong assumption that the true regression is sparse: only a subset of the coefficients are non-zero, and the convergence rate depends on the cardinality of the nonzero coefficients. However, as shown by Belloni, Chen, Chernozhukov, and Hansen (2012), strict sparsity is not required. Instead, convergence rates similar to those in Theorem \\(29.3\\) hold under the assumption of approximate sparsity.\nOnce again take the high-dimensional regression model (29.11) but do not assume that \\(\\beta\\) necessarily has a sparse structure. Instead, view a sparse model as an approximation. For each integer \\(K>0\\) let \\(B_{K}=\\) \\(\\left\\{b \\in \\mathbb{R}^{p}:\\|b\\|_{0}=K\\right\\}\\) be the set of vectors with \\(K\\) non-zero elements. Define the best sparse approximation\n\\[\n\\beta_{K}=\\underset{b \\in B_{K}}{\\operatorname{argmin}}\\left\\|\\boldsymbol{Q}_{n}(\\beta-b)\\right\\|_{\\infty}\n\\]\nwith associated approximation error\n\\[\nr_{K}=\\left\\|\\boldsymbol{Q}_{n}\\left(\\beta-\\beta_{K}\\right)\\right\\|_{\\infty} .\n\\]\nAssumption 29.2 Approximate Sparsity. For some \\(s>1, r_{K}=O\\left(K^{-s}\\right)\\).\nAssumption \\(29.2\\) states that the approximation error of the sparse approximation decreases at a power law rate. In Section \\(20.8\\) and Theorem \\(20.1\\) we learned that approximations similar to Assumption \\(29.2\\) hold for polynomial and spline series regressions with bounded regressors if the true regression function has a uniform \\(s^{t h}\\) derivative. The primary difference is that series regression requires that the econometrician knows how to order the regressors while Assumption \\(29.2\\) does not impose a specific ordering. In this sense Assumption \\(29.2\\) is weaker than the approximation conditions of Section 20.8.\nBelloni, Chen, Chernozhukov, and Hansen (2012) show that convergence results similar to Theorem 29.3 hold under the approximate sparsity condition of Assumption 29.2. The convergence rates are slowed and depend on the approximation exponent \\(s\\). As \\(s \\rightarrow \\infty\\) the convergence rates approach those under the assumption of sparsity. The reason is that as \\(s\\) increases the regression function can be approximated with a smaller number \\(K\\) of non-zero coefficients. Their results show that exact sparsity is not required for Lasso estimation, rather what is needed is approximation properties similar to those used in series regression theory.\nThe approximate sparsity condition fails when the regressors cannot be easily ordered. Suppose, for example, that \\(\\boldsymbol{Q}_{n}=\\boldsymbol{I}_{p}\\) and all elements of \\(\\beta\\) have common value \\(\\delta\\). In this case \\(r_{K}=\\delta\\) which does not decrease with \\(K\\). In this context Assumption \\(29.2\\) does not hold and the convergence results of Belloni, Chen, Chernozhukov, and Hansen (2012) do not apply."
  },
  {
    "objectID": "chpt29-ML.html#elastic-net",
    "href": "chpt29-ML.html#elastic-net",
    "title": "27  Machine Learning",
    "section": "27.13 Elastic Net",
    "text": "27.13 Elastic Net\nThe difference between Lasso and ridge regression is that the Lasso uses the 1-norm penalty while ridge uses the 2-norm penalty. Since both procedures have advantages it seems reasonable that further improvements may be obtained by a compromise. Taking a weighted average of the penalties we obtain the Elastic Net criterion\n\\[\n\\operatorname{SSE}(\\beta, \\lambda, \\alpha)=(\\boldsymbol{Y}-\\boldsymbol{X} \\beta)^{\\prime}(\\boldsymbol{Y}-\\boldsymbol{X} \\beta)+\\lambda\\left(\\alpha\\|\\beta\\|_{2}^{2}+(1-\\alpha)\\|\\beta\\|_{1}\\right)\n\\]\nwith weight \\(0 \\leq \\alpha \\leq 1\\). This includes Lasso \\((\\alpha=0)\\) and ridge regression \\((\\alpha=1)\\) as special cases. For small but positive \\(\\alpha\\) the constraint sets are similar to “rounded” versions of the Lasso constraint sets.\nTypically the parameters \\((\\alpha, \\lambda)\\) are selected by joint minimization of the K-fold cross-validation criterion. Since the elastic net penalty is linear-quadratic the solution is computationally similar to Lasso.\nElastic net can be implemented in R with the glmnet command. In Stata use elasticnet or the downloadable package lassopack."
  },
  {
    "objectID": "chpt29-ML.html#post-lasso",
    "href": "chpt29-ML.html#post-lasso",
    "title": "27  Machine Learning",
    "section": "27.14 Post-Lasso",
    "text": "27.14 Post-Lasso\nThe Lasso estimator \\(\\widehat{\\beta}_{\\text {Lasso }}\\) simultaneously selects variables and shrinks coefficients. Shrinkage introduces bias into estimation. This bias can be reduced by applying least squares after Lasso selection. This is known as the Post-Lasso estimator.\nThe procedure takes two steps. First, estimate the model \\(Y=X^{\\prime} \\beta+e\\) by Lasso. Let \\(X_{S}\\) denote the variables in \\(X\\) which have non-zero coefficients in \\(\\widehat{\\beta}_{\\text {Lasso }}\\). Let \\(\\beta_{S}\\) denote the corresponding coefficients in \\(\\beta\\). Second, the coefficient \\(\\beta_{S}\\) is estimated by least squares, thus \\(\\widehat{\\beta}_{S}=\\left(\\boldsymbol{X}_{S}^{\\prime} \\boldsymbol{X}_{S}\\right)^{-1}\\left(\\boldsymbol{X}_{S}^{\\prime} \\boldsymbol{Y}\\right)\\). This is the Post-Lasso least squares estimator. Belloni and Chernozhukov (2013) provide conditions under which the post-Lasso estimator has the same convergence rates as the Lasso estimator.\nThe post-Lasso is a hard thresholding or post-model-selection estimator. Indeed, when the regressors are orthogonal the post-Lasso estimator precisely equals a selection estimator, transforming the least squares coefficient estimates using the hard threshold function displayed in Figure 29.4(a). Consequently, the post-Lasso estimator inherits the statistical properties of PMS estimators (see Sections \\(28.16\\) and 28.17), including high variance and non-standard distributions."
  },
  {
    "objectID": "chpt29-ML.html#regression-trees",
    "href": "chpt29-ML.html#regression-trees",
    "title": "27  Machine Learning",
    "section": "27.15 Regression Trees",
    "text": "27.15 Regression Trees\nRegression trees were introduced by Breiman, Friedman, Olshen, and Stone (1984), and are also known by the acronym CART for Classification and Regression Trees. A regression tree is a nonparametric regression using a large number of step functions. The idea is that with a sufficiently large number of split points, a step function can approximate any function. Regression trees may be especially useful when there are a combination of continuous and discrete regressors so that traditional kernel and series methods are challenging to implement.\nRegression trees can be thought of as a \\(0^{t h}\\)-order spline with free knots. They are also similar to threshold regression with intercepts only (no slope coefficients) and a very large number of thresholds.\nThe goal is to estimate \\(m(x)=\\mathbb{E}[Y \\mid X=x]\\) for scalar \\(Y\\) and vector \\(X\\). The elements of \\(X\\) can be continuous, binary, or ordinal. If a regressor is categorical is should be first transformed to a set of binary variables.\nThe literature on regression trees has developed some colorful language to describe the tools based on the metaphor of a living tree. 1. A subsample is a branch.\n 1. Terminal branches are nodes or leaves.\n\nIncreasing the number of branches is growing a tree.\nDecreasing the number of branches is pruning a tree.\n\nThe basic algorithm starts with a single branch. Grow a large tree by sequentially splitting the branches. Then prune back using an information criterion. The goal of the growth stage is to develop a rich datadetermined tree which has small estimation bias. Pruning back is an application of backward stepwise regression with the goal of reducing over-parameterization and estimation variance.\nThe regression tree algorithm makes extensive use of the regression sample split algorithm. This is a simplified version of threshold regression (Section 23.7). The method uses NLLS to estimate the model\n\\[\n\\begin{aligned}\nY &=\\mu_{1} \\mathbb{1}\\left\\{X_{d} \\leq \\gamma\\right\\}+\\mu_{2} \\mathbb{1}\\left\\{X_{d}>\\gamma\\right\\}+e \\\\\n\\mathbb{E}[e \\mid X] &=0\n\\end{aligned}\n\\]\nwith the index \\(d\\) and parameter \\(\\gamma\\) as free parameters \\({ }^{2}\\). The NLLS criterion is minimized over \\((d, \\gamma)\\) by grid search. The estimates produce a sample split. The regression tree algorithm applies sequential sample splitting to make a large number of splits, each on a sub-sample of observations.\nThe basic growth algorithm is as follows. The observations are \\(\\left\\{Y_{i}, X_{1 i}, \\ldots, X_{k i}: i=1, \\ldots, n\\right\\}\\).\n\nSelect a minimum node size \\(N_{\\min }\\) (say 5). This is the minimal number of observations on each leaf.\nSequentially apply regression sample splits.\n\n\nApply the regression sample split algorithm to split each branch into two sub-branches, each with size at least \\(N_{\\min }\\).\nOn each sub-branch \\(b\\) :\n\n\nTake the sample mean \\(\\widehat{\\mu}_{b}\\) of \\(Y_{i}\\) for observations on the sub-branch.\nThis is the estimator of the regression function on this sub-branch.\nThe residuals on the sub-branch are \\(\\widehat{e}_{i}=Y_{i}-\\widehat{\\mu}_{b}\\).\n\n\nSelect the branch whose split most reduces the sum of squared errors.\nSplit this branch into two branches. Make no other split.\nRepeat (a)-(d) until each branch cannot be further split. The terminal (unsplit) branches are the leaves.\n\nAfter the growth algorithm has been run, the estimated regression is a multi-dimensional step function with a large number of branches and leaves.\nThe basic pruning algorithm is as follows.\n\nDefine the Mallows-type information criterion\n\n\\[\nC=\\sum_{i=1}^{n} \\widehat{e}_{i}^{2}+\\alpha N\n\\]\nwhere \\(N\\) is the number of leaves and \\(\\alpha\\) is a penalty parameter.\n\\({ }^{2}\\) If \\(X_{d} \\in\\{0,1\\}\\) is binary then \\(\\gamma=0\\) is fixed. 2. Compute the criterion \\(C\\) for the current tree.\n 1. Use backward stepwise regression to reduce the number of leaves:\n\nIdentify the leaf whose removal most decreases \\(C\\).\nPrune (remove) this leaf.\nIf there is no leaf whose removal decreases \\(C\\) then stop pruning.\nOtherwise, repeat (a)-(c).\n\nThe penalty parameter \\(\\alpha\\) is typically selected by K-fold cross-validation. The Mallows-type criterion is used because of its simplicity, but to my knowledge does not have a theoretical foundation for regression tree penalty selection.\nThe advantage of regression trees is that they provide a highly flexible nonparametric approximation. Their main use is prediction. One disadvantage of regression trees is that the results are difficult to interpret as there are no regression coefficients. Another disadvantage is that the fitted regression \\(\\widehat{m}(x)\\) is a discrete step function, which may be a crude approximation when \\(m(x)\\) is continuous and smooth. To obtain a good approximation a regression tree may require a high number of leaves which can result in a non-parsimonious model with high estimation variance.\nThe sampling distribution of regression trees is difficult to derive, in part because of the strong correlation between the placement of the sample splits and the estimated means. This is similar to the problems associated with post-model-selection. (See Sections \\(28.16\\) and 28.17.) A method which breaks this dependence is the honest tree proposal of Wager and Ather (2018). Split the sample into two halves \\(A\\) and \\(B\\). Use the \\(A\\) sample to place the splits and the \\(B\\) sample to do within-leaf estimation. While reducing estimation efficiency (the sample is effectively halved) the estimated conditional mean will not be distorted by the correlation between the estimated splits and means.\nRegression trees algorithms are implemented in the R package rpart."
  },
  {
    "objectID": "chpt29-ML.html#bagging",
    "href": "chpt29-ML.html#bagging",
    "title": "27  Machine Learning",
    "section": "27.16 Bagging",
    "text": "27.16 Bagging\nBagging (bootstrap aggregating) was introduced by Breiman (1996) as a method to reduce the variance of a predictor. We focus here on its use for estimation of a conditional expectation. The basic idea is simple. You generate a large number \\(B\\) of bootstrap samples, estimate your regression model on each bootstrap sample, and take the average of the bootstrap regression estimates. The average of the bootstrap estimates is the bagging estimator of the CEF.\nBagging is believed to be useful when the CEF estimator has low bias but high variance. This occurs for hard thresholding estimators such as regression trees, model selection, and post-Lasso. Bagging is a smoothing operation which reduces variance. The resulting bagging estimator can have lower MSE as a result. Bagging is believed to be less useful for estimators with high bias, as bagging may exaggerate the bias.\nWe first describe the estimation algorithm. Let \\(m(x)=\\mathbb{E}[Y \\mid X=x]\\) be the CEF and \\(\\widehat{m}(x)\\) an estimator such as a regression tree. Let \\(\\widehat{m}_{b}^{*}(x)\\) be the same estimator constructed on a bootstrap sample. The bagging estimator of \\(m(x)\\) is\n\\[\n\\widehat{m}_{\\text {bag }}(x)=\\frac{1}{B} \\sum_{B=1}^{b} \\widehat{m}_{b}^{*}(x)\n\\]\nAs \\(B\\) increases this converges in bootstrap probability to the ideal bagging estimator \\(\\mathbb{E}^{*}\\left[\\widehat{m}^{*}(x)\\right]\\).\nTo understand the bagging process we use an example from Bühlmann and Yu (2002). As in Section \\(28.16\\) suppose that \\(\\widehat{\\theta} \\sim \\mathrm{N}(\\theta, 1)\\) and consider a selection estimator based on a \\(5 %\\) test, \\(\\widehat{\\theta}_{\\mathrm{pms}}=\\widehat{\\theta} \\mathbb{1}\\left\\{\\widehat{\\theta}^{2} \\geq c\\right\\}=\\) \\(h(\\widehat{\\theta})\\) where \\(c=3.84\\) and \\(h(t)=t \\mathbb{1}\\left\\{t^{2} \\geq c\\right\\}\\). Applying Theorem 28.17, equation (28.38), we can calculate that \\(\\mathbb{E}\\left[\\widehat{\\theta}_{\\mathrm{pms}}\\right]=g(\\theta)\\) where \\(g(t)=t\\left(1-F_{3}\\left(c, t^{2}\\right)\\right)\\) and \\(F_{r}(x, \\lambda)\\) is the non-central chi-square distribution function \\({ }^{3}\\). This representation is not intuitive so it is better to visualize its graph. The functions \\(h(t)\\) and \\(g(t)\\) are plotted in Figure 29.5(a). The selection function \\(h(t)\\) is identical to the plot in Figure 29.4(a). The function \\(g(t)\\) is a smoothed version of \\(h(t)\\), everywhere continuous and differentiable.\nSuppose that the bagging estimator is constructed using the parametric bootstrap \\(\\widehat{\\theta}^{*} \\sim \\mathrm{N}(\\widehat{\\theta}, 1)\\). The bootstrap selection estimator is \\(\\widehat{\\theta}_{\\mathrm{pms}}^{*}=h\\left(\\widehat{\\theta}^{*}\\right)\\). It follows that the bagging estimator is \\(\\widehat{\\theta}_{\\mathrm{bag}}=\\mathbb{E}^{*}\\left[\\widehat{\\theta}_{\\mathrm{pms}}^{*}\\right]=\\) \\(\\mathbb{E}^{*}\\left[h\\left(\\widehat{\\theta}^{*}\\right)\\right]=g(\\widehat{\\theta})\\). Thus while the selection estimator \\(\\widehat{\\theta}_{\\mathrm{pms}}=h(\\widehat{\\theta})\\) is the hard threshold transformation \\(h(t)\\) applied to \\(\\widehat{\\theta}\\), the bagging estimator \\(\\widehat{\\theta}_{\\mathrm{bag}}=g(\\widehat{\\theta})\\) is the smoothed transformation \\(g(t)\\) applied to \\(\\widehat{\\theta}\\). Thus Figure 29.5(a) displays how \\(\\widehat{\\theta}_{\\mathrm{pms}}\\) and \\(\\hat{\\theta}_{\\mathrm{bag}}\\) are transformations of \\(\\widehat{\\theta}\\), with the bagging estimator a smooth transformation rather than a hard threshold transformation.\n\nSelection and Bagging Transformations\n\n\nMSE of Selection and Bagging Estimators\n\nFigure 29.5: Bagging and Selection\nBühlmann and Yu (2002) argue that smooth transformations generally have lower variances than hard threshold transformations, and thus argue that \\(\\widehat{\\theta}_{\\text {bag }}\\) will generally have lower variance than \\(\\widehat{\\theta}_{\\mathrm{pms}}\\). This is difficult to demonstrate as a general principle but seems satisfied in specific examples. For our example we display \\({ }^{4}\\) in Figure 29.5(b) the MSE of the selection estimator \\(\\widehat{\\theta}_{\\mathrm{pms}}\\) and its bagged verion \\(\\widehat{\\theta}_{\\text {bag }}\\) as functions of \\(\\theta\\). As we learned in Section 28.16, the MSE of the selection estimator \\(\\widehat{\\theta}_{\\text {pms }}\\) is a humpshaped function of \\(\\theta\\). In Figure 29.5(b) we can see that the MSE of the bagged estimator is considerably reduced relative to the selection estimator for most values of \\(\\theta\\). The reduction in MSE is greatest in the region where the MSE of \\(\\widehat{\\theta}_{\\text {pms }}\\) is greatest. Bühlmann and Yu (2002) also calculate that most of this MSE reduction is due to a reduction in the variance of the bagged estimator.\nThe most common application of bagging is to regression trees. Trees have a similar structure to our example selection estimator \\(\\widehat{\\theta}_{\\mathrm{pms}}\\) and are therefore expected to have a similar reduction in estimation variance and MSE relative to regression tree estimation.\n\\({ }^{3}\\) Bühlmann and Yu (2002), Proposition 2.2, provide an alternative representation using the normal cdf and pdf functions.\n\\({ }^{4}\\) For \\(\\widehat{\\theta}_{\\text {pms }}\\) the MSE is calculated using Theorem 28.10. For \\(\\widehat{\\theta}_{\\text {bag }}\\) the MSE is calculated by numerical integration. One convenient by-product of bagging is a CV proxy called the out-of-bag (OOB) prediction error. A typical nonparametric bootstrap sample contains about \\(63 %\\) of the original observations, meaning that about \\(37 %\\) of the observations are not present in that bootstrap sample. Therefore a bootstrap estimate of the regression function \\(m(x)\\) constructed on this bootstrap sample has “left out” about \\(37 %\\) of the observations, meaning that valid prediction errors can be calculated on these “left out” observations. Alternatively, for any given observation \\(i\\), out of the \\(B\\) bootstrap samples about \\(0.63 \\times B\\) samples will contain this observation and about \\(0.37 \\times B\\) samples will not include this observation. The bagging “leave \\(i\\) out” estimator \\(\\widehat{m}_{-i}(x)\\) of \\(m(x)\\) is obtained by averaging just this second set (the \\(37 %\\) which exclude the observation). The out-of-bag error is \\(\\widetilde{e}_{i}=Y_{i}-\\widehat{m}_{-i}\\left(X_{i}\\right)\\). The out-of-bag CV criterion is \\(\\sum_{i=1}^{n} \\widetilde{e}_{i}^{2}\\). This can be used as an estimator of out-of-sample MSFE and can be used to compare and select models.\nWager, Hastie, and Efron (2014) propose estimators of \\(V_{n}(x)=\\operatorname{var}\\left[\\widehat{m}_{\\text {bag }}(x)\\right]\\). Let \\(N_{i b}\\) denote the number of times observation \\(i\\) appears in the bootstrap sample \\(b\\) and \\(N_{i}=B^{-1} \\sum_{b=1}^{B} N_{i b}\\). The infinitesimal jackknife estimator of \\(V_{n}\\) is\n\\[\n\\widehat{V}_{n}(x)=\\sum_{i=1}^{n} \\operatorname{cov}^{*}\\left(N_{i}, \\widehat{m}_{\\text {bag }}(x)\\right)^{2}=\\sum_{i=1}^{n}\\left(\\frac{1}{B} \\sum_{b=1}^{B}\\left(N_{i b}-N_{i}\\right)\\left(\\widehat{m}_{b}^{*}(x)-\\widehat{m}_{\\text {bag }}(x)\\right)\\right)^{2} .\n\\]\nThis variance estimator is based on Efron (2014).\nWhile Breiman’s proposal and most applications of bagging are implemented using the nonparametric bootstrap, an alternative is to use subsampling. A subsampling estimator is based on sampling without replacement rather than with replacement as done in the conventional bootstrap. Samples of size \\(s<n\\) are drawn from the original sample and used to construct the estimator \\(\\widehat{m}_{b}^{*}(x)\\). Otherwise the methods are identical. It turns out that it is somewhat easier to develop a distribution theory for bagging under subsampling, so a subsampling assumption is frequently employed in theoretical treatments."
  },
  {
    "objectID": "chpt29-ML.html#random-forests",
    "href": "chpt29-ML.html#random-forests",
    "title": "27  Machine Learning",
    "section": "27.17 Random Forests",
    "text": "27.17 Random Forests\nRandom forests, introduced by Breiman (2001), are a modification of bagged regression trees. The modification is designed to reduce estimation variance. Random forests are popular in machine learning applications and have effectively displaced simple regression trees.\nConsider the procedure of applying bagging to regression trees. Since bootstrap samples are similar to one another the estimated bootstrap regression trees will also be similar to one another, particularly in the sense that they tend to have the splits based on the same variables. This means that conditional on the sample the bootstrap regression trees are positively correlated. This correlation means that the variance of the bootstrap average remains high even when the number of bootstrap replications \\(B\\) is large. The modification proposed by random forests is to decorrelate the bootstrap regression trees by introducing extra randomness. This decorrelation reduces the variance of the bootstrap average, thereby reducing its MSE.\nThe basic random forest algorithm is as follows. The recommended defaults are taken from the description in Hastie, Tibshirani, and Friedman (2008).\n\nPick a minimum leaf size \\(N_{\\min }\\) (default \\(=5\\) ), a minimal split fraction \\(\\alpha \\in[0,1\\) ), and a sampling number \\(m<p\\) (default \\(=p / 3\\) ).\nFor \\(b=1, \\ldots, B\\) :\n\n\nDraw a nonparametric bootstrap sample.\nGrow a regression tree on the bootstrap sample using the following steps: i. Select \\(m\\) variables at random from the \\(p\\) regressors.\n\n\nAmong these \\(m\\) variables, pick the one which produces the best regression split, where each split subsample has at least \\(N_{\\min }\\) observations and at least a fraction \\(\\alpha\\) of the observations in the branch.\nSplit the bootstrap sample accordingly.\n\n\nStop when each leaf has between \\(N_{\\min }\\) and \\(2 N_{\\min }-1\\) observations.\nSet \\(\\widehat{m}_{b}(x)\\) as the sample mean of \\(Y\\) on each leaf of the bootstrap tree.\n\n 1. \\(\\widehat{m}_{\\mathrm{rf}}(x)=B^{-1} \\sum_{B=1}^{b} \\widehat{m}_{b}(x)\\).\nUsing randomization to reduce the number of variables from \\(p\\) to \\(m\\) at each step alters the tree structure and thereby reduces the correlation between the bootstrapped regression trees. This reduces the variance of the bootstrap average.\nThe infinitesimal jackknife (29.18) can be used for variance and standard error estimation, as discussed in Wager, Hastie, and Efron (2014).\nWhile random forests are popular in applications, a distributional theory has been slow to develop. Some of the more recent results have made progress by focusing on random forests generated by subsampling rather than bootstrap (see the discussion at the end of the previous section).\nA variant proposed by Wager and Athey (2018) is to use honest trees (see the discussion at the end of Section 29.15) to remove the dependence between the sample splits and the sample means.\nConsistency and asymptotic normality has been established by Wager and Athey (2018). They assume that the conditional expectation and variance are Lipschitz-continuous in \\(x, X \\sim U[0,1]^{p}\\), and \\(p\\) is fixed \\({ }^{5}\\). They assume that the random forest is created by subsampling, estimated by honest trees, and that the minimal split fraction satisfies \\(0<\\alpha \\leq 0.2\\). Under these conditions they establish that pointwise in \\(x\\)\n\\[\n\\frac{\\widehat{m}_{\\mathrm{rf}}(x)-m(x)}{\\sqrt{V_{n}(x)}} \\underset{d}{\\longrightarrow} \\mathrm{N}(0,1)\n\\]\nfor some variance sequence \\(V_{n}(x) \\rightarrow 0\\). These results justify inference for random forest estimation of the regression function and standard error calculation. The asymptotic distribution does not contain a bias component, indicating that the estimator is undersmoothed. The Wager-Athey conditions for asymptotic normality are surprisingly weak. The theory does not give insight, however, into the convergence rate of the estimator. The essential idea of the result is as follows. The splitting algorithm and restrictions ensure that the regressor space is (in a rough sense) evenly split into \\(N \\sim n^{\\gamma}\\) leaves which grows at a power rate. This ensures that the estimator is asymptotically unbiased. With suitable control over \\(\\gamma\\) the squared bias can be made smaller than the variance. The assumption that \\(\\alpha>0\\) ensures that the number of observations per leaf increases with \\(n\\). When combined with the honest tree construction, this ensures asymptotic normality of the estimator.\nFurthermore, Wager and Athey (2018) assert (but do not provide a proof) that the variance \\(V_{n}(x)\\) can be consistently estimated by the infinitesimal jackknife (29.18), in the sense that \\(\\widehat{V}_{n}(x) / V_{n}(x) \\underset{p}{\\longrightarrow} 1\\).\nThe standard computational implementation of random forests is the R randomForest command."
  },
  {
    "objectID": "chpt29-ML.html#ensembling",
    "href": "chpt29-ML.html#ensembling",
    "title": "27  Machine Learning",
    "section": "27.18 Ensembling",
    "text": "27.18 Ensembling\nEnsembling is the term used in machine learning for model averaging across machine learning algorithms. Ensembling is popular in applied machine learning.\n\\({ }^{5}\\) The authors claim that the uniform distribution assumption on \\(X\\) can be replaced by the condition that the joint density is bounded away from 0 and infinity. Suppose you have a set of estimators (e.g., CV selection, James-Stein shrinkage, JMA, SBIC, PCA, kernel regression, series regression, ridge regression, Lasso, regression tree, bagged regression tree, and random forest). Which should you use? It is reasonable to expect that one method may work well with some types of data and other methods may work well with other types of data. The principle of model averaging suggests that you can do better by taking a weighted average rather than just selecting one.\nWe discussed model averaging models in Sections 28.26-28.31. Ensembling for machine learning can use many of the same methods. One popular method known as stacking is the same as Jackknife Model Averaging discussed in Section 28.29. This selects the model averaging weights by minimizing a cross-validation criterion, subject to the constraint that the weights are non-negative and sum to one.\nUnfortunately, the theoretical literature concerning ensembling is thin. Much of the advice concerning specific methods is based on empirical performance."
  },
  {
    "objectID": "chpt29-ML.html#lasso-iv",
    "href": "chpt29-ML.html#lasso-iv",
    "title": "27  Machine Learning",
    "section": "27.19 Lasso IV",
    "text": "27.19 Lasso IV\nBelloni, Chen, Chernozhukov, and Hansen (2012) propose Lasso for estimation of the reduced form of an instrumental variables regression.\nThe model is linear IV\n\\[\n\\begin{aligned}\nY &=X^{\\prime} \\beta+e \\\\\n\\mathbb{E}[e \\mid Z] &=0 \\\\\nX &=\\Gamma^{\\prime} Z+U \\\\\n\\mathbb{E}[U \\mid Z] &=0\n\\end{aligned}\n\\]\nwhere \\(\\beta\\) is \\(k \\times 1\\) (fixed) and \\(\\Gamma\\) is \\(p \\times n\\) with \\(p\\) large. If \\(p>n\\) the 2SLS estimator equals least squares. If \\(p<n\\) but large the 2SLS estimator suffers from the “many instruments” problem. The authors’ recommendation is to estimate \\(\\Gamma\\) by Lasso or post-Lasso 6 .\nThe reduced form equations for the endogenous regressors are \\(X_{j}=\\gamma_{j}^{\\prime} Z+U_{j}\\). Each is estimated separately by Lasso yielding coefficient estimates \\(\\widehat{\\gamma}_{j}\\) which are stacked into the matrix \\(\\widehat{\\Gamma}_{\\text {Lasso }}\\) and used to form the predicted values \\(\\widehat{\\boldsymbol{X}}_{\\text {Lasso }}=Z \\widehat{\\Gamma}_{\\text {Lasso }}\\). The Lasso IV estimator is\n\\[\n\\widehat{\\beta}_{\\text {Lasso-IV }}=\\left(\\widehat{\\boldsymbol{X}}_{\\text {Lasso }}^{\\prime} \\boldsymbol{X}\\right)^{-1}\\left(\\widehat{\\boldsymbol{X}}_{\\text {Lasso }}^{\\prime} \\boldsymbol{Y}\\right) .\n\\]\nThe paper discusses alternative formulations. One is obtained by split-sample estimation as in Angrist and Krueger (1995) (see Section 12.14). Divide the sample randomly into two independent halves \\(A\\) and \\(B\\). Use \\(A\\) to estimate the reduce form equations by Lasso. Then use \\(B\\) to estimate the structural coefficient \\(\\beta\\). Specifically, using sample \\(A\\) construct the Lasso coefficient estimate matrix \\(\\widehat{\\Gamma}_{\\text {Lasso, } A}\\). Combine this with sample \\(B\\) to create the predicted values \\(\\widehat{\\boldsymbol{X}}_{\\text {Lasso }, B}=Z_{B} \\widehat{\\Gamma}_{\\text {Lasso, } A}\\). Finally, using \\(B\\) construct the estimator\n\\[\n\\widehat{\\beta}_{\\text {Lasso }, B}=\\left(\\widehat{\\boldsymbol{X}}_{\\text {Lasso }, B}^{\\prime} \\boldsymbol{X}_{B}\\right)^{-1}\\left(\\widehat{\\boldsymbol{X}}_{\\text {Lasso }, B}^{\\prime} \\boldsymbol{Y}_{B}\\right) .\n\\]\nWe can reverse the procedure. Use \\(B\\) to estimate the reduced form coefficient matrix \\(\\widehat{\\Gamma}_{\\text {Lasso }, B}\\) by Lasso and use \\(A\\) to estimate the structural coefficient, thus \\(\\widehat{\\boldsymbol{X}}_{\\text {Lasso, } A}=\\boldsymbol{Z}_{A} \\widehat{\\Gamma}_{\\text {Lasso, } B}\\). The moments are averaged to obtain the Lasso SSIV estimator\n\\[\n\\widehat{\\beta}_{\\text {Lasso-SSIV }}=\\left(\\widehat{\\boldsymbol{X}}_{\\text {Lasso }, B}^{\\prime} \\boldsymbol{X}_{B}+\\widehat{\\boldsymbol{X}}_{\\text {Lasso }, A}^{\\prime} \\boldsymbol{X}_{A}\\right)^{-1}\\left(\\widehat{\\boldsymbol{X}}_{\\text {Lasso }, B}^{\\prime} \\boldsymbol{Y}_{B}+\\widehat{\\boldsymbol{X}}_{\\text {Lasso }, A}^{\\prime} \\boldsymbol{Y}_{A}\\right) .\n\\]\n\\({ }^{6}\\) As they discuss, any machine learning estimator can be used, though the specific assumptions listed in their paper are for Lasso estimation. In later work (see Section 29.22) the authors describe \\(\\widehat{\\beta}_{\\text {Lasso, } B}\\) as a “sample split” and \\(\\widehat{\\beta}_{\\text {Lasso-SSIV }}\\) as a “cross-fit” estimator.\nUsing the asymptotic theory for Lasso estimation the authors show that these estimators are equivalent to estimation using the infeasible instrument \\(W=\\Gamma^{\\prime} Z\\).\nTheorem 29.4 Under the Assumptions listed in Theorem 3 of Belloni, Chen, Chernozhukov, and Hansen (2012), including\n\\[\n\\|\\Gamma\\|_{0} \\frac{\\log p}{\\sqrt{n}} \\rightarrow 0,\n\\]\nthen\n\\[\n\\left(\\boldsymbol{Q}^{-1} \\Omega \\boldsymbol{Q}^{-1}\\right) \\sqrt{n}\\left(\\widehat{\\beta}_{\\text {Lasso-IV }}-\\beta\\right) \\underset{d}{\\rightarrow} \\mathrm{N}\\left(0, \\boldsymbol{I}_{k}\\right)\n\\]\nwhere \\(\\boldsymbol{Q}=\\mathbb{E}\\left[W W^{\\prime}\\right], \\Omega=\\mathbb{E}\\left[W W^{\\prime} e^{2}\\right]\\), and \\(W=\\Gamma^{\\prime} Z\\). Furthermore, the standard covariance matrix estimators are consistent for the asymptotic covariance matrix. The same distribution result holds for \\(\\widehat{\\beta}_{\\text {Lasso-SSIV }}\\) under the assumptions listed in their Theorem 7. In particular, (29.19) is replaced by\n\\[\n\\|\\Gamma\\|_{0} \\frac{\\log p}{n} \\rightarrow 0 .\n\\]\nFor a sketch of the proof see Section \\(29.23\\).\nEquation (29.19) requires that the reduced form coefficient \\(\\Gamma\\) is sparse in the sense that the number of non-zero reduced form coefficients \\(\\|\\Gamma\\|_{0}\\) grows more slowly than \\(\\sqrt{n}\\). This allows for \\(p\\) to grow exponentially with \\(n\\) but at a somewhat slower rate than allowed by Theorem 29.3. Condition (29.19) is one of the key assumptions needed for the distribution result (29.20).\nFor Lasso SSIV, equation (29.21) replaces (29.19). This rate condition is weaker, allowing \\(p\\) to grow at the same rate as for regression estimation. The difference is due to the split-sample estimation, which breaks the dependence between the reduced form coefficient estimates and the second-stage structural estimates. There are two interpretable implications of the difference between (29.19) and (29.21). First, a direct implication is that Lasso SSIV allows for larger number of variables \\(p\\). Second, an indirect implication is that for any set of variables, Lasso SSIV will have reduced bias relative to Lasso IV. Both interpretations suggest that Lasso SSIV is the preferred estimator.\nBelloni, Chen, Chernozhukov, and Hansen (2012) extend Theorem \\(29.4\\) to allow for approximate sparsity as in Section \\(29.12\\) at the cost of more restrictive rate conditions.\nAn important disadvantage of the split-sample and cross-fit estimators is that they depend on the random sorting of the observations into the samples \\(A\\) and \\(B\\). Consequently, two researchers will obtain two different estimators. Furthermore, the split-sample estimators use \\(n / 2\\) observations rather than \\(n\\), which may impact finite-sample performance. A deduction is that the split-sample estimators are not appropriate when \\(n\\) is small.\nIV Lasso can be implemented in Stata using the downloadable package ivlasso."
  },
  {
    "objectID": "chpt29-ML.html#double-selection-lasso",
    "href": "chpt29-ML.html#double-selection-lasso",
    "title": "27  Machine Learning",
    "section": "27.20 Double Selection Lasso",
    "text": "27.20 Double Selection Lasso\nPost-estimation inference is difficult with most machine learning estimators. For example, consider the post-Lasso estimator (least squares applied to the regressors selected by the Lasso). This is a post- model-selection (PMS) estimator, as discussed in Sections \\(28.16\\) and 28.17. As shown in Section 28.17, the coverage probability of standard confidence intervals applied to PMS estimators can be far from the nominal level. Belloni, Chernozhukov, and Hansen (2014b) proposed an alternative estimation and inference method which achives better coverage rates.\nConsider the linear model\n\\[\n\\begin{aligned}\nY &=D \\theta+X^{\\prime} \\beta+e \\\\\n\\mathbb{E}[e \\mid D, X] &=0\n\\end{aligned}\n\\]\nwhere \\(Y\\) and \\(D\\) are scalar and \\(X\\) is \\(p \\times 1\\). The variable \\(D\\) is the main focus of the regression; the variable \\(X\\) are controls. The goal is inference on \\(\\theta\\).\nSuppose you estimate model (29.22) by group post-Lasso, only penalizing \\(\\beta\\). This performs selection on the variables \\(X\\), resulting in a least squares regression of \\(Y\\) on \\(D\\) and the selected variables in \\(X\\). This is identical to the model studied in Section \\(28.17\\) (except that in that analysis selection was performed by testing), where Figure 28.1 (c) shows that the coverage probabilities for \\(\\theta\\) are downward biased, and the distortions are serious. The distortions are primarily affected by (and increasing in) the correlation between \\(D\\) and \\(X\\).\nBelloni, Chernozhukov, and Hansen (2014b) deduce that improved coverage accuracy can be achieved if the variable \\(X\\) is included in the regression (29.22) whenever \\(X\\) and \\(D\\) are correlated. This gives rise to the practical suggestion to perform what they call double-selection. We start by specifying an auxiliary equation for \\(D\\) :\n\\[\n\\begin{aligned}\nD &=X^{\\prime} \\gamma+V \\\\\n\\mathbb{E}[V \\mid X] &=0 .\n\\end{aligned}\n\\]\nSubstituting (29.23) into (29.22) we obtain a reduced form for \\(Y\\) :\n\\[\n\\begin{aligned}\nY &=X^{\\prime} \\eta+U \\\\\n\\mathbb{E}[U \\mid X] &=0\n\\end{aligned}\n\\]\nwhere \\(\\eta=\\beta+\\gamma \\theta\\) and \\(U=e+V \\theta\\). The proposed double-selection algorithm applies model selection (e.g., Lasso selection) separately to equations (29.23) and (29.24), takes the union of the selected regressors, and then estimates (29.22) by least squares using the selected regressors. This method ensures that a variable \\(X\\) is included if it is relevant for the regression (29.22) or if it is correlated with \\(D\\).\nThe double-selection estimator as recommended by Belloni, Chernozhukov, and Hansen (2014b) is:\n\nEstimate (29.23) by Lasso. Let \\(X_{1}\\) be the selected variables from \\(X\\).\nEstimate (29.24) by Lasso. Let \\(X_{2}\\) be the selected variables from \\(X\\).\nLet \\(\\widetilde{X}=X_{1} \\cup X_{2}\\) be the union of the variables in \\(X_{1}\\) and \\(X_{2}\\).\nRegress \\(Y\\) on \\((D, \\widetilde{X})\\) to obtain the double-selection coefficient estimate \\(\\widehat{\\theta}_{\\mathrm{DS}}\\).\nCalculate a conventional (heteroskedastic) standard error for \\(\\widehat{\\theta}_{\\mathrm{DS}}\\).\n\nBelloni, Chernozhukov, and Hansen (2014b) show that when both (29.22) and (29.23) satisfy an approximate sparsity structure (so that the regressions are well approximated by a finite set of regressors) then the double-selection estimator \\(\\widehat{\\theta}_{\\mathrm{DS}}\\) and its t-ratio are asymptotically normal so conventional inferernce methods are valid. Their proof is technically tedious so not repeated here. The essential idea is that because \\(\\tilde{X}\\) includes the variables in \\(X_{2}\\), the estimator \\(\\widehat{\\theta}_{\\mathrm{DS}}\\) is asymptotically equivalent to the regression where \\(D\\) is replaced with the error \\(V\\) from (29.23). Since \\(V\\) is uncorrelated with the regressors \\(X\\) the estimator and t-ratio satisfy the conventional non-selection asymptotic distribution.\nIt should be emphasized that this distributional claim is asymptotic; finite sample inferences remain distorted from nominal levels. Furthermore, the result rests on the adequacy of the approximate sparsity assumption for both the structural equation (29.22) and the auxillary regression (29.23).\nThe primary advantage of the double-selection estimator is its simplicity and clear intuitive structure.\nIn Stata, the double-selection Lasso estimator can be computed by the dsregress command or with the pdslasso add-on package. Double-selection is available in \\(\\mathrm{R}\\) with the hdm package."
  },
  {
    "objectID": "chpt29-ML.html#post-regularization-lasso",
    "href": "chpt29-ML.html#post-regularization-lasso",
    "title": "27  Machine Learning",
    "section": "27.21 Post-Regularization Lasso",
    "text": "27.21 Post-Regularization Lasso\nA potential improvement on double-selection Lasso is the post-regularization Lasso estimator of Chernozhukov, Hansen, and Spindler (2015), which is labeled as partialing-out Lasso in the Stata manual. The estimator is essentially the same as Robinson (1988) for the partially linear model (see Section 19.24) but estimated by Lasso rather than kernel regression.\nWe first transform the structural equation (29.22) to eliminate the high-dimensional component. Take the expected value of (29.22) conditional on \\(X\\), and subtract from each side. This leads to the equation\n\\[\nY-\\mathbb{E}[Y \\mid X]=(D-\\mathbb{E}[D \\mid X]) \\theta+e .\n\\]\nNotice that this elminates the regressor \\(X\\) and the high-dimensional coefficient \\(\\beta\\). The models (29.23)(29.24) specify \\(\\mathbb{E}[Y \\mid X]\\) and \\(\\mathbb{E}[D \\mid X]\\) as linear functions of \\(X\\). Substituting these expressions we obtain\n\\[\nY-X^{\\prime} \\eta=\\left(D-X^{\\prime} \\gamma\\right) \\theta+e .\n\\]\nIf \\(\\eta\\) and \\(\\gamma\\) were known the coefficient \\(\\theta\\) could be estimated by least squares. As \\(\\eta\\) and \\(\\gamma\\) are unknown they need to be estimated. Chernozhukov, Hansen, and Spindler (2015) recommend estimation by Lasso or post-Lasso, separately for \\(Y\\) and \\(D\\).\nThe estimator recommended by Chernozhukov, Hansen, and Spindler (2015) is:\n\nEstimate (29.23) by Lasso or post-Lasso with Lasso parameter \\(\\lambda_{1}\\). Let \\(\\widehat{\\gamma}\\) be the coefficient estimator and \\(\\widehat{V}_{i}=D_{i}-X_{i}^{\\prime} \\widehat{\\gamma}\\) the residual.\nEstimate (29.24) by Lasso or post-Lasso with Lasso parameter \\(\\lambda_{2}\\). Let \\(\\widehat{\\eta}\\) be the coefficient estimator and \\(\\widehat{U}_{i}=Y_{i}-X_{i}^{\\prime} \\widehat{\\eta}\\) the residual.\nLet \\(\\widehat{\\theta}_{\\mathrm{PR}}\\) be the OLS coefficient from the regression of \\(\\widehat{U}\\) on \\(\\widehat{V}\\).\nCalculate a conventional (heteroskedastic) standard error for \\(\\widehat{\\theta}_{\\mathrm{PR}}\\).\n\nChernozhukov, Hansen, and Spindler (2015) introduce the following insight to understand why \\(\\widehat{\\theta}_{\\mathrm{PR}}\\) may be relatively insensitive to post-model-selection. The reason why model selection invalidates inference is because when the variables \\(D\\) and \\(X\\) are correlated the moment condition for \\(\\theta\\) is sensitive to \\(\\beta\\). Specifically, the moment condition for \\(\\theta\\) based on (29.22) is\n\\[\nm(\\theta, \\beta)=\\mathbb{E}\\left[D\\left(Y-D \\theta-X^{\\prime} \\beta\\right)\\right]=0 .\n\\]\nIts sensitivity with respect to \\(\\beta\\) is its derivative evaluated at the true coefficients\n\\[\n\\frac{\\partial}{\\partial \\beta} m(\\theta, \\beta)=-\\mathbb{E}\\left[D X^{\\prime}\\right]\n\\]\nwhich is non-zero when \\(D\\) and \\(X\\) are correlated. This means that inclusion/exclusion of the variable \\(X\\) has an impact on the moment condition for \\(\\theta\\) and hence its solution.\nIn contrast, the moment condition for \\(\\theta\\) based on (29.25) is\n\\[\n\\begin{aligned}\nm_{\\mathrm{PR}}(\\theta, \\beta) &=\\mathbb{E}\\left[\\left(D-X^{\\prime} \\gamma\\right)\\left(Y-X^{\\prime} \\eta-\\left(D-X^{\\prime} \\gamma\\right) \\theta\\right)\\right] \\\\\n&=\\mathbb{E}\\left[\\left(D-X^{\\prime} \\gamma\\right)\\left(Y-D \\theta-X^{\\prime} \\beta\\right)\\right] .\n\\end{aligned}\n\\]\nIts sensitivity with respect to \\(\\beta\\) is\n\\[\n\\frac{\\partial}{\\partial \\beta} m_{\\mathrm{PR}}(\\theta, \\beta)=-\\mathbb{E}\\left[\\left(D-X^{\\prime} \\gamma\\right) X^{\\prime}\\right]=-\\mathbb{E}\\left[V X^{\\prime}\\right]=0 .\n\\]\nThis equals zero because \\(V\\) is a regression error as specified in (29.23) and thus uncorrelated with \\(X\\). Since the sensitivity of \\(m_{\\mathrm{PR}}(\\theta, \\beta)\\) with respect to \\(\\beta\\) is zero, inclusion/exclusion of the variable \\(X\\) has only a mild impact on the moment condition for \\(\\theta\\) and its estimator.\nThese insights are formalized in the following distribution theory.\nTheorem 29.5 Suppose model (29.22)-(29.23) holds and Assumption \\(29.1\\) holds for both \\(\\beta\\) and \\(\\gamma\\). Assume that each regressor has been standardized so that \\(n^{-1} \\boldsymbol{X}_{j}^{\\prime} \\boldsymbol{X}_{j}=1\\). Suppose \\(e \\mid X \\sim \\mathrm{N}\\left(0, \\sigma_{e}^{2}(X)\\right)\\) and \\(V \\mid X \\sim \\mathrm{N}\\left(0, \\sigma_{V}^{2}(X)\\right)\\) where \\(\\sigma_{e}^{2}(x) \\leq \\bar{\\sigma}_{e}^{2}<\\infty\\) and \\(\\sigma_{V}^{2}(x) \\leq \\bar{\\sigma}_{V}^{2}<\\infty\\). For some \\(C_{1}\\) and \\(C_{2}\\) sufficiently large the Lasso parameters satisfy \\(\\lambda_{1}=C_{1} \\sqrt{n \\log p}\\) and \\(\\lambda_{2}=C_{2} \\sqrt{n \\log p}\\). Assume \\(p \\rightarrow \\infty\\) and\n\\[\n\\left(\\|\\beta\\|_{0}+\\|\\gamma\\|_{0}\\right) \\frac{\\log p}{\\sqrt{n}}=o(1) .\n\\]\nThen\n\\[\n\\sqrt{n}\\left(\\widehat{\\theta}_{\\mathrm{PR}}-\\theta\\right) \\underset{d}{\\rightarrow} \\mathrm{N}\\left(0, \\frac{\\mathbb{E}\\left[V^{2} e^{2}\\right]}{\\left(\\mathbb{E}\\left[V^{2}\\right]\\right)^{2}}\\right) .\n\\]\nFurthermore, the standard variance estimator for \\(\\widehat{\\theta}_{\\mathrm{PR}}\\) is consistent for the asymptotic variance.\nFor a proof see Section \\(29.23\\).\nIn order to provide a simple proof, Theorem \\(29.5\\) uses the assumption of normal errors. This is not essential. Chernozhukov, Hansen, and Spindler (2015) state the same distributional result under weaker regularity conditions.\nTheorem \\(29.5\\) shows that the post-regularization (partialing-out) Lasso estimator has a conventional asymptotic distribution, allowing conventional inference for the coefficient \\(\\theta\\). The key rate condition is (29.26), which is stronger than required for Lasso estimation, and identical to (29.19) used for Lasso IV. (29.26) requires that both \\(\\beta\\) and \\(\\gamma\\) are sparse. The condition (29.26) can be relaxed to allow approximate sparsity as in Section \\(29.12\\) at the cost of a more restrictive rate condition. The advantage of the post-regularization estimator \\(\\widehat{\\theta}_{\\mathrm{PR}}\\) over the double-selection estimator \\(\\widehat{\\theta}_{\\mathrm{DS}}\\) is efficiency. The post-regularization estimator uses only the relevant components of \\(X\\) to separately demean \\(Y\\) and \\(D\\), leading to greater parsimony. Different components of \\(X\\) may be relevant to \\(D\\) and \\(Y\\). The post-regularization estimator allows such distinctions and estimates each separately. In contrast, the double-selection estimator uses the union of the two regressor sets for estimation of \\(\\theta\\), leading to a less parsimonious specification. As a consequence, an advantage of the double-selection estimator is reduced bias and robustness. Regarding the theory, the derivation of the asymptotic theory for the post-regularization estimator is considerably easier than that for the double-selection estimator, as it only involves the manipulation of rates of convergence, while the double-selection estimator requires a careful attention to the handling of the union of the regressor sets.\nThe partialing-out Lasso estimator is available with the poregress command in Stata (implemented with post-Lasso estimation only), or with the pdslasso add-on package. Partialing-out Lasso is available in \\(\\mathrm{R}\\) with the hdm package."
  },
  {
    "objectID": "chpt29-ML.html#doubledebiased-machine-learning",
    "href": "chpt29-ML.html#doubledebiased-machine-learning",
    "title": "27  Machine Learning",
    "section": "27.22 Double/Debiased Machine Learning",
    "text": "27.22 Double/Debiased Machine Learning\nThe most recent contribution to inference methods for model (29.22) is the Double/Debiased machine learning (DML) estimator of Chernozhukov, Chetverikov, Demirer, Duflo, Hansen, Newey, and Robins (2018). Our description will focus on linear regression estimated by Lasso, though their treatment is considerably more general. This estimation method has received considerable attention among econometricians in recent years and is considered the state-of-the-art estimation method.\nThe DML estimator extends the post-regularization estimator of the previous section by adding samplesplitting similarly to the split-sample IV estimator (see Section 29.19). The authors argue that this reduces the dependence between the estimation stages and can improve performance.\nAs presented in the previous section, the post-regularization estimator first estimates the coefficients \\(\\gamma\\) and \\(\\eta\\) in the models (29.23) and (29.24) and then estimates the coefficient \\(\\theta\\). The split-sample estimator performs these estimation steps using separate samples. The DML estimator takes this a step further by using K-fold partitioning. The estimation algorithm is as follows.\n\nRandomly partition the sample into \\(K\\) independent folds \\(A_{k}, k=1, \\ldots, K\\), of roughly equal size \\(n / K\\).\nWrite the data matrices for each fold as \\(\\left(\\boldsymbol{Y}_{k}, \\boldsymbol{D}_{k}, \\boldsymbol{X}_{k}\\right)\\).\nFor \\(k=1, \\ldots, K\\)\n\n\nUse all observations except for fold \\(k\\) to estimate the coefficients \\(\\gamma\\) and \\(\\eta\\) in (29.23) and (29.24) by Lasso or post-Lasso. Write these leave-fold-out estimators as \\(\\widehat{\\gamma}_{-k}\\) and \\(\\widehat{\\eta}_{-k}\\).\nSet \\(\\widehat{\\boldsymbol{V}}_{k}=\\boldsymbol{D}_{k}-\\boldsymbol{X}_{k} \\widehat{\\gamma}_{-k}\\) and \\(\\widehat{\\boldsymbol{U}}_{k}=\\boldsymbol{Y}_{k}-\\boldsymbol{X}_{k} \\widehat{\\eta}_{-k}\\). These are the estimated values of \\(V\\) and \\(U\\) for observations in the \\(k^{t h}\\) fold using the leave-fold-out estimators.\n\n 1. Set \\(\\widehat{\\theta}_{\\mathrm{DML}}=\\left(\\sum_{k=1}^{K} \\widehat{\\boldsymbol{V}}_{k}^{\\prime} \\widehat{\\boldsymbol{V}}_{k}\\right)^{-1}\\left(\\sum_{k=1}^{K} \\widehat{\\boldsymbol{V}}_{k}^{\\prime} \\widehat{\\boldsymbol{U}}_{k}\\right)\\). Equivalently, stack \\(\\widehat{\\boldsymbol{V}}_{k}\\) and \\(\\widehat{\\boldsymbol{U}}_{k}\\) into \\(n \\times 1\\) vectors \\(\\widehat{\\boldsymbol{V}}\\) and \\(\\widehat{\\boldsymbol{U}}\\) and set \\(\\widehat{\\theta}_{\\mathrm{DML}}=\\left(\\widehat{\\boldsymbol{V}}^{\\prime} \\widehat{\\boldsymbol{V}}\\right)^{-1}\\left(\\widehat{\\boldsymbol{V}}^{\\prime} \\widehat{\\boldsymbol{U}}\\right)\\).\n\nConstruct a conventional (heteroskedastic) standard error for \\(\\widehat{\\theta}_{\\mathrm{DML}}\\).\n\nThe authors call \\(\\widehat{\\theta}_{\\text {DML }}\\) a cross-fit estimator as in the \\(K=2\\) case it performs sample splitting in both directions and is therefore fully asymptotically efficient. The estimator as described above is labeled the “DML2” estimator by the authors. An alternative they label “DML1” is \\(\\widehat{\\theta}_{\\mathrm{DML} 1}=\\sum_{k=1}^{K}\\left(\\widehat{\\boldsymbol{V}}_{k}^{\\prime} \\widehat{\\boldsymbol{V}}_{k}\\right)^{-1}\\left(\\widehat{\\boldsymbol{V}}_{k}^{\\prime} \\widehat{\\boldsymbol{U}}_{k}\\right)\\). They are asymptotically equivalent but DML2 is preferred.\nThe estimator requires the selection of the number of folds \\(K\\). Similarly to K-fold CV the authors recommend \\(K=10\\). Computational cost is roughly proportional to \\(K\\).\nTheorem 29.6 Under the assumptions of Theorem 29.5,\n\\[\n\\sqrt{n}\\left(\\widehat{\\theta}_{\\mathrm{DML}}-\\theta\\right) \\underset{d}{\\longrightarrow} \\mathrm{N}\\left(0, \\frac{\\mathbb{E}\\left[V^{2} e^{2}\\right]}{\\left(\\mathbb{E}\\left[V^{2}\\right]\\right)^{2}}\\right) .\n\\]\nFurthermore, the standard variance estimator for \\(\\widehat{\\theta}_{\\mathrm{DML}}\\) is consistent for the asymptotic variance.\nTheorem \\(29.6\\) shows that the DML estimator achieves a standard asymptotic distribution. The proof is a straightforward extension of that for Theorem \\(29.5\\) so is omitted. Weaker (but high-level) regularity conditions are provided by Chernozhukov et. al. (2018).\nThe authors argue that the DML estimator has improved sampling performance due to an improved rate of convergence of certain error terms. If we examine the proof of Theorem 29.5, one of the error bounds is (29.44), which shows that\n\\[\n\\left|\\left(\\widehat{\\gamma}_{-k}-\\gamma\\right)^{\\prime} \\frac{1}{\\sqrt{n}} \\boldsymbol{X}_{k}^{\\prime} \\boldsymbol{e}_{k}\\right| \\leq O_{p}\\left(\\|\\gamma\\|_{0} \\frac{\\log p}{\\sqrt{n}}\\right)=o_{p}(1) .\n\\]\nUnder sample splitting, however, we have an improved rate of convergence. The components \\(\\widehat{\\gamma}_{-k}\\) and \\(\\boldsymbol{X}_{k}^{\\prime} \\boldsymbol{e}_{k}\\) are independent. Thus the left side of (29.27), conditional on \\(\\widehat{\\gamma}_{-k}\\) and \\(\\boldsymbol{X}_{k}\\), is mean zero and has conditional variance bounded by \\(\\bar{\\sigma}_{e}^{2}\\left(\\widehat{\\gamma}_{-k}-\\gamma\\right)^{\\prime} \\frac{1}{n} \\boldsymbol{X}_{k}^{\\prime} \\boldsymbol{X}_{k}\\left(\\widehat{\\gamma}_{-k}-\\gamma\\right)\\). This is \\(O_{p}\\left(\\|\\gamma\\|_{0} \\frac{\\log p}{n}\\right)\\) by Theorem 29.3. Hence (29.27) is \\(O_{p}\\left(\\sqrt{\\|\\gamma\\|_{0} \\frac{\\log p}{n}}\\right)\\), which is of smaller order. This improvement suggests that the deviations from the asymptotic approximation should be smaller under sample splitting and the DML estimator. The improvements, however, do not lead to a relaxation of the regularity conditions. The proof requires bounding the terms (29.42)-(29.43) and these are not improved by sample splititng. Consequently, it is unclear if the distributional impact of sample splitting is large or small.\nThe advantage of the DML estimator over the post-regularization estimator is that the sample splitting eliminates the dependence between the two estimation steps, thereby reducing post-model-selection bias. The procedure has several disadvantages, however. First, the estimator is random due to the sample splitting. Two researchers with the same data set but making different random splits will obtain two distinct estimators. This arbitrariness is unsettling. This randomness can be reduced by using a larger value of \\(K\\), but this increases computation cost. Another disadvantage of sample-splitting is that estimation of \\(\\gamma\\) and \\(\\eta\\) is performed using smaller samples which reduces estimation efficiency, though this effect is minor if \\(K \\geq 10\\). Regardless, these considerations suggest that DML may be most appropriate for settings with large \\(n\\) and \\(K \\geq 10\\).\nAt the beginning of this section the DML estimator was described as the “state-of-the-art”. This field is rapidly developing so this specific estimator may be soon eclipsed by a further iteration.\nIn Stata, the DML estimator is available with the xporegress command. By default it implements the DML2 estimator with \\(K=10\\) folds. The coefficients \\(\\gamma\\) and \\(\\eta\\) are estimated by post-Lasso."
  },
  {
    "objectID": "chpt29-ML.html#technical-proofs",
    "href": "chpt29-ML.html#technical-proofs",
    "title": "27  Machine Learning",
    "section": "27.23 Technical Proofs*",
    "text": "27.23 Technical Proofs*\nProof of Theorem 29.2 Combining (29.8) and (29.9) we find that\n\\[\n\\begin{aligned}\n\\operatorname{mse}\\left[\\widehat{\\beta}_{\\text {ridge }} \\mid \\boldsymbol{X}\\right] &=\\operatorname{var}\\left[\\widehat{\\beta}_{\\text {ridge }} \\mid \\boldsymbol{X}\\right]+\\operatorname{bias}\\left[\\widehat{\\beta}_{\\text {ridge }} \\mid \\boldsymbol{X}\\right] \\text { bias }\\left[\\widehat{\\beta}_{\\text {ridge }} \\mid \\boldsymbol{X}\\right]^{\\prime} \\\\\n&=\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}+\\lambda \\boldsymbol{I}_{p}\\right)^{-1}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{D} \\boldsymbol{X}+\\lambda^{2} \\beta \\beta^{\\prime}\\right)\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}+\\lambda \\boldsymbol{I}_{p}\\right)^{-1}\n\\end{aligned}\n\\]\nThe MSE of the least squares estimator is\n\\[\n\\begin{aligned}\n\\operatorname{mse}\\left[\\widehat{\\beta}_{\\mathrm{ols}} \\mid \\boldsymbol{X}\\right] &=\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{D} \\boldsymbol{X}\\right)\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1} \\\\\n&=\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}+\\lambda \\boldsymbol{I}_{p}\\right)^{-1}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}+\\lambda \\boldsymbol{I}_{p}\\right)\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{D} \\boldsymbol{X}\\right)\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}+\\lambda \\boldsymbol{I}_{p}\\right)\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}+\\lambda \\boldsymbol{I}_{p}\\right)^{-1} \\\\\n&=\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}+\\lambda \\boldsymbol{I}_{p}\\right)^{-1}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{D} \\boldsymbol{X}+\\lambda\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{D} \\boldsymbol{X}\\right)+\\lambda\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{D} \\boldsymbol{X}\\right)\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1}\\right.\\\\\n&\\left.+\\lambda^{2}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{D} \\boldsymbol{X}\\right)\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1}\\right)\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}+\\lambda \\boldsymbol{I}_{p}\\right)^{-1} \\\\\n& \\geq\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}+\\lambda \\boldsymbol{I}_{p}\\right)^{-1}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{D} \\boldsymbol{X}+\\lambda\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{D} \\boldsymbol{X}\\right)+\\lambda\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{D} \\boldsymbol{X}\\right)\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1}\\right)\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}+\\lambda \\boldsymbol{I}_{p}\\right)^{-1} .\n\\end{aligned}\n\\]\nTheir difference is\n\\[\n\\operatorname{mse}\\left[\\widehat{\\beta}_{\\text {ols }} \\mid \\boldsymbol{X}\\right]-\\operatorname{mse}\\left[\\widehat{\\beta}_{\\text {ridge }} \\mid \\boldsymbol{X}\\right] \\geq \\lambda\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}+\\lambda \\boldsymbol{I}_{p}\\right)^{-1} \\boldsymbol{A}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}+\\lambda \\boldsymbol{I}_{p}\\right)^{-1}\n\\]\nwhere\n\\[\n\\boldsymbol{A}=\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{D} \\boldsymbol{X}\\right)+\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{D} \\boldsymbol{X}\\right)\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1}-\\lambda \\beta \\beta^{\\prime} .\n\\]\nThe right-hand-side of (29.28) is positive definite if \\(\\boldsymbol{A}>0\\). Its smallest eigenvalue satisfies\n\\[\n\\lambda_{\\min }(\\boldsymbol{A})=2 \\min _{\\alpha^{\\prime} \\alpha=1} \\alpha^{\\prime}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1 / 2}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{D} \\boldsymbol{X}\\right)\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right)^{-1 / 2} \\alpha-\\lambda \\beta^{\\prime} \\beta \\geq 2 \\min _{h^{\\prime} h=1} h^{\\prime} \\boldsymbol{D} h-\\lambda \\beta^{\\prime} \\beta=2 \\underline{\\sigma}^{2}-\\lambda \\beta^{\\prime} \\beta\n\\]\nwhich is strictly positive when \\(0<\\lambda<2 \\underline{\\sigma}^{2} / \\beta^{\\prime} \\beta\\) as assumed. This shows that (29.28) is positive definite.\nProof of Theorem 29.3 Define \\(V_{n j}=n^{-1} \\sum_{i=1}^{n} X_{j i}^{2} \\sigma^{2}\\left(X_{i}\\right)\\). The normality assumption implies that for each \\(j,\\left(n V_{n j}\\right)^{-1 / 2} \\boldsymbol{X}_{j}^{\\prime} \\boldsymbol{e} \\sim \\mathrm{N}(0,1)\\). The Gaussian Tail inequality (B.39) implies that for any \\(x\\)\n\\[\n\\mathbb{P}\\left[\\left|\\frac{1}{\\sqrt{n V_{n j}}} \\boldsymbol{X}_{j}^{\\prime} \\boldsymbol{e}\\right|>x\\right] \\leq 2 \\exp \\left(-\\frac{x^{2}}{2}\\right) .\n\\]\nBy Boole’s inequality (B.24), (29.29), Jensen’s inequality, \\(V_{n j} \\leq \\bar{\\sigma}^{2}\\), and (29.14),\n\\[\n\\begin{aligned}\n\\mathbb{P}\\left[\\left\\|\\frac{1}{n} \\boldsymbol{X}^{\\prime} \\boldsymbol{e}\\right\\|_{\\infty}>\\frac{\\lambda}{4 n} \\mid \\boldsymbol{X}\\right] &=\\mathbb{P}\\left[\\max _{1 \\leq j \\leq p}\\left|\\frac{1}{n} \\boldsymbol{X}_{j}^{\\prime} \\boldsymbol{e}\\right|>\\frac{\\lambda}{4 n} \\mid \\boldsymbol{X}\\right] \\\\\n&=\\mathbb{P}\\left[\\bigcup_{1 \\leq j \\leq p}\\left|\\frac{1}{\\sqrt{n V_{n j}}} \\boldsymbol{X}_{j}^{\\prime} \\boldsymbol{e}\\right|>\\frac{\\lambda}{4 \\sqrt{n V_{n j}}} \\mid \\boldsymbol{X}\\right] \\\\\n& \\leq \\sum_{j=1}^{p}\\left[\\left|\\frac{1}{\\sqrt{n V_{n j}}} \\boldsymbol{X}_{j}^{\\prime} \\boldsymbol{e}\\right|>\\frac{\\lambda}{4 \\sqrt{n V_{n j}}} \\mid \\boldsymbol{X}\\right] \\\\\n& \\leq \\sum_{j=1}^{p} 2 \\exp \\left(-\\frac{\\lambda^{2}}{16 n V_{n j}}\\right) \\\\\n& \\leq 2 p \\exp \\left(-\\frac{C^{2}}{16 \\bar{\\sigma}^{2}} \\log p\\right) \\\\\n&=2 p^{1-C^{2} / 16 \\bar{\\sigma}^{2} .}\n\\end{aligned}\n\\]\nSince \\(p>1\\) this can be made arbitrarily small by selecting \\(C\\) sufficiently large. This shows that\n\\[\n\\left\\|\\frac{1}{n} \\boldsymbol{X}^{\\prime} \\boldsymbol{e}\\right\\|_{\\infty} \\leq \\frac{\\lambda}{4 n}\n\\]\nholds with arbitrarily large probability. The remainder of the proof is algebraic, based on manipulations of the estimation criterion function, conditional on the event (29.31).\nSince \\(\\widehat{\\beta}\\) minimizes \\(\\operatorname{SSE}_{1}(\\beta, \\lambda)\\) it satisfies \\(\\operatorname{SSE}_{1}(\\widehat{\\beta}, \\lambda) \\leq \\operatorname{SSE}_{1}(\\beta, \\lambda)\\) or\n\\[\n(\\boldsymbol{Y}-\\boldsymbol{X} \\widehat{\\beta})^{\\prime}(\\boldsymbol{Y}-\\boldsymbol{X} \\widehat{\\beta})+\\lambda\\|\\widehat{\\beta}\\|_{1} \\leq \\boldsymbol{e}^{\\prime} \\boldsymbol{e}+\\lambda\\|\\beta\\|_{1} .\n\\]\nWriting out the left side, dividing by \\(n\\), and re-arranging and defining \\(R_{n}=(\\widehat{\\beta}-\\beta)^{\\prime} \\boldsymbol{Q}_{n}(\\widehat{\\beta}-\\beta)\\), this implies\n\\[\n\\begin{aligned}\nR_{n}+\\frac{\\lambda}{n}\\|\\widehat{\\beta}\\|_{1} & \\leq \\frac{2}{n} \\boldsymbol{e}^{\\prime} \\boldsymbol{X}(\\widehat{\\beta}-\\beta)+\\frac{\\lambda}{n}\\|\\beta\\|_{1} \\\\\n& \\leq 2\\left\\|\\frac{1}{n} \\boldsymbol{X}^{\\prime} \\boldsymbol{e}\\right\\|_{\\infty}\\|\\widehat{\\beta}-\\beta\\|_{1}+\\frac{\\lambda}{n}\\|\\beta\\|_{1} \\\\\n& \\leq \\frac{\\lambda}{2 n}\\|\\widehat{\\beta}-\\beta\\|_{1}+\\frac{\\lambda}{n}\\|\\beta\\|_{1} .\n\\end{aligned}\n\\]\nThe second inequality is Hölder’s (29.2) and the third holds by (29.31).\nPartition \\(\\widehat{\\beta}=\\left(\\widehat{\\beta}_{0}, \\widehat{\\beta}_{1}\\right)\\) conformably with \\(\\beta=\\left(\\beta_{0}, \\beta_{1}\\right)\\). Using the additivity property of the 1-norm and the fact \\(\\beta_{0}=0\\), the above expression implies\n\\[\n\\begin{aligned}\nR_{n}+\\frac{\\lambda}{2 n}\\left\\|\\widehat{\\beta}_{0}-\\beta_{0}\\right\\|_{1} & \\leq \\frac{\\lambda}{2 n}\\left\\|\\widehat{\\beta}_{1}-\\beta_{1}\\right\\|_{1}+\\frac{\\lambda}{n}\\left(\\left\\|\\beta_{1}\\right\\|_{1}-\\left\\|\\widehat{\\beta}_{1}\\right\\|_{1}\\right) \\\\\n& \\leq \\frac{3 \\lambda}{2 n}\\left\\|\\widehat{\\beta}_{1}-\\beta_{1}\\right\\|_{1}\n\\end{aligned}\n\\]\nthe second inequality using the fact \\(\\|\\beta\\|_{1} \\leq\\left\\|\\widehat{\\beta}_{1}-\\beta_{1}\\right\\|_{1}+\\left\\|\\widehat{\\beta}_{1}\\right\\|_{1}\\) which follows from (29.3).\nAn implication of (29.32) is \\(\\left\\|\\widehat{\\beta}_{0}-\\beta_{0}\\right\\|_{1} \\leq 3\\left\\|\\widehat{\\beta}_{1}-\\beta_{1}\\right\\|_{1}\\). Thus \\(\\widehat{\\beta}-\\beta \\in B\\). A consequence is that we can apply Assumption \\(29.1\\) to obtain\n\\[\nR_{n}=(\\widehat{\\beta}-\\beta)^{\\prime} \\boldsymbol{Q}_{n}(\\widehat{\\beta}-\\beta) \\geq c^{2}\\|\\widehat{\\beta}-\\beta\\|_{2}^{2} .\n\\]\nThis is the only (but key) point in the proof where Assumption \\(29.1\\) is used.\nTogether with (29.32), (29.33) implies\n\\[\n\\begin{aligned}\nc^{2}\\|\\widehat{\\beta}-\\beta\\|_{2}^{2} & \\leq \\frac{3 \\lambda}{2 n}\\left\\|\\widehat{\\beta}_{1}-\\beta_{1}\\right\\|_{1} \\\\\n& \\leq \\frac{3 \\lambda}{2 n}\\left\\|\\widehat{\\beta}_{1}-\\beta_{1}\\right\\|_{2}\\left\\|\\widehat{\\beta}_{1}-\\beta_{1}\\right\\|_{0}^{1 / 2} \\\\\n& \\leq \\frac{3 \\lambda}{2 n}\\|\\widehat{\\beta}-\\beta\\|_{2}\\|\\beta\\|_{0}^{1 / 2} .\n\\end{aligned}\n\\]\nThe second inequality is (29.4). The third is \\(\\left\\|\\widehat{\\beta}_{1}-\\beta_{1}\\right\\|_{2} \\leq\\|\\widehat{\\beta}-\\beta\\|_{2}\\) and \\(\\left\\|\\widehat{\\beta}_{1}-\\beta_{1}\\right\\|_{0}=\\left\\|\\beta_{1}\\right\\|_{0}=\\|\\beta\\|_{0}\\). Rearranging and using (29.14) we obtain\n\\[\n\\|\\widehat{\\beta}-\\beta\\|_{2} \\leq \\frac{3 \\lambda}{2 c^{2} n}\\|\\beta\\|_{0}^{1 / 2}=\\frac{3 C}{2 c^{2}}\\|\\beta\\|_{0}^{1 / 2} \\sqrt{\\frac{\\log p}{n}}\n\\]\nwhich is (29.17) with \\(D=3 C / 2 c^{2}\\). (29.32), (29.4), (29.17) and (29.14) imply\n\\[\n\\begin{aligned}\nR_{n}+\\frac{\\lambda}{2 n}\\left\\|\\widehat{\\beta}_{0}-\\beta_{0}\\right\\|_{1} & \\leq \\frac{3 \\lambda}{2 n}\\left\\|\\widehat{\\beta}_{1}-\\beta_{1}\\right\\|_{2}\\left\\|\\widehat{\\beta}_{1}-\\beta_{1}\\right\\|_{0}^{1 / 2} \\\\\n& \\leq \\frac{3 \\lambda}{2 n}\\|\\widehat{\\beta}-\\beta\\|_{2}\\|\\beta\\|_{0}^{1 / 2} \\\\\n& \\leq \\frac{9 C}{4 c^{2}} \\frac{\\lambda}{n}\\|\\beta\\|_{0} \\sqrt{\\frac{\\log p}{n}} \\\\\n&=\\frac{9 C^{2}}{4 c^{2}}\\|\\beta\\|_{0} \\frac{\\log p}{n}\n\\end{aligned}\n\\]\nThis implies (29.15) with \\(D=9 C^{2} / 4 c^{2}\\).\nEquation (29.34) also implies\n\\[\n\\left\\|\\widehat{\\beta}_{0}-\\beta_{0}\\right\\|_{1} \\leq \\frac{9 C}{2 c^{2}}\\|\\beta\\|_{0} \\sqrt{\\frac{\\log p}{n}} .\n\\]\nUsing (29.4) and (29.17)\n\\[\n\\left\\|\\widehat{\\beta}_{1}-\\beta_{1}\\right\\|_{1} \\leq\\left\\|\\widehat{\\beta}_{1}-\\beta_{1}\\right\\|_{2}\\left\\|\\widehat{\\beta}_{1}-\\beta_{1}\\right\\|_{0}^{1 / 2} \\leq\\|\\widehat{\\beta}-\\beta\\|_{2}\\|\\beta\\|_{0}^{1 / 2} \\leq \\frac{3 C}{2 c^{2}}\\|\\beta\\|_{0} \\sqrt{\\frac{\\log p}{n}} .\n\\]\nHence\n\\[\n\\|\\widehat{\\beta}-\\beta\\|_{1}=\\left\\|\\widehat{\\beta}_{0}-\\beta_{0}\\right\\|_{1}+\\left\\|\\widehat{\\beta}_{1}-\\beta_{1}\\right\\|_{1} \\leq \\frac{6 C}{c^{2}}\\|\\beta\\|_{0} \\sqrt{\\frac{\\log p}{n}}\n\\]\nwhich is (29.16) with \\(D=6 C / c^{2}\\).\nProof of Theorem 29.4 We provide a sketch of the proof. We start with Lasso IV. First, consider the idealized estimator \\(\\widehat{\\beta}=\\left(\\boldsymbol{W}^{\\prime} \\boldsymbol{X}\\right)^{-1}\\left(\\boldsymbol{W}^{\\prime} \\boldsymbol{Y}\\right)\\) where \\(\\boldsymbol{W}=\\boldsymbol{Z} \\Gamma\\). If the distribution of \\(W\\) does not change with \\(n\\) (which holds when the non-zero coefficients in \\(\\Gamma\\) do not change with \\(n\\) ) then \\(\\widehat{\\beta}\\) has the asymptotic distribution (29.20) under standard assumptions. To allow the non-zero coefficients in \\(\\Gamma\\) to change with \\(n\\), Belloni, Chen, Chernozhukov, and Hansen (2012) use a triangular array central limit theory which requires some additional technical conditions. Given this, (29.20) holds if \\(\\boldsymbol{W}\\) can be replaced by the predicted values \\(\\widehat{\\boldsymbol{X}}_{\\text {Lasso }}\\) without changing (29.20). This holds if\n\\[\n\\begin{aligned}\n&\\frac{1}{n}\\left(\\widehat{\\boldsymbol{X}}_{\\text {Lasso }}-\\boldsymbol{W}\\right)^{\\prime} \\boldsymbol{X} \\underset{p}{\\longrightarrow} 0 \\\\\n&\\frac{1}{\\sqrt{n}}\\left(\\widehat{\\boldsymbol{X}}_{\\text {Lasso }}-\\boldsymbol{W}\\right)^{\\prime} \\boldsymbol{e} \\underset{p}{\\longrightarrow} 0 .\n\\end{aligned}\n\\]\nFor simplicity assume that \\(k=1\\). Theorem \\(29.3\\) shows that under the regularity conditions for the Lasso applied to the reduced form,\n\\[\n\\left|\\frac{1}{n}\\left(\\widehat{\\boldsymbol{X}}_{\\text {Lasso }}-\\boldsymbol{W}\\right)^{\\prime}\\left(\\widehat{\\boldsymbol{X}}_{\\text {Lasso }}-\\boldsymbol{W}\\right)\\right|=(\\widehat{\\Gamma}-\\Gamma)^{\\prime}\\left(\\frac{1}{n} \\boldsymbol{Z}^{\\prime} \\boldsymbol{Z}\\right)(\\widehat{\\Gamma}-\\Gamma) \\leq O_{p}\\left(\\|\\Gamma\\|_{0} \\frac{\\log p}{n}\\right)\n\\]\nand\n\\[\n\\|\\widehat{\\Gamma}-\\Gamma\\|_{1} \\leq O_{p}\\left(\\|\\Gamma\\|_{0} \\sqrt{\\frac{\\log p}{n}}\\right) .\n\\]\nSimilar to (29.30), under sufficient regularity conditions\n\\[\n\\left\\|\\frac{1}{\\sqrt{n}} \\boldsymbol{Z}^{\\prime} \\boldsymbol{e}\\right\\|_{\\infty}=O_{p}(\\sqrt{\\log p}) .\n\\]\nBy the Schwarz inequality and (29.37)\n\\[\n\\begin{aligned}\n\\left|\\frac{1}{n}\\left(\\widehat{\\boldsymbol{X}}_{\\text {Lasso }}-\\boldsymbol{W}\\right)^{\\prime} \\boldsymbol{X}\\right| & \\leq\\left|\\frac{1}{n}\\left(\\widehat{\\boldsymbol{X}}_{\\text {Lasso }}-\\boldsymbol{W}\\right)^{\\prime}\\left(\\widehat{\\boldsymbol{X}}_{\\text {Lasso }}-\\boldsymbol{W}\\right)\\right|^{1 / 2}\\left|\\frac{1}{n} \\boldsymbol{X}^{\\prime} \\boldsymbol{X}\\right|^{1 / 2} \\\\\n& \\leq O_{p}\\left(\\|\\Gamma\\|_{0} \\frac{\\log p}{n}\\right)^{1 / 2} \\leq o_{p}(1)\n\\end{aligned}\n\\]\nthe final inequality under (29.19). This establishes (29.35).\nBy the Hölder inequality (29.2), (29.38), and (29.39),\n\\[\n\\begin{aligned}\n\\left|\\frac{1}{\\sqrt{n}}\\left(\\widehat{\\boldsymbol{X}}_{\\text {Lasso }}-\\boldsymbol{W}\\right)^{\\prime} \\boldsymbol{e}\\right| &=\\left|(\\widehat{\\Gamma}-\\Gamma)^{\\prime} \\frac{1}{\\sqrt{n}} \\boldsymbol{Z}^{\\prime} \\boldsymbol{e}\\right| \\\\\n& \\leq\\|\\widehat{\\Gamma}-\\Gamma\\|_{1}\\left\\|\\frac{1}{\\sqrt{n}} \\boldsymbol{Z}^{\\prime} \\boldsymbol{e}\\right\\|_{\\infty} \\\\\n& \\leq O_{p}\\left(\\|\\Gamma\\|_{0} \\sqrt{\\frac{\\log p}{n}}\\right) O_{p}(\\sqrt{\\log p}) \\\\\n&=O_{p}\\left(\\|\\Gamma\\|_{0} \\frac{\\log p}{\\sqrt{n}}\\right) \\\\\n& \\leq o_{p}(1)\n\\end{aligned}\n\\]\nthe final inequality under (29.19). This establishes (29.36).\nNow consider Lasso SSIV. The steps are essentially the same except for (29.40). For this we use the fact that \\(\\widehat{\\Gamma}_{\\text {Lasso }, A}\\) is independent of \\(\\boldsymbol{Z}_{B}^{\\prime} \\boldsymbol{e}_{B}\\). Let \\(\\boldsymbol{D}_{B}=\\operatorname{diag}\\left(\\mathbb{E}\\left[e_{i}^{2} \\mid Z_{i}\\right]\\right)\\) for sample \\(B\\) and assume \\(\\mathbb{E}\\left[e^{2} \\mid Z\\right] \\leq\\) \\(\\bar{\\sigma}^{2}<\\infty\\). Conditionally on \\(A\\) and \\(\\boldsymbol{Z}_{B}\\)\n\\[\n\\begin{aligned}\n\\operatorname{var}\\left[\\frac{1}{\\sqrt{n}}\\left(\\widehat{\\boldsymbol{X}}_{\\text {Lasso }, B}^{\\prime}-\\boldsymbol{W}_{B}\\right)^{\\prime} \\boldsymbol{e}_{B} \\mid A, \\boldsymbol{Z}_{B}\\right] &=\\operatorname{var}\\left[\\left(\\widehat{\\Gamma}_{\\text {Lasso, } A}-\\Gamma\\right)^{\\prime} \\frac{1}{\\sqrt{n}} \\boldsymbol{Z}_{B}^{\\prime} \\boldsymbol{e}_{B} \\mid A, \\boldsymbol{Z}_{B}\\right] \\\\\n&=\\left(\\widehat{\\Gamma}_{\\text {Lasso }, A}-\\Gamma\\right)^{\\prime} \\frac{1}{n} \\boldsymbol{Z}_{B}^{\\prime} \\boldsymbol{D} \\boldsymbol{Z}_{B}\\left(\\widehat{\\Gamma}_{\\text {Lasso }, A}-\\Gamma\\right) \\\\\n& \\leq \\bar{\\sigma}^{2}\\left(\\widehat{\\Gamma}_{\\text {Lasso }, A}-\\Gamma\\right)^{\\prime} \\frac{1}{n} \\boldsymbol{Z}_{B}^{\\prime} \\boldsymbol{Z}_{B}\\left(\\widehat{\\Gamma}_{\\text {Lasso }, A}-\\Gamma\\right) \\\\\n&=O_{p}\\left(\\|\\Gamma\\|_{0} \\frac{\\log p}{n}\\right) \\\\\n& \\leq o_{p}(1)\n\\end{aligned}\n\\]\nthe final bounds by (29.37) and (29.21). Thus \\(n^{-1 / 2}\\left(\\widehat{\\boldsymbol{X}}_{\\mathrm{Lasso}, B}^{\\prime}-\\boldsymbol{W}_{B}\\right)^{\\prime} \\boldsymbol{e}_{B} \\underset{p}{\\longrightarrow} 0\\) as needed.\nProof of Theorem 29.5 The idealized estimator \\(\\widehat{\\theta}_{\\mathrm{PR}}=\\left(\\boldsymbol{V}^{\\prime} \\boldsymbol{V}\\right)^{-1}\\left(\\boldsymbol{V}^{\\prime} \\boldsymbol{U}\\right)\\) satisfies\n\\[\n\\sqrt{n}\\left(\\widehat{\\theta}_{\\mathrm{PR}}-\\theta\\right)=\\left(n^{-1} \\boldsymbol{V}^{\\prime} \\boldsymbol{V}\\right)^{-1}\\left(n^{-1 / 2} \\boldsymbol{V}^{\\prime} \\boldsymbol{e}\\right)\n\\]\nwhich has the stated asymptotic distribution. The Theorem therefore holds if replacement of \\((\\boldsymbol{V}, \\boldsymbol{U})\\) by \\((\\widehat{\\boldsymbol{V}}, \\widehat{\\boldsymbol{U}})\\) is asymptotically negligible. Since \\(\\boldsymbol{Y}=\\boldsymbol{X} \\eta+\\widehat{\\boldsymbol{V}} \\theta+\\boldsymbol{X}(\\widehat{\\gamma}-\\gamma) \\theta+\\boldsymbol{e}\\)\n\\[\n\\sqrt{n}\\left(\\widehat{\\theta}_{\\mathrm{PR}}-\\theta\\right)=\\sqrt{n} \\frac{\\widehat{\\boldsymbol{V}}^{\\prime} \\widehat{\\boldsymbol{U}}}{\\widehat{\\boldsymbol{V}}^{\\prime} \\widehat{\\boldsymbol{V}}}=\\frac{\\frac{1}{\\sqrt{n}} \\widehat{\\boldsymbol{V}}^{\\prime}(\\widehat{\\boldsymbol{V}} \\theta+\\boldsymbol{X}(\\widehat{\\gamma}-\\gamma) \\theta-\\boldsymbol{X}(\\widehat{\\eta}-\\eta)+\\boldsymbol{e})}{\\frac{1}{n} \\widehat{\\boldsymbol{V}}^{\\prime} \\widehat{\\boldsymbol{V}}} .\n\\]\nThe denominator equals\n\\[\n\\frac{1}{n} \\widehat{\\boldsymbol{V}}^{\\prime} \\widehat{\\boldsymbol{V}}=\\frac{1}{n} \\boldsymbol{V}^{\\prime} \\boldsymbol{V}-2(\\widehat{\\gamma}-\\gamma)^{\\prime} \\frac{1}{n} \\boldsymbol{X}^{\\prime} \\boldsymbol{V}+(\\widehat{\\gamma}-\\gamma)^{\\prime} \\boldsymbol{Q}_{n}(\\widehat{\\gamma}-\\gamma) .\n\\]\nThe numerator equals\n\\[\n\\begin{aligned}\n\\frac{1}{\\sqrt{n}} \\widehat{\\boldsymbol{V}}^{\\prime}(\\widehat{\\boldsymbol{V}} \\theta+\\boldsymbol{X}(\\widehat{\\gamma}-\\gamma) \\theta-\\boldsymbol{X}(\\widehat{\\eta}-\\eta)+\\boldsymbol{e}) &=\\frac{1}{\\sqrt{n}} \\boldsymbol{V}^{\\prime} \\boldsymbol{e}-(\\widehat{\\gamma}-\\gamma)^{\\prime} \\frac{1}{\\sqrt{n}} \\boldsymbol{X}^{\\prime} \\boldsymbol{e}-(\\widehat{\\eta}-\\eta)^{\\prime} \\frac{1}{\\sqrt{n}} \\boldsymbol{X}^{\\prime} \\boldsymbol{V} \\\\\n&+\\theta(\\widehat{\\gamma}-\\gamma)^{\\prime} \\frac{1}{\\sqrt{n}} \\boldsymbol{X}^{\\prime} \\boldsymbol{V}+\\sqrt{n}(\\widehat{\\gamma}-\\gamma)^{\\prime} \\boldsymbol{Q}_{n}(\\widehat{\\eta}-\\eta)-\\theta \\sqrt{n}(\\widehat{\\gamma}-\\gamma)^{\\prime} \\boldsymbol{Q}_{n}(\\widehat{\\gamma}-\\gamma) .\n\\end{aligned}\n\\]\nThe terms on the right side beyond the first are asymptotically negligible because\n\\[\n\\begin{aligned}\n&\\sqrt{n}(\\widehat{\\gamma}-\\gamma)^{\\prime} \\boldsymbol{Q}_{n}(\\widehat{\\gamma}-\\gamma) \\leq O_{p}\\left(\\|\\gamma\\|_{0} \\frac{\\log p}{\\sqrt{n}}\\right)=o_{p}(1) \\\\\n&\\sqrt{n}(\\widehat{\\eta}-\\eta)^{\\prime} \\boldsymbol{Q}_{n}(\\widehat{\\eta}-\\eta) \\leq O_{p}\\left(\\|\\eta\\|_{0} \\frac{\\log p}{\\sqrt{n}}\\right)=o_{p}(1)\n\\end{aligned}\n\\]\nby Theorem \\(29.3\\) and Assumption (29.26),\n\\[\n\\begin{aligned}\n\\sqrt{n}(\\widehat{\\gamma}-\\gamma)^{\\prime} \\boldsymbol{Q}_{n}(\\widehat{\\eta}-\\eta) & \\leq\\left(\\sqrt{n}(\\widehat{\\gamma}-\\gamma)^{\\prime} \\boldsymbol{Q}_{n}(\\widehat{\\gamma}-\\gamma)\\right)^{1 / 2}\\left(\\sqrt{n}(\\widehat{\\eta}-\\eta)^{\\prime} \\boldsymbol{Q}_{n}(\\widehat{\\eta}-\\eta)\\right)^{1 / 2} \\\\\n& \\leq O_{p}\\left(\\|\\gamma\\|_{0}^{1 / 2}\\|\\eta\\|_{0}^{1 / 2} \\frac{\\log p}{\\sqrt{n}}\\right)=o_{p}(1)\n\\end{aligned}\n\\]\nby the Schwarz inequality and the above results, and\n\\[\n\\begin{aligned}\n\\left|(\\widehat{\\gamma}-\\gamma)^{\\prime} \\frac{1}{\\sqrt{n}} \\boldsymbol{X}^{\\prime} \\boldsymbol{e}\\right| & \\leq\\|\\widehat{\\gamma}-\\gamma\\|_{1}\\left\\|\\frac{1}{\\sqrt{n}} \\boldsymbol{X}^{\\prime} \\boldsymbol{e}\\right\\|_{\\infty} \\\\\n& \\leq O_{p}\\left(\\|\\gamma\\|_{0} \\sqrt{\\frac{\\log p}{n}}\\right) O_{p}(\\sqrt{\\log p})=O_{p}\\left(\\|\\gamma\\|_{0} \\frac{\\log p}{\\sqrt{n}}\\right)=o_{p}(1)\n\\end{aligned}\n\\]\nby Hölder’s (29.2), Theorem 29.3, (29.39), and Assumption (29.26). Similarly\n\\[\n\\begin{aligned}\n&(\\widehat{\\gamma}-\\gamma)^{\\prime} \\frac{1}{\\sqrt{n}} \\boldsymbol{X}^{\\prime} \\boldsymbol{V}=o_{p}(1) \\\\\n&(\\widehat{\\eta}-\\eta)^{\\prime} \\frac{1}{\\sqrt{n}} \\boldsymbol{X}^{\\prime} \\boldsymbol{V}=o_{p}(1) .\n\\end{aligned}\n\\]\nTogether we have shown that in (29.41), the replacement of \\((\\widehat{\\boldsymbol{V}}, \\widehat{\\boldsymbol{U}})\\) by \\((\\widehat{\\boldsymbol{V}}, \\widehat{\\boldsymbol{U}})\\) is asymptotically negligible."
  },
  {
    "objectID": "chpt29-ML.html#exercises",
    "href": "chpt29-ML.html#exercises",
    "title": "27  Machine Learning",
    "section": "27.24 Exercises",
    "text": "27.24 Exercises\nExercise 29.1 Prove Theorem 29.1. Hint: The proof is similar to that of Theorem 3.7.\nExercise 29.2 Show that (29.7) is the Mallows criterion for ridge regression. For a definition of the Mallows criterion see Section \\(28.6\\).\nExercise 29.3 Derive the conditional bias (29.8) and variance (29.9) of the ridge regression estimator.\nExercise 29.4 Show that the ridge regression estimator can be computed as least squares applied to an augmented data set. Take the original data \\((\\boldsymbol{Y}, \\boldsymbol{X})\\). Add \\(p\\) ’s to \\(\\boldsymbol{Y}\\) and \\(p\\) rows of \\(\\sqrt{\\lambda} \\boldsymbol{I}_{p}\\) to \\(\\boldsymbol{X}\\), apply least squares, and show that this equals \\(\\widehat{\\beta}_{\\text {ridge }}\\).\nExercise 29.5 Which estimator produces a higher regression \\(R^{2}\\), least squares or ridge regression?\nExercise 29.6 Does ridge regression require that the columns of \\(\\boldsymbol{X}\\) linearly independent? Take a sample \\((\\boldsymbol{Y}, \\boldsymbol{X})\\). Create the augmented regressor set \\(\\widetilde{\\boldsymbol{X}}=(\\boldsymbol{X}, \\boldsymbol{X})\\) (add a duplicate of each regressor) and let \\(\\left(\\widehat{\\beta}_{1}, \\widehat{\\beta}_{2}\\right)\\) be the ridge regression coefficients for the regression of \\(\\boldsymbol{Y}\\) on \\(\\widetilde{\\boldsymbol{X}}\\). Show that \\(\\widehat{\\beta}_{1}=\\widehat{\\beta}_{2}=\\frac{1}{2}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{X}+\\boldsymbol{I}_{p} \\widetilde{\\lambda}\\right)^{-1}\\left(\\boldsymbol{X}^{\\prime} \\boldsymbol{Y}\\right)\\) with \\(\\widetilde{\\lambda}=\\lambda / 2\\).\nExercise 29.7 Repeat the previous question for Lasso regression. Show that the Lasso coefficient estimates \\(\\widehat{\\beta}_{1}\\) and \\(\\widehat{\\beta}_{2}\\) are individually indeterminate but their sum satisfies \\(\\widehat{\\beta}_{1}+\\widehat{\\beta}_{2}=\\widehat{\\beta}_{\\text {Lasso }}\\), the coefficients from the Lasso regression of \\(\\boldsymbol{Y}\\) on \\(\\boldsymbol{X}\\).\nExercise 29.8 You have the continuous variables \\((Y, X)\\) with \\(X \\geq 0\\) and you want to estimate a regression tree for \\(\\mathbb{E}[Y \\mid X]\\). A friend suggests adding a quadratic \\(X^{2}\\) to the variables for added flexibility. Does this make sense?\nExercise 29.9 Take the cpsmar09 dataset and the subsample of Asian women \\((n=1149)\\). Estimate a Lasso linear regression of \\(\\log\\) (wage) on the following variables: education; dummies for education equalling \\(12,13,14,15,16,18\\), and 20; experience/40 in powers from 1 to 9 ; dummies for marriage categories married, divorced, separated, widowed, never married; dummies for the four regions; dummy for union membership. Report the estimated model and coefficients.\nExercise 29.10 Repeat the above exercise using the subsample of Hispanic men \\((n=4547)\\)."
  },
  {
    "objectID": "summary.html#section-depth2",
    "href": "summary.html#section-depth2",
    "title": "Summary",
    "section": "section depth2",
    "text": "section depth2"
  }
]